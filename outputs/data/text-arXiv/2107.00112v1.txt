Using Self-Supervised Feature Extractors with Attention for Automatic
COVID-19 Detection from Speech
John Mendonça1,2 , Rubén Solera-Ureña1 , Alberto Abad1,2 , Isabel Trancoso 1,2
1

2

INESC-ID, Portugal
Instituto Superior Técnico, Universidade de Lisboa, Portugal

{john.mendonca, alberto.abad, isabel.trancoso}@tecnico.ulsiboa.pt,
rsolera@hlt.inesc-id.pt

arXiv:2107.00112v1 [eess.AS] 30 Jun 2021

Abstract
The ComParE 2021 COVID-19 Speech Sub-challenge provides a test-bed for the evaluation of automatic detectors of
COVID-19 from speech. Such models can be of value by providing test triaging capabilities to health authorities, working
alongside traditional testing methods. Herein, we leverage the
usage of pre-trained, problem agnostic, speech representations
and evaluate their use for this task. We compare the obtained
results against a CNN architecture trained from scratch and traditional frequency-domain representations. We also evaluate
the usage of Self-Attention Pooling as an utterance-level information aggregation method. Experimental results demonstrate
that models trained on features extracted from self-supervised
models perform similarly or outperform fully-supervised models and models based on handcrafted features. Our best model
improves the Unweighted Average Recall (UAR) from 69.0%
to 72.3% on a development set comprised of only full-band examples and achieves 64.4% on the test set. Furthermore, we
study where the network is attending, attempting to draw some
conclusions regarding its explainability. In this relatively small
dataset, we find the network attends especially to vowels and
aspirates.
Index Terms: COVID-19, computational paralinguistics, SelfSupervised Features, Attention

1. Introduction
In the wake of the global COVID-19 pandemic, extensive research power has been allocated to the development of reliable
and cost-effective methods of diagnosis. Current methods rely
on intrusive and in-person collections of samples, which may
increase the risk of exposure to the virus. Furthermore, given
the nature of such tests, their scalability may prove limited. As
such, the automatic detection of COVID-19 from speech can assist in mass-testing by providing preliminary test results that do
not require expert manpower, and can be performed without the
need of leaving one’s house, thus maintaining social distancing.
Several investigations based on different datasets have appeared
during the last months, with varied and inconclusive preliminary results. Initiatives such as the ComParE 2021 COVID-19
Speech Sub-challenge (CSS) [1] provide researchers from all
over the world a valuable space to share ideas and findings and
to compare their results in a common test-bed.
Early attempts at detecting speech-related diseases typically involved cumbersome feature engineering. Unlike mainstream Machine Learning tasks such as Speaker Recognition
or ASR (Automatic Speech Recognition), the development of
DNN (Deep Neural Network) -based architectures for paralinguistic tasks has an added limitation that data is frequently

scarce and unbalanced, which is often an impediment to outperforming results from hand-crafted features [2]. As a solution,
researchers typically resort to either data augmentation techniques [3, 4]; or by pre-training models on larger datasets, and
using them as feature extractors [5] or by fine-tuning the original model [6].
Current research on automatic detection of COVID-19 from
speech or respiratory sounds is based on previous work that
show how these are differently affected by distinct respiratory
diseases and can thus be used to detect cold, asthma, pneumonia, tuberculosis, among others [7]. These results have inspired
several ongoing research projects with a machine learning approach to the problem of automatic detection of COVID-19.
The imperative need for properly labeled datasets has been partially addressed by several initiatives, such as those conducted
by the University of Cambridge [8, 9] and the Indian Institute
of Science, Bangalore [10], besides our own on-going efforts at
INESC-ID/IST-University of Lisbon1 .
In [9], the authors leveraged reported symptoms and perform feature-level and decision-level fusion with the OPENSMILE feature. They reported an AUC (Area Under Curve) of
0.79 in a subset of data crowdsourced from a mobile app. Pinkas
et. al [11] proposed a three-stage architecture comprising of (1)
embeddings extracted from Mockingjay [12]; (2) a Recurrent
Neural Network that produces specialised sub-models for classification; (3) an ensemble stacking to fuse predictions. They
reported a Recall of 78% on their self-collected dataset.
Attention mechanisms are often used to present a certain
degree of explainability to models [13]: attention provides a
distribution over attended-to input units, and this is intuitively
presented as portraying the relative importance of inputs. However, their ability to provide transparency for model predictions
has been questioned [14], namely due to its inconsistency with
other feature-importance measures, as well as lack of consistent
outputs on multiple runs given the same prediction. Such conclusions have been refuted on the basis that existence does not
entail exclusivity [15]. Similar to the work conducted in [16],
the attention importance weights visualisations of this paper are
averaged across multiple trials.
Our research contributions are two-fold: (1) we demonstrate the use of transfer-learning methods based on selfsupervised speech representations for COVID-19 detection.
More specifically, we show that aggregating these features
and combining them with a Fully Connected Network results
in comparable performance to handcrafted features and DNN
models trained exclusively for this task; (2) we provide a qualitative analysis of the attention importance weights and evaluate
this information as a path towards explanation.
1 https://www.inesc-id.pt/covid19

This paper is organised as follows: Section 2 describes the
architecture employed, namely the Feature Extractors and Feature Aggregators. Section 3 introduces the methodology used
for the experiments, and includes a description of the COVID19 dataset. Section 4 presents the results of our experiments
and the qualitative analysis of the predictions. Section 5 draws
conclusions and presents directions for future work.

2. Proposed Methods
Our proposed architecture leverages self-supervised feature extractors for COVID-19 prediction. As such, our method is divided into 2 steps: 1) Feature Extraction and 2) Final Prediction
using a Feed Forward Network with Feature Pooling. We use
S3PRL [17], a PyTorch based toolkit, for rapid prototyping with
different feature extractors.
2.1. Feature Extraction
For feature extraction, we extract speech representations every
10ms, with window length set to 25ms. After extraction, the
feature is fed through a Fully Connected Layer paired with a
T anh, outputting a hidden representation of size k.
2.1.1. Spectral Features
Spectrogram a short-time Fourier transform (STFT) with
nf f t = 512 is used to obtain the Raw Spectrogram of the
speech signal, resulting in a feature vector of size 257 (including raw energy); Mel the Mel spectrogram is obtained
with nf req = 201 and 80 mel bins; MFCC the first 13 MelFrequency Cepstral Coefficients (MFCCs) are computed together with first and second order derivatives, resulting in a feature vector of size 39; FBank Log Mel-Filter Bank coefficients
are extracted using 80 mel bins, together with the first and second order derivatives, resulting in a feature vector of size 240.
2.1.2. CPC
Contrastive Predictive Coding (CPC) [18] is an approach for unsupervised learning from high-dimensional data by translating a
generative modelling problem to a classification problem. That
is, it combines predicting future observations (predictive coding) with a probabilistic contrastive loss. This allows for the extraction of representations that are useful for phone and speaker
recognition tasks. Although we did not find any previous work
on the use of CPC as a problem-agnostic feature extractor, contrastive loss as an objective for speech embeddings has been
used for emotion recognition [19]. We extract from the hidden
state of the encoder network feature vectors of dimension 256.
2.1.3. PASE
The Problem-Agnostic Speech Encoder (PASE) [20] encodes
the raw speech waveform into a representation that is fed
to multiple regressors and discriminators (called workers) to
produce meaningful and robust representations. The worker
architecture receives encoded representation to solve seven
self-supervised tasks (including reconstruction of waveform in
an autoencoder fashion, predicting the Log power spectrum,
MFCC and prosodic features). The authors evaluate PASE on
speaker identification, emotion classification, and ASR. For this
task we use PASE+, an improved version of PASE for noisy and
reverberant environments, to extract vectors of dimension 256
from the output of the encoder.

2.1.4. TERA
TERA, which stands for Transformer Encoder Representations from Alteration [21], uses a multi-target auxiliary task
to pre-train Transformer Encoders on a large amount of unlabelled speech. The authors evaluate TERA on phone classification, speaker recognition, and speech recognition, but its selfsupervised nature makes extraction of speech representations or
fine-tuning with downstream models possible. To the best of
our knowledge, no previous research on TERA for paralinguistic tasks exists. We extract the hidden states of the last Transformer Encoder layer, with dimension 768 as features.
2.1.5. Mockingjay
Mockingjay is a Bidirectional Transformer architecture that is
designed to predict the current frame by conditioning on both
past and future contexts [12]. Mockingjay can be considered
equivalent to TERA, when TERA is only using the time objective. The authors evaluate Mockingjay on sentiment analysis
tasks by extracting a feature representation that consists of the
weighted sum of the Transformer hidden representations (denoted LARGE-WS). In our experiments, we also use LARGEWS, which has a dimension of 768.
2.2. Feature Pooling and Prediction
The feature extraction step outputs speech vector representations at the frame level (10ms stride). For prediction, an utterance level representation is required. We hypothesise that
COVID-19, such as other respiratory diseases, affects distinct
parts of the speech production system differently, and may
present itself more distinctively in certain speech sounds. To
this end, we evaluate the inclusion of a Self-Attention Pooling
Layer [22]. This layer is a dot product attention where the keys
and the values correspond to the same representation and the
query is only a trainable parameter. The utterance-level representation can be obtained as follows:
H = sof tmax(Wc X T )X

(1)

where X = [x1 , x2 , ..., xT ] ∈ RT ×k is the sequence of features
of size k and WC ∈ Rk is a trainable parameter.
The choice of attention pooling as a frame weighting mechanism has been successfully applied to whisper detection [23]
and speaker recognition [22], which followed similar motivations to ours. We compare results obtained with Self-Attention
Pooling with a simple Mean pooling layer.
Before prediction, the pooled utterance-level representation
goes through a Dropout layer and Fully Connected Layer to produce a two-dimensional output representing unnormalized logits. We experimented different approaches including adding a
Regularisation layer (with and without Dropout) but found performance degraded with these changes.
2.3. CNN + SAP
As a comparative method, we train a DNN in a fully-supervised
manner. The architecture is a three layer CNN (Convolutional
Neural Network) which tries to find a robust representation of
the original signal. It is comprised of a series of convolutions
followed by an activation (ReLU) and a Dropout layer The architecture receives as input the spectrogram from raw audio
signal (as presented in 2.1.1). Before being fed to the convolutional network, the spectrogram is averaged pooled with a
window and stride of 50ms. The size of the kernels and the
number of channels produced by the convolutional layers are 5

and 160, respectively. Stride and padding are set to 1 and 2,
respectively. The last convolutional layer outputs speech representations of hidden-size 160 that are aggregated using SelfAttention Pooling. Before prediction, the pooled representation
is passed through a Feed-Forward Network, consisting of a Linear layer with an output of size 160, followed by a ReLU. A
final Linear Layer produces a two-dimensional vector of unnormalized logits. Considering this model is trained from scratch,
we augment the training dataset using WavAugment [24]. The
augmentation, which receives as input the raw wave representation, consists of pitch randomization, reverberance and time
clipping, in that order. This augmentation doubles the original
training set.

3. Experimental Set-up
We ran our experiments for 10,000 steps, with evaluation every
200 steps. For all models we train using CrossEntropyLoss as
the criterion. Optimisation is done with AdamW [25], with a
learning rate of 0.0004. For the CNN training, we schedule a
learning rate that decreases linearly from an initial learning rate
of 0.0002 to 0, after a warm-up period of 1400 steps. Batch
size was set to 8 due to hardware limitations. We present results
with the size of the hidden representation k set to {128, 256,
512, 768}.
3.1. COVID-19 SPEECH (C19S) corpus
The COVID-19 SPEECH (C19S) corpus is a curated subset of
the Cambridge COVID-19 Sound database [8, 9], a worldwidecrowdsourced corpus with examples of breathing, coughs and
speech recorded ”in-the-wild”. The C19S corpus contains 893
speech recordings from 366 participants and their corresponding self-reported COVID-19 status labels (positive/negative),
distributed in three speaker-independent and gender-balanced
subsets: Train (72 COVID-19 positives and 243 negatives), Development (142 positives and 153 negatives) and a blind Test set
(283 samples). All the recordings are in PCM format, singlechannel, 16 bits per sample, sampling rate of 16 kHz, and
were normalised in amplitude. In each recording, participants
recorded themselves reading a given prompt (1-3 times).
In our preliminary analysis of the C19S corpus, we noticed
some files present a reduced bandwidth of 4 kHz, hypothetically corresponding to audio samples originally recorded at a
sampling rate of 8 kHz. Namely, 16, 13 and 7 narrow-band files
were detected in the Train, Development and Test subsets, respectively, all of them corresponding to the COVID-19 positive
class 2 . From our analysis of the provided baselines and our own
systems, we consider that this issue could be affecting their performance by making classifiers pay attention to this spurious
condition during training. For this reason, we decided to remove all the narrow-band recordings in the original Train and
Development subsets, even at the cost of reducing the number
of examples in the minority positive class. The resulting dataset
contains 299 samples in the Train subset (56 positives) and 282
samples in the Development subset (129 positives). The Test
subset is kept untouched so as to stick to the original definition and evaluation conditions of the ComParE 2021 CSS Subchallenge. For the rest of the paper, all decisions regarding our
proposal are taken based on development results on the C19Creduced dataset, denoted devf band , although results with the
original C19S dataset (dev) are included for comparison.
2 Similar

issues were found in the cough (C19C) corpus.

4. Results
The results for the models with the best development performance during training in terms of UAR (Unweighted Average
Recall) are summarised in Table 1. We include the baseline
methods [26, 27, 28, 29, 30] for comparison. The best performing models for each class are marked in bold, while the best
overall model for each subset is highlighted in greyscale. We
submit to test our best performers in devf band . The presented
results were obtained using the best k for that feature. Our proposed self-supervised features show competitive results when
compared to the baseline approaches in both dev and devf band .
In fact, FBank-Mean, CPC-SAP and PASE+-SAP outperform
the best baseline model by at least 1.8% UAR in dev. Meanwhile, TERA and Mockingjay are the worse performers, with
similar performances to the ones of Spectral Features. This indicates the Transformer Encoder architecture using time, channel and magnitude reconstruction pre-training objectives fails to
produce meaningful representations for COVID-19 detection,
when compared to CPC and PASE+.
Table 1: Performance results (Unweighted Average RecallUAR) on the COVID-19 SPEECH (C19S) corpus.
Feature
openSMILE
openXBOW
deepSPEC
auDEEP
End2You
Aggregation
Spectogram
Mel
MFCC
FBank
CPC
PASE+
TERA
Mockingjay
CNN

dev
devf band
Baseline Approaches
57.9
58.4
66.3
59.3
56.0
51.6
62.2
58.8
70.5
69.0
Mean SAP Mean
Spectral Features
58.7
63.2
55.5
61.4
64.7
56.5
64.7
63.6
54.3
72.5
63.9
64.3
Self-Supervised Features
62.3
72.1
63.8
70.7
71.8
69.8
59.7
61.3
58.8
60.0
58.2
55.9
Supervised Approaches
68.9
-

test
72.1
68.7
60.4
64.2
68.7

SAP
57.6
59.5
62.7
63.0

-

68.2
66.4
57.3
56.7

56.7
58.6
-

72.3

64.4

The use of Self-Attention Pooling (SAP) fails to bring consistent improvements of results. Our self-supervised features in
dev report small improvements of performance when moving
from Mean to SAP, but when SAP is used in conjunction with
Spectral Features results are mostly worse in dev but better in
devf band . CPC appears to benefit the most from SAP, consistently improving performance in both sets (9.8% and 4.4 % in
dev and devf band , respectively).
We note an overall loss in performance when removing the
narrow-band files in devf band , as expected. The spectral features, in particular, report significant drops. Considering these
features contain information pertaining bandwidth, we believe
these models were using band-detection as means to classify
positive classes. This explains why FBank managed to outperform all other models in dev. Surprisingly, our CNN architecture’s performance improves with the removal of these files.
With respect to our model’s performance in test, we note

Spectrogram devel_021

Spectrogram devel_039

8000

7000

0.175

7000

0.35

6000

0.150

6000

0.30

5000

0.125

5000

0.25

4000

0.100

4000

0.20

3000

0.075

3000

0.15

2000

0.050

2000

0.10

1000

0.025

0.05

0

0.000

1000

1.5

2.0

2.5

Time[s]

3.0

3.5

4.0

Frequency

Attention Weight

Frequency

1.0

0.40

Attention Weight

0.200

8000

0

2

4

6

Time[s]

8

10

0.00

(a) Spectrogram of sentence: ”Confı́o en que mis datos puedan ayudar en (b) Spectrogram of sentence: ”I hope my data can help manage the virus
el manejo de la pandemia vı́rica.”, repeated once.
pandemic.”, repeated 3 times by a non-native speaker.

Figure 1: Attention importance weights (red) plotted against the spectrogram of correctly classified Covid-19 positive subjects. The
attention weights are an average across 5 trials and correspond to the best performing self-supervised model using SAP of devf band .
that there is a drop in performance when compared to results
in dev, contrary to what happens to the baseline approaches,
which report improvements. This can be partially explained by
our models being trained on full-band examples only, which
reduced the number of COVID-19 positive examples.
As detailed in Section 2, one of the hyper-parameters of our
model is the size of the hidden representation k. We present the
performance changes on devf band in Table 2. Overall performance metrics remain consistent through the different values
of k. Although minor, TERA and Mockingjay report improvements when bottlenecking, which can be due to the fact these
features are obtained by concatenating several hidden representations, and thus might contain information overlap.

around the last vowel of ”virus” and the first nasal sound of
”pandemic”; in [4-6]s and [8.10]s the peaks correspond to the
transition between the aspirate /h/ and the vowel (repetition of
”hope”).
The attention importance weights in both examples point
towards the network privileging vowels and aspirates, in English, for detecting COVID-19. The idea that COVID presents
itself in vowel sounds is aligned with previous research [31].
More specifically, COVID-19 disrupts the entrainment of the
vocal folds during phonation, especially for /i/ [32]. Aspirates
deserve a more detailed analysis with more examples, but we
hypothesise these sounds also provide informative cues due to
its similarities with coughs and expirations.

Table 2: Performance Results (UAR) using Self-Supervised Features on devf band with varying k.

5. Conclusions

Pooling
Mean
SAP
Mean
SAP
Mean
SAP
Mean
SAP

k
256
CPC
62.2 63.8
60.8 68.2
PASE+
66.2 65.3
66.4 64.8
TERA
57.5 57.5
56.4 55.8
Mockingjay
55.9 54.1
56.7 55.6
128

512

768

60.3
64.0

61.2
61.7

69.9
65.2

65.5
59.3

58.8
57.3

58.4
57.4

55.2
55.6

54.1
54.5

4.1. Attention: A Qualitative Evaluation
As a qualitative examination of the network’s classification
capabilities, we present average attention importance weights
across 5 trials in Figure 1. It can be observed that the network
focuses itself on certain points of the spectrogram in both examples.
In Figure 1a, the first segment with high attention activity
([0-1]s) corresponds to the Spanish word ”confı́o”. Maximum
attention is paid to the transition between the last two phones.
The second segment ([3.5-4]s) also shows maximum attention
to the transition between the last two vowels in ”pandemia” followed by the first vowel of ”vı́rica”, thus seeming to privilege
the vowel /i/. The remaining smaller peaks also correspond to
other vowel transitions and nasal sounds. Regarding Figure 1b,
in [2-4]s, the attention weights present themselves maximally

In this work, we proposed using Self-Supervised feature extractors for the task of COVID-19 prediction from speech. Results
show that PASE+ and CPC features are able to perform comparably, or even better, to baseline approaches in the development set. Moreover, we employed attention mechanisms as a
feature aggregation method and performed a qualitative analysis of the attention importance weights to draw some intuition
behind the predictions. Our analysis indicate the network privileges features extracted from vowels and aspirates for prediction. However, it is not clear whether models based on these
sounds alone are able to, individually, outperform ones based
on full sentences.
One of the main concerns pertaining automatic detection
of COVID-19 from speech is the performance inconsistencies
across datasets. As future work, we plan to address this problem
using different datasets, including our own COVID-19 crowdsourced dataset, which is still under construction. Additionally, we will assess predictions based only on sustained vowels,
coughs and read/spontaneous speech to better understand individual contributions, and work towards a more succinct COVID
test. An interesting research direction is to bridge this work with
previous research on breathing event detection [33] for this task.

6. Acknowledgements
This work has been supported by national funds through
Fundação para a Ciência e a Tecnologia (FCT), under project
UIDB/50021/2020, and by FEDER, Programa Operacional Regional de Lisboa, Agência Nacional de Inovação and CMU Portugal, under grant BI|2020/091.

7. References
[1] B. W. Schuller, A. Batliner, C. Bergler, C. Mascolo, J. Han,
I. Lefter, H. Kaya, S. Amiriparian, A. Baird, L. Stappen, S. Ottl,
M. Gerczuk, P. Tzirakis, C. Brown, J. Chauhan, A. Grammenos,
A. Hasthanasombat, D. Spathis, T. Xia, P. Cicuta, M. R. Leon J.
J. Zwerts, J. Treep, and C. Kaandorp, “The INTERSPEECH 2021
Computational Paralinguistics Challenge: COVID-19 Cough,
COVID-19 Speech, Escalation & Primates,” in Proc. Interspeech
2021, September 2021, to appear.
[2] J. Wagner, D. Schiller, A. Seiderer, and E. André, “Deep Learning
in Paralinguistic Recognition Tasks: Are Hand-crafted Features
Still Relevant?” in Proc. Interspeech 2018, 2018, pp. 147–151.
[3] S.-L. Yeh, G.-Y. Chao, B.-H. Su, Y.-L. Huang, M.-H. Lin, Y.-C.
Tsai, Y.-W. Tai, Z.-C. Lu, C.-Y. Chen, T.-M. Tai, C.-W. Tseng, C.K. Lee, and C.-C. Lee, “Using Attention Networks and Adversarial Augmentation for Styrian Dialect Continuous Sleepiness and
Baby Sound Recognition,” in Proc. Interspeech 2019, 2019, pp.
2398–2402.
[4] S. Illium, R. Müller, A. Sedlmeier, and C. Linnhoff-Popien,
“Surgical Mask Detection with Convolutional Neural Networks
and Data Augmentations on Spectrograms,” in Proc. Interspeech
2020, 2020, pp. 2052–2056.
[5] B. Milde and C. Biemann, “Using Representation Learning and
Out-of-domain Data for a Paralinguistic Speech Task,” in Proc.
Interspeech 2015, 2015, pp. 904–908.
[6] A. Das and M. Hasegawa-Johnson, “Cross-Lingual Transfer
Learning During Supervised Training in Low Resource Scenarios,” in Proc. Interspeech 2015, 2015, pp. 3531–3535.
[7] G. Gosztolya, R. Busa-Fekete, T. Grósz, and L. Tóth, “DNNBased Feature Extraction and Classifier Combination for ChildDirected Speech, Cold and Snoring Identification,” in Proc. Interspeech 2017, 2017, pp. 3522–3526.
[8] C. Brown, J. Chauhan, A. Grammenos, J. Han, A. Hasthanasombat, D. Spathis, T. Xia, P. Cicuta, and C. Mascolo, “Exploring Automatic Diagnosis of COVID-19 from Crowdsourced Respiratory
Sound Data,” in Proc. of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, 2020, p.
3474–3484.
[9] J. Han, C. Brown, J. Chauhan, A. Grammenos, A. Hasthanasombat, D. Spathis, T. Xia, P. Cicuta, and C. Mascolo, “Exploring
Automatic COVID-19 Diagnosis via voice and symptoms from
Crowdsourced Data,” arXiv preprint:2102.05225, Feb. 2021.
[10] N. Sharma, P. Krishnan, R. Kumar, S. Ramoji, S. R. Chetupalli,
N. R., P. K. Ghosh, and S. Ganapathy, “Coswara — A Database of
Breathing, Cough, and Voice Sounds for COVID-19 Diagnosis,”
in Proc. Interspeech 2020, 2020, pp. 4811–4815.
[11] G. Pinkas, Y. Karny, A. Malachi, G. Barkai, G. Bachar, and
V. Aharonson, “SARS-CoV-2 Detection From Voice,” IEEE Open
Journal of Engineering in Medicine and Biology, vol. 1, pp. 268–
274, 2020.
[12] A. T. Liu, S. w. Yang, P. H. Chi, P. c. Hsu, and H. y. Lee, “Mockingjay: Unsupervised Speech Representation Learning with Deep
Bidirectional Transformer Encoders,” in ICASSP 2020 - 2020
IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), 2020, pp. 6419–6423.
[13] S. Serrano and N. A. Smith, “Is attention interpretable?” in Proc.
of the 57th Annual Meeting of the Association for Computational
Linguistics, Jul. 2019, pp. 2931–2951.
[14] S. Jain and B. C. Wallace, “Attention is not Explanation,” in Proc.
of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Jun. 2019, pp. 3543–
3556.
[15] S. Wiegreffe and Y. Pinter, “Attention is not not Explanation,” in
Proc. of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP), Nov. 2019,
pp. 11–20.

[16] A. D. MacIntyre, G. Rizos, A. Batliner, A. Baird, S. Amiriparian,
A. Hamilton, and B. W. Schuller, “Deep Attentive End-to-End
Continuous Breath Sensing from Speech,” in Proc. Interspeech
2020, 2020, pp. 2082–2086.
[17] A. T. Liu and Y. Shu-wen, “S3PRL: The Self-Supervised
Speech Pre-training and Representation Learning Toolkit,” 2020.
[Online]. Available: https://github.com/s3prl/s3prl
[18] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with
contrastive predictive coding,” arXiv preprint:1807.03748, 2018.
[19] A. Nandan and J. Vepa, “Language Agnostic Speech Embeddings
for Emotion Classification,” 2020.
[20] M. Ravanelli, J. Zhong, S. Pascual, P. Swietojanski, J. Monteiro,
J. Trmal, and Y. Bengio, “Multi-task self-supervised learning for
robust speech recognition,” in ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2020, pp. 6989–6993.
[21] A. T. Liu, S.-W. Li, and H. yi Lee, “TERA: Self-Supervised
Learning of Transformer Encoder Representation for Speech,”
ArXiv, vol. abs/2007.06028, 2020.
[22] P. Safari, M. India, and J. Hernando, “Self-Attention Encoding
and Pooling for Speaker Recognition,” in Proc. Interspeech 2020,
2020, pp. 941–945.
[23] A. R. Naini, M. Satyapriya, and P. K. Ghosh, “Whisper Activity
Detection Using CNN-LSTM Based Attention Pooling Network
Trained for a Speaker Identification Task,” in Proc. Interspeech
2020, 2020, pp. 2922–2926.
[24] E. Kharitonov, M. Rivière, G. Synnaeve, L. Wolf, P.-E. Mazaré,
M. Douze, and E. Dupoux, “Data Augmenting Contrastive Learning of Speech Representations in the Time Domain,” in SLT 2020
- IEEE Spoken Language Technology Workshop, 2020.
[25] I. Loshchilov and F. Hutter, “Decoupled Weight Decay Regularization,” in International Conference on Learning Representations, 2018.
[26] F. Eyben, F. Weninger, F. Gross, and B. Schuller, “Recent Developments in OpenSMILE, the Munich Open-Source Multimedia
Feature Extractor,” in Proc. of the 21st ACM International Conference on Multimedia, ser. MM ’13, 2013, p. 835–838.
[27] M. Schmitt and B. Schuller, “OpenXBOW: Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit,” J. Mach.
Learn. Res., vol. 18, no. 1, p. 3370–3374, Jan. 2017.
[28] S. Amiriparian, M. Gerczuk, S. Ottl, N. Cummins, M. Freitag,
S. Pugachevskiy, A. Baird, and B. Schuller, “Snore Sound Classification Using Image-Based Deep Spectrum Features,” in Proc.
Interspeech 2017, pp. 3512–3516.
[29] M. Freitag, S. Amiriparian, S. Pugachevskiy, N. Cummins, and
B. Schuller, “AuDeep: Unsupervised Learning of Representations
from Audio with Deep Recurrent Neural Networks,” J. Mach.
Learn. Res., vol. 18, no. 1, p. 6340–6344, Jan. 2017.
[30] P. Tzirakis, S. Zafeiriou, and B. W. Schuller, “End2You – The Imperial Toolkit for Multimodal Profiling by End-to-End Learning,”
2018.
[31] T. F. Quatieri, T. Talkar, and J. S. Palmer, “A Framework for
Biomarkers of COVID-19 Based on Coordination of SpeechProduction Subsystems,” IEEE Open Journal of Engineering in
Medicine and Biology, vol. 1, pp. 203–206, 2020.
[32] M. A. Ismail, S. Deshmukh, and R. Singh, “Detection of
COVID-19 through the analysis of vocal fold oscillations,” arXiv
preprint:2010.10707, 2020.
[33] J. Mendonça, F. Teixeira, I. Trancoso, and A. Abad, “Analyzing
Breath Signals for the Interspeech 2020 ComParE Challenge,” in
Proc. Interspeech 2020, 2020, pp. 2077–2081.

