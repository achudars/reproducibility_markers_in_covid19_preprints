Impact of the COVID-19 outbreak on Italy’s country reputation and
stock market performance: a sentiment analysis approach
Gianpaolo Zammarchi1, Francesco Mola1, Claudio Conversano1*

Abstract
During the recent Coronavirus disease 2019 (COVID-19) outbreak, the microblogging service
Twitter has been widely used to share opinions and reactions to events. Italy was one of the first
European countries to be severely affected by the outbreak and to establish lockdown and stay-athome orders, potentially leading to country reputation damage. We resort to sentiment analysis to
investigate changes in opinions about Italy reported on Twitter before and after the COVID-19
outbreak. Using different lexicons-based methods, we find a breakpoint corresponding to the date of
the first established case of COVID-19 in Italy that causes a relevant change in sentiment scores used
as proxy of the country reputation. Next, we demonstrate that sentiment scores about Italy are strongly
associated with the levels of the FTSE-MIB index, the Italian Stock Exchange main index, as they
serve as early detection signals of changes in the values of FTSE-MIB. Finally, we make a contentbased classification of tweets into positive and negative and use two machine learning classifiers to
validate the assigned polarity of tweets posted before and after the outbreak.

Keywords: sentiment analysis, Twitter, COVID-19, stock market performance, country reputation,
machine learning

1. Introduction
Twitter is a microblogging and social networking service widely used by users to interact and publish
contents in response to events. Twitter is being largely used to share information and express
sentiment and concerns on the recent Coronavirus disease 2019 (COVID-19) outbreak, which was
first identified in December 2019 in Wuhan, Hubei, China, and resulted in a serious threat to public
health worldwide [16]. Italy was one of the first European countries to be severely affected by the
outbreak as well as to implement extraordinary measures to limit viral transmission, such as lockdown
and stay-at-home orders [30]. This situation might have led to extensive concerns towards Italy,
potentially leading to country reputation damage, loss of investments and tourism flows. In particular,
since country reputation is usually studied in terms of strategic public diplomacy, effective nation
building and nation branding (see, for example, [42]), it is very likely that a dramatic event like the
COVID-19 outbreak negatively affects all the three dimensions of the reputation of a country. In the
following, we focus specifically on the third dimension (national branding) but concentrate on the
reputation of Italy perceived by Twitter’s users, the latter measured though sentiment analysis.
Sentiment analysis [26], sometimes referred to as opinion mining or polarity classification, is aimed
at analyzing and classifying text into sentiments with a polarity or specific emotions using different
approaches. In today’s world, communication has become more and more bidirectional, involving
1

Department of Economics and Business Science, University of Cagliari, Cagliari, Italy
* Corresponding author information: Claudio Conversano, Department of Economics and Business Science, University
of Cagliari, Cagliari, Italy. e-mail: conversa@unica.it
Preprint

not only a company or other entities sending messages/information to people but also vice versa. The
widespread use of social networks allows billions of people from around the world to have a direct
interaction with companies, actors/actress, sportsmen/sportswomen or politicians, just to give some
examples. Having a tool able to understand what a text means without the need of a human
interpretation is of huge value, e.g. for a company who needs to reply to thousands of e-mails, letters,
messages or any other type of communication. Social media could be a huge source of information,
but the sheer number of messages exchanged every day makes it very difficult for any company or
organization to extract knowledge from them in a quick way. For example, it would be useful to know
that, due to a given event, many people in a specific area of the world need something. We might add
that this information could be useful in different situations, e.g. a company trying to intercept the
unexpressed needs of potential customers but also a humanitarian organization willing to help people
in areas struck by war or natural disasters. Although these two situations can be thought as very distant
cases, they share the importance of correctly interpreting a message. This is a very hard task and
sentiment analysis tackles only a part of it, as it is focused on polarity and strength of a text. Moreover,
sentiment analysis usually has two (positive, negative) or three (positive, neutral, negative) possible
outcomes and can be performed at three different levels: document level, sentence level and aspect
level. At a document level, a document can be thought as a collection of sentences (e.g. a product
review or a book chapter). Each sentence is evaluated separately and then aggregated to express an
overall sentiment. At a sentence level, the evaluation is made for every single sentence, resulting in
a more fine-grained analysis than document level. Finally, at an aspect level, the analysis can be
performed for every single aspect of interest. For example, in a product review we might be interested
in price, package, delivery and many other aspects. Every aspect is independent since we can have a
positive sentiment for price and a negative sentiment for package in the same product review.
There are two main approaches to perform sentiment analysis: a lexicon-based approach and a
machine learning approach. The former uses a list of words (lexicon) associated with a specific
sentiment polarity. The sentiment of some document, sentence or text in general is given by the
number and strength of positively/negatively associated word in that text [40]. It is possible to use an
existing lexicon or to build one. The machine learning approach can instead be thought as building a
model to accomplish a supervised classification task. Several models/classifiers can be used, e.g.
naïve Bayes (NB), Support Vector Machine (SVM), logistic regression, decision trees, neural
networks, etc. They work differently and might have different performance, but they all share the
same steps: collect, label and process data, split dataset into training and test set, build the model on
the training set, assess model performance.
In the above-described framework, the main contributions of this study are summarized as follows:
1) Analyze the temporal evolution of the sentiment towards Italy before and during the COVID-19
outbreak through a sentiment analysis on tweets. Our goal is to show that right after media started
spreading the news of the first Italian case of COVID-19 the sentiment towards Italy and the perceived
country reputation, in particular the national brand, began to drop significantly. We show, at the end
of Section 3.2, that whilst all negative emotions increase, the positive ones have different behavior,
linked to the specific type of emotion considered.
2) Compare the changes in tweets’ sentiment with the evolution of stock exchange levels
corresponding to the prices of the main Italian stock exchange index (FTSE-MIB). We follow the line
of research investigating about the ability of sentiment analysis to predict stock market movements
(see, for example, [25]) and assume the polarity of a sentiment expressed on Twitter towards Italy

2

can reverberate on several aspects of the life of a country, including the performance of the stock
market which is known to be affected by exogenous factors.
3) Evaluate the performance of different machine learning classifiers to validate the polarity of tweets
posted before and after the COVID-19 outbreak. We use two of the most popular models (NB and
SVM) to perform the task of validating a previously-made content-based classification of tweets into
positive or negative. We assume that important differences in classification accuracy observed in the
two periods further support the idea of important changes in the contents of the tweets as tweets
posted in the period following the outbreak are more concentrated on topics or sentiments related to
the ongoing spread of the pandemic as well as on the negative consequences it generates for the
country.
The rest of the paper is organized as follows. Section 2 introduces previous studies performing
sentiment analysis on tweets in response to real-world important events. Section 3 describes lexiconbased sentiment analysis to identify shifts in the sentiment towards Italy in the different phases of the
COVID-19 outbreak. In Section 4, shifts in sentiment polarity are compared with changes in stock
exchange market performance. Performances of two machine learning classifiers in the classification
of tweets are compared in Section 5 and the conclusions of the study are presented in Section 6.

2. Literature review
A number of studies perform sentiment analysis on Twitter data in response to important events. This
kind of analysis is considered a good way to measure public opinion since users are free to express
their thoughts about any topic having a (potentially) large audience (see, for example [27, 38]). Here,
we focus on those related to important events (usually negative) like migrant crisis, terrorist attacks
and disease outbreaks.
In 2015, about one million refugees and migrants came to Europe due to war, disease, terrorism,
natural disasters as the most frequent causes [41]. Most people first arrived in south Europe (e.g. Italy
or Greece, closest to the northern African costs) and then moved to northern European countries (e.g.
Germany). Backfried and Shalunts [2] observe shifts in public perception and media coverage with
respect to the growth in number of migrants arriving in Europe. Pope and Griffith [28] collect data in
English and German to assess if any difference in sentiment could be found comparing tweets in
people speaking the two languages. Öztürk and Ayvaz [24] use a similar approach comparing
approximately 350,000 tweets in English and Turkish during the Syrian refugee crisis. Authors report
that people in Turkey were more concerned about what was happening near their country, while
English-speaking people paid more attention to the political implications of the event.
A number of studies use different methods, including machine learning classifiers, to analyze Twitter
data with respect to public opinions and concerns on terrorism attacks. The study from Cheong and
Lee [5] is one of the first to analyze tweets in a terrorism-related scenario. Given the large number of
users and the ease of sharing texts, pictures and videos, people might use Twitter to share news about
the attack, ask for help, etc. For example, during the Mumbai attacks in 2008 or the Jakarta attacks in
2009, the news was first reported by Twitter users [5]. The authors of the referred study propose an
approach comprising four phases: a) scanning latest trends in Twitter to identify topics to be further
inspected, b) collect as many tweets as possible about that topic, c) extract knowledge from tweets
collected in the previous phase and d) summarize and visualize the information through reports.

3

Simon et al. [34] consider on the attack at the Westgate Mall in Kenya, focusing on the importance
of a coordinated system to avoid having multiple information sources (e.g. too many different
hashtags for the attack or responders), while Burnap et al. [4] analyze how information spreads in a
social network after a terroristic attack, using the Woolwick terrorist attack as a case study. They
collect more than 400,000 tweets and use a negative binomial regression method to study the
dimension and survival time of information related to the attack, showing that tweet sentiment is able
to predict dimension and survival time of the information flow [4]. Ashcroft et al. [1] train a machine
learning classifier to predict whether or not a message is supporting jihadists groups. Since many
terroristic groups (e.g. ISIS) use Twitter to communicate and spread propaganda, it would be very
useful to stop this flow at early stages. They use three types of features (stylometric, time-based and
sentiment-based) to classify tweets using different methods (SVM, NB and Adaboost). They present
promising results supporting the usefulness of an automated approach to aid analyst to detect radical
contents on social networks. Güneyli et al. [14] collect tweets from six Turkish political leaders in
2015. In this case study, the focus is on the presence of the terrorism topic in tweets and the attitude
of Turkish leaders towards terrorism is described, which results to be the most discussed topic during
the election campaign. Garg et al. [12] focus on the Uri (India) terroristic attack in 2016. Almost
60,000 tweets are collected in a one-month period using different hashtags. Tweet’s sentiment is
analyzed with SVM and NB classifiers and data are further analyzed in terms of reach and retweets,
showing how negative tweets have a longer surviving time than positive ones, even if they are reduced
in number. Harb et al. [15] examine the emotional response to two terrorist attacks occurred in the
UK (Manchester and London) in 2017. They collect and annotate a few hundred tweets as regard to
four negative emotions (anger, fear, sadness and disgust) plus surprise and a residual category (none),
and use two deep learning models [Convolutional Neural Network (CNN) and Long Short-Term
Memory Network (LSTM)] trained with different datasets (e.g. an existing pre-labeled dataset, a
dataset automatically labeled using hashtags, etc.). Classifier accuracy is similar for both deep
learning models with an F1-score of 63%. Conde-Cespedes et al. [8] try to solve the problem of the
identification of accounts violating Twitter rules. Having an automated system to accomplish such a
task would be a great benefit in identifying potential threats. Approximately, 200,000 tweets in
different languages (mostly Arabic tweets translated in English) are collected before and after the
Paris terroristic attacks in 2015. These tweets are classified into two categories: pro-ISIS or neutral.
Using a combination of keywords and sentiment analysis scores, SVM is able to reach over 90% of
accuracy.
Finally, several studies focus on the analysis of tweets in response to disease outbreaks. Chunara et
al. [7] analyze data from news media, Twitter and official reports by the government during the first
100 days of the 2010 Haitian cholera outbreak. This study shows how Twitter and other sources can
be used complementary to data provided by government or health institutions. Specifically, trends in
volume of informal sources are significantly correlated with official case data and are available up to
two weeks earlier, thus being potentially useful to provide timely estimates of outbreaks dynamics.
Internet represents one of the major sources used by people to obtain information during the H1N1
influenza virus outbreak [19]. In 2010, Chew and Eysenbach [6] collect about two million tweets
related to this outbreak and show that sentiment analysis performed on Twitter data is a valid tool to
measure public perception, allowing health authorities to address real as well as perceived concerns.
This is particularly important considering that misinformation might allow a disease to spread more
quickly, with a significant cost in health and human lives, while correct information might contribute
to adopt behavioral changes (e.g. social distancing), especially in the initial stage of an outbreak when
a vaccine is not available. Signorini et al. [33] show other ways in which tweets analysis is useful
4

during the outbreak. Specifically, the extraction of information from a live stream of tweets allows to
early detect hot spots. Importantly, Szomszor et al. [37] collect about three million tweets and use
them to detect trends of infection spreading one week earlier compared to the official reports, showing
how Twitter data analysis is useful as an early warning detection system. Smith et al. [35] analyze
tweets posted during the flu season in US in 2012/2013. They use machine learning models to separate
tweets about flu awareness from tweets about the infection, demonstrating that these two types of
tweets show different trends. For instance, they document that levels of awareness drop after the peak,
even when infection levels are still high. Using a different approach, Broniatowski et al. [3] analyze
Twitter data posted during the same flu season to build a model able to distinguish between tweets
reporting an infection from genetic tweets mentioning flu. Using this tool, they are able to predict
changes of influenza prevalence with an accuracy of 85%.
From 2014, several studies used Twitter data to analyze reactions to the Ebola outbreak in Africa. A
number of these articles focus on the perception of the disease among people living in Western
countries (e.g. US). Fung et al. [11] show that, despite the disease outbreak was far away from the
US (only few cases hit the US, while the large majority of cases were in Guinea, Sierra Leone and
Liberia), people were very concerned about their own safety. This is proved by the high levels of
anxiety, anger and other negative emotions that were significantly higher than those observed during
the influenza outbreak. Lazard et al. [20] use data collected during a Center for Disease Control
(CDC) live Twitter event to extrapolate the main topics people were more concerned about. They
highlighted eight topics similar to the ones discussed nowadays by authorities and everyday users
during the COVID-19 outbreak (e.g. ways of transmission, symptoms, survival of the virus outside
the body and protections). Towers et al. [39] extract Twitter data and web searches from the first days
in which the media reported some Ebola cases in the United States (September 2014) until the end of
October. They find a strong relationship between video and news related to Ebola and searches
performed on Twitter and Google (e.g. Ebola symptoms). Other papers focus on what was happening
in Africa. Oyeyemi et al. [23] collect tweets using keywords such as “Ebola” and “prevention” or
“cure” and find that many of them are carrying misleading information. They split their data into
correct information, medical misinformation and a generic category for tweets not in these two
categories (other). Since some of these remedies are medically questionable (e.g. drink salted water),
and the potential audience is very large, the Nigerian government itself decided to respond to this
misinformation using Twitter. Guidry et al. [13] study how Twitter and Instagram have been used by
CDC, World Health Organization (WHO) and Médecins Sans Frontières (MSF) to educate the public
about Ebola. They find that Instagram posts were significantly more likely than tweets to feature
contents related to risk perception, e.g. information about adverse outcomes. Another study from
Liang et al. [21] compare the relevance of dissemination of information on Twitter from important
actors (broadcasting) compared to word of mouth (viral spreading). The analysis is performed on all
tweets posted about Ebola in a 14-month period (March 2014 – May 2015) to gain insight on the
retweeting patterns. Using these data, authors identify four types of users: influential, hidden
influential, disseminator and common user. They highlight the relevance of a broadcasting-type
communication, concluding that it would be useful for health authorities to establish a partnership
with influential users to communicate more efficiently.
Finally, Twitter was an important means of communication also during the Zika virus outbreak in
Central and South America. Fu et al. [10] collect a sample of more than one billion tweets reporting
the keyword “Zika”, posted in English, Spanish and Portuguese from May 2015 to April 2016. They
identify 20 topics that were grouped into five themes (impact and reaction to Zika virus, concern for
5

pregnancy and microcephaly, transmission routes and case reports), and find that user-generated
contents play a more relevant role as information channels compared to those of the government
authorities, highlighting the need to prevent the proliferation of misleading information.
The rationale for this paper is based on previous literature suggesting that Twitter represents a useful
tool to study reactions to epidemic events. This paper makes several contributions to the literature.
First, other studies usually use a single method to assess the polarity of a text in a sentiment analysis
whilst our approach offers a comparison of different methods (Section 3) and also adds an evaluation
of text polarity performed with machine learning classifiers (Section 5). Second, while many studies
give more emphasis to graphical representation (e.g. wordcloud) or tables (e.g. most used words), in
this study we utilize statistical and machine learning tools focused on sentiment scores’ trends rather
than on the specific content of tweets. We also analyze individual emotions in order to separate the
positive part of a score from the negative one and to better understand how these two components
behave. Third, we expand the analysis using economic data from the main Italian stock exchange
index to find out if the trend in sentiment towards Italy is related to other aspects (e.g. the economy)
of the country. To this regard, we observe a strong relationship between country reputation and stock
market performance, with sentiment found to serve as an early detection signal (up to two weeks
earlier) for potential effects on the stock exchange index values. Fourth, our work considers a period
of time starting from the end of 2019 to give a more comprehensive representation of the evolution
of sentiment towards Italy, i.e., it is not strictly limited to the COVID-19 outbreak period. In this way,
we effectively show the impact that the COVID-19 outbreak had on sentiment towards Italy.
To the best of our knowledge, the present study is the first one using social media opinions to consider
the effects of the COVID-19 outbreak on both the reputation of a country and its economy.

3. Impact of the COVID-19 outbreak towards Italy’s reputation
3.1 Data collection
We use lexicon-based sentiment analysis on tweets to evaluate the temporal evolution of the
sentiment towards Italy before and during the COVID-19 outbreak. Approximately 1,000 tweets per
day, posted in the period October 2019 – May 2020, in English language and reporting the keyword
“Italy”
were
collected
using
the
Python3
GetOldTweets3
library
(https://pypi.org/project/GetOldTweets3/), version 0.0.11. In total, 244,000 tweets were retrieved.
After data cleaning, consisting in quality control and removal of tweets for which incomplete content
was downloaded, 243,846 tweets are retained and used for further analysis.
3.2 Evolution of the sentiment towards Italy before and during the COVID-19 outbreak
Preprocessing of tweets (including removal of punctuation marks, hashtags, mentions and links as
well as conversion in lower case letters) was conducted in R [29] version 3.6.3. Six different methods
were used to evaluate sentiment of collected tweets: sentimentR [31], vader [17] and four lexicons
(nrc, afinn, bing and syuzhet) included in the Syuzhet package [18]. For each method, we consider
the mean score for tweets collected in a single day. These values were then plotted to observe the
temporal evolution of the sentiment towards Italy (Figure 1a). The mean scores were then
standardized to consider the different scales used by each method to report the sentiment of a tweet.
(Figure 1b).

6

Figure 1. Sentiment score (a) and standardized sentiment score (b) of collected tweets including the keyword “Italy”
from October 2019 to May 2020

As shown in Figure 1b, a high concordance among sentiment scores assigned by different methods is
observed for standardized sentiment scores. The most positive values, corresponding to the highest
peak, are observed on New Year’s Eve. Most interestingly, while the trend of sentiment values in the
previous months is somewhat stable around neutral or slightly positive values, extremely negative
scores are observed from February 21, 2020. At this date, the first Italian case of COVID-19 is
reported. From this day, sentiment scores remain negative, although a slow trend towards less
negative / neutral values seems to be present in May.
We performed an analysis aimed at finding a structural change in order to verify the existence of a
breakpoint in sentiment scores at February 21, 2020. A structural break is a sudden change of values
in a time series occurring at one or more specific dates. Therefore, this analysis allows us to gain
7

further insights into the phenomenon object of study by knowing when a significant change occurred
in the data. Using the strucchange R package [43] we conduct the structural change analysis for all
six used lexicons. For all lexicons, the existence of a breakpoint at 21 February 2020 (observation n.
143), is observed (Table 1). Among lexicons, we notice differences as regards to the optimal number
of breakpoints identified according the minimum value of Bayesian Information Criterion (BIC): two
breakpoints are identified for bing, nrc and syuzhet (Figure 2) while three are identified for afinn,
vader and sentimentR (Figure 3). Additionally, we observe variability in the break dates identified
for breakpoints different from February, 21 2020 (Table 1). A graphical representation of the
breakpoints obtained from two lexicons (bing and afinn) is reported in Figure 2 and 3, respectively.
Table 1. Identified break dates using different lexicons
Lexicon
Afinn
Nrc
bing
syuzhet
vader
sentimentR

Break dates (observation number)
82
143
197
143
203
143
197
143
197
79
143
198
83
143
196

8

Figure 2. (a) BIC and Residual Sum of Squares using the bing lexicon, (b) Breakpoints in sentiment score using the
bing lexicon

9

Figure 3. (a) BIC and Residual Sum of Squares using the afinn lexicon; (b) Breakpoints in sentiment score using the afinn
lexicon

As we assess that on the date of February 21, 2020 a substantial change of sentiment is observed
according to all lexicons, we define two different periods for the subsequent analyses: Period A from
October 1, 2019 to February 20, 2020; and Period B from February 21, 2020 to May 31, 2020. Next,
we conduct a sub-period analysis of specific positive and negative emotions in these two periods
using the nrc lexicon. Besides reporting positive and negative sentiment, this lexicon allows us to
evaluate each tweet in terms of eight basic emotions: four negative emotions (anger, disgust, fear and
sadness) and four positive ones (anticipation, joy, surprise and trust). The scores for positive and
negative sentiment in the two Periods A and B are compared in Supplementary Figure 1, while scores
for the eight specific emotions are shown in Supplementary Figure 2. The scores obtained for
sentiment as well as for specific emotions show non-normal distribution according to the ShapiroWilk test. Moreover, the Mann-Whitney test is used to compare sentiment and emotions between
Period A and Period B. As shown in Supplementary Figure 1, general positive sentiment is not
decreasing significantly (p = 0.07) from Period A to B, whilst negative sentiment is significantly
increasing (p < 0.001).

10

Supplementary Figure 1. Boxplots showing positive and negative sentiment in Period A (October 1 2019 – 20 February
2020) and Period B (21 February – 31 May 2020)

Although a rise of negative emotions in Period B is somewhat expected and in accordance with our
hypothesis, positive emotions overall remain stable. This finding is further explored, and partly
confirmed, through the analysis of specific emotions. As shown in Supplementary Figure 2, all
negative emotions are increasing from Period A to Period B (p < 0.001). As for positive emotions,
joy is decreasing significantly (p < 0.001), whilst anticipation, surprise and trust are increasing
significantly (p < 0.001). A decrease of joy can be expected during such a hard time, whilst the other
three emotions might increase for different reasons. The rising in anticipation and surprise might be
interpreted as follows: even if the COVID-19 outbreak is a negative event, it has the power to generate
surprise and to increase the desire to know what will happen in the near future. On the other hand,
the increase in trust might depend from the willingness to believe in a speedy recovery as well as
from the attitude of people living outside of Italy towards encouraging Italians.

Supplementary Figure 2. Boxplots showing positive and negative emotions in Period A (October 1 2019 – 20 February
2020) and Period B (21 February – 31 May 2020)

11

Day-to-day detailed temporal evolution for the eight emotions from October 1 to May 31 is shown in
Figure 4. We observe that, from February 21, 2020, all negative emotions (right panel) start to rise,
whilst positive emotions (left panel) behave differently: surprise rises, joy first decreases and then
remains stable, whilst anticipation and trust first decrease and subsequently show a progressive
increase in the last days. Figure 4 gives us the chance to grasp subtle nuances not evident in the
general trend but which emerge when detailing the various emotions. The rise in the levels of negative
emotions is expected since an event such as the outbreak of an epidemic disease certainly has the
power to increase anger, disgust, fear and sadness both for those who experience the event firsthand
and for those who were not directly affected by it at that time. Positive emotions behave differently
but consistently with what could be expected for any specific emotion. For example, joy decreases,
and it no longer reaches the values of Period A, while trust has a similar huge drop, but it recovers in
a very short time (about a month). This is probably due to the fact that at the time people wanted to
express their trust that things would get better for Italy. Slogans as “Andrà tutto bene” (it will be
okay) were very popular in Italy during the spread of the pandemic.

Figure 4. Temporal evolution of the positive (on the left: anticipation, joy, surprise and trust) and negative (on the right:
anger, disgust, fear and sadness) emotions from October 1 to May 31.

4. Analysis of FTSE-MIB index from October, 1 2019 to May, 31 2020

12

To verify whether a trend similar to the one observed for the tweets’ sentiment is also observed for
changes in stock exchange values movements, data from the main Italian stock exchange index
(FTSE-MIB) are collected. Closing values of FTSE-MIB are observed from October 1, 2019 to May
31, 2020. Values concerning weekends and other festive days are linearly interpolated to be able to
compare homogeneously trends of tweets’ sentiments and stock exchange values. The analysis
performed on FTSE-MIB evidences the existence of three breakpoints on November 6, 2019, March
7, 2020 and April 26, 2020 (Figure 5). Specifically, the first breakpoint observed during the COVID19 outbreak period dates March 7, 2020: it happens 15 days after the change in sentiment scores
(February 21, 2020). Interestingly, the day after the observed breakpoint the Lombardy region was
set into lockdown.

13

Figure 5. (a) BIC and Residual Sum of Squares using the FTSE-MIB index values; (b) Breakpoints in FTSE-MIB index
values

We also investigate about a possible association between sentiment scores and stock exchange index
values. Since we observe a time lag between the main breakpoint in sentiment scores and the
corresponding breakpoint in FTSE-MIB values, we hypothesize that this association might be present
even when applying a time lag (i.e. past sentiment scores are associated with stock exchange index
values in the next days). In this respect, we estimate the regression model 𝑦𝑡 = 𝛼 (𝑘) + 𝛽 (𝑘) 𝑥𝑡−𝑘 + 𝜀𝑡 ,
where 𝑦𝑡 is the price of FTSE-MIB observed at day t; 𝑥𝑡−𝑘 is the sentiment score observed at time tk and k indicates the time lag. To evalute the strenght of the association between 𝑥𝑡−𝑘 and 𝑦𝑡 at
different time lags, we estimate separate models for k ranging from zero days (no lag, comparing
sentiment scores and stock exchange values for the same day) to 50 days (sentiment scores from one
14

day are compared with stock exchange values of 50 days later). Furthermore, to evaluate if the
association between sentiment scores and FTSE-MIB prices depends from the specificity of the
method used to perform sentiment analysis, we estimate the model separately for each method and
each lexicon used to compute sentiment scores. We are mainly intersted in observing how the sign
and the magnitude of the estimated regression coefficient 𝛽̂ (𝑘) varies respect to the different
method/lexicon used for sentiment analysis and the different time lag. In view of that, we concentrate
on the standardized value of 𝛽̂ (𝑘) obtained for each of the 306 estimated models (6 methods/lexicons
and 51 time lags).
Results obtained for standardized 𝛽̂ (𝑘) are represented in Figure 6 whilst the values of the
standardized estimated coefficient together with the R2 are reported in Supplementary Table 1. The
standardized estimated values of 𝛽 (𝑘) at lag zero indicates a strong association between sentiment
scores and FTSE-MIB values at lag zero ( 𝛽̂ (𝑘)≥ 0.70, p < 0.001, for all methods/lexicons). As showed
in Figure 6, a strong association is observed also when considering a lag up to 10-15 days, whilst the
same association decreases when the lag exceeds two weeks. Importantly, all the values of 𝛽̂ (𝑘) are
postive and significant (p < 0.001) for all the lags and no important differences are observed when
changing the method/lexicon used for sentiment analysis. Overall, these findings enforce the idea that
a change in sentiment scores can be considered as an early detection signal (up to two weeks earlier)
for potential effects on the stock market values.

15

Figure 6. Standardized betas for linear regressions with FTSE-MIB index values as response variable and sentiment
scores as predictor (from no lag up to 50 days lag)

5. Evaluation of tweets’ sentiment using machine learning classifiers
Finally, we investigate about polarity of tweets and compare the performances of two machine
learning classifiers widely used in the analysis of Twitter’s data, i.e. NB and SVM, in the prediction
of tweet polarity before and after the COVID-19 outbreak. Considering that, after the outbreak, the
topic was largely discussed on Twitter, we aim to evaluate whether the two classifiers show
differences in performance when analyzing texts pertaining to more diverse topics (whole dataset and
Period A) or a more homogeous topic (Period B). To this aim, we randomly sampled 16,128 tweets
(9,444 from Period A and 6,684 from Period B, respectively) and manually labeled them into
“positive”, “negative” or “neutral” according to their specific content. Of these, 3,623 tweets are
labeled as positive or negative (1,874 from Period A and 1,749 from Period B) and are used for the
subsequent analysis. These tweets are first preprocessed (removal of punctuation, numbers and stop
words) and used to form a document text matrix using the tm R package [9]. For both machine
learning classifiers, the dataset of 3,623 positive and negative tweets, as well as the two subsets
pertaining to Period A and B, were randomly split into a training set including 80% of cases and a
test set including 20% of cases. Naïve Bayes and Support Vector Machines are used to train the model
on the training set observations and to validate their accuracy on the test set observations. As the

16

specific content of the tweets is more related to Covid-19 in Period B, we expect the two classifiers
to perform better for tweets appearing in this period compared to those appearing in the pre-outbreak
period (Period A).
5.1 Naïve Bayes
Naïve Bayes (NB) classifiers are supervised learning algorithms used for more than two decades for
different classification problems, including spam filtering and text classification [32]. A NB classifier
is based on the Bayes theorem which states that, given two events A and B, we can compute the
conditional probability of A given B as the probability of B given A times the probability of A, all
divided by the probability of B. Notationally, we have:
P(A|B) =

P(B|A) P(A)
P(B)

(1)

A NB classifier assumes that features 𝑥𝑗 (𝑗 = 1, … . , 𝑝) are conditionally independent:
𝑃(𝑥𝑗 |𝑋 = 𝑥1 , … , 𝑥𝑝 ) = 𝑃(𝑥𝑗 )

∀ 𝑗 ∈ [1, p]

Thus, considering a categorical response variable y with k classes (𝑘 ≥ 2) the Bayes theorem is
reformulated as:
𝑝

𝑃(𝐶𝑘 ) ∏
𝑃(𝐶 = 𝐶𝑘 ∣ 𝑋 = 𝑥1 , … , 𝑥𝑝 ) =

𝑗=1

𝑃(𝑥𝑗 ∣ 𝐶𝑘 )

(2)

𝑃(𝑥1 , … , 𝑥𝑝 )

where 𝐶 is one of the 𝐶𝑘 classes of 𝑦, and 𝑋 is a vector of random variables 𝑥𝑗 . The task is thus to
assign each tweet 𝑡𝑖 (𝑖 = 1, … . , 𝑛) to one of the k classes of 𝑦. In our case, tweets can be classified
either in positive or negative (k = 2). Since the denominator in Eq. 2 is the same for each class, we
can get rid of it and approximate Eq. 2 as follows:
𝑝

(3)

𝑃(𝐶𝑘 ∣ 𝑥1 , … , 𝑥𝑝 ) ∝ 𝑃(𝐶𝑘 ) ∏ 𝑃(𝑥𝑗 ∣ 𝐶𝑘 )
𝑖=1

so that the assigned class 𝐶̂ is chosen according to:
𝑝

(4)

𝐶̂ = argmax 𝑃(𝐶𝑘 ) ∏ 𝑃(𝑥𝑗 ∣ 𝐶𝑘 )
𝑖=1

Therefore, any tweet 𝑡𝑖 is classified into one, and only one, of the 𝐶𝑘 classes.
5.2 Support Vector Machine (SVM)
SVM is a non-probabilistic binary linear classifier [36], as it is not based on a specific probability
distribution. In a linear setting, SVM detects a line, so called hyperplane, which best separates data.
As in the case of NB, the task is to assign each tweet 𝑡𝑖 (𝑖 = 1, … . , 𝑛) to one of the two classes
(positive or negative) of the categorical response y. The hyperplane that best splits the data is that
positioned as far as possible from the nearest points of the two classes. The distance between a point

17

and the hyperplane is called margin. The larger these margins (hard margins), the smaller the
→

→

probability of misclassification error. To train a linear SVM, we consider data (𝑥1 , 𝑦1 ), … , (𝑥𝑛 , 𝑦𝑛 ),
→
where 𝑥𝑖 are vectors of feature values of observation 𝑖 (𝑖 = 1, … . , 𝑛), and 𝑦𝑖 are labels (+1 or –1,
depending on which class an observation belongs to). The goal is to find a hyperplane
→

→

(5)

𝑤 ⋅ 𝑥𝑖 + 𝑏 = 0
→

that best separates the two classes (𝑤 is the normal vector to the hyperplane). It is possible to add
constraints about the margin to avoid having any observation within the margins. These constraints
→

→

are usually specified as: 𝑦𝑖 (𝑤 ⋅ 𝑥𝑖 + 𝑏) ≥ 1, for all 1 ≤ 𝑖 ≤ 𝑛.
One of the main problems that may arise when looking for the best hyper-plane is that even a single
new observation might cause a great shift of the hyper-plane, reducing the margin by a great amount.
In such cases, it is preferable to have a classifier able to tolerate a sub-optimal separation of the two
classes but apt to deliver a higher overall classification performance (soft margins). In practice, some
points are allowed to be on the wrong side of the margin, or even of the hyper-plane. Since each
→

margin is distant 1/||𝑤 || from the hyper-plane that separates the classes, the distance between the two
→

→

margins is equal to 2/||𝑤 ||. Thus, this distance is maximized when ||𝑤 || is minimized. To achieve this,
the following a quadratic programming problem needs to be solved:
min f :

1
2

(6)

||w||2
→

→

s. t. g: 𝑦𝑖 (𝑤 ⋅ 𝑥𝑖 + 𝑏) ≥ 1
Eq.6 is a constrained optimization problem that can be solved using the method of Lagrange
multipliers. It requires a so-called slack variable 𝜉𝑖 such that the constraint can be expressed as
→

→

𝑦𝑖 (𝑤 ⋅ 𝑥𝑖 + 𝑏) ≥ 𝐵(1 − 𝜉𝑖 )

(7)

→

where B = 1/||𝑤 ||, 𝜉𝑖 ≥ 0 and ∑𝑛𝑖=1 𝜉𝑖 ≤ 𝐾. Therefore, some points are allowed to be on the wrong
side, but only up to a given distance less or equal to the constant K. As a result, the optimization
problem specified in Eq 6, subject to the constraint specified in Eq 7, becomes

{

→

→

𝑦𝑖 (𝑤 ⋅ 𝑥𝑖 + 𝑏) ≥ 𝐵(1 − 𝜉𝑖 )

(8)

min ||w||2 s. t. 𝜉𝑖 ≥ 0 ∑𝑛𝑖=1 𝜉𝑖 ≤ 𝐾

5.3 Comparison between the performance of naïve Bayes and Support Vector Machine
NB and SVM are estimated using the R package e1071 [22]. Results about their test-set performance
are reported in Table 2.
Respect to the whole test set (Period A + Period B), NB reaches a good accuracy and high precision
for both positive and negative tweets (precision is 0.87 and 0.78, respectively). Restricting the
18

analysis to Period A, the classifier maintains a good accuracy and still shows a good performance in
the classification of negative tweets. However, performance in the classification of positive tweets
get worsen. An opposite result is observed when restricting on Period B. Therefore, NB does not
allow us to observe an improvement in classification accuracy when analyzing a dataset in which a
specific topic, i.e. the COVID-19 pandemic, characterizes the content of a larger part of negative
tweets (Period B) compared to periods in which more diverse topics are discussed by users (whole
dataset and Period A).
Table 2. Performance of naïve Bayes and Support Vector Machine in the classification of tweets
Naïve Bayes
Positive
Negative

Support Vector Machine
Positive
Negative

Whole dataset
Precision
Recall
F1-score
Support
Accuracy

0.87
0.75
0.81
364

0.78
0.89
0.83
360
0.82

0.83
0.76
0.79
364

0.78
0.84
0.81
360
0.80

Period A
Precision
Recall
F1-score
Support
Accuracy

0.74
0.60
0.66
212

0.81
0.89
0.84
113
0.79

0.66
0.52
0.58
212

0.77
0.85
0.81
113
0.74

Period B
Precision
Recall
F1-score
Support
Accuracy

0.84
0.80
0.86
93

0.61
0.86
0.71
257
0.81

0.88
0.83
0.86
93

0.60
0.70
0.64
257
0.79

Next, SVM with a linear kernel is estimated in the same way as NB. Again, when analyzing the whole
dataset (Period A + Period B) the classifier shows a good performance according to all metrics, with
slightly lower precision but higher recall and F1-score for negative compared to positive tweets.
When analyzing Period A, consistent with the results obtained for NB, SVM shows lower accuracy
and worse performance in the classification of positive tweets, while maintaining good precision,
recall and F1-score in the classification of negative tweets. The opposite result is observed when
analyzing Period B.
Results obtained for the whole dataset (Period A + B, Table 2) evidence that the two classifiers show
similar accuracy and F1 score, but more pronounced differences are observed in terms of precision
and recall, with NB showing a slightly better performance for most metrics. While both classifiers
show higher precision in the classification of positive compared to negative tweets, the opposite result
is observed for recall and F1-score. Results obtained for the tweets collected during Period A reveal
that, for all metrics, both classifiers show a better performance in the classification of negative
compared to positive tweets. When analyzing the tweets collected during Period B, both classifiers
19

show better precision and F1-score in the classification of positive compared to negative tweets.
Conversely, recall is higher for negative tweets using NB and for positive tweets using SVM. Overall,
these results are consistent with our initial assumption that classifiers’ performance is improving
following the beginning of the outbreak as the content of tweets is more specifically focused on the
pandemic. In conclusion, although the two classifiers do not show differences as regard to accuracy,
NB would be preferable due to its overall better performance in terms of precision for both positive
and negative tweets.

6. Concluding remarks
In this paper, we analyze the sentiment towards Italy before and after the COVID-19 outbreak using
lexicon-based and machine learning classifiers applied to real data collected from Twitter. We
observe a substantial rise in negative emotions towards Italy in correspondence of the first Italian
case of COVID-19 followed by a change towards more neutral or slightly positive values starting two
months later. Besides being useful to interpret the general sentiment towards a country as a proxy of
the perceived country reputation, we find that sentiment scores can be also used to early detect
changes in stock exchange values. Future research is addressed to assess the sentiment towards
different countries, to verify whether similar findings might be observed also in cases in which the
outbreak developed with different rates of escalation and/or in the cases when severity is managed by
the governments with different alternative or concomitant measures such as social distancing,
lockdown or travel restrictions.
The results of this research must be interpreted in the context of its limitations. First, data was
collected from a single social network (i.e., Twitter). It is possible that the results could vary in the
case many social networks are considered. However, as shown in the literature review, Twitter is
widely used to evaluate reactions to important events due to its diffusion and ease of use. As already
mentioned, future work should also focus on other countries, as our findings could vary due to the
different country’s reputation or other factors (e.g. cultural or socioeconomic factors). Despite these
limitations, we believe that this analysis is helpful to understand how the sentiment towards Italy, one
of the first countries severely affected by COVID-19, evolved through time. Furthermore, it can also
help to shed light on the relationship between country reputation and the possible economic
repercussions of an event of this magnitude.

Declarations
Funding: none
Conflicts of interest/Competing interests: none
Availability of data and material: data available upon request
Code availability: code available upon request

20

References
1. Ashcroft M, Fisher A, Kaati L, Omer E, Prucha N (2015) Detecting jihadist messages on twitter.
In 2015 European Intelligence and Security Informatics Conference. IEEE, pp 161-164.
https://doi.org/10.1109/EISIC.2015.27.
2. Backfried G, Shalunts G (2016) Sentiment analysis of media in german on the refugee crisis in europe.
In Paloma Díaz, Narjès Bellamine Ben Saoud, Julie Dugdale, and Chihab Hanachi, editors,
Information Systems for Crisis Response and Management in Mediterranean Countries. Springer
International Publishing, Cham, pp 234-241. https://doi.org/10.1007/978-3-319-47093-1
3. Broniatowski DA, Paul MJ, Dredze M (2013) National and local influenza surveillance through
Twitter: an analysis of the 2012-2013 influenza epidemic. PloS one 8(12):e83672.
https://doi.org/10.1371/journal.pone.0083672
4. Burnap P, Williams ML, Sloan L, Rana O, Housley W, Edwards A, Knight V, Procter R, Voss A
(2014) Tweeting the terror: modelling the social media reaction to the Woolwich terrorist
attack. Social Network Analysis and Mining 4(1):206. https://doi.org/10.1007/s13278-014-0206-4
5. Cheong M, Lee VC (2011) A microblogging-based approach to terrorism informatics: Exploration and
chronicling civilian sentiment and response to terrorism events via Twitter. Information Systems
Frontiers 13(1):45-59. https://doi.org/10.1007/s10796-010-9273-x
6. Chew C, Eysenbach G (2010) Pandemics in the age of Twitter: content analysis of Tweets during the
2009 H1N1 outbreak. PloS one 5(11):e14118. https://doi.org/10.1371/journal.pone.0014118
7. Chunara R, Andrews JR, Brownstein JS (2012) Social and news media enable estimation of
epidemiological patterns early in the 2010 Haitian cholera outbreak. Am J Trop Med Hyg 86(1):3945. https://doi.org/10.4269/ajtmh.2012.11-0597
8. Conde-Cespedes P, Chavando J, Deberry E (2018) Detection of Suspicious Accounts on Twitter Using
Word2Vec and Sentiment Analysis. In International Conference on Multimedia and Network
Information System. Cham, Springer, pp 362-371. https://doi.org/10.1007/978-3-319-98678-4_37
9. Feinerer I, Hornik K (2019). tm: Text Mining Package. R package version 0.7-7, https://CRAN.Rproject.org/package=tm
10. Fu KW, Liang H, Saroha N, Tse ZTH, Ip P, Fung ICH (2016) How people react to Zika virus outbreaks
on Twitter? A computational content analysis. Am J Infect Control 44(12):1700-1702.
https://doi.org/10.1016/j.ajic.2016.04.253
11. Fung ICH, Tse ZTH, Cheung CN, Miu AS, Fu KW (2014) Ebola and the social media. Lancet
384(9961):2207. https://doi.org/10.1016/S0140-6736(14)62418-1
12. Garg P, Garg H, Ranga V (2017) Sentiment analysis of the Uri terror attack using Twitter. In 2017
International conference on computing, communication and automation (ICCCA), IEEE, pp 17-20.
https://doi.org/10.1109/CCAA.2017.8229812
13. Guidry JP, Jin Y, Orr CA, Messner M, Meganck S (2017) Ebola on Instagram and Twitter: How health
organizations address the health crisis in their social media engagement. Public relations
review 43(3):477-486. https://doi.org/10.1016/j.pubrev.2017.04.009
14. Güneyli A, Ersoy M, Kiralp S (2017) Terrorism in the 2015 Election Period in Turkey: Content
Analysis
of
Political
Leaders'
Social
Media
Activity. J.
UCS 23(3):256-279.
https://doi.org/10.3217/jucs-023-03-0256
15. Harb JG, Ebeling R, Becker K (2019) Exploring deep learning for the analysis of emotional reactions
to terrorist events on Twitter. Journal of Information and Data Management 10(2):97-115
16. Hui DS, Azhar EI, Madani TA, Ntoumi F, Kock R, Dar O, Ippolito G, Mchugh TD, Memish ZA,
Drosten C, Zumla A, Petersen E (2020) The continuing 2019-nCoV epidemic threat of novel
coronaviruses to global health - The latest 2019 novel coronavirus outbreak in Wuhan, China. Int J
Infect Dis 91:264-266. https://doi.org/doi:10.1016/j.ijid.2020.01.009
17. Hutto CJ, Gilbert EE (2014) VADER: A Parsimonious Rule-based Model for Sentiment Analysis of
Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann
Arbor, MI, June 2014.

21

18. Jockers ML (2015). Syuzhet: Extract Sentiment and Plot Arcs from Text,
https://github.com/mjockers/syuzhet
19. Jones JH, Salathe M (2009) Early assessment of anxiety and behavioral response to novel swine-origin
influenza A (H1N1). PLoS one 4(12):e8032. https://doi.org/10.1371/journal.pone.0008032
20. Lazard AJ, Scheinfeld E, Bernhardt JM, Wilcox GB, Suran M (2015) Detecting themes of public
concern: a text mining analysis of the Centers for Disease Control and Prevention's Ebola live Twitter
chat. Am J Infect Control 43(10):1109-1111. https://doi.org/10.1016/j.ajic.2015.05.025
21. Liang H., Fung ICH, Tse ZTH, Yin J, Chan CH, Pechta LE, Smith BJ, Marquez-Lameda RD, Meltzer
MI, Lubell KM, Fu KW (2019) How did Ebola information spread on twitter: broadcasting or viral
spreading?. BMC public health 19(1):438. https://doi.org/10.1186/s12889-019-6747-8
22. Meyer D (2019) e1071 R package. https://cran.r-project.org/web/packages/e1071/index.html
23. Oyeyemi SO, Gabarron E, Wynn R (2014) Ebola, Twitter, and misinformation: a dangerous
combination?. BMJ 349:g6178. https://doi.org/10.1136/bmj.g6178.
24. Öztürk N, Ayvaz S (2018) Sentiment analysis on Twitter: A text mining approach to the Syrian refugee
crisis. Telematics and Informatics 35(1):136-147. https://doi.org/10.1016/j.tele.2017.10.006
25. Pagolu VS, Reddy KN, Panda G, Majhi B (2016) Sentiment analysis of Twitter data for predicting
stock market movements. 2016 International Conference on Signal Processing, Communication,
Power
and
Embedded
System
(SCOPES),
Paralakhemundi,
pp
1345-1350.
https://doi.org/10.1109/SCOPES.2016.7955659
26. Pang B, Lee L (2008) Opinion mining and sentiment analysis. Foundations and Trends in Information
Retrieval 2(1-2):1-135. https://doi.org/10.1561/1500000011
27. Paul MJ, Dredze M. (2017). Social monitoring for public health. Synthesis Lectures on Information
Concepts,
Retrieval,
and
Services 9(5):1-183.
https://doi.org/10.2200/S00791ED1V01Y201707ICR060
28. Pope D, Griffith J (2016) An Analysis of Online Twitter Sentiment Surrounding the European Refugee
Crisis.In Proceedings of the 8th International Joint Conference on Knowledge Discovery, Knowledge
Engineering and Knowledge Management - Volume 1: KDIR, (IC3K 2016), pp 299-306.
https://doi.org/10.5220/0006051902990306
29. R Core Team (2020). R: A language and environment for statistical computing. R Foundation for
Statistical Computing, Vienna, Austria. URL http://www.R-project.org/.
30. Remuzzi A, Remuzzi G (2020) COVID-19 and Italy: what next?. Lancet 395(10231):1225-1228.
https://doi.org/10.1016/S0140-6736(20)30627-9
31. Rinker
TW
(2019)
sentimentr:
Calculate
Text
Polarity
Sentiment
version
2.7.1. http://github.com/trinker/sentimentr
32. Sahami M, Dumais S, Heckerman D, Horvitz E (1998) A Bayesian approach to filtering junk e-mail.
In Learning for Text Categorization: Papers from the 1998 workshop 62:98-105
33. Signorini A, Segre AM, Polgreen PM (2011) The use of Twitter to track levels of disease activity and
public concern in the US during the influenza A H1N1 pandemic. PloS one 6(5):e19467.
https://doi.org/10.1371/journal.pone.0019467.
34. Simon T, Goldberg A, Aharonson-Daniel L, Leykin D, Adini B (2014) Twitter in the cross fire—the
use of social media in the Westgate Mall terror attack in Kenya. PloS one 9(8):e104136.
https://doi.org/10.1371/journal.pone.0104136
35. Smith M, Broniatowski DA, Paul MJ, Dredze M (2016) Towards real-time measurement of public
epidemic awareness: Monitoring influenza awareness through twitter. In AAAI spring symposium on
observational studies through social media and other human-generated content. Stanford, California
36. Suykens JAK, Vandewalle J (1999) Least Squares Support Vector Machine Classifiers. Neural
Processing Letters 9(3):293-300
37. Szomszor M, Kostkova P, De Quincey E (2010) #Swineflu: Twitter predicts swine flu outbreak in
2009. In International conference on electronic healthcare. Berlin, Heidelberg, Springer, pp 18-26.
https://doi.org/10.1007/978-3-642-23635-8_3

22

38. Tavazoee F, Conversano C, Mola F (2020) Recurrent random forest for the assessment of popularity
in social media: 2016 US election as a case study. Knowledge and Information Systems 62:1847-1879.
http://doi.org/10.1007/s10115-019-01410-w
39. Towers S, Afzal S, Bernal G, Bliss N, Brown S, Espinoza B, Jackson J, Judson-Garcia J, Khan M, Lin
M, Mamada R, Moreno VM, Nazari F, Okuneye K, Ross ML, Rodriguez C, Medlock J, Ebert D,
Castillo-Chavez C (2015) Mass media and the contagion of fear: the case of Ebola in America. PloS
one 10(6):e0129179. https://doi.org/10.1371/journal.pone.0129179
40. Turney PD (2002) Thumbs up or thumbs down? Semantic orientation applied to unsupervised
classification of reviews. In Proceedings of 40th Meeting of the Association for Computational
Linguistics, Philadelphia, PA, pp 417–424. https://doi.org/10.3115/1073083.1073153
41. Vollmer B, Karakayali S (2018) The volatility of the discourse on refugees in Germany. Journal of
immigrant & refugee studies 16(1-2):118-139. https://doi.org/10.1080/15562948.2017.1288284
42. Yang SU, Shin H, Lee JH, Wrigley B (2008) Country reputation in multidimensions: predictors,
effects, and communication channels. Journal of Public Relations Research 20(4):421-440.
https://doi.org/10.1080/10627260802153579
43. Zeileis A, Leisch F, Hornik K, Kleiber C (2002) strucchange: An R Package for Testing for Structural
Change
in
Linear
Regression
Models. Journal
of
Statistical
Software 7(2):1–
38. https://doi.org/10.18637/jss.v007.i02

23

Supplementary Table 1. Linear regression models with FTSE-MIB index values as response and
sentiment scores as predictor
Lag afinn
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41

bing

nrc

2
𝛽̂ (𝑘) R

2
𝛽̂ (𝑘) R

syuzhet
2
𝛽̂ (𝑘) R

sentimentR
2
𝛽̂ (𝑘) R

vader

2
𝛽̂ (𝑘) R

0.76
0.78
0.79
0.80
0.82
0.82
0.83
0.84
0.85
0.85
0.86
0.86
0.87
0.87
0.88
0.88
0.88
0.87
0.87
0.86
0.86
0.84
0.83
0.82
0.81
0.79
0.78
0.76
0.75
0.74
0.73
0.71
0.70
0.69
0.68
0.68
0.67
0.67
0.66
0.65
0.64
0.63

0.76
0.78
0.80
0.81
0.83
0.83
0.84
0.85
0.86
0.86
0.87
0.87
0.88
0.89
0.89
0.90
0.90
0.90
0.89
0.89
0.88
0.87
0.86
0.85
0.83
0.82
0.81
0.79
0.78
0.76
0.75
0.73
0.72
0.71
0.70
0.70
0.69
0.68
0.67
0.67
0.66
0.65

0.77
0.78
0.80
0.81
0.82
0.83
0.83
0.84
0.84
0.84
0.84
0.85
0.85
0.86
0.87
0.87
0.86
0.86
0.86
0.86
0.85
0.84
0.83
0.82
0.81
0.80
0.78
0.77
0.76
0.75
0.73
0.72
0.70
0.69
0.69
0.68
0.68
0.67
0.66
0.66
0.65
0.64

0.75
0.76
0.78
0.79
0.81
0.81
0.82
0.83
0.84
0.84
0.84
0.85
0.86
0.87
0.87
0.88
0.88
0.88
0.88
0.87
0.86
0.85
0.84
0.83
0.82
0.81
0.80
0.78
0.77
0.76
0.74
0.72
0.71
0.70
0.70
0.69
0.69
0.68
0.67
0.67
0.66
0.65

0.71
0.73
0.74
0.75
0.77
0.77
0.78
0.79
0.80
0.80
0.81
0.81
0.82
0.83
0.84
0.84
0.85
0.85
0.85
0.85
0.84
0.83
0.82
0.81
0.80
0.78
0.77
0.76
0.75
0.73
0.72
0.70
0.68
0.68
0.67
0.67
0.66
0.66
0.65
0.64
0.63
0.62

0.74
0.76
0.78
0.79
0.81
0.81
0.82
0.83
0.84
0.84
0.85
0.85
0.86
0.87
0.87
0.87
0.87
0.87
0.87
0.86
0.86
0.85
0.84
0.83
0.81
0.80
0.79
0.77
0.76
0.75
0.73
0.71
0.70
0.69
0.68
0.68
0.67
0.67
0.66
0.65
0.64
0.63

0.58
0.60
0.63
0.65
0.67
0.68
0.69
0.70
0.72
0.73
0.74
0.75
0.75
0.76
0.77
0.77
0.77
0.76
0.76
0.75
0.73
0.70
0.69
0.67
0.65
0.62
0.60
0.58
0.57
0.55
0.52
0.50
0.48
0.47
0.46
0.46
0.45
0.44
0.43
0.42
0.41
0.40

0.58
0.61
0.64
0.66
0.68
0.69
0.70
0.72
0.73
0.74
0.75
0.76
0.77
0.79
0.80
0.80
0.81
0.81
0.80
0.79
0.78
0.75
0.73
0.71
0.69
0.67
0.65
0.62
0.60
0.58
0.56
0.53
0.51
0.50
0.49
0.48
0.48
0.46
0.45
0.44
0.43
0.42

0.59
0.61
0.64
0.66
0.68
0.68
0.69
0.70
0.71
0.71
0.71
0.72
0.73
0.74
0.75
0.75
0.75
0.74
0.74
0.73
0.72
0.70
0.69
0.67
0.66
0.63
0.61
0.59
0.58
0.56
0.53
0.51
0.49
0.48
0.47
0.47
0.46
0.45
0.44
0.43
0.42
0.41

0.50
0.58
0.61
0.63
0.65
0.66
0.67
0.68
0.70
0.71
0.71
0.72
0.73
0.75
0.76
0.77
0.77
0.77
0.76
0.76
0.75
0.73
0.71
0.69
0.68
0.65
0.63
0.61
0.59
0.58
0.55
0.52
0.51
0.49
0.49
0.48
0.47
0.46
0.45
0.44
0.43
0.42

0.55
0.52
0.55
0.57
0.59
0.60
0.61
0.62
0.64
0.65
0.65
0.66
0.67
0.69
0.70
0.71
0.72
0.72
0.72
0.71
0.70
0.68
0.66
0.65
0.63
0.61
0.59
0.57
0.56
0.54
0.51
0.48
0.47
0.45
0.44
0.44
0.44
0.43
0.42
0.41
0.40
0.38

2
𝛽̂ (𝑘) R

0.55
0.58
0.60
0.62
0.65
0.66
0.67
0.68
0.70
0.71
0.72
0.73
0.74
0.75
0.76
0.76
0.76
0.76
0.76
0.75
0.74
0.71
0.70
0.68
0.66
0.64
0.62
0.59
0.58
0.56
0.53
0.50
0.49
0.47
0.46
0.46
0.45
0.44
0.43
0.42
0.41
0.39

24

42
43
44
45
46
47
48
49
50

0.62
0.61
0.60
0.59
0.58
0.57
0.56
0.55
0.54

0.39
0.37
0.36
0.34
0.33
0.32
0.31
0.30
0.29

0.64
0.63
0.62
0.61
0.60
0.59
0.58
0.57
0.56

0.41
0.39
0.38
0.37
0.36
0.34
0.33
0.32
0.31

0.63
0.62
0.60
0.59
0.58
0.57
0.56
0.55
0.54

0.39
0.38
0.36
0.35
0.34
0.32
0.31
0.30
0.29

0.64
0.62
0.61
0.60
0.59
0.58
0.57
0.56
0.55

0.40
0.38
0.37
0.36
0.35
0.33
0.32
0.31
0.30

0.61
0.60
0.59
0.58
0.57
0.56
0.55
0.54
0.53

0.37
0.36
0.34
0.33
0.32
0.31
0.29
0.29
0.28

0.62
0.61
0.60
0.58
0.57
0.56
0.55
0.54
0.53

0.38
0.36
0.35
0.34
0.32
0.31
0.30
0.29
0.28

The table reports standardized betas (p < 0.001) and model R2 for a linear regression with FTSE-MIB index
values as response variable and sentiment scores as predictor. Different time lags, from zero days (no lag,
comparing sentiment scores and stock exchange values for the same day) to 50 days (sentiment scores from
one day are compared with stock exchange values of 50 days later) have been used.

25

