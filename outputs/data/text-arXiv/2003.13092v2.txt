Efficient Tuning-Free ℓ1-Regression of Nonnegative Compressible
Signals

arXiv:2003.13092v2 [cs.IT] 6 Oct 2020

Hendrik Bernd Petersen

∗

Bubacarr Bah

†

Peter Jung

‡

Abstract
In compressed sensing the goal is to recover a signal from as few as possible noisy, linear measurements.
The general assumption is that the signal has only a few non-zero entries. The recovery can be performed
by multiple different decoders, however most of them rely on some tuning. Given an estimate for the
noise level a common convex approach to recover the signal is basis pursuit denoising. If the measurement
matrix has the robust null space property with respect to the ℓ2 -norm, basis pursuit denoising obeys stable
and robust recovery guarantees. In the case of unknown noise levels, nonnegative least squares recovers
non-negative signals if the measurement matrix fulfills an additional property (sometimes called the M + criterion). However, if the measurement matrix is the biadjacency matrix of a random left regular bipartite
graph it obeys with a high probability the null space property with respect to the ℓ1 -norm with optimal
parameters. Therefore, we discuss non-negative least absolute deviation (NNLAD). For these measurement
matrices, we prove a uniform, stable and robust recovery guarantee without the need for tuning. Such
guarantees are important, since binary expander matrices are sparse and thus allow for fast sketching and
recovery. We will further present a method to solve the NNLAD numerically and show that this is comparable
to state of the art methods. Lastly, we explain how the NNLAD can be used for viral detection in the recent
COVID-19 crisis.

1

Introduction

Since it has been realized that many signals admit a sparse representation in some frames, the question arose
whether or not such signals can be recovered from less samples than the dimension of the domain by utilizing the
low dimensional structure of the signal. The question was already answered positively in the beginning of the
millennium [CRT06][Don06]. By now there are multiple different decoders to recover a sparse signal from noisy
measurements with robust recovery guarantees. Most of them however rely on some form of tuning, depending
on either the signal or the noise.
The basis pursuit denoising requires an upper bound on the norm of the noise [FR13, Theorem 4.22], the least
shrinkage and selection operator an estimate on the ℓ1 -norm of the signal [HTW15, Theorem 11.1] and the
Lagrangian version of least shrinkage and selection operator allegedly needs to be tuned to the order of the the
noise level [HTW15, Theorem 11.1]. The expander iterative hard thresholding needs the sparsity of the signal or
an estimate of the order of the expansion property [FR13, Theorem 13.15]. The order of the expansion property
can be calculated from the measurement matrix, however there is no polynomial time method known to do this.
Variants of these methods have similar drawbacks. The non-negative basis pursuit denoising requires the same
tuning parameter as the basis pursuit denoising [DT05]. Other thresholding based decoders like sparse matching
pursuit and expander matching pursuit have the same limitations as the expander iterative hard thresholding
[GI10].
If these side information is not known a priori, many decoders yield either no recovery guarantees or, in their
imperfect tuned versions, yield sub-optimal estimation errors [FR13, Theorem 11.12]. Even though the problem
of sparse recovery from under-sampled measurements has been answered long ago, finding tuning free decoders
that achieve robust recovery guarantees is still a topic of interest.
The most prominent achievement for that is the non-negative least squares (NNLS) [BEZ08][DT10][WXT11]
[SH11][SH13]. It is completely tuning free [KKRT16] and in [KJ18][SJC19] it was proven that it achieves robust
recovery guarantees if the measurement matrix consists of certain independent sub-Gaussian random variables.

1.1

Our Contribution

We will replace the least squares in the NNLS with an arbitrary norm and obtain the non-negative least residual (NNLR). By adapting [KJ18] we prove a recovery guarantees under similar conditions as the NNLS. In
∗ Communications

and Information Theory Group, Technische Universtität Berlin, Berlin, petersen@tu-berlin.de
Institute for Mathematical Sciences (AIMS) South Africa, Cape Town, and Division of Applied Mathematics, Stellenbosch University, Stellenbosch bubacarr@aims.ac.za
‡ Communications and Information Theory Group, Technische Universtität Berlin, Berlin, peter.jung@tu-berlin.de
† African

1

particular, we consider the case where we minimize the ℓ1 -norm of the residual (NNLAD) and give a recovery
guarantee if the measurement matrix is a random walk matrix of a uniformly at random drawn D-left regular
bipartite graph.
In general, our result states that if a certain measurement is present, the basis pursuit denoising can be replaced by the tuning-less NNLR for non-negative signals. While sub-Gaussian measurement matrices rely on a
probabilistic argument to verify that this measurement is present, random walk matrices of left regular graphs
naturally have the measurement. The tuning-less nature gives the NNLR an advantage over other decoders if
the noise power can not be estimated, which is for instance the case if the noise components are multiplicative,
i.e. a random variable times the true measurements, or when the noise is Laplacian distributed. The latter noise
distribution or the existence of outliers also favors an ℓ1 regression approach over an ℓ2 regression approach and
thus motivate to use the NNLAD over the NNLS.
Further, the sparse structure of left regular graphs can reduce the encoding and decoding time to a fraction.
Using [CP11] we can solve the NNLAD with a first order method of a single optimization problem with a sparse
measurement matrix. Other state of the art decoders often use non-convex optimization, computationally
complex projections or need to solve multiple different optimization problems. For instance, to solve the basis
pursuit denoising given a tuning parameter a common approach is to solve a sequence of LASSO problems to approximate where the Pareto curve attains the value of the tuning parameter of basis pursuit denoising [vdBF09].

1.2

Relations to Other Works

We build on the theory of [KJ18] that uses the ℓ2 null space property and the M + criterion. These methods have
also been used in [KKRT16][SJC19]. To the best of the authors knowledge the M + criterion has not been used
with an ℓ1 null space property before. Other works have used adjacency matrices of graphs as measurements
matrices including [JXHC09][XH07][BGI+ 08][GI10][KDXH11]. The works [JXHC09][XH07] did not consider
noisy observations. The decoder in [BGI+ 08] is the basis pursuit denoising and thus requires tuning depending
on the noise power. [KDXH11] proposes two decoders for non-negative signals. The first is the non-negative basis
pursuit which could be extended to the non-negative basis pursuit denoising. However this again needs a tuning
parameter depending on the noise power. The second decoder, the Reverse Expansion Recovery algorithm,
requires the order of the expansion property, which is not known to be calculatable in a polynomial time. The
survey [GI10] contains multiple decoders including the basis pursuit, which again needs tuning depending on
the noise power for robustness, the expander matching pursuit and the sparse matching pursuit, which need
the order of the expansion property. Further, [DT05] considered sparse regression of non-negative signals and
also used the non-negative basis pursuit denoising as decoder, which again needs tuning dependent on the noise
power. To the best of the authors knowledge, this is the first work that considers tuning-less sparse recovery
for random walk matrices of left regular bipartite graphs. The NNLAD has been considered in [MC16] with a
structured sparsity model without the use of the M + criterion.

2

Preliminaries

For K ∈ N we denote the set of integers from 1 to K by [K]. For a set T ⊂ [N ] we denote the number of elements
in T by # (T ). Vectors are denoted by lower case bold face symbols, while its corresponding components
are denoted by lower case italic letters. Matrices are denoted by upper case bold face symbols, while its
corresponding components are denoted by upper case italic letters. For x ∈ RN we denote its ℓp -norms by kxkp .
Given A ∈ RM×N we denote its operator norm as operator from ℓq to ℓp by kAkq→p := supv∈RN ,kvkq ≤1 kAvkp .
N
By RN
+ we denote the non-negative orthant. Given a closed convex set C ⊂ R , we denote the projection onto
2
C, i.e. the unique minimizer of argmin 21 kz − vk2 , by PC (v). For a vector x ∈ RN and a set T ⊂ [N ], x|T
z∈C

denotes the vector in RN , whose n-th component is xn if n ∈ T and 0 else. Given N, S ∈ N we will often need
sets T ⊂ [N ] with # (T ) ≤ S and we abbreviate this by # (T ) ≤ S if no confusion is possible.
Given a measurement matrix
A ∈ RM×N a decoder is any map QA : RM → RN . A signal is any possible

N
x ∈ RN . If x ∈ RN
=
z
∈
R
: zn ≥ 0 for all n ∈ [N ] , we say the signal is non-negative and write shortly
+
x ≥ 0. If additionally xn > 0 for all n ∈ [N ], we write x > 0. An observation is any possible input of a decoder,
i.e. all y ∈ RM . We allow all possible inputs of the decoder as observation, since in general the transmitted
codeword Ax is disturbed by some noise. Thus, given a signal x and an observation y we call e := y − Ax
the noise. A signal x is called S-sparse if kxk0 := # ({n ∈ [N ] : xn 6= 0}) ≤ S. We denote the set of S-sparse
vectors by

ΣS := z ∈ RN : kzk0 ≤ S .
2

Given some S ∈ [N ] the compressibility of a signal x can be measured by d1 (x, ΣS ) := inf z∈ΣS kx − zk1 .
Given N and S, the general non-negative compressed sensing task is to find a measurement matrix A ∈ RM×N
and a decoder QA : RM → RN with M as small as possible such that the following holds true: There exists a
q ∈ [1, ∞] and a continuous function C : R × RM → R+ with C (0, 0) = 0 such that
M
kQA (y) − xkq ≤ C (d1 (x, ΣS ) , y − Ax) for all x ∈ RN
+ and y ∈ R

holds true. This will ensure that if we can control the compressibility and the noise, we can also control the
estimation error and in particular decode every noiseless observation of S-sparse signals exactly.

3

Main Results

Given a measurement matrix A ∈ RM×N and a norm k·k on RM we propose to define the decoder as follows:
Given y ∈ RM set QA (y) as any minimizer of
argmin kAz − yk .

(NNLR)

z≥0

We call this problem non-negative least residual (NNLR). In particular, for k·k = k·k1 this problem is called
non-negative least absolute deviation (NNLAD) and for k·k = k·k2 this problem is known as the non-negative
least squares (NNLS) studied in [KJ18]. In fact, we can translate the proof techniques fairly simple. We just
need to introduce the dual norm.
Definition 3.1. Let k·k be a norm on RM . The norm k·k∗ on RM defined by kvk∗ := supkuk≤1 hv, ui, is called
dual norm to k·k.
Note that the dual norm is actually a norm. To obtain a recovery guarantee for NNLR we have certain
requirements on the measurement matrix A. As for most other convex optimization problems in compressed
sensing, we use a null space property.
Definition 3.2. Let S ∈ [N ], q ∈ [1, ∞) and k·k be any norm on RM . Further let A ∈ RM×N . Suppose there
exists constants ρ ∈ [0, 1) and τ ∈ [0, ∞) such that
1

k v|T kq ≤ ρS q −1 k v|T c k1 + τ kAvk for all v ∈ RN and # (T ) ≤ S.
Then, we say A has the ℓq -robust null space property of order S with respect to k·k or in short A has the
ℓq -RNSP of order S with respect to k·k with constants ρ and τ . ρ is called stableness constant and τ is called
robustness constant.
In order to deal with the non-negativity, we need A to be biased in a certain way. In [KJ18] this bias was
guaranteed with the M + criterion.
Definition 3.3. Let A ∈ RM×N . Suppose there exists t ∈ RM such that AT t > 0. Then we say A obeys the
−1

the M + criterion with vector t and constant κ := maxn∈[N ] AT t n maxn∈[N ] AT t n .

Note that κ is actually a condition number of the matrix with diagonal AT t and 0 else. Condition number
numbers are frequently used in error bounds of numerical linear algebra. The general recovery guarantee is the
following and similar results have been obtained in the matrix case in [JJ20].
Theorem 3.4 (NNLR Recovery Guarantee). Let S ∈ [N ], q ∈ [1, ∞) and k·k be any norm on RM with
dual norm k·k∗ . Further, suppose that A ∈ RM×N obeys
a) the ℓq -RNSP of order S with respect to k·k with constants ρ and τ and
b) the M + criterion with vector t and constant κ.
M
If κρ < 1, the following recovery guarantee holds true: For all x ∈ RN
any minimizer x# of
+ and y ∈ R

argmin kAz − yk

(NNLR)

z≥0

obeys the bound
#

x−x

q

−1
3 + κρ
(1 + κρ)2 1q −1
max AT t n ktk∗ +
S
κτ
1 − κρ
1
− κρ
n∈[N ]

1
(1 + κρ)2
κS q −1 d1 (x, ΣS ) + 2
≤2
1 − κρ

If q = 1, this bound can be improved to
#

x−x

1

1 + κρ
κd1 (x, ΣS ) + 2
≤2
1 − κρ



−1
2
1 + κρ
max AT t n ktk∗ +
κτ
1 − κρ n∈[N ]
1 − κρ
3



!

kAx − yk .

kAx − yk .

Proof. The proof can be found in Subsection 6.1.
Given a matrix with ℓq -RNSP we can add a row of ones (or a row consisting of one minus the column sums
of the matrix) to fulfill the M + criterion with the optimal κ = 1. Certain random measurement matrices
guarantee uniform bounds on κ for fixed vectors t. In [KJ18, Theorem 12] it was proven that if Am,n are all
T
i.i.d. 0/1 Bernoulli random variables, A has M + criterion with t = (1, . . . , 1) ∈ RM and κ ≤ 3 with high
probability. This is problematic, since if κ > 1, it might happen that κρ < 1 is not fulfilled anymore. Since the
stableness constant ρ (S ′ ) as a function of S ′ is monotonically increasing, the condition κρ(S ′ ) < 1 might only
hold if S ′ < S. If that is the case, there are vectors x ∈ ΣS that arebeing recovered
by basis pursuit denoising

1 0 1
but not by NNLS! This is for instance the case for the matrix A =
, which has ℓ1 -robust null space
0 1 1
property of order 1 with stableness constant ρ := 21 and M + criterion with κ ≥ 2 for any possible choice of t.
In particular, the vector x = (0, 0, 1)T is not necessarily being recovered by the NNLAD and the NNLS.
Hence, it is crucial that the vector t is chosen to minimize κ and ideally obeys the optimal κ = 1. This motivates
us to use random walk matrices of regular graphs since they obey exactly this.
M×N

Definition 3.5. Let A ∈ {0, 1}

and D ∈ [M ]. For T ⊂ N the set
[
Row (T ) :=
{m ∈ [M ] such that Am,n = 1}
n∈T

is called the set of right vertices connected to the set of left vertices T . If
# (Row ({n})) = D for all n ∈ [N ] ,

M×N
then D−1 A ∈ 0, D−1
is called a random walk matrix of a D-left regular bipartite graph. We also say
−1
short that D A is a D-LRBG. If additionally there exists a θ ∈ [0, 1) such that
# (Row (T )) ≥ (1 − θ) D# (T ) for all # (T ) ≤ S
holds true, then D−1 A is called a random walk matrix of a (S, D, θ)-lossless expander.
We will only consider random walk matrices and no biadjacency matrices. Note that we have made a slight
abuse of notation. The term D-LRBG as a short form for D-left regular bipartite graph refers in our case to
the random walk matrix A but not the graph itself. We omit this minor technical differentiation, for the sake of
shortening the frequently used term random walk matrix of a D-left regular bipartite graph. Lossless expanders
are bipartite graphs that have a low number of edges but are still highly connected, see for instance [Vad12,
Chapter 4]. As a consequence their random walk matrices have good properties for compressed sensing. It is
well known that random walk matrices of a (2S, D, θ)-lossless expanders obey the ℓ1 -RNSP of order S with
respect to k·k1 , see [FR13, Theorem 13.11]. The dual norm of k·k1 is the norm k·k∞ and the M + criterion is
easily fulfilled, since the columns sum up to one. From Theorem 3.4 we can thus draw the following corollary.



M×N
Corollary 3.6 (Lossless Expander Recovery Guarantee). Let S ∈ [N ], θ ∈ 0, 16 . If A ∈ 0, D−1
is a random walk matrix of a (2S, D, θ)-lossless expander, then the following recovery guarantee holds true: For
M
all x ∈ RN
any minimizer x# of
+ and y ∈ R
argmin kAz − yk1

(NNLAD)

z≥0

obeys the bound
x − x#

1

≤2

3 − 2θ
1 − 2θ
d1 (x, ΣS ) + 2
kAx − yk1 .
1 − 6θ
1 − 6θ

Proof. By [FR13, Theorem 13.11] A has ℓ1 -RNSP with respect to k·k1 with constants ρ =
T

(1)
2θ
1−4θ

and τ =

1
1−4θ .

The dual norm of the norm k·k1 is k·k∞ . If we set t := (1, . . . , 1) ∈ RM , we get
X

Am,n = DD−1 = 1 for all n ∈ [N ] .
AT t n =
m∈[M]

Hence, A has the M + criterion with vector t and constant κ = 1 and the condition κρ < 1 is immediately
−1
fulfilled. We obtain ktk∗ = ktk∞ = 1 and maxn∈[N ] AT t n = 1. Applying Theorem 3.4 with improved
bound for q = 1 and these values yields


2
1+ρ
1+ρ
d1 (x, ΣS ) + 2
+
τ kAx − yk1 .
x − x# 1 ≤2
1−ρ
1−ρ 1−ρ
4

If we additionally substitute the values for ρ and τ we get


1
1 − 2θ
1 − 2θ
kAx − yk1
d1 (x, ΣS ) + 2
+2
x − x# 1 ≤2
1 − 6θ
1 − 6θ
1 − 6θ
1 − 2θ
3 − 2θ
≤2
d1 (x, ΣS ) + 2
kAx − yk1 .
1 − 6θ
1 − 6θ
This finishes the proof.

Note that [FR13, Theorem 13.11] is an adaption of [BGI+ 08, Lemma
11] to account for robustness
and skips


eN
2
ln
and
D
=
,
a
uniformly
proving the ℓ1 restricted isometry property. If M ≥ 2θ exp 2θ S ln eN
S
θ
S
at random drawn D-LRBG is a random walk matrix of a (2S, D, θ)-lossless expander with a high probability

[FR13, Theorem 13.7]. Thus, recovery with the NNLAD is possible in the optimal regime M ∈ O S log N
.
S

On the Robustness Bound for Lossless Expanders



If A is a random walk matrix of a (2S, D, θ)-lossless expander with θ ∈ 0, 61 , then we can also draw a recovery
2θ
guarantee for the NNLS. By [FR13, Theorem 13.11] A has ℓ1 -RNSP with respect to k·k1 with constants ρ = 1−4θ
1
1
′
′
and τ = 1−4θ and hence also ℓ1 -RNSP with respect to k·k2 with constants ρ = ρ and τ = τ M 2 . Similar to
the proof of Corollary 3.6 we can use Theorem 3.4 to deduce that any minimizer x# of
argmin kAz − yk2 ,

(NNLS)

z≥0

obeys the bound
x − x#

1

≤2

3 − 2θ 1
1 − 2θ
d1 (x, ΣS ) + 2
M 2 kAx − yk2 .
1 − 6θ
1 − 6θ

(2)

If the measurement error e = y − Ax is a constant vector, i.e. e = α1, then kek1 = M 2 kek2 . In this case the
error bound of the NNLS is just as good as the error bound of the NNLAD. However, if e is a standard unit
vector, then kek1 = kek2 . In this case the error bound of the NNLS is significantly worse than the error bound
of the NNLAD. Thus, the NNLAD performs better under peaky noise, while the NNLS and NNLAD are tied
under noise with evenly distributed mass. We will verify this numerically in Subsection 5.1. One can draw a
complementary result for matrices with biased sub-Gaussian entries, which obey the ℓ2 -RNSP with respect to
k·k2 and the M + criterion in the optimal regime [KJ18]. Table 1 states the methods, which have an advantage
over the other in each scenario.

Noise

peaky kek1 ≈ kek2
1
even mass kek1 ≈ M 2 kek2
unknown noise

1

Measurement Matrix
D-LRBG (ℓ1 ) biased sub-Gaussian (ℓ2 )
NNLAD
NNLS
NNLAD
NNLS

Table 1: Table of advantages of NNLAD and NNLS over each other.

4

NNLAD using a Proximal Point Method

In this section we assume that k·k = k·kp with some p ∈ [1, ∞]. If p ∈ {1, ∞}, the NNLR can be recast as a linear
program by introducing some slack variables. For an arbitrary p the NNLR is a convex optimization problem
and the objective function has a simple and globally bounded subdifferential. Thus, the NNLR can directly be
solved with a projective subgradient method
usinga problem independent step size. Such subgradient methods

1
achieve only a convergence rate of O log (k) k − 2 towards the optimal objective value [Nes04, Section 3.2.3],
where k is the number of iterations performed. In the case that the norm is the ℓ2 -norm, we can transfer the
problem into a differentiable version, i.e. the NNLS
1
2
argmin kAz − yk2 .
2
z≥0
Since the gradient of such an objective is Lipschitz, this problem can besolved by a projected gradient method
with constant step size, which achieves a convergence rate of O k −2 towards the optimal objective value
[BT09][AP16]. However this does not generalize to the ℓ1 -norm. The proximal point method proposed in
[CP11] can solve the case of the ℓ1 -norm with a convergence rate O k −1 towards the optimal objective value.
This results in the following algorithm.
5

Algorithm 4.1 (NNLAD as First Order Method).
Data: measurement y ∈ RM , measurement matrix A ∈ RM×N , parameters σ > 0, τ > 0,
initializations x0 ∈ RN , w0 ∈ RM , tolerance parameters ǫ1 ≥ 0,ǫ2 ≥ 0
Result: estimator x# ∈ RN
initialize iterates;
x ← x0 ; v ← x0 ; w ← w0 ;
initialize images;
w̃ ← AT w; x̃ ← Ax; ṽ ← Av;
while kx̃ − yk1 + hy, wi > ǫ1 or minn∈[N ] w̃n < −ǫ2 do
calculate iterates;
w ← w + σ (ṽ − y);
w ← (min {1, |wm |} sgn (wm ))m∈[M] ;
w̃ ← AT w;
v ← −x;
x ← (max {0, xn − τ w̃n })n∈[N ] ;
v ← v + 2x;
ṽ ← Av;
x̃ ← 21 (ṽ + x̃);
end
return x# ← x
−2

The following convergence guarantee can be deduced from [CP11, Theorem 1]. Let στ < kAk2→2 and let xk
and wk be the values of x and w at the end of the k-th iteration of the while loop of Algorithm 4.1. Then, the
following statements hold true:

(1) The iterates converge: The sequence xk k∈N converges to a minimizer of argmin kAz − yk1 .
z≥0

(2) The iterates are feasible: We have xk ≥ 0 and wk

∞

≤ 1 for all k ≥ 1.

(3) There is a stopping criteria for the iterates:
limk→∞ Axk − y 1 + hy, wk i = 0 and limk→∞ AT wk ≥ 0. In particular, if Axk − y
and AT wk ≥ 0, then xk is a minimizer of argmin kAz − yk1 .

1

+ hy, wk i ≤ 0

z≥0

(5) The averages obey the convergence rate towards
the optimal objective
value:


Pk
1
1
1
1
#
0 2
k′
#
0 2
A k k′ =1 x − y − Ax − y 1 ≤ k 2τ x − x 2 + 2σ w 2 + 2 w0
1

a minimizer of argmin kAz − yk1 .

+M
1



, where x# is

z≥0

The formal version and proof is given in the appendix. Note that this yields a convergence guarantee for both the
iterates and averages, but the convergence rate is only guaranteed for the averages. Algorithm 4.1 is optimized
in the sense that it uses the least possible number of matrix vector multiplications per iteration, since these
govern the computational complexity.
Remark 4.2. Let A be D-LRBG. Each iteration of Algorithm 4.1 requires at most 4DN + 8N + 16M floating
point operations and 5N + 4M assignments.

Iterates or Averages
The question arises whether or not it is better to estimate with averages or iterates. Numerical testing suggest
that the iterates reach tolerance thresholds significantly faster than the averages. We can only give a heuristically
explanation for this phenomenon. The stopping criteria of the iterates yields limk→∞ AT wk ≥ 0. In practice we
observe that AT wk ≥ 0 for all sufficiently large k. However, AT wk+1 ≥ 0 yields xk+1 ≤ xk . This monotonicity
promotes the converges of the iterates and gives a clue why the iterates seem to converge better in practice.
See Figure 5 and Figure 6.

On the Convergence Rate

As stated the NNLS achieves
the convergence rate O k −2 [AP16] while the NNLAD only achieves the conver
gence rate of O k −1 towards to optimal objective value. However, this should not be considered as weaker,
6

since the objective function of the NNLS is the square of a norm. If xk are the iterates of the NNLS implementation of [AP16], algebraic manipulation yields
#

k

Ax − y

2

− Ax − y

2

≤2

1
2



1
Axk − y
2

2
2

1
−
Ax# − y
2

2
2

 12

1

≤ 2 2 Ck −2

 12

1

≤ (2C) 2 k −1 .

Thus, the ℓ2 -norm of the residual of the NNLS iterates only decays in the same order as the ℓ1 -norm of the
residual of the NNLAD averages.

5

Numerical Experiments and Applications

In the first part of this section we will compare NNLAD with several state of the art recovery
methods inoterms
n
−1
N
of achieved sparsity levels and decoding time. For p ∈ [1, ∞], we denote SN
:=
z
∈
R
: kzkp = 1 , and
p

N −1
N −1
N
S0
:= z ∈ R : kzk0 = 1 = kzk2 = Σ1 ∩ S2 .

5.1

Properties of the NNLAD Optimizer

We recall that the goal is to recover x from the noisy linear measurements y = Ax+ e. To investigate properties
of the minimizers of NNLAD we compare it to the minimizers of the well studied problems basis pursuit (BP),
optimally tuned basis pursuit denoising (BPDN), optimally tuned ℓ1 -constrained least residual (CLR) and the
NNLS, which are given by
argmin
z:kAz−yk1 ≤ǫ

kzk1

argmin kAz − yk1

z:kzk1 ≤τ

with ǫ = kek1 ,

(BPDN)

with τ = kxk1 ,

(CLR)

argmin kzk1 ,

(BP)

z:Az=y

argmin kAz − yk2 .

(NNLS)

z≥0

Further, we compare the NNLAD to any cluster point of the sequence of the expander iterative hard thresholding
(EIHT) given by

for all k ∈ N0 and with S ′ = kxk0 ,
(EIHT)
x0 := 0 and xk+1 := PΣS′ xk + median y − Axk

where median (z)n is the median of (zm )m∈Row({n}) and PΣS (v) is a hard thresholding operator, i.e. some
2

minimizer of argmin 21 kz − vk2 . There is a whole class of thresholding based decoders for lossless expanders,
z∈ΣS

which all need either the sparsity of the signal or the order of the expansion property as tuning parameter. We
choose the EIHT as a represent of this class, since it has robust recovery guarantees [FR13, Theorem 13.5]. By
convex decoders we refer to BPDN, BP, CLR, NNLAD, and NNLS. We choose the optimal tuning ǫ = kek1
for the BPDN and τ = kxk1 for the CLR. The optimally tuned BPDN and CLR are representing a best case
benchmark. In [Kü19, Figure 1.1] it was noticed that tuning the BPDN with ǫ > kekp often leads to worse
estimation errors than tuning with ǫ < kekp for p = 2. Thus, BP is a version of BPDN with no prior knowledge
about the noise and represents a worst case benchmark. At the moment we do not care about the method to
calculate the minimizers of the optimization problems, thus we solve all optimization problems with the CVX
package of Matlab [GB14], [MS08]. For a given SN R, r, N, M, D, S we will do the following experiment multiple
times:
Experiment 1.

1. Generate a measurement matrix A ∈ 0, D−1

M×N

3. Generate a noise e uniformly at random from

kAxk1 M−1
.
SN R Sr

as a uniformly at random drawn D-LRBG.

N −1
2. Generate a signal x uniformly at random from ΣS ∩ RN
.
+ ∩ S1

4. Define the observation y := Ax + e.
5. For each decoder QA calculate an estimator x# := QA (y) and collect the relative estimation error
kx−x# k
x − x# 1 = kxk 1 .
1

7

kAxk

In this experiment we have SN R = kek 1 and since A is a D-LRBG and x ≥ 0, we further have kAxk1 =
1
kxk1 = 1. Note that for r = 0 and r = 1 we obtain two different noise distributions. If e is uniformly
distributed on SM−1
, then the absolute value hof each
1
i component |em | is a random variable with density h 7→
M−2

2

2
= M M(M+1)
=

2
M+1 . By testing one can observe a
√
1
concentration around this expected value, in particular that M 2 kek2 ≈ 2 kek1 with a high probability. If e is
M−1
uniformly distributed on S0
, then kek2 = kek1 . Thus, these two noise distributions each represent randomly
drawn noise vectors obeying one norm equivalence asymptotically tightly up to a constant. From (1) and (2)
we expect that the NNLS has roughly the same estimation errors as the NNLAD for r = 1, i.e. the evenly
distributed noise, and significantly worse estimation errors for r = 0, i.e. the peaky noise.

(M − 1) (1 − h)

for h ∈ [0, 1]. Thus, E kek2

Quality of the Estimation Error for Varying Sparsity
We fix the constants r = 1, N = 1024, M = 256, D = 10, SN R = 1000 and vary the sparsity level S ∈ [64].
For each S we repeat Experiment 1 100 times. We plot the mean of the relative ℓ1 -estimation error and the
mean of the logarithmic relative ℓ1 -estimation error, i.e.
!
!!
x − x# 1
x − x# 1
and Mean (LNℓ1 E) = Mean 10 log10
Mean (Nℓ1 E) = Mean
kxk1
kxk1
over the sparsity. The result can be found in Figure 1a and Figure 1b.
0.01
0
0.008
-10
0.006
-20
0.004
-30
0.002
-40
0
0

20

40

60

-50

80

0

(a)
NNLAD has almost the same performance as
CLR/BPDN. EIHT fails for moderate S.

20

40

60

80

100

120

(b) NNLAD and NNLS perform roughly the same.

Figure 1: Performance of NNLAD for noise with even mass noise and varying sparsity of the signal.
For S ≥ 30 the estimation error of the EIHT randomly peaks high. We deduce that the EIHT fails to recover
the signal reliably for S ≥ 30, while the NNLAD and other convex decoders succeed. This is not surprising,
since by [FR13, Theorem 13.15] the EIHT obeys a robust recovery guartanee for S-sparse signals, whenever A
1
. This is significantly stronger than the
is the random wak matrix of a (3S, D, θ′ )-lossless expander with θ′ < 12
1
(2S, Dθ)-lossless expander property with θ < 6 required for a null space property. It might also be that the
null space property is more likely than the lossless expansion property similar to the gap between ℓ2 -restricted
isometry property and null space property [DLR18]. However, if the EIHT recovers a signal, it recovers it
significantly better than any convex method. This might be the case, since the originally generated signal is
indeed from ΣS , which is being enforced by the hard thresholding of the EIHT, but not by the convex decoders.
This suggests that it might be useful to consider using thresholding on the output of any convex decoder to
increase the accuracy if the orignal signal is indeed sparse and not only compressible. For the remainder of this
subsection we focus on convex decoders.
Contrary to our expectation the BPDN achieves worse estimation errors than all other convex decoders for
S ≥ 60, even worse than the BP. The authors have no explanation for this phenomenon. Apart from that
we observe that the CLR and BP indeed perform as respectively best and worst case benchmark. However,
the difference between BP and CLR becomes rather small for high S. We deduce that tuning becomes less
important near the optimal sampling rate.
The NNLAD, NNLS and CLR perform roughly the same. This is quite strong, since BPDN and CLR are
optimally tuned using unknown prior information. As expected the NNLS performs roughly the same as the
NNLAD, see Table 1. However, this is the result of the noise distribution for r = 1. We repeat Experiment 1
kAxk
with the same constants, but r = 0, i.e. e is a unit vector scaled by ± SN R1 . We plot the mean of the relative
8

ℓ1 -estimation error and the mean of the logarithmic relative ℓ1 -estimation error over the sparsity. The result
can be found in Figure 2a and Figure 2b.
0
0.08
-20
0.07
0.06

-40

0.05
-60
0.04
0.03

-80

0.02
-100
0.01
0

-120
0

10

20

30

40

50

60

70

80

0

(a) The NNLS does not fail, but performs bad.

20

40

60

80

100

120

(b) The NNLS and NNLAD differ strongly.

Figure 2: Performance of NNLAD for noise with peaky mass and varying sparsity of the signal.
We want to note that similarly to Figure 1a the EIHT works only unreliably for S ≥ 30. Even though the
mean of the logarithmic relative ℓ1 -estimation error of NNLS is worse than the one of EIHT for 30 ≤ S ≤ 60,
the NNLS does not fail but only approximates with a weak error bound. As the theory suggests, the NNLS
performs significantly worse than the NNLAD, see Table 1. It is worth to mention, that the estimaton errors of
NNLS seem to be bounded by the estimation errors of BP. This suggests that A obeys a ℓ1 quotient property,
that bounds the estimation error of any instance optimal decoder, see [FR13, Lemma 11.15].
Noise-Blindness
Theorem 3.4 states that the NNLAD has an error bound similarly to the optimally tuned CLR and BPDN.
Further, by (1) the ratio
x − x#
x − x# 1
=
kek1 kxk1
kek1

1

should be bounded by some constant. To verify this, we fix the constants r = 1, N = 1024, M = 256, D = 10,
S = 32 and vary the signal to noise ratio SN R ∈ 10 [100]. For each SN R we repeat Experiment 1 100 times. We
plot the mean of the logarithmic relative ℓ1 -estimation error and the mean of the ratio of relative ℓ1 -estimation
error and ℓ1 -noise power, i.e.
!!
!
x − x# 1
x − x# 1
and Mean (Nℓ1 E/ℓ1 NP) = Mean
Mean (LNℓ1 E) = Mean 10 log10
kxk1
kek1 kxk1
over the sparsity. The result can be found in Figure 3a and Figure 3b.

9

-16

4.5

-18
4
-20
3.5
-22
3
-24
2.5
-26
0

200

400

600

800

2

1000

0

200

400

600

800

1000

(a) The NNLAD and NNLS recover reliably for all signal to
(b) The estimation error scales linearly with the noise power.
noise ratios.

Figure 3: Performance of NNLAD for noise with even mass and varying noise power.
The logarithmic relative ℓ1 -estimation errors of the different decoders stay in a constant relation to each
other over the whole range of SN R. This relation is roughly the relation we can find in Figure 1b for S = 32.
As expected the the ratio of relative ℓ1 -estimation error and ℓ1 -noise power stays constant independent on the
SN R for all decoders. We deduce that the NNLAD is noise-blind. We repeat the experiment with r = 0 and
obtain Figure 4a and Figure 4b.
10 -4

3

20

2.5
0
2
-20
1.5
-40

1

-60

0.5
0

-80
0

200

400

600

800

0

1000

(a) The NNLAD outperforms the NNLS.

200

400

600

800

1000

(b) The estimation error does not scale linearly with the
noise power.

Figure 4: Performance of NNLAD for noise with peaky mass and varying noise power.
kx−x# k1
kx−x# k1
kx−x# k1
Against our expectation,
and
not
seems
to
be
constant.
Since
≈ 1.0 · 10−7 is
kxk1
kxk1 kek1
kxk1
√
fairly small, we suspect that this is the result of CVX reaching a tolerance parameter1 eps ≈ 1.5 · 10−8 and
terminating, while the actual optimizer might in fact be the original signal. It is definitely noteworthy that
even with the incredibly small signal to noise ration of 10 the signal can be recovered by the NNLAD with an
estimation error of 1.0 · 10−7 for this noise distribution.

5.2

Decoding Complexity

NNLAD vs iterative methods
To investigate the convergence rates of the NNLAD as proposed in Section 4, we compare it to different types
of decoders when e = 0. There are some sublinear time recovery methods for lossless expander matrices
including [DT05][FR13, Section 13.4]. These are, as the name suggests, significantly faster than the NNLAD.
These, as several other greedy methods [JXHC09][XH07][DT05][KDXH11][FR13, Section 13.3], rely on a strong
lossless expansion property. As a representative of all greedy and sublinear time methods we will consider the
EIHT, which has a linear convergence rate O c−k towards the signal and robust recovery guarantees [FR13,
1 The

tolerance parameters of CVX are the second and fourth root of the machine precision by default [GB14], [MS08].

10

Theorem 13.15]. The EIHT also represents a best case benchmark. As a direct competitor
we consider the

NNLS implemented by the methods of [AP16] 2 , which has a convergence rate of O k −2 towards the optimal
objective value. [AP16] can also be used to calculate the least shrinkage and selection operator. However,
calculating the projection onto the ℓ1 -ball in RN , is computationally slightly more complex than the projection
onto RN
+ . Thus the NNLS will also be a lower bound for the LASSO. As a worst case benchmark we consider
a simple projected subgradient implementation of NNLAD using the Polyak step size, i.e.
!
k

Ax
−
y
T
k
1
,
(NNLAD Subgrad)
xk −
xk+1 := PRN
2 A sgn Ax − y
+
kAT sgn (Axk − y)k2
 1
which has a convergence rate of O k − 2 towards the optimal objective value [Pol87, Section 7.2.2 & Section 5.3.2] [Boy14, Section 6]. We will always initialize all iterated methods by zero vectors. The EIHT will
−1
always use the parameter S ′ = kxk0 , the NNLAD σ = τ = 0.99 kAk2→2 and the NNLS the parameters
−2
s = 0.99 kAk2→2 and α = 3.01, see [AP16]. Parameters that can be computed from A, will be calculated before
the timers start. This includes the adjacency structure of A for the EIHT, σ, τ for NNLAD, s, α for NNLS,
since these are considered to be a part of the decoder. We will do the following experiment multiple times:
Experiment 2.

M×N
1. If r = 1, generate a measurement matrix A ∈ 0, D−1
as a uniformly at random drawn D-LRBG.
If r = 2, draw each component Am,n of the measurement matrix independent and uniformly at random
from {0, 1}, i.e. as 0/1 Bernoulli random variables.
N −1
2. Generate a signal x uniformly at random from ΣS ∩ RN
.
+ ∩ Sr

3. Define the observation y := Ax.
4. For each iterative method calculate the sequence of estimators xk for all k ≤ 20000 and collect the relative
kAxk −yk1
kxk −xk
and the time to calculate the first
estimation errors kxk 1 , the relative norms of the residuals
kyk1
1
k iterations.
For r = 2 this represents a biased sub-gaussian random ensemble [KJ18] with optimal recovery guarantees for
the NNLS. For r = 1 this represents a D-LRBG random ensemble with optimal recovery guarantees for the
NNLAD. We fix the constants r = 1, N = 1024, M = 256, S = 16, D = 10 and repeat Experiment 2 100 times.
We plot the mean of the logarithmic relative ℓ1 -estimation error and the mean of the relative ℓ1 -norm of the
residual, i.e.
!!
!!
Axk − y 1
xk − x 1
and Mean (LNℓ1 R) = Mean 10 log10
Mean (LNℓ1 E) = Mean 10 log10
kxk1
kyk1
over the sparsity and the time. The result can be found in Figure 5 and Figure 6.

Figure 5: Convergence rates of certain iterated methods with respect to the number of iterations.

2 This

was the fastest method found by the authors. Other possibilities would be [CP11, Algorithm 2], [BT09].

11

Figure 6: Convergence rates of certain iterated methods with respect to the time.
The averages of NNLAD converge significantly slower than the iterates, even though we lack a convergence
rate for the iterates. We deduce that one should always use the iterates of NNLAD to recover a signal.
Surprisingly, the averages converge even slower than the subgradient method. However, this is not because
the averages converge slow, but rather because the subgradient method and all others converges faster than
expected. In particular, the NNLAD iterates, EIHT and the NNLS all converge linearly towards the optimal
objective value and towards the signal. Even the subgradient method converges almost linearly. We deduce
that the NNLS is the fastest of these methods if A is a D-LRBG.
Apart from a constant the NNLAD iterates, EIHT and NNLS converge in the same order. However, this behavior
does not hold if we consider a different distribution for A as one can verify by setting each component Am,n as
independent 0/1 Bernoulli random variables. While EIHT has better iterations compared to the NNLS, it still
takes more time to achieve the same estimation errors and residuals. We plot the mean of the time required to
calculate the first k iterations in Figure 7.

Figure 7: Time required to perform iterations of certain iterated methods.
The EIHT requires roughly 6 times as long as any other method to calculate each iteration. All methods but
the EIHT can be implemented with only two matrix vector multiplications, namely once by A and once by
AT . Both of these requires roughly 2DN floating point operations. Hence, each iteration requires O (4DN )
floating point operations. The EIHT only calculates one matrix vector multiplication, but also the median.
This calculation is significantly slower than a matrix vector multiplication. For every n ∈ [N ] we need to order
a vector with D elements, which can be performed in O (D log (D)). Hence, each iteration of EIHT requires
O (DN log (D)) floating point operations, which explains why the EIHT requires significantly more time for
each iteration.
As we have seen the NNLS is able to recover signals faster than any other method, however it also only obeys
sub-optimal robustness guarantees for uniformly at random chosen D-LRBG as we have seen in Figure 4a. We
ask ourself whether or not the NNLS is also faster with a more natural measurement scheme, i.e. if Am,n are
independent 0/1 Bernoulli random variables. We repeat Experiment 2 100 times with r = 2 for the NNLS and
r = 1 for the other methods. We again plot the mean of the logarithmic relative ℓ1 -estimation error and the
mean of the relative ℓ1 -norm of the residual in Figure 8 and Figure 9.

12

Figure 8: Convergence rates of certain iterated methods with respect to the number of iterations. A is Bernoulli
for NNLS and D-LRBG for the others.

Figure 9: Convergence rates of certain iterated methods with respect to the time. A is Bernoulli for NNLS
and D-LRBG for the others.
The NNLAD and the EIHT converge to the solution with roughly the same time. Even the subgradient
implementation of the NNLAD recovers a signal in less time than the NNLS. Further the convergence of NNLS
does not seem to be linear anymore. We deduce that sparse structure of A has a more significant influence on
the decoding time than the smoothness of the data fidelity term. Also we deduce that even the subgradient
method is a viable choice to recover a signal.
NNLAD vs SPGL1
As a last test we compare the NNLAD to the SPGL1 [vdBF09][vdBF19] toolbox for matlab.
Experiment 3.

1. Generate the measurement matrix A ∈ 0, D−1

M×N

as a uniformly at random drawn D-LRBG.

N −1
2. Generate the signal x uniformly at random from ΣS ∩ RN
.
+ ∩ Sr

3. Define the observation y := Ax.
4. Use a benchmark decoder to calculate an estimator x# and collect the relative estimation errors
kx# −xk1 kx# −xk2
, kxk
and the time to calculate x# .
kxk
1

2

kx# −xk
kxk −xk
kx# −xk
kxk −xk
5. For each iterative method calculate iterations until kxk 1 ≤ kxk 1 and kxk 2 ≤ kxk 2 . Collect
1
1
2
2
the time to perform these iterations. If this threshold can not be reached after 105 iterations, the recovery
failed and the time is set to ∞.
We again fix the dimension N = 1024, M = 256, D = 10 and vary S ∈ [128]. For both the BP implementation
of SPGL1 and the LASSO implementation of SPGL1 we repeat Experiment 3 100 times for each S. We plot
the mean of the time to calculate the estimators and plot these over the sparsity in Figure 10a and Figure 10b.

13

0.06

0.01

0.05

0.008

0.04
0.006
0.03
0.004
0.02
0.002

0.01
0

0
0

20

40

60

80

100

120

0

20

40

60

80

100

120

(a) The NNLAD is faster than the BP of SPGL1 for high (b) The NNLAD is faster than the LASSO of SPGL1 for
S.
moderate S.

Figure 10: Time of the NNLAD and NNLS to approximate better than SPGL methods.
The NNLAD implementation is slower than both
 SPGL1 methods for small S. However, if we have the
optimal number of measurements M ∈ O S log N
, the NNLAD is faster than both SPGL1 methods.
S

Summary

The implementation of NNLAD as presented in Algorithm 4.1 is a reliable recovery method for sparse nonnegative signals. There are methods that might be faster, but these either recover a smaller number of coefficients
(EIHT, greedy methods) or they obey sub-optimal recovery guarantees (NNLS). The implementation is as fast
as the commonly uses SPGL1 toolbox, but has the advantage that it requires no tuning depending on the
unknown x or e. Lastly, the NNLAD can handle peaky noise overwhelmingly good.

5.3

Application for Viral Detection

With the outbreak and rapid spread of the COVID-19 virus we are in the need of testing a lot of people for
an infection. Since we can only test a fixed number of persons in a given time, the number of persons tested
for the virus grows at most linearly. On the other hand, models suggest that the number of possibly infected
persons grows exponentially. At some point, if that is not already the case, we will have a shortage of test kits
and we will not be able to test every person. It is thus desirable to test as much persons with as few as possible
test kits.
The field group testing develops strategies to test groups of individuals instead of individuals in order to reduce
the amount of tests required to identify infected individuals. The first advances in group testing were made in
[Dor43]. For a general overview about group testing we refer to [AJS19].
The problem of testing a large group for a virus can be modeled as a compressed sensing problem in the following
way: Suppose we want to test N persons, labeled by [N ] = {1, . . . , N }, to check whether or not they are affected
by a virus. We denote by xn the quantity of viruses in the specimen of the n-th person. Suppose we have M
test kits, labeled by [M ] = {1, . . . , M }. By ym we denote the amount of viruses in the sample of the m-th test
M×N
kit. Let A ∈ [0, 1]
. For every n we put a fraction of size Am,n of the specimen of the n-th person into the
sample for the m-th test kit. The sample of the m-th test kit will then have the quantity of viruses
X
Am,n xn + econ
m ,
n∈[N ]

where econ
m is the amount of viruses in the sample originating from a possible contamination of the sample.
A quantitative reverse transcription
polymerase chain reaction estimates the quantity of viruses by ym with a
P
con
small error epcr
m = ym −
n∈[N ] Am,n xn − em . After all M tests we detect the quantity
y = Ax + e,

(3)

where e = econ + epcr . Since contamination of samples happens rarely, econ is assumed to be peaky in terms of
Table 1, while epcr is assumed to have even mass but a small norm. In total e is peaky.
Often each specimen is tested separately, meaning that A is the identity. In particular, we need at least as
much test kits as specimens. Further, we estimate the true quantity of viruses xn by x#
n := yn , which results in
con
pcr
the estimation error x#
n − xn = en = en + en . Since the noise vector e is peaky, some but few tests will be
14

inaccurate and might result in false positives or false negatives.
In general, only a fraction of persons is indeed affected by the virus. Thus, we assume that kxk0 ≤ S for some
small S. Since the amount of viruses is a non-negative value, we also have x ≥ 0. Hence, we can use the NNLR
to estimate x and in particular we should use the NNLAD due to the noise being peaky. Corollary 3.6 suggests
to choose A as the random walk matrix of a lossless expander or by [FR13, Theorem 13.7] to choose A as a
uniformly at random chosen D-LRBG. Such a matrix A has non-negative entries and the column sums of A are
not greater than one. This is a necessary requirement since each column sum is the total amount of specimen
used in the test procedure. Especially, a fraction of D−1 of each specimen is used in exactly D test kits.
By Corollary
 3.6 and [FR13, Theorem 13.7] this allows us to reduce the number of test kits required to M ≈
CS log e N
S . As we have seen in Figure 4a and Figure 4b we expect the NNLAD estimator to correct the errors
from econ and the estimation error is in the order of kepcr k1 which is assumed to be small. Hence, the NNLAD
estimator with a random walk matrix of a lossless expander might even result in less false positives and false
negatives than individual testing.
Note that the lack of knowledge about the noise e favors the NNLAD
P recovery method over a (BPDN) approach.
Further, since the total sum of viruses in all patients given by n∈[N ] xn = kxk1 is unknown, it is undesirable
to use (CLR).

Acknowledgments
The work was partially supported by DAAD grant 57417688. PJ has been supported by DFG grant JU 2795/3.
BB has been supported by BMBF through the German Research Chair at AIMS, administered by the Humboldt
Foundation.

6
6.1

Appendix
Proof of NNLR Recovery Guarantee

By 1 we denote the all ones vector in RN or RM respectively. The proof is an adaption of the steps used in
[KJ18]. As for most convex optimization problems in compressed sensing we require [FR13, Theorem 4.25] and
[FR13, Theorem 4.20] respectively.
Theorem 6.1 ( [FR13, Theorem 4.25] & [FR13, Theorem 4.20] ). Let q ∈ [1, ∞) and suppose A has
the ℓq -RNSP of order S with respect to k·k with constants ρ and τ . Then, it holds that
kx − zkq ≤

3+ρ
(1 + ρ)2 q1 −1
S
τ kA (x − z)k for all x, z ∈ Rn .
(kzk1 − kxk1 + 2d1 (x, ΣS )) +
1−ρ
1−ρ

If q = 1, this bound can be improved to
kx − zk1 ≤

1+ρ
2
(kzk1 − kxk1 + 2d1 (x, ΣS )) +
τ kA (x − z)k for all x, z ∈ Rn .
1−ρ
1−ρ

Note that by a modification of the proof this result also holds for q = ∞. The modifications on the proofs
of [FR13, Theorem 4.25] and [FR13, Theorem 4.20] are straight forward, only the modification of [FR13,
Theorem 2.5] might not be obvious. See also [PJ20]. As a consequence, all our statements also hold for q = ∞
with q1 := 0. If W ∈ RN ×N is a diagonal matrix, we can calculate some operator norms fairly easy:
kWkq→q := sup kWwkq = max |Wn,n | for all q ∈ [1, ∞] .
kwkq ≤1

n∈[N ]

We use this relation frequently over this section. Furthermore, we use [KJ18, Lemma 5] without adaption. For
the sake of completeness we add a short proof.
Lemma 6.2 ( [KJ18, Lemma 5] ). Let q ∈ [1, ∞) and suppose that A ∈ RM×N has ℓq -RNSP of order
S with respect to k·k with constants ρ and τ . Let W ∈ RN ×N be a diagonal matrix with Wn,n > 0. If
ρ′ = kWkq→q W−1 1→1 ρ < 1, then AW−1 has ℓq -RNSP of order S with respect to k·k with constants
ρ′ = kWkq→q W−1 1→1 ρ and τ ′ = kWkq→q τ .

15

Proof. Let v ∈ RN and # (T ) ≤ S. If we apply the RNSP of A for the vector W−1 v
k v|T kq = WW−1 ( v|T ) q ≤ kWkq→q W−1 ( v|T ) q = kWkq→q

 1

≤ kWkq→q ρS q −1 W−1 v T c 1 + τ AW−1 v
1

= kWkq→q ρS q −1 W−1 ( v|T c )
≤ kWkq→q W−1

1

1



, we get

W−1 v T q
T

+ kWkq→q τ AW−1 v

ρS q −1 k v|T c k1 + kWkq→q τ AW−1 v .

1→1

This finishes the proof.
Next we adapt [KJ18, Theorem 4] to account for arbitrary norms. Further, we obtain a slight improvement in
1
form of the dimensional scaling constant S q −1 . With this, our error bound becomes for S → ∞ asymptotically
the error bound of the basis pursuit denoising, whenever κ = 1 and q > 1 [FR13].
Proposition 6.3 ( Similar to [KJ18, Theorem 4] ). Let q ∈ [1, ∞) and k·k be a norm on RM with dual
norm k·k∗ . Suppose A has ℓq -RNSP of order S with respect to k·k with constants ρ and τ . Suppose A has the
M + criterion with vector t and constant κ and that κρ < 1. Then, we have
!
2
2
−1
1
1
3
+
κρ
(1
+
κρ)
(1 + κρ)
−1
−1
T
max A t n ktk∗ +
kx − zkq ≤2
κS q d1 (x, ΣS ) +
Sq
κτ kAz − Axk
n∈[N ]
1 − κρ
1 − κρ
1 − κρ
for all x, z ∈ RN
+.

If q = 1, this bound can be improved to
kx − zkq ≤2

1 + κρ
κd1 (x, ΣS ) +
1 − κρ

for all x, z ∈ RN
+.



−1
2
1 + κρ
max AT t n ktk∗ +
κτ
1 − κρ n∈[N ]
1 − κρ



kAz − Axk

Proof. Let x, z ≥ 0. In order to apply Lemma 6.2 we set W as the matrix with diagonal AT t and zero else. It
follows that Wn,n > 0 and kWkq→q W−1 1→1 ρ = κρ < 1. We can apply Lemma 6.2, which yields that AW−1

has ℓq -RNSP with constants ρ′ = kWkq→q W−1 1→1 ρ = κρ and τ ′ = kWkq→q τ = maxn∈[N ] AT t n τ . We
apply Theorem 6.1 with the matrix AW−1 , the vectors Wx, Wz and the constants ρ′ and τ ′ and get
2

kWx − Wzkq ≤

3 + ρ′ ′
(1 + ρ′ ) q1 −1
S
τ AW−1 (Wx − Wz)
(kWzk1 − kWxk1 + 2d1 (Wx, ΣS )) +
′
1−ρ
1 − ρ′
2

(1 + ρ′ ) q1 −1
3 + ρ′ ′
S
τ kAx − Azk
(kWzk1 − kWxk1 + 2 kWk1→1 d1 (x, ΣS )) +
′
1−ρ
1 − ρ′
2
2

1
(1 + κρ) q1 −1
(1 + κρ)
(kWzk1 − kWxk1 )
=2
max AT t n S q −1 d1 (x, ΣS ) +
S
1 − κρ n∈[N ]
1 − κρ

3 + κρ
+
max AT t n τ kAx − Azk .
1 − κρ n∈[N ]

≤

We lower bound the left hand side further to get
kx − zkq ≤ W−1

q→q

kWx − Wzkq = max

n∈[N ]

−1
AT t n kWx − Wzkq

2
2
−1
1
(1 + κρ)
(1 + κρ) q1 −1
max AT t n (kWzk1 − kWxk1 )
κS q −1 d1 (x, ΣS ) +
S
n∈[N ]
1 − κρ
1 − κρ
3 + κρ
+
κτ kAx − Azk .
1 − κρ

≤2

(4)


We want to estimate the term kWzk1 − kWxk1 using the M + criterion. Since z, x ≥ 0, Wn,n = AT t n > 0
and W is a diagonal matrix, we have
kWzk1 − kWxk1 =h1, Wzi − h1, Wxi = hWT 1, z − xi = hW1, z − xi
=ht, A (z − x)i ≤ ktk∗ kAz − Axk .

Applying this to (4) we get
1
(1 + κρ)2
κS q −1 d1 (x, ΣS ) +
kx − zkq ≤2
1 − κρ

−1
3 + κρ
(1 + κρ)2 1q −1
max AT t n ktk∗ +
S
κτ
n∈[N ]
1 − κρ
1 − κρ

If q = 1 we can repeat the proof with the improved bound of Theorem 6.1.
16

!

kAz − Axk .

After these auxiliary statements it remains to prove the main result of Section 3 about the properties of the
NNLR minimizer.
Proof of Theorem 3.4. By applying Proposition 6.3 with x and z := x# ≥ 0 we get
2

#

x−x

q

1
(1 + κρ)
≤2
κS q −1 d1 (x, ΣS ) +
1 − κρ

2

≤2

1
(1 + κρ)
κS q −1 d1 (x, ΣS )
1 − κρ

+

2
−1
3 + κρ
(1 + κρ) 1q −1
S
κτ
max AT t n ktk∗ +
1 − κρ
1 − κρ
n∈[N ]

2
−1
(1 + κρ) 1q −1
3 + κρ
S
κτ
max AT t n ktk∗ +
1 − κρ
1 − κρ
n∈[N ]
2

1
(1 + κρ)
≤2
κS q −1 d1 (x, ΣS ) + 2
1 − κρ

!

!

Ax# − y + kAx − yk



2
−1
3 + κρ
(1 + κρ) 1q −1
S
κτ
max AT t n ktk∗ +
1 − κρ
1 − κρ
n∈[N ]

Ax# − Ax

!

kAx − yk ,

where in the last step we used that x# is a minimizer and x is feasible. If q = 1, we can repeat the proof with
the improved bound of Proposition 6.3.

6.2

Proof of Convergence Guarantee

We provide the exact convergence guarantee of Section 4 and deduce it from [CP11].
Proposition 6.4 (Convergence Guarantee). Let A ∈ RM×N , y ∈ RM . Further, let τ, σ ∈ (0, ∞) be
−2
parameters with στ < kAk2→2 and x0 ∈ RN , w0 ∈ RM be initializations. Set v0 := x0 and for all k ∈ N0
inductively
 


k
k
+ σ Avk − y m m∈[M] ,
(iter 1)
wk+1 := min 1, wm
+ σ Avk − y m sgn wm

xk+1 :=PRN
(iter 2)
xk − τ AT wk+1 ,
+
vk+1 :=2xk+1 − xk ,
x̄k+1 :=

(iter 3)

k+1
k+1
1 X k′
1 X k′
x and w̄k+1 :=
w .
k+1 ′
k+1 ′
k =1

k =1

Then, the following statements hold true:
(1) The iterates and averages
converge:


The sequences xk k∈N and x̄k k∈N converge to a minimizer of argmin kAz − yk1 .
z≥0

(2) The iterates and averages are feasible:
We have xk ≥ 0, x̄k ≥ 0 and wk ∞ ≤ 1, w̄k

∞

≤ 1.

(3) There is a stopping criteria for the iterates:
limk→∞ Axk − y 1 + hy, wk i = 0 and limk→∞ AT wk ≥ 0. In particular, if Axk − y
and AT wk ≥ 0, then xk is a minimizer of argmin kAz − yk1 .

1

+ hy, wk i ≤ 0

z≥0

(4) The stopping criteria also holds for the averages by replacing xk with x̄k and wk with w̄k .
(5) The averages obey the convergencerate to optimal objective
 value:
2
2
1
1
Ax̄k − y 1 − Ax# − y 1 ≤ k1 2τ
x# − x0 2 + 2σ
w0 2 + 2 w0
mizer of argmin kAz − yk1 .

+M
1



, where x# is a mini-

z≥0

In order to prove Proposition 6.4 we introduce saddle point problems
and technical notations from optimization.

Let f : RN × RM → R ∪ {−∞, ∞}. If there exists x# , w# such that


sup inf f (x, w) = inf f x, w# = sup f x# , w = inf sup f (x, w)
N
w∈RM x∈R

x∈RN

x∈RN w∈RM

w∈RM


holds true, then x# , w# is called saddle point of f . In general we have for any point (x′ , w′ )
inf f (x, w′ ) ≤ f (x′ , w′ ) ≤ sup f (x′ , w) .

x∈RN

w∈RM

17

(5)

This yields that the inequality
inf f (x, w) ≤ inf

sup

N
w∈RM x∈R

sup f (x, w)

(6)

x∈RN w∈RM

holds true, but not necessarily with equality. The equality is a condition of the existence of a saddle point. The
problem inf x∈RN supw∈RM f (x, w) is called the primal problem, while the problem supw∈RM inf x∈RN f (x, w) is
called the dual problem. The difference inf x∈RN supw∈RM f (x, w) − supw∈RM inf x∈RN f (x, w) ≥ 0 is called the
duality gap. Further, (5) and (6) yield the logical statement
sup f (x′ , w) ≤ inf f (x, w′ ) ⇒ (x′ , w′ ) is a saddle point.

(7)

x∈RN

w∈RM

Given a function F : RN → R ∪ {−∞, ∞} its Fenchel conjugate is the function F ∗ : RN → [−∞, ∞], where
F ∗ (v) := supv∗ ∈RN hv, v∗ i − F (v∗ ). The fenchel conjugate has several interesting properties, however we only
require that if F is proper, convex and lower semicontinuous 3 , then also F ∗ is proper, convex and lower
semicontinuous and F ∗ ∗ = F holds true [Roc70, Theorem 12.2]. Given a proper, convex, lower-semicontinuous
function F : RN → R, the proximal point operator of F is the function ProxF (·) : RN → RN , where ProxF (v)
is the unique minimizer of
1
2
argmin kv∗ − vk2 + F (v∗ )
2
∗
N
v ∈R
[Roc70, Theorem 31.5]. For more information about saddle point problems, the fenchel conjugate and proximal
point operators we refer the reader to [Roc70]. We have now the necessary means to state [CP11, Theorem 1].
Theorem 6.5 ( [CP11, Theorem 1] ). Let F : RM → [0, ∞) be convex and lower semicontinuous. Let
G : RN → [0, ∞] and F ∗ : RM → [0, ∞) 4 be proper, convex and lower semicontinous functions and A ∈ RM×N .
Then, the function f (x, w) := hAx, wi + G (x) − F ∗ (w) has a saddle point.
−2
Further, let τ, σ ∈ (0, ∞) be parameters with στ < kAk2→2 and x0 ∈ RN , w0 ∈ RM be initializations. Set
v0 := x0 and for all k ∈ N0 inductively

(PP 1)
wk+1 =ProxσF ∗ wk + σAvk

k
T k+1
k+1
(PP 2)
x
=Proxτ G x − τ A w
vk+1 =2xk+1 − xk .

The sequence xk , w
the averages x̄k , w̄

(PP 3)


k


k

N
M
converges
point of
 f . Lastly, for any bounded sets B1 ⊂ R and B2 ⊂ R
 P to a′ saddle
P
′
k
k
:= k1 k′ =1 xk , k1 k′ =1 wk obey


 1
sup f x̄k , w − inf f x, w̄k ≤
x∈B1
k
w∈B2

sup
x∈B1 ,w∈B2



1
x − x0
2τ

2
2

+

1
w − w0
2σ

2
2



.

By a proper choice of F and G any saddle point of f will also give a minimizer of NNLAD. We denote this
proper choice in the next lemma.
Lemma 6.6 ( Relation of Saddle point and NNLAD ). Let y ∈ RM as well as


0
if
x≥0
F (w) := kw − yk1 and G (x) :=
and f (x, w) := hAx, wi + G (x) − F ∗ (w) .
∞
else
Then F, G, F ∗ , G∗ are proper, convex and lower semicontinuous. F ∗ and G∗ are given by




hw, yi
if
kwk∞ ≤ 1
0
if
x≤0
F ∗ (w) =
and G∗ (x) =
.
∞
if
kwk∞ > 1
∞
else
Further we have for x′ ∈ RN , w′ ∈ RM


kAx′ − yk1
if
x′ ≥ 0
∞
else
w∈RM


′
−hw , yi
if
kw′ k∞ ≤ 1 and AT w′ ≥ 0
′
and inf f (x, w ) =
.
−∞
else
x∈RN
sup f (x′ , w) =

3 Note that in general convex and lower semicontinuous need to be defined with the epigraph, since we allow F to attain the
values −∞ and ∞, which might result in undefined ∞ − ∞ terms. However, if F is proper as in our case, it can only attain ∞ and
thus the casual definitions of algebra coincide with the definitions used here.
4 Note that the result in [CP11] is only stated if F, G map to [0, ∞). From a private conversation with one of the authors we
learned that the result also holds if G maps to [0, ∞].

18

Proof. From the definition it is clear that F, G are proper, convex and lower semicontinuous. Hence F ∗ and G∗
are also proper, convex and lower semicontinuous. By a direct calculation we have


0
if
x≤0
∗
∗
∗
∗
G (x) = sup hx, x i − G (x ) = sup hx, x i =
.
∞
else
x∗ ≥0
x∗ ∈RN
For the other fenchel conjugate we calculate
F ∗ (w) = sup hw, w∗ i − kw∗ − yk1 = hw, yi + sup hw, w∗ i − kw∗ k1
w∗ ∈RM

w∗ ∈RM

X

=hw, yi + sup

w∗ ∈RM m∈[M]

∗
∗
wm wm
− |wm
| = hw, yi +

X

m∈[M]

sup wm w∗ − |w∗ | ,

w ∗ ∈R

where in the last step we used that each summand depends on exactly one component of w∗ . Now wm w∗ − |w∗ |
is larger for sgn (w∗ ) = sgn (wm ), than for sgn (w∗ ) 6= sgn (wm ). Hence, we can restrict the supremum to the
case sgn (w∗ ) = sgn (wm ) and obtain

X 0
X
if
|wm | ≤ 1
∗
∗
sup (|wm | − 1) w = hw, yi +
F (w) =hw, yi +
∞
if
|wm | > 1
w ∗ ∈R+
m∈[M]
m∈[M]


hw, yi
if
kwk∞ ≤ 1
=
.
∞
if
kwk∞ > 1
Since F is proper, convex and lower semicontinuous, we have F ∗ ∗ = F . Thus,

kAx′ − yk1
sup f (x′ , w) = G (x′ ) + sup hAx′ , wi − F ∗ (w) = G (x′ ) + F (Ax′ ) =
∞
w∈RM
w∈RM

if
else

x′ ≥ 0



.

And lastly we have
inf f (x, w′ ) = − F ∗ (w′ ) + inf hx, AT w′ i + G (x) = −F ∗ (w′ ) − sup hx, −AT w′ i − G (x)

x∈RN

x∈RN

∗

′

∗

= − F (w ) − G

T

−A w

which finishes the proof.


′

=



′

−hw , yi
−∞

if
else

x∈RN
kw′ k∞

≤ 1 and AT w′ ≥ 0



,

Further, we need to calculate the iterates for this choice of F and G and thus the proximal point operators. It is
well known that the proximal point operator of the ℓ1 -norm is the soft thresholding operator. Using Moreau’s
identity [Roc70, Theorem 31.5] one can find the desired iterates directly. See for instance [FR13, Example 15.7].
For the sake of completeness we added a proof.
Lemma 6.7. Let τ, σ > 0, y ∈ RM as well as
F (w) := kw − yk1 and G (x) :=



0
∞

if
else

x ∈ RN
+



.

Then
ProxσF
and in particular


 ym
(w) =  wm − σ

wm + σ

if
if
if


|wm − ym | ≤ σ 
wm − ym > σ 
for all w ∈ RM

wm − ym < −σ
m∈[M]

(x) and ProxσF ∗ (w) = (min {1, |wm − σym |} sgn (wm − σym ))m∈[M] .
Proxτ G (x) = PRN
+

(8)

(9)

Proof. The proximal point operator of an indicator function of a closed, convex set is always the projection to
the set, hence the identity for G follows. For F this is more difficult. Note that w′ is a minimizer of
1
2
argmin kw∗ − wk2 + σ kw∗ − yk1
w∗ ∈RM 2
if and only if zero is in the subdifferential at w′ , which is given by the set


′
w̃m = sgn (wm
− ym )
if
w′ − w + σ w̃ ∈ RM such that
w̃m ∈ [−1, 1]
if
19


′
wm
− ym =
6 0
.
′
wm
− ym = 0

Since the minimizer for the proximal operator is always unique, it remains to verify that zero is in the subdifferential at the vector from the statement. So let w′ be the vector from the right hand side of (8) and m ∈ [M ].
′
If wm − ym > σ, then wm
= wm − σ > ym and thus
′
′
(wm
− wm ) + σsgn (wm
− ym ) = −σ + σ = 0.
′
If wm − ym < −σ, then wm
= wm + σ < ym and thus
′
′
(wm
− wm ) + σsgn (wm
− ym ) = σ − σ = 0.
′
If |wm − ym | ≤ σ, we have wm
= ym and
′
|(wm
− wm )| = |ym − wm | ≤ σ,
′
and hence σ −1 (wm
− wm ) ∈ [−1, 1]. It follows that zero is a possible subgradient, i.e. the subdifferential
contains zero. Hence, w′ is the unique minimizer. To prove (9), we apply the first statement to calculate


ym
if
|wm − σym | ≤ 1 


if
wm − σym > 1 
.
Proxσ−1 F σ −1 w =  σ −1 (wm − 1)
 −1

σ (wm + 1)
if
wm − σym < −1
m∈[M]

It follows that

w − σProxσ−1 F


wm − σym
σ −1 w = 
1

−1


if
if
if


|wm − σym | ≤ 1 
wm − σym > 1 

wm − σym < −1
m∈[M]

= (min {1, |wm − σym |} sgn (wm − σym ))m∈[M] .

Using Moreau’s identity [Roc70, Theorem 31.5] yields

ProxσF ∗ (w) =w − Prox(σF ∗ )∗ (w) = w − σProxσ−1 F σ −1 w
= (min {1, |wm − σym |} sgn (wm − σym ))m∈[M] ,
which finishes the proof.
After proving these auxiliary statements it remains to prove the main result of Section 4 about the convergence
to a minimizer of NNLAD.
Proof of Proposition 6.4. We set
F (w) := kw − yk1 and G (x) :=



0
∞

if
else


x≥0

and f (x, w) := hAx, wi + G (x) − F ∗ (w) .

By Lemma 6.6 F, G, F ∗ , G∗ are proper, convex and lower-semicontinuous. Thus, the requirements of Theorem 6.5
are fulfilled, which yields that f has a saddle point and thus the duality gap is zero. By Lemma 6.6 and the
fact that the duality gap is zero, it follows that

argmax
− hw, yi.
(10)
x# , w# is a saddle point ⇔ x# ∈ argmin kAx − yk1 and w# ∈
w∈RM :AT w≥0,kwk∞ ≤1

x≥0

If (x′ , w′ ) are any points with x′ ≥ 0, kw′ k∞ ≤ 1 and AT w′ ≥ 0, then we have by Lemma 6.6
kAx′ − yk1 + hy, w′ i = sup f (x′ , w) − inf f (x, w′ ) .
w∈RM

x∈RN

If this is non-positive, (7) and (10) yield that x′ is a minimizer of NNLAD. Hence, it holds true that
kAx′ − yk1 + hy, w′ i ≤ 0 and x′ ≥ 0 and kw′ k∞ ≤ 1 and AT w′ ≥ 0 ⇒ x′ is minimizer of NNLAD. (11)
Lastly, by Lemma 6.7 the iterates calculated in (iter 1), (iter 2) and (iter 3) are exactly the iterates calculated
in (PP 1), (PP 2) and (PP 3) respectively.
We will now prove all statements. 

By Theorem 6.5 the sequence xk , wk converges to some saddle point x# , w# . Hence, xk converges to x# ,
which is a minimizer of NNLAD by (10). Since any sequence of averages converges to the same value as the
original sequence, statement (1) follows.
Since xk and wk are in the
of G and F ∗ respectively, they need to obey
 image of the proximal point operator
k
∗
k
k
G x < ∞ and F w < ∞. Lemma 6.6 yields the x ≥ 0 and wk ∞ ≤ 1. By convexity we obtain also
20

x̄k ≥ 0 and w̄k ∞ ≤ 1. Statement (2) is proven.


By Theorem 6.5 the sequence xk , wk converges to some saddle point x# , w# . By taking the limit, statement
(2) yields x# ≥ 0 and w# ∞ ≤ 1. The saddle point property and Lemma 6.6 implies


inf f x, w# = sup f x# , w = Ax# − y

x∈RN

w∈RM

1

< ∞.

By Lemma 6.6 again, this is only possible if w# is feasible, i.e.
AT w# ≥ 0.

(12)

Hence, limk→∞ AT wk ≥ 0 follows. By Lemma 6.6 and the feasibility of x# and w# we have


lim Axk − y 1 + hy, wk i = Ax# − y 1 + hy, w# i = sup f x# , w − inf f x, w#
k→∞

x∈RN

w∈RM


which is zero, since x# , w# is a saddle point. This yields the convergence in statement (3). Since any sequence
of averages converges to the same value as the original sequence, we also get the convergence of statement (4).
The in particular part of statements (3) and (4) follows from (11) and statement (2). Hence, statements (3)
and (4) are proven.

To prove the the remaining statement (5) we choose B1 := x# and B2 := {w : kwk∞ ≤ 1}. The bound of
Theorem 6.5 becomes
!

 1
1
1
k
#
k
#
0 2
0 2
sup f x̄ , w − f x , w̄ ≤
x −x 2+
w−w 2
sup
k 2σ
2τ kwk∞ ≤1
w∈B2


1 1
1  0 2
0
#
0 2
=
w 2+2 w 1+M
.
(13)
x −x 2+
k 2σ
2τ
By using Lemma 6.6 and the feasibility of x# we get


f x# , w̄k ≤ sup f x# , w = Ax# − y
w∈RM

1

.

(14)



Now let w̃ be a maximizer of supw∈RM f x̄k , w . By statement (2) we get G x̄k = 0 and thus w̃ is also a
minimizer of the convex function w → h−Ax̄k , wi + F ∗ (w). Hence, the subdifferential of this function needs
to contain zero at w̃. Since ∂F ∗ (w) = ∅ whenever kwk∞ > 1, we get kw̃k∞ ≤ 1. This together with the
feasibility of x̄k yields



sup f x̄k , w = f x̄k , w̃ = sup f x̄k , w = Ax̄k − y 1 .
(15)
w∈B2

w∈RM

Combining (13), (14) and (15) yields
Ax̄k − y

− Ax# − y
1

1

≤

1
k



1
x# − x0
2σ

2
2

+

1  0
w
2τ

2
2

+ 2 w0

+M
1



and finishes the proof.
We want to remark that the other feasibility assumptions AT wk ≥ 0 and AT w̄k ≥ 0 does not need to hold.

References
[AJS19]

Matthew Aldridge, Oliver Johnson, and Jonathan Scarlett. Group testing: An information theory perspective. Foundations and Trends® in Communications and Information Theory, 15(3-4):196–392, 2019.
doi:10.1561/0100000099.

[AP16]

Hedy Attouch and Juan Peypouquet.
The rate of convergence of nesterov’s accelerated forwardbackward method is actually faster than 1/k2 . SIAM Journal on Optimization, 26(3):1824–1834, 2016.
doi:10.1137/15M1046095.

[BEZ08]

Alfred M. Bruckstein, Michael Elad, and Michael Zibulevsky. On the uniqueness of non-negative sparse
& redundant representations. ICASSP, IEEE International Conference on Acoustics, Speech and Signal
Processing - Proceedings, (796):5145–5148, 2008. doi:10.1109/ICASSP.2008.4518817.

[BGI+ 08]

R. Berinde, A. C. Gilbert, P. Indyk, H. Karloff, and M. J. Strauss. Combining geometry and combinatorics:
A unified approach to sparse signal recovery. In 2008 46th Annual Allerton Conference on Communication,
Control, and Computing, pages 798–805, 2008.

21

[Boy14]

Stephen Boyd (with help from Jaehyun Park). “Subgradient Methods”, Notes for EE364b, 2013–14. URL:
http://stanford.edu/class/ee364b/lectures/subgrad_method_notes.pdf.

[BT09]

Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAM Journal on Imaging Sciences, 2(1):183–202, 2009. doi:10.1137/080716542.

[CP11]

Antonin Chambolle and Thomas Pock. A first-order primal-dual algorithm for convex problems with
applications to imaging.
Journal of Mathematical Imaging and Vision, 40(1):120–145, May 2011.
doi:10.1007/s10851-010-0251-1.

[CRT06]

E. J. Candes, J. Romberg, and T. Tao. Robust uncertainty principles: exact signal reconstruction from
highly incomplete frequency information. IEEE Transactions on Information Theory, 52(2):489–509, Feb
2006. doi:10.1109/TIT.2005.862083.

[DLR18]

S. Dirksen, G. Lecué, and H. Rauhut. On the gap between restricted isometry properties and sparse recovery
conditions. IEEE Transactions on Information Theory, 64(8):5478–5487, 2018.

[Don06]

D. L. Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289–1306, 2006.
doi:10.1109/TIT.2006.871582.

[Dor43]

Robert Dorfman. The detection of defective members of large populations. The Annals of Mathematical
Statistics, 14(4):436–440, 1943. URL: http://www.jstor.org/stable/2235930.

[DT05]

David L. Donoho and Jared Tanner. Sparse nonnegative solution of underdetermined linear equations by
linear programming. Proceedings of the National Academy of Sciences of the United States of America,
102(27):9446–9451, 2005. URL: http://www.jstor.org/stable/3375994.

[DT10]

David L. Donoho and Jared Tanner. Counting the faces of randomly-projected hypercubes and orthants, with
applications. Discrete and Computational Geometry, 43(3):522–541, 2010. doi:10.1007/s00454-009-9221-z.

[FR13]

Simon Foucart and Holger Rauhut. A Mathematical Introduction to Compressive Sensing. Birkhäuser Basel,
2013. doi:10.1007/978-0-8176-4948-7.

[GB14]

Michael Grant and Stephen Boyd. CVX: Matlab software for disciplined convex programming, version 2.1.
http://cvxr.com/cvx, March 2014.

[GI10]

A. Gilbert and P. Indyk. Sparse recovery using sparse matrices. Proceedings of the IEEE, 98(6):937–947,
2010.

[HTW15]

Trevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical Learning with Sparsity: The Lasso and
Generalizations. Chapman & Hall/CRC, 2015.

[JJ20]

Fabian Jaensch and Peter Jung. Robust recovery of sparse nonnegative weights from mixtures of positivesemidefinite matrices, 2020. URL: https://arxiv.org/abs/2003.12005, arXiv:2003.12005.

[JXHC09] S. Jafarpour, W. Xu, B. Hassibi, and R. Calderbank. Efficient and robust compressed sensing using optimized
expander graphs. IEEE Transactions on Information Theory, 55(9):4299–4308, 2009.
[Kü19]

Christian
Kümmerle.
Understanding
and
Enhancing
Data
Recovery
rithms.
Dissertation,
Technische
Universität
München,
München,
2019.
http://nbn-resolving.de/urn/resolver.pl?urn:nbn:de:bvb:91-diss-20191219-1521436-1-8.

AlgoURL:

[KDXH11] M. A. Khajehnejad, A. G. Dimakis, W. Xu, and B. Hassibi. Sparse recovery of nonnegative signals with
minimal expansion. IEEE Transactions on Signal Processing, 59(1):196–208, 2011.
[KJ18]

R. Kueng and P. Jung.
Robust nonnegative sparse recovery and the nullspace property of
0/1 measurements.
IEEE Transactions on Information Theory, 64(2):689–703, 2018.
URL:
https://arxiv.org/abs/1603.07997, arXiv:1603.07997.

[KKRT16] Maryia Kabanava, Richard Kueng, Holger Rauhut, and Ulrich Terstiege.
Stable low-rank matrix
recovery via null space properties.
Information and Inference: A Journal of the IMA, 5(4):405–
441, 08 2016.
arXiv:https://academic.oup.com/imaiai/article-pdf/5/4/405/8395013/iaw014.pdf,
doi:10.1093/imaiai/iaw014.
[MC16]

Veniamin I. Morgenshtern and Emmanuel J. Candès. Super-resolution of positive sources: The discrete setup.
SIAM Journal on Imaging Sciences, 9(1):412–444, 2016. arXiv:https://doi.org/10.1137/15M1016552,
doi:10.1137/15M1016552.

[MS08]

Grant M.C. and Boyd S.P. Graph implementations for nonsmooth convex programs. In V. Blondel, S. Boyd,
and H. Kimura, editors, Recent Advances in Learning and Control, volume 371 of Lecture Notes in Control and
Information Sciences, pages 95–110. Springer-Verlag Limited, 2008. doi:10.1007/978-1-84800-155-8_7.

[Nes04]

Yurii E. Nesterov. Introductory Lectures on Convex Optimization - A Basic Course, volume 87 of Applied
Optimization. Springer, 2004. doi:10.1007/978-1-4419-8853-9.

[PJ20]

Hendrik Bernd Petersen and Peter Jung. Robust instance-optimal recovery of sparse signals at unknown
noise levels. to appear on arXiv, 2020.

[Pol87]

Boris T. Polyak. Introduction to optimization. Translations series in mathematics and engineering. New
York: Optimization Software, Inc, 1987.

[Roc70]

R. Tyrrell Rockafellar. Convex analysis. Princeton Mathematical Series. Princeton University Press, Princeton, N. J., 1970.

22

[SH11]

Martin
Slawski
and
Matthias
Hein.
Sparse
recovery
by
thresholded
non-negative
least
squares.
pages
1926–1934,
2011.
URL:
http://papers.nips.cc/paper/4231-sparse-recovery-by-thresholded-non-negative-least-squares.pdf.

[SH13]

Martin Slawski and Matthias Hein.
Non-negative least squares for high-dimensional linear models: Consistency and sparse recovery without regularization. Electron. J. Statist., 7:3004–3056, 2013.
doi:10.1214/13-EJS868.

[SJC19]

Yonatan Shadmi, Peter Jung, and Giuseppe Caire. Sparse non-negative recovery from biased subgaussian
measurements using NNLS. CoRR, abs/1901.05727, 2019. arXiv:1901.05727.

[Vad12]

Salil P. Vadhan. Pseudorandomness. Foundations and Trends® in Theoretical Computer Science, 7(1–3):1–
336, 2012. doi:10.1561/0400000010.

[vdBF09]

Ewout van den Berg and Michael P. Friedlander. Probing the pareto frontier for basis pursuit solutions.
SIAM Journal on Scientific Computing, 31(2):890–912, 2009. doi:10.1137/080714488.

[vdBF19]

E. van den Berg and M. P. Friedlander. SPGL1: A solver for large-scale sparse reconstruction, December
2019. URL: https://friedlander.io/spgl1.

[WXT11]

Meng Wang, Weiyu Xu, and Ao Tang. A unique ”nonnegative” solution to an underdetermined
system: From vectors to matrices. IEEE Transactions on Signal Processing, 59(3):1007–1016, 2011.
doi:10.1109/TSP.2010.2089624.

[XH07]

W. Xu and B. Hassibi. Efficient compressive sensing with deterministic guarantees using expander graphs.
In 2007 IEEE Information Theory Workshop, pages 414–419, 2007.

23

