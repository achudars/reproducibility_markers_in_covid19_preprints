COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval
Xinliang (Frederick) Zhang1 , Heming Sun1 , Xiang Yue1 , Emmett Jesrani1 , Simon Lin2 , and Huan Sun1
1

The Ohio State University
Abigail Wexner Research Institute at Nationwide Children’s Hospital
{zhang.9975, sun.2164, yue.149, jesrani.2, sun.397}@osu.edu
Simon.Lin@nationwidechildrens.org
2

arXiv:2010.12800v1 [cs.CL] 24 Oct 2020

Abstract
We present a large challenging dataset,
COUGH, for COVID-19 FAQ retrieval. Specifically, similar to a standard FAQ dataset,
COUGH consists of three parts: FAQ Bank,
User Query Bank and Annotated Relevance
Set. FAQ Bank contains ∼16K FAQ items
scraped from 55 credible websites (e.g., CDC
and WHO). For evaluation, we introduce
User Query Bank and Annotated Relevance
Set, where the former contains 1201 humanparaphrased queries while the latter contains ∼32 human-annotated FAQ items for
each query. We analyze COUGH by testing different FAQ retrieval models built on
top of BM25 and BERT, among which the
best model achieves 0.29 under P@5, indicating that the dataset presents a great
challenge for future research. Our dataset
is freely available at https://github.
com/sunlab-osu/covid-faq.

1

Introduction

Considering that the contagious and often fatal
COVID-19 pandemic is still evolving on a daily
basis, the need for up-to-date information has
never been more urgent than it is today. Institutional websites (e.g., CDC and WHO) provide quality information on COVID-19 and maintain regularly-updated frequently asked questions
(FAQ) pages. However, given a user query, there
can be multiple websites mentioning similar topics, and users often need to screen through all of
them to gather the most relevant.
To advance the COVID-19 information search,
we present an FAQ dataset, COUGH1 , consisting
of three parts: FAQ Bank, User Query Bank and
Annotated Relevance Set, following the criteria
of constructing a standard FAQ dataset (Manning
et al., 2008). The FAQ Bank contains 15919 FAQ
items scraped from 55 authoritative institutional
1

The same pronunciation as “COF”: COVID-19 FAQ.

COUGH: The COVID-19 FAQ Dataset

FAQ Bank
Question1: Should children wear masks?
Answer1: In general, children 2 years and older should
wear a mask...Appropriate and consistent use of masks...
Question2: Coping with Self-Quarantine
Answer2: Remind yourself that difficult emotions are
normal during self-quarantine...
Question3: COVID-19是如何在⼈与⼈之间传播的?
(How does COVID-19 spread between people?)
Answer3: ...该病毒的⼈际传播主要通过感染者与他⼈
密切接触...(...mainly when an infected person is in close
contact with another person...)

User Query Bank
Query1: Is it possible for human beings to get sick with
COVID-19 transmitted to them from animals?
Query2: Is it possible to get infected by COVID 19 if I
touch food surface packaging?

Annotated Relevance Set
Query

Relevant FAQ in FAQ Bank

Q: Can wild animals spread the virus that causes COVID-19 to
Query1 people or pets? A: Currently, there is no evidence to suggest...
Q: How is COVID-19 transmitted? A: COVID-19 illness is
Query1 spread mainly from person to person through respiratory...
Q: What are the lab protocols for identifying the virus in food?
Query2 On surfaces?A: As food hasn't been implicated in transmission

Score
3.67
2.67
3.67

Figure 1: Examples from the COUGH dataset.

websites (see a full list in Table A1 and A2).
COUGH covers a wide spectrum of perspectives
on COVID-19, ranging from general information
about the virus to specific COVID-19-related instructions for a healthy diet. Examples from our
dataset are shown in Figure 1. For evaluation, we
further construct User Query Bank and Annotated
Relevance Set, which include 1201 crowd-sourced
user queries and their relevance to a set of FAQ
items judged by human annotators.
Our dataset poses several new challenges (e.g.,
the answers being long and noisy, and hard to
match) to existing FAQ retrieval models. The diversity of FAQ items, which is reflected in their
varying length across websites as well as in their
narrative style (since collected from distinct organizational websites), also contributes to these challenges.

The contribution of this work is two-fold. First,
we construct a challenging dataset COUGH and
make it publicly available to aid the development
of COVID-19 FAQ retrieval models. Second,
we conduct extensive experiments using various
SOTA models across different settings, and explore limitations of current FAQ retrieval models
and discuss future work along this direction.

2

Related Work

COVID-19 FAQ Datasets. Since the outbreak
of COVID-19, the community has witnessed a
great number of datasets released to advance the
research pertaining to COVID-19. For example,
CORD-19 (Wang et al., 2020), CODA-19 (Huang
et al., 2020), COVID-Q (Wei et al., 2020), WeiboCov (Hu et al., 2020), and COVID-19 Twitter
dataset (Chen et al., 2020). All of them aim to
aggregate resources and efforts to help combat
COVID-19.
The most related works to ours are (Sun and Sedoc, 2020) and (Poliak et al., 2020), both of which
constructed a collection of COVID-19 FAQs by
scraping authoritative websites. However, none
of COVID-19 FAQ datasets is publicly available yet as of the release date of COUGH. In the
open domain, several FAQ datasets appeared recently, such as FAQIR (Karan and Šnajder, 2016),
StackFAQ (Karan and Šnajder, 2018) and LocalGov (Sakata et al., 2019). FAQIR and StackFAQ were constructed upon Yahoo! and StackExchange forums, respectively, and LocalGov was
created by scraping official websites. Unfortunately, as shown in Table 1, the scale of existing
FAQ datasets is too small, and answer lengths are
also much lower compared with those in COUGH,
which may not represent the real difficulty of the
FAQ task in the real-world scenario. Moreover,
in contrast to prior datasets, COUGH covers multiple query forms (such as question form and query
string form) and has sufficiently many annotated
FAQs for a particular query, whereas queries in
existing FAQ datasets are limited to the question
form and only have few annotations.
FAQ Retrieval Methodologies. The standard
practice in FAQ retrieval focuses on retrieving
the most-matched FAQ items given a user query
(Karan and Šnajder, 2018). Many earlier works,
such as FAQ FINDER (Burke et al., 1997),
query expansion (Kim and Seo, 2006) and BM25
(Robertson and Zaragoza, 2009), resorted to tra-

ditional information retrieval techniques by leveraging lexical mapping and/or semantic similarity. Later on, in the deep learning era, many
studies show that Deep Neural Networks models are useful for FAQ retrieval tasks as they are
good at learning the semantic relevance between
queries and FAQ items. Along this line, Karan and
Šnajder (2016) adopted Convolution Neural Networks (CNN), Gupta and Carvalho (2019) utilized
Long Short-Term Memory (LSTM), and Sakata
et al. (2019) leveraged an ensemble of TSUBAKI
(Shinzato et al., 2012) and BERT (Devlin et al.,
2019). Recently, Mass et al. (2020) proposed
CombSum and PoolRank, ensembles of BM25
and BERT models, which effectively learn ranking without requiring manual annotations utilizing
weak supervision.

3

Dataset Construction

A typical research-oriented FAQ dataset (Manning
et al., 2008) consists of three parts: FAQ Bank,
User Query Bank and Annotated Relevance Set.
In this section, we will describe how we construct
each of the three in detail. We provide detailed
annotation protocols in Appendix A to facilitate
future research.
3.1

FAQ Bank Construction

We developed scrapers based on JHU-COVID-QA
library2 with modifications to enable special features of our COUGH dataset.
Web scraping: We collect FAQ items from authoritative international organizations, state governments and some other credible websites including reliable encyclopedias and medical forums.
Moreover, we scrape three types of FAQs: question form (i.e., an interrogative statement), query
string (i.e., a string of words to elicit information)
form and forum form (FAQs scrapped from medical forums). Inspired by Manning et al. (2008), we
loosen the constraint that queries must be in question form since we want to study a more generic
and challenging problem. We also scrape 6768
non-English FAQs to increase language diversity.
Overall, we scraped a total of 15919 FAQ items
covering all three types and 19 languages. All
FAQ items were collected and finalized on Aug.
30th , 2020.
2
https://github.com/JHU-COVID-QA/
scraping-qas

Domain
# of FAQs
# of Queries (Q)
# of annotations per Q
Query Length
FAQ-query Length
FAQ-answer Length
Language
# of sources

FAQIR
(Karan and Šnajder)
Yahoo!
4313
1233
8.22
7.30
12.30
33.00
English
1

StackFAQ
(Karan and Šnajder)
StackExachange
719
1249
Not Applicable
13.84
10.39
76.54
English
1

LocalGov
(Sakata et al.)
Government
1786
784
<10
**
**
**
Japanese
1

Sun and Sedoc

Poliak et al.

COUGH (ours)

COVID-19
690
6495*
5
**
**
**
English
12

COVID-19
2115
24240*
5
**
**
**
Multi-lingual
34

COVID-19
15919
1201
32.17
12.97
13.00
113.58
Multi-lingual
55

Table 1: Comparison of COUGH with representative counterparts. *: Extracted from existing resources (e.g.,
COVID-19 Twitter dataset (Chen et al., 2020)). **: Not Applicable, as either not in English or not publicly
available. We resort to crowd-sourcing similarly as open-domain FAQ datasets construction (rationales explained
in Section 3.2).

3.2

User Query Bank Construction

Type
Question
Query String
Forum
Question
Query String
-

Number
4978
2139
2034
3396
3372
15919

Q-Length
14.64
9.18
147.46
-

A-length
123.89
89.60
90.49
-

Following Karan and Šnajder (2016); Manning
et al. (2008), we do not crowdsource queries from
scratch, but instead ask annotators to paraphrase
our provided query templates. In this way, we can
ensure that 1) the collected queries are pertinent
to COVID-19; 2) the collected queries are not too
simple; 3) the chance of getting (nearly) duplicate
user queries is reduced.
Phase 1: Query Template Creation: We sample
5% of FAQ items from each English non-forum
source and use the question part as the template.
Phase 2: Paraphrasing for Queries: In this
phase, each annotator is expected to give three
paraphrases for each query template. Annotators are encouraged to give deep paraphrases (i.e.,
grammatically different but semantically similar/same) to simulate the noisy and diverse environment in real scenarios. In the end, we obtain
1236 user queries.

et al. (2019). In order to reduce the variance and
bias in annotation, each tuple has at least 3 annotation scores. In our finalized Annotated Relevance
Set, we keep all raw scores and include two additional labels: 1) mean of raw annotation scores;
2) binary label (positive/negative). We identify all
tuples with mean score greater than 3 as positive
examples.
Among 1236 user queries, we find that there
are 35 “unanswerable” queries that have no associated positive FAQ item. In the end, there are 1201
user queries involved for evaluation after removing “unanswerable” queries.

3.3

4

Annotated Relevance Set Construction

Phase 1: Initial Candidate Pool Construction:
For each user query, as suggested by previous
works (Manning et al., 2008; Karan and Šnajder,
2016; Sakata et al., 2019), we run 4 models3 ,
BM25 (Q-q), BM25 (Q-q+a), BERT (Q-q) and
BERT (Q-a) fine-tuned on COUGH, to instantiate
a candidate FAQ pool. Each model complements
the others and contributes its top-10 relevant FAQ
items. We then take the union to remove duplicates, giving an average pool size of 32.2.
Phase 2: Human Annotation: Each annotator
gives each hQuery, FAQ itemi tuple a score based
on the annotation scheme (i.e., Matched (4), Useful (3), Useless (2) and Non-relevant (1)) which
is adapted from Karan and Šnajder (2016); Sakata
3

Explanations of these models are in Section 5.2.

# English
# Non-English
# Total

Table 2: Basic statistics of FAQ bank in COUGH.

Dataset Analysis

Besides the generic features of large size, diversity, and low noise in annotations, we list four additional special features of COUGH.
Varying Query Forms: As indicated in Table
2, there are multiple query forms. In evaluation,
we include both question and query string forms.
These two distinct forms are significantly different
in terms of query format (interrogative v.s. declarative), average answer length (123.89 v.s. 89.60)
and topics. Question form is usually related to
general information about the virus while query
string form is often searching for more specific
instructions concerning COVID-19 (e.g., healthy
diet during pandemic). In Figure 1, the first FAQ
item is in question form while the second one is in
query string form.

Answer Nature: Table 1 shows the answer fields
in COUGH are much longer than those in any prior
dataset. Moreover, we also observe that answers
might contain some contents which are not directly pertinent to the query, partially resulting in
the long length nature. For example, in COUGH,
the answer to a query “What is novel coronavirus” contains extra information about comparisons against other virus in the history, which is
not directly germane to the query. Such natures
better manifest the difficulty of FAQ retrieval in
real scenarios, where FAQs are long and noisy.
Large-scale Relevance Annotation: Many existing FAQ datasets seemed to overlook annotation
scale (see Table 1). However, that would hurt the
reliability of evaluating since many true positive
hQuery, FAQ itemi tuples are omitted. As suggested in (Manning et al., 2008), for each user
query, we constructed a relatively large-scale candidate pool to reduce the chance of missing true
positive tuples. Based on this annotation procedure, we obtained 39760 annotated hQuery, FAQ
itemi tuples, each of which is annotated by at least
three annotators to reduce annotation bias. Furthermore, we find that there are 7856 (19.76%)
positive tuples (i.e., mean score > 3). Besides,
from the perspective of FAQ Bank, 6648 FAQ
items among 7117 English non-forum items appear at least once in Initial Candidate Pool (as described in Section 3.3), and 3790 of them have at
least one “matched” (i.e., positive) user query.
Multilingual: COUGH includes 6768 FAQ items
covering 18 non-English languages. However,
since we do not include them in evaluation at
the current stage (see Section 5.3), we leave the
discussion of this multilingual perspective to Appendix B.
Table A1 and A2 in the Appendix show the detailed breakdown of COUGH dataset by sources for
English and non-English FAQ items, respectively.

5
5.1

Experiment
Experimental Setup

The FAQ retrieval task is defined as ranking FAQ
items {(qi , ai )} from an FAQ Bank given a user
query Q. In the FAQ retrieval literature (Karan and
Šnajder, 2016; Sakata et al., 2019), a user query
Q can be learned to match with the question field
qi , the answer field ai or their concatenation (i.e.,
FAQ tuple) qi + ai .
In this work, we only focus on the unsuper-

BM25 (Q-q)
BM25 (Q-a)
BM25 (Q-q+a)
BERT (Q-q) w/o finetune
+ finetune on pesudo Q-q
BERT (Q-a) w/o finetune
+ finetune on FAQ Bank
Ensemble: BERT (Q-q) + BERT (Q-a)
+ fintune on pesudo Q-q
+ fintune on pesudo Q-q and FAQ Bank

P@5
0.27
0.16
0.25
0.29
0.26
0.06
0.14
0.23
0.26
0.27

MAP
0.38
0.23
0.34
0.42
0.38
0.12
0.22
0.31
0.35
0.39

MRR
0.56
0.34
0.52
0.59
0.60
0.17
0.34
0.52
0.57
0.59

Table 3: Automatic Evaluation on COUGH. BERT
refers to Sentence-BERT (Reimers and Gurevych,
2019).

vised models since the size of User Query Bank is
not sufficiently large for supervised learning, especially for fine-tuning complex language models
like BERT. We experiment with commonly-used
and SOTA unsupervised baseline models to understand their limitations.
5.2

Methods

In this work, we test three types of baseline models, which are described below. For each baseline model, there are three configurable modes, Qq, Q-a and Q-q+a, where we match user queries
(Q) against the question field (q) and answer field
(a) of an FAQ item as well as their concatenation
(q+a), respectively.
(1) BM25 is a nonlinear combination of term
frequency, document frequency and document
length, and is a commonly adopted IR baseline.
(2) BERT (Devlin et al., 2019) is a pretrained
language model, which has significantly altered
the NLP landscape. We utilize Sentence-BERT4
(Reimers and Gurevych, 2019) in experiments,
an improved version of BERT specialized in
generating meaningful sentence-level representations. Sentence-BERT model is pre-trained
via a Siamese network built for comparison between sentence-pair embeddings (Reimers and
Gurevych, 2019) on Multi-NLI (Williams et al.,
2018), SNLI (Bowman et al., 2015) and Quora
dataset (Iyer et al., 2017).
Fine-tuning: We further fine-tune SentenceBERT on our FAQ bank using Multiple Negatives
Ranking Loss (Henderson et al., 2017)5 . For the
Q-q mode, similar to Mass et al. (2020), we leverage T5 (Raffel et al., 2019) to generate synthetic
4

https://www.sbert.net/docs/
pretrained_models.html
5
For efficiency and simplicity, Multiple Negatives Ranking Loss is computed using answers of other FAQs in the
same training batch of the true FAQ item as negative answers.

questions as positive examples. We further filter
out low-quality synthetic questions utilizing Elasticsearch6 . For the Q-a mode, an FAQ item itself
constitutes a positive example. For both modes,
negative examples are randomly sampled.
(3) Ensemble method, used by Mass et al.
(2020)7 : It first computes three matching scores
between the user query and FAQ items via BM25
Q-q+a, a particular BERT Q-q8 and a particular
BERT Q-a model, respectively. Then the three raw
scores are normalized, and combined by averaging.
5.3

Evaluation

Evaluation Setting: For the scope of this work,
we only use English non-forum FAQ items in evaluation, and leave non-English and forum FAQ
items for future research as we have already observed great challenges under the current setting.
Though forum or non-English items are not included in evaluation, we do encourage investigators who use COUGH for future research consider
utilizing these two categories for other potential
applications (e.g., multi-lingual IR, transfer learning in IR).
Evaluation Metric: We adopt our binary label
(positive/negative) as ground truth labels. Following previous work (Karan and Šnajder, 2016,
2018; Sakata et al., 2019), we adopt the widelyused MAP (Mean Average Precision)9 , MRR
(Mean Reciprocal Rank) and P@5 (Precision at
top 5) metrics.

structures and semantic meanings. As discussed in
Section 4, the answer nature (lengthy and noisy),
albeit well characterizes FAQ retrieval in real scenarios, does bring a great challenge to current FAQ
retrieval models.
We observe that fine-tuning in the way we experimented with can only help improve the performance of Q-a mode by a small margin, but might
slightly hurt the Q-q mode due to the noises introduced in generating synthetic queries. Moreover, ensemble models don’t perform as well as
expected, since the particular Q-a model involved
is weak (even after fine-tuning), which negatively
impacts the performance. In consequence, doing
straightforward fine-tuning or ensembing simply
by stacking models wouldn’t improve the performance significantly, which confirms that COUGH
is a challenging dataset. Interesting future work
includes developing more advanced techniques to
handle long and noisy answer fields

6

Conclusion

In this paper, we introduce COUGH, a large challenging dataset for COVID-19 FAQ retrieval.
COUGH features varying query forms, long and
noisy answer natures and multilingualism. COUGH
will also serve as a better evaluation benchmark
since it has large-scale relevance annotations. Results show the limitations of current FAQ retrieval
models. Our COUGH dataset is freely available online to encourage more research on FAQ retrieval.

Acknowledgments
5.4

Results

The results of baseline models are listed in Table
3. Sentence-BERT Q-q achieves the best overall
performance across all metrics. Among the three
metrics, P@5 is most critical when it comes to giving users the most relevant FAQ items. However,
the current best score 0.29 under this metric is far
from satisfactory, showing a large room for improvement.
It is not surprising to see that Q-q mode consistently performs better than Q-a mode regardless of
underlying models. This is mainly caused by the
fact that question fields are more similar to user
queries than answer fields, in terms of syntactic
6

https://www.elastic.co/
We only present CombSum, since PoolRank proposed by
Mass et al. (2020) gives inferior results on COUGH.
8
For example, sentence-BERT (Q-q) w/o finetune
9
Evaluated on top-100 retrieved FAQ items.
7

This research was sponsored in part by the PatientCentered Outcomes Research Institute Funding ME-2017C1-6413, the Army Research Office under cooperative agreements W911NF-171-0412, NSF Grant IIS1815674, NSF CAREER
#1942980, and Ohio Supercomputer Center (Center, 1987). The views and conclusions contained
herein are those of the authors and should not be
interpreted as representing the official policies, either expressed or implied, of the Army Research
Office or the U.S.Government. The U.S. Government is authorized to reproduce and distribute
reprints for Government purposes notwithstanding
any copyright notice herein.

References
Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference.
In EMNLP’15, pages 632–642.
Robin Burke, Kristian Hammond, Vladimir Kulyukin,
Steven Lytinen, Noriko Tomuro, and Scott Schoenberg. 1997. Question answering from frequently
asked question files: Experiences with the faq finder
system. AI Magazine, pages 57–66.
Ohio Supercomputer Center. 1987. Ohio supercomputer center.
Emily Chen, Kristina Lerman, and Emilio Ferrara.
2020. Tracking social media discourse about the
covid-19 pandemic: Development of a public coronavirus twitter data set. JMIR Public Health and
Surveillance.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understanding. In NAACL ’19, pages 4171–4186.

Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schütze. 2008. Introduction to Information
Retrieval. Cambridge University Press.
Yosi Mass, Boaz Carmeli, Haggai Roitman, and David
Konopnicki. 2020. Unsupervised faq retrieval with
question generation and bert. In ACL’20, pages
807–812.
Adam Poliak, Max Fleming, Cash Costello, Kenton W
Murray, Mahsa Yarmohammadi, Shivani Pandya,
Darius Irani, Milind Agarwal, Udit Sharma, Shuo
Sun, Nicola Ivanov, Lingxi Shang, Kaushik Srinivasan, Seolhwa Lee, Xu Han, Smisha Agarwal, and
João Sedoc. 2020. Collecting verified covid-19
question answer pairs.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2019. Exploring the limits
of transfer learning with a unified text-to-text transformer. CoRR, abs/1910.10683.
Nils Reimers and Iryna Gurevych. 2019. Sentencebert: Sentence embeddings using siamese bertnetworks. In EMNLP ’19, pages 3980–3990.

Sparsh Gupta and Vitor R. Carvalho. 2019. Faq retrieval using attentive matching. In SIGIR’19, page
929–932.

Stephen Robertson and Hugo Zaragoza. 2009. The
probabilistic relevance framework: Bm25 and beyond. Foundations and Trends® in Information
Retrieval, pages 333–389.

Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun
hsuan Sung, Laszlo Lukacs, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, and Ray Kurzweil. 2017. Efficient natural language response suggestion for smart
reply. CoRR, abs/1705.00652.

Wataru Sakata, Tomohide Shibata, Ribeka Tanaka, and
Sadao Kurohashi. 2019. FAQ retrieval using query
question similarity and bert-based query-answer relevance. In SIGIR’19, pages 1113–1116.

Yong Hu, He yan Huang, An fan Chen, and Xian Ling
Mao. 2020. Weibo-cov: A large-scale covidCoRR,
19 social media dataset from weibo.
abs/2005.09174.

Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
and Sadao Kurohashi. 2012. Tsubaki: An open
search engine infrastructure for developing information access methodology. Journal of Information
Processing, pages 216–227.

Ting-Hao (Kenneth) Huang, Chieh-Yang Huang,
Chien-Kuang Cornelia Ding, Yen-Chia Hsu, and
C. Lee Giles. 2020. Coda-19: Using a non-expert
crowd to annotate research aspects on 10,000+ abstracts in the covid-19 open research dataset.
Shankar Iyer, Nikhil Dandekar, and Kornél Csernai. 2017. First quora dataset release: Question
pairs.
Https://www.quora.com/q/quoradata/FirstQuora-Dataset-Release-Question-Pairs.
Mladen Karan and Jan Šnajder. 2016. Faqir - a frequently asked questions retrieval test collection. In
Text, Speech, and Dialogue (TSD), pages 74–81.
Mladen Karan and Jan Šnajder. 2018. Paraphrasefocused learning to rank for domain-specific frequently asked questions retrieval. In Expert Systems
with Applications,, pages 418–433.
Harksoo Kim and Jungyun Seo. 2006.
Highperformance faq retrieval using an automatic clustering method of query logs. Information Processing &
Management, pages 650 – 661.

Shuo Sun and João Sedoc. 2020. An analysis of bert
faq retrieval models for covid-19 infobot.
Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar,
Russell Reas, Jiangjiang Yang, Doug Burdick, Darrin Eide, Kathryn Funk, Yannis Katsis, Rodney
Kinney, Yunyao Li, Ziyang Liu, William Merrill,
Paul Mooney, Dewey Murdick, Devvret Rishi, Jerry
Sheehan, Zhihong Shen, Brandon Stilson, Alex
Wade, Kuansan Wang, Nancy Xin Ru Wang, Chris
Wilhelm, Boya Xie, Douglas Raymond, Daniel S.
Weld, Oren Etzioni, and Sebastian Kohlmeier. 2020.
Cord-19: The covid-19 open research dataset.
CoRR, abs/2004.10706.
Jerry Wei, Chengyu Huang, Soroush Vosoughi, and Jason Wei. 2020. What are people asking about covid19? a question classification dataset.
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL
’18, pages 1112–1122.

Appendix A

Annotation Protocols

We published our annotation batches on Amazon
Mechanical Turk platform. Annotation protocols
are provided below to facilitate future research in
FAQ retrieval:
A.1

Task 1: Query Bank Construction

For this task, you are expected to give one shallow
paraphrase and two deep paraphrases for the query
template. Note that query can be either in question
form or query string form.
• Shallow paraphrase: Applying word substitution,
Figure A1: Language distribution for non-English FAQ
sentence reordering and other lexical tricks (e.g.
items
extracting salient phrases from response) to the
original query to come up with another query
without changing the meaning.
• Non-relevant: The candidate FAQ is completely
unrelated to the query.
• Deep paraphrase: The paraphrased ones should
look dramatically (i.e. grammatically) differAppendix B Multilingual Perspective
ent from the original query which is more than
Figure A1 shows the language distribution (exshallow paraphrasing. However, the paraphrased
cluding English) of FAQ items in COUGH dataset.
query should share the same (or almost same) seLike English FAQ items, non-English FAQ items
mantic meaning as the original query.
are also presented in both question and query
A.2 Task 2: Annotated Relevance Set
string forms. Statistics of non-English items can
Construction
be found in Table 2. The detailed breakdown
of our COUGH dataset by sources and languages
For this task, you will see a FAQ item retrieved by
(other than English) is shown in Table A2.
an automatic tool for a particular user query, and
your job is to judge the relevance of the FAQ item
based on the annotation scheme shown below.
• Matched: The candidate FAQ matches the user
query perfectly. (Query part of FAQ is semantically identical to the user query, and answer part
of FAQ well answers the user query.)
• Useful: The candidate FAQ doesn’t perfectly
match the user query but may still give some or
enough information to help answer the user query.
(Query part of FAQ is semantically similar to the
user query, and you can either extract or infer some
information from the answer which could be useful to the user query. Or alternatively, the candidate FAQ provides too much extra information
which is not necessary.)
• Useless: The candidate FAQ is topically related to
the user query but doesn’t provide useful information. (Query part of FAQ is somewhat related to
the user query, but you can’t get any useful information out of the answer part to confidently answer the user query.)

Arizona Health Care Cost Containment System
Alabama Public Health
American Medical Association
California Department of Health
Government of Canada
Centers for Disease Control and Prevention
Children’s Hospital Los Angeles
Bloomberg Harvard City Leadership Initiative
Cleveland Clinic
CNN
Government of Colorado
Delaware Department of Health
U.S. Food and Drug Administration
European Centre for Disease Prevention and Control
Florida Department of Health
Georgia Department of Labor
Explore Georgia
Government of United Kingdom
Harvard Health Publishing
Illinois Department of Public Health
Inspire
JHU HUB
JHU Medicine
Kids Health from Nemours
King County, Washington
Government of Massachusetts
Medical News Today
MedHelp
Government of Michigan
Minnesota Department of Health
New York Times
Government of New Jersey
National Institute of Health
Government of North Carolina
Government of New York
New York State Electric and Gas
New York Department of Financial Services
Pennsylvania Office of Unemployment Compensation
Government of Pennsylvania
University of Pennsylvania Health System
Sante Clara Department of Health
San Mateo County Health
Texas Health Services
Tricare
United Nations
United States Department of Agriculture
United States Department of Labor
Virginia Department of Health
United States Department of Veterans Affairs
Washington Department of Health
WHO
World Health Organization
WikiHow
Total

# of FAQ Items
138
89
14
28
131
378
73
186
15
112
66
71
139
55
47
16
13
53
104
37
1753
7
14
121
26
17
28
282
75
98
100
322
105
59
75
68
45
222
66
63
103
47
39
94
40
152
43
435
16
137
29
395
2371
9151

Table A1: Number of English FAQ-19 items scrapped
from each source.

Centers for Disease Control and Prevention
Centers for Disease Control and Prevention
Centers for Disease Control and Prevention
Centers for Disease Control and Prevention
Children’s Hospital Los Angeles
Children’s Hospital Los Angeles
Children’s Hospital Los Angeles
Children’s Hospital Los Angeles
Children’s Hospital Los Angeles
Children’s Hospital Los Angeles
U.S. Food and Drug Administration
Japan Health
Japan Labor
United Nations
United Nations
United Nations
United Nations
World Health Organization
World Health Organization
World Health Organization
World Health Organization
World Health Organization
WikiHow
WikiHow
WikiHow
WikiHow
WikiHow
WikiHow
WikiHow
WikiHow
WikiHow
WikiHow
WikiHow
WikiHow
WikiHow
WikiHow
WikiHow
WikiHow
WikiHow
Total

language
Spanish
Korean
Vietnamese
Chinese
Arabic
Spanish
Persian
Armenian
Kanuri
Chinese
Spanish
Japanese
Japanese
Arabic
Spanish
French
Chinese
Arabic
Spanish
French
Russian
Chinese
Arabic
Czech
German
Spanish
Persian
French
Hindi
Indonesian
Italian
Japanese
Korean
Dutch
Portuguese
Russian
Thai
Vietnamese
Chinese
-

# of FAQ Items
268
244
244
244
30
45
39
38
32
34
83
226
63
39
38
37
38
328
356
387
301
367
144
22
525
310
49
301
286
166
263
286
128
142
303
142
90
101
117
6768

Table A2: Number of non-English FAQ-19 items
scrapped from each website and language.

