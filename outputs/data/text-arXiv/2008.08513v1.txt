Covid-19 infodemic reveals new tipping point epidemiology and a revised R formula
N.F. Johnson1,2, N. Velásquez2, O.K. Jha2, H. Niyazi2, R. Leahy2,3, N. Johnson Restrepo2,3, R. Sear4, P. Manrique5, Y. Lupu6, P. Devkota7, S. Wuchty7
1Physics Department, George Washington University, Washington D.C. 20052
2Institute for Data, Democracy and Politics, George Washington University, Washington D.C. 20052
3ClustrX LLC, Washington D.C.
4Department of Computer Science, George Washington University, Washington D.C. 20052
5Theoretical Biology and Biophysics Group, Los Alamos National Laboratory, Los Alamos, New Mexico, 87545
6Department of Political Science, George Washington University, Washington D.C. 20052
7Department of Computer Science, University of Miami, Miami, Florida 33146

Many governments have managed to control their COVID-19 outbreak with a simple message: keep
the effective 'R number' R<1 to prevent widespread contagion and flatten the curve. This raises the
question whether a similar policy could control dangerous online 'infodemics' of information,
misinformation and disinformation1,2,3,4,5,6,7,8,9,10,11,12,13. Here we show, using multi-platform data
from the COVID-19 infodemic, that its online spreading instead encompasses a different dynamical
regime where communities and users within and across independent platforms, sporadically form
temporary active links on similar timescales to the viral spreading. This allows material that might
have died out, to evolve and even mutate14,15,16. This has enabled niche networks that were already
successfully spreading hate and anti-vaccination material, to rapidly become global superspreaders of narratives featuring fake COVID-19 treatments, anti-Asian sentiment and conspiracy
theories. We derive new tools that incorporate these coupled social-viral dynamics, including an
online R, to help prevent infodemic spreading at all scales: from spreading across platforms (e.g.
Facebook, 4Chan) to spreading within a given subpopulation, or community, or topic. By accounting
for similar social and viral timescales, the same mathematical theory also offers a quantitative
description of other unconventional infection profiles such as rumors spreading in financial
markets and colds spreading in schools.
Despite the many physical differences between a piece of information on social media and a biological
virus, the term 'infodemic' adopted by the World Health Organization is attractive since it suggests
importing policies from epidemiology1,17,18,19,20 Starbird et al.'s continually updated literature review3 spells
out the dangers of infodemics of online information, misinformation, and disinformation21,22,23,24,25,26. The
current infodemic now extends beyond COVID-19 topics such as lockdowns, protests and masks to include
the 2020 U.S. elections. The cross-platform data that we have compiled of such COVID-related material
from Dec 2019-June 2020, shows that the online activity ('infection') at all scales is typically very different
to the single-peak infection profile and diffusive spreading predicted by standard epidemiological models
(Figs. 1,2, Extended Data Figs. 3 and 5). Hence, there is a need to develop a quantitative science of such
'infodemiology' to guide any adaptation of policies from epidemiology.
Figure 1a illustrates a key online mechanism that is essentially absent from offline epidemics: online
communities such as Facebook Pages or VKontakte Groups, each of which is a cluster (i.e. node) with up to
a million or more members, frequently form links to other online communities both across and within
platforms on a similar timescale to the effective infection and recovery times for viral material, e.g. a few
days. When combined with occasional link-breaking, either intentional or by moderators, these link
dynamics generate a perpetual reclustering-of-clusters within and across platforms and hence across
languages and continents27. This means that although interest in a particular piece of (mis/dis)information
may be waning within a given community, it can get injected into new communities where it can revive,
evolve and mutate. Crucially, no continuous path need exist at any given time across the entire social media
multiverse. Instead, paths that are piecewise in time can open up from sporadic sequences of links.
Extended Data Figure 1 gives an explicit example. This is akin to crossing a wide river by serendipitous
positioning of a short plank to bridge adjacent rocks. An immediate consequence is that subnetworks that
were already adept at spreading contentious material under these conditions prior to 2020, including those
for health misinformation (Fig. 2a) and hate (Fig. 1a), acted as early global super-spreaders for COVID-19

1

narratives (Fig. 1a inset), including anti-Asian racism and dis/misinformation about COVID-19 causes,
cures and vaccines.

Figure 1: Transient pathways within and across platforms for spreading. a: Each node is a cluster of members (e.g. Gab
community). For simplicity, we only show clusters whose narratives focus around hate (black circles) and clusters they
link into (no rings), since these became a strong conduit for COVID-19 misinformation from December 2019 onwards
(inset, top). Snapshot is from the start of COVID-19 infodemic (Jan 2020): it involves ~10,000,000 users across languages
and continents who formed themselves into ~6,000 inter-linked clusters. Small green square shows example of a
community that suddenly emerged and trafficked COVID-19 misinformation (see Methods). b: Infection profiles of COVID19 activity feature multiple peaks, unlike standard epidemiology. Values are non-zero but too small to see on this scale
prior to the major infodemic onset in mid-January. Solid black curves show the theory's predictions (SI) without
employing any parameter optimization to improve fitting. Left two theory curves are identical, just rescaled in magnitude,
and so are the right two theory curves: the only difference in parameter values is left two theory curves have a recovery
time per cluster which is 0.5 times that for the right two theory curves. These profiles illustrate how time-dependent links
allow the 'infection' to die down but then re-emerge.

2

The Supplementary Information (SI) details our simple approach to incorporating these missing social-viral
dynamics into standard epidemiology, starting from fundamental equations. The resulting theory's
predictions are in good agreement with full-scale computer simulations (Extended Data Fig. 2) and produce
infection profiles consistent with those observed across all online scales (e.g. Fig. 1b and Extended Data Fig.
3). Solving these equations yields a new online 'R number' !!"#$"% and a tipping point condition to prevent
spreading across " online universes (e.g. " social media platforms):
!!"#$"% = $

∑#
!,"%& "! "" '!" )!
%)
∑#
'
!%& "! (!

<1

(1)

where all the quantities on the right side of Eq. 1 are physical quantities that can be counted directly from
the data, e.g. Fig. 1a. ($ and (* are the fraction of clusters (e.g. Facebook Pages) in universes (platforms) )
and * respectively. In Fig. 1a, for example, ($ for )=VKontakte is the fraction of red nodes. +$*+, is the typical
(technically, the average) time it takes for a link to form between a cluster in ) and a cluster in *, e.g. if ) and
* are both on Facebook (FB), a link is created when FB Page A (i.e. cluster A) 'likes' FB Page B (i.e. cluster B)
and hence content is more likely to be shared, though Eq. 1 also applies to other link definitions. Our data
shows this is on the scale of days. ,$+, is the typical time it takes for the links of a cluster in universe ) to be
removed. - = .$ ⁄.- is the contact ratio and it is independent of the cluster network: .$+, is the typical time
it takes for a cluster to pass a piece of material to another cluster to which it is already linked, which our
data shows is also on the scale of days, while .-+, is the typical time it takes until new content added to a
cluster does not mention that material. In the mass-action limit, !!"#$"% → [2] .$ ⁄.- giving the familiar R
number for a real virus in a contact population size 2, which is equivalent to the average number of people
infected by each infected person. Since Eq. 1 is derived mathematically, not by empirical fit or statistical
scaling, it can be generalized in a systematic way to address different what-if scenarios.
Equation 1 has a variety of consequences for potential policy-making:
First, Eq. 1 takes on a transparent form when the policy focus is on policing pairs of subpopulations, e.g. a
pair of platforms such as Facebook (FB) and a platform (A) that FB may not control. If both platforms have
similar gross features, +./ ./ ~+11 ≡ + and +./ 1 ~+1 ./ ≡ 6+, hence Eq. 1 predicts that Facebook could
prevent spreading by ensuring the fraction of inter-platform links 6 < 62-$3 , where
( " 4( " ,
,
62-$3 = 7 () () * *8 + 71 −
8 .
(2)
5'"() "*

6

5"() "*

62-$3 vs. 1⁄- is a convenient straight-line graph and 1⁄- is independent of the network. Equation 2 means
that Facebook, for example, can increase ,./ to raise 62-$3 above 6, or directly reduce 6 by reducing interplatform links. Hence it avoids the need for blanket 'social distancing' between Facebook and platform A, or
'quarantining' of platform A, and hence avoids claims of over-policing.
Second, Eq. 1 applies at other scales simply by changing the specification of 'cluster' and 'universe', e.g. to
prevent spreading between a subpopulation on Telegram that share adult or borderline extremist content,
and a subpopulation of vulnerable individuals on Facebook (e.g. minors); or to prevent spreading of COVID
narratives within a platform (e.g. Facebook) from anti-vaccination communities to mainstream
communities (e.g. pet-lovers)33. It also applies to spreading within a single community by classifying nodes
as individual users who then form a cluster (i.e. single community such as a Facebook Page). Since they
tend to join and leave individually, the mathematics reduces primarily to monomer dynamics. Similarly
anomalous infection profiles emerge (see Extended Data Fig. 3 and SI for details).

3

Figure 2: Spreading within platform and topic. a: Illustration of spreading of COVID-19 information, misinformation and
disinformation between anti-vaccination (red), pro-vaccination (blue) and 'undecided' (green) clusters (each node is a
Facebook Page). Each subpopulation shows similarly anomalous infection profiles to Fig. 1b and are unlike standard
epidemiological diffusive spreading, e.g. the high initial spread within pro-vaccination subpopulation then decays before
re-emerging. b: Spreading of a single topic on same network as panel a, shown at intervals of 2 weeks, is also unlike
standard epidemiological diffusive spreading. This topic was identified using dynamic LDA machine-learning34 as having
the fastest rising coherence score (see SI). Word clouds illustrate how this topic's time-dependent word weightings evolve
and the topic itself mutates toward protests, lockdowns and then reopenings (red rings).

Third, Eq. 1 predicts the online herd immunity required to prevent spreading for policy schemes that
propose to 'innoculate' online35. The minimum fraction of clusters that if innoculated will prevent an
infodemic, is estimated to be ;1 − !!"#$"% +,/8 <. Hence !!"#$"% =10 means at least 54% of clusters require
innoculation, which is similar to the value we obtain using computer simulations.
Fourth, Eq. 1 says that even if .$ ⁄.- > 1 and there is no such online 'vaccine', the prefactor can be adjusted
by changing the link forming and breaking timescales, to force !!"#$"% < 1 and hence prevent the
infodemic. Even if .$ ⁄.- is not accurately known, reducing the prefactor will still lower the risk.
4

Fifth, Eq. 1 allows for different types of material to have different infectiousness, e.g. COVID-19
misinformation vs. racism, by having different contact ratio (- = .$ ⁄.- ) values. The same is true regarding
its level of truthfulness, e.g. fake news may have a higher -. Given the same network dynamics, an
infodemic can therefore occur in one case but not another.
Sixth, the physical variables in the prefactor in Eq. 1 do not need to be the same before and after the viral
material is introduced, so Eq. 1 allows for the system as a whole to subsequently react or adapt. Likewise by
allowing a dependence on the contact ratio (.$ ⁄.- ) or time, Eq. 1 can account for material that is so
infectious that it rewires the system as it spreads, or it mutates as it spreads as in Fig. 2b.
Seventh, Eq. 1 applies for other definitions of links between clusters. The links forming or breaking
between a pair of clusters may be based on the time-dependent correlations, or the amount of material
shared, or the topic or nature of its content (e.g. videos vs. text) or common membership.
Eighth, since the underlying mathematical analysis centers around the derivation of the probability of links
existing at a specific time, generalizations of Eq. 1 for other choices of infection process beyond SIR
(Susceptible-Infected-Recovered) follow very similar mathematics.
Ninth, Eq. 1 accounts for the empirical observation that while viral material can appear isolated and largely
eradicated on a given platform, it may have moved though inter-cluster links to other platforms where it
revives before later re-emerging on the original platform. Moderators reviewing blue clusters in Fig. 1a
might conclude that they have largely rid their platform of certain unwanted material in certain clusters,
only to see it re-emerge at a later date in completely different clusters.
Tenth, this mathematical generalization of standard epidemiology also produces profiles similar to other
real-world systems that have an interplay between social and viral timescales, e.g. online rumors in
financial markets and contagious diseases (e.g. colds) within schools (see Extended Data Fig. 4).
There are limitations to our study. The 'infection' terminology will always be an imperfect analogy. While
better fits to the profile shapes can be obtained by parameterizing the variables in Eq. 1, and settingspecific effects may contribute to particular profile anomalies, our focus is instead on providing a
benchmark scientific framework using a minimal, parsimonious description36. Other social dynamics that
are routine, e.g. nightly sleep, should average out globally. We have not followed specific pieces of
information, e.g. the Plandemic movie, because our focus is on the system level. Our mathematical formulae
are approximations, however they agree with full-scale computer simulations and reproduce key features
of the empirical profiles. There will be errors in estimating the physical variables in Eq. 1, however these
can be combined to give a useful error range for !!"#$"% . People may be members of multiple clusters,
however our prior work found that only a small percentage are actually active across multiple clusters.
Some people may avoid certain material while for others it may incite them37, however these two types
could be considered crudely as two subpopulations in Eq. 1 and 2. Clandestine agencies may contribute to
spreading: however the decentralized nature of Fig. 1a makes any central control unlikely. For example, for
material with a hate component, we only found a small portion of clusters linking to Kremlin-affiliated
domains and these links accounted for <0.5% of all posts shared. This is consistent with claims that in-built
communities (e.g. Facebook Pages) self-police for trolls and bots, as opposed to Twitter where there is no
such in-built community structure.

5

Methods
The SI provides the derivation and analysis of the mathematical equations and our methods for obtaining
the empirical dynamical networks of clusters and classifying their content (e.g. Fig. 1a, 2a) which follow our
prior works38,39,40,41. Each cluster (i.e. node) in Fig. 1a is an interest-based online community (e.g. Facebook
Page or VKontakte Group) and each link is a hyperlink that appears between them. We do not consider
Twitter since it does not have the in-built cluster tools of Facebook etc.: such clusters are known to
facilitate coordination and play a greater role in nurturing. Figure 1a includes the mainstream platforms
Facebook, VKontakte, and Instagram, that have and enforce content policies, and fringe platforms with
minimal content policies: Gab, Telegram, and 4Chan. Figure 1b shows typical output from our model with
the same values +$* = 0.95, ,$ = 0.05, .$ = 0.05 for all four panels. For 4Chan and Telegram .- = 0.01; for
Gab and Facebook .- = 0.005. All four fits are with these same two model profile outputs suitably scaled.
Output is smoothed over timepoints like the empirical data which is collected daily. All but one node in Fig.
1a is plotted using the ForceAtlas2 algorithm, which simulates a physical system where nodes (clusters)
repel each other while links act as springs, and nodes that are connected through a link attract each other.
Hence nodes (clusters) closer to each other have more highly interconnected local environments while
those farther apart do not. The exception is Gab group 407* (“Chinese Coronavirus”,
https://gab.com/groups/407*, small green square in Fig. 1a) which was manually placed to facilitate
visibility. It was created in early 2020 with a focus on discussing the COVID19 pandemic: however, it
immediately mixed COVID-19 fake news and hate, as well as conspiratorial content. In Fig. 1b, the vertical
scale is only shown on one plot for clarity, but the others have somewhat similar scales (see Extended Data
Fig. 5). Our dynamic LDA machine learning process and analysis (Fig. 2b) is described in the SI.
Data Availability: Dataset provided with the Supplementary Information (SI). All computer programs are
given in the SI or are described fully in previous publications (see references).
Author Contributions: All authors contributed to the research design, the analysis, and writing the paper.
Materials and Correspondence should be sent to NFJ neiljohnson@gwu.edu
References
1st World Health Organization (WHO) Infodemiology Conference, June 2020. Geneva.
https://www.who.int/teams/risk-communication/infodemic-management/1st-who-infodemiologyconference
2 Bechmann, A. Tackling Disinformation and Infodemics Demands Media Policy Changes. Digital
Journalism, 1-13 (2020)
3 Brown, R. et al. Counteracting Dangerous Narratives in the Time of COVID-19. Over Zero (2020).
https://projectoverzero.org/newsandpublications
4 Wasserman, H., Madrid-Morales, D. An Exploratory Study of “Fake News” and Media Trust in Kenya,
Nigeria and South Africa, African Journalism Studies, 40:1, 107-123 (2019). DOI:
10.1080/23743670.2019.1627230
5 Starbird, K., Spiro, E.S., Koltai, K. Misinformation, Crisis, and Public Health—Reviewing the Literature
V1.0. Social Science Research Council, MediaWell. June 25, 2020. https://mediawell.ssrc.org/literaturereviews/misinformation-crisis-and-public-health. http://doi.org/10.35650/MD.2063.d.2020
6 Larson, H. A lack of information can become misinformation. Nature, 580, 306 (2020)
7 Ball, P., Maxmen, A. The epic battle against coronavirus misinformation and conspiracy theories.
Nature, May 27 (2020). https://www.nature.com/articles/d41586-020-01452-z
8 Cornwall, W. Just 50% of Americans plan to get a COVID-19 vaccine. Science, Jun. 30 (2020).
https://www.sciencemag.org/news/2020/06/just-50-americans-plan-get-covid-19-vaccine-here-s-howwin-over-rest
1

6

9 Guilbeault, D., Centola, D. Networked collective intelligence improves dissemination of scientific
information regarding smoking risks. PLoS ONE 15(2): e0227813.
https://doi.org/10.1371/journal.pone.0227813 (2020)
10 Siddiqui, M., Salmon, D.A., Omer S.B. Epidemiology of vaccine hesitancy in the United States. Hum Vaccin
Immunother. 9, 2643-2648 (2013). doi:10.4161/hv.27243
11 Forrest, A. Coronavirus: 700 dead in Iran after drinking toxic methanol alcohol to ‘cure Covid-19’. The
Independent. April 28, 2020. https://www.independent.co.uk/news/world/middle-east/coronavirusiran-deaths-toxic-methanol-alcohol-fake-news-rumours-a9487801.html
12 Lazer, D.M.J., Baum, M.A., Benkler, Y., Berinsky, A.J., Greenhill, K.M., Menczer, F., Metzger, M.J., Nyhan, B.,
Pennycook, G., Rothschild, D., Schudson, M., Sloman, S.A., Sunstein, C.R., Thorson, E.A., Watts, D.J., Zittrain,
J.L. The science of fake news. Science 359, 1094–1096 (2018).
doi:10.1126/science.aao2998pmid:29590025
13 Fischhoff, B., Brewer, N.T., Downs, J.S. Communicating Risks and Benefits: An Evidence-Based User’s
Guide. US Department of Health and Human Services, August 2011.
http://www.fda.gov/ScienceResearch/SpecialTopics/RiskCommunication/ default.htm
14 Chen, E., Lerman, K., Ferrara, E. Tracking Social Media Discourse About the COVID-19 Pandemic:
Development of a Public Coronavirus Twitter Data Set. JMIR Public Health and Surveillance 6, e19273
(2020)
15 Brennen, J.S., Simon, F., Howard, P.N., Nielsen, R.K. Types, sources, and claims of COVID-19
misinformation. 7 April 2020. https://reutersinstitute.politics.ox.ac.uk/types-sources-and-claims-covid19-misinformation
16 Donovan, J. Social-media companies must flatten the curve of misinformation. Nature 14 April, 2020.
https://www.nature.com/articles/d41586-020-01107-z
17 Onnela, J.P., Saramäki, J., Hyvönen, J., Szabó, G., Lazer, D., Kaski, K., Kertész, J., Barabási, A.L. Structure
and tie strengths in mobile communication networks. PNAS 104, 7332-7336 (2007).
https://doi.org/10.1073/pnas.0610245104
18 Dodds P.S., Harris K.D., Kloumann I.M., Bliss C.A., Danforth C.M. Temporal Patterns of Happiness and
Information in a Global Social Network: Hedonometrics and Twitter. PLoS ONE 6, e26752 (2011).
19 Watts, D.J., Muhamad, R., Medina, D.C., Dodds, P.S. Multiscale, resurgent epidemics in a hierarchical
metapopulation model. PNAS 102, 11157-11162 (2005). https://doi.org/10.1371/journal.pone.0026752
20 Centola, D., Becker, J., Brackbill, D. & Baronchelli, A. Experimental evidence for tipping points in social
convention. Science 360, 1116–1119 (2018)
21 Murray, J.D. Mathematical Biology (Springer, 2003)
22 Lavezzo, E., Franchin, E., Ciavarella, C. et al. Suppression of a SARS-CoV-2 outbreak in the Italian
municipality of Vo’. Nature (2020). https://doi.org/10.1038/s41586-020-2488-1
23 Fowler, J.H., Christakis, N.A. Dynamic spread of happiness in a large social network: longitudinal
analysis over 20 years in the Framingham Heart Study. BMJ 337
(2008). doi: https://doi.org/10.1136/bmj.a2338
24 Adam, D. A guide to R — the pandemic’s misunderstood metric. Nature July 3, 2020.
https://www.nature.com/articles/d41586-020-02009
w?utm_source=Nature+Briefing&utm_campaign=e9079e5ba2-briefing-dy20200703&utm_medium=email&utm_term=0_c9dfd39373-e9079e5ba2-44549989
25 Bessi, A., Coletto, M., Davidescu, G. A., Scala, A., Caldarelli, G., Quattrociocchi, W. Science vs conspiracy:
Collective narratives in the age of misinformation. PloS one, 10(2) e0118093 (2015).
https://journals.plos.org/plosone/article%3Fid%3D10.1371/journal.pone.0118093
26 DiResta, R. Of virality and viruses: the anti-vaccine movement and social media. NAPSNet Special
Reports https://nautilus.org/napsnet/napsnet-special-reports/of-virality-and- viruses-the-anti-vaccinemovement-and-social-media/ (8 November 2018).
27 Starbird, K. Disinformation’s spread: bots, trolls and all of us. Nature 571, 449 (2019)
28 Gill, P. et al. Terrorist use of the internet by the numbers quantifying behaviors, patterns, and processes.
Criminol. Public Pol. 16, 99–117 (2017)

7

29 Hedströ m, P., Sandell, R. & Stern, C. Meso-level networks and the diffusion of social movements. Am. J.
Sociol. 106, 145–172 (2000)
30 Havlin, S., Kenett, D. Y., Bashan, A., Gao, J., Stanley, H. E. Vulnerability of network of networks. Eur. Phys.
J. Spec. Top. 223, 2087 (2014)
31 Gelfand M.J., Harrington J.R., Jackson J.C. The Strength of Social Norms Across Human
Groups. Perspectives On Psychological Science : a Journal of the Association For Psychological Science. 12:
800-809. PMID 28972845 DOI: 10.1177/1745691617708631
32 Gavrilets, S. Collective action and the collaborative brain. J. R. Soc. Interface 12, 20141067 (2015)
33 Johnson, N.F., N. Velasquez, N. Johnson Restrepo, R. Leahy, N. Gabriel, S. El Oud, M. Zheng, P. Manrique,
S. Wuchty, Y. Lupu. The online competition between pro- and anti-vaccination views. Nature 582, 230–
233 (2020). https://doi.org/10.1038/s41586-020-2281-1
34 Blei, D.M., Lafferty, J.D. Dynamic Topic Models. ICML '06: Proceedings of the 23rd international
conference on Machine learning, June 2006, p.113–120. https://doi.org/10.1145/1143844.1143859
35 Roozenbeek, J., Van der Linden, S. Fake news game confers psychological resistance against online
misinformation. Palgrave Communications, 2019 DOI: 10.1057/s41599-019-0279-9
36 Epstein, J.M., Axtell, R. Growing Artificial Societies: Social Science from the Bottom Up. The MIT Press
(1996)
37 Dittrich, P., Liljeros, F., Soulier, A., Banzhaf, W. Spontaneous Group Formation in the Seceder Model.
Phys. Rev. Lett. 84, 3205 (2000)
38 Johnson, N.F. R. Leahy, N. Johnson Restrepo, N. Velasquez, M. Zheng, P. Manrique, P. Devkota, S. Wuchty .
Hidden resilience and adaptive dynamics of the global online hate ecology. Nature 573, 261–265 (2019)
39 Johnson, N.F. M. Zheng, Y. Vorobyeva, A. Gabriel, H. Qi, N. Velasquez, P. Manrique, D. Johnson, E.
Restrepo, C. Song, S. Wuchty. New online ecology of adversarial aggregates: ISIS and beyond. Science 352,
1459–1463 (2016)
40 Zhao, Z. et al. Effect of social group dynamics on contagion. Phys. Rev. E 81, 056107 (2010)
41 Manrique, P.D., Xu, C., Hui, P.M., Johnson, N.F. Atypical viral dynamics from transport through popular
places. Phys. Rev. E 94, 022304 (2016)

8

