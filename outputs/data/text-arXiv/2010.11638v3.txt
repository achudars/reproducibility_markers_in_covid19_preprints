“It is just a flu”: Assessing the Effect of Watch History on YouTube’s
Pseudoscientific Video Recommendations
Kostantinos Papadamou? , Savvas Zannettou∓ , Jeremy Blackburn†
Emiliano De Cristofaro‡ , Gianluca Stringhini , Michael Sirivianos?
Cyprus University of Technology, ∓ Max Planck Institute, † Binghamton University
‡
University College London,  Boston University
ck.papadamou@edu.cut.ac.cy, szannett@mpiinf.mpg.de, jblackbu@binghamton.edu
e.decristofaro@ucl.ac.uk, gian@bu.edu, michael.sirivianos@cut.ac.cy

arXiv:2010.11638v3 [cs.CY] 17 Jan 2021

?

Abstract

also recommend potentially harmful content [31], and their
opaque nature makes them difficult to audit.
For certain types of content, e.g., health-related topics, harmful videos can have devastating effects on society, especially
during crises like the COVID-19 pandemic [40]. For instance,
conspiracy theories have suggested that COVID-19 is caused by
5G [27] or Bill Gates [15], hindering social distancing, masking, and vaccination efforts [11]. Conspiracy theories are usually built on tenuous connections between various events, with
little to no actual evidence to support them; on user-generated
content platforms like YouTube, these are often presented as
facts, regardless of whether they are supported by facts and
even though they have been widely debunked. Motivated by the
pressing need to mitigate the spread of pseudoscientific content,
we focus on detecting and characterizing pseudoscientific and
conspiratorial content on YouTube. In particular, we aim to: 1)
assess how likely it is for users with different watch histories to
come across pseudoscientific content on YouTube, and 2) analyze how YouTube’s recommendation algorithm contributes to
promoting pseudoscience. To do so, we set out to answer the
following two research questions:

The role played by YouTube’s recommendation algorithm in
unwittingly promoting misinformation and conspiracy theories
is not entirely understood. Yet, this can have dire real-world
consequences, especially when pseudoscientific content is promoted to users at critical times, such as the COVID-19 pandemic. In this paper, we set out to characterize and detect
pseudoscientific misinformation on YouTube. We collect 6.6K
videos related to COVID-19, the Flat Earth theory, as well as
the anti-vaccination and anti-mask movements. Using crowdsourcing, we annotate them as pseudoscience, legitimate science, or irrelevant and train a deep learning classifier to detect
pseudoscientific videos with an accuracy of 0.79.
We quantify user exposure to this content on various parts
of the platform and how this exposure changes based on the
user’s watch history. We find that YouTube suggests more pseudoscientific content regarding traditional pseudoscientific topics (e.g., flat earth, anti-vaccination) than for emerging ones
(like COVID-19). At the same time, these recommendations
are more common on the search results page than on a user’s
homepage or when actively watching videos. Finally, we shed
light on how a user’s watch history substantially affects the type RQ1 Can we effectively detect and characterize pseudoscienof recommended videos.
tific content on YouTube?

RQ2 What is the proportion of pseudoscientific content on the
homepage of a YouTube user, in search results, and the
video recommendations section of YouTube? How are
User-generated video platforms like YouTube have exploded in
these proportions affected by the user’s watch history?
popularity over the last decade. For many users, it has also become one of the most important information sources for news,
world events, and various topics [30]. Alas, such platforms are Methodology. We focus on four pseudoscientific topics:
also often fertile ground for the spread of misleading and poten- 1) COVID-19, 2) Flat Earth theory, 3) anti-vaccination, and
tially harmful information like conspiracy theories and health- 4) anti-mask movement. We collect 6.6K unique videos and use
related disinformation [5].
crowdsourcing to label them in three categories: science, pseuYouTube and other social media platforms have struggled doscience, or irrelevant. We then train a deep learning classiwith mitigating the harm from this type of content. The dif- fier to detect pseudoscientific content across multiple topics on
ficulty is partly due to the sheer scale and also because of the YouTube.
deployment of recommendation algorithms [44]. Purely autoNext, we use three carefully crafted user profiles, each with
mated moderation tools have thus far been insufficient to mod- a different watch history, while all other account information
erate content, and human moderators had to be brought back remains the same, to simulate logged-in users. We also perinto the loop [43]. Additionally, the machine learning algo- form a set of experiments using a browser without a Google
rithms that YouTube relies on to recommend content to users account to simulate non-logged-in users and another set using

1

Introduction

1

the YouTube Data API exclusively. To populate the watch history of the three user profiles, we devise a methodology to identify the minimum amount of videos that must be watched by a
user before YouTube’s recommendation algorithm starts generating substantially personalized recommendations. We build
three distinct profiles: 1) a user interested in scientific content;
2) a user interested in pseudoscientific content; and 3) a user
interested in both scientific and pseudoscientific content. Using these profiles, we perform three experiments to quantify the
user’s exposure to pseudoscientific content on various parts of
the platform and how this exposure changes based on a user’s
watch history. Note that we manually review all the videos classified as pseudoscientific in all experiments.

Pseudoscientific Topic #Seed #Recommended
COVID-19
Anti-vaccination
Anti-mask
Flat Earth
Total

1,645
1,759
912
1,211

1,123

5,527

Table 1: Overview of the collected data: number of seed videos and
number of their recommended videos.

2.1

Data Collection

Since we aim to detect pseudoscientific video content automatically, we collect a set of YouTube videos related to four, arguably relevant, topics: 1) COVID-19 [10], 2) the anti-vaccine
movement [4], 3) the anti-mask movement [35], and 4) the Flat
Earth theory [39]. We focus on COVID-19 and the anti-mask
movement because both are timely topics of great societal interest. We also choose anti-vaccination because it is both an increasingly popular and traditional pseudoscientific topic. Last,
we include the Flat Earth theory because it is a “long-standing”
pseudoscientific subject.
Next, we use the YouTube Data API [7] and perform a search
query for each selected topic obtaining the first 200 videos as
returned by YouTube’s Data API search functionality. We refer to those videos as the “seed” videos of our data collection
methodology. Additionally, for each seed video, we collect the
top 10 recommended videos associated with it, as returned by
the YouTube Data API. The list of the search terms used for
each topic is available at [1]. We perform our data collection on
August 1-20, 2020, collecting 6.6K unique videos (1.1K seed
videos and 5.5K videos recommended from the seed videos).
Table 1 summarizes our dataset.
For each video in our dataset, we collect: 1) the transcript
of the video; 2) the video title and description; 3) a set of tags
defined by the uploader; 4) video statistics such as the number
of views, likes, etc.; and 5) the 200 top comments, defined by
YouTube’s relevance metric, without their replies.

Findings. Overall, our study leads to the following findings:
1. We can detect pseudoscientific content, as our deep learning classifier yields 0.79 accuracy and outperforms SVM,
Random Forest, and BERT-based classifiers (RQ1).
2. The watch history of the user substantially affects search
results and related video recommendations. At the same
time, pseudoscientific videos are more likely to appear in
search results than in the video recommendations section
or the user’s homepage (RQ2).
3. In traditional pseudoscience topics (e.g., Flat Earth), there
is a higher rate of recommended pseudoscientific content than in more recent issues like COVID-19, antivaccination, and anti-mask. For COVID-19, we find an
even smaller amount of pseudoscientific content being
suggested. This indicates that YouTube took partly effective measures to mitigate pseudoscientific misinformation
related to the COVID-19 pandemic (RQ2).
4. The YouTube Data API results are similar to those of
the non-logged-in profile with no watch history (using a
browser); this indicates that recommendations returned using the API are not subject to personalization.

2.2
Contributions. To the best of our knowledge, we present
the first study focusing on multiple pseudoscientific topics on
YouTube while accounting for the effect of a user’s watch history. Our methodology can be re-used for other studies focusing on other topics of interest. We will also publish our groundtruth dataset, the classifier, and the source code/crawlers used in
our experiments, along with the final version of the paper. We
are confident that this will help the research community shed
additional light on YouTube’s recommendation algorithm and
its potential influence. Meanwhile, the dataset is available for
review, anonymously, through the PC chairs.

2

378
346
199
200

Crowdsourcing Data Annotation

To create a ground-truth dataset of scientific and pseudoscientific videos, we use the Appen platform [3] to get crowdsourced annotations for all the collected videos. We present
each video to three annotators who inspect its content and metadata to assign one of three labels:
1. Science. The content is related to any scientific field that
systematically studies the natural world’s structure and the
behavior or humanity’s artifacts (e.g., Chemistry, Biology,
Mathematics, Computer Science, etc.). Videos that debunk science-related conspiracy theories (e.g., explaining
why 5G technology is not harmful) also fall in this category. For example, a COVID-19 video with an expert
estimating the total number of cases or excess deaths falls
in this category if the estimation rests on the scientific consensus and official data.

Dataset & Annotation

In this section, we present our data collection and crowdsourced annotation methodology. We collect a set of YouTube
videos related to science and then use crowdsourcing to annotate videos as pseudoscientific or not.

2. Pseudoscience. The video meets at least one of the fol2

Topic
COVID-19
Anti-vaccination
Anti-mask
Flat Earth
Total

annotate them. Using the first author’s annotations as groundtruth, we calculate the precision, recall, and F1 score of our
crowdsourced annotation, yielding respectively 0.90, 0.89, and
0.89. We argue that this represents an acceptable performance
given the subjective nature of scientific and pseudoscientific
content.

#Science #Pseudoscience #Irrelevant
607
363
65
162

368
394
188
375

721
1,060
724
707

1,197

1,325

3,212

Ethics. We only collect publicly available data, we do not attempt to de-anonymize users, and overall follow standard ethical guidelines [8, 37]. We also note that we obtained institutional ethics approval from the first author’s national ethics
committee to ensure that our crowdsourced annotation process
does not pose risks to the annotators.

Table 2: Overview of our ground-truth dataset.

lowing criteria: a) holds a view of the world that goes
against the scientific consensus (e.g., anti-vaccine movement); b) comprises statements or beliefs that are selffulfilling or unfalsifiable (e.g., Meditation); c) develops
hypotheses that are not evaluated following the scientific
method (e.g., Astrology); or d) explains events as secret
plots by powerful forces rather than overt activities or accidents (e.g., the 5G-coronavirus conspiracy theory).

3

Detection of Pseudoscientific Videos
(RQ1)

In this section, we present our classifier geared to detect pseudoscientific videos. To train and test it, we use our groundtruth dataset of 5,734 videos. We collapse our three labels into
two, combining the science with the irrelevant videos into an
“Other” category; this yields a ground-truth dataset with 1,325
pseudoscience and 4,409 other videos. Below we describe the
input features and the architecture of our proposed classifier.
We perform an experimental evaluation to assess the classifier’s
performance and an ablation study to understand which of the
input features contribute the most to the classification task.

3. Irrelevant. The content is not relevant to any scientific
field and does not fall in the Pseudoscience category. For
example, music videos and cartoon videos are considered
irrelevant. Conspiracy theory debunking videos that are
not relevant to a scientific field are deemed irrelevant (e.g.,
a video debunking the Pizzagate conspiracy theory).
Annotation. The annotation process is carried out by 992 annotators, both male and female, recruited through the Appen
platform. We give annotators instructions on what constitutes
scientific and pseudoscientific content using appropriate descriptions and several examples. They are offered $0.03 per
annotation. Three annotators label each video. To ease the annotation process, we provide a clear description of the task and
our labels, and all video information that an annotator needs
to inspect and correctly annotate a video. Screenshots of the
instructions are available at [1].
Appen provides no demographic information about the annotators, other than an assurance that they are experienced and
attained high accuracy in other tasks. To assess the annotators’ quality, before allowing them to submit annotations, we
ask them to annotate 5 test videos randomly selected from a set
of 54 test videos (20 science, 21 pseudoscience, and 13 irrelevant) annotated by the first author of this paper. An annotator
can submit annotations only when she labels at least 3 out of
the 5 test videos correctly.
We also calculate the Fleiss’ Kappa Score (k) [14] to assess
annotators’ agreement. We get k = 0.24, which is considered
“fair” agreement. This relatively low agreement score is not
surprising due to the subjective nature of the problem. We assign one of the labels for each video according to the majority agreement of all the annotators, except a small percentage
(13.8%) where all annotators disagreed with each other, which
we exclude from our ground-truth dataset. The final groundtruth dataset includes 1,197 science, 1,325 pseudoscience, and
3,212 irrelevant videos (see Table 2).
Performance Evaluation. To evaluate our crowdsourced annotation performance, we randomly select 600 videos from our
ground-truth dataset (200 videos from each class) and manually

3.1

Classifier Architecture

Figure 1 depicts the architecture of our classifier. The classifier consists of four different branches, each processing a distinct input feature type: snippet, video tags, transcript, and the
top 200 comments of a video. Then, all four branches’ outputs are concatenated to form a five-layer, fully-connected neural network that merges their output and drives the final classification. The classifier uses fastText [12], a library for efficient learning of word/document-level vector representations
and sentence classification. We use fastText to generate vector
representations (embeddings) for all the available video metadata in text. For each input feature, we use the pre-trained fastText models released in [29] and fine-tune them using each of
our input features. These models extract a 300-dimensional
vector representation for each of the following input features
of our dataset:
• Snippet. Concatenation of the title and the description of
the video.
• Tags. Words defined by the uploader of a video to describe
the content of the video.
• Transcript. Naturally, this is one of the most important
features, as it describes the video’s actual content. (It includes the subtitles uploaded by the creator of the video or
auto-generated by YouTube.) The classifier uses the finetuned model to learn a vector representation of the concatenated text of the transcript.
3

Fusing Network
Video
Snippet

fastText
Embedding
Layer

Video
Tags

fastText
Embedding
Layer

Video
Comments

fastText
Embedding
Layer

Video
Transcript

fastText
Embedding
Layer

Dropout
d=0.5

Dropout
d=0.5
Dropout
d=0.5
Softmax
(Dense Layer)

Video
Input

1st Hidden
Layer

2nd Hidden
Layer

3rd Hidden
Layer

4th Hidden
Layer

Figure 1: Architecture of our deep learning classifier for the detection of pseudoscientific videos.

Classifier

mization, we use Adam with an initial learning rate of 1e−3,
and  = 1e−8.
We then compare the performance of the classifier, in terms
of accuracy, precision, recall, and F1 score, using three baselines: 1) a Support Vector Machine (SVM) classifier with parameters γ = 0.1 and C = 10, 2) a Random Forest classifier
with an entropy criterion and number of minimum samples leaf
equal to 2, and 3) a neural network with the same architecture as
our classifier that uses a pre-trained BERT model [42] to learn
document-level representations from all the available input features (BERT-based). For hyper-parameter tuning of baselines
(1) and (2), we use the grid search strategy, while for (3), we
use the same hyper-parameters as the proposed classifier. Note
that all evaluated models use all available input features.
Table 3 reports the performance of all classifiers. We observe that our classifier outperforms all baseline models across
all performance metrics. To further reduce false positives and
improve the performance of our classifier, we apply a thresholdmoving approach, which tunes the threshold used to map probabilities to class labels [33]. We use the grid-search technique
to find the optimal lower bound probability above which we
consider a video pseudoscientific, and find it to be 0.7. Using
this threshold, we train and re-evaluate the proposed classifier,
which yields, respectively, 0.79, 0.77, 0.79, and 0.74 on the
accuracy, precision, recall, and F1 score (see the last row in
Table 3).

Accuracy Precision Recall F1 Score

SVM
Random Forest
BERT-based Classifier

0.68
0.72
0.73

0.72
0.70
0.64

0.68
0.72
0.73

0.70
0.71
0.67

Proposed Classifier

0.76

0.74

0.76

0.74

Proposed Classifier
(threshold-moving)

0.79

0.77

0.79

0.74

Table 3: Performance of the evaluated baselines and of the proposed
deep learning classifier.

• Comments. We consider the top 200 comments of the
video as returned by the YouTube Data API. We first concatenate each video’s comments and use them to fine-tune
the fastText model and extract vector representations.
The second part of the classifier (the “Fusing Network” in
Figure 1) is essentially a four-layer, fully-connected, dense neural network. We use a Flatten utility layer to merge the outputs
of the four branches of the first part of the classifier, creating a
1200-dimensional vector. This vector is processed by the four
subsequent layers comprising 256, 128, 64, and 32 units, respectively, with ReLU activation. To avoid overfitting, we regularize using the Dropout technique; at each fully-connected
layer, we apply a Dropout level of d = 0.5, i.e., during each
iteration of training, half of each layer’s units do not update
their parameters. Finally, the Fusing Network output is fed to
the last neural network of two units with softmax activation,
which yields the probabilities that a particular video is pseudoscientific or not. We implement our classifier using Keras with
Tensorflow as the back-end.

3.2

Ablation Study. To understand which of the input features contribute the most to the classification of pseudoscientific videos,
we perform an ablation study. We systematically remove each
of the four input feature types (and their branch in the classifier) and retrain the classifier. Again, we use ten-fold crossvalidation and oversampling to deal with data imbalance and
use the classification threshold of 0.7. Table 4 reports the performance metrics for each combination of inputs. Video tags
and transcripts yield the best performance, indicating that they
are the most informative input features. However, using all the
available input features yields better performance, which indicates that all four input features are ultimately crucial for the

Experimental Evaluation

We use ten-fold stratified cross-validation, training and testing the classifier for binary classification using all the aforementioned input features. To deal with data imbalance, we use
the Synthetic Minority Over-sampling Technique [6] and oversample only the training set at each fold. For stochastic opti4

Input Features

profile differences. Additionally, we perform experiments on
a browser without a Google account to simulate not logged-in
users. Moreover, we perform experiments using the YouTube
Data API (when the API provides the required functionality) to
investigate the differences between YouTube as an application
and the API.

Accuracy Precision Recall F1 Score

Snippet
Tags
Transcript
Comments

0.78
0.78
0.78
0.78

0.76
0.77
0.74
0.71

0.78
0.78
0.78
0.77

0.71
0.72
0.71
0.68

Snippet, Tags
Snippet, Transcript
Snippet, Comments
Tags, Transcript
Tags, Comments
Transcript, Comments

0.78
0.78
0.78
0.79
0.78
0.78

0.76
0.75
0.77
0.77
0.76
0.76

0.78
0.78
0.78
0.79
0.78
0.78

0.72
0.72
0.71
0.73
0.72
0.73

Snippet, Tags, Transcript
Snippet, Tags, Comments
Snippet, Transcript, Comments
Tags, Transcript, Comments

0.78
0.78
0.78
0.78

0.75
0.75
0.76
0.76

0.78
0.78
0.78
0.78

0.72
0.72
0.73
0.73

All Features

0.79

0.77

0.79

0.74

User Profile Creation. According to Hussein et al. [17], once
a user forms a watch history, user profile attributes (i.e., demographics) affect future video recommendations. Hence, since
we are only interested in the watch history, each of the three
accounts has the same profile: 30 years old and female. To
decrease the likelihood of Google automatically detecting our
user profiles, we carefully crafted each one assigning them a
unique name and surname and performed standard phone verification. None of the created profiles were banned or flagged
by Google during or after our experiments.
Watch History. Next, we build the watch history of each
profile, aiming to create the following three profiles: 1) a
user interested in legitimate science videos (“Science Profile”);
2) a user interested in pseudoscientific content (“Pseudoscience
Profile”); and 3) a user interested in both science and pseudoscience videos (“Science/Pseudoscience Profile”).
To find the minimum number of videos a profile needs to
watch before YouTube learns the user’s interests and starts generating more personalized recommendations, we use a newly
created Google account with no watch history and perform the
following experiment. First, we randomly select a video, which
we refer to as the “reference” one, from the COVID-19 pseudoscientific videos of our ground-truth dataset, and we collect
its top 20 recommended videos. Next, we create a list of 100
randomly selected videos from the COVID-19 pseudoscientific
videos of our ground-truth dataset, and we repeat the following
process iteratively:

Table 4: Performance of the proposed classifier (considering the 0.7
classification threshold) trained with all the possible combinations of
the four input feature types.

classification task.
Remarks. Although our classifier outperforms all the baselines, ultimately, its accuracy (0.74 F1-score) reflects the subjective nature of pseudoscientific vs. scientific content classification on YouTube. This relates to our crowdsourced annotation’s relatively low agreement score, which highlights the
difficulty in identifying whether a video is pseudoscientific. It
is also evidence of the hurdles in devising models that automatically discover pseudoscientific content. Nonetheless, we argue
that our classifier is only the first step in this direction and can
be further improved; overall, it does provide a meaningful signal on whether a video is pseudoscientific (RQ1). It can also be
used to derive a lower bound on the tendency of YouTube’s recommendation algorithm to recommend pseudoscience by uncovering a substantial portion of pseudoscientific videos while
also eliminating all false positives with manual review of all the
videos classified as pseudoscientific (see Section 4.1).

4

Pseudoscientific Content
YouTube Platform (RQ2)

on

1. We start by watching a video from the list of the randomly
selected pseudoscientific videos;
2. We visit the reference video, and we collect the top 20 recommendations, store them, and compare them using the
Jaccard similarity index with all the recommendations of
the reference video collected in the previous iterations;

the

3. If all the recommended videos of the reference video at
the current iteration have also been recommended in the
previous iterations, we stop our experiment. Otherwise,
we delete the user’s watch history, increase the number of
videos we watch at Step 1 by one and proceed to the next
iteration.

In this section, we analyze the prominence of pseudoscientific
videos on various parts of the platform.

4.1

Experimental Design

We focus on three parts of the platform: 1) the homepage; 2)
the search results page; and 3) the video recommendations section (recommendations when watching videos). Examples of
each part of the platform are available at [1]. We aim to simulate the logged-in and non-logged-in users’ behavior with varying interests and measure how the watch history affects pseudoscientific content recommendation.
To do so, we create three different Google accounts, each
one with a different watch history, while all the other account
information is the same to avoid confounding effects caused by

Using this procedure, we find that the minimum amount of
videos required to be watched by a user for YouTube to start
generating more personalized recommendations is 22. However, to create more representative watch histories and get even
more personalized recommendations, we increase this number
to 100. Finally, we select the most popular science and pseudoscience videos from the ground-truth dataset, based on the
number of views, likes, comments, etc., and use them to personalize the three Google accounts’ profiles. Since it is not
5

% unique pseudoscientific videos

4.2

25
Science Profile
Pseudoscience Profile
Science/Pseudoscience Profile
No Profile (Browser)

20
15

Homepage. We begin by assessing the magnitude of the pseudoscientific content problem on the YouTube homepage. To do
so, we use each one of the three user profiles (Science, Pseudoscience, and Science/Pseudoscience), as well as another user
with no account (No Profile) that simulates the behavior of not
logged-in users. We then visit each profile’s homepage to collect and classify the top 30 videos as ranked by YouTube. Note
that we cannot perform this experiment using the YouTube Data
API since it does not support this functionality. We repeat the
same experiment 50 times with a waiting time of 10 minutes between each repetition because YouTube shows different videos
on the homepage each time a user visits YouTube. We perform
this experiment on December 15–16, 2020.
Figure 2 shows the percentage of unique pseudoscientific
videos on the homepage of each user profile. We find that 2.4%,
9.8%, 4.4%, and 1.9% of all the unique videos found in the
top 30 videos of the homepage of the Science, Pseudoscience,
Science/Pseudoscience, and the No profile (browser) users, respectively, are pseudoscientific. Overall, the Pseudoscience and
the Science/Pseudoscience profile get shown a higher amount
of pseudoscientific content on their homepage. We also verify
the significance of the difference in the amount of pseudoscientific content in the homepage of the Pseudoscience and the
Science/Pseudoscience profiles compared to the one of the No
profile (browser) using the Fisher’s Exact test (p < 0.05).We
obtain similarly high significance (p < 0.05) when we compare
the Pseudoscience and Science/Pseudoscience profiles with the
Science profile. This indicates that the users’ watch history substantially affects the number of pseudoscientific recommendations on their homepage. Nevertheless, users who are not interested in this type of content (i.e., science profile) still receive
a non-negligible amount of pseudoscientific content. We also
observe that as the number of videos on the user’s homepage
increases (e.g., when a user scrolls down), the pseudoscientific
videos’ percentage remains approximately identical.

10
5
0

0

5

10
15
20
25
# top homepage videos

Pseudoscientific Content on Homepage,
Search Results, and Video Recommendations

30

Figure 2: Percentage of pseudoscience videos found in the homepage
of each user profile.

clear how YouTube measures the satisfaction score on videos
and how watch time affects this score, during profile training,
we always watch the same proportion of the video (50% of the
total duration).
Controlling for noise. Some differences in search results and
recommendations are likely due to factors other than the user’s
watch history and personalization in general. To reduce the
possibility of this noise affecting our results, we take the following steps: 1) We execute, in parallel, experiments with
identical search queries for all accounts to avoid updates to
search results over time for specific search queries; 2) All requests to YouTube are sent through the same US-based hosts
to avoid location-related issues (i.e., differences in localized results); 3) We perform all experiments using the same browser
user-agent and operating system; 4) To avoid the carry-over effect (previous search and watch activity affecting subsequent
searches and recommendations), at each repetition of our experiments, we use the “Delete Watch and Search History” function; to erase the activity of the user on YouTube from the date
after we built the user profiles; and 5) Similarly to the profiles’
watch history creation, we always watch the same proportion
of the video (50% of the total duration).

Search Results. Next, we focus on quantifying the prevalence
of pseudoscientific content when users search for videos on
YouTube. For this experiment, we use the four pseudoscientific topics in our ground-truth dataset, and we perform search
queries on YouTube for each topic. We retrieve the top 20
videos for each search query and use our classifier to classify
each video in the result set. We repeat this experiment 50 times
for each pseudoscientific topic using all three user profiles and
two non-logged-in users with no profile (one using a browser
and another using YouTube’s Data API). Recall that we delete
the user’s watch history at each experiment repetition and between those performed with different search queries to ensure
that future search results are not affected by previous activity
other than our controlled watch history. We perform this experiment on December 17–19, 2020.
Overall, we find a large variation in the results across pseudoscientific topics (see Fig. 3). For more traditional pseudoscientific topics like Flat Earth, YouTube search returns even

Implementation. The experiments are written as custom
scripts using Selenium in Python 3.7. For each Google account,
we create a separate Selenium instance for which we set a custom data directory, thus being able to perform manual actions
on the browser before starting our experiments, e.g., performing Google authentication, installing AdBlock Plus to prevent
advertisements within YouTube videos from interfering with
our simulations, etc. Finally, for all our experiments, we use
Chromedriver 83.0.4 that runs in headless mode and stores all
received cookies.
Video Annotation. To annotate the videos, we initially use our
classifier to dig up pseudoscientific videos. Then, the first author of this paper manually inspects all the videos classified as
pseudoscientific to confirm that they are indeed such. Following this approach, we eliminate all the false positives.
6

4
3
2
1
0

0

20

5
10
15
# top search results videos

(a) COVID-19

20

25

Flat Earth
25

20

20

15
10
5
0

0

5
10
15
# top search results videos

20

(b) Anti-vaccination

No Profile (YouTube Data API)

15
10
5
0

0

5
10
15
# top search results videos

(c) Anti-mask

20

% unique pseudoscientific videos

25

Science/Pseudoscience Profile
No Profile (Browser)
% unique pseudoscientific videos

5

% unique pseudoscientific videos

% unique pseudoscientific videos

Science Profile
Pseudoscience Profile

50
40
30
20
10
0

0

5
10
15
# top search results videos

20

(d) Flat Earth

Figure 3: Percentage of unique pseudoscience videos found in the search results of each user profile.

% unique pseudoscientific videos

more pseudoscientific content. In particular, when search- one video from the top 20 search results for each topic. Then,
15
ing for Flat earth,
the Science profile, Pseudoscience profile, we watch the selected video, obtain its top ten recommended
Science/Pseudoscience profile, no profile (browser), and the videos, and randomly select one. Again, we watch that selected
YouTube Data API encounter, respectively, 5.0%, 2.0%, 3.9%, video and randomly choose one of its top ten recommendations.
5.0%, and 5.6% more unique pseudoscientific content than This simulates the behavior of a user who watches videos based
when searching for Anti-vaccination, which is the topic with on recommendations, selecting the next video randomly from
the second-highest amount of pseudoscientific content across among the top ten recommendations until he reaches five hops
(i.e., six total videos viewed), thus ending a single live random
all profiles. For
10 topics like COVID-19, all the recommended
videos are not pseudoscientific, suggesting that YouTube’s rec- walk. We repeat this process for 50 random walks for each
ommendation algorithm does a better job in recommending less search term related to our pseudoscientific topics while autoharmful videos—at least for COVID-19. This also signifies that matically classifying each video we visit. We perform this exYouTube has made substantial efforts to tackle COVID-related periment with all three Google accounts (logged-in users), the
misinformation [20], establishing an official, dedicated policy user with no profile (browser), and the YouTube Data API befor that [46]. However, this is not the case for other contro- tween December 20–31, 2020.
5
versial and timely pseudoscientific topics like Anti-vaccination
For each user profile’s random walks, we calculate the peror Anti-mask. Nevertheless, YouTube has recently announced centage of pseudoscientific videos encountered over all unique
that they will also attempt to target COVID-19 vaccine misin- videos that the random walker visits up to the k-th hop. Note
formation [45].
that we have already assessed the amount of pseudoscientific
For Anti-vaccination, Anti-mask, and Flat earth searches, content in the search results. Hence, in this experiment, we foYouTube outputs more pseudoscientific content to the Pseudo- cus on video recommendations and do not consider, in our cal0
science and Science/Pseudoscience
profiles
than to the Science 3 culations, the initial video
1
2
4
5
of each random walk
selected from
# hop
one. Specifically, the amount of unique pseudoscientific videos the search results.
in the top 20 search results of the Pseudoscience profile is, reFigure 4 plots this percentage per hop for each of the pseudospectively, 18.0%, 9.5%, and 20.0% for Anti-vaccination, Anti- scientific topics explored. Looking at the percentage of pseudomask, and Flat Earth. For the Science/Pseudoscience profile, it scientific videos encountered by each user profile in all the ranis 16.1%, 9.5%, and 20.0%, while for the Science one is 10.0%, dom walks of each pseudoscientific topic, we highlight some
4.8%, and 18.0%.
interesting findings. For all topics, the amount of pseudoscientific content being suggested to the Pseudoscience profile after
five hops is higher than the Science profile (see Fig. 4). In particular, the portion of unique pseudoscientific videos encountered by the Pseudoscience profile after five hops is 2.1%, 3.6%,
2.8%, and 7.1% for COVID-19, Anti-vaccination, Anti-mask,
and Flat Earth, respectively, while for the Science profile, it is
0.8%, 1.9%, 0.0%, and 3.1%. We also validate the statistical
significance of the differences in the portion of pseudoscientific content suggested to the Pseudoscience profile compared
to the Science profile for Anti-vaccination, Anti-mask, and Flat
Earth, via the Fisher’s Exact test (p < 0.05).
Lastly, we find that for more traditional pseudoscientific topics like Flat Earth, YouTube suggests more pseudoscientific
content to all types of users, except the YouTube Data API,
compared to the other three more recent pseudoscientific topics. Using Fisher’s exact test, we confirm that this difference

Video Recommendations. Last but not least, we set out to
assess YouTube’s recommendation algorithm’s pseudoscience
problem by performing controlled, live random walks on the
recommendation graph while again measuring the effect of a
user’s watch history. This allows us to simulate the behavior of
users with varying interests who search the platform for a video
and subsequently watch several videos according to recommendations. Note that videos are nodes in YouTube’s recommendation graph, and video recommendations are directed edges
connecting a video to its recommended videos. For example,
a YouTube video page can be seen as a snapshot of YouTube’s
recommendation graph showing a single node (video) and all
the directed edges to all its recommended videos in the graph.
For our experiments, we use the four pseudoscientific topics
considered for the creation of our ground-truth dataset. We initially perform a search query on YouTube and randomly select
7

8
6
4
2
0

1

20

2

3
# hop

4

5

(a) COVID-19

10

Flat Earth
25

8

20

6
4
2
0

1

2

3
# hop

4

5

No Profile (YouTube Data API)

15
10
5
0

1

(b) Anti-vaccination

2

3
# hop

4

(c) Anti-mask

5

% unique pseudoscientific videos

25

Science/Pseudoscience Profile
No Profile (Browser)
% unique pseudoscientific videos

10

% unique pseudoscientific videos

% unique pseudoscientific videos

Science Profile
Pseudoscience Profile

25
20
15
10
5
0

1

2

3
# hop

4

5

(d) Flat Earth

Home (Top 30)

% unique pseudoscientific videos

Figure 4: Percentage of unique pseudoscientific videos that the random walker encounters at hop k per user profile.

15

COVID-19
Anti-vacc
Search
Anti-mask
(Top 20)
10
Flat Earth
All Topics

3.1%. However, this difference is not statistically significant
and this indicates that the YouTube Data API results do not
account for user personalization, neither the API maintains a
watch history. On the other hand, this difference may indicate
that the YouTube Data API is more sensitive to item-to-item
mapping [26] of the videos by the recommendation engine.

Profile
No Profile
Sci Pseudo Sci/Pseudo Browser API
2.4%

9.8%

4.4%

1.9%

-

0.0% 0.0%
10.0% 18.0%
4.8% 9.5%
15.0% 20.0%
6.6% 10.9%

0.0%
16.1%
9.5%
20.0%
10.3%

0.0%
15.0%
9.1%
20.0%
9.8%

0.0%
13.4%
10.0%
19.0%
9.2%

4.3

Take Aways

We now summarize the main findings of our experiments.
Table
5 reports the percentage of unique pseudoscientific videos
COVID-19 0.8% 2.1%
0.7%
0.7% 1.3%
appearing
on the YouTube homepage, search results, and the
Anti-vacc 2.1% 3.6%
1.9%
0.7% 2.0%
Video
Anti-mask 0.0% 2.8%
1.3%
1.6% 4.6%
video recommendations section for each user profile out of all
Recs
Flat Earth 3.1% 7.1%
4.7%
3.1% 4.2%
the unique videos encountered by each user profile in each exAll Topics
1.5% 3.6%
1.9%
1.2% 2.5%
5
periment.
The highest percentage of pseudoscientific videos occurs in
Table 5: Percentage of unique pseudoscientific videos encountered by
each user profile in the three main parts of YouTube.
the search results. That experiment shows that, for all pseudoscientific topics except COVID-19, the Pseudoscience and
the Science/Pseudoscience profiles encounter more pseudoscibetween Flat Earth and COVID-19 is statistically significant for entific content when searching for these topics than the Science
all types of users (p < 0.05), while for Anti-mask this holds for profile. For COVID-19, none of the profiles see any pseudosci0
1
2
3
4
5
the Science profile
and the no profile (browser),
and for Anti- # hop
entific content. When it comes to recommendations, in all the
vaccination this holds for the Pseudoscience profile and the no random walks (except Anti-mask), the Pseudoscience profile
profile (browser). This is another indication that YouTube has gets more pseudoscientific content than all the other profiles.
taken measures to counter the spread of pseudoscientific misin- For Anti-mask, we find a higher proportion of pseudoscientific
formation related to important topics like the COVID-19 pan- content using the Data API.
demic.
Overall, the main findings of our analysis are:
Overall, in most cases, the watch history of the user does
affect user recommendations and the amount of pseudoscien1. The watch history of the user substantially affects what
tific content suggested by YouTube’s algorithm. This is also
videos are suggested to the user.
evident from the results of the random walks performed on the
browser by the user with no profile. This profile does not main2. It is more likely to encounter pseudoscientific videos in the
tain a watch history. It is recommended less pseudoscientific
search results (i.e., when searching for a specific topic)
content than all the other profiles after five hops when starting
than in the video recommendations section or the homefrom a video related to COVID-19 (0.7%), and mainly to Antipage of a user, except in the case of the COVID-19 topic.
vaccination (0.7%) and Flat earth (3.1%).
3. For “traditional” pseudoscience topics (e.g., Flat Earth),
Finally, we find a higher amount of pseudoscientific content
there is a higher rate of recommended pseudoscientific
in the random walks performed using the YouTube Data API
than the random walks performed with the other non-loggedcontent than for more emerging/controversial topics like
in user on the browser. In particular, the amount of unique
COVID-19, anti-vaccination, and anti-mask. For COVID19, we find an even smaller amount of pseudoscientific
pseudoscientific videos encountered by the YouTube Data API
after five hops is 1.3%, 2.0%, 4.6%, and 4.2% for COVIDcontent being suggested, which may result from measures
YouTube took to mitigate misinformation concerning the
19, Anti-vaccination, Anti-mask, and Flat earth, respectively,
COVID-19 pandemic.
while, for the no profile (browser), it is 0.7%, 0.7%, 1.6%, and
8

4. Although YouTube seems to tackle COVID-19 related
misinformation in its search results, all profiles used in our
experiments still receive recommendations to questionable
content related to the pandemic.

et al. [48] introduce a large-scale ranking system for YouTube
recommendations, which ranks the candidate recommendations
of a given video, taking into account user engagement and satisfaction metrics (e.g., video likes). Next, Ribeiro et al. [36] perform a large-scale audit of user radicalization on YouTube: they
analyze videos from Intellectual Dark Web, Alt-lite, and Altright channels, showing that they increasingly share the same
user base. Papadamou et al. [31] focus on detecting disturbing
videos on YouTube targeting young children finding that young
children are likely to encounter disturbing videos when they
randomly browse the platform.

5. The difference between the results of the YouTube Data
API and the no profile (browser) is statistically insignificant; this indicates that recommendations returned using
the API are not subject to personalization.This finding can
be helpful to other researchers that use YouTube’s Data
API for their experiments.

5

User Personalization. Most of the work on user personalization focuses on Web search engines and is motivated by the concerns around the Filter Bubble effect [32]. Hannak [16] propose
a methodology for measuring personalization in Web search
results, while Kliman-Silver et al. [21] propose a methodology for exploring the impact of location-based personalization
on Google search results. Robertson et al. [38] focus on the
personalization and composition of politically-related search
engine results, and they propose a methodology for auditing
Google Search. Le et al. [24] investigate whether politically
oriented Google news search results are personalized based on
the user’s browsing history; using a “sock puppet” audit system,
they find significant personalization, which tends to reinforce
the presumed partisanship of a user.
Stöcker et al. [41] analyze the effect of extreme recommendations on YouTube, finding that YouTube’s auto-play feature
is problematic. Finally, Hussein et al. [17] focus on measuring misinformation on YouTube considering five popular
topics (e.g., the chemtrail conspiracy theory) to investigate
whether personalization contributes to amplifying misinformation. They find that, once a user develops a watch history, the
demographic attributes affect the extend of misinformation recommended to the users. More importantly, they find a filter
bubble effect in the video recommendations section for almost
all the topics they analyze. Unlike [17], we build a classifier and
use it to characterize and detect pseudoscientific misinformation on YouTube, aiming to understand how a user’s watch history affects YouTube’s recommendations across multiple parts
of the platform (i.e., homepage, search results page, and video
recommendations). We introduce a novel methodology that
includes the simulation of the behavior of users with distinct
watch histories who search the platform for a video and subsequently watch several videos according to recommendations.

Related Work

This section reviews prior work on pseudoscience, misinformation, and other malicious activity on YouTube, the recommendation algorithm, and user personalization across the Web.
Pseudoscience and Misinformation. The scientific community has extensively studied the phenomenon of misinformation
and the credibility issues of online content [22]. The majority of
previous work focuses on analyzing misinformation and pseudoscientific content on other social networks [19, 34], although
some study specific misinformative and conspiratorial topics on
YouTube.
For instance, Li et al. [25] study misinformation related to
the COVID-19 pandemic on YouTube; they search YouTube
using the terms “coronavirus” and “COVID-19,” and analyze
the top 75 viewed videos from each search term, finding 27.5%
of them to be misinformation. Donzelli et al. [9] focus on misinformation surrounding vaccines that supposedly cause autism
by performing a quantitative analysis of YouTube videos. Landrum et al. [23] investigate how users with varying science
comprehension and attitude towards conspiracies are susceptible to Flat Earth arguments on YouTube. They find that users
with lower science intelligence and higher conspiracy mentality
are more likely to be recommended Flat Earth-related videos.
Last, Faddoul et al. [13] develop a classifier to detect conspiratorial videos on YouTube and use it to perform a longitudinal
analysis of conspiracy videos simulating YouTube’s autoplay
feature, without user personalization. Our work extends prior
research as we focus on multiple health-related and other traditional misinformation topics on YouTube. We present a classifier and a novel methodology which allow us to assess the
effects of a user’s watch history on YouTube’s pseudoscientific
recommendations in multiple parts of the platform.
Malicious activity on YouTube. A substantial body of
work focuses on detecting and studying malicious content on
YouTube. Jiang et al. [18] investigate how channel partisanship affects comment moderation on YouTube. Zannettou et
al. [47] propose a deep learning classifier for identifying videos
on YouTube that use manipulative techniques to increase their
views, i.e., clickbait. Agarwal et al. [2] present a binary classifier trained with user and video features to detect videos promoting hate and extremism on YouTube, while Mariconti et
al. [28] build a classifier to predict, at upload time, whether
a YouTube video will be “raided” by hateful users.

6

Discussion & Conclusion

In this work, we studied pseudoscientific content on the
YouTube platform. We collected a dataset of 6.6K YouTube
videos, and by using crowdsourcing, we annotated them according to whether or not they include pseudoscientific content.
We then trained a deep learning classifier to detect pseudoscientific videos. We used the classifier to perform experiments
assessing the prevalence of pseudoscientific content on various parts of the platform while accounting for the effects of
the user’s watch history. To do so, we crafted a set of accounts
with different watch histories.

YouTube’s Recommendation Algorithm and Audits. Zhao
9

Main Results. Overall, we found that the user’s watch history does substantially affect future user recommendations by
YouTube’s algorithm. This should be taken into consideration by research communities aiming to audit the recommendation algorithm and understand how it drives users’ content
consumption patterns. We also found that YouTube search results are more likely to return pseudoscientific content than
other parts of the platform like the video recommendations section or a user’s homepage. However, we also observed a nonnegligible number of pseudoscientific videos on both the video
recommendations section and the users’ homepage. By investigating the differences across multiple pseudoscientific topics,
we showed that the recommendation algorithm is more likely
to recommend pseudoscientific content from traditional pseudoscience topics, e.g., Flat Earth, compared to more controversial topics like COVID-19. This likely indicates that YouTube
takes measures to counter the spread of harmful information
related to critical and emerging topics like the COVID-19 pandemic. However, achieving this in a proactive and timely manner across topics remains a challenge.
Looking Forward. The relatively low agreement score of
our crowdsourced annotation points to the difficulty in objectively identifying whether a video is pseudoscientific or not and
also confirms that it is not easy to automate the discovery of
misinformation. Hence, we believe that the most proper way
for YouTube to cope with misinformation on the platform effectively is to use deep learning models that signal potential
pseudoscientific videos to human annotators who examine the
videos and make the final decision.
Our work provides insights on pseudoscientific videos on
YouTube and provides a set of resources to the research community (we will make the dataset, the classifier, and all the
source code of our experiments publicly available). In particular, the ability to run this kind of experiments while taking
into account users’ viewing history will be beneficial to researchers focusing on demystifying YouTube’s recommendation algorithm—irrespective of the topic of interest. In other
words, our methodology and codebase are generic and can be
used to study other topics besides pseudoscience, e.g., additional conspiracy theories.
Limitations. Naturally, our work is not without limitations.
First, we use crowdworkers who are unlikely to have any expertise in identifying pseudoscientific content. Hence, a small
percentage of the annotated videos may be misclassified. However, we mitigated this issue by not including annotators with
low accuracy on a classification task performed on a test dataset
and annotating each video based on the majority agreement.
We also evaluated our crowsourced annotation’s performance
by manually reviewing a randomly selected set of videos from
our ground-truth dataset, yielding 0.90 precision, 0.89 recall,
and 0.89 F1 score.
Second, our ground-truth dataset is relatively small for such
a subjective classification task. Nonetheless, the classifier provides a meaningful signal, which, supported by manual review,
allows us to assess YouTube’s recommendation algorithm’s behavior with respect to pseudoscientific content. Third, there
might be videos in our experiments that are pseudoscientific

and have been classified as “Other.” Hence, to verify our results’ accuracy, we manually reviewed a random sample (10%)
of the videos encountered during our experiments and classified them as “Other,” finding that 98.0% of them were correctly
classified. Finally, as for user personalization, we only work
with watch history, which is a fraction of YouTube’s signals for
user personalization.
Future Work. A more comprehensive user personalization
methodology to account for factors outside of watch history,
such as account characteristics and user engagement, is a clear
direction for future research. We also plan to conduct studies to
understand how people share and view pseudoscientific content
on other social networks, including Twitter and Facebook, and
how people engage with such content.
Acknowledgments. This project has received funding from
the European Union’s Horizon 2020 Research and Innovation
program under the CONCORDIA project (Grant Agreement
No. 830927), and from the Innovation and Networks Executive Agency (INEA) under the CYberSafety II project (Grant
Agreement No. 1614254). This work reflects only the authors’
views; the funding agencies are not responsible for any use that
may be made of the information it contains.

References
[1] Supplemental Material:
Pseudoscientific Topics List,
Annotation Platform Instructions, and YouTube Parts
Analyzed.
https://drive.google.com/drive/folders/
1URunbvaDMvo7KEl7dUtGbBsQ MYfug4A, 2021.
[2] S. Agarwal and A. Sureka. A Focused Crawler for Mining Hate
and Extremism Promoting Videos on YouTube. In ACM Hypertext, 2014.
[3] Appen. AI Solutions with confident Training Data. https://appen.
com/solutions/training-data/, 2020.
[4] P. Ball. Anti-vaccine Movement Could Undermine Efforts to End
Coronavirus Pandemic, Researchers Warn. In Nature, 2020.
[5] N. Carne. ”conspiracies” dominate youtube videos. http://bit.ly/
climate-conspiracies, 2019.
[6] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer.
SMOTE: Synthetic Minority Over-Sampling Technique. In Journal of Artificial Intelligence Research, 2002.
[7] G. Developers. YouTube Data API. http://bit.ly/3qbSN0p, 2020.
[8] D. Dittrich, E. Kenneally, et al. The Menlo Report: Ethical Principles Guiding Information and Communication Technology Research. Technical report, US Department of Homeland Security,
2012.
[9] G. Donzelli, G. Palomba, I. Federigi, F. Aquino, L. Cioni, M. Verani, A. Carducci, and P. Lopalco. Misinformation on Vaccination: a Quantitative Analysis of YouTube Videos. In Human
vaccines & Immunotherapeutics, 2018.
[10] J. D’Urso and A. Wickham. YouTube Is Letting Millions Of People Watch Videos Promoting Misinformation About The Coronavirus. http://bzfd.it/2L9xn5f, 2020.
[11] M. Enserink and J. Cohen. Fact-checking Judy Mikovits, the
Controversial Virologist Attacking Anthony Fauci in a Viral
Conspiracy Video. In Science, 2020.
[12] Facebook. fastText - Efficient Text Classification and Representation Learning. http://bit.ly/fasttext-library, 2020.

10

[30] N. Newman, R. Fletcher, A. Schulz, S. Andi, and R. K. Nielsen.
Reuters Digital News Report. http://bit.ly/rtrs-report, 2020.
[31] K. Papadamou, A. Papasavva, S. Zannettou, J. Blackburn,
N. Kourtellis, I. Leontiadis, G. Stringhini, and M. Sirivianos.
Disturbed YouTube for Kids: Characterizing and Detecting Inappropriate Videos Targeting Young Children. In ICWSM, 2020.
[32] E. Pariser. The filter Bubble: How the New Personalized Web is
Changing What We Read and How We Think. Penguin, 2011.
[33] F. Provost. Machine Learning from Imbalanced Data Sets 101.
In WLIDS, 2000.
[34] M. Rajdev and K. Lee. Fake and Spam Messages: Detecting
Misinformation During Natural Disasters on Social Media. In
WI-IAT, 2015.
[35] K. Renic. Coronavirus: Dozens show up at anti-mask rally in
Moncton, N.B. http://bit.ly/covid-anti-mask, 2020.
[36] M. H. Ribeiro, R. Ottoni, R. West, V. A. Almeida, and
W. Meira Jr. Auditing Radicalization Pathways on YouTube. In
ACM FAT*, 2020.
[37] C. M. Rivers and B. L. Lewis. Ethical Research Standards in a
World of Big Data. In F1000Research, 2014.
[38] R. E. Robertson, D. Lazer, and C. Wilson. Auditing the Personalization and Composition of Politically-Related Search Engine
Results Pages. In TheWebConf, 2018.
[39] E. Scott. Why people believe the Earth is flat and we should
listen to anti-vaxxers. http://bit.ly/guardian-flat-earth, 2019.
[40] M. Spring. Coronavirus: False claims viewed by millions on
YouTube. http://bbc.in/2MSlJMz, 2020.
[41] C. Stöcker and M. Preuss. Riding the Wave of Misclassification:
How We End up with Extreme YouTube Content. In CHI, 2020.
[42] I. Turc, M.-W. Chang, K. Lee, and K. Toutanova. Well-read Students Learn Better: On the Importance of Pre-Training Compact
Models. In arXiv preprint arXiv:1908.08962, 2019.
[43] J. Vincent. YouTube brings back more human moderators after
AI systems over-censor. http://bit.ly/youtube-moderators, 2020.
[44] C. G. Weissman. Despite recent crackdown, YouTube still promotes plenty of conspiracies. http://bit.ly/youtube-consp, 2019.
[45] N. Westman. YouTube will remove videos with COVID-19 vaccine misinformation. http://bit.ly/youtube-anti-vaxx, 2020.
[46] YouTube Help. COVID-19 Medical Misinformation Policy. http:
//bit.ly/youtube-covid-policy, 2020.
[47] S. Zannettou, S. Chatzis, K. Papadamou, and M. Sirivianos.
The Good, the Bad and The Bait: Detecting and Characterizing
Clickbait on YouTube. In IEEE SPW, 2018.
[48] Z. Zhao, L. Hong, L. Wei, J. Chen, A. Nath, S. Andrews,
A. Kumthekar, M. Sathiamoorthy, X. Yi, and E. Chi. Recommending What Video to Watch Next: A Multitask Ranking System. In ACM RecSys, 2019.

[13] M. Faddoul, G. Chaslot, and H. Farid.
A Longitudinal
Analysis of YouTube’s Promotion of Conspiracy Videos. In
arXiv:2003.03318, 2020.
[14] J. L. Fleiss. Measuring Nominal Scale Agreement Among Many
Raters. In Psychological bulletin, 1971.
[15] J. Goodman and F. Carmichael. Coronavirus: Bill Gates
”microchip” conspiracy theory and other vaccine claims factchecked. http://bbc.in/3igJEkl, 2020.
[16] A. Hannak, P. Sapiezynski, A. Molavi Kakhki, B. Krishnamurthy, D. Lazer, A. Mislove, and C. Wilson. Measuring Personalization of Web Search. In TheWebConf, 2013.
[17] E. Hussein, P. Juneja, and T. Mitra. Measuring Misinformation
in Video Search Platforms: An Audit Study on YouTube. In CHI,
2020.
[18] S. Jiang, R. E. Robertson, and C. Wilson. Bias Misperceived:
The Role of Partisanship and Misinformation in YouTube Comment Moderation. In ICWSM, 2019.
[19] N. F. Johnson, N. Velásquez, N. J. Restrepo, R. Leahy,
N. Gabriel, S. El Oud, M. Zheng, P. Manrique, S. Wuchty,
and Y. Lupu. The Online Competition Between Pro-and Antivaccination Views. In Nature, 2020.
[20] L. Kelion. Coronavirus: YouTube Tightens Rules After David
Icke 5G Interview. http://bbc.in/38xHhqe, 2020.
[21] C. Kliman-Silver, A. Hannak, D. Lazer, C. Wilson, and A. Mislove. Location, Location, Location: The Impact of Geolocation
on Web Search Personalization. In IMC, 2015.
[22] S. Kumar and N. Shah. False Information on Web and Social
Media: A Survey. In arXiv:1804.08559, 2018.
[23] A. R. Landrum, A. Olshansky, and O. Richards. Differential
Susceptibility to Misleading Flat Earth Arguments on YouTube.
In Media Psychology, 2019.
[24] H. Le, R. Maragh, B. Ekdale, A. High, T. Havens, and Z. Shafiq.
Measuring Political Personalization of Google News Search. In
TheWebConf, 2019.
[25] H. O.-Y. Li, A. Bailey, D. Huynh, and J. Chan. YouTube as A
Source of Information on COVID-19: A Pandemic of Misinformation? In BMJ Global Health, 2020.
[26] G. Linden, B. Smith, and J. York. Amazon.com Recommendations: Item-to-item Collaborative Filtering. In IEEE Internet
computing, 2003.
[27] M. Lynas. 5G: What’s behind the latest COVID conspiracy theory? http://bit.ly/covid-5g, 2020.
[28] E. Mariconti, G. Suarez-Tangil, J. Blackburn, E. De Cristofaro, N. Kourtellis, I. Leontiadis, J. L. Serrano, and G. Stringhini. “You Know What to Do”: Proactive Detection of YouTube
Videos Targeted by Coordinated Hate Attacks. In CSCW, 2019.
[29] T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, and A. Joulin.
Advances in Pre-Training Distributed Word Representations. In
LREC, 2018.

11

