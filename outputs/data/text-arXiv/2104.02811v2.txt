1

C2CL: Contact to Contactless Fingerprint Matching
Steven A. Grosz, Joshua J. Engelsma and Anil K. Jain, Life Fellow, IEEE

Fingerprint Acquisition

Contactless

Segmentation

Enhancement

Matching
Warp + Scale

Contact-based

Contactless

Operator-Capture

Minutiae Matching

Contact-based

Legacy Database

Match Score

arXiv:2104.02811v2 [cs.CV] 8 Apr 2021

Self-capture

Cosine
Similarity

Texture Matching

Fig. 1: Overview of matching contactless fingerprint images with a legacy database of contact-based fingerprint impressions. While only a specific scenario
is shown here where contact-based images are obtained from optical FTIR readers (slap or single finger capture) and contactless images are captured by a
smartphone camera, our approach can be applied to any heterogeneous fingerprint matching problem.

Abstract—Matching contactless fingerprints or finger photos
to contact-based fingerprint impressions has received increased
attention in the wake of COVID-19 due to the superior hygiene
of the contactless acquisition and the widespread availability of
low cost mobile phones capable of capturing photos of fingerprints with sufficient resolution for verification purposes. This
paper presents an end-to-end automated system, called C2CL,
comprised of a mobile finger photo capture app, preprocessing,
and matching algorithms to handle the challenges inhibiting
previous cross-matching methods; namely i) low ridge-valley
contrast of contactless fingerprints, ii) varying roll, pitch, yaw,
and distance of the finger to the camera, iii) non-linear distortion
of contact-based fingerprints, and vi) different image qualities
of smartphone cameras. Our preprocessing algorithm segments,
enhances, scales, and unwarps contactless fingerprints, while
our matching algorithm extracts both minutiae and texture
representations. A sequestered dataset of 9, 888 contactless 2D
fingerprints and corresponding contact-based fingerprints from
206 subjects (2 thumbs and 2 index fingers for each subject)
acquired using our mobile capture app is used to evaluate the
cross-database performance of our proposed algorithm. Furthermore, additional experimental results on 3 publicly available
datasets demonstrate, for the first time, contact to contactless
fingerprint matching accuracy that is comparable to existing
contact to contact fingerprint matching systems (TAR in the
range of 96.67% to 98.15% at FAR=0.01%).
Index Terms—Fingerprint recognition, Sensor interoperability,
Contact to contactless fingerprint matching
S.A. Grosz, J.J. Engelsma, and A. K. Jain are with the Department of
Computer Science and Engineering, Michigan State University, East Lansing,
MI, 48824. E-mail: {groszste, engelsm7, jain}@cse.msu.edu

I. I NTRODUCTION

D

UE to their presumed uniqueness and permanence, fingerprints are one of the most widely used biometric
traits for secure authentication and search [1], [2]. Over the
years many different types of fingerprint readers have been
developed to obtain a digital image of a finger’s friction ridge
pattern. These readers vary in a number of different ways,
including the underlying sensing technology (e.g., optical,
capacitive, ultrasonic, etc.) or in the manner in which a user
interacts with the reader (i.e. contactless, 4-4-2 slap, or single
finger contact-based acquisition). Most prevailing fingerprint
readers in use today necessitate physical contact of the user’s
finger with the imaging surface of the reader; however, this
direct contact presents certain challenges in processing the
acquired fingerprint images. Most notably, elastic human skin
introduces a non-linear deformation upon contact with the
imaging surface which has been shown to significantly degrade
matching performance [3], [4], [5]. Furthermore, contact with
the surface is likely to leave a latent impression on the imaging
surface [6], which presents a security risk as an imposter
could illegally gain access to the system though creation of
a presentation (i.e., spoof) attack.
In light of the ongoing Covid-19 pandemic, contactless
fingerprint recognition has gained renewed interest as a hygienic alternative to contact-based fingerprint acquisition [7].
This is further supported by a recent survey that showed
that the majority of users prefer touchless capture methods in
terms of usability and hygeine considerations [8]. Prior studies

2

(a)

(b)
Fig. 2: Examples of contactless fingerprints (a) and their corresponding
contact-based fingerprint images (b). Varying viewing angle (indicated by the
orientation axes), resolution, and illumination of contactless images and nonlinear distortion of contact-based fingerprints contribute to the degradation
of cross-matching performance. The contactless images shown were captured
using our smartphone app and the contact-based impressions are from a URU
4500 optical scanner.

have explored the use of customized 2D or 3D sensing for
contactless fingerprint acquisition [9], [10], [11], [12], [13],
[14], while others have explored the low-cost alternative of
using readily available smartphone cameras to capture “finger
photos”1 [15], [16], [17].
Despite the benefits of contactless fingerprint acquisition,
imaging and subsequently matching a contactless fingerprint
presents its own set of unique challenges. These include
(i) low ridge-valley contrast, (ii) non-uniform illumination,
(iii) varying roll, pitch, and yaw of the finger, (iv) varying
background, (v) perspective distortions due to the varying
distances of the finger from the camera, and (vi) lack of
cross-compatibility with legacy databases of contact-based
fingerprints (see Figure 2). For widespread adoption, contactless fingerprint recognition must overcome the aforementioned
challenges and achieve the same levels of accuracy as in
contact-contact fingerprint matching.
The most significant factor limiting the adoption of contactless fingerprint technology is cross-compatibility with legacy
databases of contact-based fingerprints, which is particularly
important for governmental agencies and large-scale national
ID programs such as India’s Aadhaar National ID program
which has already enrolled over 1 billion users based upon
contact-based fingerprints. Several studies have aimed at improving the compatibility of matching legacy slap images
to contactless fingerprint images [21], [20], [24], [25], [23],
[19]; however, none have achieved the same levels of ac-

curacy as state-of-the-art contact-contact fingerprint matching
(such as the results reported in FVC-ongoing [26] and NIST
FpVTE [27]). Furthermore, all of these works focus on solving
only a subset of the challenges in an effort to obtain contactcontactless matching accuracy which is comparable to state-ofthe-art contact-contact matching systems. Indeed, to the best
of our knowledge, this study presents the most comprehensive,
end-to-end solution in the open academic literature for contactcontactless fingerprint matching that addresses the challenges
inherent to each step in the contact to contactless matching
process (mobile capture, segmentation, enhancement, scaling,
non-linear warping, representation extraction, and matching).
We show that our end-to-end matcher, called C2CL, is able
to significantly improve contact-contactless matching performance over the prevailing state-of-the-art methods through
experimental results on a number of different datasets, collected by various research groups using their own app and
fingerprint readers. We also demonstrate that our matcher
generalizes well to datasets which were not included during
training. This cross-database evaluation solves a shortcoming
of many existing studies which train and evaluate algorithms
on different training and test splits of the same contactcontactless dataset. Furthermore, despite multiple evaluation
datasets, we train only a single model for our evaluations,
rather than fine-tuning individual models to fit a specific
dataset (as is the case in many previous studies).
Concretely, the contributions of this work are summarized
as:
1) An end-to-end system, called C2CL, for contactcontactless fingerprint matching. C2CL is comprised of
preprocessing, (segmentation, enhancement, scaling, and
deformation correction), feature extraction (minutiae and
texture representations), and matching modules. Our approach is more comprehensive than published methods
which focus on only a subset of these modules needed
for state-of-the-art performance.
2) A fully automated, preprocessing pipeline to map contactless fingerprints into the domain of contact-based fingerprints and a contactless-contact adaptation of DeepPrint [28] for representation extraction. Our preprocessing and representation extraction is generalizable across
multiple datasets and contactless capture devices.
3) State-of-the-art cross-matching verification and largescale identification accuracy using C2CL on both publicly available contact-contactless matching datasets as
well as on a completely sequestered dataset collected
at Zhejiang University, China. Our evaluation includes
the most diverse set of contactless fingerprint acquisition
devices, yet we employ just a single trained model for
evaluation.
4) A smartphone contactless fingerprint capture app that
was developed in-house for improved throughput and
user-convenience. This app will be made available to
the public to promote further research in this area2 .

1 In

general, contactless fingerprints refers to fingerprint images acquired by
a contactless fingerprint sensor, whereas finger photo refers to fingerprint
images acquired by a mobile phone. In this paper, we use the two terms
interchangeably.

2 The

project repository for the smartphone contactless fingerprint capture app
is available at https://github.com/ronny3050/FingerPhotos.

3

(a)

(b)

(c)

(d)

(e)

(f)

Fig. 3: Example contactless and contact-based fingerprint image pairs from databases which we have obtained from different research groups: (a) IIT
Bombay [18], (b) ISPFDv2 [19], (c) MSU [20], (d) PolyU [21], (e) UWA [22], and (f) ZJU datasets. In general, contactless fingerprints suffer from low
ridge-valley contrast, varying roll, pitch, and yaw, and perspective distortions, especially those captured by smartphone cameras (e.g., (a), (b), (c) and (f)). We
believe our study involves the largest collection of public domain databases of contactless and contact-based fingerprints.

II. P RIOR W ORK
Prior studies on contact-contactless fingerprint matching
primarily focus on only one of the sub-modules needed to
obtain matching accuracy comparable to contact-contact based
fingerprint matching systems (e.g. segmentation, distortion
correction, or feature extraction only). These studies are categorized and discussed below.
A. Segmentation
The first challenge in contact-contactless matching is segmenting the relevant fingerprint region from the captured
contactless fingerprint images. Malhotra et al. [19] proposed
a combination of a saliency map and a skin-color map to

segment the distal phalange (i.e., fingertip) of contactless
fingerprint images in presence of varying background, illumination and resolution. Despite impressive results, the algorithm
requires extensive hyperparameter tuning and still fails to accurately segment fingerprints in severe illumination conditions
or noisy backgrounds. To alleviate these issues, we incorporate
segmentation via an autoencoder trained to robustly segment
the distal phalange of input contactless images.
B. Enhancement
One of the better known challenges with contactless fingerprint images is the low ridge-valley contrast (Figure 3). The
literature has addressed this in a number of different ways,

4

TABLE I: Summary of Published Cross-Matching Contact to Contactless Fingerprint Recognition Studies.
Study

Approach

Database

Accuracy†

Lin and Kumar, 2018 [21]

Robust TPS deformation correction model,
minutiae and ridge matching

1, 800 contactless and contact fingerprints
from 300 fingers [21].
2, 000 contactless and 4, 000 contact fingerprints from 1, 000 fingers [22]

EER = 4.46% [21]
EER = 19.81% [22]

Deb et al., 2018 [20]

COTS matcher

2, 472 contactless and contact fingerprints
from 1, 236 fingers [20]

TAR = 92.4% − 98.6%
@ FAR = 0.1% [20]

Lin and Kumar, 2019 [23]

Fusion of three Siamese CNNs

960 contactless and contact fingerprints
from 160 fingers [21].
1, 000 contactless and 2, 000 contact fingerprints from 500 fingers [22]

EER = 7.93% [21]
EER = 7.11% [22]

Wild et al., 2019 [24]

Filtering based on NFIQ 2.0 quality measure, COTS matcher

1, 728 contactless and 2, 582 contact fingerprints from 108 fingers [24]

TAR = 95.5% − 98.6%
@ FAR = 0.1% [24]

Dabouei et al., 2019 [25]

TPS spatial transformer network for deformation correction and binary ridge-map
extraction network, COTS matcher

2, 000 contactless and 4, 000 contact
fingerprints from 1, 000 fingers [22]

EER = 7.71% [22]

Malhotra et al., 2020 [19]

Feature extraction with deep scattering network, random decision forest matcher

Priesnitz et al., 2021 [8]

Neural network-based minutiae feature extraction, open-source minutiae matcher

Proposed Approach

TPS spatial transformer for 500 ppi
scaling and deformation correction of
contactless fingerprints. Fusion of
minutiae and CNN texture representations.

8, 512 contactless and 1, 216 contact
fingerprints from 152 fingers [19]
896 contactless from two different capture
setups and 464 contact fingerprints from
232 fingers [8]
8, 512 contactless and 1, 216 contact fingerprints from 152 fingers [19].
2, 000 contactless and 4, 000 contact fingerprints from 1, 000 fingers [22].
960 contactless and contact fingerprints
from 160 fingers [21].
9, 888 contactless and 9, 888 contact fingerprints from 824 fingers (ZJU Finger Photo and Touch-based Fingerprint
Database)

EER = 2.11% − 5.23% [19]
EER = 15.71% and
32.02% [8]

EER
EER
EER
EER

=
=
=
=

1.20%
0.77%
0.30%
0.62%

[19]
[22]
[21]
(ZJU dataset)

† Some studies only report EER while other studies only report TAR @ FAR = 0.1%.

including adaptive histogram equalization, Gabor filtering,
median filtering, and sharpening by subtraction of the Gaussian
blurred image (with σ = 2) from the captured image ([25],
[23], [19]). We also incorporate adaptive contrast enhancement
in our work; however, one simple consideration that is lacking
in existing approaches is the ridge inversion that occurs with
Frustrated Total Internal Reflection (FTIR) optical imaging.
In particular, the ridges and valleys of an FTIR fingerprint
image will appear dark and light, respectively, while the
opposite is true in contactless fingerprint images. Therefore, a
simple binary inversion of the contactless fingerprint images
is expected to improve the correspondence with their contactbased counterparts.
C. Scaling
After segmenting and enhancing a contactless fingerprint,
the varying distances between fingers captured and the camera
(standoff) must be accounted for. In particular, since contactbased fingerprints are almost always captured at 500 pixels
per inch (ppi), the contactless fingerprints need to be scaled
to be as close to 500 ppi as possible. Previous studies have
applied a fixed manual scaling, set for a specific dataset, or
have employed contact-based fingerprint ridge frequency estimation and/or normalization algorithms that rely on accurate
ridge extraction - which is often unreliable for contactless
fingerprints. In contrast, we incorporate a spatial transformer
network [30] which has been trained to automatically normalize the resolution of the contactless fingerprints to match

that of the 500 ppi contact images. This scaling is performed
dynamically, i.e. every input contactless fingerprint image is
independently scaled.
D. Distortion Correction
A final preprocessing step necessary for contact-contactless
fingerprint matching is non-linear distortion correction. In
particular, non-linear distortions are present in contact-based
fingerprints (due to interaction between the finger and the
sensor platen which “unrolls” a contactless fingerprint into a
distorted contact-based fingerprint). To address this problem,
[21] used thin-plate-spline (TPS) deformation correction models (previously applied for contact-contact matching [3], [31],
[32], [5], [33], [34]) using the alignment between minutiae
annotations of corresponding contactless and contact fingerprints. A limitation is that the transformation is limited to
one of six possible parameterizations. In a different study,
Dabouei et al. [31] train a spatial transformer to learn the
TPS distortion correction that is dynamically computed for
each input image. In [31], a contact-based image is used as the
reference for learning the distortion correction for a contactless
image. However, we argue that this is not a reliable ground
truth since the deformation varies among different contactbased fingerprint impressions. In our attempt to re-implement
the algorithm in [31], we found that this lack of a reliable and
consistent ground truth makes training quite unstable, making
it difficult to learn sound distortion parameters. In our work,
rather than using the contact-based image as a reference, we

5

TABLE II: Summary of contact to contactless fingerprint recognition datasets used in this study.
Dataset
UWA Benchmark 3D Fingerprint Database, 2014 [22]

# Subjects

# Unique
Fingers

# Images
(Contactless / Contact)

150

1, 500

3, 000 / 6, 000

Contactless Capture Device

Contact Capture Device

3D Scanner (TBS S120E)

CROSSMATCH Verifier 300
LC2.0

AOS ANDI On-The-Go
(OTG), MorphoTrak
Finger-On-The-Fly (FOTF),
IDair innerID on iPhone 4.

Cross Match Guardian R2,
Cross Match SEEK Avenger,
MorphoTrak MorphoIDent,
MorphoTrust
TouchPrint
5300, Northrop Grumman
BioSled

ManTech Phase2, 2015 [29]

496

4, 960

N/A / N/A∗

PolyU Contactless 2D to
Contact-based 2D Images
Database, 2018 [21]

N/A

336

2, 976 / 2, 976

Low-cost camera and lens
(specific device not given)

URU 4000

MSU Finger Photo and
Slap Fingerprint Database,
2018 [20]

309

1, 236

2, 472 / 2, 472

Xiaomi Redmi Note 4 smartphone

CrossMatch Guardian 200,
SilkID (SLK20R)

IIT Bombay Touchless and
Touch-Based
Fingerprint
Database, 2019 [18]

N/A

200

800 / 800

Lenovo Vibe k5 smartphone

eNBioScan-C1 (HFDU08)

ISPFDv2, 2020 [19]

76

304

17, 024 / 2, 432

OnePlus One (OPO) and Micromax Canvas Knight smartphones

Secugen Hamster IV

ZJU Finger Photo and Touchbased Fingerprint Database

206

824

9, 888 / 9, 888

∗

HuaWei P20, Samsung s9+,
and OnePlus 8 smartphones

URU 4500

The number of contact and contactless images acquired per finger varies for each device and the exact number is not provided.

Fig. 4: System architecture of C2CL. (a) A contactless fingerprint is captured and used as input to the preprocessing module, consisting of segmentation,
enhancement, 500 ppi ridge frequency scaling, and deformation correction; (b) the transformed image output by the preprocessing module is fed to
DeepPrint [28], which extracts a texture representation (shown in red). Without performing any additional preprocessing, the corresponding contact-based
fingerprint is again fed to DeepPrint to extract a texture representation (shown in blue). Simultaneously, a minutiae representation is extracted using the
Verifinger 12.0 SDK from both the contactless and contact-based fingerprint images.

use the match scores of our texture matcher as supervision for
generating robust distortion correction. In other words, in our
approach, the distortion correction is optimized to maximize
the match scores between genuine contactless and contact
fingerprint pairs.
E. Representation Extraction and Matching
After preprocessing a contactless fingerprint image to lie
within the same domain as a contact-based fingerprint, a
discriminative representation must be extracted for matching.
In the prior literature there are two main approaches to feature
representation: (i) minutiae representation ([31], [21]) and
(ii) deep learning representation ([23], [19]). Minutiae-based

approaches rely on clever preprocessing and other techniques
to improve the compatability of contactless fingerprint images
for traditional contact-based minutiae extraction algorithms.
On the other hand, deep learning approaches place less emphasis on preprocessing to manipulate the contactless fingerprint image to improve correspondence with their contactbased fingerprints, rather the responsibility is placed on the
representation network to learn the correspondence despite the
differences. For example, Lin and Kumar [35] and Dabouei et
al. [31] both apply a deformation correction to the contactless
image to improve the minutiae correspondence while matching contact-based fingerprints. In contrast, the deep learning
approach taken in [23] applies very little preprocessing to
the contactless image (just contrast enhancement and Gabor

6

(a)

(b)

1) Segmentation: Many contactless fingerprint datasets are
unsegmented; for example, the ISPFDv2 dataset [19] contains
unsegmented, (4, 208 × 3, 120) images with varying illumination, resolution, and background conditions. Thus, the first step
in our preprocessing pipeline is to segment the distal phalange
of the fingerprint using a U-net segmentation network [36].
Our segmentation algorithm is a network S(·) which takes as
input the unsegmented contactless fingerprint Icl of dimension
(m × n) and outputs a segmentation mask M̂ ∈ {0, 1} of
dimension (m × n). The obtained segmentation mask, M̂ , is
element-wise multiplied with Icl to (i) crop out only the distal
phalange of the contactless fingerprints and (ii) eliminate the
remaining background to avoid detection of spurious minutiae
in the later representation extraction stage. The segmented
0
image Icl
is then resized to 480 × 480 by maintaining the
aspect ratio with appropriate padding for further processing.
Formally, this process of segmentation is given by Eqs. 1 and
2.

Fig. 5: Example segmentation successes (a) and failures (b) from images in
the ISPFDv2 dataset using our segmentation algorithm. Sources of failure
are presence of skin-like color tones in the background and varying skin
complexion due to varying illumination.

filtering) and leverages a Siamese CNN to extract features
for matching. Similarly, Malhotra et al. [19] utilize a deep
scattering network to extract multi-scale and multi-directional
feature representations.
In contrast to prior studies, our approach utilizes both a
texture representation and a minutiae representation. Given the
lower contrast and quality of contactless fingerprints (causing
missing or spurious minutiae) and the non-linear distortion
and scaling discrepancies between contact and contactless
fingerprints (negatively impacting minutiae graph matching
algorithms) a global texture representation is useful to improve
the contact-contactless matching accuracy. We demonstrate
this hypothesis empirically in the experimental results.
III. M ETHODS
Our matcher, C2CL, aims to improve contact to contactless
fingerprint recognition through a multi-stage preprocessing algorithm and matching algorithm comprised of both a minutiae
representation and a texture feature representation. The preprocessing is employed to minimize the domain-gap between the
contactless fingerprints residing in a domain Dcl and contactbased fingerprints residing in another domain Dc and consists
of segmentation, enhancement, ridge frequency scaling to 500
ppi, and deformation correction through a learned spatial
transformation network. After preprocessing, we extract deeptextural and minutiae representations (unordered, variable
length sets T = {(x1 , y1 , θ1 ), ..., (xn , yn , θn )}) for matching.
The final match scores are obtained via a score-level fusion
between the texture representation score and the minutiae
matching score.
A. Preprocessing
Here we discuss the details of each stage of our preprocessing algorithm. The subsequent outputs of our preprocessing
pipeline are illustrated in Figure 6.

M̂ = S(Icl )

0
Icl
= M̂

Icl

(1)

(2)

For training S(·), we manually marked ground truth segmentation masks M of the distal phalange of 496 contactless
fingerprints from the ISPFDv2 dataset3 . Initially, a total of 200
images were randomly selected to have varying resolutions of
either 5MP, 8MP, or 16MP and another 200 were selected
with varying backgrounds and illumination. An additional 96
images from the training set of ISPFDv2 were specifically selected for their greater perceived difficulty, particularly images
with skin tone backgrounds. The optimization function for
training S(·) is a pixel-wise binary cross-entropy loss between
M̂ and M (Eq. 3).

Lseg (Icl , Ic , M ) = −

X

[Mi,j log(Mˆi,j |Icl )

i,j

+ (1 − Mi,j ) log(1 − Mˆi,j |Icl )] (3)
2) Enhancement: Following segmentation, we apply a series of image enhancements E(·) to increase the contrast of
the ridge-valley structure of the contactless images, including:
(i) an adaptive histogram equalization4 to improve the ridgevalley contrast and (ii) pixel gray-level inversion to correct for
the inversion of ridges between contact-based and contactless
fingerprints. We also experimented with state-of-the-art super
resolution and de-blurring techniques, such as RDN [38], to
further improve the contactless image quality, but found only
minimal matching accuracy improvements at the expense of
significant additional computational cost.
3 We

used the open source Labelme segmentation tool found on GitHub [37].
used the Contrast Limited Adaptive Histogram Equalization (CLAHE)
function in open-cv: https://docs.opencv.org/3.4/d6/db6/classcv 1 1CLAHE.
html

4 We

7

(a)

(b)

(c)

(d)

(e)

Fig. 6: Illustration of our preprocessing pipeline including (a) segmentation, (b) enhancement, (c) scaling, and (d) warping. For reference, a corresponding
contact-based fingerprint is shown in (e).

3) Distortion Correction and Scaling: After segmenting
and enhancing the contactless fingerprints, the non-linear distortions that separate the domains of a contactless fingerprint
and a contact-based fingerprint must be removed. In particular,
both a perspective distortion (caused by the varying distance of
a finger from the camera) and a non-linear distortion (caused
when the elastic human skin flattens against a platen) must be
accounted for.
To correct for these discrepancies, we train a spatial
transformer network (STN [30]) T (·) that takes as input a
e
0
segmented, enhanced contactless image Icl
= E(Icl
) and
aligns the ridge structure to better match the corresponding
contact-based image domain Dc . The goal of the STN is twofold: (i) an affine transformation Ts (·) to normalize the ridge
frequency of the contactless images to match the 500 ppi
ridge spacing of the contact-based impressions and (ii) a TPS
deformation warping Td (·) of the contactless images to match
the deformation present in contact-based images due to the
elasticity of the human skin.
Both Ts (·) and Td (·) are comprised of a shared localization
network l(·, w) and individual differentiable grid-samplers.
e
e
Given an enhanced contactless fingerprint Icl
, l(Icl
, w) outputs
the scale (s), rotation (θ), and translation (tx , ty ) of an affine
transformation matrix As (Eq. 4) and a distortion field Θ
which is characterized by a grid of n × n pixel displacements
{(x1 , y1 )...(xn , yn )}. Subsequently, a scaled, warped image
w
Icl
is obtained via Equation 5.
To learn the weights w of the localization network such
that Ts (·) and Td (·) correctly scale the contactless fingerprints
to 500 ppi, and unroll them into a contact-based fingerprint,
we minimize the distance between DeepPrint representations
extracted from genuine pairs of scaled, warped contactless
w
fingerprints (Icl
) and contact-based fingerprints (Ic ). In particular, let f (·) be a frozen DeepPrint network pretrained
on contact-based fingerprints. Then, we can obtain a pair
of 192D DeepPrint identity representations Rcl and Rc via
w
Rcl = f (Icl
) and Rc = f (Ic ). Our loss can then be computed
from Equation 6. By using the DeepPrint identity features
extracted from contact-based fingerprint images to compute
the loss, we are able to utilize the contact-based impressions
as a ground truth of sorts. In particular, we are training our
localization network to output better scalings and warpings
such that the distortion and scale corrected contactless images

have DeepPrint representations closer to their corresponding
“ground truth” contact-based image.
We note that this approach has key differences to that which
was proposed in [25] where the distortion corrected contactless
image (scale was not learned in [25]) would be more directly
compared to the ground-truth contact-based fingerprint via a
w
cross-entropy loss between “binarized” versions of Icl
and
Ic . We found that directly comparing the contactless and
contact images via a cross entropy loss was quite difficult in
practice since the ground truth contact image and the corresponding contactless image will have different rotations and
translations separating them (even after scaling and distortion
correction - resulting in a high loss value even if the scaling
and distortion are correct). Furthermore, the contact-based
image itself varies based upon the pressure applied during
the acquisition, environmental conditions, sensor model, etc.,
meaning that directly using the contact-based image as ground
truth is unreliable. In contrast, since DeepPrint has been
trained to be invariant to pressure, environmental conditions,
and sensor model, our ground truth (DeepPrint representations
from contact-based images) will remain stable across different
contact-based impressions. In short, unlike [25], we learn both
distortion correction and scaling correction simultaneously,
and we use the DeepPrint identity loss to stabilize training
of T (·) and to enable predictions of warpings and scalings
which better improve matching accuracy.
"
#
s cos(θ) −s sin(θ) tx
As =
(4)
s sin(θ) s cos(θ) ty
w
e
e
Icl
= T (Icl
; As , Θ) = Td (Ts (Icl
, As ), Θ)

(5)

LST N = kRcl − Rc k22

(6)

B. Representation Extraction
After performing all of the aforementioned preprocessing steps, we enter the second major stage of our contactcontactless matcher, namely the representation extraction
stage. Our representation extraction algorithm extracts both
a textural representation (using a CNN) and a minutiae-set.
Scores are computed using both of these representations and

8

TABLE III: Number of contactless and contact fingerprint images used in training each component of C2CL.
Segmentation
S(·)

Deformation Correction & Scaling
T(·)

UWA Benchmark 3D Fingerprint Database [22]

0/0

0/0

1, 000/2, 000

ManTech Phase2, 2015 [29]

0/0

0/0

21, 352/28, 574

PolyU Contactless 2D to Contact-based 2D Images Database [21]

0/0

1, 920/1, 920

1, 920/1, 920

MSU Finger Photo and Slap Fingerprint Database [20]

0/0

2, 472/2, 472

2, 472/2, 472

IIT Bombay Touchless and Touch-based Fingerprint Database [18]

0/0

800/800

800/800

496/0

8, 400/1, 200

8, 400/1, 200

Dataset

ISPFDv2 [19]

0/0

0/0

0/0

496/0

13, 592/6, 392

35, 944/36, 966

ZJU Finger Photo and Touch-based Fingerprint Database
Total (# contactless / # contact)

DeepPrint
f (·)

then fused together using a sum score fusion for a final
similarity score.
1) Texture Representation: To extract our textural representation, we fine-tune the DeepPrint network proposed by
Engelsma et al. in [28] on a training partition of the publicly
available datasets which we aggregated (Table III). We note
that we do not include any data from our newly collected
ZJU dataset for fine-tuning as we want this dataset to remain
completely unseen for a more rigorous evaluation. Unlike
the deep networks used in [23], [19] for extraction of textural representations from contactless fingerprints, DeepPrint
is a deep-network which has been specifically designed for
fingerprint representation extraction via a built-in alignment
module and minutiae domain knowledge. Therefore, in this
work, we seek to adopt DeepPrint (originally utilized for
contact-contact fingerprint matching) for contactless-contact
fingerprint matching.
Formally, DeepPrint is a network f (·) with parameters
w that takes as input a fingerprint image I and outputs a
fixed-length fingerprint representation R (which encodes the
textural related features). During training, DeepPrint is guided
to encode features related to fingerprint minutiae via a multitask learning objective including: (i) a cross-entropy loss on
both the minutiae branch identity classification probability ŷ1
and texture branch identity classification probability ŷ2 (Eq.
7), (ii) minimize the intra-class variance of class y via a center
loss between the predicted minutiae feature vector R1 and its
mean feature vector R1y and the predicted texture feature vector
R2 and its mean feature vector R2y (where R1 concatenated
with R2 form the full representation R), and (iii) a mean
squared error loss on the predicted minutiae maps Ĥ output by
DeepPrint’s minutiae branch and ground truth minutiae maps
H (Eq. 9). These losses are combined to form the DeepPrint
identity loss, LID (Eq. 10), where λ1 = 1, λ2 = 0.00125,
λ3 = 0.095 are empirically set.

LID (I, y, H) = argmin
w

N
X
[λ1 L1 (I i , y i ) + λ2 L2 (I i , y)
i=1

+ λ3 L3 (I i , H i )] (10)
Due to the large differences in resolution, illumination,
and backgrounds observed between different datasets of contactless fingerprint images, generalization to images captured
on unseen cameras becomes critical. The problem of crosssensor generalization in fingerprint biometrics (e.g., optical
reader to capacitive reader), of which contact to contactless
matching is an extreme example, has been noted in the literature [39], [40], [41], [42], with many previous works aimed
at improving the interoperability [43], [44], [45]. Motivated
by the recent work employing adversarial learning to improve
cross-sensor generalization of fingerprint spoof detection [46],
we incorporate an adversarial loss to encourage robustness
of DeepPrint to differences between acquisition devices or
smartphone cameras. The adversarial loss LA is defined as the
cross-entropy on the output of an adversary network q(·, θA )
across C classes of sensors, where the adversarial ground
truth y 0 is assigned equal probabilities across these C classes
(Eq. 11). The adversarial loss LA and identity loss LID form
the overall loss function LD used to train DeepPrint (Eq.
12), where λ4 = 0.1 is empirically selected. The adversary
network, q(·, θA ), is a two layer fully connected network, with
weights θA , that predicts the probability of the class of input
device used to capture each image, i.e., minimizes the crossentropy of the predicted device and the ground truth device
label y (Eq. 13). Intuitively, if DeepPrint learns to fool the
adversary, it has learned to encode identifying features which
are independent of the acquisition device or camera.
LA (I, y 0 ) = −

C
X

yc0 log qA (yc |f (I; w); θA )

(11)

c=1

L1 (I, y) = − log(ŷ1j=y |I, w) − log(ŷ2j=y |I, w)]

(7)
0

LD (I, y, H, y ) = argmin
L2 (I, y) = kR1 − R̄1y k22 + kR2 − R̄2y k22

L3 (I, H) =

X
j,k,l

(Ĥj,k,l − Hj,k,l )2

(8)

w

N
X

[LID (I, y, H)

i=1

+ λ4 LA (I i , y 0i )] (12)
(9)

LC (I, yc ) = −yc log qA (yc |f (I; w); θA )]

(13)

9

In addition to the adversarial loss, we also increased the
DeepPrint representation dimensionality from the original
192D to 512D and added perspective distortion and scaling
augmentations during training. In an ablation study (Table VI),
we show how each of our DeepPrint modifications (finetuning, adversarial loss, perspective and scaling augmentations, and dimensionality change) improves the contactlesscontact fingerprint matching performance on our newly collected ZJU dataset.
2) Minutiae Representation: Finally, after extracting a textural representation with our modified DeepPrint network, we
extract a minutiae-based representation from our preprocessed
contactless fingerprints with the Verifinger 12.0 SDK.
C. Matching
Following feature extraction, from which we obtain texture
representations (Rtc , Rtcl ) and Verifinger minutiae represenc
cl
, Rm
) for a given pair of contact and contactless
tations (Rm
fingerprint images (Ic , Icl ), we compute a final match score as
a weighted fusion of the individual scores computed between
cl
c
). Concretely, let st denote the sim, Rm
(Rtc , Rtcl ) and (Rm
ilarity score between (Rtc , Rtcl ) and sm denote the similarity
cl
c
), then the final similarity score is
, Rm
score between (Rm
computed from a sum score fusion shown in Equation 14. For
our implementation, wt = wm = 0.5 was selected empirically.
s = wt st + wm sm

(14)

IV. E XPERIMENTS
In this section, we give details on various experimental
evaluations to determine the effectiveness of C2CL for contact
to contactless fingerprint matching. We employed various
publicly available datasets for the evaluation of our algorithms,
as well as a new database of contactless and corresponding contact-based fingerprints which was collected using our
mobile-app in coordination with Zhejiang University (ZJU).
A. Datasets
Table II gives a detailed description of the publicly available
datasets for contact to contactless matching used in this
study and Figure 3 shows some example images from these
datasets. For comparison with previous studies, we use the
same train/test split of the PolyU dataset that was used in
[23], which consists of 160 fingers for training with 12 impressions each and the remaining 160 fingers for testing with 6
impressions each. Similarily, we split the UWA Benchmark 3D
dataset into 500 training fingers and 1, 000 unique test fingers.
Furthermore, following the protocol of Malhotra et al. [19], we
split the ISPFDv2 dataset evenly into 50% train and 50% test
subjects. Finally, we captured and sequestered a new dataset of
contactless fingerprints and contact-based fingerprint images
in coordination with ZJU for a cross-database evaluation (e.g.
not seen during training) to demonstrate generalizability of
our algorithm. The cross-database evaluation is much more
stringent than existing approaches which only train/test on

different partitions of the same dataset. Indeed, the crossdatabase evaluation is a much better measure of how C2CL
would perform in the real world.
The new ZJU Finger Photo and Touch-based Fingerprint
Database contains a total of 206 subjects, with 12 contactless
images and 12 contact-based impressions per finger. The
thumb and index fingers of both hands were collected for
each subject, giving a total of 9, 888 contactless and 9, 888
contact-based images. The contactless images were captured
using three commodity smartphones: HuaWei P20, Samsung
s9+, and OnePlus 8, whereas the contact-based fingerprint
impressions were captured on a URU 4500 optical-based
scanner at 512 ppi. An Android fingerphoto capture app was
developed to improve the ease and efficiency of the data
collection. To initiate the capturing process, a user or operator
enters the transaction ID for the user and uses an on screen
viewing window to help guide and capture the fingerprint
image. Furthermore, a counter displayed on the screen keeps
track of subsequent captures to streamline the data collection
process.
B. Implementation Details
All the deep learning components (segmentation network,
deformation correction and scaling network, and DeepPrint)
are implemented using the Tensorflow deep learning framework. Each network is trained independently and information
regarding how many of the contactless and contact fingerprint
images from each of the datasets used in training each component of our algorithm is given in Table III.
1) Segmentation Network: A total of 496 contactless fingerprint images from the ISPFDv2 were manually labeled
with segmentation masks outlining the distal phalange were
used for training. Input images were resized to 256 × 256
during training to reduce the time to convergence, which
occurred around 100, 000 iterations. During inference, the
contactless fingerprint images are also resized to 256 × 256
and resulting segmentation masks are upsampled back to
the original resolution. Due to limited number of manually
marked images, we employed random rotations, translations,
and brightness augmentations to avoid over-fitting.
2) Deformation Correction and Scaling Network: The pretrained DeepPrint model in [28] was used to provide supervision of our spatial transformation network in line with Eq 6.
The motivation for using a network pretrained on contactbased fingerprints, rather than our new finetuned model on
contactless fingerprints, is that the goal of our transformation
network is to transform the contactless fingerprint images
to better resemble their contact-based counterparts. Thus, a
supervisory network trained on solely contact-based fingerprint
images is more suitable for this purpose. The architectural
details of our STN localization network are given in Table VII.
For our implementation, we set the number of sampling points
for the distortion grid to n = 4 × 4. Data augmentations
of random rotations, translations, brightness adjustments, and
perspective distortions were employed to avoid over-fitting.
3) DeepPrint: The DeepPrint network utilized the same
optimizer and hyper-parameters as originally reported in [28].

10

TABLE IV: Verification performance of the C2CL.
Dataset

EER (%)

Verifinger 12.0
TAR @ FAR=0.01%

EER (%)

DeepPrint
TAR @ FAR=0.01%

DeepPrint + Verifinger 12.0
EER (%)
TAR @ FAR=0.01%

Previous SOTA
EER (%)

PolyU

0.46

97.20

2.37

72.07

0.30

97.74

UWA

0.44

98.64

5.29

83.40

0.77

98.15

7.11 [23]

ISPFDv2

1.44

96.02

2.33

84.33

1.20

96.67

3.40‡ [19]

ZJU†

0.79

96.88

2.08

86.42

0.62

97.60

N/A

‡
†

[19] reports results on the ISPFDv2 dataset per individual capture condition; 3.40 is the average EER across these data splits.
Cross-database evaluation, i.e., not seen during training.

TABLE V: Ablation study of C2CL using only Verifinger 12.0 for matching∗ .
S = segmentation, E = enhancement Ts = scaling, Td = deformation
correction.

TABLE VII: Deformation Correction and Scaling Spatial Transformation
Network Architecture, T (·).
Layer

Dataset

PolyU

S
X
X
X
X

Modules
E
Ts
X
X
X

X
X

Td

EER

Overall (%)
TAR @ FAR = 0.01%

X

0.86
0.45
0.48
0.46

93.19
96.96
96.44
97.2
97.03
98.64

X
X

X

0.81
0.44

ISPFDv2

X
X
X
X

X
X
X

X

13.76
7.83
2.02
1.44

23.93
38.53
93.3
96.02

ZJU†

X
X
X
X

X
X
X

X

3.35
1.88
0.9
0.79

82.8
89.9
96.97
96.88

UWA‡

X
X

X
X

∗

Ablation results for DeepPrint are not shown since only a single
model was trained on the final E+S+Ts +Td images.
‡ We do not apply our STN here since these images are captured with
a 3D scanner and are already unrolled and at a resolution of 500 PPI.
† Cross-database evaluation, i.e., not seen during training.
TABLE VI: DeepPrint Ablation Study

Method
DeepPrint [28]
+ finetune
+ 512D
+ Augmentations
+ Adversarial Loss
∗

7.93 [23]

ZJU EER (%)
4.07
2.68
2.64
2.35
2.08

Each row adds on to the previous row.

A small validation set was partitioned from DeepPrint finetuning data outlined in Table III to stop the training (which
occurred at 73,000 steps). The added adversary was trained
with the RMSProp optimizer.
C. Evaluation Protocol
To evaluate the cross-matching performance of our algorithms, we conduct both verification (1:1 comparison) and
identification (1:N comparison) experiments on four datasets
of contactless and contact-based fingerprints. For the verification experiments, we report the Receiver Operating Character-

#Filters, Filter Size, Stride

Output Dim.

0, 0, 0

480 × 480 × 1

1. Convolution

32, 3 × 3, 2

240 × 240 × 32

2. Convolution

64, 3 × 3, 2

120 × 120 × 64

3. Convolution

128, 3 × 3, 2

60 × 60 × 128

4. Convolution

256, 3 × 3, 2

30, 30, 256

6 × 4, 2

46592

0. Input

5. Max Pool
7. Dense

1024

1024

8. Dense

2 × n0 + 4

2 × n0 + 4

The final dense layer contains output neurons for a 2 × n0 grid of n0 =
n×n pixel displacements for the deformation correction and 4 neurons for
the affine transformation matrix (s, θ, tx and ty .). In our implementation,
n = 4.

istic (ROC) curves at specific operating points and equal error
rates (EER). Note that we report the TAR @ FAR=0.01%,
which is a stricter threshold than is currently reported in
the literature, and which is also a threshold expected for
field deployment. For the search experiments, the Cumulative
Match Characteristics (CMC) curves and rank-one search
accuracy are given against an augmented large scale gallery
of 1.1 contact million fingerprints taken from an operational
forensics database [2]. This is a much larger gallery than has
previously been evaluated against in the literature and is again
more indicative of what C2CL would face in the real world.
The first three datasets were split into their respective train and
testing sets with no overlapping subjects and used to compare
C2CL with existing approaches, whereas the fourth dataset
(ZJU) is reserved for a cross-database evaluation. Finally, we
present ablation results on each significant component of our
proposed system.

D. Verification Experiments
The verification experiments are conducted in a manner consistent with previous approaches to facilitate a fair comparison.
In particular, (i) the PolyU testing dataset yields 5, 760 (160 ×
6×6) genuine scores and 915, 840 (160×159×6×6) imposter
scores, (ii) the UWA Benchmark 3D dataset yields 8, 000
(1, 000 × 4 × 2) genuine and 7, 992, 000 (1, 000 × 999 × 4 × 2)
imposter scores, (iii) the ISPFDv2 dataset (which is split into 7

11

(a)

(b)

(c)

(d)

Fig. 7: ROCs of the proposed contact-contactless matcher on (a) PolyU, (b) UWA, (c) ISPFDv2, and (d) ZJU cross-matching fingerprint datasets.

different capture variations)5 yields 68, 096 ((152 × 8 × 8) × 7)
genuine and 10, 282, 496 ((152 × 151 × 8 × 8) × 7) imposter
scores, and (iv) the ZJU dataset yields 118, 656 (824×12×12)
genuine and 97, 653, 888 (824×823×12×12) imposter scores.
Due to the very high number of possible imposter scores for
ZJU, we limit the number of imposter scores computed to only
include the first impression of each imposter fingerprint. This
process results in 678, 152 imposter scores out of the possible
97, 653, 888 scores. It is assumed for all experiments that the
contactless fingerprints and contact-based impressions are the
probe and enrollment images, respectively.
Table IV provides the Equal Error Rate (EER) and True
Acceptance Rate (TAR) at a False Acceptance Rate (FAR) of
0.01% of C2CL on the different datasets. Additionally, the full
5 The

7 scenarios consist of different background, illumination, and resolution
variations (e.g., white background & indoor lighting, white background &
outdoor lighting, natural background & indoor lighting, natural background
& outdoor lighting, 5MP resolution, 8MP resolution, and 16MP resolution.
For our evaluation, we combine each of these into a single dataset.

ROC plots are given in Figure 7 for each dataset. For comparison with previous methods, rather than implement the relevant
state-of-the-art approaches that have been proposed and risk
under representing those methods, we directly compare our
approach to the results reported in each of the respective
papers. In terms of EER, our method outperforms all the
previous approaches in the verification setting. Not only does
our individual performance of the minutiae and textural representations alone exceed that of the previous SOTA methods (in
particular, even if we remove Verifinger, we still beat state-ofthe-art in all cases), the fusion performance attains matching
accuracy (EER = 0.30% − 1.20%) comparable to contactcontact fingerprint matching [26]. Even in the most challenging cross-database evaluation (ZJU), C2CL attains competitive
performance with contact-contact matching - demonstrating
the generalizability of C2CL to unseen datasets. Note that we
report the TAR @ FAR=0.01% only for C2CL since most of
the prior approaches only report EER and none report TAR

12

TABLE VIII: Multi-finger fusion verification results of the proposed matcher
on the ZJU dataset.

Finger Type
Thumb
Index
LT + LI
RT + RI
RT + LT
RI + LI

EER (%)

TAR (%) @ FAR = 0.01%

0.95
0.48
0.00
0.00
0.00
0.00

95.89
98.31
99.77
99.74
99.80
99.89

Fig. 8: CMC for ZJU dataset

@ FAR =0.01%.
1) Ablation study: We present an ablation study (Table V)
to fully understand the contribution of the main components
of our algorithm; namely, segmentation, enhancement, 500
ppi frequency scaling, and TPS deformation correction. From
the ablation, we notice there is a substantial improvement in
both EER and TAR @ FAR=0.01% just from incorporating
proper enhancement of the contactless images. In most cases,
there is almost a 50% reduction in EER from including
both contrast enhancement and binary pixel inversion. For
brevity, not shown in the table is the individual contribution
of inverting the ridges of the contactless images aside from
contrast enhancement. For reference, the EER of DeepPrint on
ZJU warped images with only contrast enhancement is 2.49%.
This is in comparison to the EER of 2.08% on the warped
images with both contrast enhancement and pixel inversion.
Furthermore, we observe that for the smartphone captured
contactless fingerprints in the ISPFDv2 and ZJU datasets,
there is a dramatic performance jump when incorporating our
500 PPI scaling network. Finally, there is another noticeable
improvement when incorporating the deformation correction
branch of our STN, most notably for the ISPFDv2 dataset.
Since the ZJU dataset contains equal numbers of thumb and index fingers, where the majority of our training datasets contain
mostly non-thumb fingers, we observed that the deformation
correction is less beneficial on average for the ZJU dataset
compared to ISPFDv2. However, from Table VIII, we see
that the EER of just index fingers of ZJU is noticably lower
than the EER on thumbs. This is a limitation of the available
training data, which could be alleviated in the future with more
training examples of thumbs.
2) Multi-finger fusion verification: The final set of verification experiments is to investigate the effects of finger position
and multiple finger fusion in the verification accuracy for the
ZJU dataset. Table VIII, shows the individual performance
per finger position and the fusion of multiple fingers; namely,
thumb only, index only, fusion of right thumb and right index,
fusion of left thumb and left index, and four finger fusion.
The motivation for considering fusion of the thumb and index
on each hand is that from a usability standpoint, a user may
be able to use their dominant hand when capturing their own
fingerprints. Notably, when fusing multiple fingers (e.g., right
index and left index), we obtain nearly perfect verification
accuracy.

TABLE IX: Search performance of the proposed matcher on the ZJU dataset
with a gallery of 1.1 million.

Method

Rank 1

Rank 10

Rank 100

Rank 500

DeepPrint
Verifinger
12.0
DeepPrint
+ Verifinger
12.0

83.56%
95.25%

93.06%
96.47%

95.86%
96.95%

97.08%
97.20%

95.49%

96.10%

96.95%

97.08%

DeepPrint + Verifinger 12.0 refers to indexing the top-500 candidates with DeepPrint and then re-sorting those 500 candidates
using a fusion of the Verifinger and DeepPrint score.

E. Search Experiments
For the identification (or search) experiments, we utilize the
first impressions of both the contact-based fingerprints and the
contactless fingerphotos of the ZJU dataset. The contact-based
fingerprints are placed in the gallery which is augmented with
1.1 million fingerprint images from an operational forensic
database [2]. The contactless fingerprint images serve as the
probes. We note that our 1.1 million augmented gallery is
significantly larger than any of the existing galleries used to
evaluate contactless-contact fingerprint search, and is more
indicative of the real world use-case of cross fingerprint
matching (e.g. in a National ID system like Aadhaar where a
large gallery of contact-based fingerprints is already enrolled
and used for de-duplication).
We evaluate 3 different search algorithms on the ZJU
augmented gallery: (i) Verifinger 1:N search, (ii) search via
our DeepPrint texture matcher (scores st from Eq. 14 are
computed between a given preprocessed, contactless probe,
and all 1.1 million contact-based fingerprints in the gallery),
and (iii) a two-stage search algorithm [28] where the DeepPrint
texture scores are first used to retrieve the top-500 candidates,
followed by a reordering using the 1:1 minutiae matching
scores (sm from Eq. 14) from Verifinger. The advantage of
the two-stage search scheme is that it balances both speed
and accuracy by utilizing the matching speed of DeepPrint
to locate the first list of 500 candidates and the accuracy of
Verifinger to further refine this list.
From Figure 8 and Table IX, we can observe that Verifinger
outperforms DeepPrint stand-alone. However, we note from
Table X that the Verifinger search time against 1.1 million

13

TABLE X: Search time against 1.1 million background gallery.

Method

Search Time (seconds)
0.4
60.1
10.5

DeepPrint
Verifinger 12.0
DeepPrint + Verifinger 12.0

TABLE XI: Intersection Over Union (IOU) of the proposed segmentation
network S(·).

Method

IOU

Baseline [19]
Proposed

0.747
0.899
(a)

is quite slow in comparison to DeepPrint. This motivates
combining both approaches into the aforementioned two-stage
search algorithm which actually outperforms Verifinger at
Rank-1, and reduces the search time by 50 seconds. In short,
our two stage search algorithm obtains high levels of search
accuracy on a large-scale gallery at a significant search time
savings.
F. Segmentation Evaluation
A successful segmentation algorithm for contactless fingerprint images must not only reliably detect the distal phalange
of the contactless fingerprint, but also be robust to varying
illumination, background, and resolution that is expected to occur in highly unconstrained capture environments. The method
by Malhotra et al. [19] performed well on the ISDFPDv2
dataset using certain hyperparameters that were fit to this
particular dataset; however, the authors did not evaluate it
on unseen datasets. In contrast, our algorithm requires no
hyperparameter tuning and still performs well across a variety
of different evaluation datasets, both seen and unseen. Table XI
gives a comparison on the unseen ZJU dataset between our
method and our implementation of the baseline approach of
Malhotra et al., which was trained on the ISPFDv2 dataset.
For this evaluation, we manually marked the first contactless
fingerprint image of each unique finger in the ZJU dataset
with ground truth segmentation masks of the distal phalange,
and then computed the Intersection Over Union (IOU) metric
between the predicted segmentation masks of our algorithm
and our implementation of the benchmark algorithm in [19].
Our method does not require any hyper-parameter tuning and
still achieves higher IOU compared to [19].
A qualitative analysis of our segmentation network (see Figure 5) shows our algorithm is robust to varying illumination,
background, and resolution and generalizes across multiple
datasets of contactless fingerprints. However, as seen in Figure 5 (b), the network may still fail in extremely challenging
background and illumination settings. An additional consideration, which is of importance for real-time deployment,
is the processing speed of the segmentation network. Our
segmentation algorithm is extremely fast compared to existing
methods - requiring just 12.6 ms to segment a (900 × 1200)
resolution image. In contrast, our parallel implemenation of the
baseline approach of Malhotra et al. requires 3s per image.

(b)
Fig. 9: Example failure cases from the ISPFDv2 dataset: (a) falsely rejected
image pairs and (b) falsely accepted image pairs.

V. D ISCUSSION
Despite the low error rates achieved across each dataset,
there are many factors that complicate the cross-matching
performance and lead to both type I (false rejects) and type
II (false accepts) errors. Many of the type I and type II
errors are attributed to a failure to correctly segment and scale
only the distal phalange of the input contactless fingerprint.
Incorrect segmentation can lead the large amounts of the image
containing background rather than the relevant fingerprint
region, as is the case in the middle column of Figure 9.
Other errors can be attributed to the inherent low-contrast
of the contactless fingerprints, despite any effort of contrast
or resolution enhancement (illustrated in the first column of
Figure 9). The only way to mitigate these types of failures is
to include a quality assurance algorithm at the point of capture
of the contactless fingerprint images. Lastly, minimal overlap
in the fingerprint ridge structure between genuine probe and
gallery fingerprint images is the cause of many false rejections,
whereas very similar ridge structure between imposter fingerprint pairs leads to a number of false accepts. This challenge is
present in contact-contact matching; however, is exaggerated
in C2CL because of the unconstrained pose variance of the
finger in 3D space.
The potential for greater variance in the capture conditions

14

(a)

(b)
Fig. 10: Comparison of ridge overlap (a) with and (b) without the unwarping
module. The left image in each row is the contactless fingerprint image. Use
of unwarping module results in better ridge alignment between contactless
and contact-based images. The alignment is shown for the ridges in the blue
window.

when capturing contactless fingerprint images necessitates
more robust preprocessing to reliably match contactless fingerprints. Thus, performance will likely be markedly lower in
unconstrained scenarios compared to highly controlled capture
environments that employ dedicated hardware for the image
acquisition, such as the PolyU and UWA datasets. However, C2CL has pushed the SOTA forward both in matching
more unconstrained fingerphotos and the more constrained
dedicated-device captured contactless fingerprints.
As highlighted in the ablation study of Table IV, most
of the improvement in interoperability between contactless
and contact-based fingerprints is due to appropriate 500 ppi
scaling of the contactless prints; however, incorporating a
deformation correction module is also shown, with statistical
significance6 , to further improve the compatibility. Figure 10
aims to highlight this fact through an overlay of the fingerprint
ridge structure of one contactless fingerprint to it’s corresponding contact-based impression before and after applying the
deformation correction. The improved alignment indubitably
leads to better minutiae-based and texture-based matching, as
verified by our experiments.
VI. C OMPUTATIONAL E FFICIENCY
Our system architecture consists of a variety of deep networks (segmentation network, enhancement, deformation correction and scaling spatial transformer, CNN feature extractor
6 The

Mann-Whitney rank test [47] was used to compute the statistical
significance between the ROC curves of E+S and E+S+T in Figure 7. For all
four datasets, the p value is smaller than 0.05, indicating that the difference
is statistically significant to reject the hypothesis that the two curves are
similar with a confidence of 95%.

and minutiae matcher). The deep network components of our
algorithm are capable of very fast inference per input image;
however, the system as a whole consumes a large amount
of memory (400 MB). To fit into a resource constrained
environment, such as a mobile phone, further optimization
to the system architecture can easily be implemented with
very little, if any, performance drop. First, the intermediate
step of generating a scaled image prior to the deformation
correction is not required for deployment and was just included
for the ablation study. Instead, we can remove the affine
transformation layer of our STN and directly scale and warp
the input images in one step. Furthermore, rather than rely on
a COTS system for minutiae extraction and matching, we can
directly use the minutiae sets output by DeepPrint and a computationally efficient minutiae matcher to obtain the minutiae
match scores, such as MSU’s Latent AFIS Matcher [48]. With
these optimizations to improve the efficiency and reduce the
size of the networks, the inference time on a mobile phone
(Google Pixel 2) is ≈ 2 seconds.
VII. C ONCLUSION AND F UTURE W ORK
W have presented the first end-to-end system for matching
contactless fingerprints (i.e., finger photos) to contact-based
fingerprint impressions that achieves error rates comparable to
contact-contact fingerprint matching. In particular, our contact
to contactless matcher achieves less than 1% EER across multiple datasets employing a variety of contactless and contactbased acquisition devices with varying background, illumination, and resolution settings. Critical to the success of our
system is our extensive preprocessing pipeline consisting of
segmentation, contrast enhancement, 500 ppi scale normalization, deformation correction, and our adaptation of DeepPrint
for contactless-contact matching. Our cross-database evaluations and large-scale search experiments are more rigorous
evaluations than what is reported in the open literature, and
it enables us to confidently demonstrate a contactless-contact
fingerprint matcher with similar levels of accuracy to state-ofthe-art contact-contact fingerprint matching accuracy.
ACKNOWLEDGMENT
This material is based upon work supported by the Center for Identification Technology Research and the National
Science Foundation under Grant No. 1841517. The authors
would like to thank Debayan Deb for his help in developing
the mobile phone contactless fingerprint capture application,
Dr. Eryun Liu’s research group at Zhejiang University for
overseeing the data collection effort for this project, and the
various research groups who have shared their datasets that
were used in this study.
R EFERENCES
[1] S. Pankanti, S. Prabhakar, and A. K. Jain, “On the individuality
of fingerprints,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 24, no. 8, pp. 1010–1025, 2002.
[2] S. Yoon and A. K. Jain, “Longitudinal study of fingerprint recognition,”
Proceedings of the National Academy of Sciences, vol. 112, no. 28, pp.
8555–8560, 2015.

15

[3] A. M. Bazen and S. H. Gerez, “Fingerprint matching by thin-plate spline
modelling of elastic deformations,” Pattern Recognition, vol. 36, no. 8,
pp. 1859–1867, 2003.
[4] R. Cappelli, D. Maio, and D. Maltoni, “Modelling plastic distortion in
fingerprint images,” in International Conference on Advances in Pattern
Recognition. Springer, 2001, pp. 371–378.
[5] A. Ross, S. Dass, and A. Jain, “A deformable model for fingerprint
matching,” Pattern Recognition, vol. 38, no. 1, pp. 95–103, 2005.
[6] G. Parziale, “Touchless fingerprinting technology,” in Advances in
Biometrics. Springer, 2008, pp. 25–48.
[7] K. Okereafor, I. Ekong, I. O. Markson, and K. Enwere, “Fingerprint
biometric system hygiene and the risk of covid-19 transmission,” JMIR
Biomedical Engineering, vol. 5, no. 1, p. e19623, 2020.
[8] J. Priesnitz, R. Huesmann, C. Rathgeb, N. Buchmann, and C. Busch,
“Mobile touchless fingerprint recognition: Implementation, performance
and usability aspects,” arXiv preprint arXiv:2103.03038, 2021.
[9] Y. Song, C. Lee, and J. Kim, “A new scheme for touchless fingerprint
recognition system,” in Proceedings of 2004 International Symposium
on Intelligent Signal Processing and Communication Systems, (ISPACS).
IEEE, 2004, pp. 524–527.
[10] C. Lee, S. Lee, and J. Kim, “A study of touchless fingerprint recognition system,” in Joint IAPR International Workshops on Statistical
Techniques in Pattern Recognition (SPR) and Structural and Syntactic
Pattern Recognition (SSPR). Springer, 2006, pp. 358–365.
[11] G. Parziale and Y. Chen, “Advanced technologies for touchless fingerprint recognition,” in Handbook of Remote Biometrics. Springer, 2009,
pp. 83–109.
[12] B. Y. Hiew, A. B. Teoh, and Y.-H. Pang, “Touch-less fingerprint
recognition system,” in 2007 IEEE Workshop on Automatic Identification
Advanced Technologies. IEEE, 2007, pp. 24–29.
[13] A. Kumar and Y. Zhou, “Contactless fingerprint identification using level
zero features,” in CVPR 2011 Workshops. IEEE, 2011, pp. 114–119.
[14] X. Yin, Y. Zhu, and J. Hu, “Contactless fingerprint recognition based
on global minutia topology and loose genetic algorithm,” IEEE Transactions on Information Forensics and Security, vol. 15, pp. 28–41, 2019.
[15] A. Sankaran, A. Malhotra, A. Mittal, M. Vatsa, and R. Singh, “On
smartphone camera based fingerphoto authentication,” in 2015 IEEE
7th International Conference on Biometrics Theory, Applications and
Systems (BTAS). IEEE, 2015, pp. 1–7.
[16] A. Malhotra, A. Sankaran, A. Mittal, M. Vatsa, and R. Singh, “Fingerphoto authentication using smartphone camera captured under varying
environmental conditions,” in Human Recognition in Unconstrained
Environments. Elsevier, 2017, pp. 119–144.
[17] C. Stein, C. Nickel, and C. Busch, “Fingerphoto recognition with
smartphone cameras,” in International Conference of Biometrics Special
Interest Group (BIOSIG). IEEE, 2012, pp. 1–12.
[18] P. Birajadar, M. Haria, P. Kulkarni, S. Gupta, P. Joshi, B. Singh, and
V. Gadre, “Towards smartphone-based touchless fingerprint recognition,”
Sādhanā, vol. 44, no. 7, pp. 1–15, 2019.
[19] A. Malhotra, A. Sankaran, M. Vatsa, and R. Singh, “On matching
finger-selfies using deep scattering networks,” IEEE Transactions on
Biometrics, Behavior, and Identity Science, vol. 2, no. 4, pp. 350–362,
2020.
[20] D. Deb, T. Chugh, J. Engelsma, K. Cao, N. Nain, J. Kendall, and A. K.
Jain, “Matching fingerphotos to slap fingerprint images,” arXiv preprint
arXiv:1804.08122, 2018.
[21] C. Lin and A. Kumar, “Matching contactless and contact-based conventional fingerprint images for biometrics identification,” IEEE Transactions on Image Processing, vol. 27, no. 4, pp. 2008–2021, 2018.
[22] W. Zhou, J. Hu, I. Petersen, S. Wang, and M. Bennamoun, “A benchmark
3d fingerprint database,” in 2014 11th International Conference on Fuzzy
Systems and Knowledge Discovery (FSKD), 2014, pp. 935–940.
[23] C. Lin and A. Kumar, “A cnn-based framework for comparison of contactless to contact-based fingerprints,” IEEE Transactions on Information
Forensics and Security, vol. 14, no. 3, pp. 662–676, 2019.
[24] P. Wild, F. Daubner, H. Penz, and G. F. Domı́nguez, “Comparative
test of smartphone finger photo vs. touch-based cross-sensor fingerprint
recognition,” in 2019 7th International Workshop on Biometrics and
Forensics (IWBF). IEEE, 2019, pp. 1–6.
[25] A. Dabouei, S. Soleymani, J. Dawson, and N. M. Nasrabadi, “Deep
contactless fingerprint unwarping,” in 2019 International Conference on
Biometrics (ICB), 2019, pp. 1–8.
[26] B. Dorizzi, R. Cappelli, M. Ferrara, D. Maio, D. Maltoni, N. Houmani,
S. Garcia-Salicetti, and A. Mayoue, “Fingerprint and on-line signature
verification competitions at ICB 2009,” in International Conference on
Biometrics. Springer, 2009, pp. 725–732.

[27] C. I. Watson, G. P. Fiumara, E. Tabassi, S. L. Cheng, P. A. Flanagan,
and W. J. Salamon, “Fingerprint vendor technology evaluation,” 2015.
[28] J. J. Engelsma, K. Cao, and A. K. Jain, “Learning a fixed-length
fingerprint representation,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, 2019.
[29] L. Ericson and S. Shine, “Evalutaion of contactless versus contact
fingerprint data phase 2 (version 1.1),” DOJ Office Justice Programs,
I. ManTech Adv. Syst. Int., Fairmont, WV, Tech. Rep. 249552, 2015.
[30] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu, “Spatial transformer networks,” arXiv preprint arXiv:1506.02025, 2015.
[31] A. Dabouei, H. Kazemi, S. M. Iranmanesh, J. Dawson, and N. M.
Nasrabadi, “Fingerprint distortion rectification using deep convolutional
neural networks,” in 2018 International Conference on Biometrics (ICB).
IEEE, 2018, pp. 1–8.
[32] A. Ross, S. C. Dass, and A. K. Jain, “Fingerprint warping using ridge
curve correspondences,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 28, no. 1, pp. 19–30, 2005.
[33] A. W. Senior and R. M. Bolle, “Improved fingerprint matching by
distortion removal,” IEICE Transactions on Information and Systems,
vol. 84, no. 7, pp. 825–832, 2001.
[34] X. Si, J. Feng, J. Zhou, and Y. Luo, “Detection and rectification
of distorted fingerprints,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 37, no. 3, pp. 555–568, 2015.
[35] C. Lin and A. Kumar, “Contactless and partial 3d fingerprint recognition
using multi-view deep representation,” Pattern Recognition, vol. 83, pp.
314–327, 2018.
[36] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
for biomedical image segmentation,” in International Conference on
Medical image computing and computer-assisted intervention. Springer,
2015, pp. 234–241.
[37] K. Wada, “labelme: Image Polygonal Annotation with Python,” https:
//github.com/wkentaro/labelme, 2016.
[38] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu, “Residual dense
network for image super-resolution,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 2472–
2481.
[39] L. Lugini, E. Marasco, B. Cukic, and I. Gashi, “Interoperability in
fingerprint recognition: A large-scale empirical study,” in 43rd Annual
IEEE/IFIP Conference on Dependable Systems and Networks Workshop
(DSN-W), 2013, pp. 1–6.
[40] A. Ross and A. Jain, “Biometric sensor interoperability: A case study
in fingerprints,” in Biometric Authentication, D. Maltoni and A. K. Jain,
Eds. Springer, 2004, pp. 134–145.
[41] F. Alonso-Fernandez, R. N. Veldhuis, A. M. Bazen, J. Fiérrez-Aguilar,
and J. Ortega-Garcia, “Sensor interoperability and fusion in fingerprint
verification: A case study using minutiae-and ridge-based matchers,” in
International Conference on Control, Automation, Robotics and Vision.
IEEE, 2006, pp. 1–6.
[42] H. AlShehri, M. Hussain, H. AboAlSamh, and M. AlZuair, “A largescale study of fingerprint matching systems for sensor interoperability
problem,” Sensors, vol. 18, no. 4, p. 1008, 2018.
[43] E. Marasco, L. Lugini, B. Cukic, and T. Bourlai, “Minimizing the impact
of low interoperability between optical fingerprints sensors,” in IEEE
Sixth International Conference on Biometrics: Theory, Applications and
Systems (BTAS). IEEE, 2013, pp. 1–8.
[44] J. Jang, S. J. Elliott, and H. Kim, “On improving interoperability of
fingerprint recognition using resolution compensation based on sensor
evaluation,” in Advances in Biometrics, S.-W. Lee and S. Z. Li, Eds.
Berlin, Heidelberg: Springer Berlin Heidelberg, 2007, pp. 455–463.
[45] A. Ross and R. Nadgir, “A calibration model for fingerprint sensor
interoperability,” in Biometric Technology for Human Identification III,
vol. 6202. International Society for Optics and Photonics, 2006, p.
62020B.
[46] S. A. Grosz, T. Chugh, and A. K. Jain, “Fingerprint presentation attack
detection: A sensor and material agnostic approach,” in 2020 IEEE
International Joint Conference on Biometrics (IJCB). IEEE, 2020,
pp. 1–10.
[47] E. R. DeLong, D. M. DeLong, and D. L. Clarke-Pearson, “Comparing
the areas under two or more correlated receiver operating characteristic
curves: a nonparametric approach,” Biometrics, pp. 837–845, 1988.
[48] K. Cao, D.-L. Nguyen, C. Tymoszek, and A. K. Jain, “End-to-end latent
fingerprint search,” IEEE Transactions on Information Forensics and
Security, vol. 15, pp. 880–894, 2019.

16

Steven A. Grosz received his B.S. degree with highest honors in Electrical Engineering from Michigan
State University, East Lansing, Michigan, in 2019.
He is currently a doctoral student in the Department
of Computer Science and Engineering at Michigan
State University. His primary research interests are
in the areas of machine learning and computer vision
with applications in biometrics.

Joshua J. Engelsma graduated magna cum laude
with a B.S. degree in computer science from Grand
Valley State University, Allendale, Michigan, in
2016. He is currently working towards a PhD degree in the Department of Computer Science and
Engineering at Michigan State University, East Lansing, Michigan. His research interests include pattern
recognition, computer vision, and image processing
with applications in biometrics. He won best paper
award at ICB 2019 and is a student member of IEEE.

Anil K. Jain is a University distinguished professor
in the Department of Computer Science and Engineering at Michigan State University. He served
as the editor-in-chief of the IEEE Transactions on
Pattern Analysis and Machine Intelligence and was a
member of the United States Defense Science Board.
He has received Fulbright, Guggenheim, Alexander
von Humboldt, and IAPR King Sun Fu awards. He is
a member of the National Academy of Engineering
and foreign fellow of the Indian National Academy
of Engineering and Chinese Academy of Sciences.

