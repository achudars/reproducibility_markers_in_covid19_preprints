R EAL - TIME TRACKING OF COVID-19 AND CORONAVIRUS

arXiv:2102.07640v1 [cs.IR] 9 Feb 2021

RESEARCH UPDATES THROUGH TEXT MINING

Yutong Jin, Jie Li, Xinyu Wang, Peiyao Li, Jinjiang Guo, Junfeng Wu, Dawei Leng,
AIDD Group
Global Health Drug Discovery Institute, Beijing, China
yutong.jin@ghddi.org and lurong.pan@ghddi.org

Lurong Pan*

A BSTRACT
The novel coronavirus (SARS-CoV-2) which causes COVID-19 is an ongoing pandemic. There are
ongoing studies with up to hundreds of publications uploaded to databases daily. We are exploring the
use-case of artificial intelligence and natural language processing in order to efficiently sort through
these publications. We demonstrate that clinical trial information, preclinical studies, and a general
topic model can be used as text mining data intelligence tools for scientists all over the world to use
as a resource for their own research. To evaluate our method, several metrics are used to measure the
information extraction and clustering results. In addition, we demonstrate that our workflow not only
have a use-case for COVID-19, but for other disease areas as well. Overall, our system aims to allow
scientists to more efficiently research coronavirus. Our automatically updating modules are available
on our information portal at https://ghddi-ailab.github.io/Targeting2019-nCoV/ for public viewing.
Keywords Natural Language Processing · Text Mining · Clustering · COVID-19

1

Introduction

The COVID-19 pandemic is an ongoing pandemic caused by the novel coronavirus (SARS-CoV-2)[1]. The symptoms
are highly variable, and the virus, which spreads through the air and contaminated surfaces, is highly contagious. As
of January 2021, there has yet to be a small molecule drug that is specific and effective for COVID-19. During the
pandemic, countries around the world made efforts to overcome the difficulties, further reflecting the importance of
unity and cooperation and resource sharing. We are continuously exploring the value chain provided by artificial
intelligence (AI) in the drug discovery process. In terms of pathological mechanisms, AI natural language processing
(NLP) technology can replace manual curation of data and efficiently collect and sort data from global databases.
Data mining is a process in which algorithms convert raw data into useful structured data. This technique is then
integrated with NLP algorithms to analyze and organize the collected information from areas such as a disease field
either through rule-based text mining or a model-based tool. In our effort, we have launched the GHDDI Targeting
COVID-19 platform [2]. Since its launch on January 29, the platform has been continuously updated and maintained
with new functions and modules continuously added. Several of these functional modules include NLP data mining
module for SARS-CoV-2 small molecule drug in vitro experimental data that updates new experimental information
daily, an automated NLP COVID-19 clinical trial module allowing up-to-date summarization of clinical trial data, and
an NLP-based scientific literature recommendation module. Overall, we present the details behind these three modules
to support real-time scientific intelligence of COVID-19.

2

Methods

In this section, we briefly introduce the three modules and how they were built using different databases. All of the
NLP systems were built using a standard Python 3.6 environment from Anaconda and associated packages mentioned
below. Our system’s backend and database is hosted on a Ubuntu 18.04 server using the same environment.

Real-time tracking of COVID-19 and coronavirus research updates through text mining

2.1

Data Aggregation and Preprocessing

The data was aggregated using a variety of sources. Through automated download scripts and given Application
Programming Interfaces (API), abstract data was downloaded from: PubMed, preprint sources, and dimensions.ai [3]
using a string query "SARS-Cov-2 OR COVID-19 OR novel coronavirus". These data sources were compiled together
and the string data was cleaned using simple Python scripts such as lower-casing all words and removing noise data
such as spaces or tabs. Duplicate data was then removed through a sequence of steps by dropping DOI, title, and
abstract strings, respectively. This aggregated dataset will be used for subsequent NLP workflows and models.
2.2

Data Dictionaries

There were several dictionaries that were compiled and utilized for information filtering and information extraction.
First, a dictionary of all drug names was compiled using DrugBank drug names and aliases [4], FDA drug list [5], and
ChEMBL [6]. All of these drug names were compiled into one list, and string length was computed to filter out outliers.
Overall, the final list consisted of unigram, bigrams, and trigrams; it also included drug names with a string length
between 5 and 75 characters.
The second dictionary involved a filter list to clean out unwanted items from the drug dictionary. In the DrugBank
database, several entries that do not necessarily represent drug names can be found such as large biologic molecules
and antigens. Likewise, in a similar Kaggle competition [7], a list of filtered items were compiled, and this list was
aggregated and used to filter out unwanted terms in the final drug dictionary used in subsequent workflows.
2.3

Preclinical Data Extraction

This step uses the aggregated dataset mentioned in the previous section. The dataset is then sent through a filter of
keywords [EC50, IC50, CC50] to get a subset of only abstracts mentioning these keywords. Then, each abstract from
the subset is matched with a corpus of known drug names and sentences are extracted. If sentences contain both a
key word and a drug name, the sentence will be searched for a numerical value or descriptive phrase describing the
relationship in that sentence. This is done using either regex (Rule 1) or a Spacy noun chunk model (Rule 2). Using
regex, the system extracts the numerical value closest in word distance to the keyword or using a rule-based logic.
Similarily, the Spacy noun chunk model extracts the noun chunk describing the keyword. The spacy model is an
open-source English language model. Several features include POS tagging, noun chunk extraction, and grammar NER.
The noun chunks are a descriptive phase that has significant relations to a keyword. This extracted values list is then
extracted and mapped onto the drug name with a direct correlation. Additionally, if sentences mentioning the drug and
keyword are different, then the system tries to extract a value similar to above but prints the drug name and experimental
assay value relationship as an indirect correlation. These results are all tabulated and updated to the website.
• Rule 1: Using a regex query, all numbers are extracted. The closest numerical token to the experimental
keyword is mapped to the closest drug name.
• Rule 2: Using a Spacy model, all noun chunks are extracted from the sentence. The noun chunk closest in
distance to that of the experimental keyword is identified and mapped.
Using these two rules, all data following this logic can be extracted and mapped. Because this text mining procedure is
done using a list of known drugs, several metrics are used to validate this workflow. We have evaluated the text mining
results based on a similar text mining study[8]. In that study, 25 unique text items (notes) were randomly sampled and
manually reviewed as a gold standard. Overall, we evaluated Precision, Recall, and F-measure in the preclinical data
mining results by randomly sampling 25 papers by DOI.
P = Ncorrect /Ntotal

(1)

R = Ncorrect /Ntotal possible

(2)

The above equations are derived from calculating precision and recall in an information extraction context[9].
2.4

NLP Topic Model Recommendation Engine

The aggregated dataset built in Section 2.1 is utilized in this step. Figure 1 shows the monthly amount of articles
uploaded onto our database. As a result of this large number, it was important to split the articles into different categories
2

Real-time tracking of COVID-19 and coronavirus research updates through text mining

Figure 1: Workflow of real-time system to update modules
and recommend them by topic. After preprocessing, the data is then checked and tagged for up to trigrams. Additionally,
data lemmatization is used and stop words are removed; a bag-of-words is subsequently created for each abstract. A
Latent Dirichlet Allocation (LDA) algorithm is used to build the topic model. This is an unsupervised machine learning
model that measures the distribution of words and attempts to cluster this distribution into a specified number of hidden
distributions. The word distributions per abstract determine which topic or hidden distribution that abstract best fits into.
The bag-of-words object is then sent to the Gensim LDA API [10] for model training, and subsequent Python pickle
objects and metadata were used for daily updates.
A gridsearch optimization of this topic model was performed by maxmizing the coherence score, and the best scoring
model was used for the final output where each topic was hand-labeled. In this dataset, the best scoring model was a
30-topic model which was used for the final output. After this model was trained, inferencing was performed on the
original dataset, and then each abstract was assigned a topic. This result was recorded, and a data-driven filter was used
to filter out topics that did not meet the amount papers required to form a topic. Then for each topic, the top papers are
ranked and sorted by the model’s output weight, being the gamma value in the Gensim model, and the top 10 papers
are output into a final tabulated format. This can then be done for new data which can be automatically updated in the
future for this module.
2.5

Clinical Trials Text Mining

In the clinical trials module, the open-access Figshare data shared by dimensions.ai was used [3]. As of January 2021,
there were 7000 clinical trials records around the world. Clinical trials contain different phase human experiments
relating to drugs or biologics, including vaccines. The data is first preprocessed similar to the methods mentioned
above. The data is then tagged for unigrams, bigrams, and trigrams similar to the NLP topic model. Afterwards, the
information extraction process clusters the clinical trials into one of these three types.
• Using the known drugs dictionary mentioned in the preclinical information extraction section, all small
molecule drug names are extracted from the clinical trial descriptive phrases. This list is filtered and extracted
samples containing animal, food, and other non-small molecule drug words are removed.
• Given the keyword “vaccine” and all its derivatives, the database was searched for these keywords and a list of
vaccine clinical trials was compiled and output. This list is filtered and trials containing words in the blacklist
are removed.
• Given several keywords relating to biological products such as plasma, antibody, stem cell, and all of their
derivative words, a list of biological products was compiled and output. This list is filtered and trials containing
words in the blacklist are removed.
Using these three rules, all information pertaining to drugs, biologicals, and vaccines were extracted from the tabulated
data. The data was visualized in our information portal [2] together with word clouds for biological drug and vaccine
trials as a validation.
2.6

Real-time Updates

All of these modules are supported by real-time daily updates provided by a server setup to update automatically. This
system, as shown in Figure 1, provides daily incremental updates of clinical trial data and research articles using open
3

Real-time tracking of COVID-19 and coronavirus research updates through text mining

Figure 2: Count of papers published every month starting from January 2020.

APIs provided by PubMed, Figshare, and other sources. This data is then stored in our database which is updated daily.
After updating the database, we use metadata to track changes in the clinical trial and research article databases. The
clinical trial script is run automatically every day and completes the data processing if there is a new update. Likewise,
new articles recently updated to our database is preprocessed and then run with the preclinical NLP processing workflow,
and updates are appended to a master list that is updated onto our portal. Finally, the entire abstracts database is
preprocessed then run with the topic model; afterwards, the top 10 titles are uploaded per topic to the recommendation
page. This model is retrained and updated monthly as new data becomes available.

3

Results

The results of our modules are presented below. Full results can be found on the Targeting COVID-19 GitHub portal
[2]. This section gives an in-depth description of the results that were published to the website.
Figure 2 visualizes the number of papers uploaded to the databases by month. Due to the number of papers published
exponentially increasing in March 2020, it became impossible to track all experimental and clinical results published
to a journal or uploaded onto a preprint service. Therefore, we used this data to automatically data mine and extract
valuable information that may be of use to scientists of different fields all around the world.
3.1

Small Molecule Drug Text Mining

For small molecule drugs, the compiled drug dictionary is used and the matches are tabulated in the following tables.
Table 1 shows the top 10 most common drugs found through information extraction of clinical trial records where there
are currently over 1100 clinical trials for small molecule drugs, while Table 2 shows several of the best experimental
results of small molecule drugs extracted from preclinical studies literature text. It is noted that the units are extracted
from the sentence of the experimental value; standard units are typically given as a molar concentration such as
micromolar or nanomolar units.
3.2

Text Mining Examples from Unstructured Abstract Text

Using the rules previously described in the Methods section, Figure 3 shows several examples of direct correlation
sentences of nafamostat, which is also shown in Table 2, labelled with an experimental value along with the experiment
type from three different article abstracts[11] [12] [13]. Nafamostat is a small molecule drug which had the best
experimental value out of all of the extracted data samples. These sentences were taken directly from the preprint or
published abstracts aggregated in our database, and Figure 3 visualizes what the rule-based search engine looked for in
each abstract. It is noted that several other drugs are also labeled in Figure 3 for visualization purposes, but these drugs
are not described in further detail.
4

Real-time tracking of COVID-19 and coronavirus research updates through text mining

Table 1: Top 20 known small molecule drugs undergoing COVID-19 clinical trials.
Treatment

Count

Hydroxychloroquine
Ritonavir
Lopinavir
Azithromycin
Tocilizumab
Ivermectin
Favipiravir
Remdesivir
Chloroquine
Colchicine
Dexamethasone
Methylprednisolone
Enoxaparin
Nitazoxanide
Ruxolitinib
Anakinra
Angiotensin
Heparin
Baricitinib
Interferon beta

153
65
61
60
55
51
38
33
32
24
23
23
22
20
19
15
15
15
14
14

Table 2: Five small molecule drugs with in vitro assay results.
Drug name

Assay

Value

Units (uM)

Nafamostat
Azithromycin
Pralatrexate
Adenosine
Remdesivir

IC50
EC50
EC50
EC50
EC50

0.0022
0.008
0.008
0.01
0.01

micro
um
um
um
um

Figure 3: Several sentences from different abstracts containing the drug “nafamostat”. All drugs were labeled in red,
experiments in green, and numerical values in blue.

5

Real-time tracking of COVID-19 and coronavirus research updates through text mining

Table 3: Topic keywords for five select topics in our optimized topic model.
AI

Mental Health

Disease Analysis

Genetics

PPE

covid
ct
score
use
image
diagnosis
pneumonia
feature
lung
base

covid
health
mental
pandemic
anxiety
participant
report
study
survey
psychological

covid
risk
age
high
population
mortality
factor
infection
disease
increase

sars_cov
protein
ace
viral
virus
human
cell
host
analysis
genome

mask
use
air
respirator
particle
surface
wear
environmental
device
transmission

Figure 4: Select Titles of the five topics in our optimized topic model in Table 3. Some of the title names were truncated
because of the large string size.

3.3

Topic Model Examples

Table 3 prints 5 of the topics taken from the LDA topic model along with their manually assigned label. The top topics
were taken from a grid-search optimized number of topics while maximizing the coherence score using a corpus with
over 20,000 abstracts. The best topic model was found to contain 30 topics. Among these topics, there were several
that had minimal samples for that cluster. These topics were removed from the final presentation using a data driven
approach. Several paper titles for each topic are shown in Figure 4 justifying the manual label attached to each LDA
topic.

4

Discussion

We have developed several automatic modules from the openly available data. The full data results are publicly available at our website COVID-19: GHDDI Info Sharing Portal: https://ghddi-ailab.github.io/
Targeting2019-nCoV
4.1

Evaluation of Text Mining Results

We evaluated a random subset of article abstracts with a gold standard that was manually read and labeled for the same
information extraction task. The manual gold standard results are then evaluated with the results in Table 4.
6

Real-time tracking of COVID-19 and coronavirus research updates through text mining

Table 4: Results of data extraction system on a random subset of abstracts.
Metric

Value

Precision
Recall
F1 Score

0.808
0.689
0.743

It can be seen that the precision of the system was assessed to be around 0.8 showing that our system can indeed extract
most of the drug names and experimental values correctly. Therefore, this validates that our system can be used as a
recommendation for users to follow-up on these articles. After a manual review of the articles, it was found that many
of the drug names that could not be extracted were not found in our drug dictionary that we had compiled. Additionally,
wrong experimental values and wrong drug name mappings were attributed to the fact that the rule-based system cannot
robustly handle some content such as when multiple numerical values appear at multiple locations in a sentence. More
fine-tuning of this system’s rules is needed to boost the precision. It is however noted that a high precision system is not
necessarily important in this module as the intended purpose is mainly to gather and recommend preclinical studies for
further research. Model-based systems that could be built in the future may be able to rectify these mistakes and output
a higher precision final result.
4.2

Evaluation of n-grams

In an earlier iteration of the clinical trial analysis module, only unigrams were used for drug data extraction. This
caused an error such as “chloroquine” and “phosphate” being double counted in some instances. Another error included
instances where the keyword “interferon” was present in the clinical trial, but the drug dictionary did not have a unigram
instance of this keyword. Therefore, the drug was not able to be matched with the dictionary. However, upon adding
bigrams and trigrams to this module, “interferon beta” and “interferon alpha” were both successfully extracted from the
clinical trial data.
Likewise, this addition was utilized in the preclinical workflow, but during preliminary analysis of the results, this
workflow was ultimately not included in the current module because the initial results showed no significant improvement
of relevant data extraction precision for this workflow. The opposite occured and only unfiltered noise data was extracted
using n-grams. This is likely due to the fact that unlike the somewhat cleaned and structured clinical trial text, the
abstract text is completely freeform, so the backend algorithm best captures different pieces of information such as an
experimental value or a reported experiment using single token keywords. The inclusion of multi-word sequences in
this workflow can be extremely superfluous and confusing.
Similarly, n-grams up to trigrams were built into the topic model data preprocessing pipeline. This was because there
are some word pairs that are necessary for topics to be accurate and differentiable. One key example shown in Table 3
is ’sars cov’ being one token instead of the two tokens ’sars’ and ’cov’. This will give the LDA model cleaner input data
especially if differentiating between “SARS” virus and “SARS-CoV-2”.
4.3

Topic Model Recommendation System

In Figure 5, the optimal topic model by maximum coherence contained 30 topics. Several topics did not include many
papers, so they were excluded in the final results. This filter was done using a data-driven technique where topics that
contained less than a fifth of the average amount of papers per topic were excluded in the final results. This meant that
topics clustered with very few papers were excluded from the final module results, and the quality of the recommended
papers per topic remains clear and differentiable from the evidence shown in Figure 4. This figure showed that most
of the Top 5 highest-weighted papers from the corpus in each topic contained something in the title pertinent to this
topic. One example is that the hand labeled "AI" topic indeed contained papers with titles discussing deep learning or
CT images. Another interesting example is the "PPE" topic where the papers clustered into this topic contained titles
talking about N95 respirators, masks, and respiratory aerosols.
4.4

Limitations and Future Work

The major limitation for the clinical trial and preclinical text mining section is that they both rely on known drug
dictionaries for the exact text search. This means that all drug names contain known, approved, or experimental drugs.
However, some experimental drugs have not yet been added into the database, so tracking newly published data on
experimental drugs is a major challenge for future AI models.
7

Real-time tracking of COVID-19 and coronavirus research updates through text mining

Figure 5: The coherence score compared to number of topics. This gave an optimized number of topics for the final
topic model.

The granularity of the preliminary database search was impacted by two important factors: computational cost and
precision. In the preclinical text mining and information extraction workflow, we have tried to optimize the performance
of the initial preclinical abstract search to have high precision while minimizing the computational cost of text mining.
Therefore, during the development of this workflow, we have looked at many different search queries to get the optimal
number of abstracts to text mine. Some keywords that were used include: “covid”, “coronavirus”, “preclinical”, “in
vitro”, “EC50”, and “experiment”. While all of these keywords yielded the papers of interest, we optimized our search
by first using keywords such as “coronavirus” and “sars-cov-2” and then later searched the keywords “EC50” and
“IC50”. Compared to searching keywords such as “preclinical” and/or “in vitro”, this allowed a more optimal precision
to mining texts of interest while minimizing the number of papers that did not have any useful information as searching
the more general keywords gives more samples with useless information and increases the computational effort.
In our workflow, we primarily used a rule-based text-mining system to extract drug names and experimental values
evidenced in Table 1, Table 2, and Figure 3. Due to the nature of the pandemic, the first efforts all revolved around drug
repurposing; therefore, the use of a dictionary for text mining should be sufficient for this purpose as all known drug
names and aliases were captured in our dictionary. However, if this system were to be tracking a long-term disease or
pandemic, a more robust data extraction system should be built. In our preliminary analysis, a Scispacy Named-entity
recognition (NER) model[14] was assessed and compared to the rule-based system. Comparing this model in Figure 6,
it can be observed that although unique chemicals are identified, the precision is very low compared to a rule-based
system especially since it captures information such as "h=15.26" and "ptdtytsvylgkfrg" as chemicals. Future work can
include building an NER model with curated data labels from a set of papers in one specific disease area, or an NER
model trained with more specific data labels to avoid false positives with experimental values and protein sequences.
As an extension to these text mining modules, there are several applications to AI models. Once the data mining
modules are mature, data curation efforts for small molecule drugs can be reduced and automated since robust research
datasets can be produced[15]. Furthermore, the backend scripts can all be extended and developed for other use-cases.
One such use-case can be a module performing data mining on known genes and knock-in, knock-down, or knock-out
relationships. Additionally, for the topic model, this can be useful for a variety of scientific fields including cancer or
infectious diseases. Future work can look into the topic models of these fields or at a specific area in one of these fields.

5

Conclusion

The modules presented on our portal and in this article showcase NLP techniques that may be useful in a global
pandemic where lots of text data is being generated daily. Our modules aim to ease the burden of reading thousands
of articles daily to the recommended ones automatically updated daily by our text mining systems. This will allow
8

Real-time tracking of COVID-19 and coronavirus research updates through text mining

Figure 6: A comparison of an NER model (left) compared to our rule-based system (right).
a significant reduction of time spent on reader articles and more time dedicated to research on coronavirus. These
modules also have the potential to be scaled to other applications in life sciences and may have use-cases in these areas.

References
[1] Covid-19 pandemic - wikipedia. https://en.wikipedia.org/wiki/COVID-19_pandemic. Accessed: 202101-30.
[2] Ghddi targeting covid-19 portal. https://ghddi-ailab.github.io/Targeting2019-nCoV/. Accessed:
2021-02-03.
[3] Dimensions Resources. Dimensions covid-19 publications, datasets and clinical trials, Mar 2020. URL
https://dimensions.figshare.com/articles/dataset/Dimensions_COVID-19_publications_
datasets_and_clinical_trials/11961063/37.
[4] Drugbank. https://www.drugbank.ca/. Accessed: 2021-02-03.
[5] Drugs@fda: Fda-approved drugs. https://www.accessdata.fda.gov/scripts/cder/daf/. Accessed:
2021-02-03.
[6] Mark Davies, Michał Nowotka, George Papadatos, Nathan Dedman, Anna Gaulton, Francis Atkinson, Louisa
Bellis, and John P. Overington. ChEMBL web services: streamlining access to drug discovery data and utilities.
Nucleic Acids Research, 43(W1):W612–W620, 04 2015. ISSN 0305-1048. doi:10.1093/nar/gkv352. URL
https://doi.org/10.1093/nar/gkv352.
[7] Kaggle
-drug
treatment
extraction
(taskvt).
https://www.kaggle.com/benjpjones/
drug-treatment-extraction-taskvt/. Accessed: 2021-02-03.
[8] Hua Xu, Shane P Stenner, Son Doan, Kevin B Johnson, Lemuel R Waitman, and Joshua C Denny. MedEx: a
medication information extraction system for clinical narratives. Journal of the American Medical Informatics
Association, 17(1):19–24, 01 2010. ISSN 1067-5027. doi:10.1197/jamia.M3378. URL https://doi.org/10.
1197/jamia.M3378.
[9] Kai Ming Ting. Precision and Recall, pages 781–781. Springer US, Boston, MA, 2010. ISBN 978-0-387-30164-8.
doi:10.1007/978-0-387-30164-8_652. URL https://doi.org/10.1007/978-0-387-30164-8_652.
[10] Radim Řehůřek and Petr Sojka. Software Framework for Topic Modelling with Large Corpora. In Proceedings of
the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50, Valletta, Malta, May 2010.
ELRA. http://is.muni.cz/publication/884893/en.
[11] Jonathan H. Shrimp, Stephen C. Kales, Philip E. Sanderson, Anton Simeonov, Min Shen, and Matthew D. Hall.
An enzymatic tmprss2 assay for assessment of clinical candidates and discovery of inhibitors as potential treatment
of covid-19. bioRxiv, 2020. doi:10.1101/2020.06.23.167544. URL https://www.biorxiv.org/content/
early/2020/08/06/2020.06.23.167544.
[12] Meehyun Ko, Sangeun Jeon, Wang-Shick Ryu, and Seungtaek Kim. Comparative analysis of antiviral efficacy of
fda-approved drugs against sars-cov-2 in human lung cells: Nafamostat is the most potent antiviral drug candidate.
bioRxiv, 2020. doi:10.1101/2020.05.12.090035. URL https://www.biorxiv.org/content/early/2020/
05/12/2020.05.12.090035.
[13] Mizuki Yamamoto, Maki Kiso, Yuko Sakai-Tagawa, Kiyoko Iwatsuki-Horimoto, Masaki Imai, Makoto Takeda,
Noriko Kinoshita, Norio Ohmagari, Jin Gohda, Kentaro Semba, Zene Matsuda, Yasushi Kawaguchi, Yoshihiro
9

Real-time tracking of COVID-19 and coronavirus research updates through text mining

Kawaoka, and Jun-ichiro Inoue. The anticoagulant nafamostat potently inhibits sars-cov-2 infection in vitro: an
existing drug with multiple possible therapeutic effects. bioRxiv, 2020. doi:10.1101/2020.04.22.054981. URL
https://www.biorxiv.org/content/early/2020/04/23/2020.04.22.054981.
[14] Mark Neumann, Daniel King, Iz Beltagy, and Waleed Ammar. ScispaCy: Fast and Robust Models for Biomedical
Natural Language Processing. In Proceedings of the 18th BioNLP Workshop and Shared Task, pages 319–327,
Florence, Italy, August 2019. Association for Computational Linguistics. doi:10.18653/v1/W19-5034. URL
https://www.aclweb.org/anthology/W19-5034.
[15] Hannah L. Weeks, Cole Beck, Elizabeth McNeer, Cosmin A. Bejan, Joshua C. Denny, and Leena Choi. medextractr:
A medication extraction algorithm for electronic health records using the r programming language. medRxiv,
2019. doi:10.1101/19007286. URL https://www.medrxiv.org/content/early/2019/09/23/19007286.

10

