Misinformation Has High Perplexity
Nayeon Lee∗, Yejin Bang∗ , Andrea Madotto, Pascale Fung
Center for Artificial Intelligence Research (CAiRE)
Hong Kong University of Science and Technology
[nyleeaa,yjbang,amadotto].connect.ust.hk, pascale@ece.ust.hk

Abstract

arXiv:2006.04666v2 [cs.CL] 10 Jun 2020

...

Debunking misinformation is an important and
time-critical task as there could be adverse
consequences when misinformation is not
quashed promptly. However, the usual supervised approach to debunking via misinformation classification requires human-annotated
data and is not suited to the fast time-frame of
newly emerging events such as the COVID-19
outbreak. In this paper, we postulate that misinformation itself has higher perplexity compared to truthful statements, and propose to
leverage the perplexity to debunk false claims
in an unsupervised manner. First, we extract reliable evidence from scientific and news
sources according to sentence similarity to the
claims. Second, we prime a language model
with the extracted evidence and finally evaluate the correctness of given claims based on
the perplexity scores at debunking time. We
construct two new COVID-19-related test sets,
one is scientific, and another is political in content, and empirically verify that our system
performs favorably compared to existing systems. We are releasing these datasets1 publicly
to encourage more research in debunking misinformation on COVID-19 and other topics.

1

Introduction

Debunking misinformation is a process of exposing the falseness of given claims based on relevant
evidence. The failure to debunk misinformation
in a timely manner can result in catastrophic consequences, as illustrated by the recent death of a
man who tried to self-medicate with chloroquine
phosphate to prevent COVID-19 (Vigdor, 2020).
Amid the COVID-19 pandemic and infodemic, the
need for an automatic debunking system has never
been more dire.
∗
1

These two authors contributed equally.
https://github.com/HLTCHKUST/covid19-misinfo-data

Figure 1: Proposed approach of using the language
model (LM) as a debunker. We prime an LM with extracted evidence relevant to the whole set of claims, and
then we compute the perplexities during the debunking
stage.

A lack of data is the major obstacle for debunking misinformation related to newly emerged
events like the COVID-19 pandemic. There are
no labeled data available to adopt supervised deeplearning approaches (Etzioni et al., 2008; Wu et al.,
2014; Ciampaglia et al., 2015; Popat et al., 2018;
Alhindi et al., 2018), and an insufficient amount
of social- or meta-data to apply feature-based approaches (Long et al., 2017; Karimi et al., 2018;
Kirilin and Strube, 2018). To overcome this bottleneck, we propose an unsupervised approach of
using a large language model (LM) as a debunker.
Misinformation is a piece of text that contains
false information regarding its subject, resulting in
a rare co-occurrence of the subject and its neighboring words in a truthful corpus. When a language
model primed with truthful knowledge is asked
to reconstruct the missing words in a claim, such
as “5G communication transmits ... ”, it would
predict word “information” with the highest probability (7 × 10−1 ). On the other hand, it would

predict the word ”COVID-19” in a false claim such
as “5G communication transmits COVID-19” with
very low probability (i.e., 2 × 10−4 ). It follows
that truthful statements would give low perplexity
whereas false claims tend to have high perplexity,
when scored by a truth-grounded language model.
Since perplexity is a score for quantifying the likelihood of a given sentence based on previously
encountered distribution, we propose a novel interpretation of perplexity as a degree of falseness.
To further address the problem of data scarcity,
we leverage the large pre-trained LM, such as GPT2 (Radford et al., 2019), which are shown to be
helpful in a low-resource setting by allowing the
transfer learning of features learned from their huge
training corpus (Devlin et al., 2018; Radford et al.,
2018; Liu et al., 2019; Lewis et al., 2019). It is
also illustrated the potential of LMs in learning
useful knowledge without any explicit task-specific
supervision to perform well on tasks such as question answering and summarization (Radford et al.,
2019; Petroni et al., 2019).
Moreover, it is crucial to ensure that the LM is
populated with “relevant and clean evidence” before assessing the claims, especially when these are
related to newly emerging events. There are two
main ways of obtaining evidence in fact-checking
tasks. One way is to rely on evidence from the
structured knowledge base such as Wikipedia and
knowledge-graph (Wu et al., 2014; Ciampaglia
et al., 2015; Thorne et al., 2018; Yoneda et al.,
2018a; Nie et al., 2019). Another approach is to
obtain evidence directly from unstructured data online (Etzioni et al., 2008; Magdy and Wanas, 2010).
However, the former approach faces a challenge in
maintaining up-to-date knowledge, making it vulnerable to unprecedented outbreaks. On the other
hand, the latter approach suffers from the credibility issue and the noise of the evidence. In our work,
we attempt to combine the best of both worlds in
the evidence selection step by extracting evidence
from unstructured-data and ensuring quality by filtering noise.
The contribution of our work is threefold. First,
we propose a novel dimension of using language
model perplexity to debunk false claims, as illustrated in Figure 1. It is not only data efficient but
also achieves promising results comparable to supervised baseline models. We also carry out qualitative analysis to understand the optimal ways of
exploiting perplexity as a debunker. Second, we

present an additional evidence-filtering step to improve the evidence quality, which consequentially
improves the overall debunking performance. Finally, we construct and release two new COVID19-pandemic-related debunking test sets.

2

Motivation

Language Modeling Given a sequence of tokens
X = {x0 , · · · , xn }, language models are trained
to compute the probability of the sequence p(X).
This probability is factorized as a product of conditional probability by applying the chain rules
(Manning et al., 1999; Bengio et al., 2003):
p(X) =

n
Y

p(xi |x0 , · · · , xi−1 )

(1)

i=1

In recent years, large transformer-based (Vaswani
et al., 2017) language models have been trained to
minimize the negative log-likelihood over a large
collection of documents.
Leveraging Perplexity Perplexity, a commonly
used metric for measuring the performance of an
LM, is defined as the inverse of the probability of
the test set normalized by the number of words:
v
u n
uY
n
P P L(X) = t

1
p(x
|x
,
·
i 0 · · , xi−1 )
i=1

(2)

Another way of interpreting perplexity is the measure of the likelihood of a given test sentence in
reference to the training corpus. Based on this
intuition, we hypothesize the following:
“When a language model is primed with
a collection of relevant evidence about
given claims, the perplexity can serve as
an indicator for falseness.”
The rationale behind is that the extracted evidence sentences for a True claim would share
more similarities (e.g., common terms or synonyms) with its associated claim. This leads to
True claims to have lower perplexity while the
False claims remain having higher perplexity.

3

Methodology

Task Definition Debunking is the task of exposing the falseness of a given claim by extracting
relevant evidence and verifying the claims upon it.
More formally, given a claim c with its corresponding source document D, the task of debunking is

False Claims

Perplexity

Ordering or buying products shipped from overseas will make a person get COVID-19.
Sunlight actually can kill the novel COVID-19.
5G helps COVID-19 spread.
Home remedies can cure or prevent the COVID-19.
True Claims

556.2
385.0
178.2
146.2
Perplexity

The main way the COVID-19 spreads is through respiratory droplets.
The most common symptoms of COVID-19 are fever, tiredness, and dry cough.
The source of SARS-CoV-2, the coronavirus (CoV) causing COVID-19 is unknown.
Currently, there is no vaccine and no specific antiviral medicine to prevent or treat COVID-19.

5.8
6.0
8.1
8.4

Table 1: Relationship between claims and perplexity. False claims have higher perplexity compared to True claims.

to assigning a label from {True, False} by retrieving and utilizing a relevant set of evidence
E = {e1 , e2 , e3 } from D.
Our approach involves three steps in the inference phase: 1) Evidence selection to retrieve
the most relevant evidence from D. 2) Evidence
grounding step to obtain our evidence-primed language model (LM Debunker). 3) Debunking step to
obtain perplexity scores from the evidence-primed
LM Debunker and debunking labels.
3.1

Evidence Selection

Given a claim c, our Evidence Selector retrieves
the top-3 relevant evidence E = {e1 , e2 , e3 } in the
following two steps.
Evidence Candidate Selection Given the
source documents D, we select the top-10 most
relevant evidence sentences for each claim.
Depending on the domain of the claim and
source documents, we rely on generic TF-IDF
method to select the tuples of evidence candidates with their corresponding relevancy scores
{(e1 , s1 ), · · · , (e10 , s10 )}. Note that any evidence
extractor can be used here.
Evidence Filtering After selecting the top candidate tuples for the claim c, we attempt to filter
out the noise and unreliable evidence based on the
following rulings:
1) When an evidence candidate is a quote from a
low-credibility speaker such as an Internet meme2
or social-media-post, we discard it (e.g., “quote
according to a social media post.”). Note that this
approach leverages the speaker information inherent in the extracted evidence; 2) If a speaker of
the claim is available, any quote or statement by
2
An idea, image, or video that is spread very quickly on
the internet.

the speaker him/herself is inadmissible to the evidence candidate pool; 3) Any evidence identical
to the given claim is considered to be “invalid evidence (i.e. direct quotation of true/false claim);
4) We filter out reciprocal questions, which only
add noise but have no supporting or contradicting
information to the claim from our observation. The
examples of before and after this filtration is shown
in the Appendix.
The final top-3 evidence E is selected after the filtering based on the provided extractor score s. An
example of a claim and its corresponding extracted
evidence are shown in Table 2.
3.2

Grounding LM with Evidence

For the purpose of priming the language model,
all the extracted evidence for a batch of
claims C = {c1 , · · · , ck } are aggregated as
E = {e11 , e12 , e13 , · · · , ek1 , ek2 , ek3 }. We obtain
our evidence-grounded language model (LM Debunker) by minimizing the following loss L:
L(E) = −

|E| n
X
X

log pθ (xji |xj0 , · · · , xji−1 ), (3)

j=1 i=1

where the xji denotes a tokens in the evidence ej ,
and θ the parameters of the language model. It
is important to highlight that none of the debunking labels or claims are involved in this evidence
grounding step and that our proposed methodology
is model agnostic.
3.3

Debunking with Perplexity

The last step is to obtain debunking labels based
on the perplexity values from the LM Debunker.
As shown in Table 1, perplexity values reveal a
pattern that aligns with our hypothesis regarding
its association with falseness; the false claims have

Claim: The main way the COVID-19 spreads is through respiratory droplets.

Label: True

Evidence 1: The main mode of COVID-19 transmission is via respiratory droplets, although the
potential of transmission by opportunistic airborne routes via aerosol-generating procedures
in health care facilities, and environmental factors, as in the case of Amoy Gardens, is known.
Evidence 2: The main way that influenza viruses are spread is from person to person via
virus-laden respiratory droplets (particles with size ranging from 0.1 to 100 µm in diameter) that
are generated when infected persons cough or sneeze.
Evidence 3: The respiratory droplets spread can occur only through direct person-to-person
contact or at a close distance.

Table 2: Illustration of evidence extracted using our Evidence Selector

higher perplexity than the true claims (For more examples of perplexity values, refer to the Appendix).
Perplexity scores can be translated to debunking
labels by comparing to a perplexity threshold th
that defines the False boundary in the perplexity
space. Any claim with a perplexity score higher
than the threshold is classified as False, and vice
versa for True.
The optimal method of selecting the hyperparameter threshold th is an open research question. From an application perspective, any value
can serve as a threshold depending on the desired
level of “strictness” towards false claims. We define “strictness” as the degree of precaution towards
false negative error, which is the most undesirable
form of error in debunking (refer to Section 7 for
details). From an experimental analysis perspective, a small validation set could be leveraged for
hyper-parameter tuning of the threshold (th). In
this paper, since we have small test sets, we do kfold cross-validation (k = 4) to obtain the average
performance reported in Section 6.

4
4.1

Dataset
COVID19 Related Test Sets

Covid19-scientific A new test set is constructed
by collecting COVID-19-related myths and scientific truths labeled by reliable sources like MedicalNewsToday, Centers for Disease Control and
Prevention (CDC), and World Health Organization
(WHO). It consists of the most common scientific
or medical myths about COVID-19, which must
be debunked correctly to ensure the safety of the
public (e.g., “drinking a bleach solution will prevent you from getting the COVID-19.”). There
are 142 claims (Table 3) with labels obtained from
the aforementioned reliable sources. According to
WHO and CDC, some myths are unverifiable from
current findings, and we assigned False labels
to them (e.g., “The coronavirus will die off when

Test sets

False
claims

True
claims

Total

Covid19-scientific
Covid19-politifact

101
263

41
77

142
340

Table 3: COVID-19 Related Test Set Statistics

temperatures rise in the spring.”).
Covid19-politifact Another test set is constructed by crawling COVID-19-related claims factchecked by journalists from a website called Politifact 3 . Unlike the Covid19-scientific test set, it
contains non-scientific and political claims such
as “For the coronavirus, the death rate in Texas,
per capita of 29 million people, we’re one of the
lowest in the country”. Such political claims may
not be life-and-death matters, but they still have
the potential to bring negative sociopolitical effects.
Originally, these claims are labeled into six classes
{pants-fire, false, barely-true, half-true, mostlytrue, true}, which represent the decreasing degree
of fakeness. We use a binary setup for consistency
with our setup for Covid19-scientific by assigning
the first three classes as False and the rest as
True. For detailed data statistics, refer to Table 3.
Gold Source Documents Different gold source
document D are used depending on the domain of
the test sets. For the Covid19-scientific, we use
CORD-19 dataset4 , a free open research resource
for combating the COVID-19 pandemic. It is a
resource of over 59,000 scholarly articles, including over 47,000 with full text, about COVID-19,
SARS-CoV-2, and other related coronaviruses.
For the Covid19-politifact, we leverage the resources of the Politifact website. When journalists
verify the claims on Politifact, they provide pieces
3

https://www.politifact.com/
https://www.kaggle.com/allen-institute-for-ai/CORD19-research-challenge
4

of text that contain: i) the snippets of relevant information from various sources, and ii) a paragraph
of their justification for the verification decisions.
We only take the first part (i) to be our gold source
documents, to avoid using explicit verdicts on test
claims as evidence.

5

• LiarPlus: Similar to LiarPlusMeta model, but
without meta-information. Our replication
also outperforms the SOTA by absolute 8% in
accuracy. This baseline is important because
the focus of this paper is to explore the debunking ability in a data-scarce setting, where
meta-information may not exist.

Experiments

5.1

Baseline Models

Although unrelated to COVID-19 misinformation,
there are notable state-of-the-art (SOTA) models
and their associated datasets in fact-based misinformation field.
FEVER (Thorne et al., 2018) Fact Extraction
and Verification (FEVER) is a publicly released
large-scale dataset generated by altering sentences
extracted from Wikipedia to promote research in
fact-checking systems. We use one of the winning
systems from the FEVER workshop5 as our first
baseline model.
• Fever-HexaF (Yoneda et al., 2018b) Given
a claim and a set of evidence, it leverages a
natural language inference model to get entailment scores between claim and each evidence
(se1 , se2 , ..., sen ), and obtains the final prediction label by aggregating the entailment scores
using a Multi-Layer Perceptron (MLP).
LIAR-Politifact (Wang, 2017) LIAR is a publicly available dataset collected from the Politifact
website, which consists of 12.8k claims. The label setup is the same as our Covid19-politifact test
set, and the data characteristics are very similar,
but LIAR does not contain any claims related to
COVID-19.
We also report three strong BERT-based (Devlin
et al., 2018) baseline models trained on LIAR data:
• LiarPlusMeta: Our BERT-large-based replication of SOTA paper from Alhindi et al..
It uses meta-information and “justification,”
human-written reasoning for verification decision in Politifact article, as evidence for the
claim. Our replication is a more robust baseline, outperforming the reported SOTA accuracy by absolute 9% (refer to Appendix for
detailed result table).
5

https://fever.ai/2018/task.html. We use the 2nd team because we had problems running the 1st team’s codebase. Note
that the accuracy between 1st and 2nd is very minimal

• LiarOurs: Another BERT-large model finetuned on LIAR-Politifact claims with evidence from our Evidence Selector, instead of
using human-written “justification.”
5.2

Experiment Settings

Out-of-distribution Setting For the Covid19scientific test set, all models are tested in the outof-distribution (OOD) setting because the test set is
from different distribution compared to all the train
sets used in baseline models; Fever-HexaF model
is trained on FEVER dataset, all other Liar baseline
models are trained on LIAR-Politifact dataset. For
the Covid19-politifact test set, Fever-HexaF model
is again tested in OOD setting. However, all the
Liar-models are not because both their train sets
and the Covid19-politifact test set are from a similar distribution (Politifact). We use blue highlights
in the Table 4 to indicate models tested in OOD
settings.
Evidence Input for Testing Recalling the task
definition explained in Section 3, we test a claim
with its relevant evidence. To make fair comparisons among all baseline models and our LM Denbunker, we use the same evidence extracted in the
Evidence Selector step while evaluating the models
on the COVID-19-related test sets.
Language Model Setting For our experiments,
GPT-2 (Wolf et al., 2019) model is selected as our
base language model to build LM Debunker. We
use the pre-trained GPT-2 model (base), with 117
million parameters. Since the COVID-19 pandemic
is a recent event, it is guaranteed that the GPT-2 has
not seen any COVID-19 related information during
its pre-training. Thus, very clean and unbiased
language model to test our hypothesis.
5.3

Experiment Details

We evaluate the performance LM Debunker by
comparing it to other baselines on two commonly
used metrics: accuracy and F1-Macro score. Since
identifying False claims is important in debunking, we also report F1 of False class (F1-Binary).

Covid19-scientific

Covid19-politifact

Accuracy

F1-Macro

F1-Binary

Accuracy

F1-Macro

F1-Binary

Fever-HexaF
LiarPlusMeta
LiarPlus
LiarOurs

64.8%
42.3%
45.1%
61.5%

58.1%
41.1%
44.8%
59.2%

74.8%
32.8%
44.9%
82.8%

46.6%
80.3%
54.4%
78.5%

37.9%
66.8%
52.8%
67.7%

61.2%
86.5%
61.5%
86.4%

LM Debunker

75.4%

69.8%

83.1%

74.4%

58.8%

84.2%

Models

Table 4: Result comparison of our LM Debunker and baselines on two COVID-19 related test sets. Blue highlights
represent the models tested in out-of-distribution setting (i.e., train set and test set are from different distribution).
Note that all accuracy scores are statistically significant (p < 0.05).

Covid19-politifact

LiarPlusMeta

LiarOurs

Train Size

Accuracy

F1-Macro

F1-Binary

Accuracy

F1-Macro

F1-Binary

0
10
100
500
1000
10000 (All)

N/A
22.6%
22.6%
73.5%
72.4%
80.3%

N/A
18.5%
18.5%
65.2%
63.4%
66.8%

N/A
0.0%
0.0%
82.2%
81.5%
86.5%

N/A
22.6%
22.6%
64.4%
70.9%
78.5%

N/A
18.5%
18.5%
59.4%
64.2%
67.8%

N/A
0.0%
0.0%
73.6%
79.7%
86.4%

LM Debunker

74.4%

58.8%

84.2%

74.4%

58.8%

84.2%

Table 5: Performance comparison between our LM Debunker and two baseline models trained on varying train set
sizes. LiarPlusMeta and LiarOurs have shown the best performance on Covid-politifact test set in accuracy and
F1-Macro, respectively. Gray highlights represent the first scores that surpass the LM Debunker scores.

Recall that we report average results obtained kfold cross-validation. The thresholds used in each
fold are {18, 19, 17, 22} for Covid-politifact and
{15, 24, 17, 20} for Covid-scientific6 .
For the evidence grounding step, a learning rate
of 5e-5 was adopted, and different epoch sizes
were explored {1, 2, 3, 5, 10, 20}. We reported the
choice with the highest performance in both accuracy and F1-Macro. Each trial was run on Nvidia
GTX 1080 Ti, taking 39 seconds per epoch for
Covid-scientific and 113 seconds per epoch for
Covid-politifact.

6
6.1

Experimental Results
Performance of LM Debunker

From Table 4, we can observe that our unsupervised LM debunker portrays notable strength in the
out-of-distribution setting (highlighted with blue)
over other supervised baselines. For the Covid19scientific test set, it achieved state-of-the-art results
across all metrics and marginally improved in accu6
Note that we use k-fold cross-validation to obtain the
average performance, not the average optimal threshold.

racy and F1-binary by an absolute 13.9% ∼ 30.3%
and 10% ∼ 28.7% respectively. Considering
the severe consequences Covid19-scientific myths
could potentially bring, this result is valuable.
For the Covid19-politifact test set, our LM debunker also outperformed Fever-HexaF model and
LiarPlus with a significant margin, but it underperformed the LiarOurs model and the LiarPlusMeta
model. Nonetheless, it is still encouraging considering the fact that these two models were trained
with task-specific supervision on Politifact dataset
(LIAR-Politifact), which is similar to the Covid19politifact test set.
The results of LiarPlus and LiarPlusMeta clearly
show the incongruity of the meta-based approach
for cases in which meta-information is not guaranteed. LiarPlusMeta struggled to debunk the claims
from the Covid19-scientific test set in contrast to
achieving SOTA in Covid19-politifact test set. This
is because the absence of meta-information for
Covid19-scientific test set hindered LiarPlusMeta
from performing to its maximum capacity. Going further, the fact that LiarPlusMeta performed

80

0.70

60

0.65

Count

F1 Macro

0.75

0.60

0
1

2

3

5

Epochs

10

20

Figure 2: Trend of highest F1-Macro score over different numbers of epochs for evidence grounding

even worse than LiarPlus emphasizes the difficulty
faced by meta-based models in the absence of metainformation.
FEVER dataset and many FEVER-trained systems successfully showcased the advancement of
systematic fact-checking. Nevertheless, FeverHexaF model exhibited rather low performance
on COVID-19 test sets (10% ∼ 30% behind LMdebunker in accuracy). One possible justification is the way how FEVER data was constructed.
FEVER claims were generated by altering sentences from Wikipedia (e.g., “Hearts is a musical composition by Minogue”, label: SUPPORTS).
It makes the nature of FEVER-claims have a discrepancy with the naturally occurring myths and
false claims flooding the internet. This implies that
FEVER might not be the most suitable dataset for
training non-wikipedia-based fact-checkers.
6.2

Data Efficiency Comparison

In Table 5, we report the performance of LiarOurs
and LiarPlusMeta classifiers trained on randomly
sampled train sets of differing sizes {10, 100, 500,
1000, 10000}.
As shown by the gray highlights in Table 5,
both classifiers overtake our debunker in F1-Macro
score with 500 labeled training data, but they require 10000 to outperform on the rest of evaluation metric. Considering the scarcity of labeled
misinformation data for newly emerged events, a
data-efficient debunker is extremely meaningful.

7
7.1

40
20

0.55
0.50

False Negative
False Positive

Analysis and Discussion
LM Debunker Behavior Analysis

Number of Epoch for Evidence Grounding
The relationship between the number of epoch in
the evidence grounding step and the debunking
performance is explored. The best performance is
obtained with epoch=5, as shown in Figure 2. We

80

60

40

20

Threshold

0

Figure 3: Trend of false negative and false positive
counts over varying threshold.

Before
After

Covid19-scientific

Covid19-politifact

Acc.

F1-Macro

Acc.

F1-Macro

74.6%
75.4%

56.3%
69.8%

75.0%
74.4%

50.6%
58.9%

Table 6: Performance comparison between the “before”
and “after” filtering steps in Evidence Selector

believe this is because a low number of epochs does
not allow enough updates to encode the content of
evidence into the language model sufficiently. On
the contrary, a higher number of epochs over-fit the
language model to the given evidence and harms
the generalizability of the language model.
Threshold Perplexity Selection As aforesaid,
the threshold th is controllable to reflect the desired “strictness” of the debunking system. Figure
3 depicts that decreasing the threshold helps to reduce the FN errors, which is the most dangerous
form of error. Such controllability over strictness
would be beneficial to the real-world applications,
where the level of “strictness” matters greatly depending on the purpose of the applications.
Meanwhile, FN reduction comes with a tradeoff of increased false positive errors (FP). For a
more balanced debunker, an alternative threshold
choice could be the intersection point of FN and
FP frequencies.
7.2

Evidence Analysis

Our ablation study of the evidence filtering and
cleaning steps (Table 6) shows that improved evidence quality brings big gains in F1-Macro scores
(13.5% and 8.3%) with only 1% loss in accuracy.
Moreover, comparing the performances of LM
Debunker on each of the two test sets, Covid19scientific scores surpass Covid19-politifact scores,
especially in F1-Macro, by 11.1%. This is due
to the disparate natures of the gold source doc-

uments used in evidence selection; the Covid19scientific claims obtain evidence from scholarly
articles, whereas Covid19-politifact claims extract
evidence from news articles and other unverified
internet sources. Consequently, this resulted in
a different quality of extracted evidence. Therefore, an important insight would be that evidence
quality is crucial to our approach, and additional
performance gain would be fostered from further
improvement in evidence quality.
7.3

Error analysis and Future Work

We identified areas for improvement in future work
through qualitative analysis of wrongly-predicted
samples from the LM debunker. First, since perplexity originally serves as a measure of sentence
likelihood, when a true claim has an abnormal sentence structure, our LM deunker makes a mistake
by assigning high perplexity. For example, a true
claim “So Oscar Helth, the company tapped by
Trump to profit from covid tests, is a Kushner company. Imagine that, profits over national safety” has
extremely high perplexity. One interesting future
direction would be to explore a way of disentangling “perplexity as a sentence quality measure”
from “perplexity as a falseness indicator”.
Second, our LM debunker makes mistakes when
selected evidence is refuting the False claim by
simply negating the content of the paraphrased
claim. For instance, for a false claim “Taking
ibuprofen worsens symptoms of COVID-19,” the
top most relevant evidence from the scholarly articles is “there is no current evidence indicating that
ibuprofen worsens the clinical course of COVID19.” Another future direction would be to learn a
better way of assigning additional weight/emphasis
on special linguistic features such as negation.

8
8.1

information (Etzioni et al., 2008; Wu et al., 2014;
Ciampaglia et al., 2015; Popat et al., 2018; Alhindi et al., 2018; Baly et al., 2018; Lee et al.,
2018; Hanselowski et al., 2018). These approaches
based on the logic of “the information is correct if
evidence from credible sources or a group of online sources is supporting it.” Furthermore, some
works focus on reasoning and evidence selecting
ability by restricting the scope of facts to those
from Wikipedia (Thorne et al., 2018; Nie et al.,
2019; Yoneda et al., 2018a)

Related Work
Misinformation

Previous approaches (Long et al., 2017; Karimi
et al., 2018; Kirilin and Strube, 2018; Shu et al.,
2018; Monti et al., 2019) show that using metainformation (e.g. credibility score of the speaker)
with text input helps improve the performance of
misinformation detection. Considering the availability of meta-information is not always guaranteed, building a model independent from it is crucial to detect misinformation. There exist works
with fact-based approaches, using evidence from
external sources for assessing the truthfulness of

8.2

Language Model Applications

lead to significant advancements in wide variety
of NLP tasks, including question-answering, commonsense reasoning, and semantic relatedness (Devlin et al., 2018; Radford et al., 2019; Peters et al.,
2018; Radford et al., 2018). These models are typically trained on documents mined from Wikipedia
(among other websites). Recently, a number of
works have found that LMs store a surprising
amount of world knowledge, focusing particularly
on the task of open-domain question answering
(Petroni et al., 2019; Roberts et al., 2020). Going
further, Guu et al.; Roberts et al. show that task
specific fine-tuning of LM can achieve impressive
results, proving the power of LMs. In this paper,
we explore to confirm if large pre-trained LM can
also be helpful in the field of debunking.

9

Conclusion

In this paper, we show that misinformation has
high perplexity from the language model primed
with relevant evidence. By proposing the new application of perplexity, we build an unsupervised
debunker that shows promising results, especially
in the absence of labeled data. Moreover, we emphasize the importance of evidence quality in our
methodology by showing the improvement in the
final performance with the addition of a filtering
step in the evidence selection. We are also releasing two new COVID-19 related test sets publicly
to promote transparency and prevent the spread of
misinformation. Based on this successful leverage
of language model perplexity for debunking, we
hope to foster more research in this new direction.

Acknowledgements
We would like to thank Madian Khabsa for the
helpful discussion and inspiration.

References
Tariq Alhindi, Savvas Petridis, and Smaranda Muresan. 2018. Where is your evidence: Improving factchecking by justification modeling. In Proceedings
of the First Workshop on Fact Extraction and VERification (FEVER), pages 85–90.
Ramy Baly, Mitra Mohtarami, James Glass, Lluı́s
Màrquez, Alessandro Moschitti, and Preslav Nakov.
2018.
Integrating stance detection and fact
checking in a unified corpus.
arXiv preprint
arXiv:1804.08012.
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic language model. Journal of machine learning research,
3(Feb):1137–1155.
Giovanni Luca Ciampaglia, Prashant Shiralkar, Luis M
Rocha, Johan Bollen, Filippo Menczer, and
Alessandro Flammini. 2015. Computational fact
checking from knowledge networks. PloS one,
10(6):e0128193.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.

Bart: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and
comprehension. arXiv preprint arXiv:1910.13461.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
Yunfei Long, Qin Lu, Rong Xiang, Minglei Li,
and Chu-Ren Huang. 2017. Fake news detection
through multi-perspective speaker profiles. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2:
Short Papers), pages 252–256.
Amr Magdy and Nayer Wanas. 2010. Web-based
statistical fact checking of textual documents. In
Proceedings of the 2nd international workshop on
Search and mining user-generated contents, pages
103–110.
Christopher D Manning, Christopher D Manning, and
Hinrich Schütze. 1999. Foundations of statistical
natural language processing. MIT press.

Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extraction
from the web. Commun. ACM, 51(12):68–74.

Federico Monti, Fabrizio Frasca, Davide Eynard, Damon Mannion, and Michael M Bronstein. 2019.
Fake news detection on social media using geometric deep learning. arXiv preprint arXiv:1902.06673.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrievalaugmented language model pre-training. arXiv
preprint arXiv:2002.08909.

Yixin Nie, Haonan Chen, and Mohit Bansal. 2019.
Combining fact extraction and verification with neural semantic matching networks. In Association for
the Advancement of Artificial Intelligence (AAAI).

Andreas Hanselowski, Hao Zhang, Zile Li, Daniil
Sorokin, Benjamin Schiller, Claudia Schulz, and
Iryna Gurevych. 2018. Ukp-athene: Multi-sentence
textual entailment for claim verification. arXiv
preprint arXiv:1809.01479.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word representations. arXiv preprint arXiv:1802.05365.

Hamid Karimi, Proteek Roy, Sari Saba-Sadiya, and
Jiliang Tang. 2018. Multi-source multi-class fake
news detection. In Proceedings of the 27th International Conference on Computational Linguistics,
pages 1546–1557.
Angelika Kirilin and Micheal Strube. 2018. Exploiting a speakers credibility to detect fake news. In
Proceedings of Data Science, Journalism & Media
workshop at KDD (DSJM18).
Nayeon Lee, Chien-Sheng Wu, and Pascale Fung. 2018.
Improving large-scale fact-checking using decomposable attention models and lexical tagging. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1133–
1138.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.

Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton
Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2019. Language models as knowledge bases? arXiv preprint arXiv:1909.01066.
Kashyap Popat, Subhabrata Mukherjee, Andrew Yates,
and Gerhard Weikum. 2018. Declare: Debunking
fake news and false claims using evidence-aware
deep learning. arXiv preprint arXiv:1809.06416.
Alec Radford, Karthik Narasimhan, Tim Salimans,
and Ilya Sutskever. 2018. Improving language
understanding by generative pre-training. URL
https://s3-us-west-2. amazonaws. com/openaiassets/researchcovers/languageunsupervised/language
understanding paper. pdf.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. OpenAI
Blog, 1(8):9.

Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.
How much knowledge can you pack into the parameters of a language model?
arXiv preprint
arXiv:2002.08910.
Kai Shu, Suhang Wang, and Huan Liu. 2018. Understanding user profiles on social media for fake news
detection. In 2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR),
pages 430–435. IEEE.
James Thorne,
Andreas Vlachos,
Christos
Christodoulopoulos, and Arpit Mittal. 2018.
Fever: a large-scale dataset for fact extraction and
verification. arXiv preprint arXiv:1803.05355.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information processing systems, pages 5998–6008.
Neil Vigdor. 2020. Man fatally poisons himself while
self-medicating for coronavirus, doctor says.
William Yang Wang. 2017. ” liar, liar pants on fire”:
A new benchmark dataset for fake news detection.
arXiv preprint arXiv:1705.00648.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771.
You Wu, Pankaj K Agarwal, Chengkai Li, Jun Yang,
and Cong Yu. 2014. Toward computational factchecking. Proceedings of the VLDB Endowment,
7(7):589–600.
Takuma Yoneda, Jeff Mitchell, Johannes Welbl, Pontus
Stenetorp, and Sebastian Riedel. 2018a. UCL machine reading group: Four factor framework for fact
finding (HexaF). In Proceedings of the First Workshop on Fact Extraction and VERification (FEVER),
pages 97–102, Brussels, Belgium. Association for
Computational Linguistics.
Takuma Yoneda, Jeff Mitchell, Johannes Welbl, Pontus
Stenetorp, and Sebastian Riedel. 2018b. ”UCL machine reading group: Four factor framework for fact
finding (HexaF)”. In Proceedings of the First Workshop on Fact Extraction and VERification (FEVER),
Brussels, Belgium. Association for Computational
Linguistics.

Appendix

A

Relationship between Claims and Perplexity (PPL)

False Claims

PPL

Ordering or buying products shipped from overseas will make a person get COVID-19.
Sunlight actually can kill the novel COVID-19.
One should stay at home on particular days when helicopters spray disinfectant over homes for
killing off COVID-19.
Wearing warm socks and mustard patches as well as spreading goose fat on one’s chest are
treatments for the COVID-19.
5G helps COVID-19 spread.
Fish tank additive may treat COVID-19.
Home remedies can cure or prevent the COVID-19.
Drinking a lot of water and gargling with warm salt water eliminates the COVID-19 virus.
Steam your face with and inhale neem can prevent COVID-19.
Drinking alcohol protect you against COVID-19 and can be dangerous.
Vitamin D pills could help prevent the COVID-19.
Ultraviolet disinfection lamp kill the new coronavirus on your hands.
Taking extra amounts of Vitamin C can prevent COVID-19.

556.2
385.0
201.6

178.2
151.6
146.2
85.2
98.2
69.7
54.7
46.1
42.0

True Claims

PPL

Avoid touching eyes, nose and mouth help prevent COVID-19.
For COVID-19, other flu-like symptoms such as aches and pains, nasal congestion, runny nose,
sore throat or diarrhoea are also common.
SARS was more deadly but much less infectious than COVID-19.
Everyone is at risk of getting COVID-19.
COVID-19 is different to SARS.
Antibiotics does not work to kill COVID-19 virus.
Some people become infected by COVID-19 but don’t develop any symptoms and don’t feel unwell.
Currently, there is no vaccine and no specific antiviral medicine to prevent or treat COVID-19.
The source of SARS-CoV-2, the coronavirus (CoV) causing COVID-19 is unknown.
The incubation period for COVID-19 range from 1 to 14 days.
People with other health conditions, such as asthma, heart diseases and diabetes, are
at higher risk of getting seriously ill from COVID-19.
The most common symptoms of COVID-19 are fever, tiredness, and dry cough.
The main way the COVID-19 spreads is through respiratory droplets.
COVID-19 spreads from person to person.

26.5
24.2

200.5

21.5
18.0
17.8
17.2
10.4
8.4
8.1
7.5
6.3
6.0
5.8
3.4

Table A: Extended example to show relationship between claims and perplexity (PPL). False claims have higher perplexity
compared to True claims.

1

B

Before vs After Filtering Step

The following examples illustrate different evidence selected to be top-3, before and after the filtering step of our Evidence
Selector. Filtered evidence is the one detected one of policies we mentioned in the Section for Evidence Selection (3.1) and
Replacing Evidence is evidence that comes into top-3 after the filtered evidence gets discarded from the evidence candidate
pool.

Claim: “The coronavirus has made it to Mississippi and the lady that caught it wasn’t
around nobody with it which means it is airborne. That means if the wind blows it
your direction you’ll have it also."
Label: False
Filtered Evidence: More than 5,000 people have shared a Feb. 28 Facebook post,
for example, that warns “the coronavirus has made it to Mississippi."
Replacing Evidence: First of all, as of March 3, it doesn’t appear that there are
any confirmed cases in Mississippi of COVID-19, the disease caused by the coronavirus.
Claim: Says for the coronavirus, “the death rate in Texas, per capita of 29 million people,
we’re one of the lowest in the country."
Label: True
Filtered Evidence: Patrick’s statement also referenced deaths
“per capita of 29 million people" in Texas.’.
Replacing Evidence: Looking at all 50 states, the District of Columbia and Puerto Rico,
Texas is among the areas with the lowest coronavirus death rate.
Claim: Smokers are likely to be more vulnerable to COVID-19 as the act of smoking
means that fingers are in contact with lips which increases the possibility of
transmission of virus from hand to mouth.
Label: True
Filtered Evidence: Are demographics with high smoking rates more vulnerable to
Covid-19 outbreaks?
Replacing Evidence: However, from their published data we can calculate that the
smokers were 1.4 times more likely (rr=1.4, 95% ci: 0.98-2.00) to have severe
symptoms of COVID-19 and approximately 2.4 times more likely to be admitted to an
icu, need mechanical ventilation or die compared to non-smokers (rr=2.4, 95% ci: 1.43-4.04).’
Table B: Illustration of filtered evidence and following replacing evidence through evidence filtering step.

2

C

Performance Comparisons for LIAR-Politifact

Author / Model Name

Evidence

Meta-data

Base Model

Binary Class

Six Classes

Kirilin
Alhindi
Alhindi(Meta)
LiarPlus (Ours)
LiarPlusMeta (Ours)

no
yes
yes
yes
yes

yes
no
yes
no
yes

LSTM
BiLSTM
BiLSTIM
BERT
BERT

67.0%
70.0%
75.1%
79.4%

45.7%
35.0%
36.0%
48.1%
49.8%

Table C: The results for LIAR-Politifact for both binary class setup and six classes setup.

3

