Rapid Domain Adaptation for Machine Translation with Monolingual
Data
Mahdis Mahdieh, Mia X. Chen, Yuan Cao, Orhan Firat
Google Research
{mahdis, miachen, yuancao, orhanf}@google.com

arXiv:2010.12652v1 [cs.CL] 23 Oct 2020

Abstract
One challenge of machine translation is how
to quickly adapt to unseen domains in face of
surging events like COVID-19, in which case
timely and accurate translation of in-domain
information into multiple languages is critical
but little parallel data is available yet. In this
paper, we propose an approach that enables
rapid domain adaptation from the perspective
of unsupervised translation. Our proposed approach only requires in-domain monolingual
data and can be quickly applied to a preexisting translation system trained on general domain, reaching significant gains on in-domain
translation quality with little or no drop on
general-domain. We also propose an effective
procedure of simultaneous adaptation for multiple domains and languages. To the best of our
knowledge, this is the first attempt that aims
to address unsupervised multilingual domain
adaptation.

1

Introduction

COVID-19 is an unexpected world-wide major
event that hit almost all aspects of human life.
Facing such an unprecedented pandemic, how to
timely and accurately communicate and share latest
authoritative information and medical knowledge
across the world in multiple languages is critical to
the well-being of the human society. This naturally
raises a question of how an existing translation
system, usually trained on data from general domains, can rapidly adapt to emerging domains like
COVID-19, before any parallel training data is yet
available.
Domain adaptation is one of the traditional research topics for machine translation for which a
lot of proposals have been made (Chu and Wang,
2018). Nevertheless most of them are not suitable
for the purpose of rapid adaptation to emerging
events. A large body of the existing adaptation approaches are supervised, requiring time-consuming

data collection procedure, and while there has been
some recent progress made in unsupervised domain
adaptation (for example (Jin et al., 2020; Dou et al.,
2019, 2020; Hu et al., 2019)), they are not designed
specifically to fulfil the requirement of rapidity in
domain adaptation, often involving costly algorithmic steps like lexicon induction, pseudo-sample
selection, or building models from scratch etc.
In this paper, we propose a novel approach for
rapid domain adaptation for NMT, with the goal
of enabling the development and deployment of a
domain-adapted model as quickly a possible. For
this purpose, we keep the following principles in
mind when designing the procedure:
Simplicity: The procedure should be as simple as
possible, requiring only in-domain monolingual
data and avoiding excessive auxiliary algorithmic
steps as much as possible.
Scalability: The procedure should be easy to scale
up for multiple languages and multiple domains
simultaneously.
Quality: The adapted model should not sacrifice
its quality on general domains for the improvement
on new domains.
Our approach casts domain adaptation as an unsupervised translation problem, and organically
integrates unsupervised NMT techniques with a
pre-existing model trained on general domain.
Specifically, we engage MASS (Song et al., 2019),
an effective unsupervised MT procedure, for the
purpose of inducing translations from in-domain
monolingual data. It is mingled with supervised
general-domain training to form a composite objective in a continual learning setup.
We demonstrate the efficacy of our approach
on multiple adaptation tasks including COVID-

19 (Anastasopoulos et al., 2020), OPUS medical (Tiedemann, 2012) as well as an in-house
sports/travel adaptation challenge. What is more,
we show that this procedure can be effectively extended to multiple languages and domains simultaneously, and to the best of our knowledge, this is
the first attempt of unsupervised domain adaptation
for multilingual MT.

2
2.1

Background
Unsupervised machine translation

One of the most intriguing research topics in MT
is how to enable translation without the presence
of parallel data, for which the collection process
is costly. Throughout the history of MT research,
many approaches for unsupervised MT have been
proposed, but it is not until recent years that significant progress has been made on this topic (Artetxe
et al., 2018; Lample et al., 2018a,b; Conneau and
Lample, 2019; Artetxe et al., 2019; Song et al.,
2019; Liu et al., 2020; Zhu et al., 2020), together
with the rapid advancement in neural translation
models. For example, the BLEU score on WMT14
English-French improved from 15 (Artetxe et al.,
2018) to 38 (Liu et al., 2020) within just two years.
The approach we propose in this paper, to be detailed in Sec 3.1, engages unsupervised MT methods for the purpose of domain adaptation. The specific technique we focus on is named MASS (Song
et al., 2019), for which we give a brief account as
follows. In a nutshell, MASS is an encoder-decoder
version of the popular BERT (Devlin et al., 2019)
pre-training procedure, in which blocks of the encoder inputs are masked, and are forced to be predicted on the decoder side with only the remaining
context available. This procedure is done for monolingual data from both source and target languages,
which forces the representation learned for both
languages through this denoising auto-encoding
process to live in the same space. As a result, even
with monolingual inputs, at the end of the MASS
training procedure the model’s translation ability
already starts to emerge. To further boost the translation quality, it is a common practice to continue
the training process with online back-translation,
which translates target inputs back into source side
to form pseudo parallel data to guide model training.
Overall, the algorithm of MASS is simple and
elegant while demonstrating superior performance
almost comparable to supervised approaches. It

naturally fits the encoder-decoder framework and
can be easily extended for rapid continual domain
adaptation scenario. We therefore adopt this approach as the backbone of our proposed method.
2.2

Domain adaptation for Machine
Translation

When directly applying an existing NMT system to
translation tasks for emerging events like COVID19, the results often contain numerous errors as the
model was never trained on data from this novel
domain. The challenging part of this adaptation
scenario is that at the beginning of such events, no
in-domain parallel corpus is available yet but the
NMT system is required to respond properly in
time. Therefore an unsupervised and rapid adaptation procedure needs to be in place to fulfil such
requirements.
Although domain adaptation has been a traditional research area of MT, most of the existing
approaches assume the availability of parallel indomain data (Freitag and Al-Onaizan, 2016; Wang
et al., 2017; Zhang et al., 2019; Thompson et al.,
2019; Saunders et al., 2019; Li et al., 2020). While
there are also approaches that require only monolingual data (Farajian et al., 2017; Hu et al., 2019; Dou
et al., 2019; Jin et al., 2020),, their adaptation procedures are often heavy-weight (for example training
data selection, or retrain model from scratch) and
not suitable for the purpose of rapid adaptation.
What is more, existing approaches usually only
consider adaptation towards a single domain for
a single language pair. How to rapidly adapt to
multiple domains across multiple language pairs
remains an under-explored topic.
To address the aforementioned problems, we develop a light-weight, unsupervised continual adaptation procedure that effectively handles multiple
domains and languages simultaneously. We now
detail our methodology in the following section.

3
3.1

Proposed Approach
Training Procedure Configuration

We treat unsupervised domain adaptation as unsupervised learning of a new language and leverage
MASS, introduced in Sec2.1, as a central building
block in our procedure. In order to find out the
most suitable way for domain adaptation tasks, we
start by investigating different training procedure
configurations outlined in Fig 1. Our training procedures consist of three main components that can

under-explored topic. We show that our approaches
outlined in Sec. 3.1 can be easily extended to multilingual settings.
3.3

Figure 1: Different configurations of the training procedure. Components in light orange, dark orange and
green colors are trained with general monolingual data,
in-domain monolingual data and general parallel data
respectively.

Almost all existing work focus on adapting an existing model to one single domain. We explore novel
setups where the model is adapted to multiple domains in an unsupervised manner. This provides
an insight into the model’s ability of retaining previously acquired knowledge while absorbing new
information.
With a given general model G, trained using the
first two steps of the S4 training procedure, we
explore three different setups to adapt G to two
new domains A and B:
1. G → Domain A → Domain B
2. G → Domain B → Domain A
3. G → {Domain A, Domain B}

be trained sequentially or jointly:
1. Supervised training with general parallel data.
2. MASS pre-training on monolingual data.
3. Online back-translation using monolingual
data.
The monolingual data used for training these
components can be either general or in-domain
data. Components trained using in-domain data are
represented with dark orange color in Fig 1.
In this paper, we focus on the S4 configuration
as it achieves the highest quality improvement on
the adapted domain. Also it provides faster domain adaptation compared to other approaches as
it only requires in-domain data in the last step of
the training process. In section 4.3, we compare
these approaches in more details.
S4 consists of three training steps as shown in
Fig 1. The first two steps rely on general parallel
and monolingual data, while the third step makes
use of in-domain monolingual data. This final step
allows us to adapt the model to a new domain
rapidly while not suffering from quality loss on
the general domain.
3.2

Multilingual Domain Adaptation

It has become common for a neural machine translation system to handle multiple languages simultaneously. However, efficiently adapting a multilingual translation model to new domains is still an

Multi-domain Adaptation

Each → indicates an adaptation process by
jointly training on general parallel data and domain
monolingual data based on the third step of the S4
configuration.

4
4.1

Experiments
Setup

We conduct our experiments on OPUS (Tiedemann, 2012) (law and medical domains), COVID19 (Anastasopoulos et al., 2020) as well as an inhouse dataset in sports/travel domain. For OPUS
and COVID-19 experiments, the general-domain
parallel and monolingual data comes from WMT,
the same corpus as in (Siddhant et al., 2020). Detailed dataset statistics can be found in Table 1 and
Table 2. Our in-house datasets are collected from
the web. The general-domain parallel data sizes
range from 130M to 800M and the sports/traveldomain monolingual data sizes are between 13K
and 2M.
We evaluate our approaches with both bilingual
and multilingual tasks on each dataset. For OPUS
medical and law domains, the bilingual tasks are
en→de, en→fr, en→ro and the multilingual task is
en→{de, fr, ro}. For COVID-19, they are en→fr,
en→es, en→zh and en→{fr, es, zh}. For the inhouse sports/travel domain data, we report results
on zh→ja and a 12-language pair ({en, ja, zh,
ko}→{en, ja, zh, ko}) multilingual model setup.

Language Pair
en-de
en-fr
en-ro

General
Train
Dev
4508785
3000
40449146 3000
610320
1999

Test
3003
3003
1999

Med Domain
Train
Dev
Test
1104752 2000 2000
1088568 2000 2000
990499
2000 2000

Law Domain
Train
Dev
Test
715372 2000 2000
810167 2000 2000
451171 2000 2000

(a) OPUS
Language Pair
en-fr
en-es
en-zh

General
Train
Dev
40449146 3000
15182374 3004
25986436 3981

Test
3003
3000
2000

Domain
Train
Dev Test
885606 971 2100
879926 971 2100
450507 971 2100

(b) COVID-19

Table 1: Statistics of parallel data.

Language
en
fr
de
ro

# Samples
Med
Law
1088568 810167
1088568 810167
1104752 715372
990499
451171
(a) OPUS

Language
en
es
fr
zh

# Samples
2315190
879926
885606
450507

(b) COVID-19

Table 2: Statistics of in-domain monolingual data.

All the experiments are performed with the
Transformer architecture (Vaswani et al., 2017) using the Tensorflow-Lingvo implementation (Shen
et al., 2019). We use the Transformer Big (Chen
et al., 2018) model with 375M parameters and
shared source-target SentencePiece model (SPM)
(Kudo and Richardson, 2018) with a vocabulary
size of 32k.
4.2

Results

Baselines We compare the results of our proposed unsupervised domain adaptation approach
with the corresponding bilingual and multilingual
models trained only with general-domain parallel
data, without any adaptation. For datasets that
have in-domain parallel data available, such as
OPUS and COVID-19, we also compare our performance against supervised domain adaptation results, which are produced by experimenting with
both continued and simultaneous training using different mixing strategies of in-domain and general
parallel data and selecting the best results for each
task. In all cases, we report BLEU scores on both
general and in-domain test sets.

Single-domain adaptation Our bilingual results
are shown in Table 3. Compared with the unadapted baseline models, our unsupervised approach achieves significant quality gain on the indomain test sets with almost always no quality loss
on the general test sets (i.e. learning without forgetting). This improvement is consistent across
all three datasets and all languages, with BLEU
gains of +13 to +24 on OPUS medical domain,
+8 to +15 on OPUS law domain (with the exception of en-fr), +2.3 to +2.8 on COVID-19 and +3.5
on sports/travel domain. Moreover, our method
is able to almost match or even surpass the best
supervised adaptation performance on a few tasks
(e.g., COVID-19 en-fr, en-es, en-zh, OPUS medical
en-fr, OPUS law en-ro).
Table 4 and Figure 2 show our multilingual results. We can see that our approach can be effectively extended to multilingual models. There
is large quality improvement across all supported
language pairs on the adapted new domains while
there is almost no quality regression on the general
domains. The improvement ranges from +5 to +9
on OPUS medical domain, +3 to +10 on OPUS law
domain, +0.4 to +2.3 on COVID-19 and up to +3
BLEU on sports/travel domain.
Multi-domain adaptation We demonstrate our
multi-domain adaption approaches with a twodomain setup on OPUS medical and law domains.
We report the results of the three different setups
described in Section 3.3 for both bilingual and multilingual scenarios, shown in Table 5 and Table 6
respectively.
Our results suggest that the two-domain simultaneous adaptation approach is able to match the quality of individual single-domain adaptation, with a
gap of less than 1.5 BLEU points on both domains
and all language pairs for the bilingual models. For

Domain
Med
Law

en-de
32.4 (28.2)
45.8 (28.8)
53.5 (28.2)
43.1 (28.2)
58.2 (29.0)
67.9 (28.1)

en-fr
40.4 (38.8)
60.1 (37.2)
63.9 (38.5)
60.9 (38.8)
59.5 (38.6)
60.0 (38.7)

en-ro
25.8 (26.8)
50.1 (29.3)
59.7 (27.1)
35.7 (26.8)
43.9 (29.6)
45.1 (27.4)

(a)
Domain
COVID-19

en-fr
33.5 (38.8)
35.8 (38.5)
36.3 (38.5)

en-es
43.8 (33.7)
46.1 (33.6)
47.9 (33.8)

en-zh
38.2 (28.3)
41.0 (28.7)
37.8 (28.2)

(b)
Domain
Sports/Travel

zh-ja
21.5 (17.8)
25.0 (17.7)

(c)

Table 3: Bilingual, single-domain adaptation results.
Table (a), (b), (c) correspond to OPUS medical/law,
COVID-19 and in-house sports/travel domain respectively. For Table (a) and (b), each domain contains
three rows. The first row represents the baseline model
trained with general-domain parallel data without adaptation. The second row is our proposed unsupervised
adaptation approach. The third row shows the supervised domain adaptation baseline, serving as upperbound. For sports/travel domain, we do not report supervised adaption results due to lack of in-domain parallel data. In each table cell, the numbers outside and
inside parentheses stand for the BLEU scores on the indomain test set and on the general test set respectively.

the multilingual model, our two-domain adaptation approach matches or outperforms the singledomain adaptation method on the medical domain,
while there is a gap of between 0.9 and 4.1 BLEU
points on the law domain. Since multi-domain
adaptation with a multilingual model requires joint
training with both general and in-domain data from
all supported language, data mixing/sampling strategy becomes more important in order to achieve
balanced quality improvement across multiple domains as well as multiple language pairs.
We further observed that among the three multidomain adaptation setups, simultaneous adaptation
to all domains is the most effective approach. In
the sequential setups, there is almost always certain
quality regression on the previous domain when
the model is being adapted to the second domain.
4.3

Comparison of Training Procedure
Configurations

In this section, we compare the different training
procedure configurations described in Section 3.1

Figure 2: BLEU diff on general and sports/travel domain test sets for multilingual single-domain adaptation.

on the in-house zh→ja task in sports/travel domain.
Table 7 shows the best results we were able to obtain for each configuration after experimenting with
different data sampling ratios and training parameters. Our main observations are the following:
• Comparing with the baseline model, initializing the supervised training stage with a model
pretrained using domain monolingual data either with MASS (S1) or both MASS and online back-translation (S2) can result in slight
quality improvement (less than 1 BLEU) on
the adapted domain.
• Comparing {S1, S2} vs. {S3, S4, S5, S6},
joint MASS, online back-translation and supervised training (with both parallel and
monolingual data) always seems more effective in boosting the model quality on the
adapted domain than purely pipe-lined procedures.
• It is always helpful to initialize the joint training phase with pretrained models (e.g., S3,
S4, S5). Otherwise, it can be hard to find the
right sampling ratios among MASS, online
back-translation and supervised tasks during
a single training process so that the model can
improve towards the adapted domain while
not having any quality regression on the general domain.
• Among all the pretraining procedures, it is
better to include both MASS and supervised
training phases, instead of only supervised
training. This way the model would be able to
also pick up the language-dependent compo-

Domain
Med
Law

en-de
33.7 (28.4)
40.5 (28.6)
56.2 (28.4)
43.9 (28.4)
54.6 (28.7)
73.4 (28.2)

en-fr
40.0 (37.8)
45.8 (38.1)
63.8 (37.3)
57.1 (37.8)
64.3 (38)
39.3 (36.9)

en-ro
33.6 (30.4)
42.9 (31.0)
60.5 (30.7)
40.9 (30.4)
43.6 (30.8)
46.9 (30.4)

Pair

(a)
Domain
COVID-19

en-fr
33.1 (38.0)
33.5 (38.1)
33.8 (36.6)

en-es
44.3 (34.4)
45.8 (34.4)
46.5 (33.6)

en-zh
36.0 (26.5)
38.3 (28.4)
38.2 (28.1)

en-de

en-fr

(b)
Domain
Sports/Travel

Sports/Travel

Sports/Travel

Sports/Travel

en-ja
23.4 (23.6)
23.5 (23.7)
ja-en
22.5 (32.3)
22.6 (31.8)
ko-en
22.4 (28.8)
23.5 (28.4)
zh-en
21.0 (28.0)
21.2 (27.7)

en-ko
36.9 (40.4)
37.1 (40.0)
ja-ko
59.7 (36.1)
60.9 (35.8)
ko-ja
45.4 (19.9)
47 (19.8)
zh-ja
21.0 (17.7)
23.2 (17.9)

en-zh
30.5 (31.3)
29.6 (31.1)
ja-zh
27.9 (26.8)
29.5 (26.7)
ko-zh
29.8 (25.9)
31.9 (26.2)
zh-ko
29.4 (34.7)
32.2 (34.8)

(c)

Table 4: Multilingual, single-domain adaptation results.
Meaning of rows are the same as Table 3, except that
the models are trained and adapted with multilingual
setup.

nents inside the architecture during pretraining, which is beneficial for the subsequent
joint training phase. Overall, we find that S4
is our most preferable setup. It also offers the
advantage of “rapid” adaptation, as the MASS
and supervised training phases only require
general-domain data, thus can be prepared in
advance.

5

Related Work

Domain adaptation is an active topic for MT
research (Chu and Wang, 2018) and has been
considered as one of the major challenges for
NMT (Koehn and Knowles, 2017), especially when
no or little in-domain parallel data is available.
Perhaps mostly related to our work is (Jin et al.,
2020), which also relies on denoising autoencoder,
iterative back-translation as well as supervision
from general domain data for unsupervised domain
adaptation. Our work differs from theirs in the
following ways: First of all, our work is motivated by rapid adaptation from existing models
via continual learning, whereas their work builds

en-ro

Order
single-domain
Med→ Law
Law→Med
{Med, Law}
single-domain
Med→ Law
Law→Med
{Med, Law}
single-domain
Med→ Law
Law→Med
{Med, Law}

Med
45.8
38.0
45.4
44.8
60.1
44.0
59.1
58.7
50.1
38.2
49.8
48.9

Law
58.2
57.6
46.1
57.4
59.5
66.7
65.6
65.6
43.9
43.4
41.3
43.5

Table 5: Results of bilingual, two-domain adaptation
results. “Order” represents the order of the two domains we adapt to during joint training stage, same as
the three setups described in Section 3.3. Specifically,
X → Y indicates adapting to domain X first, then continually adapting to domain Y ; {X, Y } means adapting
to domains X and Y simultaneously. “single-domain”
shows the results of single-domain adaption to each domain as reported in Table 3 (a).

Pair
en-de

en-fr

en-ro

Order
single-domain
Med→ Law
Law→Med
{Med, Law}
single-domain
Med→ Law
Law→Med
{Med, Law}
single-domain
Med→ Law
Law→Med
{Med, Law}

Med
40.5
37.4
39.9
41
45.8
43.7
45.3
46.2
42.9
40.4
41.8
42.7

Law
54.6
52.7
51.5
50.5
64.3
62.6
62.2
60.6
43.6
43.1
42.7
42.7

Table 6: Results of multilingual, two-domain adaptation results. Meaning of each cell is the same as Table 5.
The single-domain results are from the corresponding
multilingual model as in Table 4 (a).

Configuration
Baseline
S1
S2
S3
S4
S5
S6

test BLEU
21.5 (17.8)
22.3 (17.6)
22.3 (17.5)
23.3 (16.4)
25.0 (17.7)
23.7 (17.5)
22.8 (16.4)

Table 7: Results of the different configurations of the
training process on the in-house sports/travel zh→ja
dataset.

in-domain model from scratch, therefore we pay
close attention to the prevention of catastrophic
forgetting. What is more, we also investigate the
problems of simultaneous unsupervised domain
adaptation across multiple languages and domains,
topics rarely studied before.
While our work is inspired by recent progress
made in unsupervised MT, other approaches of
using monolingual data for domain adaptation exist.
(Dou et al., 2020) presents an approach that wisely
select examples from general domain data that are
representative of target domain and simple enough
for back-translation. (Dou et al., 2019) propose to
use both in- and out-of-domain monolingual data to
learn domain-specific features which allow model
to specify domain-specific representations of words
and sentences. (Hu et al., 2019) creates pseudoparallel training data via lexicon induction from
both general-domain parallel data and in-domain
monolingual data. (Farajian et al., 2017) adapts to
any in-domain inputs by selecting a subset of outof-domain training samples mostly similar to new
inputs, then fine-tune the model on this specific
subset only for the adaption to the new inputs.
Besides unsupervised domain adaptation, traditionally many approaches have been proposed for
supervised domain adaptation. For example model
ensembling between in- and out-of-domain models (Freitag and Al-Onaizan, 2016; Saunders et al.,
2019), applying regularization that prevents catastrophic forgetting (Thompson et al., 2019), training data selection based on in- and out-of-domain
sample similarity (Wang et al., 2017; Zhang et al.,
2019), meta-learning for domain-specific model
parameters (Li et al., 2020).
We also note that our approach is tightly related
to techniques for improving NMT quality for lowresource language pairs by making use of monolingual data. For example (Siddhant et al., 2020)
proposed an approach of improving low-resource
translation quality by mingling MASS objective

on monolingual data with supervised objectives
for high-resource languages during training, and
observed significant gains.

6

Conclusion

We presented an unsupervised rapid domain adaptation approach for machine translation inspired
by unsupervised NMT techniques. Our approach
continually adapts an existing model to novel domains using only monolingual data based on a
MASS-inspired procedure, which is shown to have
significantly boosted quality for unseen domains
without quality drop on existing ones. We further
demonstrate that this approach is flexible enough to
accommodate multiple domains and languages simultaneously with almost equal efficacy. While the
problems of domain adaptation, unsupervised and
multilingual translation are usually treated as separate research topics, indeed the boundaries between
them can be blurred so that a unified procedure can
serve all purposes, as our study finds.

References
Antonios Anastasopoulos, Alessandro Cattelan, ZiYi Dou, Marcello Federico, Christian Federman,
Dmitriy Genzel, Francisco Guzmán, Junjie Hu,
Macduff Hughes, Philipp Koehn, Rosie Lazar,
Will Lewis, Graham Neubig, Mengmeng Niu, Alp
Öktem, Eric Paquin, Grace Tang, and Sylwia Tur.
2020. Tico-19: the translation initiative for covid19.
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2019.
An effective approach to unsupervised machine
translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 194–203, Florence, Italy. Association for
Computational Linguistics.
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and
Kyunghyun Cho. 2018. Unsupervised neural machine translation. In International Conference on
Learning Representations.
Mia X. Chen, Orhan Firat, Ankur Bapna, Melvin
Johnson, Wolfgang Macherey, George Foster, Llion
Jones, Mike Schuster, Noam Shazeer, Niki Parmar,
Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser,
Zhifeng Chen, Yonghui Wu, and Macduff Hughes.
2018. The best of both worlds: Combining recent
advances in neural machine translation. In Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), pages 76–86, Melbourne, Australia. Association for Computational Linguistics.
Chenhui Chu and Rui Wang. 2018. A survey of domain adaptation for neural machine translation. In

Proceedings of the 27th International Conference on
Computational Linguistics, pages 1304–1319, Santa
Fe, New Mexico, USA. Association for Computational Linguistics.
Alexis Conneau and Guillaume Lample. 2019. Crosslingual language model pretraining. In Advances
in Neural Information Processing Systems 32, pages
7059–7069. Curran Associates, Inc.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.
Zi-Yi Dou, Antonios Anastasopoulos, and Graham
Neubig. 2020. Dynamic data selection and weighting for iterative back-translation.
Zi-Yi Dou, Junjie Hu, Antonios Anastasopoulos, and
Graham Neubig. 2019. Unsupervised domain adaptation for neural machine translation with domainaware feature embeddings. In Proceedings of the
2019 Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 1417–1422. Association
for Computational Linguistics.

Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations, pages 66–71.
Guillaume Lample, Alexis Conneau, Ludovic Denoyer,
and Marc’Aurelio Ranzato. 2018a. Unsupervised
machine translation using monolingual corpora only.
In International Conference on Learning Representations.
Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. 2018b.
Phrase-based & neural unsupervised machine translation. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing,
pages 5039–5049, Brussels, Belgium. Association
for Computational Linguistics.
Rumeng Li, X. Wang, and Hong Yu. 2020. Metamt,
a metalearning method leveraging multiple domain
data for low resource machine translation. In AAAI.
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey
Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020. Multilingual denoising
pre-training for neural machine translation.
Danielle Saunders, Felix Stahlberg, Adrià de Gispert,
and Bill Byrne. 2019. Domain adaptive inference
for neural machine translation. In Proceedings of
the 57th Annual Meeting of the Association for Computational Linguistics, pages 222–228, Florence,
Italy. Association for Computational Linguistics.

M. Amin Farajian, Marco Turchi, Matteo Negri, and
Marcello Federico. 2017. Multi-domain neural
machine translation through unsupervised adaptation. In Proceedings of the Second Conference on
Machine Translation, pages 127–137, Copenhagen,
Denmark. Association for Computational Linguistics.

Jonathan Shen, Patrick Nguyen, Yonghui Wu, Zhifeng
Chen, Mia X. Chen, Ye Jia, Anjuli Kannan, Tara
Sainath, Yuan Cao, Chung-Cheng Chiu, Yanzhang
He, Jan Chorowski, and Smit Hinsu et al. 2019.
Lingvo: a modular and scalable framework for
sequence-to-sequence modeling. arXiv preprint
arXiv:1902.08295.

Markus Freitag and Yaser Al-Onaizan. 2016. Fast domain adaptation for neural machine translation.

Aditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Firat,
Mia X. Chen, Sneha Kudugunta, Naveen Arivazhagan, and Yonghui Wu. 2020. Leveraging monolingual data with self-supervision for multilingual
neural machine translation. In Proceedings of the
58th Annual Meeting of the Association for Computational Linguistics, pages 2827–2835, Online. Association for Computational Linguistics.

Junjie Hu, Mengzhou Xia, Graham Neubig, and Jaime
Carbonell. 2019. Domain adaptation of neural machine translation by lexicon induction. In Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 2989–3001.
Association for Computational Linguistics.
Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2020. A simple baseline to semisupervised domain adaptation for machine translation.
Philipp Koehn and Rebecca Knowles. 2017. Six challenges for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation, pages 28–39, Vancouver. Association for
Computational Linguistics.
Taku Kudo and John Richardson. 2018. Sentencepiece:
A simple and language independent subword tokenizer and detokenizer for neural text processing. In

Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu. 2019. MASS: Masked sequence to sequence pre-training for language generation. volume 97 of Proceedings of Machine Learning Research, pages 5926–5936, Long Beach, California,
USA. PMLR.
Brian Thompson, Jeremy Gwinnup, Huda Khayrallah,
Kevin Duh, and Philipp Koehn. 2019. Overcoming
catastrophic forgetting during domain adaptation of
neural machine translation. In Proceedings of the
2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and

Short Papers), pages 2062–2068. Association for
Computational Linguistics.
Jörg Tiedemann. 2012. Parallel data, tools and interfaces in opus. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey. European Language Resources Association (ELRA).
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Processing Systems, pages 5998–6008.
Rui Wang, Andrew Finch, Masao Utiyama, and Eiichiro Sumita. 2017. Sentence embedding for neural
machine translation domain adaptation. In Proceedings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers), pages 560–566, Vancouver, Canada. Association for Computational Linguistics.
Xuan Zhang, Pamela Shapiro, Gaurav Kumar, Paul McNamee, Marine Carpuat, and Kevin Duh. 2019. Curriculum learning for domain adaptation in neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1903–1915. Association for Computational Linguistics.
Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,
Wengang Zhou, Houqiang Li, and Tieyan Liu. 2020.
Incorporating bert into neural machine translation.
In International Conference on Learning Representations.

