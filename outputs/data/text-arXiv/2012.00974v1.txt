Extracting COVID-19 Diagnoses and Symptoms From Clinical Text:
A New Annotated Corpus and Neural Event Extraction Framework
Kevin Lybargera,∗, Mari Ostendorfb , Matthew Thompsonc , Meliha Yetisgena

arXiv:2012.00974v1 [cs.CL] 2 Dec 2020

b Department

a Biomedical & Health Informatics, University of Washington, Box 358047, Seattle, WA 98109, USA
of Electrical & Computer Engineering, University of Washington, Campus Box 352500 185, Seattle, WA 98195-2500, USA
c Department of Family Medicine, University of Washington, Box 354696, Seattle, WA 98195-2500, USA

Abstract
Coronavirus disease 2019 (COVID-19) is a global pandemic. Although much has been learned about the
novel coronavirus since its emergence, there are many open questions related to tracking its spread, describing
symptomology, predicting the severity of infection, and forecasting healthcare utilization. Free-text clinical
notes contain critical information for resolving these questions. Data-driven, automatic information extraction
models are needed to use this text-encoded information in large-scale studies. This work presents a new
clinical corpus, referred to as the COVID-19 Annotated Clinical Text (CACT) Corpus, which comprises 1,472
notes with detailed annotations characterizing COVID-19 diagnoses, testing, and clinical presentation. We
introduce a span-based event extraction model that jointly extracts all annotated phenomena, achieving high
performance in identifying COVID-19 and symptom events with associated assertion values (0.83-0.97 F1
for events and 0.73-0.79 F1 for assertions). In a secondary use application, we explored the prediction of
COVID-19 test results using structured patient data (e.g. vital signs and laboratory results) and automatically
extracted symptom information. The automatically extracted symptoms improve prediction performance,
beyond structured data alone.
Keywords: COVID-19; coronavirus; machine learning; natural language processing; information extraction

1. Introduction
As of October 11, 2020, there were over 37 million confirmed COVID-19 cases globally, resulting in 1
million related deaths [1]. Surveillance efforts to track the spread of COVID-19 and estimate the true number
of infections remains a challenge for policy makers, healthcare workers, and researchers, even as testing
availability increases. Symptom information provides useful indicators for tracking potential COVID-19
infections and disease clusters [2]. Certain symptoms and underlying comorbidities have directed COVID-19
∗ Corresponding

author
Email address: lybarger@uw.edu (Kevin Lybarger)

Preprint submitted to arXiv

December 3, 2020

testing. However, the clinical presentation of COVID-19 varies significantly in severity and symptom profiles
[3].
The most prevalent COVID-19 symptoms reported to date are fever, cough, fatigue, and dyspnea [4],
but emerging reports identify additional symptoms, including diarrhea and neurological symptoms, such as
changes in taste or smell [5–7]. Certain initial symptoms may be associated with higher risk of complications;
in one study, dyspnea was associated with a two-fold increased risk of acute respiratory distress syndrome
[8]. However, correlations between symptoms, positive tests, and rapid clinical deterioration are not well
understood in ambulatory care and emergency department settings.
Routinely collected information in the Electronic Health Record (EHR) can provide crucial COVID-19
testing, diagnosis, and symptom data needed to address these knowledge gaps. Laboratory results, vital signs,
and other structured data results can easily be queried and analyzed at scale; however, more detailed and
nuanced descriptions of COVID-19 diagnoses, exposure history, symptoms, and clinical decision-making are
typically only documented in the clinical narrative. To leverage this textual information in large-scale studies,
the salient COVID-19 and symptom information must be automatically extracted.
This work presents a new corpus of clinical text annotated for COVID-19, referred to as the COVID-19
Annotated Clinical Text (CACT) Corpus. CACT consists of 1,472 notes from the University of Washington (UW) clinical repository with detailed event-based annotations for COVID-19 diagnosis, testing, and
symptoms. To the best of our knowledge, CACT is the first clinical data set with COVID-19 annotations,
and it includes 29.9K distinct events. We present the first information extraction results on CACT using
an end-to-end neural event extraction model, establishing a strong baseline for identifying COVID-19 and
symptom events. We explore the prediction of COVID-19 test results (positive or negative) using structured
EHR data and automatically extracted symptoms and find that the automatically extracted symptoms improve
prediction performance.
2. Related Work
2.1. Annotated Corpora
Given the recent onset of COVID-19, there are limited COVID-19 corpora for natural language processing
(NLP) experimentation. Corpora of scientific papers related to COVID-19 are available [9, 10], and automatic
labels for biomedical entity types are available for some of these research papers [11]. However, we are
unaware of corpora of clinical text with supervised COVID-19 annotations.
Multiple clinical corpora are annotated for symptoms. As examples, South et al. [12] annotated symptoms
and other medical concepts with negation (present/not present), temporality, and other attributes. Koeling et al.
[13] annotated a pre-defined set of symptoms related to ovarian cancer. For the i2b2/VA challenge, Uzuner
et al. [14] annotated medical concepts, including symptoms, with assertion values and relations. While
some of these corpora may include symptom annotations relevant to COVID-19 (e.g. “cough” or “fever”),
the distribution and characterization of symptoms in these corpora may not be consistent with COVID-19

2

presentation. To fill the gap in clinical COVID-19 annotations, including symptoms, we introduce CACT to
provide a relatively large corpus with COVID-19 diagnosis, testing, and symptom annotations.
2.2. Relation and Event Extraction
There is a significant body of information extraction (IE) work related to coreference resolution, relation
extraction, and event extraction tasks. In these tasks, spans of interest are identified, and linkages between
spans are predicted. Many contemporary IE systems use end-to-end multi-layer neural models that encode
an input word sequence using recurrent or transformer layers, classify spans (entities, arguments, etc.), and
predict the relationship between spans (coreference, relation, role, etc.) [15–20]. Of most relevance to our
work is a series of developments starting with Lee et al. [21], which introduces a span-based coreference
resolution model that enumerates all spans in a word sequence, predicts entities using a feed-forward neural
network (FFNN) operating on span representations, and resolves coreferences using a FFNN operating on
entity span-pairs. Luan et al. [22] adapts this framework to entity and relation extraction, with a specific focus
on scientific literature. Luan et al. [23] extends the method to take advantage both of co-reference and relation
links in a graph-based approach to jointly predict entity spans, co-reference, and relations. By updating span
representations in multi-sentence co-reference chains, the graph-based approach achieved state-of-the-art on
several IE tasks representing a range of different genres. Wadden et al. [24] expands on Luan et al. [23]’s
approach, adapting it to event extraction tasks. We build on Luan et al. [22] and Wadden et al. [24]’s work,
augmenting the modeling framework to fit the CACT annotation scheme. In CACT, event arguments are
generally close to the associated trigger, and inter-sentence events linked by co-reference are infrequent, so
the graph-based extension, which adds complexity, is unlikely to benefit our extraction task.
Many recent NLP systems use pre-trained language models (LMs), such as ELMo, BERT, and XLNet,
that leverage unannotated text [25–27]. A variety of strategies for incorporating the LM output are used in IE
systems, including using the contextualized word embedding sequence: as the input to a Conditional Random
Field entity extraction layer [28], as the basis for building span representations [23, 24], or by adding an
entity-aware attention mechanism and pooled output states to a fully transformer-based model [29]. There are
many domain-specific LM variants. Here, we use Alsentzer et al. [30]’s Bio+Clinical BERT, which is trained
on PubMed papers and MIMIC-III [31] clinical notes, for building span representations.
2.3. COVID-19 Outcome Prediction
There are many pre-print and published works exploring the prediction of COVID-19 outcomes, including
COVID-19 infection, hospitalization, acute respiratory distress syndrome, need for intensive care unit (ICU),
need for a ventilator, and mortality [32–44]. These COVID-19 outcomes are typically predicted using existing
structured data within the EHR, including demographics, diagnosis codes, vitals, and lab results, although
Izquierdo et al. [37] incorporates automatically extracted information using the existing EHRead tool. Our
literature review identified 24 laboratory, vital sign, and demographic structured fields that are predictive
of COVID-19, including: age, alanine aminotransferase, albumin, alkaline phosphatase (ALP), aspartate

3

aminotransferase (AST), basophils, calcium, C-reactive protein (CRP), D-dimer, eosinophils, gamma-glutamyl
transferase (GGT), gender, heart rate, lactate dehydrogenase (LDH), lymphocytes, monocytes, neutrophils,
oxygen saturation, platelets, prothrombin time (PT), respiratory rate, temperature, troponin, and white
blood cell (WBC) count. Table 3 in the Appendix details of the specific publications associated with each
of these fields. While there are some frequently cited fields (e.g., age, AST, CRP, LDH, lymphyocytes,
neutrophils, and temperature), there does not appear to be a consensus across the literature regarding the
most prominent predictors of COVID-19 infection. These 24 predictive fields informed the development of
our COVID-19 prediction work in Section 5. Prediction architectures includes logistic regression, Support
Vector Machines (SVM), decision trees, random forest, K-nearest neighbors, Naı̈ve Bayes, and multilayer
perceptron [37, 38, 42–44].
3. Materials
3.1. Data
This work used inpatient and outpatient clinical notes from the UW clinical repository. COVID-19-related
notes were identified by searching for variations of the terms coronavirus, covid, sars-cov, and sars-2 in notes
authored between February 20-March 31, 2020, resulting in a pool of 92K notes. Samples were randomly
selected for annotation from a subset of 53K notes that include at least five sentences and correspond to the
note types: telephone encounters, outpatient progress, emergency department, inpatient nursing, intensive
care unit, and general inpatient medicine. Multiple note types were used to improve the extraction model
generalizability.
Early in the outbreak, the UW EHR did not include COVID-19 specific structured data; however, structured
fields indicating COVID-19 test types and results were added as testing expanded. We used these structured
fields to assign a COVID-19 Test label describing COVID-19 polymerase chain reaction (PCR) testing to each
note based on patient test status within the UW system (no data external to UW was used):
• none: patient testing information is not available
• positive: patient will have at least one future positive test
• negative: patient will only have future negative tests
More nuanced descriptions of COVID-19 testing (e.g. conditional or unordered tests) or diagnoses (e.g.
possible infection or exposure) are not available as structured data. For the 53K note subset, the COVID-19
Test label distribution is 90.8% none, 7.9% negative, and 1.3% positive.1
Given the sparsity of positive and negative notes, CACT is intentionally biased to increase the prevalence
of these labels. To ensure adequate positive training samples, the CACT training partition includes 46% none,
5% negative, and 49% positive notes. Ideally, the test set would be representative of the true distribution;
1 The COVID-19 test positivity rate cannot be inferred from these label distributions, as there can be multiple test results associated
with each note-level label.

4

however, the expected number of positive labels with random selection is insufficient to evaluate extraction
performance. Consequently, the CACT test partition was biased to include 50% none, 46% negative, and 4%
positive notes. Notes were randomly selected in equal proportions from the six note types. CACT includes
1,472 annotated notes, including 1,028 train and 444 test notes.
3.2. Annotation Scheme
Event type, e

Argument type, a

Argument subtypes, Ll

Span examples

–

“COVID,” “COVID-19”

Test Status†

{positive, negative, pending, conditional,
not ordered, not patient, indeterminate }

“tested positive”

Assertion†

{present, absent, possible, hypothetical,
not patient}

“positive,” “low suspicion”

Trigger*

–

“cough,” “shortness of breath”

Assertion*

{present, absent, possible, conditional,
hypothetical, not patient}

“admits,” “denies”

Change

{no change, worsened, improved, resolved}

“improved,” “continues”

Severity

{mild, moderate, severe}

“mild,” “required ventilation”

Anatomy

–

“chest wall,” “lower back”

Characteristics

–

“wet productive,” “diffuse”

Duration

–

“for two days,” “1 week”

Frequency

–

“occasional,” “chronic”

Trigger

*

COVID

Symptom

Table 1: Annotation guideline summary. * indicates the argument is required.
Assertion, is required

†

indicates at least one of the arguments, Test Status or

We created detailed annotation guidelines for two event types, COVID and Symptom, which are summarized in Table 1. COVID and Symptom are annotated as events, where each event includes a trigger that
identifies and anchors the event and arguments that characterize the event. For COVID events, the trigger
is generally an explicit COVID-19 reference, like “COVID-19” or “coronavirus.” Test Status characterizes
implicit and explicit references to COVID-19 testing, and Assertion captures diagnoses and hypothetical
references to COVID-19. Symptom events capture subjective, often patient reported, indications of disorders
and diseases (e.g “cough”). For Symptom events, the trigger identifies the specific symptom, for example
“wheezing” or “fever,” which is characterized through Assertion, Change, Severity, Anatomy, Characteristics,
Duration, and Frequency arguments. Symptoms were annotated for all conditions/diseases, not just COVID19. The annotation scheme includes two types of arguments: labeled arguments and span-only arguments.
Labeled arguments (e.g. Assertion) include an argument span, type, and subtype (e.g. present). Span-only
arguments, like Characteristics, include an argument span and type, without a subtype label. Notes were
annotated using the BRAT annotation tool [45]. Figure 1 presents BRAT annotation examples.
5

Figure 1: BRAT annotation examples for COVID and Symptom (SSx) event types

3.3. Annotation Scoring and Evaluation
Annotation and extraction is scored as a slot filling task, focusing on information most relevant to
secondary use applications. Figure 2 presents the same sentence annotated by two annotators, along with
the populated slots for the Symptom event. Both annotations include the same trigger and Frequency spans
(“cough” and “intermittent”, respectively). The Assertion spans differ (“presenting with” vs. “presenting”),
but the assigned subtypes (present) are the same, so the annotations are equivalent for purposes of populating
a database. Annotator agreement and extraction performance are assessed using scoring criteria that reflects
this slot filling interpretation of the labeling task.

⇓
SSx(trigger=“cough”, Assertion=present, Frequency=“intermittent”)
Figure 2: Annotation examples describing event extraction as a slot filling task

The Symptom trigger span identifies the specific symptom. For COVID, the trigger anchors the event,
although the span text is not salient to downstream applications. For labeled arguments, the subtype label
captures the most salient information, and the identified span is less informative. For span-only arguments,
the spans are not easily mapped to a fixed label set, so the selected span contains the salient information.
Performance is evaluated using precision, recall, and F1.
Trigger: Triggers, Ti , are represented by a pair (event type, ei ; token indices, xi ). Trigger equivalence is
defined as
Ti ≡ Tj if (ei ≡ ej ) ∧ (xi ≡ xj ).
Arguments: Events are aligned based on trigger equivalence. The arguments of events with equivalent
triggers are compared using different criteria for labeled arguments and span-only arguments. Labeled
arguments, Li , are represented as a triple (argument type, ai ; token indices, xi ; subtype, li ). For labeled

6

arguments, the argument type, a, and subtype, l, capture the salient information and equivalence is defined as
Li ≡ Lj if (Ti ≡ Tj ) ∧ (ai ≡ aj ) ∧ (li ≡ lj ).
Span-only arguments, Si , are represented as a pair (argument type, ai ; token indices, xi ). Span-only arguments
with equivalent triggers and argument types, (Ti ≡ Tj ) ∧ (ai ≡ aj ), are compared at the token-level (rather
than the span-level) to allow partial matches. Partial match scoring is used as partial matches can still contain
useful information.
3.4. Annotation Statistics
CACT includes 1,472 notes with a 70%/30% train/test split and 29.9K events annotated (5.4K COVID
and 24.4K Symptom). Figure 3 contains a summary of the COVID annotation statistics for the train/test
subsets. By design, the training and test sets include high rates of COVID-19 infection (present subtype for
Assertion and positive subtype for Test Status), with higher rates in the training set. CACT includes high rates
of Assertion hypothetical and possible subtypes. The hypothetical subtype applies to sentences like, “She
is mildly concerned about the coronavirus” and “She cancelled nexplanon replacement due to COVID-19.”
The possible subtype applies to sentences like, “risk of Covid exposure” and “Concern for respiratory illness

1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

conditional
indeterminate
negative
not ordered
not patient
pending
positive

train
test

absent
conditional
hypothetical
not patient
possible
present

Frequency

(including COVID-19 and influenza).” Test Status pending is also frequent.

Assertion

Test Status

Figure 3: COVID annotation summary

There is some variability in the endpoints of the annotated COVID trigger spans (e.g. “COVID” vs.
“COVID test”); however 98% of the COVID trigger spans in the training set start with the tokens “COVID,”
“COVID19,” or “coronavirus.” Since the COVID trigger span is only used to anchor and disambiguate events,
the COVID trigger spans were truncated to the first token of the annotated span in all experimentation and
results.

7

The training set includes 1,756 distinct uncased Symptom trigger spans, 1,425 of which occur fewer than
five times. Figure 4 presents the frequency of the 20 most common Symptom trigger spans in the training set
by Assertion subtypes present, absent, and other (possible, conditional, hypothetical, or not patient). The
extracted symptoms in Figure 4 were manually normalized to aggregate different extracted spans with similar
meanings (e.g. “sob” and “short of breath” → “shortness of breath”; “febrile” and “fevers” → “fever”). These
20 symptoms account for 62% of the training set Symptom events. There is ambiguity in delineating between
some symptoms and other clinical phenomena (e.g. exam findings and medical problems), which introduces
some annotation noise.

Figure 4: Most frequent symptoms in the training set broken down by Assertion subtype

Given the long tail of the symptom distribution and our desire to understand the more prominent COVID19 symptoms, we focused annotator agreement assessment and extraction model training/evaluation on the
symptoms that occurred at least 10 times in the training set, resulting in 185 distinct, unnormalized symptoms
that cover 82% of the training set Symptom events. The set of 185 symptoms was determined only using
the training set, to allow unbiased experimentation on the test set. The subsequent annotator agreement and
information extraction experimentation only incorporate these 185 most frequent symptoms.
3.5. Annotator Agreement
All annotation was performed by four UW medical students. After the first round of annotation, annotator
disagreements were carefully reviewed, the annotation guidelines were updated, and annotators received
additional training. Additionally, potential COVID triggers were pre-annotated using pattern matching
(“COVID,” “COVID-19,” “coronavirus,” etc.), to improve the recall of COVID annotations. Pre-annotated
8

COVID triggers were modified as needed by the annotators, including removing, shifting, and adding trigger
spans. Figure 5 presents the annotator agreement for the second round of annotation, which included 96
doubly annotated notes. For labeled arguments, F1 scores are micro-average across subtypes.

Figure 5: Annotator agreement

4. Event Extraction
4.1. Methods
Event extraction tasks, like ACE05 [46], typically require prediction of the following event phenomena:
• trigger span identification
• trigger type (event type) classification
• argument span identification
• argument type/role classification
The CACT annotation scheme differs from this configuration in that labeled arguments require the argument
type (e.g. Assertion) and the subtype (e.g. present, absent, etc.) to be predicted. Resolving the argument
subtypes require a classifier with additional predictive capacity.
We implement a span-based, end-to-end, multi-layer event extraction model that jointly predicts all event
phenomena, including the trigger span, event type, and argument spans, types, and subtypes. Figure 6 presents
our Span-based Event Extractor framework, which differs from prior related work in that multiple span
classifiers are used to accommodate the argument subtypes.
Each input sentence consists of tokens, X = {x1 , x2 , ...xn }, where n is the number of tokens. For each
sentence, the set of all possible spans, S = {s1 , s2 , ...sm }, is enumerated, where m is the number of spans
with token length less than or equal to M tokens. The model generates trigger and argument predictions for
each span in S and predicts the pairing between arguments and triggers to create events from individual span
predictions.
9

ψassertion(sj, sk)

Role scoring (ψd)
Span scoring (ϕc)

ϕassertion(sk)

ϕtrigger(sj)

∑

∑

Span rep. (gc,i)

Attention
bi-LSTM
BERT
She

has been short

of breath

Figure 6: Span-based Event Extractor

Input encoding: Input sentences are mapped to contextualized word embeddings using Bio+Clinical
BERT [30]. To limit computational cost, the contextualized word embeddings feed into a bi-LSTM without
fine tuning BERT (no backpropagation to BERT). The bi-LSTM has hidden size vh . The forward and
backward states, ht,f and ht,b , are concatenated to form the 1 × 2vh dimensional vector ht = [ht,f , ht,b ],
where t is the token position.
Span representation: Each span is represented as the attention weighted sum of the bi-LSTM hidden
states. Separate attention mechanisms, c, are implemented for trigger and each labeled argument, and a single
attention mechanism is implemented for all span-only arguments, c ∈ {1, 2 . . . 6} (1 for trigger, 4 for labeled
arguments, and 1 for span-only arguments). The attention score for span representation c at token position t is
calculated as
αc,t = wα,c hTt

(1)

where wα,c is a learned 1 × 2vh vector. For span representation c, span i, and token position t, the attention
weights are calculated by normalizing the attention scores as
exp(αc,t )

ac,i,t =

end(s
P i)

,

(2)

exp(αc,k )

k=start(si )

where start(si ) and end(si ) denote the start and end token indices of span si . Span representation c for span
i is calculated as the attention-weighted sum of the bi-LSTM hidden state as
end(si )

gc,i =

X

ac,i,t ht .

(3)

t=start(si )

Span prediction: Similar to the span representations, separate span classifiers, c, are implemented for
trigger and each labeled argument, and a single classifier predicts all span-only arguments, c ∈ {1, 2 . . . 6} (1
for trigger, 4 for labeled arguments, and 1 for span-only arguments). Label scores for classifier c and span i
10

are calculated as
φc (si ) = ws,c FFNNs,c (gc,i ),

(4)

where φc (si ) yields a vector of label scores of size |Lc |, FFNNs,c is a non-linear projection from size 2vh to
vs , and ws,c has size |Lc | × vs . The trigger prediction label set is Ltrigger = {null, COVID, Symptom}. Separate classifiers are used for each labeled argument (Assertion, Change, Severity, and Test Status) with label set,
Lc = {null∪Ll }, where Ll is defined in Table 1.2 For example, LSeverity = {null, mild, moderate, severe}.
A single classifier predicts all span-only arguments with label set, Lspan−only = {null, Anatomy, Characteristics,
Duration, F requency}.
Argument role prediction: The argument role layer predicts the assignment of arguments to triggers
using separate binary classifiers, d, for each labeled argument and one classifier for all span-only arguments,
d ∈ {1, 2, . . . 5} (4 for labeled arguments and 1 for span-only arguments). Argument role scores for trigger j
and argument k using argument role classifier d are calculated as
ψd (sj , sk ) = wr,d FFNNr,d ([gj , gk ])

(5)

where ψd (sj , sk ) is a vector of size 2, FFNNr,d is a non-linear projection from size 4vh to vr , and wr,d has
size 2 × vr .
Span pruning: To limit the time and space complexity of the pairwise argument role predictions, only
the top-K spans for each span classifier, c, are considered during argument role prediction. The span score is
calculated as the maximum label score in φc , excluding the null label score.
4.2. Model Configuration
The model configuration was selected using 3-fold cross validation (CV) on the training set. Table 4 in
the Appendix summarizes the selected configuration. Training loss was calculated by summing the cross
entropy across all span and argument role classifiers. Models were implemented using the Python PyTorch
module [47].
4.3. Data Representation
During initial experimentation, Symptom Assertion extraction performance was high for the absent subtype
and lower for present. The higher absent performance is primarily associated with the consistent presence
of negation cues, like “denies” or “no.” While there are affirming cues, like “reports” or “has,” the present
subtype is often implied by a lack of negation cues. For example, an entire sentence could be “Short of
breath.” To provide the Symptom Assertion span classifier with a more consistent span representation, we
substituted the Symptom trigger token indices for the Symptom Assertion token indices in each event and
found that performance improved. We extended this trigger token indices substitution approach to all labeled
2 The

assertion classifier uses the larger label set associateed with Symptom.

11

arguments (Assertion, Change, Severity, and Test Status) and found performance improved. By substituting
the trigger indices for the labeled argument indices, trigger and labeled argument prediction is roughly treated
as a multi-label classification problem, although the model is not constrained to require trigger and labeled
argument predictions to be associated with the same spans. As previously discussed, the scoring routine does
not consider the span indices of labeled arguments.
4.4. Results
Event type

COVID

Symptom

Train-CV

Argument

Test

# Gold

P

R

F1

# Gold

P

R

F1

Trigger

3,931

0.95

0.97

0.96

1,497

0.96

0.97

0.97

Assertion
Test Status

2,936
1,068

0.70
0.60

0.74
0.62

0.72
0.61

1,075
457

0.72
0.63

0.74
0.60

0.73
0.62

Trigger

13,823

0.82

0.85

0.83

5,789

0.81

0.85

0.83

Assertion
Change
Severity

13,833
739
743

0.77
0.45
0.47

0.79
0.03
0.30

0.78
0.06
0.37

5,791
341
327

0.77
0.45
0.45

0.80
0.05
0.31

0.79
0.09
0.37

3,839
3,145
3,744
801

0.76
0.59
0.62
0.64

0.59
0.26
0.44
0.39

0.66
0.36
0.51
0.48

1,959
1,441
1,344
250

0.78
0.66
0.54
0.60

0.50
0.25
0.56
0.51

0.61
0.36
0.55
0.55

Anatomy
Characteristics
Duration
Frequency

Table 2: Extraction performance

Table 2 presents the extraction performance on the training set using CV and the withheld test set.
Extraction performance is similar on the train and test sets, even though the training set has higher rates of
COVID-19 positive notes. COVID trigger extraction performance is very high (0.97 F1) and comparable to
the annotator agreement (0.95 F1). The COVID Assertion performance (0.73 F1) is higher than Test Status
performance (0.62 F1), which is likely due to the more consistent Assertion annotation. Symptom trigger
and Assertion extraction performance is high (0.83 F1 and 0.79 F1, respectively), approaching the annotator
agreement (0.86 F1 and 0.83 F1, respectively). Anatomy extraction performance (0.61 F1) is lower than
expected, given the high annotator agreement (0.81 F1). Duration extraction performance is comparable to
annotator agreement, and Frequency extraction performance is lower than annotation agreement. Change,
Severity, and Characteristics extraction performance is low, again likely related to low annotator agreement
for these cases.
5. COVID-19 Prediction
This section explores the prediction of COVID-19 test results, utilizing structured EHR data and automatically extracted symptom information from clinical notes.
12

5.1. Data
An existing clinical data set from the UW from January 2020 through May 2020 was used to explore the
prediction of COVID-19 test results and identify the most prominent predictors of COVID-19. The data set
represents 230K patients, including 28K patients with at least one COVID-19 PCR test result. The data set
includes telephone encounters, outpatient progress notes, and emergency department (ED) notes, as well as
structured data (demographics, vitals, laboratory results, etc.).
For each patient in this data set, all of the COVID-19 tests with either a positive or negative result and at
least one note within the seven days preceding the test result were identified. COVID-19 tests without a note
within the previous seven days were not included in experimentation, to provide a fair comparison for the
predictive power of the structured and automatically extracted data. Each of these test results was treated as a
sample in this binary classification task (positive or negative). The likelihood of COVID-19 positivity was
predicted using structured EHR data and notes within a 7-day window preceding the test result. The pairing
of notes and COVID-19 test results was independently performed for each of the note types (ED, outpatient
progress, and telephone encounter notes). From this pool of data, we identified the following negative and
positive test counts by note type: 2,226 negative and 148 positive for ED; 7,599 negative and 381 positive
for progress; and 7,374 negative and 448 positive for telephone. Within the 7-day window of this subset of
COVID-19 test results, there are 5.3K ED, 14.5K progress, and 27.5K telephone notes. This data set has some
overlap with the data set used in Section 4.1 but is treated as a separate data set in this COVID-19 prediction
task. The notes in the CACT training set are less than 1% of the notes used in this secondary use application.
5.2. Methods
Features: Symptom information was automatically extracted from the notes using the Span-based Event
Extractor trained on CACT.3 The extracted symptoms were manually normalized to aggregate different
extracted spans with similar meanings. Each extracted symptom with an Assertion value of “present” was
assigned a feature value of 1. The 24 identified predictors of COVID-19 from existing literature (see Section
2) were mapped to 32 distinct fields within the UW EHR and used in experimentation. Identified fields are
listed in Table 5 of the Appendix. For the coded data (e.g. structured fields like “basophils”), experimentation
was limited to this subset of literature-supported COVID-19 predictors, given the limited number of positive
COVID-19 tests in this data set.
Within the 7-day history, features may occur multiple times (e.g. multiple temperature measurements).
For each feature, the series of values was represented as the minimum or maximum of the values depending
on the specific feature. For example, temperature was represented as the maximum of the measurements to
detect any fever, and oxygen saturation was represented as the minimum of the values to capture any low
oxygenation events. Table 5 in the Appendix includes the aggregating function, f , used for each structured
field.
3 Only

automatically extracted symptom data were used. No supervised (hand annotated) labels were used.

13

Where symptom features were missing, the feature value was set to 0. For features from the structured
EHR data, which are predominantly numerical, missing features were assigned the mean feature value in the
set used to train the COVID-19 prediction model.
Model: COVID-19 was predicted using the Random Forest framework, because it facilitates nonlinear
modeling with interdependent features and interpretability analyses (Scikit-learn Python implementation
used [48]). Alternative prediction algorithms considered include Logistic Regression, SVM, and FFNN.
Logistic Regression assumes feature independence and linearity, which is not valid for this task. For example,
the feature set includes both the symptom “fever” and temperature measurements (e.g. “38.6◦ C”). Model
interpretability is less clear with SVM, and the number of positive test samples is relatively small for a FFNN.
The relative importance of features in predicting COVID-19 was explored using Lundberg et al. [49]’s
SHAP (SHapley Additive exPlanations) approach, which is implemented in the SHAP Python module.4
SHAP generates interpretable, feature-level explanations for nonlinear model predictions. For each prediction,
SHAP scores are estimated for each feature, where larger absolute scores indicate more important features,
and the absolute values of the scores sum to 1.0 for each prediction.
Experimental paradigm: The available data was split into train/test sets using an 80%/20% split by
patient, although training and evaluation was performed at the test-level (i.e. each COVID-19 test result is a
sample). Performance was evaluated using the receiver operating characteristic (ROC) and the associated
area under the curve (AUC). Given the relatively small number of positive samples, the train/test splits were
randomly created 1,000 times through repeated hold-out testing [50]. Kim [50] demonstrated that repeated
hold-out testing can improve the robustness of the results in low resource settings. For each train/test split, the
AUC was calculated, and an average AUC was calculated across all random hold-out iterations. The random
holdout iterations yield a distribution of AUC values, which facilitate significance testing. The significance
of the AUC performance was assessed using a two-sided T-test. The Random Forest models were tuned
using 3-fold cross validation on the training set and evaluated on the withheld test set. COVID-19 prediction
experimentation included three feature sets: structured (32 structured EHR fields), notes (automatically
extracted symptoms), and all (combination of structured fields and automatically extracted symptoms).
Separate models were trained and evaluated for each note type (ED, progress, and telephone) and feature set
(structured, notes, and all). The selected Random Forest hyperparameters are summarized in Table 6 in the
Appendix.
5.3. Results
Figure 7 presents the ROC for the COVID-19 predictors with the average AUC across repeated hold-out
partitions. For all note types, the inclusion of the automatically extracted symptom information (all feature
set) improves the AUC over structured data only (structured-only feature set) with significance (p < 0.001 per
two-sided T-test). The structured features achieve higher performance in the ED note experimentation, than
4 https://pypi.org/project/shap/

14

Figure 7: Receiver operating characteristic by note type and feature set combination for repeated hold-out iterations. The solid line
indicates the average ROC, and the shaded region around the solid line indicates one standard deviation.

experimentation with progress and telephone notes, due to the higher prevalence of vital sign measurements
and laboratory testing in proximity to ED visits. In ED note experimentation, over 99% of samples include
vital signs and 72% include blood work. In progress and telephone note experimentation, 23-38% of samples
includes vital signs and 19-26% include blood work. The automatically extracted symptoms are especially
important in clinical contexts, like outpatient and tele-visit, where vital signs, laboratory results, and other
structured data are less available.
Figure 8 presents a SHAP value plot for the five most predictive features from a single Random Forest
model from the ED note experimentation with the all feature set. In this SHAP plot, each point represents a
single test prediction, and the SHAP value (x-axis) describes the feature importance. Positive SHAP values
indicate support for COVID-19 positivity, and negative values indicate support for negative test result. The
color coding indicates the feature value, where red indicates higher feature values and blue indicates lower
feature values. For example, high and moderate basophils values (coded in red and purple, respectively) have
negative SHAP values, indicating support COVID-19 negativity. Low basophils values (coded in blue) have
positive SHAP values, indicating support COVID-19 positivity.
Given the relatively small sample size and low proportion of positive COVID-19 tests, the SHAP impact
values presented in Figure 8 were aggregated across repeated hold-out runs. Figure 9 presents the averaged
SHAP values for each repeated hold-out run for the eight most predictive features for the all feature set. For

15

Figure 8: SHAP plot for a single Random Forest model from the ED note experimentation with the all feature set, explaining the
importance of features in making predictions for the withheld test set. * indicates the feature is an automatically extracted symptom

each repeated hold-out run, the absolute value of the SHAP values were averaged, yielding a single feature
score per repetition. The mean SHAP values (x-axis) represents the importance of the feature with predicting
COVID-19, where positive values indicate a positive correlation between the feature values and COVID-19
positivity and negative values indicate a negative correlation. The most predictive features vary by note
type, although fever is a prominent indicator of COVID-19 across note types. For each note type, the top
five symptoms indicating COVID-19 positivity include: ED - fever, cough, myalgia, fatigue, and flu-like
symptoms; progress - fever, myalgia, respiratory symptoms, cough, and ill; and telephone - fever, cough,
myalgia, fatigue, and sore throat. The differences in symptom importance by note type reflects differences in
documentation in the clinical settings (e.g., emergency department, outpatient, and tele-visit).
6. Conclusions
We present CACT, a novel corpus with detailed annotations for COVID-19 diagnoses, testing, and
symptoms. CACT includes 1,472 unique notes across six note types with more than 500 notes from patients
with future COVID-19 testing. We implement the Span-based Event Extractor, which jointly extracts all
annotated phenomena, including argument types and subtypes. The Span-based Event Extractor achieves
near-human performance high performance in the extraction of COVID triggers (0.97 F1) and Symptom
triggers (0.83 F1) and Assertions (0.79 F1). In a COVID-19 prediction task, automatically extracted symptom
information improved the prediction of COVID-19 test results (with significance) beyond just using structured
data, and top predictive symptoms include fever, cough, and myalgia.
The secondary use application is limited by the size and scope of the available data. In future work,
the extractor will be applied to a much larger set of clinical ambulatory care and emergency department
notes from UW. The extracted symptom information will also be combined with routinely coded data (e.g.
diagnosis and procedure codes, demographics) and automatically extracted data (e.g. social determinants of
health [51]). Using these data, we will develop models for predicting risk of COVID-19 infection amongst
16

Figure 9: Distribution of averaged SHAP values by note type with the all feature set. The vertical lines in each violin indicate the
quartiles. * indicates the feature is an automatically extracted symptom.

individuals who are tested. These models could better inform clinical indications for prioritizing testing with
constrained test availability. Additionally, the presence or absence of certain symptoms can be used to inform
clinical care decisions with greater precision. This future work may also identify combinations of symptoms
(including their presence, absence, severity, sequence of appearance, duration, etc.) associated with clinical
outcomes and health service utilization, such as deteriorating clinical course and need for repeat consultation
or hospital admission. The use of detailed symptom information will be highly valuable in informing these
models, but potentially only with the level of nuance that our extraction models provide. With the COVID-19
pandemic continuing for the foreseeable future, accelerating the research outlined in this paper will inform
key clinical and health service decision making.
Acknowledgments
This work has been funded by Gordon and Betty Moore Foundation and by the National Center For
Advancing Translational Sciences of the National Institutes of Health under Award Number UL1 TR002319.
We want to acknowledge Elizabeth Chang, Kylie Kerker, Jolie Shen, and Erica Qiao for their contributions to
the gold standard annotations and Nicholas Dobbins for data management and curation.
Research and results reported in this publication was partially facilitated by the generous contribution of
computational resources from the University of Washington Department of Radiology.

17

References
[1] World Health Organization, Coronavirus disease (COVID-19) weekly epidemiological update and
weekly operational update, 28 september 2020, 2020. URL: https://www.who.int/emergencies/
diseases/novel-coronavirus-2019/situation-reports.

[2] H. Rossman, A. Keshet, S. Shilo, A. Gavrieli, T. Bauman, O. Cohen, E. Shelly, R. Balicer, B. Geiger,
Y. Dor, et al., A framework for identifying regional outbreak and spread of COVID-19 from one-minute
population-wide surveys, Nature Medicine (2020) 1–4. doi:10.1038/s41591-020-0857-9.
[3] Z. Wu, J. M. McGoogan, Characteristics of and important lessons from the coronavirus disease 2019
(COVID-19) outbreak in China: Summary of a report of 72314 cases from the Chinese Center for
Disease Control and Prevention, Journal of the American Medical Association 323 (2020) 1239–1242.
doi:10.1001/jama.2020.2648.
[4] J. Yang, Y. Zheng, X. Gou, K. Pu, Z. Chen, Q. Guo, R. Ji, H. Wang, Y. Wang, Y. Zhou, Prevalence of
comorbidities in the novel Wuhan coronavirus (COVID-19) infection: a systematic review and metaanalysis, International Journal of Infectious Diseases (2020). doi:10.1016/j.ijid.2020.03.017.
[5] P. Vetter, D. L. Vu, A. G. L’Huillier, M. Schibler, L. Kaiser, F. Jacquerioz, Clinical features of COVID-19,
British Medical Journal (2020). doi:10.1136/bmj.m1470.
[6] G. Qian, N. Yang, A. H. Y. Ma, L. Wang, G. Li, X. Chen, X. Chen, COVID-19 transmission within a
family cluster by presymptomatic carriers in China, Clinical Infectious Diseases (2020). doi:10.1093/
cid/ciaa316.
[7] W. E. Wei, Z. Li, C. J. Chiew, S. E. Yong, M. P. Toh, V. J. Lee, Presymptomatic transmission of
SARS-CoV-2—Singapore, January 23–March 16, 2020, Morbidity and Mortality Weekly Report 69
(2020) 411. doi:10.15585/mmwr.mm6914e1.
[8] C. Wu, X. Chen, Y. Cai, X. Zhou, S. Xu, H. Huang, L. Zhang, X. Zhou, C. Du, Y. Zhang,
et al.,

Risk factors associated with acute respiratory distress syndrome and death in patients

with coronavirus disease 2019 pneumonia in Wuhan, China,

JAMA Internal Medicine (2020).

doi:10.1001/jamainternmed.2020.0994.
[9] L. L. Wang, K. Lo, Y. Chandrasekhar, R. Reas, J. Yang, D. Eide, K. Funk, R. M. Kinney, Z. Liu,
W. Merrill, P. Mooney, D. A. Murdick, D. Rishi, J. Sheehan, Z. Shen, B. Stilson, A. D. Wade, K. Wang,
C. Wilhelm, B. Xie, D. M. Raymond, D. S. Weld, O. Etzioni, S. Kohlmeier, CORD-19: The COVID-19
open research dataset, arXiv (2020). URL: https://arxiv.org/abs/2004.10706.
[10] World Health Organization, Global literature on coronavirus disease, 2020. URL: https://
search.bvsalud.org/global-literature-on-novel-coronavirus-2019-ncov/.

18

[11] X. Wang, X. Song, Y. Guan, B. Li, J. Han, Comprehensive named entity recognition on CORD-19 with
distant or weak supervision, arXiv (2020). URL: https://arxiv.org/abs/2003.12218.
[12] B. R. South, S. Shen, M. Jones, J. Garvin, M. H. Samore, W. W. Chapman, A. V. Gundlapalli, Developing
a manually annotated clinical document corpus to identify phenotypic information for inflammatory
bowel disease, BMC Bioinformatics 10 (2009). doi:10.1186/1471-2105-10-s9-s12.
[13] R. Koeling, J. Carroll, R. Tate, A. Nicholson, Annotating a corpus of clinical text records for learning to
recognize symptoms automatically, in: International Workshop on Health Text Mining and Information
Analysis, 2011, pp. 43–50. URL: http://sro.sussex.ac.uk/id/eprint/22351.
[14] Ö. Uzuner, B. R. South, S. Shen, S. L. DuVall, 2010 i2b2/VA challenge on concepts, assertions, and
relations in clinical text, Journal of the American Medical Informatics Association 18 (2011) 552–556.
doi:10.1136/amiajnl-2011-000203.
[15] S. Zheng, Y. Hao, D. Lu, H. Bao, J. Xu, H. Hao, B. Xu, Joint entity and relation extraction based on a
hybrid neural network, Neurocomputing 257 (2017) 59 – 66. doi:10.1016/j.neucom.2016.12.075,
machine Learning and Signal Processing for Big Multimedia Analysis.
[16] W. Orr, P. Tadepalli, X. Fern, Event detection with neural networks: A rigorous empirical evaluation, in:
Conference on Empirical Methods in Natural Language Processing, 2018, pp. 999–1004. doi:http:
//dx.doi.org/10.18653/v1/D18-1122.
[17] X. Shi, Y. Yi, Y. Xiong, B. Tang, Q. Chen, X. Wang, Z. Ji, Y. Zhang, H. Xu, Extracting entities
with attributes in clinical text via joint deep learning, Journal of the American Medical Informatics
Association 26 (2019) 1584–1591. doi:10.1093/jamia/ocz158.
[18] Y. Pang, J. Liu, L. Liu, Z. Yu, K. Zhang, A deep neural network model for joint entity and relation
extraction, IEEE Access 7 (2019) 179143–179150. doi:10.1109/ACCESS.2019.2949086.
[19] L. Chen, Y. Gu, X. Ji, Z. Sun, H. Li, Y. Gao, Y. Huang, Extracting medications and associated adverse
drug events using a natural language processing system combining knowledge base and deep learning,
Journal of the American Medical Informatics Association 27 (2019) 56–64. doi:10.1093/jamia/
ocz141.
[20] F. Christopoulou, T. T. Tran, S. K. Sahu, M. Miwa, S. Ananiadou, Adverse drug events and medication
relation extraction in electronic health records with ensemble deep learning methods, Journal of the
American Medical Informatics Association 27 (2020) 39–46. doi:10.1093/jamia/ocz101.
[21] K. Lee, L. He, M. Lewis, L. Zettlemoyer, End-to-end neural coreference resolution, in: Empirical
Methods in Natural Language Processing, 2017, pp. 188–197. doi:10.18653/v1/D17-1018.

19

[22] Y. Luan, L. He, M. Ostendorf, H. Hajishirzi, Multi-task identification of entities, relations, and
coreference for scientific knowledge graph construction, in: Empirical Methods in Natural Language
Processing, 2018, pp. 3219–3232. doi:10.18653/v1/D18-1360.
[23] Y. Luan, D. Wadden, L. He, A. Shah, M. Ostendorf, H. Hajishirzi, A general framework for information
extraction using dynamic span graphs, in: North American Chapter of the Association for Computational
Linguistics, 2019, pp. 3036–3046. doi:10.18653/v1/N19-1308.
[24] D. Wadden, U. Wennberg, Y. Luan, H. Hajishirzi,
contextualized span representations,

Entity, relation, and event extraction with

in: Empirical Methods in Natural Language Processing

and the International Joint Conference on Natural Language Processing, 2019, pp. 5788–5793.
doi:10.18653/v1/D19-1585.
[25] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, L. Zettlemoyer, Deep contextualized
word representations, in: North American Chapter of the Association for Computational Linguistics,
2018, pp. 2227–2237. doi:10.18653/v1/N18-1202.
[26] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training of deep bidirectional transformers for
language understanding, in: North American Chapter of the Association for Computational Linguistics,
2019, pp. 4171–4186. doi:http://dx.doi.org/10.18653/v1/N19-1423.
[27] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, Q. V. Le, XLNet: Generalized autoregressive pretraining for language understanding, in: Advances in neural information processing systems, 2019, pp. 5753–5763. URL: http://papers.nips.cc/paper/8812-xlnet-generalizedautoregressive-pretraining-for-language-understanding.pdf.

[28] W. Huang, X. Cheng, T. Wang, W. Chu, BERT-based multi-head selection for joint entity-relation
extraction, in: International Conference on Natural Language Processing and Chinese Computing, 2019,
pp. 713–723. doi:10.1007/978-3-030-32236-6_65.
[29] H. Wang, M. Tan, M. Yu, S. Chang, D. Wang, K. Xu, X. Guo, S. Potdar, Extracting multiple-relations
in one-pass with pre-trained transformers, in: Association for Computational Linguistics, 2019, pp.
1371–1377. doi:10.18653/v1/P19-1132.
[30] E. Alsentzer, J. Murphy, W. Boag, W.-H. Weng, D. Jin, T. Naumann, M. McDermott, Publicly available
clinical BERT embeddings, in: Clinical Natural Language Processing Workshop, 2019, pp. 72–78.
doi:10.18653/v1/W19-1909.
[31] A. E. Johnson, T. J. Pollard, L. Shen, H. L. Li-wei, M. Feng, M. Ghassemi, B. Moody, P. Szolovits, L. A.
Celi, R. G. Mark, MIMIC-III, a freely accessible critical care database, Scientific Data 3 (2016) 160035.
doi:10.1038/sdata.2016.35.

20

[32] W. Tian, W. Jiang, J. Yao, C. J. Nicholson, R. H. Li, H. H. Sigurslid, L. Wooster, J. I. Rotter, X. Guo,
R. Malhotra, Predictors of mortality in hospitalized COVID-19 patients: A systematic review and
meta-analysis, Journal of Medical Virology (2020). doi:10.1002/jmv.26050.
[33] S. Figliozzi, P. G. Masci, N. Ahmadi, L. Tondi, E. Koutli, A. Aimo, K. Stamatelopoulos, M.-A.
Dimopoulos, A. L. Caforio, G. Georgiopoulos, Predictors of adverse prognosis in COVID-19: A
systematic review and meta-analysis, European Journal of Clinical Investigation (2020) e13362.
doi:10.1111/eci.13362.
[34] V. Jain, J.-M. Yuan, Predictive symptoms and comorbidities for severe COVID-19 and intensive care
unit admission: a systematic review and meta-analysis, International Journal of Public Health (2020) 1.
doi:10.1007/s00038-020-01390-7.
[35] Y. Dong, H. Zhou, M. Li, Z. Zhang, W. Guo, T. Yu, Y. Gui, Q. Wang, L. Zhao, S. Luo, et al., A novel
simple scoring model for predicting severity of patients with sars-cov-2 infection, Transboundary and
Emerging Diseases (2020). doi:10.1111/tbed.13651.
[36] P. P. Xu, R. H. Tian, S. Luo, Z. Y. Zu, B. Fan, X. M. Wang, K. Xu, J. T. Wang, J. Zhu, J. C. Shi,
et al., Risk factors for adverse clinical outcomes with COVID-19 in China: a multicenter, retrospective,
observational study, Theranostics 10 (2020) 6372. doi:10.7150/thno.46833.
[37] J. L. Izquierdo, J. Ancochea, J. B. Soriano, Clinical characteristics and prognostic factors for ICU
admission of patients with COVID-19 using machine learning and natural language processing, medRxiv
preprint (2020). doi:10.1101/2020.05.22.20109959.
[38] D. Bertsimas, L. Boussioux, R. C. Wright, A. Delarue, V. D. Jr., A. Jacquillat, D. L. Kitane, G. Lukin,
M. L. Li, L. Mingardi, O. Nohadani, A. Orfanoudaki, T. Papalexopoulos, I. Paskov, J. Pauphilet,
O. S. Lami, B. Stellato, H. T. Bouardi, K. V. Carballo, H. Wiberg, C. Zeng, From predictions
to prescriptions: A data-driven response to COVID-19, arXiv preprint 2006.16509 (2020). URL:
https://arxiv.org/abs/2006.16509.

[39] L. Wynants, B. Van Calster, G. S. Collins, R. D. Riley, G. Heinze, E. Schuit, M. M. J. Bonten,
J. A. A. Damen, T. P. A. Debray, M. De Vos, P. Dhiman, M. C. Haller, M. O. Harhay, L. Henckaerts,
N. Kreuzberger, A. Lohmann, K. Luijken, J. Ma, C. L. Andaur Navarro, J. B. Reitsma, J. C. Sergeant,
C. Shi, N. Skoetz, L. J. M. Smits, K. I. E. Snell, M. Sperrin, R. Spijker, E. W. Steyerberg, T. Takada,
S. M. J. van Kuijk, F. S. van Royen, C. Wallisch, L. Hooft, K. G. M. Moons, M. van Smeden, Prediction
models for diagnosis and prognosis of COVID-19: systematic review and critical appraisal, BMJ 369
(2020). doi:10.1136/bmj.m1328.
[40] J. A. Siordia, Epidemiology and clinical features of COVID-19: A review of current literature, Journal
of Clinical Virology 127 (2020) 104357. doi:10.1016/j.jcv.2020.104357.
21

[41] J. J. Zhang, K. S. Lee, L. W. Ang, Y. S. Leo, B. E. Young, Risk factors of severe disease and efficacy of
treatment in patients infected with COVID-19: A systematic review, meta-analysis and meta-regression
analysis, Clinical Infectious Diseases (2020). doi:10.1093/cid/ciaa576.
[42] D. Brinati, A. Campagner, D. Ferrari, M. Locatelli, G. Banfi, F. Cabitza, Detection of COVID-19
infection from routine blood exams with machine learning: A feasibility study, Journal of Medical
Systems 44 (2020) 135. doi:10.1007/s10916-020-01597-4.
[43] X. Mei, H.-C. Lee, K.-y. Diao, M. Huang, B. Lin, C. Liu, Z. Xie, Y. Ma, P. M. Robson, M. Chung,
A. Bernheim, V. Mani, C. Calcagno, K. Li, S. Li, H. Shan, J. Lv, T. Zhao, J. Xia, Q. Long, S. Steinberger,
A. Jacobi, T. Deyer, M. Luksza, F. Liu, B. P. Little, Z. A. Fayad, Y. Yang, Artificial intelligence–enabled
rapid diagnosis of patients with COVID-19, Nature Medicine 26 (2020) 1224–1228. doi:10.1038/
s41591-020-0931-3.
[44] S. Wollenstein-Betech, C. Cassandras, I. Paschalidis, Personalized predictive models for symptomatic COVID-19 patients using basic preconditions: Hospitalizations, mortality, and the need
for an ICU or ventilator.,

International Journal of Medical Informatics (2020). doi:10.1016/

j.ijmedinf.2020.104258.
[45] P. Stenetorp, S. Pyysalo, G. Topić, T. Ohta, S. Ananiadou, J. Tsujii, BRAT: a web-based tool for NLPassisted text annotation, in: Conference of the European Chapter of the Association for Computational
Linguistics, 2012, pp. 102–107. URL: https://www.aclweb.org/anthology/E12-2021.
[46] C. Walker, S. Strassel, J. Medero, K. Maeda, ACE 2005 multilingual training corpus ldc2006t06, 2006.
URL: https://catalog.ldc.upenn.edu/LDC2006T06.
[47] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,
B. Steiner, L. Fang, J. Bai, S. Chintala, Pytorch: An imperative style, high-performance deep
learning library, in: H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, R. Garnett (Eds.), Advances in Neural Information Processing Systems 32, Curran Associates, Inc., 2019,
pp. 8024–8035. URL: http://papers.neurips.cc/paper/9015-pytorch-an-imperativestyle-high-performance-deep-learning-library.pdf.

[48] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, et al., Scikit-learn:
machine learning in Python, Journal of Machine Learning Research 12 (2011) 2825–2830. doi:https:
//dl.acm.org/doi/pdf/10.5555/1953048.2078195.
[49] S. M. Lundberg, G. Erion, H. Chen, A. DeGrave, J. M. Prutkin, B. Nair, R. Katz, J. Himmelfarb,
N. Bansal, S.-I. Lee, From local explanations to global understanding with explainable AI for trees,
Nature Machine Intelligence 2 (2020) 2522–5839. doi:10.1038/s42256-019-0138-9.
22

[50] J.-H. Kim,

Estimating classification error rate: Repeated cross-validation, repeated hold-out

and bootstrap,

Computational statistics & data analysis 53 (2009) 3735–3745. doi:10.1016/

j.csda.2009.04.009.
[51] K. Lybarger, M. Ostendorf, M. Yetisgen, Annotating social determinants of health using active
learning, and characterizing determinants using neural event extraction, arXiv (2020). URL: https:
//arxiv.org/abs/2004.05438.

7. Appendix
Table 3: Demographic, vital signs, and laboratory fields that are predictive of COVID-19 infection in current literature

Parameter

Sources

age
alanine aminotransferase (ALT)
albumin
alkaline phosphatase (ALP)
aspartate aminotransferase (AST)
basophils
calcium
C-reactive protein (CRP)
D-dimer
eosinophils
gamma-glutamyl transferase (GGT)
gender
heart rate
lactate dehydrogenase (LDH)
lymphocytes
monocytes
neutrophils
oxygen saturation
platelets
prothrombin time (PT)
respiratory rate
temperature
troponin
white blood cell (WBC) count

[38, 39, 42, 43]
[40, 42]
[41]
[42]
[38, 40, 42]
[42]
[38]
[38, 40, 41]
[40]
[40, 42]
[42]
[43]
[38]
[40–42]
[39–43]
[42]
[39, 40, 42, 43]
[38]
[42]
[40]
[38]
[38, 39, 43]
[40]
[38, 42, 43]

23

Table 4: Hyperparameters for the Span-based Event Extractor

Parameter

Value

Maximum sentence length, n
Maximum span length, M
Top-K spans per classifier
Batch size
Number of epochs
Learning rate
Optimizer
Maximum gradient L2-norm
BERT embedding dropout
bi-LSTM hidden size, vh
bi-LSTM activation function
bi-LSTM dropout
Span classifier projection size, vs
Span classifier activation function
Span classifier dropout
Role classifier projection size, vr
Role classifier activation function
Role classifier dropout

24

30
6
sentence token count
100
100
0.001
Adam
100
0.3
200
tanh
0.3
100
ReLU
0.3
100
ReLU
0.3

Table 5: Structured fields from UW EHR used to predict COVID-19 infection. f indicates the with function used to aggregate multiple
measurements/values. Fields that measure the same phenomena and were treated as a single feature, resulting in 29 distinct structured
EHR fields: {“Temperature - C,” “Temperature (C)”}, {“HR,” “Heart Rate”}, and {“O2 Saturation (%),” “Oxygen Saturation”}. All
fields numerical (e.g. “Temperature (C)”=38.1), except “Troponin I Interpretation” and ”Gender”

Parameter

Fields in UW EHR

f

age

“AgeIn2020”

max

ALT

“ALT (GPT)”

max

albumin

“Albumin”

min

ALP

“Alkaline Phosphatase (Total)”

max

AST

“AST (GOT)”

max

basophils

“Basophils” and “% Basophils”

min

calcium

“Calcium”

min

CRP

“CRP, high sensitivity”

max

D-dimer

“D Dimer Quant”

max

eosinophils

“Eosinophils” and “% Eosinophils”

min

GGT

“Gamma Glutamyl Transferase”

max

gender

“Gender”

last

heart rate

“Heart Rate” and “HR”

max

LDH

“Lactate Dehydrogenase”

max

lymphocytes

“Lymphocytes” and “% Lymphocytes”

min

monocyptes

“Monocytes”

max

neutrophils

“Neutrophils” and “% Neutrophils”

max

oxygen saturation

“Oxygen Saturation” and “O2 Saturation (%)”

min

platelets

“Platelet Count”

min

PT

“Prothrombin Time Patient” and “Prothrombin INR”

max

respiratory rate

“Respiratory Rate”

max

temperature

“Temperature - C” and “Temperature (C)”

max

troponin

“Troponin I” and “Troponin I Interpretation”

max

WBC count

“WBC”

min

25

Table 6: COVID-19 prediction hyperparameters for Random Forest Models

Note type

Features

# estimators

Maximum
depth

Minimum
samples per
split

Minimum
samples per
leaf

Class
weight ratio
(pos./neg.)

ED
ED
ED

structured
notes
all

200
200
200

4
4
6

2
4
3

1
1
1

10
6
8

Progress
Progress
Progress

structured
notes
all

200
200
200

18
10
8

6
4
4

1
1
1

6
4
6

Telephone
Telephone
Telephone

structured
notes
all

200
200
200

10
6
10

2
2
8

1
1
1

2
4
4

26

