medRxiv preprint doi: https://doi.org/10.1101/2021.03.14.21253554; this version posted March 24, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY-NC-ND 4.0 International license .

Improved Prediction of COVID-19 Transmission and Mortality Using Google Search Trends for
Symptoms in the United States
Meshrif Alruily1,*, Mohamed Ezz1,2,*, Ayman Mohamed Mostafa1,3, Nacim Yanes1,4, Mostafa Abbas5, and
Yasser EL-Manzalawy5,$
1

College of Computer and Information Sciences, Jouf University, Sakaka 72314, Saudi Arabia
2
Faculty of Engineering, Al-Azhar University, Cairo 11651, Egypt
3
Faculty of Computers and Informatics, Zagazig University, Zagazig 44519, Egypt
4
RIADI Laboratory, La Manouba University, Manouba 2010, Tunisia
5
Department of Translational Data Science and Informatics, Geisinger, Danville, PA 17822, USA
*

These two authors contributed equally

$

Corresponding author

ABSTRACT
Accurate forecasting of emerging infectious diseases can guide public health officials in making
appropriate decisions related to the allocation of public health resources. Due to the exponential spread
of the COVID-19 infection worldwide, several computational models for forecasting the transmission and
mortality rates of COVID-19 have been proposed in the literature. To accelerate scientific and public
health insights into the spread and impact of COVID-19, Google released the Google COVID-19 search
trends symptoms open-access dataset. Our objective is to develop 7 and 14 -day-ahead forecasting
models of COVID-19 transmission and mortality in the US using the Google search trends for COVID-19
related symptoms. Specifically, we propose a stacked long short-term memory (SLSTM) architecture for
predicting COVID-19 confirmed and death cases using historical time series data combined with auxiliary
time series data from the Google COVID-19 search trends symptoms dataset. Considering the SLSTM
networks trained using historical data only as the base models, our base models for 7 and 14 -day-ahead
forecasting of COVID cases had the mean absolute percentage error (MAPE) values of 6.6% and 8.8%,
respectively. On the other side, our proposed models had improved MAPE values of 3.2% and 5.6%,
respectively. For 7 and 14 -day-ahead forecasting of COVID-19 deaths, the MAPE values of the base
models were 4.8% and 11.4%, while the improved MAPE values of our proposed models were 4.7% and
7.8%, respectively. We found that the Google search trends for "pneumonia," "shortness of breath," and
"fever" are the most informative search trends for predicting COVID-19 transmission. We also found that
the search trends for "hypoxia" and "fever" were the most informative trends for forecasting COVID-19
mortality.

Keywords: Forecasting COVID-19 transmission and mortality in the US, Stacked LSTM, SARS-COV-2,
Google COVID-19 Search Trends.

1
NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.

medRxiv preprint doi: https://doi.org/10.1101/2021.03.14.21253554; this version posted March 24, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY-NC-ND 4.0 International license .

Introduction
In March 1st, 2020, the COVID-19 outbreak was declared a national emergency in the US. After exactly one
year of this declaration and according to the JHU dashboard, the numbers of COVID-19 confirmed and
death cases have reached more than 500K and 17M, respectively. This rapid spread of the virus in the US
had negative impacts on several sectors including economy [1], education [2-4], health [5, 6]. Reliable
real-time forecasting of the spread of infectious diseases, including COVID-19, can improve public health
response to outbreaks and save lives [7, 8].
Since the emergence of the COVID-19 outbreak in late 2019, several scientists have developed
computational models for forecasting COVID-19 confirmed cases, deaths, and recovery [9, 10]. Commonly
used statistical methods for time series forecasting such as autoregressive integrated moving average
(ARIMA) [11] have been used in multiple studies for forecasting COVID-19 (e.g., [12-14]). These methods
are typically based on historical data and do not account directly for disease transmission dynamics or any
relevant biological process [15-17]. To account for these factors, epidemiological methods have been
proposed for forecasting infectious diseases. Examples of the application of epidemiological methods for
modeling the spread of COVID-19 infection include several frameworks based on the adaption of the SEIR
(Susceptible, Exposed, Infected, Recovered) method [18-20]. Because the time series forecasting task can
be formulated as a supervised learning problem [21], several machine learning algorithms have been used
for forecasting COVID-19 (e.g., [22-25]).
Recurrent neural networks (RNNs) [26, 27] are machine learning based models that have been successfully
applied to the problem of forecasting time series [28-30]. In RNNs, recurrent layers consist of a sequence
of recurrent cells whose states are determined by past states and current inputs. Long short-term memory
(LSTM) [31, 32] is a variant of RNN designed to capture long-term dependencies by introducing gate
functions into the recurrent cell structure. Since its introduction, LSTM is probably the most widely used
form of RNNs and have been successfully used in a broad range of sequence classification tasks including
financial time series prediction [33], speech recognition [34], sentiment classification [35], traffic
forecasting [36], and anomaly detection [37]. Deep neural network architectures can better model realworld time series with complex non-linear relationships [38, 39]. A deep LSTM architecture, also called
stacked LSTM (SLSTM), consists of several hidden LSTM layers and has been shown more effective in
modeling complex sequence data [40].
Recently, Abbas et al. [41] have shown that Google search trends for nine COVID-19 related symptoms
(namely, hypoxemia, ageusia, anosmia, dysgeusia, hypoxia, fever, pneumonia, chills, and shortness of
breath (SOB)) are strongly associated with COVID-19 confirmed as well as death cases in the US. Results
of their analysis suggested that these Google trends can be used (in combination with COVID-19 historical
data) to forecast COVID-19 spread and mortality up to three weeks ahead in time. The main goal of this
study is to validate this finding. Specifically, we propose a stacked LSTM (SLSTM) model for forecasting
state-level daily cumulative COVID-19 confirmed and death cases in the US. We then use this model to
quantify the importance of each Google search trend for forecasting COVID-19 transmission and mortality
in the US. Finally, we demonstrate substantial improvements in the predictive performance of the SLTSM
models when the data for up to three Google search trends are incorporated in the training of these
models. Our results demonstrate the viability of incorporating Google search trends for COVID-19 related
symptoms into deep learning models for forecasting COVID-19 transmission and mortality in the US.

2

medRxiv preprint doi: https://doi.org/10.1101/2021.03.14.21253554; this version posted March 24, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY-NC-ND 4.0 International license .

Methods
Data
Daily cumulative counts for COVID-19 confirmed and death cases were downloaded from a publicly
available repository maintained by the Center for Systems Science and Engineering (CSSE) at Johns
Hopkins University (JHU) [42]. We considered the data aggregated at the state level for the 50 US states
plus the District of Columbia. We experimented with the data from March 1 st, 2020 to September 31st,
2020. The downloaded counts were then normalized to count per million people in each state using 2019
census population estimates.
State-level aggregated and normalized Google COVID-19 search trends symptoms [43] were downloaded
from https://github.com/google-research/open-covid-19-data/. The data includes search trends for 422
symptoms that might be related to COVID-19. However, we limited our experiments to the nine symptoms
suggested by the exploratory functional data analysis [44] provided in [41]. Therefore, our final symptoms
dataset includes time series for the following nine symptoms: hypoxemia, ageusia, anosmia, dysgeusia,
hypoxia, fever, pneumonia, chills, and shortness of breath (SOB).
Each time series were split into three sets for training, validation, and testing. The test set covers the last
45 days in our study interval (i.e., from August 17th to September 31st). The data for the remaining study
time (from March 1st to August 16th) were split into training and validation such that the data from the
last 45 days in this interval were used for validation.
k-day-ahead forecasting
Given a time series with ùëõ time points, ùë• , ‚Ä¶ . , ùë• , the goal is to predict time series at the future time
points (i.e., ùëõ + 1, ùëõ + 2, etc.). For ùëò-step-ahead forecasting, the predictive model is required to predict
ùë•
given historical data up to ùë• . Often, the predictive model does not use the entire historical data but
only uses the ùë§ most recent time points (e.g., ùë•
, ‚Ä¶ , ùë• ). These fixed-length windows labeled with
the target outcome ùë¶ = ùë•
are ideal for training machine learning models. In this study, since every
step in the time series is a day, we call the ùëò-step-ahead forecasting a ùëò-day-ahead forecasting.
In the presence of an auxiliary time series (i.e., one time series corresponding to the Google search trends
for COVID-19 related symptoms), ùë† , ‚Ä¶ , ùë† , we can generate labeled samples in two spaces: i) symptom
space, < [ùë†
, ‚Ä¶ , ùë† ], ùë•
>; ii) historical + symptom space,
< [ùë•
,‚Ä¶,ùë• ,‚Ä¶,ùë†
, ‚Ä¶ , ùë† ], ùë•
auxiliary time series are available.

>. This can also be generalized to the case when multiple

Our proposed deep learning model
We used the Long short-term memory (LSTM) networks [31, 32] for developing predictive models for the
four COVID-19 forecasting tasks considered in this study. LSTM is a type of Recurrent Neural Network
(RNN) that is suitable for learning long-term dependencies in sequence data [45]. Long-terms
dependencies are modeled using memory blocks [46]. A memory block or a single LSTM cell is a
recurrently connected sub-network that contains a memory cell and three gates. Fig. 1 shows the
architectures of an LSTM cell in an LSTM layer. The memory cell remembers the temporal state of the cell
and the gates control the pattern of information flow. Input and output gates control information flow

3

medRxiv preprint doi: https://doi.org/10.1101/2021.03.14.21253554; this version posted March 24, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY-NC-ND 4.0 International license .

into and from the cell, respectively. The forget gate controls what information will be thrown away from
the memory cell. Mathematically, an LSTM is expressed using the following equations:
ùëì =ùúé ùëä ‚Ñé
ùëñ = ùúé(ùëä ‚Ñé

+ùëä ùë• +ùëè
+ùëä ùë• +ùëè)
+ ùëä ÃÉ ùë• + ùëè ÃÉ)

ùëêÃÉ = tanh(ùëä ÃÉ ‚Ñé
ùëê = ùëì .ùëê

+ ùëñ . ùëêÃÉ
+ùëä ùë• +ùëè )

ùëú = ùúé(ùëä ‚Ñé

‚Ñé = ùëú . tanh (ùëê )
A simple LSTM network includes a single LSTM layer. To add capacity and depth, multiple LSTM layers
could be stacked together to form a multilayer fully connected structure [47]. Fig. 2 shows the structure
of our proposed stacked LSTM (SLSTM) network, which included two LSTM layers. The first and second
LSTM layers had 128 and 64 hidden units, respectively. The output of the second LSTM layer represents
the deep features learned from the sequence data, which is then fed to a dense layer with 64 units
followed by a single neuron, fully connected to the 64 neurons from the previous layer, for output. It is
worth noting that this architecture had been used across all forecasting tasks and data representations as
shown in the following subsection.
Experimental settings
In our experiments, we considered ùëò-day-ahead forecasting of COVID-19 confirmed cases for ùëò = 7 and
14 days. We also experimented with ùëò-day-ahead forecasting of COVID-19 death cases for ùëò = 7 and 14
days. For these four prediction tasks, we experimented with the following types of input features: i)
historical data; ii) individual Google search trend for nine COVID-19 related symptoms; iii) historical data
and single Google search trend for nine COVID-19 related symptoms; iv) historical data and top two
Google search trends for nine COVID-19 related symptoms determined using the validation set in step 3
experiments; v) historical data and top three Google search trends for nine COVID-19 related symptoms
determined using the validation set in step 3 experiments. Hence, the number of input features ranged
from ùë§ to ùë§ + ùë§ √ó ùëë, where ùë§ is the window size parameter and ùëë = {0,1,2,3} is the number of auxiliary
time series used. Because the optimal window size, ùë§, is often data and task -dependent, we
experimented with ùë§ = {2,3, ‚Ä¶ ,9} and determined the optimal value for ùë§ using the performance of
the learned model on the validation set.
For implementing and evaluating the stacked LSTM models, we used the Keras version 2.4.3 and
tensorflow version 2.3.1 libraries. For all experiments resulting from all possible combinations of the four
forecasting tasks and the five types of inputs, we used the architecture and configurations shown in Fig.
2. The activation functions used were tanh, linear, and softplus for the two LSTM layers, dense layer, and
output neuron, respectively. For the two LSTM layers, the recurrent dropout rate was set to 0.5. For
training our models, we used the Mean Squared Logarithmic Error (MSLE) loss and an early stopping
technique [48] such that the training process was stopped if no improvement in the model performance,
in terms of MAPE on the validation set, was noted for 20 iterations.

4

medRxiv preprint doi: https://doi.org/10.1101/2021.03.14.21253554; this version posted March 24, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY-NC-ND 4.0 International license .

We assessed the predictive performance of our models using the mean absolute percentage error (MAPE)
defined as
ùëñ

|

|

√ó 100%, where ùëÅ is the number of time points in the test time series, ùë¶ is the

target outcome, and ùë¶ is the ùëñ

predicted outcome.

Results
Trajectories of COVID-19 confirmed and death cases
Fig. 3 shows the cumulative COVID-19 confirmed case (left) and death (right) trajectories for the 51 US
states. For COVID-19 confirmed cases, we noted that NY and NJ had the largest total per million counts of
confirmed COVID-19 cases during mid-March until the third week of July. Starting the third week of July,
several states, including LA, FL, and AZ, exceeded the number of confirmed cases in NY and NY as the rates
of COVID-19 spread started to drop substantially in these two states. For COVID-19 death cases, we found
that NJ and NY consistently had the highest number of total deaths and that their curves seemed to be
flat starting the third week of July.
Prediction of COVID-19 confirmed and death cases using historical data and univariate SLSTM
We report the performance of SLSTM models for predicting COVID-19 cases and mortality ùëò-day-ahead
for ùëò equals 7 and 14 days. Table 1 shows the performance of 16 LSTM models for forecasting COVID-19
cases. For the 7-day-ahead COVID-19 case prediction, we found that the best model determined using its
MAPE score on the validation set has MAPE scores of 7.4% and 6.6% on validation and test sets,
respectively. This model used a window of size equals 9. We also noted that a better MAPE score of 5.5%
was obtained using a window of size equals 5, but its performance on the validation set did not
recommend selecting it as the optimal learned model. For 14-day-ahead COVID-19 case prediction, the
best performing model used a window of size equals 9 and had the best observed MAPE scores of 11.8%
and 8.8% on validation and test sets, respectively.
Table 2 shows the performance of 16 SLSTM models for forecasting COVID-19 death cases. For the 7-dayahead COVID-19 death prediction, the best SLSTM model was obtained using a window of size equals 6
and had the best observed MAPE scores of 7.3% and 4.8% on the validation and test sets, respectively.
For the 14-day-ahead COVID-19 death prediction, the best model used a window of size equals 7 and had
the lowest noted MAPE scores of 13.1% and 11.4% on the validation and test sets, respectively.
Interestingly, we found that: i) MAPE scores for best performing COVID-19 case prediction models were
better than those for the best performing COVID-19 death prediction models; ii) Performance of the 7day-ahead forecasting models was better than the performance of the 14-day-ahead forecasting models;
iii) Performance of the models estimated using the validation sets was consistently lower than the
performance of the models estimated using the test set. However, the validation test successfully
identified the best performing model on the test set for 3 out of four prediction tasks.

5

medRxiv preprint doi: https://doi.org/10.1101/2021.03.14.21253554; this version posted March 24, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY-NC-ND 4.0 International license .

Table 1: Performance (in terms of MAPE) of different SLSTM for forecasting COVID-19 confirmed cases
using historical data only and different window sizes.
Window Size
2
3
4
5
6
7
8
9

7-day-ahead
Validation
Test
9.4
8.8
9.8
7.8
10.1
7.7
7.9
5.5
7.9
6.3
8.0
6.4
8.3
7.1
7.4
6.6

14-day ahead
Validation
Test
13.5
11.6
13.7
11.6
13.9
12.6
14.2
11.9
13.1
9.8
14.5
11.1
12.7
9.1
11.8
8.8

Table 2: Performance (in terms of MAPE) of different SLSTM for forecasting COVID-19 death cases
using historical data only and different window sizes.
Window Size
2
3
4
5
6
7
8
9

7-day ahead
Validation
Test
24.1
13.0
24.8
14.2
11.5
9.1
9.4
7.2
7.3
4.8
10.3
8.0
100
100.0
100
100.0

14-day ahead
Validation
Test
27.2
14.4
26.7
14.5
19.3
15
19.6
14.3
21.2
13.6
13.1
11.4
99.9
100
100
100

Improved prediction of COVID-19 confirmed and death cases using Google search trends for COVID-19
symptoms
We proceed with reporting our experimental results for testing two hypotheses regarding the Google
search trends for nine COVID-19 related symptoms: i) SLSTM models trained using any single symptom
can predict COVID-19 confirmed and death cases; ii) SLSTM models trained using any single symptom
combined with COVID-19 historical data can better predict COVID-19 confirmed and death cases
compared with SLSTM models trained using historical data only. Our rationale is that these nine symptoms
had been shown to have strong associations with both of COVID-19 transmission and mortality [41] and,
therefore, could be used for forecasting COVID-19 transmission and mortality or for improving the
performance of the models developed for the four tasks considered in this study.
Tables 3 and 4 show that the SLSTM models based on any of these nine symptoms failed to accurately
predict COVID-19 cases 7 and 14 -day-ahead. For these models, the best observed MAPE score was around
60%. However, when any of these symptoms were combined with historical COVID-19 cases data, the
best performing models, identified using the validation set, had MAPE scores on the test that was lower
than the MAPE scores of the best performing SLSTM models trained using historical data only (i.e., MAPE

6

medRxiv preprint doi: https://doi.org/10.1101/2021.03.14.21253554; this version posted March 24, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY-NC-ND 4.0 International license .

Table 3: Performance of optimal SLSTM models for 7-day-ahead forecasting of COVID-19 confirmed
cases when the models were trained using a single symptom time series alone or combined with
historical data.
Symptom
Ageusia
Anosmia
Chills
Fever
Pneumonia
SOB
Dysgeusia
Hypoxemia
Hypoxia

Symptom Only
Optimal Window
4
6
8
9
8
8
9
9
9

Validation
63.1
62.1
59.7
63.6
64.6
59.2
63.3
64.6
64.1

Test
71.9
72.3
65.7
64.8
67.1
67.1
71.8
72.8
72.3

Symptom + Historical
Optimal Window
4
5
3
4
3
3
4
4
9

Validation Test
6.2
4.9
6.9
5.6
6.6
4.7
5.6
4.5
5.2
3.8
5.4
3.6
8.3
5.5
6.4
4.8
7.1
6.0

Table 4: Performance of optimal SLSTM models for 14-day-ahead forecasting of COVID-19 confirmed
cases when the models were trained using a single symptom time series alone or combined with
historical data.
Symptom Only
Symptom
Ageusia
Anosmia
Chills
Fever
Pneumonia
SOB
Dysgeusia
Hypoxemia
Hypoxia

Optimal Window
7
9
7
3
3
3
6
7
3

Symptom + Historical
Validation
(rank)
62.2 (3.0)
61.4 (1.5)
61.4 (1.5)
62.7 (5.0)
63.5 (8.0)
62.4 (4.0)
62.9 (7.0)
64.4 (9.0)
62.8 (6.0)

Test
70.0
71.2
66.6
67.3
65.3
67.1
71.6
72.5
71.0

Optimal Window
4
5
3
3
5
3
7
7
9

Validation
(rank)
10.7 (4.0)
11.2 (5.0)
11.9 (8.0)
9.7 (2.0)
9.6 (1.0)
10.0 (3.0)
14.4 (9.0)
11.6 (6.0)
11.8 (7.0)

Test
8.1
8.7
8.1
6.5
7.4
6.1
9.6
8.2
9.3

scores of 6.6% and 8.8% for forecasting COVID-19 cases 7 and 14 -day-ahead, respectively). We also noted
that, for predicting COVID-19 cases 7-day-ahead, the optimal SLSTM model used a window of size equals
3 and a combination of pneumonia and historical data as input. On the other hand, for predicting COVID19 cases 14-day-ahead, the optimal SLSTM model used a window of size equals 5 and a combination of
pneumonia and historical data as input. Despite these performance improvements, we observed that the
MAPE scores estimated using the validation set failed to identify the best performing model on the test
set. Overall, our results rejected the first hypothesis and accepted the second one for forecasting COVID19 cases.
Similar findings were found for forecasting COVID-19 deaths (See Tables 5 and 6). Using any single
symptom for training the SLSTM models for predicting COVID-19 death 7 and 14 -day-ahead yielded poor

7

medRxiv preprint doi: https://doi.org/10.1101/2021.03.14.21253554; this version posted March 24, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY-NC-ND 4.0 International license .

models with MAPE scores of 75% or higher. When using search trends for hypoxia combined with
historical death counts, the best SLSTM had a MAPE score of 4.7%. That was a slight performance
improvement over the best SLSTM model using historical death counts only with MAPE score of 4.8% for
predicting COVID-19 deaths 7-day-ahead. However, there existed a model with an improved MAPES score
of 3.9%, but our selection criteria of the best model based on its performance on the validation set failed
to suggest it. For 14-day-ahead forecasting of COVID-19 deaths, the best SLSTM model, based on search
trends for favor and historical death counts, had a MAPE score of 8.7%, which is a considerable
performance improvement compared with the SLSTM model based on historical data only with a MAPE
score of 11.4%.
In summary, our results suggested that developing SLSTM models trained using historical data and search
trends for pneumonia yielded improvements in predicting 7 and 14 -day-ahead COVID-19 cases. Including
search trends for fever in the development of the SLSTM models for forecasting 7-day-ahead COVID-19
deaths improved the performance. However, the model did not perform the best on the evaluation set.
Thus, we failed to identify it as the optimal model. Finally, incorporating search trends for fever in training
the SLSTM models led to improvement in the performance of the best model for predicting 14-day-ahead
COVID-19 deaths. Next, we show that including search trends for two or three symptoms (instead of just
one) yielded consistent improvements in the performance of the learned models. Besides, it also
improved the agreement between the validation and test sets for identifying the best performing models.

Table 5: Performance of optimal SLSTM models for 7-day-ahead forecasting of COVID-19 death cases
when the models were trained using a single symptom time series alone or combined with historical
data.
Symptom Only
Symptom
Ageusia
Anosmia
Chills
Fever
Pneumonia
SOB
Dysgeusia
Hypoxemia
Hypoxia

Optimal Window
8
8
8
4
3
3
3
3
5

Symptom + Historical
Validation
(rank)
77.5 (3.0)
75.0 (1.5)
87.7 (6.0)
112.7 (7.0)
129.2 (9.0)
112.8 (8.0)
75.0 (1.5)
80.8 (5.0)
78.7 (4.0)

Test
57.3
56.4
57.9
69.9
72.5
70.5
54.7
60.1
60.4

8

Optimal Window
6
6
7
6
7
7
6
7
6

Validation
(rank)
15.4 (9.0)
11.4 (7.0)
7.8 (2.0)
8.4 (4.0)
9.2 (5.0)
8.3 (3.0)
12.6 (8.0)
9.7 (6.0)
6.7 (1.0)

Test
9.6
6.6
6.2
3.9
6.1
5.2
10.3
6.3
4.7

medRxiv preprint doi: https://doi.org/10.1101/2021.03.14.21253554; this version posted March 24, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY-NC-ND 4.0 International license .

Table 6: Performance of optimal SLSTM models for 14-day-ahead forecasting of COVID-19 death cases
when the models were trained using a single symptom time series alone or combined with historical
data.

Symptom
Ageusia
Anosmia
Chills
Fever
Pneumonia
SOB
Dysgeusia
Hypoxemia
Hypoxia

Symptom Only
Optimal
Window
4
9
4
3
3
3
4
6
6

Symptom + Historical
Validation
(rank)
91.2 (5.0)
83.2 (2.0)
91.7 (6.0)
115.0 (8.0)
130.6 (9.0)
114.6 (7.0)
78.6 (1.0)
90.1 (4.0)
84.8 (3.0)

Test
61.2
55.4
57.7
66.9
70.5
68.5
59.8
62.6
60.8

Optimal Window
7
7
7
8
6
6
7
8
8

Validation
(rank)
28.2 (9.0)
19.6 (7.0)
15.8 (6.0)
11.7 (1.0)
12.6 (3.0)
13.4 (4.0)
25.4 (8.0)
15.6 (5.0)
11.8 (2.0)

Test
16.6
12.2
11.4
8.7
11
8.3
17
9.6
9.6

Further Improved forecasting of COVID-19 using Google search trends for more than one COVID-19
symptom
Results reported in Tables 3-6 demonstrated the viability of including Google search trends for one COVID19 related symptom in training the SLSTM models. Here, we assessed whether including Google search
trends for two or three COVID-19 related symptoms could further improve the predictive performance of
the models. In this experiment, for each prediction task, we ranked the symptoms based on their MAPE
scores obtained using the validation set for the bivariate SLSTM models (See Tables 3-6). Then, we
considered the combinations of historical data with the top two and top three symptoms data series.
Table 7 shows the best performing SLSTM models trained using historical data and Google search trends
for up to three symptoms. For 7 and 14 -day-ahead forecasting of COVID-19 transmission, the best
performed models used a window of size equals 3 and utilized the historical data and the top three
symptoms data series to achieve the best reported MAPE values of 3.2% and 5.6%, respectively. Although
including the time series for the top three symptoms relevant for forecasting COVID-19 mortality provided
SLSTM models with better performance than those trained using historical data only, the best performing
models for 7 and 14 -day-ahead forecasting of COVID-19 mortality were obtained using one and two
symptoms, respectively. Interestingly, all optimal models highlighted in bold in Table 7 had their lowest
MAPE values on both validation and test sets. Another interesting observation is that results in Table 7
suggest that forecasting COVID-19 mortality is more challenging than forecasting COVID-19 transmission:
training COVID-19 mortality forecasting models required longer window sizes that span 6 or 7 days and
MAPE scores for these models is worse than those for the COVID-19 transmission forecasting models.

9

medRxiv preprint doi: https://doi.org/10.1101/2021.03.14.21253554; this version posted March 24, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY-NC-ND 4.0 International license .

Table 7: Performance of the best SLSTM models trained using historical data plus Google search
trends for 0-3 COVID-19 related symptoms.
Task
Cases-7

Cases-14

Mortality-7

Mortality-14

Symptoms
None
Pneumonia
Pneumonia + SOB
Pneumonia + SOB + Fever
None
Pneumonia
Pneumonia + Fever
Pneumonia + Fever + SOB
None
Hypoxia
Hypoxia + Chills
Hypoxia + Chills + SOB
None
Fever
Fever + Hypoxia
Fever + Hypoxia + Pneumonia

Optimal
Window
9
3
3
3
9
5
3
3
6
6
5
4
7
8
7
7

Validation
7.4
5.2
5.3
5.18
11.8
9.6
9.8
9.5
7.3
6.7
8
8.2
13.1
11.7
11.4
11.6

Test
6.6
3.8
3.2
3.2
8.8
7.4
6.5
5.6
4.8
4.7
6.1
5.2
11.4
8.7
7.8
8.6

Analysis of state-level predictions of the best performing models
State-level predictions of the four best performing models highlighted in Table 7 are provided in
Supplementary files 1-4 and Tables S1-S4. Fig. 4 shows sample test results for three states (AZ, UT, and
CA) on predicting 7-day-ahead COVID-19 confirmed cases, where true and predicted trajectories are
highlighted in blue and red, respectively. Based on this figure, we categorized the 51 states into one of
three categories based on the relationship between true and predicted trajectories: i) over-estimated
group, where at least 90% of the predictions are over-estimated; ii) under-estimated group, where at least
90% of the predictions are under-estimated; and iii) others. For the four classification tasks considered in
this study, Fig. 5 shows the categorization of states into three groups such that states with over-estimated
predictions are highlighted in red, states with under-estimated predictions are highlighted in yellow and
the remaining states are highlighted in gray. Interestingly, the US states with shared borders were more
likely to be assigned to the same category. An interesting exception is WA, which always appeared as an
isolated state belonging to the over-estimated category.
To get insights into when we should expect our best performing models to have over/under -estimated
predictions, we examined the trajectories of the member states in the three groups for each of the four
tasks (See Supplementary Figs. S1-S4). These four figures are also summarized in Fig. 6 by representing
the trajectories in each cluster using their pointwise mean curve. For COVID-19 cases prediction tasks,
Cases-7 and Cases-14, we found that, during the test interval, states in the over-estimated groups had
mean curves at the top of the entire data mean curve, while states in the ‚Äòothers‚Äô group had a mean curve
that is close to the whole data mean curve. Surprisingly, this observation did not apply to the COVID-19
death prediction tasks, Mortality-7 and Mortality-14.

10

medRxiv preprint doi: https://doi.org/10.1101/2021.03.14.21253554; this version posted March 24, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY-NC-ND 4.0 International license .

Discussion
Recent advances in machine learning based time series forecasting, particularly deep LSTM networks,
enabled the development of complex non-linear deep architectures for modeling dynamics and long-term
dependencies in real-world time series data. To date, several COVID-19 forecasting models have been
developed using several variants of the LSTM architecture (e.g. [21, 49-51]). However, the size of the test
data in existing studies was relatively small because it equals the number of days in the test time interval.
Fortunately, this is not the case in this study. Though our test data spans 45 days interval (from August
17th to end of September), the size of our test data was 51 ùë• 45 = 2295 samples because we
experimented with the US data at the state level. Generally, a large test data is required to accurately
evaluate the performance of a prediction model [52]. Thus, or performance estimates are thought to be
more robust as the size of our test data is more than 10 times the size of the test data used in related
studies.
Since the beginning of the COVID-19 pandemic in 2020, tracking and modeling its spread have gained
considerable attention from health agencies. Therefore, it is not surprising that numerous computational
methods for forecasting the transmission, mortality, and recovery of COVID-19 have been proposed in the
literature. Among these methods, few methods incorporated other sources of relevant data such as
Google trends [53, 54], climate [55], and mobility [56] in their forecasting systems. To the best of our
knowledge, our proposed models are the first models that use queries from the Google COVID-19
symptoms database [43] to improve their predictive performance on forecasting COVID-19 transmission
and mortality.
It is worse noting that our results demonstrated the added value of using Google search trends for COVID19 related symptoms (i.e., fever, pneumonia, shortness of breath, and hypoxia) combined with historical
data in forecasting COVID-19 confirmed and death cases. However, despite their strong and significant
correlations with COVID-19 spread and death trajectories reported in [41], we found that SLSTM models
trained using any of these symptoms alone yielded models with poor performance. In addition to the
improvement in predictive performance obtained via using Google symptoms time series along with the
historical data in training our models, another significant gain is improving the generalizability of the
models on both validation and test sets (i.e., the best performing model on the validation set is the model
with the best performance on the test set).
Our analysis of state-level predictions (Tables S1-S4) demonstrated high variability in the observed
performance across various US states. This observation suggests that the identified Google search trends,
though leading to the optimal overall performance, might not be the best for some US states. To overcome
this limitation, we are interested in developing state-specific models, which is the subject of our ongoing
work.

Conclusions
We have developed deep learning models for forecasting COVID-19 infection and mortality in the US using
historical data and Google search trends for COVID-19 related symptoms. Out of 422 symptoms included
in the Google COVID-19 symptoms database [43], we have focused on the nine symptoms identified in
[41] using dynamic correlation analysis. We then re-ranked these nine symptoms based on the
11

medRxiv preprint doi: https://doi.org/10.1101/2021.03.14.21253554; this version posted March 24, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY-NC-ND 4.0 International license .

performance of deep learning models trained using historical data and every single symptom. Finally, we
used the top three symptoms to develop our final and best performing models. Our results suggest that
Google search trends for the symptoms related to a target infectious disease could potentially improve
the performance of the forecasting models for that disease. Our future work aims at: including more
relevant time series data (e.g., mobility and climate) and assessing the improvement in performance for
forecasting over longer intervals (e.g., 21 and 30 days); experimenting with other deep learning models
for time series (e.g., Convolutional LSTM [57] and Bidirectional LSTM [47]); and developing state-specific
forecasting models.

Competing interests
The authors declare that they have no competing interests.
Funding
This work is supported in part by the Deanship of Scientific Research at Jouf University under grant no
(CV-28-41).
Supplementary Information
Supplementary file 1: Per-state predictions of the best performing model versus COVID-19 cases for the
CASES-7 task.
Supplementary file 2: Per-state predictions of the best performing model versus COVID-19 cases for the
CASES-14 task.
Supplementary file 3: Per-state predictions of the best performing model versus COVID-19 death cases
for the Mortality-7 task.
Supplementary file 4: Per-state predictions of the best performing model versus COVID-19 death cases
for the Mortality-14 task.
Supplementary file 5: Tables S1-S4
Supplementary file 6: Figures S1-S4

References
1.
2.
3.
4.
5.

McKibbin W, Fernando R: The economic impact of COVID-19. Economics in the Time of COVID19, 45.
Marinoni G, Van‚Äôt Land H, Jensen T: The impact of Covid-19 on higher education around the
world. IAU Global Survey Report 2020.
Ferrel MN, Ryan JJ: The impact of COVID-19 on medical education. Cureus 2020, 12(3).
Aucejo EM, French J, Araya MPU, Zafar B: The impact of COVID-19 on student experiences and
expectations: Evidence from a survey. Journal of public economics 2020, 191:104271.
Xiong J, Lipsitz O, Nasri F, Lui LM, Gill H, Phan L, Chen-Li D, Iacobucci M, Ho R, Majeed A: Impact
of COVID-19 pandemic on mental health in the general population: A systematic review.
Journal of affective disorders 2020:55-64.
12

medRxiv preprint doi: https://doi.org/10.1101/2021.03.14.21253554; this version posted March 24, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY-NC-ND 4.0 International license .

6.
7.
8.
9.
10.
11.
12.

13.

14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.

Mann DM, Chen J, Chunara R, Testa PA, Nov O: COVID-19 transforms health care through
telemedicine: evidence from the field. Journal of the American Medical Informatics Association
2020, 27(7):1132-1135.
George DB, Taylor W, Shaman J, Rivers C, Paul B, O‚ÄôToole T, Johansson MA, Hirschman L,
Biggerstaff M, Asher J: Technology to advance infectious disease forecasting for outbreak
management. Nature communications 2019, 10(1):1-4.
Buckee C: Improving epidemic surveillance and response: big data is dead, long live big data.
The Lancet Digital Health 2020, 2(5):e218-e220.
Rahimi I, Chen F, Gandomi AH: A review on COVID-19 forecasting models. Neural Computing
and Applications 2021:1-11.
Ioannidis JP, Cripps S, Tanner MA: Forecasting for COVID-19 has failed. International journal of
forecasting 2020.
Nelson BK: Time series analysis using autoregressive integrated moving average (ARIMA)
models. Academic emergency medicine 1998, 5(7):739-744.
Singh RK, Rani M, Bhagavathula AS, Sah R, Rodriguez-Morales AJ, Kalita H, Nanda C, Sharma S,
Sharma YD, Rabaan AA: Prediction of the COVID-19 pandemic for the top 15 affected
countries: Advanced autoregressive integrated moving average (ARIMA) model. JMIR public
health and surveillance 2020, 6(2):e19115.
Singh S, Parmar KS, Kumar J, Makkhan SJS: Development of new hybrid model of discrete
wavelet decomposition and autoregressive integrated moving average (ARIMA) models in
application to one month forecast the casualties cases of COVID-19. Chaos, Solitons & Fractals
2020, 135:109866.
Roy S, Bhunia GS, Shit PK: Spatial prediction of COVID-19 epidemic using ARIMA techniques in
India. Modeling earth systems and environment 2020:1-7.
Petropoulos F, Makridakis S, Stylianou N: COVID-19: Forecasting confirmed cases and deaths
with a simple time series model. International Journal of Forecasting 2020.
Brooks LC, Farrow DC, Hyun S, Tibshirani RJ, Rosenfeld R: Flexible modeling of epidemics with
an empirical Bayes framework. PLoS Comput Biol 2015, 11(8):e1004382.
Kandula S, Yamana T, Pei S, Yang W, Morita H, Shaman J: Evaluation of mechanistic and
statistical methods in forecasting influenza-like illness. Journal of The Royal Society Interface
2018, 15(144):20180174.
Gatto M, Bertuzzo E, Mari L, Miccoli S, Carraro L, Casagrandi R, Rinaldo A: Spread and dynamics
of the COVID-19 epidemic in Italy: Effects of emergency containment measures. Proceedings of
the National Academy of Sciences 2020, 117(19):10484-10491.
Pandey G, Chaudhary P, Gupta R, Pal S: SEIR and Regression Model based COVID-19 outbreak
predictions in India. arXiv preprint arXiv:200400958 2020.
Picchiotti N, Salvioli M, Zanardini E, Missale F: COVID-19 pandemic: a mobility-dependent SEIR
model with undetected cases in Italy, Europe and US. arXiv preprint arXiv:200508882 2020.
Lim B, Zohren S: Time-series forecasting with deep learning: a survey. Philosophical
Transactions of the Royal Society A 2021, 379(2194):20200209.
da Silva RG, Ribeiro MHDM, Mariani VC, dos Santos Coelho L: Forecasting Brazilian and
American COVID-19 cases based on artificial intelligence coupled with climatic exogenous
variables. Chaos, Solitons & Fractals 2020, 139:110027.
Ballƒ± S: Data analysis of Covid-19 pandemic and short-term cumulative case forecasting using
machine learning time series methods. Chaos, Solitons & Fractals, 142:110512.
Majhi R, Thangeda R, Sugasi RP, Kumar N: Analysis and prediction of COVID-19 trajectory: A
machine learning approach. Journal of public affairs:e2537.
13

medRxiv preprint doi: https://doi.org/10.1101/2021.03.14.21253554; this version posted March 24, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY-NC-ND 4.0 International license .

25.
26.
27.
28.
29.
30.
31.
32.
33.
34.

35.
36.
37.
38.
39.
40.
41.
42.
43.
44.
45.
46.
47.

Pinter G, Felde I, Mosavi A, Ghamisi P, Gloaguen R: COVID-19 pandemic prediction for Hungary;
a hybrid machine learning approach. Mathematics 2020, 8(6):890.
Medsker L, Jain LC: Recurrent neural networks: design and applications: CRC press; 1999.
Pascanu R, Mikolov T, Bengio Y: On the difficulty of training recurrent neural networks. In:
International conference on machine learning: 2013: PMLR; 2013: 1310-1318.
Connor JT, Martin RD, Atlas LE: Recurrent neural networks and robust time series prediction.
IEEE transactions on neural networks 1994, 5(2):240-254.
Che Z, Purushotham S, Cho K, Sontag D, Liu Y: Recurrent neural networks for multivariate time
series with missing values. Scientific reports 2018, 8(1):1-12.
Hewamalage H, Bergmeir C, Bandara K: Recurrent neural networks for time series forecasting:
Current status and future directions. International Journal of Forecasting 2021, 37(1):388-427.
Hochreiter S, Schmidhuber J: Long short-term memory. Neural computation 1997, 9(8):17351780.
Gers FA, Schmidhuber J, Cummins F: Learning to forget: Continual prediction with LSTM. 1999.
Chen K, Zhou Y, Dai F: A LSTM-based method for stock returns prediction: A case study of
China stock market. In: 2015 IEEE international conference on big data (big data): 2015: IEEE;
2015: 2823-2824.
Weninger F, Erdogan H, Watanabe S, Vincent E, Le Roux J, Hershey JR, Schuller B: Speech
enhancement with LSTM recurrent neural networks and its application to noise-robust ASR.
In: International conference on latent variable analysis and signal separation: 2015: Springer;
2015: 91-99.
Wang Y, Huang M, Zhu X, Zhao L: Attention-based LSTM for aspect-level sentiment
classification. In: Proceedings of the 2016 conference on empirical methods in natural language
processing: 2016; 2016: 606-615.
Zhao Z, Chen W, Wu X, Chen PC, Liu J: LSTM network: a deep learning approach for short-term
traffic forecast. IET Intelligent Transport Systems 2017, 11(2):68-75.
Ergen T, Kozat SS: Unsupervised anomaly detection with LSTM neural networks. IEEE
transactions on neural networks and learning systems 2019, 31(8):3127-3141.
Bao W, Yue J, Rao Y: A deep learning framework for financial time series using stacked
autoencoders and long-short term memory. PloS one 2017, 12(7):e0180944.
LeCun Y, Bengio Y, Hinton G: Deep learning. nature 2015, 521(7553):436-444.
Sun L, Wang Y, He J, Li H, Peng D, Wang Y: A stacked LSTM for atrial fibrillation prediction
based on multivariate ECGs. Health information science and systems 2020, 8(1):1-7.
Abbas M, Morland TB, Hall ES, El-Manzalawy Y: Associations Between Google Search Trends for
Symptoms and COVID-19 Confirmed and Death Cases in the United States. medRxiv 2021.
Dong E, Du H, Gardner L: An interactive web-based dashboard to track COVID-19 in real time.
The Lancet infectious diseases 2020, 20(5):533-534.
Bavadekar S, Dai A, Davis J, Desfontaines D, Eckstein I, Everett K, Fabrikant A, Flores G,
Gabrilovich E, Gadepalli K: Google COVID-19 Search Trends Symptoms Dataset: Anonymization
Process Description (version 1.0). arXiv preprint arXiv:200901265 2020.
Ramsey JO, Silverman BW: Functional Data Analysis, 2nd edn: Springer; 2005.
Hua Y, Zhao Z, Li R, Chen X, Liu Z, Zhang H: Deep learning with long short-term memory for time
series prediction. IEEE Communications Magazine 2019, 57(6):114-119.
Yu Y, Si X, Hu C, Zhang J: A review of recurrent neural networks: LSTM cells and network
architectures. Neural computation 2019, 31(7):1235-1270.
Cui Z, Ke R, Pu Z, Wang Y: Stacked bidirectional and unidirectional LSTM recurrent neural
network for forecasting network-wide traffic state with missing values. Transportation
Research Part C: Emerging Technologies 2020, 118:102674.
14

medRxiv preprint doi: https://doi.org/10.1101/2021.03.14.21253554; this version posted March 24, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY-NC-ND 4.0 International license .

48.
49.
50.
51.
52.
53.
54.
55.
56.
57.

Prechelt L: Early stopping-but when? In: Neural Networks: Tricks of the trade. Springer; 1998:
55-69.
Kƒ±rba≈ü ƒ∞, S√∂zen A, Tuncer AD, Kazancƒ±oƒülu F≈û: Comparative analysis and forecasting of COVID-19
cases in various European countries with ARIMA, NARNN and LSTM approaches. Chaos,
Solitons & Fractals 2020, 138:110015.
Zeroual A, Harrou F, Dairi A, Sun Y: Deep learning methods for forecasting COVID-19 timeSeries data: A Comparative study. Chaos, Solitons & Fractals 2020, 140:110121.
Liu F, Wang J, Liu J, Li Y, Liu D, Tong J, Li Z, Yu D, Fan Y, Bi X: Predicting and analyzing the COVID19 epidemic in China: Based on SEIRD, LSTM and GWR models. Plos one 2020, 15(8):e0238280.
Raudys SJ, Jain AK: Small sample size effects in statistical pattern recognition:
Recommendations for practitioners. IEEE Transactions on pattern analysis and machine
intelligence 1991, 13(3):252-264.
Prasanth S, Singh U, Kumar A, Tikkiwal VA, Chong PH: Forecasting spread of COVID-19 using
Google Trends: A hybrid GWO-Deep learning approach. Chaos, Solitons & Fractals 2021,
142:110336.
Walker A, Hopkins C, Surda P: Use of Google Trends to investigate loss-of-smell‚Äírelated
searches during the COVID-19 outbreak. In: International forum of allergy & rhinology: 2020:
Wiley Online Library; 2020: 839-847.
Karimuzzaman M, Afroz S, Hossain MM, Rahman A: Forecasting the covid-19 pandemic with
climate variables for top five burdening and three south asian countries. Medrxiv 2020.
Nouvellet P, Bhatia S, Cori A, Ainslie KE, Baguelin M, Bhatt S, Boonyasiri A, Brazeau NF, Cattarino
L, Cooper LV: Reduction in mobility and COVID-19 transmission. Nature communications 2021,
12(1):1-9.
Shi X, Chen Z, Wang H, Yeung DY, Wong WK, Woo WC: Convolutional LSTM network: A machine
learning approach for precipitation nowcasting. Advances in neural information processing
systems 2015, 2015:802-810.

Figures

Figure 1: LSTM Layer

15

medRxiv preprint doi: https://doi.org/10.1101/2021.03.14.21253554; this version posted March 24, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY-NC-ND 4.0 International license .

Figure 2: Architecture of our proposed SLSTM model

Figure 3: Trajectories for COVID-19 daily cumulative per million confirmed (left) and death (right)
cases. The pointwise mean curve is highlighted in black.

Figure 4: Examples of (left) over-estimated predictions (middle) under-estimated predictions (right)
other predictions.

16

medRxiv preprint doi: https://doi.org/10.1101/2021.03.14.21253554; this version posted March 24, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY-NC-ND 4.0 International license .

Figure 5: US States with over-estimated (red), under-estimated (yellow), and others (gray) for the four
prediction tasks: (a) Cases-7; (b) Cases-14; (c) Mortality-7; (d) Mortality-14.

17

medRxiv preprint doi: https://doi.org/10.1101/2021.03.14.21253554; this version posted March 24, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY-NC-ND 4.0 International license .

Figure 6: Mean curve for: over-estimated (red); under-estimated (yellow); others (gray); and all (blue)
US states. Vertical dotted line indicates the beginning of the test time series.

18

