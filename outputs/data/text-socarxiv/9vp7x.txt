Financial Incentives for Downloading COVID–19 Digital Contact Tracing Apps

Jemima A. Frimpong1
Stéphane Helleringer2

NOTE: THIS IS A WORKING PAPER. ADDITIONAL ANALYSES OF STUDY DATA ARE
ONGOING AND WILL BE POSTED IN FUTURE REVISIONS.

Abstract: Contact tracing is a key approach for controlling the COVID–19 pandemic. Traditional
tracing methods might however miss a number of contacts between infected and susceptible
persons. Digital contact tracing apps have been developed to assist health departments in
notifying individuals of recent exposures to SARS-CoV-2. These apps are used in several
countries throughout the world, and some US states have either launched or are planning to
launch such apps. The potential effects of digital contact tracing apps depend however on their
widespread adoption. Most investigations of the determinants of adoption among potential users
have focused on issues related to privacy features (e.g., who can access data, whether location
is recorded) and the accuracy of the app in notifying users of exposures to SARS-CoV-2 (e.g.,
false notifications). In this paper, we investigate whether financial incentives might help further
accelerate the adoption of digital contact tracing apps. We conducted a discrete choice
experiment with an online sample of 394 US residents aged 18–69 years old. We asked
participants to make a series of choices between two hypothetical versions of a digital contact
tracing app characterized by several randomly selected attributes, including varying levels of
financial cost or incentives to download. In this experiment, financial incentives were more than
twice as important in the decision-making process about DCT app downloads than privacy and
accuracy. In order to accelerate adoption, US States planning to launch digital contact tracing
apps should consider offering financial incentives for download to potential users.

1
2

Carey Business School, Johns Hopkins University, Baltimore (USA), Email: jafrimpong@jhu.edu
Bloomberg School of Public Health, Johns Hopkins University, Baltimore (USA), Email: sheller7@jhu.edu

1

INTRODUCTION

The COVID–19 pandemic has caused large numbers of deaths, particularly in the United
States. In the absence of a vaccine or an effective treatment, health departments are seeking
effective approaches to control the spread of SARS-CoV-2 [1], the virus that causes COVID–19.
One such tool is contact tracing, i.e., the process of listing and notifying the contacts an
individual recently infected with SARS-CoV-2 [2]. Following contact tracing, individuals who
have been exposed to the virus are encouraged to self-isolate, in an effort to break chains of
transmission. Existing approaches to contact tracing are however time and labor-intensive, and
could be rapidly overwhelmed by the magnitude of the COVID–19 pandemic [3].

Digital contact tracing apps (“DCT apps”, thereafter) might be a viable tool to improve the
effectiveness of contact tracing for SARS-CoV-2 control. DCT apps often use Bluetooth
technology to keep track of the contacts between app users [4]. They produce logs of these
contacts; and once an app user becomes infected or sick, they can enter this information on the
app, so that recent contacts can be automatically notified and encouraged to self-isolate.
Different versions of DCT apps are used around the world (e.g., Singapore, Australia), or are
currently being launched (e.g., UK, France). Technology firms like Google and Apple have also
developed infrastructure to facilitate the deployment of DCT apps. In the US, reports on
preparations for future phases of COVID–19 control have stressed the role that such apps may
play in complementing standard contact tracing protocols [5]. A few states (e.g., North Dakota)
have recently launched DCT apps.

The effectiveness of DCT apps in reducing the spread of SARS-CoV-2 depends on various
features, including their accuracy in recording contacts between app users and how fast

2

infected users report their status to the app [3]. The main determinant of effectiveness is
however the level of adoption among potential users in affected populations. Modeling studies
suggest that at least 80% of mobile subscribers should download the app to achieve epidemic
control [3]. In prior implementations (e.g., in Singapore), fewer than 20% of subscribers
downloaded the app. In the US, one study found significantly less support for the development
of DCT apps than for the expansion of contact tracing as conducted by health departments [6].
There is thus an urgent need to devise new strategies to promote adoption of DCT apps.

Current research on barriers to the adoption of DCT apps has focused on two domains: privacy
concerns (e.g., GPS location tracking and vulnerabilities for app users), and accuracy of
exposure notifications generated by a DCT app [7,8]. In a recent survey, potential app users
were particularly influenced by an app’s sensitivity in detecting exposures to the coronavirus
(i.e., its true positive rate) [8]. In another survey, respondents expressed preferences for DCT
apps with enhanced privacy features [6].

In this paper, we assess whether providing financial incentives to potential users might
accelerate the adoption of DCT apps. The impact of incentives on the adoption of innovations is
well documented, including in healthcare [9]. The ethical basis for providing incentives to
download a DCT app to potential users has also been discussed extensively [10]. Yet, to the
best of our knowledge, this approach has not been explored with potential users of DCT apps.
We conducted a survey-based experiment to measure the importance of financial incentives in
decision-making processes about DCT apps.

3

DATA SOURCES

We conducted a discrete choice experiment with potential users of DCT apps in the US. This is
a survey methodology in which respondents repeatedly choose between hypothetical versions
of a good or service characterized by a small number of randomly selected attributes [11].
Discrete choice experiments are widely used in marketing and management [12]. Recently,
Zhang et al. [6] used this approach to assess users’ preferences for DCT apps, but their
experimental design did not include financial incentives.

We recruited a sample of residents of the United States, via the Qualtrics online platform. To be
eligible, potential participants also had to a) be aged 18–69 years old, b) own a smart phone, c)
read and understand English, and d) report never having tested positive for SARS-CoV-2. We
excluded people aged 70 years and older because mathematical models of the potential effects
of a DCT app on SARS-CoV-2 transmission [3] have assumed that individuals in that age group
will adopt other preventive measures (e.g., social distancing). We also excluded individuals who
had ever tested positive for SARS-CoV-2 because we assumed that most of them will have
developped (temporary) immunity to the disease, and thus are not at-risk of (re)acquiring or
transmitting the disease. Since DCT apps aim to detect contacts during which the virus can be
transmitted, this “recovered” group would not benefit from information generated by the app.

We asked eligible respondents to complete a survey about attitudes and behaviors related to
the COVID–19 pandemic, including a discrete choice experiment designed to elicit their
preferences about DCT apps. During the experiment, we presented respondents with a series of
10 choices between two randomly selected versions of an DCT app, defined by 6 attributes. We
selected these attributes and their various levels based on a review of the literature on contact
tracing for COVID–19, expert consultations, and descriptions of the development of Bluetooth-

4

based contact tracing apps [4]. Three of these attributes were related to privacy features: (a)
whether the DCT app asks users for their phone number or email at download, (b) whether the
DCT app collects location data (e.g., through GPS tracking), and (c) whether users share their
anonymized data with other app users or the health department. Two of these attributes
concerned the accuracy of the DCT app in notifying users of possible exposures to SARS-CoV2: (d) the rate at which the DCT app makes errors in notifying users of SARS-CoV-2 exposure
(“false positive rate”), and (e) the proportion of exposures to other app users who carry SARSCoV-2 that are notified to users (“true positive rate”). The final attribute was (f) the price or
incentive that a user might pay or receive for downloading the app. The levels of these attributes
are defined in table 1.

[TABLE 1 ABOUT HERE]

We included 5 levels of pricing or incentives, reflecting possible strategies for the roll-out of DCT
apps. In one level, a potential user would be asked to pay $4.99 to download the app, thus
possibly contributing to costs associated with app development, advertising, computing and
maintenance at a time when state budgets face substantial constraints. In another level, the
DCT app would be free to download, whereas in 3 other levels, a potential user would receive
different amounts of financial incentives to download the app ($10, $50 and s$100).

Based on standard sample size calculations, we required at least 250 respondents to detect the
effects of attribute levels listed in table 1 [13]. We used the Qualtrics conjoint tool to generate
the pairs of randomly selected versions of a DCT application, using a fractional factorial design
[14]. In each choice set, the competing versions of the DCT app were labeled as “app 1” and

5

“app 2”. Respondents also had the option not to download any of the two proposed DCT apps.
This ‘opt-out’ option helps increase the external validity of DCE data because downloading DCT
apps will not be mandatory in US States [15]. Respondents are thus not forced to choose
between two (possibly unrealistic) alternatives.

Before making choices between hypothetical versions of a DCT app, respondents were
provided with detailed explanations about contact tracing, DCT apps and their attributes. This
included harmonizing their assumptions about characteristics of a DCT app not represented in
our experimental design (e.g., effects of an app on battery life, data usage). We pre-tested the
online survey with a small number of respondents (n = 6), and we revised our instrument based
on initial feedback. Then, we conducted a larger online pilot (n = 50), during which we also
asked respondents to express their feedback on survey questions and procedures. We further
revised our instrument after this pilot. The dataset analyzed in this paper does not include the
50 respondents who completed the pilot.

As is common practice in online surveys, we included an attention check, approximately a third
of the way through the survey. Respondents who failed that attention check were immediately
excluded from completing the survey. Similarly, we checked the speed at which people
answered questions. Respondents whose response time was less than half of the median time
to complete questions were also excluded from completing the survey. We evaluated
respondents’ understanding of the explanations about contact tracing and DCT apps using six
“quiz” questions, after which we provided them with feedback about the right answers. We also
asked respondents to rate the difficulty of these explanations (on a 5-point Likert scale).

6

METHODS

We described the demographic characteristics of study respondents (i.e., age, gender,
race/ethnicity). We then provided descriptive statistics about the choice data collected during
the survey experiment. This includes the number of choices completed, as well as the number
of choices in which respondents selected the opt-out “no download” option. We also described
respondents’ understanding of explanations provided about contact tracing and DCT apps. Our
analyses of choice data relied on the assumption that potential users of DCT apps are rational
actors, who make choices that maximize their individual utility [16]. We thus estimated a
hierarchical Bayes model of respondents’ preferences for each attribute [17]. We assessed the
effects of the levels of each attribute on individual utility. All variables were effects-coded [18].
We also included an alternative-specific constant to account for the presence of the opt-out, “no
download”, option [15,19]. From the hierarchical models, we obtained utility scores for each
individual, i.e., an indication of how much respondents value each level of an attribute. We
plotted these individual utility scores to obtain an evaluation of the heterogeneity in individual
preferences for various attributes across our sample. Finally, we compared the importance of
each domain of attributes in table 1 (privacy, accuracy, financial incentives) in determining
respondents’ choices. We calculated the importance of each attribute as the difference in
average utility score between the best or most preferred level of an attribute and the worst or
least preferred level of the same attribute [18]. To evaluate the importance of domains (table 1)
measured by several attributes (e.g., accuracy, privacy), we summed the importance of all the
attributes of a domain.

7

RESULTS

We contacted 726 internet users for participation. Among those, 11 did not meet age
requirements, 14 reported not reading/understanding English, 38 did not own a smart phone
and 45 reported having tested positive for coronavirus. Forty-two internet users expressed no
interest in the study at the time of screening, and 143 refused to provide consent for study
participation. Finally, an additional 39 participants were enrolled in the study but later failed an
attention check inserted in the course of the survey. They were thus excluded from the rest of
the study, as per recommendations for online surveys.

The demographic characteristics of the 394 study participants are presented in figure 1. The
sample was predominantly women (n = 268, 68.0%). Close to 1 in 4 respondents belonged to
the 18–24 years old age group. Slightly more than half of the respondents were non-Hispanic
whites (210/394, 53.3%), but other groups were also represented including non-Hispanic blacks
(54/394, 10.8%) and Hispanics (90/394, 22.8%).

[FIGURE 1 ABOUT HERE]

Only 32 out of 394 respondents (8.1%) reported finding explanations about DCT apps “difficult”
or “very difficult”. Out of 6 quiz questions designed to understand respondents’ understanding of
explanations and instructions, the median number of correct answers was 4 among women and
3 among men (figure 2). Three respondents (0.8%) did not answer any of these questions
correctly, whereas 81 respondents (20.6%) answered all questions correctly.

8

[FIGURE 2 ABOUT HERE]

Out of the 3,940 choices they were asked to make, study respondents selected the opt-out “no
download” option 850 times (21.6%). Approximately half of respondents never selected the optout “no download” option (figure 3), whereas close to 10% selected this option at every choice
set.

[FIGURE 3 ABOUT HERE]

Results from hierarchical Bayes models of respondents’ choices indicate that the price/incentive
to download has large effects on utility derived from selecting a DCT app (figure 4). Having to
pay to download an app was associated with significant disutility. Downloading the application
for free, or receiving a $10 incentive to download, were associated with slightly higher utility
levels. Receiving larger financial incentives, on the other hand, increased respondents’ utility,
particularly the highest level of incentive (i.e., $100). Figure 4 also indicates that there was
substantial heterogeneity between respondents in the weight they placed on financial incentives
in their decision-making process. By comparison, the effects of the accuracy of a DCT app on
respondents’ choices were smaller and less heterogeneous (figure 5).

[FIGURES 4 & 5 ABOUT HERE]

9

In comparing the importance of attributes in respondents’ choices to download DCT app (figure
5), we found that prices/incentives carried more weight than effectiveness and privacy. In the
context of this experiment, variations in price/incentive accounted for >50% of respondents’
decision-making process, whereas attributes related to accuracy of the DCT app in detecting
exposures to SARS-CoV-2 accounted for approximately 25% of the decision-making process.
Attributes related to privacy features of the application accounted for <20% of the respondents’
decision-making process.

[FIGURE 6 ABOUT HERE]

DISCUSSION

The available data on the adoption of DCT apps [6,8] suggest that there is a need for strategies
that will motivate potential users to adopt this new tool. In this study, we documented large
effects of prices/incentives on decisions to download DCT apps. In the context of a survey
experiment, prices/incentives were the most important determinant of potential users’ decisionmaking process. We found that charging a relatively small price for the DCT app (i.e., $4.99)
generated significant disutility among potential users. On the other hand, providing incentives of
at least 50$ was associated with an increased utility and a high likelihood of downloading a DCT
app. Other attributes of DCT apps, e.g., their privacy settings and their accuracy, also
influenced the choices of potential users, but at a lesser extent.

10

Our study has several limitations. First, we conducted our discrete choice experiment among an
online sample, which is not representative of the population of potential DCT app users. For
example, women were over-represented in our sample, as were members of younger age
groups (e.g., 18–24 years old). Second, our estimates of the importance of price/incentives in
decisions to download DCT apps pertain solely to the context of our experiment. In another
experimental set-up in which there are no pricing levels requiring potential users to pay for the
DCT app, the importance of prices/incentives in the decision-making process may be attenuated
[18]. However, we repeated our analyses after excluding the choice sets that included an app
version for which potential users were asked to pay for download. In this subset of choices,
incentives to download remained the most important attribute in the decision-making process.

Third, some of the respondents (<10%) reported finding study instructions difficult or very
difficult. Others also failed to answer correctly several of the quiz questions included to assess
their understanding of the experimental set-up. However, we repeated study analyses after
excluding these groups, and we found similar patterns of preferences for attributes of DCT
apps. In addition, in real-world settings, significant numbers of potential users might make
decisions about downloading DCT apps based on partial, incomplete and potentially
misunderstood information. Fourth, we did not systematically investigate heterogeneity in
preferences for attributes of DCT apps across respondents. In particular, there were larger
variations in individual utility associated with pricing/incentives levels than with accuracy or
privacy attributes (figures 4 and 5). Future investigations should investigate which potential
users are more receptive to pricing and incentive signals than others.

Fifth, we did not investigate interactions between attributes. For example, respondents may
express stronger preferences for financial incentives when the proposed DCT app is less
sensitive or yields additional false notifications. When potential users are considering whether or

11

not to download a DCT app, financial incentives might offset the limitations of imperfect apps.
Sixth, we only presented respondents with DCT apps that were characterized by a limited set of
attributes. Instead, we asked respondents to assume specific values for other parameters that
would likely be important in “real-life” conditions. For example, we asked respondents to
assume that they had enough storage space on their phone to download and use each app. We
also asked them to consider that the data charges associated with using a DCT app would not
count towards their data plan usage. Finally, data from discrete choice experiments might be
affected by hypothetical bias: what people indicate they would do during an experiment might
differ from the choices they will make in real-life conditions [20].

Additional research is needed in several areas. On the one hand, future studies should
investigate how potential users react to financial incentives in real-world settings [20], e.g.,
during rapid randomized trials of different incentive levels in states that have already launched
DCT apps. These data might form a stronger basis for projecting the future adoption of DCT
apps, with and without financial incentives to download. On the other hand, it would also be
necessary to consider various incentives structures, with particular attention to individuals who
may be slow to adopt DCT apps and may thus require further inducements. Future studies
should investigate amounts of incentives that may nudge potential adopters at the different
stages of the diffusion of DCTs [21]. There might also be a threshold level of adoption in the
population above which incentives may no longer be needed to foster additional downloads.
Finally, there is a need to understand preferences and the role of financial incentives, beyond
the initial step of downloading a DCT app. For example, future investigations should assess
what are the factors that might sustain use of DCT apps and/or encourage to comply with
requirements to report their COVID status.

12

Our results have important implications. The effectiveness of DCT apps depends on reaching a
critical mass of adopters in a reasonable time frame. Failure to generate a sufficiently high
uptake of DCT apps will be a missed opportunity to improve contact tracing outcomes and to
control the COVID-19 pandemic. It is unlikely however that improvements in privacy features
and accuracy of DCT apps alone will help achieve levels of uptake that are sufficient to affect
epidemic dynamics. Health departments that are considering launching DCT apps should plan
to offer financial incentives to potential users in order to accelerate early adoption of these apps
[21].

ACKNOWLEDGMENTS

We thank Crystal Watson and David Bishai for comments and suggestions on our study
protocol.

13

REFERENCES

1

Wilder-Smith A, Chiew CJ, Lee VJ. Can we contain the COVID-19 outbreak with the
same measures as for SARS? The Lancet Infectious Diseases Published Online First: 5
March 2020. doi:10.1016/S1473-3099(20)30129-8

2

Steinbrook R. Contact Tracing, Testing, and Control of COVID-19-Learning From
Taiwan. JAMA Intern Med Published Online First: 1 May 2020.
doi:10.1001/jamainternmed.2020.2072

3

Ferretti L, Wymant C, Kendall M, Zhao L, Nurtay A, Abeler-Dörner L, et al. Quantifying
SARS-CoV-2 transmission suggests epidemic control with digital contact tracing.
Science Published Online First: 31 March 2020. doi:10.1126/science.abb6936

4

Troncoso C, Payer M, Hubaux J-P, Salathé M, Larus J, Bugnion E, et al. Decentralized
Privacy-Preserving Proximity Tracing. arXiv:200512273 [cs] Published Online First: 25
May 2020.http://arxiv.org/abs/2005.12273 (accessed 30 May2020).

5

A National Plan to Enable Comprehensive COVID-19 Case Finding and Contact Tracing in
the US. Johns Hopkins Center for Health Security.
https://www.centerforhealthsecurity.org/our-work/publications/2020/a-national-plan-toenable-comprehensive-covid-19-case-finding-and-contact-tracing-in-the-us (accessed 26
Apr2020).

6

Zhang B, Kreps S, McMurry N. Americans’ perceptions of privacy and surveillance in
the COVID-19 Pandemic. Published Online First: 13 May 2020. doi:10.31219/osf.io/9wz3y

7

Redmiles EM. User Concerns & Tradeoffs in Technology-Facilitated Contact Tracing.
arXiv:200413219 [cs] Published Online First: 12 May 2020.http://arxiv.org/abs/2004.13219
(accessed 30 May2020).

8

Kaptchuk G, Goldstein DG, Hargittai E, Hofman J, Redmiles EM. How good is good
enough for COVID19 apps? The influence of benefits, accuracy, and privacy on
willingness to adopt. arXiv:200504343 [cs] Published Online First: 18 May
2020.http://arxiv.org/abs/2005.04343 (accessed 30 May2020).

9

Giuffrida A, Torgerson DJ. Should we pay the patient? Review of financial incentives to
enhance patient compliance. BMJ 1997; 315:703–707.

10 Kahn J, Technologies JHP on E and G of DCT. Digital Contact Tracing for Pandemic
Response: Ethics and Governance Guidance. Johns Hopkins University Press; 2020.
https://muse.jhu.edu/book/75831 (accessed 30 May2020).
11 Ryan M. Discrete choice experiments in health care. BMJ 2004; 328:360–361.
12 Zwerina K. Introduction. In: Discrete Choice Experiments in Marketing: Use of Priors in
Efficient Choice Designs and Their Application to Individual Preference Measurement.
Zwerina K (editor). . Heidelberg: Physica-Verlag HD; 1997. pp. 1–10.

14

13 de Bekker-Grob EW, Donkers B, Jonker MF, Stolk EA. Sample Size Requirements for
Discrete-Choice Experiments in Healthcare: a Practical Guide. Patient 2015; 8:373–
384.
14 Conjoint Analysis White Paper. Qualtrics Support.
2019.https://www.qualtrics.com/support/conjoint-project/getting-started-conjoints/gettingstarted-choice-based/conjoint-analysis-white-paper/ (accessed 30 May2020).
15 Campbell D, Erdem S. Including Opt-Out Options in Discrete Choice Experiments:
Issues to Consider. Patient 2019; 12:1–14.
16 McFadden D. The Choice Theory Approach to Market Research. Marketing Science
1986; 5:275–297.
17 A Bayesian hierarchical model for discrete choice data in health care - Anna Liza M Antonio,
Robert E Weiss, Christopher S Saigal, Ely Dahan, Catherine M Crespi, 2018.
https://journals.sagepub.com/doi/full/10.1177/0962280217704226?casa_token=ziRK8XPXQ
fQAAAAA%3A2VMhE8ulNC98QtftI_Fi2gDZNYf3VOxauDtzZ3Y_QYeqNFXaHPx-gU13NIj5sgbHdFdkP6l7IWt (accessed 31 May2020).
18 Hauber AB, González JM, Groothuis-Oudshoorn CGM, Prior T, Marshall DA, Cunningham
C, et al. Statistical Methods for the Analysis of Discrete Choice Experiments: A Report
of the ISPOR Conjoint Analysis Good Research Practices Task Force. Value Health
2016; 19:300–315.
19 Veldwijk J, Lambooij MS, Bekker-Grob EW de, Smit HA, Wit GA de. The Effect of
Including an Opt-Out Option in Discrete Choice Experiments. PLOS ONE 2014;
9:e111805.
20 Buckell J, Hess S. Stubbing out hypothetical bias: improving tobacco market
predictions by combining stated and revealed preference data. Journal of Health
Economics 2019; 65:93–102.
21 Rogers EM. Diffusion of innovations. 3rd ed. New York : London: Free Press ; Collier
Macmillan; 1983.

15

Age distribution
.2

.4

.6

Racial/ethnic distribution
.8

1

0

1

1

.75

.75
Proportion by gender

Proportion by gender

0

.5

.25

.2

.4

.6

.8

1

.5

.25

0
18–24y

25–34y

35–44y

45–54y

≥55y

Women

0
Non-Hispanic
blacks
Non-Hispanic
whites

Hispanics
Other
groups

Men

Figure 1: distribution of demographic characteristics among study participants

16

Women

4
2
0

Number of quiz questions
correctly answered (out of 6)

6

Men

Figure 2: respondents’ performance in comprehension checks

17

.6
.4
Proportion
.2
0

0

2
4
6
8
Number of times selected the "opt-out" option
Men

10

Women

Figure 3: distribution of respondents by number of times selected the “opt-out” option
during the discrete choice experiment (out of 10 choices).

18

Effects of price/incentives to download on utility
Users pays $4.99

App is free

User gets $10

User gets $50

User gets $100
-5

-4

-3

-2
-1
0
1
2
Individual utility scores

3

4

5

Figure 4: effects of price/incentives to download on respondents’ utility.
Notes: the box plots represent the distribution individual scores for each level of this attribute.
Individual scores were obtained from Hierarchical Bayes’ models.

19

Effects of false notifications on utility
1 in 100 notification
is an error

5 in 100 notifications
are errors

15 in 100 notifications
are errors

-5

-4

-3

-2
-1
0
1
2
Individual utility scores

3

4

5

Effects of DCT app sensitivity on utility
60% of contacts
with infected
app users
are notified

80% of contacts
with infected
app users
are notified

95% of contacts
with infected
app users
are notified
-5

-4

-3

-2
-1
0
1
2
Individual utility scores

3

4

5

Figure 5: effects of accuracy attributes of a DCT app on respondents’ utility.
Notes: the box plots represent the distribution individual scores for each level of this attribute.
Individual scores were obtained from Hierarchical Bayes’ models. The box plots are plotted on
the same scale as figure 4, to allow visualizing differences in attribute importance for decisionmaking.

20

100
75
50

Attribute importance (in %)

25
0

Price/incentives

Accuracy

Privacy

Figure 6: estimates of attribute importance in determining respondents’ choices about
downloading digital contact tracing apps.
Notes: these estimates were obtained by comparing average utility levels associated with lowest
and highest levels of an attribute for each respondent, then summing this importance estimates
across all respondents. Finally, we divided each estimate of attribute importance by the sum of
all attributes’ importance estimates.

21

Domain
Accuracy

Attributes
False notifications

Levels
1 in 100 notifications received from
the app is an error
5 in 100 notifications received from
the app are errors
15 in 100 notifications received from
the app are errors
Sensitivity
You are notified about 60% of your
contacts with infected app users
You are notified about 80% of your
contacts with infected app users
You are notified about 95% of your
contacts with infected app users
Privacy
User details
App does not ask for user details
(health dept. cannot contact you)
App asks for phone number or email
(health dept. can contact you)
Location
App does not collect any location data
App asks user for zip code
App tracks location (by GPS)
Data sharing
You make your own COVID status
available only to other app users
You make your own COVID status
available to health department
You make your own COVID status
and list of contacts available to health
department
Price/incentives
Price/incentive to
User pays $4.99
download
App is free
User gets $10
User gets $50
User gets $100
Table 1: attributes and levels included in the discrete choice experiment.

22

