Original Research Article

Making sense of algorithms: Relational
perception of contact tracing and risk
assessment during COVID-19
Chuncheng Liu

Big Data & Society
January–June: 1–13
! The Author(s) 2021
Article reuse guidelines:
sagepub.com/journals-permissions
DOI: 10.1177/2053951721995218
journals.sagepub.com/home/bds

and Ross Graham

Abstract
Governments and citizens of nearly every nation have been compelled to respond to COVID-19. Many measures have
been adopted, including contact tracing and risk assessment algorithms, whereby citizen whereabouts are monitored to
trace contact with other infectious individuals in order to generate a risk status via algorithmic evaluation. Based on
38 in-depth interviews, we investigate how people make sense of Health Code (jiankangma), the Chinese contact tracing
and risk assessment algorithmic sociotechnical assemblage. We probe how people accept or resist Health Code by
examining their ongoing, dynamic, and relational interactions with it. Participants display a rich variety of attitudes
toward privacy and surveillance, ranging from fatalism to the possibility of privacy to trade-offs for surveillance in
exchange for public health, which is mediated by the perceived effectiveness of Health Code and changing views on
the intentions of institutions who deploy it. We show how perceived competency varies not just on how well the
technology works, but on the social and cultural enforcement of various non-technical aspects like quarantine, citizen
data inputs, and cell reception. Furthermore, we illustrate how perceptions of Health Code are nested in people’s
broader interpretations of disease control at the national and global level, and unexpectedly strengthen the Chinese
authority’s legitimacy. None of the Chinese public, Health Code, or people’s perceptions toward Health Code are
predetermined, fixed, or categorically consistent, but are co-constitutive and dynamic over time. We conclude with a
theorization of a relational perception and methodological reflections to study algorithmic sociotechnical assemblages
beyond COVID-19.
Keywords
Sociotechnical assemblage, algorithm, contact tracing, surveillance, COVID-19

Introduction
The relationship between algorithms and individuals is
dynamic and tempestuous in the era of Big Data.
Algorithms and related technologies have been
employed for multiple social purposes, including but
not limited to public health efforts to fight COVID19 (French and Monahan, 2020; Kitchin, 2020).
Accordingly, an enormous commercial market has
emerged for data about individuals (Fourcade and
Healy, 2017; Zuboff, 2019). Additionally, state institutions have increased appetite for this data, which is
useful for various algorithmic governance measures
(Liu, 2019; Rona-Tas, 2020). Today, issues regarding
trust, privacy, and surveillance dominate the ethical
and social debates on these technologies. Examples
include the fairest ownership models for data on

individuals, effective trade-offs between privacy and
safety, and the appropriateness of deploying algorithms with inscrutable, or black-boxed, statistical
reasoning (Eubanks, 2018; Mittelstadt et al., 2016;
Pasquale, 2015; Zuboff, 2019).
Scholarship in this area often focuses upon the technical function of the algorithm, attempting to open the

Department of Sociology, University of California San Diego, La Jolla,
USA
Both authors contributed equally to this work.
Corresponding author:
Chuncheng Liu, Department of Sociology, University of California San
Diego, La Jolla, CA 92093-0021, USA.
Email: chchliu@ucsd.edu

Creative Commons NonCommercial-NoDerivs CC BY-NC-ND: This article is distributed under the terms of the Creative Commons
Attribution-NonCommercial-NoDerivs 4.0 License (https://creativecommons.org/licenses/by-nc-nd/4.0/) which permits non-commercial
use, reproduction and distribution of the work as published without adaptation or alteration, without further permission provided the original work is
attributed as specified on the SAGE and Open Access pages (https://us.sagepub.com/en-us/nam/open-access-at-sage).

2
“black box” (Pasquale, 2015). The implicit logic of
these studies is that, by understanding what the algorithm does in terms of information processing, it can be
altered and tweaked to mitigate undesirable social outcomes or enhance desirable ones. But algorithms are
not merely lines of code or sequences of automated,
executable digital rules. They are not solely digital or
technical objects. Rather, algorithms are created by
people, often within institutions, and constantly undergo change and development (Kitchin, 2017; Liu, 2020;
Seaver, 2017). They are designed toward certain ends –
these ends can shift over time in response to social,
economic, cultural, or political phenomena, thereby
affecting the algorithms function. Moreover, the ends
of one algorithm can conflict with and be affected by
other algorithmic systems targeting different ends, as
well as the goals or values of other people and groups.
These incentives and influences partially shape the digital operations of the algorithm. Shoshanna Zuboff, for
instance, advocates thinking of algorithms as both
products and expressions of the institutional logics
that afforded their creation – this amounts primarily
to a “logic of accumulation,” whereby “hyperscale
assemblages of objective and subjective data about
individuals” are used for “knowing, controlling and
modifying behavior to produce new varieties of commodification, monetization and control” (Zuboff,
2019). This kind of analytical emphasis remains
“technology-side” – namely large technology corporations like Google, Facebook, and Alibaba. The general
public, despite being the locus of socio-ethical concerns, are homogenously framed as pliable, acquiescent, or unwitting subjects to corporate agendas. By
extension, analysis of the needs and perspectives of
the public on issues like privacy or surveillance
protections are similarly group-based, static, and
homogenous.
But the meaning of technologies is constantly
formed and reformed in response to how varied individuals, groups, and institutions interact with them
over time (Knorr Cetina, 1999). The designers, creators, and owners of a technology offer only a partial
account of the meaning and function a technology has
in society, and algorithms are no different. They
change over time through dynamic interplay not only
with the data they receive and the coders making
updates and edits but also the heterogeneity of individuals who use it (Amelang and Bauer, 2019; Brayne and
Christin, 2020). Indeed, in recent years, public cognizance of algorithms has advanced immensely as they
interact with them increasingly regularly, in a greater
variety of contexts and with a diversifying array of
practices. An operational algorithm is intrinsically
entangled with the individuals engaging with it, and
this engagement varies widely. Therefore, how people

Big Data & Society
engage with an algorithm is not merely an “input”’ –
rather, it is a constituent part of the algorithm itself
(Introna, 2016; Kitchin, 2017). Furthermore, these
interactions are shaped in an ongoing fashion by the
perception each individual has of the algorithm, as
their perspective guides their interpretation of the technology and their behavior while using it. This concept
has been coined as the “algorithmic imaginary” – “the
way in which people imagine, perceive and experience
algorithms and what these imaginations make possible” (Bucher, 2017: 13).
Accordingly, critical algorithm scholars suggest that
apt social or cultural investigations of algorithms must
treat them as dynamic and unstable objects, ones that
are enacted not only through an automated digital process but by the perceptions, imaginaries, and practices
of the individuals who interact with them in their everyday lives (Christin, 2017; Introna, 2016; Liu, 2020;
Seaver, 2017). This requires “an approach to algorithms informed by their empirical profusions and
practical existence in the wild – always at the boundaries of diverse communities of practice” (Seaver, 2017:
2). Algorithms, we claim, are more accurately conceptualized as sociotechnical assemblages, whereby the
technology cannot be separated from the social factors
that constantly mediate it. The social is heterogenous –
the public is not a monolithic bloc of “users” of an
algorithm, but instead a diverse collection of individuals loosely bound by their distance from the algorithms
technical components. We thus advocate for a relational view of the social, taking guidance from Pierre
Bourdieu:
one has to avoid turning into necessary and intrinsic
properties of some group . . . the properties which
belong to this group at a given moment in time because
of its position in a determinate social space and in a
determinate state of the supply of possible goods and
practices . . . one has to deal with a set of social positions which is bound by a relation of homology to a set
of activities . . . or of goods . . . that are themselves characterized relationally. (Bourdieu, 1998: 4–5)

Angele Christin (2017) has conducted studies in this
vein, addressing how people make sense of an algorithm in various contexts. Christin uncovered notable
gaps between the intended and actual uses of algorithms designed for journalists and criminal court
employees. She observed an iterated and unstable
reformulation of the algorithms meaning, animated
by ongoing critique and behavioral modification
while using it. Specific, dynamic imaginaries of the
algorithm were cultivated over time that were
“mobilized differently to comply with or resist algorithmic technologies” (Christin, 2017: 11). In a similar

Liu and Graham
recent study of policing and criminal courts, Brayne
and Christin (2020) show how the initial justifications
for using algorithms – neutrality and impartiality –
clash with employees who perceive a threat to their
agency, expertise, and discretion. This sparked distinctive internal methods of resistance, motivating the
masking and relocation of discretionary individual
decisions to unseen areas of the organization. These
studies highlight how understanding algorithms in
social settings requires paying attention to sensemaking practices and interpretation in local context.
In this paper, we present qualitative data on the
interplay of practice and sense-making by individuals
using Health Code (jiankangma), the algorithmic contact tracing and risk assessment sociotechnical assemblage for COVID-19 in China. One way people make
sense of Health Code is in terms of privacy. Some consider the invasion of privacy justifiable in the name of
public health, while others cynically consider privacy
illusory. A minority, mainly young people, find the
extra violations of privacy intolerable. Individuals
also understand Health Code in relation to wider technological trends. Some communicate acceptance of,
and faith in, the advancing Chinese digital society
and its associated benefits, generating trust in the technology. Others consider China’s overall digital progress
and utility to be overblown, informing skepticism of
the technology. Finally, people understand Health
Code with respect to its implementation. To some,
the social rearrangements necessary to accommodate
the sociotechnical assemblage are viewed as an effective
expression of care by the state. To others, it is overwrought, disruptive, and labor-intensive when compared with other more traditional disease control
methods. Individuals express concern about the accuracy of data input by citizens, as well as the normalization of the surveillance infrastructure once the
pandemic is resolved.

Perceiving algorithms in a pandemic
The perceived threat algorithms pose to individuals,
including notions like privacy and surveillance, has
been altered by the onset of COVID-19 – for instance,
citizens of various countries all show some greater willingness to forego civil liberties in order for their governments to better attend to the pandemic, civil
liberties that protect digital privacy and surveillance
(Alsan et al., 2020). Public health systems rely on surveillance infrastructure and risk assessment for
disease detection and intervention (Armstrong, 1995;
Foucault, 1990). Since pandemics like COVID-19 are
intrinsically inclusive, either directly or indirectly
affecting everyone (French and Monahan, 2020),
public health authorities have more latitude to deploy

3
surveillance structures than before the pandemic.
A combination of measures known as contact tracing
and risk assessment has become widespread as a result.
Contact tracing is a public health technology that has
been used to combat infectious diseases for some decades. In short, it identifies as many infected individuals
as possible, in addition to individuals they have come
into contact with (Center for Disease Control, 2020).
This effort helps prevent disease transmission in two
ways. First, by pinpointing individuals for direct interventions (e.g. quarantine). Second, by generating
macro-level data from which trends can be assessed
for further public health and policy interventions
(Tang, 2020).
The predominant method for contact tracing for
COVID-19 is algorithmic, using mainly smartphones
to collect data (Collado-Borrell et al., 2020). Their
ubiquity, the high likelihood that they will remain on
an individual’s person, and powerful inbuilt datacapture technologies make smartphones a fairly comprehensive and systematic way of collecting contact
information. Data from contact tracing is processed
by algorithms to provide users with risk assessment
on the same digital device. For COVID-19, risk assessment refers to the risk that an individual may be infectious and, therefore, must quarantine. Real-time
automated digital deployment via algorithms means
contact tracing and risk assessment in COVID-19 are
tightly coupled together – both the input data and the
output risk status occur via the same device. Despite
the public health urgency of COVID-19, some scholars
still critique the notion of digital contact tracing and
risk assessment, suggesting it opens the door for civil
liberties violations (Bengio et al., 2020; Kitchin, 2020;
Tang, 2020). This includes country-specific critiques in
the United Kingdom (Edwards et al., 2020), Poland
(Nielsen, 2020), and Russia (Ilyushina, 2020).
But are these potential violations of civil liberties
important to users? How do individuals perceive and
make sense of these measures in their everyday contexts? To investigate this, we take China as our case.
China was one of the first countries to adopt contact
tracing and risk assessment algorithms in the
COVID-19 pandemic (Cha, 2020; Loder, 2020). This
continues a pattern of acceptance by Chinese residents
of surveillance that scholars have tried to explain in a
few ways. Lü (2005) suggests that, culturally speaking,
privacy is an instrumental good in China (i.e. its value
is in service of some higher ideal), contrasting with
being an intrinsic good in the West (i.e. its importance
is self-justifying). As a result, it is easier for Chinese to
trade-off “privacy” with other social goods such as
safety (Zhang et al., 2019) or trust (Kostka, 2019).
Others have suggested that the Chinese public simply
responds with a sophisticated, judicious brand of

4
self-censorship (Shao, 2020). This is possible due to
higher knowledge of surveillance technologies in
China than other nations (Liang and Chang, 2006),
and high awareness of liberal data-sharing between
the state and private companies producing these digital
goods (Hou, 2017). The SARS (2003) and MERS
(2012) outbreaks also previously inculcated infrastructure and awareness of contact tracing and risk assessment for infectious disease outbreaks (Au et al., 2020;
Cha, 2020) – early evidence suggests Chinese people are
generically more accepting of privacy invasions for
COVID-19 too (Alsan et al., 2020). These arguments
have merit but are overly generalized and static for
such a large and varied polity. Channeling critical algorithm studies, we believe a fuller understanding of algorithmic sense-making necessitates examining the
algorithm in dynamic interplay with the practices and
perceptions of the individuals who use it during the
pandemic.
China’s contact tracing and risk assessment procedure employs an algorithmic sociotechnical assemblage
called Health Code; both the technology and its implementation are a conjoined effort between national and
local governments and two large technology companies, Alibaba and Tencent (Fang, 2020). It is run on
the ubiquitous Chinese smartphones APPs WeChat
(owned by Tencent), Alipay (owned by Alibaba), or
on its own. While much of the personal and demographic data used for the APP is already freely available to the government (Fang, 2020), other novel data
is collected passively in real-time (e.g. location) or
through mandated daily user entries (e.g. health
status, symptoms). By the end of March, almost all
Chinese cities had their own Health Code variant.
Although nationwide in terms of coverage, Health
Code is not a fully centralized system. While both
Tencent and Alibaba leverage a system architecture
known as Elasticsearch, each city’s contact tracing is
slightly different. Today, a suite of national, provincial,
and municipal-level systems co-exist and are used for
slightly different ends. Officially, the government
organizes the data into four categories: (1) personal
data (name, gender, ID card number, phone number,
etc.); (2) personal health data (temperature, symptoms,
high-risk people contact history, etc.); (3) location data
(visit history from provincial level to district level, commute history, etc.); and (4) health status data (health
status, testing result, etc.).
A user is first required to verify his/her real name
and finish a long survey to initiate Health Code. In
some cities, facial data is also required. In addition to
passive collection of a user’s location, Health Code
requires individuals to scan QR codes strategically
placed at checkpoints in public or busy spaces, such
as buses, supermarkets, or residential communities

Big Data & Society
(Liu, 2020). This data effectively traces the individual.
When combined with health data, Health Code generates real-time risk status evaluations presented to the
user after scanning a QR code checkpoint. Symbols of
an individual’s “health status” come in three colors:
green, yellow, or red. Green means healthy and low
risk, yellow moderate risk, and red high risk or symptomatic. Individuals must produce their color code on
demand for inspectors at checkpoints. For example, in
some cities, a passenger needs to scan the QR code at
the door and show the driver his/her color code before
boarding a bus. The immediate consequences of
obtaining yellow or red Health Codes vary regionally.
Generally, they include isolation or quarantine, and
more rigorous health reporting until they are reclassified as green.

Method
This study used qualitative data from 38 in-depth,
semi-structured interviews conducted by the first
author and two research assistants from late April to
late June. We used diverse channels to recruit interviewees, including social media platforms and snowballing methods for contacts of those we had already
interviewed. We recruited our interviewees with a
poster inviting people who had used Health Code in
the past two months to discuss their experiences. To
ensure we captured the breadth of people’s experiences
and perceptions of Health Code, we purposively diversified our sample based on gender (23 females and
15 males), location (15 provinces), age (18 to 55, mean:
29), and cross-city mobility during the past two months
(21 traveled to other cities during the pandemic). We use
pseudonyms to protect interviewees’ privacy.
We conducted telephone interviews via WeChat, a
popular Chinese social media APP in Mandarin
Chinese. Questions regarding personal experiences of
the pandemic, experiences and perceptions of Health
Code, and privacy concerns were asked. While questions
specifically referenced “Health Code (jiankangma),” our
interviewees often discussed their experiences and perceptions of the “algorithm (suanfa),” “implementation
(shishi),” “platform (pingtai),” “big data (dashuju)” of
Health Code. All interviews were recorded with consent
and transcribed to text for analysis. Each interview
lasted from 30 to 90 minutes. In addition to interview
data, we also collected policy documents, newspaper
articles, and social media posts that are related to
Health Code to further contextualize our study. This
study was reviewed and approved by the IRB at
University of California, San Diego (#200729XX).
The first author translated the Chinese quotes to
English and used MAXQDA 12 to analyze all the qualitative materials. As we aim to show a diverse range of

Liu and Graham
sense-making processes, we used thematic analysis. We
first familiarized ourselves with the data by unstructured coding and discussed the themes that emerged.
We then reviewed, revised, and reclassified those
unstructured codes into a structured codebook for
analysis of the data. We adopted the abductive analysis
approach (Timmermans and Tavory, 2012) in code and
theme development. In this paper, we focused on three
general themes of sense-making about Health Code:
privacy, technology, and implementation. We do not
claim to exhaustively present every type of sensemaking for Health Code – instead, we order our results
based on perceived importance and current literature
(such as the “privacy tradeoffism” subtheme) and our
interviewees’ responses (such as the reoccurrence of
“Not (that) private” subtheme). These different
themes and sub-themes do not necessarily map different kinds of people. As we will show, different themes
are interconnected and often coexist in one person.
Sometimes they are complementary, while other times
they conflict with each other.

Making sense of privacy
Privacy tradeoffism
With only two exceptions, the majority of our interviewees agree that contact tracing surveillance and
risk assessment are necessary. As the country first-hit
by the COVID-19 pandemic, and having experienced
prior social panic during the SARS and epidemics, the
Chinese public is generally cooperative with the state’s
strict disease control strategy (Au et al., 2020). Most
people understand the potential privacy invasions of
Health Code, rendering the use of it as a trade-off.
“Nothing is absolute” is a common response (n ¼ 6)
from older interviewees (above 35 years old) on the
issue of privacy. “There is no absolute freedom”
Cuiping, a bank manager said, “A society is filled
with more and more people and thoughts, facing a
pandemic like this, how to manage them if freedom is
absolute?” Some people consider using Health Code as
a trade-off between privacy and personal safety. As
Qiangzi put explicitly, “It’s not really a time to discuss
privacy. Nothing matters when it comes to life.” Others
consider the trade-off as being between privacy and
collective safety. Haiyan further elaborated:
Health Code is an invasion of individual privacy.
However, we do not know much about the virus,
how to treat or prevent it. It is reasonable in this situation to constrain mobility and collect personal data –
to protect us, to save time (for treatment). (emphasis
added)

5
People also consider the use of Health Code as a tradeoff between personal interest and the collective economic good. Yuejin, a retired high school biology
teacher believed that “we (retired people) could stay
at home forever. Yet if our country and young people
stay quarantined for too long without economic activities, they will both suffer greatly.” Health Code’s surveillance is essential for identifying “risky” individuals
and therefore liberating the “healthy” population,
thereby allowing the national economy to reopen. As
Aimi put, “it released the society’s mobility and productivity, so we as a society could go back to normal.”

Privacy fatalism
A more cynical response to the privacy concern is what
we call “privacy fatalism,” a belief that privacy concerns are irrelevant because people cannot and will
not have privacy in China in the era of Big Data.
This fatalism is a response to the power nexus of the
authoritarian state and surveillance capitalism of large
technology corporations. Many interviewees believed
there was simply no need to discuss the privacy problem of contact tracing at all when probed by the interviewer about their concern on privacy of Health Code.
Aimi, an avid supporter of the Health Code, argued
that “worries about privacy are useless. Why?
Because in China no one has privacy. Health Code
will not be the last straw that crushes the camel.”
The Chinese government has imposed a variety of
intensive surveillance and censorship systems on its citizens (Hou, 2017; Liang and Chang, 2006; Liu, 2019).
Public monitors are prevalent in urban spaces. One
needs to use their ID card for practically all civic activities, from purchasing a long-distance bus ticket to
booking a hotel room. Using the Chinese internet
demands use of your real name. Runze, a journalist
locked down in his hometown of Hubei, therefore did
not hesitate to submit all information requested by
Health Code and was unsurprised when the police
immediately contacted him by phone and then went
to his home for information checking. “There are no
new problems here [. . .] everyone is naked now; you
just simply don’t have privacy in front of this state.
[. . .] What’s the point of worry when you cannot
change it?”
Meanwhile, the intensive surveillance of citizens by
large technology companies before the pandemic also
generated fatalism. Alibaba and Tencent, the two most
influential technology companies in China, have huge
user bases and thus influence in every aspect of Chinese
life (Chen et al., 2018). As Haiyan joked, “It is impossible to not using them. They did give me lots of convenience. I cannot uninstall them all and live like a
primate.” The Chinese government’s relatively weak

6

Big Data & Society

legislation and enforcement of public data privacy protections has fueled frenzied collection of personal data.
People are accustomed to it and, more importantly, are
unable to contest it. As Jianguo mentioned, “Alibaba
and Tencent are so powerful like monsters. An individual is powerless in front of them.”

Privacy protectionism
Although almost all interviewees express some degree
of dissatisfaction with Health Code’s surveillance, most
of them make sense of it either through tradeoffism or
fatalism. Only a small fraction (n ¼ 5) outright refuses to
justify its invasion of privacy. Sociodemographically,
these people are mostly below 30, live in big cities
(Beijing, Shanghai, and Guangzhou), have been educated at elite institutions, and often have studied
abroad. Their responses to the indifference or defenses
of the surveillance from Health Code by others vary.
For example, When Wutong was asked “why should a
good person care about privacy” – a common response
from the people who are indifferent to this issue – he
joked, “It’s like people have to use the toilet. It’s
normal and nothing wrong but I just don’t want the
world to know where I shit.” Ningning argued, “it is
like watching porn – of course, it is not bad, but do you
want others to know your Pornhub browse history?”
These positions did not necessarily exist before Health
Code was released – some were realized and articulated
alongside its implementation. For example, Jianguo
thought:
Health Code brought the state’s surveillance system
from the background to the front. In the past, it
looks at you silently. Wherever you go and whatever
you say, some kinds of surveillance are there. Yet the
surveillance became the front stage now (during the use
of the Health Code), and you have to accept it to maintain the normal social interactions [. . .] this makes me
feel quite uncomfortable.

The quick deployment of the Health Code materializes
and discursively emphasizes the previously hidden surveillance infrastructures in China. It explicitly highlights the data harvesting and analytic capacity of the
state. For many people, Health Code triggered a realization of the state’s facility for collecting private data
and the significance this holds for everyday life. While
the state of emergency temporarily limits some of
Health Code’s more quotidian effects, it nevertheless
solicits reflection among people about how life and
society have changed from before the pandemic to
after it. As Pingping expressed, “In the past at least
the government collect your data behind you back,

now they are doing this in front of your face, and
who knows what will happen next?”

Not (that) private
The three subsections above show people’s different
sense-making processes regarding Health Code’s invasion of privacy. Yet, for some, data collected by Health
Code is considered general information insufficiently
private to warrant protection. Some interviewees, particularly older ones, say that it does not matter that
Health Code collects this data as a leak would be
inconsequential to them. Caixia, a community government staffer expressed this idea:
Interviewer: Do you think there are some privacy issues
here?
Caixia: I don’t think so! Privacy . . . I am just a laobaixing (normal person), what are things I don’t want
others to know? I am not a famous person or
something.
Interviewer: How about data leaking?
Caixia: That’s for rich or famous people, we laobaixing
have nothing to leak!
Interviewer: Have you ever received any fraud message
with your accurate personal information or something?
Caixia: Yes, but if you don’t covet things you don’t
deserve, then you should not fall into fraud.

Such strong indifference of personal data is rare among
young people. Yet many young people also believe the
data Health Code collected is disconnected from the
reality of privacy, highlighting the variety of the data
it collected. Ningning, for example, argued that “it
(data) is just a record of my location, but it does not
know what I was doing at that location, so it’s not
privacy.” For similar reasons, many people believe
the “big data surveillance” (Lyon, 2018) of the
Health Code is not actual surveillance. Ningning was
very cautious about privacy, yet she hesitated in calling
Health Code a surveillance system: “surveillance is like. . . having a person behind a monitor that specifically
stares at me for 360 degrees.” Lingling expressed a similar idea, “surveillance is like it is looking at me specifically, yet this kind of big data does not focus on me
but a group of people, so that feels very different.”
Indeed, the boundary between private and general
information is not absolute or fixed. Nor is it considered a binary. Many people define privacy as personal
data that could be used to harm them. The critical
reason why the variety of the data matters is that
only certain data can be used for other, consequential
purposes. As Dawei expressed, “I don’t think the location data is privacy – what’s the harm I can receive
from these?” Yet, when we discussed the facial data

Liu and Graham
that some Health Code systems collect, he was more
concerned: “I heard that frauds could use your facial
data to apply for loans from different financial APPs,
that would be very dangerous. So that’s privacy.”
Yet merging different databases allows data of
seemingly limited variety to be combined into a thick
bundle that opens new possibilities for problems and
profiteering, transforming “non-private” information
into information directly pertaining to privacy concerns. For many, this concern applied to private companies specifically. As Ningning said:
It is just unsafe for companies to have too much data.
At least the state just collects data without using them
(for other purposes). Yet companies collect, sell, and
combine different databases to harass you constantly.
[. . .] All the private companies are interconnected, and
I don’t know where my data in one APP will be shared
to another and form a full profile of myself.

Depending on who is using it, and how it is used, the
same information could be considered as private and
non-private simultaneously. When personal information is collected, people like Ningning admitted they
preferred to have this data collected by the state than
private companies. Huadong expressed caution about a
government power expansion, yet ultimately admitted
that:
It feels different if this information is handed to the
government, at least it will not use my data to sell
stuff [. . .] Companies are so unreliable. We often get
some spam messages or phones with our personal
information used; I believe most of them are leaked
from different companies.

Making sense of technology
Trust in technology
“It is big data!” was the common refrain (n ¼ 13) when
we asked, “how do you think the Health Code determines your risk status?” Yet for our inevitable followup question – “what is big data?” – drew long pauses,
hesitation, and uncertain answers like “just . . . big
data?” Clearly, most people do not intimately understand how Health Code collects, organizes, and processes data to determine risk status. But this did not
stop people from trusting and revering Health Code.
As Jianguo expressed:
People are very accepting of these new things in China.
Big data, AI, or cloud computing [. . .] they are inevitable to be used and are in fact dominating some

7
aspects of our life. Also, we did see how much these
technologies could do, and that explains the
admiration.

Besides this generic faith in technological advancement
and its promise of human flourishing, many Chinese
people also perceive technology as objective and ameliorable. Problems might happen, but many give the
technology the benefit of doubt. Aimi, a product manager working in the technology sector, argued that:
I don’t believe the algorithms (suanfa) would misjudge.
If that ever happened, it must be the problem of
humans who operate it [. . .] algorithms execute what
was programmed efficiently and never makes mistakes,
human does. But mistakes could be solved, and the
algorithms could be improved. Algorithms need time
to optimize. After all, this was developed in such a
short time.

This perceived technical objectivity is particularly
favorable when compared with other solutions necessitating a greater degree of human involvement. For
example, Nana believed using Health Code to
collect health information to be a good choice because
information is directly uploaded to the platform without a human intermediary: “It’s safer compared with
filling surveys and handling my data to government
staff in my neighborhood, who might have a peep
and invade my privacy – similar things happened
before.”
Trust in the technology generates a sense of safety
for people who use Health Code, which accordingly
enhances people’s trust in it further. From the simple
three-color system, people are classified as healthy and
risky. The green code is not a clinical confirmation of
an individual’s healthiness. Nevertheless, it relieves a
cognitive burden and gives the appearance of safety
when the majority of a society is also green and safe.
As Caixia put it “no matter if it is scientific or not,
psychologically I feel safer when I see my code green
and knowing people around me on the street is green.”
This type of mindset explains the refrain of “better safe
than sorry” paraphrased by many of our interviewees,
indicating a strong preference for false positives over
false negatives in risk assessment from Health Code.
For example, Yuejin argued:
Yes, staying at home for two weeks might be an inconvenience for those people who were misjudged. Yet we
should all have a scale in our heart: is this personal
inconvenience worse than letting those positive and
risky people go? My city was closed because of a new
outbreak. And now everyone stays at home.

8

Big Data & Society

This precautionary approach further contributes to
people’s trust in technology and facilitates positive attitudes about the use of Health Code in the pandemic.

Doubting the algorithm
Although often promoted as a reliable technology
based on Big Data and artificial intelligence, cloud
computing, and blockchain, Health Code never fully
discloses its mechanism. Ordinary people have only a
vague idea about what kind of information Health
Code collects, and an even vaguer idea about how it
handles data and assesses risk. While many people trust
the generic technologies that support Health Code,
some also challenge it due to their experiences using
it. The most common reason for doubt and mistrust
is experiencing or hearing anecdotes of noteworthy
algorithmic misjudgments (n ¼ 6), which were
common especially during the first month of release.
Qiangzi was misjudged in March; it happened unexpectedly when trying to enter his residential community. Having been turned away by security, he jumped the
wall to re-enter his home. When he called a government
representative to appeal, they did not offer a clear
reason why it happened:
I was pretty sure I didn’t go anywhere risky [. . .] The
representative said she can only help me file the appeal
and wait for other staff and algorithms to check and
did not tell me what was wrong or why this happened.
I know I cannot go out, if I am not green then I cannot
go anywhere anyway. [. . .] I waited and waited, until
2 am, it suddenly became green again.

The whole process was terrifying and confusing. Even
after the problem was resolved, no one told him why
Health Code faltered. Qiangzi is a programmer, so he
was sympathetic to the existence of bugs. Yet nevertheless he admitted that he perceived Health Code as less
magical and functional as a result of his experience.
For people who did not experience or hear about
misjudgment incidents, they analogized from their
experiences with other “big data and artificial
intelligence” algorithms to challenge Health Code.
When being asked about her trust in artificial intelligence used in Health Code, Lingling said that:
Big data and artificial intelligence are just bragging.
After I bought something from Taobao (an online
shopping website owned by Alibaba), it immediately
recommended me to other similar stuff. But I just
bought that thing. Why would I need the same thing
right now again? [. . .] Artificial intelligence . . . I say it is
artificial intelligently challenged.

Furthermore, people doubt the accuracy of Health
Code based on the kind of data it collects. For example, Longzi traveled for business a lot, worrying that:
You cannot judge someone’s risk status simply based
on the place he or she went. What if he or she just
passed by that place? With a mask on all the time?
With a normal temperature? The Health Code is too
crude, too simple, and too naı̈ve. Quarantine based on
it is a waste of time.

Like many others, she is challenging the accuracy of
Health Code’s algorithm based on the limited variety,
and decontextualized nature, of the data it collects. As
scientists found the majority of people infected with
COVID-19 to be asymptomatic, more concerns were
raised about how Health Code can collect data relevant
for asymptomatic individuals. Runze worried that
“Asymptomatic patients might not know their own
status, and they have no symptoms to report on the
Health Code. Health Code has no way to identify
these people.”

Doubting the data
Even when people believe Health Code can collect all
essential data to make accurate judgments, they might
still doubt and challenge Health Code based on their
experiences and understanding of how the data is collected. People problematize data in two ways: false
data and missing data. The problem of false data considers Health Code’s heavy reliance on self-reporting.
When initiating Health Code, individuals fill out an
epidemiological survey that covers his/her travel history, contact history, and symptoms. Many interviewees
admitted that they have input false data. Sometimes
this was due to inconvenience. When traveling to
another province in April, Dawei entered false information when initiating the destination’s Health Code,
explaining:
When I arrived, the staff started to ask you to initiate
two Health Codes, one is the State Council’s, and
another is the Chongqing one. It was so annoying.
My phone was slow, and people are lining up, so
I just filled something right and made some random
others up to save time.

Other times it was due to the concern of the Health
Code’s potential to misjudge and quarantine individuals. Every time Lisha fills in Health Code information,
he hides his symptoms: “I have chronic sore throat and
cough for months. Should I fill in that I have the symptoms? Of course not! If I said that who knows what
will happen and how much trouble it might cause?”

Liu and Graham
These personal experiences of worrying about others’
false information reporting practices create doubt in
Health Code’s accuracy and general reliability. Nana
expressed: “The more I use Health Code the more
doubts I have. All the information was self-reported,
there is no verification. People can easily choose not to
upload their symptoms to it to get a green code.”
Another concern is missing data. As many interviewees understood, once initiated, the main information Health Code collects are location via scanning QR
codes at the entrance of various spaces. People
quickly realized that inspectors at many checkpoints
do not scrutinize people entering sufficiently. Feng
recalled that:
The security of my office building stands at the revolving door checking people’s Health Code. In the morning, most people must go through the same door, so it
was always overcrowded. The security cannot handle
so many people at the same time so he will just let
people go.

Sometimes the checking is just relational. Lulu, for
example, found out that “it depends on how familiar
you are with the security [. . .] if you speak Shanghai
dialect to the security (of her resident community), you
can just go without having your Health Code checked.”
Alternatively, people attempt to game Health
Code’s surveillance, such as using green screenshots
(of either themselves or another person) to pass checkpoints without launching Health Code itself. These
easy bypasses further undermine people’s faith in
Health Code’s accuracy. Ningning said “it cannot
trace everyone all the time and thus will not work as
it claimed. It just offers some false confidence.” These
two kinds of doubt about Health Code data are interconnected. Aimi took a bus in April and found the bus
driver did not really check her Health Code. She asked
why, to which the driver responded, “What’s the point
for doing this? People made the information up.” The
perception of the false data is interconnected with the
practice that results in missing data, creating a vicious
loop of mistrust in Health Code data.

Making sense of implementation
Paternalistic intervention
People’s trust and mistrust in the Health Code is not
only about how it detects risk but also how it is implemented. The first part of the implementation considers
how surveillance measures ensure data is collected and
analyzed. The second part is about what comes next
after a risk is detected: contact people who are classified as risky, enforce strict quarantine for risky

9
individuals, and punish those who break the quarantine. Implementing Health Code meant a broader reorganization of Chinese society (Liu, 2020). For example,
entry points to different spaces are either blocked or
equipped with checkpoints, where inspectors stop
people to check their Health Code status. People
deemed risky were asked to quarantine and put under
closer surveillance. Some might argue this to be an
invasion of privacy and a constraint of civil liberty.
But for many people, this amounted to a kind of
caring paternalism. Yuejin, a retired high school teacher who kindly urged the interviewer to be careful in the
US, said:
These Western countries give people too much freedom, but they also don’t care about their people, you
see what people got there? [. . .] The implementation of
the Health Code also has a ‘side effect’ – so many outlaws are arrested because they cannot go anywhere
without a Health Code!

People often used foreign polities routinely exhibiting
what they considered irresponsible civic behaviors as
counterexamples to China. For Yuejin and many
others, use of Health Code and its corresponding
social reorganization functioned to protect people, in
contrast with the ineffective decentralized responses of
“Western countries” – the assumed competitors for
Chinese growing nationalist’s “civilizational competition” imaginaries (Wu, 2020). Therefore, increasing
surveillance so the state can penetrate deeper into the
corners of society is seen as a positive change that
makes society safer, and unexpectedly strengthen
Health Code as well as the regime’s legitimacy.

Doubting the implementation
For many people, even if the Health Code could accurately surveil everyone and correctly assess risk status,
the technology is still unreliably enforced. On the one
hand, lax execution of disease control and people’s
gaming of the system could mean a high-risk individual
freely moving around a city undetected. Linda’s father
mistakenly received a red code once. Yet, he was still
able to enter a supermarket as “business wants everyone to get in and buy things.” This incident made
Linda lose confidence in Health Code, “If people
didn’t use this seriously enough, it is totally unreliable.”
What further triggered people’s dissatisfaction is
when implementation of Health Code was replaced
by other disease control approaches that people locally
considered more effective. Ningning observed that:
“Before Health Code came out, staff in the public
transportation will take the real-time temperature
of passengers, which is more accurate and useful.

10
But now they only use this Health Code that relied on
self-reported data.” For Ningning and many others,
Health Code seems to be gradually diverging from its
stated ends of disease control. After months of zero
new cases in her city, Ningning observed that “no
one asked people to wear a mask, but we are still
required to scan the Health Code every time and everywhere, which really makes you wonder: what is this
really for?” Lisha found similar practices in her city,
and worried Health Code excused a reduced level of
accountability for the government. “they just set up
the Health Code and only focus on its implementation,
claiming that they have fulfilled all the responsibility.
Whatever happened next is not their problem.”
On the other hand, the extreme enforcement of disease control may also generate mistrust and doubt in
Health Code. During March and April, controlling the
transmission of the virus became extremely politicized.
Government officials were punished if a new outbreak
happened in their district. Accordingly, many local
authorities used additional criteria to identify people’s
risk status besides Health Code. Runze’s Health Code
turned green while staying at home in a small town in
Hubei province for the month of March. However, the
local authority still demanded he stay at home as “the
Health Code is green, but that was produced by ‘big
data.’ It will still depend on local policy and interpretation to determine if one can go out.” After he went to
another city where he worked, people still required him
to fill in extra documents and certifications to prove his
“low-riskiness” beyond his green Health Code, often
simply rejecting his interview requests because he was
from Hubei province. These experiences significantly
compromised his trust in Health Code, as the legitimacy of its risk status was denied by the same authorities
who enforce it.
In addition, ethical concerns were raised about
demanding a reliance on digital technology in order
to move freely. Health Code excludes or adds a significant amount of inconvenience to those who do not
have a smartphone, particularly poor and elderly
people. Lisha’s grandma cannot take the subway
because she doesn’t have a mobile phone to show her
Health Code. Nana’s grandpa stayed at home for
months because she was confused about how to use
Health Code, despite having a phone. She was concerned that “Some people who have to get out and
now they are excluded. [. . .] Eventually, the Health
Code only applies for people with good economic
and educational status. It is discrimination.”

Worries about normalization
While the state of emergency has faded as the pandemic
comes under control in China, many places still enforce

Big Data & Society
strict Health Code inspections, which concerns people
regarding the normalization of Health Code surveillance in the post-pandemic society, alongside a routinization of expanded government power. Although
claims that no freedom is absolute, people like
Cuiping firmly disagree with the continued use of
Health Code after the COVID-19: “If my life is not
threatened by the disease, why would it be necessary
to leave my trace all the time?” For those living in cities
that were never impacted heavily by the pandemic, this
worry is accentuated. Longzi complained when being
asked about current Health Code use:
I understand when the outbreak was a problem in my
city, we need to use the Health Code. But now everyone
is back to work, all schools are reopened, and we
haven’t had any cases for months, why are we still
using it? What is the data it collected really for?

Many people like Huadong worried that only the government has the power to determine when to stop using
Health Code. “The government said at the beginning
that this will be stopped once the pandemic is over. But
it is them to define when the pandemic is over – when
there are no domestic cases? When there is a vaccine?”
These worries were accelerated by Hangzhou municipal government in May, who proposed to reform
Health Code for post-pandemic use. This new Health
Code system sought to collect data beyond location
information, such as medical records, physical examination reports, and lifestyle data such as food, drink,
exercise, and sleep cycles. The proposal expanded risk
classification from the crude tripartite healthy/risky
colors into a more granular score-based system that
quantifies people’s health and subsequently ranks
them. Hangzhou government’s move caused huge controversies. Although it later clarified that it was just a
“thought” instead of an actual plan, many suspected
that this augured the system the government wants to
build eventually. When being asked about the opinions
on the Hangzhou Health Code, Xiangzi commented
that:
The Hangzhou government was just too impatient.
Look at all the government platforms in recent years,
what that plan was proposing has been on the same
road for a while [. . .] But unlike what they usually do
like ‘boiling frog in warm water,’ they are now trying to
boil the frog with boiling water directly!

These concerns about Health Code are not only about
the imagined future. They interact with people’s perceptions of their past experiences and their present use
of Health Code. At the end of an interview, Lulu said:
“now think about our life before the Health Code, you

Liu and Graham
can only then realize how many things were already
there.” Feifei, a law student worried that “power is
addictive to knowing and controlling everything.
Now think about it, the Health Code shouldn’t be
used like this at the beginning.” Many of them started
to actively game or bypass the system to avoid being
captured by it.

Discussion
In this paper, we examine how Chinese people make
sense of Health Code, the algorithmic contract tracing
and risk assessment sociotechnical assemblage.
Echoing a growing scholarly focus on people’s perception of algorithms in practice (Amelang and Bauer,
2019; Brayne and Christin, 2020; Bucher, 2017;
Christin, 2017), we demonstrate how people perceive
Health Code along three axes: privacy issues, technology use, and implementation. We show that people’s
concerns over COVID-19 contact tracing and risk
assessment algorithms are multifaceted, intertwined,
and dynamic. It is thus inappropriate to divide people
into a simple support/reject binary for contact tracing
and risk assessment, as individuals commonly have differing and contradictory stances on different aspects of
it. For example, Linda trusted in the technical configuration of Health Code and was willing to trade-off her
privacy for safety. Yet, this trust was lost when she
found out that people do not check Health Code as
diligently as they should, despite little change on the
“technology-side” of Health Code.
These shifting, self-contradictory sense-making processes illustrate the multiplex, dynamic imaginaries of
the algorithm as it is perceived by individuals over time.
When people talk about an “algorithm,” they are often
talking about an algorithmic sociotechnical assemblage, engaged and entangled with diverse sociomaterial actors that contribute to its ontological status
through their performances, beliefs, and interpretations
(Introna, 2016; Kitchin, 2017; Seaver, 2017). The technical part of an algorithm, i.e. pre-defined codes that
articulate and specify steps and procedures to assess
inputs, comprise only part of what an algorithm does,
how it is perceived, and what it ultimately is. None of
our interviewees know how Health Code really works
in this technical sense. Their perceptions of Health
Code are instead a form of “algorithmic imaginary”
(Bucher, 2017; Lupton and Michael, 2017), informed
by a diffuse socio-cognitive process, shaped and
reshaped through past experiences, daily interactions,
and future anticipations, containing mystery and educated guesswork.
Building on relational sociology’s emphasis on interdependent, processual analysis and its rejection of voluntarism and determinism (Bourdieu, 1998; Depelteau,

11
2008; Emirbayer, 1997), we argue that people’s sensemaking of algorithms is a form of relational perception. Depending on who designs or implements the
algorithm, people’s imagination and sense-making of
the same technical solution could be dramatically different. These differences are related, but the relationship is contextual not deterministic. None of the
Chinese public, Health Code, or people’s perceptions
and attitudes toward Health Code are predetermined,
fixed, or categorically consistent. Our study shows people’s sense-making of algorithms connects to their biographies and prior experiences with other algorithmic
systems. It is informed by the future applications they
foresee – such as how Health Code uses their data, and
whether Health Code will be normalized. More importantly, making sense of algorithms is a dynamic,
co-constitutive process as a diverse set of human and
non-human actors form part of the algorithmic sociotechnical assemblage through interaction. We show
how encounters with Health Code inspectors, other
users, or smartphones effect this sense-making process.
This local variability is nested in social surroundings,
and broader perceptions of disease control strategies at
the national and global level. For example, many
people attribute a necessity to Health Code because
other societies’ lack an equivalent, or have controlled
COVID-19 incompetently, fitting the accelerating the
Chinese nationalist “civilizational competition” discourse (Wu, 2020), thereby unexpectedly legitimizing
the scope of expanded surveillance and Chinese
authority.
This relational and co-constitutive understanding of
algorithmic sociotechnical assemblages sheds light on
why many Chinese people favor state surveillance over
private companies during the pandemic. The Chinese
state’s power matters. While the state relies on private
companies’ technical capacity to enforce algorithmic
governance, they nevertheless exert strong control
over key decision-making and agenda-setting
(Fourcade and Gordon, 2020; Hou, 2017). Similar to
Chinese people’s high support and low concern for the
new social credit systems’ privacy invasions (Kostka,
2019), people’s overall sense of fatalism and acceptance
of Health Code’s surveillance acknowledges a dearth of
options of choice under the state-run compulsory
system. However, this question cannot only be
explained by demonstrated Chinese state coercion,
overreaching cultural preference or a generalized
sense of security in China as previous studies indicate
(Liang and Chang, 2006; Lü, 2005; Zhang et al., 2019).
We show a key factor that impacts people’s perception
of Health Code is not that they are being surveilled or
that their privacy is being invaded, but their perception
of how they are being surveilled, how the data will be
used and for what purposes by which party, echoing

12
scholarship outside the Chinese context (Bucher, 2017;
Lupton and Michael, 2017; Zimmer et al., 2020). The
surveillance and data collection of the state is considered passive, routine, bureaucratic, and thus acceptable. In contrast, private tech companies’ surveillance
and data collection is broadly considered intrusive,
promising targeted advertising or spam, and thus
more visible and liable to create antipathy among the
public. These two different uses of surveillance data,
derived of differing institutional logics from the state
and the market, provide the substrate on which individual algorithmic imaginaries grow.
We conclude with two methodological reflections.
The first is regarding conducting qualitative research
on algorithms during the COVID-19. The ongoing
pandemic brings many challenges, such as social distancing, forcing reliance upon digital platforms for
conducting research. These platforms allow researchers
to reach more people but also generate systematic limitations. We show how people’s knowledge of those
who are excluded by Health Code will change their
perception of the system, yet our sample did not capture people who were themselves systematically marginalized, such as the elderly and technophobic.
These exclusions, ironically, mirror a common criticism
of Health Code. During the pandemic, digital inclusion
has become more essential across a wider array of contexts (Fourcade and Gordon, 2020), requiring scholars’
special attention.
The second reflection is regarding the extent of generalizability in the Chinese case. China is special in terms
of its authoritarian governance regime, one-party system,
prevalent censorship, etc. These “Chinese characteristics
(zhongguo tese)” remain critical scholastic avenues, as we
consider the sociopolitical system in China a direct facilitator for Health Code in multiple ways. But these
Chinese characteristics are often caricatured in scholarship. We endorse careful empirical examination on a
case-by-case basis when undergoing research on
Chinese society. By doing so, we avoid Chinese exceptionalism discourse whereby dynamic reality is unfairly
reduced to authoritarian determinism (Guan, 2019; Hou,
2017) – a uniformly oppressive Chinese state herding a
brainwashed and lemming-like citizenry. This lens conceals more than it reveals, and also hampers the capacity
to learn about other societies from experiences in China.
The relational perception of privacy, technology, and
implementation of Health Code we illustrate could
offer critical insights into similar algorithmic regimes
elsewhere. Our research presented above is in this spirit.
Acknowledgements
We are grateful to all our study participants that accepted our
interviews during this difficult time. John Evans, Lilly Irani,
and reviewers from Big Data & Society offered constructive

Big Data & Society
feedback, for which we are thankful. We would like to
express special thanks to Marie Xu and Zhengyun Zhou,
who provided great assistance in data collection.

Declaration of conflicting interests
The author(s) declared no potential conflicts of interest with
respect to the research, authorship, and/or publication of this
article.

Funding
The author(s) received no financial support for the research,
authorship, and/or publication of this article.

ORCID iDs
Chuncheng Liu
Ross Graham

https://orcid.org/0000-0001-5842-7988
https://orcid.org/0000-0003-4578-7022

References
Alsan M, Braghieri L, Eichmeyer S, et al. (2020) Civil
Liberties in Times of Crisis. Cambridge, MA: National
Bureau of Economic Research.
Amelang K and Bauer S (2019) Following the algorithm:
How epidemiological risk-scores do accountability.
Social Studies of Science 49(4): 476–502.
Armstrong D (1995) The rise of surveillance medicine.
Sociology of Health & Illness 17(3): 393–404.
Au L, Fu Z and Liu C (2020) It is (not) like the flu: Expertise
and temporality in United States, China, and Hong Kong.
In: Covid-19 conference, 10-11 July, 2020.
Bengio Y, Janda R, Yu YW, et al. (2020) The need for privacy with public digital contact tracing during the
COVID-19 pandemic. The Lancet Digital Health 2(7):
e342–e344.
Bourdieu P (1998) Practical Reason: On the Theory of Action
(Trans. R Johnson). 1st ed. Stanford: Stanford University
Press.
Brayne S and Christin A (2020) Technologies of crime prediction: The reception of algorithms in policing and criminal courts. Social Problems. Epub ahead of print 5 March
2020. doi: 10.1093/socpro/spaa004
Bucher T (2017) The algorithmic imaginary: Exploring the
ordinary affects of Facebook algorithms. Information,
Communication & Society 20(1): 30–44.
Center for Disease Control (2020) Contact tracing. Available
at: www.cdc.gov/coronavirus/2019-ncov/daily-life-coping/
contact-tracing.html (accessed 8 February 2021).
Cha V (2020) Asia’s COVID-19 lessons for the west: Public
goods, privacy, and social tagging. The Washington
Quarterly 43(2): 1–18.
Chen Y, Mao Z and Qiu JL (2018) Super-Sticky Wechat and
Chinese Society. Bingley: Emerald Publishing.
Christin A (2017) Algorithms in practice: Comparing web
journalism and criminal justice. Big Data & Society 4(2):
1–17.
Collado-Borrell R, Escudero-Vilaplana V, Villanueva-Bueno
C, et al. (2020) Features and functionalities of smartphone
apps related to COVID-19. Journal of Medical Internet
Research 22(8): e20334.

Liu and Graham
Depelteau F (2008) Relational thinking: A critique of codeterministic theories of structure and agency.
Sociological Theory 26(1): 51–73.
Edwards L, Veale M, Lynskey O, et al. (2020) The coronavirus (safeguards) bill 2020: Proposed protections for digital
interventions and in relation to immunity certificates.
Preprint, 13 April. LawArXiv. DOI: 10.31228/osf.io/yc6xu
Emirbayer M (1997) Manifesto for a relational sociology.
American Journal of Sociology 103(2): 281–317.
Eubanks V (2018) Automating Inequality: How High-Tech
Tools Profile, Police, and Punish the Poor. New York:
St. Martin’s Press.
Fang TM (2020) Sending a red signal: When a contact tracing
app went wrong a journalist was forced to stay in their
home in China. Index on Censorship 49(2): 34–36.
Foucault M (1990) The History of Sexuality, Vol. 1: An
Introduction. Reissue ed. New York: Vintage.
Fourcade M and Gordon J (2020) Learning like a state:
Statecraft in the digital age. Journal of Law and Political
Economy 1(1): 78–108.
Fourcade M and Healy K (2017) Seeing like a market. SocioEconomic Review 15(1): 9–29.
French M and Monahan T (2020) Dis-ease surveillance: How
might
surveillance
studies
address
COVID-19?
Surveillance & Society 18(1): 1–11.
Guan T (2019) The ‘authoritarian determinism’ and reductionisms in China-focused political communication studies. Media, Culture & Society 41(5): 738–750.
Hou R (2017) Neoliberal governance or digitalized autocracy? The rising market for online opinion surveillance in
China. Surveillance & Society 15(3/4): 418–424.
Ilyushina M (2020) Moscow rolls out digital tracking to
enforce lockdown. Critics dub it a ‘cyber Gulag’.
Available at: www.cnn.com/2020/04/14/world/moscowcyber-tracking-qr-code-intl/index.html
(accessed
24
August 2020).
Introna LD (2016) Algorithms, governance, and governmentality: On governing academic writing. Science,
Technology, & Human Values 41(1): 17–49.
Kitchin R (2017) Thinking critically about and researching
algorithms. Information, Communication & Society 20(1):
14–29.
Kitchin R (2020) Civil liberties or public health, or civil liberties and public health? Using surveillance technologies
to tackle the spread of COVID-19. Space and Polity 24(3):
362–381
Knorr Cetina K (1999) Epistemic Cultures: How the Sciences
Make Knowledge. Cambridge: Harvard University Press.
Kostka G (2019) China’s social credit systems and public
opinion: Explaining high levels of approval. New Media
& Society 21(7): 1565–1593.
Liang G and Chang H (2006) Surveillance and privacy in
urban China. Available at: www.sscqueens.org/sites/
sscqueens.org/files/China_Report_March_07.pdf
(accessed 8 February 2021).

13
Liu C (2019) Multiple social credit systems in China.
Economic Sociology: The European Electronic Newsletter
21(1): 22–32.
Liu C (2020) Algorithms in action: Reassembling contact
tracing and risk assessment during the covid-19 pandemic
in China. SocArXiv. DOI: 10/ghcwnx.
Loder E (2020) Getting it right in the pandemic. BMJ 370:
m2637.
Lü Y-H (2005) Privacy and data privacy issues in contemporary China. Ethics and Information Technology 7(1): 7–15.
Lupton D and Michael M (2017) ‘Depends on who’s got the
data’: Public understandings of personal digital dataveillance. Surveillance & Society 15(2): 254–268.
Lyon D (2018) The Culture of Surveillance: Watching as a
Way of Life. 1st ed. Cambridge, UK: Polity.
Mittelstadt BD, Allo P, Taddeo M, et al. (2016) The ethics of
algorithms: Mapping the debate. Big Data & Society 3(2):
1–21.
Nielsen N (2020) Privacy issues arise as governments track
virus. Available at: https://euobserver.com/coronavirus/
147828 (accessed 24 August 2020).
Pasquale F (2015) The Black Box Society: The Secret
Algorithms That Control Money and Information.
Cambridge: Harvard University Press.
Rona-Tas A (2020) Predicting the future: Art and algorithms.
Socio-Economic Review 18(3): 893–911.
Seaver N (2017) Algorithms as culture: Some tactics for the
ethnography of algorithmic systems. Big Data & Society
4(2): 1–12.
Shao C (2020) The Surveillance Experience of Chinese
University Students and the Value of Privacy in the
Surveillance Society. Chapel Hill: University of North
Carolina.
Tang Q (2020) Privacy-preserving contact tracing: Current
solutions and open questions. arXiv:2004.06818 [cs].
Available at: http://arxiv.org/abs/2004.06818 (accessed
10 August 2020).
Timmermans S and Tavory I (2012) Theory construction in
qualitative research: From grounded theory to abductive
analysis. Sociological Theory 30(3): 167–186.
Wu AX (2020) The evolution of regime imaginaries on the
Chinese Internet. Journal of Political Ideologies 25(2):
139–161.
Zhang H, Guo J, Deng C, et al. (2019) Can video surveillance
systems promote the perception of safety? Evidence from
surveys on residents in Beijing, China. Sustainability 11(6):
1595.
Zimmer M, Kumar P, Vitak J, et al. (2020) ‘There’s nothing
really they can do with this information’: Unpacking how
users manage privacy boundaries for personal fitness
information. Information, Communication & Society
23(7): 1020–1037.
Zuboff S (2019) The Age of Surveillance Capitalism: The Fight
for a Human Future at the New Frontier of Power. 1st ed.
New York: Public Affairs.

