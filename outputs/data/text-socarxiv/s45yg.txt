The Generalizability of Online Experiments Conducted
During the COVID-19 Pandemic
Kyle Peyton, Gregory A. Huber, and Alexander Coppock

∗

Forthcoming, Journal of Experimental Political Science
May 11, 2021

Abstract
The COVID-19 pandemic imposed new constraints on empirical research, and online data collection by social scientists increased. Generalizing from experiments conducted during this period of persistent crisis may be challenging due to changes in how
participants respond to treatments or the composition of online samples. We investigate the generalizability of COVID-era survey experiments with 33 replications of 12
pre-pandemic designs, fielded across 13 quota samples of Americans between March
and July of 2020. We find strong evidence that pre-pandemic experiments replicate in
terms of sign and significance, but at somewhat reduced magnitudes. Indirect evidence
suggests an increased share of inattentive subjects on online platforms during this period, which may have contributed to smaller estimated treatment e↵ects. Overall, we
conclude that the pandemic does not pose a fundamental threat to the generalizability
of online experiments to other time periods.
∗

Kyle Peyton is Postdoctoral Fellow in Law and Social Science, Yale Law School
(kyle.peyton@yale.edu); Gregory A. Huber is Forst Family Professor of Political Science, Yale University
(gregory.huber@yale.edu); Alexander Coppock is Assistant Professor of Political Science, Yale University
(alex.coppock@yale.edu). Thanks to the Center for the Study of American Politics and the Institution
for Social and Policy Studies for research support, to Peter Aronow, Josh Kalla, Lilla Orr, John Ternovski,
and Baobao Zhang for helpful comments and feedback, to Ethan Porter for sharing replication materials,
to Antonio Arechar, Matt Graham, David Rand, Patrick Tucker, Chloe Wittenberg, and Baobao Zhang for
sharing pre-COVID survey data. We thank Allison St. Martin and Patrick Comer of Lucid for enlightening
conversations and providing aggregate data on purchases made by academic clients. Previous versions of
the manuscript were presented and benefited from feedback at Université de Montréal, Australian National
University, UMass Amherst, and York University. This research was approved by the Yale University
Institutional Review Board (Protocol number 1312013102).

During the COVID-19 pandemic, social scientists across the globe have been forced to
abandon or postpone research projects that require face-to-face interaction, travel, or even
simply leaving the house. Not surprisingly, the use of online surveys – a relatively low
cost form of empirical research that does not require in-person data collection – expanded
dramatically during the COVID-19 pandemic. For example, purchases by academic clients
on Lucid – a widely used marketplace for survey respondents – nearly tripled between 2019
and 2020. Amid this boom in online research activity, concerns have been raised about the
external validity of experiments conducted during this period (e.g., IJzerman et al., 2020;
Rosenfeld et al., forthcoming). Here we investigate whether extrapolations from studies
conducted during the pandemic to other time periods will be misleading.
Empirically demonstrating that experiments conducted during the COVID era do or do
not generalize to other times is straightforward in principle. Once the pandemic has passed
and the social, economic, and political aftershocks have dissipated, replications of COVIDera studies can settle the question of whether those results generalize to the post-crisis period.
Unfortunately, it may be a while before normal times fully resume. In this paper, we take
up a closely-related question that we can answer much sooner: do experiments conducted
prior to the pandemic generalize to COVID times?
We provide an answer using 33 replications fielded during the onset of the COVID-19
pandemic of 12 previously-published survey experiments. Consistent with the findings from
recent replication attempts on di↵erent samples, we find strong evidence of correspondence.
Our COVID-era replication estimates nearly always agree with pre-COVID estimates in
terms of sign and significance, but are somewhat smaller in magnitude at an average of 73%
of the pre-COVID e↵ect size. We argue that this pattern may be explained by lower levels
of attentiveness among online survey respondents during the initial onset of the pandemic.
Overall, the replication estimates presented here should mitigate concerns that results from
survey experiments conducted during the pandemic will not generalize to other times and
contexts.

1

Background

A common framework for understanding how results generalize from one sample to a target
population is the “UTOS” framework, which stands for Units, Treatments, Outcomes, and
Settings (Cronbach and Shapiro, 1982). The framework predicts that results will exhibit
greater correspondence (or generalizability or transportability) between a sample and the
1

target population with increasing similarity of their UTOS components. A decade’s worth
of studies in political science have invoked the UTOS framework to argue that survey experiments conducted with online samples generalize to the U.S. national population (Berinsky,
Huber and Lenz, 2012; Mullinix et al., 2015; Coppock, 2019; Coppock, Leeper and Mullinix,
2018; Coppock and McClellan, 2019). Recent work by Findley, Kikuta and Denly (2020)
elaborates the UTOS framework to di↵erentiate the time dimension from other features of
settings, pointing out that the mechanisms that link treatments to outcomes may change
over time. Munger (2020) describes growing concerns about generalizability across time as
a question of “temporal validity.”
The COVID-19 pandemic led to changes in at least two important factors relevant to
online survey experiments. First, nearly everyone experienced massive disruptions to their
daily lives. These changes may have a↵ected how online survey participants respond to
treatment. For example, Americans may have been in a heightened state of anxiety due to a
variety of factors, including an unmitigated public health crisis, rising economic insecurity,
and chronic political instability. Anxiety and other emotional state variables have been
shown to a↵ect information processing (e.g., Gadarian and Albertson, 2014), willingness to
dissent (e.g., Young, 2019), and to condition e↵ects of other treatments (e.g., Valentino et al.,
2009).
Another important factor is that the pandemic altered, or was coincident with, changes
in the composition of markets for online survey respondents. Arechar and Rand (2020),
for example, find that an influx of new workers to the Amazon Mechanical Turk (MTurk)
platform in March 2020 led to samples that were less attentive, but more demographically
diverse and representative of the U.S. population. On the Lucid platform, the demand
for survey respondents in 2020 increased by roughly 40% among commercial clients, and
by 200% among academic clients (see Figure 1). In the immediate wake of the pandemic,
Lucid suppliers – who furnish survey respondents for both academic and commercial clients
– struggled to recruit enough new participants to meet this increased demand.1 Consistent
with the possibility that suppliers attempted to meet this increased demand with relatively
lower-quality respondents, Aronow et al. (2020) find that respondent attentiveness on Lucid
was declining over the period March-July 2020. This decline in attentiveness may also have
reflected changes in the types of people who were previously attentive becoming less attentive
because of COVID-19 related disruptions.
1

Conversation with Lucid representatives on 18 March 2021. According to Lucid, supply gradually
increased over the summer of 2020, just as demand for survey responses intensified in the lead up to the
2020 election.

2

Figure 1: Total weekly survey responses completed and sold to academic buyers by Lucid
between January 2019 and March 2021
Week before
2020 election
90K

60K

30K

0
Mar '19

Jun '19

Sep '19

Dec '19

Mar '20

Jun '20

Sep '20

Dec '20

Mar '21

Notes: Points denote weekly survey responses completed and sold to academic customers by Lucid (in
thousands). Dark lines and 95% confidence bands from loess smoother. The WHO declared COVID-19 a
pandemic on 11 March 2020. Proprietary data provided by Lucid cover the period 1 January 2019 - 18
March 2021. In 2019 Lucid sold 729,284 completed survey responses to academic buyers, compared with
2,185,387 in 2020.

1.1

Design

Given the proliferation of online research conducted during the COVID-19 pandemic, we can
articulate concerns about external validity as an empirical question: would the results from
an experiment conducted during the pandemic have been di↵erent if the experiment were
instead conducted in a di↵erent period?
Between March and July 2020, we recruited weekly samples of approximately 1,000 U.S.
based participants via Lucid. Each survey was between 10 and 15 minutes long (median
duration: 12.8 minutes) and was structured in discrete 3-5 minute modules. Lucid collects
demographic information from all respondents before they enter any particular survey, enabling the construction of quota samples that approximate U.S. census margins (Coppock
3

and McClellan, 2019). Like previous investigations on the suitability of convenience samples
for academic research, we focus on survey experiments in particular. Using online convenience samples for descriptive work is generally inadvisable because the samples may di↵er
from target populations in both observable and unobservable ways.2 However, for theoretical
settings in which an average treatment e↵ect among a diverse (but nevertheless unrepresentative) sample would be informative, survey experiments with online convenience samples
are an e↵ective and widely-used tool.
1.1.1

Selection criteria

We conducted 33 replications across 12 unique studies, chosen based on the following criteria:
1. Suitable for online survey environment. All replications were administered through a
web browser using Qualtrics survey software, and the original studies must have been
fielded in a similar format.
2. Length of study. Time constraints ruled out studies that could not be administered in
a survey module of 3-5 minutes.
3. Design transparency. The original study design and outcome measures were clearly
described in the published manuscript or supporting appendix.
4. Design complexity and e↵ect size. Sample size constraints ruled out some two-arm
studies with small e↵ect sizes, as well as more complex studies with elaborate factorial
designs.
5. Theoretical and political importance. The studies all considered theoretically important
questions, with many being published in top journals.
These criteria are similar to those used in other meta-scientific replication projects (e.g.,
Klein et al., 2014, 2018). We also aimed for a mix of classic and contemporary studies, and
for coverage across political science sub-fields. Our set of studies is not a random sample
of all survey experiments conducted by social and political scientists, but they do cover a
wide range of designs. The full set of studies is listed in Table 1. We conducted at least
one direct replication of each study. Modified versions of studies 2, 3, 5, and 7 were also
replicated using COVID-specific content. A description of each modified version and their
corresponding replication estimates are provided in Online Appendix Section A.
2

Balance on the demographic marginal distributions does not imply balance on the joint distributions.
Moreover, because these are not probability samples, the degree of imbalance on the distribution of unobserved attributes is unknown.

4

We categorize each replication as a “success” if the COVID-era estimate(s) are correctly
signed and statistically distinguishable from zero. For studies with “null” results (studies
11-12), replication was declared successful if estimate(s) were indistinguishable from zero and
their pre-COVID benchmarks. A replication “failure” occurs when estimate(s) are incorrectly
signed, regardless of whether they are distinguishable from zero. For studies with multiple
treatment arms/outcomes, we concluded replication was successful if the preponderance of
evidence supports success. In one case, the preponderance of evidence was ambiguous and
we concluded the replication attempt was a partial success. Additional details are available
in Online Appendix Section A.
Table 1: Summary of thirty-three replications conducted across twelve original studies
Original study
1. Russian reporters and American news
(Hyman & Sheatsley, 1950)
2. E↵ect of framing on decision making
(Tversky & Kaheneman, 1981)
3. Gain versus loss framing
(Tversky & Kaheneman, 1981)
4. Welfare versus aid to the poor
(Smith, 1987)
5. Gain vs. loss framing + party endorsements
(Druckman, 2001)
6. Foreign aid misperceptions
(Gilens, 2001)
7. Perceived intentionality for side e↵ects
(Knobe, 2003)
8. Atomic aversion
(Press, Sagan, & Valentino, 2013)
9. Attitudes towards immigrants
(Hainmueller & Hopkins, 2015)
10. Fake news corrections
(Porter, Wood, & Kirby, 2018)
11. Inequality and system justification
(Trump & White, 2018)
12. Trust in government and redistribution
(Peyton, 2020)

Design

Replications

Success

Two-arm

Week 3

Yes

Two-arm

Week 7

Yes

Two-arm

Weeks 1, 3, 7, 8, 13

Yes

Two-arm

Weeks 1-9, 11-13

Yes

Six-arm

Weeks 7, 8, 13

Yes

Two-arm

Week 3

No

Two-arm

Week 7

Yes

Five-arm

Weeks 5, 6, 13

Partial

Factorial (conjoint)

Week 8

Yes

Mixed factorial (2x6)

Week 4

Yes

Two-arm

Week 2

Yes

Three-arm

Week 9

Yes

5

2

Results

We were able to obtain the original e↵ect size(s) for each of the 12 pre-COVID studies listed
in Table 1, and at least one pre-COVID replication estimate of the original e↵ect size(s) for
7 of these studies. In total, we obtained 89 pre-COVID estimates and 101 replication estimates.3 A detailed description of each study, their pre-COVID estimates, and the individual
replication estimates that we obtained are provided in Section A of the Online Appendix.
Here we provide an overall summary for each study by comparing the pooled estimates from
our COVID-era replications with the pooled estimates from the pre-COVID benchmarks
(original estimates and any pre-COVID replications).
For each study, we calculate summary e↵ect sizes for each treatment-outcome pair using
a precision-weighted average, with weights based on the reciprocal of the variance. We can
then compare the pre-COVID and replication estimates by taking the di↵erences between
their summary e↵ect size estimates. For studies with one outcome and a simple experimental design, we compute a single di↵erence; and for studies with multiple outcomes and/or
treatments, we compute a di↵erence for each unique treatment-outcome pair. Except for the
conjoint experiment, all within-study estimates (binary and ordinal) are standardized using
Glass’s , which scales outcomes by the standard deviation in the control group (Glass,
1976).
Across the 12 studies, we obtained 138 summary e↵ect size estimates – 82 for the conjoint
studies and 56 for the remaining studies. For the non-conjoint studies, Figure 2 compares
the 28 estimated summary e↵ects from the pre-COVID studies (horizontal axis) with their
28 replications (vertical axis). All replication summary estimates were smaller in magnitude
than their pre-COVID estimates, with 24 of 28 signed in the same direction. Of the 24
correctly signed estimates, 10 were significantly smaller in replication. Of the 4 incorrectly
signed estimates, 3 were significantly di↵erent – the foreign aid misperceptions study and
2 of 6 estimates from the atomic aversion study. Figure 3 plots the analogous information
for the 41 conjoint estimates and their 41 replications, all of which are signed in the same
direction. Of these, 35 of 41 were smaller in replication (6 statistically significant di↵erences)
and 6 of 41 were larger in replication (1 of 6 significant di↵erences).
Pooling across all 65 of 69 correctly signed pairs presented in Figures 2-3, the replication
3

The General Social Survey (GSS) has administered the welfare versus aid to the poor experiment 20
times between 1986 and 2018. If all GSS estimates are included, we have 108 pre-COVID estimates in total.
Given the variation in question wordings and survey modes across time, we use the 1986 GSS experiment as
our “original” estimate, and take the first two replications conducted on online samples by Huber and Paris
(2013) as our pre-COVID replication estimates.

6

estimates were, on average, 73% as large as the pre-COVID estimates. The 41 correctly
signed estimates from the conjoint replication were, on average, 87% as large as the original.
The 24 of 28 correctly signed estimates from the other replications were, on average, 49% as
large as the pre-COVID summary e↵ect sizes.
When compared with other replication e↵orts, the correspondence for the conjoint experiment is high whereas the correspondence for the others is modest. For example, in
one of the earliest large-scale replications, Open Science Collaboration (2015) replicated 100
experiments from three top psychology journals and found that about 40% replicated and
e↵ect sizes were, on average, about half the magnitude of the original e↵ects. In economics,
Camerer et al. (2016) replicated 18 survey and lab-based experiments and found e↵ect sizes
in the 11 studies that successfully replicated were approximately 66% of the original. Coppock (2019) replicated 40 treatment e↵ect estimates from 12 survey experiments, finding
that the median MTurk estimate was 66% of the original e↵ect size.
Common explanations for somewhat diminished e↵ect sizes in replication studies include
imperfect adherence to the original experimental protocol on the part of replicators and
publication pressures that systematically select for larger e↵ect sizes in original study journal
submissions. These mechanisms may be operative here as well. At the same time, we do not
think they are the main reasons for the smaller e↵ect sizes we find because the experimental
protocols are relatively straightforward and the bulk of these studies had been successfully
replicated by others before us with larger e↵ect sizes than we find. Prior replications of
studies 7 and 8, for example, have found e↵ect sizes at least as large as the original (see
Klein et al., 2018; Aronow, Baron and Pinson, 2019).

7

Figure 2: Comparison of 28 summary e↵ect sizes across 11 studies (conjoint excluded)

COVID−era summary estimate

2.0

1.5

1.0

0.5

0.0

0.0

0.5

Difference−in−Difference:

1.0
Pre−COVID summary estimate
Not Significant

1.5

Significant

Notes: Estimated summary e↵ect sizes and 95% confidence intervals from pre-COVID experiments
(horizontal axis) and COVID-era replications (vertical axis). 13 of 28 replication estimates were
significantly di↵erent from their pre-COVID benchmark at p < 0.05.

8

2.0

Figure 3: Comparison of 41 summary e↵ect sizes in conjoint experiments

COVID−era summary estimate

0.3

0.2

0.1

0.0

0.0

Difference−in−Difference:

0.1
0.2
Pre−COVID summary estimate
Not Significant

0.3

Significant

Notes: Estimated summary e↵ect sizes and 95% confidence intervals from pre-COVID experiments
(horizontal axis) and COVID-era replications (vertical axis). 7 of 41 replication estimates were significantly
di↵erent from their pre-COVID benchmark at p < 0.05.

9

3

Can inattention explain attenuated replication estimations?

In this section, we consider declining attentiveness among survey respondents during the
COVID-era as a potential explanation for attenuated treatment e↵ect estimates. This decline
in attention – as measured by increased failure rates on Attention Check Questions (ACQs)
in Aronow et al. (2020) and Arechar and Rand (2020) – might reflect a genuine decline
in attention among online survey respondents, underlying changes in the types of people
who participate in online research, or both. Although we cannot distinguish among these
possibilities, prior work has demonstrated that inattention leads to measurement error in
self-administered surveys. Berinsky, Margolis and Sances (2014), for example, replicated
the gain versus loss framing experiment from Tversky and Kahneman (1981) and found
an estimated treatment e↵ect of approximately zero among respondents that failed a pretreatment attention screener.
Measurement error induced by inattention is nonrandom, so the correlations across covariates and survey outcomes can be overstated or understated in descriptive survey work.
In the experimental setting, however, inattention generates a form of treatment noncompliance. To fix ideas, suppose that an experimental sample can be partitioned into two types
of individuals: the “attentive” and the “inattentive”. Imagine the average e↵ect among
the attentive is positive and the average e↵ect among the inattentive is zero, because these
subjects do not engage with treatment and are therefore una↵ected by it. The Average
Treatment E↵ect (ATE) in the full sample is therefore a weighted average of two quantities:
the average e↵ect among the attentive, and the average e↵ect among the inattentive. The
ATE estimated for the full sample will therefore be closer to zero than the average e↵ect
among the attentive. Estimates for the overall ATE attenuate towards zero as the share of
inattentive subjects in the sample grows.
For ease of exposition, we have dichotomized subjects as attentive or inattentive, but
respondent attention in online surveys is both continuous and dynamic (Berinsky, Margolis
and Sances, 2016; Berinsky et al., 2019). It is continuous because subjects may be more or
less attentive and it is dynamic because attention may change during a survey. Accounting
for these subtleties is important, but the core argument is that estimated treatment e↵ects
will attenuate towards zero as inattention increases. If subjects can be classified as one type
or the other, then we expect smaller e↵ects where the share of the inattentive is higher.
If subjects pay partial attention, we expect that estimated treatment e↵ects among the
10

partially attentive will be smaller what they would have been had these subjects paid closer
attention. Given that the proportion of inattentive subjects in online samples appears to
have increased during the period in which our replications were conducted (see Aronow et al.,
2020; Arechar and Rand, 2020), this may partly explain why our replication estimates are
smaller when compared to pre-COVID estimates.
We can examine this possibility by estimating e↵ects among the attentive and inattentive separately. The most straightforward approach is to include pre-treatment ACQs (see
Oppenheimer, Meyvis and Davidenko, 2009; Paolacci et al., 2010).4 However, only two of
our surveys included ACQs, so we consider alternative approaches as well.
A second approach is to rely on individual-level meta-data. One possibility is that increased inattention during the COVID-era stems from an increasing proportion of respondents arriving to the survey environment from mobile games.5 The “User Agent” string
captured from the end user’s browser by online survey software like Qualtrics provides detailed information about how respondents arrive at the survey. We o↵er two approaches for
classifying respondents as inattentive on the basis of this information: 1) if they come from
a web-application rather than a web-browser; 2) if they come from a mobile phone rather
than a tablet or desktop.
Applying this approach across the 13 surveys used for the replication studies, we estimate that the proportion of participants coming from web-applications (rather than internet
browsers) ranged from 0.19 to 0.61, and the proportion of participants coming from mobiles
(rather than desktops or tablets) ranged from 0.31 to 0.73. Based on an additional sample of
63,245 respondents obtained from Lucid surveys fielded prior to March 2020, we observe that
the proportion of participants from both web-applications and mobiles was trending upward
prior to the COVID-19 outbreak. In the 2020 surveys, approximately 41% of respondents
came from web applications (56% from mobiles), an increase from 33% (56% from mobiles)
in 2019 and just 13% (33% from mobiles) in 2018. These results, reported in Figure 4, are
consistent with the declining data quality documented by Aronow et al. (2020).

4

Kane, Velez and Barabas (2020) suggest including “mock vignettes” which can be viewed as a taskspecific ACQ. Berinsky, Margolis and Sances (2014); Berinsky et al. (2019) urge researchers to use multiple
ACQs and classify subjects based on di↵erent levels of attentiveness.
5
In a separate Lucid survey fielded on October 29th we filtered out respondents who failed an ACQ at the
beginning of the survey (the “Easy” ACQ from Table 2). Immediately prior to this, we asked respondents to
self-report whether they were recruited to the survey from a game, and 51% reported they were. We observed
substantial di↵erences in pass rates: among those coming from an online game, only 38% passed the ACQ,
versus 82% of those not coming from a game. Interviews with Lucid representatives in 2021 suggest suppliers
that have since been removed from the platform were providing these respondents from mobile games.

11

Figure 4: Respondents from mobile devices and web applications from Jun 2018 to Jul 2020

Respondents from mobile phones

Respondents from web applications

80%

60%

40%

20%

0%
Jul '18

Jan '19

Jul '19

Jan '20

Jul '20 Jul '18

< 1,000
1,000−2,000
Sample size:
No
Yes
Pre−COVID sample:

Jan '19

2,000−3,000

Jul '19

Jan '20

Jul '20

3,000+

Notes: Points denote percentage of respondents in each Lucid survey, sized in proportion to sample size.
Dark lines and 95% confidence bands from loess smoother. Estimates are based on individual-level browser
meta-data and come from a combination of surveys conducted by the authors and UserAgent data shared
by Antonio Arechar, Matt Graham, Patrick Tucker, Chloe Wittenberg, and Baobao Zhang.

Pooling across the 13 surveys used for our replication studies, 97% of participants from
web-applications were also on mobile devices, and 72% on mobile devices arrived from webapplications. Those from web-applications spent approximately 7 minutes less time completing surveys than those from web browsers, who spent an average of 21.5 minutes. Respondents from mobile devices spent roughly 6 minutes less time completing surveys than
subjects from non-mobile devices. Additionally, in two of the 13 surveys we included ACQs
of varying difficulty and found those on web-applications (or mobiles) were significantly less
likely to pass the ACQs, compared to those coming from browsers (or non-mobiles). These
results are reported in Table 2.

12

Table 2: ACQ pass rates by attentiveness and level of difficulty
Difficulty

Browser

Web-App

Di↵erence

Non-Mobile

Mobile

Di↵erence

Easy
Medium
Hard

0.66 (0.02)
0.53 (0.02)
0.22 (0.02)

0.55 (0.02)
0.37 (0.02)
0.15 (0.01)

0.11 (0.03)*
0.16 (0.03)*
0.07 (0.02)*

0.63 (0.02)
0.50 (0.02)
0.24 (0.02)

0.58 (0.02)
0.42 (0.02)
0.16 (0.01)

0.05 (0.03)
0.08 (0.03)*
0.08 (0.02)*

Notes: The “Easy” and “Medium” questions were novel ACQs that subjects completed after reading
a short news article (see Fig. C.9-C.10). The “Hard” ACQ comes from Peyton (2020) and was
included with the direct replication of the original study in Week 9. This ACQ, is analogous to an
“Instructional Manipulation Check” (see Oppenheimer, Meyvis and Davidenko, 2009) and was passed
by 87% of respondents in the original study (see Fig. C.11). Standard errors in parentheses. p < 0.05⇤ .

While most of our surveys did not include pre-treatment ACQs, we did include one in the
direct replication of Peyton (2020), allowing for a direct comparison of estimates among the
attentive using both the ACQ and metadata approaches. Figure 5 shows that regardless of
the approach used to classify subjects as attentive, estimated treatment e↵ects among the
attentive are positive and significant. Among those who passed the ACQ, the average e↵ect
of treatment was 0.39 (SE: 0.15). Among those coming from browsers, the estimate was 0.33
(SE: 0.10), and among non-mobile users it was 0.52 (SE: 0.12). Among those who failed the
ACQ (estimate: 0.08, SE: 0.09), came from a web-application (estimate: -0.03, SE: 0.12), or
used a mobile phone (estimate: -0.04, SE: 0.10), estimated treatment e↵ects were all close
to zero and nonsignificant.

13

Figure 5: Reanalysis of estimated treatment e↵ects on trust in government for Peyton
(2020) replication
Original experiments:
MTurk sample, Jun 2014
Qualtrics sample, Sep 2014
MTurk sample, Mar 2017

Lucid replication:
Full Sample
Attentive: Passed ACQ
Inattentive: Failed ACQ
Attentive: Internet browser
Inattentive: Web−Application
Attentive: Non−mobile device
Inattentive: Mobile device
−0.3

0.0
0.3
0.6
Average causal effect estimate (standard units)

0.9

Notes: Estimates for the Lucid replication, fielded in May 2020, are presented for the full sample, and each
sub-group partition created by three di↵erent methods for classifying attentive v. inattentive respondents.
Pairwise correlations between these methods: 0.67 for Non-mobile and Browsers, 0.10 for Non-mobiles and
ACQ pass, and 0.09 for Browsers and ACQ pass. Estimates for the original experiments come from the
replication archive for Peyton (2020) at https://doi.org/10.7910/DVN/L3NT6P. The Qualtrics sample
was a quota sample that, like our Lucid replication sample, approximates U.S. census margins on
respondents demographics. In the MTurk sample from March 2017, Peyton (2020) Appendix S5.8 reports
87% of respondents passed their pre-treatment ACQ. In our direct replication, 19% of respondents passed
this same ACQ.

A final approach is to reason about the plausible scope for inattention to explain the
attenuated treatment e↵ects we find. If we assume that treatments do not a↵ect outcomes
among inattentive respondents, we can recover the e↵ect among attentive respondents by
dividing the overall estimates by the proportion of attentive respondents. If this assumption is incorrect, either because treatment e↵ects among the inattentive are non-zero or
because certain treatments a↵ect attention, then this adjustment could be biased upward or
downward.
14

Pooling across all replication studies, we find e↵ect sizes(s) were, on average, 73% their
pre-COVID magnitude. If 73% of respondents were attentive and 26% inattentive, our replication estimate would, on average, match our pre-COVID benchmarks. Empirical estimates
of inattention suggest this rate is not unreasonable. Aronow et al. (2020) report that approximately 70% of respondents passed ACQs in surveys fielded between March and July
2020. Dividing each of the COVID-era summary estimates that we obtained by 0.70 yields
an estimated replication correspondence of 104% the pre-COVID e↵ect sizes.
Of course, it is unlikely the case that pre-COVID samples were entirely attentive, in which
case we would also need to adjust those estimates upward, implying that even accounting for
growing inattention cannot fully explain smaller e↵ect sizes during COVID-19. We emphasize
that this does not constitute proof that inattention explains the attenuated e↵ect sizes across
our replications. Instead, we o↵er it as a partial explanation that is consistent with the
evidence provided in Figure 5 and the broader trend of declining attentiveness among online
survey respondents during the early onset of the pandemic.

4

Discussion

Our goal in this replication study was to understand whether online experiments conducted
during the COVID-19 pandemic would generalize to other periods. We investigated by
conducting 33 replications during the early COVID era of 12 published experimental studies.
Overall, we find strong evidence that these experiments replicated in terms of sign and
significance, but at reduced magnitudes. Because these pre-COVID experiments were, with
one exception, successfully replicated we infer that the pandemic does not pose a fundamental
threat to the generalizability of results from online experiments conducted during this period.
We considered two ways in which treatment e↵ects might have been di↵erent during this
period. First, we considered the possibility that the pandemic and the significant changes it
brought to all aspects of daily life changed individuals’ attitudes and patterns of information
processing. If so, then the same experiment conducted on otherwise similar groups of people
(online survey takers) before and during the pandemic might yield fundamentally di↵erent
results. We do not find support for this idea. Second, it’s possible that the pandemic
changed, or occurred alongside changes to, the online survey respondent pool. Decreased
attentiveness, as documented by others (Arechar and Rand, 2020; Aronow et al., 2020),
provides some indication that the composition of online samples changed, at least during the
early onset of the pandemic.
15

Using a variety of means – attention check questions, subject meta-data, and external
estimates of attentiveness – we conclude that declining attentiveness in online samples may
at least partly explain why the replication estimates presented here are, on average, smaller
than pre-COVID benchmarks. Inattention can reduce respondents’ compliance with experimental stimuli (shrinking average treatment e↵ect estimates towards zero) and it adds
nonrandom measurement error to outcome variables (decreasing the precision of treatment
e↵ect estimates). One important implication is that rising inattention in online samples
can decrease the false positive rate at the cost of increasing the false negative rate. That is,
inattention can bias estimates in the conservative direction such that small but substantively
important treatment e↵ects may be harder to detect.
Ultimately, answering the empirical question of whether treatment e↵ects are equivalent
for those who are attentive and those who are not requires a design that induces the inattentive to pay attention. Unfortunately, prior research has shown that inducing attentiveness
among inattentive subjects is difficult (Berinsky, Margolis and Sances, 2016). We experienced these same challenges in our own work. In a separate Lucid study, we attempted to
induce attentiveness by randomly assigning some respondents who failed an initial attention
check to a condition in which we told them that they failed and gave them a second chance
to pass. Only 17 of the 410 subjects (4%) in this treatment group passed when specifically
reminded to re-read the prompt carefully because they had missed something, suggesting it
is still difficult to induce attentiveness.
We therefore recommend that researchers concerned about inattention in online samples
include pre-treatment attention checks (Berinsky, Margolis and Sances, 2014; Permut, Fisher
and Oppenheimer, 2019). Post-treatment attention checks can induce bias (Montgomery,
Nyhan and Torres, 2018; Aronow, Baron and Pinson, 2019), but pre-treatment attention
checks allow researchers to either terminate the interview without paying for inattentive
respondents at the design stage, or retain them and use attention checks to compare estimates
between attentive and inattentive subgroups at the analysis stage. We caution, however,
that the former strategy necessarily restricts inferences to the sub-group of respondents that
passed an attention check task, and these respondents may di↵er from those that failed on
other dimensions as well (see Berinsky, Margolis and Sances, 2014, Online Appendix C).
In sum, we conclude that the pandemic does not pose a fundamental threat to the generalizability of online experiments, provided respondents pay attention to treatment content
and outcome questions. People still prefer riskier options for responding to unusual disease
outbreaks when in a loss frame – even when the real-world analogue could hardly be more

16

salient. People still believe misinformation, but can be corrected with fact checks. People
still have preferences over immigrants that can be measured via a conjoint experiment. People still prefer funding “aid to the poor” to “welfare.” Even in extraordinary times, we find
mostly ordinary responses to treatment in online samples.

17

References
Arechar, Antonio A and David Rand. 2020. “Turking in the time of COVID.” PsyArXiv .
Aronow, Peter M, Jonathon Baron and Lauren Pinson. 2019. “A note on dropping experimental subjects who fail a manipulation check.” Political Analysis 27(4):572–589.
Aronow, Peter Michael, Joshua Kalla, Lilla Orr and John Ternovski. 2020. “Evidence of
Rising Rates of Inattentiveness on Lucid in 2020.” SocArXiv .
Berinsky, Adam J, Gregory A Huber and Gabriel S Lenz. 2012. “Evaluating online labor
markets for experimental research: Amazon.com’s Mechanical Turk.” Political analysis
20(3):351–368.
Berinsky, Adam J, Michele F Margolis and Michael W Sances. 2014. “Separating the shirkers
from the workers? Making sure respondents pay attention on self-administered surveys.”
American Journal of Political Science 58(3):739–753.
Berinsky, Adam J, Michele F Margolis and Michael W Sances. 2016. “Can we turn shirkers
into workers?” Journal of Experimental Social Psychology 66:20–28.
Berinsky, Adam J, Michele F Margolis, Michael W Sances and Christopher Warshaw. 2019.
“Using screeners to measure respondent attention on self-administered surveys: Which
items and how many?” Political Science Research and Methods pp. 1–8.
Camerer, Colin F, Anna Dreber, Eskil Forsell, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, Johan Almenberg, Adam Altmejd, Taizan Chan
et al. 2016. “Evaluating replicability of laboratory experiments in economics.” Science
351(6280):1433–1436.
Coppock, Alexander. 2019. “Generalizing from Survey Experiments Conducted on Mechanical Turk: A Replication Approach.” Political Science Research and Methods 7(3):613–628.
Coppock, Alexander and Oliver A McClellan. 2019. “Validating the demographic, political, psychological, and experimental results obtained from a new source of online survey
respondents.” Research & Politics 6(1):2053168018822174.
Coppock, Alexander, Thomas J. Leeper and Kevin J. Mullinix. 2018. “Generalizability
of heterogeneous treatment e↵ect estimates across samples.” Proceedings of the National
Academy of Sciences 115(49):12441–12446.
18

Cronbach, Lee J. and Karen. Shapiro. 1982. Designing evaluations of educational and social
programs. San Francisco: Jossey-Bass.
Findley, Michael G, Kyosuke Kikuta and Michael Denly. 2020. “External Validity.” Annual
Review of Political Science .
Gadarian, Shana Kushner and Bethany Albertson. 2014. “Anxiety, Immigration, and the
Search for Information.” Political Psychology 35(2):133–164.
Glass, Gene V. 1976. “Primary, secondary, and meta-analysis of research.” Educational
researcher 5(10):3–8.
Huber, Gregory A and Celia Paris. 2013. “Assessing the programmatic equivalence assumption in question wording experiments: Understanding why Americans like assistance to
the poor more than welfare.” Public Opinion Quarterly 77(1):385–397.
IJzerman, Hans, Neil A Lewis, Andrew K Przybylski, Netta Weinstein, Lisa DeBruine,
Stuart J Ritchie, Simine Vazire, Patrick S Forscher, Richard D Morey, James D Ivory
et al. 2020. “Use caution when applying behavioural science to policy.” Nature Human
Behaviour 4(11):1092–1094.
Kane, John V., Yamil R. Velez and Jason Barabas. 2020. “Analyze the Attentive and Bypass
Bias: Mock Vignette Checks in Survey Experiments.” Unpublished Manuscript .
Klein, Richard A, Kate A Ratli↵, Michelangelo Vianello, Reginald B Adams Jr, Štěpán
Bahnı́k, Michael J Bernstein, Konrad Bocian, Mark J Brandt, Beach Brooks, Claudia Chloe Brumbaugh et al. 2014. “Investigating variation in replicability.” Social psychology .
Klein, Richard A, Michelangelo Vianello, Fred Hasselman, Byron G Adams, Reginald B
Adams Jr, Sinan Alper, Mark Aveyard, Jordan R Axt, Mayowa T Babalola, Štěpán Bahnı́k
et al. 2018. “Many Labs 2: Investigating variation in replicability across samples and
settings.” Advances in Methods and Practices in Psychological Science 1(4):443–490.
Montgomery, Jacob M, Brendan Nyhan and Michelle Torres. 2018. “How conditioning on
posttreatment variables can ruin your experiment and what to do about it.” American
Journal of Political Science 62(3):760–775.

19

Mullinix, Kevin J., Thomas J. Leeper, James N. Druckman and Jeremy Freese. 2015. “The
Generalizability of Survey Experiments.” Journal of Experimental Political Science 2:109–
138.
Munger, Kevin. 2020. “Knowledge Decays: Temporal Validity and Social Science in a Changing World.” Unpublished Manuscript .
Open Science Collaboration. 2015. “Estimating the reproducibility of psychological science.”
Science 349(6251).
Oppenheimer, Daniel M, Tom Meyvis and Nicolas Davidenko. 2009. “Instructional manipulation checks: Detecting satisficing to increase statistical power.” Journal of experimental
social psychology 45(4):867–872.
Paolacci, Gabriele, Jesse Chandler, Panagiotis G Ipeirotis et al. 2010. “Running experiments
on Amazon Mechanical Turk.” Judgment and Decision Making 5(5):411–419.
Permut, Stephanie, Matthew Fisher and Daniel M Oppenheimer. 2019. “Taskmaster: A
tool for determining when subjects are on task.” Advances in Methods and Practices in
Psychological Science 2(2):188–196.
Peyton, Kyle. 2020. “Does Trust in Government Increase Support for Redistribution?
Evidence from Randomized Survey Experiments.” American Political Science Review
114(2):596–602.
Rosenfeld, Daniel L, Emily Balcetis, Brock Bastian, Elliot Berkman, Jennifer Bosson, Ti↵any
Brannon, Anthony L Burrow, Daryl Cameron, CHEN Serena, Jonathan E Cook et al.
forthcoming. “Conducting Social Psychological Research in the Wake of COVID-19.”
Perspectives on Psychological Science. .
Tversky, Amos and Daniel Kahneman. 1981. “The framing of decisions and the psychology
of choice.” science 211(4481):453–458.
Valentino, Nicholas A., Antoine J. Banks, Vincent L. Hutchings and Anne K. Davis. 2009.
“Selective Exposure in the Internet Age: The Interaction between Anxiety and Information
Utility.” Political Psychology 30(4):591–613.
Young, Lauren E. 2019. “The Psychology of State Repression: Fear and Dissent Decisions
in Zimbabwe.” American Political Science Review 113(1):140–155.
20

Online Appendix for:
The Generalizability of Online Experiments Conducted
During The COVID-19 Pandemic

Contents
A Individual Studies
A.1 Russian reporters and American news . . . . . . .
A.2 E↵ect of framing on decision making . . . . . . .
A.3 Gain versus loss framing . . . . . . . . . . . . . .
A.4 Welfare versus aid to the poor . . . . . . . . . . .
A.5 Gain versus loss framing with party endorsements
A.6 Foreign aid misperceptions . . . . . . . . . . . . .
A.7 Perceived intentionality for side e↵ects . . . . . .
A.8 Atomic aversion . . . . . . . . . . . . . . . . . . .
A.9 Attitudes toward immigrants . . . . . . . . . . . .
A.10 Fake news corrections . . . . . . . . . . . . . . . .
A.11 Inequality and System Justification . . . . . . . .
A.12 Trust in government and redistribution . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

1
1
2
4
6
10
13
14
17
21
25
26
28

B Covariate distributions

31

C Treatment descriptions
C.1 Attention Check Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . .

38
43

List of Figures
A.1 E↵ect of question ordering on support for Russian journalists in U.S. . . . .
A.2 E↵ect of “Cheap” vs. “Expensive” frame on decision to travel . . . . . . . .
A.3 E↵ect of gain vs. loss frame in “Asian disease” problem . . . . . . . . . . . .

2
4
6

A.4 E↵ect of “Aid to Poor” vs. “Welfare” on support for government spending .
A.5 E↵ect of gain vs. loss framing experiment with party endorsement . . . . . .
A.6 E↵ect of policy-specific information on support for foreign aid . . . . . . . .
A.7 E↵ect of Harm vs. Help frame on perceived intentionality . . . . . . . . . . .
A.8 Support for prospective U.S. strike on Al Queda nuclear weapons lab in Syria
A.9 Support for retrospective U.S. strike on Al Queda nuclear weapons lab in Syria
A.10 E↵ects of immigrant attributes on support for admission to U.S. . . . . . . .
A.11 E↵ect of corrections on agreement with inaccurate statements . . . . . . . .
A.12 E↵ect of “high inequality” treatment on comprehension questions and system
justification scales . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.13 E↵ect of corruption information on trust in government and support for redistribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.1 Region proportions by sample . . . . . . . . . . . . . . . . . . . . . . . . . .
B.2 Education proportions by sample . . . . . . . . . . . . . . . . . . . . . . . .
B.3 Household income proportions by sample . . . . . . . . . . . . . . . . . . . .
B.4 Age proportions by sample . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.5 Male v. Female proportions by sample . . . . . . . . . . . . . . . . . . . . .
B.6 Race/Ethnicity proportions by sample . . . . . . . . . . . . . . . . . . . . .
B.7 Partisanship proportions by sample . . . . . . . . . . . . . . . . . . . . . . .
B.8 Voting behavior in 2016 proportions by sample . . . . . . . . . . . . . . . . .
C.1 E↵ect of framing on decision making: cheap condition (original) . . . . . . .
C.2 E↵ect of framing on decision making: expensive condition (original) . . . . .
C.3 E↵ect of framing on decision making: cheap condition (modified) . . . . . . .
C.4 E↵ect of framing on decision making: expensive condition (modified) . . . .
C.5 Perceived intentionality for side e↵ects: helped condition (original) . . . . . .
C.6 Perceived intentionality for side e↵ects: harmed condition (original) . . . . .
C.7 Perceived intentionality for side e↵ects: helped condition (modified) . . . . .
C.8 Perceived intentionality for side e↵ects: harmed condition (modified) . . . .
C.9 Pre-ACQ article for “‘Easy” and “Medium” ACQ . . . . . . . . . . . . . . .
C.10 “Easy” and “Medium” ACQ with correct responses highlighted . . . . . . .
C.11 “Hard” ACQ with correct response highlighted . . . . . . . . . . . . . . . . .

9
12
14
16
20
21
24
26
28
30
31
32
33
34
35
35
36
37
38
38
38
38
39
40
41
42
43
44
44

A

Individual Studies

Here we provide a detailed description of each study, their pre-COVID estimates, and the
individual replication estimates that we obtained. All within-study estimates for non-binary
outcomes are standardized using Glass’s

, which scales outcomes by the standard deviation

in the control group (Glass, 1976). For all binary outcomes (e.g., the conjoint experiment)
we report unstandardized estimates as these have a straightforward interpretation as the
percentage point di↵erence between the reference category and the level(s) of treatment.

A.1

Russian reporters and American news

This classic study by Hyman and Sheatsley (1950) shows that American subjects express
more tolerance for Russian journalists to “come in here and send back to their papers the
news as they see it” if they are first asked whether American journalists should be allowed
to operate in Russia. The operating principle seems to be one of reciprocal fairness – after
affirming that American journalists should be allowed to work in Russia, subjects appear to
feel constrained to allow Russian journalists to work in America.
Pooling across the pre-COVID studies, the summary e↵ect size estimate is a 30 percentage
point increase in support for Russian journalists. Our e↵ect estimate of 25.5 points is 83%
of this magnitude, and this di↵erence of 5.5 percentage points is not statistically significant
(P = 0.16). Our replication estimate is also not statistically distinguishable from the two
earlier replications reported in Schuman and Presser (1996). The baseline levels of support
for Russian journalists in the control condition among Americans in 1950 (36%), 1980 (55%)
and 1983 (44%) are quite similar to COVID-era Lucid respondents (45%).

1

Figure A.1: E↵ect of question ordering on support for Russian journalists in U.S.
Hyman and Sheatsley (1950)
Schuman and Presser (1980)
Schuman et al. (1983)
Week 3
0%

A.2

10%

20%
30%
Average treatment effect estimate

40%

E↵ect of framing on decision making

In a classic framing experiment by Tversky and Kahneman (1981, Study 10), undergraduates
were instructed to imagine a scenario in which they were buying two items, one for $15 and
another for $125. Participants in the “cheap” condition read the following prompt, with
those in the “expensive” condition seeing the prices in parentheses: “Imagine that you are
about to purchase a jacket for $125 ($15), and a calculator for $15 ($125). The salesman
informs you that the calculator you wish to buy is on sale for $10 ($120) at the other branch
of the store, located 20 minutes drive away. Would you make the trip to the other store?”
Although the total cost of both items was $140 in each condition, with a potential of $5
in savings for traveling, 68% of participants said they would travel when they could save $5
on the $15 item, whereas 29% said they would travel when they could save $5 on the $125
item. According to Tversky and Kahneman (1981), this di↵erence of 39 percentage points
illustrates how individuals’ assess the potential gains and losses of outcomes in relative,
rather than absolute, terms. When paying $15 for an item, a $5 discount seems substantial,
whereas a $5 discount on a $125 item seems negligible.
This experiment has been replicated numerous times in both student and online samples.
We use a slightly modified version of the original study from Klein et al. (2018) as our preCOVID benchmark for online samples. In this study, participants were presented with the
2

following prompt, with those in the “expensive” condition seeing the prices in parentheses:
“Imagine that you are about to purchase a ceramic vase for $250 ($30), and a wall hanging
for $30 ($250). The salesman informs you that the wall hanging you wish to buy is on sale
for $20 ($240) at the other branch of the store, located 20 minutes drive away. Would you
make the trip to the other store?”
In the replication by Klein et al. (2018), 49% of participants said they would travel to
save $10 on the “cheap” wall-hanging whereas 32% said they would travel to save $10 on
the “expensive” wall-hanging. In our Week 7 replication study, half the participants were
assigned to an experiment using this same scenario (wall hanging and ceramic vase). The
other half were assigned to a COVID-specific scenario, where “ceramic vase” was replaced
with “a box of Clorox disinfecting wipes,” and “wall hanging” was replaced with “a box of
N-95 respirator masks”. See Figures C.1-C.4 for a full description of each condition.
Figure A.2 plots estimated treatment e↵ects from the original study (student sample)
alongside the pre-COVID replication (online sample) and our replication. The estimated
e↵ect in our direct replication of 15 percentage points was indistinguishable from the preCOVID replication (16 percentage points). For the COVID-specific experiment, the estimated treatment e↵ect of 7 percentage points was indistinguishable from zero, and smaller
than both the pre-COVID replication (di↵erence of 9 percentage points, SE = 0.05, P = 0.02)
and our direct replication (di↵erence of 8 percentage points, SE = 0.06, P = 0.09). Pooling across the pre-COVID replication and original, the summary e↵ect size estimate is 0.17
(SE = 0.01, P < 0.01), compared to 0.11 (SE = 0.03, P < 0.01) for our replications for a
correspondence of 64%.

3

Figure A.2: E↵ect of “Cheap” vs. “Expensive” frame on decision to travel
Tversky and Kahneman (1981)
Many Labs (2018)
Direct replication
COVID−specific
0%

20%
40%
Average treatment effect estimate

Although our replications estimates are, on average, smaller than the pre-COVID average,
the direct replication closely approximates the 2018 study and all replication estimates are in
the expected direction. We also note that the estimated e↵ect from the COVID-specific replication is smaller but statistically indistinguishable from the direct replication. This raises
the possibility that the COVID-specific language in the replication slightly decreased the
power of the framing e↵ect. We nevertheless conclude that the replications were successful.

A.3

Gain versus loss framing

In this classic framing experiment by Tversky and Kahneman (1981, Study 1), undergraduates were instructed to imagine the U.S. was preparing for the outbreak of an unusual
“Asian disease”, which was expected to kill 600 people. In the “gain” framing condition,
participants were asked to select between two courses of action to combat the disease: if
Program A is adopted, 200 people will be saved; if Program B is adopted, there is a 1/3
probability that 600 people will be saved, and 2/3 probability that no people will be saved.
In the “loss” framing condition, participants were asked to select between two di↵erent formulations: if Program A is adopted, 400 people will die; if Program B is adopted, there is a
1/3 probability that nobody will die, and 2/3 probability that 600 people will die.
In both framing conditions, the expected number of deaths is 400 for both Program A

4

and Program B. In the original study, 72% selected Program A in the gain frame, whereas
22% selected Program A in the loss frame, for an estimated treatment e↵ect of 50 percentage points. According to Tversky and Kahneman (1981), the observed preference reversal
illustrates how individuals’ choices involving gains are risk averse whereas choices involving
losses are risk seeking.
This experiment has been widely replicated across time in both student samples and
online samples. Figure A.3 plots estimated treatment e↵ects from the original study (student
sample) alongside those obtained from pre-COVID studies (online samples) in the Many Labs
replication project (Klein et al., 2014) in 2013, in 2013 on MTurk (Berinsky, Huber and Lenz,
2012), in 2016 on Lucid (Coppock and McClellan, 2019), and in five of our replications. The
first four of our replications are COVID-specific versions of the original. Participants were
instead asked to imagine that “the Mayor of a U.S. city is preparing for another outbreak
of the novel coronavirus in the Spring of 2021, which is expected to kill 600 city residents.”
The fifth replication is a direct replication of the pre-COVID experiments using the original
wording.

5

Figure A.3: E↵ect of gain vs. loss frame in “Asian disease” problem
Kahneman & Tversky (1981)
Many Labs (2013) − Project Implicit
Many Labs (2013) − MTurk
Coppock & McClellan (2016) − Lucid
Coppock & McClellan (2016) − MTurk
Week 1 − COVID−specific
Week 3 − COVID−specific
Week 7 − COVID−specific
Week 8 − COVID−specific
Week 13 − Direct replication
0%

20%
40%
Average treatment effect estimate

60%

The summary e↵ect size for our five replications is 0.15 (SE = 0.02, P < 0.01), approximately 50% the size of the summary e↵ect size for the five pre-COVID experiments
(summary e↵ect size: 0.29, SE = 0.01, P < 0.01). Although the replications estimates are,
on average, smaller than those from pre-COVID experiments (di↵erence: 0.14, SE = 0.02,
P < 0.01), all COVID-era replication estimates are statistically distinguishable from zero,
and in the expected direction. We therefore conclude that the replications were successful,
regardless of whether COVID-specific language was used in the scenario description.

A.4

Welfare versus aid to the poor

The large e↵ect of describing government assistance as “welfare” rather than “aid to the
poor” is one of the most robust experimental findings in political science. In the original
experiment (Smith, 1987), a sample of U.S. adults from the General Social Survey (GSS)
were asked by telephone survey whether they believed there was “too much”, “about the

6

right amount”, or “too little” spending across multiple issues, with social welfare benefits
being described as “assistance for the poor” in the treatment arm and “welfare” in the control
arm. This experiment has been replicated on GSS surveys from 1986 to 2018. Respondents
are consistently more supportive of government spending on “assistance for the poor” than
“welfare”.1
Figure A.4 plots estimated treatment e↵ects from nine replications of the original experiment on GSS respondents using computer-assisted personal interviews (CAPI) and paper
and pencil interviews (PAPI), alongside those obtained from our twelve replications, and
two pre-COVID replications, using computer-assisted web interviews (CAWI). Following
prior replications on online samples (e.g., Huber and Paris, 2013), responses are coded as
-1 (“too much”), 0 (“about the right amount”), and 1 (“too little”). Estimated treatment
e↵ects are standardized using Glass’s

, and positive estimates indicate respondents are

more supportive of spending on “Aid to the Poor” than “Welfare”. Two of our replications
(Week 9 and Week 5) were within-subject experiments that asked respondents both spending questions in randomized order. All replication estimates are statistically distinguishable
from zero and are in the expected direction.2
The summary e↵ect size for the experimental estimates from CAWI surveys is 0.44 (SE
= 0.02, P < 0.01), approximately 44% the size of the summary e↵ect sizes for the CAPI
(1.00, SE = 0.01, P < 0.01) and PAPI (1.05, SE = 0.01, P < 0.01) surveys from the GSS.
Within the CAWI surveys, however, our experimental estimates (summary e↵ect size: 0.43,
SE = 0.02, P < 0.01) are indistinguishable from the pre-COVID estimates (0.46, SE = 0.05,
P < 0.01) from Huber and Paris (2013). Unlike in the CAWI surveys, there is also strong
1

See Huber and Paris (2013) for evidence that individuals believe these labels describe di↵erent social
programs.
2
Interestingly, the non-experimental within-subjects estimates are solidly in line with the experimental
estimates, suggesting that subjects feel no pressure to keep their support for welfare consistent with their
support for aid to the poor. This pattern contrasts strongly with the evident pressure for consistency in the
Russian journalists experiment. For further discussion on tradeo↵s in the choice of within versus between
subjects designs, see Cli↵ord, Sheagley and Piston (2020).

7

evidence of statistical heterogeneity among estimates from the CAPI and PAPI surveys.3
Given the variation in GSS question wordings, survey modes, and estimates across time,
we benchmark against the original 1986 estimate and the two replications on online samples
from Huber and Paris (2013). The summary e↵ect size for these three estimates is 0.70 (SE
= 0.04, P < 0.01), and our COVID-era replication estimates are approximately 61% the size
of this pre-COVID benchmark. We note that significant di↵erences between estimates across
CAWI, PAPI, and CAPI surveys was a feature of experimental research prior to COVID.

3

CAWI

2

: 12.72, P = 0.31, CAPI

2

: 17.21, P = 0.03, PAPI

8

2

: 85.19, P < 0.01

Figure A.4: E↵ect of “Aid to Poor” vs. “Welfare” on support for government spending

Computer−assisted web interviews
Huber & Paris (2013) − MTurk
Huber & Paris (2013) − YouGov
Week 1
Week 2
Week 3
Week 4
Week 5
Week 6
Week 7
Week 8
Week 9
Week 11
Week 12
Week 13

Observational
Experimental

Computer−assisted personal interviews
GSS 2002
GSS 2004
GSS 2006
GSS 2008
GSS 2010
GSS 2012
GSS 2014
GSS 2016
GSS 2018

Paper and pencil interviews
GSS 1986
GSS 1987
GSS 1988
GSS 1989
GSS 1990
GSS 1991
GSS 1993
GSS 1994
GSS 1996
GSS 1998
GSS 2000
0.0

0.2
0.4
0.6
0.8
1.0
1.2
Average treatment effect estimate (standard units)

Notes: Starting in 2002, the GSS replaced “assistance to the poor” with “assistance for the poor.”
Week 13 is a direct replication of Huber and Paris (2013) using the GSS question wordings. The other
replications use the ANES question wording, which asks whether respondents think spending should
be “increased” (coded 1), “kept the same” (coded 0), or “decreased” (coded -1).

9

A.5

Gain versus loss framing with party endorsements

Druckman (2001) extended the “Asian disease” protocol to explicitly incorporate political
considerations. In his original study, a sample of undergraduates were randomly assigned to
the classic version of the study or a modified version that randomly assigned party endorsements instead of “Program A” and “Program B”. In the “gain” framing condition, participants were asked to select between two courses of action, with one of three randomly assigned
labels: If [Program A, the Democrats’ Program, the Republicans’ Program] is adopted, 200
people will be saved; If [Program B, the Republicans’ Program, the Democrats’ Program],
there is a 1/3 probability that 600 people will be saved, and a 2/3 probability that no people
will be saved. In the “loss” framing condition, the descriptions were: If [Program A, the
Democrats’ Program, the Republicans’ Program] is adopted, 400 people will die; If [Program B, the Republicans’ Program, the Democrats’ Program], there is a 1/3 probability
that nobody will die, and a 2/3 probability that 600 people will die.
In the original study, the preference reversal e↵ect from Tversky and Kahneman (1981)
was replicated when “Program A” and “Program B” were used as labels. However, these
e↵ects were greatly attenuated (or indistinguishable from zero) when the programs were
labeled with party endorsements. According to Druckman (2001), this di↵erence illustrates
how partisans’ desire to choose their party’s program can overwhelm preference reversals
due to “pure” framing e↵ects.
Figure A.5 plots estimated treatment e↵ects from the original study (student sample)
alongside three replications. Two of our replications (Week 7 and Week 8) are COVIDspecific versions of the original where “unusual Asian disease” was replaced with “another
outbreak of the novel coronavirus”. The Week 13 replication is a direct replication of the
original Asian disease experiment. All estimates are statistically distinguishable from zero
and in the expected direction when “Program A” is used to describe the “risk-averse alternative” (e.g. save 200 people versus 400 people will die). Consistent with the original
10

experiment, however, adding the partisan labels attenuate (or eliminate) preferences reversals
among partisans: among Democrats, preference reversal e↵ects are indistinguishable from
zero when “Program A” is replaced with “Republicans’ Program”; among Republicans, preference reversal e↵ects are indistinguishable from zero when “Program A” is replaced with
“Democrats’ Program”.
Table A.5 provides a summary of di↵erences between the original study and the replications by treatment arm. Although the replications estimates are, on average, smaller than
those from the original study, all replication estimates are in the expected direction. We
therefore conclude that the original study replicated.

11

Figure A.5: E↵ect of gain vs. loss framing experiment with party endorsement

Framing effects by label of risk−averse alternative, among Democrats
Week 7

Program A

Druckman (2001)
Week 8
Week 13 (direct replication)

Democrats'
Program

Republicans'
Program

Framing effects by label of risk−averse alternative, among Republicans
Program A

Democrats'
Program

Republicans'
Program
−40%

−20%

0%
20%
40%
Average treatment effect estimate

12

60%

80%

Table 1: Summary e↵ect sizes in gain vs. loss framing experiment with party endorsement
Label of risk-averse

Partisan

Replication

Original

alternative

subgroup

Summary

Study

Program A

Democrats

0.15 (0.04)*

0.46 (0.11)*

-0.31 (0.12)*

0.32

Program A

Republicans

0.20 (0.05)*

0.41 (0.15)*

-0.22 (0.15)

0.47

Democrats’ Program

Democrats

0.11 (0.03)*

0.21 (0.11)

-0.11 (0.12)

0.50

Democrats’ Program

Republicans

0.06 (0.04)

0.04 (0.13)

0.02 (0.14)

1.66

Republicans’ Program

Democrats

0.05 (0.04)

0.15 (0.10)

-0.09 (0.10)

0.35

Republicans’ Program

Republicans

0.10 (0.03)*

0.06 (0.14)

0.04 (0.14)

1.70

Di↵erence

Relative
E↵ect Size

Notes: Relative size is the summary e↵ect size divided by the original e↵ect size: values less than 1
indicate summary e↵ect sizes smaller than original e↵ect sizes. P < 0.05⇤ .

A.6

Foreign aid misperceptions

In the original experiment (Gilens, 2001), respondents from a nationally representative telephone survey fielded in 1998 were queried about their support for spending on foreign aid
after being read a randomly assigned prompt about a hypothetical news story. In the control
condition, the prompt read “The story is about a news report that was just released about
American foreign aid to help other countries. Have you heard about this story?” In the
treatment condition, the prompt added factual information designed to correct misperceptions about foreign aid spending: “It said that the amount of money we spend for foreign aid
has been going down and now makes up less than one cent of every dollar that the federal
government spends.”
Following the prompt, respondents were asked: “How do you feel about the amount of
money the federal government (in Washington) spends on foreign aid to other countries? Do
you think the federal government should spend more of foreign aid, less, or about the same
as it does not?” The original study reported that respondents in the treatment condition
13

were 16.6 percentage points less likely to support cuts for foreign aid than respondents
in the control group. According to Gilens (2001), this illustrates that the general public
over-estimates the percentage of the budget allocated to foreign aid, but correcting this
misperception with policy-specific information can decrease opposition to foreign aid.
In the original study, support for foreign aid was measured on a 3-point scale: “Less”
(coded -1), “About the same” (coded 0), “More” (coded 1). The 16.6 percentage point e↵ect
reported in Gilens (2001) was obtained from a logistic regression of the binary treatment
indicator on a truncated outcome, i.e. Yi = 1 if respondent selected “Less”; 0otherwise, and
a variety of control variables. In our reanalysis of the original data, we estimate treatment
e↵ects using di↵erence-in-means with the raw three-point outcome variable.
Figure A.6 plots estimated treatment e↵ects from the original study (telephone interview)
alongside our replication (online interview). The estimated treatment e↵ect in the original
study is an increase in support for foreign aid of 0.21 (SE = 0.06, P =< 0.01). The estimated
treatment e↵ect in the replication study is a decrease in support for foreign aid by 0.05 (SE
= 0.06, P = 0.40). The estimate from the original study is therefore 0.26 units larger – in
the opposite direction – than the replication study (SE = 0.09, P < 0.01 ). This is the only
experimental result among our set that we classify as a replication failure.
Figure A.6: E↵ect of policy-specific information on support for foreign aid
Gilens (2001)
Week 3
−0.1

A.7

0.0
0.1
0.2
Average treatment effect estimate (standard units)

0.3

Perceived intentionality for side e↵ects

In the original study (Knobe, 2003, Study 1), individuals were recruited from a New York
City park to participate in an experiment based on the following vignette:
14

The vice-president of a company went to the chairman of the board and said, “We are
thinking of starting a new program. It will help us increase profits, and it will also help
the environment.”
The chairman of the board answered, “I don’t care at all about helping the environment.
I just want to make as much profit as I can. Let’s start the new program.”

Respondents assigned to the Help condition read that the chairman’s decision had a
helpful side e↵ect: “They started the new program. Sure enough, the environment was
helped.” Those assigned to the Harm condition read “They started the new program. Sure
enough, the environment was harmed.” In the Help condition, 23% of subjects agreed with
the statement “The chairman helped the environment intentionally,” whereas 82% of those
in the Harm condition agreed that “The chairman harmed the environment intentionally.”
According to Knobe (2003), this estimated treatment e↵ect of 59 percentage points illustrates how the perceived intentionality of individual actions depends upon whether their
consequences are helpful or harmful.
This experiment has been replicated multiple times in both student and online samples.
We use the replication study from Klein et al. (2018) – which found an estimated treatment
e↵ect of 64 percentage points – as our pre-COVID benchmark for online samples. In our
replications, half the participants were assigned to an experiment using this same scenario
as the original study. The other half were assigned to a COVID-specific scenario, based on
the following vignette:
The vice-president of a company went to the chairman of the board and said, “We are
thinking of marketing a new drug to treat COVID-19. It will help us increase profits,
and the drug will also help older people with heart conditions.”
The chairman of the board answered, “I don’t care at all about helping older people with
heart conditions. I just want to make as much profit as I can. Let’s start marketing
the new drug.”

15

Those assigned to the Help condition read that the chairman’s decision had a harmful
side e↵ect: “They started marketing the new drug. Sure enough, older people with heart
conditions were helped.” Those assigned to the Harm condition instead read that older
people with heart conditions were “harmed”. See Figures C.5-C.8 for full text of each
condition.
Figure A.7 plots estimated treatment e↵ects from the original study (sample from Manhattan park) alongside the Many Labs (2018) replication (online sample) and our replications
(online sample). The estimated treatment e↵ect is 0.39 (SE = 0.04, P < 0.01) in the direct replication and 0.38 in the COVID-specific replication (SE = 0.04, P < 0.01). The
estimated treatment e↵ect for the COVID-specific replication is about 0.01 points smaller
than the direct replication (SE = 0.04, P = 0.46). The direct replication is approximately
60% the size of the pre-COVID benchmark (di↵erence of 0.25 points, SE = 0.04, P < 0.01).
The replication estimates are considerably smaller than the pre-COVID benchmark, but all
estimates are in the expected direction and statistically distinguishable from zero. We therefore conclude that the pre-COVID study replicated, regardless of whether COVID-specific
language was used in the vignettes.
Figure A.7: E↵ect of Harm vs. Help frame on perceived intentionality
Knobe (2003)
Many Labs (2018)
Direct replication
COVID−specific
0%

20%

40%
Average treatment effect estimate

16

60%

80%

A.8

Atomic aversion

In the original study (Press, Sagan and Valentino, 2013), a quota sample of online survey
participants recruited by YouGov were randomly assigned to participate in one of two independent experiments. In the prospective experiment, subjects read a hypothetical news
article that reported U.S. officials were deciding between nuclear and conventional military
options for destroying an Al Qaeda nuclear weapons lab in Syria. The article compared the
expected e↵ectiveness of each military option, and estimated 1,000 Syrian civilian deaths regardless of which option was pursued. Within this experiment, subjects were assigned to one
of three treatment arms that varied only in the likely success of the conventional strike: 1)
a “90/90” condition in which the nuclear and conventional strike both had a 90% chance of
success; 2) a “90/70“ condition in which the conventional strike had a 75% chance of success;
3) a “90/45” condition in which the conventional strike had a 45% chance of success. The
relative e↵ectiveness of each option was described in the article text, alongside a two-by-two
matrix that compared the chances of success (90/90, 90/70, or 90/45) and estimated civilian
causalities (fixed) for both options.
In the retrospective experiment, subjects read a hypothetical news article that described
a U.S. military strike that had already been carried out on the Al Qaeda lab. Within this
experiment, subjects were assigned to one of two treatment arms that described the weapons
used to carry out the attack: 1) a “conventional strike” condition in which 100 conventional
cruise missiles were used; 2) a “nuclear strike” condition in which 2 nuclear cruise missiles
were used. The number of civilian casualties and the outcome (the lab was successfully
destroyed) were fixed across conditions.
In both experiments, all subjects were informed prior to random assignment that, if they
failed to pass comprehension questions about the article, they could be ineligible to finish.
If they instead answered the comprehension questions correctly, they were told they would
be eligible to participate in a ra✏e for a $100 gift certificate. subjects who failed to pass the
17

post-treatment comprehension questions were excluded from the analysis sample.
Aronow, Baron and Pinson (2019) noted this practice of dropping subjects who fail posttreatment “manipulation checks” can induce bias in estimates of treatment e↵ects. They
replicated the original study on a sample of MTurk workers and found that retaining subjects
who failed the comprehension questions resulted in di↵erent estimates than the original
study. In this replication, subjects were also told in advance that they would be ineligible
to complete the survey if they failed the comprehension questions, but were entered into a
ra✏e for a $100 bonus payment if they passed. In addition, respondents in the prospective
experiment viewed a large version of the two-by-two graphic that appeared in the article after
the comprehension questions were answered, but before viewing any outcome questions.
Lucid does not give researchers the ability to pay survey respondents bonuses, so no incentives could be o↵ered for those who passed the comprehension questions in our replications.
In addition, subjects in our replications of the prospective experiment viewed the article
once, and a two-by-two graphic was not presented after the comprehension questions (as in
Aronow, Baron and Pinson, 2019). All other design details were the same as in the original
study.
Figure A.8 plots estimated treatment e↵ects in the prospective experiment, with the 90/90
condition as the control group, for the original study, the replication by Aronow, Baron
and Pinson (2019), and our three replications. In the original study, the 90/70 condition
caused an increase in the proportion of subject that preferred the nuclear option by about
37 percentage points relative to the 90/90 condition. The estimated e↵ect of the 90/45
condition, relative to the 90/90 condition, was 51 percentage points. Similarly, the 90/70
condition caused an increase in the proportion of subjects that approved of the nuclear
option by about 17 percentage points, and the 90/45 condition caused an increase of about
27 percentage points. In other words, estimated treatment e↵ects increased monotonically
with the relative e↵ectiveness of nuclear weapons.
18

Estimated treatment e↵ects in the pre-COVID replication by Aronow, Baron and Pinson (2019) were similar to the original study for both outcome measures. The estimated
treatment e↵ects in our replications were considerably smaller for the “Prefer Nuclear Use”
outcome, but of the expected sign and statistically distinguishable from zero. Estimates for
the “Approve Nuclear Use” outcome, however, were of the opposite sign and not distinguishable from zero in 2/3 of our replications. Table 2 provides a summary of the di↵erences in
estimates from the prospective experiment in our replications, the original study, and the
ABP replication.
Figure A.9 plots estimated treatment e↵ects in the retrospective experiment, with the
“conventional strike” condition as the control group. The original study reported that differences between the “nuclear strike” (treatment) and “conventional strike” (control) were
“substantively small and not statistically significant” (Press, Sagan and Valentino, 2013,
p. 197). The pre-COVID replication byAronow, Baron and Pinson (2019) found, however,
that the nuclear strike caused a 12 percentage point reduction in the proportion of respondents who “approved” the strike, and a 13 percentage point reduction in the proportion
who believed the strike was “ethical”. Table 3 provides a summary of the di↵erences in
estimates from the retrospective experiment in our replications, the original study, and the
ABP replication.
When compared to the original study and the ABP replication, our replication estimates
for the “Prefer Nuclear Use” outcome in the prospective experiment are significantly smaller,
but of the expected sign and statistically distinguishable from zero. The estimates for the
“Approve Nuclear Use” outcome are, however, signed in the opposite direction and indistinguishable from zero. Estimates for both outcomes in the retrospective experiment are
comparable to those reported in the original study and the ABP replication. We therefore
conclude that the atomic aversion experiment was partially replicated.

19

Figure A.8: Support for prospective U.S. strike on Al Queda nuclear weapons lab in Syria

Prefer Nuclear Use

Approve Nuclear Use

90/45

Press et al. (2013)

90/70

Aronow et al. (2019)

Week 5

Week 6

Week 13

−20%

0%

20%

40%
60% −20%
0%
Average treatment effect estimate

20%

40%

60%

Table 2: Summary of estimates in prospective atomic aversion experiment
Group

Outcome

Replication
Summary

Original
Study

Di↵erence

Relative
Size

ABP
Replication

Di↵erence

Relative
Size

90/70
90/45

Prefer
Prefer

0.08 (0.02)*
0.13 (0.03)*

0.30 (0.05)*
0.48 (0.05)*

-0.22 (0.06)*
-0.35 (0.06)*

0.27
0.27

0.37 (0.03)*
0.51 (0.03)*

-0.29 (0.04)*
-0.39 (0.04)*

0.21
0.25

90/70
90/45

Approve
Approve

-0.04 (0.03)
-0.05 (0.03)

0.05 (0.06)
0.28 (0.05)*

-0.09 (0.07)
-0.32 (0.06)*

-

0.17 (0.03)*
0.27 (0.03)*

-0.21 (0.04)*
-0.32 (0.04)*

-

Notes: Relative e↵ect sizes are the replication summary e↵ect sizes divided by the ABP replication or
original e↵ect size: values less than 1 indicate summary e↵ect size is smaller than the ABP or original
e↵ect size. Relative e↵ect sizes are not calculated if replication estimates are the opposite sign of
comparison estimates. P < 0.05⇤ .

20

Figure A.9: Support for retrospective U.S. strike on Al Queda nuclear weapons lab in Syria

Approve Strike

Ethical Strike

Press et al. (2013)

Aronow et al. (2019)

Week 5

Week 6

Week 13
−20%

−10%

0%
10%
−20%
−10%
Average treatment effect estimate

0%

10%

Table 3: Summary of estimates in retrospective atomic aversion experiment
Outcome

Replication
Summary

Original
Study

Di↵erence

Relative
Size

ABP
Replication

Di↵erence

Relative
Size

Approve

-0.06 (0.03)*

-0.07 (0.05)

0.00 (0.06)

0.95

-0.12 (0.03)*

0.06 (0.04)

0.53

Ethical

-0.04 (0.03)

-0.06 (0.06)

0.02 (0.06)

0.64

-0.13 (0.03)*

0.09 (0.04)*

0.28

Notes: Relative e↵ect sizes are the replication summary e↵ect sizes divided by the ABP replication or
original e↵ect size: values less than 1 indicate summary e↵ect size is smaller than the ABP or original
e↵ect size. Relative e↵ect sizes are not calculated if replication estimates are the opposite sign of
comparison estimates. P < 0.05⇤ .

A.9

Attitudes toward immigrants

In the original study (Hainmueller and Hopkins, 2015), 1,407 U.S. respondents from a nationally representative online survey fielded by Knowledge Networks in 2012 participated
in a conjoint experiment that asked them to choose between di↵erent pairs of hypothetical
21

immigrants applying for admission. Each respondent evaluated five di↵erent pairs of immigrants, with immigrants’ backgrounds varying along nine randomly assigned attributes:
gender, education, employment plans, job experience, profession, language skills, country of
origin, reasons for applying, and prior trips to the United States. Each attribute contained
multiple levels (e.g. country was 10 levels and gender was 2) for a total of approximately
900,000 unique immigrant profiles.
After viewing each immigrant pair subjects were presented with a binary choice: “If you
had to choose between them, which of these two immigrants should be given priority to come
to the United States to live?” Each subject evaluated 5 pairs of immigrants for a total of
14,070 observations (1,407 respondents ⇥ 5 pairs ⇥ 2 immigrants per pair). We conducted a
direct replication of this conjoint experiment in May 2020 on a sample of 1,328 respondents,
for a total of 13,280 observations.
Following Hainmueller and Hopkins (2015), we estimate the Average Marginal Component
E↵ects (AMCEs) for each attribute level using a regression of the binary response (1 if the
immigrant profile is preferred, 0 otherwise) on a set of indicators for each attribute level,
with standard errors clustered at the level of the survey respondent. Figure A.10 plots the
results for the original study alongside our direct replication. The top of each panel describes
the omitted reference level for each attribute; for example, the negative point estimates for
“Male” indicate that male immigrants are about 2 percentage points less likely to be selected
than female immigrants.
In total, there are 81 estimated AMCEs in Figure A.10 – 41 for the original study and
41 for the replication. When compared to the original study, the replication estimates are
remarkably similar in both direction and magnitude. Only one of 41 replication estimates
is signed in the opposite direction when compared to the original study – the AMCE for
“Gardener” is 0.02 points in the original study and approximately zero in the replication,
but neither estimate is distinguishable from zero. The majority of the replication estimates
22

(27 of 41) are smaller in magnitude than the original estimates.4 We therefore conclude that
the conjoint experiment was successfully replicated.

4

only 7 of 41 di↵erences are statistically significant after correcting for multiple comparisons to control
the false discovery rate (see e.g. Benjamini and Hochberg, 1995).

23

Figure A.10: E↵ects of immigrant attributes on support for admission to U.S.
Gender (reference: Female applicant)
Pre−COVID
Replication

Male
Education (reference: No formal education)
4th grade
8th grade
High school
Two−year college
College degree
Graduate degree
Language (reference: Applicant spoke fluent English)
Broken English
Tried English but unable
Used interpreter
Country of origin (reference: Germany)
France
Mexico
Philippines
Poland
India
China
Sudan
Somalia
Iraq
Profession (reference: Janitor)
Waiter
Child care provider
Gardener
Financial analyst
Construction worker
Teacher
Computer programmer
Nurse
Research scientist
Doctor

Job experience (reference: No job training or prior experience)
1−2 years
3−5 years
5+ years
Jobs plans (reference: Contract with a U.S. employer)
Interviews with employer
Will look for work
No plans to look for work
Application reason (reference: Reunite with family members already in the U.S.)
Seek better job
Escape persecution
Prior trips to U.S. (reference: Never been to the U.S.)
Once as tourist
Many times as tourist
Six months with family
Once w/o authorization
−30%

−20%

−10%
0%
10%
Average treatment effect estimate

24

20%

A.10

Fake news corrections

In the original study (Porter, Wood and Kirby, 2018), 2,742 MTurk workers were exposed
to two fake news stories randomly selected from a sample of six fake news stories that
were previously circulated (e.g. that Obama’s birth certificate is fake). For each fake news
story, subjects were randomly assigned to see either a correction following the story, or no
correction. Therefore subjects in the “correction” (treatment) condition read a randomly
assigned story followed by a correction stating that the story was false, whereas subjects
in the “no correct” (control) condition simply read the story without seeing a correction.
Therefore, the experiment has 6 ⇥ 2 = 12 treatment arms, with each respondent being
exposed to two unique stories with or without a correction.
The goal of this study was to test whether corrections could reduce individuals’ beliefs in
the veracity of fake news stories. Following exposure to the fake news story (and the correction if assigned treatment) subjects were asked to indicate their agreement or disagreement
about the truth value of the claim advanced on a 5-point scale. Across all six fake news
stories, the authors found that exposure to corrections caused a significant reduction in respondents’ beliefs that the stories were true, with average treatment e↵ects ranging from
-0.24 scale points on the low end to -0.95 scale points on the high end.
We conducted a direct replication of this experiment on a sample of 1,415 respondents in
April 2020. Following the original study, we scale outcomes so that higher values indicate
stronger agreement that the fake news stories were true. Within randomly assigned story,
outcomes are standardized by dividing the response vector by the standard deviation in the
“no correction” (control) group. Figure A.11 compares estimated treatment e↵ects between
the original study and replication, for each of the six stories. All replication estimates,
ranging from -0.04 to -0.67, are uniformly smaller then those in the original study, but all
are correctly signed.5 We therefore conclude that that the replication was successful.
5

Although all 6 replication estimates are smaller in magnitude than those in the original study, only two

25

Figure A.11: E↵ect of corrections on agreement with inaccurate statements
President Obama fakes
birth certificate

Pre−COVID
Replication

President Trump orders
crackdown on sex trafficking
Anthony Scaramucci subject of
Senate Russia investigation
Russian government hacks
Vermont power plant
John Podesta implicated in
disappearance of Madeleine McCann
DC pizza restaurant concealed sex
dungeon used by Democratic elites
0.0
0.4
0.8
Average treatment effect estimate (standard units)

A.11

Inequality and System Justification

In the original study (Trump and White, 2018), 1,020 U.S. respondents from a nationally
representative online survey fielded by Knowledge Networks in 2015 participated in a survey
experiment with random assignment to two conditions. In the “low-inequality” (control)
condition, respondents were exposed to information about trends in U.S. income inequality,
as measured by the Gini coefficient over the period 1968-2010. In the “high-inequality”
(treatment) condition, respondents were exposed to the same information, but the y-axis in
the plot was truncated to make the upward trend appear much steeper.
The goal of this study was the test a hypothesis that exposure to inequality increases
“system justification” – broadly, the psychological need to support the status quo, even at
of these di↵erences (John Podesta and Trump stories) are statistically distinguishable from zero.

26

the expense of their self-interest, or the interests of their group (see e.g. Jost and Banaji,
1994). Trump and White (2018) test the hypothesis that higher inequality causes higher
system justification by comparing di↵erences in subjects’ system justification scores between
the high and low inequality conditions. The key prediction is that increasing subjects’ beliefs
that inequality is rising should decrease system justification.
Following exposure to one of the two data visualizations, respondents completed two
“manipulation check” questions. For the first, respondents were asked to say whether the
statement “Income inequality in the United States has increased dramatically over time” was
correct or incorrect. The second asked respondents whether the statement “the share of total
income of the very rich has not changed much over time in the United States” was correct
or incorrect. According to Trump and White (2018), the high-inequality treatment should
cause an increase in the proportion of respondents stating “correct” to the first question and
“incorrect” to the second question, relative to control. Responses to the first question are
therefore coded 1 if the subject answers “correct” and 0 otherwise. Responses to the second
questions are coded 1 if the subject answers “incorrect” and 0 otherwise.
After this, respondents were randomly assigned to complete one of three batteries of
questions the authors used to measure system justification: an institutional trust scale (6items), a system justification scale (8-items), or an economic system justification scale (15items). In total, 339 subjects (169 in control, 170 in treatment) completed the system
justification scale, 338 completed the institutional trust scale (169 in treatment, 169 in
control), and 336 completed the economic system justification scale (167 in treatment and 169
in control). In our replication, 804 subjects were presented with all three system justification
scales in randomized order. Following the original study, we scale outcomes so that higher
values indicate higher levels of system justification.
Figure A.12 compares estimated treatment e↵ects on the manipulation check questions
(top panel) and outcomes scales (bottom panel) between the original study and the replica27

tion. All replication estimates are in the expected direction when compared to the original
study. Although all 5 replication estimates are smaller in magnitude than those in the original study, none of these di↵erences are statistically distinguishable from zero. We therefore
conclude that the replication was successful.
Figure A.12: E↵ect of “high inequality” treatment on comprehension questions and system
justification scales

Manipulation Checks (binary)
Pre−COVID

Richs' share of income changed

Replication

Inequality increased

0.0

0.1

0.2

0.3

Outcome Scales (standard units)
Econ. System Justification

Institutional Trust

System Justification
−0.4

A.12

−0.3

−0.2
−0.1
Average treatment effect estimate

0.0

0.1

Trust in government and redistribution

In the original study (Peyton, 2020), a total of 3,837 U.S. respondents were exposed to
information about corruption in American government across three separate experiments:
Experiment 1 (624 MTurk workers in 2014); Experiment 2 (nationally representative sample
28

of 1,324 U.S. adults in 2014); Experiment 3 (1,870 MTurk workers in 2017). In each experiment, participants were randomly assigned to one of three treatment arms: “Corrupt”,
“Honest”, or “Control”. In the “Corrupt” arm, subjects read an Op-Ed by a former DOJ
prosecutor that described high levels of political corruption in American politics; the “Honest” arm used contrasting language to describe low levels of political corruption. In the
“Control” arm, subjects read an article of similar length that was devoid of political content.
Experiment 2 was a direct replication of Experiment 1, and Experiment 3 supplemented the
articles with data visualizations that supported the writers’ arguments.
These experiments were used to test a theory that increasing trust in government causes
Americans to become more supportive of redistribution. Peyton (2020) tests this by experimentally manipulating respondents’ trust in government and testing for downstream e↵ects
on respondent’s support for redistribution using a causal instrumental variables framework.
Following exposure to treatment, subjects’ trust in government was measured using a 4item scale. Next, subject’s support for redistribution was measured using a 4-item scale
about federal spending redistributive social policies. The author found significant e↵ects on
subjects’ trust in government in all three experiments, but support for redistribution was
indistinguishable from zero.
We conducted a direct replication of Experiment 3 in the original study on a sample
of 1,424 respondents in May 2020. Following the original study, treatment was coded 0 if
a subject was assigned to the “Corrupt” arm, 0.5 if assigned “Control”, and 1 if assigned
“Honest”. Outcomes were scaled so that higher values indicate more trust in government,
and more support for redistribution.
Figure A.13 compares estimated treatment e↵ects on trust in government (top panel) and
support for redistribution (bottom panel) between the replication and Experiments 1-3 in the
original study. The estimated treatment e↵ect on trust in government in the replication is
statistically distinguishable from zero, and in the expected direction. However, this estimate
29

is significantly smaller in magnitude than all of the estimates in the original study. The
estimated treatment e↵ects on support for redistribution are indistinguishable from zero in
the replication, and statistically indistinguishable from the estimates in the original study.
We therefore conclude this was a successful replication.
Figure A.13: E↵ect of corruption information on trust in government and support for
redistribution

Trust in Government
Original: Experiment 1
Original: Experiment 2
Original: Experiment 3
Week 9 Replication

Support for Redistribution
Original: Experiment 1
Original: Experiment 2
Original: Experiment 3
Week 9 Replication
−0.3

0.0
0.3
0.6
Average treatment effect estimate (standard units)

30

0.9

B

Covariate distributions

In the manuscript, we note that attention check questions (ACQs) can be used to either
screen out inattentive respondents at the design stage, or to estimate treatment e↵ects
among the subset of attentive participants at the analysis stage. We caution, however, that
any procedure for measuring inattention is itself an estimator for a latent individual-level
characteristic (i.e., “attentiveness”). As Berinsky, Margolis and Sances (2014) have shown,
respondents that fail attention checks can di↵er markedly from those who pass on observed
characteristics. We find that classifiers based on respondents’ metadata are correlated with
some covariates as well, suggesting that survey designs that restrict participation to respondents from non-mobile devices or internet browsers could also have implications for sample
composition. These estimates are presented below for the pooled sample of respondents
across all 13 surveys that we conducted between March and July 2020.
Figure B.1: Region proportions by sample

Midwest

Northeast

South

West

Full sample
Browsers
Web Apps
Mobiles
Non−Mobiles

Full sample
Browsers
Web Apps
Mobiles
Non−Mobiles
20%

25%

30%

35% 40% 45%
20% 25%
Proportion of characteristic in sample

31

30%

35%

40%

45%

Figure B.2: Education proportions by sample

No high school diploma

High school diploma

Some college

Associate's degree

Bachelor's degree

Graduate degree

Full sample
Browsers
Web Apps
Mobiles
Non−Mobiles

Full sample
Browsers
Web Apps
Mobiles
Non−Mobiles
0%

10%

20%

30%

0%
10% 20% 30%
0%
Proportion of characteristic in sample

32

10%

20%

30%

Figure B.3: Household income proportions by sample

$19,999 or less

$20,000 − $34,999

$35,000 − $49,999

$50,000 − $64,999

$65,000 − $79,999

$80,000 − $99,999

$100,000 − $124,999

$125,000 − $199,999

$200,000 and above

Full sample
Browsers
Web Apps
Mobiles
Non−Mobiles

Full sample
Browsers
Web Apps
Mobiles
Non−Mobiles

Full sample
Browsers
Web Apps
Mobiles
Non−Mobiles
0%

10%

20%

30%

0%
10% 20% 30%
0%
Proportion of characteristic in sample

33

10%

20%

30%

Figure B.4: Age proportions by sample

18−23

24−29

30−39

40−49

50−59

60−69

Full sample
Browsers
Web Apps
Mobiles
Non−Mobiles

Full sample
Browsers
Web Apps
Mobiles
Non−Mobiles
0%

70+

10%

20%

0%

Full sample
Browsers
Web Apps
Mobiles
Non−Mobiles
0%

10%

20%
Proportion of characteristic in sample

34

10%

20%

Figure B.5: Male v. Female proportions by sample

Female

Male

Full sample
Browsers
Web Apps
Mobiles
Non−Mobiles
45%

50%

55%
45%
Proportion of characteristic in sample

50%

55%

Figure B.6: Race/Ethnicity proportions by sample

AAPI

Black

Hispanic

White

Other

0%

Full sample
Browsers
Web Apps
Mobiles
Non−Mobiles

Full sample
Browsers
Web Apps
Mobiles
Non−Mobiles
20%

Full sample
Browsers
Web Apps
Mobiles
Non−Mobiles
0%

20%

40%

60%
80%
Proportion of characteristic in sample

35

40%

60%

80%

Figure B.7: Partisanship proportions by sample

Democrat
Full sample
Browsers
Web Apps
Mobiles
Non−Mobiles

Republican
Full sample
Browsers
Web Apps
Mobiles
Non−Mobiles

Independent
Full sample
Browsers
Web Apps
Mobiles
Non−Mobiles
10%

20%

30%
Proportion of characteristic in sample

36

40%

Figure B.8: Voting behavior in 2016 proportions by sample

Clinton

Trump

No vote

Other vote

Full sample
Browsers
Web Apps
Mobiles
Non−Mobiles

Full sample
Browsers
Web Apps
Mobiles
Non−Mobiles
0%

10%

20%

30%
40%
0%
10%
20%
Proportion of characteristic in sample

37

30%

40%

C

Treatment descriptions

Figure C.1: E↵ect of framing on decision making: cheap condition (original)

Figure C.2: E↵ect of framing on decision making: expensive condition (original)

Figure C.3: E↵ect of framing on decision making: cheap condition (modified)

Figure C.4: E↵ect of framing on decision making: expensive condition (modified)

38

Figure C.5: Perceived intentionality for side e↵ects: helped condition (original)

39

Figure C.6: Perceived intentionality for side e↵ects: harmed condition (original)

40

Figure C.7: Perceived intentionality for side e↵ects: helped condition (modified)

41

Figure C.8: Perceived intentionality for side e↵ects: harmed condition (modified)

42

C.1

Attention Check Questions

Figure C.9: Pre-ACQ article for “‘Easy” and “Medium” ACQ

43

Figure C.10: “Easy” and “Medium” ACQ with correct responses highlighted

Figure C.11: “Hard” ACQ with correct response highlighted

44

References
Aronow, Peter M, Jonathon Baron and Lauren Pinson. 2019. “A note on dropping experimental subjects who fail a manipulation check.” Political Analysis 27(4):572–589.
Benjamini, Yoav and Yosef Hochberg. 1995. “Controlling the false discovery rate: a practical
and powerful approach to multiple testing.” Journal of the Royal statistical society: series
B (Methodological) 57(1):289–300.
Berinsky, Adam J, Gregory A Huber and Gabriel S Lenz. 2012. “Evaluating online labor
markets for experimental research: Amazon.com’s Mechanical Turk.” Political analysis
20(3):351–368.
Berinsky, Adam J, Michele F Margolis and Michael W Sances. 2014. “Separating the shirkers
from the workers? Making sure respondents pay attention on self-administered surveys.”
American Journal of Political Science 58(3):739–753.
Cli↵ord, Scott, Geo↵rey Sheagley and Spencer Piston. 2020. “Increasing Precision in Survey
Experiments Without Introducing Bias.” Unpublished Manuscript .
Coppock, Alexander and Oliver A McClellan. 2019. “Validating the demographic, political, psychological, and experimental results obtained from a new source of online survey
respondents.” Research & Politics 6(1):2053168018822174.
Druckman, James N. 2001. “Using credible advice to overcome framing e↵ects.” Journal of
Law, Economics, and Organization 17(1):62–82.
Gilens, Martin. 2001. “Political ignorance and collective policy preferences.” American Political Science Review pp. 379–396.
Glass, Gene V. 1976. “Primary, secondary, and meta-analysis of research.” Educational
researcher 5(10):3–8.
45

Hainmueller, Jens and Daniel J Hopkins. 2015. “The hidden American immigration consensus: A conjoint analysis of attitudes toward immigrants.” American Journal of Political
Science 59(3):529–548.
Huber, Gregory A and Celia Paris. 2013. “Assessing the programmatic equivalence assumption in question wording experiments: Understanding why Americans like assistance to
the poor more than welfare.” Public Opinion Quarterly 77(1):385–397.
Hyman, Herbert H and Paul B Sheatsley. 1950. “The Current Status of American public
opinion.” The Teaching of Contemporary A↵airs pp. 11–34.
Jost, John T and Mahzarin R Banaji. 1994. “The role of stereotyping in system-justification
and the production of false consciousness.” British journal of social psychology 33(1):1–27.
Klein, Richard A, Kate A Ratli↵, Michelangelo Vianello, Reginald B Adams Jr, Štěpán
Bahnı́k, Michael J Bernstein, Konrad Bocian, Mark J Brandt, Beach Brooks, Claudia Chloe Brumbaugh et al. 2014. “Investigating variation in replicability.” Social psychology .
Klein, Richard A, Michelangelo Vianello, Fred Hasselman, Byron G Adams, Reginald B
Adams Jr, Sinan Alper, Mark Aveyard, Jordan R Axt, Mayowa T Babalola, Štěpán Bahnı́k
et al. 2018. “Many Labs 2: Investigating variation in replicability across samples and
settings.” Advances in Methods and Practices in Psychological Science 1(4):443–490.
Knobe, Joshua. 2003. “Intentional action and side e↵ects in ordinary language.” Analysis
63(3):190–194.
Peyton, Kyle. 2020.

“Does Trust in Government Increase Support for Redistribution?

Evidence from Randomized Survey Experiments.” American Political Science Review
114(2):596–602.
46

Porter, Ethan, Thomas J Wood and David Kirby. 2018. “Sex trafficking, Russian infiltration,
birth certificates, and pedophilia: A survey experiment correcting fake news.” Journal of
Experimental Political Science 5(2):159–164.
Press, Daryl G, Scott D Sagan and Benjamin A Valentino. 2013. “Atomic aversion: Experimental evidence on taboos, traditions, and the non-use of nuclear weapons.” American
Political Science Review pp. 188–206.
Schuman, Howard and Stanley Presser. 1996. Questions and answers in attitude surveys:
Experiments on question form, wording, and context. Sage.
Smith, Tom W. 1987. “That which we call welfare by any other name would smell sweeter an
analysis of the impact of question wording on response patterns.” Public opinion quarterly
51(1):75–83.
Trump, Kris-Stella and Ariel White. 2018. “Does inequality beget inequality? Experimental
tests of the prediction that inequality increases system justification motivation.” Journal
of Experimental Political Science 5(3):206–216.
Tversky, Amos and Daniel Kahneman. 1981. “The framing of decisions and the psychology
of choice.” science 211(4481):453–458.

47

