arXiv:1903.04663v1 [math.PR] 11 Mar 2019

Calibrating dependence between random
elements
Abram M. Kagan∗
Dept. of Mathematics, University of Maryland, College Park, MD 20742, USA
E-mail: amk@math.umd.edu
Gabor J. Székely
National Science Foundation, Alexandria, VA 22314 and
Rényi Institute of Mathematics, Hungarian Acad. of Sciences
E-mail:gszekely@nsf.gov

Abstract
Attempts to quantify dependence between random elements X and Y via maximal correlation go back to Gebelein (1941) and Rényi (1959). After summarizing properties (including some new) of the Rényi measure of dependence, a
calibrated scale of dependence is introduced. It is based on the “complexity“
of approximating functions of X by functions of Y.
Keywords: Quantification, measures of dependence, projection
MSC 2010 classifications: Primary 60H99; secondary 62E10

∗

Corresponding author

1

1

Introduction

Let X, Y be a pair of random elements taking values in a general probability space
{X × Y, A ⊗ B, P } where A and B are σ-algebras of subsets of X and B, respectively,
and P is a probability measure on A ⊗ B.
The study of dependence of X from Y attracted much attention; see, e. g., Székely,
Rizzo and Bakirov (2007), Székely and Rizzo (2009), Reshef et al.(2011), Reimherr
and Nicolae (2013). In Rényi (1959) natural (“necessary”) requirements to measures
of dependence were stated. Essential features of dependence of X from Y are contained in properties of the conditional expectation E{ϕ(X)|Y } defined for all ϕ(X)
with E{|ϕ(X)|} < ∞. If one reduces the domain to ϕ(X) with E{|ϕ(X)|2} < ∞,
the conditional expectation becomes the projection of the subspace L2 (X) into
the subspace of LR2 (YR ) of L2 (X, Y ), the Hilbert space of functions φ(X, Y ) with
E{|φ(X, Y )|2 } = X Y |φ(x, y)|2dP (x, y) < ∞ and the standard inner product
(φ1 , φ2 ) = E{φ1 (X, Y )φ2 (X, Y )}.

According to Rényi (1959),in the classical monograph Kolmogorov (1933) introduced
the following ratios of variances to measure dependence between X and Y :
var E{ϕ(X)|Y }/var ϕ(X)
Its supremum
D(X : Y ) = sup var E{ϕ(X)|Y }

(1)

||ϕ||=1

possesses many properties required from a measure of dependence. These properties
are summarized in the following.
Theorem 1.

(i) 0 ≤ D(X : Y ) ≤ 1.

(ii) D(X : Y ) = 0 iff X and Y are independent.
(iii) D(X : Y ) = 1 if X = h(Y ) (but not iff ).
(iv) D(X : Y ) is monotone in the following sense: for any random element
Z, D(X : (Y, Z)) ≥ D(X : Y ).
(v) If Z is independent of the pair (X, Y ), then D(X : (Y, Z)) = D(X : Y ).
(vi) For a bivariate Gaussian vector (X, Y ) with corr(X, Y ) = ρ, D(X : Y ) = |ρ|.
(vii) For any bivariate vector (X, Y ) with corr(X, Y ) = ρ, D(X : Y ) ≥ |ρ|.
2

(viii) For an (m + n)-dimensional Gaussian random vector (X, Y ) with variancecovariance matrix


V11 V12
V21 V22 ,
assuming the submatrices V11 = var(X), V22 = var(Y ) non-singular,
p
D(X : Y ) = λ(Σ),
−1/2

where λ(Σ) is the maximal eigenvalue of the matrix Σ = V11

−1/2

V12 V22−1 V21 V11

.

(ix) For an arbitrary (m + n)-dimensional vector (X, Y ) with the above variancecovariance matrix
p
D(X : Y ) ≥ λ(Σ).
(x) If a self-decomposable Z is independent of (X, Y ), then as a function of λ
D(X : Y + λZ) increases on (−∞. 0) and decreases on (0, ∞).
(xi)
D(X : Y ) = {R(X, Y )}2
where R(X, Y ) = sup ρ(ϕ(X), φ(Y ), ρ is the classical (Pearson) correlation
coefficient and the supremum is taken over all ϕ(X), φ(Y ) with 0 < varϕ(X) <
∞,
0 < varφ(Y ) < ∞.
(xii) D(X : Y ) = D(Y : X).
Proofs. Properties (i)-(iv) and (vi)-(ix) are well known, (v) follows from that if
Z is independent of (X, Y ) then for any ϕ(X) with finite expectation
E{ϕ(X)|Y, Z} = E{ϕ(X)|Y }
(see, e. g., Meyer (1966), Theorem 61). Actually, (v) holds if X and Z are conditionally independent given Y which is a weaker condition than independence of Z
and (X, Y ).
To prove (x), recall that a random variable Z is called self-decomposable if for any
c, 0 < c < 1 there is an independent of Z random variable Zc such that Z is equidistributed with cZ + Zc , Z ∼
= cZ + Zc . All random variables having stable distributions
are self-decomposable. See Lukacs (1970), Ch. 5 for properties of self-decomposable
random variables.
3

Let now λ1 > λ2 > 0, λ2 = cλ1 for some 0 < c < 1 and let Zc be an independent of
X, Y, Z random variable such that Z ∼
= cZ + Zc . Then
D(X : Y + λ1 Z) = D(X : Y + λ1 (cZ + Zc )) = D(X : Y + λ2 Z + Zc ) ≤ D(X : (Y + λ2 Z, Zc))
= D(X : Y + λ2 Z)
due to monotonicity (iv) and (v).
Property (xi) follows from ρ(ϕ(X), φ(Y )) = ρ(E{ϕ(X)|Y }, φ(Y )) and the CauchySchwarz inequality
|ρ(ϕ(X), φ(Y ))|2 ≤ varφ(Y )varE{ϕ(X)|Y }
with the equality sign holding for φ(Y ) = cE{ϕ(X)|Y }.
Some special forms of dependence are worth studying. One example may be X, Y
such that
E{ϕ(X)|Y } = const ⇒ P {ϕ(X) = const} = 1.
(2)
If p(x|y) is the conditional probability density function of X given Y = y, (2) is
what in statistics is known as completeness of the family {p(x|y), y ∈ Y} with y as
a family parameter.
Plainly, if X is fully dependent of Y , i. e.,X = h(Y ), the relation (2) holds.
For independent X, Y, E{ϕ(X)|Y } = const for any ϕ(X) with finite expectation, so
that (2) moves X, Y far away from independence.

2

The main result

Here a family Dm (X : Y ), m = 0, 1, 2, . . . of measures of dependence is defined that
generalizes D(X : Y ) in the following direction.
A random element X is called m-dependent of Y if for any ϕ(X) ∈ L2 (X), the conditional expectation E{ϕ(X)|Y } = φ(Y ) belongs to an (m + 1)-dimensional subspace
of L2 (Y ). It is proved that X is m-dependent of Y if and only if Dm (X : Y ) = 0.
For m = 0,
D0 (X : Y ) = D(X : Y ) so that 0-dependence means independence when for any
ϕ(X), E{ϕ(X)|Y } belongs to the one-dimensional subspace of constants.
Denote by V (ϕ0 , . . . , ϕm ) the covariance matrix of the (m + 1)-dimensional vector
(ϕ0 (X), . . . , ϕm (X)) of random variables in L2 (X) and by |V | the determinant of
V (sometimes referred to as the generalized variance of the vector with covariance
4

matrix V .
For ϕ(X) ∈ L2 (X) set φ(Y ) = E{ϕ(X)|Y }.
Definition 1. The index of m-dependence of X from Y is defined as
Dm (X : Y ) = sup |V (φ0 (Y ), φ1 (Y ), . . . , φm (Y ))|,

(3)

where the supremum is taken over all ϕ0 (X), ϕ1 (X), . . . , ϕm (X) with
V (ϕ0 , ϕ1 , . . . , ϕm ) = Idm+1 , the identity matrix of order m + 1.
The matrix V (φ0 (Y ), φ1 (Y ), . . . , φm (Y )) is non-negative definite and, as well
known,
V (φ0 (Y ), φ1 (Y ), . . . , φm (Y )) ≤ V (ϕ0 , ϕ1 , . . . , ϕm ),
so that 0 ≤ Dm (X : Y ) ≤ 1.
Plainly, Dm (X : Y ) is monotone in the same sense as D(X : Y ) (see (iv)).
Since D0 (X : Y ) = supϕ:varϕ(X)=1 varE{ϕ(X)|Y }, D0 (X : Y ) = 0 if and only if for
any ϕ ∈ L2 (X), E(ϕ|Y ) belongs to the one-dimensional subspace of constants in
L2 (Y ). A similar property of Dm (X : Y ) is proved in the next theorem.
Theorem 2. Dm (X : Y ) = 0 if and only if for any ϕ ∈ L2 (X), E(ϕ|Y ) belongs to
a subspace of L2 (Y ) of dimension ≤ m + 1.
Proof. First notice that the image of ϕ ≡ 1 ∈ L2 (X) is φ ≡ 1 ∈ L2 (Y ). For any
ϕ ∈ L2 (X) with E(ϕ(X)) = 0 its image E(ϕ|Y ) = φ(Y ) also has E(φ(Y )) = 0. We
shall show that Dm (X : Y ) = 0 if and only if for any ϕ ∈ L2 (X), with E(ϕ) = 0 its
image E(ϕ|Y ) belongs to a subspace of L2 (Y ) of dimension ≤ m.
Now take elements ϕ1 , . . . , ϕm+1 in L2 (X) with zero expectations and the identity
covariance matrix (that is why ϕ ≡ 1 is treated separately).
Due to Dm (X : Y ) = 0, the covariance matrix V (φ1 , . . . , φm+1 ) of their images
φ1 (Y ) = E(ϕ1 |Y ), . . . , φm+1 (Y ) = E(ϕm+1 |Y ) is degenerate and thus
c1 φ0 + . . . + cm+1 φm = 0
with probability 1 for some constants c1 , . . . , cm+1 .
Assuming without loss in generality φ1 , . . . , φm linearly independent and thus cm+1 6=
0, Dm (X : Y ) = 0 implies that for the chosen ϕ1 , . . . , ϕm+1 their images belong to the
m-dimensional subspace span{φ1 , . . . , φm }. If ϕ ∈ span{ϕ1 , . . . , ϕm+1 }, then plainly
E(ϕ|Y ) = φ ∈ span{φ1 , . . . , φm }.
Let now ϕ ∈ L2 (X) belong to the orthocomplement of span{ϕ1 , . . . , ϕm+1 }, ϕ ∈
5

(span{ϕ1 , . . . , ϕm+1 })⊥ . Assuming varϕ(X) = 1, consider the vector (ϕ1 , . . . , ϕm , ϕ).
The covariance matrix V (φ1 , . . . , φm , φ) where φ = E(ϕ|Y ), is degenerate and for
some constants c′1 , . . . , c′m+1
c′1 φ1 + . . . + c′m φm + c′m+1 φm
so that φ ∈ span{φ1 , . . . , φm }.
Since any ϕ ∈ L2 (X) can be represented as
ϕ = ϕ′ + ϕ′′ , ϕ′ ∈ span{φ1 , . . . , φm+1 }, ϕ′′ ∈ (span{φ1 , . . . , φm+1 })⊥ ,
the above proves the sufficiency part of Theorem 2.
To prove necessity, take any set ϕ1 , . . . , ϕm+1 of elements of L2 (Y ) with zero expectations and identity covariance matrix. If their images
φ1 = E(ϕ1 |Y ), . . . , φm+1 = E(ϕm+1 |Y )
belong to an m-dimensional subspace of L2 (Y ) and thus are linearly dependent, their
covariance matrix is degenerate and |V (φ1 , . . . , φm+1 )| = 0.
The images of constants form an one-dimensional subspace of constants and for any
ϕ(X), ϕ = E(ϕ) + [ϕ − E(ϕ)] proving the necessity part of Theorem 2.
A trivial corollary is Dm (X : Y ) = 0 ⇒ Dm+1 (X : Y ) = 0.
Starting with the class C0 of independent X, Y , the indices Dm lead to a scale of
classes Cm ,
C0 ⊃ C1 ⊃ C2 ⊃ . . . ,
where (X, Y ) ∈ Cm ⇔ Dm (X : Y ) = 0.
The classes Cm are non-empty. If the conditional probability density function p(x|y)
of X given Y has a form of
p(x|y) = p0 (x) + p1 (x)q1 (y) + . . . + pm (x)qm (y),
with q1 ∈ L2 (Y ), . . . , qm ∈ L2 (Y ), then
E(ϕ(X)|Y ) =

Z

ϕ(x)p(x|Y )dx

belongs to the (m + 1)-dimensional subspace span {1, q1 (Y ), . . . , qm (Y )} of L2 (Y ),
so that (X, Y ) ∈ Cm .
Of some interest may be an approach that starts not at independent X, Y but at the
other end when X and Y are arbitrarily dependent. Let us call X m-codependent
of Y if for any ϕ ∈ L2 (X), E(ϕ|Y ) belongs to a subspace of L2 (Y ) of codimension
m. As m increases, X and Y become less and less dependent, in a sense. How to
quantify the codependence?
6

References
Gebelein, H. (1941). Das statistische Problem der Korrelation als Variations- und
Eigenwert-problem und sein Zusammenhang mit der Ausgleichungsrechnung. Z.
Angew. Math. Mech., 21, 364–379.
Kolmogoroff, A. N. (1933). Grundbegriffe der Wahrscheinlichkeitsrechnung, Springer,
Berlin.
Lukacs, E. (1970).Characteristic Functions, Griffin, London.
Meyer, P. A. (1966) Probability and Potentials, Blaisdel.
Reimherr, R., and Nicolae, D. L. (2013). On quantifying dependence: a framework for developing interpretable measures. Statistical Science, 28, 116-130.
Rényi, A. (1959). On measures of dependence. Acta Math. Acad. Sci. , 10, 441-451.
Reshef, D. N., Reshef, Y. A., Finucane, H. K., Grossman, S. R., McVean, G., Turnbaugh, P. J., Lander, E. S., Mitzenmacher, M., Sabeti, P. C. (2011). Detecting novel
associations in large data sets. Science, 334, 1518-1524.
Székely, G. J., Rizzo, M. L., and Bakirov, N. K. (2007).Measuring and testing dependence by correlation of distances. Ann. Statist., 35,2769-2794.
Székely, G. J., and Rizzo, M. L. (2009). Brownian distance covariance. Ann. Appl.
Stat.,3, 1236-1265.

7

