RSN: Randomized Subspace Newton

arXiv:1905.10874v2 [math.OC] 3 Oct 2019

Robert M. Gower
LTCI, Télécom Paristech, IPP, France
gowerrobert@gmail.com
Felix Lieder
Heinrich-Heine-Universität Düsseldorf, Germany
lieder@opt.uni-duesseldorf.de

Dmitry Kovalev
KAUST, Saudi Arabia
dmitry.kovalev@kaust.edu.sa
Peter Richtárik
KAUST, Saudi Arabia and MIPT, Russia
peter.richtarik@kaust.edu.sa

Abstract
We develop a randomized Newton method capable of solving learning problems
with huge dimensional feature spaces, which is a common setting in applications
such as medical imaging, genomics and seismology. Our method leverages randomized sketching in a new way, by finding the Newton direction constrained to
the space spanned by a random sketch. We develop a simple global linear convergence theory that holds for practically all sketching techniques, which gives
the practitioners the freedom to design custom sketching approaches suitable for
particular applications. We perform numerical experiments which demonstrate
the efficiency of our method as compared to accelerated gradient descent and the
full Newton method. Our method can be seen as a refinement and randomized
extension of the results of Karimireddy, Stich, and Jaggi [18].

1

Introduction

In this paper we are interested in unconstrained optimization problems of the form
min f (x),

x∈Rd

(1)

where f : Rd → R is a sufficiently well behaved function, in the large dimensional setting, i.e.,
when d is very large. Large dimensional optimization problems are becoming ever more common
in applications. Indeed, d often stands for the dimensionality of captured data, and due to fast-paced
advances in technology, this only keeps growing. One of key driving forces behind this is the rapid
increase in the resolution of sensors used in medicine [19], genomics [26, 8], seismology [2] and
weather forecasting [1]. To make predictions using such high dimensional data, typically one needs
to solve an optimization problem such as (1). The traditional off-the-shelf solvers for such problems
are based on Newton’s method, but in this large dimensional setting they cannot be applied due to
the high memory footprint and computational costs of solving the Newton system. We offer a new
solution to this, by iteratively performing Newton steps in random subspaces of sufficiently low dimensions. The resulting randomized Newton’s method need only solve small randomly compressed
Newton systems and can be applied to solving (1) no matter how big the dimension d.
1.1

Background and contributions

Newton’s method dates back to even before Newton, making an earlier appearance in the work of the
Persian astronomer and mathematician al-Kashi 1427 in his “Key to Arithmetic” [33]. In the 80’s
Newton’s method became the workhorse of nonlinear optimization methods such as trust region [9],
augmented Lagrangian [4] and interior point methods. The research into interior point methods
Preprint. Under review.

culminated with Nesterov and Nemirovskii’s [22] ground breaking work proving that minimizing
a convex (self-concordant) function could be done in a polynomial number of steps, where in each
step a Newton system was solved.
Amongst the properties that make Newton type methods so attractive is that they are invariant to
rescaling and coordinate transformations. This property makes them particularly appealing for offthe-shelf solvers since they work well independently of how the user chooses to scale or represent
the variables. This in turn means that Newton based methods need little or no tuning of hyperparameters. This is in contrast with first-order methods1 , where even rescaling the function can result in
a significantly different sequence of iterates, and their efficient execution relies on parameter tuning
(typically the stepsize).
Despite these advantages, Newton based solvers are now facing a challenge that renders most of
them inapplicable: large dimensional feature spaces. Indeed, solving a generic Newton system costs
O(d3 ). While inexact Newton methods [11, 5] made significant headway to diminishing this high
cost by relying on Krylov based solvers whose iterations cost O(d2 ), this too can be prohibitive, and
this is why first order methods such as accelerated gradient descent [24] are often used in the large
dimensional setting.
In this work we develop a family of randomized Newton methods which work by leveraging randomized sketching and projecting [16]. The resulting randomized Newton method has a global linear
convergence for virtually any type and size of sketching matrix. In particular, one can choose a
sketch of size one, which yields a low iteration complexity of as little as O(1) if one assumes that
scalar derivatives can be computed in constant time. Our main assumptions are the recently introduced [18] relative smoothness and convexity2 of f , which are in a certain sense weaker than the
more common strong convexity and smoothness assumptions. Our method is also scale invariant,
which facilitates setting the stepsize. We further propose an efficient line search strategy that does
not increase the iteration complexity.
There are only a handful of Newton type methods in the literature that use iterative sketching, including the sketched Newton algorithm [28], SDNA (Stochastic Dual Newton Ascent) [29], RBCN
(Randomized Block Cubic Newton) [12] and SON [21]. In the unconstrained case the sketched
Newton algorithm [28] requires a sketching matrix that is proportional to the global rank of the Hessian, an unknown constant related to high probability statements and −2 , where  > 0 is the desired
tolerance. Consequently, the required sketch size could be as large as d, which defeats the purpose.
The SDNA algorithm in [29] relies on the existence of a positive definite matrix M ∈ Rd×d that
globally upper bounds the Hessian, which is a stronger assumption than our relative smoothness
assumption. The method then proceeds by selecting random principal submatrices of M that it then
uses to form and solve an approximate Newton system. The theory in [29] allows for any sketch
size, including size of one. Our method could be seen as an extension of SDNA to allow for any
sketch, one that is directly applied to the Hessian (as opposed to M) and one that relies on a set
of more relaxed assumptions. The RBCN method combines the ideas of randomized coordinate
descent [23] and cubic regularization [25]. The method requires the optimization problem to be
block separable and is hence not applicable to the problem we consider here. Finally, SON [21]
uses random and deterministic streaming sketches to scale up a second-order method, akin to a
Gauss–Newton method, for solving online learning problems.
1.2

Key Assumptions

We assume throughout that f : Rd → R is a convex and twice differentiable function. Further, we
assume that f is bounded below and the set of minimizers X∗ nonempty. We denote the optimal
value of (1) by f∗ ∈ R.
Let H(x) := ∇2 f (x) (resp. g(x) = ∇f (x)) be the Hessian (resp. gradient) of f at x. We fix an
initial iterate x0 ∈ Rd throughout and define Q to be a level set of function f (x) associated with x0 :

Q := x ∈ Rd : f (x) ≤ f (x0 ) .
(2)
Let hx, yiH(xk ) := hH(xk )x, yi for all x, y ∈ Rd . Our main assumption on f is given next.
1
2

An exception to this is, for instance, the optimal first order affine-invariant method in [10].
These notions are different from the relative smoothness and convexity concepts considered in [20].

2

Assumption 1. There exist constants L̂ ≥ µ̂ > 0 such that for all x, y ∈ Q:
L̂
f (x) ≤ f (y) + hg(y), x − yi + kx − yk2H(y) ,
2
{z
}
|

(3)

:=T (x,y)

µ̂
kx − yk2H(y) .
2
We refer to L̂ and µ̂ as the relative smoothness and relative convexity constant, respectively.
f (x) ≥ f (y) + hg(y), x − yi +

(4)

Relative smoothness and convexity is a direct consequence of smoothness and strong convexity. It
is also a consequence of the recently introduced [18] c–stability condition, which served to us as an
inspiration. Specifically, as shown in Lemma 2 in [18] and also formally (for convenience) stated in
Proposition 2 in the supplementary material, we have that
L–smooth + µ–strongly convex

⇒

c–stability

⇒

relative smoothness & relative convexity.

We will also further assume:
Assumption 2. g(x) ∈ Range (H(x)) for all x ∈ Rd .
Assumption 2 holds if the Hessian is positive definite for all x, and for generalized linear models.
1.3

The full Newton method

Our baseline method for solving (1), is the following variant of the Newton Method (NM):
xk+1 = xk + γn(xk ) := xk − γH† (xk )g(xk ),

(5)

where H† (xk ) is the Moore-Penrose pseudoinverse of H(xk ) and n(xk ) := −H† (xk )g(xk ) is the
Newton direction. A property (which we recall from [18]) that will be important for our analysis is
that for a suitable stepsize, Newton’s method is a descent method.
Lemma 1. Consider the iterates {xk }k≥0 defined recursively by (5). If γ ≤ 1/L̂ and (3) holds, then
f (xk+1 ) ≤ f (xk ) for all k ≥ 0, and in particular, xk ∈ Q for all k ≥ 0.
The proof follows by using (3), twice differentiability and convexity of f . See [18, Lemma 3].
The relative smoothness assumption (3) is particularily important for motivating Newton’s method.
Indeed, a Newton step is the exact minimizer of the upper bound in (3).
Lemma 2. If Assumption 2 is satisfied, then the quadratic x 7→ T (x, xk ) defined in (3) has a global
minimizer xk+1 given by xk+1 = xk − L̂1 H† (xk )g(xk ) ∈ Q.
Proof. Lemma 1 implies that xk+1 ∈ Q, and Lemma 9 in the appendix shows that (5) is a global
minimizer for γ = 1/L̂.

2

Randomized Subspace Newton

Solving a Newton system exactly is costly and may be a waste of resources. Indeed, this is the reason
for the existence of inexact variants of Newton methods [11]. For these inexact Newton methods, an
accurate solution is only needed when close to the optimal point.
In this work we introduce a different inexactness idea: we propose to solve an exact Newton system,
but in an inexact randomly selected subspace. In other words, we propose a randomized subspace
Newton method, where the randomness is introduced via sketching matrices, defined next.
Definition 1. Let D be a (discrete or continuous) distribution over matrices in Rd×s . We say that
S ∼ D is a random sketching matrix and s ∈ N is the sketch size.
We will often assume that the random sketching is nullspace preserving.
Assumption 3. We say that S ∼ D is nullspace preserving if with probability one we have that

Null S> H(x)S = Null(S),
∀x ∈ Q.
(6)
3

By sampling a sketching matrix Sk ∼ D in the kth iteration, we can form a sketched Newton
s×s
direction using only the sketched Hessian S>
; see line 5 in Algorithm 1. Note
k H(xk )Sk ∈ R
that the sketched Hessian is the result of twice differentiating the function λ 7→ f (xk + Sk λ), which
can be done efficiently using a single backpropation pass [14] or s backpropagation passes [7] which
costs at most s times the cost of evaluating the function f .
Algorithm 1 RSN: Randomized Subspace Newton
1: input: x0 ∈ Rd
2: parameters: D = distribution over random matrices
3: for k = 0, 1, 2, . . . do
4:
sample a fresh sketching matrix: Sk ∼ D
5:

xk+1 = xk −

1
S
L̂ k

S>
k H(xk )Sk

†

S>
k g(xk )

6: output: last iterate xk

First we show that much like the full Newton method (5), Algorithm 1 is a descent method.
Lemma 3 (Descent). Consider the iterates xk given Algorithm 1. If Assumptions 1, 2 and 3 hold,
then f (xk+1 ) ≤ f (xk ) and consequently xk ∈ Q for all k ≥ 0.
While common in the literature of randomized coordinate (subspace) descent method, this is a rare
result for randomized stochastic gradient descent methods, which do not enjoy a descent property.
Lemma 3 is useful in monitoring the progress of the method in cases when function evaluations are
not too prohibitive. However, we use it solely for establishing a tighter convergence theory.
Interestingly, the iterations of Algorithm 1 can be equivalently formulated as a random projection of
the full Newton step, as we detail next.
Lemma 4. Let Assumptions 1 and 2 hold. Consider the projection matrix Pk with respect to the
2
seminorm k·kH(xk ) := h·, ·iH(xk ) given by
Pk := Sk S>
k H(xk )Sk

†

d×d
S>
.
k H(xk ) ∈ R

(7)

The iterates of Algorithm 1 can be viewed as a projection of the Newton step given by
xk+1 = xk +

1
L̂

Pk n(xk ) .

(8)

Proof. To verify that Pk is an oblique projection matrix, it suffices to check that
hPk x, Pk yiH(xk ) = hPk x, yiH(xk ) ,

∀x, y ∈ Rd ,

which in turn relies on the identity M† MM† = M† , which holds for all matrices M ∈ Rd×d . Since
g(xk ) ∈ Range (H(xk )) , we have again by the same identity of the pseudoinverse that
g(xk ) = H(xk )H† (xk )g(xk ) = −H(xk )n(xk ).
† >
Consequently, Pk n(xk ) = Sk S>
Sk g(xk ).
k H(xk )Sk

(9)

We will refer to Pk n(xk ) as the sketched Newton direction. If we add one more simple assumption
to the selection of the sketching matrices, we have the following equivalent formulations of the
sketched Newton direction.
Lemma 5. Let Assumptions 1, 2 and 3 hold. It follows that the xk+1 iterate of Algorithm 1 can be
equivalently seen as
1. The minimizer of T (x, xk ) over the random subspace x ∈ xk + Range (Sk ) :
xk+1 = xk + Sk λk ,

where λk ∈ arg mins T (xk + Sk λ, xk ) .

Furthermore,
T (xk+1 , xk ) = f (xk ) −

λ∈R

1

2

2L̂
4

kg(xk )kSk (S> H(xk )Sk )† Sk .
k

(10)

(11)

2. A projection of the Newton direction onto a random subspace:
xk+1

=

arg

x − xk −

min

x∈Rd ,

λ∈Rs

1
L̂


n(xk )

2
H(xk )

subject to x = xk + Sk λ. (12)

3. A projection of the previous iterate onto the sketched Newton system given by:
xk+1

2

∈ arg min kx − xk kH(xk )

1 >
subject to S>
k H(xk )(x − xk ) = − Sk g(xk ). (13)
L̂

Furthermore, if Range (Sk ) ⊂ Range (Hk (xk )), then xk+1 is the unique solution to the above.

3

Convergence Theory

We now present two main convergence theorems.
h
† i
Theorem 2. Let G(x) := ES∼D S S> H(x)S S and define
ρ(x) :=

H1/2 (x)G(x)H1/2 (x)v, v

min

2

kvk2

v∈Range(H(x))

and

ρ := min ρ(x).
x∈Q

(14)

If Assumptions 1 and 2 hold, then
E [f (xk )] − f∗ ≤


k
µ̂
1−ρ
(f (x0 ) − f∗ ).
L̂

Consequently, given  > 0, if ρ > 0 and if


f (x0 ) − f∗
1 L̂
log
k≥
,
ρ µ̂


then

E [f (xk ) − f∗ ] < .

(15)

(16)

Theorem 2 includes the convergence of the full Newton method as a special case. Indeed, when
we choose3 Sk = I ∈ Rd×d , it is not hard to show that ρ(xk ) ≡ 1, and thus (16) recovers the
L̂/µ̂ log (1/) complexity given in [18]. We provide yet an additional sublinear O(1/k) convergence
result that holds even when µ̂ = 0.
Theorem 3. Let Assumption 2 hold and Assumption 1 be satisfied with L̂ > µ̂ = 0. If
R := inf sup kx − x∗ kH(x) < +∞ ,
x∗ ∈X∗ x∈Q

and ρ > 0 then E [f (xk )] − f∗ ≤

(17)

2L̂R2
.
ρk

As a new result of Theorem 3, we can also show that the full Newton method has a O(L̂R−1 )
iteration complexity.
Both of the above theorems rely on ρ > 0. So in the next Section 3.1 we give sufficient conditions
for ρ > 0 that holds for virtually all sketching matrices.
The sketched condition number ρ(xk )

3.1

The parameters ρ(xk ) and ρ in Theorem 2 characterize the trade-off between the cost of the iterations
and the convergence rate of RSN. Here we show that ρ is always bounded between one and zero, and
further, we give conditions under which ρ(xk ) is the smallest non-zero eigenvalue of an expected
projection matrix, and is thus bounded away from zero.
Lemma 6. The parameter ρ(xk ) appearing in Theorem 2 satisfies 0 ≤ ρ(xk ) ≤ 1. Letting
† > 1/2
P̂(xk ) := H1/2 (xk )Sk S>
Sk H (xk ) ,
(18)
k H(xk )Sk
3

Or when Sk is an invertible matrix.

5

and if we assume that the exactness4 condition

h
i
Range (H(xk )) = Range ES∼D P̂(xk )

h
i
holds then ρ(xk ) = λ+
> 0.
min ES∼D P̂(xk )

(19)

Since (19) is in general hard to verify, we give simpler sufficient conditions for ρ > 0 in the next
lemma.
Lemma 7 (Sufficient condition for exactness). If Assumption 3 and

Range (H(xk )) ⊂ Range E[Sk S>
(20)
k] ,
holds then (19) holds and consequently 0 < ρ ≤ 1.


Clearly, condition (20) is immediately satisfied if E Sk S>
k is invertible, and this is the case for
Gaussian sketches, weighted coordinate sketched, sub-sampled Hadamard or Fourier transforms,
and the entire class of randomized orthonormal system sketches [27].
3.2

The relative smoothness and strong convexity constants

In the next lemma we give an insightful formula for calculating the relative smoothness and convexity constants defined in Assumption 1, and in particular, show how L̂ and µ̂ depend on the relative
change of the Hessian.
Lemma 8. Let f be twice differentiable, satisfying Assumption 1. If moreover H(x) is invertible for
every x ∈ Rd , then
Z 1
kx − yk2H(x)
kzt − yk2H(zt )
:= c
dt
≤
max
(21)
L̂ =
max
2(1 − t)
x,y∈Q kx − yk2
kzt − yk2H(y)
x, y ∈ Q
t=0
H(y)
Z 1
kzt − yk2H(zt )
1
µ̂ =
min
2(1 − t)
dt ≥ ,
(22)
2
kzt − ykH(y)
c
x, y ∈ Q
t=0
where zt := y + t(x − y).
The constant c on the right hand side of (21) is known as the c-stability constant [18]. As a byproduct, the above lemma establishes that the rates for the deterministic Newton method obtained as
a special case of our general theorems are at least as good as those obtained in [18] using c-stability.

4

Examples

With the freedom of choosing the sketch size, we can consider the extreme case s = 1, i.e., the case
with the sketching matrices having only a single column.
Corollary 1 (Single column sketches). Let 0 ≺ U ∈ Rn×n be a symmetric positive definite matrix
such that H(x)  U, ∀x ∈ Rd . Let D = [d1 , . . . , dn ] ∈ Rn×n be a given invertible matrix such
that d>
i H(x)di 6= 0 for all x ∈ Q and i = 1, . . . , n. If we sample according to
P[Sk = di ] = pi :=

d>
i Udi
,
Trace (D> UD)

then the update on line 5 of Algorithm 1 is given by
xk+1 = xk −

1 d>
i g(xk )
di ,
L̂ d>
i H(xk )di

with probability pi ,

and under the assumptions of Theorem 2, Algorithm 1 converges according to

k
λ+ (H1/2 (x)DD> H1/2 (x)) µ̂
E [f (xk )] − f∗ ≤ 1 − min min
(f (x0 ) − f∗ ).
x∈Q
Trace (D> UD)
L̂

(23)

(24)

4
An “exactness” condition similar to (19) was introduced in [30] in a program of “exactly” reformulating
a linear system into a stochastic optimization problem. Our condition has a similar meaning, but we do not
elaborate on this as this is not central to the developments in this paper.

6

Each iteration of single colum sketching Newton method (23) requires only three scalar derivatives
of the function t 7→ f (xk + tdk ) and thus if f (x) can be evaluated in constant time, this amounts to
O(1) cost per iteration. Indeed (23) is much like coordinate descent, except we descent along the di
5
directions, and with a stepsize that adapts depending on the curvature information d>
i H(xk )di .
The rate of convergence in (24) suggests that we should choose D ≈ U−1/2 so that ρ is large. If
there is no efficient way to approximate U−1/2 , then the simple choice of D = I gives ρ(xk ) =
λ+
min (H(xk ))/Trace (U) .
An expressive family of functions that satisfy Assumption 1 are generalized linear models.
Definition 4. Let 0 ≤ u ≤ `. Let φi : R 7→ R+ be a twice differentiable function such that
u ≤ φ00i (t) ≤ `,

for i = 1, . . . , n.

d

d×n

(25)
d

Let ai ∈ R for i = 1, . . . , n and A = [a1 , . . . , an ] ∈ R
. We say that f : R → R is a
generalized linear model when
n
P
2
λ
φ(a>
(26)
f (x) = n1
i x) + 2 kxk2 .
i=1

The structure of the Hessian of a generalized linear model is such that highly efficient fast JohnsonLindenstrauss sketches [3] can be used. Indeed, the Hessian is given by
n
P
1
00 >
00
>
>
H(x) = n1
ai a>
i φi (ai x) + λI = n AΦ (A x)A + λI ,
i=1

and consequently, for computing the sketch Hessian S>
k H(xk )Sk we only need to sketch the fixed
>
matrix S>
A
and
compute
S
S
efficiently,
and
thus
no
backpropgation is required. This is exactly
k
k
k
the setting where fast Johnson–Lindenstrauss transforms can be effective [17, 3].
We now give a simple expression for computing the relative smoothness and convexity constant for
generalized linear models.
Proposition 1. Let f : Rd → R be a generalized linear model with 0 ≤ u ≤ `. Then Assumption 1
is satisfied with
2
`σmax
(A) + nλ
2
uσmax
(A) + nλ

2
uσmax
(A) + nλ
.
(27)
2
`σmax
(A) + nλ


Furthermore, if we apply Algorithm 1 with a sketch such that E SS> is invertible, then the iteration
complexity (16) of applying Algorithm 1 is given by
 2
2
 
(A) + nλ
1 `σmax
1
k≥
log
.
(28)
2
ρ uσmax
(A) + nλ


L̂ =

and

µ̂ =

This complexity estimate (28) should be contrasted with that of gradient descent. When x0 ∈
Range (A) , the iteration complexity of GD (gradient descent) applied to a smooth generalized

`σ 2 (A)+nλ
linear model is given by uσ2max (A)+nλ log 1 , where σmin+ (A) is the smallest non-zero singular
min+

value of A. To simplify the discussion, and as a santiy check, consider the full Newton method
with Sk = I for all k, and consequently ρ = 1. In view of (28) Newton method does not depend
on the smallest singular values nor the condition number of the data matrix. This suggests that for
ill-conditioned problems Newton method can be superior to gradient descent, as is well known.

5

Experiments and Heuristics

In this section we evaluate and compare the computational performance of RSN (Algorithm 1)
on generalized linear models (26). Specifically, we focus on logistic regression, i.e., φi (t) =
ln (1 + e−yi t ) , where yi ∈ {−1, 1} are the target values for i = 1, . . . , n. Gradient descent (GD), accelerated gradient descent (AGD) [24] and full Newton methods6 are compared
5

There in fact exists a block coordinate method that also incorporates second order information [13].
To implement the Newton’s method efficiently, of course we exploit the ShermanMorrisonWoodbury matrix identity [32] when appropriate
6

7

10 0

10 0

10 0

10 -2

10 -2

10 -4

10 -4

10 -6

10 -6

RSN, s=250
RSN, s=500
RSN, s=750
RSN, s=1000
GD
AGD
Newton

10 -1

10 -2

10 -3

10 -4

10 -5

0

500

1000

1500

2000

2500

3000

3500

0

4000

0.05

0.1

0.15

0.2

10 -6

0.25

0

500

1500 0

1000

5

10

15

20

25

30

35

40

Figure 1: Highly dense problems, favoring RSN methods.

10 -2

10 -2

10 -3

10 -3

10 -4

10 -4

-5

10 -5

RSN, s=250
RSN, s=500
RSN, s=750
RSN, s=1000
GD
AGD
Newton

10 -2

10 -3

10

10 -4

10 -5

10 -6

10 -6
0

1

2

3

4

5

10 -6

0

50

100

150

200

0

2000

4000

6000

8000

10000

12000

14000

16000 0

50

100

150

200

10 4

Figure 2: Due to extreme sparsity, accelerated gradient is competitive with the Newton type methods.

with RSN. For simplicity, block coordinate sketches are used; these are random sketch matrices of the form Sk ∈ {0, 1}d×s with exactly one non-zero entry per row and per column. We will refer to s ∈ N as the sketch size. To ensure fairness and for comparability purposes, all methods were supplied with the exact Lipschitz constants and equipped
with the same line-search strategy (see Algorithm 3 in the supplementary material). We consider 6 datasets with a diverse number of features and samples (see Table 1 for details) which
were modified by removing all zero features and adding an intercept, i.e., a constant feature.
For regularization we used λ = Table 1: Details of the data sets taken from LIBSM [6] and
10−10 and stopped methods once OpenML [31].
the gradients norm was below
dataset
non-zero features (d) samples (n) density
tol = 10−6 or some maximal
chemotherapy
61,359 + 1
158 +1
1
number of iterations had been exgisette
5,000 + 1
6000
0.9910
hausted. In Figures 1 to 3 we plotnews20
1,355,191 + 1
19996
0.0003
ted iterations and wall-clock time
rcv1
47,237 + 1
20,241
0.0016
real-sim
20,958 + 1
72,309
0.0025
vs gradient norm, respectively.
webspam

680,715 + 1

350,000

0.0055

Newton’s method, when not limited by the immense costs of forming and solving linear systems, is
competitive as we can see in the gisette problem in Figure 1. In most real-world applications however, the bottleneck is exactly within the linear systems which may, even if they can be formed at
all, require significant solving time. On the other end of the spectrum, GD and AGD need usually
more iterations and therefore may suffer from expensive full gradient evaluations, for example due
to higher density of the data matrix, see Figure 3. RSN seems like a good compromise here: As the

10 -1

10 -1

10 -2

10 -2

10 -3

10 -3

10 -3

10 -4

10 -4

10 -4

10 -5

10 -5

10 -5

10 -6

10 -6
0

0.5

1

1.5

2

2.5

10 -1

10 -6

0

3
10 4

RSN,
RSN,
RSN,
RSN,
GD
AGD

10 -2

0.5

1

1.5

2

0

1000

2000

3000

4000

5000

6000 0

20

40

60

80

s=250
s=500
s=750
s=1000

100

10 4

Figure 3: Moderately sparse problems favor the RSN method. The full Newton method is infeasible due to
high dimensionality.
8

sketch size and type can be controlled by the user, the involved linear systems can be kept reasonably
sized. As a result, the RSN is the fastest method in all the above experiments, with the exception
of the extremely sparse problem news20 in Figure 2, where AGD outruns RSN with s = 750 by
approximately 20 seconds.

6

Conclusions and Future Work

We have laid out the foundational theory of a class of randomized Newton methods, and also performed numerical experiments validating the methods. There are now several venues of work to
explore including 1) combining the randomized Newton method with subsampling so that it can be
applied to data that is both high dimensional and abundant 2) leveraging the potential fast JohnsonLindenstrauss sketches to design even faster variants of RSN 3) develop heuristic sketches based on
past descent directions inspired on the quasi-Newton methods [15].

9

References
[1] John T. Abatzoglou, Solomon Z. Dobrowski, Sean A. Parks, and Katherine C. Hegewisch.
Data descriptor: Terraclimate, a high-resolution global dataset of monthly climate and climatic
water balance from 1958-2015. Scientific Data, 5, 2018.
[2] T. G. Addair, D. A. Dodge, W. R. Walter, and S. D. Ruppert. Large-scale seismic signal
analysis with hadoop. Computers and Geosciences, 66(C), 2014.
[3] Nir Ailon and Bernard Chazelle. The fast johnson-lindenstrauss transform and approximate
nearest neighbors. SIAM J. Comput., 39(1):302–322, May 2009.
[4] Dimitri P. Bertsekas. Constrained Optimization and Lagrange Multiplier Methods (Optimization and Neural Computation Series). Athena Scientific, 1996.
[5] Richard H Byrd, Gillian M Chin, Will Neveitt, and Jorge Nocedal. On the use of stochastic
Hessian information in optimization methods for machine learning. SIAM Journal on Optimization, 21(3):977–995, 2011.
[6] Chih Chung Chang and Chih Jen Lin. LIBSVM : A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology, 2(3):1–27, April 2011.
[7] Bruce Christianson. Automatic Hessians by reverse accumulation. IMA Journal of Numerical
Analysis, 12(2):135–150, 1992.
[8] James R. Cole, Qiong Wang, Jordan A. Fish, Benli Chai, Donna M. McGarrell, Yanni Sun,
C. Titus Brown, Andrea Porras-Alfaro, Cheryl R. Kuske, and James M. Tiedje. Ribosomal
Database Project: data and tools for high throughput rRNA analysis. Nucleic Acids Research,
42(D1):D633–D642, 11 2013.
[9] Andrew R. Conn, Nicholas I. M. Gould, and Philippe L. Toint. Trust-region Methods. Society
for Industrial and Applied Mathematics, Philadelphia, PA, USA, 2000.
[10] Alexandre d’Aspremont, Guzmán Cristóbal, and Martin Jaggi. Optimal affine-invariant smooth
minimization algorithms. SIAM Journal on Optimization, 28(3):2384–2405, 2018.
[11] Ron S. Dembo, Stanley C. Eisenstat, and Trond Steihaug. Inexact Newton methods. SIAM
Journal on Numerical Analysis, 19(2):400–408, 1982.
[12] Nikita Doikov and Peter Richtárik. Randomized block cubic Newton method. In Proceedings
of the 35th International Conference on Machine Learning, 2018.
[13] Kimon Fountoulakis and Rachael Tappenden. A flexible coordinate descent method. Computational Optimization and Applications, 70(2):351–394, Jun 2018.
[14] R M Gower and M P Mello. A new framework for the computation of hessians. Optimization
Methods and Software, 27(2):251–273, 2012.
[15] Robert M. Gower, Donald Goldfarb, and Peter Richtárik. Stochastic block BFGS: Squeezing
more curvature out of data. Proceedings of the 33rd International Conference on Machine
Learning, 2016.
[16] Robert Mansel Gower and Peter Richtárik. Randomized iterative methods for linear systems.
SIAM Journal on Matrix Analysis and Applications, 36(4):1660–1690, 2015.
[17] William Johnson and Joram Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert
space. In Conference in modern analysis and probability (New Haven, Conn., 1982), volume 26 of Contemporary Mathematics, pages 189–206. American Mathematical Society,
1984.
[18] Sai Praneeth Karimireddy, Sebastian U. Stich, and Martin Jaggi. Global linear convergence of
Newtons method without strong-convexity or Lipschitz gradients. arXiv:1806:0041, 2018.
[19] C. H. Lee and H. J. Yoon. Medical big data: promise and challenges. kidney research and
clinical practice. Kidney Res Clin Pract, 36(4):3–1, 2017.
[20] Haihao Lu, Robert M. Freund, and Yurii Nesterov. Relatively smooth convex optimization by
first-order methods, and applications. SIAM Journal on Optimization, 28(1):333–354, 2018.
[21] Haipeng Luo, Alekh Agarwal, Nicolò Cesa-Bianchi, and John Langford. Efficient second
order online learning by sketching. In Advances in Neural Information Processing Systems 29,
pages 902–910. 2016.
10

[22] Y. Nesterov and A. Nemirovskii. Interior Point Polynomial Algorithms in Convex Programming. Studies in Applied Mathematics. Society for Industrial and Applied Mathematics, 1987.
[23] Yurii Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems.
SIAM Journal on Optimization, 22(2):341–362, 2012.
[24] Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer
Publishing Company, Incorporated, 1 edition, 2014.
[25] Yurii Nesterov and Boris T. Polyak. Cubic regularization of Newton method and its global
performance. Mathematical Programming, 108(1):177–205, 2006.
[26] Ross A. Overbeek, Niels Larsen, Gordon D. Pusch, Mark D’Souza, Evgeni Selkov Jr., Nikos
Kyrpides, Michael Fonstein, Natalia Maltsev, and Evgeni Selkov. WIT: integrated system
for high-throughput genome sequence analysis and metabolic reconstruction. Nucleic Acids
Research, 28(1):123–125, 2000.
[27] Mert Pilanci and Martin J. Wainwright. Iterative Hessian sketch : Fast and accurate solution
approximation for constrained least-squares. Journal of Machine Learning Research, 17:1–33,
2016.
[28] Mert Pilanci and Martin J. Wainwright. Newton sketch: A near linear-time optimization algorithm with linear-quadratic convergence. SIAM Journal on Optimization, 27(1):205–245,
2017.
[29] Zheng Qu, Peter Richtárik, Martin Takáč, and Olivier Fercoq. SDNA: Stochastic dual Newton
ascent for empirical risk minimization. In Proceedings of the 33rd International Conference
on Machine Learning, 2016.
[30] Peter Richtárik and Martin Takáč. Stochastic reformulations of linear systems: algorithms and
convergence theory. arXiv:1706.01108, 2017.
[31] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. Openml: Networked
science in machine learning. SIGKDD Explorations, 15(2):49–60, 2013.
[32] Max A Woodbury. Inverting modified matrices. Technical report, Rep. no. 42, Statistical
Research Group, Princeton University, 1950.
[33] Tjalling J. Ypma. Historical development of the newton-raphson method. SIAM Rev.,
37(4):531–551, December 1995.

11

Supplementary Material: Randomized
Subspace Newton Method
A

Key Lemmas

Lemma 9. Let y ∈ Rd , c > 0 and H ∈ Rd×d be a symmetric positive semi-definite matrix. Let
g ∈ Range (H) . The set of solutions to
c
2
x̂ ∈ arg min hg, x − yi + kx − ykH ,
(29)
d
2
x∈R
is given by


1
x̂ ∈ H† Hy − g + Null(H).
(30)
c
Two particular solutions in the above set are given by
1
x̂ = y − H† g,
(31)
c
and the least norm solution


1
†
†
(32)
x = H Hy − g .
c
The minimum of (29) is
1
c
2
2
(33)
hg, x̂ − yi + kx̂ − ykH = − kgkH† .
2
2c
Proof. Taking the derivative in x and setting to zero gives
1
g + H(x − y) = 0.
c
The above linear system is guaranteed to have a solution because g ∈ Range(H). The solution set
to this linear system is the set
1
H† (Hy − g) + Null(H).
c
The point (31) belong to the above set by noting that (I − H† H)y ∈ Null(H), which in turn follows
by the H = HH† H property of pseudoinverse matrices. Clearly (32) is the least norm solution.
Finally, using any solution (30) we have that
1
x̂ − y ∈ (H† H − I)y − H† g + Null(H),
c
which when substituted into (29) gives


2
1
c
1
(29) =
g, (H† H − I)y − H† g + (H† H − I)y − H† g
.
c
2
c
H
|
{z
}
|
{z
}
α

>

β

2

†

Since g ∈ Range (H) we have that g (H H − I) = 0 and thus α = − 1c kgkH† . Furthermore
2

1
(H† H − I)y − H† g
c
H
2
1
2
†
=
(H H − I)y H −
H(H† H − I)y, H† g + 2 H† g
c
c
1
1
1
2
2
†
†
†
=
H g H =
g, H HH g
=
kgkH† ,
c2
c2
c2
where we used that H† HH† = H† . Using the above calculations in (34) gives
1
1
1
2
2
2
(29) = − kgkH† +
kgkH† = − kgkH† .
c
2c
2c
β

=

12

2
H

(34)

Lemma 10. For any matrix W and symmetric positive semidefinite matrix G such that
Null(G) ⊂ Null(W> ),

(35)

Null(W) = Null(W> GW)

(36)

Range(W> ) = Range(W> GW).

(37)

we have that
and

Proof. In order to establish (36), it suffices to show the inclusion Null(W) ⊇ Null(W> GW) since
the reverse inclusion trivially holds. Letting s ∈ Null(W> GW), we see that kG1/2 Wsk2 = 0,
which implies G1/2 Ws = 0. Consequently
(35)

Ws ∈ Null(G1/2 ) = Null(G) ⊂ Null(W> ).
Thus Ws ∈ Null(W> ) ∩ Range(W) which are orthogonal complements which shows that Ws =
0.
Finally, (37) follows from (36) by taking orthogonal complements. Indeed, Range(W> ) is the
orthogonal complement of Null(W) and Range(W> GW) is the orthogonal complement of
Null(W> GW).
Our assumptions are inspired on the c–stability assumption in [18]:
Proposition 2 ([18] c-stable). We say that f is c–stable if for every y, z ∈ Q, z 6= y we have that
kz − yk2H(y) > 0, and there exists a constant c ≥ 1 such that
c

=

max

y,z∈Q

kz − yk2H(z)
kz − yk2H(y)

.

(38)

We say that f is L–smooth if
f (x) ≤ f (y) + hg(y), x − yi +

L
kx − yk22 ,
2

(39)

f (x) ≥ f (y) + hg(y), x − yi +

µ
kx − yk22 .
2

(40)

and µ–strongly convex if

If f is µ–strongly convex and L–smooth, then f is L/µ–stable. Furthermore if f is c–stable then
Assumption 1 holds with L̂ ≤ c and µ̂ ≥ 1c .
Proof. Lemma 2 in [18] proves that c–stability implies c relative smoothness and c relative convexity. The inequalities L̂ ≤ c and µ̂ ≥ 1c follow from (38) compared to (22) and (21).

B

Proof of Lemma 2

Proof. Lemma 1 implies that xk+1 ∈ Q, and Lemma 9 in the appendix shows that (5) is a global
minimizer for γ = 1/L̂.

C

Proof of Lemma 3

Proof. Due to (10) we have that
(3)

f (xk+1 ) ≤ T (xk , xk+1 ) = mins T (xk , xk + λSk ) ≤ T (xk , xk ) = f (xk ).
λ∈R

13

D

Proof of Lemma 5

Proof.

1. Plugging in y = xk and x = xk + Sk λ into (3) we have that
T (xk + Sk λ, xk )

L̂
kSk λk2H(y)
2
L̂
= f (xk ) + hS>
kλk2S> H(xk )Sk .
k g(xk ), λi +
k
2
= f (xk ) + hg(xk ), Sk λi +

(41)

By taking the orthogonal
components in (6) we have that S>
∈
k g(xk )

>
Range Sk H(xk )Sk , and consequently from Lemma 9 we have that the minimizer
is given by
†

1 >
>
λk ∈ −
Sk H(xk )Sk S>
(42)
k g(xk ) + Null Sk H(xk )Sk .
L̂
Left multiplying by S>
k gives
Sk λk

† >

1
Sk g(xk ) + Sk Null S>
− Sk S>
k H(xk )Sk
k H(xk )Sk
L̂
† >
1
Sk g(xk )
− Sk S>
k H(xk )Sk
L̂
1
Pk n(xk ).
(43)
L̂

=
(6)

=

Lemma 4

=

Consequently xk + Sk λk = xk +

1
P n(xk ).
L̂ k

Furthermore, since λk is the minimizer of (41), we have from Lemma 9 and (33) that
T (xk+1 , xk )

= T (xk + Sk λk ) = f (xk ) −
= f (xk ) −

1
2L̂

1
2L̂

S>
k g(xk )

2
†
(S>
k H(xk )Sk )

2

kg(xk )kSk (S> H(xk )Sk )† S> .
k

k

2. Plugging in the constraint into the objective in (12) gives
Sk λ +

1
L̂

2

=

n(xk )

2

kλkS> H(xk )Sk +
k

H(xk )
(9)

2

kλkS> H(xk )Sk +

2
L̂
2

S>
k H(xk )n(xk ), λ +
S>
k g(xk ), λ +

1
L̂2

2

kn(xk )kH(xk )

1

2

kn(xk )kH(xk ) .
L̂
L̂2
Consequently minimizing the above is equivalent to minimizing (41), and thus Sk λ is given
by (43).
=

k

3. The Lagrangian of (13) is
L(d, λ) = kx −

2
xk kH(xk )


+

λ, S>
k H(xk )(x

− xk ) +

1
L̂

S>
k g(xk )


.

Differentiating in d and setting to zero gives
H(xk )(x − xk ) + H(xk )Sk λ = 0.

(44)

Left multiplying by S>
k and using the constraint in (13) gives
S>
k H(xk )Sk λ =

1
L̂

S>
k g(xk ).

(45)


>
Again we have that S>
k g(xk ) ∈ Range Sk H(xk )Sk by (6). Consequently by Lemma 9
we have that the solution set in λ is given by
†
1 >
>
λ=
Sk H(xk )Sk S>
k g(xk ) + Null(Sk H(xk )Sk ).
L̂
14

Plugging the above into (44) gives
† >
1
H(xk )(x − xk ) = − H(xk )Sk S>
Sk g(xk ) + H(xk )Sk Null(S>
k H(xk )Sk
k H(xk )Sk )
L̂
† >
1
(6)
Sk g(xk ).
(46)
= − H(xk )Sk S>
k H(xk )Sk
L̂
Thus (8) is a solution to the above. If Range (Sk ) ⊂ Range (Hk (xk )) then
H†k (xk )Hk (xk )Sk = Sk and the least norm solution is given by (8).

E

Proof of Theorem 2

Proof. Consider the iterates xk given by Algorithm 1 and let Ek [·] denote the expectation conditioned on xk , that is Ek [·] = E [· | xk ] . Setting y = xk in (4) and minimizing both sides7 using (33)
in Lemma 9, we obtain the inequality
1
2
(47)
f∗ ≥ f (xk ) −
kg(xk )kH† (xk ) .
2µ̂
From (11) and (3) we have that
1
2
f (xk+1 ) ≤ f (xk ) −
kg(xk )kSk (S> H(xk )Sk )† Sk .
(48)
k
2L̂
Taking expectation conditioned on xk gives
1
2
kg(xk )kG(xk ) .
(49)
Ek [f (xk+1 )] ≤ f (xk ) −
2L̂

Assumption 2 together with Range (H(xk )) = Range H1/2 (xk ) gives that
H†/2 (xk )H1/2 (xk )g(xk ) = g(xk ),
where H†/2 (xk ) = (H† (xk ))1/2 . Consequently
2

2

(50)
2

kg(xk )kG(xk ) = kg(xk )kH†/2 (xk )H1/2 (xk )G(xk )H1/2 (xk )H†/2 (xk ) ≥ ρ(xk ) kg(xk )kH† (xk ) , (51)
where we used the definition (14) of ρ(xk ) together with H†/2 (xk )g(xk ) ∈ Range (H(xk )) in the
inequality. Using (51) and (47) in (49) gives
ρ(xk )
2
kg(xk )kH† (xk )
(52)
Ek [f (xk+1 )] ≤ f (xk ) −
2L̂
ρ(xk )µ̂
(f (xk ) − f∗ ).
(53)
≤ f (xk ) −
L̂
Subtracting f∗ from both sides gives


µ̂
Ek [f (xk+1 ) − f∗ ] ≤ 1 − ρ(xk )
(f (xk ) − f∗ ).
(54)
L̂
Finally, since xk ∈ Q from Lemma 3, we have that ρ ≤ ρ(xk ) and taking total expectation gives the
result (15).

F

Proof of Theorem 3

Proof. From (52) it follows that
h
i
2
E kg(xk )kH† (xk )

≤

#
2L̂
E
(f (xk ) − Ek [f (xk+1 )])
ρ(xk )

=

2L̂
E [f (xk ) − f (xk+1 )]
ρ(xk )

(52)

(14)

≤

"

2L̂
E [f (xk ) − f (xk+1 )] .
ρ

(55)

Note that x∗ ∈ Q but the global minimizer of (33) is not necessarily in Q. This is not an issue, since the
global minima is a lower bound on the minima constrained to Q.
7

15

From (48) we have that
f (xk+1 ) ≤ f (xk ),

(56)

and thus
xk ∈ Q for all k = 0, 1, 2, . . .
Using the convexity of f (x), for every x∗ ∈ X∗ := arg min f we get
f∗

≥

(57)

=

f (xk ) + hg(xk ), x∗ − xk i
D
E
f (xk ) + H1/2 (xk )H†/2 (xk )g(xk ), x∗ − xk

≥

f (xk ) − kg(xk )kH† (xk ) kxk − x∗ kH(xk )

(50)

(57)

≥

f (xk ) − kg(xk )kH† (xk ) sup kx − x∗ kH(x) ,
x∈Q

hence
f (xk ) − f∗ ≤ kg(xk )kH† (xk ) sup kx − x∗ kH(x) .
x∈Q

Taking infimum among all x∗ ∈ X∗ and using (17) we get
f (xk ) − f∗ ≤ R kg(xk )kH† (xk ) .

(58)

Hence by Jensen’s inequality
2

(E [f (xk )] − f∗ )

h
i
2
E (f (xk ) − f∗ )
h
i
2
E R2 kg(xk )kH† (xk )

≤
(58)

≤

2L̂R2
E [f (xk ) − f (xk+1 )] .
ρ

(55)

≤

(59)

Now we put everything together:
1
1
−
E [f (xk+1 ) − f∗ ] E [f (xk ) − f∗ ]

=
(56)

≥

E [f (xk ) − f (xk+1 )]
E [f (xk+1 ) − f∗ ] E [f (xk ) − f∗ ]
E [f (xk ) − f (xk+1 )]
2

(E [f (xk ) − f∗ ])
(59)
ρ
≥
.
2L̂R2
Summing up (60) for k = 0, . . . , T − 1 and using telescopic cancellation we get
ρT
2L̂R2

≤

1
1
1
−
≤
,
E [f (xT ) − f∗ ] E [f (x0 ) − f∗ ]
E [f (xT ) − f∗ ]

(60)

(61)

which after re-arranging concludes the proof.

G

Proof of Lemma 6

Proof. If (19) holds then by taking orthogonal complements we have that

⊥
⊥
Range (H(xk )) = Null (H(xk )) = Null E[P̂(xk )] ,
and consequently
ρ(xk )

(14)+(62)

=

min
v∈Null(E[P̂(xk )])⊥

=

min
v∈Null(E[P̂(xk )])⊥

H1/2 (xk )G(xk )H1/2 (xk )v, v
2

kvk2
D
E
E[P̂(xk )]v, v
= λ+
min (E[P̂(xk )]) > 0.
2
kvk2

16

(62)

H

Proof of Lemma 7

Proof. Let XS be a random subset of Rd , where S ∼ D. We define stochastic intersection of XS :
\

XS = x ∈ Rd : x ∈ XS with probability 1 .
(63)
S∼D

Using this definition for Null(Gk ) we have
Null (Gk )

i

h
†
Null ES∼D S S> H(xk )S S>


\
†
Null S S> H(xk )S S> ,

=
=

(64)

S∼D

†
where the last equality follows from the fact that S S> H(xk )S S> is a symmetric positive
semidefinite matrix. From the properties of pseudoinverse it follows that

† 

Null S> H(xk )S
= Null S> H(xk )S = Null (S) ,
thus, we can apply Lemma 10 and obtain


†

Null S S> H(xk )S S> = Null S> .

(65)

Furthermore,
Null (Gk )

(64)

=

\



†
Null S S> H(xk )S S>

S∼D
(65)

=

\

Null S>



S∼D

=

\

Null SS>



S∼D

=



Null ES∼D SS> .

(66)

From (20) and (66) it follows that


Null (Gk ) ⊂ Null (H(xk )) = Null H1/2 (xk ) ,

(67)

hence, Lemma 10 implies that


Range (H(xk )) = Range H1/2 (xk )Gk H1/2 (xk ) ,

(68)

which concludes the proof.

I

Proof of Lemma 8

Proof. Using Taylor’s theorem, for every x, y ∈ Q we have that
Z 1
f (x) = f (y) + hg(y), x − yi +
(1 − t)kx − yk2H(y+t(x−y)) dt.

(69)

t=0

Comparing the above with (3) we have that
Z 1
L̂
(1 − t)kx − yk2H(y+t(x−y)) dt,
kx − yk2H(y) ≥
2
t=0

∀x, y ∈ Q, x 6= y.

(70)

Let x 6= y. Since we assume that kx − yk2H(y) 6= 0 we have that the relative smoothness constant
satisfies
Z 1 (1 − t)kx − yk2
L̂
H(y+t(x−y))
= max
dt.
(71)
x,y∈Q t=0
2
kx − yk2H(y)
17

Let zt = y + t(x − y)). Substituting x − y = (zt − y)/ t in the above gives the equality in (21).
Following an analogous argument for the relative convexity constant µ̂ gives the equality in (21).
Since f (x) is convex, the set Q is convex and thus zt ∈ Q for all t ∈ [0, 1]. By alternating the order
of the maximization and integral in (21) that
Z 1
kzt − yk2H(zt )
L̂ (21)
≤
(1 − t) max
dt
x,y∈Q kzt − yk2
2
t=0
H(y)
Z 1
kx − yk2H(x)
kx − yk2H(x)
zt ∈Q
1
=
.
≤
(1 − t)dt max
max
x,y∈Q kx − yk2
2 x,y∈Q kx − yk2H(y)
t=0
H(y)
Following an analogous argument for the relative convexity constant µ̂ we have that
Z 1
kzt − yk2H(zt )
µ̂ (22)
(1 − t) min
dt
≥
x,y∈Q kzt − yk2
2
t=0
H(y)
Z 1
kx − yk2H(x)
zt ∈Q
1
1
≥
(1 − t)dt min
=
.
2
kx−yk2H(x)
x,y∈Q kx − yk
2 max
t=0
H(y)
x,y∈Q kx−yk2
H(y)

J

Proof of Corollary 1

Proof. Using that
0 < d>
i H(x)di ≤ di Udi ,
which follows from H  U and our assumption that d>
i H(x)di 6= 0, we have that
G(x)

=

d
X


>
† >
Ek S(S H(x)S) S
=
i=1

(72)



1
Trace (D> UD)

d
X

di d>
i =

i=1

(72)

di Udi
di d>
i
>
Trace (D> UD) di H(x)di

1
DD> .
Trace (D> UD)

Furthermore since D is invertible we have by Lemma 10 that



Range H1/2 (x)DD> H1/2 = Range H1/2 (x) = Range (H(x)) .

(73)

(74)

And thus from Lemma 6 we have that
(18)

ρ =

K

1/2
λ+
(x)DD> H1/2 (x))
min (H
.
x∈Q
Trace (D> UD)

min λ+
min (P̂(x)) ≥ min
x∈Q

(75)

Proof of Proposition 1

Proof. The gradient and Hessian of (26) are given by
n

g(x)

=

1X
1
ai φ0i (a>
AΦ0 (A> x) + λx,
i x) + λx =
n i=1
n

H(x)

=

1X
1
00 >
ai a>
AΦ00 (A> x)A> + λI,
i φi (ai x) + λI =
n i=1
n

(76)

n

(77)

where
Φ0 (A> x) :=

0 >
n
[φ0i (a>
1 x), . . . , φi (an x)] ∈ R ,

00 >
00 >
Φ (A x) := diag φi (a1 x), . . . , φi (an x) .
00

>

18

(78)
(79)

Consequently the g(x) ∈ Range (H(x)) for all x ∈ Rd .
Using Lemma 8 and (77) we have that
2

L̂

≤

ky − zk 1 AΦ00 (A> y)A> +λI
n

max

y,z∈Rd

2

ky − zk 1 AΦ00 (A> z)A> +λI
n

2

ky − zk ` AA> +λI

(25)

≤

n

max

y,z∈Rd

2

ky − zk u AA> +λI
n

2

2

=

ky − zk `−u AA> + ky − zk u AA> +λI
n

n

max

2

ky − zk u AA> +λI

y,z∈Rd

n

2

=

1 + max

y,z∈Rd

ky − zk `−u AA>
n

(80)

2

ky − zk u AA> +λI
n

Now note that
2

max

y,z∈Rd

ky − zk `−u AA>
n

1

=

2

ky − zk u AA> +λI
n

2

ky − zk u AA> +λI
n

min

y,z∈Rd

2

ky − zk `−u AA>
n

=

1
2

ky − zk2
u
+ λ min
2
d
`−u
y,z∈R ky − zk `−u
AA>
n

=

1
,
u
nλ
1
+
2
` − u ` − u σmax
(A)

(81)

where we used that
2

min

y,z∈Rd

ky − zk2
ky −

2
zkAA>

1

=

maxy,z∈Rd

ky−zk2

AA>
ky−zk22

=

1
2
σmax
(A)

.

(82)

Inserting (81) into (80) gives
L̂ ≤ 1 +

`−u
`σ 2 (A) + nλ
= max
.
nλ
2
uσmax
(A) + nλ
u + σ2 (A)
max

The bounds for µ̂ follows from (22).
Finally turing to Lemma 7 we have that (6)
 holds
 since H(xk ) is positive definite and by Lemma 10,
and (20) holds by our assumption that E SS> is invertible. Thus by Lemma 7 we have that ρ > 0
and the total complexity result in Theorem 2 holds.

L

Uniform single coordinate sketch

Further to our results on using single column sketches with non-uniform sampling in Corollary 1,
here we present the case for uniform sampling that does not rely on the Hessian having a uniform
>
upper bound as is assumed in Corollary 1. Let Hii (x) := e>
i H(x)ei and gi (x) := ei g(x). In this
case (8) is given by
gi (xk )
xk+1 = xk −
ei .
(83)
L̂Hii (xk )
19

Algorithm 2 RSNxls: Randomized Subspace Newton with exact Line-Search
1:
2:
3:
4:
5:
6:
7:
8:
9:

input: x0 ∈ Rd
parameters: D = distribution over random matrices
for k = 0, 1, 2, . . . do
Sk ∼ D
† >
λk = − S>
k H(xk )Sk ) Sk g(xk )
dk = Sk λk
tk = argmint∈R f (xk + tdk )
xk+1 = xk + tk dk
output: last iterate xk

Corollary 2. Let P[Sk = ei ] =

1
d

and let
2

α

=

min

min

x∈Rd w∈Range(H(x))

kwkDiag(H(x))−1
2

kwkH† (x)

.

Under the assumptions of Theorem 2 we have that Algorithm 1 converges according to

k
α µ̂
E [f (xk ) − f∗ ] ≤ 1 −
(f (x0 ) − f∗ ).
d L̂
Proof. It follows by direct computation that
d

 1X
ei e>
1
−1
i
G(x) = Ek S(S> H(x)S)† S> =
= Diag (H(x)) .
d i=1 Hii (x)
d

Thus from the definition (14) we have
D
E
−1
H1/2 (x)Diag (H(x)) H1/2 (x)v, v

1
min
min
.
2
d x∈Rd v∈Range(H(x))
kvk2

Since Range H†/2 (x) = Range (H(x)) and v ∈ Range (H(x)) we can re-write v = H†/2 (x)w
where w ∈ Range (H(x)) and consequently
D
E
−1
1/2
†/2
1/2
†/2
Diag
(H(x))
H
(x)H
(x)w,
H
(x)H
(x)w
1
ρ
=
min
min
2
d x∈Rd w∈Range(H(x))
kwkH† (x)
D
E
−1
Diag (H(x)) w, w
1
α
H1/2 (x)H†/2 (x)w=w
:= .
=
min
min
2
d
d x∈R w∈Range(H(x))
d
hH(x)w, wi2
ρ=

M

Experimental details

All tests were performed in MATLAB 2018b on a PC with an Intel quad-core i7-4770 CPU and 32
Gigabyte of DDR3 RAM running Ubuntu 18.04.
M.1

Sketched Line-Search

In order to speed up convergence we can modify Algorithm 1 by introducing an exact Line-Search
and obtain Algorithm 2.
In this section we focus on heuristics for performing an exact Line-Search under the assumption
that our direction is of the form d = Sλ. This allows us to only work with sketched gradients
20

Algorithm 3 Generic Line Search - Pseudocode
1: input: increasing continuous function l : R → R with l(0) < 0 and at least one root t∗ ∈ R+
2: tolerance:  > 0
3: set [a, b] ← [0, 1]
4: while l(b) < −
5:
choose t > b
. either fixed enlargement (t = 2b) or via spline extrapolation
6:
set [a, b] ← [b, t]
7: endwhile
. end of first phase: either |l(b)| ≤  or l(a) < 0 <  ≤ l(b), i.e. t∗ ∈ [a, b]
8: set t ← b
9: while |l(t)| > 
10:
if l(t) < 0
11:
[a, b] ← [t, b]
12:
else l(t) > 0
13:
[a, b] ← [a, t]
14:
endif
15: choose t with a < t < b
. either middle of interval (t = a+b
2 ) or via spline interpolation
16: endwhile
. end of second phase
17: output: t > 0 with |l(t)| ≤ 

and sketched Hessians. This potentially allows for significant computational savings. Specifically
consider the problem of finding
t∗ := argmint∈R f (x + td),

(84)

which is, for differentiable and convex f , equivalent to finding a root of the objectives first derivative.
Defining
∂f (x + td)
l(t) :=
= d> g(x + td) = λ> (S> g(x + td))
(85)
∂t
gives us the task of solving
l(t∗ ) = 0
(86)
and differentiating once more
∂ 2 f (x + td)
= d> H(x + td)d = λ> (S> H(x + td)S)λ,
(87)
∂2t
reveals that we do not need full, but only sketched gradient and Hessian access, in order to evaluate
l respectively l0 . Note that the evaluation of
l0 (t) =

l(0) = λ> S> g(x)
l0 (0) = λ> (S> H(x)S)λ

(88)

are essentially a by-product from the computation of λ in Algorithm 2 and therefore add almost no
computational cost. Furthermore, if f is convex and λ = −(S> H(x)S)† S> g(x) is given , then
l(0) = −g(x)> S(S> H(x)S)† S> g(x) ≤ 0

(89)

implies that d is a weak descent direction of f . Since in this case, l(0) = 0 implies t∗ = 0, let us
focus on the situation that we actually have a strong descent direction, i.e. that
l(0) < 0

(90)

is satisfied. The line-search 3 ensures an output t > 0 satisfying |l(t)| ≤  and is best explained by
strengthening Step 4 of (3) to “while l(b) < 0”, as this would ensure that the final values of a and b
box the minimum t∗ ∈ [a, b]: The first phase is to identify an interval [a, b] with 0 ≤ a < b such that
l(a) < 0 ≤ l(b)

(91)

which guarantees the existence of at least one minimum t∗ ∈ [a, b]. In the second phase, we can then
decrease the intervals length with a ≤ ā < b̄ ≤ b such that 0 ≤ l(t) ≤  is satisfied for all t ∈ [ā, b̄]
and some given tolerance  > 0. Both steps should be safeguarded and can be assisted by using
cubic splines inter- or extrapolating l(t). This approach has the potential of reducing computational
costs and the benefit of avoiding function evaluations of f entirely.
21

