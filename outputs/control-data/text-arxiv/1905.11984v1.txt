Minimizing Time-to-Rank:
A Learning and Recommendation Approach
Haoming Lia , Sujoy Sikdarb , Rohit Vaishc , Junming Wangd , Lirong Xiae , and Chaonan Yef
a

Duke University, haoming.li@duke.edu
Rensselaer Polytechnic Institute, sikdas@rpi.edu
c
Rensselaer Polytechnic Institute, vaishr2@rpi.edu
d
Rensselaer Polytechnic Institute, wangj33@rpi.edu
e
Rensselaer Polytechnic Institute, xial@cs.rpi.edu
f
Stanford University, canonyeee@gmail.com

arXiv:1905.11984v1 [cs.HC] 27 May 2019

b

May 30, 2019

Abstract
Consider the following problem faced by an online voting platform: A user is provided with
a list of alternatives, and is asked to rank them in order of preference using only drag-and-drop
operations. The platform’s goal is to recommend an initial ranking that minimizes the time spent
by the user in arriving at her desired ranking. We develop the first optimization framework to
address this problem, and make theoretical as well as practical contributions. On the practical
side, our experiments on Amazon Mechanical Turk provide two interesting insights about user
behavior: First, that users’ ranking strategies closely resemble selection or insertion sort, and second,
that the time taken for a drag-and-drop operation depends linearly on the number of positions
moved. These insights directly motivate our theoretical model of the optimization problem. We
show that computing an optimal recommendation is NP-hard, and provide exact and approximation
algorithms for a variety of special cases of the problem. Experimental evaluation on MTurk shows
that, compared to a random recommendation strategy, the proposed approach reduces the (average)
time-to-rank by up to 50%.

1

Introduction

Eliciting preferences in the form of rankings over a set of alternatives is a common task in social choice,
crowdsourcing, and in daily life. For example, the organizer of a meeting might ask the participants to
rank a set of time-slots based on their individual schedules. Likewise, in an election, voters might be
required to rank a set of candidates in order of preference.
Over the years, computerized systems have been increasingly used in carrying out preference elicitation tasks such as the ones mentioned above. Indeed, recently there has been a proliferation of online
voting platforms such as CIVS, OPRA, Pnyx, RoboVote, and Whale4 .1 In many of these platforms, a
user is presented with an arbitrarily ordered list of alternatives, and is asked to shuffle them around
in-place using drag-and-drop operations until her desired preference ordering is achieved. Figure 1
illustrates the use of drag-and-drop operations in sorting a given list of numbers.
Our focus in this work is on time-to-rank, i.e., the time it takes for a user to arrive at her desired
ranking, starting from a ranking suggested by the platform and using only drag-and-drop operations.
We study this problem from the perspective of the voting platform that wants to recommend an optimal
1

CIVS (https://civs.cs.cornell.edu/), OPRA (opra.io), Pnyx (https://pnyx.dss.in.tum.de/), RoboVote (http://robovote.org/),
Whale4 (https://whale.imag.fr/).

1

Figure 1: Sorting via drag-and-drop operations.

initial ranking to the user (i.e., one that minimizes time-to-rank). Time to accomplish a designated task
is widely considered as a key consideration in the usability of automated systems (Bevan et al., 2015;
Albert and Tullis, 2013), and serves as a proxy for user effort. Indeed, ‘time on task’ was identified as
a key factor in the usability and efficiency of computerized voting systems in a 2004 report by NIST to
the U.S. Congress for the Help America Vote Act (HAVA) (Laskowski et al., 2004). In crowdsourcing,
too, time on task plays a key role in the recruitment of workers, quality of worker participation, and
in determining payments (Cheng et al., 2015; Maddalena et al., 2016).
Note that the initial ranking suggested by the platform can have a significant impact on the time
spent by the user on the ranking task. Indeed, if the user’s preferences are known beforehand, then
the platform can simply recommended it to her and she will only need to verify that the ordering
is correct. In practice, however, users’ preferences are often unknown. Furthermore, users employ a
wide variety of ranking strategies, and based on their proficiency with the interface, users can have
very different drag-and-drop times. All these factors make the task of predicting the time-to-rank and
finding an optimal recommendation challenging and non-trivial.
We emphasize the subtle difference between our problem and that of preference elicitation. The
latter involves repeatedly asking questions to the users (e.g., in the form of pairwise comparisons
between alternatives) to gather enough information about their preferences. By contrast, our problem
involves a one-shot recommendation followed by a series of drag-and-drop operations by the user
until her desired ranking is achieved. There is an extensive literature on preference eliciation (Conen
and Sandholm, 2001; Conitzer and Sandholm, 2002; Blum et al., 2004; Boutilier, 2013; Busa-Fekete et al.,
2014; Soufiani et al., 2013; Zhao et al., 2018). Yet, somewhat surprisingly, the problem of recommending
a ranking that minimizes users’ time and effort has received little attention. Our work aims to address
this gap.
Our Contributions We make contributions on three fronts:
• On the conceptual side, we propose the problem of minimizing time-to-rank and outline a framework for addressing it (Figure 2).
• On the theoretical side, we formulate the optimization problem of finding a recommendation
to minimize time-to-rank (Section 4). We show that computing an optimal recommendation is
NP-hard, even under highly restricted settings (Theorem 3). We complement the intractability
results by providing a number of exact (Theorem 2) and approximation algorithms (Theorems 4
to 6) for special cases of the problem.
• We use experimental analysis for the dual purpose of motivating our modeling assumptions as
well as justifying the effectiveness of our approach (Section 5). Our experiments on Amazon
Mechanical Turk reveal two insights about user behavior (Section 5.1): (1) The ranking strategies
of real-world users closely resemble insertion/selection sort, and (2) the drag-and-drop time of an
alternative varies linearly with the distance moved. Additionally, we find that a simple adaptive
strategy (based on the Borda count voting rule) can reduce time-to-rank by up to 50% compared
to a random recommendation strategy (Section 5.2), validating the usefulness of the proposed
framework.
2

Figure 2: High level overview of our framework. Our technical contributions are highlighted in blue.

1.1

Overview of Our Framework

Figure 2 illustrates the proposed framework which consists of three key steps. In Step 1, we learn
user preferences from historical data by developing a statistical ranking model, typically in the form
of a distribution D over the space of all rankings (refer to Section 2 for examples of ranking models).
In Step 2, which runs in parallel to Step 1, we learn user behavior; in particular, we identify their
sorting strategies (Section 3.1) as well as their drag-and-drop times (Section 3.2). Together, these two
components define the time function which models the time taken by a user in transforming a given
initial ranking σ into a target ranking τ , denoted by time(σ, τ ). The ranking model D from Step
1 and the time function from Step 2 together define the recommendation problem in Step 3, called
(D, w)-Recommendation (the parameter w is closely related to the time function; we elaborate on
this below). This is the optimization problem of computing a ranking σ that minimizes the expected
time-to-rank of the user, i.e., minimizes Eτ ∼D [time(σ, τ )]. The user is then recommended σ, and her
preference history is updated.
The literature on learning statistical ranking models is already well-developed (Guiver and Snelson,
2009; Awasthi et al., 2014; Lu and Boutilier, 2014; Zhao et al., 2016). Thus, while this is a key ingredient
of our framework (Step 1), in this work we choose to focus on Steps 2 and 3, namely, learning user
behavior and solving the recommendation problem.
Recall that the time function defines the time taken by a user in transforming a given ranking σ into
a target ranking τ . For a user who follows a fixed sorting algorithm (e.g., insertion or selection sort),
the time function can be broken down into (1) the number of drag-and-drop operations suggested by
the sorting algorithm, and, (2) the (average) time taken for each drag-and-drop operation by the user.
As we will show in Lemma 1 in Section 3.1, point (1) above is independent of the choice of the sorting
algorithm. Therefore, the time function can be equivalently defined in terms of the weight function w,
which describes the time taken by a user, denoted by w(`), in moving an alternative by ` positions via
a drag-and-drop operation. For this reason, we use w in the formulation of (D, w)-Recommendation.
Applicability Our framework is best suited for the users who have already formed their preferences,
so that the recommended ranking does not bias their preferences. This is a natural assumption in
some applications, such as in the meeting organization example in Section 1. In general, however, it is
possible that a user, who is undecided between options A and B, might prefer A over B if presented
in that order by the recommended ranking. A careful study of such biases (aka “framing effect”) is an
interesting direction for future work.
Additional Related Work Our work is related to the literature on inferring a ground truth ordering
from noisy information (Braverman and Mossel, 2008), and aggregating preferences by minimizing
some notion of distance to the observed rankings such as the total Kendall’s Tau distance (Procaccia
and Shah, 2016). Previous work on preference learning and learning to rank can also be integrated in
our framework (Liu, 2011; Lu and Boutilier, 2014; Khetan and Oh, 2016; Agarwal, 2016; Negahban et al.,
2017; Zhao and Xia, 2018).

3

2

Preliminaries

Let A = {a1 , . . . , am } denote a set of m alternatives, and let L(A) be the set of all linear orders over A.
For any σ ∈ L(A), ai σ aj denotes that ai is preferred over aj under σ, and let σ(k) denote the k th
most preferred alternative in σ. A set of n linear orders {σ (1) , . . . , σ (n) } is called a preference profile.
Definition 1 (Kendall’s Tau distance; Kendall, 1938). Given two linear orders σ, σ 0 ∈ L(A), the Kendall’s
0
0
0 :=
Tau
P distance dkt (σ, σ ) is the number of pairwise disagreements between σ and σ . That is, dkt (σ, σ )
ai ,aj ∈A 1[aj σ 0 ai and ai σ aj ], where 1 is the indicator function.
:= (θ1 , . . . , θm ) be such that
Definition 2 (Plackett-Luce model;
P Plackett, 1975; Luce, 1959). Let θ
θi ∈ (0, 1) for each i ∈ [m] and i∈[m] θi = 1. Let Θ denote the corresponding parameter space. The
Plackett-Luce (PL) model parameterized by θ ∈ Θ defines a distribution over the set of linear orders L(A)
as follows: The probability of generating σ := (ai1  ai2  . . .  aim ) is given by
Pr(σ|θ) =

θ
Pmi1

`=1 θi`

·

θ
Pmi2

`=2 θi`

· ··· ·

θim−1
θim−1 +θim .

More generally, a k-mixture Plackett-Luce model (k-PL) is parameterized by {γ (`) , θ(`) }k`=1 , where
(`) = 1, γ (`) ≥ 0 for all ` ∈ [k], and θ(`) ∈ Θ for all ` ∈ [k]. The probability of generating
`=1 γ
P
σ ∈ L(A) is given by Pr(σ|{γ (`) , θ(`) }k`=1 ) = k`=1 γ (`) Pr(σ|θ(`) ).
Pk

Definition 3 (Mallows model; Mallows, 1957). The Mallows model (MM) is specified by a reference
ranking σ ∗ ∈ L(A) and a dispersion parameter φ ∈ (0, 1]. The probability of generating a ranking σ is
P
dkt (σ,σ ∗ )
0 ∗
given by Pr(σ|σ ∗ , φ) = φ Z
, where Z = σ0 ∈L(A) φdkt (σ ,σ ) .
∗ , φ }k , where
More generally, a k-mixture Mallows model (k-MM) is parameterized by {γ (`) , σ(`)
(`) `=1
Pk
(`)
(`)
∗
= 1, γ ≥ 0 for all ` ∈ [k], and σ(`) ∈ L(A), φ(`) ∈ (0, 1] for all ` ∈ [k]. The probability of
`=1 γ
Pk
(`) Pr(σ|σ ∗ , φ ).
∗ , φ }k ) =
generating σ ∈ L(A) is given by Pr(σ|{γ (`) , σ(`)
(`) `=1
`=1 γ
(`) (`)
Definition 4 (Uniform distribution). Under the uniform distribution (Unif) supported on a preference
profile {σ (i) }ni=1 , the probability of generating σ ∈ L(A) is n1 if σ ∈ {σ (i) }ni=1 and 0 otherwise.

3

Modeling User Behavior

In this section, we will model the time spent by the user in transforming the recommended ranking σ
into the target ranking τ . Our formulation involves the sorting strategy of the user (Section 3.1) as well
as her drag-and-drop time (Section 3.2).

3.1

Sorting Algorithms

A sorting algorithm takes as input a ranking σ ∈ L(A) and performs a sequence of drag-and-drop
operations until the target ranking is achieved. At each step, an alternative is moved from its current position to another (possibly different) position and the current ranking is updated accordingly.
Below we will describe two well-known examples of sorting algorithms: selection sort and insertion
sort. Let σ (k) denote the current list at time step k ∈ {1, 2, . . . } (i.e., before the sorting operation at
time step k takes place). Thus, σ (1) = σ. For any σ ∈ L(A), define the k-prefix set of σ as Pk (σ) :=
{σ(1), σ(2), . . . , σ(k)} (where P0 (σ) := ∅) and corresponding suffix set as Sk (σ) := A \ Pk (σ).
Selection Sort Let ai denote the most preferred alternative according to τ in the set Sk−1 (σ (k) ). At
step k of selection sort, the alternative ai is promoted to a position such that the top k alternatives in
the new list are ordered according to τ . Note that this step is well-defined only under the sorted-prefix
property, i.e., at the beginning of step k of the algorithm, the alternatives in Pk−1 (σ (k) ) are sorted
according to τ . This property is maintained by selection sort.

4

Insertion Sort Let ai denote the most preferred alternative in Sk−1 (σ (k) ) according to σ (k) . At step
k of insertion sort, the alternative ai is promoted to a position such that the top k alternatives in the
new list are ordered according to τ . Note that this step is well-defined only under the sorted-prefix
property, which is maintained by insertion sort.
Sorting Algorithms In this work, we will be concerned with sorting algorithms that involve a combination of insertion and selection sort. Specifically, we will use the term sorting algorithm to refer to a
sequence of steps s1 , s2 , . . . such that each step sk corresponds to either selection or insertion sort, i.e.,
sk ∈ {SEL,INS} for every k. If sk = SEL, then the algorithm promotes the most preferred alternative
in Sk−1 (σ (k) ) (according to τ ) to a position such that the top k alternatives in the new list are ordered
according to τ . If sk = INS, then the algorithm promotes the most preferred alternative in Sk−1 (σ (k) )
(according to σ (k) ) to a position such that the top k alternatives in the new list are ordered according
to τ .
For example, in Figure 1, starting from the recommended list at the extreme left, the user performs
a selection sort operation (promoting 19 to the top of the current list) followed by an insertion sort
operation (promoting 30 to its correct position in the sorted prefix {19, 22, 40}) followed by either
selection or insertion sort operation (promoting 23 to its correct position). We will denote a generic
sorting algorithm by A and the class of all sorting algorithms by A.
Count Function Given a sorting algorithm A, a source ranking σ ∈ L(A) and a target ranking
σ→τ : [m − 1] → Z ∪ {0} keeps track of the number of drag-and-drop
τ ∈ L(A), the count function fA
+
operations (and the number of positions by which some alternative is moved in each such operation)
σ→τ (`) is the number of times some alternative is ‘moved up
during the execution of A. Formally, fA
by ` positions’ during the execution of algorithm A when the source and target rankings are σ and τ
respectively.2 For example, let A be insertion sort, σ = (d, c, a, b), and τ = (a, b, c, d). In step 1, the
user considers the alternative d and no move-up operation is required. In step 2, the user promotes
c by one position (since c τ d) to obtain the new list (c, d, a, b). In step 3, the user promotes a by
two positions to obtain (a, c, d, b). Finally, the user promotes b by two positions to obtain the target
list (a, b, c, d). Overall, the user performs one ‘move up by one position’ operation and two ‘move up
σ→τ (1) = 1, f σ→τ (2) = 2, and f σ→τ (3) = 0. We will write
by two positions’ operations. Hence, fA
A
A
#moves to denote
the
total
number
of
drag-and-drop
operations
performed
during the execution of A,
Pm−1 σ→τ
i.e., #moves = `=1 fA (`).
Remark 1. Notice the difference between the number of drag-and-drop operations (#moves) and the total
distance covered (i.e., the number of positions by which alternatives are moved). Indeed, the above example
involves three drag-and-drop operations (#moves = 3), but the total distance moved is 0 + 1 + 2 + 2 = 5.
The latter quantity is equal to dkt (σ, τ ).
σ→τ (`) =
Lemma 1. For any two sorting algorithms A, A0 ∈ A, any σ, τ ∈ L(A), and any ` ∈ [m−1], fA
σ→τ (`).
fA
0
σ→τ .
In light of Lemma 1, we will hereafter drop the subscript A and simply write f σ→τ instead of fA
The proof of Lemma 1 appears in Section 7.3.

3.2

Drag-and-Drop Time

Weight function The weight function w : [m − 1] → R≥0 models the time taken for each dragand-drop operation; specifically, w(`) denotes the time taken by the user in moving an alternative up
by ` positions.3 Of particular interest to us will be the linear weight function wlin (`) = ` for each
2

Notice that we do not keep track of which alternative is moved by ` positions. Indeed, we believe it is reasonable to
assume that moving the alternative a1 up by ` positions takes the same time as it will for a2 . Also, we do not need to define
the count function for move down operations as neither selection sort nor insertion sort will ever make such a move.
3
Here, ‘time taken’ includes the time spent in thinking about which alternative to move as well as actually carrying out
the move.

5

Linear Weights

Distribution D

General Weights

Hardness

Exact Algo.

Approx. Algo.

Approx. Algo.

k-mixture
Plackett-Luce (k-PL)

NP-c even for k = 4
(Theorem 3)

Poly for k = 1
(Theorem 2)

PTAS (Theorem 4)
5-approx. (Theorem 5)

αβ-approx.
(Theorem 6)

k-mixture Mallows
(k-MM)

NP-c even for k = 4
(Theorem 3)

Poly for k = 1
(Theorem 2)

PTAS (Theorem 4)
5-approx. (Theorem 5)

αβ-approx.
(Theorem 6)

Uniform (Unif)

NP-c even for n = 4
(Theorem 3)

Poly for n ∈ {1, 2}
(Theorem 2)

PTAS (Theorem 4)
5-approx. (Theorem 5)

αβ-approx.
(Theorem 6)

Table 1: Computational complexity results for (D, w)-Recommendation. Each row corresponds to a preference
model and each column corresponds to a weight function. We use the shorthands Poly, NP-c, PTAS, and αβapprox. to denote polynomial-time (exact) algorithm, NP-complete, polynomial-time approximation scheme, and
αβ-approximation algorithm respectively. The parameters α and β capture how closely a given weight function
approximates a linear weight function; see Definition 6.

` ∈ [m − 1] and the affine weight function waff (`) = c` + d for each ` ∈ [m − 1] and fixed constants
c, d ∈ N.
Time Function Given the count function f σ→τ and the weight
w, the time function is
P function
σ→τ (`) · w(`).
f
defined as their inner product, i.e., timew (σ, τ ) = hf σ→τ , wi = m−1
`=1
Theorem 1 shows that for the linear weight function wlin , time is equal to the Kendall’s Tau distance, and for the affine weight function, time is equal to a weighted combination of Kendall’s Tau
distance and the total number of moves.
Theorem 1. For any σ, τ ∈ L(A), timewlin (σ, τ ) = dkt (σ, τ ) and timewaff (σ, τ ) = c · dkt (σ, τ ) + d ·
#moves.
The proof of Theorem 1 appears in Section 7.4.

4

Formulation of Recommendation Problem and Theoretical Results

We model the recommendation problem as the following computational problem: Given the preference
distribution D of the user and her time function (which, in turn, is determined by the weight function
w), find a ranking that minimizes the expected time taken by the user to transform the recommended
ranking σ into her preference τ .
Definition 5 ((D, w)-Recommendation). Given a distribution D over L(A), a weight function w, and
a number δ ∈ Q, does there exist σ ∈ L(A) so that Eτ ∼D [timew (σ, τ )] ≤ δ?
We will focus on settings where the distribution D is Plackett-Luce, Mallows, or Uniform, and
the weight function w is Linear, Affine, or General. Note that if the quantity Eτ ∼D [timew (σ, τ )]
can be computed in polynomial time for a given distribution D and weight function w, then (D, w)Recommendation is in NP.
Our computational results for (D, w)-Recommendation are summarized in Table 1. We show
that this problem is NP-hard, even when the weight function is linear (Theorem 3). On the algorithmic
side, we provide a polynomial-time approximation scheme (PTAS) and a 5-approximation algorithm
for the linear weight function (Theorems 4 and 5), and an approximation scheme for non-linear weights
(Theorem 6).
Theorem 2 (Exact Algorithms). (D, w)-Recommendation is solvable in polynomial time when w
is linear and D is either (a) k-mixture Plackett-Luce (k-PL) with k = 1, (b) k-mixture Mallows model
(k-MM) with k = 1, or (c) a uniform distribution with support size n ≤ 2.

6

Theorem 3 (Hardness results). (D, w)-Recommendation is NP-complete even when w is linear and
D is either (a) k-mixture Plackett-Luce model (k-PL) for k = 4, (b) k-mixture Mallows model (k-MM)
for k = 4, or (c) a uniform distribution over n = 4 linear orders.
Theorem 4 (PTAS). (D, w)-Recommendation admits a polynomial time approximation scheme (PTAS)
when w is linear and D is either (a) k-mixture Plackett-Luce model (k-PL) for k ∈ N, (b) k-mixture Mallows model (k-MM) for k ∈ N, or (c) a uniform distribution (Unif).
The PTAS in Theorem 4 is quite complicated and is primarily of theoretical interest (indeed, for
Õ(1/ε)
any fixed ε > 0, the running time of the algorithm is m2
, making it difficult to be applied in
experiments). A simpler and more practical algorithm (although with a worse approximation) is based
on the well-known Borda count voting rule (Theorem 5).
Theorem 5 (5-approximation). (D, w)-Recommendation admits a polynomial time 5-approximation
algorithm when w is linear and D is either (a) k-mixture Plackett-Luce model (k-PL) for k ∈ N, (b) kmixture Mallows model (k-MM) for k ∈ N, or (c) a uniform distribution (Unif).
Our next result (Theorem 6) provides an approximation guarantee for (D, w)-Recommendation
that applies to non-linear weight functions, as long as they are “close” to the linear weight function in
the following sense:
Definition 6 (Closeness-of-weights). A weight function w is said to be (α, β)-close to another weight
function w0 if there exist α, β ≥ 1 such that for every ` ∈ [m − 1], we have
w0 (`)/β ≤ w(`) ≤ αw0 (`).
For any (possibly non-linear) weight function w that is (α, β) close to the linear weight function
wlin , Theorem 6 provides an αβ-approximation scheme for (D, w)-Recommendation.
Theorem 6 (Approximation for general weights). Given any ε > 0 and any weight function w that is
Õ(1/ε)
(α, β)-close to the linear weight function wlin , there exists an algorithm that runs in time m2
and
returns a linear order σ such that
Eτ ∼D [timew (σ, τ )] ≤ αβ(1 + ε)Eτ ∼D [timew (σ ∗ , τ )],
where σ ∗ ∈ arg minσ0 ∈L(A) Eτ ∼D [timew (σ 0 , τ )].
Remark 2. Notice that the PTAS of Theorem 4 is applicable for any affine weight function waff =
c · wlin + d for some fixed constants c, d ∈ N. As a result, the approximation guarantee of Theorem 6 also
extends to any weight function that is (α, β)-close to some affine weight function.

5

Experimental Results

We perform two sets of experiments on Amazon Mechanical Turk (MTurk). The first set of experiments (Section 5.1) is aimed at identifying the sorting strategies of the users as well as a model of their
drag-and-drop behavior. The observations from these experiments directly motivate the formulation
of our theoretical model, which we have already presented in Section 4. The second set of experiments
(Section 5.2) is aimed at evaluating the practical usefulness of our approach.
In both sets of experiments, the crowdworkers were asked to sort in increasing order a randomly
generated list of numbers between 0 and 100 (the specifics about the length of the lists and how they
are generated can be found in Sections 5.1 and 5.2). Figure 3 shows an example of the instructions
provided to the crowdworkers.
In each experiment, the task length was advertised as 10 minutes, and the payment offered was
$0.25 per task. The crowdworkers were provided a user interface (see Figure 1) that allows for dragand-drop operations. To ensure data quality, we removed those workers from the data who failed to
successfully order the integers more than 80% of the time, or did not complete all the polls. We also
removed the workers with high variance in their sorting time; in particular, those with coefficient of
variation above the 80th percentile. The reported results are for the workers whose data was retained.
7

Figure 3: Instructions given to the MTurk workers.

5.1

Identifying User Behavior

To identify user behavior, we performed two experiments: (a) Rank10, where each crowdworker participated in 20 polls, each consisting of a list of 10 integers (between 0 and 100) generated uniformly
at random, and (b) Rank5, which is a similar task with 30 polls and lists of length 5. In each poll,
we recorded the time taken by a crowdworker to move an alternative (via drag-and-drop operation)
and the number of positions by which the alternative was moved. After the initial pruning (as described above), we retained 9840 polls submitted by 492 workers in the Rank10 experiment, and 10320
polls submitted by 344 workers retained in the Rank5 experiment. Table 2 summarizes the aggregate
statistics. Our observations are discussed below.

Sorting time
Total number of drag-and-drop operations
Total number of positions moved during drag-and-drop operations
Number of operations coinciding with selection/insertion sort
Kendall’s Tau distance between the initial and final rankings

Mean
24.41
7.69
22.59
5.09
22.55

Rank10
Median
Std. Dev.
22.65
9.12
8
1.8
23
5.59
6
2.28
22
5.6

Mean
7.75
2.91
5.05
2.21
5.04

Rank5
Median
6.99
3
5
2
5

Std. Dev.
3.54
1.13
2.01
1.06
2.01

Table 2: Summary of the user statistics recorded in the experiments in Section 5.1.

Sorting Behavior Our hypothesis regarding the ranking behavior of human crowdworkers was that
they use (some combination of) natural sorting algorithms such as selection sort or insertion sort
(Section 3.1). To test our hypothesis, we examined the fraction of the drag-and-drop operations that
coincided with an iteration of selection/insertion sort. (Given a ranking σ, a drag-and-drop operation
on σ coincides with selection/insertion sort if the order of alternatives resulting from the drag-and-drop
operation exactly matches the order of alternatives when one iteration of either selection or insertion
sort is applied on σ.) We found that, on average, 2.21
2.91 = 76% of all drag-and-drop operations in Rank5
=
66.2%
in
the
Rank10)
coincided
with
selection/insertion sort.
(and 5.09
7.69
Drag-and-Drop Behavior To identify the drag-and-drop behavior of the users, we plot the timeto-rank as a function of the total number of positions by which the alternatives are moved in each poll
(Figure 4). Recall from Remark 1 that for an ideal user who uses only insertion/selection sort, the latter
quantity is equal to dkt (σ, τ ).
Dataset
Rank10
Rank5

Avg. MSE
(in seconds2 )
42.98
7.74

√
Avg. MSE
(in seconds)
6.56
2.78

Avg. Sorting Time
(in seconds)
24.41
7.75

Number of users based on their best-fit model
Only dkt Only #moves Both dkt and #moves
217
199
76
138
180
26

Table 3: Average 5-fold cross-validation MSE over all workers using the best model for each worker, and the number of users
for which each of the models was identified to be the best. # moves is the number of times alternatives are moved using selection
or insertion sort.

Our hypothesis was that the sorting time varies linearly with the total number of drag-and-drop
operations (#moves) and the Kendall’s Tau distance (dkt (σ, τ )). To verify this, we used linear regression
with time-to-rank (or sorting time) as the target variable and measured the mean squared error (MSE)
using 5-fold cross-validation for three different choices of independent variables: (1) Only dkt , (2) only
8

Figure 4: Relationship between the number of positions moved and the total sorting time for Rank10 (left) and
Rank5 (right).

#moves, and (3) both dkt and #moves. For each user, we picked the model with the smallest MSE
(see Table 3 for the resulting distribution of the number of users). We found that the predicted drag6.56
and-drop times (using the best-fit model for each user) are, on average, within 24.41
= 26.8% of the
2.78
observed times for Rank10 and within 7.75 = 35.8% for Rank5.

5.2

Evaluating the Proposed Framework

To evaluate the usefulness of our framework, we compared a random recommendation strategy with
one that forms an increasingly accurate estimate of users’ preferences with time. Specifically, we first
fix the ground truth ranking of 10 alternatives consisting of randomly generated integers between 0
and 100. Each crowdworker then participates in two sets of 10 polls each. In one set of polls, the
crowdworkers are provided with initial rankings generated by adding independent Gaussian noise to
the ground truth (to simulate a random recommendation strategy), and their sorting times are recorded.
In the second set of polls, the recommended set of alternatives is the same as under the random
strategy but ordered order to a Borda ranking. Specifically, the ordering in the k th iteration is determined by the Borda ranking aggregated from the previous k − 1 iterations.

Figure 5: Relationship between sorting time and the number of polls completed by the users for std dev=10 (left)
and std dev=20 (right).

Figure 5 shows the average sorting time of the crowdworkers as a function of the index of the polls
under two different noise settings: std. dev. = 10 and std. dev. = 20. We can make two important
observations: First, that Borda recommendation strategy (in green) provides a significant reduction in
the sorting time of the users compared to the random strategy (in blue). Indeed, the sorting time of the
users is reduced by up to 50%, thus validating the practical usefulness of our framework. The second

9

observation is that the reduction in sorting time is not due to increasing familiarity with the interface.
This is because the average sorting time for the random strategy remains almost constant throughout
the duration of the poll.

6

Conclusion and Future Work

We proposed a recommendation framework to minimize time-to-rank. We formulated a theoretical
model of the recommendation problem (including NP-hardness results and associated approximation
algorithms), and illustrated the practical effectiveness of our approach in real-world experiments.
Our work opens up a number of directions for future research. In terms of theoretical questions,
it would be interesting to analyze the complexity of the recommendation problem for other distance
measures, e.g., Ulam distance. On the practical side, it would be interesting to analyze the effect of
cognitive biases such as the framing effect (Tversky and Kahneman, 1981) and list position bias (Lerman and Hogg, 2014) on the recommendation problem. Progress in this direction can, in turn, have
implications on the fairness of recommendation algorithms.

Acknowledgments
We are grateful to IJCAI-19 reviewers for their helpful comments. This work is supported by NSF
#1453542 and ONR #N00014-17-1-2621.

References
Shivani Agarwal. On Ranking and Choice Models. In Proceedings of the Twenty-Fifth International Joint
Conference on Artificial Intelligence, pages 4050–4053, 2016. (Cited on page 3)
Nir Ailon, Moses Charikar, and Alantha Newman. Aggregating Inconsistent Information: Ranking and
Clustering. Journal of the ACM, 55(5):23, 2008. (Cited on page 13)
William Albert and Thomas Tullis. Measuring the User Experience: Collecting, Analyzing, and Presenting
Usability Metrics. Newnes, 2013. (Cited on page 2)
Noga Alon. Ranking Tournaments. SIAM Journal on Discrete Mathematics, 20(1):137–142, 2006. (Cited
on page 13)
Pranjal Awasthi, Avrim Blum, Or Sheffet, and Aravindan Vijayaraghavan. Learning Mixtures of Ranking Models. In Proceedings of Advances in Neural Information Processing Systems, pages 2609–2617,
2014. (Cited on page 3)
Nigel Bevan, James Carter, and Susan Harker. ISO 9241-11 revised: What have we Learnt about Usability Since 1998? In International Conference on Human-Computer Interaction, pages 143–151. Springer,
2015. (Cited on page 2)
Avrim Blum, Jeffrey Jackson, Tuomas Sandholm, and Martin Zinkevich. Preference Elicitation and
Query Learning. Journal of Machine Learning Research, 5:649–667, 2004. (Cited on page 2)
Craig Boutilier. Computational Decision Support: Regret-Based Models for Optimization and Preference Elicitation. In P. H. Crowley and T. R. Zentall, editors, Comparative Decision Making: Analysis
and Support Across Disciplines and Applications. Oxford University Press, 2013. (Cited on page 2)
Mark Braverman and Elchanan Mossel. Noisy Sorting Without Resampling. In Proceedings of the
Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 268–276, 2008. (Cited on
page 3)

10

Róbert Busa-Fekete, Eyke Hüllermeier, and Balázs Szörényi. Preference-Based Rank Elicitation using Statistical Models: The Case of Mallows. In Proceedings of the 31st International Conference on
International Conference on Machine Learning, pages II:1071–1079, 2014. (Cited on page 2)
Irène Charon and Olivier Hudry. An Updated Survey on the Linear Ordering Problem for Weighted or
Unweighted Tournaments. Annals of Operations Research, 175(1):107–158, 2010. (Cited on page 13)
Justin Cheng, Jaime Teevan, and Michael S Bernstein. Measuring Crowdsourcing Effort with ErrorTime Curves. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing
Systems, pages 1365–1374, 2015. (Cited on page 2)
Wolfram Conen and Tuomas Sandholm. Minimal Preference Elicitation in Combinatorial Auctions. In
IJCAI-2001 Workshop on Economic Agents, Models, and Mechanisms, pages 71–80, 2001. (Cited on
page 2)
Vincent Conitzer. Computing Slater Rankings Using Similarities among Candidates. In Proceedings
of the 21st National Conference on Artificial Intelligence, volume 1, pages 613–619, 2006. (Cited on
page 13)
Vincent Conitzer and Tuomas Sandholm. Vote Elicitation: Complexity and Strategy-Proofness. In
Eighteenth National Conference on Artificial Intelligence, pages 392–397, 2002. (Cited on page 2)
Don Coppersmith, Lisa K Fleischer, and Atri Rurda. Ordering by Weighted Number of Wins Gives a
Good Ranking for Weighted Tournaments. ACM Transactions on Algorithms (TALG), 6(3):55, 2010.
(Cited on page 14)
Cynthia Dwork, Ravi Kumar, Moni Naor, and D. Sivakumar. Rank Aggregation Methods for the Web.
In Proceedings of the 10th World Wide Web Conference, pages 613–622, 2001. (Cited on pages 13
and 17)
John Guiver and Edward Snelson. Bayesian Inference for Plackett-Luce Ranking Models. In Proceedings
of the 26th Annual International Conference on Machine Learning, ICML-09, pages 377–384, Montreal,
Quebec, Canada, 2009. (Cited on page 3)
Maurice G Kendall. A New Measure of Rank Correlation. Biometrika, 30(1/2):81–93, 1938. (Cited on
page 4)
Claire Kenyon-Mathieu and Warren Schudy. How to Rank with Few Errors: A PTAS for Weighted
Feedback Arc Set on Tournaments. In Proceedings of the Thirty-Ninth Annual ACM Symposium on
Theory of Computing, pages 95–103, 2007. (Cited on page 13)
Ashish Khetan and Sewoong Oh. Data-Driven Rank Breaking for Efficient Rank Aggregation. Journal
of Machine Learning Research, 17(193):1–54, 2016. (Cited on page 3)
Sharon J Laskowski, Marguerite Autry, John Cugini, and William Killam. Improving the Usability and
Accessibility of Voting Systems and Products. NIST Special Publication, 500:256, 2004. (Cited on
page 2)
Kristina Lerman and Tad Hogg. Leveraging Position Bias to Improve Peer Recommendation. PloS one,
9(6):e98914, 2014. (Cited on page 10)
Tie-Yan Liu. Learning to Rank for Information Retrieval. Springer, 2011. (Cited on page 3)
Tyler Lu and Craig Boutilier. Effective Sampling and Learning for Mallows Models with PairwisePreference Data. Journal of Machine Learning Research, 15:3963–4009, 2014. (Cited on page 3)
Robert Duncan Luce. Individual Choice Behavior: A Theoretical Analysis. Wiley, 1959. (Cited on page 4)
11

Eddy Maddalena, Marco Basaldella, Dario De Nart, Dante Degl’Innocenti, Stefano Mizzaro, and Gianluca Demartini. Crowdsourcing Relevance Assessments: The Unexpected Benefits of Limiting the
Time to Judge. In Fourth AAAI Conference on Human Computation and Crowdsourcing, 2016. (Cited
on page 2)
Colin L. Mallows. Non-Null Ranking Model. Biometrika, 44(1/2):114–130, 1957. (Cited on pages 4
and 13)
Sahand Negahban, Sewoong Oh, and Devavrat Shah. Rank Centrality: Ranking from Pairwise Comparisons. Operations Research, 65(1):266–287, 2017. (Cited on page 3)
Robin L. Plackett. The Analysis of Permutations. Journal of the Royal Statistical Society. Series C (Applied
Statistics), 24(2):193–202, 1975. (Cited on page 4)
Ariel D Procaccia and Nisarg Shah. Optimal Aggregation of Uncertain Preferences. In Thirtieth AAAI
Conference on Artificial Intelligence, pages 608–614, 2016. (Cited on page 3)
Hossein Azari Soufiani, David C Parkes, and Lirong Xia. Preference Elicitation For General Random
Utility Models. In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence,
pages 596–605, 2013. (Cited on page 2)
Amos Tversky and Daniel Kahneman. The Framing of Decisions and the Psychology of Choice. Science,
211(4481):453–458, 1981. (Cited on page 10)
Zhibing Zhao and Lirong Xia. Composite Marginal Likelihood Methods for Random Utility Models.
In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of
Machine Learning Research, pages 5922–5931, 2018. (Cited on page 3)
Zhibing Zhao, Peter Piech, and Lirong Xia. Learning Mixtures of Plackett-Luce Models. In Proceedings
of the 33rd International Conference on Machine Learning (ICML-16), 2016. (Cited on page 3)
Zhibing Zhao, Haoming Li, Junming Wang, Jeffrey Kephart, Nicholas Mattei, Hui Su, and Lirong Xia.
A Cost-Effective Framework for Preference Elicitation and Aggregation. In Proceedings of the 2018
Conference on Uncertainty in Artificial Intelligence (UAI), 2018. (Cited on page 2)

12

7
7.1

Appendix
Additional Preliminaries

The pairwise marginal distribution for the k-mixture Plackett-Luce model is given by
Prσ∼k-PL (ai σ aj ) =

Pk

`=1 γ

(`)

·

(`)
θi
(`)
(`)
θi +θj

.

(1)

Proposition 1 (Mallows, 1957). Let σ ∗ , φ be the parameters of a Mallows model (MM), and let ai , aj ∈ A
be such that ai σ∗ aj . Let ∆ = rank(σ ∗ , aj ) − rank(σ ∗ , ai ). Then,
Prσ∼(σ∗ ,φ) (ai σ aj ) =

P∆
zφz−1
P∆−1z=1
P
.
z
( z=0 φz )( ∆
z=0 φ )

∗ , φ }k , can
The pairwise marginal for a k-mixture Mallows model, parameterized by {γ (`) , σ(`)
(`) `=1
∗
∗
be derived similarly. Fix a pair ai , aj ∈ A. For each ` ∈ [k], let ∆i,j
` := rank(σ(`) , aj ) − rank(σ(`) , ai ).
Define the function g` : Z \ {0} → R≥0 as
P∆

z−1
z=1 zφ(`)

P
P


if ∆ > 0,
 ∆−1 φz
∆
z
z=0 φ(`)
z=0
(`)
P|∆|
g` (∆) :=
z−1

z=1 zφ(`)


if ∆ < 0.
1 − P|∆|−1 z P
|∆|
z
z=0

φ(`)

z=0

φ(`)

th
Thus, g` (∆i,j
` ) is the pairwise marginal probability induced by the ` mixture, i.e.,
∗
g` (∆i,j
` ) = Prσ∼(σ(`) ,φ(`) ) (ai σ aj ).

The pairwise marginal for the k-MM model is given by
P
Prσ∼k-MM (ai σ aj ) = k`=1 γ (`) g` (∆i,j
` ).

7.2

(2)

Relevant Computational Problems

Definition 7 (Kemeny).
preference profile {σ (i) }ni=1 and a number δ ∈ Q, does there exist
Pn Given a (i)
σ ∈ L(A) such that i=1 dkt (σ, σ ) ≤ δ?
Kemeny is known to be NP-complete even for n = 4 (Dwork et al., 2001).
Definition 8 (Weighted Feedback Arc Set in Tournaments (WFAST)). Given a complete directed
graph G = (V, E), a set of non-negative edge weights {wi,j , wj,i }(i,j)∈E where wi,jP+ wj,i = b for some
fixed constant b ∈ (0, 1], and a number δ ∈ Q, does there exist σ ∈ L(A) such that i,j∈V wj,i · 1[ai σ
aj ] ≤ δ?
WFAST is known to be NP-complete even when wi,j = 1 if (i, j) ∈ E and 0 otherwise (Ailon
et al., 2008; Alon, 2006; Conitzer, 2006; Charon and Hudry, 2010). A polynomial-time approximation
scheme (PTAS) for WFAST is also known (Kenyon-Mathieu and Schudy, 2007). Proposition 2 recalls
this result.
Proposition 2 (Kenyon-Mathieu and Schudy, 2007). Given any ε > 0 and an instance of WFAST, there
Õ(1/ε)

exists an algorithm that runs in time |V |2
and returns a linear order σ such that
X
X
wj,i · 1[ai σ aj ] ≤ (1 + ε)
wj,i · 1[ai σ∗ aj ],
i,j∈V

where σ ∗ ∈ arg minτ ∈L(A)

i,j∈V

P

i,j∈V

wj,i · 1[ai τ aj ].
13

When b = 1, WFAST admits a 5-approximation algorithm based on the Borda count voting rule
(i.e., ordering the vertices in increasing order of their weighted indegrees).
Proposition 3 (Coppersmith et al., 2010). There is a polynomial-time algorithm that, given any instance
of WFAST with b = 1, returns a linear order σ such that
P
P
i,j∈V wj,i · 1[ai σ aj ] ≤ 5 ·
i,j∈V wj,i · 1[ai σ ∗ aj ],
P
where σ ∗ ∈ arg minτ ∈L(A) i,j∈V wj,i · 1[ai τ aj ].

7.3

Proof of Lemma 1

σ→τ (`) =
Lemma 1. For any two sorting algorithms A, A0 ∈ A, any σ, τ ∈ L(A), and any ` ∈ [m−1], fA
σ→τ
fA0 (`).

Proof. We will prove Lemma 1 via induction on the number of alternatives m. The base case of m = 1
is trivial. Suppose the lemma holds for all alternative sets of size m ≤ n − 1. We will show that the
lemma also holds for m = n.
Let σ, τ be any two linear orders over the same set of n alternatives, namely A. Let a := τ (1) be
the most preferred alternative under τ , and let a be ranked k th under σ, i.e., σ(k) = a. Let σ−a and τ−a
denote the truncated linear orders obtained by dropping the alternative a from σ and τ respectively.
We will show that for any sorting algorithm A ∈ A, the following conditions hold:
If k = n, then
( σ →τ
fA−a −a (`) for all ` ∈ [n − 2], and
σ→τ
fA (`) =
(3)
1
for ` = n − 1;
and if k < n, then
 σ−a →τ−a

(`)
fA
σ−a →τ−a
σ→τ
fA (`) = fA
(`) + 1


0

for all ` ∈ [n − 2] \ {k − 1},
for ` = k − 1, and
for ` = n − 1.

(4)

Note that the claims in Equations (3) and (4) suffice to prove the lemma: Indeed, σ−a and τ−a are
valid linear orders over the same set of (n−1) alternatives, namely A\{a}. Therefore, by the induction
hypothesis, we have that for any two sorting algorithms A, A0 ∈ A and any ` ∈ [n − 2],
σ

fA−a

→τ−a

σ

(`) = fA−a
0

→τ−a

(`).

(5)

σ→τ (`) = f σ→τ (`) for all ` ∈ [n − 1], as desired.
Equations (3) to (5) together give us that fA
A0
To prove the claims in Equations (3) and (4), recall from Section 3.1 that a sorting algorithm A is a
sequence of steps s1 , s2 , . . . such that every step corresponds to either selection or insertion sort, i.e.,
sj = {SEL, INS} for every j. We will prove the claims via case analysis based on whether A performs
a selection sort operation during the first k steps or not.

Case I: At least one of the first k steps s1 , . . . , sk is selection sort.
Let 1 ≤ i ≤ k be such that si = SEL and sj = INS for all 1 ≤ j < i. In the first (i − 1) steps
(which are all insertion sort operations), the algorithm A only considers the top (i − 1) alternatives in
σ, namely Pi−1 (σ). Furthermore, since i − 1 < k, we have that a ∈
/ Pi−1 (σ). Therefore, the top (i − 1)
alternatives in σ are identical to those in σ−a , and the execution of A during σ → τ is identical to
σ ,i
σ,i
that during σ−a → τ−a for the first (i − 1) steps. Stated differently, if fA
(`) and fA−a (`) denote the
number of move-up-by-`-positions operations performed by A during the first i steps for the input σ
σ ,i−1
σ,i−1
σ,i−1
and σ−a respectively, then fA
(`) = fA−a
(`) for all ` ∈ [n − 2] and fA
(n − 1) = 0.
14

At the ith step, A performs a selection sort operation. This involves promoting the alternative a by
(k − 1) positions to the top of the current list. Therefore, at the end of the first i steps, we have:
If k = n, then
( σ ,i−1
fA−a
(`) for all ` ∈ [n − 2], and
σ,i
fA (`) =
(6)
1
for ` = n − 1;
and if k < n, then
 σ−a ,i−1

(`)
fA
σ,i
σ−a ,i−1
fA (`) = fA
(`) + 1


0

for all ` ∈ [n − 2] \ {k − 1},
for ` = k − 1, and
for ` = n − 1.

(7)

Let σ 0 denote the list maintained by A at the end of the ith step during σ → τ . In addition, let σ 00
denote the list maintained by A at the end of the (i − 1)th step during σ−a → τ−a . We therefore have
that
0

σ→τ (`) = f σ,i (`) + f σ →τ (`) for every ` ∈ [n − 1], and
fA
A
A
σ

fA−a

→τ−a

σ

(`) = fA−a

,i−1

σ 00 →τ−a

(`) + fA

(`) for every ` ∈ [n − 2].

(8)

Observe that σ 0 = (a, σ 00 ). Consider the execution of A during σ 0 → τ and during σ 00 → τ−a .
From Lemma 2 (stated below), we have that
0

σ 00 →τ−a

σ →τ (`) = f
fA
A

0

σ →τ (n − 1) = 0.
(`) for all ` ∈ [n − 2] and fA

(9)

Equations (6) to (9) together give the desired claim.
Case II: Each of the first k steps is insertion sort, i.e., s1 = INS, . . . , sk = INS.
The analysis in this case is identical to that of Case I for the first (k − 1) steps. That is, at the end
σ→τ (`) = f σ−a →τ−a (`) for all ` ∈ [n − 2] and f σ→τ (n − 1) = 0. Note that
of the first (k − 1) steps, fA
A
A
alternative a continues to be at the k th position in the current list at the end of the first (k − 1) steps.
At the k th step, A performs an insertion sort operation. Since a is the most preferred alternative
under τ , this step once again involves promoting a by (k − 1) positions to the top of the current list,
i.e., the count function is modified exactly as in Case I. The rest of the analysis is identical to Case I as
well. This finishes the proof of Lemma 1.
Lemma 2. Let a ∈ A and σ−a , τ−a ∈ L(A \ {a}). Let σ, τ ∈ L(A) be such that σ := (a, σ−a ) and
σ→τ (`) = f σ−a →τ−a (`) for all ` ∈ [m − 2] and
τ := (a, τ−a ). Then, for any sorting algorithm A ∈ A, fA
A
σ→τ
fA (m − 1) = 0, where |A| = m.
σ→τ (m − 1) = 0. Suppose, for contradiction, that f σ→τ (m − 1) > 0,
Proof. We will first argue that fA
A
that is, some alternative (say, b) is promoted by (m − 1) positions during the execution of A. Since
both selection and insertion sort maintain the sorted prefix property at every time step, it must be that
b τ a, which is a contradiction since a is the most preferred alternative under τ .
σ→τ (`) = f σ−a →τ−a (`) for all ` ∈ [m − 2]. Once again, by the sorted
Next, we will argue that fA
A
prefix property, no alternative is promoted above a at any time step during σ → τ . Since the top
position remains fixed, the execution of A during σ−a → τ−a can be mimicked to obtain the execution
of A during σ → τ . The lemma now follows.

15

7.4

Proof of Theorem 1

Theorem 1. For any σ, τ ∈ L(A), timewlin (σ, τ ) = dkt (σ, τ ) and timewaff (σ, τ ) = c · dkt (σ, τ ) + d ·
#moves.
P
σ→τ (`) · `. Regardless of
Proof. For the linear weight function wlin , we have timewlin (σ, τ ) = m−1
`=1 f
the choice of the sorting algorithm, any fixed pair of alternatives is swapped at most once during the
transformation from σ to τ . As a result, each “move up by ` slots” operation, which contributes ` units
to the time function, also contributes ` units to the Kendall’s Tau distance, giving us the desired claim.
For the affine weight function waff , we therefore have
P
σ→τ (`).
timewaff (σ, τ ) = c · timewlin (σ, τ ) + d · m−1
`=1 f
= c · dkt (σ, τ ) + d · #moves,
as desired.

7.5

Proof of Theorem 2

Theorem 2 (Exact Algorithms). (D, w)-Recommendation is solvable in polynomial time when w
is linear and D is either (a) k-mixture Plackett-Luce (k-PL) with k = 1, (b) k-mixture Mallows model
(k-MM) with k = 1, or (c) a uniform distribution with support size n ≤ 2.
Proof. (a) When D is a k-mixture Plackett-Luce (k-PL) with k = 1
The expected time for any σ ∈ L(A) under the PL model with the parameter θ is given by
Eτ ∼θ [timewlin (σ, τ )] = Eτ ∼θ [dkt (σ, τ )]
i
hP
1[a

a
]
= Eτ ∼θ
j
τ i
ai ,aj ∈A : ai σ aj
P
= ai ,aj ∈A : ai σ aj Eτ ∼θ [1[aj τ ai ]]
P
= ai ,aj ∈A : ai σ aj Prτ ∼θ (aj τ ai )
P
θj
= ai ,aj ∈A : ai σ aj θi +θ
j

(Theorem 1)
(Definition 1)
(Linearity of Expectation)

(Definition 2).

(10)

Let σ ∗ ∈ L(A) be a linear order that is consistent with the parameter θ. That is, for any ai , aj ∈ A,
ai σ∗ aj if and only if either θi > θj or i < j in case θi = θj . We will show via an exchange argument
that for any σ ∈ L(A), Eτ ∼θ [timewlin (σ ∗ , τ )] ≤ Eτ ∼θ [timewlin (σ, τ )]. The desired implication will
then follow by simply computing σ ∗ , which can be done in polynomial time.
Consider a pair of alternatives ai , aj ∈ A that are adjacent in σ such that ai σ∗ aj and aj σ ai
(such a pair must exist as long as σ 6= σ ∗ ).4 Let σ 0 ∈ L(A) be derived from σ by swapping ai and aj
(and making no other changes). Then, from Equation (10), we have that
Eτ ∼θ [timewlin (σ 0 , τ )] − Eτ ∼θ [timewlin (σ, τ )] =

θj
θi +θj

−

θi
θi +θj

≤ 0,

where the inequality holds because σ ∗ is consistent with θ and ai σ∗ aj . By repeated use of the above
argument—with σ 0 taking the role of σ, and so on—we get the desired claim.
(b) When D is a k-mixture Mallows model (k-MM) with k = 1
The proof is similar to case (a). Once again, we let σ and σ 0 be two linear orders that are identical
except for the pair ai , aj ∈ A that are adjacent in σ such that ai σ∗ aj , aj σ ai , and ai σ0 aj ; here
4

Note that ai , aj need not be adjacent in σ ∗ .

16

σ ∗ is the reference ranking for the Mallows model. Then,
Eτ ∼(σ∗ ,φ) [timewlin (σ 0 , τ )]−Eτ ∼(σ∗ ,φ) [timewlin (σ, τ )]
= Eτ ∼(σ∗ ,φ) [dkt (σ 0 , τ )] − Eτ ∼(σ∗ ,φ) [dkt (σ, τ )]

(by Theorem 1)

= Prτ ∼(σ∗ ,φ) (aj τ ai ) − Prτ ∼(σ∗ ,φ) (ai τ aj )

= 2 12 − Prτ ∼(σ∗ ,φ) (ai τ aj )


P∆
zφz−1
P
= 2 12 − P∆−1z=1
(by Proposition 1),
z
( z=0 φz )( ∆
z=0 φ )
where ∆ = rank(σ ∗ , aj ) − rank(σ ∗ , ai ). It is easy to verify that g(∆) :=

P∆
zφz−1
P∆−1z=1
P
z
z
( z=0 φ )( ∆
z=0 φ )

≥

1
2

for

all integral ∆ ≥ 1 whenever φ ∈ [0, 1]. This implies that
Eτ ∼(σ∗ ,φ) [timewlin (σ 0 , τ )] ≤ Eτ ∼(σ∗ ,φ) [timewlin (σ, τ )].
Repeated application of the above argument shows that for any linear order σ ∈ L(A),
Eτ ∼(σ∗ ,φ) [timewlin (σ ∗ , τ )] ≤ Eτ ∼(σ∗ ,φ) [timewlin (σ, τ )].
The desired implication follows by simply returning the reference ranking σ ∗ as the output.
(c) When D is a uniform distribution with support size n ≤ 2
(i) n
Let D be a uniform distribution over the set of n linear orders
Pn {σ }i=1 .(i)From Theorem 1, we
know that for any σ ∈ L(A), we have Eτ ∼D [timewlin (σ, τ )] = i=1 dkt (σ, σ ). When n = 1, it is
clear that σ = σ (1) is the unique minimizer of the expected cost. When n = 2, it can be argued that
σ ∈ {σ (1) , σ (2) } is the desired solution. Indeed, let S := {(ai , aj ) ∈ A × A : ai σ(1) aj and aj σ(2)
ai } be the set of (ordered) pairs of alternatives over which σ (1) and σ (2) disagree. Any linear order
σ∈
/ {σ (1) , σ (2) } contributes at least |S| to the expected time in addition to the number of pairs over
which σ differs from σ (1) or σ (2) . Hence, the expected time is minimized when σ ∈ {σ (1) , σ (2) }.

7.6

Proof of Theorem 3

Theorem 3 (Hardness results). (D, w)-Recommendation is NP-complete even when w is linear and
D is either (a) k-mixture Plackett-Luce model (k-PL) for k = 4, (b) k-mixture Mallows model (k-MM)
for k = 4, or (c) a uniform distribution over n = 4 linear orders.
Proof. (a) When D is k-mixture Plackett-Luce model (k-PL) for k = 4
Let D be a k-mixture Plackett-Luce model with the parameters {γ (`) , θ(`) }k`=1 , and let σ ∈ L(A).
By an argument similar to that in the proof of Theorem 2, we have that
Eτ ∼k-PL [timewlin (σ, τ )] =

P

ai ,aj ∈A : ai σ aj

(`)

θj

Pk

(`) ·
`=1 γ

(`)

(`)

θi +θj

,

(11)

which can be computed in polynomial time. Hence the problem is in NP.
To prove NP-hardness, we will show a reduction from a restricted version of Kemeny for four
agents, which is known to be NP-complete (Dwork et al., 2001). Given any instance of Kemeny with
the preference profile {σ (`) }n`=1 where n = 4, the parameters of D are set up as follows: The number of
(`)
(`)
mixtures is set to k = n = 4. For each ` ∈ [n], γ (`) = n1 , and for each ai ∈ A, θi = m4(m−rank(σ ,ai )) .
(1)

(1)

(1)

Thus, for instance, if σ (1) = (a1  a2  . . . am ), then θ1 = m4(m−1) , θ2 = m4(m−2) , . . . , θm = 1.
(`)
Notice that despite being exponential in m, the parameters {θi }`∈[n],i∈[m] can each be specified in
poly(m) number of bits, and are therefore polynomial in the input size.
17

Pn
(i)
We will now argue that a linear order σ ∈ L(A) satisfies P
i=1 dkt (σ, σ ) ≤ δ if and only if
n
σ (i) ) ≤ θ. Define, for
Eτ ∼k-PL [timewlin (σ, τ )] ≤ δ + 0.5. First, suppose that σ satisfies i=1 dkt
P(σ,
n
each ` ∈ [n], S` := {(ai , aj ) ∈ A × A : ai σ aj and aj σ(`) ai }. Thus, `=1 |S` | ≤ δ. Then, from
Equation (11), we have that
Eτ ∼k-PL [timewlin (σ, τ )] =
=
≤

P

ai ,aj ∈A : ai σ aj

1
n
1
n

Pn



Pn

P

1
`=1 n

·

(`)
θj
(`)
(`)
θi +θj

(`)

(`)

θj

P

`=1

Pn

(ai ,aj )∈S` θ(`) +θ(`)
i
j

`=1
(ai ,aj )∈S` 1 +

 1 
P
≤ n1 n`=1 δ + m
2 1+m4

1
=δ+ m
2 1+m4

+

θj

P



(ai ,aj )∈S
/ ` θ(`) +θ(`)
i
j

1
(ai ,aj )∈S
/ ` 1+m4

P



≤ δ + 0.5.
The first inequality follows from the choice of parameters in our construction. Indeed, for any (ai , aj ) ∈
(`)

(`)

θj

(`)

S` , we have θi , θj ≥ 1 and therefore

(`)

(`)

θi +θj

≤ 1. In addition, for any (ai , aj ) ∈
/ S` , we have that

rank(σ (`) , ai ) < rank(σ (`) , aj ), and therefore
(`)
θj
(`)
(`)
θi +θj

1

=
1+m

(

)

4 rank(σ (`) ,aj )−rank(σ (`) ,ai )

≤

1
.
1+m4

P
The second inequality uses the fact that n`=1 |S` | ≤ δ. The final inequality holds because m4 +
1−2 m
2 > 0 for all m ≥ 1.
P
Now suppose that σ satisfies Eτ ∼k-PL [timewlin (σ, τ )] ≤ δ+0.5. We will argue that ni=1 dkt (σ, σ (i) )
must be strictly smaller
P will give us the desired claim. Suppose, for
P than δ + 1, which, by integrality,
contradiction, that ni=1 dkt (σ, σ (i) ) ≥ δ + 1. Thus, n`=1 |S` | ≥ δ + 1. We can use this relation to
construct a lower bound on the expected time, as follows:


(`)
(`)
P
P
θj
θj
1 Pn
Eτ ∼k-PL [timewlin (σ, τ )] = n `=1
(ai ,aj )∈S` (`) (`) +
(ai ,aj )∈S
/ ` (`) (`)
θi +θj

≥ (δ + 1)

θi +θj

m4
m4 +1

> δ + 0.5.
(`)

The first inequality holds because for any (ai , aj ) ∈
/ S` , we have that

θj
(`)

(`)

θi +θj

≥ 0, and for any

(ai , aj ) ∈ S` , we have that
(`)

θj

(`)
(`)
θi +θj

≥

m4(rank(σ

(`) ,a )−rank(σ (`) ,a ))
i
j

4(rank(σ (`) ,ai )−rank(σ (`) ,aj ))

1+m

≥

m4
.
1+m4

δ+1
1
The second inequality holds because 1+m
4 < 2 for all m ≥ 2 (note that we can assume m ≥ 2
withoutP
loss of generality). The chain of inequalities give us the desired contradiction. Hence, it must
be that ni=1 dkt (σ, σ (i) ) ≤ δ. This finishes the proof of part (a) of Theorem 3.

(b) When D is k-mixture Mallows model (k-MM) for k = 4
∗ , φ }k , and let σ ∈ L(A).
Let D be a k-mixture Mallows model with the parameters {γ (`) , σ(`)
(`) `=1
By an argument similar to that in the proof of Theorem 2, we have that
P
P
∗ ,φ
Eτ ∼k-MM [timewlin (σ, τ )] = ai ,aj ∈A : ai σ aj k`=1 γ (`) · Prτ ∼(σ(`)
(aj τ ai ),
(`) )

18

which can be computed in polynomial time (Equation (2)). Therefore, the problem is in NP.
To prove NP-hardness, we will show a reduction from Kemeny. Given any instance of Kemeny
with the preference profile {σ (`) }n`=1 , the parameters of D are set up as follows: The number of mix∗ = σ (`) , and φ
tures k is set to n. For each ` ∈ [n], γ(`) = n1 , σ(`)
(`) = 0. The expected time for any
linear order σ is simply its average Kendall’s Tau distance from the profile {σ (`) }n`=1 , hence the equivalence of the solutions follows. Finally, since Kemeny is known to be NP-complete even for n = 4, a
similar implication holds for (D, w)-Recommendation when k = 4.
(c) When D is a uniform distribution over n = 4 linear orders
Membership in NP follows from Theorem 1, since for the linear weight function, the expected
time for any linear order σ ∈ L(A) is equal to its average Kendall’s Tau distance from the preference
profile that supports D, which can be computed in polynomial time. In addition, NP-hardness follows
from a straightforward reduction from Kemeny: Given any instance of Kemeny with the preference
profile {σ (i) }ni=1 , the distribution D in (D, w)-Recommendation is simply a uniform distribution over
{σ (i) }ni=1 . The equivalence of the solutions follows once again from Theorem 1. Finally, since Kemeny
is known to be NP-complete even for n = 4, a similar implication holds for (D, w)-Recommendation
as well.

7.7

Proof of Theorem 4

Theorem 4 (PTAS). (D, w)-Recommendation admits a polynomial time approximation scheme (PTAS)
when w is linear and D is either (a) k-mixture Plackett-Luce model (k-PL) for k ∈ N, (b) k-mixture Mallows model (k-MM) for k ∈ N, or (c) a uniform distribution (Unif).
Proof. We will show that for each of the three settings in Theorem 4, (D, w)-Recommendation turns
out to be a special case of WFAST, and therefore the PTAS of Proposition 2 from Section 7.2 applies.
(a) When D is k-mixture Plackett-Luce model (k-PL)
Recall from Theorem 1 that when the weight function is linear, the expected cost of σ ∈ L(A) is
given by Eτ ∼D [timewlin (σ, τ )] = Eτ ∼D [dkt (σ, τ )]. When D is a k-mixture Plackett-Luce model (k-PL)
with the parameters {γ (`) , θ(`) }k`=1 , the expected cost of σ under D is given by (refer to Equation (10)
in the proof of Theorem 2):
Eτ ∼k-PL [timewlin (σ, τ )] =

P

ai ,aj ∈A : ai σ aj

Pk

(`) ·
`=1 γ

(`)

θj
(`)

(`)

θi +θj

.

Consider a complete, directed, and weighted graph G = (A, E) defined over the set of alternatives,
where for every pair of alternatives ai , aj , we have (ai , aj ) ∈ E if and only if either θi > θj or i < j
(`)
P
θ
in case θi = θj . Each edge (ai , aj ) ∈ E is associated with a pair of weights wi,j = k`=1 γ (`) · (`) i (`)
θi +θj

and wj,i =

Pk

`=1 γ

(`)

(`)

·

θj
(`)

(`)

θi +θj

. Notice that wi,j + wj,i = 1 for every (ai , aj ) ∈ E. Furthermore, the

expected cost of σ can be expressed in terms of the edge-weights as follows:
X
Eτ ∼k-PL [timewlin (σ, τ )] =
wj,i · 1[ai σ aj ].
ai ,aj ∈A

Therefore, σ is a solution of (D, w)-Recommendation if and only if it is a solution of WFAST for the
graph G constructed above (with b = 1).
(b) When D is k-mixture Mallows model (k-MM)

19

An analogous argument works for the case when D is a k-mixture Mallows
(k-MM) with the
Pkmodel
(`)
∗
k
(`)
parameters {γ , σ(`) , φ(`) }`=1 . In this case, we set the weights to be wi,j = `=1 γ ·g` (∆i,j
` ) and wj,i =


Pk
i,j
i,j
(`)
· 1 − g` (∆` ) , where ∆` and g` (·) are as defined in Equation (2) in Section 7.1.
`=1 γ
(c) When D is a uniform distribution
(`) n
when D is a uniform distribution
PnFinally,
Pn 1 over {σ }`=1 , an analogous argument works for wi,j =
1
`=1 n · 1[ai σ (`) aj ] and wj,i =
`=1 n · 1[aj σ (`) ai ].

7.8

Proof of Theorem 5

Theorem 5 (5-approximation). (D, w)-Recommendation admits a polynomial time 5-approximation
algorithm when w is linear and D is either (a) k-mixture Plackett-Luce model (k-PL) for k ∈ N, (b) kmixture Mallows model (k-MM) for k ∈ N, or (c) a uniform distribution (Unif).
Proof. (Sketch) The proof is similar to that of Theorem 4 in Section 7.7. The only difference is that
we use the algorithm in Proposition 3 (from Section 7.2) instead of Proposition 2 as a subroutine.
Notice that the condition wi,j + wj,i = 1 is satisfied for every (ai , aj ) ∈ E, and thus Proposition 3 is
applicable.

7.9

Proof of Theorem 6

Theorem 6 (Approximation for general weights). Given any ε > 0 and any weight function w that is
Õ(1/ε)
(α, β)-close to the linear weight function wlin , there exists an algorithm that runs in time m2
and
returns a linear order σ such that
Eτ ∼D [timew (σ, τ )] ≤ αβ(1 + ε)Eτ ∼D [timew (σ ∗ , τ )],
where σ ∗ ∈ arg minσ0 ∈L(A) Eτ ∼D [timew (σ 0 , τ )].
Proof. We will show that the linear order σ constructed in Theorem 4 provides the desired approximation guarantee. Let σ lin ∈ arg minσ∈L(A) Eτ ∼D [timewlin (σ, τ )]. Then,
Eτ ∼D [timew (σ, τ )] ≤ αEτ ∼D [timewlin (σ, τ )]

(by closeness-of-weights)

≤ α(1 + ε)Eτ ∼D [timewlin

(σ lin , τ )]

≤ α(1 + ε)Eτ ∼D [timewlin

(σ ∗ , τ )]

(optimality of σ lin )

≤ αβ(1 + ε)Eτ ∼D [timew (σ ∗ , τ )]

(by closeness-of-weights).

20

(Theorem 4)

