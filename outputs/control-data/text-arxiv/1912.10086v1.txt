BIG DATA APPROACHES TO KNOT THEORY: UNDERSTANDING THE
STRUCTURE OF THE JONES POLYNOMIAL

arXiv:1912.10086v1 [math.GT] 20 Dec 2019

JESSE S F LEVITT, MUSTAFA HAJIJ, AND RADMILA SAZDANOVIC
Abstract. We examine the structure and dimensionality of the Jones polynomial using manifold
learning techniques. Our data set consists of more than 10 million knots up to 17 crossings and two
other special families up to 2001 crossings. We introduce and describe a method for using filtrations
to analyze infinite data sets where representative sampling is impossible or impractical, an essential
requirement for working with knots and the data from knot invariants. In particular, this method
provides a new approach for analyzing knot invariants using Principal Component Analysis. Using
this approach on the Jones polynomial data we find that it can be viewed as an approximately 3
dimensional manifold, that this description is surprisingly stable with respect to the filtration by the
crossing number, and that the results suggest further structures to be examined and understood.

1. Introduction
Throughout the development of low-dimensional topology, there has been an emphasis on the
study of invariants from algebraic, combinatorial and geometric perspectives. The scarcity of results
considering the statistical nature of these invariants is quite surprising given the abundance of
available data. Examining the distributions that arise from invariants should reveal and illuminate
structures that are difficult to see using traditional tools. We consider the Jones polynomial from
a statistical perspective and provide an outline of how to use filtrations to investigate infinite data
sets of this type.
The techniques of big data and deep learning provide useful tools for analyzing the statistical
nature of knot invariants. Multiple advances have resulted from melding traditional methods in
Physics and Mathematics with the emerging data-driven techniques of scientific computing. These
have ranged from solving previously intractable problems in Computer Vision to providing significant improvements in earthquake prediction models. Despite the wide number of techniques
available, the study of how to use these powerful statistical tools in pure mathematics is in its
infancy. Machine learning and data mining techniques have just started to attract attention in
Knot theory, see [27, 30, 50] and to our knowledge have largely been used for predicting valuations.
This is the first in a series of papers examining how to apply these techniques to low-dimensional
topology to gather structural insights. We start by focusing on dimensionality reduction, with
further analysis using supervised machine learning techniques [24] and persistent homology [15]
forthcoming.
Low-dimensional topology, and knot theory in particular, is among the most data-rich of mathematical sub-fields. Tabulating data concerning knots is a longstanding tradition dating back to
the 1860s [48]. As computing power improves and people continue to search for answers to fundamental questions in knot theory such as the Jones unknot conjecture [31] and the hyperbolic
volume conjecture [40], people have continued to enlarge our tabulations of known knots. Recently,
Burton tabulated all the prime knots up to and including 19 crossings [9], finding over 350 million
total prime knots. This tabulation was summarized as part of the software package Regina [8] with
published DT-codes as defined in [14]. A separate effort at tabulating large numbers of unique
knots was also recently undertaken by Tuzun and Sikora in their demonstration that no counterexamples to the Jones unknot conjecture exist up to 23 crossings [47]. In their tabulation, well over
10 trillion knot diagrams were considered using distinct methods from Burton’s. While the exact
1

2

JESSE S F LEVITT, MUSTAFA HAJIJ, AND RADMILA SAZDANOVIC

number of distinct knots with a certain number of crossings is still unknown, we do know that this
number grows at an exponential rate as we increase the number of crossings [19]. This ensures that
data-related questions arising from Knot Theory naturally fit into a big data framework.
The first major contribution of this paper is to demonstrate a reliable technique by which manifold learning can be applied to infinite data sets where representative sampling is impossible or
impractical. We describe how to analyze and construct a usable filtration on the infinite set of knots,
where the Jones polynomial is unbounded in degree. The experimental results of this demonstrates
that our Jones invariant data is well approximated by a three dimensional manifold, consistent
across our filtration up to computation limits.
Meaningfully applying dimensionality reduction techniques to our data proved difficult. It is
unknown how to create a representative subsample of Jones polynomial data. Any conclusions
drawn must remain consistent when choosing comparable subsets of the data. Results were sensitive
to the choice of how encode the data for comparison. Using the same approach as was used to find
patterns in the more general coloured Jones polynomial [2, 6, 7, 17, 37, 39] provided data where any
structures proved transient. Fortunately, exactly one model for encoding the data provided results
that were both remarkable and persistent, it is discussed in Section 3.
The requirement that results be persistent across comparable subsets of the data required detailed analysis of how to filter sets of knots into related families. Knot Theory has always driven
researchers to calculate knot invariants and organize them into data tables in a process called knot
tabulation [25, 26]. Originally envisioned as a way to distinguish different atomic properties [48],
modern work has suggested that a classification system could assist in the understanding of glueball particles [20]. Since then a series of systems have been suggested for ordering or relating knots
within these ever expanding tabulations [10, 13].
Upon generating our Jones polynomial data for all knots up to 17 crossings we examined several
methods for organizing the data. We considered the crossing number, Rasmussen s-invariant [43],
signature, unknotting number, and a wide variety of properties intrinsic to the Jones data itself.
As we discuss in Section 5 below, organizing the data by crossing number yields persistent results despite the manner in which the set varies dramatically in both the ratio of alternating to
nonalternating knots present in the sample and the expanding size of the data considered.
The second major result from this study is to demonstrate a new tool for comparing knot
invariants and understanding their structure. Applying dimensionality reduction to the Jones data
using Principal Component Analysis (PCA) [51] as in Figure 1 we see a rich three dimensional
structure with large scale features differentiated by their signature with subfeatures of smaller
‘tendrils’ with as yet unknown significance. We propose the following definition for understanding
the results of dimensionality reduction via PCA, given the discussion of Remark 2.1.

Figure 1. A PCA projection of the Jones polynomial data into 3 dimensions colored by the knot signature to highlight internal structure.

STRUCTURE OF THE JONES POLYNOMIAL

3

Definition 1.1 (Dimension of a polynomial knot invariant, P ). Let k be the value for which the
normalized explained variance of the first k PCA components sums to more than 95%. If this
remains stable across the crossing filtration, then the knot invariant P has dimension k.
Under this definition the Jones polynomial is 3 dimensional, the Z0 polynomial of Bar-Natan
and van der Veen [5] is 2 dimensional and the Alexander polynomial [1] is 1 dimensional. In this
paper we will focus solely on understanding how the Jones polynomial is 3 dimensional under this
definition, while the remaining calculations will be published in upcoming work.
To perform this analysis we relied on the tools from the KnotTheory package [4] for calculating
the Jones polynomial for all knots. The DT codes we used for knots up to 16 crossings were exactly
those in the KnotTheory package, while to calculate data for the 17 crossing knots we added the
DT codes from Burton’s Regina program [9] to the KnotTheory data tables. The PCA calculations
were done using the scikit-learn library [42] and in Mathematica [28]. Finally, knot figures were
generated using Inkscape [29].
2. Background
In this section we briefly provide the reader with an overview of the definitions and notions used
in the article. We begin with the definition of the Jones polynomial, followed by an overview of the
basic properties of the PCA technique.
2.1. The Jones Polynomial. The Jones polynomial [31] and its generalizations [41, 44, 49] play
a fundamental role in low-dimensional topology [32, 40, 41]. The discovery of the Jones polynomial
has led to multiple major discoveries in various areas of low-dimensional topology [3, 12, 21, 33–36].
Understanding the discriminative power of the Jones polynomial, its relations to other classical
invariants of knots and links, as well as the information encoded in its coefficients, conjectured
to be related to the hyperbolic volume of the knot, are important problems in low-dimensional
topology. Furthermore, the coefficients of the Jones polynomial and its generalizations have been
proven to be related to many interesting areas in number theory, and have been the subject of an
extensive research effort [2, 6, 7, 16, 17, 22, 23, 37, 39].
Let K be a knot in S3 . The Jones polynomial, denoted by JK (q), is a Laurent polynomial in
Z[q ±1 ]. The Jones polynomial can be characterized by the requirements that JK (q) = 1, when K
is the unknot, and that it satisfies the following skein relation: (q 1/2 − q −1/2 )JL0 (q) = q −1 JL+ (q) −
qJL− (q). Here L0 , L− and L+ are three oriented link diagrams that are identical everywhere except
at single crossing as appears on the right in Figure 2. The skein relation can be used to compute
the Jones polynomial for any given link L.

Figure 2. The three pictures on the left represent a positive crossing, a smoothing,
and a negative crossing, denoted L+ , L0 and L− respectively. The right picture
represents the same concept, fixed on the top crossing in the trefoil diagram L first
with L+ , a diagram of an unknot with L− next and of the Hopf link with L0 at the
end.

2.2. Principal Component Analysis. Principal Component Analysis (PCA) is one of the most
popular multivariate statistical techniques in big data. PCA is defined as an orthogonal linear
transformation that maps a given data set X to a new orthonormal basis that is aligned with the

4

JESSE S F LEVITT, MUSTAFA HAJIJ, AND RADMILA SAZDANOVIC

core properties of the data. To accomplish this, PCA finds and ranks the linear directions along
which the data has maximal variance.
Given the data set X, where |X| = n, and X ⊂ Rd , the PCA linear transformation is obtained
by computing the eigendecomposition of the covariance matrix K defined by
n
1X
1
K=
(X − x̄)T (X − x̄)
where
x̄ =
xi .
n−1
n
i=1

The matrix K is a symmetric matrix by definition and hence is diagonalizable by an orthogonal
basis. Therefore we can find an orthogonal matrix P , and a diagonal matrix Λ, such that: K =
P ΛP T . Thus, the matrix K defines, via P and Λ, an orthonormal eigensystem {(λi , vi )}di=1 , where
Kvi = λi vi , with
λ1 ≥ λ2 ≥ · · · ≥ λd .

(2.1)

To compute the orthonormal basis in Rd whose directions maximize the variance, PCA finds the
first principal component, which is the direction in Rd along which projections have the largest
variance among all possible directions and then iterates. In particular,
X
X
(x − x̄) · v
and
vi =
max
(x − x̄) · v
(2.2)
v1 = max
v∈S d−1
v∈(Sd−1 ⊥Span{vj }j<i )
x∈X
x∈X
where S d−1 ⊂ Rd in the standard manner. The second principal component vector, v2 , is the
direction that maximizes variance among all directions that are orthogonal to the first principal
component, with similar properties for vi , 1 < i ≤ d. Nontrivially, the vectors {vi }di=1 defined in
(2.2) are precisely the eigenvectors of the covariance matrix, K, we refer the reader to [45] for
further details.
Each eigenvalue, λi , from above explains the variance associated with its paired eigenvector. As
noted in (2.1) the highest eigenvalue denotes to the direction of highest variance in the data, which
is the first principal component, v1 . Correspondingly, the eigenvalues obtained from the covariance
matrix are often referred to as the explained variances. The normalized explained variance λi is
defined by: λi = Pd 1 λi .
j=1

λj

Remark 2.1. The value λi describes the percentage of variance in the ith direction. Intuitively, the
normalized explained variance provides a measure of the degree of importance for each corresponding eigenvector. As these eigenvalues are ordered (2.1), one can also refer to the most important
k PCA directions, where k ≤ d, and these are the vectors v1 , . . . , vk corresponding to the largest k
eigenvalues. These properties of the PCA orthonormal eigensystem {(λi , vi )}di=1 can be used to obtain a heuristic assessment of the dimensionality of distinguishing features within the original data
set X. This heuristic is obtained by measuring the cumulative values of the normalized explained
P
variance Sk := ki λi , where 1 ≤ k ≤ d. In practice, we choose k such that Sk ≥ r where r is a
reasonable percentage value for which the chosen PCA vectors still capture the original data. In
this work we have chosen r to be 0.95.
3. Preparing the Data
Regarding the data sets used in this paper we note the following. Most knots exist in pairs,
(K, mir(K)), such as the left and right handed trefoil knots, where one knot becomes the other
when all positive and negative crossings are switched, so L+ ←→ L− everywhere, giving what is
referred to as the mirror image of the knot. Knot tabulation efforts have generally accepted that it
is not necessary to enumerate both of these paired knots when listing all knots, but many invariants
are sensitive to this choice. For the Jones polynomial Jmir(K) (q) = JK (q −1 ) [31, 38], while for the
signature σ(mir(K)) = −σ(K) [38], and for the Rasmussen s-invariant s(mir(K)) = −s(K) as
well [43].

STRUCTURE OF THE JONES POLYNOMIAL

5

In light of this symmetry, to reduce memory overhead and computation time, as well as to
enhance the clarity of the associated data visualizations, we have chosen to include just one of
either K or mir(K) in our data set. We first chose the embedding where the signature was positive.
If the signature was zero, we then chose K or mir(K), to ensure the Rasmussen s-invariant was
positive. When both the signature and s-invariant are zero, or one was unknown, we chose the
embedding of K or mir(K) for which the most extreme degree was positive (e.g. by choosing
mir(61 ) over 61 as Jmir(61 ) (q) = q 4 − q 3 + q 2 − 2q + 2 − 1q + q12 has extreme degrees of 4 and -2) as
in Table 1. Once a choice between K and mir(K) was made we constructed each point cloud by
the following general method.
Given a finite family of knots F, and a single variable knot polynomial invariant I, we want to
construct a point cloud CFI ⊂ Rn . The procedure we describe here works for any finite set of knots
and any single variable polynomial knot invariant I. The steps for creating this point cloud are as
follows:
(1) For each K ∈ F we compute the polynomial invariant I(K) or I(mir(K)) as in the second
column of Table 1.
(2) Convert each polynomial I(K) to a coefficient vector P (I(K)) - or P (I(mir(K))).
(3) The set of coefficient vectors {P (I(K)) | K ∈ F} are aligned by padding each vector with
zeroes to ensure that the coefficient of q 0 is in a consistent position and all vectors are of
the same length as in the right column of Table 1 and elaborated upon below. Each padded
vector is denoted PF (I(K)).
An example of this method is given for a small set of knots in Table 1. There we present the
choice of embedding for each knot, corresponding Jones polynomial and the resulting vector in the
point cloud of the family of knots up to 6 crossings.
J(K)
q0
K
01
1
( 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0)
mir(31 ) q + q 3 − q 4
( 0, 0, 0, 0, 1, 0, 1, -1, 0, 0, 0)
41
q −2 − q −1 + 1 − q + q 2
( 0, 1, -1, 1, -1, 1, 0, 0, 0, 0, 0)
( 0, 0, 0, 0, 0, 1, 0, 1, -1, 1, -1)
mir(51 ) q 2 + q 4 − q 5 + q 6 − q 7
2
3
4
5
6
mir(52 ) q − q + 2q − q + q − q
( 0, 0, 0, 0, 1, -1, 2, -1, 1, -1, 0)
mir(61 ) q −2 − q −1 + 2 − 2q + q 2 − q 3 + q 4
( 0, 1, -1, 2, -2, 1, -1, 1, 0, 0, 0)
mir(62 ) q −1 − 1 + 2q − 2q 2 + 2q 3 − 2q 4 + q 5
( 0, 0, 1, -1, 2, -2, 2, -2, 1, 0, 0)
63
−q −3 + 2q −2 − 2q −1 + 3 − 2q + 2q 2 − q 3 (-1, 2, -2, 3, -2, 2, -1, 0, 0, 0, 0)
Table 1. Jones polynomials with positive extreme degree and their padded coefficient vectors aligned at q 0 .

Observe that each vector PF (I(K)) does not solely depend on the knot, K, but rather also
depends on the family F. Indeed, the coefficient vectors, P (I(K)) for various knots frequently are
of differing lengths and belong to different Euclidean spaces. Constructing CFI depends explicitly
on the family F, since we padded the shorter vectors in this set to match the longest ones. Even
when two polynomials in a family have the same length coefficient vector as with P (I(mir(51 ))) and
P (I(mir(52 ))) in Table 1 on the left, alignment frequently pads the vectors differently. In order to
obtain the point cloud data CFJ , where F is all knots with at most 6 crossings, we align the vectors
as shown in the right column of Table 1, padding with zeros as necessary.
For generating the data here we used the KnotTheory package [4] to compute the signature and
the Jones polynomial for all knots up to 17 crossings and received a collection of the s-invariants
for all knots up to 15 crossings from Alex Shumakovitch [46].
For each invariant, we then defined coefficient vectors for each polynomial, padding the vectors
with zeroes as needed to align the position of q 0 as done to go from left to right in Table 1.

6

JESSE S F LEVITT, MUSTAFA HAJIJ, AND RADMILA SAZDANOVIC

4. The Filtration Method
In data analysis one commonly has to draw conclusions based on incomplete data sets. Therefore,
one hopes to find those properties that do not evolve, but rather persist unchanged throughout the
data set, suggesting that the conclusions are fundamental to all of the data rather than as a property
of whichever special subset is being considered. To address this issue, we have applied manifold
learning to filtrations of our data, that is, nested sequences of point clouds indexed with respect to
some increasing parameter. This allows us to both detect essential, conjecturally constant, features,
while also detecting those that meaningfully evolve with respect to different parameters.
Definition 4.1. A filtration of a set F is a finite sequence {Fi }ni=1 of nested sets such that:
F1 ⊂ F2 ⊂ · · · ⊂ Fn = F.
For our purposes, let each set Fi be a family of knots, and I be a single variable polynomial
knot invariant. The nested sequence in Definition 4.1 induces a nested sequence on a corresponding
filtration of point clouds, denoted:
(4.1)

CFI 1 ⊂ CFI 2 ⊂ · · · ⊂ CFI n .

Now even though Fj ⊂ Fj+1 , the corresponding vectors PFj (I(K)) and PFj+1 (I(K)) often belong
to different Euclidean spaces, but there is always a natural mapping that sends a point in PFj (I(K))
to the corresponding point in PFj+1 (I(K)). Namely the two vectors can be aligned on the position
of q 0 , padding the necessary zeros in PFj (I(K)) so it is embedded in the same Euclidean space
as PFj+1 (I(K)). Using this mapping we can meaningfully embed a point cloud CFI j inside CFI j+1
whenever Fj ⊂ Fj+1 .
Studying the eigensystems generated by PCA on a nested sequence of point clouds provides
insight on how their principal component vectors and corresponding normalized explained variances
evolve as the size of the point cloud grows. This can provide more information about the distribution
from which this point cloud is drawn. For instance, by considering the relative sizes between
consecutive normalized explained variances from the filtration we get information on the shape of
the point cloud.
There are over 50 distinct well-known invariants used to distinguish knots in tabulations [11].
Most of these naturally define ordered families of knots. One such natural filtration of families of
knots is the one obtained by considering all knots, up to ambient isotopy, with minimal number of
crossings less than or equal to a particular value k. Let Fdke denote the family of all different knots
with crossing number less than or equal to k. We wish to study the point clouds obtained by the
following filtration
(4.2)

Fd3e ⊂ Fd4e ⊂ · · · ⊂ Fdje ⊂ · · · ⊂ Fdke .

When paired with a single variable polynomial knot invariant I, the filtration (4.2) induces a
filtration of point clouds as in (4.1). We will refer to the point cloud filtration induced by (4.2) as
a crossing filtration. To specify the PCA eigensystem obtained at each point
cloud in a crossing

I
filtration we will associate to the point cloud CFdje the PCA eigensystem λi (j), vi (j) .
The second natural filtration on point clouds associated with polynomial knot invariants is induced by the norm. We default to the l2 −norm due to its ease of use and scalability within the
scikit-learn package, but comparisons against other norms did not produce significantly different
results. Given F, a finite family of knots with I, a single variable polynomial knot invariant, then
for any r ∈ R+ , we define
CFI (r) := {p ∈ CFI | kpk ≤ r}.
For a finite sequence of positive real numbers r1 ≤ · · · ≤ rmax , we define the filtration:
(4.3)

CFI dke (r1 ) ⊂ CFI dke (r2 ) ⊂ CFI dke (r3 ) ⊂ · · · ⊂ CFI dke (rmax ) = CFI dke ,

STRUCTURE OF THE JONES POLYNOMIAL

7

where rmax = maxp∈C I

(kpk) . We call the point cloud filtration given in (4.3) the norm filtration

and will denote the PCA eigensystem obtained from CFI dke (rj ) by λi (rj ), vi (rj ) .
Fdke

5. Application to the Jones polynomial
In this section we outline the results of running PCA on point clouds obtained from the Jones
polynomial. In total we use all 9,755,329 knots of at most 17 crossings. In Subsection 5.1 we
discuss how filtering by the l2 -norm illustrates the shape of the point cloud CFJ d17e using the filtration
discussed in Section 4. While in Subsection 5.2 we examine how the crossing filtration illuminates
persistent features of the point cloud. To see if this behavior continues at higher crossing number
we consider the point cloud for two special subfamilies of knots up to 2001 crossings in Section 6.
Our analysis utilizes two primary visualizations of the PCA data. In the first, the relative
importance of each PCA eigenvector can be visualized by plotting the normalized explained variance
as a function of the ordered indices of the eigensystem as discussed in Remark 2.1. Similarly, one
can also plot the cumulative values of the normalized explained variance Sk as a function of k. Our
second visualization follows the trajectory of a sequence of PCA eigensystems λi , vi j across each
step of the filtration by comparing the values of λi and the directions of the principal components
vi . Ideally, all the principal component vectors would overlap across the filtration, we measure their
deviation by using the classical dot product between them:
(5.1)

vi (rj+1 ) · vi (rj ) = cos θi,j kvi (rj+1 )kkvi (rj )k

in the norm filtration;

(5.2)

vi (j + 1) · vi (j) = cos θi,j kvi (j + 1)kkvi (j)k

in the crossing filtration.

Here θi,j is the angle between the ith principal component in the j and j + 1st eigensystem.
5.1. Structure from the Norm Filtration. Having prepped and filtered the data as in Sections 3
and 4, we set up the norm filtration of (4.3) using the radii {r7 , . . . , ri , . . . , r0 }, denoted in Figure 3.
Each radius is chosen to restrict to the central 21i of the point cloud, doubling the number of points
considered with each iteration.

Figure 3. The normalized explained variance and cumulative normalized explained
variance of each principal component in the norm filtration on the Jones polynomial,
plotted on a log scale to highlight the behavior of the most significant components.
Figure 3 illustrates the first type of visualization, where we consider the plotted values of the
normalized explained variance. The left hand set of curves show the normalized explained variance
of each principal component, ordered as in (2.1), while the right hand graph shows the cumulative normalized explained variance. In both graphs the PCA calculation is done on the filtration

8

JESSE S F LEVITT, MUSTAFA HAJIJ, AND RADMILA SAZDANOVIC

CFJ d17e (r7 ) ⊂ CFJ d17e (r6 ) ⊂ · · · ⊂ CFJ d17e (r0 ) with each family denoted by a distinct color. The exponential division of the point cloud is both for eventual contrast with the exponential growth of the
crossing filtration and to ensure that at each step the amount of new data is equal to the amount
of preexisting data.
Two trends quickly appear in the Figure 3. The first principal component becomes more significant as the bounding radius of the point cloud increases, while subsequent components decrease
in prominence. This increase in the first principal component exceeds the decrease in subsequent
components and as a result the cumulative normalized explained variance, Sk , increases for each
k. Following the bound set out in Remark 2.1 we see that for r7 , r6 , and r5 S5 ≥ 0.968, while for
r4 , r3 , and r2 , S4 ≥ 0.959, and then r1 and r0 have S3 ≥ 0.969 and S3 ≥ 0.988 respectively, which
suggests that CFJ d17e approximates a 3-dimensional manifold.
These trends are affirmed by the second type of visualization in Figure 4, where the trajectory
of the first six components of the normalized PCA eigensystem are followed across each step of
the filtration. The left graph illustrates the gradual growth of the first principal component at
the expense of the remaining components as the radius of our point cloud CFJ d17e (ri ) increases.
Quantitatively, we solely note that the maximum relative spread for any of the 3 significant PCA
components is ∼ 91%.
The quality of a filtration is not only reflected in the stability of the λj ’s, we can also measure the
alignment of sequential eigenvectors as in (5.1). On the right hand side of Figure 4 we plot these
angles for principal components 1 ≤ j ≤ 6 across radii 15 < r < 2000. The principal components
stabilize as the filtration radius increases, but two details stand out. First, from r4 to r2 the
variation between important eigenvectors reduces to a stable point. Secondly, the angles between
secondary sequential components begins to stabilize from r2 to r0 . Furthermore, the significance of
the principal component does not appear to correlate directly with the relative degree of stability
across the filtration.

Figure 4. The left hand chart plots the normalized explained variance against the
radius of the norm filtration of CFJ d17e for the first 6 principal components on a log
scale in accordance with our doubling sample size. Here λ1 grows as the size of this
knot family increases, while the λi for i ≥ 2 principally decrease. The figure on
the right provides insight on any trends toward stability in the angle θi,j . In this
notation, the x axis represents the log of the bounding norm of rj , while the y axis
represents the angle θi,j (measured in radians) and the index i is depicted using
different colors.
Figure 4 suggests that in the center of the point cloud, the data spreads fairly evenly in 2
directions, before changing direction between radii of 50-200 and pronouncedly extending out in

STRUCTURE OF THE JONES POLYNOMIAL

9

a single new direction. The cutoffs in the data based on the doubling radii of the point cloud
suggests that the data is disproportionately densely packed towards the center of the distribution
and is sparse towards the extremes. We consider the shape of the data further when discussing
Figure 7.
5.2. Persistent Properties in the Crossing Filtration. Following the same steps we now consider the crossing number filtration following from (4.2). This filtration presents distinct features
from the norm filtration. The number of knots in each step of the filtration increases exponentially [19], so to ensure a sufficient number of data points in the smallest filtration, we only consider
the cases CFJ d11e ⊆ CFJ d12e ⊆ CFJ d13e ⊆ CFJ d14e ⊆ CFJ d15e ⊆ CFJ d16e ⊆ CFJ d17e . The visualization of Figure 5
when compared to Figure 3 presents a strong contrast.

Figure 5. The normalized explained variance and cumulative normalized explained
variance of each principal component in the crossing filtration on the Jones polynomial plotted on a log scale to emphasize the behaviour of the most important
components.
In Figure 5 the normalized explained variances and cumulative normalized explained variances
are essentially indistinguishable. Following the bound set out in Remark 2.1, we find that 0.992 ≥
S3 ≥ 0.988 for every family in the filtration and that S2 < 0.95 except for the 12 and 13 crossing
families, where S2 =0.9507.
˙
We also consider the crossing number filtration analogue of Figure 4 in
Figure 6.
Figure 6 stands in marked contrast to its analogue for the norm filtration. The normalized
explained variance is remarkably stable for the 3 significant components with a maximal relative
spread of ∼ 3.5%, a significant improvement in consistency. The principal components also behave
differently when measuring the angle between vi ∈ CFJ dke and vi ∈ CFJ dk+1e compared to the norm
filtered case. There may be more total variation across the filtration, but that variation is more
orderly, with θi,k < θi+1,k for all k and all significant i, where λi > 0.00001. It is not surprising that
these filtrations have some amount of variation as the minimal dimension of Rn in which CFJ dke can
be embedded strictly increases with k. Of further interest is a mild periodicity in the variation of
θi across k, suggesting that the change in distribution of knots in CFJ dke depends on the parity of k.
The disparity in stabilization behavior between the norm filtration and the crossing number
filtration begs the question of whether there is something special about either one. Looking at
the distribution of norms for CFJ d17e as in the lower right of Figure 7 it becomes immediately
apparent that the norm filtration suffers from some structural deficiencies. Namely, the subfamily
of nonalternating knots has a skewed distribution towards lower norms, while the l2 -norms of
alternating knots favor a broader distribution. We observed in talking about Figure 4 that the angles

10

JESSE S F LEVITT, MUSTAFA HAJIJ, AND RADMILA SAZDANOVIC

Figure 6. The left hand chart shows the explained variance plotted against the
crossing filtration on CFJ d17e . It is remarkably level across the filtration. The right
hand chart shows the stability of the principal components. Notably, the more
significant the component, the more stable it is.
between principal components experienced an inflection and rapid stabilization in the three most
.
.
important PCA components between r4 = 45 and r1 = 215. Additional experimentation has shown
that these characteristics stabilize almost completely for r ≥ 1000. In Figure 7 we see that, for
every family in the crossing filtration, the nonalternating knots contribute an insignificant fraction
of new data points to the PCA calculation by the point where the alternating knot distribution
peaks. Similarly, the tail of this distribution continues stretching as the crossing number increases,
so in each case only a small number of data points are added to the point cloud after an r << rmax
so it is of little surprise that the principal components mostly stabilize after a given point.

Figure 7. The distribution of the l2 -norms (total count vs. norm) for the Alternating (Green), NonAlternating (Blue), and Combined (Grey) knots up to 12, 13,
14, 15, 16, and then 17 crossings when taken left to right, top to bottom.
This dependence on the norm distributions of the alternating and nonalternating knots suggests
that we should also consider these knot classes by themselves. Let CFI dke denote the point cloud
n
of nonalternating knots of at most k crossings built using the single polynomial invariant I, and
CFI dke will denote the analogous point cloud of alternating knots.
a

STRUCTURE OF THE JONES POLYNOMIAL

11

In Figure 8 we first consider the persistence of the PCA eigensystem features under the crossing
number filtration of CFJ d11e ⊂ CFJ d12e ⊂ · · · ⊂ CFJ d17e on alternating knots. The normalized exa
a
a
plained variances on the left of Figure 8, suggests they follow the same general pattern as expressed
.
in Figure 4, but with a value of λ1 = 0.782 and relative spread of ∼ 1.4%.

Figure 8. The left figure plots the normalized explained variance against the radius
of the l2 -norm filtration of CFJ d17e . The figure on the right shows how the PCA bases
a
obtained from the filtration stabilize as we increase the radius. (Note that the larger
the contribution the less the deviation)
Next we consider the persistence of the PCA eigensystem features under the crossing number
filtration of CFJ d11e ⊂ CFJ d12e ⊂ · · · ⊂ CFJ d17e on nonalternating knots, as in Figure 9. Like the
n
n
n
crossing filtration on alternating knots, the PCA eigensystem values for the crossing filtration on
.
nonalternating knots are stable, but with λ1 = 0.728 and relative spread of ∼ 1.6%. It is worth
noting that the normalized explained variances of the alternating knots and nonalternating knots
settle at different values, but their combination, at steadily diverging weights, as illustrated by the
relative proportions in Figure 7, still remains not just consistent as noted by Figure 6, but has
.
even less relative spread with λ1 = 0.766 and relative spread of ∼ 1.0%. Even considering our final

Figure 9. The left figure plots the normalized explained variance of CFJ d17e . The
n
figure on the right shows the how the PCA bases obtained from the filtration change
with crossing number. Note that here the trends are consistent at high crossing
number, but not at low crossing number where a dearth of examples likely lead to
noise.
normalized eigenvalue deemed significant by Remark 2.1, λ3 , we find that the relative spread in all

12

JESSE S F LEVITT, MUSTAFA HAJIJ, AND RADMILA SAZDANOVIC

knots is ∼ 3.5%, while the alternating and nonalternating knots have relative spreads of ∼ 9.2% and
∼ 13% respectively, when considering crossing filtrations for 12 ≤ k ≤ 17 to ensure at least 1000
knots in every filtration. The possible implications of these observations bear further investigation.
6. Examining Jones Structure at Higher Crossing Number
To provide insight into what happens for higher crossing numbers we looked at two subfamilies
of knots whose Jones polynomials were easily computed at higher crossing number. We consider
the torus knots up to 2000 crossings and the positive double twist links of up to 2001 crossings. Let
CFJ dke denote the point clouds of torus knots up to k crossings, and CFJ dke the point clouds of single
T
P
strand positive double twist link knots up to k crossings, which were calculated using [4] and [18]
respectively. The three dimensional PCA projections of CFj d2001e and CFJ d2000e are presented in
T
P
Figure 10 and suggest interesting structures exist. Yet our results are inconsistent with those of
Section 5 and reveal more about the challenges of using manifold learning than they do specifically
about the dimensions of CFJ dke .

Figure 10. PCA projection into three dimensions of the positive double twist link
knots up to 2001 crossings (Left) and the torus knots up to 2000 crossings (Right).
Studying the PCA eigensystems of CFJ d2001e ⊂ R5003 using the top row of Figure 11 it is easy to
P
.
see that CFJ d2001e should not be considered a 5003d manifold. In fact, S4 > 0.969 and S3 = 0.948,
P

which by our heuristic suggests that CFJ d2001e approximates a 4 dimensional manifold. This suggests
P

that this submanifold of CFJ d2001e approximates a higher dimensional manifold than we measured
for CFJ d17e and that the apparent stability in λi ’s seen in Figure 6 might slowly evolve as crossing
number increases.
We investigated this phenomenon further for torus knots. A very different picture emerges
from the analysis of the bottom row of CFJ d2000e in Figure 11. While the left visualization is
T

broadly similar to the results for CFJ d2001e , the right chart displays a significant difference. Here
P
the cumulative normalized explained variance approaches 1 much more slowly with S25 > 0.95 and
taking even longer to reach a stricter restriction used by some of S224 > 0.99.
Two details about CFJ d2000e stand out in contrast to CFJ d2001e . First, CFJ d2000e contains a mere
T

P

T

4501 data points unlike the over 500,000 in CFJ d2001e . Second, while CFJ d2001e
dimensional space,

CFJ d2000e
T

P

lives in a 5003
P

lives in an 2998 dimensional space. It is apparent that these two point

clouds are not directly comparable even though they both are contained in CFJ d2001e . This suggests
that the approximate dimension of a point cloud is dependent on how it is sampled especially
for nonrandom samples. Furthermore, a direct examination of the sparsely populated CFJ d2000e ,
T
supports the idea that for a sample size that doesn’t even double the dimensionality of the space
it is embedded in it is difficult to have dimensions with λi << dim1C I .
F

STRUCTURE OF THE JONES POLYNOMIAL

13

Figure 11. Top row: PCA on double twist link knots up to 2001 crossings. Top
left: λi for component i. Top right: Cumulative explained variance up to component
i. Bottom row: PCA on torus knots up to 2000 crossings. Bottom left: λi for
component i. Bottom right: Cumulative explained variance up to component i.

7. Conclusions and future work
Studying the features of datasets that arise in pure mathematics has distinct challenges from
those one faces when working with real world data. In this paper we have outlined how to utilize
one of the most traditional dimensionality reduction techniques, Principal Component Analysis, to
study point clouds of data in this context. In particular, we introduced the notion of filtrations to
analyze a nested sequence of datasets. The method introduced here is general and applicable to
other scenarios where a conclusion about an infinite dataset is required.
Having explicitly described how this technique can be used to analyze the structure of the Jones
polynomial data, immediate extensions of this work are to study point clouds arising from other
one variable polynomial invariants such as the Alexander polynomial and to investigate the substructures illustrated in Figure 1. In our upcoming works we will use other big data analysis
techniques in the context of data in low-dimensional topology, further outlining how they can be
used to compare numerical and polynomial knot invariants. Additional dimensionality reduction
calculations using ISOMAP on the Z0 polynomial data affirm results obtained using PCA. Preliminary research indicates that persistence homology confirms the existence of the substructures in the
Jones polynomial data that also reflect potential relations of the Jones polynomial and signature.

Acknowledgements
Computation for the work described in this paper was supported by the University of Southern
California’s Center for High-Performance Computing (hpcc.usc.edu). RS was partially supported
by the Simons Collaboration Grant 318086 and NSF DMS 1854705.

14

JESSE S F LEVITT, MUSTAFA HAJIJ, AND RADMILA SAZDANOVIC

References
[1] James W Alexander, Topological invariants of knots and links, Transactions of the American Mathematical
Society 30 (1928), no. 2, 275–306.
[2] Cody Armond and Oliver T Dasbach, Rogers-Ramanujan type identities and the head and tail of the colored
Jones polynomial, arXiv preprint arXiv:1106.3948 (2011).
[3] Dror Bar-Natan and Stavros Garoufalidis, On the Melvin–Morton–Rozansky conjecture, Inventiones mathematicae 125 (1996), no. 1, 103–133.
[4] Dror Bar-Natan, Scott Morrison, and et al, The Knot Atlas, 2011. http://katlas.org.
[5] Dror Bar-Natan and Roland van der Veen, A polynomial time knot polynomial, arXiv preprint arXiv:1708.04853
(2017).
[6] Khaled Bataineh, Mohamed Elhamdadi, and Mustafa Hajij, The colored Jones polynomial of singular knots, New
York J. Math 22 (2016), 1439–1456.
[7] Paul Beirne and Robert Osburn, q-series and tails of colored Jones polynomials, Indagationes Mathematicae 28
(2017), no. 1, 247–260.
[8] Benjamin A. Burton, Ryan Budney, William Pettersson, et al., Regina: Software for low-dimensional topology,
1999. http://regina-normal.github.io/.
[9] Benjamin A. Burton, The next 350 million knots, 2018. http://regina-normal.github.io/data.html.
[10] Jason Cantarella, Allison Henrich, Elsa Magness, Oliver O’Keefe, Kayla Perez, Eric Rawdon, and Briana Zimmer,
Knot fertility and lineage, Journal of Knot Theory and Its Ramifications 26 (201705).
[11] J Cha and C Livingston, Knotinfo: Table of knot invariants, http://www.indiana.edu/ knotinfo (December 29,
2017).
[12] Oliver T Dasbach and Xiao-Song Lin, On the head and the tail of the colored Jones polynomial, Compositio
Mathematica 142 (2006), no. 5, 1332–1342.
[13] Yuanan Diao, Claus Ernst, and Andrzej Stasiak, A partial ordering of knots through diagrammatic unknotting,
Journal of Knot Theory and Its Ramifications 18 (200904).
[14] C. H. Dowker and Morwen B. Thistlethwaite, Classification of knot projections, Topology and its Applications
16 (1983), no. 1, 19–31.
[15] Herbert Edelsbrunner, David Letscher, and Afra Zomorodian, Topological persistence and simplification, Proceedings 41st annual symposium on foundations of computer science, 2000, pp. 454–463.
[16] Mohamed Elhamdadi and Mustafa Hajij, Pretzel knots and q-series, Osaka Journal of Mathematics 54 (2017),
no. 2, 363–381.
[17]
, Foundations of the colored Jones polynomial of singular knots, Bull. Korean Math. Soc (2018).
[18] Mohamed Elhamdadi, Mustafa Hajij, and Masahico Saito, Twist regions and coefficients stability of the colored
jones polynomial, Transactions of the American Mathematical Society 370 (2018), no. 7, 5155–5177.
[19] Claus Ernst and DW Sumners, The growth of the number of prime knots, Mathematical proceedings of the
cambridge philosophical society, 1987, pp. 303–315.
[20] Alessandro Flammini and Andrzej Stasiak, Natural classification of knots, Proceedings of The Royal Society A:
Mathematical, Physical and Engineering Sciences 463 (200702).
[21] Stavros Garoufalidis and Thang TQ Lê, The colored Jones function is q-holonomic, Geometry & Topology 9
(2005), no. 3, 1253–1293.
[22] Mustafa Hajij, The tail of a quantum spin network, The Ramanujan Journal 40 (2016), no. 1, 135–176.
[23]
, The colored Kauffman skein relation and the head and tail of the colored Jones polynomial, Journal of
Knot Theory and Its Ramifications 26 (2017), no. 03, 1741002.
[24] Trevor Hastie, Robert Tibshirani, Jerome Friedman, and James Franklin, The elements of statistical learning:
data mining, inference and prediction, The Mathematical Intelligencer 27 (2005), no. 2, 83–85.
[25] Jim Hoste, The enumeration and classification of knots and links, Handbook of knot theory, 2005, pp. 209–232.
[26] Jim Hoste, Morwen Thistlethwaite, and Jeff Weeks, The first 1,701,936 knots, The Mathematical Intelligencer
20 (1998), no. 4, 33–48.
[27] Mark C Hughes, A neural network approach to predicting and computing knot invariants, arXiv preprint
arXiv:1610.05744 (2016).
[28] Wolfram Research, Inc., Mathematica, Version 12.0, 2019. Champaign, IL.
[29] Inkscape Project, Inkscape, 2011. https://inkscape.org.
[30] Vishnu Jejjala, Arjun Kar, and Onkar Parrikar, Deep learning the hyperbolic volume of a knot, arXiv preprint
arXiv:1902.05547 (2019).
[31] Vaughan FR Jones, A polynomial invariant for knots via von neumann algebras, Fields medallists’ lectures, 1997,
pp. 448–458.
[32] Rinat M Kashaev, The hyperbolic volume of knots from the quantum dilogarithm, Letters in Mathematical Physics
39 (1997), no. 3, 269–275.

STRUCTURE OF THE JONES POLYNOMIAL

15

[33] Mikhail Khovanov, Categorifications of the colored Jones polynomial, Journal of Knot Theory and its Ramifications 14 (2005), no. 01, 111–130.
[34] Thang TQ Lê, The colored Jones polynomial and the A-polynomial of knots, Advances in Mathematics 207
(2006), no. 2, 782–804.
[35] Thang TQ Lê et al., Integrality and symmetry of quantum link invariants, Duke Mathematical Journal 102
(2000), no. 2, 273–306.
[36] Thang TQ Lê, Anh T Tran, and Vu Q Huynh, On the AJ conjecture for knots, Indiana University Mathematics
Journal 64 (1905), no. 4.
[37] Christine Ruey Shan Lee, A trivial tail homology for non-A–adequate links, Algebraic & Geometric Topology 18
(2018), no. 3, 1481–1513.
[38] WB Raymond Lickorish, An introduction to knot theory, Vol. 175, Springer Science & Business Media, 2012.
[39] Jeremy Lovejoy and Robert Osburn, The Bailey chain and mock theta functions, Advances in Mathematics 238
(2013), 442–458.
[40] Hitoshi Murakami, An introduction to the volume conjecture, Interactions between hyperbolic geometry, quantum
topology and number theory 541 (2011), 1–40.
[41] Hitoshi Murakami and Jun Murakami, The colored Jones polynomials and the simplicial volume of a knot, Acta
Mathematica 186 (2001), no. 1, 85–104.
[42] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, Scikitlearn: Machine learning in Python, Journal of Machine Learning Research 12 (2011), 2825–2830.
[43] Jacob Rasmussen, Khovanov homology and the slice genus, Inventiones mathematicae 182 (2010), no. 2, 419–447.
[44] Nicolai Yu Reshetikhin and Vladimir G Turaev, Ribbon graphs and their invariants derived from quantum groups,
Communications in Mathematical Physics 127 (1990), no. 1, 1–26.
[45] Jonathon Shlens, A tutorial on principal component analysis, arXiv preprint arXiv:1404.1100 (2014).
[46] Alex Shumakovitch, Private communication, George Washington University, 2019.
[47] Adam Sikora and Robert Tuzun, Verification of the Jones unknot conjecture up to 23 crossings, 2018.
[48] W. Thomson, On vortex motion, Trans. R. Soc. Edinburgh 25 (1869), 217–260.
[49] Vladimir G Turaev, The Yang-Baxter equation and invariants of links, Inventiones mathematicae 92 (1988),
no. 3, 527–553.
[50] Matt Ward, Using neural networks to classify knots: Data mining and deep learning in knot theory (2018).
[51] Svante Wold, Kim Esbensen, and Paul Geladi, Principal component analysis, Chemometrics and intelligent
laboratory systems 2 (1987), no. 1-3, 37–52.
University of Southern California, Los Angeles, CA USA
E-mail address: jslevitt@usc.edu
KLA corporation, Ann Arbor, MI USA
E-mail address: mustafahajij@gmail.com
North Carolina State University, Raleigh, NC USA
E-mail address: rsazdanovic@math.ncsu.edu

