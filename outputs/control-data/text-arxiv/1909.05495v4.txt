arXiv:1909.05495v4 [math.ST] 17 Feb 2020

OPTIMAL CHOICE OF k FOR k-NEAREST NEIGHBOR
REGRESSION
MONA AZADKIA
Abstract. The k-nearest neighbor algorithm (k-NN) is a widely used
non-parametric method for classification and regression. Finding an optimal k in k-NN regression on a given dataset is a problem that has
received considerable attention in the literature. Several practical algorithms for solving this problem have been suggested recently. The main
result of this paper shows that the value of k obtained by the simple
and quick leave-one-out cross-validation (LOOCV) procedure is optimal
under fairly general conditions.

1. Introduction
Non-parametric regression is an important problem in statistics and machine learning [28, 40, 43]. The k-nearest neighbors algorithm (k-NN) is a
popular non-parametric method of classification and regression. For a given
set of n data points (xi , yi ) ∈ Rd × R, where xi ’s are deterministic measurements and yi ’s are noisy observations, then for a point x ∈ Rd the k-NN
algorithm outputs
1 X
yj
(1.1)
ŷ = mk,n (x) =
k
j∈Nk (x)

as an estimate of m(x) := E[y | x], where Nk (x) is the set of indices of the
k nearest neighbors of x among the xi ’s and E[y | x] denotes the expected
value of the response given that the vector of predictors equals x.
The consistency of k-NN estimator for classification and regression has
been studied by many researchers, some examples are [1, 2, 5, 9, 11–20, 23,
26, 35, 38, 39, 44]. The k-NN estimator with a fixed value of k was first
analyzed in [15]. For k = 1, under mild assumptions the risk of the nearest
neighbor estimator is as twice as the Bayes risk [14]. In general, as long as
k is fixed, the risk of the k-NN estimator does not converge to the Bayes
risk [3]. This is an intuitive result, since for fixed value of k, the variance
of the estimator does not converge to zero. It was proved in [38] that if k
is chosen such that k → ∞ and k/n → 0 as n → ∞, the k-NN estimator is
universally consistent and the risk converges to the Bayes risk. The extra
assumption k/n → 0 is to control the bias of the estimator [38]. Later it was
Key words and phrases. Non-parametric estimation, k-NN regression, non-parametric
regression, cross-validation.
1

2

MONA AZADKIA

shown that for universal consistency we only need k/ log n → 0 as n → ∞
[20].
The problem of finding the optimal growth rate for k and finiding the
convergence rate of the error has been studied by many researchers, some
examples are [3, 4, 8, 10, 20, 21, 24, 25, 27, 32, 33, 36]. As a result of these
efforts, the theoretically optimal value of k is now quite well-understood in
various circumstances. But from a practical point of view, these results are
hard to implement. This is because the theoretically optimal choice of k
often involves knowledge that is not available to the user. For example, it
usually involves the error variance, but we cannot get our hands on it before
solving the regression problem.
To circumvent such issues, various interesting ways of estimating the optimal k from the data have been suggested in recent years [6, 31, 45, 46].
In, [27] the convergence rate of the local risk of the k-NN estimator for a
given query point x has been studied and it was shown that for α > 0
and k = n1/(1+(d/2α)) the rate of convergence in probability is at least
n−1/(2+(d/α)) . It has been shown [3, 21] that if the density of x is bounded
from zero on its support, then the convergence rate of the local risk and the
risk are the same. In the general case, it is interesting to find the optimal
value of k for given query point x. In [31], value of k has been chosen adaptively for each query point x such that the k-NN estimator nearly achieve
the minimax rate at x. This suggested value of k requires a tuning parameter that depends on the unknown intrinsic dimension of the vicinity of x.
In the classification setting, and when we have a sample of unlabeled data,
it has been suggetsed in [6] to use the unlabeled data for estimating the
density of the xi ’s and then use that information for choosing the value of
k. Unfortunately, in many situations, we do not have this extra unlabeled
sample. With a similar idea and for both classification and regression purposes, in [45, 46], value of k has been chosen adaptively for each query point.
Although this work does not require an estimate of the density, it requires
three tuning parameters which are chosen by the user.
The main contribution of this paper is a non-asymptotic error bound
which shows that a very old and simple method of choosing k by a certain
kind of cross-validation, known as leave-one-out cross-validation (LOOCV),
provides an optimal value of k quickly and efficiently. The consistency of this
procedure has been known for a long time [34], but the fact that this method
is able to produce an optimal k was not known before. The advantage of this
simple method is that it does not require estimating the density or choosing
any tuning parameter.
2. Main result
Let x be a deterministic d-dimensional vector and
y = m(x) + ǫ,

OPTIMAL CHOICE OF k FOR k-NEAREST NEIGHBOR REGRESSION

3

where m : Rd → R is a measurable function and ǫ is a mean-zero random
variable that is independent of x. Let µ := m(x). Assume that there exists
a finite constant K such that E(exp(ǫ2 /K)) ≤ 2.
Let ǫ1 , · · · , ǫn be a sampe of i.i.d. copies of ǫ. For n deterministic measurements x1 , · · · , xn , we have a data set of n pairs (x1 , y1 ), · · · , (xn , yn ).
For each i, let Nk (i) be the indices of the k nearest neighbors of xi among
x1 , . . . , xi−1 , xi+1 , . . . , xn , where ties are broken at random. Define
1 X
yj .
m̂k,n (xi ) :=
k
j∈Nk (i)

We define the mean squared error of this estimator as
n

MSE(k) = E[

1X
(m(xi ) − m̂k,n (xi ))2 ],
n
i=1

where the expectation is taken over the ǫi ’s. Note that this is not exactly the
common mean squared error defined in the literature, since we are excluding
xi from its set of nearest neighbors. We are considering this to be the MSE,
since the xi ’s are deterministic, and so there is no concept of a new x from
the distribution of the xi ’s, and using xi in the estimate for µi would cause
overfitting.
Our object of interest is the number
k∗ := argmin1≤k≤n−1 MSE(k).

(2.1)

Of course, we cannot directly compute k∗ from the data since the function
m is unknown. Instead, we produce a surrogate. Define
2
n 
1X
1 X
yj ,
f (k) :=
yi −
n
k
i=1

j∈Nk (i)

and let
k̃ := argmin1≤k≤n−1 f (k).

(2.2)

Note that k̃, unlike k∗ , is computed from the data. The intention is to use k̃
as the chosen value of k in k-NN regression. This procedure for selecting k is
known as leave-one-out cross-validation (LOOCV). The following theorem,
which is the main result of this paper, shows that
r

log n
∗
|MSE(k ) − MSE(k̃)| = O
n
when K and d are fixed. One of the main strengths of this theorem is that
no other condition is needed.
Theorem 2.1. Let K, k∗ , and k̃ be as above and µ = (µ1 , · · · , µn ) where
µi = m(xi ). Then there are positive constants A, B and C depending on d

4

MONA AZADKIA

and K, such that for any t ≥ 0,
P(|MSE(k∗ ) − MSE(k̃)| ≥ t) ≤
4n exp(−n min{At2 , Bt}) + 4n exp(−Cn2 t2 /kµk2 ).
A remarkable consequence of the above theorem is that the choice of k by
LOOCV adapts automatically to the smoothness of the regression function
m, because the bound on the right does not depend on the smoothness of
m.
In many situations, MSE(k∗ ) is much greater than n−1/2 (log n)1/2 . For example, by [28, Theorem 3.2], for Lipschitz functions with bounded support,
the lower minimax rate of convergence (in terms of MSE) is O(n−2/(2+d) )
and for d ≥ 3. In such cases, this result implies that MSE(k∗ )/MSE(k̃) → 1
as n → ∞.
3. Proof
For a matrix A, recall that the 2-norm kAk2 and the Frobenius norm
kAkF are defined as
kAk2 = sup kAxk,

kAkF =

kxk=1

X
i,j

a2ij

1/2

.

Throughout this proof, γd will denote any constant that depends only on d.
The value of γd may change from line to line.
Let µi := m(xi ), and ǫ = (ǫ1 , . . . , ǫn ) and µ = (µ1 , . . . , µn ). By writing
yi = µi + ǫi , we have
2
n 
1 X
1X
yj
yi −
f (k) =
n
k
i=1

=

=

1
n
1
n

n 
X
i=1

j∈Nk (i)

2
1 X
yj
µ i + ǫi −
k

n 
X
i=1

j∈Nk (i)

2


1 X
1 X
2
yj + ǫi + 2ǫi µi −
yj .
µi −
k
k
j∈Nk (i)

j∈Nk (i)

Since the ǫi ’s are independent with mean zero, taking expectation on both
sides gives
E[f (k)] = E[ǫ2 ] + MSE(k).

(3.1)

Define g(k) := E[f (k)]. By definition of k∗ , MSE(k∗ ) ≤ MSE(k) for all k.
In particular, MSE(k∗ ) ≤ MSE(k̃), which implies that g(k∗ ) ≤ g(k̃). Also
by definition of k̃, we have f (k̃) ≤ f (k∗ ). Putting these two together, we

OPTIMAL CHOICE OF k FOR k-NEAREST NEIGHBOR REGRESSION

5

get
P(|MSE(k∗ ) − MSE(k̃)| ≥ t)
= P(g(k̃) − g(k∗ ) ≥ t)
≤ P(g(k̃) − g(k∗ ) ≥ t + f (k̃) − f (k∗ ))
≤ P(|g(k∗ ) − f (k∗ )| ≥ t/2) + P(|g(k̃) − f (k̃)| ≥ t/2)
≤2

n−1
X

P(|f (k) − g(k)| ≥ t/2).

k=1

Thus, the proof will be complete if we can prove the following lemma.
Lemma 3.1. There are positive constants A, B and C depending on d and
K such that for any 1 ≤ k ≤ n − 1 and any t > 0,
2 ,Bt}

P(|f (k) − g(k)| ≥ t) ≤ 2e−n min{At
Define a nonsymmetric n × n matrix

 1
0
bij :=

−1/k

+ 2e−Cn

2 t2 /kµk2

.

B = [bij ] as
i = j,
j 6∈ Nk (i),
j ∈ Nk (i).

(3.2)

Let A = [aij ] = BT B/n. Then we can rewrite f (k) in the following form:
f (k) = (ǫ + µ)T A(ǫ + µ).

(3.3)

Using the triangle inequality, we have
|f (k) − g(k)| = |ǫT Aǫ − E[ǫT Aǫ] + 2ǫT Aµ|
≤ |ǫT Aǫ − E[ǫT Aǫ]| + 2|ǫT Aµ|.
Therefore it is enough to find probability tail bounds on |ǫT Aǫ − E[ǫT Aǫ]|
and |ǫT Aµ|. To find such bounds we need to have bounds on the Frobenius
norm and the 2-norm of A. The following lemmas give such bounds.
Lemma 3.2. For the matrix A defined above,
γd
kAk2F ≤ ,
n
where γd is a constant that only depends on d.

(3.4)

Lemma 3.3. For the matrix A defined above,
γd
kAk2 ≤ ,
n
where γd is a constant that only depends on d.

(3.5)

We will prove these lemmas later.
Proof of Lemma 3.1. Throughout this proof as usual, γd denotes any constant that depends only on d, and c will denote any universal constant.

6

MONA AZADKIA

First, let us obtain a tail bound for |ǫT Aǫ − E[ǫT Aǫ]|. By the Hanson–
Wright inequality [37] and Lemmas 3.2 and 3.3, we have
P(|ǫT Aǫ − E[ǫT Aǫ]| ≥ t)



t
t2
,
≤ 2 exp −c min
K 2 kAk2F KkAk2

 2

t
t
≤ 2 exp −n min
,
.
K 2 γd K1 γd

(3.6)

An easy computation gives
2

n
1 X
1X
ǫj
E ǫi −
E[ǫ Aǫ] =
n
k
i=1
j∈Nk (i)


1
=
1+
E(ǫ2 ).
k
T

(3.7)

Putting (3.7) and (3.6) together gives us
P(|ǫT Aǫ − E[ǫT Aǫ]| ≥ t) = P(|ǫT Aǫ − E[ǫT Aǫ]| ≥ t)

 2

t
t
,
≤ 2 exp −n min
. (3.8)
K 2 γd Kγd
2

Next, we obtain a tail bound for |ǫT Aµ|. Remember that E[eǫi /K ] ≤ 2 and
therefore ǫi ’s are sub-Gaussian. Then by the equivalent properties of subGaussian random variables [41], there exist a constant C that only depends
2
on K such that E[eλǫ ] ≤ eλ C/2 for all λ. Then by using the Hoeffding bound
for sub-Gaussian variables [42, Proposition 2.5], we have
X


n X
n
T
aji µi ǫj ≥ t
P(|ǫ Aµ| ≥ t) = P
j=1

i=1

t2
Pn P n
≤ 2 exp −
2C j=1 ( i=1 aji µi )2

Note that

n X
n
X
j=1

i=1

aji µj

2

≤ kAk22 kµk2 .

Inequalities (3.9) and (3.10) together give

T
P(|ǫ Aµ| ≥ t) ≤ 2 exp −

Ct2
2kAk22 kµk2

!

.

(3.9)

(3.10)



.

(3.11)

Therefore by Lemma 3.3,


Cn2 t2
.
P(|ǫT Aµ| ≥ t) ≤ 2 exp −
γd kµk2

(3.12)

OPTIMAL CHOICE OF k FOR k-NEAREST NEIGHBOR REGRESSION

7

Combining (3.8) and (3.12), we get
t
t2
,
P(|f (k) − g(k)| ≥ t) ≤ 2 exp −n min
2
K γd Kγd


2
2
Cn t
.
2 exp −
γd kµk2






+


Proof of Lemma 3.2. Let bi be the i-th row of matrix B. Then
kAk2F =
=

1 X
hbi , bj i2
n2
i,j


n
1
1
1 XX
hbi , bj i2 .
1+
+ 2
n
k
n
i=1 j6=i

For any distinct i, j,
1
1
hbi , bj i = − [1{i∈Nk (j)} + 1{j∈Nk (i)} ] + 2 |Nk (i) ∩ Nk (j)|,
k
k
and therefore
|hbi , bj i| ≤

2
.
k

(3.13)

This shows that for any i,
X

hbi , bj i2 ≤

j6=i

4
|{j : hbi , bj i 6= 0}|.
k2

(3.14)

But if hbi , bj i =
6 0, then
({i} ∪ Nk (i)) ∩ ({j} ∪ Nk (j)) 6= ∅.

(3.15)

By definition |Nk (i)| = k, and for any ℓ, by [28, Corollary 6.1] there are at
most γd k indices j such that ℓ ∈ Nk (j). Therefore for any i,
|{j : hbi , bj i 6= 0}| ≤ γd k(k + 1).
This gives the required bound on kAk2F .

(3.16)


8

MONA AZADKIA

Proof of Lemma 3.3. Take any x such that kxk = 1. Then by [28, Corollary
6.1],
2

kBxk

=

n
X

hbi , xi2

i=1
n
X

≤ 2

x2i

+2

n 
X
1 X
i=1

i=1

k

j∈Nk (i)

xj

2

n
2X X 2
xj
≤ 2kxk +
k
2

= 2kxk2 +
2

2
k

i=1 j∈Nk (i)
n
X
X

x2j

j=1 i:j∈Nk (i)

≤ γd kxk .
Therefore kBk2 ≤ γd and hence kAk2 ≤ γd /n.



An R language package knnopt will soon be made available on the CRAN
repository.
Acknowledgement
The author is very grateful to her advisor Sourav Chatterjee for his constant encouragement and insightful conversations and comments.
References
[1] Beck, J.(1979) The exponential rate of convergence of error for kn NN non-parametric regression and decision. Problems of Control and
Information Theory. no. 8 (1979), 303-311.
[2] Bhattacharya, P. K. and Mack, Y. P.(1987) Weak convergence of
knn density and regression estimators with varying k and applications.
Ann. Statist.,15 no. 3, 976-994
[3] Biau, G. and Devroye, L. (2015). Lectures on the Nearest Neighbor
Method. Springer Series in the Data Sciences. Springer, Cham. (2015)
[4] Biau, G.; Cérou, F. and Guyader, A. (2010). Rates of convergence of the functional k-nearest neighbor estimate. IEEE Trans. Inform. Theory. 56 no. 4 (2010), 2034–2040.
[5] Bickel, P. J. and Breiman, L. (1983) Sums of functions of nearest
neighbor distances, moment bounds, limit theorems and a goodness of
fit test. Ann. Probab. 11 no. 1, 185-214.
[6] Cannings, T. I.; Berrett, T. B and Samworth, R. J. (2019). Local
nearest neighbour classification with applications to semi-supervised
learning. Ann. Statist. (to appear) (2019).

OPTIMAL CHOICE OF k FOR k-NEAREST NEIGHBOR REGRESSION

9

[7] Celisse, A. and Mary-Huard, T.(2018). Theoretical Analysis of
Cross-Validation for Estimating the Risk of the k-Nearest Neighbor
Classifier. J. Mach. Learn. Res. 19 no. 58 (2018), 1–54.
[8] Chaudhuri, K. and Dasgupta, S.(2014). Rates of convergence for
nearest neighbor classification. Advances in Neural Information Processing Systems (2014) 27 3427–3445
[9] Cheng, E. C. Strong consistency of nearest neighbor regression function estimators. J. Multivariate Anal. 15 no. 1 (1984), 63–72.
[10] Cheng, P. E. (1995). A note on strong convergence rates in nonparametric regression. Statistics and Probability Letters, 24 (1995), 357364.
[11] Collomb, G. (1979) Estimation de la regression par la méthode des k
points les plus proches: propriétés de convergence ponctuelle. Comptes
Rendus de l’Académie des Sciences de Paris, 289 (1979), 245-247
[12] Collomb, G.(1980) Estimation de la regression par la méthode des k
points les plus proches avec noyau. Lecture Notes in Mathematics 821,
Springer Verlag, Berlin. 159-175
[13] Collomb, G.(1981) Estimation non parametrique de la regression: revue bibliographique.International Statistical Review, 49 75-93
[14] Cover, T. M.(1968) Estimation by the nearest neighbor rule.IEEE
Trans. Inf. Theory., 14 no. (1968), 50-55
[15] Cover, T. and Hart, P.(1967). Nearest neighbor pattern classification. IEEE Trans. Inform. Theory, 13 no. 1 (1967), 21–27.
[16] Devroye, L. (1978a) The uniform convergence of nearest neighbor
regression function estimators and their application in optimization.
IEEE Trans. Inf. Theory., 24 no. 2 (1978), 142-151
[17] Devroye, L.(1981) On the almost everywhere convergence of nonparametric regression function estimates. Ann. Statist., 9 no. 6 (1981),
1310-1319
[18] Devroye, L. (1982). Necessary and Sufficient Conditions For The
Pointwise Convergence of Nearest Neighbor Regression Function Estimates. Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete., 61 no. 4 (1982), 467–481.
[19] Devroye, L. and Györfi, L. (1985). Nonparametric Density Estimation: The L1 View. Wiley, New York. (1985)
[20] Devroye, L.; Györfi, L; Krzyzak, A and Lugosi, G.(1994). On
the Strong Universal Consistency of Nearest Neighbor Regression Function Estimates. Ann. Statist. 22 no. 3 (1994), 1371–1385.
[21] Döring, M.;Györfi, L. and Walk, H. (2017). Rate of convergence
of k-nearest-neighbor classification rule. J. Mach. Learn. Res. 18 (2017)
[22] Fan, J. (1993) Local Linear Regression Smoothers and Their Minimax
Efficiencies. Ann. Statist. 21 no. 1, 196–216.
[23] Fix, E. and Hodges, J.L.(1989). Discriminatory analysis nonparametric discrimination: Consistency properties. Internat. Statist. Rev.
(1989) 57 238–247

10

MONA AZADKIA

[24] Gadat, S.; Klein, T. and Marteau, C. (2016) Classification In
General Finite Dimensional Spaces With The k-Nearest Neighbor Rule.
Ann. Statist. 44 no. 3, 982–1009.
[25] Giné, E. and Guillou, A.(2002). Rates of strong uniform consistency for multivariate kernel density estimators. Ann. Inst. H. Poincare
Probab. Statist. (2002) 38 907–921
[26] Guerre, E. (2000) Design Adaptive Nearest Neighbor Regression Estimation. J. Multivariate Anal. 75 no. 2, 219–244.
[27] Györfi, K. C. (1981) The Rate of Convergence of kn -NN Regression
Estimates and Classification Rules. IEEE Trans. Inform. Theory. 27
no. 3, 362–364.
[28] Györfi, L., Kohler, M., Krzyźak, A. and Walk, H. (2002). A
Distribution-Free Theory of Non-parametric Regression. Springer.
[29] Hall, P. Marron, J. S. Neumann, M. H. and Titterington, D.
M. (1997) Curve Estimation When The Design Density Is Low. Ann.
Statist. 25 no. 2, 756–770.
[30] James, G. Witten, D. Hastie, T. and Tibshirani, R. (2013) An
Introduction to Statistical Learning : with Applications in R. Springer.
[31] Kpotufe, S.(2011). k-NN Regression Adapts to Local Intrinsic Dimension. Advances in Neural Information Processing Systems 24(2011),
729–737
[32] Krzyzak, A. (1986) The rates of convergence of kernel regression estimates and classification rules. IEEE Trans. Inform. Theory. 32 no. 5,
668–679.
[33] Kulkarni, S.R. and Ponser, S.E. (1995) Rates of Convergence of
Nearest Neighbor Estimation Under Arbitrary Sampling. IEEE Trans.
Inform. Theory. 41 no. 4, 1028–1039.
[34] Li, K. C. (1984) Consistency For Cross-validated Nearest Neighbor
Estimates in Non-parametric Regression. Ann. Statist. 12 no. 1, 230–
240.
[35] Mack, Y. P. (1981). Local properties of knearest neighbor regression
estimates. SIAM Journal on Algebraic and Discrete Methods, 2 no. 3
(1981), 311-323
[36] Mack, Y. P.(1983). Rate of strong uniform convergence of k-NN density estimates. J. Statist. Planning and Inference (1983) 8 185–192
[37] Rudelson, M. and Vershynin, R. (2013). Hanson-Wright Inequality and sub-Gaussian Concentration. Electron. Commun. Probab.,
18(2013)
[38] Stone, C. J.(1977). Consistent Nonparametric Regression. Ann.
Statist. 5 no. 4 (1977), 595-620.
[39] Stute, W.(1994). Asymptotic Normality of Nearest Neighbor Regression Function Estimates. Ann. Statist. 12 no. 3 (1984), 917–929.
[40] Tsybakov, A. B.(2004). Introduction to nonparametric estimation.
Revised and extended from the 2004 French original. Translated by
Vladimir Zaiats. Springer Series in Statistics. Springer, New York

OPTIMAL CHOICE OF k FOR k-NEAREST NEIGHBOR REGRESSION

11

(2009)
[41] Vershynin, R. (2012). Introduction to the non-asymptotic analysis of
random matrices Cambridge University Press. (2012)
[42] Wainwright, M. J. (2019). High-Dimensional Statistics: A NonAsymptotic Viewpoint. Cambridge University Press. (2019)
[43] Wassermann, L.(2006). All of nonparametric statistics. Springer
Texts in Statistics Springer, New York (2006)
[44] Zhao, L. C.(1987). Exponential bounds of mean error for the nearest
neighbor estimates of regression functions. J. Multivariate Anal., 21 no.
1 (1987), 168–178
[45] Zhao, P. and Lai, L.(2019). Minimax regression via adaptive nearest
neighbor. IEEE Intl. Symposium on Inform. Theory. (2019)
[46] Zhao, P. and Lai, L.(2019). Minimax rate optimal adaptive nearest
neighbor classification and regression. (2019)
Department of Statistics
Stanford University
Sequoia Hall, 390 Jane Stanford Way
Stanford, CA 94305
mazadkia@stanford.edu

