Cost-aware Multi-objective Bayesian optimisation
Majid Abdolshah, Alistair Shilton, Santu Rana, Sunil Gupta, Svetha Venkatesh

Abstract
The notion of ‚Äúexpense‚Äù in Bayesian optimisation generally
refers to the uniformly expensive cost of function evaluations
over the whole search space. However, in some scenarios,
the cost of evaluation for black-box objective functions is
non-uniform since different inputs from search space may incur different costs for function evaluations. We introduce a
cost-aware multi-objective Bayesian optimisation with nonuniform evaluation cost over objective functions by defining cost-aware constraints over the search space. The costaware constraints are a sorted tuple of indexes that demonstrate the ordering of dimensions of the search space based
on the user‚Äôs prior knowledge about their cost of usage. We
formulate a new multi-objective Bayesian optimisation acquisition function with detailed analysis of the convergence that
incorporates this cost-aware constraints while optimising the
objective functions. We demonstrate our algorithm based on
synthetic and real-world problems in hyperparameter tuning
of neural networks and random forests.

Introduction
Bayesian optimisation is a well-known algorithm for optimising black-box and expensive to evaluate functions.
Multi-objective Bayesian optimisation is a generalised form
with multiple conflicting objectives to be optimised simultaneously (Khan, Goldberg, and Pelikan 2002). In such scenarios, the optimiser is seeking for a set of Pareto optimal outcomes often called the Pareto front. Finding Pareto
front is an expensive procedure. The notion of expense in
single/multi-objective Bayesian optimisation refers to uniform cost of function evaluations. In some real-world situations, however, the cost of function evaluation may not
be uniform because of differential costs across inputs in the
search space.
As an example, consider designing steel using Nickel and
Chromium as two key minority (< 20%) ingredients in the
mix where the rest is Iron. We may consider this differential component cost: ‚ÄúNickel is very expensive compared to
Chromium‚Äù and we would like to find steel with high yield
strength. A plain Bayesian optimisation will progress without awareness of the costs and may sample anywhere in the
Copyright c 2020, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

Cost ‚àí awareness
No cost ‚àí awareness

10

Strength

High

8
Chromium (g.)

arXiv:1909.03600v1 [cs.LG] 9 Sep 2019

Applied Artificial Intelligence Institute (A2 I2 ), Deakin University
{majid, alistair.shilton, santu.rana, sunil.gupta, svetha.venkatesh}@deakin.edu.au

6
Low

4
2
0

0

2

4
6
Nickel (g.)

8

10

Figure 1: Searching for optimal alloy composition to maximise strength, given component Nickel is more expensive
than Chromium. Optimisation paths with and without costawareness are shown. ‚Äú‚Ä¢‚Äù are initial observation and ‚Äú√ó‚Äù
indicates the remaining points.

search space. The judicious approach is to start with small
amounts of the expensive component (Nickel) initially and
gradually increase it if the user is not satisfied with solutions
obtained with higher quantities of the cheaper component
(Chromium). The optimisation path in a single-objective
case with and without awareness of costs is shown in Figure
1. Both paths found the optimum, but the cost-aware path
uses a much lower amount of Nickel (32.1 g ) compared to
the vanilla option with no cost-awareness (54.1 g).
Such non-uniform cost of function evaluation has not
been investigated in the context of Bayesian optimisation.
The closest related works investigate budget constraints
leading to the need to find solutions within specified number of iterations (Hoffman, Shahriari, and Freitas 2014;
Lam, Willcox, and Wolpert 2016; Li and Yao 2019). This
problem is quite different and the solutions do not translate.

There are studies based on evolutionary methods related to
cost-effective search space optimisation but these are out of
the context in Bayesian optimisation and our proposed problem since their precise definition of cost vary for each case
of study such as cache allocation and assignment review (Li
et al. 2017; Wang et al. 2017).
To formalise this notion of non-uniform evaluation cost of
objective functions, we define a new constraint on the search
space (independent of the objective space) based on user‚Äôs
prior knowledge . This knowledge is of the form: ‚Äúdimension i of the search space is more expensive than dimension
j‚Äù. We term these as cost-aware preferences over the input
domain. Our goal is to formulate a solution to incorporate
non-uniform cost of functions evaluation through a Costaware Multi-objective Bayesian Optimisation (CA-MOBO)
acquisition function. Our motivation follows:
1. Initial optimiser suggestions should avoid expensive inputs in search space since a desirable solution may be possible with a cheaper combination of inputs - as an example, Figure 1 shows a cost-aware optimiser starting from
a cheap combination.
2. As optimisation progresses, the influence of cost-aware
constraints must diminish to ensure the optimiser is able
to eventually find all Pareto front solutions.
Our proposed acquisition function has two criteria: Chebyshev scalarisation for objective functions to ensure the solutions satisfy Pareto optimality, and a cost function as a
component of the acquisition function that incorporates the
user‚Äôs prior knowledge of the search space. Our algorithm
(CA-MOBO) favors solutions that comply with the costaware constraints whilst finding ‚Äúgood‚Äù quality solutions in
the optimisation process.
Our main contributions are:

Background
Gaussian Processes
We briefly review Gaussian process (GP) (Rasmussen and
Williams 2006). Given that X ‚äÇ RN is compact, a Gaussian
process GP(¬µ, K) is a distribution on the function space
f : X ‚Üí R defined by mean ¬µ : X ‚Üí R (assumed
zero without loss of generality) and kernel (covariance) K :
X √ó X ‚Üí R. If f (x) ‚àº GP(0, K(x, x0 )) then the posterior
of f given D = {(xj , yj ) ‚àà RN √ó R|yj = f (xj ) + ,  ‚àº
2
2
0
N (0, œÉnoise
), j ‚àà Z+
N } is f (x)|D ‚àº N (¬µ(x), œÉ (x, x )),
where:
‚àí1
2
¬µ (x) = kT (x) K + œÉnoise
I
y
‚àí1
2
0
0
T
2
k (x0 )
œÉ (x, x ) = K (x, x ) ‚àí k (x) K + œÉnoise
I
(1)
and y, k(x) ‚àà R|D| , K ‚àà R|D|√ó|D| , k(x)j = K(x, xj ),
Kjk = K(xj , xk ). Typically in Bayesian optimisation, the
black-box, expensive to evaluate objective function is modeled as a draw from a Gaussian process.

Multi-Objective Optimisation
Multi-objective Optimisation (MOO) problem is defined as:
argmax f (x)

where the components of f : X ‚äÇ RN ‚Üí Y ‚äÇ RM represent the M distinct objectives fi : X ‚Üí R. X and Y are
called design space and objective space, respectively. These
objectives must be optimised simultaneously. A Paretooptimal solution is a point x? ‚àà X for which it is not possible to find another solution x ‚àà X such that fi (x) > fi (x? )
for all objectives fi , ‚àÄi ‚àà Z+
M . The set of all Pareto optimal
solutions is called the Pareto set (Deb 2001):
X? = { x? ‚àà X| @x ‚àà X : f (x)  f (x? )}
0

‚Ä¢ Construction of a new Bayesian optimisation framework
to incorporate non-uniform cost of function evaluations;
‚Ä¢ Design of a new acquisition function that scales linearly
with the number of objective functions. This is significant
as comparing to hypervolume based methods, this algorithm is able to tackle many-objective problems;
‚Ä¢ Definition of regret for this problem, and theoretical proof
that it is upper bounded; and,
‚Ä¢ Demonstration of the method on real and synthetic experiments.

Notation
Z+ = {1, 2, . . .}, Zn = {0, 1, . . . , n ‚àí 1}, and Z+
n =
{1, 2, . . . , n}. X is the search space, D is the set of observations and R is the set of real numbers. |X| is the cardinality
of the set X. Tuples (ordered sets) are denoted A, B, C, . . ..
Column vectors are bold lower case a, b, c, . . . and matrices
bold upper case A, B, C, . . .. Element i of vector a is ai , and
element i, j of matrix A is Ai,j (all indexed i, j = 1, 2, . . .).

(2)

x‚ààX

0

0

(3)
yi0

where y  y (y dominates y ) means y 6= y , yi ‚â• ‚àÄi,
and y  y0 means y  y0 or y = y0 . Given observations
D = {(xj , yj ) ‚àà RN √óRM |yj = f (xj )+, i ‚àº N (0, œÉi2 )}
of f the dominant set:
D‚àó = { (x‚àó , y‚àó ) ‚àà D| @ (x, y) ‚àà D : y  y‚àó }

(4)

is the most optimal subset of D (in the Pareto sense).

Multi-objective Bayesian Optimisation
Multi-objective Bayesian Optimisation (MOBO) is an iterative optimisation algorithm designed to simultaneously
optimise objective functions that are black-box and expensive to evaluate. At every iteration a sample is selected
by maximising a cheap acquisition function Œ±t : X ‚Üí
R constructed based on a model of f , given the previous observations D (Brochu, Cora, and De Freitas 2010;
Khan, Goldberg, and Pelikan 2002). MOBO aims to obtain
the Pareto front in least number of function evaluations.
Generally, in MOBO, three approaches are considered to obtain the Pareto front. (a) Predictive Entropy
Search (HernaÃÅndez-Lobato et al. 2016; Garrido-MerchaÃÅn
and HernaÃÅndez-Lobato 2019) finds the most expected solution to reduce the entropy of posterior estimate of the Pareto

set. (b) Dominated Hypervolume Improvement (S-metric)
(Zitzler 1999; Emmerich, Deutz, and Klinkenberg 2011;
Abdolshah et al. 2018) maximises the dominated hypervolume (the volume of points in functional space above
the Pareto front, with respect to a given reference point
z ‚àà RM ). (c) Scalarisation (Boyd and Vandenberghe 2004)
is a standard technique for finding Pareto optimal points by
transforming a MOO problem into a single objective optimisation problem S : RM ‚Üí R.
Many different types of scalarisation functions have been
studied in multi-objective optimisation problems (Miettinen and MaÃàkelaÃà 2002; Chugh 2019). The most simple
form of scalarised functions for a multi-objective optimisation problem is weighted sum of the objective functions.
The weighted sum combines different objectives linearly
and has been widely used (Miettinen 2012). However, it
fails for non-convex regions of the Pareto front (Emmerich
and Deutz 2018). Chebyshev scalarisation (Borwein and
Zhuang 1993) was introduced to overcome this limitation
of weighted sum scalarisation function. Chebyshev scalarisation function is defined as:
m=1

x‚àóŒ∏ = argmax SŒ∏ (f (x))

(6)

x‚ààX

Generally, MOBO models the objective functions fi , ‚àÄi ‚àà
Z+
M by sampling from M independent GP posteriors
GP 1...M (¬µt (x), œÉ t (x)). Inspired by (Srinivas et al. 2009)
and MOBO modeling of objective functions, we can define
the scalarised GP-UCB as:
p
Q(x, Œ∏t ) = SŒ∏t (¬µt‚àí1 (x) + Œ≤t œÉ t‚àí1 (x)) = ...


p
M

t‚àí1
Œ≤t œÉ m
(x) ‚àí Rm
(7)
= min Œ∏m ¬µt‚àí1
m (x) +
m=1

t‚àí1

M

SŒ∏ (f (x)) = min Œ∏m (f (x)m ‚àí Rm )

that maximises qt (x), i.e. xt = argmaxx‚ààX qt (x). Recently, an scalarised modification of GP-UCB has been introduced (Paria, Kandasamy, and PoÃÅczos 2018). The core
idea of scalarised GP-UCB follows from (Roijers et al. 2013;
Zintgraf et al. 2015) and the assumption of SŒ∏ (f (x)) monotonically increasing in all coordinates. Given the mentioned
assumptions and Proposition 1, optimising SŒ∏ (f (x)) as a
maximisation problem returns a single optimal point lying
on the Pareto front:

(5)

where R = [R1 , . . . , RM ] is a preferred reference point and
Œ∏ is the weight vector. Œ∏ can be selected randomly or sampled based on a distribution such as UŒ∏ .
Proposition 1 For a given set of mutually non-dominated
solutions (e.g., a Pareto front) in objective space RM , for
every non-dominated point such as x0 there exists a set of
weight vectors for a Chebyshev scalarisation, that makes
this point a maximum of a Chebyshev scalarisation problem provided that the reference point R is properly selected.
As explained and proved in (Emmerich and Deutz 2018),
Proposition 1 ensures that by modification of the weight vectors, all points of the Pareto front are also a solution of (5).
As a result, in Chebyshev scalarising function, there exist
theoretical results stating that the solutions of (5) will be at
least weakly Pareto optimal for any weighting vector (Emmerich and Deutz 2018).
In this study we are defining a new cost-aware acquisition
function for multi-objective Bayesian optimisation based on
the Chebyshev scalarisation function. We will formulate the
new algorithm in the next section.

¬µ (x) and œÉ t‚àí1 (x) are M dimensional vectors denoting
the posterior means and variances at x for the M objectives
respectively at iteration t ‚àí 1. Having defined scalarised GPUCB, we now introduce CA-MOBO acquisition function.

CA-MOBO Acquisition Function
In our proposed problem, for a sample input vector as x ‚àà X,
xi and xj , i 6= j may incur different costs for the optimiser
based on the cost-aware constraints defined by user. In order
to calculate the incurred cost of objective function evaluation
for input x, we first formulate cost-aware constraints.
Definition 1 (Cost-Aware Constraints) Let
I
=
0
0
(i1 , i2 , ..., ik )|{i1 , i2 , ..., ik } ‚äÇ Z+
,
i
=
6
i
‚àÄk
=
6
k
k
N k
be a cost-aware constraint over k dimensions of the search
space (1 ‚â§ k ‚â§ N ). Then selecting x(ij ) value as a input
from dimension ij of the search space is more expensive
than selecting the same value of x from dimension ij+1 of
the search space given that x1...k are in the same normalised
range and j ‚àà Z+
k‚àí1 .
Based on Definition 1, cost-aware constraints are a sorted tuple of indexes that demonstrate the ordering of dimensions
of the search space based on the user‚Äôs knowledge about
their cost of usage. Given a cost-aware preference as defined
in Definition 1 and a candidate solution such as x, we formulate the cost of selecting x at iteration t as a cost function:

Problem Formulation
Our proposed acquisition function is based on two criteria:
scalarised Gaussian Process Upper Confidence Bound (GPUCB), and a cost function that operates as an independent
cost-aware agent. We first start with scalarised GP-UCB.

Scalarised GP-UCB
GP-UCB (Srinivas et al. 2009)‚àö
defines an upper confidence
bound as qt (x) = ¬µt‚àí1 (x) + Œ≤ t œÉt‚àí1 (x) where ¬µt‚àí1 (x)
and œÉt‚àí1 (x) is defined as in (1) and Œ≤t is a trade-off parameter that grows with O(log(t)). In a single-objective BO
framework, GP-UCB at each time step t, selects the point

C(x, t) =

k
Y

(1 ‚àí œÄ(xI(j) , t))

(8)

j=1

where I is a tuple with size of k, consist of cost-aware constraints orderings as defined in Definition 1 and œÄ(xI(j) , t)
is sampled from an exponential distribution as:
œÄ(xI(j) , t) ‚àº Exp(xI(j) , Œª),

Œª=

1
wI(j) t + 1

where wI(1) , ..., wI(k) ‚àº Dir(1, ..., 1) such that wI(j) >
wI(j+1) , ‚àÄj ‚àà Z+
k‚àí1 .

0.675

x2

x2
x2

0.994

0.994

0.994

x2

0.681
0.678

1.0

0.994
0.994

0.994
0.994

0.994
0.994

0.994

0.61687
0.59259

0.993802
0.993796

0.994

0.994

0.994

0.994

0.994

(c) t = 20

0.994
0.994

0.0
0.0

0.994

1.0

0.2

0.994

0.8

0.4

0.994

0.76245

0.994
0.994

0.77195

0.6

0.994

0.786

0.777

0.782

0.773

0.768

0.764

0.6

x1

0.64116

Algorithm 1 CA-UCB
Input: Initial observations D, GP prior ¬µt=1 (x) = 0, K
for t = 1, ..., T do
Sample Œ∏t ‚àº UŒ∏t
. for Eq (5)
Find xt = argmaxx‚ààX Œ±(x, Œ∏t , t) . defined in Eq (9)
Update D ‚à™ {x, f (x)}
Update GP m ‚àÄm ‚àà Z+
M
end for

C(x, t)
0.993809
0.994

0.788

0.783

0.779

0.774

0.770

0.4

0.8

0.78091

0.75294
0.2

0.8

1.0
0.994
0.994

0.789

0.785

0.780

0.776

0.771

0.767

0.758

0.0
0.0

0.765

0.755

0.2

0.759

0.4

0.762
0.761

0.756

0.6

0.6

x1

0.66332

(b) t = 10
C(x, t)
0.78987

1.0
0.8

0.672

(a) t = 1

0.4

0.663

0.2

0.651

1.0

0.6607
0.65

0.8

0.669
0.666

0.6

x1

0.639

0.4

0.0
0.0

0.627

0.2

0.0741

0.615

0.2

0

5
0.1

22
5

0.4

0.603

95

0.1616

0.645

0.2491

0.633

0.
36
0

0.621

5

0.597

0.1

0
27
0.

31

0.6

0.609

00

45

3
0.

0.3

5

0.

0.8

0.3418
0

0.654
0.648
0.642
0.636
0.630
0.624
0.618
0.612
0.606
0.600

5

75

0.3

30

55

0
0.21

0.2

0.
39

C(x, t)
0.68548

1.0

0.

5

0.090

0.13

0.0
0.0

0.3

0

85

0.24

0.16

0.105

0.4
0.2

0.2

0.120

0.6

0.180

0.8

C(x, t)
0.4345

40
0.

1.0

0.993789
0.993783

0.2

0.4

0.6

x1

0.8

1.0

(d) t = 1000

Figure 2: Illustration of the cost function with cost-aware
constraint I = (1, 2). Figure 2a shows the cost of selecting
a combination of x1 and x2 when t = 1. When high values
of x1 and x2 is selected, C(x, t) is significantly higher than
the cost of low values of x1 and x2 . As the optimisation
progress, C(x, t) increases for any combination of x1 and x2
(Figure 2b and 2c) and the difference of cost for a cheap and
expensive combination reduces. As for Figure 2d, C(x, t) is
close to 1 for all the combinations.

The idea behind the C(x, t) comes from the natural
properties of exponential distribution in modeling situations
where certain events occur with a constant probability per
unit length. Given that the cheaper regions of the search
space can be selected more often than the expensive regions,
the cost function constructs different exponential distributions based on each dimension of the search space with respect to their ordering in cost-aware preferences. This is
achieved by using different Œª values. Having a higher cost
of usage for a dimension of search space will result in a
higher value of wI(k) and accordingly smaller values of Œª
for its corresponding distribution for that dimension which
reshapes the distribution based on cost-aware constraints.
C(x, t) follows our motivations introduced before and it is
also independent from objective space since cost-aware preference has been defined uniquely on the search space and t.
2

2

Consider the example of f : R ‚Üí R and I = (1, 2),
i.e. using dimension 1 of the search space is more expensive
than second dimension. Since the cost function is independent of f (objective space), we plot the cost function C(x, t)
with respect to different values of x1 , x2 and t. Figure 2 confirms that at t = 1, there is a significant difference between
the cost of a candidate point with an expensive combination of inputs (i.e. x1 = 0.9 and x2 = 0.9) and the cost of
a cheaper combination of the inputs from the search space
(i.e. x1 = 0.1, x2 = 0.2). As the optimisation progresses,
this gap will shrink and the effects of cost-aware constraints
will diminish (see Figure 2b and Figure 2c). Finally, when

t ‚Üí ‚àû, C(x, t) ‚âà 1, ‚àÄx (see Figure 2d).
We now define CA-MOBO cost-aware acquisition function based on scalarised UCB and the cost-aware constraints
incorporated in a separate cost function as:
Œ±(x, Œ∏t , t) = Q(x, Œ∏t ) √ó (1 ‚àí C(x, t))
(9)
where Q(x, Œ∏t ) is the scalarised UCB as defined in (7) and
C(x, t) calculates the cost of selecting x as the input at iteration t (see (8)). Algorithm 1 details the cost-aware multiobjective Bayesian Optimisation.

Theoretical Bounds
We first define our proposed instantaneous and cumulative
regret for a cost-aware multi-objective optimisation problem
and then derive the theoretical upper bound on the cumulative regret. The proof builds on the ideas in (Srinivas et al.
2009) and recently published work in multi-objective optimisation (Paria, Kandasamy, and PoÃÅczos 2018). In this section we assume that the kernel hyperparameters are known.
The regret defined in this problem must be representative
of both compliance with cost-aware constraints and also the
goodness of the Pareto front. The instantaneous regret (simple regret) incurred by CA-MOBO at iteration number t is:


r(xt , Œ∏t ) = max SŒ∏t f (x) 1 ‚àí C(x, t) ‚àí ...
(10)
x‚ààX


...SŒ∏t f (xt ) 1 ‚àí C(xt , t)
where: xt = argmaxx‚ààX Œ±(x, Œ∏t , t). Œ∏t is the sampled
weights for scalarisation function S based on an arbitrary
distribution UŒ∏ as defined in (5) and C(x, t) is the cost function for input x at iteration t as defined in (8). Accordingly
the cumulative regret is calculated as:
T
T 
X
X

R(T ) =
r(xt , Œ∏t ) =
max SŒ∏t f (x) ... (11)
t=1

t=1

x‚ààX



... 1 ‚àí C(x, t) ‚àí SŒ∏t f (xt ) 1 ‚àí C(xt , t)


Theorem 1 Given the cost-aware acquisition function de2
), the cumulative regret
fined in (9) with Œ≤t = 2ln( t‚àö|X|
2œÄ
R(T ) is upper bounded as:
M

X
Œ≥T (m)  12
œÄ2
UÃÑŒ∏ M T Œ≤T
+
M E[UŒ∏ ]
‚àí2
3
ln(1 + œÉm
)
m=1
where Œ≥T (m) is the maximum information gain after T iteration for objective function m as defined
in (Srinivas et al.
q 1 PT

2
2009). UÃÑŒ∏ is also defined as UÃÑŒ∏ = E
t=1 UŒ∏t .
T

Proof:
rials.

The proof is provided in the supplementary mate

Based on the Theorem 1, we can see that the upper bound
on the regret for CA-MOBO is no worse (in order) than the
cumulative regret bound introduced in (Paria, Kandasamy,
and PoÃÅczos 2018) even though cost-aware constraints restricts exploration during early iterations.

Experiments
We now present our experimental results comparing the performance of CA-MOBO to other strategies without cost
awareness. These experiments including multiple synthetic
and two real-world problems of optimising the hyperparameters of a feed-forward neural network and a random forest.
For all synthetic functions, the experiment was repeated 50
times with 500 iterations and we map the search space and
objective space to [0, 1]. For real-world experiments, we report the average values of 10 runs in 300 iterations and the
initial observations are randomly selected. The hyperparameters of the GP are updated based on the observed data every
10 evaluations. Squared Exponential (SE) kernel is used in
all experiments.
To the best of our knowledge there are no studies aiming to solve our proposed problem. However, we compare
our results with (Paria, Kandasamy, and PoÃÅczos 2018), as
a scalarised multi-objective UCB (MO-UCB) with no costawareness. Additional experiments and CA-MOBO source
codes are available in supplementary materials.

Synthetic Functions
We first compare performance on minimising synthetic
functions. To better illustrate the theoretical analysis on regret in CA-MOBO, we define the average regret at time
t ‚àà {1, ..., T } based on the mean of obtained regrets in 1...t
iterations, i.e.:
Pt
r(xt0 , Œ∏t0 )
0
R0 (t) = t =1
(12)
t
The average regret is easier to interpret than the instantaneous regret.
We start our experiments with Zitzler-Deb-Thiele‚Äôs
function N. 3 (Zitzler, Deb, and Thiele 2000) with
5‚àídimensional input and 2‚àídimensional output. Without
loss of generality, we define the cost-aware preferences as
I = (1, 2, 3, 4, 5) - i.e. selecting x1 value as a input from dimension 1 of the search space is more expensive than selecting the same value of x from dimension 2 and so on. Figure
3a shows the whole Pareto front as the ground-truth. Figure
3e and 3g show that both CA-MOBO and MO-UCB obtain
regions of Pareto front, and both methods achieved approximately the same dominated hypervolume but with different
exploration strategies on the search space (see Figure 3b).
PT
Figure 3f shows the value of t=1 xi , i ‚àà {1, ..., 5} with
respect to t and it confirms that CA-MOBO is complying
with the cost-aware constraints since the cumulative selected
values of x1 (the most expensive dimension of search space)
PT =500
is t=1
x1 ‚âà 17.8 for CA-MOBO while in Figure 3h

PT =500
this values is approximately t=1
x1 ‚âà 85.2 for MOUCB. Moreover, based on Figure 3f, our proposed method
successfully followed the ordering of dimensions of search
space based on cost-aware constraint I as the highest usage is for dimension 5 and the lowest one is for dimension
1. Whereas in Figure 3h with no cost-awareness, x3 has the
highest value of cumulative usage during the optimisation.
It is noteworthy to mention that while both methods approximately achieved the same value of dominated hypervolume
in finding Pareto front, given the cost-aware constraints, CAMOBO uses smaller amounts of expensive inputs during the
optimisation. Figure 3c shows the average regret as defined
in (12). MO-UCB achieves a better regret initially due to
its capacity for exploration in more expensive regions of
search space that allows it to find higher number of dominating solutions. CA-MOBO obtains a better regret at the
end of the optimisation due to its compliance with the costaware constraints and gradually explores more expensive regions. Likewise, Figure 3d illustrates the cumulative regret
incurred by CA-MOBO comparing to MO-UCB.
Figure 4 shows the results for minimising Cross-in-tray
function as the first objective and Hlder table function (Jamil
and Yang 2013) as the second objective:
p
h
i0.1
x21 + x22
f1 (x) = ‚àí10‚àí4 sin(x1 )sin(x2 )exp( 100‚àí
) +1
œÄ
p
2
x1 + x22 
f2 (x) = ‚àí sin(x1 )cos(x2 )exp 1 ‚àí
œÄ

For this problem, we define I = {1, 2} as the cost-aware
constraint. Unlike the previous experiment, this constraint
results in better initial observations for CA-MOBO as Figure 4b illustrates the initial values of dominated hypervolume for CA-MOBO is higher than MO-UCB. Correspondingly, Figure 4c and Figure 4d show an advantage in average regret and cumulative regret for CA-MOBO at initial
iterations of optimisation. Comparing Figure 4f and FigPT =500
PT =500
ure 4h, t=1
x1 <
x2 for CA-MOBO (with
t=1
PT =500
PT =500
low uncertainty) and t=1
x1 ‚âà t=1
x2 for MOUCB. That implies CA-MOBO complies with cost-aware
constraints while achieving the same percentage of dominated hypervolume in comparison to MO-UCB with no costawareness.

Hyperparameters of Random Forest
For random forest, we have defined two objectives - training
time and prediction error. The hyperparameters are the number of estimators (x1 ‚àà [1, 100]) and the depth of estimators
(x2 ‚àà [1, 100]). The defined estimators are decision trees.
The Scikit-Learn python package (Pedregosa et al. 2011) is
used for implementation.
We define the cost-aware constraints for CA-MOBO
as I = {1, 2} - i.e. the changes in number of estimators are considered to be more expensive compared to the
depth of estimators. Figure 5a compares the Pareto fronts
found by CA-MOBO and MO-UCB. Our proposed method
achieved more diverse Pareto front in comparison to MOUCB, specifically when the training time is low and error
high. We believe the reason for more diverse solutions in

100

0.4
0.2
0.2

0.4

f1

0.6

0.8

0.15

40
CA ‚àí MOBO
MO ‚àí UCB

1.0

0

200

10

0.05

0

400

0

200

t

(a) Full Pareto front

0

400

(b) Dominated Hypervolume

(c) Average Regret

(d) Cumulative Regret

0.8

Pt

0.4

100

0.4

50

0.2

0.2
0.4

0.6

f1

0.8

1.0

0

200

0

0.0
0.0

400

0.2

0.4

t

(e) Pareto front (CA-MOBO)

200
100

0

0.2

x1
x2
x3
x4
x5

300

0.6

f2

xi

150

i=1

f2

MO ‚àí UCB Pareto Frontier

x1
x2
x3
x4
x5

200

0.6

400

xi

250

CA ‚àí MOBO Pareto Frontier

400

t

1.0

0.8

200

t

1.0

0.0
0.0

20

0.10

20

MO ‚àí UCB
CA ‚àí MOBO

30

i=1

0.0
0.0

60

40

MO ‚àí UCB
CA ‚àí MOBO

0.20

Pt

f2

0.6

80

R(T )

0.8

R0(T )

Pareto Frontier

Hypervolume(%)

1.0

(f) Sum of selected inputs (CAMOBO)

f1

0.6

0.8

1.0

0

200

400

t

(g) Pareto front (MO-UCB)

(h) Sum of selected inputs (MOUCB)

Figure 3: Minimising Zitzler-Deb-Thiele‚Äôs N. 3 (Zitzler, Deb, and Thiele 2000) function. Figure 3a shows the full Pareto front.
Figure 3b shows the comparison of dominated hypervolume for CA-MOBO and MO-UCB. Figure 3c illustrates the average
regret of both methods. Figure 3d demonstrates the cumulative regret. Figure 3e illustrates the Pareto front obtained by CAMOBO. Figure 3f shows the cumulative amount of selected inputs for each dimension of search space. Figure 3g and Figure 3h
illustrate the obtained Pareto front by MO-UCB and the corresponding cumulative amount of selected inputs respectively.

100

0.4
0.2
0.2

0.4

f1

0.6

0.8

40
20

CA ‚àí MOBO
MO ‚àí UCB

0

1.0

0.10

20

0

200

0.05

0

400

0

200

t

(a) Full Pareto front

0

400

(b) Dominated Hypervolume

(c) Average Regret

(d) Cumulative Regret

xi

0.6

f2

xi

200

Pt

0.4

x1
x2

0.8

i=1

f2

0.6

0.4

100

0.2

400

t

1.0
x1
x2

0.8

200

t

1.0

0.0
0.0

40

0.15

200

i=1

0.0
0.0

0.20

60

MO ‚àí UCB
CA ‚àí MOBO

60

MO ‚àí UCB
CA ‚àí MOBO

Pt

f2

0.6

0.25

80

R(T )

Hypervolume(%)

Pareto Frontier

0.8

R0(T )

1.0

100

0.2
CA ‚àí MOBO Pareto Frontier

0.2

0.4

f1

0.6

0

0.8

(e) Pareto front (CA-MOBO)

1.0

0

200

400

t

(f) Sum of selected inputs (CAMOBO)

0.0
0.0

MO ‚àí UCB Pareto Frontier

0.2

0.4

f1

0.6

0
0.8

(g) Pareto front (MO-UCB)

1.0

0

200

400

t

(h) Sum of selected inputs (MOUCB)

Figure 4: Minimising Cross-in-tray function as first objective and Hlder table function and the second objective. Figure 4a shows
the full Pareto front. Figure 4b demonstrates a comparison of the dominated hypervolume for MO-UCB and CA-MOBO. Figure
4c and Figure 4d demonstrate the comparison of the average regret and cumulative regret for both methods respectively. Figure
4e and Figure 4g show Obtained Pareto front by CA-MOBO and MO-UCB, respectively. Figure 4f shows the calculation of
PT
i=1 xi as the sum of selected inputs for CA-MOBO and it confirms the compliance of CA-MOBO with the cost-aware
constraints. Figure 4h illustrates sum of selected inputs for MO-UCB.

x2 (depth)

5

20

i=1

100

100

Pt

10

40

xi

xi

150
i=1

60

50

50

0
0.03

0.04

0.05

0.06

20

0.07

40

60

0
0

80

100

(a) Obtained Pareto front

200

300

0

100

t

No. estimators

Error

x1 (estimators)

x2 (depth)

150

Pt

15

200

x1 (estimators)

CA ‚àí MOBO
MO ‚àí UCB

80

20

Depth

Time (s)

100

CA ‚àí MOBO
MO ‚àí UCB

25

200

300

t

(b) Pareto fronts solutions found (c) Sum of selected inputs (CA- (d) Sum of selected inputs (MOin X
MOBO)
UCB)

Figure 5: Optimising a random forest model with two hyperparameters of number of estimators and depth of estimators.

0.2

100

0.020

0.025

0.030

0.035

1

2

3

No. layers

Error

(a) Obtained Pareto front

xi

300
200

15000
10000

100

50

x2 (MO ‚àí UCB)

20000
i=1

i=1

150

x2 (CA ‚àí MOBO)

25000

x1 (MO ‚àí UCB)

Pt

200

x1 (CA ‚àí MOBO)

400

xi

0.4

CA ‚àí MOBO
MO ‚àí UCB

250

Pt

CA ‚àí MOBO
MO ‚àí UCB

HiddenUnits

Time (s)

0.6

5000
0

0
0

25

50

75

100

0

25

50

75

100

t

t

(b) Pareto fronts solutions found in (c) Sum of selected inputs for x1 (d) Sum of selected inputs for x2
X

Figure 6: Optimising a neural network with six hyperparameters with two conflicting objectives of prediction error and prediction time.
the region with low training time and high error is related to
the cost-aware constraints on the search space which in turn
discourages the optimiser to initially explore the regions of
the search space with high values of estimators for the random forest. As a result, initial solutions with high error are
favored for CA-MOBO. However, MO-UCB obtains more
number of Pareto front solutions in the region with low error
after t = 300 iterations. Figure 5b confirms our conclusion
as most of the selected solutions by CA-MOBO in X (search
space) have low values in number of estimators (x1 ). Figure
5c shows the sum of all the selected inputs from the search
PT
space - CA-MOBO has i=1 x1 ‚âà 71.4 the sum over the
number of estimators,
increases almost to twice the
Pwhich
T
value for MO-UCB ( i=1 x1 ‚âà 141.8)(see Figure 5d).

Fast and Accurate Neural Network
In neural networks, low prediction error and small prediction time is desirable, however, these are conflicting objectives. We define the aim of this experiment to find fast and
accurate neural networks by tuning six hyperparameters on
the MNIST dataset (LeCun et al. 1998). This problem first
proposed by (HernaÃÅndez-Lobato et al. 2016) and we have
slightly modified this experiment to fit in our problem framework. The hyperparameters to be tuned are: Number of hidden layers (x1 ‚àà [1, 4]), the number of hidden units per layer
(x2 ‚àà [50, 300]), the learning rate (x3 ‚àà (0, 0.2]), amount of
dropout (x4 ‚àà [0.4, 0.8]), and the level of l1 (x5 ‚àà (0, 0.1])
and l2 (x6 ‚àà (0, 0.1]) regularization. We consider a feedforward networks with ReLUs at the hidden layers and a
soft-max output layer. The networks are coded in the Keras

library and trained using Adam (Kingma and Ba 2014) with
a batch size of 4000 instances in 64 epochs.
We define the cost-aware constraints for CA-MOBO as
I = {1, 2}, which means dimension 1 from the search space
is more expensive comparing to second dimension - i.e. increasing the number of hidden layers is more expensive than
the number of hidden units per layer. Figure 6a illustrates
more diverse solutions for CA-MOBO as compared to MOUCB, mainly in the region with high error and low test time
due to the constraints imposed by cost-awareness initially.
As in the random forest experiment, Figure 6b shows our
proposed method favors cheaper regions of X. Finally, Figure 6c indicates in 300 iterations of the optimisation for
PT
CA-MOBO with i=1 x1 ‚âà 205.7, whereas for MO-UCB
PT
i=1 x1 ‚âà 349.5. Figure 6d illustrates the sum of inputs
over number of hidden units (x2 ) for both algorithms.

Conclusion
We have proposed a novel algorithm for multi-objective
Bayesian optimization to incorporate non-uniform blackbox function evaluations. We have defined cost-aware constraints over the search space in order to model the varied
cost of function evaluations for different combination of inputs from the search space. CA-MOBO initially explores
less expensive regions of the search space and gradually
moves towards the more expensive regions of the search
space. Experimental results show the effectiveness of our algorithm.

References
Abdolshah, M.; Shilton, A.; Rana, S.; Gupta, S.; and
Venkatesh, S. 2018. Expected hypervolume improvement
with constraints. In 2018 24th International Conference on
Pattern Recognition (ICPR), 3238‚Äì3243. IEEE.
Borwein, J. M., and Zhuang, D. 1993. Super efficiency in
vector optimization. Transactions of the American Mathematical Society 338(1):105‚Äì122.
Boyd, S., and Vandenberghe, L. 2004. Convex optimization.
Cambridge university press.
Brochu, E.; Cora, V. M.; and De Freitas, N. 2010. A tutorial on bayesian optimization of expensive cost functions,
with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599.
Chugh, T. 2019. Scalarizing functions in bayesian multiobjective optimization. arXiv preprint arXiv:1904.05760.
Deb, K. 2001. Multi-objective optimization using evolutionary algorithms, volume 16. John Wiley & Sons.
Emmerich, M. T., and Deutz, A. H. 2018. A tutorial on
multiobjective optimization: fundamentals and evolutionary
methods. Natural computing 17(3):585‚Äì609.
Emmerich, M. T.; Deutz, A. H.; and Klinkenberg, J. W.
2011. Hypervolume-based expected improvement: Monotonicity properties and exact computation. In 2011 IEEE
Congress of Evolutionary Computation (CEC), 2147‚Äì2154.
IEEE.
Garrido-MerchaÃÅn, E. C., and HernaÃÅndez-Lobato, D. 2019.
Predictive entropy search for multi-objective bayesian optimization with constraints. Neurocomputing.
HernaÃÅndez-Lobato, D.; Hernandez-Lobato, J.; Shah, A.; and
Adams, R. 2016. Predictive entropy search for multiobjective bayesian optimization. In International Conference on Machine Learning, 1492‚Äì1501.
Hoffman, M.; Shahriari, B.; and Freitas, N. 2014. On correlation and budget constraints in model-based bandit optimization with application to automatic machine learning. In
Artificial Intelligence and Statistics, 365‚Äì374.
Jamil, M., and Yang, X.-S. 2013. A literature survey
of benchmark functions for global optimization problems.
arXiv preprint arXiv:1308.4008.
Khan, N.; Goldberg, D. E.; and Pelikan, M. 2002. Multiobjective bayesian optimization algorithm. IlliGAL Report
(2002009):114‚Äì125.
Kingma, D. P., and Ba, J. 2014. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980.
Lam, R.; Willcox, K.; and Wolpert, D. H. 2016. Bayesian
optimization with a finite budget: An approximate dynamic
programming approach. In Advances in Neural Information
Processing Systems, 883‚Äì891.
LeCun, Y.; Bottou, L.; Bengio, Y.; Haffner, P.; et al. 1998.
Gradient-based learning applied to document recognition.
Proceedings of the IEEE 86(11):2278‚Äì2324.
Li, Y., and Yao, Y.
2019.
Bayesian optimization
with directionally constrained search.
arXiv preprint
arXiv:1906.09459.

Li, Y.; Yue, T.; Ali, S.; and Zhang, L. 2017. A multiobjective and cost-aware optimization of requirements assignment for review. In 2017 IEEE Congress on Evolutionary Computation (CEC), 89‚Äì96. IEEE.
Miettinen, K., and MaÃàkelaÃà, M. M. 2002. On scalarizing functions in multiobjective optimization. OR spectrum
24(2):193‚Äì213.
Miettinen, K. 2012. Nonlinear multiobjective optimization,
volume 12. Springer Science & Business Media.
Paria, B.; Kandasamy, K.; and PoÃÅczos, B. 2018. A flexible
multi-objective bayesian optimization approach using random scalarizations. CoRR abs/1805.12168.
Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.;
Thirion, B.; Grisel, O.; Blondel, M.; Prettenhofer, P.; Weiss,
R.; Dubourg, V.; et al. 2011. Scikit-learn: Machine
learning in python. Journal of machine learning research
12(Oct):2825‚Äì2830.
Rasmussen, C. E., and Williams, C. K. I. 2006. Gaussian
Processes for Machine Learning. MIT Press.
Roijers, D. M.; Vamplew, P.; Whiteson, S.; and Dazeley,
R. 2013. A survey of multi-objective sequential decisionmaking. Journal of Artificial Intelligence Research 48:67‚Äì
113.
Srinivas, N.; Krause, A.; Kakade, S. M.; and Seeger, M.
2009. Gaussian process optimization in the bandit setting: No regret and experimental design. arXiv preprint
arXiv:0912.3995.
Wang, H.; Hu, J.; Min, G.; Miao, W.; and Georgalas, N.
2017. Cost-aware optimisation of cache allocation for
information-centric networking. In GLOBECOM 20172017 IEEE Global Communications Conference, 1‚Äì6. IEEE.
Zintgraf, L. M.; Kanters, T. V.; Roijers, D. M.; Oliehoek,
F. A.; and Beau, P. 2015. Quality assessment of morl algorithms: A utility-based approach. In Benelearn 2015: Proceedings of the Twenty-Fourth Belgian-Dutch Conference on
Machine Learning.
Zitzler, E.; Deb, K.; and Thiele, L. 2000. Comparison
of multiobjective evolutionary algorithms: Empirical results.
Evolutionary computation 8(2):173‚Äì195.
Zitzler, E. 1999. Evolutionary Algorithms for Multiobjective Optimization: Methods and Applications. Ph.D. Dissertation, Swiss Federal Institute of Technology Zurich.

Supplementary Materials

Following the Gaussian process, we know that



‚àö
t
fm (x) ‚àí ¬µtm (x) ‚àí Œ≤t œÉm
(x) + |Ht
‚àº N ‚àí


‚àö
2
t
t
Œ≤t œÉ m
(x), œÉm
(x)
. As it has been described in (Paria,
Kandasamy, and PoÃÅczos 2018):
t
p
 œÉm


(x) ‚àí Œ≤t
1
t
fm (x)‚àí¬µtm (x)‚àí Œ≤t œÉm
(x) + |Ht ‚â§ ‚àö
e 2 ‚â§ 2
t |X |
2œÄ
Using tower property of expectation:
h
i

M
E SŒ∏t (f (x‚àót )) ‚àí Œ±(x‚àót , Œ∏t , t) .(1 ‚àí C(x‚àót , t)) ‚â§ E[UŒ∏ ] 2
t
PT 1
œÄ2
As t=1 t2 ‚â§ 6 , finally:
" T
#
X

E
SŒ∏t (f (x‚àót )) ‚àí Œ±(x‚àót , Œ∏t , t) .(1 ‚àí C(x‚àót , t))

Proof of Theorem 1
Considering Œ±(x, Œ∏t , t) as defined in (9) (CA-MOBO acquisition function), E[R(T )] can be written as:
T 
hX



E
max SŒ∏t f (x) 1 ‚àí C(x, t) ‚àí SŒ∏t f (xt ) ...
t=1

x‚ààX


... 1 ‚àí C(xt , t)
" T
#


X
‚â§E
Œ±(xt , Œ∏t , t) ‚àí SŒ∏t (f (xt )) . 1 ‚àí C(xt , t) +
t=1

{z

|

}

(I)

"
E

T 
X

SŒ∏t (f (x‚àót ))

‚àí

Œ±(x‚àót , Œ∏t , t)



. 1 ‚àí C(x‚àót , t)

#

t=1

t=1

|

{z

‚â§

}

(II)

‚àó
given that: xt = argmaxx‚ààX Œ±(x,
 Œ∏t , t) and xt =
argmaxx‚ààX SŒ∏t (f (x)) 1 ‚àí C(x, t) . Based on the definition of xt and x‚àót , it is clear that Œ±(xt , Œ∏t , t) ‚â• Œ±(x‚àót , Œ∏, t)
since xt results in the highest value of Œ±(xt , Œ∏t , t). Based on
(I) and (II), we are defining corollary 1 and corollary 2.
Corollary 1 It can be proved that (II) is bounded as:
" T
#


X
E
SŒ∏t (f (x‚àót )) ‚àí Œ±(x‚àót , Œ∏t , t) . 1 ‚àí C(x‚àót , t) ...

E[UŒ∏ ] √ó œÄ 2
M
6


Corollary 2 It can be proved that (I) is bounded as:
" T
#


X
E
Œ±(xt , Œ∏t , t) ‚àí SŒ∏t (f (xt )) . 1 ‚àí C(xt , t)
t=1
M

X
‚â§ UÃÑŒ∏ M T Œ≤T

 21

Œ≥T (m)

‚àí2
ln(1 + œÉm
)
m=1
q
 1 PT

2
where UÃÑŒ∏ = E
t=1 UŒ∏t .
T

t=1

œÄ2
E[UŒ∏ ]M
6
where UŒ∏ is an arbitrary distribution over the weights of
Chebyshev scalarisation.

+

œÄ2
M E[UŒ∏ ]
6

... ‚â§

Proof:

t=1

Proof:

We start the proof by:
h
i

E SŒ∏t (f (x‚àót )) ‚àí Œ±(x‚àót , Œ∏t , t) .(1 ‚àí C(x‚àót , t))
h
i

‚â§ E SŒ∏t (f (x‚àót )) ‚àí Œ±(x‚àót , Œ∏t , t) .(1 ‚àí C(x‚àót , t))
+
i
hX

‚àó
‚àó
‚àó
SŒ∏t (f (xt )) ‚àí Œ±(xt , Œ∏t , t) .(1 ‚àí C(xt , t))
‚â§E

"

M 
X

‚â§ E UŒ∏t

¬µtm (xt ) +

p



t
Œ≤t œÉ m
(xt ) ‚àí fm (xt ) . 1 ‚àí C(xt , t) +

m=1

UŒ∏t

M
X



fm (xt ) ‚àí

m=1

+

x‚ààX

We are starting the proof by:
" T
#


X
E
Œ±(xt , Œ∏t , t) ‚àí SŒ∏t (f (xt )) . 1 ‚àí C(xt , t) |Ht

¬µtm (xt )

‚àí

p

 

. 1 ‚àí C(xt , t)

t
Œ≤t œÉ m
(xt )

+

where [z]+ = max(0, z). Based on the assumption of
Based on the corollary 1, fm (x) ‚àí ¬µtm (x) ‚àí
E[UŒ∏ ]-Lipschitz for S scalarisation function, it can be con‚àö
1
t
Œ≤t œÉ m
(x) + ‚â§ t2 |X
cluded that: h
| and since we know the cost
i
X

‚àó
‚àó
‚àó
function
is
bounded
C(x
, t) ‚àà (0, 1), the inequality can be
t
E
SŒ∏t (f (xt )) ‚àí Œ±(xt , Œ∏t , t) .(1 ‚àí C(xt , t))
+written as:
x‚ààX
"
#
"
#
M p
X
M
M
U
h
i
Œ∏
X
p
t

‚â§ E UŒ∏t
Œ≤t œÉ m
(xt ) + E 2 t
t
‚â§ E[UŒ∏ ]
E fm (x) ‚àí ¬µtm (x) ‚àí Œ≤t œÉm
(x) .(1 ‚àí C(x, t))
t |X |
+
m=1
m=1

Since C(x, t) is bounded as C(x, t) ‚àà (0, 1):
M
h
i
X
p

t
E[UŒ∏ ]
E fm (x) ‚àí ¬µtm (x) ‚àí Œ≤t œÉm
(x) .(1 ‚àí C(x, t))

+

m=1

M
h
i
X
p
t
‚â§ E[UŒ∏ ]
E fm (x) ‚àí ¬µtm (x) ‚àí Œ≤t œÉm
(x)
m=1

+

Adding the sum over T iterations:
" T
#


X
E
Œ±(xt , Œ∏t , t) ‚àí SŒ∏t (f (xt )) . 1 ‚àí C(xt , t)
t=1

"

#
" T
#
X M UŒ∏
p
t
t
‚â§E
UŒ∏t Œ≤t œÉm (xt ) + E
t2 |X |
m=1 t=1
t=1
M X
T
X

#

0.99

0.98
0.97

0.98

0.99

1.00

f1

80

0.20

20

0.15

60

0.10

40

CA ‚àí MOBO
MO ‚àí UCB

20

1.01

0

200

10

0.05
0.00

400

0
0

200

t

(a) Full Pareto front

i=1

300

Pt

Pt

f2

xi

0.9

200

0.8

100

0.99

x1
x2

400

i=1

xi

1.00

400

(d) Cumulative Regret
500

MO ‚àí UCB Pareto Frontier

x1
x2

f2

CA ‚àí MOBO Pareto Frontier

200

t

(c) Average Regret
1.0

300

0

400

t

(b) Dominated Hypervolume

1.01

MO ‚àí UCB
CA ‚àí MOBO

30

R(T )

f2

1.00

MO ‚àí UCB
CA ‚àí MOBO

0.25

R0(T )

Pareto Frontier

Hypervolume(%)

100

1.01

200
100

0

0.98
0.97

0.98

0.99

1.00

f1

0

1.01

200

0.7
0.80

400

t

(e) Pareto front (CA-MOBO)

(f) Sum of selected inputs (CAMOBO)

0

0.85

0.90

f1

0.95

1.00

(g) Pareto front (MO-UCB)

0

200

400

t

(h) Sum of selected inputs (MOUCB)

Figure 7: Minimising Matyas function as first objective and Booth function as the second objective. Figure 7a shows the full
Pareto front for this problem. Figure 7b show the comparison of dominated hypervolume. Figure 7c and Figure 7d demonstrate
the average and cumulative regret respectively. Figure 7e and 7g shows the obtained Pareto front for CA-MOBO and MO-UCB
PT
PT
respectively. Figure 7e confirms the role of cost-aware constraints as t=1 x1 < t=1 x2 , ‚àÄt with low uncertainty. Whereas
Figure 7h indicates higher usage of dimension 1 during the optimisation for MO-UCB.
#
p
œÄ 2 M E[UŒ∏t ]
t
UŒ∏t Œ≤t œÉm (xt ) +
‚â§E
6
|X |
m=1 t=1
"

T
M X
X

Additional Experiments

Based on the sums over T and M , (Paria, Kandasamy, and
PoÃÅczos 2018) and (Srinivas et al. 2009) proved that:
" M T
#
XX
p
t
UŒ∏t Œ≤t œÉm (xt )
E
m=1 t=1

"

T
M

 12  X
X
‚â§ E M Œ≤T
UŒ∏2t
t=1

m=1

Œ≥T (m)

 12

#

‚àí2
)
ln(1 + œÉm

So it can be concluded that
" T
#


X
E
Œ±(xt , Œ∏t , t) ‚àí SŒ∏t (f (xt )) . 1 ‚àí C(xt , t)
t=1
M

X
‚â§ UÃÑŒ∏ M T Œ≤T
m=1

 12

Œ≥T (m)
‚àí2
ln(1 + œÉm
)

+

œÄ2
M E[UŒ∏ ]
6

Finally based on the corollary 1 and corollary 2, it proves
that:
T 
X



max SŒ∏t f (x) 1 ‚àí C(x, t) ‚àí SŒ∏t f (xt ) 1 ‚àí C(xt , t)
t=1

Figure 7 shows the results for minimising Matyas function
as first objective and Booth function (Jamil and Yang 2013)
as the second objective:
f1 (x) = 0.26(x21 + x22 ) ‚àí 0.48x1 x2
f2 (x) = (x1 + 2x2 ‚àí 7)2 ‚àí (2x1 + x2 ‚àí 5)2
Figure 7a show the whole Pareto front solution for this
problem. A comparison between Figure 7e and Figure 7g
shows that CA-MOBO finds the regions of Pareto front in
a different regions of objective space compared to MOBO
that found the solutions with higher exploration in expensive regions of search space due to lack of cost-aware
PT =500
constraints. Comparing Figure 7f and 7h, t=1
x1 <
PT =500
x2 , ‚àÄt for CA-MOBO which indicates that the
t=1
expensive dimension of search space has been used less
than the cheaper dimension of search space, however in
PT =500
PT =500
x2 < t=1
x1 , ‚àÄt. As Figure 7b
Figure 7h, t=1
demonstrates, even by imposing cost-aware constraints, CAMOBO achieved better dominance in hypervolume comparing to MO-UCB. Figure 7c and Figure 7d illustrate the average and cumulative regret respectively.

x‚ààX

M

X
‚â§ UÃÑŒ∏ M T Œ≤T
m=1

Œ≥T (m)
‚àí2
ln(1 + œÉm
)

q 1 PT

2
where UÃÑŒ∏ is defined as UÃÑŒ∏ = E
t=1 UŒ∏t .
T

 21

+

œÄ2
M E[UŒ∏ ]
3


