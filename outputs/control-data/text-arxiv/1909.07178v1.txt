arXiv:1909.07178v1 [math.ST] 16 Sep 2019

Estimating change points in nonparametric
time series regression models
Maria Mohr and Leonie Selk∗
Department of Mathematics, University of Hamburg
September 17, 2019

Abstract
In this paper we consider a regression model that allows for time series covariates
as well as heteroscedasticity with a regression function that is modelled nonparametrically. We assume that the regression function changes at some unknown time
bns0 c, s0 ∈ (0, 1), and our aim is to estimate the (rescaled) change point s0 . The
considered estimator is based on a Kolmogorov-Smirnov functional of the marked
empirical process of residuals. We show consistency of the estimator and prove a
rate of convergence of OP (n−1 ) which in this case is clearly optimal as there are
only n points in the sequence. Additionally we investigate the case of lagged dependent covariates, that is, autoregression models with a change in the nonparametric
(auto-) regression function and give a consistency result. The method of proof also
allows for different kinds of functionals such that Cramér-von Mises type estimators
can be considered similarly. The approach extends existing literature by allowing
nonparametric models, time series data as well as heteroscedasticity. Finite sample
simulations indicate the good performance of our estimator in regression as well as
autoregression models and a real data example shows its applicability in practise.

Key words: change point estimation, time series, nonparametric regression, autoregression, conditional heteroscedasticity, consistency, rates of convergence
AMS 2010 Classification: Primary 62G05, Secondary 62G08, 62G20, 62M10

1

Introduction

Change point analysis has gained attention for decades in mathematical statistics. There
is a vast literature on testing for structural breaks when the possible timing of such a
∗

Financial support by the DFG (Research Unit FOR 1735 Structural Inference in Statistics: Adaptation and Efficiency) is gratefully acknowledged.

1

break, the change point, is unknown, see for instance Kirch and Kamgaing (2012) and
reference mentioned therein. This paper, however, is concerned with the estimation of
the change point when assuming its existence.
The most simple set of models can be described as follows
Yt = µ1 I{t ≤ bns0 c} + µ2 I{t > bns0 c} + εt , t = 1, . . . , n,
where s0 ∈ (0, 1) is the (rescaled) change point, µ1 and µ2 the signal before and after the
break, respectively, and (εt )t being stationary and centred errors. These models are often
referred to as AMOC-models (at most one change). The problem naturally moved from
the standard case with independent errors (see Ferger and Stute (1992) among others)
to the time series context. Both Bai (1994) and Antoch et al. (1997) allow for linear
processes and Hušková and Kirch (2008) more generally for dependent errors.
Additional information on the form of the signal can be expressed through a process
of covariates (Xt )t resulting in linear regression models with a change in the regression
parameter, such as
Yt = β1 Xt I{t ≤ bns0 c} + β2 Xt I{t > bns0 c} + εt , t = 1, . . . , n,
where β1 and β2 are the regression coefficients before and after the break, respectively.
Bai (1997), Horváth et al. (1997), Aue et al. (2012) among others consider the estimation
of a change point in (multiple) linear regression models making use of least squares estimation. Considering Xt = Yt−1 in the linear regression model from above, one obtains
autoregressive models with one change in the autoregressive parameter. The estimation
of the parameters and the unknown change point in AR(1) models was for instance considered by Chong (2001), Pang et al. (2014) and Pang and Zhang (2015).
Our aim is to propose an estimator for the change point s0 in a nonparametric version
of the regression model from above, namely
Yt = m(1) (Xt )I{t ≤ bns0 c} + m(2) (Xt )I{t > bns0 c} + εt , t = 1, . . . , n,
for some nonparametric regression functions m(1) , m(2) (before and after the break) and
in addition also investigate the autoregressive case where Xt = Yt−1 . While the investigation of points of discontinuity in (nonparametric) regression functions has been studied
to some extend (see for instance Döring and Jensen (2015) for an overview), not that
much research has been devoted to change point analysis in nonparametric models as
the one above, where the change occurs in time. Delgado and Hidalgo (2000) propose
estimators for the location and size of structural breaks in a nonparametric regression
model imposing scalar breaks in time or values taken by some regressors, as in threshold
models. Their rates of convergence and limiting distribution depends on a bandwidth,
chosen for the kernel estimation. Chen et al. (2005) estimate the time of a scalar change
in the conditional variance function in nonparametric heteroscedastic regression models
using a hybrid procedure that combines the least squares and nonparametric methods.
2

The paper at hand extends existing literature, on the one hand by allowing for nonparametric heteroscedastic regression models with a general change in the unknown regression
function where both errors and covariates are allowed to be time series, and on the other
hand by investigating the autoregressive case. The achieved rate of convergence for the
proposed estimator of OP (n−1 ) is optimal as described in Hariz et al. (2007).
The remainder of the paper is organized as follows. The model and the considered
estimator are introduced in section 2. Section 3 contains the regularity assumptions as
well as the asymptotic results for the proposed estimator. Section 4 is concerned with the
special case of lagged dependent covariates, that is the autoregressive case. In section 5 we
describe a simulation study and discuss a real data example, whereas section 6 concludes
the paper. Proofs of the main results as well as auxiliary lemmata can be found in the
appendix.

2

The model and estimator

Let {(Yt , Xt ) : t ∈ N} be a weakly dependent stochastic process in R × Rd following the
regression model
Yt = mt (Xt ) + Ut , t ∈ N.
(2.1)
The unobservable innovations are assumed to fulfill E[Ut |F t ] = 0 almost surely for
the sigma-field F t = σ(Uj−1 , Xj : j ≤ t). We assume there exists a change point in the
regression function such that
(
m(1) (·),
t = 1, . . . , bns0 c
mn,t (·) = mt (·) =
,
m(1) 6≡ m(2)
(2.2)
m(2) (·),
t = bns0 c + 1, . . . , n
where bns0 c with s0 ∈ (0, 1) is the unknown time the change occurs. Note that we
keep above notations for simplicity reasons, however, the considered process is in fact a
triangular array process {(Yn,t , Xn,t ) : 1 ≤ t ≤ n, n ∈ N} and will be treated appropriately.
Assuming (Y1 , X1 ), . . . , (Yn , Xn ) have been observed, the aim is to estimate s0 . The
idea is to base the estimator on the sequential marked empirical process of residuals,
namely
bnsc
1X
T̂n (s, z) :=
(Yi − m̂n (Xi ))ωn (Xi )I{Xi ≤ z},
n i=1
for s ∈ [0, 1] and z ∈ Rd , where x ≤ y is short for xj ≤ yj for all j = 1, . . . , d,
ωn (·) = I{· ∈ Jn } being from assumption (J) below and m̂n being the Nadaraya-Watson
estimator, that is


Pn
x−Xj
Yj
j=1 K
hn

 ,
m̂n (x) = P
x−Xj
n
j=1 K
hn

3

with kernel function K and bandwidth hn as considered in the assumptions below. Then
we want to estimate s0 by
)
(
ŝn := min s : sup |T̂n (s, z)| = sup sup |T̂n (s, z)| .

(2.3)

s∈[0,1] z∈Rd

z∈Rd

Note that ŝn = bnŝn c /n.
Remark. The advantage of using marked residuals in comparison to using the classical
CUSUM T̂n (s, ∞) to estimate the change point is that in the first case the estimator is
consistent for all changes of the form (2.2) whereas there are several examples in which
the use of T̂n (s, ∞) leads to a non-consistent estimator. To this end see the remark below
the proof of Theorem 3.1 and compare to Mohr and Neumeyer (2019).
Remark. Mohr and Neumeyer (2019) constructed procedures based on functionals of
T̂n , e.g. a Kolmogorov-Smirnov test statistic sups∈[0,1] supz∈Rd |T̂n (s, z)|, to test the null
hypothesis of no changes in the unknown regression function against change point alternatives as in (2.2). Given that such a test has rejected the null, the use of an M-estimator
as in (2.3) seems natural. Furthermore, Cramér-von Mises type test statistics of the form
R
sups∈[0,1] Rd |T̂n (s, z)|2 ν(z)dz for some integrable ν : Rd → R were also considered by
Mohr and Neumeyer (2019). Assuming strict stationarity of the covariates and the existence of a density f such that Xt ∼ f for all t, as in (X1) below, the Cramér-von Mises
approach from above with ν ≡ f leads to an alternative estimator for s0 , namely
( Z

 )
Z
1/2

|T̂n (s, z)|2 f (z)dz

s̃n := min s :

1/2

|T̂n (s, z)|2 f (z)dz

= sup

Rd

s∈[0,1]

.

Rd

R
However, to obtain a feasible estimator one needs to replace the integral Rd |T̂n (s, z)|2 f (z)dz
P
by its empirical counterpart n1 nk=1 |T̂n (s, Xk )|2 in practise as f is not known.

3

Asymptotic results

In this section we will derive asymptotic properties for ŝn . To this end we introduce the
following assumptions.
(U) For all t ∈ Z let E[Ut |F t ] = 0 a.s. for F t = σ(Uj−1 , Xj : j ≤ t) and E[|Ut |q ] ≤ CU
for some CU < ∞ and q > 2.
(M) For all t ∈ Z let E[|m(1) (Xt ) − m(2) (Xt )|r ] ≤ Cm for some Cm < ∞ and r > 2.
(P) Let {(Yt , Xt ) : 1 ≤ t ≤ n, n ∈ N} be strongly mixing with mixing coefficient α(·).
For q, r from assumptions (U) and (M) and b := min(q, r) let α(t) = O(t−ᾱ ) with
some ᾱ > (1 + (b − 1)(1 + d))/(b − 2).

4

(N) For b from assumption (P) let E[|Yt |b ] < ∞ and let Xt be absolutely continuous
with density function ft : Rd → R that satisfies supx∈Rd E[|Yt |b |Xt = x]ft (x) < ∞
and supx∈Rd ft (x) < ∞ for all t ∈ {1, . . . , n} and n ∈ N. Let there exist some
N ≥ 0 such that sup|i−j|≥N supxi ,xj E[|Yi Yj ||Xi = xi , Xj = xj ]fij (xi , xj ) < ∞ for
all n ∈ N, where fij is the density function of (Xi , Xj ).
(J) Let (cn )n∈N be a positive sequence of real valued numbers satisfying cn → ∞ and
cn = O((log n)1/d ) and let Jn = [−cn , cn ]d .
(F) For some C < ∞ and cn from assumption (J) let In = [−cn − Chn , cn + Chn ]d and
let δn−1 = inf x∈Jn inf 1≤t≤n ft (x) > 0 for all n ∈ N. Further, let for all n ∈ N
pn = max sup sup |Dk ft (x)| < ∞
|k|=1 x∈In 1≤t≤n

0 < qn = max sup max |Dk m(j) (x)| < ∞,
0≤|k|≤1 x∈In j=1,2

where |i| =

Pd

j=1 ij

and Di =

∂ |i|
i
i
∂x11 ...∂xdd

for i = (i1 , . . . , id ) ∈ Nd0 .

R
(K) Let K : Rd → R be symmetric in each component with Rd K(z)dz = 1 and compact
support [−C, C]d . Additionally let |K(u)| < ∞ for all u ∈ Rd and |K(u)−K(u0 )| ≤
Λku − u0 k for some Λ < ∞ and for all u, u0 ∈ Rd , where kxk = maxi=1,...,d |xi |.
(B) With b and ᾱ from assumption (P) let
ᾱ − 1 − d −
log (n)
=
o(1)
for
θ
=
nθ hdn
ᾱ + 3 − d −

1+ᾱ
b−1
1+ᾱ
b−1

.

For δn , pn , qn from assumption (F) let
s
!
log(n)
+ hn pn pn qn δn = o(n−ζ )
nhdn
for some ζ > 0.
(X1) For all 1 ≤ t ≤ n, n ∈ N let ft (·) = f (·), for some density f .
(X2) For all 1 ≤ t ≤ n, n ∈ N let ft (·) = f(1) (·) for all t = 1, . . . , bns0 c and ft (·) = f(2) (·)
for all t = bns0 c + 1, . . . , n, for some densities f(1) , f(2) .
Remark. The assumptions on the error terms and the mixing assumptions particularly
allow for conditional heteroscedasticity. Assumptions (U), (M) and (P) are a trade off
between the existence of moments and the rate of decay of the mixing coefficient. Assumptions (P), (N), (K) and the first part of (B) are reproduced from Kristensen (2012).
Together with (J) and (F), they are used to obtain uniform rates of convergence for m̂n
stated in Lemma A.1 in the appendix. In (X1), we assume stationarity of the covariates
for the whole observation period, while in the case of (X2) we assume stationarity before
5

and right after the change occurs. Nevertheless both assumptions rule out general autoregressive effects such as Xt = (Yt−1 , . . . , Yt−d ). We will address this issue separately in
section 4.
Theorem 3.1. Assume (U), (M), (P), (N), (J), (F), (K) and (B). Furthermore let either
(X1) or (X2) hold. Then the change point estimator ŝn is consistent, i. e.
|ŝn − s0 | = oP (1).
Theorem 3.2. Under the assumptions of Theorem 3.1 for the change point estimator ŝn
it holds that
|ŝn − s0 | = OP (rn−1 ),
where rn = n.
The proofs of the theorems can be found in appendix A.2. We state both theorems
seperately since we need Theorem 3.1 to prove Theorem 3.2.
Remark. To obtain the rates of convergence we make use of the fact that ŝn can be
expressed using the sup norm on l∞ (Rd ), i.e.
N : l∞ (Rd ) → R, g 7→ N (g) := sup |g(z)|,
z∈Rd

where l∞ (Rd ) is the space of all uniformly bounded real valued functions on Rd . Note that
similarly s̃n can be expressed using the L2 (P ) norm, when (Xt )t is strictly stationary with
marginal distribution P , namely
1/2
Z
2
∞
d
|g(z)| f (z)dz
.
Ñ : l (R ) → R, g 7→ Ñ (g) :=
Rd

Using Ñ (g) ≤ N (g) for all g ∈ l∞ (Rd ), corresponding results for s̃n as in Theorem 3.1
and Theorem 3.2 can be proven in a similar matter.

4

The autoregressive case

In this section we will consider the case where the exogenous variables include finitely
many lagged values of the endogenous variable, we will refer to this model as the autoregressive case. We will focus on one dimensional covariates, however, the results do
not depend on the dimension and can also be formulated for higher order autoregression
models. Consider the nonparametric autoregression
Yt = mt (Yt−1 ) + Ut , t = 1, . . . , n,

(4.1)

with unobservable innovations Ut and one change in the regression function occurring at
some unknown time bns0 c as in (2.2).
Furthermore assume the following.
6

(X3) For all 1 ≤ t ≤ n, n ∈ N let Xt := Yt−1 be absolutely continuous with density ft .
Let there exist densities f(1) and f(2) such that ft (·) = f(1) (·) for all t = 1, . . . , bns0 c
P
0c
and Rn (x) := n1 nj=bns0 c+1 fj (x) − n−bns
f(2) (x) → 0 for all x ∈ R and n → ∞.
n
Remark. Note that (X3) requires on the one hand strict stationarity up to the time of
change bns0 c. On the other hand the time series needs to reach its (new) stationary
distribution fast enough after the change. This is a generalization of (X2) where we
assumed stationarity both before and right after the change point, which can not be fulfilled
in the model (4.1). A necessary condition then is that there exists a stationary solution
of equation (4.1) under both m(1) (·) and m(2) (·) as regression functions.
Example. Consider the AR(1)-model
Yt = at · Yt−1 + εt
with standard normally distributed innovations (εt )t and at = a ∈ (−1, 1) for t ≤ bns0 c,
at = b ∈ (−1, 1) for t > bns0 c, a 6= b. Then assumption (X3) is fulfilled. Note to this
end that Xt := Yt−1 ∼ N (0, 1/(1 − a2 )) for t ≤ bns0 c. The distribution after the change

P
point is given by Xbns0 c+1+k ∼ N 0, b2(k+1) /(1 − a2 ) + ki=0 b2i for all k > 0. Thus with
Pj−bns0 c−1 2i
b by the mean value theorem it holds for some ξj
σj2 := b2(j−bns0 c) /(1 − a2 ) + i=0
2
2 −1
between σj and (1 − b ) that






n
2
2
X
1
x
x
1
q 1

Rn (x) =
exp − 2 − p
exp −
2 )−1
2
−1
n
2σ
2(1
−
b
2
2π(1
−
b
)
j
2πσj
j=bns0 c+1
!




n
2
2
1
1
1 X
1
x
1
x
− ·
=
σj2 −
exp −
+
1 ·
2
n
1 − b2
2ξj
2 (2πξj ) 32
(2πξj ) 2 2ξj
≤ C

j=bns0 c+1
n
X

1
n

j=bns0 c+1

σj2 −

1
1 − b2

for some constant C < ∞ for all x ∈ R. Further we can conclude
1
n

n
X

σj2

j=bns0 c+1

1
1
−
=
2
1−b
n

n
X
j=bns0 c+1

b2(j−bns0 c) 1 − b2(j−bns0 c)
1
+
−
2
2
1−a
1−b
1 − b2

1
1
1
=
−
2
2
1−a
1−b n

n
X

b2(j−bns0 c)

j=bns0 c+1

and thus Rn (x) −−−→ 0 for all x ∈ R.
n→∞

In general verifying assumption (X3) for model (4.1) means to compare the distribution
of a stochastic process that is not yet in balance with its stationary distribution. A well
known technique to deal with this task is coupling, see e. g. Franke et al. (2002).
Under (X3) we get the following consistency result for our change point estimator in
the autoregressive case.
7

Theorem 4.1. Assume model (4.1) under (U), (M), (P), (N), (J), (F), (K), (B) and
(X3). Then the change point estimator ŝn is consistent, i. e.
|ŝn − s0 | = oP (1).
The proof can be found in appendix A.2.
Remark. Another possibility to handle the autoregressive case would be to model the
change in a different way, namely
( (1)
(1) 
(1)
Yt = m(1) Yt−1 + Ut ,
t = 1, . . . , bns0 c
Yt =
,
m(1) 6≡ m(2) ,
(2)
(2) 
(2)
Yt = m(2) Yt−1 + Ut ,
t = bns0 c + 1, . . . , n
(1) 
(2) 
for two stationary processes Yt t , Yt t , see e. g. Kirch et al. (2015). In this case
assumption (X2) is fulfilled and thus Theorem 3.1 and Theorem 3.2 apply.

5
5.1

Finite sample properties
Simulations

To investigate the finite sample performance of our estimator, we generate data from two
different basic models, namely
(IID) Yt = mt (Xt ) + σ(Xt )εt , where the observations (Xt )t are i.i.d., univariate and standard normally distributed, just as the errors (εt )t .
(TS) Yt = mt (Xt ) + σ(Xt )εt , where (εt )t i.i.d. ∼ N (0, 1) and the univariate observations
(Xt )t stem from a time series Xt = 0.4Xt−1 + ηt with standard normal innovations
(ηt )t .
For both models we generate data both for the homoscedastic case σ ≡ 1 as well as for
√
the heteroscedastic case σ(x) = 1 + 0.5x2 . The results for both are very similar in all
situations, thus we only present the results for the heteroscedastic case. To model the
change in the regression function we use three different scenarios
(
−0.5x,
t = 1, . . . , bns0 c
(C1) mt =
0.5x
t = bns0 c + 1, . . . , n,
(C2) mt =

(C3) mt =

(
0.1x,
0.9x

t = 1, . . . , bns0 c
t = bns0 c + 1, . . . , n,

(
0.5x,
(0.5 + 3 exp(−0.8x2 ))x

t = 1, . . . , bns0 c
t = bns0 c + 1, . . . , n,

8

where we let s0 range from 0.1 to 0.9. In Figure 1 the results for 1000 replications and
sample sizes n = 100, 500, 1000 are shown, where we plot s0 against the estimated mean
squared error of our estimator ŝn . The kernel for m̂n is chosen as the Epanechnikov
kernel of order four and the bandwidth is determined by a cross-validation method. It
can be seen that our estimator performs quite well even for the smallest sample size
n = 100 when s0 is 0.5 or close to it whereas for a change point that lies closer to the
boundaries of the observation interval a larger sample size is needed to get satisfying
results. This is due to the fact that if s0 = 0.1 or s0 = 0.9 there are only 10 observations
before and after the change point respectively for n = 100 and thus the estimation of
m(1) and m(2) respectively are poor. Moreover an asymmetry in the results is striking.
This stems from the CUSUM type statistic that our estimator is based on. For s0 = 0.1
e. g. the sum consists of only 0.1n summands and thus the estimation of e. g. E[Ut ] is
worse than if s0 = 0.9 and the estimation is based on 0.9n summands. The effect of a
decreasing performance of the estimators the closer s0 gets to the boundaries is typical
for change point estimators based on CUSUM statistics and can be antagonized by the
use of appropriate weights, see e. g. Ferger (2005).
To stress our estimator a little further we simulate the scenario that there is also a
change in the variance function σ at a different time point than the change in the regression
function m. In this situation the estimator should still be able to detect s0 , the change
point in the regression function. The results are shown in Figure 2 for model (IID) and
√
model (TS) with change point scenario (C1) where σt (x) = 1 + 0.1x2 for t ≤ 0.4n and
√
σt (x) = 1 + 0.8x2 for t > 0.4n. They confirm the good performance of our estimator
even in this more difficult situation.
As discussed in section 4 our estimator can also be applied to the autoregressive case.
To investigate the finite sample performance in this situation we generate data according
to the model
(AR) Yt = mt (Yt−1 ) + σ(Yt−1 )εt , where (εt )t i.i.d. ∼ N (0, 1).
For σ ≡ 1 and change point scenario (C1) as well as (C2) assumption (X3) is fulfilled,
see the example in section 4. Simulation results for these cases are shown in Figure 3
where the setting is the same as described above. They look very similar to the results
of model (IID) and (TS) and thus confirm the theoretical result of Theorem 4.1. Even
for examples where assumption (X3) can not be verified easily the performance of our
estimator is satisfying, see Figure 4 for model (AR) with σ ≡ 1 and change point scenario
√
(C3) as well as the heteroscedastic model (AR) with σ = 1 + 0.5x2 and change point
scenario (C1).
As stated in the remark in section 2 it is also possible to base the estimator on a
Cramér-von Mises type functional of the marked empirical process of residuals. The
simulation results for this type of estimator are very similar to those presented here for
the Kolmogorov-Smirnov type estimator ŝn and are omitted for the sake of brevity.

9

0.10

MSE

n=100
n=500
n=1000

0.00

0.10

n=100
n=500
n=1000

0.00

MSE

0.20

(TS)

0.20

(IID)

0.2

0.4

0.6

0.8

0.2

0.4

0.8

0.20
0.10

MSE

n=100
n=500
n=1000

0.00

0.10
0.00

MSE

n=100
n=500
n=1000

0.2

0.4

0.6

0.8

0.2

0.4

0.6

0.8

0.20

s0

MSE

n=100
n=500
n=1000

0.00

0.00

0.10

n=100
n=500
n=1000

0.10

0.20

s0

MSE

0.6
s0

0.20

s0

0.2

0.4

0.6

0.8

0.2

s0

0.4

0.6

0.8

s0

Figure 1: Simulation results for model (IID) (left), model (TS) (right) and change point
scenario (C1) (top), change point scenario (C2) (middle), change point scenario (C3)
(bottom)

5.2

Data example

Finally, we will consider a real data example. The data at hand contains 36 measurements
of the annual flow volume of the small Czech river, Ráztoka, recorded between 1954 and
1989 as well as the annual rainfall during that time. It was considered by Hušková and
Antoch (2003) to investigate the effect of controlled deforestation on the capability for
water retention of the soil. To this end it is of interest if and when the relationship
between rainfall and flow volume changes. We set Xt as the annual rainfall and Yt as
the annual flow volume. Mohr and Neumeyer (2019) applied their Kolmogorov-Smirnov
test to this data set, which clearly rejects the null of no change in the conditional mean
function, indicating the existence of a change in the relationship between rainfall and flow
volume. Using ŝn to estimate the unknown time of change suggests a change in 1979.
Note that this is consistent with the literature. As was pointed out by Hušková and
Antoch (2003) large scale deforestation had started around that time. Figure 5 shows on
the left-hand side the scatterplot Xt against Yt using dots for the observations after the
10

0.10

MSE

n=100
n=500
n=1000

0.00

0.10

n=100
n=500
n=1000

0.00

MSE

0.20

(TS)

0.20

(IID)

0.2

0.4

0.6

0.8

0.2

0.4

s0

0.6

0.8

s0

0.20
0.10

MSE

n=100
n=500
n=1000

0.00

0.10

n=100
n=500
n=1000

0.00

MSE

0.20

Figure 2: Simulation results for model (IID) (left) and model (TS) (right) with change
point scenario (C1) and an additional change in the variance function

0.2

0.4

0.6

0.8

0.2

s0

0.4

0.6

0.8

s0

Figure 3: Simulation results for model (AR) and change point scenario (C1) (left), change
point scenario (C2) (right)
estimated change and crosses for the observations before the estimated change. On the
right-hand side the figure shows the cumulative sum, n1/2 supz∈R |T̂n (·, z)|, as well as the
critical value of the test used in Mohr and Neumeyer (2019) (red horizontal line) and the
estimated change (green vertical line). Note that s̃n leads to the same result.

6

Concluding remarks

In this paper we consider nonparametric regression models with a change in the unknown
regression function that allows for time series data as well as conditional heteroscedasticity.
We propose an estimator for the rescaled change point that is based on the sequential
marked empirical process of residuals and show consistency as well as a rate of convergence
of OP (n−1 ). In an autoregressive setting we additionally give a consistency result for the
proposed estimator.
If more than one change occurs, the proposed estimator is not consistent for one of
the changes in some situations. For detecting multiple changes we refer the reader to
alternative procedures such as the MOSUM procedure proposed by Kirch and Eichinger
(2018) or the wild binary segmentation procedure by Fryzlewicz (2014) (see also Fryzlewicz

11

0.20
0.00

MSE

n=100
n=500
n=1000

0.10

0.20
0.10
0.00

MSE

n=100
n=500
n=1000

0.2

0.4

0.6

0.8

0.2

0.4

0.6

s0

0.8

s0

2.0
1.5
0.5

1.0

Cumulative sum

600
500
400
300

0.0

100

200

Annual flow volume

700

Figure 4: Simulation results for homoscedastic model (AR) with change point scenario
(C3) (left) and heteroscedastic model (AR) with change point scenario (C1) (right)

400

500

600

700

800

900

1000

1955

Annual rainfall

1965

1975

1985

Year

Figure 5: Ráztoka data: scatterplot (left) and CUSUM (right)
(2019)).
Investigating the asymptotic distribution of the proposed estimator is a subsequent
issue. Certainly, it is of great interest as it can be used to obtain confidence intervals.
However, this subject goes beyond the scope of the paper at hand and is postponed to
future research.

A
A.1

Proofs
Auxiliary results

Lemma A.1. Under the assumptions (P), (N), (J), (F), (K) and (B), it holds that
s
!
!
log(n)
sup |m̂n (x) − m̄n (x)| = OP
+ hn pn δn pn qn ,
nhdn
x∈Jn
where

Pn
f (x)mi (x)
Pn i
m̄n (x) = i=1
.
i=1 fi (x)
12

The proof is similar to the proof of Lemma 2.2 in Mohr (2018). The key tool is an
application of Theorem 1 in Kristensen (2009). Details are omitted for the sake of brevity.
Remark. Under (X1) we have
m̄n (x) =

bns0 c
n − bns0 c
m(1) (x) +
m(2) (x),
n
n

under (X2) and (X3) we have
bns0 c
f(1) (x)
n

m̄n (x) =

f¯n (x)

(m(1) (x) − m(2) (x)) + m(2) (x),

where
n

1X
f¯n (x) :=
fi (x) =
n i=1

(

bns0 c
f(1) (x)
n
bns0 c
f(1) (x)
n

+
+

n−bns0 c
f(2) (x),
n
n−bns0 c
f(2) (x) +
n

for (X2)
Rn (x), for (X3)

with Rn (·) from assumption (X3).
The proofs of the following lemmata can be found in appendix A.3.
Lemma A.2. Under the assumptions of Theorem 3.1 as well as under those of Theorem
4.1 there exists a constant C̄ = C̄(C) < ∞ such that


L+bκn sc
1
X
−1
P  sup sup
Ui ωn (Xi )I{Xi ≤ z} > Cκn  ≤ C̄κnq
s∈[0,1] z∈Rd

i=L+1

for all L = 0, 1, . . . , n − κn , 1 ≤ κn ≤ n, n ∈ N and all C > 0 with q from assumption
(U).
Lemma A.3. Under the assumptions of Theorem 3.1 as well as under those of Theorem
4.1 there exists a constant C̄ = C̄(C) < ∞ such that
P

(L+bκn sc)∧bns0 c 

X

sup sup
s∈[0,1] z∈Rd

(m(1) (Xi ) − m̄n (Xi ))ωn (Xi )I{Xi ≤ z}

i=L+1



−E (m(1) (Xi ) − m̄n (Xi ))ωn (Xi )I{Xi ≤ z}



!
> Cκn

1

≤ C̄κnr

−1

and
L+bκn sc

P

sup sup
s∈[0,1] z∈Rd



X

(m(2) (Xi ) − m̄n (Xi ))ωn (Xi )I{Xi ≤ z}

i=L∨bns0 c+1



−E (m(2) (Xi ) − m̄n (Xi ))ωn (Xi )I{Xi ≤ z}



!
> Cκn

1

≤ C̄κnr

for all L = 0, 1, . . . , n − κn , 1 ≤ κn ≤ n, n ∈ N and all C > 0 with r from assumption
(M).
13

−1

Lemma A.4. Under the assumptions of Theorem 3.1 as well as under those of Theorem
4.1 it holds


L+bκn sc
X
P  sup sup
(m̄n (Xi ) − m̂n (Xi ))ωn (Xi )I{Xi ≤ z} > Cκn  ≤ C −1 κ−ζ
n
s∈[0,1] z∈Rd

i=L+1

for all L = 0, 1, . . . , n − κn , 1 ≤ κn ≤ n, n ∈ N and all C > 0 with ζ > 0 from assumption
(B).

A.2

Proof of main results

We will proof Theorem 3.1 under the assumption (X1) and simply make a note on the
parts that change under (X2).
Proof of Theorem 3.1. First note that for all s ∈ [0, 1] and z ∈ Rd
T̂n (s, z) = An (s, z) + ∆n,1 (s)∆n,2 (z),

(A.1)

where An (s, z) = An,1 (s, z) + An,2 (s, z) + An,3 (s, z) + An,4 (s, z) with
bnsc

1X
An,1 (s, z) :=
Ui ωn (Xi )I{Xi ≤ z}
n i=1
1
An,2 (s, z) :=
n

bn(s∧s0 )c 

X

(A.2)

(m(1) (Xi ) − m̄n (Xi ))ωn (Xi )I{Xi ≤ z}

i=1



− E (m(1) (Xi ) − m̄n (Xi ))ωn (Xi )I{Xi ≤ z}
1
An,3 (s, z) := I{s > s0 }
n

bnsc
X





(A.3)

(m(2) (Xi ) − m̄n (Xi ))ωn (Xi )I{Xi ≤ z}

i=bns0 c+1



− E (m(2) (Xi ) − m̄n (Xi ))ωn (Xi )I{Xi ≤ z}

(A.4)

bnsc

1X
An,4 (s, z) :=
(m̄n (Xi ) − m̂n (Xi ))ωn (Xi )I{Xi ≤ z}
n i=1
and
n − bns0 c bnsc
n − bnsc bns0 c
∆n,1 (s) := I{s ≤ s0 }
+ I{s > s0 }
n
n
n
n
Z
∆n,2 (z) :=
(m(1) (x) − m(2) (x))f (x)ωn (x)dx,
(−∞,z]

since by inserting the definition of m̄n we obtain for s ≤ s0
bnsc


1X 
E (m(1) (Xi ) − m̄n (Xi ))ωn (Xi )I{Xi ≤ z}
n i=1
14

(A.5)

bnsc


n − bns0 c 1 X 
=
E (m(1) (Xi ) − m(2) (Xi ))ωn (Xi )I{Xi ≤ z}
n
n i=1
=

n − bns0 c bnsc
∆n,2 (z)
n
n

and for s > s0
bns0 c

1 X 
E (m(1) (Xi ) − m̄n (Xi ))ωn (Xi )I{Xi ≤ z}
n i=1

1
+
n

bnsc
X



E (m(2) (Xi ) − m̄n (Xi ))ωn (Xi )I{Xi ≤ z}

i=bns0 c+1

bns0 c

n − bns0 c 1 X 
E (m(1) (Xi ) − m(2) (Xi ))ωn (Xi )I{Xi ≤ z}
=
n
n i=1

bns0 c 1
−
n n

bnsc
X



E (m(1) (Xi ) − m(2) (Xi ))ωn (Xi )I{Xi ≤ z}

i=bns0 c+1

n − bnsc bns0 c
∆n,2 (z).
n
n
R
R zd
R z1
Note that we use the notation (∞,z] g(x)dx = −∞
· · · −∞
g(x1 , . . . , xd )dx1 . . . dxd here.
Due to the dominated convergence theorem and assumption (M), it holds that
=

∆n,1 (s)∆n,2 (z) = ∆1 (s)∆2 (z) + o(1),
uniformly in s ∈ [0, 1] and z ∈ Rd , where
∆1 (s) := I{s ≤ s0 }(1 − s0 )s + I{s > s0 }(1 − s)s0 ,
Z
∆2 (z) :=
(m(1) (x) − m(2) (x))f (x)dx.
(−∞,z]

Note that under (X2) the same assertion holds with
Z
f(1) (x)f(2) (x)
∆n,2 (z) :=
(m(1) (x) − m(2) (x)) bns0 c
ωn (x)dx
n−bns0 c
f
(x)
+
f
(x)
(−∞,z]
(1)
(2)
n
n
and
Z
(m(1) (x) − m(2) (x))

∆2 (z) :=
(−∞,z]

f(1) (x)f(2) (x)
dx.
s0 f(1) (x) + (1 − s0 )f(2) (x)

By Lemma A.2, Lemma A.3 and Lemma A.4 with κn = n, it holds that An (s, z) = oP (1)
uniformly in s ∈ [0, 1] and z ∈ Rd . Hence, we have shown that
sup |T̂n (s, z)| = ∆1 (s) sup |∆2 (z)| + oP (1)
z∈Rd

z∈Rd

uniformly in s ∈ [0, 1] under both cases (X1) and (X2). The assertion then follows by
Theorem 2.12 in Kosorok (2008) as s0 is well-separated maximum of [0, 1] → R, s 7→
∆1 (s).
15

Remark. Note that there are examples of m(1) , m(2) and f resp. f(1) , f(2) that lead to
∆2 (∞) = 0. In those cases a change point estimator based on the classical CUSUM
T̂n (s, ∞) is not consistent.
Proof of Theorem 3.2. First note that s0 =
consider

bnŝn c
n

−

bns0 c
n

bns0 c
n

+ O(n−1 ) and ŝn =

bnŝn c
.
n

Thus we can

instead of |ŝn − s0 |. The proof follows mainly along the same lines

as the proof of Theorem 1 in Hariz et al. (2007). Consider the norm N : l∞ (Rd ) → R, g 7→
supz∈Rd |g(z)| and let M > 0. We will show below that for all η > 0 and b, c > 0 it holds




bnŝn c bns0 c
bnŝn c bns0 c
−1 M
M
= P rn 2 <
P rn
−
>2
−
≤η
n
n
n
n


bnŝn c bns0 c
+P
−
>η
n
n
≤ En,1 + En,2 + En,3 + En,4 ,

(A.6)

where

En,1 := P

rn−1 2M

bnŝn c bns0 c
bnŝn c bns0 c
−
≤ η, N (An (ŝn , ·) − An (s0 , ·)) ≥ C
−
<
n
n
n
n



En,2 := P (N (An (s0 , ·)) > c)
En,3 := P (∆n,1 (s0 )N (∆n,2 (·)) ≤ b)
En,4 := P (|ŝn − s0 | > η) ,
with C := b − 2c. Now it holds that En,4 → 0 for all η > 0, due to Theorem 3.1.
Further, En,2 → 0 for all c > 0 as An (s0 , z) = oP (1) holds uniformly in z ∈ Rd . Finally
choose b > 0 and n0 = n0 (b) ∈ N such that En,3 = 0 for all n ≥ n0 , which exists as
∆1 (s0 )N (∆2 (·)) > 0 and ∆n,1 (s0 )N (∆n,2 (·)) = ∆1 (s0 )N (∆2 (·)) + o(1). We then choose
c > 0 such that b − 2c > 0. To see the validity of (A.6) first note that for all s ∈ [0, 1]
T̂n (s, ·) = An (s, ·) + ∆n,1 (s)∆n,2 (·)


∆n,1 (s)
∆n,1 (s)
= An (s, ·) − An (s0 , ·) + An (s0 , ·) 1 −
+
T̂n (s0 , ·).
∆n,1 (s0 )
∆n,1 (s0 )
Applying the norm and triangular inequality we obtain for all s ∈ [0, 1]




∆n,1 (s)
∆n,1 (s)
N (T̂n (s, ·)) ≤ N (An (s, ·) − An (s0 , ·)) + 1 −
N (An (s0 , ·)) +
N (T̂n (s0 , ·))
∆n,1 (s0 )
∆n,1 (s0 )
which is equivalent to

N (T̂n (s, ·)) − N (T̂n (s0 , ·)) ≤ N (An (s, ·) − An (s0 , ·)) +



∆n,1 (s)
− 1 N (T̂n (s0 , ·)) − N (An (s0 , ·)) .
∆n,1 (s0 )

Due to the definition of ŝn it holds that N (T̂n (ŝn , ·)) − N (T̂n (s0 , ·)) ≥ 0. Additionally
using the specific definition of ∆n,1 we obtain



∆n,1 (ŝn ) 
N (An (ŝn , ·) − An (s0 , ·)) ≥ 1 −
N (T̂n (s0 , ·)) − N (An (s0 , ·))
∆n,1 (s0 )
16


≥ min
|
≥



bnŝn c bns0 c 
n
n
,
−
N (T̂n (s0 , ·)) − N (An (s0 , ·))
bns0 c n − bns0 c
n
n
{z
}
>1

bnŝn c bns0 c
−
(∆n,1 (s0 )N (∆n,2 (·)) − 2N (An (s0 , ·))) ,
n
n

where we again make use of the triangular inequality in the last step. Putting the results
together we obtain


bnŝn c bns0 c
−1 M
P rn 2 <
−
≤η
n
n


bnŝn c bns0 c
−1 M
−
≤ η, ∆n,1 (s0 )N (∆n,2 (·)) > b, N (An (s0 , ·)) ≤ c
≤ P rn 2 <
n
n
+ P (∆n,1 (s0 )N (∆n,2 (·)) ≤ b) + P (N (An (s0 , ·)) > c)


bnŝn c bns0 c
bnŝn c bns0 c
−1 M
−
≤ η, N (An (ŝn , ·) − An (s0 , ·)) ≥ C
−
≤ P rn 2 <
n
n
n
n
+ P (∆n,1 (s0 )N (∆n,2 (·)) ≤ b) + P (N (An (s0 , ·)) > c).
Finally we will investigate En,1 . To do this we define shells


bns0 c
l
l+1
Sn,l = t ∈ [0, 1] : 2 < rn t −
≤2
n
and choose Ln = Ln (η) such that 2Ln < rn η ≤ 2Ln +1 for some η ≤ 21 . Then
En,1

Ln
X



bnŝn c
bnŝn c bns0 c
P
∈ Sn,l , N (An (ŝn , ·) − An (s0 , ·)) ≥ C
−
≤
n
n
n
l=M


Ln
X
P
sup
≤
N (An (s, ·) − An (s0 , ·)) ≥ C2l rn−1 
s:

l=M

≤

Ln X
4
X

≤ C̃

−1
≤2l+1 rn





P

n
rn

N (An,i (s, ·) − An,i (s0 , ·)) ≥

sup
s:

l=M i=1



bns c
bnsc
− n0
n



bns c
bnsc
− n0
n

 1q −1 X
Ln
l=M

(2

1
−1
q

−1
≤2l+1 rn

l

) +



n
rn

 r1 −1 X
Ln
l=M

(2

1
−1
r

l

) +



n
rn

C l −1 
2r
4 n

−ζ X
Ln

!
−ζ l

(2 )

l=M

j
k
for some constant C̃ < ∞ by Lemmata A.2, A.3 and A.4 with κn = 2l+1 rnn with q from
assumption (U), r from assumption (M) and ζ > 0 from assumption (B). Now choosing
rn = n and letting n and thus Ln tend to infinity and then M to infinity, the assertion of
Theorem 3.2 follows.
Proof of Theorem 4.1. Under (X3) we have for all s ∈ [0, 1] and z ∈ R
˜ n (s, z),
T̂n (s, z) = An (s, z) + ∆n,1 (s)∆n,2 (z) + ∆
17

with An (s, z) and ∆n,1 (s) from the proof of Theorem 3.1, and with
Z
f(1) (x)f(2) (x)
(m(1) (x) − m(2) (x)) bns0 c
ωn (x)dx
∆n,2 (z) :=
n−bns0 c
f
(x)
+
f
(x)
+
R
(x)
(−∞,z]
n
(1)
(2)
n
n
and
˜ n (s, z) :=
∆

Z
(m(1) (x)−m(2) (x))I{s ≤ s0 }
(−∞,z]

f(1) (x)Rn (x)
bnsc
ωn (x)dx.
bns
c
0
n
f(1) (x) + n−bns0 c f(2) (x) + Rn (x)
n

n

Now it holds that
Z
∆n,2 (z) →

(m(1) (x) − m(2) (x))
(−∞,z]

f(1) (x)f(2) (x)
dx =: ∆2 (z)
s0 f(1) (x) + (1 − s0 )f(2) (x)

˜ n (s, z) → 0 uniformly in s ∈ [0, 1] and z ∈ R, due to dominated convergence and
and ∆
assumption (M). Hence we have uniformly in s and z
T̂n (s, z) = An (s, z) + ∆1 (s)∆2 (z) + o(1),
with ∆1 (s) as in the proof of Theorem 3.1. The rest goes analogously to the proof of
Theorem 3.1.
Remark. Note that for finite n ∈ N we do not get the decomposition of T̂n as in (A.1) in
the proof of Theorem 3.1. We only obtain this kind of decomposition when letting n tend
to infinity. The decomposition for finite n, however, is essential for the proof of the rates
of convergence in Theorem 3.2.

A.3

Proofs of lemmata

Proof of Lemma A.2. The proof follows along similar lines as the proof of Lemma A.3 in
Mohr (2018). Throughout the proof the values of C and C̄ may vary from line to line
but they are always positive, finite and independent of n. Further note that deterministic
terms that are of order O(κn ) can be omitted as we can choose constants appropriately.
It holds that
L+bκn sc

sup sup
s∈[0,1] z∈Rd

X

Ui ωn (Xi )I{Xi ≤ z}

i=L+1



L+bκn sc

= sup sup
s∈[0,1] z∈Rd

X

Ui ωn (Xi )I{Xi ≤ z} − E 

i=L+1

s∈[0,1] z∈Rd

X

X

Ui ωn (Xi )I{Xi ≤ z}

i=L+1

L+bκn sc

≤ sup sup



L+bκn sc

1

Ui I{|Ui | > κnq }ωn (Xi )I{Xi ≤ z}

i=L+1





L+bκn sc

−E 

X

1
q

Ui I{|Ui | > κn }ωn (Xi )I{Xi ≤ z}

i=L+1

18

(A.7)

L+bκn sc

+ sup sup
s∈[0,1] z∈Rd

1

X

Ui I{|Ui | ≤ κnq }ωn (Xi )I{Xi ≤ z}

i=L+1





L+bκn sc

1
q

X

−E 

Ui I{|Ui | ≤ κn }ωn (Xi )I{Xi ≤ z}

(A.8)

i=L+1

where (A.7) is of the desired rate in probability since
!
L+κ
1
1
Xn
−1
q
P
|Ui |I{|Ui | > κn } > Cκn ≤ C −1 CU κnq
i=L+1

by Markov’s inequality with




1
1
q
q
q
−(q−1)
E |Ui |I{|Ui | > κn } = E |Ui | |Ui |
I{|Ui | > κn }
− q−1
q

≤ κn

E[|Ui |q ]

1
−1
q

≤ CU κn

for all i and for CU < ∞ from assumption (U).

Considering the term (A.8) we define the function class


1
q
d
Fn := (u, x) 7→ uI{|u| ≤ κn }ωn (x)I{x ≤ z} : z ∈ R
to rewrite the assertion as

P  sup sup

s∈[0,1] ϕ∈Fn

L+bκn sc 

X



Z

ϕ(Ui , Xi ) −

ϕdP


1

> Cκn  ≤ C̄κnq

−1

.

i=L+1

Now we will cover [0, 1] by finitely many intervals and Fn by finitely many brackets to
replace the supremum by a maximum. Let therefore
0 = s1 < . . . < sKn = 1
−1

part the interval [0, 1] in Kn subintervals of length ¯n with ¯n = κn q . Then
L+bκn sc 

sup sup
s∈[0,1] ϕ∈Fn

X



Z
ϕ(Ui , Xi ) −

ϕdP

i=L+1
L+bκn sc 

= max
k

sup

sup

s∈[0,1] ϕ∈Fn
|s−sk |≤¯
n

X

ϕ(Ui , Xi ) −

≤ max sup
k

ϕ∈Fn

X

ϕdP

i=L+1

L+bκn sk c 

ϕ(Ui , Xi ) −



Z



Z
ϕdP

i=L+1

19

+ max
k

sup

L+κ
Xn

sup



Z
ϕ(Ui , Xi ) −
{z

s∈[0,1] ϕ∈Fn i=L+1
|
|s−sk |≤¯
n

ϕdP I
}

1




i−L
i−L
≤s −I
≤ sk
κn
κn

≤2κnq
L+bκn sk c 

X

≤ max sup
k

ϕ∈Fn



Z

ϕ(Ui , Xi ) −

1

+ 2κnq (κn ¯n + 1)

ϕdP

i=L+1

1

1

and 2κnq (κn ¯n + 1) = 2(κn + κnq ) = O(κn ). Further let
1

1

ϕuj (u, x) := uI{|u| ≤ κnq }I{u ≥ 0}ωn (x)I{x ≤ zj }+uI{|u| ≤ κnq }I{u < 0}ωn (x)I{x ≤ zj−1 }
and
1

1

ϕlj (u, x) := uI{|u| ≤ κnq }I{u ≥ 0}ωn (x)I{x ≤ zj−1 }+uI{|u| ≤ κnq }I{u < 0}ωn (x)I{x ≤ zj }
form the brackets [ϕlj , ϕuj ]j∈×di=1 {1,...,Ni } of Fn , where zj = (zj1 ,1 , . . . , zjd ,d ) and
−∞ = z0,i < . . . < zNi ,i = ∞
gives a partition of R for all i = 1, . . . , d. The total number of brackets Jn := N[ ] (n , Fn , k·
kL1 (P ) ) needed to cover Fn is of order Jn = O(−d
n ), which follows analogously to but easier
than the proof of Lemma A.7 in Mohr (2018).
For all ϕ ∈ Fn there exists a j with ϕlj ≤ ϕ ≤ ϕuj and thus
Z
Z
Z
u
u
ϕ − ϕdP ≤ ϕj − ϕj dP + (ϕuj − ϕlj )dP
and

Z
ϕ−

ϕdP ≥

ϕlj

Z
−

ϕlj dP

Z
−

(ϕuj − ϕlj )dP.

Therefore for all s ∈ [0, 1]
L+bκn sc 

X

sup
ϕ∈Fn



Z
ϕ(Ui , Xi ) −

ϕdP

i=L+1
L+bκn sc 

= max
j

sup
ϕ∈[ϕlj ,ϕu
j]

≤ max max
j




Z

+κn max
j

X



Z
ϕ(Ui , Xi ) −

ϕdP

i=L+1
L+bκn sc 

X

ϕuj (Ui , Xi ) −

Z

ϕuj dP

i=L+1

L+bκn sc 


,

X

i=L+1

(ϕuj − ϕlj )dP
{z
}
|
≤n

20

ϕlj (Ui , Xi ) −

Z

ϕlj dP





and κn n = O(κn ) if we choose n constant. Thus it remains to show that



Z
L+bκn sk c 
1
X
−1
ϕuj (Ui , Xi ) − ϕuj dP > Cκn  ≤ C̄κnq
P max
j,k

i=L+1

and the same with ϕuj replaced by ϕlj . Recall that
L+bκn sk c 

X

max
j,k

≤ max

ϕuj (Ui , Xi )

Z

ϕuj dP

−



i=L+1
L+bκn sk c 

X

j,k

1

Ui I{|Ui | ≤ κnq }I{Ui ≥ 0}ωn (Xi )I{Xi ≤ zj }

i=L+1



1
q
−E Ui I{|Ui | ≤ κn }I{Ui ≥ 0}ωn (Xi )I{Xi ≤ zj }
+ max
j,k

L+bκn sk c 

X

(A.9)

1

Ui I{|Ui | ≤ κnq }I{Ui < 0}ωn (Xi )I{Xi ≤ zj−1 }

i=L+1



1
q
.
−E Ui I{|Ui | ≤ κn }I{Ui < 0}ωn (Xi )I{Xi ≤ zj−1 }
We will only consider the first summand in more detail since the rest works analogously.
To prove that (A.9) is stochastically of the desired rate we apply a Bernstein type inequality for α-mixing processes, see Liebscher (1996) Therorem 2.1. Following his notation we
define

1
Zi := Ui+L I{|Ui+L | ≤ κnq }I{Ui+L ≥ 0}ωn (Xi+L )I{Xi+L ≤ z}


 
1
i
q
≤ sk
−E Ui+L I{|Ui+L | ≤ κn }I{Ui+L ≥ 0}ωn (Xi+L )I{Xi+L ≤ z} I
κn
1

for fixed z ∈ Rd and s ∈ [0, 1]. Note that S(κn ) := |Zi | ≤ 2κnq , Zi is centered and

D(κn , N ) :=

sup
0≤T ≤κn −1

(T +N )∧κn

X

E 

2 
Zj   ≤ N 2 E[Zi2 ] ≤ CU N 2

j=T +1
1− 2q

by assumption (U). Thus Liebscher’s Theorem can be applied with N = bκn
means that
L+bκn sk c


P

max
j,k

X

1

Ui I{|Ui | ≤ κnq }I{Ui ≥ 0}ωn (Xi )I{Xi ≤ zj }

i=L+1





1
q

−E Ui I{|Ui | ≤ κn }I{Ui ≥ 0}ωn (Xi )I{Xi ≤ zj }

21


> Cκn

c. This

X 
≤
P

L+bκn sk c 

X

1

Ui I{|Ui | ≤ κnq }I{Ui ≥ 0}ωn (Xi )I{Xi ≤ zj }

i=L+1

j,k



≤
≤
≤
≤



1
q



−E Ui I{|Ui | ≤ κn }I{Ui ≥ 0}ωn (Xi )I{Xi ≤ zj }
> Cκn




κn
C 2 κ2n
+ 4 α(N )
Jn Kn 4 exp − κn
N
64 N D(κn , N ) + 83 Cκn N S(κn )




2 2
2
2
C κn
q 
 + 4κnq α(κ1−
Jn Kn 4 exp −
)
n
2
1
2− q
2−
q
16
64CU κn + 3 Cκn




1
2
1− 2q
q
q
Jn Kn 4 exp −C1 κn + 4κn α(κn )


1
1
2
−ᾱ+ 2qᾱ
q
q −q
q
C2 κn (C1 κn ) + κn
1

≤ C̄κnq

−1

for some constants C1 , C2 , C̄ where the second to last inequality follows from the fact that
exp(−x) < x−k k! for all k ∈ N and x ∈ R>0 and the last inequality is true by assumption
(P) which implies ᾱ > (q + 2)/(q − 2). This completes the proof.
Proof of Lemma A.3. First we will distinguish between the cases L + bκn sc ≤ bns0 c and
L + bκn sc > bns0 c. In the first case we can write
L+bκn sc

X

(m(1) (Xi ) − m̄n (Xi ))ωn (Xi )I{Xi ≤ z}

i=L+1

=

m(1) (Xi )
Pn
m(1) (Xi ) −

X

Pbns0 c

Pn

fj (Xi ) m(2) (Xi ) j=bns0 c+1 fj (Xi )
Pn
−
j=1 fj (Xi )
j=1 fj (Xi )

L+bκn sc 
i=L+1

j=1

·ωn (Xi )I{Xi ≤ z}
Pn

L+bκn sc

=

X

(m(1) (Xi ) − m(2) (Xi ))

i=L+1

j=bns0 c+1 fj (Xi )
Pn
ωn (Xi )I{Xi
j=1 fj (Xi )

≤ z}

and analogously for the second case
L+bκn sc

X

(m(2) (Xi ) − m̄n (Xi ))ωn (Xi )I{Xi ≤ z}

i=L∨bns0 c+1
L+bκn sc

=

X

Pbns0 c
j=1 fj (Xi )
(m(2) (Xi ) − m(1) (Xi )) Pn
ωn (Xi )I{Xi ≤ z}.
j=1 fj (Xi )

i=L∨bns0 c+1

We will only examine the case L + bκn sc ≤ bns0 c in detail since the other case works
analogously.
22

The remainder of the proof is similar Pto the proof of Lemma A.2. With g(Xi ) :=
n
fj (Xi )
(s )
Pn 0 c+1
(m(1) (Xi ) − m(2) (Xi )) and f¯n 0 (Xi ) = j=bns
it holds
fj (Xi )
j=1

L+bκn sc

sup sup
s∈[0,1] z∈Rd

≤

L+κ
Xn

X

1

g(Xi )I{|g(Xi )| > κnr }f¯n(s0 ) (Xi )ωn (Xi )I{Xi ≤ z}

i=L+1
1

|g(Xi )|I{|g(Xi )| > κnr }

i=L+1

and further
P

L+κ
Xn

!

1
r

|g(Xi )|I{|g(Xi )| > κn } > Cκn

1

r
≤ C −1 κ−1
n Cm κn κn

−1

i=L+1

by the Markov inequality with
h
h
1 i
1 i
r
−(r−1)
r
r
E |g(Xi )|I{|g(Xi )| > κn } = E |g(Xi )| |g(Xi )|
I{|g(Xi )| > κn }
− r−1
r

≤ κn

1

≤ Cm κnr

E[|g(Xi )|r ]
−1

for all i and for some Cm < ∞ by assumption (M). Thus we can rewrite our assertion as




Z
L+bκn sc
X
1
−1
P  sup sup 
ϕ(Xi ) − ϕdP  > Cκn  ≤ C̄κnr ,
s∈[0,1] ϕ∈Fn

i=L+1

with the function class
o
n
1
Fn := x 7→ g(x)I{|g(x)| ≤ κnr }f¯n(s0 ) (x)ωn (x)I{x ≤ z} : z ∈ Rd .
To replace the supremum over ϕ by a maximum we cover Fn by finitely many brackets
[ϕlj , ϕuj ]j∈×di=1 {1,...,Ni } where
1

ϕuj (x) := g(x)I{|g(x)| ≤ κnr }I{g(x) ≥ 0}f¯n(s0 ) (x)ωn (x)I{x ≤ zj }
1

+g(x)I{|g(x)| ≤ κnr }I{g(x) < 0}f¯n(s0 ) (x)ωn (x)I{x ≤ zj−1 }
and
1

ϕuj (x) := g(x)I{|g(x)| ≤ κnr }I{g(x) ≥ 0}f¯n(s0 ) (x)ωn (x)I{x ≤ zj−1 }
1

+g(x)I{|g(x)| ≤ κnr }I{g(x) < 0}f¯n(s0 ) (x)ωn (x)I{x ≤ zj }
and j, zj are defined as in the proof of Lemma A.2. The total number of brackets
Jn := N[ ] (n , Fn , k · kL1 (P ) ) needed to cover Fn is again of order Jn = O(−d
n ), which
follows analogously to but easier than the proof of Lemma A.7 in Mohr (2018). Now we
proceed completely analogously to the proof of Lemma A.2 by replacing the supremum
over s by a maximum as well and applying Liebscher’s Theorem. Since the arguments are
the same as in the aforementioned proof we omit this part for the sake of brevity.
23

Proof of Lemma A.4. It holds


L+bκn sc
X
P  sup sup
(m̄n (Xi ) − m̂n (Xi ))ωn (Xi )I{Xi ≤ z} > Cκn 
s∈[0,1] z∈Rd
L+κ
Xn

≤ P

i=L+1

!
|m̄n (Xi ) − m̂n (Xi )|ωn (Xi ) > Cκn

i=L+1




sup |m̄n (x) − m̂n (x)| > C

≤ P
≤ C

x∈Jn
−1

E[ sup |m̄n (x) − m̂n (x)|]
x∈Jn

by the Markov inequality. Further by Lemma A.1 with assumption (B) it holds that
supx∈Jn |m̄n (x) − m̂n (x)| P
−−−→ 0
n→∞
n−ζ
which implies
E[supx∈Jn |m̄n (x) − m̂n (x)|]
−−−→ 0
n→∞
n−ζ
and thus for sufficiently large n
E[ sup |m̄n (x) − m̂n (x)|] ≤ n−ζ
x∈Jn

≤ κ−ζ
n
for κn ≤ n. This completes the proof.

References
Antoch, J., Hušková, M., and Prášková, Z. (1997). Effect of dependence on statistics for
determination of change. J. Stat. Plan. Inference, 60:291–310.
Aue, A., Horváth, L., and Hušková, M. (2012). Segmenting mean-nonstationary time
series via trending regressions. J. Econom., 168:367–381.
Bai, J. (1994). Least squares estimation of a shift in linear processes. J. Time Ser. Anal.,
15:435–472.
Bai, J. (1997). Estimation of a Change Point in Multiple Regression Models. Rev. Econ.
Stat., 79:551–563.
Chen, G., Choi, Y. K., and Zhou, Y. (2005). Nonparametric estimation of structural
change points in volatility models for time series. J. Econom., 126:79–114.
Chong, T. T.-L. (2001). Structural Change in AR(1) Models. Econom. Theory, 17:87–155.
24

Delgado, M. A. and Hidalgo, J. (2000). Nonparametric inference on structural breaks. J.
Econom., 96:113–144.
Döring, M. and Jensen, U. (2015). Smooth change point estimation in regression models
with random design. Ann. Inst. Stat. Math., 67:595–619.
Ferger, D. (2005). Weighted Least Squares Estimators for a Change-Point. Econ. Qual.
Contr., 20:255–270.
Ferger, D. and Stute, W. (1992). Convergence of changepoint estimators. Stochastic
Process. Appl., 42:345–351.
Franke, J., Kreiss, J.-P., Mammen, E., and Neumann, M. (2002). Properties of the
nonparametric autoregressive bootstrap. J. Time Ser. Anal., 23:555–585.
Fryzlewicz, P. (2014). Wild binary segmentation for multiple change-point detection.
Ann. Statist., 42:2243–2281.
Fryzlewicz, P. (2019). Detecting possibly frequent change-points: Wild Binary Segmentation 2 and steepest-drop model selection. preprint on arXiv. https://arxiv.org/
abs/1812.06880.
Hariz, S. B., Wylie, J. J., and Zhang, Q. (2007). Optimal rate of convergence for nonparametric change-point estimators for nonstationary sequences. Ann. Statist., 35:1802–
1826.
Horváth, L., Hušková, M., and Serbinowska, M. (1997). Estimators for the Time of
Change in Linear Models. Statistics, 29:109–130.
Hušková, M. and Antoch, J. (2003). Detection of structural changes in regression. Tatra
Mt. Math. Publ., 26:201–215.
Hušková, M. and Kirch, C. (2008).
Bootstrapping confidence intervals for the
change?point of time series. J. Time Ser. Anal., 29:947–972.
Kirch, C. and Eichinger, B. (2018). A MOSUM procedure for the estimation of multiple
random change points. Bernoulli, 24:526–564.
Kirch, C. and Kamgaing, J. T. (2012). Testing for parameter stability in nonlinear autoregressive models. J. Time Ser. Anal., 33:365–385.
Kirch, C., Muhsal, B., and Ombao, H. (2015). Detection of Changes in Multivariate Time
Series With Application to EEG Data. J. Amer. Statist. Assoc., 110:1197–1216.
Kosorok, M. R. (2008). Introduction to empirical processes and semiparametric inference.
Springer, New York.

25

Kristensen, D. (2009). Uniform convergence rates of kernel estimators with heterogeneous
dependent data. Econom. Theory, 25:1433–1445.
Kristensen, D. (2012). Non-parametric detection and estimation of structural change.
Econom. J., 15:420–461.
Liebscher, E. (1996). Strong convergence of sums of α-mixing random variables with
applications to density estimation. Stochastic Process. Appl., 65:69–80.
Mohr, M. (2018). Changepoint detection in a nonparametric time series regression model.
PhD thesis, University of Hamburg. http://ediss.sub.uni-hamburg.de/volltexte/
2018/9416/.
Mohr, M. and Neumeyer, N. (2019). Consistent nonparametric change point detection combining CUSUM and marked empirical processes. preprint on arXiv. https:
//arxiv.org/abs/1901.08491.
Pang, T. and Zhang, D. (2015). Asymptotic Inferences for an AR(1) Model with a Change
Point and Possibly Infinite Variance. Commun. Stat. - Theory Methods, 44:4848–4865.
Pang, T., Zhang, D., and Chong, T. T.-L. (2014). Asymptotic inferences for an AR(1)
model with a change point: stationary and nearly non-stationary cases. J. Time Ser.
Anal., 35:133–150.

26

