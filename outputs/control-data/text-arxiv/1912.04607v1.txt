CONTROLLING FALSE DISCOVERY EXCEEDANCE FOR
HETEROGENEOUS TESTS

arXiv:1912.04607v1 [stat.ME] 10 Dec 2019

SEBASTIAN DÖHLER AND ETIENNE ROQUAIN
Abstract. Several classical methods exist for controlling the false discovery exceedance
(FDX) for large scale multiple testing problems, among them the Lehmann-Romano procedure Lehmann and Romano (2005) ([LR] below) and the Guo-Romano procedure Guo and
Romano (2007) ([GR] below). While these two procedures are the most prominent, they
were originally designed for homogeneous test statistics, that is, when the null distribution
functions of the p-values Fi , 1 ≤ i ≤ m, are all equal. In many applications, however, the
data are heterogeneous which leads to heterogeneous null distribution functions. Ignoring
this heterogeneity usually induces a conservativeness for the aforementioned procedures. In
this paper, we develop three new procedures that incorporate the Fi ’s, while ensuring the
FDX control. The heterogeneous version of [LR], denoted [HLR], is based on the arithmetic
average of the Fi ’s, while the heterogeneous version of [GR], denoted [HGR], is based on
the geometric average of the Fi ’s. We also introduce a procedure [PB], that is based on
the Poisson-binomial distribution and that uniformly improves [HLR] and [HGR], at the
price of a higher computational complexity. Perhaps surprisingly, this shows that, contrary
to the known theory of false discovery rate (FDR) control under heterogeneity, the way to
incorporate the Fi ’s can be particularly simple in the case of FDX control, and does not
require any further correction term. The performances of the new proposed procedures are
illustrated by real and simulated data in two important heterogeneous settings: first, when
the test statistics are continuous but the p-values are weighted by some known independent
weight vector, e.g., coming from co-data sets; second, when the test statistics are discretely
distributed, as is the case for data representing frequencies or counts.

1. Introduction
1.1. Background. When many statistical tests are performed simultaneously, a ubiquitous
way to account for the erroneous rejections of the procedure is the false discovery proportion
(FDP), that is, the proportion of errors in the rejected sets, as introduced in the seminal paper
Benjamini and Hochberg (1995). Most of the related literature studies the expected value of
this quantity, which is the false discovery rate (FDR), e.g., building procedures that improve
the original Benjamini-Hochberg procedure by trying to adapt to some underlying structure
of the data. In particular, a fruitful direction is to take into account the heterogeneous
structure of the different tests. Heterogeneity may originate from various sources. The two
main examples we have in mind, and which have been intensively investigated in the statistical
literature recently, is heterogeneity caused by p-value weighting and discrete data.
The p-value weighting is a popular approach that can be traced back to Holm (1979) and
that has been further developed specifically for FDR in, e.g., Genovese et al. (2006); Blanchard
and Roquain (2008); Hu et al. (2010); Zhao and Zhang (2014); Ramdas et al. (2017). Here,
the heterogeneity can be for instance driven by sample size, groups, or more generally by
some covariates. In particular, finding optimal weighting in the sense of maximizing the
Date: December 11, 2019.
1

2

SEBASTIAN DÖHLER AND ETIENNE ROQUAIN

number of true rejections has been investigated in Wasserman and Roeder (2006); Rubin
et al. (2006); Roquain and van de Wiel (2009); Ignatiadis et al. (2016); Durand (2019), either
from independent weighting or from the same data set. As a result, the weighted p-values
have heterogeneous null distribution functions {Fi , 1 ≤ i ≤ m} that must be properly taken
into account by multiple testing procedures.
On the other hand, multiple testing for discrete distributions is a well identified research
field Tarone (1990); Westfall and Wolfinger (1997); Gilbert (2005) that has received a growing
attention in the last decade, see, e.g., Heyse (2011); Heller and Gur (2011); Dickhaus et al.
(2012); Habiger (2015); Chen et al. (2015); Döhler (2016); Chen et al. (2018); Döhler et al.
(2018); Durand et al. (2019) and references therein. The most typical setting is the case
for which each test is performed according to a contingency table. In that situation, the
heterogeneity is induced by the fact that marginal counts naturally vary from one table
to another. The approach is then to suitably combine the heterogeneous null distributions
to compensate the natural conservativeness of individual discrete tests. Namely, Heyse’s
approach Heyse (2011) is to consider the transformation
(1)

−1

F (t) = m

m
X

Fi (t), t ∈ [0, 1],

i=1

and to apply BH to the transformed p-values {F (pi ), 1 ≤ i ≤ m}. Unfortunately, the latter
does not rigorously control the FDR, as it has been proven in Döhler (2016); Döhler et al.
(2018). Appropriate corrections of the F expression have been proposed in Döhler et al.
(2018) in order to recover rigorous FDR control.
1.2. FDX control. A common criticism of FDR is that it captures only the average behavior
of the FDP. In particular, controlling the FDR does not prevent the FDP from possessing
undesirable fluctuations and we may aim to stochastically control of FDP in other ways. A
classical approach is to control the false probability exceedance (FDX) in the following sense:
for α, ζ ∈ (0, 1),
(2)

FDX = P(FDP > α) ≤ ζ.

This corresponds to control the (1 − ζ)-quantile of the FDP distribution at level α, see, e.g.,
Genovese and Wasserman (2004); Perone Pacifico et al. (2004); Korn et al. (2004); Lehmann
and Romano (2005); Genovese and Wasserman (2006); Romano and Wolf (2007); Guo et al.
(2014); Delattre and Roquain (2015). Let us also mention that the probabilistic fluctuation
of the FDP process is of interest in its own, see, e.g., Neuvial (2008); Roquain and Villers
(2011); Delattre and Roquain (2011); Delattre and Roquain (2015); Ditzhaus and Janssen
(2019).
Among multiple testing procedures, step-down procedures have been shown to be particularly useful for FDX control. Two prominent step-down procedures have been proven to
control the FDX under different distributional assumptions:
• The Lehmann-Romano procedure [LR], introduced in Lehmann and Romano (2005),
is defined as the step-down procedure with critical values
bα`c + 1
(3)
, 1 ≤ ` ≤ m,
τ`LR = ζ
m(`)
where we denote
(4)

m(`) = m − ` + bα`c + 1.

CONTROLLING FALSE DISCOVERY EXCEEDANCE FOR HETEROGENEOUS TESTS

3

It has been shown to control the FDX under various dependence assumptions between
the p-values, e.g., when each p-value under the null is independent of the family of the
p-values under the alternative (Theorem 3.1 in Lehmann and Romano (2005)), which
we will refer to (Indep0) below, or when the Simes inequality holds true among the
family of true null p-values (Theorem 3.2 in Lehmann and Romano (2005)). 1
• The procedure [LR] has been improved by the Guo-Romano procedure [GR], see Guo
and Romano (2007), defined as the step-down procedure with critical values
(5)

τ`GR = max{t ∈ [0, 1] : P(Bin[m(`), t] ≥ bα`c + 1) ≤ ζ}, 1 ≤ ` ≤ m,
where Bin[n, p] denotes any variable following a binomial distribution with parameters
n and p. While making more rejections, the procedure [GR] controls the FDX under
a stronger assumption: the null p-value family and the alternative p-value family are
independent and that the null p-values are independent, which we will refer to (Indep)
below.

1.3. Contributions. The global aim of the paper is to improve procedures [LR] and [GR] by
incorporating the null distribution functions {Fi , 1 ≤ i ≤ m} of the p-values while maintaining
rigorous FDX control. More specifically, the contributions of this work are as follows:
• we introduce the heterogeneous Lehmann Romano procedure [HLR], which controls
the FDX under (Indep0) and is an uniform improvement of [LR] (when the marginals
of the null p-values are super-uniform, see (SuperUnif) further on);
• we introduce the heterogeneous Guo Romano procedure [HGR], which controls the
FDX under (Indep) and is an uniform improvement of [GR] (under (SuperUnif));
• at the price of additional computational complexity, we introduce the Poisson-binomial
procedure [PB], which controls the FDX under (Indep) and is a uniform improvement
of [HLR] and [HGR];
• we apply this new technology to weighted p-values to provide the first weighted procedures that control the FDX (to our knowledge), called [wLR] and [wGR]. They
are able to improve their non-weighted counterparts [LR] and [GR], respectively, see
Section 4;
• in the discrete context, our new procedures re-named [DLR], [DGR] are shown to
be uniform improvements with respect to the continuous procedures [LR] and [GR],
respectively. To the best of our knowledge, these are the first FDX controlling procedures tailored specifically to discrete p-value distributions. The amplitude of the
improvement can be substantial, as we show both with simulated and real data examples, see Section 5.
The paper is organized as follows: Section 2 introduces the statistical setting, the procedures
and FDX criterion, as well as a shortcut to compute our step-down procedures without
evaluating the critical values. Section 3 is the main section of the paper, which introduces
the new heterogeneous procedures and their FDX controlling properties. Our methodology
is then applied in two particular frameworks: new weighted procedures controlling the FDX
are derived in Section 4 while Section 5 is devoted to the case where the tests are discrete.
Both sections include numerical illustrations. A discussion is provided in Section 6 and most
1Under the latter condition, it has also been proven later that the step-up version of [LR], that is, the

step-up procedure using the critical values (3) also controls the FDX, see the proof of Theorem 3.1 in Guo
et al. (2014).

4

SEBASTIAN DÖHLER AND ETIENNE ROQUAIN

of technical details are deferred to Section 7. Appendix A gives additional numerical details
for simulations.
2. Framework
2.1. Setting. We use here a classical formal setting for heterogeneous nulls, see, e.g., Döhler
et al. (2018). We observe X, defined on an abstract probabilistic space, valued in an observation space (X , X) and of distribution P that belongs to a set P of possible distributions. We
consider m null hypotheses for P , denoted H0,i , 1 ≤ i ≤ m, and we denote the corresponding
set of true null hypotheses by H0 (P ) = {1 ≤ i ≤ m : H0,i is satisfied by P }. We also denote
by H1 (P ) the complement of H0 (P ) in {1, . . . , m} and by m0 (P ) = |H0 (P )| the number of
true nulls.
We assume that there exists a set of p-values that is, a set of random variables {pi (X), 1 ≤
i ≤ m}, valued in [0, 1]. We introduce the following dependence assumptions between the
p-values:
(Indep0)
for all P ∈ P, {pi (X), i ∈ H0 (P )} is independent of {pi (X), i ∈ H1 (P )};
(Indep)
(Indep0) holds and for all P ∈ P, {pi (X), i ∈ H0 (P )} consists of independent variables.
Note that (Indep0) and (Indep) are both satisfied when all the p-values pi (X), 1 ≤ i ≤ m, are
mutually independent in the model P. The (maximum) null cumulative distribution function
of each p-value is denoted
(6)

Fi (t) =

sup

{PX∼P (pi (X) ≤ t)}, t ∈ [0, 1], 1 ≤ i ≤ m.

P ∈P : i∈H0 (P )

We let F = {Fi , 1 ≤ i ≤ m} that we supposed to be known and we consider the following
possible situations for the functions in F:
(Cont)

for all i ∈ {1, . . . , m}, Fi is continuous on [0, 1]

(Discrete)

for all i ∈ {1, . . . , m}, there exists some countable set Ai ⊂ [0, 1] such that
Fi is a step function, right continuous, that jumps only at some points of Ai .

The case (Discrete) typically arises when for all P ∈ P and i ∈ {1, . . . , m}, PX∼P (pi (X) ∈
Ai ) = 1. Throughout the paper, we will assume that we are either in the case (Cont) or
(Discrete) and we denote A = ∪m
i=1 Ai , with by convention Ai = [0, 1] when (Cont) holds. For
comparison with the homogeneous case, we also let
(SuperUnif)

for all i ∈ {1, . . . , m}, Fi (t) ≤ t for all t ∈ [0, 1].

2.2. False Discovery Exceedance and step-down procedures. In general, a multiple
testing procedure is defined as a random subset R = R(X) ⊂ {1, . . . , m} which corresponds
to the indices of the rejected nulls. For α ∈ (0, 1), the false discovery exceedance of R is
defined as follows:


|R(X) ∩ H0 (P )|
(7)
> α , P ∈ P.
FDXα (R, P ) = PX∼P
|R(X)| ∨ 1
In this paper, we consider particular multiple testing procedures, called step-down procedures. Given some p-value family (pi )1≤i≤m and some non-decreasing sequence (τ` )1≤`≤m ∈

CONTROLLING FALSE DISCOVERY EXCEEDANCE FOR HETEROGENEOUS TESTS

5

[0, 1]m , the step-down procedure with critical values (τ` )1≤`≤m ∈ [0, 1]m rejects the null hypotheses corresponding to the set
(8)
(9)

R = {i ∈ {1, . . . , m} : pi (X) ≤ τ`ˆ}
`b = max{` ∈ {0, . . . , m} : ∀`0 ≤ `, pσ(`0 ) ≤ τ`0 },

(convention pσ(0) = 0),

for which pσ(1) ≤ · · · ≤ pσ(m) denotes the p-values {pi (X), 1 ≤ i ≤ m} ordered increasingly
(for some data-dependent permutation σ).
2.3. Transformation function family and computational shortcut. In this paper, the
critical values will be obtained by inverting some functional, that is,
(10)

τ` = ξ`−1 (ζ) = max{t ∈ A : ξ` (t) ≤ ζ}, (τ` = 0 if the set is empty), 1 ≤ ` ≤ m,

for ξ` : [0, 1] 7→ [0, ∞), 1 ≤ ` ≤ m, a given set of functions. In order for (10) to be welldefined and ` 7→ τ` to be nondecreasing, we will say that the function set {ξ` , 1 ≤ ` ≤ m} is
a transformation function family if it satisfies the following conditions:
(11)

for all ` ∈ {1, . . . , m}, ξ` is a non-decreasing function;
for all t ∈ [0, 1] and all ` ∈ {1, . . . , m − 1}, we have ξ`+1 (t) ≤ ξ` (t);
in case (Cont), for all ` ∈ {1, . . . , m}, ξ` is continuous on [0, 1].

For instance, the critical values of the procedure [LR] can be rewritten as (10) for the
functions
m(`)
t, t ∈ [0, 1], 1 ≤ ` ≤ m.
(12)
ξ`LR (t) =
bα`c + 1
We easily check that the function set {ξ`LR , 1 ≤ ` ≤ m} is a family of transformation functions
is non-increasing both in ` ∈ {1, . . . , m} and i ∈
(in the sense of (11)). Indeed, m−`+i
i
{1, . . . , bαmc + 1}. A second example is given by the procedure [GR] for which
(13)

ξ`GR (t) = P(Bin[m(`), t] ≥ bα`c + 1), 1 ≤ ` ≤ m, t ∈ [0, 1],

can be proved to form a family of transformation functions. Indeed, the only non-obvious
GR
argument to prove (11) is that for a fixed t ∈ [0, 1], and ` ∈ {1, . . . , m − 1} we have ξ`+1
(t) ≤
GR
ξ` (t). This comes from the fact that P(Bin[m−`+i, t] ≥ i) = P(Bin[m−`+i, 1−t] ≤ m−`)
is non-increasing both in i and `.
Finally, because of the inversion, computing the critical values via (10) can be time consuming. Fortunately, computing the critical values is actually not necessary if we are solely
interested in determining the rejection set R given by (8). As the following result shows, we
may determine R by working directly with the transformation functions.
Proposition 2.1. Let us consider any transformation function family {ξ` , 1 ≤ ` ≤ m} and
the corresponding critical values τ` , 1 ≤ ` ≤ m, defined by (10). Then, for all P ∈ P, with P probability 1, the step-down procedure R with critical values (τ` )1≤`≤m can equivalently written
as
(14)

R = {i ∈ {1, . . . , m} : p̃i ≤ ζ};

(15)

p̃i = max {ξ` (pσ(`) )}, 1 ≤ i ≤ m.
1≤`≤m
pσ(`) ≤pi

Proposition 2.1 is proved in Section 7.2.

6

SEBASTIAN DÖHLER AND ETIENNE ROQUAIN

3. New FDX controlling procedures
In this section, we introduce new procedures R that control the false discovery exceedance
at some level ζ ∈ (0, 1), that is,
for all P ∈ P, FDXα (R, P ) ≤ ζ,

(16)

while incorporating the family {Fi , 1 ≤ i ≤ m} in an appropriate way.
3.1. Tool. Our main tool is the following bound: For any step-down procedure R with critical
values τ = (τ` )1≤`≤m , we have
(17)

sup {FDXα (R, P )} ≤ B(τ, α)
P ∈P


(18)

for B(τ, α) = sup
1≤`≤m

sup
P ∈P
|H0 (P )|≤m(`)

PX∼P 


X

1{pi (X) ≤ τ` } ≥ bα`c + 1 .

i∈H0 (P )

Inequality (17) is valid under the distributional assumption (Indep0). This bound comes
from a reformulation of Theorem 5.2 in Roquain (2011) in our heterogenous framework, see
Theorem 7.1 below. Our new procedures are derived by further upper-bounding B(τ, α)
via various probabilistic devices. More specifically, we will introduce several transformation
function families {ξ` , 1 ≤ ` ≤ m} such that for all τ = {τ` }` ,
B(τ, α) ≤ sup {ξ` (τ` )}.
1≤`≤m

According to (17), the step-down procedure using the corresponding critical values (10) will
then control the FDX in the sense of (16).
3.2. Heterogeneous Lehmann-Romano procedure. By using the Markov inequality, we
obtain
P
Pm(`)
i∈H0 (P ) Fi (τ` )
j=1 (F (τ` ))(j)
(19)
= sup
,
B(τ, α) ≤ sup
sup
bα`c + 1
bα`c + 1
P ∈P
1≤`≤m
1≤`≤m
|H0 (P )|≤m(`)

where (F (t))(1) ≥ · · · ≥ (F (t))(m) denotes the values of {Fi (t), 1 ≤ i ≤ m} ordered decreasingly. Bounding the above quantity by ζ entails the following procedure.
Definition 3.1. The heterogeneous Lehmann-Romano procedure, denoted by [HLR], is defined
as the step-down procedure using the critical values defined by
(20)
(21)

τ`HLR = max{t ∈ A : ξ`HLR (t) ≤ ζ}, 1 ≤ ` ≤ m;
Pm(`)
j=1 (F (t))(j)
HLR
ξ` (t) =
, 1 ≤ ` ≤ m, t ∈ [0, 1],
bα`c + 1

where (F (t))(1) ≥ · · · ≥ (F (t))(m) denotes the values of {Fi (t), 1 ≤ i ≤ m} ordered decreasingly
and m(`) is defined by (4).
The quantity ξ`HLR (t) is thus similar to ξ`LR (t), in which t has been replaced by the average of
the m(`) largest values of {Fi (t), 1 ≤ i ≤ m}. To check that the functions ξ`HLR form a transfor1 Pm(`)
mation function family in the sense of (11), we note that m(`)
j=1 (F (t))(j) is non-increasing
in ` (averaging on smaller values makes the average smaller) and continuous in t under (Cont)

CONTROLLING FALSE DISCOVERY EXCEEDANCE FOR HETEROGENEOUS TESTS

7

P
(because t 7→ (Fi (t))1≤i≤m is continuous and x ∈ (Rm , k · k∞ ) 7→ N −1 N
k=1 x(k) ∈ (R, | · |) is
1-Lipschitz).
In the classical case (SuperUnif), we have ξ`HLR (t) ≤ ξ`LR (t) for all t ∈ [0, 1] and 1 ≤ ` ≤ m.
Hence, [HLR] is less conservative than [LR] in that situation. A technical detail is that this
only holds almost surely because the range A in (20) can be different from [0, 1] in the case
(Discrete). This entails the following result.
Proposition 3.1. In the setting defined in Section 2.1, the procedure [HLR] satisfies the
following
• Under (Indep0), [HLR] controls the FDX in the sense (16);
• Under (SuperUnif), the set of nulls rejected by [HLR] contains the one of [LR] with
P -probability 1, for all P ∈ P.
3.3. Poisson-binomial procedure. Here, we propose to bound (18) by using the Poissonbinomial distribution. To this end, recall that the Poisson-Binomial distribution of parameters
P
π = (πi )1≤i≤n ∈ [0, 1]n , denoted PBin[π] below, corresponds to the distribution of ni=1 εi ,
where the εi are all independent and each εi follows a Bernoulli distribution of parameter πi
for 1 ≤ i ≤ n.
First note that for all i ∈ H0 (P ) and t ∈ [0, 1], we have that 1{pi (X) ≤ t} is stochastically
upper bounded by a Bernoulli variable of parameter Fi (t), see (6). As a consequence, by
assuming (Indep), we have for all critical values (τ` )1≤`≤m ,
B(τ, α) ≤ sup

sup

P (PBin [(Fi (τ` ))i∈A ] ≥ bα`c + 1)

1≤`≤m A⊂{1,...,m}
|A|≤m(`)

(22)




= sup P PBin ((F (τ` ))(j) )1≤j≤m(`) ≥ bα`c + 1 .
1≤`≤m

Bounding the latter by ζ leads to the following procedure.
Definition 3.2. The Poisson-binomial procedure, denoted by [PB], is defined as the step-down
procedure using the critical values
(23)
(24)

τ`PB = max{t ∈ A : ξ`PB (t) ≤ ζ}, 1 ≤ ` ≤ m;



ξ`PB (t) = P PBin ((F (t))(j) )1≤j≤m(`) ≥ bα`c + 1 , 1 ≤ ` ≤ m, t ∈ [0, 1],

where (F (t))(1) ≥ · · · ≥ (F (t))(m) denotes the values of {Fi (t), 1 ≤ i ≤ m} ordered decreasingly
and m(`) is defined by (4).
Let us now check that {ξ`PB , 1 ≤ ` ≤ m} is a transformation function family, that is,
satisfy (11). The continuity assumption holds because, under (Cont), the mapping t ∈
[0, 1] 7→ ((F (t))(j) )1≤j≤m(`) is continuous (argument similar to above) and the cumulative
distribution function of PBin[π] is a continuous function of π ∈ [0, 1]n . The monotonic

HGR
property ξ`+1
(t) ≤ ξ`HGR (t) comes from the fact that P(PBin ((F (t))(j) )1≤j≤m−`+i ≥ i) =


P(PBin (1 − (F (t))(j) )1≤j≤m−`+i ≤ m − `) is non-increasing both in i and `.


Since under (SuperUnif), the distribution PBin ((F (t))(j) )1≤j≤m(`) is stochastically smaller
than the distribution Bin[m(`), t], the following holds.
Proposition 3.2. In the setting defined in Section 2.1, the procedure [PB] satisfies the following
• Under (Indep), [PB] controls the FDX in the sense (16);

8

SEBASTIAN DÖHLER AND ETIENNE ROQUAIN

• Under (SuperUnif), the set of nulls rejected by [PB] contains the one of [GR] with
P -probability 1, for all P ∈ P.
However, in general, the procedure [PB] is computationally demanding, even with the
shortcut mentioned in Section 2.3. This comes from the computation of ξ`PB (t) which involves
the distribution function of a Poisson-binomial variable. In the next section, we make [PB]
slightly more conservative for recovering the computational price of [GR].
3.4. Heterogeneous Guo-Romano procedure. In this section, we further upper-bound
(25) byh using that any PBin [(πii )1≤i≤n ] random variable is stochastically upper-bounded by
Q
a Bin n, 1 − ( ni=1 (1 − πi ))1/n random variable (see Example 1.A.25 in Shaked, M. and
Shanthikumar, J.G. (2007)). This yields

h
i

(25)
B(τ, α) ≤ sup P Bin m(`), F̃m(`) (τ` ) ≥ bα`c + 1 ,
1≤`≤m

where we let

(26)

F̃j (t) = 1 − 

j
Y

1/j
(1 − (F (t))(j 0 ) )

, 1 ≤ j ≤ m, t ∈ [0, 1],

j 0 =1

where (F (t))(1) ≥ · · · ≥ (F (t))(m) denotes the values of {Fi (t), 1 ≤ i ≤ m} ordered decreasingly.
This reasoning suggests another heterogeneous procedure, based on the binomial distribution. Since [GR] also uses the binomial device, we name this new procedure the heterogeneous
Guo-Romano procedure.
Definition 3.3. The heterogeneous Guo-Romano procedure, denoted by [HGR], is defined as
the step-down procedure using the critical values defined by
(27)
(28)

τ`HGR = max{t ∈ A : ξ`HGR (t) ≤ ζ}, 1 ≤ ` ≤ m;

h
i

ξ`HGR (t) = P Bin m(`), F̃m(`) (t) ≥ bα`c + 1 , 1 ≤ ` ≤ m, t ∈ [0, 1],

where F̃j (t) is defined in (26) and m(`) is defined by (4).
The condition (11) also holds in that case. However, the proof of monotonicity of ξ`HGR (t)
is slightly more involved than above and is deferred to Lemma 7.1. In addition, since under
(SuperUnif) we have F̃m(`) (t) ≤ t, we deduce that [HGR], although more conservative than
[PB], is still a uniform improvement over [GR].
Proposition 3.3. In the setting defined in Section 2.1, the procedure [HGR] satisfies the
following
• Under (Indep), [HGR] controls the FDX in the sense (16);
• Under (SuperUnif), the set of nulls rejected by [HGR] contains the one of [GR] with
P -probability 1, for all P ∈ P.
Remark 3.1. The numerical results in Sections 4 and 5 suggest that the conservatism of
[HGR] with respect to [PB] is usually quite small. In addition, since the computational effort
required by [HGR] is comparable to that of [GR], the gain in efficiency may be great, especially
for large m. We therefore think that [HGR] may be especially useful for very high dimensional
heterogeneous data.

CONTROLLING FALSE DISCOVERY EXCEEDANCE FOR HETEROGENEOUS TESTS

9

Remark 3.2. We can also define a non-adaptive version of [HGR], defined as the step-down
procedure of critical values (10) based on the transformation functional

h
i

ξ` (t) = P Bin m, F̃ (t) ≥ bα`c + 1 , 1 ≤ ` ≤ m, t ∈ [0, 1],
Q
1/m
m
. While being more conservative than [HGR], it still
(1
−
F
(t))
where F̃ (t) = 1 −
j
j=1
controls the FDX in the sense (16) . So, while controlling the FDR is linked to the arithmetic
average of the Fi ’s (Heyse’s procedure, see text below (1)), this shows that controlling the
FDX is linked to geometric averaging. In addition, this shows that the situation is even more
simple for FDX, because no further correction is needed here, whereas the arithmetic average
should be slightly modified in order to yield a rigorous FDR control Döhler et al. (2018).
4. Application to weighting
It is well known that p-value weighting can improve the power of multiple testing procedures, see, e.g., Genovese et al. (2006); Roquain and van de Wiel (2009); Ignatiadis et al.
(2016); Durand (2017); Ramdas et al. (2017) and references therein. However, to the best of
our our knowledge, except for the augmentation approach described in Genovese et al. (2006),
no methods are available that incorporate weighting for FDX control. We show in this section that such methods can be obtained directly from the bounds on B(τ, α) introduced in
Section 3.
Throughout this section, we assume that we have at hand a p-value family satisfying (Cont)
and (SuperUnif). As explained in our introduction section (see references therein), while
the null distributions of the p-values are typically uniform, the point is that they can have
heterogeneous alternative distributions, so that it could be desirable to weigh the p-values in
some way. For this, we consider a fixed weight vector (wi )1≤i≤m ∈ Rm
+ . The ordered
P weights
are denoted w(1) ≥ w(2) ≥ · · · ≥ w(m) , the average weight is denoted w = m−1 m
i=1 wi and
Pj
−1
the average over the j largest weights is denoted by wj = j
j 0 =1 w(j 0 ) .
Since the heterogeneous procedures [HLR], [PB] and [HGR] introduced in Section 3 yield
valid control for any collection of distribution functions {Fi , 1 ≤ i ≤ m}, it is possible to use
very flexible weighting schemes. In order to limit the scope of this paper, we consider only
two simple types of weighting approaches in more detail:
• for arithmetic mean weighting (abbreviated in what follows as AM), we define the
weighted p-value family as
(29)

(30)

pw
i = pi w̄/wi , 1 ≤ i ≤ m.
The weighted p-values thus have the heterogeneous distribution functions
w 
i
FiAM (t) =
t ∧ 1, 1 ≤ i ≤ m,
w̄
under the null. This corresponds to classical weighting approaches established for
FWER and FDR control.
• for geometric mean weighting (abbreviated as GM), we define

(31)

w̄/wi
pw
, 1 ≤ i ≤ m.
i = 1 − (1 − pi )

10

SEBASTIAN DÖHLER AND ETIENNE ROQUAIN

The weighted p-values therefore have the following heterogeneous distribution functions under the null:
FiGM (t) = 1 − (1 − t)wi /w̄ , 1 ≤ i ≤ m.

(32)

Thus, combining these two weighting approaches with the three heterogeneous procedures
introduced in the previous section yields a total of six weighted procedures which we discuss
in more detail below. Note that a Taylor expansion provides FiAM (t) ≈ FiGM (t) for small
values of t. Therefore, we expect that AM and GM procedures will yield similar rejection sets
for small p-values.
4.1. Weighted Lehmann-Romano procedures. Using (17) and (19) (Markov device),
we get that any step-down procedure using the weighted p-values (29) and critical values
(τi )1≤i≤m has a FDX smaller than or equal to
m(`)
m(`) 

X
X
w(j) 
wm(`)
1
m(`)
1
AM
(F (t))(j) =
t ∧1 ≤
×
t =: ξ`wLR-AM (t).
bα`c + 1
bα`c + 1
w̄
bα`c + 1
w̄
j=1

j=1

Since the ξ`wLR-AM form a transformation function family, bounding the latter by ζ leads to
an FDX controlling procedure, that we call the AM-weighted Lehmann-Romano procedure,
denoted by [wLR-AM] in the sequel. It thus corresponds to the step-down procedure using
the weighted p-values (29) and the critical values
bα`c + 1
w̄
τ`wLR-AM = ζ Pm(`)
w̄ = τ`LR ·
,
wm(`)
j=1 w(j)

1 ≤ ` ≤ m.

In particular, if the weight vector is uniform, that is, wi = 1 for all i, then [wLR-AM] reduces
to [LR].
Similarly to above, using the GM weighting (31) gives an FDX smaller than or equal to
m(`)
m(`)
X
X
1
1
GM
(F
(t))(j) =
(1 − (1 − t)w(j) /w̄ ) =: ξ`wLR-GM (t).
bα`c + 1
bα`c + 1
j=1

j=1

This gives rise to the GM-weighted Lehmann-Romano procedure, denoted [wLR-GM], defined
as the step-down procedure using the weighted p-values (31) and the critical values
τ`wLR-GM = max{t ∈ [0, 1] : ξ`wLR-GM (t) ≤ ζ}, 1 ≤ ` ≤ m.
In general, no domination relationship holds between [wLR-GM] and [wLR-AM]. Finally,
again, in case of uniform weighting, [wLR-GM] reduces to [LR].
4.2. Weighted Poisson-binomial procedures. Applying the strategy of Section 3.3 with
the c.d.f. sets {FiAM , 1 ≤ i ≤ m} and {FiGM , 1 ≤ i ≤ m}, we can use the two transformation
function families given by





w(j) 
wPB-AM
ξ`
(t) = P PBin
t ∧1
≥ bα`c + 1 , 1 ≤ ` ≤ m, t ∈ [0, 1];
w̄
1≤j≤m(`)

h
i

ξ`wPB-GM (t) = P PBin (1 − (1 − t)w(j) /w̄ )1≤j≤m(`) ≥ bα`c + 1 , 1 ≤ ` ≤ m, t ∈ [0, 1],
to define new step-down procedures, denoted [wPB-AM] and [wPB-GM] respectively, that
both ensure FDX control.

CONTROLLING FALSE DISCOVERY EXCEEDANCE FOR HETEROGENEOUS TESTS

11

4.3. Weighted Guo-Romano procedures. We apply here the strategy of Section 3.4 for
the c.d.f. sets {FiAM , 1 ≤ i ≤ m} and {FiGM , 1 ≤ i ≤ m}. According to (26), let us define
1/j

j 

w 0 
Y
(j )
t ∧ 1  , 1 ≤ j ≤ m, t ∈ [0, 1];
F̃jAM (t) = 1 − 
(1 −
w̄
0
j =1


F̃jGM (t) = 1 − 

j
Y

1/j
(1 − t)w(j 0 ) /w̄ 

= 1 − (1 − t)wj /w̄ , 1 ≤ j ≤ m, t ∈ [0, 1].

j 0 =1

This gives rise to the transformation function families

h
i

AM
ξ`wGR-AM (t) = P Bin m(`), F̃m(`)
(t) ≥ bα`c + 1 , 1 ≤ ` ≤ m, t ∈ [0, 1];
i


h
GM
(t) ≥ bα`c + 1 , 1 ≤ ` ≤ m, t ∈ [0, 1].
ξ`wGR-GM (t) = P Bin m(`), F̃m(`)
Critical values τ wGR-AM and τ wGR-GM are obtained via (10) from families ξ wGR-AM and ξ wGR-GM ,
respectively. This yields two new FDX controlling step-down procedures that are denoted by
[wGR-AM] and [wGR-GM], respectively. Note that, similar to arithmetic weighting for the
[LR] procedure, geometric weighting leads to a simple transformation of the original [GR]
critical values, given by
w̄/wm(`)
(33)
τ`wGR-GM = 1 − 1 − τ`GR
.
Thus, this particular procedure combines simplicity with a close relationship to the original
Guo-Romano procedure. By contrast, as for the heterogeneous version, the weighted Poissonbinomial procedures require the evaluation of the Poisson-binomial distribution function which
may be computationally demanding for large m. The weighted Guo-Romano procedures, on
the other hand, while possibly sacrificing some power, only require evaluation of the standard
binomial distribution.
4.4. Analysis of RNA-Seq data. We revisit an analysis of the RNA-Seq data set ’airway’
using results from the independent hypothesis weighting (IHW) approach (for details, see
Ignatiadis et al. (2016) and the vignette accompanying its software implementation). Loosely
speaking, this method aims to increase power by assigning a weight wi to each hypothesis
and subsequently applying e.g. the Bonferroni or the Benjamini-Hochberg procedure [BH] to
the weighted p-values while aiming for control of FWER or FDR.
In what follows, we present some results for weighted FDX control, using the procedures
introduced in Sections 4.2 and 4.3. For this data set we have m = 64102 and the weights
w1 , . . . , wm are taken from the output of the ihw function from the bioconductor package
’IHW’. For the sake of illustration we assume the p-values to be independent. A large portion
(about 45%) of these weights are 0, Figure 1 presents a histogram of the (strictly) positive
weights.
Table 1 shows that controlling the mean (i.e. FDR) or the median of the FDP leads to
similar number of rejections. For both error rates, incorporating weights leads to similar
gains in power. For weighted FDX control, the more conservative weighted Guo-Romano
procedures exhibit only a slight loss of power with respect to the weighted Poisson-binomial
approaches. The difference between arithmetic and geometric weighting is negligible for this
data.

12

SEBASTIAN DÖHLER AND ETIENNE ROQUAIN

2000
0

1000

Frequency

3000

4000

Histogram of weights > 0

0.0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

weights

Figure 1. Histogram of positive weights generated by the ihw function for
the airway data
Table 1. Number of rejections for the airway data. The FDR procedures
control FDR at level 10%, the FDX procedures control P (FDP > 10%) ≤ 0.5.

[BH] [wBH] [GR] [wPB-AM] [wPB-GM] [wGR-AM] [wGR-GM]
Rejections 4099

4896

4243

4868

4865

4853

4852

Figure 2 indicates that for the FDX controlling procedures, the mapping of the confidence
level to the number of rejections is quite flat. This means that statements about the FDP
can be made with high confidence without losing too much power. For instance, requiring
that FDP ≤ 10% with confidence at least 95% still allows for 4145 and 4771 rejections using
the [GR] and [wGR-GM] procedures.
5. Application to discrete tests
5.1. Discrete FDX procedures. Discrete FDX procedures can be defined in a straightforward way by directly using the distribution functions F1 , . . . , Fm of the discretely distributed
p-values. The prototypical example we have in mind are multiple conditional tests like Fisher’s
exact test. In this case, discreteness and heterogeneity arise from conditioning on the observed
table margins. We denote the resulting heterogeneous procedures from section 3 by [DLR]
(for [HLR]), [DPB] (for [HPB]) and [DGR] (for [HGR]).

CONTROLLING FALSE DISCOVERY EXCEEDANCE FOR HETEROGENEOUS TESTS

13

4000
3000
2000
1000

number of rejections

5000

6000

airway data: Controlling P( FDP > 0.1) ≤ ζ

0

Guo−Romano (uniform weights)
Weighted Guo−Romano
BH (uniform weights)
Weighted BH

0.0

0.2

0.4

0.6

0.8

1.0

ζ

Figure 2. Number of rejections (y-axis) for the airway data when using the
[GR] and [wGR-GM] procedure. Both procedures control the tail probabilities
(on the x-axis) for the FDP exceeding 10%. The horizontal lines represent the
rejections of the BH and weighted BH procedures at FDR-level 10%.

5.2. Simulation study. We now investigate the power of the [DLR], [DPB] and [DGR] procedures in a simulation study similar to those described in (Gilbert, 2005), (Heller and Gur,
2011) and (Döhler et al., 2018). We focus on comparing the performance of the new discrete
procedures to their continuous counterparts. Since the analysis with [DPB] is computationally demanding, we are also interested in investigating the performance of the slightly more
conservative, but numerically more efficient [DGR] procedure. Finally, as above, we also
include [BH] (Benjamini-Hochberg procedure) as a benchmark.
5.2.1. Simulated Scenarios. We simulate a two-sample problem in which a vector of m independent binary responses (“adverse events”) is observed for each subject in two groups,
where each group consists of N = 25 subjects. Then, the goal is to simultaneously test the m
null hypotheses H0i : “p1i = p2i ”, i = 1, . . . , m, where p1i and p2i are the success probabilities
for the ith binary response in group 1 and 2, respectively. Before we describe the simulation
framework in more detail, we explain how this set-up leads to discrete and heterogeneous
p-value distributions. Suppose we have simulated two vectors of dimension m where each
component represents a count in {0, . . . , 25}. This data can be represented by m contingency
tables. Now each hypothesis is tested using Fisher’s exact test (two-sided) for each contingency table, which is performed by conditioning on the (simulated) pair of marginal counts.
Thus, we can determine for every contingency table i the discrete distribution function Fi of

14

SEBASTIAN DÖHLER AND ETIENNE ROQUAIN

the p-values for Fisher’s exact test under the null hypothesis. For differing (simulated) contingency tables, these induced distributions will generally be heterogeneous and our inference
is conditionally on the marginal counts.
We take m = 800, 2000 where m = m1 + m2 + m3 and data are generated so that the
response is Bernoulli(0.01) at m1 positions for both groups, Bernoulli(0.10) at m2 positions
for both groups and Bernoulli(0.10) at m3 positions for group 1 and Bernoulli(q) at m3
positions for group 2 where q = 0.15, 0.25, 0.4 represents weak, moderate and strong effects,
respectively. The null hypothesis is true for the m1 and m2 positions while the alternative
hypothesis is true for the m3 positions. We also take different configurations for the proportion
of false null hypotheses, m3 is set to be 10%, 30% and 80% of the value of m, which represents
small, intermediate and large proportion of effects, respectively (the proportion of true nulls
π0 is 0.9, 0.7, 0.2, respectively). Then, m1 is set to be 20%, 50% and 80% of the number of
true nulls (that is, m − m3 ) and m2 is taken accordingly as m − m1 − m3 .
For each of the 54 possible parameter configurations specified by m, m3 , m1 and q, 10000
Monte Carlo trials are performed, that is, 10000 data sets are generated and for each data
set, an unadjusted two-sided p-value from Fisher’s exact test is computed for each of the m
positions, and the multiple testing procedures mentioned above are applied at level α = 0.05.
The power of each procedure was estimated as the fraction of the m3 false null hypotheses that
were rejected, averaged over the 10000 simulations (TDP, true discovery proportion). Note
that while our procedures are designed to control the FDP conditionally on the marginal
counts, our power results are presented in an unconditional way for the sake of simplicity. For
random number generation the R-function rbinom was used. The two-sided p-values from
Fisher’s exact test were computed using the R-function fisher.test.
5.2.2. Results. Table 3 in Appendix A shows that the (average) power of the compared procedures depends primarily on the strength of the signal q3 ∈ {0.15, 0.25, 0.4}. More specifically,
Figure 3 contains some typical plots of the simulation results.
• For q3 = 0.15, the power of [BH], [LR] and [GR] is practically zero, whereas the discrete
procedures are able to reject at least a few hypotheses, see panel (a) of Figure 3.
• For q3 = 0.25, the power of [BH] and [LR] stays close to zero, [GR] performs slighty
better and the discrete variants perform best as illustrated in panel (b) of Figure 3.
• For q3 = 0.4, the power of [LR] stays close to zero, while [BH] now rejects a significant
amount of hypotheses. The [DPB] and [DGR] procedures perform best. If there is a
large amount of alternatives, [GR] performs better than [DLR] (see panel (c) of Figure
3) in the other cases, [GR] is outperformed by [DLR] (see panel (d) of Figure 3).
• There is no relevant difference in power between the procedures [DPB] and [DGR].
In summary, these results show that for [LR] and [GR], significant improvements are possible
by using discreteness.
5.3. Analysis of pharmacovigilance data. We revisit the analysis of pharmacovigilance
data from Heller and Gur (2011) presented in Döhler et al. (2018). This data set is obtained
from a database for reporting, investigating and monitoring adverse drug reactions due to the
Medicines and Healthcare products Regulatory Agency in the United Kingdom. It contains
the number of reported cases of amnesia as well as the total number of adverse events reported
for each of the m = 2446 drugs in the database. For a more detailed description of the data
which is contained in the R-packages Heller et al. (2012) and Durand and Junge (2019) we
refer to Heller and Gur (2011). The works Heller and Gur (2011) and Döhler et al. (2018)

CONTROLLING FALSE DISCOVERY EXCEEDANCE FOR HETEROGENEOUS TESTS

0.04

●

●

●

●

●

●

●

●

●

●

●

●

●

●

[DPB]

[DGR]

●
●

●
●

●

●

[DPB]

[DGR]

TDP

0.03

●

●
●

0.05

0.02

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

[BH]

[LR]

0.00

●

[BH]

[LR]

[DLR]

(c)

[GR]

FDX procedure

●
●
●

[DLR]

(d)

[GR]

FDX procedure

0.7

[DGR]

●
●
●

●
●
●
●

0.0

●
●
●
●
●
●

●
●
●
●
●
●
●
●

[LR]

●

●
●
●
●
●
●

●
●
●
●
●
●

●
●
●

●

[BH]

●

[DLR]

[GR]

0.5
0.4

●

●
●
●
●
●
●
●
●
●
●
●
●
●

[DPB]

●
●
●
●
●

0.3

●
●
●
●
●
●
●
●

●
●
●
●

0.2

●
●
●
●
●
●
●
●
●
●

TDP

0.3

●

0.1

0.2

TDP

0.4

0.6

0.5

0.6

●
●
●
●
●
●
●
●
●
●
●

[DPB]

0.8

0.00

TDP

●

0.10

●

●

0.01

TDP

(b)

●

0.15

0.05

(a)

15

[DGR]

[BH]

FDX procedure

[LR]

[DLR]

[GR]

FDX procedure
Procedures

Figure 3. Boxplots of the simulated true discovery proportions (TDP) for the
[LR] and [GR] procedures, their discrete modifications and the [BH] procedure
for m = 800. Panel (a) - (c) show results for m3 = 80, m1 = 144 with
q3 = 0.15, 0.25, 0.4, panel (d) shows results for m3 = 640, m1 = 80 and q = 0.4.
investigate the association between reports of amnesia and suspected drugs by performing
for each drug a Fisher’s exact test (one-sided) for testing association between the drug and
amnesia while adjusting for multiplicity by using several (discrete) FDR procedures. Applying
the Benjamini-Hochberg procedure to this data set yields 24 candidate drugs which could be
associated with amnesia. Using the discrete FDR controlling procedures from Döhler et al.
(2018) yields 27 candidate drugs.
In what follows, we investigate the performance of the [LR], [DLR], [GR], [DPB] and
[DGR] procedures for analyzing this data set. First, we compare these procedures when
the goal is control of the median FDX instead of FDR at the 5% level, i.e., we require
P(FDP > 5%) ≤ 0.5. Figure 4 illustrates the data and the critical constants of the involved
FDX controlling procedures.
The benefit of taking discreteness into account is evident: the discrete critical values are
considerably (by a factor of 2.5 ∼ 4) larger than their respective classical counterparts which
leads to more powerful procedures, see also the first row of Table 2.
Note that the critical values of [DPB] are not displayed in Figure 4 since they are visually
indistinguishable from the [DGR] critical values. Figure 5 shows that this is in fact true for
all indices, thus [DGR] is not only an efficient, but also quite accurate approximation of the
[DPB] values, at least for the discrete distribution involved in this example.
We also compare the performance of the above procedures over the full range of possible
values for ζ. Figure 6 depicts the number of rejections when controlling P(FDP > 5%) ≤ ζ

16

SEBASTIAN DÖHLER AND ETIENNE ROQUAIN
0.010

Pharmacovigilance data: critical values for P( FDP > 0.05) ≤ 0.5
●
●

0.006

●

[GR]
[DGR]
[LR]
[DLR]
raw p−values (sorted)

●●●●●●●●●●●●●●●●●●●●●

0.004
0.000

0.002

critical values

0.008

●

●●●●●●●●●●●●●●●●●●●●

●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●●
●
●●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●●
●
●●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●

0

20

40

60

80

100

Index

Figure 4. Critical constants and sorted p-values (represented by black
crosses) for median FDX control using the [LR], [DLR], [GR] and [DGR] procedures for the pharmacovigilance data.
Table 2. Number of rejections for the pharmacovigilance data.
Procedure controls
P(FDP > 5%) ≤ 0.5
P(FDP > 5%) ≤ 0.05

[LR] [DLR] [GR] [DPB] [DGR]
23
16

27
21

24
16

29
24

29
24

[D−GR]/[D−GR*]

● ●●●●

1.000

1.005

1.010

● ●
●● ●

●● ●

●

1.015

Figure 5. Boxplot for the ratio of the [DPB] to the [DGR] critical values.
for ζ ∈ (0, 1). As expected from Propositions 3.1, 3.2 and 3.3, the discrete variants reject
more hypotheses than their classical counterparts for all values of ζ. For central values of ζ,
the gain is about three to four additionally rejected hypotheses, which corresponds roughly
to the gain from using the discrete version of BH instead of [BH] (see Table 1 in Döhler
et al. (2018)). Figure 6 also shows that for more extreme values of ζ the gain may be more

CONTROLLING FALSE DISCOVERY EXCEEDANCE FOR HETEROGENEOUS TESTS

Guo−Romano (continuous)
Discrete Guo−Romano
Lehmann−Romano (continuous)
Discrete Lehmann−Romano

30

●

●
●

20
0

10

number of rejections

40

50

Pharmacovigilance data: Controlling P( FDP > 0.05) ≤ ζ

●

●
●
● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.0

●

●
●

●
●
●

●

●

●
●

●
●

●
●

●

●

●
●

●

●

●

●
●

●

●

17

●
●
●
●
●
●
●
●
●
●
●
●●
●●
● ●
● ●
●
●
●
●
●
●
●
●
●

●

●

●

●
●
●
●

0.2

0.4

0.6

0.8

1.0

ζ

Figure 6. Number of rejections (y-axis) for the pharmacovigilance data when
using the [LR], [DLR], [GR] and [DGR] procedures. All procedures control the
tail probabilities (on the x-axis) for the FDP exceeding 5%. The horizontal
lines represent the rejections of the BH and discrete BH procedures at FDRlevel 5%.
pronounced, e.g., when P(FDP > 5%) ≤ 0.05 is to be guaranteed, the [GR] procedure rejects
16 hypotheses, whereas the [DGR] procedure rejects 24 hypotheses (see the second row of
Table 2).
6. Discussion
In this paper, we presented new procedures controlling the FDX while incorporating the
(heterogeneous) family of null distribution {Fi , 1 ≤ i ≤ m}. Markedly, it put forward that
the geometric averaging of the Fi ’s is a suitable operation for FDX control. This is new to
our knowledge, as all previous works are mostly based on arithmetic averaging of the Fi ’s
(or variation thereof). Maybe more importantly, our approach led to a substantial power
improvement in two common situations, under continuity of the tests statistics via weighting
schemes, and for discrete test statistics when performing multiple individual Fisher’s exact
tests.
This work opens several directions of research. First, the proofs of all our FDX bounds rely
on using a kind of independence between the p-values (see (Indep0) and (Indep)). While this
assumption is classical, it is desirable to remove this condition to better stick to the reality
of the experiments. This generalization seems however challenging, as FDX control under
dependence is already delicate to study in the homogeneous case, see Delattre and Roquain
(2015). A second interesting avenue is to derive theoretical bounds for the true discovery

18

SEBASTIAN DÖHLER AND ETIENNE ROQUAIN

proportion (TDP) of our procedure. In particular, a useful concern would be to assess whether
our way to account for heterogeneity (via arithmetic or geometric averaging of the Fi ’s)
is optimal in some sense. Lastly, our work paves the way to control other simultaneous
inference criteria based on an event probability, e.g., to establish post hoc bounds in the
discrete heterogeneous case, see Genovese and Wasserman (2006); Goeman and Solari (2011);
Blanchard et al. (pear). While challenging, this is a very exciting direction for future research.
Acknowledgements
This work has been supported by ANR-16-CE40-0019 (SansSouci), ANR-17-CE40-0001
(BASICS) and by the GDR ISIS through the ”projets exploratoires” program (project TASTY).
The authors thank Florian Junge for implementing the discrete FDX procedures and improved
Poisson-binomial distribution functions in R, and for running the simulations.
References
Benjamini, Y. and Hochberg, Y. (1995). Controlling the false discovery rate: A practical and
powerful approach to multiple testing. Journal of the Royal Statistical Society. Series B,
57(1):289–300.
Blanchard, G., Neuvial, P., and Roquain, E. (to appear). Post hoc confidence bounds on false
positives using reference families. Annals of Statistics.
Blanchard, G. and Roquain, E. (2008). Two simple sufficient conditions for FDR control.
Electron. J. Stat., 2:963–992.
Chen, X., Doerge, R. W., and Heyse, J. F. (2018). Multiple testing with discrete data:
proportion of true null hypotheses and two adaptive FDR procedures. Biom. J., 60(4):761–
779.
Chen, X., Doerge, R. W., and Sarkar, S. K. (2015). A weighted FDR procedure under discrete
and heterogeneous null distributions. arXiv e-prints, page arXiv:1502.00973.
Delattre, S. and Roquain, E. (2011). On the false discovery proportion convergence under
Gaussian equi-correlation. Statist. Probab. Lett., 81(1):111–115.
Delattre, S. and Roquain, E. (2015). New procedures controlling the false discovery proportion
via Romano-Wolf’s heuristic. Ann. Statist., 43(3):1141–1177.
Delattre, S. and Roquain, E. (2015). On empirical distribution function of high-dimensional
Gaussian vector components with an application to multiple testing. Bernoulli. To appear.
Dickhaus, T., Straßburger, K., Schunk, D., Morcillo-Suarez, C., Illig, T., and Navarro, A.
(2012). How to analyze many contingency tables simultaneously in genetic association
studies. Statistical applications in genetics and molecular biology, 11(4).
Ditzhaus, M. and Janssen, A. (2019). Variability and stability of the false discovery proportion. Electron. J. Statist., 13(1):882–910.
Döhler, S. (2016). A discrete modification of the Benjamini-Yekutieli procedure. Econometrics
and Statistics.
Döhler, S., Durand, G., and Roquain, E. (2018). New FDR bounds for discrete and heterogeneous tests. Electron. J. Statist., 12(1):1867–1900.
Durand, G. (2017). Adaptive p-value weighting with power optimality. ArXiv e-prints.
Durand, G. (2019). Adaptive p-value weighting with power optimality. Electron. J. Statist.,
13(2):3336–3385.
Durand, G. and Junge, F. (2019). DiscreteFDR: Multiple Testing Procedures with Adaptation
for Discrete Tests. R package version 1.2.

CONTROLLING FALSE DISCOVERY EXCEEDANCE FOR HETEROGENEOUS TESTS

19

Durand, G., Junge, F., Döhler, S., and Roquain, E. (2019). DiscreteFDR: An R package
for controlling the false discovery rate for discrete test statistics. arXiv e-prints, page
arXiv:1904.02054.
Genovese, C. and Wasserman, L. (2004). A stochastic process approach to false discovery
control. Ann. Statist., 32(3):1035–1061.
Genovese, C. R., Roeder, K., and Wasserman, L. (2006). False discovery control with p-value
weighting. Biometrika, 93(3):509–524.
Genovese, C. R. and Wasserman, L. (2006). Exceedance control of the false discovery proportion. J. Amer. Statist. Assoc., 101(476):1408–1417.
Gilbert, P. (2005). A modified false discovery rate multiple-comparisons procedure for discrete
data, applied to human immunodeficiency virus genetics. Journal of the Royal Statistical
Society. Series C, 54(1):143–158.
Goeman, J. J. and Solari, A. (2011). Multiple testing for exploratory research. Statistical
Science, pages 584–597.
Guo, W., He, L., and Sarkar, S. K. (2014). Further results on controlling the false discovery
proportion. The Annals of Statistics, 42(3):1070–1101.
Guo, W. and Romano, J. (2007). A generalized Sidak-Holm procedure and control of generalized error rates under independence. Stat. Appl. Genet. Mol. Biol., 6:Art. 3, 35 pp.
(electronic).
Habiger, J. D. (2015). Multiple test functions and adjusted p-values for test statistics with
discrete distributions. J. Statist. Plann. Inference, 167:1–13.
Heller, R. and Gur, H. (2011). False discovery rate controlling procedures for discrete tests.
ArXiv e-prints.
Heller, R., Gur, H., and Yaacoby, S. (2012). discreteMTP: Multiple testing procedures for
discrete test statistics. R package version 0.1-2.
Heyse, J. F. (2011). A false discovery rate procedure for categorical data. In Recent Advances
in Bio- statistics: False Discovery Rates, Survival Analysis, and Related Topics, pages
43–58.
Holm, S. (1979). A simple sequentially rejective multiple test procedure. Scand. J. Statist.,
6(2):65–70.
Hu, J. X., Zhao, H., and Zhou, H. H. (2010). False discovery rate control with groups. J.
Amer. Statist. Assoc., 105(491):1215–1227.
Ignatiadis, N., Klaus, B., Zaugg, J., and Huber, W. (2016). Data-driven hypothesis weighting
increases detection power in genome-scale multiple testing. 13.
Korn, E. L., Troendle, J. F., McShane, L. M., and Simon, R. (2004). Controlling the number of
false discoveries: application to high-dimensional genomic data. J. Statist. Plann. Inference,
124(2):379–398.
Lehmann, E. L. and Romano, J. P. (2005). Generalizations of the familywise error rate. Ann.
Statist., 33:1138–1154.
Neuvial, P. (2008). Asymptotic properties of false discovery rate controlling procedures under
independence. Electron. J. Stat., 2:1065–1110.
Perone Pacifico, M., Genovese, C., Verdinelli, I., and Wasserman, L. (2004). False discovery
control for random fields. J. Amer. Statist. Assoc., 99(468):1002–1014.
Ramdas, A., Foygel Barber, R., Wainwright, M. J., and Jordan, M. I. (2017). A unified
treatment of multiple testing with prior knowledge using the p-filter. arXiv e-prints, page
arXiv:1703.06222.

20

SEBASTIAN DÖHLER AND ETIENNE ROQUAIN

Romano, J. P. and Wolf, M. (2007). Control of generalized error rates in multiple testing.
Ann. Statist., 35(4):1378–1408.
Roquain, E. (2011). Type I error rate control for testing many hypotheses: a survey with
proofs. J. Soc. Fr. Stat., 152(2):3–38.
Roquain, E. and van de Wiel, M. (2009). Optimal weighting for false discovery rate control.
Electron. J. Stat., 3:678–711.
Roquain, E. and Villers, F. (2011). Exact calculations for false discovery proportion with
application to least favorable configurations. Ann. Statist., 39(1):584–612.
Rubin, D., Dudoit, S., and van der Laan, M. (2006). A method to increase the power of
multiple testing procedures through sample splitting. Stat. Appl. Genet. Mol. Biol., 5:Art.
19, 20 pp. (electronic).
Shaked, M. and Shanthikumar, J.G. (2007). Stochastic Orders. Springer Series in Statistics.
Springer New York.
Tarone, R. E. (1990). A modified bonferroni method for discrete data. Biometrics, 46(2):515–
522.
Wasserman, L. and Roeder, K. (2006). Weighted hypothesis testing. Technical report, Dept.
of statistics, Carnegie Mellon University.
Westfall, P. and Wolfinger, R. (1997). Multiple tests with discrete distributions. The American
Statistician, 51(1):3–8.
Zhao, H. and Zhang, J. (2014). Weighted p-value procedures for controlling FDR of grouped
hypotheses. J. Statist. Plann. Inference, 151/152:90–106.

7. Materials for the proofs
7.1. Proving the main tool. The proof is based on the following result, which is a reformulation of Theorem 5.2 in Roquain (2011) in our context.
Theorem 7.1 (Roquain (2011)). In the setting defined in Section 2.1, consider any step-down
procedure R with critical values τ` , 1 ≤ ` ≤ m. Then for all P ∈ P, we have
FDX(R, P )
(34)

≤

m
X


1{|H0 (P )| ≤ m(`)} PX∼P 

`=1


X

e ) = ` ,
1{pi (X) ≤ τ` } ≥ bα`c + 1, `(P

i∈H0 (P )

n
o
e ) = min ` ∈ {1, . . . , m} : ` − P
e )=
where `(P
1{p
(X)
≤
τ
}
≥
bα`c
+
1
(with `(P
i
`
i∈H1 (P )
m + 1 if the set is empty).
e ) is indeLet us show that Theorem 7.1 implies (17) under (Indep0). Under (Indep0), `(P
pendent of the variable family

 X


i∈H0 (P )



1{pi (X) ≤ τ` }, 1 ≤ ` ≤ m .


CONTROLLING FALSE DISCOVERY EXCEEDANCE FOR HETEROGENEOUS TESTS

21

Hence, (34) provides that FDX(R, P ) is smaller or equal to


m


X
X
e )=`
1{|H0 (P )| ≤ m(`)} PX∼P 
1{pi (X) ≤ τ` } ≥ bα`c + 1 PX∼P `(P
`=1

i∈H0 (P )

≤ B(τ, α),
which gives (17).
P
Finally, for completeness, let us now prove Theorem 7.1. Let R` = m
i=1 1{pi (X) ≤ τ` } for
all `. First, we have for any ` ∈ {1,. . . , m} such that |R` | = `:
{FDP(R` , P ) > α} = {|H0 (P ) ∩ R` | > α`} = {|H0 (P ) ∩ R` | ≥ bα`c + 1}
e )},
= {` − |H1 (P ) ∩ R` | ≥ bα`c + 1} ⊂ {` ≥ `(P
e ). Assuming now |R`0 | ≥ `0 for any `0 ≤ `, we obtain
by using the definition of `(P
e ), |Re | ≥ `(P
e )} ⊂ {|H0 (P ) ∩ Re | ≥ bα`(P
e )c + 1},
{FDP(R` , P ) > α} ⊂ {` ≥ `(P
`(P )
`(P )
e ). Moreover, if `(P
e ) ≥ 2, again by definition of
where the last step uses the definition of `(P
P
e ), we have (`(P
e ) − 1) −
e
`(P
e )−1 } < bα(`(P ) − 1)c + 1. Hence, we
i∈H1 (P ) 1{pi (X) ≤ τ`(P
obtain the following upper-bound for |H0 (P )|:
e
e
|H0 (P )| = m − |H1 (P )| ≤ m − |H1 (P ) ∩ R`(P
e )−1 | ≤ m − `(P ) + bα(`(P ) − 1)c + 1.
e ) = 1, it holds for any possible value of `(P
e ) ≤ m.
Since the above bound is also true when `(P
0
0
b
Since ` = ` in (9) satisfies both |R` | = ` and |R`0 | ≥ ` for any ` ≤ `, combining the above
displays gives (34).
7.2. Proof of Proposition 2.1. First, we have with P -probability 1, for all i ∈ {1, . . . , m},
pi ∈ A, both under (Cont) or (Discrete). Hence, by (10), we have {` ∈ {1, . . . , m} : ξ` (pσ(`) ) ≤
ζ} = {` ∈ {1, . . . , m} : pσ(`) ≤ τ` }. By (9), this gives
`b = max{` ∈ {0, . . . , m} : ∀`0 ≤ `, p0`0 ≤ ζ},
where we have denoted p0` = ξ` (pσ(`) ) for all `. Now note that
b = {i ∈ {1, . . . , m} : σ −1 (i) ∈ {1, . . . , `}}
b
{σ(1), . . . , σ(`)}
= {i ∈ {1, . . . , m} : ∀` ∈ {1, . . . , σ −1 (i)}, p0` ≤ ζ}
= {i ∈ {1, . . . , m} :

max

{p0` } ≤ ζ},

`∈{1,...,σ −1 (i)}

hence it is sufficient to prove that p̃i = max`∈{1,...,σ−1 (i)} {p0` } for any i ∈ {1, . . . , m}. For this,
let us fix i ∈ {1, . . . , m} and write {` ∈ {1, . . . , m} : pσ(`) ≤ pi } = {` ∈ {1, . . . , m} : ` ≤
σ −1 (i)} ∪ A, for A = {` ∈ {1, . . . , m} : pσ(`) ≤ pi , ` > σ −1 (i)}. This is possible because, by
definition, ` ≤ σ −1 (i) implies pσ(`) ≤ pi . Next, for any ` ∈ A, we have both pσ(`) ≤ pi and
pσ(`) ≥ pi , which entails pσ(`) = pi and thus ξ` (pσ(`) ) = ξ` (pi ). Since σ −1 (i) ≤ ` and by the
nonincreasing property of ` 7→ ξ` (pi ), we have ξ` (pi ) ≤ ξσ−1 (i) (pi ) = ξσ−1 (i) (pσ(σ−1 (i)) ). This
gives p0` ≤ p0σ−1 (i) for all ` ∈ A. Therefore,
max {p0` } = max {p0` } ∨ max{p0` } = max {p0` },

1≤`≤m
pσ(`) ≤pi

1≤`≤m
`≤σ −1 (i)

`∈A

1≤`≤m
`≤σ −1 (i)

22

SEBASTIAN DÖHLER AND ETIENNE ROQUAIN

which leads to the result.
7.3. An auxiliary lemma.
Lemma 7.1. With the notation in (26) the quantity

h
i


h
i

(35) P Bin m − ` + i, F̃m−`+i (t) ≥ i = P Bin m − ` + i, 1 − F̃m−`+i (t) ≤ m − `
is non-increasing both in i ∈ {1, . . . , bα`c + 1} and ` ∈ {1, . . . , m}.
Proof. First note that

1 − F̃j (t) = 

j
Y

1/j
(1 − (F (t))(j 0 ) )

j 0 =1

in non-decreasing in j (because the geometric average of larger numbers is larger). The
quantity (35) is thus non-increasing with respect to i, so that the only thing to check is
that hthis quantity is
i non-increasing with respect to `. hFor this,
i it is sufficient to prove that
Bin j + 1, F̃j+1 (t) is stochastically larger than Bin j, F̃j (t) for any j ∈ {1, . . . , m − 1}
(which is not obvious because F̃j (t) ≥ F̃j+1 (t)). Let n1 = j, p1 = F̃j (t), n2 = 1, p2 =
(F (t))(j+1) , n = j + 1 and p = F̃j+1 (t). We easily check that n = n1 + n2 and by (26),
n

(1 − p) =

j+1
Y

(1 − (F (t))(j 0 ) )

j 0 =1

=

j
Y

(1 − (F (t))(j 0 ) ) × (1 − (F (t))(j+1) ) = (1 − p1 )n1 (1 − p2 )n2 .

j 0 =1

Applying Example 1.A.25 in Shaked, M. and Shanthikumar, J.G. (2007) (m = 2 with the
notation therein), we obtain that the sum of a Bin [n1 , p1 ] variable and a Bin [n2 , p2 ] variable
(with independence) is stochastically smaller than a Bin [n, p] variable. In particular, a
Bin [n1 , p1 ] variable is stochastically smaller than a Bin [n, p] variable. This gives the result.

Appendix A. Additional numerical details

CONTROLLING FALSE DISCOVERY EXCEEDANCE FOR HETEROGENEOUS TESTS

Table 3. Average power (i.e. average of true discovery proportion) of FDX
controlling procedures (at ζ = 0.5) for N = 25.
m

m3

m1

q3

[BH]

[LR]

[DLR]

[GR]

[DPB]

[DGR]

800

80

144
144
144
360
360
360
576
576
576
112
112
112
280
280
280
448
448
448
32
32
32
80
80
80
128
128
128
360
360
360
900
900
900
1440
1440
1440
280
280
280
700
700
700
1120
1120
1120
80
80
80
200
200
200
320
320
320

0.15
0.25
0.4
0.15
0.25
0.4
0.15
0.25
0.4
0.15
0.25
0.4
0.15
0.25
0.4
0.15
0.25
0.4
0.15
0.25
0.4
0.15
0.25
0.4
0.15
0.25
0.4
0.15
0.25
0.4
0.15
0.25
0.4
0.15
0.25
0.4
0.15
0.25
0.4
0.15
0.25
0.4
0.15
0.25
0.4
0.15
0.25
0.4
0.15
0.25
0.4
0.15
0.25
0.4

0
0.0004
0.0803
0
0.0004
0.0803
0
0.0004
0.0803
0
0.0005
0.2148
0
0.0005
0.2147
0
0.0005
0.2145
0
0.001
0.4243
0
0.001
0.4242
0
0.001
0.424
0
0.0001
0.073
0
0.0001
0.073
0
0.0001
0.0729
0
0.0001
0.2058
0
0.0001
0.2058
0
0.0001
0.2057
0
0.0003
0.4223
0
0.0003
0.4222
0
0.0003
0.422

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.0003
0.0203
0
0.0003
0.0203
0
0.0003
0.0203
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.0001
0.0114
0
0.0001
0.0114
0
0.0001
0.0114

0.0025
0.043
0.3328
0.0025
0.043
0.3766
0.0071
0.0528
0.4474
0.0025
0.0289
0.425
0.0025
0.0336
0.4413
0.0025
0.0389
0.4609
0.0018
0.0203
0.4908
0.002
0.0203
0.4974
0.0021
0.0203
0.5048
0.0007
0.0198
0.3331
0.0022
0.021
0.338
0.0024
0.0378
0.432
0.0007
0.0197
0.4093
0.0007
0.02
0.4374
0.0014
0.0201
0.4545
0.0007
0.009
0.4823
0.0007
0.009
0.4866
0.0007
0.009
0.4935

0.0002
0.0077
0.1195
0.0002
0.0077
0.1195
0.0002
0.0077
0.1195
0.0002
0.0076
0.1984
0.0002
0.0076
0.1983
0.0002
0.0076
0.1983
0.0002
0.0075
0.5379
0.0002
0.0075
0.5374
0.0002
0.0075
0.5369
0
0.0029
0.0792
0
0.0029
0.0792
0
0.0029
0.0792
0
0.0029
0.196
0
0.0029
0.196
0
0.0029
0.1959
0
0.0029
0.5288
0
0.0029
0.5286
0
0.0029
0.5283

0.0025
0.043
0.4412
0.0043
0.0444
0.4512
0.0076
0.077
0.5141
0.0025
0.0422
0.5153
0.0025
0.0429
0.5728
0.0037
0.043
0.5921
0.0025
0.0212
0.673
0.0025
0.0212
0.6746
0.0025
0.0212
0.6753
0.0022
0.0222
0.4315
0.0024
0.0373
0.4515
0.0024
0.0428
0.5173
0.0007
0.0205
0.5194
0.002
0.0205
0.5678
0.0024
0.0206
0.5908
0.0007
0.0172
0.6665
0.0007
0.0184
0.6689
0.0007
0.0194
0.6724

0.0025
0.043
0.4406
0.0043
0.0444
0.4511
0.0076
0.077
0.5128
0.0025
0.0422
0.5139
0.0025
0.0429
0.5716
0.0037
0.043
0.5917
0.0025
0.0212
0.6724
0.0025
0.0212
0.6743
0.0025
0.0212
0.675
0.0022
0.0222
0.4311
0.0024
0.0373
0.4515
0.0024
0.0428
0.5144
0.0007
0.0205
0.5176
0.002
0.0205
0.5657
0.0024
0.0206
0.5906
0.0007
0.0172
0.6658
0.0007
0.0184
0.6683
0.0007
0.0194
0.6715

240

640

2000

200

600

1600

23

