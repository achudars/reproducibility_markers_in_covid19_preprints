Large fork-join queues with nearly deterministic
arrival and service times

arXiv:1912.11661v2 [math.PR] 17 Dec 2020

Dennis Schol
Eindhoven University of Technology

Maria Vlasiou
Eindhoven University of Technology, University of Twente

Bert Zwart
Eindhoven University of Technology, CWI

In this paper, we study an N server fork-join queue with nearly deterministic arrival and service times.
Specifically, we present a fluid limit for the maximum queue length as N → ∞. This fluid limit depends on
the initial number of tasks. In order to prove these results, we develop extreme value theory and diffusion
approximations for the queue lengths.
Key words : queueing network; heavy traffic; fluid limit; extreme value theory
MSC2000 subject classification : 60K25
OR/MS subject classification : Primary: Queues: networks; limit theorems; secondary: probability: Markov
processes; random walk; stochastic model applications
History : Received December 24, 2019; revised December 15, 2020.

1. Introduction. Fork-join queues are widely studied in many applications, such as communication systems and production processes. However, due to the fact that all service stations see
exactly the same arrival process, which is the main characteristic of fork-join queues, these fork-join
queues are very challenging to analyze. Hence, there are only a few exact results, which are mainly
for systems in stationarity and are restricted to fork-join queues with two service stations.
In this paper, we focus on a fork-join queue where the number of service stations is large. Our
objective is to analyze the queue length of the longest queue. We explore a discrete-time forkjoin queue where the arrival and service times are nearly deterministic. In addition, we consider
a heavily loaded system. That is, we assume that the arrival rate to a queue times the expected
service time of that queue, i.e. the traffic intensity per queue ρN , depends on the number of service
N →∞
stations N and satisfies (1 − ρN )N 2 −→ β, with β > 0. Our main result is a fluid limit of the
maximum queue length of the system as N goes to infinity, which holds under very mild conditions
on the distribution of the number of jobs at time 0.
Both the model and the scaling studied in this paper are inspired by assembly systems. In particular, we are inspired by problems faced by original equipment manufacturers (OEMs) that assemble
thousands of components, each produced using specialized equipment, into complex systems. Examples of such OEMs are Airbus and ASML. If one component is missing, the final product cannot
be assembled, giving rise to costly delays. In reality, for some components, OEMs may hedge the
shortage risk by investing in capacity or by keeping an inventory of finished components. However,
we study the maximum queue length, which is only relevant for components where there is no
inventory. As such, our model is a somewhat stylized model of reality.
An interesting question is whether the manufacturer can produce on schedule. To answer this
question, we consider a make-to-order system, i.e. suppliers only produce when they have an order,
1

2

and we assume that the manufacturer sends orders to all the suppliers at the same time. Now,
we can model this process by a fork-join queueing system, where the various servers represent
suppliers, jobs in the system represent orders requested by the manufacturer and queue lengths
in front of each server represent the number of unfinished components each supplier has. As the
slowest supplier determines the delay that the manufacturer observes, we wish to study the longest
queue. Additionally, we consider a supply chain network operating under full capacity, which is
indeed the situation in this industry. Last, we capture the property that in high-tech manufacturing
arrival and service times have a low variance by considering nearly deterministic arrival and service
times. A visualization of the fork-join queue as a simple representation of a high-tech supply chain,
is given in Figure 1. Note that in this paper we focus on the backlogs of the suppliers and not on
the assembly phase.
Backlogs of suppliers
1
Assembly of
components
Arrival stream
of manufacturer

2
..
.
N
Figure 1. Fork-join queue with N servers

We now turn to a survey of related literature. As mentioned, the earliest literature on forkjoin queues focuses on systems with two service stations. Analytic results, such as asymptotics on
limiting distributions, can be found in [3, 9, 11, 24]. However, due to the complexity of fork-join
queues, these results cannot be expanded to fork-join queues with more than two service stations.
Thus, most of the work on fork-join queues with more than two service stations is focused on finding
approximations of performance measures. For example, an approximation of the distribution of the
response time in M/M/s fork-join queues is given in Ko and Serfozo [12]. Upper and lower bounds
for the mean response time of servers, and other performance measures, are given by Nelson,
Tantawi [17] and Baccelli, Makowski [4].
A common property of the aforementioned classic literature is that it mainly focuses on steadystate distributions or other one-dimensional performance measures. Some work on the heavy-traffic
process limit has been done, for example, Varma [23] derives a heavy-traffic analysis for fork-join
queues, and shows weak convergence of several processes, such as the joint queue lengths in front
of each server. Furthermore, Nguyen [18] proves that various appearing limiting processes are in
fact multi-dimensional reflected Brownian motions. In [19], Nguyen extends this result to a forkjoin queue with multiple job types. Lu and Pang study fork-join networks in [13, 14, 15]. In [13],
they investigate a fork-join network where each service station has multiple servers under nonexchangeable synchronization, and operates in the quality-driven regime. They derive functional
central limit theorems for the number of tasks waiting in the waiting buffers for synchronization
and for the number of synchronized jobs. In [14], they extend this analysis to a fork-join network

3

with a fixed number of service stations, each having many servers, where the system operates in the
Halfin-Whitt regime. In [15], the authors investigate these heavy-traffic limits for a fixed number of
infinite-server stations, where services are dependent and could be disrupted. Finally, we mention
Atar, Mandelbaum and Zviran [2], who investigate the control of a fork-join queue in heavy traffic
by using feedback procedures. Our work contributes to this literature on process-level analysis of
fork-join networks. To be precise, we derive a fluid limit of the stochastic process that keeps track
of the largest queue length. This study seems to be the first explicit process-level approximation
of a large fork-join queue.
Moreover, our work also adds to the literature on queueing systems with nearly deterministic
arrivals and services. The only research line on queueing systems with nearly deterministic service
times that we are aware of is Sigman and Whitt [21, 22], who investigate the G/G/1 and G/D/N
queues and establish heavy-traffic results on waiting times, queue lengths and other performance
measures in stationarity, as well as functional central limit theorems on the waiting time and
on other performance measures. In these papers, they distinguish two cases, one in which (1 −
√ N →∞
N →∞
ρN ) N −→ β and one in which (1 − ρN )N −→ β, with ρN the traffic intensity and β some
constant.
We now turn to an overview of the techniques that we use in this paper. Because of the fact
that we aim to obtain a fluid limit of a maximum of N queue lengths, we mainly use techniques
from extreme value theory in our proofs. This is, however, quite a challenge, since on the hand, the
queue lengths of the servers are mutually dependent. On the other hand, most results on extreme
values hinge heavily on the assumption of mutual independence. Furthermore, we consider a forkjoin queue where the arrival and service probability depend on N , which makes the queue lengths
to be triangular arrays with respect to N . This makes our paper also rather unusual, as studies
on triangular arrays are rare. One paper on this subject, relevant for us, is Anderson, Coles and
Hüsler [1], where they study the maximum of a sum of a large number of triangular arrays.
In order to get fluid limits for the maximum queue lengths, we need to study diffusion limits for
the individual queue lengths. We thus combine ideas from the literature on extreme value theory
with literature on diffusion approximations, which we show in Section 2.2. In order to be able to
analyze the queue lengths through diffusion approximations, we impose a heavy-traffic assumption,
namely (1 − ρN )N 2 → β. Then, for each separate queue length, we have a reflected Brownian
motion as diffusion approximation. By using the well-known formula for the cumulative distribution
function of a reflected Brownian motion (cf. Harrison [10, p. 49]), we investigate the maximum of
N independent reflected Brownian motions to get an idea of the scaling of the maximum queue
length.
Now, we give a brief sketch of how we apply these ideas to prove the fluid limit, we start by
considering the slightly simpler scenario that each queue is empty at time 0. Because we want
to prove a fluid limit that holds uniformly on compact intervals, we need to prove pointwise
convergence of the process and tightness of the collection of processes. Our first step in proving this
is by showing that each queue length is in distribution the same as a supremum of an arrival process
minus a service process. We then show in Section 2.2 that under a temporal scaling of tN 3 log N
and a spatial scaling of N log N , the arrival process minus a drift term converges to −βt, as N
√ → ∞.
Furthermore, we derive under that same temporal scaling but under a spatial scaling of N log N ,
that the centralized service process satisfies the central limit theorem. This scaled centralized
service process is given in Equation (4.4). We use the non-uniform Berry-Esséen inequality, which is
described by Michel in [16], to deduce the convergence rate of the cumulative distribution function
of this scaled centralized service process to the cumulative distribution function of a normally
distributed random variable, which is given in Equation (4.8). It turns out that this convergence
rate is fast enough, so that we can replace the scaled centralized service process with a normally
distributed random variable in the expression of the maximum queue length in order to get the

4

same limit. By Pickands’ result [20] on convergence of moments of the maximum of N scaled
random variables, we know that the expectation of the
√ maximum of standard normally distributed
√
random variables divided by log N converges to 2, as N → ∞. This gives us the convergence
of the maximum of N scaled centralized service processes. After we have obtained these limiting
results for the scaled arrival and service process, we use these, together with Doob’s maximal
submartingale inequality to prove convergence in probability of the maximum queue length, we
show this in Section 4.3. Finally, in Section 4.4 we use Doob’s maximal submartingale inequality
to bound the probability that the process makes large jumps and prove that this probability is
small, so that the maximum queue length is a tight process.
After we have considered the maximum queue length for the process with empty queues at time
0, we then turn to the scenario that the length of each queue at time 0 is identically distributed.
In this case, we can use Lindley’s recursion to express the maximum queue length as the pairwise
maximum of the maximum queue length with empty queues at time 0 and a part depending on the
number of jobs at time 0, this formula is given in Equation (2.3). How to prove the fluid limit for
the first part is already sketched above. In order to derive a fluid limit for the latter part, we first
observe that this part equals the maximum of N times the sum of the number of jobs at time 0 at
each server plus the number of arrivals minus the number of services at each server. Following a
similar path as earlier, we can prove that the scaled centralized service process at server i behaves
like a normally distributed random variable. Thus, we have to analyze a maximum of N pairwise
sums of normally distributed random variables and random variables describing the number of jobs
at time 0, which is stated in more detail in Lemma 4.9.
In Lemma 4.4 we prove a convergence result of this maximum, this is quite a challenge, because
we need to apply extreme value theory on pairwise sums. In order to do this, we use the results from
Davis, Mulrow & Resnick [7] and Fisher [8] on convergence of samples of random variables to lim(1)
(k)
iting sets. The authors prove convergence results of the convex hull of {(Zi /bN , . . . , Zi /bN )i≤N }
(j)
(j)
(l)
to a limiting set, as N → ∞, with (Zi , i ≤ N ) i.i.d., Zi and Zm are independent and bN is a
proper scaling sequence. We show in the proof of Lemma B.1 that these results can be extended in
Pk
(j)
(j)
(l)
(m)
establishing convergence of extreme values of maxi≤N j=1 Yi /aN , where aN and aN are not
necessarily the same, which is a stand-alone result of independent interest. We did not find this
extension in other literature. The result in Lemma 4.4 follows from Lemma B.1.
The rest of the paper is organized as follows. In Section 2, we describe the fork-join system in
more detail; we give a definition of the arrival and service processes and we present a scaled version
of the queueing model. In Section 2.1, we introduce the fluid limit and explain it heuristically.
We elaborate a bit more on the scaling and the shape of the fluid limit in Sections 2.2 and 2.3.
Furthermore, we give some examples and numerical results in Section 2.4. We finish with some
concluding remarks in Section 3. The proof of the fluid limit is given in Section 4. In Appendix A,
we elaborate on the convergence of the upper bound that was given in Lemma 4.7. We prove in
Pk
(j)
(j)
Appendix B a convergence result of maxi≤N j=1 Yi /aN . In Appendix C, we prove the lemmas
stated in Section 4.2. An overview of all notation is given in Appendix D.
2. Model description and main results. We now turn to a formal definition of the forkjoin queue that we study. We consider a fork-join queue with integer valued arrivals and services.
In this queueing system, there is one arrival process. The arriving tasks are divided in N subtasks
which are completed by N servers. We assume that both the number of arrivals and services per
time step are Bernoulli distributed. The parameters of the Bernoulli random variables depend on
the number of servers. This is formalized in Definitions 2.1 and 2.2.

5

Definition 2.1 (Arrival process).
arrivals up to time n and equals

The random variable A(N ) (n) indicates the number of

A(N ) (n) =

bnc
X

X (N ) (j)

j=1

with X (N ) (j) indicating whether or not there is an arrival at time j. X (N ) (j) is a Bernoulli random
variable with parameter p(N ) . So,

X

(N )


1 w.p.
p(N ) ,
(j) =
0 w.p. 1− p(N ) .
(N )

Definition 2.2 (Service process i-th server). The random variable Si (n) describes
the number of potentially completed tasks of the i-th server in the fork-join queue at time n with

(N )

Si

(n) =

bnc
X

(N )

Yi

(j) ,

j=1
(N )

where Yi (j) is a Bernoulli random variable with parameter q (N ) indicating whether the i-th
server completed a service at time j.

1 w.p.
q (N ) ,
(N )
Yi (j) =
0 w.p. 1− q (N ) .
Both p(N ) and q (N ) are taken as functions of N , which we specify in Definition 2.3 below.
We assume that for all N ≥ 1 the random variables (X (N ) (j), j ≥ 1) are mutually independent
(N )
for all j and (Yi (j) , j ≥ 1, i ≤ N ) are mutually independent for all j and i. We also assume that
an incoming task can be completed in the same time slot as in which the task arrived. Finally, we
(N )
(N )
assume that X (N ) (j) and Yi (j) are independent, in other words, Yi (j) could still be 1 while
there are no tasks to be served at server i at time j. Due to this assumption, we have on the hand
(N )
the beneficial situation that (A(N ) (n) , n ≥ 0) and (Si (n) , n ≥ 0) are independent processes, but
on the other hand we should be careful with defining the queue length. However, it is a well known
result that we can use Lindley’s recursion, and write the queue length of the i-th server at time n
as
h
 
i
(N )
(N )
sup
A(N ) (n) − A(N ) (k) − Si (n) − Si (k) ,
0≤k≤n

provided that the queue length is 0 at time 0. This is in distribution equal to


(N )
sup A(N ) (k) − Si (k) .
0≤k≤n

As can be seen in this expression, the queue lengths of different servers are mutually dependent,
since the arrival process is the same. When at time 0 there are already jobs in queue, then we can,
after again applying Lindley’s recursion, write the queue length of the i-th server at time n as


 
i
h
(N )
(N )
(N )
(N )
(N )
(N )
(N )
max sup
A (n) − A (k) − Si (n) − Si (k) , Qi (0) + A (n) − Si (n) ,
0≤k≤n

6
(N )

with Qi (0) the number of jobs in front of the i-th server at time 0. Observe that the queue length
of the i-th server equals the maximum of the queue length when the number of jobs at time 0
would be 0, and a random variable that depends on the initial number of jobs.
The aim of this work is to investigate the behavior of the fork-join queue when the number of
servers N is very large. The main objective is deriving the distribution of the largest queue, as this
represents the slowest supplier, which is the bottleneck for the manufacturer. Therefore, we define
in Definition 2.3 a random variable indicating the maximum queue length at time n. Furthermore,
we explore this model in the heavy-traffic regime. To this end, we let p(N ) and q (N ) go to 1 at
similar rates, so that the arrivals and services are nearly deterministic processes.
Definition 2.3 (Maximum queue length at time n). Let p(N ) = 1 − α/N − β/N 2 and
(N )
(N )
q
= 1 − α/N , with α, β > 0. Let Q(α,β) (n) be the maximum queue length of N parallel servers
(N )
at time n, with Q(α,β) (0) = 0. Then
(N )
Q(α,β) (n)

= max sup

h

i≤N 0≤k≤n

(N )

A

(N )

(n) − A



(k) −



(N )
(N )
Si (n) − Si (k)

i

.

(2.1)

So,
(N )

d

Q(α,β) (n) = max sup

i≤N 0≤k≤n



(N )

A(N ) (k) − Si


(k) ,

(2.2)

(N )

under the assumption that Q(α,β) (0) = 0. From these choices of p(N ) and q (N ) , it follows that
the traffic intensity ρN of a single queue satisfies (1 − ρN )N 2 → β, as N → ∞. Furthermore, if
(N )
Qi (0) > 0, the maximum queue length at time n can be written as

h
 
i
(N )
(N )
(N )
Q(α,β) (n) = max max sup
A(N ) (n) − A(N ) (k) − Si (n) − Si (k)
i≤N
0≤k≤n

(N )
(N )
(N )
,Qi (0) + A (n) − Si (n) .
(2.3)
Observe that we can interchange the order of the maxi≤N term and the max term, and rewrite the
expression in (2.3) as the pairwise maximum of two random variables, one random variable is the
maximum of N queue lengths with initial condition 0, as given in Equation (2.1), and the other
is the maximum of N sums of the queue length at time 0 plus the number of arrivals minus the
number of services.
2.1. Fluid limit. As we just have formally defined the fork-join queue that we study, with
the particular nearly deterministic setting, we now state and explain the main result of this paper.
Our central result is a fluid approximation for the rescaled maximum queue length process, which
is given in Theorem 2.1. We prove that under a certain spatial and temporal scaling the maximum
queue length converges to a continuous function, which depends on time t.
There is, however, not a straightforward procedure in choosing the temporal and spatial scaling,
there are namely more possibilities that lead to a non-trivial
limit. For instance, when we choose
√
3
a temporal scaling of N and a spatial scaling of N log N , we get the fluid limit that is given in
Proposition 2.1. Here, we assume that the initial condition is 0.
√
Proposition 2.1 (Temporal scaling of N 3 and spatial scaling of N log N ). For
(N )
Q(α,β) (0) = 0, α > 0 and β > 0, with
1. p(N ) = 1 − α/N − β/N 2 ,
2. q (N ) = 1 − α/N , we have

7

sup

P

0≤s≤T

!
(N )
Q(α,β) (sN 3 ) √
N →∞
√
− 2αs >  −→ 0 ∀  > 0.
N log N

However, we can also derive a steady-state limit, which is given in Proposition 2.2.
Proposition 2.2 (Steady-state convergence). For α > 0 and β > 0, with
1. p(N ) = 1 − α/N − β/N 2 ,
2. q (N ) = 1 − α/N , we have
(N )

Q(α,β) (∞)
N log N

P

−→

α
as N → ∞.
2β

As we can see in Proposition 2.2, to obtain a non-trivial steady-state limit, we need a spatial scaling
of N log N . Since this is the only choice which leads to a non-trivial limit, it is a natural choice
to look for a fluid limit which also has this spatial scaling. Our main result, stated in Theorem
2.1, is such a fluid limit, and it turns out that for establishing this limit, we need a temporal
scaling of N 3 log N . In Section 2.2 we explain why these scalings are natural. We omit the proof of
Proposition 2.1, but we do explain how Proposition 2.1 is connected to Theorem 2.1 at the end of
this section. Furthermore, we give a proof of Proposition 2.2 in Section 4.
We now mention and discuss some assumptions under which our main result holds. First of all,
we assume that we have nearly deterministic arrivals and services.
Assumption 2.1. p(N ) = 1 − α/N − β/N 2 and q (N ) = 1 − α/N , with α, β > 0.
Secondly, we have a basic assumption on the initial condition.
(N )

Assumption 2.2. (Qi

(0), i ≤ N ) are i.i.d. and non-negative for all N .

Furthermore, we want to prove a fluid limit with a spatial scaling of N log N . Therefore, we need to
assume that the maximum number of jobs at time 0 also scales with N log N . In order to do so, we
(N )
allow (Qi (0), i ≤ N, N ≥ 1) to be a triangular array, i.e. a doubly indexed sequence with i ≤ N .
This is a necessity, because otherwise we would be limited to distributions where the maximum
scales like N log N , which would lead us to the family of the heavy-tailed distributions for which we
(N )
(N +1)
do not have convergence in probability of its maximum. Thus in our setting, Qi (0) and Qi
(0)
(N )
do not need to be the same. Consequently, we need to have some regularity on Qi (0) as N
increases to be able to prove a limit theorem.
(N )

P

(N )

Assumption 2.3. Q(α,β) (0) /(N log N ) −→ q(0), with q(0) ≥ 0, as N → ∞, with Qi
brN Ui c, where rN is a scaling sequence.

(0) =

Finally, we can distinguish two cases in which Theorem 2.1 holds.
Assumption 2.4. Ui has a finite right endpoint.
Assumption 2.5. Ui is a continuous random variable and for all v ∈ [0, 1],
lim

t→∞

− log (P(Ui > vt))
= h(v).
− log (P(Ui > t))

Before stating the theorem, we would like to give two remarks on Assumption 2.5. First of all, the
function h has the property that for all u, v ∈ [0, 1], h(uv) = h(u)h(v). Thus, if h is continuous,
h(v) = v a , with a > 0. When h is discontinuous, there are two possibilities: h(v) = 1(v > 0), or
h(v) = 1(v = 1), this corresponds to h(v) = v a with a = 0 and a = ∞, respectively. Secondly, the
assumption of continuity of Ui can be removed, which would lead to more cumbersome proofs.

8

Theorem 2.1 (Fluid limit with non-zero initial condition). If Assumptions 2.1, 2.2
and 2.3 hold, and either Assumption 2.4 or Assumption 2.5 holds, then we have ∀ T > 0, that
(N )


P

sup

Q(α,β) (tN 3 log N )

0≤t≤T

N log N


N →∞
− q(t) >  −→ 0 ∀  > 0,

(2.4)

with

q(t) = max

√

 




α
α
α
2αt − βt 1 t < 2 +
1 t ≥ 2 , g(t, q(0)) − βt .
2β
2β
2β

(2.5)

The function g(t, q(0)) has the following properties:
1. If Assumption 2.4 holds, then
√
g(t, q(0)) = q(0) + 2αt.

(2.6)

2. If Assumption 2.5 holds, then
√
g(t, q(0)) = sup { 2αtu + q(0)v |u2 + h(v) ≤ 1, 0 ≤ u ≤ 1, 0 ≤ v ≤ 1}.

(2.7)

(u,v)

There is a connection between Assumptions 2.4 and 2.5 on Ui and extreme value theory. If Assumption 2.4 holds, then this means that Ui is either a degenerate random variable or is in the
domain of attraction of the Weibull distribution. On the other hand, if Assumption 2.5 holds, then
Ui is in the domain of attraction of the Gumbel distribution.
In order to allow dependence between the initial number of jobs at different servers, we can also
replace Assumptions 2.2 and 2.3 with the following assumption.
(N )

(N )

(N )

(N )

Assumption 2.6. Let Qi (0) = Ui + Vi , with Ui = brN Ui c, where (Ui , i ≤ N ) are i.i.d.
(N )
and non-negative, and satisfy either Assumption 2.4 or 2.5. Furthermore, Vi
is non-negative,
P
(N )
and maxi≤N Vi /(N log N ) −→ 0, as N → ∞.
(N )

(N )

When Assumption 2.6 is satisfied, there may be mutual dependence between Qi (0) and Qj (0),
(N )
(N )
because Vi
and Vj
may be mutually dependent.
As can be seen in Theorem 2.1, the fluid limit has an unusual form, q(t) is namely a maximum
of two functions. The first part of this maximum is the fluid limit when the initial number of jobs
equals 0 and the second part is caused by the initial number of jobs. We elaborate on this more in
Section 2.3. The log N term in the spatial and temporal scaling of the process is also unusual. We
show in Section 2.2 that this is due to the fact that we take a maximum of N random variables,
with N large. Scaling terms like (log N )c are in this context very natural.
We mentioned earlier that different choices for temporal and spatial scalings lead to a fluid limit.
We gave Proposition 2.1 as an example. Since we analyze one and only one system, the two fluid
limits that we presented should be connected to each other. An easy way to see this, is by observing
(N )
that from Theorem 2.1 it follows that when Q(α,β) (0) = 0,
(N )

Q(α,β) (tN 3 log N )
N log N

P

−→

√

2αt − βt as N → ∞,

√
(N )
for t < α/(2β 2 ). Thus, for all t > 0 and for N large, we expect that Q(α,β) (tN 3 ) /(N log N ) ≈
√
√
N →∞ √
2αt − βt/ log N −→ 2αt. This shows heuristically how Proposition 2.1 is connected with
Theorem 2.1. The formal proof of Proposition 2.1 is analogous to the proof of Theorem 2.1 and is
omitted in this paper.

9

2.2. Scaling. In Section 2.1, we presented the fluid limit under the rather unusual temporal
scaling of N 3 log N and spatial scaling of N log N . A heuristic justification for these scalings can
be given by using extreme value theory and ideas from literature on diffusion approximations.
In particular, for the spatial scaling we argue as follows: as we are interested in the convergence
of the maximum queue length, we can use a central limit result to replace each separate queue
length with a reflected Brownian motion and use extreme value theory to get a heuristic idea of the
convergence of the scaled maximum queue length. To argue this, first observe that the arrival and
service processesare binomially distributed random variables,
and we can compute the expectation

√
(N )
(N )
3
3
and variance of A (tN log N ) − Si (tN log N ) /(N log N ) as



p


1
(N )
(N )
3
3
√
E
(2.8)
A
tN log N − Si
tN log N
= −βt log N + oN (1),
N log N
and






1
(N )
(N )
3
3
√
Var
A
tN log N − Si
tN log N
N log N





α
β
α
β
α
α
1
3
btN log N c
+
1− − 2 +
1−
= 2
N log N
N N2
N N
N
N
=2αt + oN (1).

(2.9)

From this, a non-trivial scaling limit can be easily deduced: observe that A(N ) (tN 3 log N ) −
(N )
Si (tN 3 log N ) is a sum of independent and identically distributed random variables, so this
implies that


 d
1
(N )
√
tN 3 log N ≈ Zi ,
A(N ) tN 3 log N − Si
N log N

√
as N is large, with Zi ∼ N −βt log N , 2αt . Furthermore, because A(N ) (tN 3 log N ) −
(N )
Si (tN 3 log N ) is in fact the difference of two random walks, we also have

d
1
(N )
√
A(N ) (n) − Si (n) ≈ Ri (t),
sup
0≤n≤tN 3 log N N log N
as N is large, with Ri (t) a reflected Brownian√motion for t fixed. We can apply extreme value
theory to show that maxi≤N Ri (t) scales with log N . This can be deduced from the cumulative
distribution function of the reflected Brownian motion which is given in [10, p. 49]. Concluding,
the proper spatial scaling of the fluid limit in Theorem 2.1 is 1/(N log N ).
As Equations (2.8) and (2.9) show, the right temporal and spatial scalings are determined by
the choice of the arrival and service probability. When we change the arrival probability to p(N ) =
1 − α/N − β/N 1+c , with c ≥ 1, and keep the service probability the same, we can derive in the same
manner, that under a different temporal and spatial scaling of the queueing process, the fluid limit
result still holds; we state this in Proposition 2.3.
Proposition 2.3 (Other arrival and service probabilities). For c ≥ 1, α > 0 and β > 0,
with
1. p(N ) = 1 − α/N − β/N 1+c ,
2. q (N ) = 1 − α/N ,
(N )
and Q(α,β) (0) = O(N c log N ) and satisfies the same assumptions as in Theorem 2.1, then
(N )


P

sup
0≤t≤T

Q(α,β) (tN 1+2c log N )
N c log N


− q(t) > 

N →∞

−→ 0 ∀  > 0.

The proof of this proposition is very similar to the proof of Theorem 2.1. Thus we omit it here.

10

2.3. Shape of the fluid limit. In Section 2.2, we gave a heuristic explanation of the temporal
and spatial scaling of the process. Here we do the same for the shape of the fluid limit. First of all,
we rewrite the expression in (2.3) and get that the scaled maximum queue length satisfies
(N )

Q(α,β) (tN 3 log N )

=
N log N

 

(N )
(N )
A(N ) (tN 3 log N ) − A(N ) (sN 3 log N ) − Si (tN 3 log N ) − Si (sN 3 log N )

max 
sup
,
 max
i≤N 0≤s≤t
N log N



(N )

max
i≤N

(N )

A(N ) (tN 3 log N ) + Si (tN 3 log N ) + Qi
N log N

(0) 
.


(2.10)

(N )

Now, observe that when Qi (0) = 0 for all i, the pairwise maximum in (2.10) simplifies to the
first part of the maximum. Furthermore, it turns out that the first and the second part of this
maximum converge to the first and second part of the maximum in (2.5), respectively. To see the
first limit heuristically, observe that, due to the central limit theorem,


 d
1
(N )
√
tN 3 log N ≈ ϑi + ζ,
A(N ) tN 3 log N − Si
N log N
√
with ϑi ∼ N (0, αt), independently for all i, and ζ ∼ N (−βt log N , αt). We can write maxi≤N (ϑi +
ζ) = maxi≤N (ϑi ) + ζ. Then, by the basic convergence result that the maximum of N i.i.d. standard
√
√
P
normal random variables scales like 2 log N , it is easy to see that maxi≤N (ϑi + ζ)/ log N −→
√
2αt − βt as N → ∞. Because of the fact that a queue length which is 0 at time 0, can be written
as the supremum
of the arrival process minus the service process up to time t, the fluid limit yields
√
sup0≤s≤t ( 2αs − βs), which equals the first part of the maximum in (2.5).
Similarly, for the second part in (2.10) we observe that
(N )

(N )

A(N ) (tN 3 log N ) − Si (tN 3 log N ) + Qi (0)
i≤N
N log N
(N )
(N )
A(N ) (tN 3 log N ) − (1 − α/N ) tN 3 log N
(1 − α/N ) tN 3 log N − Si (tN 3 log N ) + Qi (0)
=
+ max
.
i≤N
N log N
N log N
(2.11)
max

It is easy to see that the first term converges to −βt as N → ∞, and we prove later on that the
second term converges to g(t, q(0)). This explains the second part of the fluid limit in (2.5).
Specific properties of the function g can be deduced. First of all, Assumption 2.4 considers
(N )
the case that Ui has a finite right endpoint. In this scenario, we have that Qi (0)/(N log N ) =
b√rN Ui c/(N log N ) = bN log N Ui c/(N log N ) ≈ Ui . Now, the theorem says that g(t, q(0)) = q(0) +
2αt. This actually means that for large N ,
!
(N )
(1 − α/N ) tN 3 log N − Si (tN 3 log N )
max Ui +
i≤N
N log N
(N )

≈ max Ui + max
i≤N

i≤N

(1 − α/N ) tN 3 log N − Si
N log N

(tN 3 log N )

.

This behavior can be very well explained, because due to the assumption that Ui has a finite
right endpoint, there will be many observations of Ui that are close to the right endpoint, as

11

N
 becomes large, and thus it will be more
.and more
 likely that there is a large observation
(N )
3
3
N log N , for which the observation Ui? will also be
(1 − α/N ) (tN log N ) − Si? (tN log N )
large.
Furthermore, when Assumption 2.5 holds, g(t, q(0)) can be written as a supremum over a set.
To give an idea why this is the case, we first observe that we can write the last term in (2.11) as
!
(N )
(N )
(1 − α/N ) (tN 3 log N ) − Si (tN 3 log N ) Qi (0)
+
.
(2.12)
max
i≤N
N log N
N log N
Thus, this maximum can be viewed as a maximum of N pairwise sums of random variables. For
any N > 0, we can write down all the N pairs of random variables as
!
)
(
(N )
(N )
1 (1 − α/N ) (tN 3 log N ) − Si (tN 3 log N ) 1 Qi (0)
√
.
(2.13)
,
N log N
q(0) N log N
2αt
i≤N

√

Now, the expression
√ in Equation (2.12) can be written as 2αtu + q(0)v with (u, v) in the set in
(2.13), such that 2αtu + q(0)v is√
maximized. Due to the central limit theorem, the first term in
(2.13) can be approximated by ϑi / 2αt with ϑi ∼ N (0, αt) when N is large. Therefore, the convex
hull of the set in (2.13) looks like the convex hull of the set
(
!
)
(N )
1
ϑi
1 Qi (0)
√
√
,
.
2αt log N q(0) N log N
i≤N

The convex hull of this set can be seen as a random variable, and converges, under an appropriate
metric, in probability to the limiting set
{(u, v)|u2 + h(v) ≤ 1, −1 ≤ u ≤ 1, 0 ≤ v ≤ 1},

(2.14)

in R2 , as N → ∞, cf. [7] and [8] for details on this. Our intuition says that the limit of the expression
in (2.12)
√ is attained at the coordinate (u, v) in the closure of the limiting set given in (2.14), such
that 2αtu + q(0)v is maximized. We show that this is indeed correct. In fact, we prove this in
Lemma 4.4 in a more general context than in [7] and [8]. In [7] and [8], the authors make the
assumption that the scaling sequences are the same, so the analysis is restricted to samples of the
type {(Xi /aN , Yi /aN )i≤N }. However, we show that for proving convergence of the maximum of the
pairwise sum, the scaling sequences do not need to be the same.
2.4. Examples and numerics. In Section 2.3, we showed that the shape of the fluid limit
depends on the distribution of the number of jobs at time 0. Here, we give some basic examples
how the fluid limit is influenced by the distribution of the number of jobs at time 0. We also present
and discuss some numerical results.
As a first example, for Ui = Xi+ , with Xi ∼ N (0, 1), we can write for v > 0, P(Ui > v) =
exp(−v 2 L(v)), such that L is slowly varying. Thus for v ∈ [0, 1],
− log (P(Ui > vt))
(vt)2 L(vt)
= lim
= v2.
t→∞ − log (P(Ui > t))
t→∞
t2 L(t)

h(v) = lim
Thus,

p
√
g(t, q(0)) = sup { 2αtu + q(0)v |u2 + v 2 ≤ 1, −1 ≤ u ≤ 1, 0 ≤ v ≤ 1} = q(0)2 + 2αt.
(u,v)

12

Concluding,
(N )

(N )

(1 − α/N ) (tN 3 log N ) − Si (tN 3 log N ) + Qi (0)
max
i≤N
N log N
√
(N )
3
(1 − α/N ) (tN log N ) − Si (tN 3 log N ) + bq(0)N log N Ui / 2 log N c
= max
i≤N
N log N
p
P
2
−→ q(0) + 2αt − βt as N → ∞,
√
P
(N )
where rN = q(0)N log N/ 2 log N , such that Q(α,β) (0) /(N log N ) −→ q(0), as N → ∞.
Another example is, when we assume that Ui is lognormally distributed, we know that
P(Ui > v) = P(Xi > log v), with Xi ∼ N (0, 1). Thus, P(Ui > v) = exp(−1(v > 0) log(v)2 L(log v)).
Then, for v ∈ [0, 1],

h(v) = lim

t→∞

1(v > 0) log(vt)2 L(log(vt))
log(t)2 L(log(t))

= 1(v > 0).

In this case, we have that
√
√
g(t, q(0)) = sup { 2αtu + q(0)v |u2 + 1(v > 0) ≤ 1, −1 ≤ u ≤ 1, 0 ≤ v ≤ 1} = max(q(0), 2αt).
(u,v)

We also consider the case P(Ui > v) = exp(1 − exp(v)), then for v ∈ [0, 1],
exp(vt) − 1
− log (P(Ui > vt))
= lim
= 1(v = 1).
t→∞ exp(t) − 1
t→∞ − log (P(Ui > t))
lim

Then,
√
√
g(t, q(0)) = sup { 2αtu + q(0)v |u2 + 1(v = 1) ≤ 1, −1 ≤ u ≤ 1, 0 ≤ v ≤ 1} = q(0) + 2αt.
(u,v)

As a last example, we observe the scenario that P(Ui > v) = exp(−vL(v)), thus h(v) = v. Then,
√
g(t, q(0)) = sup { 2αtu + q(0)v |u2 + v ≤ 1, 0 ≤ u ≤ 1, 0 ≤ v ≤ 1}
(u,v)

 



√
αt
2q(0)2
2q(0)2
= q(0) +
1 t<
+ 2αt1 t ≥
.
2q(0)
α
α

We would like to give some extra attention to the case where q(0) = α/(2β). Then, it is not difficult
to see that q(t) ≡ α/(2β). Thus, for these choices of h(v) and q(0), the system starts and stays
in steady state. One can show that this limit is only obtained for h(v) = v, so this gives us some
information on the joint steady-state distribution of all the queue lengths in the fork-join system.
Now, we turn to some numerical examples. In Figure 2, the simulated maximum queue length
is plotted together with the scaled fluid limit N log N q(t/(N 3 log N )), with q given in Theorem
2.1, and N = 1000. The queue lengths at time zero in Figures 2a, 2b and 2c are exponentially
distributed. These figures show that for N = 1000, the maximum queue length is not close to its
fluid limit.

13

(a) α = 1, β = 1, q(0) = 0.6

(b) α = 1, β = 1, q(0) = 0.75

(c) α = 1, β = 1, q(0) = 1

(d) α = 1, β = 1, q(0) = 0

(e) α = 1, β = 10, q(0) = 0

(f) α = 1, β = 100, q(0) = 0

Figure 2. Maximum queue length and fluid limit approximation (Thm. 2.1) for N = 1000

As these figures show, for N = 1000, the variance of the maximum queue length is still high. We
could however give some heuristic arguments why these results are not very accurate. As mentioned
before, we have that
A(N ) (tN 3 log N ) − (1 − α/N ) (tN 3 log N ) P
−→ −βt as N → ∞,
N log N
which is one building block of the fluid limit.
For (A(N ) (tN 3 log N ) − (1 − α/N ) tN 3 log N )/(N log N ), we can compute the standard deviation.
We have for α = β = t = 1 and N = 1000 that
s
r






α
α
β
α
β
(N
)
3
3
1− − 2
Var A (tN log N ) − 1 −
(tN log N ) =
+
btN 3 log N c
N
N N
N N2
=2628.26.
This is of the order of magnitude of the errors that we see in the figures.
Another
way of seeing that there is  a significant deviation is by looking

(N )
maxi≤N (1 − α/N ) tN 3 log N − Si (tN 3 log N ) . As mentioned in Section 2.3, we have that
(N )

(1 − α/N ) tN 3 log N − Si
√
N log N

(tN 3 log N )

at

d

≈ ϑi ,

with ϑi ∼ N (0, αt). Thus, this means that

p
 d
α 3
(N )
max 1 −
tN log N − Si
tN 3 log N ≈ max ϑi N log N .
i≤N
i≤N
N
√
When we choose N = 1000, α = t = 1, and simulate enough samples of maxi≤N ϑi N log N , we
observe a standard deviation which is higher than 900.
In Figures 2a, 2b and 2c, the high standard deviation is also caused by the distribution of the
number of jobs at time 0. For example, for Ei ∼ Exp(1/N ), i.i.d. for all i, and N = 1000, we have

14

p
that Var (maxi≤N Ei ) = 1282.16, so this is also of the order of magnitude of the errors that we
see.
As mentioned, one can prove fluid limits under several temporal and spatial scalings. In Figure
3, the maximum queue length is plotted against the rescaled fluid limit given in Proposition 2.1,
which is in orange, and the rescaled
limit, which is in green. In these plots, N =
p steady-state
√
1000. The rescaled fluid limit is 2αt/N 3 N log N , and the rescaled steady-state limit satisfies
α/(2β)N log N .

(a) α = 1, β = 1

(b) α = 1, β = 10

(c) α = 1, β = 100

Figure 3. Maximum queue length, fluid limit approximation (Prop. 2.1) and steady-state approximation for N =
1000

When we observe Figure 3, we see that for small time instances, the maximum queue length
follows the fluid limit described in Proposition 2.1 with a negligible deviation, and we also see that,
from the point that the fluid limit and steady state have intersected, the maximum queue length
follows the steady state, though with a significant deviation. This latter behavior can be very well
explained when we plot the same maximum queue lengths together with the fluid limit in Theorem
2.1, this is shown in Figure 4.

(a) α = 1, β = 1

(b) α = 1, β = 10

(c) α = 1, β = 100

Figure 4. Maximum queue length and fluid limit approximation (Thm. 2.1) for N = 1000

In Figure 5, we zoom in on the graphs given in Figure 3a and 3b. As these figures show, for small
time instances, the maximum queue length follows the fluid limit described in Proposition 2.1 quite
well. Again, p
we can heuristically explain the deviations by approximating the maximum queue
length with 1/N 3 N maxi≤N ϑi , with ϑi ∼ N (0, αt), i.i.d. For α = 1, and t = 7 · 107 , simulations
show that this approximation has a standard deviation around 95, and for t = 7 · 106 , we get a
standard deviation around 30, this is of the order of magnitude of the errors in Figure 5a and 5b,
respectively.

15

(a) α = 1, β = 1

(b) α = 1, β = 10

Figure 5. Maximum queue length, fluid limit approximation (Prop. 2.1) and steady-state approximation for N =
1000

3. Conclusion. In this paper, we analyzed a fork-join queue with N servers in heavy traffic.
We considered the case of nearly deterministic arrivals and service times, and we derived a fluid
limit of the maximum queue length, in Theorem 2.1, as N grows large.
Furthermore, we assumed delays to be memoryless. However, we are confident that these results
can be extended to nearly deterministic settings where the delays have general distributions. Another, but less straightforward extension of this result, would be to assume an arrival and service
process that are not Markovian.
Moreover, as the figures in Section 2.4 show, it should be possible to derive a more refined limit.
Therefore, it is interesting to look at second order convergence of the maximum queue length.
We are currently exploring this for the system in steady state. In other words, we try to gain
(N )
more insight in the process by finding a convergence result of Q(α,β) (∞) /N − α/(2β) log N . For
the process limit, proving a second order convergence result is much harder and more technical,
because the scaled maximum of N independent Brownian motions converges to a Brown-Resnick
process [6].
4. Proofs. In this section, we prove Theorem 2.1. Since each server has the same arrival process, the queue lengths are dependent. The general idea of proving Theorem 2.1 is to approximate
the scaled centralized service process in (4.4) by a normally distributed random variable. We can
use extreme value theory to prove convergence of the maximum of these normally distributed random variables in probability. By using the non-uniform version of the Berry-Esséen theorem, cf.
[16], we show that the convergence result of the original process is the same as the convergence
result with normally distributed random variables. Furthermore, we prove convergence of the part
involving non-zero starting points. This gives us the pointwise convergence of the process, which
we prove in Section 4.3. In this section, we also prove convergence of the finite-dimensional distributions. Finally, we prove in Section 4.4 that the process is tight. These three results together
prove the theorem.
4.1. Definitions. For the sake of notation, we use the expressions given in Definition 4.1 to
prove the tightness.
(N )
Definition 4.1. We define the random walk R̃i (n) as
(N )

(N )

R̃i

(n) =

Ã(N ) (n) + S̃i
log N

(n)

,

(4.1)

16

where
Ã(N ) (n) =

A(N ) (n) 
α  bnc
− 1−
,
N
N N

(4.2)

and
(N )

(N )

S̃i

(n) = −

Si

α  bnc
(n) 
+ 1−
.
N
N N

(4.3)

Furthermore,
(N )
Mi (t)

√ 3
(N )
tN log N
S̃i (tN 3 log N )
p
p
,
=
αt(1 − α/N ) log N btN 3 log N c

(4.4)

(N )

with A(N ) (n) and Si (n) given in Definitions 2.1 and Definition 2.2 respectively.
(N )
As mentioned in Section 2.3, when Q(α,β) (0) = 0, the quantity in (2.10) simplifies to
(N )

Q(α,β) (tN 3 log N )
N logN
 

(N )
(N )
A(N ) (tN 3 log N ) − A(N ) (sN 3 log N ) − Si (tN 3 log N ) − Si (sN 3 log N )
.
= max sup
i≤N 0≤s≤t
N log N
Consequently, we can rewrite
(N )

Q(α,β) (tN 3 log N )
N log N
(N )
(N )
Ã(N ) (tN 3 log N ) − Ã(N ) (rN 3 log N ) + S̃i (tN 3 log N ) − S̃i (rN 3 log N )
= max sup
i≤N 0≤r≤t
log N



(N )
(N )
3
3
= max sup R̃i
tN log N − R̃i
rN log N .
(4.5)
i≤N 0≤r≤t

4.2. Useful lemmas. In order to prove Theorem 2.1, a few preliminary results are needed.
(N )
As stated in Definition 4.1, we can write R̃i (n) as
(N )

Ã(N ) (n) + S̃i
log N

(n)

.

(N )

Observe that Ã(N ) (n) does not depend on i, while S̃i (n) does. Hence, it is intuitively clear
that Ã(N ) (n) pays no contribution to the maximum queue length. Therefore, in order to prove
(N )
the pointwise convergence of the maximum queue length, we need to analyze S̃i (n) / log N .
Specifically, we use the fact that
(N )

Mi

d

(t) −→ Z as N → ∞,

with Z a standard normal random variable, which can be shown by the central limit theorem. We
can use this result to approximate the maximum queue length, because we know that the scaled
maximum of N independent and normally distributed random variables converges to a Gumbel
distributed random variable. To prove the tightness of the maximum queue length, we have to
prove that
!
(N )
(N )
Q(α,β) (sN 3 log N ) Q(α,β) (tN 3 log N )
1
lim lim sup P
sup
−
>  = 0.
(4.6)
δ↓0 N →∞ δ
N log N
N log N
t≤s≤t+δ
In Lemma 4.1, a useful upper bound for the absolute value in (4.6) is obtained, which we use to
prove the tightness of the process.

17
(N )

Lemma 4.1. For t > 0, δ > 0 and Q(α,β) (0) = 0, we have that
(N )

(N )

Q(α,β) (sN 3 log N )

sup

N log N

t≤s≤t+δ

−

Q(α,β) (tN 3 log N )
N log N


(N )
sN log N − R̃i
tN 3 log N
t≤s≤t+δ i≤N 


(N )
(N )
tN 3 log N − R̃i
sN 3 log N .
+2 sup max R̃i
≤ sup max



(N )
R̃i

3



t≤s≤t+δ i≤N

(4.7)

(N )

In our proofs we use the fact that Mi (t) converges in distribution to a normally distributed
random variable. To be able to use this convergence result, we prove an upper bound of the
convergence rate in Lemma 4.2.
Lemma 4.2. For t > 0, we
p have that an upper bound of the rate of convergence of
√ 3
(N )
3
±S̃i (tN log N ) tN log N / αt(1 − α/N ) log N btN 3 log N c to a standard normal random variable is given by


(N )
P Mi (t) < y − Φ(y) ≤

1
c
√t
,
N log N 1 + |y |3

(4.8)

with ct > 0.
Lemma 4.2 follows from the main result in [16], where the author proves the non-uniform BerryEsséen inequality. To prove tightness, we need the following lemma:
Lemma 4.3. For t > 0,


(N )

lim sup Emax max
N →∞

±S̃i

i≤N

(tN 3 log N )
,0
log N

!5/2 
 ≤ (2αt)5/4 .

(4.9)

In order to prove pointwise convergence of the starting position, we show in Lemma 4.9 that
(N )

max

S̃i

i≤N

(N )

(tN 3 log N ) Qi (0)
+
log N
N log N

!
≈ max
i≤N

!
√
(N )
αtXi
Qi (0)
√
+
,
log N N log N

with Xi ∼ N (0, 1), as N is large.
√

√
(N )
In Lemma 4.4, we prove the convergence of maxi≤N
αtXi / log N + Qi (0)/(N log N ) .
Lemma 4.4 (Pointwise convergence approximation starting position).
max
i≤N

!
√
(N )
αtXi
Qi (0)
P
√
+
−→ g(t, q(0)) as N → ∞,
log N N log N

with Xi ∼ N (0, 1) i.i.d. and the function g as given in Theorem 2.1.
The proofs of Lemmas 4.1, 4.2, 4.3, and 4.4 can be found in Appendix C. Lemma 4.4 follows from
Pk
(j)
(j)
Lemma B.1, where a more general result is proven on maxi≤N j=1 Yi /aN .

18

4.3. Pointwise convergence. In this section, we prove pointwise convergence of the scaled
maximum queue length appearing in Theorem 2.1.
Theorem 4.1 (Pointwise convergence). For t > 0,
(N )

Q(α,β) (tN 3 log N )
N log N

P

−→ q(t) as N → ∞,

(4.10)

with q(t) given in Equation (2.5).
As Equation (2.10) shows, we can write the scaled maximum queue length as a maximum of two
random variables, namely, one pertaining to a system starting empty and one pertaining to a
system starting non-empty. We prove the pointwise convergence of the first part of this maximum
in Lemma 4.5. In Lemma 4.9 we prove the pointwise convergence of the second part. In order to
do so, we need some extra results, which are stated in Lemmas 4.4, 4.6, 4.7, and 4.8.
(N )

Lemma 4.5. For t > 0 and Q(α,β) (0) = 0,

 



(N )
√
Q(α,β) (tN 3 log N ) P
α
α
α
−→
2αt − βt 1 t < 2 +
1 t ≥ 2 as N → ∞.
N log N
2β
2β
2β
To prove convergence of sequences of real valued random variables to a constant it suffices to show
convergence in distribution. Therefore, we use Lemmas 4.6, 4.7 and 4.8 below to prove that the
upper and lower bound of the cumulative distribution function converge to the same function.
(N )

Lemma 4.6. For δ > 0, t < α/(2β 2 ) and Q(α,β) (0) = 0,
(N )

lim sup P
N →∞

Q(α,β) (tN 3 log N )
N log N

>

√

!
2αt − βt + δ

= 0.

(4.11)

Proof Let δ > 0 be given. Let us assume that t < α/(2β 2 ). We then have that
!
(N )
Q(α,β) (tN 3 log N ) √
P
> 2αt − βt + δ
N log N
!
!
(N )
√
Ã(N ) (sN 3 log N ) + S̃i (sN 3 log N )
− 2αt + βt > δ .
= P max sup
i≤N 0≤s≤t
log N
√
For t < α/(2β 2 ), 2αt − βt is an increasing function. Therefore,
!
!
(N )
√
Ã(N ) (sN 3 log N ) + S̃i (sN 3 log N )
P max sup
− 2αt + βt > δ
i≤N 0≤s≤t
log N
!
!
(N )
Ã(N ) (sN 3 log N ) + S̃i (sN 3 log N ) √
≤ P max sup
− 2αs + βs > δ
i≤N 0≤s≤t
log N
!
!
(N )
Ã(N ) (sN 3 log N ) + S̃i (sN 3 log N ) √
= P sup max
− 2αs + βs > δ .
i≤N
log N
0≤s≤t
Observe that
!
!
√
− 2αs + βs > δ
P sup
0≤s≤t
!
(N )
Ã(N ) (sN 3 log N ) + S̃i (sN 3 log N ) √
≤ P sup max
− 2αs + βs > δ
log N
0≤s≤t i≤N
!
!
(N )
Ã(N ) (sN 3 log N )
δ
maxi≤N S̃i (sN 3 log N ) √
δ
≤ P sup
+ βs >
+ P sup
− 2αs >
.
log N
2
log N
2
0≤s≤t
0≤s≤t
(N )

Ã(N ) (sN 3 log N ) + S̃i
max
i≤N
log N

(sN 3 log N )

19

Moreover, Ã(N ) (n) / log N + βn/(N 3 log N ) is a martingale with mean 0. Therefore, by Doob’s
maximal submartingale inequality
!
Ã(N ) (sN 3 log N )
δ
P sup
+ βs >
log N
2
0≤s≤t
!
Ã(N ) (sN 3 log N )
bsN 3 log N c
δ
bsN 3 log N c
≤ P sup
+β
− βs >
+ sup β
log N
N 3 log N
N 3 log N
2
0≤s≤t
0≤s≤t
!


bsN 3 log N c
bsN 3 log N c
δ
δ
Ã(N ) (sN 3 log N )
+ P sup β
+β
− βs >
≤ P sup
>
log N
N 3 log N
4
N 3 log N
4
0≤s≤t
0≤s≤t
!
(N )
3
16
Ã (tN log N )
≤ 2 Var
+ oN (1)
δ
log N



16
α
β
α
β btN 3 log N c
N →∞
= 2 1− − 2
+ 2
+ oN (1) −→ 0.
(4.12)
δ
N N
N N
N 2 (log N )2
Furthermore, in order to have
δ
maxi≤N S̃i (sN 3 log N ) √
P sup
− 2αs >
log N
2
0≤s≤t
(N )



we need to have that
u.o.c. Thus

(N )

maxi≤N S̃i

lim P

N →∞

(sN 3 log N ) / log N, s ∈ [0, t]



!
N →∞

−→ 0,

converges to

(4.13)
√


2αs, s ∈ [0, t]

!
(N )
maxi≤N S̃i (sN 3 log N ) √
− 2αs >  = 0,
log N

(4.14)

and for all r ∈ [0, t],
1
lim lim sup P
η↓0 N →∞ η

sup
r≤s≤r+η

!
(N )
(N )
maxi≤N S̃i (sN 3 log N ) maxi≤N S̃i (rN 3 log N )
−
>  = 0.
log N
log N

To prove the limit in (4.14), we use the result of Lemma 4.2 and observe that for all δ > 0,
!
(N )
maxi≤N S̃i (sN 3 log N ) √
> 2αs + δ
P
log N
!N
(N )
S̃i (sN 3 log N ) √
=1 − P
< 2αs + δ
log N
!N
√
√ 3
2αs + δ p
sN log N
(N )
=1 − P Mi (s) < p
log N p
bsN 3 log N c
αs(1 − α/N )
!
!N
√
√ 3
2αs + δ p
sN log N
cs
≤1 − Φ p
log N p
− √
N log N
bsN 3 log N c
αs(1 − α/N )
!N 
√
√ 3
N
2αs + δ p
sN log N
cs
≤1 − Φ p
log N p
+ 1+ √
−1
N log N
bsN 3 log N c
αs(1 − α/N )
N →∞

−→ 0.

(4.15)

20

The proof that
maxi≤N S̃i (sN 3 log N ) √
P
< 2αs − δ
log N
(N )

!
N →∞

−→ 0,
(N )

goes analogously. To prove the quantity in (4.15), we observe that due to the facts that S̃i (n) is
a random walk that satisfies the duality principle, maxi≤N xi − maxi≤N yi ≤ maxi≤N (xi − yi ), and
P(|X | > ) ≤ P(X > ) + P(−X > ), we have the upper bound
!
(N )
(N )
1
maxi≤N S̃i (sN 3 log N ) maxi≤N S̃i (rN 3 log N )
P
sup
−
>
η
log N
log N
r≤s≤r+η
!
!
(N )
(N )
S̃i (sN 3 log N )
1
−S̃i (sN 3 log N )
1
>  + P sup max
>  + oN (1).
≤ P sup max
η
log N
η
log N
0≤s≤η i≤N
0≤s≤η i≤N
The oN (1) term appears since b(r + η)N 3 log N c − brN 3 log N c ∈ {bηN 3 log N c, bηN 3 log N c + 1}.
(N )
Now, we have that ±S̃i (n) is a martingale with mean 0. The maximum of independent mar

5/2
(N )
tingales is a submartingale; therefore, max 0, maxi≤N ±S̃i (ηN 3 log N ) / log N
is a nonnegative submartingale. Hence, by use Doob’s maximal submartingale inequality we can conclude
that
!
!
(N )
(N )
S̃i (sN 3 log N )
−S̃i (sN 3 log N )
1
1
P sup max
>  + P sup max
>
η
log N
η
log N
0≤s≤η i≤N
0≤s≤η i≤N


!5/2 
!5/2 
(N )
(N )
3
3
1
S̃
log
N
1
−
S̃
log
N
(ηN
)
(ηN
)
i
+
.
≤ 5/2 Emax max i
,0
Emax max
,0
i≤N
i≤N
η
log N
η5/2
log N
By taking the lim supN →∞ in this expression and applying Lemma 4.3, we see that this is upper bounded by 2η 1/4 (2α)5/4 /5/2 . This can be made as small as possible when η is chosen small
(N )
enough. We also know that maxi≤N S̃i (0) / log 
N = 0, and that the finite-dimensional distribu(N )
3
tions of maxi≤N S̃i (sN log N ) / log N, s ∈ [0, t] converge to the finite-dimensional distributions
√

2αs, s ∈ [0, t] , which follows from Theorem 4.2. The lemma follows.

of
2
2
Having examined t ∈ [0, α/(2β )), we now turn to t ∈ [α/(2β ), ∞].
(N )

Lemma 4.7. For δ > 0, α/(2β 2 ) ≤ t ≤ ∞ and Q(α,β) (0) = 0,
(N )

lim sup P

Q(α,β) (tN 3 log N )
N log N

N →∞

α
>
+δ
2β

!
= 0.

Proof We write
(u,N )

A

(n) =

n
X

X (u,N ) (j)

j=1

with
X

(u,N )


(j) =

α/N + β/N 2 − m/N 2 w.p. 1 − α/N − β/N 2 ,
−1 + α/N + β/N 2 − m/N 2 w.p.
α/N + β/N 2 ,

21

with 0 < m < β. Furthermore, we write
(u,N )
Si
(n)

=

n
X

(u,N )

Yi

(j) ,

j=1

with
(u,N )
Yi
(j)


=

− α/N − β/N 2 + m/N 2 w.p. 1 − α/N,
1 − α/N − β/N 2 + m/N 2 w.p.
α/N.

Thus,
(N )

A(N ) (n) − Si

(u,N )

(n) = A(u,N ) (n) + Si

(n) ,

and
sup
0≤k≤n



(N )

A(N ) (k) − Si


(u,N )
(k) ≤ sup A(u,N ) (k) + sup Si
(k) .
0≤k≤n

0≤k≤n

We obtain by using Doob’s maximal submartingale inequality that


h (u,N ) (u,N ) i
(u,N )
(u,N )
(u,N )
(j)
P sup A
(k) ≥ x ≤ E eθA X
e−θA x = e−θA x ,
0≤k≤n
(u,N )

with θA

the solution to the equation




h (u,N ) (u,N ) i  α
β
α
β
m
(u,N )
θA
X
(j)
+ 2 exp θA
−1 + + 2 − 2
=
E e
N
N N


 N N
α
β
α
β
m
(u,N )
+ 1 − − 2 exp θA
+
−
= 1.
N N
N N2 N2

When we consider the second order Taylor approximation of this expression with 1/N around 0,
we obtain


2mN 2
1
(u,N )
θA
=
+O
.
−α2 N 2 + αN 3 − 2αβN − β 2 + m2 + βN 2
N2
(u,N )

Consequently, we have for N large θA
≈ 2m/(αN ). By the monotone convergence theorem, we
know that


(u,N )
(u,N )
P sup A
(k) ≥ x ≤ e−θA x ≈ e−2m/(αN )x .
k≥0

In conclusion,
supk≥0 A(u,N ) (k) P
−→ 0 as N → ∞.
N log N
Similarly, by using Doob’s maximal submartingale inequality, we obtain that


(u,N )
(u,N )
x
P sup Si
,
(n) ≥ x ≤ e−θi
n≥0

22
(u,N )

with θi

the solution to the equation



h (u,N ) (u,N ) i α
β
m
α
(u,N )
(j)
Yi
θi
= exp θi
E e
1− − 2 + 2
N
N N
N




α
m
β
α
(u,N )
+ 1−
exp θi
= 1.
− − 2+ 2
N
N N
N
h (u,N ) (u,N ) i
(j)
Yi
with 1/N around 0 gives
The second order Taylor approximation of E eθi
(u,N )
θi

2N 2 (β − m)
=
2 +O
−α2 N 2 + αN 3 + (β − m)



(u,N )

1
N2


.

(u,N )

Thus, for N large, θi
≈ 2(β − m)/(αN ). Concluding, supn≥0 Si
(n) is stochastically domi(u,N )
nated by an exponentially distributed random variable Ei
with mean αN/(2(β − m)). Because
(u,N )
(u,N )
(u,N )
(u,N )
supn≥0 Si
⊥ Ej
for i 6= j.
(n) ⊥ supn≥0 Sj
(n) for i 6= j, we can conclude that also Ei
Therefore,
!
(u,N )
−x
α
maxi≤N Ei
N →∞
≤
P
(x + log N ) −→ e−e ,
N
2(β − m)
and
(u,N )

maxi≤N Ei
N log N

P

−→

α
as N → ∞.
2(β − m)

Because,
(N )

Q(α,β) (tN 3 log N )
N log N

(N )

Q(α,β) (∞)

≤st.

N log N

(N )

≤

supk≥0 A(u,N ) (k) maxi≤N supk≥0 Si
+
N log N
N log N

(k)

,

the lemma follows.



Lemma 4.8. For δ > 0 and

(N )
Q(α,β) (0)

(N )

lim inf P
N →∞

Q(α,β) (tN 3 log N )
N log N


≥

√

= 0,

!
 



α
α
α
2αt − βt 1 t < 2 +
1 t ≥ 2 − δ = 1.
2β
2β
2β

(4.16)

Proof Let us first assume that t ≤ α/(2β 2 ). We have the lower bound
(N )

Q(α,β) (tN 3 log N )
N log N

(N )

≥st. max
i≤N

A(N ) (tN 3 log N ) − Si
N log N

(tN 3 log N )

.

By Equations (4.12) and (4.13), we know that
(N )

A(N ) (tN 3 log N ) − Si
max
i≤N
N log N

(tN 3 log N )

P

−→

√
2αt − βt as N → ∞.

Let us now assume that t > α/(2β 2 ). We have that




(N )
α
3
(N )
A(N ) 2βα2 N 3 log N − Si
N
log
N
2
Q(α,β) (tN 3 log N )
2β
α
P
≥st. max
−→
,
i≤N
N log N
N log N
2β
as N → ∞, by again using Lemma 4.6. This proves the lemma.



23

Proof of Lemma 4.5 By combining the results of Lemmas 4.6, 4.7 and 4.8, Lemma 4.5 follows.

In Lemma 4.9, we connect the convergence of
(N )

max
i≤N

(N )

A(N ) (tN 3 log N ) − Si (tN 3 log N ) + Qi
N log N

(0)

to the convergence of
max
i≤N

!
√
(N )
αtXi
Qi (0)
√
.
+
log N N log N

Lemma 4.9 (Convergence starting position). Assume that for Xi i.i.d. standard normally
distributed,
!
√
(N )
αtXi
Qi (0)
P
max √
+
−→ g(t, q(0)) as N → ∞,
(4.17)
i≤N
log N N log N
for a certain function g. Then
(N )

max
i≤N

(N )

A(N ) (tN 3 log N ) − Si (tN 3 log N ) + Qi
N log N

(0)

P

−→ g(t, q(0)) − βt as N → ∞.

Proof We have
(N )

(N )

A(N ) (tN 3 log N ) − Si (tN 3 log N ) + Qi (0)
(4.18)
i≤N
N log N
(N )
(N )
A(N ) (tN 3 log N ) − (1 − α/N ) tN 3 log N
(1 − α/N ) tN 3 log N − Si (tN 3 log N ) + Qi (0)
=
+ max
.
i≤N
N log N
N log N
(4.19)
max

We already proved in Equation (4.12) that the first term in (4.19) converges to −βt. Furthermore,
we can rewrite the second term as

!
(N )
(N )
1
S̃i (tN 3 log N ) Qi (0)
+
+ ON
.
max
i≤N
log N
N log N
N log N
We can easily deduce from Lemma 4.2 that
!
!
p
p
(N )
αt(1 − α/N ) btN 3 log N c
S̃i (tN 3 log N )
ct
√
√ 3
P
<y −P
Xi < y ≤ √
,
log N
log N
N log N
tN log N
with Xi ∼ N (0, 1), and ct given in Lemma 4.2. Then, it is easy to see that
!
!
p
p
(N )
(N )
(N )
αt(1 − α/N ) btN 3 log N c
Qi (0)
S̃i (tN 3 log N ) Qi (0)
√
√ 3
P
+
<y −P
Xi +
<y
log N
N log N
N log N
log N
tN log N
ct
≤ √
.
(4.20)
N log N

24

Now, because of the facts that we assume the convergence result in (4.17), and
p

p
√


αt(1 − α/N ) btN 3 log N c
αtXi
1
√
√ 3
Xi = √
Xi ,
+ oN √
log N
log N
log N
tN log N

it is easy to see that
p
max
i≤N

!
p
(N )
αt(1 − α/N ) btN 3 log N c
Qi (0)
P
√
√ 3
−→ g(t, q(0)) as N → ∞.
Xi +
N log N
log N
tN log N

Let  > 0, then because of the bound given in (4.20), and the convergence result in (4.17),
!
!
(N )
(N )
S̃i (tN 3 log N ) Qi (0)
P max
+
< g(t, q(0)) − 
i≤N
log N
N log N
!N
(N )
(N )
S̃i (tN 3 log N ) Qi (0)
+
< g(t, q(0)) − 
=P
log N
N log N
!N 
p
p
N
(N )
αt(1 − α/N ) btN 3 log N c
c
Qi (0)
√
√ 3
√t
+
≤P
< g(t, q(0)) − 
Xi +
+1
−1
N log N
log N
N log N
tN log N
N →∞

−→ 0.

The proof that
(N )

P max
i≤N

S̃i

(N )

(tN 3 log N ) Qi (0)
+
log N
N log N

!

!
> g(t, q(0)) + 

N →∞

−→ 0,

goes analogously. Hence, the lemma follows.



Proof of Theorem 4.1 In Lemmas 4.5 and 4.9 we have proven that both parts in the maximum
in (2.10) converge to a limit. The lemma follows.

We can easily extend this result to finite-dimensional distributions.
Theorem 4.2 (The finite-dimensional distributions converge). If
P

X (N ) (t) −→ f (t)
for all t > 0, then for (t1 , t2 , . . . , tk )
 P
X (N ) (t1 ), X (N ) (t2 ), . . . , X (N ) (tk ) −→ (f (t1 ), f (t2 ), . . . , f (tk )) as N → ∞.
Proof


P X (N ) (t1 ), X (N ) (t2 ), . . . , X (N ) (tk ) − (f (t1 ), f (t2 ), . . . , f (tk )) > 
≤ P X (N ) (t1 ) − f (t1 ) + · · ·+ X (N ) (tk) − f (tk ) > 

  N →∞
≤ P X (N ) (t1 ) − f (t1 ) >
+ · · · + P X (N ) (tk ) − f (tk ) >
−→ 0,
k
k
with k·k the Euclidean distance in Rk .



25

4.4. Tightness. It is known that when a sequence of random processes is tight and its finitedimensional distributions converge, then this sequence converges u.o.c., cf. [5, Thm. 7.1, p. 80].
From [5, Thm. 7.3, p. 82], we know that a process (X (N ) (t), t ∈ [0, T ]) is tight when for all positive
η there exists an a and an integer N0 such that for all N ≥ N0

P X (N ) (0) > a ≤ η,

(4.21)

and for all  > 0 and η > 0, there exists a 0 < δ < 1 and an integer N0 such that for all N ≥ N0


1
(N )
(N )
P sup X (s) − X (t) >  ≤ η.
(4.22)
δ
t≤s≤t+δ
The conditions given in Equations (4.21) and (4.22) hold
processes
in the space
 for stochastic


(N )
of continuous functions. The process Q(α,β) tN 3 log N
N log N , t ∈ [0, T ] does not lie in this
(N )
(N )
space, because Q(α,β) (n) = Q(α,β) (bnc). However, since q(t) is a continuous function, the conditions



(N )
in (4.21) and (4.22) do also apply on Q(α,β) tN 3 log N
N log N , t ∈ [0, T ] , cf. [5, Cor. 13.4,
p. 142].
In order to prove tightness for the process given in Theorem 2.1, we need to prove tightness of
the maximum of two processes, as Equation (2.10) shows. In Lemma 4.10, we show that it suffices
to prove tightness of the two processes separately. Then, in Lemmas 4.11 and 4.12, we prove the
tightness of the two parts.
Lemma 4.10. Assume that (X (N ) (s), s ∈ [0, t]) and (Y (N ) (s), s ∈ [0, t]) converge to functions
(k(s), s ∈ [0, t]) and (l(s), s ∈ [0, t]) u.o.c., respectively, then (max(X (N ) (s), Y (N ) (s)), s ∈ [0, t]) converges to (max(k(s), l(s)), s ∈ [0, t]) u.o.c.
Proof The lemma holds because of the fact that


(N )
(N )
P sup max(X (s), Y (s)) − max(k(s), l(s)) > 
0≤s≤t

(N )
(N )
≤ P sup (max(X (s), Y (s)) − max(k(s), l(s))) > 
0≤s≤t

(N )
(N )
+ P sup (max(k(s), l(s)) − max(X (s), Y (s))) > 
0≤s≤t

(N )
(N )
≤ P sup max(X (s) − k(s), Y (s) − l(s)) > 
0≤s≤t

(N )
(N )
+ P sup max(k(s) − X (s), l(s) − Y (s)) > 
0≤s≤t




N →∞
(N )
(N )
≤2 P sup X (s) − k(s) >  + 2 P sup Y (s) − l(s) >  −→ 0.
0≤s≤t

0≤s≤t


(N )

Lemma 4.11 (Tightness of the first part). For  > 0, η > 0, T > 0 and Q(α,β) (0) = 0, ∃ 0 <
δ < 1 and an integer N0 such that ∀ N ≥ N0 and t ∈ [0, T ]

1
P
δ

(N )

sup
t≤s≤t+δ

Q(α,β) (sN 3 log N )
N log N

(N )

−

Q(α,β) (tN 3 log N )
N log N

!
≥  ≤ η.

(4.23)

26
(N )

Proof We take t > 0. From Lemma 4.1, and the fact that R̃i is a random walk that satisfies
the duality principle, we know that for N large enough,
!
(N )
(N )
Q(α,β) (sN 3 log N ) Q(α,β) (tN 3 log N )
1
P
sup
−
≥
(4.24)
δ
N log N
N log N
t≤s≤t+δ




1
(N )
(N )
3
3
≤ P sup max R̃i
sN log N + 2 sup max −R̃i
sN log N ≥  + oN (1)
(4.25)
i≤N
δ 0≤s≤δ i≤N
0≤s≤δ



 
 
1
1
(N )
(N )
≤ P sup max R̃i
sN 3 log N ≥
+ P 2 sup max −R̃i
sN 3 log N ≥
+ oN (1).
δ
2
δ
2
0≤s≤δ i≤N
0≤s≤δ i≤N
(4.26)
Now we focus on the first term in (4.26). The analysis of the second term goes analogously.


 
1
(N )
3
P sup max R̃i
sN log N ≥
(4.27)
δ
2
0≤s≤δ i≤N
!
(N )
Ã(N ) (sN 3 log N ) + S̃i (sN 3 log N ) 
1
(4.28)
≥
= P sup max
δ
log N
2
0≤s≤δ i≤N
!
!
(N )
1
Ã(N ) (sN 3 log N ) 
S̃ (sN 3 log N ) 
1
≤ P sup
≥
+ P sup max i
≥
.
(4.29)
δ
log N
4
δ
log N
4
0≤s≤δ
0≤s≤δ i≤N
In the proof of Lemma 4.6, we already showed that the second term in (4.29) is small. With
a similar proof asin Lemma
the first term is small. Concluding,
 4.6, one can also prove that
(N )
(N )
Q(α,β) tN 3 log N
N log N , t ∈ [0, T ] is tight, when Q(α,β) (0) = 0.

Lemma 4.12 (Tightness of the second part). For  > 0, η > 0 and T > 0, ∃ 0 < δ < 1 and
an integer N0 such that ∀ N ≥ N0 and t ∈ [0, T ]
1
P
δ



(N )

(N )

A(N ) (sN 3 log N ) − Si (sN 3 log N ) + Qi (0)
sup max
N log N
t≤s≤t+δ i≤N

(N )
(N )
A(N ) (tN 3 log N ) − Si (tN 3 log N ) + Qi (0)
− max
>  < η.
i≤N
N log N

(4.30)

Furthermore, for all η there exists an a > 0 such that
(N )

P

Q(α,β) (0)
N log N

!
> a < η.

(4.31)

Proof First of all, we observe that for a random variable X, P(|X | > ) ≤ P(X > ) + P(−X > ).
Thus, we can remove the absolute values in (4.30) and examine both cases. Since both cases satisfy
analogous proofs, we only write down the proof for the first case.
(N )

(N )

A(N ) (sN 3 log N ) − Si (sN 3 log N ) + Qi (0)
i≤N
N log N
t≤s≤t+δ


(N )
(N )
A(N ) (tN 3 log N ) − Si (tN 3 log N ) + Qi (0)
− max
>
i≤N
N log N
≤
(N )
A(N ) (sN 3 log N ) − Si (sN 3 log N )
1
P
sup
max
i≤N
δ
N log N
t≤s≤t+δ
1
P
δ





sup

max

27
(N )

A(N ) (tN 3 log N ) − Si
−
N log N
1
= P
δ

(tN 3 log N )
(N )

sup
0≤s≤δ

A(N ) (sN 3 log N ) − Si
max
i≤N
N log N

!

!
>

(sN 3 log N )

!

!
>  + oN (1).

This is the same expression as Equation (4.28). In Lemma 4.11, it is proven that this expression
will be small. At t = 0, we should choose a > 0 such that (4.31) holds for N ≥ N0 . This is the case,
P
(N )
because we know that Q(α,β) (0) /(N log N ) −→ q(0) as N → ∞. The lemma follows.



(N )
Corollary
4.1 (Tightness of the process). The process Q(α,β) tN 3 log N
N log N , t ∈

[0, T ] is tight.



(N )
Proof The process Q(α,β) tN 3 log N
N log N , t ∈ [0, T ] can be written as a maximum of two
processes. In Lemmas 4.11 and 4.12 itis proven that theseprocesses are tight. Then from Lemma
(N )
4.10 it follows that Q(α,β) tN 3 log N
N log N , t ∈ [0, T ] is tight.

Proof of Theorem 2.1 In Theorem 4.1, we proved that for fixed t, the stochastic process converges
in probability to a constant, in Theorem 4.2, we proved that the finite-dimensional distributions
converge and in Corollary 4.1, we showed that the process is tight. Thus the convergence holds
u.o.c.

We now prove that the scaled process in steady state converges to the constant α/(2β).
Proof of Proposition 2.2. Since we look at the system in steady state, we can assume w.l.o.g.
(N )
that Q(α,β) (0) = 0. Then, we have
(N )

(N )

Q(α,β) (∞)
N log N
(N )

≥st.

Q(α,β) (α/(2β 2 )N 3 log N )
N log N

d

(N )

because Q(α,β) (n) = maxi≤N sup0≤k≤n (A(N ) (k) − Si

(k)). We know by Lemma 4.5 that

(N )

Q(α,β) (α/(2β 2 )N 3 log N )
N log N

,

P

−→

α
as N → ∞.
2β

Furthermore, we know by Lemma 4.7 that for all δ > 0,
(N )

lim sup P
N →∞

Q(α,β) (∞)

α
>
+δ
N log N
2β

!
= 0.

The proposition follows.


(u,N )

(u,N )

Appendix A: Taylor expansion of θA . The parameter θA
is the strictly positive
solution to the equation




h (u,N ) (u,N ) i  α
β
α
β
(u,N )
θA
X
(j)
E e
=
+ 2 exp θA
−1 + + 2 − (N )
N N


 N N

β
α
β
α
(u,N )
+ 1 − − 2 exp θA
+
− (N )
= 1,
N N
N N2
(u,N )

with (N ) = m/N 2 . We found an approximation of θA , of 2m/(αN ). To investigate the behavior
(u,N )
of θA
more carefully, we look at the function θ(x) such that



f (x, θ(x)) = αx + βx2 exp
θ(x)
−1 + αx + βx2 − mx2


+ 1 − αx − βx2 exp θ(x) αx + βx2 − mx2 = 1.

28

h (u,N ) (u,N ) i
(j)
When we set xN = 1/N , we get f (xN , θ(xN )) = E eθA X
. We are interested in the case
that N is large, therefore we have to investigate f for x around 0. Since f (x, θ(x)) = 1, we know
that f (n) (0, θ(0)) = 0 for all n ≥ 1. When we solve these equations for θ iteratively, we can find
θ(i) (0) for all i ≥ 0 and we get a Taylor expansion of θ(x) around 0. Since f (x, θ(x)) = 1, we know
that
d
f (x, θ(x))
dx

= −α + αe−θ(0) + αθ(0) = 0.
x=0

Hence, θ(0) = 0. When we look at the second and the third derivative of f (x, θ(x)) around 0, while
using that θ(0) = 0, we see
d2
f (x, θ(x))
dx2

= 0,
x=0

and
d3
f (x, θ(x))
dx3

= 3θ0 (0) (αθ0 (0) − 2m) .
x=0

Because we know that f (x, θ(x)) = 1, we solve 3θ0 (0) (αθ0 (0) − 2m) = 0. This gives θ0 (0) = 0 or
θ0 (0) = 2m/α. θ0 (0) = 0 indicates the situation that θ ≡ 0. If we now use the information that
θ0 (0) = 2m/α and look at the fourth derivative of f we see that


d4
4m (3α2 − 3β + 2m)
00
f (x, θ(x))
= 4m 3θ (0) −
= 0.
dx4
α2
x=0
This gives that θ00 (0) = 4m (3α2 − 3β + 2m) /3α2 . In general, we can compute each derivative of
θ(0) iteratively. This gives
θ(x) =

2m
4m (3α2 − 3β + 2m) x2
x+
+ O(x3 ).
α
3α2
2

Since the function f (x, θ) − 1 is analytic we know by the implicit function theorem that the solution
(u,N )
θ(x) is also analytic. So for x = 1/N and N is large enough we know that θA
= 2m/(αN ) +
2
O (1/N ) .
Appendix B: Extreme values of sums of random variables. In this section, we prove
a convergence result of the maximum of N sums of n random variables. In order to do so, we use
and extend results from [7] and [8].
(1)

(2)

Lemma B.1. Consider sequences of continuous random variables (Yi , i ≥ 1), (Yi , i ≥ 1), . . . ,
(k)
(j)
(Yi , i ≥ 1), where all random variables in the sequence (Yi , i ≥ 1) are identically and indepen(j)
dently distributed and have infinite right endpoints. Furthermore, Yi and Ym(l) are independent for
(j)
all j, l ∈ {1, 2, . . . , k } and i, m ≥ 1, and Yi satisfies
2.5 with function h(j) (u(j) ). Fi Assumption

(j)
(j)
(j)
nally, we have sequences (aN , N ≥ 1) such that P Yi ≥ aN = 1/N . We assume that the random
(j)

(j)

(j)

P

are relatively stable, thus maxi≤N Yi /aN −→ 1, as N → ∞. Then
!
( k
)
k
k
(j)
X
X
X
Yi
P
(j)
(j)
(j)
(j)
max
−→ sup
u
h (u ) ≤ 1, u ≤ 1∀ j ≤ k as N → ∞.
(j)
i≤N
(u(j) ,j≤k)
j=1 aN
j=1
j=1

variables Yi

29

Proof First of all, let us choose u(1) , . . . , u(k) such that u(j) ≤ 1 for all j. It is a well-known result
that






N →∞
N →∞
(j)
(j)
k
(j) (j)
k
(j) (j)
P ∪N
∩
Y
≥
u
a
−→
1
⇐⇒
N
P
∩
Y
≥
u
a
−→ ∞.
i
i
i=1 j=1
j=1
N
N
From this, it follows that
log N +

k
X

 

N →∞
(j)
(j)
log P Yi ≥ u(j) aN
−→ ∞.

j=1

This is the case when
 
 

k − log P Y (j) ≥ u(j) a(j)
X
i
N
 < 1.
lim sup 
log
N
N →∞
j=1
Similarly,
 
 




k − log P Y (j) ≥ u(j) a(j)
X
i
N
N →∞
(j)
N
k
(j) (j)


> 1 ⇒ P ∪i=1 ∩j=1 Yi ≥ u aN
−→ 0.
lim inf
N →∞
log N
j=1


(j)
(j)
Because of the fact that we have P Yi ≥ aN = 1/N , we can conclude that

lim 

N →∞

k
X
j=1

 
 
 
 

(j)
(j)
(j)
(j) (j)
k
k
− log P Yi ≥ u(j) aN
−
log
P
Y
≥
u
a
X
X
i
N
 = lim 
 
  =
h(j) (u(j) ).
(j)
(j)
N →∞
log N
− log P Y ≥ a
j=1
j=1
i

N

Let us now call
c? =

sup
(u(j) ,j≤k)

( k
X
j=1

u(j)

k
X

)
h(j) (u(j) ) ≤ 1, u(j) ≤ 1∀ j ≤ k ,

j=1

and let  > 0 be small. Then, we
the case that
 distinguish two scenarios. First of all, we consider
(k)
(j)
(j)
(j)
(1)
|{1, . . . , k |h (u ) = 1 u > 0 }| ≤ k − 2. Then, there exists a sequence (u , . . . , u ) such that
Pk
Pk
(j)
?
(j)
(j)
j=1 u = c − , and
j=1 h (u ) < 1. Therefore,
!
!



k
(j)
X
Yi
N →∞
(j)
?
N
k
(j) (j)
P max
> c −  > P ∪i=1 ∩j=1 Yi ≥ u aN
−→ 1.
(j)
i≤N
j=1 aN

If |{1, . . . , k |h(j) (u(j) ) = 1 u(j) > 0 }| ≥ k − 1, we know that c? = 1, and we know that
!
!
k
k
(j)
(1)
(j)
X
X
Yi
Yi
Yi
P
max
−→ 1 as N → ∞.
≥
max
+
st.
(j)
(1)
(j)
i≤N
i≤N
aN
j=1 aN
j=2 aN
Thus, at this moment we can conclude that the limit cannot be smaller than c? . To prove that
!
!
k
(j)
X
Yi
N →∞
?
P max
> c +  −→ 0,
(B.1)
(j)
i≤N
j=1 aN

30
(j)
we first observe
that the

 boundary is given by {(u , j ≤ k)|
N
→∞
(j)
that N P Yi > u(j) aN −→ 0, for u(j) > 1. Hence,

lim sup P max
N →∞

i≤N

k
(j)
X
Y
i
(j)

j=1

aN

!

Pk

j=1 u

(j)

= c? + }. We already know

!
> c? +  > 0,

Pk
means that there are limiting points in the set {(u(j) , j ≤ k)| j=1 u(j) = c? +, u(j) ≤ 1∀ j }. However,
Pk
N →∞
(j)
(j)
we know that for all (u(j) , j ≤ k) with c? < j=1 u(j) < c? +  that N P(∩kj=1 {Yi ≥ u(j)
 aN }) −→ 0.
Thus, we know that there
Pkare no limiting points in the positive quadrants with starting points
(u(1) , . . . , u(k) ) with c? < j=1 u(j) < c? + . The union of a finite number of quadrants covers the
Pk
set {(u(j) , j ≤ k)| j=1 u(j) = c? + , u(j) ≤ 1∀ j }. For example, in the case that k = 2,


 m
m
d2/−1/2e+1
?
?
,v ≥ 1+ −
.
{(u, v)|u + v ≥ c + , u ∈ [0, 1], v ∈ [0, 1]} ⊂ ∪m=1
(u, v) u ≥ c − 1 +
2
4
2
For k > 2, an analogous proof can be given. Hence, the limit in (B.1) and the lemma follows.



Appendix C: Proof of Lemmas 4.1, 4.2, 4.3, and 4.4.
Proof of Lemma 4.1. We take s > t > 0. We write t(N ) = tN 3 log N , s(N ) = sN 3 log N , etc. We
first prove that for s(N ) > t(N ) , the following upper bound holds:


(N )
(N )
Q(α,β) t(N )
Q(α,β) s(N )


(N )
(N )
t(N )
−
≤ max R̃i
s(N ) − R̃i
i≤N
N log N
N log N



(N )
(N )
(C.1)
R̃i
t(N ) − R̃i (r) .
+ max
sup
i≤N

t(N ) ≤r≤s(N )

Due to the defined auxiliary processes in Definition 4.1, we can write the maximum
queue


(N )
(N )
length in terms of R̃i as in Equation (4.5). Similarly, we can rewrite Q(α,β) s(N ) / N log N −


(N )
Q(α,β) t(N ) / N log N as






(N )
(N )
(N )
(N )
R̃i
t(N ) − R̃i (u)
max sup
R̃i
s(N ) − R̃i (r) − max sup
i≤N
i≤N
0≤u≤t(N )
0≤r≤s(N )
#
"





(N )
(N )
(N )
(N )
R̃i
t(N ) − R̃i (r)
t(N ) + sup
= max R̃i
s(N ) − R̃i
i≤N
(N
)
0≤r≤s



(N )
(N )
(N )
− max sup
t
− R̃i (u) .
R̃i
i≤N

0≤u≤t(N )





(N )
(N )
Now, the following upper bounds for Q(α,β) s(N ) / N log N − Q(α,β) t(N ) / N log N hold:


(N )
(N )
Q(α,β) s(N )
Q(α,β) t(N )
−
N log
N log N
 N





(N )
(N )
(N )
(N )
(N )
≤ max R̃i
s
− R̃i
t(N ) + max sup
R̃i
t(N ) − R̃i (r)
i≤N
i≤N

 0≤r≤s(N )

(N )
(N )
(N )
− max sup
R̃i
t
− R̃i (u)
i≤N
(N )
0≤u≤t



(N )
(N )
≤ max R̃i
s(N ) − R̃i
t(N )
i≤N "
#






(N )
(N )
(N )
(N )
+ max
sup
R̃i
t(N ) − R̃i (r) − sup
R̃i
t(N ) − R̃i (u) .
i≤N

0≤r≤s(N )

0≤u≤t(N )

31







(N )
(N )
(N )
(N )
t(N ) − R̃i (r) and sup0≤u≤t(N ) R̃i
Observe that both sup0≤r≤s(N ) R̃i
t(N ) − R̃i (u) are
non-negative random variables. Furthermore,






(N )
(N )
(N )
(N )
sup
R̃i
t(N ) − R̃i (r) − sup
R̃i
t(N ) − R̃i (u)
0≤r≤s(N )

 0≤u≤t(N )

(N )
(N )
(N )
≤
sup
R̃i
t
− R̃i (r) .
t(N ) ≤r≤s(N )

Now, we can conclude that


(N )
(N )
Q(α,β) t(N )
Q(α,β) s(N )
−
N log
N log N
 N


(N )
(N )
(N )
s
− R̃i
t(N )
≤ max R̃i
i≤N "



(N )
(N )
+ max
sup
R̃i
t(N ) − R̃i (r) −
i≤N

0≤r≤s(N )
(N )

≤ max R̃i

sup

(N )

R̃i

0≤u≤t(N )



(N )
s(N ) − R̃i
t(N ) + max
i≤N

i≤N




sup

(N )

R̃i

t


(N )

(N )

− R̃i



#

(u)


(N )
t(N ) − R̃i (r) ,

t(N ) ≤r≤s(N )

and hence the inequality in Equation (C.1) is satisfied. We can similarly deduce the lower bound


(N )
(N )
Q(α,β) s(N )
Q(α,β) t(N )


(N )
(N )
s(N ) .
(C.2)
t(N ) − R̃i
−
≥ − max R̃i
i≤N
N log N
N log N
To show this, we write


(N )
(N )
Q(α,β) s(N )
Q(α,β) t(N )
−
N log N  N log N





(N )
(N )
(N )
(N )
= max sup
R̃i
s(N ) − R̃i (r) − max sup
R̃i
t(N ) − R̃i (u)
i≤N
i≤N
0≤r≤s(N ) 
0≤u≤t(N )


(N
)
(N )
R̃i
s(N ) − R̃i (r)
= max sup
i≤N
(N )
0≤r≤s
"
#





(N
)
(N
)
(N
)
(N )
R̃i
s(N ) − R̃i (u)
s(N ) + sup
− max R̃i
t(N ) − R̃i
i≤N
(N )
0≤u≤t



(N )
(N )
≥ max sup
R̃i
s(N ) − R̃i (r)
i≤N
0≤r≤s(N )






(N )
(N )
(N )
(N )
R̃i
s(N ) − R̃i (u) .
s(N ) − max sup
− max R̃i
t(N ) − R̃i
i≤N

i≤N

0≤u≤t(N )

Observe that
sup
0≤r≤s(N )



(N )

R̃i



(N )
s(N ) − R̃i (r) ≥

sup



(N )

R̃i



(N )
s(N ) − R̃i (u) ,

0≤u≤t(N )

because s(N ) > t(N ) , so on the left side of the inequality, the supremum is taken over a larger interval
than on the right side of the inequality. From this we can conclude that


(N )
(N )
Q(α,β) s(N )
Q(α,β) t(N )
−
N log N  N log N


(N )
(N )
≥ max sup
R̃i
s(N ) − R̃i (r)
i≤N
(N )
0≤r≤s






(N )
(N )
(N )
(N )
− max R̃i
t(N ) − R̃i
s(N ) − max sup
R̃i
s(N ) − R̃i (u)
i≤N
i≤N
0≤u≤t(N )






(N )
(N )
(N )
(N )
(N )
(N )
≥ − max R̃i
t
− R̃i
s
≥ − max R̃i
t(N ) − R̃i
s(N ) ,
i≤N

i≤N

32

and indeed (C.2) holds. Combining (C.1) and (C.2) gives


(N )
(N )
Q(α,β) t(N )
Q(α,β) s(N )
−
N log N
N log N


(N )
(N )
(N )
≤ max R̃i
s
− R̃i
t(N ) + max
sup
i≤N

i≤N



(N )

R̃i



(N )
t(N ) − R̃i (r) .

t(N ) ≤r≤s(N )

Thus,
(N )

(N )

Q(α,β) (s)

sup
t(N ) ≤s≤t(N ) +δ (N )

N log N

−

Q(α,β) t(N )



(N )
(N )
max R̃i (s) − R̃i
t(N )
i≤N
t(N ) ≤s≤t(N ) +δ (N )



(N )
(N )
t(N ) − R̃i (s) .
+
sup
max R̃i
≤

N log N

sup

t(N ) ≤s≤t(N ) +δ (N )

i≤N

(C.3)




(N )
(N )
t(N ) − R̃i (s) and
Since both supt(N ) ≤s≤t(N ) +δ(N ) R̃i


(N )
(N )
supt(N ) ≤s≤t(N ) +δ(N ) R̃i (s) − R̃i
t(N ) are non-negative random variables, we have that
sup
t(N ) ≤s≤t(N ) +δ (N )

(N )

max R̃i
i≤N

(N )

(s) − R̃i

t(N )





(N )
(N )
max R̃i (s) − R̃i
t(N )
i≤N
t(N ) ≤s≤t(N ) +δ (N )



(N )
(N )
t(N ) − R̃i (s) . (C.4)
+
sup
max R̃i
≤

sup

t(N ) ≤s≤t(N ) +δ (N )

i≤N

Combining the inequalities in (C.3) and (C.4) gives us the desired result.



(N )

Proof of Lemmah 4.2. S̃i i(n) is a sum of independent
 and identically distributed random
(N )
(N )
(N )
variables with E ±S̃i (1) = 0, and Var ±S̃i (1) = (1 − α/N ) α/N 3 . So, ±Mi (t) =
p
√
(N )
±S̃i (tN 3 log N ) tN 3 log N / αt(1 − α/N ) log N btN 3 log N c has mean 0 and variance 1, and
satisfies the central limit theorem. From [16] it follows that for all y,


3
(N )


p
1
±
S̃
1
(1)
(N )
i
P ±Mi (t) < y − Φ(y) ≤C p
E p
tN 3 log N 
.
1 + |y |3
btN 3 log N c
αt(1 − α/N ) log N
Observe that for N large enough and 0 <  < t, btN 3 log N c > (t − )N 3 log N . We also have that


3
(N )
p
±S̃i (1)
tN 3 log N 
E p
αt(1 − α/N ) log N
√


√ (1 + α2 )
N4 N
α 3 α
α3 
α
p
1−
=
+ 3 1−
≤2 N √
,
N N N
N
α
α(1 − α/N ) α(1 − α/N )N 3
which holds for N > max(1,
2α). Thus, the statement of Lemma 4.2 follows for N large enough,
p
2
with ct = 2C(1 + α )/ α(t − ).

Proof of Lemma 4.3. We have

!5/2  Z
!
(N )
(N )
∞
3
3
max
±
S̃
log
N
max
±
S̃
log
N
(tN
)
(tN
)
i≤N
i≤N
i
i
=
Emax 0,
P
> x2/5 dx
log N
log
N
0

33

!
√ 3
log
N
tN
log
N
p
=
P
> x2/5 p
dx
αt (1 − α/N ) log N btN 3 log N c
0
!N
√
Z ∞
x2/5 N 3 log N
(N )
=
1 − P ±Mi (t) < p
dx
α (1 − α/N ) btN 3 log N c
0

∞

Z

(N )
max ±Mi (t)
i≤N

∞

Z
≤
0

!
√
ct
x2/5 N 3 log N
p
− √
3
N
log N
α (1 − α/N ) btN log N c



1−
Φ


N

1

1+

√

x2/5

√
N 3 log N

α(1−α/N )btN 3 log N c



3 
 dx

N


√
x2/5 N 3 log N

∞

Z
≤

−Φ
0

p

!N


ct

+ 1 + √

N log N

α (1 − α/N ) btN 3 log N c

1

1+

√

x2/5

√

N 3 log N

α(1−α/N )btN 3 log N c

!5/2 
p
3 log N c
b
tN
αt(1
−
α/N
)
max
X
i≤N
i

√
√ 3
= Emax 0,
log N
tN log N
N



Z
+
0

∞



3  dx


p


ct

−1 + 1 + √

N log N

1

1+

√

x2/5

√

(C.5)



3  dx,


N 3 log N

(C.6)

α(1−α/N )btN 3 log N c

with Xi standard normally distributed. By Pickands [20, Thm. 3.2, p. 888], we know that the
expectation in (C.5) converges to (2αt)5/4 . Furthermore, the term in (C.6) is upper bounded by


Z
0

∞



−1 + exp 

√
log N



We substitute y = 1
1+ √


btN 3 log N c
N 3 log N (log N )

x

2/5

√

ct

1+

N 3 log N

α(1−α/N )btN 3 log N c

5/4 Z
0

1

√

x2/5

√

N 3 log N



!
3 
 dx.


(C.7)

α(1−α/N )btN 3 log N c

3 !
, then the term in (C.7) can be rewritten as

p

c
5( α (1 − α/N ))5/2 
√ t
y
N →∞
log N
−
1
+
e
dy −→ 0.
6(1 − y)1/6 y 11/6

The lemma follows.



Proof of Lemma 4.4 In order to prove that
!
√
(N )
αtXi
Qi (0)
P
−→ g(t, q(0)),
max √
+
i≤N
log N N log N
(N )

we first observe that, from the definition of Qi (0) in Theorem 2.1, it is easy to see that
!
√
√

(N )
(N )
αtXi
Qi (0)
αtXi
rN Ui
Vi
1
P
max √
+
− max √
+
≤ max
+
−→ 0
i≤N
i≤N
i≤N N log N
N log N
log N N log N
log N N log N

34

as N → ∞. Thus, from this it follows that
!
√
(N )
αtXi
Qi (0)
P
max √
+
−→ g(t, q(0)) ⇐⇒ max
i≤N
i≤N
log N N log N

√

Ui
αtXi + Nr√Nlog
N
√
log N

!
P

−→ g(t, q(0)) as N → ∞.

Let us first consider that Ui satisfies Assumption 2.4, thus Ui has a finite√right endpoint. Theorem 2.1 says that when Ui has a finite right endpoint, that g(t, q(0)) = 2αt + q(0). To prove
√
√
√
P √
this, first observe that g(t, q(0)) ≤ 2αt + q(0) because maxi≤N αtXi / log N −→ 2αt and
√
P
(N )
Q(α,β) (0) /(N log N ) −→ q(0). Hence, the only thing we need to establish is that for all γ < 2αt +
q(0),


p
√
rN Ui
N →∞
NP
αtXi + √
≥ γ log N −→ ∞.
N log N
√
√
√
P √
When γ < 2αt,
this
is
obvious,
because
U
>
0,
and
max
αtX
/
log
N
−→
2αt.√So, let us
i
i≤N
i
√
√
assume
that
2αt
≤
γ
<
2αt
+
q(0).
Because
U
has
a
finite
right
endpoint,
r
/(N
log N ) =
i
N
√
log N . By convolution, we have that

√
p
p
N P αtXi + log N Ui ≥ γ log N
Z γ √log N p
√

 e−z2 /(2αt)
p
p
=N P αtXi ≥ γ log N + N
P
log N Ui > γ log N − z √
dz
2αtπ
−∞
Z
Z γ
2
2
γ
N 1−v /(2αt) p
N −v /(2αt) p
log N dv =
log N dv.
≥N
P(Ui > γ − v) √
P(Ui > γ − v) √
2αtπ
2αtπ
γ−q(0)
−∞
√
2
From
this
it
follows,
that
when
1
−
v
/(2αt)
>
0,
this
integral
converges
to
∞
.
We
chose
2αt ≤
√
√
q(0) in the integral is smaller than 2αt and hence this
γ < 2αt + q(0), thus the lower bound γ −√
integral converges to ∞. Thus g(t, q(0)) = 2αt + q(0).
Let us now consider the scenario described in Assumption 2.5. Then g(t, q(0)) satisfies the limit
given in (2.7). We have the straightforward limit result that for standard normally distributed Xi ,
limt→∞ − log(P(Xi ≥ ut))/ − log(P(Xi ≥ t)) = u2 . Furthermore, following the assumptions on Ui in
Theorem 2.1, we know that limt→∞ − log(P(Ui ≥ vt))/ − log(P(Ui ≥ t)) = h(v). Thus from Lemma
B.1, we know that for sequences (aN , N ≥ 1), (bN , N ≥ 1) with P(Xi ≥ aN ) = P(Ui ≥ bN ) = 1/N ,
that


Xi Ui
P
+
−→ sup {u + v |u2 + h(v) ≤ 1, 0 ≤ u ≤ 1, 0 ≤ v ≤ 1} as N → ∞.
max
i≤N
aN bN
(u,v)
√

√
Now, we can use this result to prove that maxi≤N
αtXi / log N + rN Ui /(N log N ) converges to
the limit in (2.7). We first observe that
√



√
αtXi
rN Ui
Xi
rN Ui
max √
+
= max
2αt √
+ q(0)
.
i≤N
i≤N
q(0)N log N
log N N log N
2 log N
√
√
N →∞
P
P
We have that aN / 2 log N −→ 1, because maxi≤N Xi /aN −→ 1, and maxi≤N Xi / 2 log N −→
N →∞
1 as N → ∞. Analogously, bN q(0)N log N/rN −→ 1. Thus,


√

√
Xi
Ui
αtXi
rN Ui
P
2αt
+ q(0)
− max √
+
−→ 0 as N → ∞.
max
i≤N
i≤N
aN
bN
log N N log N
√

With an analogous proof as before, maxi≤N
2αtXi /aN + q(0)Ui /bN converges to the limit in
(2.7).


35

Appendix D: Notation.
• N : the number of servers.
• A(N ) (n): the number of arrivals up to time bnc.
• X (N ) (n): Bernoulli random variable indicating a potential arrival at time n ∈ N.
(N )
• Si (n): the number of finished services of server i up to time bnc.
(N )
• Yi (n): Bernoulli distributed random variable indicating a potential completed service at
server i at time n ∈ N.
• α, β are system parameters.
• p(N ) : the arrival probability, p(N ) = 1 − α/N − β/N 2 .
• q (N ) : the service probability, q (N ) = 1 − α/N .
(N )
• Q(α,β) (n): the maximum queue length at time bnc.
(N )
(N )
(N )
(N )
• Qi (0): the number of tasks at time 0 at queue i, Qi (0) = Ui + Vi .
(N )
(N )
• Ui : the independent part of the number of tasks at time 0 at queue i, Ui = brN Ui c.
(N )
• Vi : the dependent part of the number of tasks at time 0 at queue i.
• Ui : continuously distributed and positive random variable.
• rN : positive scaling sequence.
• h(v) = limt→∞ − log (P(Ui > vt)) / − log (P(Ui > t)).
• q(t): fluid limit of the process.


(N )
(N )
• g(t, q(0)): limit of maxi≤N A(N ) (tN 3 log N ) − Si (tN 3 log N ) + Qi (0) / N log N .
(N )
(N )
• R̃i (n) = (Ã(N ) (n) + S̃i (n))/ log N .
(N )
(N )
• Ã (n) = A (n) /N − (1 − α/N ) bnc/N .
(N )
(N )
• S̃i (n) = −Si (n) /N + (1
p.
p

√− α/N ) bnc/N
(N )
(N )
• Mi (t) = S̃i (tN 3 log N ) tN 3 log N / αt(1 − α/N ) log N btN 3 log N c .
P
n
• A(u,N ) (n) = j=1 X (u,N ) (j).
• X (u,N ) (j):
X

(u,N )


(j) =

α/N + β/N 2 − m/N 2 w.p. 1 − α/N − β/N 2 ,
−1 + α/N + β/N 2 − m/N 2 w.p.
α/N + β/N 2 ,

with 0 < m < β.
Pn
(u,N )
(u,N )
• Si
(n) = j=1 Yi
(j).
(u,N )
• Yi
(j):
(u,N )
Yi
(j)

• θA

(u,N )


=

− α/N − β/N 2 + m/N 2 w.p. 1 − α/N,
1 − α/N − β/N 2 + m/N 2 w.p.
α/N.

solves:
h (u,N ) (u,N ) i
(j)
E eθA X
= 1.

• Ei

(u,N )

∼ Exp (2(β − m)/(αN )).

Acknowledgments. This research is supported by the Netherlands Organisation for Scientific
Research through the programmes Grip on Complexity [Schol: 438.16.121], MEERVOUD [Vlasiou:
632.003.002], and Talent VICI [Zwart: 639.033.413]. We thank the referees for prompting us to
investigate the system where the number of jobs at time 0 increases with N . We thank Prof.
Balkema for referring us to literature on convergence of samples.

36
References
[1] Anderson CW, Coles SG, Hüsler J, et al. (1997) Maxima of Poisson-like variables and related triangular
arrays. Ann. Appl. Probab. 7(4):953–971.
[2] Atar R, Mandelbaum A, Zviran A (2012) Control of fork-join networks in heavy traffic. Proc. 50th
Allerton Conf. Comm., Control Comput. (IEEE, Piscataway, NJ), 823–830.
[3] Baccelli F (1985) Two parallel queues created by arrivals with two demands: The M/G/2 symmetrical
case. Technical report RR-0426, INRIA, Montbonnot-Saint-Martin, France.
[4] Baccelli F, Makowski AM (1989) Queueing models for systems with synchronization constraints. Proc.
of the IEEE 77(1):138–161.
[5] Billingsley P (1968) Convergence of probability measures, 2nd ed. (John Wiley & Sons, New York).
[6] Brown BM, Resnick SI (1977) Extreme values of independent stochastic processes. J. Appl. Probab.
14(4):732–739.
[7] Davis RA, Mulrow E, Resnick SI (1988) Almost sure limit sets of random samples in Rd . Adv. Appl.
Probab. 20(3):573–599.
[8] Fisher L (1969) Limiting sets and convex hulls of samples from product measures. Ann. Math. Statist.
40(5):1824–1832.
[9] Flatto L, Hahn S (1984) Two parallel queues created by arrivals with two demands I. SIAM J. Appl.
Math. 44(5):1041–1053.
[10] Harrison JM (1985) Brownian Motion and Stochastic Flow Systems (John Wiley & Sons, New York).
[11] de Klein SJ (1988) Fredholm integral equations in queueing analysis. Ph.D. thesis, Rijksuniversiteit
Utrecht.
[12] Ko SS, Serfozo RF (2004) Response times in M/M/s fork-join networks. Adv. Appl. Probab. 36(3):854–
871.
[13] Lu H, Pang G (2015) Gaussian limits for a fork-join network with nonexchangeable synchronization in
heavy traffic. Math. Oper. Res. 41(2):560–595.
[14] Lu H, Pang G (2017) Heavy-traffic limits for a fork-join network in the Halfin-Whitt regime. Stochastic
Systems 6(2):519–600.
[15] Lu H, Pang G (2017) Heavy-traffic limits for an infinite-server fork–join queueing system with dependent
and disruptive services. Queueing Systems 85(1-2):67–115.
[16] Michel R (1976) On the constant in the nonuniform version of the Berry-Esséen theorem. Zeitschrift
für Wahrscheinlichkeitstheorie und verwandte Gebiete 55(1):109–117.
[17] Nelson R, Tantawi AN (1988) Approximate analysis of fork/join synchronization in parallel queues.
IEEE Trans. Comput. 37(6):739–743.
[18] Nguyen V (1993) Processing networks with parallel and sequential tasks: Heavy traffic analysis and
Brownian limits. Ann. Appl. Probab. 3(1):28–55.
[19] Nguyen V (1994) The trouble with diversity: Fork-join networks with heterogeneous customer population. Ann. Appl. Probab. 4(1):1–25.
[20] Pickands III J (1968) Moment convergence of sample extremes. Ann. Math. Statist. 39(3):881–889.
[21] Sigman K, Whitt W (2011) Heavy-traffic limits for nearly deterministic queues. J. Appl. Probab.
48(3):657–678.
[22] Sigman K, Whitt W (2011) Heavy-traffic limits for nearly deterministic queues: stationary distributions.
Queueing Systems 69(2):145.
[23] Varma S (1990) Heavy and light traffic approximations for queues with synchronization constraints.
Ph.D. thesis, University of Maryland.
[24] Wright PE (1992) Two parallel processors with coupled inputs. Adv. Appl. Probab. 24(4):986–1007.

