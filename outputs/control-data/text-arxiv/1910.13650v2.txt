arXiv:1910.13650v2 [math.OC] 3 Nov 2019

Bundle methods for dual atomic pursuit
Zhenan Fan

Yifan Sun

Michael P. Friedlander

Department of Computer Science
University of British Columbia
Vancouver, Canada
zhenanf@cs.ubc.ca

Department of Computer Science
University of British Columbia
Vancouver, Canada
ysun13@cs.ubc.ca

Department of Computer Science
University of British Columbia
Vancouver, Canada
mpf@cs.ubc.ca

Abstract—The aim of structured optimization is to assemble
a solution, using a given set of (possibly uncountably infinite)
atoms, to fit a model to data. A two-stage algorithm based on
gauge duality and bundle method is proposed. The first stage
discovers the optimal atomic support for the primal problem by
solving a sequence of approximations of the dual problem using a
bundle-type method. The second stage recovers the approximate
primal solution using the atoms discovered in the first stage. The
overall approach leads to implementable and efficient algorithms
for large problems.

I. I NTRODUCTION
A recurring approach for solving inverse problems that arise
in statistics, signal processing, and machine learning is based on
recognizing that the desired solution can often be represented
as the superposition of a relatively few canonical atoms
as compared to the signal’s ambient dimension. Canonical
examples include compressed sensing and model selection,
where the aim is to obtain sparse vector solutions; and
recommender systems, where low-rank matrix solutions are
required. Our aim is to design a set of algorithms that leverages
this sparse atomic structure in order to gain computational
efficiencies necessary for large problems.
Define the set of atoms by a set A ⊂ Rn . The atomic set
may be finite or infinite, but in either case, we assume that
the set is closed and bounded. A point x ∈ Rn is said to be
sparse with respect to A if it can be written as a nonnegative
superposition of a few atoms in A, i.e.,
X
x=
ca a, ca ≥ 0,
a∈A

where most of the coefficients ca associated with each atom
a are zero. Two examples are compressed sensing, where the
atoms are the canonical unit vectors and a sparse decomposition
is equivalent to sparsity in x and low-rank matrix completion;
where the atoms are the set of rank-1 matrices and a sparse
decomposition is equivalent to low rank. In each of these
cases, there is a convex optimization problem whose solution
is sparse relative to the required atomic set. There now exists
a substantial literature that delineates conditions under which
the correct solution is identified, typically in a probabilistic
sense [1]–[4].
Date: November 5, 2019. This work was supported by ONR award N0001416-1-2242.

Our focus here is on the approach advocated by Chandrasekaran et al. [5], who identified a set of convex analytical techniques based on gauge functions, which are normlike functions that are especially well suited to the atomic
description of the underlying model. We describe below a
linear inverse problem that generalizes the models analyzed by
Chandrasekaran et al.
II. ATOMIC PURSUIT
The atomic set A induces the gauge function
n
o
γA (x) = inf µ ≥ 0 x ∈ µÂ ,

(II.1)

where Ab = conv(A ∪ {0}) denotes the convex hull of A and
0. The gauge to A can be expressed equivalently as
 X

X
γA (x) = inf
ca x =
ca a ;
(II.2)
a∈A,ca ≥0

a∈A

see Bonsall [6]. The atomic pursuit problem minimizes the
gauge function over a set of linear measurements M x ∈ B,
where M : Rn → Rm is a linear operator, and B ⊂ Rm \ {0}
denotes the admissible set:
minimize
n
x∈R

γA (x)

subject to

M x ∈ B.

(II.3)

Chandrasekaran et al. [5] and Amelunxen et al. [7] describe
conditions under which a solution to this convex optimization
problem yields a good approximation to the underlying ground
truth.
Although (II.3) is convex and in theory amenable to efficient
algorithms, in practice the computational and memory requirements of general-purpose algorithms are prohibitively expensive.
However, algorithms specially tailored to recognize the sparse
atomic structure can be made to be effective in practice. In
particular, if we had information on which atoms participate
meaningfully in constructing a solution x∗ , then (II.3) can be
reduced to a problem over just those atoms.
Formally, define the set of supports of a vector x with respect
to A to be all the sets S ∈ A that satisfy
X
X
x=
ca a, ca > 0, γA (x) =
ca ,
(II.4)
a∈S

a∈S

i.e., all sets of atoms in A that contribute non-trivially to the
construction of x. If we can identify any support set S ∈

Atomic sparsity

A

Â

γA (x)

suppA (x)

σA (z)

non-negative
elementwise
low rank
PSD & low rank

cone({ e1 , . . . , en })
{ ±e1 , . . . , ±en }
{ uv T | kuk2 = kvk2 = 1 }
{ uuT | kuk2 = 1 }

non-negative orthant
cross polytope
nuclear-norm ball
{ X  0 | tr X ≤ 1 }

δ≥0
k · k1
nuclear norm
tr +δ0

cone({ ei | xi > 0 })
{ sign(xi )ei | xi 6= 0 }
singular vectors of x
eigenvectors of x

δ≤0
k · k∞
spectral norm
max { λmax , 0 }

TABLE I: Commonly used sets of atoms and their gauge and support function representations. The indicator function δC (x) is zero if x ∈ C
and +∞ otherwise.

suppA (x∗ ) for any solution x∗ , then (II.3) is equivalent to the
reduced problem
minimize
n
x∈R

γS (x)

subject to

M x ∈ B.

(II.5)

This is a potentially easier problem to solve, particularly in
the case where it is possible to identify a support set S ∈
suppA (x∗ ) that has small cardinality. For example, when Ab is
the cross-polytope, then identifying a small support S means
that the reduced problem (II.5) only involves the few variables
in S. Similarly, when Ab is the set of rank-1 positive semidefinite
matrices, then identifying a small support S corresponds to
finding the eigenspace of a low-rank solution X ∗ . In both
cases, knowing S can reduce the computational complexity
significantly.
III. D UAL ATOMIC PURSUIT
Our approach for constructing the optimal support set supp∗A
is founded on approximately solving the a dual problem that
is particular to gauge optimization (II.3). These dual problems
take the form
minimize
m
y∈R

σA (M ∗ y)

subject to

y ∈ B0 ,

(III.1)

where σA (z) = supa∈Ab ha, zi is the support function to the set
b and B 0 = { y ∈ Rm | hb, yi ≥ 1 ∀b ∈ B } is the antipolar
A,
to B. The dual relation between the pair (II.3) and (III.1) is
encapsulated by the inequality
1 ≤ hx, M ∗ yi ≤ γA (x) · σA (M ∗ y),

(III.2)

which holds for all pairs (x, y) that are primal-dual feasible,
i.e., M x ∈ B and y ∈ B 0 . Moreover, under a suitable constraint
qualification, (x, y) is optimal if and only if all of the above
inequalities hold with equality, in which case strong duality
holds [8, Corollary 5.4].
The following theorem shows that the gauge dual reveals
the optimal support for the primal solution.
Theorem III.1 (Optimal support identification). Let (x, y) ∈
Rn × Rm be any optimal primal-dual solution of the dual
pair (II.3) and (III.1). Then
S ⊆ FA (M ∗ y) ∀S ∈ supp∗A ,
where FA (z) := { x ∈ A | hx, zi = σA (z) } = ∂σA (z) is the
face of Ab exposed by z.
This result can be interpreted geometrically: the optimal
support atoms lie in the face of Ab exposed by M ∗ y. Moreover,
each atom is a subgradient of σA . The theoretical basis for

this approach is outlined by Friedlander et al. [8] and Aravkin
et al. [9].
IV. B UNDLE - TYPE TWO - STAGE ALGORITHM
The cutting-plane method for general nonsmooth convex
optimization was first introduced by Kelley [10]. It solves
the optimization problem via approximating the objective
function by a bundle of linear inequalities, called cutting
planes. The approximation is iteratively refined by adding
new cutting planes computed from the responses of the oracle.
The method is not to approximate the objective function over
its entire domain by a convex polyhedron, but to construct
an approximate valid lower minorant near the optimum.
Several stabilized versions, usually known as bundle methods,
were subsequently developed by Lemarechal et al. [11] and
Kiewel [12].
We give a simplified description of the construction of the
lower minorant in the context of a generic convex function
k
f : Rn → R. Let { x(j) , g (j) ∈ ∂f (x(j) ) }j=1 be the set of
pairs of iterates and subgradients visited through iteration k.
The cutting plane model at iteration k is
f (k) (x) = max

j=1,...,k

n

f (x(j) ) + hg (j) , x − x(j) i

o

.

(IV.1)

In our simplified description, the cutting-plane model is
polyhedral. We can, however, define these more generally, as
described in Section IV-B, where the models are spectrahedral.
The next proposition shows that, when specialized to support
functions, the cutting-plane models are themeselves support
functions, which only depends on the previous subgradients.
Proposition IV.1. (Cutting-plane model for support functions)
k
The cutting-plane model (IV.1) for f = σA and { z (j) , a(j) }j=1
being the set of pairs of iterates and subgradients is
f (k) (z) = σA(k) (z) with A(k) = {a(1) , . . . , a(k) }.
It follows that the cutting-plane model for this objective
of (III.1) takes the form
b
σAk (M ∗ y) with Ak ⊂ Ak+1 ⊂ · · · ⊂ A.
Coupled with Theorem III.1, we observe that that the sets
Ak ⊂ Ab that define the cutting-plane model are constructed
from the faces of Ab exposed by previous iterates {M ∗ yi }ki=1 .
The sets Ak thus contain atoms that are candidates for the
support of the optimal solution. In order to make this approach
computationally useful, care must be taken to ensure that

Algorithm 1 Generic bundle method
Input:
δ > 0 (tolerance)
y (1) ∈ B 0 (initial point)
d∗ (optimal dual value)
1: (Initialize bundle) Construct A(1) such that
A(1) ⊆ FA (M ∗ y (1) )
2: (Set center) ŷ = y (1)
3: for k = 1, 2, ... do
4:
(Upper bound) U (k) = min σA (M ∗ y (i) )
i=1,....k

5:
6:
7:

8:
9:
10:
11:

(Gap) δ (k) = U (k) − d∗
(Stopping criterion) If δ (k) ≤ δ then stop
(Level set)
L(k) := { y | σA(k) (M ∗ y) ≤ d∗ }
n
o
H (k) := y | hy − y (k) , ŷ − y (k) i ≤ 0
Y (k) := L(k) ∩ H (k) ∩ B 0
(Next iterate)
y (k+1) = projY (k) (ŷ)
(Update bundle) Construct A(k+1) satisfying (IV.2)
end for
return A(k)

atomic sets. In the polyhedral case, we construct Ak by
the traditional polyhedral cutting-plane model described by
Proposition IV.1 and show in that the optimal atomic support
is identified in finite time. In the spectral case, we follow
Helmberg and Rendl [15] who replace the polyhedral cuttingplane model by a semidefinite cutting-plane model. Principly,
the convergence of both methods follows from Brännlund et
al. [14, Theorem 3.7].
For simplicity, we assume that we know the optimal value
d∗ of (III.1). This assumption is valid in cases where it is
possible to normalize the ground truth, and then the optimal
value d∗ is known in advance.
A. Polyhedral Atomic Set
In the case where the atomic set A is finite, and the convex
hull Ab is polyhedral. Our specialized bundle update, which
satisfies (IV.2), is given by
A(k+1) = Fδ,A (z (k+1) ) ∪ {a(k+1) }

(IV.3)

where
Fδ,A (z (k+1) ) = { a ∈ A(k) | ha, z (k+1) i ≥ σA(k) (z (k+1) ) − δ }

is the relaxed exposed face.
Assume the first stage stops at iteration T with a candidate
atomic set A(T ) . Then we solve the primal atomic pursuit
the sets Ak do not grow too large. Proposition IV.1 is thus on the discovered atoms, namely the reduced problem (II.5)
most useful as a guide, and we consider below an algorithmic with S = A(T ) . The following theorem shows the recovery
variation that allows us to periodically trim the inscribing sets. guarantee.
Our method is based on the level bundle method introduced
by Bello Cruz and Oliveira [13]. Each iterate is computed via Theorem IV.3. If the atomic set is finite, then Algorithm 1
a projection onto the level set of the corresponding lower with specialized bundle update (IV.3) terminates in a finite
minorant. The sequence of candidate atomic sets { A(k) } number of iterations T for all δ ≥ 0:
b but does not necessarily grow monotonically as
inscribes A,
• if δ > 0, then
per Proposition IV.1. Instead, we follow the recipe outlined
δ
0 ≤ γA (x) − γA (x∗ ) ≤ ∗ ∗
,
by Brännlund and Kiwiel [14], which only requires A(k+1) to
d (d − δ)
contain the elements that contribute to y (k+1) . However, this
where x∗ and x denote the optimizer for atomic pursuit
only works for the polyhedral atomic sets A(k) . Applied to
and recovered solution respectively;
more general atomic sets, not necessarily polyhedral,
∗
• if δ = 0, then S ⊆ AT , for all S ∈ suppA (x ).
(k+1)
(k+1)
(k+1)
FA(k) (z
) ∪ {a
}⊆A
(IV.2)
where y (k+1) is the latest iterate, z (k+1) := M ∗ y (k+1) , and B. Spectral Atomic Set
a(k+1) ∈ FA (z (k+1) ). This rule ensures that the updates to the
candidate atomic set A(k) always contain exposed atoms that
define the lower minorant, and at least one exposed atom from
the full set. The general version of our proposed method is
outlined in Algorithm 1.
Let S ∗ denote the solution set to the problem (III.1), and
assume that S ∗ 6= ∅. Our next theorem shows the convergence
of Algorithm 1.
Theorem IV.2 (Convergence of Algorithm 1). The sequence
∞
{ y (k) }k=1 converges to the point y ∗ := projS ∗ (ŷ).
The proof for this theorem follows directly from Bello Cruz
and Oliveira [13, Theorem 3.4] with small modification.
We propose two approaches for constructing the candidate
sets Ak specialized for polyhedral atomic sets and for spectral

The bunlde method on the gauge dual (III.1) that we have
so far described can be interpreted as forming inscribing
polyhedral approximations to the atomic set. However, when
the atomic set is not polyhedral, which is usually the case
when dealing with semidefinite programs, these polyhedral
bundle-types do not perform that well. Can we form richer
non-polyhedral approximations to the atomic set? Helmberg and
Rendl [15] instead propose a semidefinite cutting plane model
that is formed by restricting the feasible set to an appropriate
face of the semidefinite cone. Here we will apply a similar
idea.
The spectral atomic set A = { uuT | kuk2 = 1 } contains
uncountably many atoms. The support function corresponding
to A has the explicit form σA (z) = max{λmax (z), 0} [8,
Proposition 7.2].

Now consider problem (III.1). Let y (k) be the current iterate
and P (k) be an n-by-r orthogonal matrix whose range intersects
the leading eigenspace of M ∗ y (k) . (In this setting, the adjoint
operator maps m-vectors to n-by-n symmetric matrices.) Then
we can build a local spectral inner approximation of A by
(k)

A

It is shown by Ding in [17] that under certain assumptions,
the recovery quality can be guaranteed.
Assumption IV.5 (Assumptions to ensure recovery quality).
The following three assumptions are critical for the recovery
guarantee of (IV.7).

T

= { P (k) V P (k) | V  0, tr(V ) ≤ 1 } .

This definition only uses information from the current iterate
y (k) . Now we consider all the previous iterates y (1) , . . . , y (k−1) .
Following the aggregation step proposed by Helmberg and
Rendl [15], we aggragate the information from previous iterates
b and get a richer spectral inner
into a single matrix W (k) ∈ A,
approximation by
A(k) = { αW (k) + P (k) V P

(k) T

| α + tr(V ) ≤ 1, V  0 } .
(IV.4)
The following result shows that the corresponding lower
minorant is easy to compute.
Proposition IV.4 (Spectral cutting-plane model). With A(k)
as defined in (IV.4), the spectral cutting-plane model is given
by
σA(k) (M ∗ y) = max { 0, λmax (T (k) ), hW (k) , M ∗ yi } . (IV.5)

a) (Uniqueness) Both primal problem (II.3) and dual problem (III.1) have unique solution x∗ and y ∗ .
b) (Strong duality) Every solution pair (x∗ , y ∗ ) satisfies
strong duality [8, Corollary 5.4]:
1 = hx∗ , M ∗ y ∗ i = γA (x∗ )σA (M ∗ y ∗ ).
c) (Strict complementarity) Every solution pair (x∗ , y ∗ )
satisfies the strict complementarity condition:
rank(x∗ ) = Mutiplicity of λmax (M ∗ y).
Theorem IV.6 (Spectral recovery guarantee). Assume Assumption IV.5 holds and the first stage stops at iteration T with
UT − d∗ ≤ δ. Let x∗ and x respectively denote the optimizer
for (II.3) and (IV.7). Then
√
kx∗ − xkF = O( δ)

T

where T (k) = P (k) M ∗ yP (k) .
The update of the bundle is as follows. Take any matrix
T
W̄ := ᾱW (k) + P (k) V̄ P (k) ∈ A(k) exposed by the latest
iterate M ∗ y (k+1) , i.e., hW̄ , M ∗ y (k+1) i = σA(k) (M ∗ y (k+1) ).
Then the important information of W̄ is contained in the
spectrum spanned by the eigenvectors of V̄ associated with
maximal eigenvalues. Consider the eigenvalue decomposition
V̄ = QΛQT , where Λ = diag(λ1 , . . . , λr ) with λ1 ≥ · · · ≥
λr . Split the spectrum Λ into two parts: Λ1 = λ1 I contains the
maximal eigenvalue with possible multiplicity, and Λ2 contains
the remaining eigenvalues. Let Q1 and Q2 be the corresponding
eigenvectors. Then we update the bundle by
T

ᾱW (k) + P (k) Q2 Λ2 QT2 P (k)
,
ᾱ + tr(Λ2 )
= orthog[P (k) Q1 , v (k+1) ],

W (k+1) =
P (k+1)

(IV.6)

where v (k+1) is any leading normalized eigenvector of
M ∗ y (k+1) .
Because the spectral atomic set is a continuum, we do not
expect to exactly obtain the optimal atomic support, and thus
exact recovery of the primal solution in finite time is not
possible. Friedlander and Macêdo [16, Corollary 4] show that
an approximate primal solution can be recovered by solving a
semidefinite least-squares problem. Given a set of candidate
optimal atoms A(T ) , an approximate primal solution can be
obtained as the solution of

1
minimize
kM x − bk22 subject to x ∈ cone A(T ) .
2
n
x∈R
(IV.7)

for any solution x∗ of (II.3).
The proof for this theorem follows directly from Ding et
al. [17, Theorem 1.2] with small modification.
V. E XPERIMENTS
A. Basis pursuit denoising
The basis pursuit denoising (BPDN) [18] problem arises
in sparse recovery applications. Let M : Rn → Rm be some
measurement matrix. Let x0 denote the original signal and
b = M x0 be the vector ofobservations, where x0 is sparse and
the observation b might be noisy. For some expected noise
level  > 0, the BPDN model is
minimize
n
x∈R

kxk1 subject to kM x − bk2 ≤ .

(V.1)

In this case, the atomic set A is the set of signed unit onehot vectors A = { ±e1 , . . . , ±en } and B is the 2-norm ball
centered at b with radius . The corresponding gauge dual
problem is given by
minimize
m
y∈R

kM T yk∞ subject to y ∈ B 0 ,

(V.2)

where the antipolar B 0 = { y | hb, yi − kyk2 ≥ 1 } follows
directly from the definition.

where the adjoint operator M ∗ applied to a vector y is defined
as
m
X
M ∗y =
yi Mi .
i=1

Fig. V.1: The coefficients of the original signal and the signals
recovered by cvx and by bundle.

Fig. V.3: Convergence of objective value and primal infeasibility.

Fig. V.2: Size of bundle and its intersection with the atomic support
for both original signal and signal recovered by cvx.

B. Phase retrieval
Phase retrieval is a problem of recovering signal from
magnitude-only measurements. Specifically, let x0 ∈ Rn be
some unknown signal and the measurements are given by
bi = |hx0 , mi i|2 , where each vector mi encodes illumination
i = 1, . . . , m. Candés et al. [19] advocate “lifting” the signal
as X0 = x0 xT0 so that the measurements are linear in X0 :
bi = hx0 xT0 , mi mTi i = hX0 , Mi i, i = 1, . . . , k,

VI. C ONCLUSION

where Mi = mi mTi . The following semidefinite program can
be used to recover X ∗ ≈ x0 xT0 :
minimize

tr(X) + δ0 (X)

subject to

hX, Mi i = bi , i = 1, . . . , m.

X

(V.3)

Define A as the set of normalized positive semidefinite rank-1
matrices, and define the linear operator M as
M (X) = [hX, Mi i]i=1,...,m .

(V.4)

Then the atomic pursuit problem (II.3) is equivalent to (V.3)
with B = {b}. The corresponding gauge dual problem is
minimize
m
y∈R

Fig. V.4: Images recovered at intermediate iterations of the spectral
bundle method for recovering a ground-truth image.

max { 0, λmax (M ∗ y) } subject to hb, yi ≥ 1,
(V.5)

Convex optimization formulations of inverse problems often
come with very strong recovery guarantees, but the formulations
may be too large to be practical for large problems. This
is especially true of spectral problems, which require very
expensive computational kernels. Our atomic pursuit approach
shifts the focus from the solution of the full convex optimization
problem to a sequence of “reduced” problems meant to expose
the constituent atoms that form the final solution. In some sense,
atomic pursuit is a generalization of more classical active-set
algorithms. Future avenues of research include the design of
specialized SDP solvers for the solution of the highly-structured
bundle subproblems, and applying the algorithm framework to
other atomic sets.

R EFERENCES
[1] B. Recht, W. Xu, and B. Hassibi, “Necessary and sufficient conditions
for success of the nuclear norm heuristic for rank minimization,” in
2008 47th IEEE Conference on Decision and Control. IEEE, 2008, pp.
3065–3070.
[2] D. Donoho, “For most large underdetermined systems of linear equations
the minimal `1 -norm solution is also the sparsest solution,” Communications on pure and applied mathematics, vol. 59, no. 6, pp. 797–829,
2006.
[3] E. Candes, J. Romberg, and T. Tao, “Robust uncertainty principles: Exact
signal reconstruction from highly incomplete frequency information,”
arXiv preprint math/0409186, 2004.
[4] B. Recht, M. Fazel, and P. A. Parrilo, “Guaranteed minimum-rank
solutions of linear matrix equations via nuclear norm minimization,”
SIAM review, vol. 52, no. 3, pp. 471–501, 2010.
[5] V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky, “The convex
geometry of linear inverse problems,” Foundations of Computational
mathematics, vol. 12, no. 6, pp. 805–849, 2012.
[6] F. F. Bonsall, “A general atomic decomposition theorem and banach’s
closed range theorem,” The Quarterly Journal of Mathematics, vol. 42,
no. 1, pp. 9–14, 1991.
[7] D. Amelunxen, M. Lotz, M. B. McCoy, and J. A. Tropp, “Living on
the edge: Phase transitions in convex programs with random data,”
Information and Inference: A Journal of the IMA, vol. 3, no. 3, pp.
224–294, 2014.
[8] M. P. Friedlander, I. Macedo, and T. K. Pong, “Gauge optimization and
duality,” SIAM Journal on Optimization, vol. 24, no. 4, pp. 1999–2022,
2014.
[9] A. Y. Aravkin, J. V. Burke, D. Drusvyatskiy, M. P. Friedlander, and
K. MacPhee, “Foundations of gauge and perspective duality,” arXiv
preprint arXiv:1702.08649, 2017.
[10] J. E. Kelley, Jr, “The cutting-plane method for solving convex programs,”
Journal of the society for Industrial and Applied Mathematics, vol. 8,
no. 4, pp. 703–712, 1960.
[11] C. Lemaréchal, A. Nemirovskii, and Y. Nesterov, “New variants of bundle
methods,” Mathematical programming, vol. 69, no. 1-3, pp. 111–147,
1995.
[12] K. C. Kiwiel, “Proximity control in bundle methods for convex nondifferentiable minimization,” Mathematical programming, vol. 46, no. 1-3,
pp. 105–122, 1990.
[13] J. B. Cruz and W. de Oliveira, “Level bundle-like algorithms for convex
optimization,” Journal of Global Optimization, vol. 59, no. 4, pp. 787–
809, 2014.
[14] U. Brännlund, K. C. Kiwiel, and P. O. Lindberg, “A descent proximal level
bundle method for convex nondifferentiable optimization,” Operations
Research Letters, vol. 17, no. 3, pp. 121–126, 1995.
[15] C. Helmberg and F. Rendl, “A spectral bundle method for semidefinite
programming,” SIAM Journal on Optimization, vol. 10, no. 3, pp. 673–
696, 2000.
[16] M. P. Friedlander and I. Macedo, “Low-rank spectral optimization via
gauge duality,” SIAM Journal on Scientific Computing, vol. 38, no. 3,
pp. A1616–A1638, 2016.
[17] L. Ding, A. Yurtsever, V. Cevher, J. A. Tropp, and M. Udell, “An
optimal-storage approach to semidefinite programming using approximate
complementarity,” arXiv preprint arXiv:1902.03373, 2019.
[18] S. S. Chen, D. L. Donoho, and M. A. Saunders, “Atomic decomposition
by basis pursuit,” SIAM review, vol. 43, no. 1, pp. 129–159, 2001.
[19] E. J. Candes, Y. C. Eldar, T. Strohmer, and V. Voroninski, “Phase retrieval
via matrix completion,” SIAM review, vol. 57, no. 2, pp. 225–251, 2015.

A PPENDIX A
P ROOFS
A. Proof for Theorem III.1
Proof: Let S ∈ suppA (x). It follows from (II.2) that there
exist strictly positive numbers { ca | a ∈ S } such that
X
X
x∗ =
ca a and γA (x) =
ca ,
a∈S

a∈S

Let x̂ = x/γA (x) be a normalized solution. Then
X ca
X ca
x̂ =
a and γA (x̂) =
≡ 1.
γA (x)
γA (x)
a∈S

a∈A

This implies that x̂ is necessarily a strict convex combination of
every point in S. Thus in order to establish that S ⊆ FA (M ∗ y),
it is sufficient to show x̂ ∈ FA (M ∗ y). By strong duality,
hx̂, M ∗ yi = σA (M ∗ y),
and it follows from the definition of exposed faces that x̂ ∈
FA (M ∗ y ∗ ).
B. Proof for Proposition IV.1
Proof: By the definition of subdifferential of support
functions, we have
f (k) (z)

=
=
=

max σA (z (j) ) + ha(j) , z − z (j) i

j=1,...,k

max ha(j) , zi

j=1,...,k

σA(k) (z).

C. Proof for Proposition IV.1
Proof: By the definition of subdifferential of support
functions, we have
f (k) (z)

=
=
=

max σA (z (j) ) + ha(j) , z − z (j) i

j=1,...,k

max ha(j) , zi

j=1,...,k

σA(k) (z).

D. Proof for Theorem IV.3
•

Proof:
When δ > 0, by strong duality( ??), we know that
σA (M ∗ y ∗ )γA (x∗ ) = 1,
σÂ (M ∗ ŷ)γÂ (x̂) = 1.
Then it follows that,
γÂ (x̂) − γA (x∗ )

=
=
≤

1
1
−
∗
σÂ (M ŷ) σA (M ∗ y ∗ )
σA (M ∗ y ∗ ) − σÂ (M ∗ ŷ)
σÂ (M ∗ ŷ)σA (M ∗ y ∗ )

.
d∗ (d∗ − )

Now by the fact that γÂ (x̂) ≥ γA (x̂) ≥ γA (x∗ ), we can
conclude that

γA (x̂) − γA (x∗ ) ≤ ∗ ∗
.
d (d − )
•

When δ = 0, first, we show that the algorithm will
terminate in finite steps with stopping criteria 2. Define
(k)

Sδ

= {a ∈ A | ha, M ∗ yk i ≥ d∗ − δ}.

By Theorem IV.2, we know that y (k) → y ∗ , where y ∗ is
some optimal solution to (III.1). Then there exist positive
number K such that ∀k ≥ K,
(k)

FA (M ∗ y ∗ ) ⊆ Sδ .
And by the construction of A(k+1) , we know that
(k)
A(k+1) ⊆ Sδ for all k. We can thus conclude that
there exist some finite number T such that AT +1 = AT .
Next, we show that when the algorithm terminate, the
bundle will contain suppA (x∗ ). From the discussion
above, we know that
(T )

AT = Sδ

and

(T )

FA (M ∗ y ∗ ) ⊆ Sδ .

Then by Theorem III.1, the result follows.
E. Proof for Proposition IV.4
Proof:
σA(k) (M ∗ y)

T
= max hM ∗ y, αW (k) + P (k) V P (k) i |

α ≥ 0, α + tr(V ) ≤ 1, V  0

= max αhW (k) , M ∗ yi + (1 − α)
0≤α≤1

(k)

(k) T

∗

max{hP V P
, M yi | tr(V ) ≤ 1, V  0}

= max αhW (k) , M ∗ yi +
0≤α≤1

T
(1 − α) max { 0, λmax (P (k) M ∗ yP (k) ) }
=

T

max { 0, λmax (P (k) M ∗ yPk ), hW (k) , M ∗ yi }



