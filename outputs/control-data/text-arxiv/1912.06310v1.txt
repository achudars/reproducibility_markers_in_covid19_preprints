Recruitment-imitation Mechanism for Evolutionary
Reinforcement Learning
Shuai Lüa,b,∗, Shuai Hana,b , Wenbo Zhoua,b , Junwei Zhanga,b

arXiv:1912.06310v1 [cs.LG] 13 Dec 2019

a Key

Laboratory of Symbolic Computation and Knowledge Engineering (Jilin University), Ministry of
Education, Changchun 130012, China
b College of Computer Science and Technology, Jilin University, Changchun 130012, China

Abstract
Reinforcement learning, evolutionary algorithms and imitation learning are three principal methods to deal with continuous control tasks. Reinforcement learning is sample efficient, yet sensitive to hyper-parameters setting and needs efficient exploration;
Evolutionary algorithms are stable, but with low sample efficiency; Imitation learning
is both sample efficient and stable, however it requires the guidance of expert data.
In this paper, we propose Recruitment-imitation Mechanism (RIM) for evolutionary
reinforcement learning, a scalable framework that combines advantages of the three
methods mentioned above. The core of this framework is a dual-actors and single critic
reinforcement learning agent. This agent can recruit high-fitness actors from the population of evolutionary algorithms, which instructs itself to learn from experience replay
buffer. At the same time, low-fitness actors in the evolutionary population can imitate
behavior patterns of the reinforcement learning agent and improve their adaptability.
Reinforcement and imitation learners in this framework can be replaced with any offpolicy actor-critic reinforcement learner or data-driven imitation learner. We evaluate
RIM on a series of benchmarks for continuous control tasks in Mujoco. The experimental results show that RIM outperforms prior evolutionary or reinforcement learning
methods. The performance of RIM’s components is significantly better than components of previous evolutionary reinforcement learning algorithm, and the recruitment
using soft update enables reinforcement learning agent to learn faster than that using
∗ Corresponding

author
Email address: lus@jlu.edu.cn (Shuai Lü)

Preprint submitted to Journal of Information Sciences

December 16, 2019

hard update.
Keywords: evolutionary reinforcement learning, reinforcement learning, evolutionary
algorithms, imitation learning

1. Introduction
An important goal of artificial intelligence is to develop agents with excellent decisionmaking capabilities in complex and uncertain environments. In recent years, the rapid
development of deep neural network enables agents based on reinforcement learning
methods to perform well in complex control tasks [1] [2] [3]. However, reinforcement
learning methods cannot always handle reward sparse problems effectively and their
parameters are very sensitive to disturbances. Recent studies have shown that evolutionary algorithms are better at dealing with sparse reward environments [4], and can
be employed as an extensible alternative to reinforcement learning in various tasks.
When a specific control task has expert data as a guide, imitative learning can also train
agents efficiently. Currently, imitative learning-based methods have been successfully
applied to drones [5], automated driving [6], and other fields.
These artificial intelligence algorithms have close relationships with principles of
biology [7]. Evolutionary algorithms are inspired by evolution of species. In each generation of the population, there are plenty of random mutations in different individuals.
Individuals with positive mutations will be selected by the environment. In imitative
learning, individuals with poor fitness can learn behavior patterns of individuals with
good fitness in a supervised learning way. At the same time, the diversity of genes
(parameters) is preserved. The biological basis of reinforcement learning is that individuals learn correct or wrong actions through trials and errors during interactions with
the environment. Off-policy reinforcement learning approaches allow individuals to
learn and update their policies from historical data [8] [1]. If reinforcement learning
individuals are allowed to learn from the entire historical interactions between population and environment, and regularly copy reinforcement learning individuals into the
population to participate in the evolution, both the learning process of reinforcement
learning and the evolutionary process of evolutionary algorithms can be accelerated [9]

2

Figure 1: Framework of RIM for evolutionary reinforcement learning

[10]. However, there are still two problems.
• Reinforcement learning agents can only learn from experience, but not directly
accept the guidance of elites in the current population..
• Reinforcement learning agents must have the same structure as individuals in the
population, or at least have a similar parameterized form. Otherwise, reinforcement learning individuals that are copied into the population may not be able to
participate in the evolutionary process.
In response to the above problems, this paper proposes Recruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning. The framework of RIM for evolutionary reinforcement learning is presented in Figure 1. The recruitment process
allows reinforcement learning (RL) agent to recruit the best individual from population to participate in RL agent decision and learning process. This requires RL agent
to have the structure of Figure 2. In Figure 2, gradient policy network and recruitment policy network simultaneously accept the current state as input, and respectively
produce actions as output. Critic network compares the potential rewards of actions
produced by the two policy networks, and outputs the action with higher reward. The
dual-policy decision-making mechanism not only enables the RL agent to produce better experiences, but also enables the RL agent to better estimate Q value, which enables
the learning process of RL agent to directly accept the guidance of excellent policy in
3

Figure 2: Structure of dual policy RL agent

the population. The imitation process permits low-fitness individuals in the population
to learn behavioral patterns of RL agent. Since the structure of RL agent is inconsistent
with that of individuals in the population, RL agent cannot be directly injected into
the population and participate in the evolution. The imitation process is designed for
addressing this problem.
The main contributions of this paper are as follows:
• We propose Recruitment-imitation Mechanism (RIM) and an evolutionary reinforcement learning framework that applies this mechanism. At the core of RIM,
a dual-policy RL structure is designed, which allows critic network to determines
actions.
• We present a series of optimization techniques for RIM, including an off-policy
imitation learning algorithm that directly uses experience replay buffer and soft
updating strategies for recruiting networks.
• We compare the performance of RIM with that of previous algorithms on Mujoco

4

benchmarks. Moreover, we discuss the impact of optimization techniques on
RIMs performance.
The rest of this paper is organized as follows. Section 2 briefly introduces the
background. Section 3 presents the proposed recruitment-imitation mechanism in detail. Section 4 shows the experimental results and comparisons. Section 5 discusses
advantages of RIM and outlines the future work.

2. Related Work
Combining reinforcement learning with imitation learning or evolutionary algorithms is not a new idea [11] [12] [13] [14]. The traditional combination methods
mostly consider imitation learning or evolutionary algorithms as a sub-process of reinforcement learning. Typical practices in such methods include: leveraging the distribution estimation method to improve the exploration noise in reinforcement learning
[15], utilizing the evolutionary method to optimize the approximation function in Q
learning [16], or using the imitation learning to optimize the initial stage of the RL process [17]. Unlike these traditional practices, evolution and imitation learning in RIM
are not sub-processes of reinforcement learning. In RIM, evolution and reinforcement
learning are two relatively independent learning processes, while imitation learning is
a way of synchronizing the behavioral policy of dual-policy agents into populations.
Evolutionary Reinforcement Learning (ERL) [9] provides a new paradigm for the
combination of evolutionary algorithms and reinforcement learning. ERL’s approach is
to reuse the interaction data between the population and the environment, and inject RL
policy into the population. RIM can be viewed as an extension of ERL. The expanded
parts include:
1) Combining evolutionary algorithms from the perspective of reinforcement learning. Namely, recruiting policy directly from the population to participate in reinforcement learning processes;
2) Constructing a dual-policy agent in the RL component. The agent can integrate
two policies to generate experience, and give better estimation of Q value when policy
learning is insufficient;
5

3) Leveraging off-policy imitation learning to synchronize the behavioral policy
of dual-policy agent into the population. By the way, the RL agent does not have to
maintain the same structure as the individuals in the population.
In the latest work, there are other ways to extend ERL: Collaborative Evolutionary Reinforcement Learning (CERL) [18] uses different combinations of learners to
complete exploration in different time-horizons of tasks; Cross-entropy Method Reinforcement Learning (CEM-RL) [10] replaces the combination of standard evolutionary algorithm and DDPG [1] in ERL with a combination of Cross-entropy Method
(CEM) and Twin Delayed Deep Deterministic policy gradient (TD3) [19]. Unlike the
above works, RIM is neither an optimization for task exploration nor a replacement of
sub-components in ERL. Instead, it introduces a recruitment imitation mechanism that
enables reinforcement learning, imitation learning, and evolutionary algorithms to be
more effectively combined. In other words, RIM, CERL and CEM-RL are independent
optimizations in different directions of ERL.

3. Background
This section introduces the background of deep reinforcement learning, evolutionary algorithms and imitation learning.
3.1. Deep reinforcement learning
Reinforcement learning methods abstract the interaction between an agent and environments into a Markov decision process. At each discrete time step t, the environment provides an observation st to the agent, and the agent takes an action at as a
response to this observation. Then, the environment returns a reward rt and the next
state st+1 to the agent. The goal of reinforcement learning is to maximize the expecPK
tation of Rt = k=0 γ k rt+k , where Rt indicates the sum of cumulative discounted
rewards of K steps from the current moment. γ ∈ (0, 1] is a discount factor.
Deep deterministic policy gradient (DDPG) is a widely used model-free reinforcement learning algorithm based on actor-critic structure [1]. In DDPG, actor and critic
are parameterized as π(s|θπ ) and Q(s, a|θQ ) respectively. They are called current net0

0

works. In addition, replications of actor and critic networks π 0 (s|θπ ) and Q0 (s, a|θQ )
6

are used to provide consistent targets for the learning process. They are called target
networks. During the learning process, target networks will be soft updated based on
current networks and a weighting factor τ . DDPG completes off-policy learning by
sampling experiences from a repaly buffer B. That is, for each interaction between
an agent and environments, the tuple (st , at , rt , st+1 ) is stored into the replay buffer.
One advantage of off-policy reinforcement learning is the sample efficiency of these
algorithms. The other advantage is that learning from historical data can decouple exploration process and learning process. DDPG’s exploration policy is constructed from
actor policy and noise: πe (st ) = π(st |θπ ) + N . The actor and critic networks are
updated by sampling mini-batch experiences from the replay buffer. Critic network is
updated by minimizing the loss function as follows:
L=

(i)

where yt

(i)

1 X (i)
(i) (i)
(y − Q(st , at |θQ ))2
N i t

(i)

(i)

0

(1)

0

= rt + γQ0 (st+1 , π 0 (st+1 |θπ )|θQ ). The actor network is updated ac-

cording to sampled policy gradient:
5θπ π|s(i) ≈

1 X
5a Q(s(i) , a|θQ )|a=π(s(i) ) 5θπ π(s(i) |θπ )
N i

(2)

3.2. Evolutionary algorithms
Evolutionary algorithms (EAs) are black-box optimization algorithms that are inspired by natural evolutionary processes. The evolutionary process acts on a population
composed of several parameterized candidate solutions (individuals). During each iteration, parameters of individuals in the population are randomly perturbed (mutated),
enabling new individuals to be generated. After generating new individuals, the environment evaluates their fitness. Individuals with higher fitness have higher probability
to be selected, and selected individuals will participate in the next iterative process.
When evolutionary algorithms are implemented to solve control tasks, several actor
networks will participate in the evolutionary process. During the iterative process, actor networks with higher fitness will be retained as elites and shielded mutation step [9].
EAs can be employed as an alternative for reinforcement learning techniques based on
7

Markov decision processes [4].
3.3. Imitation learning
In imitation learning, an actor imitates expert behaviors to obtain a policy that is
close to expert performance. Dataset aggregation (DAgger) method [20] is a widely
used imitation learning algorithm. It is an iterative reduction to online policy training
method. Suppose that there is an expert data set D = {s1 , a1 , ..., sn , an }. During each
iteration, the actor is trained on this dataset in a supervised manner. The loss function
PK
1
during training can be denoted as: J(θ) = K
k=1 L(πil (sk ), ak ), where πil is a
policy to be trained and K is the batch size of samples. After convergence, policy
πil is carried out to generate a state access set Dπ = {sn+1 , ..., sn+m }. Then Dπ is
labeled by actions output from the expert policy, and the expert data is accumulated:
D ← D ∪ Dπ . Then proceed to the next iteration. The advantage of DAgger is that it
can employ expert policy to teach actors how to recover from errors. DAgger is a class
of Follow-The-Leader algorithms [21].

4. Recruitment-imitation Mechanism
Recruitment-imitation mechanism in evolutionary reinforcement learning is presented in this section.
4.1. Dual policy reinforcement learning agent
The core of recruitment-imitation mechanism is a dual policy RL agent with dualactors and single-critic. In the iterative process of the evolutionary reinforcement learning algorithm, the dual policy RL agent can recruit excellent individuals from the population to participate in decision-making or reinforcement learning process. In addition,
individuals with poor performance in the population can periodically imitate behavior
patterns of dual policy RL agent to accelerate evolutionary speed of the population.
Dual policy RL agent in recruitment-imitation mechanism still uses the actor-critic
structure but has two actor networks, including a gradient policy network πpg (st |θpg )
and a recruitment policy network πea (st |θea ). When the agent makes decisions, the

8

critic network will identify which policy under the current state st is potentially more
profitable. Therefore, policy of the dual policy RL agent can be indicated as:

πrl =




π ,

 pg

Q(st , πpg (st |θpg )|θQ ) ≥ Q(st , πea (st |θea )|θQ )




 π ,
ea

otherwise

(3)

where πrl indicates the overall behavior policy of the dual policy RL agent. Similar
to DDPG, during the learning process, the critic network is updated by minimizing the
(i)

loss function of (1), except that yt is estimated using πrl :
(i)

(i)

(i)

(i)

0

yt = rt + γQ0 (st+1 , πrl (st+1 )|θQ )

(4)

The gradient policy network is still updated with the sampled policy gradient according
to equation (2).
There are two reasons that the dual policy RL agent can make RIM perform better:
• The dual policy RL agent can generate better experiences according to the behavior pattern in equation (3).
• When gradient update is performed, πrl used in equation (4) can make more
accurate estimation of Q0 .
Then we prove that equation (4) can give more accurate estimation of Q0 .
Theorem 1. Suppose that Q0 and Q converge at the same position for policy π ∗ . The
converged policy to maximize Q0 or Q is π ∗ , and the estimation value using π ∗ is Q∗ .
The mean of Q0 estimation of DDPG is denoted as Eddpg (Q̂0 ) and the mean of Q0
estimation of RIM is denoted as Erim (Q̂0 ). Then, Eddpg (Q̂0 ) ≤ Erim (Q̂0 ) ≤ Q∗ .
Proof:

Since π ∗ is the policy that maximizes Q0 (i.e., Q∗ ), any policy that is not

π ∗ will cause the estimate of Q0 to be less than Q∗ . So, there is Erim (Q̂0 ) ≤ Q∗ .
Erim (Q̂0 ) = Q∗ if and only if the policy of πrl in equation (3) is always π ∗ . Also

9

there is Eddpg (Q̂0 ) ≤ Q∗ . Eddpg (Q̂0 ) = Q∗ if and only if the policy πddpg of DDPG is
always π ∗ .
Assume that DDPG’s behavior policy πddpg causes the estimation of Q0 to be
R +∞
shifted downward by x with the probability of p(x), then Eddpg (Q̂0 ) = −∞ (Q∗ −
x)p(x)dx. Since RIM uses the policy in equation (3) to estimate Q0 in equation (4).
When πpg (i.e., πddpg ) estimates Q0 downwards by x with the probability of p(x), πea
can upwards correct y (y ≥ 0) with the probability of q(y|x). Then we have:

Erim (Q̂0 ) =

Z

+∞

Z

+∞

−∞
Z +∞

−∞
Z +∞

−∞
Z +∞

−∞

=
=
−∞
+∞

Z
=

(Q∗ − x + y)q(y|x)p(x)dydx
(Q∗ − x)q(y|x)p(x)dydx +

(Q∗ − x)(

Z

Z

+∞

+∞

yq(y|x)p(x)dydx

q(y|x)dy)p(x)dx +

yq(y|x)p(x)dydx
−∞

(Q∗ − x)p(x)dx +

Z

+∞ Z
−∞

+∞ Z

−∞

+∞

yq(y|x)p(x)dydx

−∞

= Eddpg (Q̂0 ) +

+∞

−∞
−∞
Z +∞ Z +∞

−∞

Z

Z

−∞

+∞

yq(y|x)p(x)dydx
−∞

−∞

Considering y ≥ 0 and q(y|x)p(x) ≥ 0, then
so it holds that Eddpg (Q̂0 ) ≤ Erim (Q̂0 ).

R +∞ R +∞
−∞

−∞

yq(y|x)p(x)dydx ≥ 0,


In Theorem 1, we can assume that Q and Q0 converge at the same position because
Q and Q0 are very close using soft update setup in continuous domains [19]. Theorem
1 states that the estimation of Q0 using πrim is more accurate than that estimated using
πddpg , or closer to Q∗ . Unlike overestimation problems [22] [23], Theorem 1 states
that when πddpg does not converge sufficiently during the updating process according
to equation (2), if the policy πea in population gives better actions, the Q0 estimation
of RIM will be more accurate. This is one of the reasons the RL component of RIM
can learn better.
4.2. Off-policy imitation learning
Due to the inconsistencies in structures of a dual policy RL agent (dual-actors and
single-critic) and an individual (single-actor) in the population, the dual policy RL

10

Figure 3: Framework of off-policy imitation learning in RIM.

agent cannot be directly injected into the population to participate in the evolution. In
this case, an ideal solution is to use imitation learning to unify behavior patterns of the
RL agent and individuals in the population.
As a typical imitation learning algorithm, DAgger can get better performance with
only a few iterations when the amount of data is large. Evolutionary reinforcement
learning system usually has a huge experience replay buffer for storing historical data.
The actors in the population to be trained can directly sample states and actions in the
experience replay buffer and be trained according to the following loss function:

J0 (θwt ) =

K
1 X
L(πwt (sk |θwt ), πrl (sk ))
K

(5)

k=1

where πwt indicates the policy of an actor to be trained. πwt is usually a policy of
an actor with the worst performance in the population. Figure 3 shows the framework of off-policy imitation learning and Algorithm 1 describes the detailed process
of off-policy imitation learning in RIM. We cancel the data labeling process and accumulation process of DAgger so that the imitation process is off-policy completely.
The main function of DAgger’s data labeling process and accumulation process is to
recover the imitation learner from errors. In the RIM environment setting, there is a

11

Algorithm 1 Off-policy imitation learning in RIM
1: Get the worst policy πwt from population
2: for n = 1 to N do
3:
Get replay buffer size: size ← len(B)
4:
while size > l0 do
5:
Sample random batch {si } from replay buffer
wt
6:
Get awt
i according to πwt (si |θ )
7:
Get πrl according to equation (3)
8:
Get expert action ai according to πrl (si )
9:
Calculate loss L between expert action awt
i and agent action ai
10:
Calculate 5θwt L
11:
Update θwt ← θwt + α · 5θwt L

(a) walker2d-v2

(b) Hopper-v2

Figure 4: Performance of different individuals in the population.

large amount of erroneous data in the experience replay buffer, and most of the states
sampled from the experience replay buffer is generated by a suboptimal or wrong policy. As shown in Figure 4, the performance of individuals in the population varies
widely, and even the interaction information of the worst individual is stored into the
replay buffer. Therefore the training process using equation (5) is also a process to
teach actors how to recover from an error state.
4.3. Learning and evolution process of RIM
The core idea of RIM is to 1) accelerate the learning process by recruiting elite
individuals in the population to guide the policy gradient network learning, and 2)
accelerate the evolutionary process by enabling individuals with poor performance in
the population to imitate behavioral patterns of the dual policy RL agent.
12

The process of RIM is as follows:
1) Some actor networks in the population and actor and critic networks in the dual
policy agent are initialized with random weights.
2) In the evolution process, the environment evaluates the fitness of actors in the
population and select a part of the actors to survive with a probability according to the
fitness.
3) The actors are perturbed by mutation and crossover to generate the next generation of actors.
4) After generating the next generation, the dual policy RL agent recruits an actor
with the highest fitness in the population, copies its parameters to the recruitment policy
network, and learns for a certain number of times.
Algorithm 2 describes the detailed learning process of dual policy RL agent. The
worst actor in the population imitates behavioral patterns of the dual policy RL agent
every several generations so that the learning outcomes of the dual policy RL agent can
be injected into the population.
During the evolutionary process, interaction information between actors and environments will be stored into the experience replay buffer. These experiences are not
only used for the sampling process of gradient policy network learning in Algorithm 2,
but also for the sampling process of the worst actor imitating in population. In the RIM
setting of this paper, the structure of gradient policy network is same as that of actors
in the population, so when the imitation learning is not performed, the gradient policy
network will be periodically copied into the population to accelerate the evolutionary
process.
The recruitment policy network participates in the decision process of πrl . There(i)

fore, it affects the value of yt . The learning process of critic network needs to be
(i)

provided with a consistent yt . Drawing on the idea of soft update in DDPG, we soft
update the target gradient policy network and critic network. In order to further ensure consistency in the learning process, we decide to add a target recruitment policy
network. Current recruitment policy network πea (st |θea ) uses hard update to copy directly from the population, and the target recruitment policy network uses soft update

13

Algorithm 2 Learning process of RL agent in RIM
1: Randomly initialize critic Q(s, a|θ Q ), RL actor πpg (s|θ pg ) and EA actor
πea (s|θea )
0
0
0
0
0
2: Initialize target network Q0 , πpg
, πea
with θQ ← θQ , θpg ← θpg , θea ← θea
3: Initialize replay buffer B, environment Env
4: for episode = 1 to M do
5:
Recruit champion from populations: θea ← θchamp
6:
Reset Env and get a state s
7:
done ← F alse
8:
while !done do
9:
π(s) ← maxπ {Q(s, πpg (s)), Q(s, πea (s))}
10:
Get action a according to π(s) + N
11:
Execute a and observe (r, s0 , done) ∼ Env
12:
Store experience (s, a, r, s0 ) into replay buffer B
13:
Update state: s ← s0
14:
if len(B) > l0 then
15:
Sample random batch {(st , at , rt , st+1 )i } from replay buffer
0
0
0
(i)
(i)
(i)
(i)
(i)
(i)
0
0
(st+1 )|θQ )}
(st+1 )|θQ ), Q0 (st+1 , πea
16:
Q0 (st+1 , at+1 |θQ ) ← max{Q0 (st+1 , πpg
P (i)
0
(i)
(i)
(i) (i)
17:
LQ ← N1 i (rt + Q0 (st+1 , at+1 |θQ ) − Q(st , at |θQ ))2
P
(i)
(i)
18:
Lπ ← − N1 i Q(st , πpg (st |θpg )|θQ )
19:
Update θQ , θpg according to 5θQ LQ , 5θpg Lπ
0
0
20:
θea ← (1 − τ0 )θea + τ0 θea
0
0
21:
θQ ← (1 − τ1 )θQ + τ1 θQ
0
0
22:
θpg ← (1 − τ1 )θpg + τ1 θpg

to update parameters:
0

0

θea ← (1 − τ0 )θea + τ0 θea

(6)

where τ0 should be much less than 1. The recruitment process using target recruitment
network is called soft update in recruitment, and the recruitment process without target
recruitment network is called hard update in recruitment. We will perform comparative
experiments on these two recruitment models in the experimental part.

5. Experiments
In this section, we show comparative experiments and a series of analytical results.

14

5.1. Experimental settings
We evaluated the performance of RIM on four continuous control tasks in Mujoco
[24] hosted through the OpenAI gym [25], which are widely used in the evaluation of
reinforcement learning continuous domains [26] [27] [28]. These tasks are shown in
Figure 5.

(a) Walker2d-v2

(b) Hopper-v2

(c) HalfCheetah-v2

(d) Swimmer-v2

Figure 5: Four continuous control tasks in Mujoco-based environments

Baselines for comparison include Evolutionary Reinforcement Learning (ERL) algorithm [9], DDPG, and a standard evolutionary algorithm (EA). ERL is a state of the
art algorithm in the Mujoco benchmarks; DDPG is considered as one of the best reinforcement learning algorithms and is widely used in a variety of continuous control
tasks.
The reinforcement learning parameters of RIM are consistent with those of ERL
and DDPG. The crossover and mutation probability is 0 and 0.01 respectively in population of RIM, ERL and EA. In the comparative experiments, RIM uses soft update
15

to recruit policies from population. All actor networks and critic networks have two
hidden layers, with rectified linear units (ReLU) between hidden layers. The last layer
of actor networks is linked to a tanh unit. We use Adam [29] to update all network
parameters. The learning rates of the gradient policy network and the critic network
are 10−4 and 10−3 , respectively. The number of individuals in the population is 10.
Imitation learning is performed every 10 generations. The loss function of the imitation
learning is least absolute error (L1), and the learning rate is 10−3 . The sample batch
size of reinforcement learning and imitation learning are 128 and 32, respectively.
The performance of the algorithms involved in the comparison is tested using different random seeds and is recorded every 10,000 frames. When testing, the average of
five test results is recorded as the performance at this time. For DDPG, the actor’s exploration noise was removed during the test. For EA, ERL and RIM, the performance
of the best individuals in their population are recorded as the final performance. The
scores of these algorithms are the rewards returned by environments, and the corresponding number of frames is the cumulative number of frames in which an algorithm
interacts with the environment as a whole. These recording methods are consistent
with previous comparative experiments [9].
5.2. Comparison
The results in Table 1 and Figure 6 show the final performance and learning curves
of the four algorithms on Mujoco-based continuous control benchmarks. Each algorithm is trained for 5M frames in Walker2d-v2, 2.5M frames in Hopper-v2, and 1M
frames in HalfCheetah-v2 and Swimmer-v2. Table 1 presents the maximum (Max) reward obtained by each algorithm in the whole learning process, as well as the average
(Mean), median (Median) and standard deviation (Std.) of 5 test results of each algorithm under different random seeds. The best statistical results have been bolded in
each task. Figure 6 presents the average proformance and error bars, which have been
smoothed using a window of size 10.
Table 1 shows that the average performance of RIM exceeds the previous methods
in all environments, and the maximum reward obtained in the whole learning process
and median performance exceed the previous methods in most environments. Both
16

Table 1: Final performance of EA, DDPG, ERL and RIM on 4 Mujoco-based continuous control benchmarks

EA

DDPG

ERL

RIM(ours)

Max
Mean
Median
Std.
Max
Mean
Median
Std.
Max
Mean
Median
Std.
Max
Mean
Median
Std.

Walker2d-v2

Hopper-v2

Halfcheetah-v2

Swimmer-v2

1339.11
1143.68
1073.38
11.37%
3184.08
543.94
492.85
39.77%
4472.21
1375.24
1476.77
23.51%
5532.66
2852.08
3242.61
26.66%

1051.19
1032.92
1027.82
0.08%
3575.19
484.29
590.66
67.12%
2392.49
1780.90
1824.88
30.23%
3439.74
2385.05
2386.84
19.66%

1755.27
920.60
864.62
65.49%
6012.01
2601.53
2796.43
60.66%
5597.36
5098.46
5064.25
5.54%
6151.81
5399.68
5367.22
6.94%

356.01
239.04
301.93
43.21%
55.32
26.78
29.25
27.86%
332.69
231.38
222.56
21.37%
343.62
297.43
287.49
9.78%

RIM and ERL have higher variances on different random seeds because the components, such as EA and DDPG, have high variances. However, the standard deviation of
RIM performance is much lower than ERL in Hopper-v2 and Swimmer-v2.
The experimental results in Figure 6 show that RIM can learn better than the previous algorithms in four tasks. In Walker2d-v2 and Hopper-v2 environments, the superiority of RIM is more significant, because the traditional off-policy reinforcement
learning method cannot explore efficiently in such environments. EA has a stagnation
period in the evolution process. Off-policy reinforcement learning individuals in ERL
can help the evolutionary population to break through the stagnation period, but this
often requires superior individuals in the population to generate a large amount of experiences. The recruitment mechanism of RIM enables outstanding individuals of EA
to directly guide reinforcement learning individuals, instead of guiding them by generating experience. Therefore, RIM can break through the stagnation period earlier than
ERL, which allows RIM to learn faster.
We reimplemented the EA, DDPG and ERL algorithms. The ERL results we presented in Walker2d and Hopper environments are consistent with Khadka et al. [9].

17

(a) Walker2d-v2

(b) Hopper-v2

(c) HalfCheetah-v2

(d) Swimmer-v2

Figure 6: Learning curves of EA, DDPG, ERL and RIM on 4 Mujoco-based continuous control benchmarks

The results in the Halfcheetah environment are lower than Khadka et al., but close to
those reported by Pourchot et al. [10]. In addition, we found that in the Swimmer
environment, the performance of EA, ERL and RIM is unstable. In the best case, they
can reach 330 or more, but in the worst case, they perform less than 250, even less than
150 in EA.
5.3. Component performance
Figure 7 shows performance of the three components in Walker2d and Hopper
environments: RL components of RIM, imitation learning (IL) actors in RIM, and RL
agent in ERL. The performance of RL agent in RIM is better than RL agent in ERL,
which means that the recruitment mechanism can accelerate the learning process of RL
actors. The performance of individuals trained by imitation learning in RIM improves
18

as the performance of the RL components improves, which illustrates the effectiveness
of off-policy imitation learning.

(a) Walker2d-v2

(b) Hopper-v2

Figure 7: Performance of components of different algorithms

The externally injected individuals accepted by RIM’s population are imitation
learning individuals, while the externally injected individuals accepted by the ERL’s
population are RL actors. In Figure 7, the performance of imitation learner in RIM
outperforms RL agent in ERL, suggesting that externally injected individuals of the
population of RIM are better. This can directly explain why the learning speed of RIM
is better than that of ERL.
Table 2 presents the selection rate of actors trained through imitation learning during evolution. When the population selects excellent individuals, πwt with higher perTable 2: Selection rate for πwt after training

Walker2d-v2
Hopper-v2
HalfCheetah-v2
Swimmer-v2

Selected
59.87%
30.92%
70.37%
24.44%

Discarded
40.13%
69.08%
29.63%
75.56%

formance has higher probability to be selected. On the contrary, πwt with lower performance will have a higher probability of being discarded. The RL components performs
better in Walker2d and Halfcheetah, so the selected probability of well-learned πwt is
higher. In Hopper environment, the RL components performs poorly before breaking
19

(a) HalfCheetah-v2

(b) Hopper-v2

Figure 8: Comparison curves between soft update recruitment and hard update recruitment

through the long stagnation period. In the Swimmer environment, the RL components
cannot play a critical role in the whole learning process, which can be seen from curve
of DDPG in Figure 6. These reasons cause πwt cannot imitate a good policy, so the
probability that πwt is selected is lower. On the whole, individuals performing imitation learning can be selected in each environment, indicating that the policy learned by
imitation learning can accelerate the evolution of the population.
5.4. Soft update in recruitment
We tested the performance of RIM using hard update, and compared it with the
original RIM. As shown in Figure 8, RIM using soft update recruitment performed
slightly better than RIM using hard update recruitment. Because in the process of
guiding the gradient policy network learning, the soft update recruitment policy network can provide more stable y (i) when calculating equation (4), which ensures better
consistency for learning process. However, recruitment using hard update does not
make the policy gradient network unable to learn. In fact, as the population iterates,
the changes of parameters between generations are usually small. Therefore, parameter changes of the recruitment policy network are small, which can also bring a certain
degree of consistency to the learning process.

20

Table 3: Final performance of ERL, RIM and variants of RIM on 4 Mujoco-based continuous control benchmarks

RIM

RIM-IL

RIM-EA

RIM-PG

ERL

Walker2d-v2

Hopper-v2

Halfcheetah-v2

Swimmer-v2

2852.08
3242.61
26.66%
2075.33
1937.64
26.43%
1948.16
2018.73
49.10%
1777.30
1208.66
54.97%
1375.24
1476.77
23.51%

2385.05
2386.84
19.66%
1574.41
1095.01
56.14%
1753.66
1749.26
29.14%
1141.54
1078.56
15.23%
1780.90
1824.88
30.23%

5399.68
5367.22
6.94%
5229.43
5253.77
7.5%
5001.21
4923.92
7.67%
5142.67
5197.68
13.96%
5098.46
5064.25
5.54%

297.43
287.49
9.78%
269.68
267.89
9.45%
258.96
261.63
20.63%
222.60
209.84
32.61%
231.38
222.56
21.37%

Mean
Median
Std.
Mean
Median
Std.
Mean
Median
Std.
Mean
Median
Std.
Mean
Median
Std.

5.5. Ablation experiments
In order to further prove the effectiveness of the RIM components, we performed
ablation experiments on RIM. We tested the performance of RIM without offline imitation learning (RIM-IL), RIM without using the recruitment network to estimate Q0
(RIM-EA), and RIM without using the gradient policy network to estimate Q0 (RIMPG). We compare the test performance of these three RIM variants with RIM as well
as ERL. The comparison results are presented in Table 3 and Figure 9.
According to the results in Table 3 and Figure 9, RIM outperforms RIM-IL, which
shows that imitation learning works. We believe that imitation learning can inject individuals with dual-policy RL agent’s behavior patterns into the population, thereby
accelerating the evolution of the population. Performance of RIM is better than that
of RIM-EA and RIM-PG, which shows that the dual-policy learning mode can learn
better policy. This stems from the fact that the dual-policy can better estimate the Q0
value, as revealed in Theorem 1.
To further illustrate the effectiveness of the dual-policy learning model in evolutionary reinforcement learning algorithms, we compared learning curves of RL com-

21

(a) Walker2d-v2

(b) Hopper-v2

(c) HalfCheetah-v2

(d) Swimmer-v2

Figure 9: Learning curves of ERL, RIM and variants of RIM on 4 Mujoco-based continuous control benchmarks

22

(a) Wakler2d-v2

(b) Hopper-v2

Figure 10: Learning curves of RL components in RIM, RIM-PG, and RIM-EA

ponents in RIM, RIM-EA, and RIM-PG algorithms. These curves are shown in Figure
10. RIM’s RL policy is better than RIM-EA, which indicates that RIM estimates the Q0
value better than DDPG. RIM-PG’s RL components performs poorly. This results from
the fact that the Q0 estimated by the recruitment network is not accurate in Walker2d-v2
and Hopper-v2.

6. Conclusion and Future Work
In this paper, we develop a dual-actors and single critic RL agent, which can be well
combined with evolutionary algorithms. Based on this, we propose RIM for evolutionary reinforcement learning. This method not only outperforms the previous evolutionary and off-policy reinforcement learning algorithms, but also exceeds ERL both in
overall performance and component performance. Finally, we experimentally demonstrated that the recruitment using soft update enables RL agent to learn faster than that
using hard update.
Future work can be explored from the following three aspects:
• RIM uses a standard evolutionary algorithm to iterate populations. Other more
efficient evolutionary algorithms may provide immediate improvements to RIM’s
performance, such as Natural Evolution Strategy (NES) [30], NeuroEvolution of
Augmenting Topologies (NEAT) [31].

23

• The dual-actors and single critic RL agent in RIM is extended based on DDPG,
but in fact, any off-policy actor-critic deep reinforcement learning method can
apply this structure, such as Soft Actor-Critic (SAC) [32], Twin Delayed Deep
Deterministic policy gradient algorithm (TD3) [19]. Since these methods are
superior to DDPG in most cases, applying these methods to the dual-actors and
single critic RL agent may improve RIM’s performance.
• In RIM’s experimental settings, at each time 100,000 reinforcement learning gradient updates are performed, then 30,000 imitation learning gradient updates are
required. Parallel methods can be employed to reduce time for training models.

Acknowledgement
This work was supported by the National Key R&D Program of China under Grant
No. 2017YFB1003103; the National Natural Science Foundation of China under Grant
Nos. 61300049, 61763003; and the Natural Science Research Foundation of Jilin
Province of China under Grant Nos. 20180101053JC, 20190201193JC.

References
References
[1] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver,
D. Wierstra, Continuous control with deep reinforcement learning, arXiv preprint
arXiv:1509.02971.
[2] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
K. Kavukcuoglu, Asynchronous methods for deep reinforcement learning, in: International Conference on Machine Learning, 2016, pp. 1928–1937.
[3] J. Schulman, S. Levine, P. Abbeel, M. Jordan, P. Moritz, Trust region policy
optimization, in: International Conference on Machine Learning, 2015, pp. 1889–
1897.

24

[4] T. Salimans, J. Ho, X. Chen, S. Sidor, I. Sutskever, Evolution strategies as a
scalable alternative to reinforcement learning, arXiv preprint arXiv:1703.03864.
[5] A. Giusti, J. Guzzi, D. C. Cireşan, F.-L. He, J. P. Rodrı́guez, F. Fontana,
M. Faessler, C. Forster, J. Schmidhuber, G. Di Caro, et al., A machine learning
approach to visual perception of forest trails for mobile robots, IEEE Robotics
and Automation Letters 1 (2) (2016) 661–667.
[6] F. Codevilla, M. Miiller, A. López, V. Koltun, A. Dosovitskiy, End-to-end driving
via conditional imitation learning, in: 2018 IEEE International Conference on
Robotics and Automation (ICRA), IEEE, 2018, pp. 1–9.
[7] S. Zhang, O. R. Zaiane, Comparing deep reinforcement learning and evolutionary
methods in continuous control, arXiv preprint arXiv:1712.00006.
[8] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., Human-level
control through deep reinforcement learning, Nature 518 (7540) (2015) 529–533.
[9] S. Khadka, K. Tumer, Evolution-guided policy gradient in reinforcement learning, in: Advances in Neural Information Processing Systems, 2018, pp. 1196–
1208.
[10] A. Pourchot, O. Sigaud, Cem-rl: Combining evolutionary and gradient-based
methods for policy search, arXiv preprint arXiv:1810.01222.
[11] S. Ross, J. A. Bagnell, Reinforcement and imitation learning via interactive noregret learning, arXiv preprint arXiv:1406.5979.
[12] E. Uchibe, Cooperative and competitive reinforcement and imitation learning
for a mixture of heterogeneous learning modules, Frontiers in neurorobotics 12
(2018) 61.
[13] D. V. Vargas, Evolutionary reinforcement learning: general models and adaptation., in: GECCO (Companion), 2018, pp. 1017–1038.

25

[14] M. M. Drugan, Reinforcement learning versus evolutionary computation: A survey on hybrid algorithms, Swarm and evolutionary computation 44 (2019) 228–
246.
[15] H. Tan, K. Balajee, D. Lynn, Integration of evolutionary computing and reinforcement learning for robotic imitation learning, in: 2014 IEEE International
Conference on Systems, Man, and Cybernetics (SMC), IEEE, 2014, pp. 407–412.
[16] S. Whiteson, P. Stone, Evolutionary function approximation for reinforcement
learning, Journal of Machine Learning Research 7 (May) (2006) 877–917.
[17] J. Kober, J. Peters, Imitation and reinforcement learning, IEEE Robotics & Automation Magazine 17 (2) (2010) 55–62.
[18] S. Khadka, S. Majumdar, S. Miret, E. Tumer, T. Nassar, Z. Dwiel, Y. Liu,
K. Tumer, Collaborative evolutionary reinforcement learning, in: International
Conference on Machine Learning, 2019, pp. 3341–3350.
[19] S. Fujimoto, H. van Hoof, D. Meger, Addressing function approximation error
in actor-critic methods, in: International Conference on Machine Learning, 2018,
pp. 1582–1591.
[20] S. Ross, G. J. Gordon, J. A. Bagnell, No-regret reductions for imitation learning
and structured prediction, in: AISTATS, Citeseer, 2011.
[21] A. Attia, S. Dayan, Global overview of imitation learning, arXiv preprint
arXiv:1801.06503.
[22] H. V. Hasselt, Double q-learning, in: Advances in Neural Information Processing
Systems, 2010, pp. 2613–2621.
[23] H. van Hasselt, A. Guez, D. Silver, Deep reinforcement learning with double
q-learning, in: Thirtieth AAAI conference on artificial intelligence, 2016.
[24] E. Todorov, T. Erez, Y. Tassa, Mujoco: A physics engine for model-based control,
in: 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,
IEEE, 2012, pp. 5026–5033.
26

[25] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang,
W. Zaremba, Openai gym, arXiv preprint arXiv:1606.01540.
[26] Y. Duan, X. Chen, R. Houthooft, J. Schulman, P. Abbeel, Benchmarking deep
reinforcement learning for continuous control, in: International Conference on
Machine Learning, 2016, pp. 1329–1338.
[27] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, D. Meger, Deep reinforcement learning that matters, in: Thirty-Second AAAI Conference on Artificial Intelligence, 2018, pp. 3207–3214.
[28] R. Islam, P. Henderson, M. Gomrokchi, D. Precup, Reproducibility of benchmarked deep reinforcement learning tasks for continuous control, arXiv preprint
arXiv:1708.04133.
[29] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, arXiv preprint
arXiv:1412.6980.
[30] D. Wierstra, T. Schaul, J. Peters, J. Schmidhuber, Natural evolution strategies, in:
2008 IEEE Congress on Evolutionary Computation, IEEE, 2008, pp. 3381–3387.
[31] K. O. Stanley, R. Miikkulainen, Evolving neural networks through augmenting
topologies, Evolutionary Computation 10 (2) (2002) 99–127.
[32] T. Haarnoja, A. Zhou, P. Abbeel, S. Levine, Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor, in: International Conference on Machine Learning, 2018, pp. 1856–1865.

27

