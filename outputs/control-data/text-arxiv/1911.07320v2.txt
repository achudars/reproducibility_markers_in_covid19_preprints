Sparse `1 and `2 Center Classifiers∗

arXiv:1911.07320v2 [cs.LG] 25 Nov 2019

Giuseppe C. Calafiore and Giulia Fracastoro

†

Abstract
The nearest-centroid classifier is a simple linear-time classifier based on computing the
centroids of the data classes in the training phase, and then assigning a new datum to the
class corresponding to its nearest centroid. Thanks to its very low computational cost, the
nearest-centroid classifier is still widely used in machine learning, despite the development
of many other more sophisticated classification methods. In this paper, we propose two
sparse variants of the nearest-centroid classifier, based respectively on `1 and `2 distance
criteria. The proposed sparse classifiers perform simultaneous classification and feature
selection, by detecting the features that are most relevant for the classification purpose.
We show that training of the proposed sparse models, with both distance criteria, can be
performed exactly (i.e., the globally optimal set of features is selected) and at a quasilinear computational cost. The experimental results show that the proposed methods are
competitive in accuracy with state-of-the-art feature selection techniques, while having a
significantly lower computational cost.

1

Introduction

In the last years, the technological development has led to a massive proliferation of largescale datasets. The processing of these large amounts of data poses many new challenges and
there is a strong need of algorithms that mildly scales (e.g., linearly or quasi-linearly) with
the dataset size. For this reason, classification methods with a very low computational cost,
such as Naive Bayes and the nearest centroid classifier, are an appealing choice in this endeavour. Sometimes these methods are the only feasible approaches, since more sophisticated
techniques would be too slow.
When the number of features in a datasets is very high, feature selection is a necessary step
of any machine learning algorithm. Feature selection consists in selecting a subset of features
of the dataset, choosing the most relevant ones. Besides reducing the dataset size, feature
selection has some other important advantages. First, it eliminates noisy or irrelevant features,
reducing the risk of overfitting. Second, by selecting only the most significant features, it
improves the interpretability of the model. State-of-the-art feature selection methods are
usually based on some heuristics without any guarantee of optimality. Some of them, such
as LASSO [12] or `1 -regularized logistic regression [10], are based on a convex optimization
problem with a `1 -norm penalty on the regression coefficients to promote sparsity. The main
drawback of these techniques is that they are usually computationally expensive. Other
methods, such as Odds Ratio [9], propose a different approch, performing a feature ranking
∗

This research was funded in part by sumup.ai.
Dipartimento di Elettronica e Telecomunicazioni, Politecnico di Torino, Italy. Tel.: +39-011-564.7071;
E-mail: giuseppe.calafiore@polito.it
†

1

based on their inherent characteristics. These methods are usually very fast, but often their
performance in terms of accuracy is very poor. Recently, [1] have presented a feature selection
method targeted for a Naive Bayes classifier. This method can provide an optimal solution
in the case of binary data, and an approximate upper bound for general data.
In this paper, we propose a sparse centroid classifier. The proposed method can simultaneously perform feature selection and classification. We introduce two different variants of
the method, namely `1 -sparse centroids and `2 -sparse centroids, where we consider the `1 and
the `2 distance criteria, respectively. The `2 case is a sparse variant of the nearest centroids
classifier [7], which is a widely used classifier especially in text classification. Instead, the
`1 case is related to the median classifier [6], that is more robust to noise than the nearest
centroid classifier. We prove that both the proposed method can select the optimal subset
of features for the corresponding classifier. The experimental results show that the proposed
techniques achieves similar performance as state-of-the-art feature selection methods, but
with a significantly lower computational cost.

2
Let

Preliminaries on center-based classifiers
i
h
X = x(1) · · · x(n) ∈ Rm,n ,

(1)

be a given data matrix whose columns x(j) ∈ Rm , j = 1, . . . , n, contain feature vectors from
n observations, and let y ∈ Rn be a given vector such that yj ∈ {−1, +1} is the class label
corresponding to the j-th observation. We consider a binary classification problem, in which
a new observation vector x ∈ Rm is to be assigned to the positive class C+ (corresponding to
y = +1) or to the negative class C− (corresponding to y = −1). To this purpose, the nearest
centroid classifier [7, 8, 13] is a well-known classification model, which works by assigning the
class label based on the least Euclidean distance from x to the centroids of the classes. The
centroids are computed on the basis of the training data as
1 X (j)
1 X (j)
x̄+ =
x , x̄− =
x ,
(2)
n+
n−
+
−
j∈J

j∈J

.
where J + = {j ∈ {1, . . . , n} : yj = +1} contains the indices of the observations in the
.
positive class, J − = {j ∈ {1, . . . , n} : yj = −1} contains the indices of the observations
in the negative class, and n+ , n− are the cardinalities of J + and J − , respectively. A new
observation vector x is classified as positive or negative according to the sign of
∆2 (x) = kx − x̄− k22 − kx − x̄+ k22 ,
that is, x is classified in the positive class if its Euclidean distance from the positive centroid
is smaller that its distance from the negative centroid, and viceversa for the negative class.
The discrimination surface for the centroid classifier is linear with respect to x, since
∆2 (x) = kxk22 + kx̄− k22 − 2x> x̄− − kxk22 − kx̄+ k22 + 2x> x̄+
= (kx̄− k22 − kx̄+ k22 ) + 2x> (x̄+ − x̄− ),

(3)

.
where the coefficient in the linear term of the classifier is given by vector w = x̄+ − x̄− . Notice
−
that, whenever x̄+
i = x̄i for some component i (i.e., wi = 0), the corresponding feature xi in
x is irrelevant for the purpose of classification.
2

Remark 1 We observe that the centroids in (2) can be seen as the optimal solutions to the
following optimization problem:
min

θ+ ,θ− ∈Rm

1 X
1 X
kx(j) − θ+ k22 +
kx(j) − θ− k22 .
n+
n
−
+
−
j∈J

(4)

j∈J

That is, the centroids are the points that minimize the average squared distance to the samples
within each class. A proof of this fact is immediate, by taking the gradient of the objective in
(4) with respect to θ+ and equating it to zero, and then doing the same thing for θ− . The two
problems are actually decoupled, so the two coefficients n1+ and n1− play no role here in terms
of the optimal solution. However, they have been introduced for balancing the contribution
of the residuals of the two classes.
?
We shall call (4) the (plain) `2 -center classifier training problem, and ∆2 in (3) the corresponding discrimination function. The usual centroids in (2) are thus the points that minimize
the average `2 distance from the respective class representatives. This interpretation opens
the way to considering different types of metrics for computing centers. In particular, there
exist an extensive literature on the favorable properties of the `1 norm criterion, which is
well known to provide center estimates that are robust to outliers. The natural `1 version of
problem (4) is
min

θ+ ,θ− ∈Rm

1 X
1 X
kx(j) − θ+ k1 +
kx(j) − θ− k1 ,
n+
n
−
+
−
j∈J

(5)

j∈J

which we shall call the (plain) `1 -center classifier training problem. It is known that an
optimal solution to problem (5) is obtained, for each i = 1, . . . , m, by taking θi+ to be the
(j)
(j)
median of the values xi in the positive class, and θi− to be the median of the values xi in
the negative class, see also the more general result given in Proposition 2. We let
.
µ+ = med({x(j) }j∈J + ),

.
µ− = med({x(j) }j∈J − ),

(6)

where med computes the median of its input vector sequence along each component, i.e.,
(j)
(j)
−
for each i = 1, . . . , m, µ+
i is the median of {xi }j∈J + , and µi is the median of {xi }j∈J − .
The classification in the `1 -center classifier is made by computing the distances from the new
datum x and the `1 centers of the classes, and assigning x to the closest center, that is, we
compute
.
∆1 (x) = kx − µ− k1 − kx − µ+ k1 ,
and assign x to the positive or negative class depending on the sign of ∆1 (x). We observe
that, contrary to the `2 case, the discrimination criterion based on the sign of ∆1 (x) is not
linear in x. However, expressed more explicitly in its components, ∆1 (x) is written as
∆1 (x) =

m
X


+
|xi − µ−
i | − |xi − µi | ,

i=1

and we observe again, like in the `2 case, that the contribution to ∆1 (x) from the ith feature
+
xi is identically zero when µ−
i = µi .

3

3

Sparse `1 and `2 center classifiers

In Section 2 we observed that, for both the `2 and the `1 distance criteria, the discrimination is
insensitive to the ith feature whenever θi+ −θi− = 0, where θ+ , θ− are the two class centers. The
sparse classifiers that we introduce in this section are aimed precisely at computing optimal
class centers such that the center difference θ+ − θ− is k-sparse, meaning that kθ+ − θ− k0 ≤ k,
where k · k0 denotes the number of nonzero entries (i.e., the cardinality) of its argument,
and k ≤ m is a given cardinality bound. Such type of sparse classifiers will thus perform
simultaneous classification and feature selection, by detecting which k out of the total m
features are relevant for the classification purposes. We next formally define the sparse `2
and `1 center classifier training problems.
Definition 1 (Sparse `2 -center classifier) A sparse `2 -center classifier is a model which
classifies an input feature vector x ∈ Rm into a positive or a negative class, according to the
sign of the discrimination function
∆2 (x) = kx − θ− k22 − kx − θ+ k22
= (kθ− k22 − kθ+ k22 ) + 2x> (θ+ − θ− ),

(7)

where the sparse `2 -centers θ+ , θ− are learned from a data batch (1) as the optimal solutions
of the problem
P
1 P
(j) − θ + k2 + 1
(j) − θ − k2
min
(8)
2
2
j∈J + kx
j∈J − kx
n+
n−
θ+ ,θ− ∈Rm

subject to:

kθ+ − θ− k0 ≤ k,

where k ≤ m is a given upper bound on the cardinality of θ+ − θ− .
Definition 2 (Sparse `1 -center classifier) A sparse `1 -center classifier is a model which
classifies an input feature vector x ∈ Rm into a positive or a negative class, according to the
sign of the discrimination function
.
∆1 (x) = kx − θ− k1 − kx − θ+ k1 ,

(9)

where the sparse `1 -centers θ+ , θ− are learned from a data batch (1) as the optimal solutions
of the problem
P
1 P
(j) − θ + k + 1
(j) − θ − k
min
(10)
1
1
j∈J + kx
j∈J − kx
n+
n−
θ+ ,θ− ∈Rm

subject to:

kθ+ − θ− k0 ≤ k,

where k ≤ m is a given upper bound on the cardinality of θ+ − θ− .
A perhaps notable fact is that both the sparse `2 and the sparse `1 classifier training problems can be solved exactly and with almost-linear-time complexity (this fact is proved in the
next sections), which also makes them good candidates for efficient feature selection methods
in two-phase (feature selection + actual classifier training) classifier training procedures.

4

4

Training the sparse `2 -center classifier

We next discuss how to solve the training problem in (8). Let us denote by J the objective
to be minimized in (8). By expanding the squares and using (2), we have
J

=

1 X
1 X
kx(j) k22 +
kx(j) k22 + kθ+ k22 + kθ− k22 − 2x̄+> θ+ − 2x̄−> θ−
n+
n
−
+
−
j∈J

= cost. +

j∈J

kθ+ k22

+

kθ− k22

− 2x̄+> θ+ − 2x̄−> θ− .

Let now E denote a fixed set of indices of cardinality m − k, and D denote the complementary
set, that is, D = {1, . . . , m} \ E. For any vector x ∈ Rm we next use the notation xD to denote
a vector of the same dimension as x which coincides with x at the locations in D and it is
zero elsewhere. We define analogously xE , so that x = xD + xE . We then let
+
+ θE+
θ + = θD
−
+ θE− .
θ − = θD

Suppose that we fixed the set E of the indices where θ+ − θ− is zero (we shall discuss later
how to eventually optimize over this choice of the index set), so that θE+ − θE− = 0. We can
therefore set
.
θE+ = θE− = θE ,
whence
+
+ θE
θ + = θD
−
+ θE .
θ − = θD

With such given choice of the zero index set, and using the above expressions for θ+ , θ− , the
problem objective becomes
JE

= cost. + kθ+ k22 + kθ− k22 − 2x̄+> θ+ − 2x̄−> θ−
−
+ 2
− 2
+
,
k2 + kθD
k2 − 2x̄+> θD
− 2x̄−> θD
= cost. + 2kθE k22 − 4x̃> θE + kθD

where we defined

. x̄+ + x̄−
x̃ =
.
2

(11)

+
−
For given zero index set E we can therefore minimize JE with respect to θE , θD
, and θD
. By
simply equating the respective gradients to zero, we obtain that the optimal parameter values
are
−∗
+∗
θE∗ = x̃E , θD
= x̄+
θD
= x̄−
D,
D.

5

Substituting these optimal values back into JE we obtain
− 2
2
JE∗ = cost. − 2kx̃E k22 − kx̄+
D k2 − kx̄D k2
1
− 2
+ 2
− 2
= cost. − kx̄+
E + x̄E k2 − kx̄D k2 − kx̄D k2
2
1 − 2
1
+> −
+ 2
− 2
2
= cost. − kx̄+
E k2 − kx̄E k2 − x̄E x̄E − kx̄D k2 − kx̄D k2
2
2
1
1
1
− 2
+ 2
+ 2
− 2
− 2
+> −
2
= cost. − (kx̄+
E k2 + kx̄D k2 ) − (kx̄E k2 + kx̄D k2 ) − x̄E x̄E − (kx̄D k2 + kx̄D k2 )
2
2
2
1
1
1
+
−
− 2
+> −
= cost. − kx̄+ k22 − kx̄− k22 − x̄+>
E x̄E − (kx̄D − x̄D k2 + 2x̄D x̄D )
2
2
2
1
1
1 +
−
− 2
+> −
= cost. − kx̄+ k22 − kx̄− k22 − (x̄+>
E x̄E + x̄D x̄D ) − kx̄D − x̄D k2
2
2
2
1
1
1
2
− x̄−
= cost. − kx̄+ k22 − kx̄− k22 − x̄+> x̄− − kx̄+
D k2
2
2
2 D
1
1
2
= cost. − kx̄+ + x̄− k22 − kx̄+
− x̄−
D k2 .
2
2 D

This last expression shows that JE∗ depends on the choice of the zero index set E only via
− 2
∗
the term kx̄+
D − x̄D k2 involving the complementary set D. Minimizing JE with respect to the
− 2
+
index set E thus amounts to maximizing kx̄D − x̄D k2 with respect to the complementary index
set D, that is
1
2
J ∗ = cost.0 − max kx̄+
− x̄−
D k2 .
2 |D|≤k D
.
The solution to this problem is immediate: we construct the difference vector δ = x̄+ − x̄−
and let D∗ contain the indices of the k largest elements of |δ|. We have therefore proved the
following
Proposition 1 The optimal solution of problem (8) is obtained as follows:
1. Compute the standard class centroids x̄+ , x̄− according to (2);
.
2. Compute the centroids midpoint x̃ according to (11), and the centroids difference δ =
x̄+ − x̄− ;
3. Let D be the set of the indices of the k largest absolute value elements in vector δ, and
let E be the complementary index set;
4. The optimal parameters θ+ , θ− are given by
θ+ = x̄+
D + x̃E
θ− = x̄−
D + x̃E .
Remark 2 (Numerical complexity for training the sparse `2 classifier) Steps 1-2 in
Proposition 1 essentially require computing mn sums. Finding the k largest elements in Step 3
takes O(m log k) operations (using, e.g., min-heap sorting), whence the whole procedure takes
O(mn) + O(m log k) operations. Thus, while training a plain centroid classifier takes O(mn)
operations (which, incidentally, is also the complexity figure for training a classical Naive
Bayes classifier), adding exact sparsity comes at the quite moderate extra cost of O(m log k)
operations.
?
6

Remark 3 (Online recursive training) The sparse `2 center classifier training procedure
is amenable to efficient online implementation, since the class centers are easily updatable as
soon as new data comes in. Denote by x̄(ν) the centroid ofP
one of the two classes when ν
1
(1)
(ν)
observations ξ , . . . , ξ in that class are present: x̄(ν) = ν νj=1 ξ (j) . If a new observation
ξ (ν+1) in the same class becomes available, the new centroid will be


ν+1
ν
X
X
1
1 
x̄(ν + 1) =
ξ (j) =
ξ (j) + ξ (ν+1) 
ν+1
ν+1
j=1

=

j=1

ν
1 (ν+1)
x̄(ν) +
ξ
.
ν+1
ν+1

This latter formula gives the new centroid as a weighted linear combination of the previous
centroid and of the new observation. An online version of the procedure in Proposition 1
is thus readily obtained, in which only the current centroids are kept into memory and, as
soon as a new datum is available, the corresponding centroid is updated (this takes O(m)
operations, or less if the datum is sparse) and the feature ranking is recomputed (this takes
O(m log k) operations). A sparse `2 center classifier can therefore be trained online with O(m)
memory storage and O(m log k) operations per update.
?
Remark 4 (Sparsity-accuracy tradeoff ) As it is customary with sparse methods, in practice a whole sequence of training problems is solved at different levels of sparsity, say from
k = 1 (only one feature selected) to k = m (all features selected), accuracy is evaluated for
each model via cross validation, and then the resulting sparsity-accuracy tradeoff curve is
examined for the purpose of selection of the most suitable k level. Most feature selection
methods, including sparse SVM, the Lasso [12], and the sparse Naive Bayes method [1], require repeatedly solving the training problem for each k, albeit typically warm-starting the
optimization procedure with the solution from the previous k value. In the sparse `2 classifier, instead, one can fully order the vector |x̄+ − x̄− | only once, at a computational cost of
O(m log m), and then the optimal solutions are obtained, for any k, by simply selecting in
Step 3 of Proposition 1 the first k elements of the ordered vector.
?

4.1

Mahalanobis distance classifier

A variant of the `2 centroid classifier is obtained by considering the Mahalanobis distance
instead of the Euclidean distance. Letting S denote an estimated data covariance matrix, the
Mahalanobis distance from a point z to a center θ± is defined by
distS (z, θ± ) = (z − θ± )> S −1 (z − θ± ).
This leads to the Mahalanobis training problem
1 P
(j) − θ + )> S −1 (x(j) − θ + ) +
min
j∈J+ (x
n+
θ+ ,θ− ∈Rm

1
n−

(j)
j∈J− (x

P

− θ− )> S −1 (x(j) − θ− )

Classification of a new observation x in this setting is performed according to the sign of
∆M (x) = (x − θ− )> S −1 (x − θ− ) − (x − θ+ )> S −1 (x − θ+ )
= (θ− S −1 θ− − θ+ S −1 θ+ ) + 2(θ+ − θ− )> S −1 x.
7

(12)

By introducing a change of variables of the type
.
ξ (j) = S −1/2 x(j) , j = 1, . . . , n;

.
ω ± = S −1/2 θ± ,

where S −1/2 is the matrix square root of S −1 , we see that the Mahalanobis training problem,
in the new variables, becomes
P
1 P
(j) − ω + k2 + 1
(j) − ω − k2
min
(13)
2
2
j∈J+ kξ
j∈J− kξ
n+
n−
ω + ,ω − ∈Rm

and the discrimination function, for ξ = S −1/2 x, becomes
∆M (ξ) = (kω − k22 − kω + k22 ) + 2(ω + − ω − )> ξ.
Problem (13) is now a standard `2 center classifier problem, hence its sparse version can be
readily solved by means of the algorithm outlined in Proposition 1. It should however be
observed that in this case one obtains sparsity in the transformed center difference ω + − ω − ,
which implies a selection of the transformed features in ξ = S −1/2 x. One relevant special
2 ), in which case the data transformation ξ = S −1/2 x
case arises when S = diag(σ12 , . . . , σm
simply amounts to normalizing each feature xi by its standard deviation σi , that is ξi = xi /σi ,
i = 1, . . . , m.

5

Training the sparse `1 -center classifier

We next present an efficient and exact method for training a sparse `1 -center classifier. We
start by stating a preliminary instrumental result, whose proof is reported in the appendix
Section 8.1, and an ensuing definition.
Proposition 2 (Weighted `1 center) Given a real vector z = (z1 , z2 , . . . , zp ) and a nonnegative vector w = (w1 , . . . , wp ), consider the weighted `1 centering problem
p

X
.
dw (z) = min
wi |zi − ϑ|.
ϑ∈R

Let

p

.
W (ζ) =

X

wi ,

. X
W̄ =
wi ,
i=1

{i: zi ≤ζ}

and

(14)

i=1

.
ζ̄ = inf{ζ : W (ζ) ≥ W̄ /2}.

(15)

Then, an optimal solution for problem (14) is given by
.
ϑ = medw (z) =
∗





ζ̄

if W (ζ̄) >

W̄
2

(16)
1
2 (ζ̄

+ ζ̄+ ) if W (ζ̄) =

W̄
2 ,

.
where ζ̄+ = min{zi , i = 1, . . . , p : zi > ζ̄} is the smallest element in z that is strictly larger
than ζ̄.
?
8

Definition 3 (Weighted median and dispersion) Given a row vector z and a nonnegative vector w of the same size, we define as the weighted median of z the optimal solution of
problem (14) given in (16), and we denote it by medw (z). We define as the weighted median
dispersion the optimal value dw (z) of problem (14). We extend this notation to matrices, so
that for a matrix X ∈ Rm,n we denote by medw (X) ∈ Rm a vector whose ith component
is medw (Xi,: ), where Xi,: is the ith row of X, and we denote by dw (X) ∈ Rm the vector of
corresponding dispersions.
?
We now let E and D be defined as in Section 4, and we use the same notation as before
±
for θD
, θE± , xD , xE . Let then J denote the objective to be minimized in (10). For fixed index
set D, we have that J = JD , where
JD =

1 X
1 X
+
−
kx(j) − θD
kx(j) − θD
− θE k 1 +
− θE k1
n+
n
−
+
−
j∈J

=

j∈J

1 X
1 X
(j)
(j)
(j)
(j)
+
−
k(xD − θD
) + (xE − θE )k1 +
k(xD − θD
) + (xE − θE )k1
n+
n−
+
−
j∈J

=

j∈J

1 X
1 X
1 X
1 X
(j)
(j)
(j)
(j)
+
−
k1 +
k1 +
kxD − θD
kxE − θE k1 +
kxD − θD
kxE − θE k1
n+
n
n
n
+
−
−
+
+
−
−
j∈J

=

j∈J

j∈J

1 X
1 X
(j)
(j)
+
−
k1 +
k1 +
kxD − θD
kxD − θD
n+
n
−
+
−
j∈J

j∈J

where

(
wj =

1
n+
1
n−

if j ∈ J +
,
if j ∈ J −

n
X

j∈J

(j)

wj kxE − θE k1 ,

j=1

j = 1, . . . , n.

+
−
We will next find the minimum
Pm of JD with respect to θD , θD and θE . To this end, we observe
that JD decouples as JD = i=1 JD,i , where for i = 1, . . . , m,

1 X (j)
1 X (j)

+

|x
−
θ
|
+
|xi − θi− |, if i ∈ D

i
i

n
n
 +
−
j∈J +
j∈J −
.
JD,i =
(17)
n
X

(j)


w
|x
−
θ
|,
if
i
∈
6
D.
j i
i


j=1

The minimum of JD is hence obtained by minimizing separately each component JD,i . For
(j)
i ∈ D, we have that the optimal θi+ , θi− are given by the (plain) medians of the xi values in
the positive and in the negative class, respectively, that is, recalling (6),
i∈D

⇒

.
(j)
θi+∗ = µ+
i = med({xi }j∈J + )
.
(j)
θi−∗ = µ−
i = med({xi }j∈J − )

⇒

−
∗
JD,i
= d+
i + di ,

where d+ , d− are the vectors of median dispersions in the positive and negative class, respectively, whose components are, for i = 1, . . . , m,
.
d+
=
i
.
d−
=
i

(j)
+
1 P
j∈J + |xi − µi |
n+
P
(j)
−
1
j∈J − |xi − µi |.
n−

9

(18)

For i 6∈ D, instead, by observing that the entries of w in (17) are nonnegative, and applying Proposition 2, we obtain that the optimal solution is the weighted median of all the
observations, that is
i 6∈ D

⇒

.
(j)
θi∗ = µi = medw ({xi }j=1,...,n )

⇒

∗
JD,i
= di ,

where d is the vector of weighted median dispersions over all the observations, whose components are, for i = 1, . . . , m,
n
1 X (j)
1 X (j)
. X
(j)
di =
wj |xi − µi | =
|xi − µi | +
|xi − µi |.
n+
n−
+
−
j=1

j∈J

(19)

j∈J

We are now in position to discuss how to optimize over the choice of the set D, that is how
−
to decide which are the k indices that should belong to D. First observe that (d+
i + di ) ≤ di ,
for all i = 1, . . . , m, since di is the optimal value of a minimization that constrains θi+ to be
−
equal to θi− , whereas d+
i + di is the optimal value of the same minimization without such
constraint, and therefore its optimal objective value is no larger than di . Consider then the
vector of differences
.
e = (d+ + d− ) − d ≤ 0.
The smallest (i.e., most negative) entry in e corresponds to an index i for which it is maximally
convenient (in terms of objective J decrease) choosing i ∈ D rather than i 6∈ D; the second
smallest entry in e corresponds to the second best choice, and so on. The best k indices to
be included in D are therefore those corresponding to the k smallest entries of vector e. We
have therefore proved the following
Proposition 3 The optimal solution of problem (10) is obtained as follows:
1. Compute the plain class medians
.
µ+ = med({x(j) }j∈J + )
.
µ− = med({x(j) }j∈J − )
and the weighted median of all observations
.
(j)
µ = medw ({xi }j=1,...,n ),
where the weight vector w is such that, for j = 1, . . . , n, wj = 1/n+ if j ∈ J + , and
wj = 1/n− if j ∈ J − .
2. Compute the median dispersion vectors d+ , d− according to (18), and the weighted
median dispersion vector d according to (19), and compute the difference vector
.
e = (d+ + d− ) − d.
3. Let D be the set of the indices of the k smallest elements in vector e, and let E be the
complementary index set.
4. The optimal parameters θ+ , θ− are given by
θ+ = µ+
D + µE
θ− = µ−
D + µE .
10

Table 1: Text dataset sizes
TWTR MPQA
Number of features 273779
6208
Number of samples 1600000 10606

SST
16599
79654

Remark 5 (Numerical complexity for training the sparse `1 classifier) Computation
of the medians in Step 1 of Proposition 3 can be performed with in O(m) operations, see,
e.g., [2]. Computation of the median dispersions requires O(mn) operations, and finding the
k smallest elements in vector e can be performed in O(m log k) operations, hence the whole
procedure in Proposition 3 is performed in O(mn) + O(m log k) operations. Similar to the
case discussed in Remark 4, also in the sparse `1 center classifier one need to do a full ordering
of an m-vector only once in order to obtain all the sparse classifiers for any sparsity level k. ?

6

Experiments

In this section, we perform an experimental evaluation of the proposed methods, comparing
their performance with other feature selection techniques. The sparse `2 -center classifier is
tested in the context of sentiment classification on text datasets. This is one of the most
common application fields of the nearest centroid classifier. Instead, the sparse `1 -center
classifier is evaluated on gene expression datasets. Since this type of data is usually affected
by the presence of many outliers, the classifier with the `1 distance criteria can be preferred
over the `2 version [6].

6.1

Sparse `2 -center classifier

We compared the proposed sparse `2 -center classifier with other feature selection methods for
sentiment classification on text datasets. We considered three different datasets: the TwitterSentiment140 (TWTR) dataset, the MPQA Opinion Corpus Dataset, and the Stanford
Sentiment Treebank (SST). Table 1 gives some details on the dataset sizes. Before classification, the dataset are preprocessed rescaling each feature by the inverse of its variance. Each
dataset was randomly split in a training (80% of the dataset) and test (20% of the dataset)
set. The results reported in this section are an average of 50 different random splits of the
dataset.
For each dataset, we performed a two-stage classification procedure. In the first stage,
we applied a feature selection method in order to reduce the number of features. Then,
in the second stage we trained a classifier model, by employing only the selected features.
In order to have a fair comparison, we used the same classifier for all the feature selection
methods, namely a linear support vector machine classifier. We compared different feature
selection methods: sparse `2 -centers (`2 -SC), sparse multinomial naive Bayes (SMNB), logistic
regression with recursive feature selection (Logistic-RFE), `1 -regularized logistic regression
(Logistic-`1 ), Lasso, and Odds Ratio. Logistic-RFE, Logistic-`1 and Lasso are not considered
on some datasets, due to their high computational cost that makes them not viable when
the dataset size is very large. Fig. 1 shows the accuracy performance and the average run
time of the different feature selection methods. These plots show that the sparse `2 -centers
is competitive with other feature selection methods in terms of accuracy performance, while
its run time is significantly lower than most of the other feature selection methods. The only
11

Table 2: RNA gene expression dataset sizes
Chowdary
Chin
Singh
(Breast Cancer) (Breast Cancer) (Prostate Cancer)
N. features
22283
22215
12600
N. samples
104
118
102
method that has a comparable computational time is Odds Ratio, but its performance is poor
in terms of accuracy.

6.2

Sparse `1 -center classifiers

We compared the proposed sparse `1 -center classifier with other feature selection methods for
RNA gene expression classification. We considered three datasets: Chin dataset [3], Chowdary
dataset [4], and Singh dataset [11]. The details of the datasets are summarized in Table 2.
As done in the `2 case, we subdivided each dataset in a training (80% of the dataset) and
test (20% of the dataset) set, and we tested 50 random splits.
For each dataset, we performed a two-stage procedure, as explained in the previous section. In the first stage, we compared five feature selection methods: sparse `1 -centers (`1 -SC),
`1 -regularized logistic regression (Logistic-`1 ), logistic regression with recursive feature elimination (Logistic-RFE), Lasso, and Odds Ratio. Sparse Multinomial Naive Bayes (SMNB) is
not taken into account in this experiment since the gene expression datasets can have negative features and SMNB can only be applied to datasets with positive features. In the second
stage, we used a linear SVM classifier, as in the previous section. Figure 2 shows the balanced
accuracy and average run time of the feature selection methods. Also in this experiment we
observe that the proposed method provides an accuracy performance which is similar to that
of state-of-the-art techniques, but with a significantly lower computational time.

7

Conclusion

In this paper we proposed two types of sparse center classifiers, based respectively on `1
and the `2 distance metrics. The proposed methods perform simultaneous classification and
feature selection, and in both cases the proposed training method selects the optimal set of
features in a quasi-linear computing time. The experimental results also show that the proposed methods achieve accuracy levels that are on par with state-of-the-art feature selection
methods, while being substantially faster.

8
8.1

Appendix
Proof of Proposition 2

Pp
.
Let w̃ = w/W̄ . Since w̃ ≥ 0 and
i=1 w̃i = 1, it can be interpreted as the probability
distribution of a discrete random variable Z with support in z1 , . . . , zp , and corresponding
probability mass w̃1 , . . . , w̃p . Note that values in vector z may be repeated, in which case
the probability mass relative to a repeated support point is the sum of the corresponding
probability values in vector w̃. With such stochastic interpretation, the objective in (14) can

12

TWTR

TWTR

0.75

10 0

0.7

Run time (s)

Accuracy

0.65

0.6

0.55

0.5

0.45
0.01

0.02

0.84

0.05
0.1
Feature cardinality (%)
MPQA

1.0

0.01

0.02
0.05
0.1
Feature cardinality (%)
MPQA

0.1

0.5
1.0
5.0
Feature cardinality (%)
SST

0.02

0.05
0.1
0.5
Feature cardinality (%)

10 2

1.0

0.82

10 1

0.8

Run time (s)

Accuracy

0.78
0.76
0.74

10 0

10 -1

0.72

10 -2

0.7
0.68

10 -3

0.66
0.1

0.72

0.5

1.0
5.0
10.0
Feature cardinality (%)
SST

10 2

10.0

0.7

10 1

0.68

Run time (s)

Accuracy

0.66
0.64
0.62

10 0

10 -1

0.6

10 -2

0.58
0.56

10 -3

0.54
0.02

0.05
0.1
0.5
Feature cardinality (%)

1.0

1.0

Figure 1: Classification accuracy and average run time.

13

Chowdary
1

Chowdary

10 2

0.95
0.9
0.85

10 0

Run time (s)

Accuracy

0.8
0.75
0.7

10 -2

0.65

10 -4

0.6
0.55

10 -6
0.05

0.1
0.5
1.0
Feature cardinality (%)
Chin

2.0

0.05

0.1
0.5
1.0
Feature cardinality (%)
Chin

2.0

0.05

0.1
0.5
1.0
Feature cardinality (%)
Singh

2.0

0.05

0.1
0.5
1.0
Feature cardinality (%)

2.0

10 2

0.9
0.85
0.8

10 0

Run time (s)

Accuracy

0.75
0.7
0.65
0.6

10 -2

10 -4

0.55

0.5

10 -6
0.05

0.1
0.5
1.0
Feature cardinality (%)
Singh

2.0

10 2

0.95
0.9
0.85

10 0

0.8

Run time (s)

Accuracy

0.75
0.7
0.65
0.6

10 -2

10 -4

0.55
0.5

10 -6
0.05

0.1
0.5
1.0
Feature cardinality (%)

2.0

Figure 2: Classification accuracy and average run time.

14

be written in terms of the expectation E{|Z − ϑ|}, and then the problem becomes
dw (z) = W̄ min E{|Z − ϑ|}.

(20)

ϑ∈R

When Z has an absolutely continuous distribution, it is well known (see, e.g., [5]) that the
value ϑ∗ that minimizes the absolute expected loss is the median of the probability distribution
of Z, that is, the 0.5 quantile of the distribution. In the case of a discrete probability
distribution, the definition of median is any value µ such that
1
Prob{Z ≤ µ} ≥ ,
2

and

1
Prob{Z ≥ µ} ≥ .
2

(21)

Now, suppose that µ is a median for our discrete random variable Z, and consider any given
ϑ > µ. If Z ≤ µ, then |Z − µ| = µ − Z and since µ < ϑ we also have Z < ϑ whence
|Z − ϑ| = ϑ − Z, and therefore
|Z − ϑ| − |Z − µ| = (ϑ − Z) − (µ − Z) = ϑ − µ,

for Z ≤ µ.

If instead Z > µ, then
|Z − ϑ| − |Z − µ| = |Z − ϑ| − (Z − µ) = |Z − µ + µ − ϑ| − (Z − µ)
≥ |Z − µ| − |µ − ϑ| − (Z − µ) = (Z − µ) − (ϑ − µ) − (Z − µ)
= −(ϑ − µ),

for Z > µ.

Therefore, for any given ϑ > µ, we have that
E{|Z − ϑ| − |Z − µ|} ≥ (ϑ − µ)Prob{Z ≤ µ} − (ϑ − µ)Prob{Z > µ}
= (ϑ − µ) (Prob{Z ≤ µ} − Prob{Z > µ})
= (ϑ − µ) (2Prob{Z ≤ µ} − 1})
≥ 0,

for all ϑ > µ.

where the last inequality follows from the fact that µ is a distribution median and hence from
the definition in (21) it holds that Prob{Z ≤ µ} ≥ 1/2. The whole reasoning can be repeated
symmetrically for any given ϑ < µ, obtaining
|Z − ϑ| − |Z − µ| ≥ −(µ − ϑ),
|Z − ϑ| − |Z − µ| = (µ − ϑ),

for Z < µ,
for Z ≥ µ.

Then again
E{|Z − ϑ| − |Z − µ|} ≥ −(µ − ϑ)Prob{Z < µ} + (µ − ϑ)Prob{Z ≥ µ}
= (µ − ϑ) (Prob{Z ≥ µ} − Prob{Z < µ})
= (µ − ϑ) (2Prob{Z ≥ µ} − 1})
≥ 0,

for all ϑ < µ,

where the last inequality follows from the fact that µ is a distribution median and hence from
the definition in (21) it holds that Prob{Z ≥ µ} ≥ 1/2. Putting things together, we have
that
E{|Z − ϑ|} − E{|Z − µ|} = E{|Z − ϑ| − |Z − µ|} ≥ 0,
15

∀ϑ,

which implies that the minimum of E{|Z − µ|} is attained at ϑ = µ, where µ is a median of
the distribution.
We next conclude the proof by showing thatPϑ∗ in (16) is indeed a median, in the sense
.
of definition (21). Observe first that W (ζ) = i:zi ≤ζ wi is proportional to the cumulative
distribution function of Z, that is
W (ζ) = W̄ W̃ (ζ),

.
W̃ (ζ) = Prob{Z ≤ ζ},

and that (15) implies that W̃ (ζ̄) ≥ 1/2, and W̃ (ζ) < 1/2 for all ζ < ζ̄. Also, since by definition
of ζ̄+ no probability mass is present in the interior of the interval [ζ̄, ζ̄+ ], we have from (16)
that W̃ (ϑ∗ ) ≡ W̃ (ζ̄). Then, from (16) it follows immediately that Prob{Z ≤ ϑ∗ } = W̃ (ϑ∗ ) ≡
W̃ (ζ̄) ≥ 1/2, which shows that ϑ∗ satisfies the condition on the left in (21). We next analyze
the condition on the right in (21), which concerns verifying that Prob{Z ≥ ϑ∗ } ≥ 1/2. To
this purpose, we distinguish two cases: case (a), where W̃ (ϑ∗ ) > 1/2, and case (b), where
W̃ (ϑ∗ ) = 1/2. In case (a), we have ϑ∗ ≡ ζ̄ and hence, as discussed above, W̃ (ζ) < 1/2 for all
ζ < ϑ∗ , which implies that Prob{Z < ϑ∗ } < 1/2 (while Prob{Z ≤ ϑ∗ } ≥ 1/2, since there is a
positive probability mass at ϑ∗ ), and therefore
Prob{Z ≥ ϑ∗ } = 1 − Prob{Z < ϑ∗ } > 1/2.
In case (b), we have instead
Prob{Z ≥ ϑ∗ } = Prob{Z = ϑ∗ } + Prob{Z > ϑ∗ }
= Prob{Z = ϑ∗ } + 1 − Prob{Z ≤ ϑ∗ } = Prob{Z = ϑ∗ } + 1/2
= 1/2,
where the last equality follows from the fact that in case (b) we have Prob{Z = ϑ∗ } = 0,
since ϑ∗ is the mid point of the interval [ζ̄, ζ̄+ ], in the interior of which there is no probability
mass, by construction.


References
[1] A. Askari, A. d’Aspremont, and L. E. Ghaoui. Naive feature selection: Sparsity in naive
bayes. arXiv preprint arXiv:1905.09884, 2019.
[2] M. Blum, R. W. Floyd, V. R. Pratt, R. L. Rivest, and R. E. Tarjan. Time bounds for
selection. J. Comput. Syst. Sci., 7(4):448–461, 1973.
[3] K. Chin, S. DeVries, J. Fridlyand, P. T. Spellman, R. Roydasgupta, W.-L. Kuo, A. Lapuk,
R. M. Neve, Z. Qian, T. Ryder, et al. Genomic and transcriptional aberrations linked to
breast cancer pathophysiologies. Cancer cell, 10(6):529–541, 2006.
[4] D. Chowdary, J. Lathrop, J. Skelton, K. Curtin, T. Briggs, Y. Zhang, J. Yu, Y. Wang, and
A. Mazumder. Prognostic gene expression signatures can be measured in tissues collected
in rnalater preservative. The journal of molecular diagnostics, 8(1):31–39, 2006.
[5] J. Haldane. Note on the median of a multivariate distribution. Biometrika, 35(3-4):414–
417, 1948.

16

[6] P. Hall, D. Titterington, and J.-H. Xue. Median-based classifiers for high-dimensional
data. Journal of the American Statistical Association, 104(488):1597–1608, 2009.
[7] E.-H. S. Han and G. Karypis. Centroid-based document classification: Analysis and
experimental results. In European conference on principles of data mining and knowledge
discovery, pages 424–431. Springer, 2000.
[8] C. Manning, P. Raghavan, and H. Schütze. Vector space classification. Introduction to
Information Retrieval, 2008.
[9] D. Mladenic and M. Grobelnik. Feature selection for unbalanced class distribution and
naive bayes. In ICML, volume 99, pages 258–267, 1999.
[10] A. Y. Ng. Feature selection, l 1 vs. l 2 regularization, and rotational invariance. In
Proceedings of the twenty-first international conference on Machine learning, page 78.
ACM, 2004.
[11] D. Singh, P. G. Febbo, K. Ross, D. G. Jackson, J. Manola, C. Ladd, P. Tamayo, A. A.
Renshaw, A. V. D’Amico, J. P. Richie, et al. Gene expression correlates of clinical
prostate cancer behavior. Cancer cell, 1(2):203–209, 2002.
[12] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society: Series B (Methodological), 58(1):267–288, 1996.
[13] R. Tibshirani, T. Hastie, B. Narasimhan, and G. Chu. Diagnosis of multiple cancer
types by shrunken centroids of gene expression. Proceedings of the National Academy of
Sciences, 99(10):6567–6572, 2002.

17

