Smooth Operator - The Use of Smooth Integers
in Fast Generation of RSA Keys

arXiv:1912.11546v1 [cs.CR] 24 Dec 2019

Vassil Dimitrov∗1 , Luigi Vigneri†1 , and Vidal Attias‡1
1

IOTA Foundation

Abstract
Primality generation is the cornerstone of several essential cryptographic system, most notably, the RSA cryptosystem. The problem has
been a subject of deep investigations by the computational number theorists, but there is still room for improvement. Typically, the algorithms
used have two parts – trial divisions aimed at eliminating numbers with
small prime factors and primality tests based on an easy-to-compute statement that is valid for primes and invalid for composites. In this paper
we will showcase a technique that will eliminate the first phase of the
primality testing algorithms. It is particularly suitable for a decentralized
RSA key generation. The computational simulations show reduction of
the primality generation time for about 30% in the case of 1024-bit RSA
private keys. We are also proposing one new one-way function that can
be used either as a hash function or as cryptographic puzzle for mining
purposes.

1

Introduction

Additive number theory is a fascinating area of mathematics. In it one can
find problems with extreme difficulties that can be posed in a really simple way.
Goldbach conjecture [13] or twin-primes conjectures are, perhaps, the simplest
examples. One of the most challenging problems in modern number theory is
the so-called abc-conjecture [3]. In a most intuitive way, it can be posed like
this: the equation a + b = c (gcd(a, b, c) = 1) does not have large solutions in
numbers with only small prime factors. Numbers without large prime factors
are usually called smooth numbers. They have very substantial use in various
cryptographic primitives – both as a tool to speed up specific operations (e.g.,
point multiplications over various types of elliptic curves [9]) and also as a
cryptanalytic tool (in implementation of several algorithms for factoring and
discrete logarithm problem).[1, 2, 4, 5, 6, 7, 9, 8, 11, 12]
In this research work we will outline some properties of the smooth numbers,
showcase how they can be used to produce an extremely simple function that we
suspect to be a one-way function. They also can be used to improve the speed
∗ vassil@iota.org
† luigi.vigneri@iota.org
‡ vidal.attias@iota.org

1

of various primality generation algorithms. Whilst the later application is more
concrete, we hope that the other specific properties discussed in the paper will
open a large room for discussion and other potential uses of the smooth numbers
in cryptographic applications.

2

Smooth numbers

2.1

Definition

An integer number x is called an s-integer if its largest prime factor is smaller
than or equal to the s-th prime. Before going into more general consideration,
we will use as an example the use of 2-integers. Numbers with only small prime
factors are often called smooth integers. Examples:
1. The perfect powers of 2 are 1-integers (their largest prime factor is 2).
2. The numbers from the sequence:
1, 2, 3, 4, 6, 8, 9, 12, 16, 18, 24, 27, 32, 36, 48, 54, 64, 72, 81, 96, . . .,

(1)

are 2-integers since their largest prime factors is at most 3 (the second
prime).
Representation of an integer as the sum of different 1-integer is simply their
binary representation. The representation is unique.
If one wants to represent an integer number as the sum of 2-integers (that is,
elements from the set of Eq. (1)), then there are many possible representations.
For example, 100 has exactly 402 different representations as the sum of 2integers. The shortest ones are 100 = 96 + 4 = 64 + 36.
It is surprising that the number of different representations of a given integer
as the sum of numbers of the form 2a · 3b (that is, 2-integers) can be predicted
with extreme accuracy. For example, 40000 has exactly 2,611,771,518,060,603
different representations. The reason why it is possible to prove the following
formula for p(n), where p(n) defines the number of different representations of
n as the sum of 2-integers:

p(n − 1)
if n ≡ 1(mod3) or n ≡ 2(mod3)
p(n) =
p(n − 1) + p( n3 ) if n ≡ 0(mod3)
This particular recursive equation has been firstly investigated by [15] and
an extremely accurate approximation of p(n) has been obtained by [16]. A less
accurate approximation of log(p(n)) is given by:
log(p(n)) ≈

log2 n
2 log(3)

The most interesting representations are the sparsest ones. (In the previous
example we pointed out that for number 100, the sparsest representations are
96 + 4 and 64 + 36, requiring only 2 terms). The following theorem provides a
very good information about the sparsity of the multiple-base representations:
Theorem 1. Let n be a positive integer. Then it can be represented as the sum
of a most O( logloglogn n ) 2-integers.
2

Some facts about the sparsity of the representation of integers as the sum of
2-integers:
• 23 is the smallest integer that cannot be represented as the sum of two
2-integers;
• 431 is the smallest integer that cannot be represented as the sum of three
2-integers;
• 18,431 is the smallest integer that cannot be represented as the sum of
four 2-integers;
• 3,448,733 is the smallest integer that cannot be represented as the sum of
five 2-integers
• 1,441,896,119 is the smallest integer that cannot be represented as the
sum of six 2-integers.
In other words, every 30-bits integer can be represented as the sum of at
most six numbers of the form 2a 3b , (a,b – non-negative integers).

2.2

Density of smooth integers

The number of 2-integers less than x is approximately:
ln2 x
2 ln 2 ln 3

(2)

The number of 3-integers less than x is approximately:
ln3 x
6 ln 2 ln 3 ln 5

(3)

The number of 4-integers less than x is approximately:
ln4 x
24 ln 2 ln 3 ln 5 ln 7

(4)

By extending the size of s, one reduces the number of terms, necessary to
represent integers as the sum of s-integers.
Many of the main theoretical properties of the smooth integers stem from
the transcendental number theory and the theory of linear forms of logarithms.
We will present some of the most essential ones here:
Theorem 2 [17]. Let x and y be two consecutive s-integers (x > y). Then their
differences is bounded from above and below like:
x
x
<x−y <
logc1 x
logc2 x
where c1 and c2 are effectively computable constants.
Many results can be proved from this theorem. For example, one can prove
that the multiplication by an n-bit constant can be achieved by using a sublinear
number of additions only (e.g. logn n ).
The following theorem provides an information about the representation of
integers as the sum of s-integers.
3

Theorem 3 [11, 14]. Every integer n can be represented as the sum of at most
n
O( loglog
log n) ) s-integers.
Sketch of proof. Consider the case s = 2 (that is, representing n as numbers of
the form 2a · 3b ). The number of 2-integers in the interval [2k−1 , 2k ] is approximately 0.63 · k. Tijdeman’s theorem guarantees that they cannot be concentrated in a cluster and, therefore, if one applied the greedy algorithm to find a
suitable representation (greedy algorithms in this case do not guarantee minimality, but the representations obtained by them is nevertheless asymptotically
optimal), then after subtracting n minus the closest 2-integers one will get a
number with O(log log n) bits less in its binary representation. Repeating the
same procedure, one gets the bound in the above theorem.
The theorem however does not tell us anything about the constant, associated with the big-O notation. Experimental and probabilistic evidence suggests
that it is probably equal to 2s . Meaning that if s is very large, then one might anticipate representation of integers as the sum of very few smooth integers. When
does ‘very few’ is expected to become two? That is: under what condition an
integer n can be written as the sum of two smooth integers?
Conjecture 1. Let n be an integer. Then there exist a pair of integers a and
b such that their largest prime factor is O((log n)2+ǫ ).
We call this conjecture anti-Goldbach. The original Goldbach conjecture states that every odd integer can be represented as the sum of three
primes (proved for every sufficiently large odd number in 1937 by Vinogradov
and unconditionally by [13]) and every even integer is the sum of two primes
(still unproven). Prime numbers have the largest possible prime factors (themselves), whereas smooth numbers have only small prime factors, thus, the name
of the conjecture.

3

The simplest one-way function

One-way functions are the cornerstone of the public-key cryptography. For
example, the RSA algorithm is based on the conjectured difficulty of factoring
problems. The full definition of these one-way function should take into account
the complexity of producing two big primes and not only the complexity of
their multiplication. Modern computational number theory offers a variety of
algorithms for generating big primes, but any of these algorithms requires a
large number of multi-word divisions and they are time consuming.
The situation is drastically different, if we work with smooth integers.
Whilst it is reallly easy to produce two large smooth numbers (much easier
than, say, to generate two big primes), it appears to be very difficult to solve
the following problem:
Reversing the sum of two smooth numbers: Given an integer n, find a
representation of n as the sum of two smooth numbers with largest prime factor
bounded by O((log n)2+ǫ )
This seems to be the simplest one-way function known so far, since indeed the
generation of the input (smooth) numbers and their addition requires practically
now efforts at all, whereas solving the reverse problem seems rather difficult.

4

One can use the LLL algorithm to a smooth number (say, x) close to n, but the
probability that n − x will be also smooth is negligible.

4

Generating RSA keys

RSA algorithm requires a generation of two large prime numbers that serve as
a secret key for a user. Here we will briefly outline the primality generation
problem and the main practical issues, associated with it.

4.1

Primality generation problem

One of the most important problems in computational number theory is the
problem on primality testing. That is, given a large integer p, determine whether
it is a prime or a composite number. It is clear that the exhaustive search
algorithm that tests all the potential prime divisors of p is computationally
infeasible. The algorithms that are used in the public key encryption systems
are based on the following general idea:
1. Trial division of p aimed at detecting small prime divisors. Typically one
divides p by the first 30-100 primes and if none of the divisions produces
a residue zero.
2. Apply one of the existing probabilistic primality testing. For example, one
can test if 2p−1 ≡ 1(modp) and, if so, then either p is a prime or p is a
2-pseudoprime (this is simply an example of the Fermat’s Little Theorem).
The smallest composite number, for which this test fails is 341. One can
substitute 2 with larger values, but still there is a set of composite numbers, called Carmichael numbers, for which the test produces an incorrect
answer. The fact that the set of Carmichael numbers is infinite has been
established in 1994. So, instead of using FLT-based tests, we can use more
precise Rabin-Miller’s test. If in computing ap−1 mod p one gets 1 as an
answer, the algorithm performs a ‘forensic’ investigation on how exactly
this outcome “1” has been obtained. In this case the one can use only a
very small number of witnesses (a) in order to test the primality of p, but
the proof that only small number of witnesses is sufficient depends on the
correctness of the extended Riemann hypothesis.
There are some other primality testing algorithms. For example, a similar
algorithm is based on the following interesting property of Fibonacci numbers:
For every prime number, except 5, the following congruence holds:
Fp2 −1 ≡ 0(modp)
Since the value of Fp2 −1 (modp) can be obtained in O(log p) operations, the
algorithm is attractive. Again, it fails for very few, specific composite numbers,
called Fibonacci pseudo-primes, the smallest one being 161.
Until 2002, there was no primality testing algorithm that works in polynomial time and which computational complexity analysis does not depend on
any unproven hypothesis. In 2002, in a famous article Primes-is-in-P, [1] finally
demonstrated such an algorithm. The initial version of the algorithm has a

5

complexity of O(log12 p), subsequently improved to O(log6 p) after the efforts of
many researchers in the field.
Whilst the primality testing algorithm is a fascinating problem, the RSA
algorithm needs more, namely, generating prime numbers.
The prime number theorem says that the density of primes less than a given
bound x is lnxx .
So, if x is 21024 , then the probability that a randomly selected number less
than x would be prime is about 1 out 700. Since we will test only odd numbers,
then obviously we have a chance about 1 out of 350 to find a prime. Primality
testing algorithms normally require up to a few hundred microseconds, then it
is reasonable to expect that in a sub-second period one will be able to obtain a
prime number of this size.

4.2

Complexity of the primality generation

While the primality testing is provably computationally tractable in deterministic polynomial time, for the primality generation one has to assume some strong
number-theoretic conjectures to prove the computational efficiency. Even assuming the correctness of the Riemann hypothesis is not enough. But there is
conjecture, due to Cramer, that states:
Conjecture 2. There is at least one prime in the interval [x, x + ln2 x].
Assuming the correctness of this conjecture and the existence of efficient
primality testing algorithms, one can easily deduce that the primality generation
problem is solvable in polynomial time.

5

Algorithmic improvements based on the use
of smooth numbers

Every existing RSA key generation algorithm contains the two steps outlined
above – trial divisions to test the existence of small prime factors and a general
primality testing algorithm.
The trial division part is often overlooked from the analysis. But in fact
it requires a very large number of multiword divisions and, depending on the
limit of the small primes to be testes, it may take from 5 to 30 percent of the
actual timing in the case of primality testing algorithms. More to the point, in
the case of decentralized RSA key generation, different nodes are required to
produce different numbers and their sum is supposed to be a prime number. If
the numbers produced fail to deliver a prime number, the process is repeated.
The properties of the smooth numbers allow us to produce a new algorithm,
that can produce RSA keys faster than the existing ones. Let us segment the
explanation in two parts:
Generation of two smooth numbers in a way that their sum can be
prime is high
Let us consider the set of first 100 hundred primes. Let us also divide this
set into two subsets that do not have a common element. For example, S1 =
2, 5, 11, 17, 23, . . . and S2 = 3, 7, 13, 19, 29, . . .. Then we generate a set of small
6

random exponents ri (e.g, from the interval [1, 4]). Now we produce two smooth
numbers:
a = 2r1 5r3 11r5 17r7 . . .
and
b = 3r2 7r4 13r6 19r8 . . .
Then the sum a + b is for sure not divisible by the first 100 primes. So, it has
considerably higher chances to be a prime in comparison to a randomly selected
number of the same size.

5.1

Computational experiments

We used the first 100 primes for our test and the parameters provided above to
produce one million pairs of smooth numbers, a and b, and test the primality
f their sum. The size of the numbers produced is, on average, 768 bits. A
randomly chosen odd integer of this size has a chance around 0.4% to be prime.
On the other hand, the numbers produced as the sum of two smooth numbers
have a chance for primality around 2%. So, the probability is increased by a
factor of five and, more to the point, the primality testing with Rabin-Miller
test does not require any trial division part.

5.2

Extension to more than two smooth numbers

As we have pointed out, there is a need to generalize the solution to more than
two smooth numbers. Namely, to produce several (say, a few dozens) random
integers, d1 , d2 , . . . , dk , in a way that their sum will be prime. The above
outlined solution for the case k = 2 (two smooth integers only) can be easily
generalized to any k > 2.
Here is one possible solution: let the pair (d1 , d2 ) be generated in the way
outlined above. For the other numbers, d3 , d4 , . . . , dk , we use smooth numbers
of the form:
di = 2h1 3h2 5h3 7h4 11h5 . . .
Where the exponents hi are randomly chosen integers from the set [1, 2].
The reason why we use the set [1, 2] is that in the selection of d1 and d2 we
used only half of the first 100 primes with exponents in [1, 4]. For the rest of
the nodes we use all the primes and random exponents one or two; in this case,
the sizes of all numbers will be compatible.

5.3

Primality testing without trial divisions

The primality testing in practice comprises of two steps. The trial division part
tests if the number is divisible by a small prime up to a given level and, after that,
one applies Rabin-Miller’s primality testing algorithm. It is important to point
out that the first part of the testing procedure is not negligible. The primality
testing algorithms used by VDF alliance group test the first 150 primes (that
is, up to 863). The complexity of Rabin-Miller test is essentially one modular
exponentiation. So, it is relatively straightforward to quantify the role of the
trial divisions in the overall primality testing process.
7

(1, 2)

(1, 2, 3)

(1, 2, 3, 4)

(1, 2, 3, 4, 5)

Average size of the numbers generated

450 bit

712 bit

1000 bit

1313 bit

Probability to obtain a prime number
generated by adding two 100-integers

2.9%

2.1%

1.7%

Exponent range

1.45%

Table 1: Experimental data based on the generation of two co-prime smooth
100-integers.
Exponent range

(1, 2)

(1, 2, 3)

(1, 2, 3, 4)

Expected number of modular exponentiations to test a random integer for primality

156.5

243.9

344.8

Expected number of modular exponentiations to test for primality an integer
produced by our algorithm

34.5

47.6

58.8

Improvement factor

4.53

5.13

5.86

(1, 2, 3, 4, 5)
476.2

69.0

6.90

Table 2: Expected savings in terms of number of modular exponentiations
through the usage of smooth numbers.
Since the process of primality testing consists of trial divisions and modular
exponentiations (Rabin-Miller, Solovay-Strassen, Fermat and Fibonacci-based
tests all use modular exponentiations), it is essential to know – as accurately as
possible – the time ratio between one modular exponentiation and one division
by a small prime. It is platform dependent and also depends on how well
optimized division by a small constant is. For numbers of size 512-bit we have
experimented (Mathematica and Python) and have found a ratio about 40:1,
whereas for 1024-bit numbers, the ratio is about 1000:1.
We have also experimented with various choices for s, that is, the largest
prime factor of the shares of different nodes is bounded by the s-th prime. For
1024-bit numbers, we have obtained that if one uses s = 100 then the chance
that the number generated (provably not divisible by the first 100 primes) is
about 2%. This estimation is inline with the asymptotic estimation, provided
by Mertens and deBruijn [MdeB]. This is about seven times better than the
chance to obtain a prime number by randomly selecting an odd integer of this
size.
Table 1 provides some experimental data, obtained as follows. We produce
two co-prime smooth 100-integers, that is, their largest prime factor is the 100th
prime and also impose restrictions on the exponent used. The Table provides
the interval for the exponents, the average size of the numbers produced and
– the most important component – the probability that the sum of these two
numbers will be prime.
Table 2 showcases the expected savings in terms of the number of modular
exponentiations, needed to be performed – on average – until finding a prime
number. The sizes of the numbers are like in the previous table.

8

6

Conclusion and open problems

In this research report we showcase the possible use of smooth integers for
various cryptographic problems of substantial importance in modern world of
decentralized ledgers.
On a concrete level, we propose an algorithm for selecting shares of different
communicating parties in a way that can significantly reduce the communication
and computational cost of producing RSA keys.
On a more abstract level, we showcase some unusually simple one-way functions that can be investigates either as a tool for creating a new public key
cryptosystem, or as a hash function or as a cryptographic puzzle. The duality
between the two discussed in the paper one-way functions (e.g. the product of
two large primes is difficult to invert versus the sum of two large smooth numbers is difficult to invert) could be a subject of deep complexity theory research
work.

References
[1] Manindra Agrawal, Neeraj Kayal, and Nitin Saxena. PRIMES is in P.
Technical report, 2004.
[2] Roberto Avanzi, Vassil Dimitrov, Christophe Doche, and Francesco Sica.
Extending Scalar Multiplication Using Double Bases. pages 130–144.
Springer, Berlin, Heidelberg, 2006.
[3] B. M. M. de Weger. Algorithms for Diophantine Equations. Centrum voor
Wiskunde en Informatica, 1989.
[4] Daniel J. Bernstein. Arbitrarily Tight Bounds On The Distribution Of
Smooth Integers. PROCEEDINGS OF THE MILLENNIAL CONFERENCE ON NUMBER THEORY, 1:49—-66, 2002.
[5] Daniel J Bernstein, Peter Birkner, Tanja Lange, and Christiane Peters.
Optimizing double-base elliptic-curve single-scalar multiplication. Technical report.
[6] Daniel J Bernstein, Chitchanok Chuengsatiansup, and Tanja Lange.
Double-base scalar multiplication revisited. Technical report.
[7] Alex Capuñay and Nicolas Thériault. Computing Optimal 2-3 Chains for
Pairings. In Proceedings of the 4th International Conference on Progress in
Cryptology – LATINCRYPT 2015 - Volume 9230, pages 225–244. SpringerVerlag, 2015.
[8] Vassil Dimitrov, Laurent Imbert, and Pradeep K. Mishra. The doublebase number system and its application to elliptic curve cryptography.
Mathematics of Computation, 77(262):1075–1105, dec 2007.
[9] Vassil Dimitrov, Laurent Imbert, and Pradeep Kumar Mishra. Efficient
and Secure Elliptic Curve Point Multiplication Using Double-Base Chains.
pages 59–78. Springer, Berlin, Heidelberg, 2005.

9

[10] Vassil Dimitrov and Pradeep Mishra. A combinatorial interpretation of
double base number system and some consequences. Advances in Mathematics of Communications, 2(2):159–173, apr 2008.
[11] V.S. Dimitrov, G.A. Jullien, and W.C. Miller. An algorithm for modular
exponentiation. Information Processing Letters, 66(3):155–159, may 1998.
[12] Christophe Doche. On the Enumeration of Double-Base Chains with Applications to Elliptic Curve Cryptography. pages 297–316. Springer, Berlin,
Heidelberg, 2014.
[13] H A Helfgott. The Ternary Goldbach Conjecture Is True. Technical report.
[14] Daniel Krenn, Vorapong Suppakitpaisarn, and Stephan Wagner. On the
minimal Hamming weight of a multi-base representation. aug 2018.
[15] Kurt Mahler. On a Special Functional Equation. Journal of the London
Mathematical Society, s1-15(2):115–123, apr 1940.
[16] W. B. Pennington. On Mahler’s Partition Problem. The Annals of Mathematics, 57(3):531, may 1953.
[17] R. Tijdeman. On the maximal distance between integers composed of small
primes. Compositio Mathematica, 28(2):159–162, 1974.

10

