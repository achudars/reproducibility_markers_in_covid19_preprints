arXiv:1911.08877v1 [cs.CV] 20 Nov 2019

Improving Semantic Segmentation of Aerial Images Using Patch-based Attention
Lei Ding
University of Trento
Via Sommarive 5, Trento, Italy

Hao Tang
University of Trento
Via Sommarive 5, Trento, Italy

lei.ding@unitn.it

hao.tang@unitn.it

Lorenzo Bruzzone
University of Trento
Via Sommarive 5, Trento, Italy
lorenzo.bruzzone@unitn.it; bruzzone@ieee.org

Abstract
The trade-off between feature representation power and
spatial localization accuracy is crucial for the dense classification/semantic segmentation of aerial images. High-level
features extracted from the late layers of a neural network
are rich in semantic information, yet have blurred spatial
details; low-level features extracted from the early layers
of a network contain more pixel-level information, but are
isolated and noisy. It is therefore difficult to bridge the gap
between high and low-level features due to their difference
in terms of physical information content and spatial distribution. In this work, we contribute to solve this problem
by enhancing the feature representation in two ways. On
the one hand, a patch attention module (PAM) is proposed
to enhance the embedding of context information based on
a patch-wise calculation of local attention. On the other
hand, an attention embedding module (AEM) is proposed
to enrich the semantic information of low-level features
by embedding local focus from high-level features. Both
of the proposed modules are light-weight and can be applied to process the extracted features of convolutional neural networks (CNNs). Experiments show that, by integrating the proposed modules into the baseline Fully Convolutional Network (FCN), the resulting local attention network
(LANet) greatly improves the performance over the baseline and outperforms other attention based methods on two
aerial image datasets.

Figure 1: Examples of the image-level information for
aerial images. The information of a whole aerial image
cannot be deduced more specifically than just ‘aerial image’, but the information of image patches can be easily
attributed to classes like ‘car, ‘tree’ and ‘building’.

of images, is a crucial step for the automatic analysis and
explanation in applications of these aerial data. The rise of
convolutional neural networks (CNNs) and the emergence
of Fully Convolutional Networks (FCNs) [18] has brought a
breakthrough in semantic segmentation of aerial images [2].
Typical CNN architectures used in visual recognition tasks
employ cascade spatial-reduction operations to force the
networks to learn intrinsic representations of the observed
objects [8]. However, this so-called ‘encoding’ design has
the side-effect of losing spatial information. The classification maps produced by encoding networks usually suffer a
loss of localization accuracy (e.g., the boundaries of classified objects are blurred and some small targets may be neglected). Although there are ‘decoding’ designs to recover
spatial information by using features extracted from early
layers of the CNNs [3, 26, 14], their effectiveness is limited
due to the gap between the high-level and low-level features
in both semantic information and spatial distribution [35].

1. Introduction
Images collected from aerial platforms are widely used
in a variety of applications, such as land-use mapping, urban resources management and disaster monitoring. Semantic segmentation, namely the pixel-wise classification

This trade-off between feature embedding power and
4321

spatial localization accuracy is crucial for the semantic segmentation of aerial images. On the one hand, different categories of the ground objects may share similar spectral features in aerial images, thus requiring for an aggregation of
the context information. On the other hand, many applications of analysing aerial images require high precision in
mapping contours of ground objects. Therefore, detailed
spatial information is needed for identifying accurately both
the boundary of regions and small objects.
The introduction of attention mechanism is an effective strategy to reduce the confusion in predicted categories
without losing spatial information. With the global statistics
aggregated from the whole image, scene information can be
embedded to highlight (or suppress) the features with strong
correlations [9]. However, the spatial size of aerial images
is usually much larger than that of natural images, whereas
the number of object categories is smaller. For example,
each image in the ISPRS semantic labelling dataset (Potsdam area) [11] has 6000 × 6000 pixels divided into 6 object
categories in this dataset. As a result, almost every image
contains all the object categories, and no clear global scene
information can be embedded at the global level. In other
words, we argue that the typical attention-based techniques
cannot be directly applied to the semantic segmentation of
large-size aerial images.
In this paper, we propose the generation of patch-level
attention to improve the semantic segmentation of aerial images. The proposed approach is based on the finding that,
although the semantic information of a whole aerial image
cannot be specifically concluded, the image patches still
have clear semantic reference (an illustration example of
this observation is given in Fig. 1). Therefore, we propose a
novel Patch Attention Module (PAM) to exploit patch-wise
local attention. This module operates on extracted feature
maps and can aggregate context information from the local patch to reduce confusions. In our model, the PAM is
appended after both the high-level and low-level features
to enhance their representation. Moreover, to bridge the
gap between high-level and low-level features, an Attention Embedding Module (AEM) is proposed to embed semantic focus from high-level features to low-level features.
This module can greatly improve the semantic representation of low-level features without losing their spatial details, thus improving the effectiveness of the fusion between
high-level and low-level features. The proposed modules
are light-weight and can be incorporated into existing CNN
architectures to improve the classification accuracy. Using
a FCN as baseline network, we performed experiments on
two aerial datasets and proved the the effectiveness of the
proposed techniques.
Our contributions can be summarized as follows:

enhancing the scene-related representation in both encoding and decoding phases.
• Proposing both patch attention module (PAM) to embed scene information from local patches, and attention embedding module (AEM) to enhance the semantic representation of low-level features by introducing
attention from high-level features.
• Extensive ablation studies have been performed by incorporating the proposed modules into the baseline
FCN network in sequence. The resulting LANet is
further compared with other networks with decoding
or attention-based designs to evaluate its performance.

2. Related Work
2.1. Semantic Segmentation of Aerial Images
Semantic segmentation on aerial scenes has drawn great
research interests after the rising of CNNs and the publish of
several open datasets/contests such as ISPRS Benchmarks1 ,
DeepGlobe contest2 , and SpaceNet competition3 . Several
studies incorporate multiple models to increase the prediction certainty [24, 33, 32]. The prediction of object contours is an issue of concern. Detection of edges is explicitly
added in [20], while [16] introduced an edge loss to enhance
the preservation of objects boundaries. The utilization of
other forms of data (e.g., Lidar data, digital surface models
and OpenStreetMap) is also widely studied [13, 28, 27, 1].
However, there are limited studies focused on the special
characteristics of aerial images (e.g. large spatial size, fixed
imaging angle and small number of classes). In this work
we consider these characteristics when designing the specific modules.

2.2. Encoder-Decoder Designs
The encoder-decoder networks have been successfully
used in many computer vision tasks such as image generation [10, 29], object/saliency detection [15, 21], crowd
counting [12] and semantic segmentation [4, 19]. Usually, the encoder-decoder networks contain two sub-nets:
(i) an encoder sub-net that gradually reduces the feature
maps and captures higher semantic information, and (ii) a
decoder sub-net that gradually recovers the spatial information. The encoder sub-net is the focus of most existing studies. There are many works related to enlarging the receptive
field without significantly increasing the number of parameters [32, 5]. Although in some studies there are cascade
decoding designs that aim to exploit the features from early
1 http://www2.isprs.org/commissions/comm3/wg4/semanticlabeling.html
2 http://deepglobe.org/challenge.html
3 https://spacenetchallenge.github.io/

• Proposing a local attention network (LANet) to improve the semantic segmentation of aerial images by
4322

Figure 2: Architecture of the proposed local attention network (LANet). The patch attention module (PAM) generates
attention maps to highlight patch-wise focus in feature maps. The attention embedding module (AEM) embeds semantic
information from high-level features to low-level ones.
CNN layers [26, 3, 14, 35], these features are usually concatenated or summed to the high-level features without enhancing their semantic representation. Thus, they provide
limited contribution to the classification accuracy. To overcome this limitation, we propose the use of attention mechanism for enhancing the representation of low-level features
during the decoding phase.

lations but it is implemented by applying dilated convolutions. PSANet [36] introduced the modelling of long-range
correlation for each spatial position, but the channels of its
inner layers are related to the input image size and cannot be
applied to the prediction of full-size aerial images. A parallel design that models both channel-wise and point-wise
attention is introduced in DANet [23]. A limitation of this
network is that the reasoning of global spatial correlation is
calculation intensive. A light-weight graph-based module
for reasoning latent correlations has been presented in [6].
Building on top of these studies, we propose a simple yet
effective approach that extends the use of attention mechanism to the spatial dimension without significantly increasing the computational load.

2.3. Attention Mechanism
Attention mechanism refers to the strategy of allocating
biased computational resources to the processed signal to
highlight its informative parts. In the tasks related to the
understanding of image content, a typical solution for generating attention statistics is to gather information from a
global scale, namely to exploit the scene or image-level information. This is because the scene information may provide clues about the possible contents in an image. In [30],
the attention of the feature map is aggregated using an hourglass module in a residual manner. This residual attention network introduced a chunk-and-mask module, where
the global attention is aggregated in the Soft Mask Branch
through stacked down-sampling convolutions. In [9], a
Squeeze-and-Excitation (SE) block is proposed, which uses
global-pooling to generate channel-wise attention. In this
way, spatial-irrelevant information can be learned to emphasize the scene-relevant feature channels. The design of
squeezing spatial information and the parallel connection of
attention branch introduced in this work have been widely
adopted in subsequent studies. In EncNet [34], a context encoding module is proposed to capture the scene-dependent
global context as channel-wise attention. CBAM [31] introduced a spatial attention module to highlight the informative spatial regions. The spatial attention maps are generated by using pooling operations along the channel axis.
BAM [25] has a similar module to exploit spatial corre-

3. Proposed Approach
In this section we present the proposed LANet devised for improving semantic segmentation of aerial images. Firstly, an overview of the network is given to introduce the general motivation and architecture. After that,
the proposed modules are described in detail. Finally, a further explanation on aggregation strategy for different levels
of features is given.

3.1. Overview of the Proposed LANet
The motivation of this work is to strengthen the representation of features extracted from backbone CNNs while
minimizing the loss of spatial details. To achieve this goal,
we propose a LANet with two separate modules: a patch attention module (PAM) to enhance the embedding of local
context information, and an attention embedding module
(AEM) to improve the use of spatial information. Specifically, we designed two parallel branches to process features from different layers. As shown in Fig. 2, in the upper branch, high-level features (produced by late layers of a
4323

Figure 4: Detailed design of the AEM. Low-level features
are semantically enriched by embedding local focus from
high-level features.
Figure 3: Detailed design of the PAM. Descriptors are calculated patch-wisely to aggregate local context information.
they can be applied to process other patches without assigning extra weights. The gating operation to generate attention maps can be symbolized as:

CNN) go through a PAM to enhance their feature representation; in the lower branch, low-level features (produced by
early layers of a CNN) are first enhanced by PAM, then embedded with semantic information from high-level through
AEM. The final classification results are produced by the
fusion of the features from both branches.

ap = FU {σ[Hi δ(Hr zp )]},

where σ and δ denote Sigmoid and ReLU functions [22],
respectively; Hr denotes the 1×1 dimension-reduction convolution with the reduction ratio r, Hi denotes the 1×1
dimension-increasing convolution that recovers the feature
dimension back to c. FU is the upsampling operation.
This is the case for a single local patch. Now we consider
it at the global level. Given a feature map X ∈ RC×H×W ,
0
0
maps of descriptors Z ∈ RC×H ×W can be generated. H 0
and W 0 are determined by the size of each patch (pooling
window) as:
H
W
H0 =
,W0 =
,
(3)
hp
wp

3.2. Patch Attention Module
Semantic segmentation of aerial images suffers greatly
from the problem of intra-class inconsistence, since the
classification of ground objects is a comprehensive task affected by both the surface type and the context of an image.
To alleviate this problem, we propose a patch attention module to enhance the aggregation of context information in the
extracted features.
Fig. 3 shows the design of the PAM. Our work is inspired
by the design of the SE-block [9]. The original SE-block introduced global average pooling to generate one single descriptor for each feature channel. However, as discussed in
Section 1, this cannot be applied to the processing of largesize aerial images. In our approach, we limit the generation
of descriptors in patches, so that each descriptor contains
meaningful information of the local context. The descriptor
zc for the c-th channel of a patch is calculated as:
zc =

hp wp
X
X
1
xc (i, j),
hp × wp i=1 j=1

(2)

where hp and wp are set according to the spatial reduction
ratio of the corresponding encoding layer to ensure a remarkable enlargement of the receptive field. An alternative
is to use a sliding window for generating the descriptors, so
that the descriptor maps have the same size of input images.
However, this option will tremendously increase the calculation, thus, it is not adopted in our implementation. After the convolutional layers, attention maps A ∈ RC×H×W
can be produced. Finally, the original input features X are
multiplied element-wisely with A to enhance their representation. A residual design is adopted to ensure the stable
back-propagation of gradients.

(1)

3.3. Attention Embedding Module

where hp and wp denote the spatial size of the pooling
window, xc denotes a pixel at cth channel. In this way, a
c-channel vector zp can be generated, which contains the
statistics describing the patch p. After that, we follow the
bottleneck gating design in [9] to learn an attention vector
ap ∈ Rc×hp ×wp for the patch p. Instead of using fully connected layers, we employ convolutional operations so that

An effective exploitation of low-level features is difficult
due to their difference with high-level features in terms of
spatial distribution and physical meaning. The most frequently used way of employing low-level features is to concatenate them with high-level features, which brings only
slight improvement in performance (refer to discussion in
Section ??). To make the best use of low-level features, we
4324

propose an attention embedding module to enrich their semantic meaning. This operation bridges the gap between
high-level and low-level features without sacrificing the
spatial details of the latter.
Fig. 4 shows the design of the proposed AEM. The intuition of this approach is to embed local attention from
high-level features into the low-level features. In this way,
low-level features are embedded with context information
that goes beyond the limitation of their receptive fields,
while their spatial details are kept. First, we generate descriptors from high-level features through the same calculation as in Eq. (1). Denote these maps of descriptors as
0
0
Zh ∈ RCh ×H ×W , and the low-level features as Xl ∈
RCl ×Hl ×Wl . We generate attention maps for the low-level
features Al by transforming Zh through bottleneck convolutions as:
Al = FU {σ[Hl δ(Hr Zh )]},
(4)

large building blocks. 24 imageries are used for training
and the remaining 14 for testing. There are four spectral
bands in each TOP (red, green, blue and near infrared) and
one band in each DSM. All data files have the same spatial size, equal to 6000 × 6000 pixels. The ground sampling distance (GSD) of this dataset is 5cm. The reference
data are labeled according to six land-cover types: impervious surfaces, building, low vegetation, tree, car and clutter/background.
(ii) The vaihingen dataset [11] contains 33 true orthophoto (TOP) tiles and the corresponding digital surface
models (DSMs) collected from a small village. 16 imageries are used for training and the remaining 17 ones for
testing. Different from the Potsdam dataset, each TOP in
the Vaihingen dataset contains three spectral bands (near
infrared, red and green bands) and one DSM band. The
spatial size of the images varies from 1996 × 1995 pixels
to 3816 × 2550 pixels. The GSD of this dataset is 9 cm.
The reference data are divided into the same six categories
as the Potsdam dataset.

where Hr is a dimension reduction convolution and Hl
changes the number of channels to be the same as Xl . To
avoid excessive interference of high-level features, we add
a residual design to emphasize the importance of low-level
features. The enhanced low-level features are calculated as:
Xl = Xl + Xl Al

Evaluation Metrics. Following the evaluation method
provided by the data publisher [11] and used in literature [20, 32, 17], three evaluation metrics are used to evaluate the performance of methods, i.e, overall accuracy (OA),
per-class F1 score and average F1 score. OA is calculated
by dividing the correctly classified number of pixels with
the total number of pixels. The F1 score for a certain class
is defined as the harmonic mean of precision and recall:

(5)

3.4. Feature Fusion between Different Layers
After being processed by AEM, low-level features are
semantically enriched and can potentially give a higher contribution to the prediction of the pixel class. Both the highlevel and low-level features keep their dimensions after the
processing of PAM and AEM. Accordingly, classic feature
fusion operations (e.g., concatenation) can be applied to the
outputs of the two branches. Since the specific feature fusion operation is not the focus of this work, also considering
the convenience of validating the output from each branch,
we simply train two separate classifiers for each branch, and
perform an element-wise sum to generate the final results.

F1 = 2 ×

precision × recall
precision + recall

(6)

Implementation Settings. The same preprocessing, data
augmentation and weight initialization settings have been
used in all the experiments. The DSMs are concatenated
with TOPs as input data, so that we obtain five channels
for the Potsdam dataset and four channels for the Vaihingen
dataset. Due to the limitation of computational resources,
the input data are cropped using a 512 × 512 window during the training phase. However, the prediction for the test
set is performed whole-image-wise to obtain an accurate
evaluation of the compared methods. Random-flipping and
random-cropping operations are conducted during each iteration of the training phase as an augmentation approach. We
use ResNet50 as the backbones for all compared networks
with the pretrained weight for Pascal VOC dataset loaded
from the PyTorch library. Considering the different GSD
of the two datasets, the down-sampling stride for the Potsdam dataset is set to 32, while for the Vaihingen dataset it is
set to 16. The networks are implemented with PyTorch and
the experiments are conducted on a server with a NVIDIA
Quadro P6000 23GB GPU.

4. Experiments
To assess the effectiveness of the proposed method,
experiments have been conducted on two aerial image
datasets, i.e, the Potsdam dataset and the Vaihingen dataset.
First we provide a short description of both datasets and
implementation details. Then we test the proposed modules through an ablation study. Finally, we compare the
proposed LANet with state-of-the-art methods and draw the
conclusion of our experimental validation.

4.1. Experimental Setting
Datasets. We employ two public available datasets to evaluate the proposed methods.
(i) The potsdam dataset [11] consists of 38 TOP tiles and
the corresponding DSMs collected from a historic city with
4325

Table 1: Results of the ablation study on the Potsdam
dataset. (∗ ) low-feat indicates the use of low-level features.
Method
FCN
FCN+PAM
FCN
FCN+PAM
FCN+AEM
LANet

low-feat∗

PAM

AEM

√
√
√
√
√

√
√

√
√

mean F1
88.66
89.03
91.23
91.76
91.78
91.95

OA
89.42
89.61
89.58
90.65
90.60
90.84

Table 2: Results of the ablation study on the Vaihingen
dataset.
Method
FCN
FCN+PAM
FCN
FCN+PAM
FCN+AEM
LANet

low-feat

PAM

AEM

√
√
√
√
√

√
√

√
√

mean F1
86.14
86.42
86.52
87.49
86.80
88.09

Image(IRRG)

Reference

Before PAM

After PAM

Figure 5: Comparison of classified high-level features before and after the use of PAM (the Potddam dataset).

OA
88.66
88.68
88.84
89.36
89.05
89.83

PAM module for high-level features. Since high-level layers already have relatively large receptive field before using
PAM, the enhancement is not significant. However, one can
still observe that some of the meaningless small areas are
removed, and the large objects become more complete.
Fig. 6 shows changes of the classified low-level features before and after the use of PAM and AEM. In the
original low-level feature maps, pixels are only related to
their neighborhoods due to the limitation of small receptive
field. This leads to fragmented results and confusion of object class. However, after enhancement obtained with the
proposed modules, the semantic representation of low-level
features are significantly improved. The pixels are classified based on not only the surface type of objects but also
the context information. Moreover, one can verify from the
clearly classified boundaries that the spatial details of lowlevel features are kept.
Comparison with State-of-the-Art Methods. Comparisons are made between the proposed LANet and state-ofthe-art approaches presented in literatures. All the tested
approaches use the same backbone network (resnet50) and
conduct the prediction on full-size test data. The experiments cover several recent works with the use of attention mechanism, including SE block [9], BAM [25],
CBAM[31] GloRe [6] and DANet [23]. The PSPNet [7]
and DeepLabv3+ [5] with receptive-field-enlarging designs
are also compared. Table 3 and Table 4 reports the quantitative results on the Potsdam dataset and the Vaihingen
dataset, respectively. Compared with the baseline FCN, the
use of most attention-based modules such as SE, BAM and
CBAM do not lead to noticeable performance improvement.
The use of SE-block even causes decreases in F1 scores, especially for the car class. This is because the channel-wise
descriptors are calculated on the whole feature map, and
the classes that account for a small portion of total pixels
are suppressed. This proves our assumption that the globallevel calculation of attention descriptors is not suitable for

4.2. Experimental Results
Ablation Study. In order to verify the effectiveness of the
proposed modules, ablation studies have been conducted on
the two datasets. FCN (ResNet-50) is used as the baseline
network for comparison. Since the proposed LANet uses
low-level features, the effect of considering low-level features has also been measured.
Table 1 shows the results of the ablation study on the
Potsdam dataset. Three groups of observations can be done
from the results. When no low-level features are involved
in the decoding stage, the use of only one PAM (added on
top of the FCN) increases the OA of 0.19%. With the inclusion of low-level features (concatenated with high-level features), the OA of the baseline FCN increases of only 0.16%.
However, when two PAMs are added to process the highlevel and low-level features separately, the OA increases of
another 1.07%. When the proposed AEM is used instead
to enhance low-level features, the OA increases of 1.02%.
With the use of both PAM and AEM, the proposed LANet
increases the OA and average F1 compared with the baseline FCN (with the use of low-level features) of 1.26% and
0.72%, respectively.
The ablation study on the Vaihingen dataset is presented
in Table 2. Under the condition that low-level features are
considered, the proposed LANet improves the average F1
score and OA of 1.57% and 0.99%, respectively.
Visualization of Features. To visually confirm the effect of
the proposed modules, we present comparisons of the classified features generated independently before and after the
use of the proposed modules. Fig. 5 shows the effect of the
4326

Table 3: Results on the Potsdam dataset. Per-class F1 score, average F1 score and overall accuracy (OA) are listed (%).

Method
FCN
FCN+SE [9]
FCN+BAM [25]
FCN+CBAM [31]
FCN+GloRe [6]
DANet [23]
PSPNet [7]
DeepLabv3+ [5]
LANet

Impervious Surface
91.46
91.47
90.43
91.37
91.55
91.61
91.61
92.35
93.05

Per-class F1 Score
Building low vegetation
96.63
85.99
96.57
86.21
94.97
85.84
96.49
86.00
96.54
86.17
96.44
86.11
96.30
86.41
96.77
85.22
97.19
87.30

Tree
86.94
87.51
87.47
87.40
87.42
88.04
86.84
86.79
88.04

Car
82.28
81.07
85.63
83.22
82.69
83.54
91.38
93.58
94.19

average F1

OA

88.66
88.56
88.87
88.89
88.87
89.14
90.51
90.94
91.95

89.42
89.55
88.83
89.46
89.57
89.72
89.45
89.74
90.84

Table 4: Results on the Vaihingen dataset. Per-class F1 score, average F1 score and overall accuracy (OA) are listed (%).

Method
FCN
FCN+SE [9]
FCN+BAM [25]
FCN+CBAM [31]
FCN+GloRe [6]
DANet [23]
PSPNet [7]
DeepLabv3+ [5]
LANet

Image(IRRG) Reference

Impervious Surface
94.10
93.95
94.01
94.03
93.99
94.11
94.38
94.34
94.90

Before

After PAM

Per-class F1 Score
Building low vegetation
90.98
81.25
90.43
81.33
90.77
81.54
90.86
81.16
90.57
81.28
90.78
81.40
91.44
81.52
91.35
81.32
92.41
82.89

Tree
87.58
87.50
87.78
87.63
87.49
87.42
87.91
87.84
88.92

Car
76.80
63.33
71.76
76.26
70.09
75.85
78.02
78.14
81.31

average F1

OA

86.14
83.31
85.17
85.99
84.68
85.91
86.65
86.60
88.09

88.66
88.27
88.62
88.61
88.41
88.59
88.99
88.91
89.83

processing large-size aerial images. The DANet with a spatial dependency modelling design improves the OA of 0.3%
on the Potsdam dataset, but there is a decrease of OA on the
Vaihingen dataset. DeepLabv3+, which uses both low-level
features and dilated convolutions, has good performance in
F1 scores. The proposed LANet outperforms existing approaches in terms of both average F1 score and OA, and has
a leading in the F1 scores of all the categories.
Visual Analysis of the Results. Some examples of the predicted patches on the two datasets are shown in Fig. 7 and
Fig. 8, respectively. With the aggregation of local context
information, results of the proposed LANet are less fragmented, while the contours of some small objects are more
clear. Fig. 9 shows an example of large-size prediction result on the Potsdam dataset. The proposed LANet obtains
good classification results for both large objects (e.g. buildings) and small objects (e.g. cars, paths).

After AEM

Figure 6: Comparison of classified low-level features before
and after the use of PAM and AEM (the Potddam dataset).

5. Conclusions
In this paper, we have presented a local attention network (LANet) to improve semantic segmentation of aerial
4327

Image (IRRG)

Ground truth

FCN

DANet

FCN+SE

PSPNet

DeepLabv3+

LANet

Figure 7: Examples of semantic segmentation results on the Potsdam dataset.

Image (IRRG)

Ground truth

FCN

DANet

FCN+SE

PSPNet

DeepLabv3+

LANet

Figure 8: Examples of semantic segmentation results on the Vaihingen dataset.

Test image (IRRG)

Ground truth

FCN

FCN+SE

DANet

PSPNet

DeepLabv3+

LANet

Figure 9: Example of large-size segmentation results on the Potsdam dataset. Major differences are highlighted.

4328

images. Two modules are proposed for enhancing the representation of features based on the attention mechanism.
Specifically, patch attention module (PAM) enhances encoding of context information based on patch-wise calculation of local descriptors, attention embedding module
(AEM) embeds attention from high-level layers into lowlevel ones to enrich their semantic information. Experimental results on two aerial datasets (Potsdam dataset and
Vaihingen dataset) show that the proposed approach greatly
improves the representation of extracted features and outperform other global-attention and receptive-field-enlarging
based techniques. However, there is still room for improving the encoding and enhancement of high-level features,
which left for future work.

[13] Pascal Kaiser, Jan Dirk Wegner, Aurélien Lucchi, Martin
Jaggi, Thomas Hofmann, and Konrad Schindler. Learning
aerial image segmentation from online maps. IEEE Transactions on Geoscience and Remote Sensing, 55(11):6054–
6068, 2017.
[14] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian
Reid. Refinenet: Multi-path refinement networks for highresolution semantic segmentation. In CVPR, 2017.
[15] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyramid
networks for object detection. In CVPR, 2017.
[16] Shuo Liu, Wenrui Ding, Chunhui Liu, Yu Liu, Yufeng Wang,
and Hongguang Li. Ern: Edge loss reinforced semantic segmentation network for remote sensing images. Remote Sensing, 10(9):1339, 2018.
[17] Yu Liu, Duc Minh Nguyen, Nikos Deligiannis, Wenrui Ding,
and Adrian Munteanu. Hourglass-shapenetwork based semantic segmentation for high resolution aerial imagery. Remote Sensing, 9(6):522, 2017.
[18] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In
CVPR, 2015.
[19] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In
CVPR, 2015.
[20] Dimitrios Marmanis, Konrad Schindler, Jan Dirk Wegner,
Silvano Galliani, Mihai Datcu, and Uwe Stilla. Classification with an edge: Improving semantic image segmentation
with boundary detection. ISPRS Journal of Photogrammetry
and Remote Sensing, 135:158–172, 2018.
[21] Kyle Min and Jason J Corso. Tased-net: Temporallyaggregating spatial encoder-decoder network for video
saliency detection. In CVPR, 2019.
[22] Vinod Nair and Geoffrey E Hinton. Rectified linear units
improve restricted boltzmann machines. In ICML, 2010.
[23] Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. Dual
attention networks for multimodal reasoning and matching.
In CVPR, 2017.
[24] Sakrapee Paisitkriangkrai, Jamie Sherrah, Pranam Janney,
Van-Den Hengel, et al. Effective semantic pixel labelling
with convolutional networks and conditional random fields.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition Workshops, pages 36–43, 2015.
[25] Jongchan Park, Sanghyun Woo, Joon-Young Lee, and In So
Kweon. Bam: Bottleneck attention module. arXiv preprint
arXiv:1807.06514, 2018.
[26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention. Springer, 2015.
[27] Shihao Sun, Lei Yang, Wenjie Liu, and Ruirui Li. Feature fusion through multitask cnn for large-scale remote sensing image segmentation. In 2018 10th IAPR Workshop on Pattern
Recognition in Remote Sensing (PRRS), pages 1–4. IEEE,
2018.
[28] Weiwei Sun and Ruisheng Wang. Fully convolutional networks for semantic segmentation of very high resolution re-

References
[1] Nicolas Audebert, Bertrand Le Saux, and Sébastien Lefèvre.
Joint learning from earth observation and openstreetmap data
to get faster better semantic maps.
[2] Nicolas Audebert, Bertrand Le Saux, and Sébastien Lefèvre.
Beyond rgb: Very high resolution urban remote sensing with
multimodal deep networks. ISPRS Journal of Photogrammetry and Remote Sensing, 140:20–32, 2018.
[3] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.
Segnet: A deep convolutional encoder-decoder architecture
for image segmentation. TPAMI, 39(12):2481–2495, 2017.
[4] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-decoder with atrous
separable convolution for semantic image segmentation. In
ECCV, 2018.
[5] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-decoder with atrous
separable convolution for semantic image segmentation. In
ECCV, 2018.
[6] Yunpeng Chen, Marcus Rohrbach, Zhicheng Yan, Yan
Shuicheng, Jiashi Feng, and Yannis Kalantidis. Graph-based
global reasoning networks. In CVPR, 2019.
[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Spatial pyramid pooling in deep convolutional networks for
visual recognition. TPAMI, 37(9):1904–1916, 2015.
[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR,
2016.
[9] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018.
[10] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017.
[11] ISPRS.
2d semantic labeling contest - potsdam.
http://www2.isprs.org/commissions/comm3/
wg4/2d-sem-label-potsdam.html, Online; accessed on 4 September, 2018.
[12] Xiaolong Jiang, Zehao Xiao, Baochang Zhang, Xiantong
Zhen, Xianbin Cao, David Doermann, and Ling Shao.
Crowd counting and density estimation by trellis encoderdecoder networks. In CVPR, 2019.

4329

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

motely sensed images combined with dsm. IEEE Geoscience
and Remote Sensing Letters, 15(3):474–478, 2018.
Hao Tang, Dan Xu, Nicu Sebe, Yanzhi Wang, Jason J Corso,
and Yan Yan. Multi-channel attention selection gan with cascaded semantic guidance for cross-view image translation. In
CVPR, 2019.
Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng
Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang.
Residual attention network for image classification. In
CVPR, 2017.
Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In
So Kweon. Cbam: Convolutional block attention module.
In ECCV, 2018.
Bo Yu, Lu Yang, and Fang Chen. Semantic segmentation for
high spatial resolution remote sensing images based on convolution neural network and pyramid pooling module. IEEE
Journal of Selected Topics in Applied Earth Observations
and Remote Sensing, (99):1–10, 2018.
Ce Zhang, Isabel Sargent, Xin Pan, Andy Gardiner, Jonathon
Hare, and Peter M Atkinson. Vprs-based regional decision
fusion of cnn and mrf classifications for very fine resolution
remotely sensed images. IEEE Transactions on Geoscience
and Remote Sensing, 2018.
Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang,
Xiaogang Wang, Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation. In CVPR, 2018.
Zhenli Zhang, Xiangyu Zhang, Chao Peng, Xiangyang Xue,
and Jian Sun. Exfuse: Enhancing feature fusion for semantic
segmentation. In ECCV, 2018.
Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen
Change Loy, Dahua Lin, and Jiaya Jia. Psanet: Point-wise
spatial attention network for scene parsing. In ECCV, 2018.

4330

