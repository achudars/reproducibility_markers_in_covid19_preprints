FIBS: A Generic Framework for Classifying
Interval-based Temporal Sequences
S. Mohammad Mirbagheri(

)

and Howard J. Hamilton

arXiv:1912.09445v2 [cs.LG] 14 Sep 2020

Department of Computer Science, University of Regina, Regina, Canada
{mirbaghs,Howard.Hamilton}@uregina.ca

Abstract. We study the problem of classifying interval-based temporal sequences (IBTSs). Since common classification algorithms cannot
be directly applied to IBTSs, the main challenge is to define a set of
features that effectively represents the data such that classifiers can be
applied. Most prior work utilizes frequent pattern mining to define a feature set based on discovered patterns. However, frequent pattern mining
is computationally expensive and often discovers many irrelevant patterns. To address this shortcoming, we propose the FIBS framework for
classifying IBTSs. FIBS extracts features relevant to classification from
IBTSs based on relative frequency and temporal relations. To avoid selecting irrelevant features, a filter-based selection strategy is incorporated into FIBS. Our empirical evaluation on eight real-world datasets
demonstrates the effectiveness of our methods in practice. The results
provide evidence that FIBS effectively represents IBTSs for classification algorithms, which contributes to similar or significantly better accuracy compared to state-of-the-art competitors. It also suggests that
the feature selection strategy is beneficial to FIBS’s performance.
Keywords: Interval-based events, Temporal interval sequences, Featurebased classification framework

1

Introduction

Interval-based temporal sequence (IBTS) data are collected from application
domains in which events persist over intervals of time of varying lengths. Such
domains include medicine [1,2,3], sensor networks [4], sign languages [5], and
motion capture [6]. Applications that need to deal with this type of data are
common in industrial, commercial, government, and health sectors. For example,
some companies offer multiple service packages to customers that persist over
varying periods of time and may be held concurrently. The sequence of packages
that a customer holds can be represented as an IBTS.
IBTSs can be obtained either directly from the applications or indirectly
by data transformation. In particular, temporal abstraction of a univariate or
This research was supported by funding from ISM Canada and the Natural Sciences
and Engineering Research Council of Canada.

multivariate time series may yield such data. Segmentation or aggregation of a
time series into a succinct symbolic representation is called temporal abstraction
(TA) [3]. TA transforms a numerical time series to a symbolic time series. This
high-level qualitative form of data provides a description of the raw time series data that is suitable for a human decision-maker (beacause it helps them to
understand the data better) or for data mining. TA may be based on knowledgebased abstraction performed by a domain expert. An alternative is data-driven
abstraction utilizing temporal discretization. Common unsupervised discretization methods are Equal Width, Symbolic Aggregate Approximation (SAX) [7],
and Persist [8]. Depending on the application scenario, symbolic time series
may be categorized as point-based or as interval-based. Point-based data reflect
scenarios in which events happen instantaneously or events are considered to
have equal time intervals. Duration has no impact on extracting patterns for
this type. Interval-based data, which is the focus of this study, reflect scenarios
where events have unequal time intervals; here, duration plays an important role.
Fig. 1 depicts the process of obtaining interval-based temporal sequences.

Fig. 1: Process of obtaining interval-based temporal sequences

Classifying IBTSs is a relatively new research area. Although classification
is an important machine learning task and has achieved great success in a wide
range of applications (fields), the classification of IBTSs has not received much
attention. A dataset of IBTSs contains longitudinal data where instances are described by a series of event intervals over time rather than features with a single
value. Such dataset does not match the format required by standard classification algorithms to build predictive models. Standard classification methods for
multivariate time series (e.g., Hidden Markov Models [9] and recurrent neural
networks), time series similarity measures (e.g., Euclidean distance and Dynamic
Time Warping (DTW) [10]), and time series feature extraction methods (e.g.,
discrete Fourier transform, discrete wavelet transform and singular value decomposition) cannot be directly applied to such temporal data.
2

In this paper, we formalize the problem of classification of IBTSs based on
feature-based classifiers and propose a new framework to solve this problem. The
major contributions of this work are as follows:
– We propose a generic framework named FIBS for classifying IBTSs. It represents IBTSs by extracting features relevant to classification from IBTSs
based on relative frequency and temporal relations.
– To avoid selecting irrelevant features, we propose a heuristic filter-based
feature selection strategy. FIBS utilizes this strategy to reduce the feature
space and improve the classification accuracy.
– We report on an experimental evaluation that shows the proposed framework
is able to represent IBTSs effectively and classifying them efficiently.
The rest of the paper is organized as follows. Related work is presented in
Section 2. Section 3 provides preliminaries and the problem statement. Section
4 presents the details of the FIBS framework and the feature selection strategy.
Experimental results on real datasets and evaluation are given in Section 5.
Section 6 presents conclusions.

2

Related Work

To date, only a few approaches to IBTS classification can be found in the literature. Most of them are domain-specific and based on frequent pattern mining
techniques. Patel et al. [2] proposed the first work in the area of IBTS classification. They developed a method that first mines all frequent temporal patterns
in an unsupervised setting and then uses some of these patterns as features for
classification. They used Information Gain, a measure of discriminative power,
as the selection criterion. After features are extracted, common classification
techniques, such as decision trees or Support Vector Machines (SVM), are used
to predict the classifications for unseen IBTSs.
Extracting features from frequent temporal patterns presents some challenges. Firstly, a well-known limitation of frequent pattern mining algorithms
is that they extract too many frequent patterns, many of which are redundant
or uninformative. Several attempts have been made to address this limitation
by discovering frequent temporal patterns in a supervised setting [11,12]. For
example, Batal et al. [11] proposed the Minimal Temporal Patterns (MPTP)
framework to filter out non-predictive and spurious temporal patterns to define
a set of features for classifying Electronic Health Record (EHR) data. Secondly,
discovering frequent patterns is computationally expensive. Lastly, classification
based on features extracted from frequent patterns does not guarantee better
performance than other methods.
In contrast to the approaches based on frequent pattern mining, a few studies offer similarity-based approaches to IBTS classification. A robust similarity
(or distance) measure allows machine learning tasks such as similarity search,
clustering and classification to be performed. Towards this direction, Kostakis
et al. [13] proposed two methods for comparing IBTSs. The first method, called
3

the DTW-based method, maps each IBTS to a set of vectors where one vector is
created for each start- or end-point of any event-interval. The distances between
the vectors are then computed using DTW. The second method, called Artemis,
measures the similarity between two IBTSs based on the temporal relations
that are shared between events. To do so, the IBTSs are mapped into a bipartite
graph and the Hungarian algorithm is employed. Kotsifakos et al. [14] proposed
the Interval-Based Sequence Matching (IBSM ) method where each time point
is represented by a binary vector that indicates which events are active at that
particular time point. The distances between the vectors are then computed
by Euclidean distance. In all three methods, IBTSs are classified by the k-NN
classification algorithm and it was shown that IBSM outperforms the two other
methods [14] with respect to both classification accuracy and runtime. Although
the reported results are promising, such classifiers still suffer from major limitations. While Artemis ignores the duration of the event intervals, DWT-based
and IBSM ignore the temporal relations between event intervals.
A feature-based framework for IBTS classification, called STIFE, was recently proposed by Bornemann et al. [15]. STIFE extracts features using a combination of basic statistical metrics, shapelet [16] discovery and selection, and
distance-based approaches. Then, a random forest is constructed using the extracted features to perform classification. It was shown that such a random forest
achieves similar or better classification accuracy than k-NN using IBSM.

3

Problem Statement

We adapt definitions given in earlier research [5] and describe the problem statement formally.
Definition 1. (Event interval ) Let Σ = {A, B, ...} denote a finite alphabet. A
triple e = (l, tb , tf ) is called an event interval, where l ∈ Σ is the event label and
tb , tf ∈ N, (tb < tf ) are the beginning and finishing times, respectively. We also
use e.x to denote element x of event interval e, e.g. e.tb is the beginning time of
event interval e. The duration of event interval e is d(e) = tf − tb .
Definition 2. (E-sequence) An event-interval sequence or e-sequence s = he1 , e2 ,
..., em i is a list of m event intervals placed in ascending order based on their
beginning times. If event intervals have equal beginning times, then they are
ordered lexicographically by their labels. Multiple occurrences of an event are
allowed in an e-sequence if they do not happen concurrently. The duration of an
e-sequence s is d(s) = max{e1 .tf , e2 .tf , ..., em .tf } − min{e1 .tb , e2 .tb , ..., em .tb }.
Definition 3. (E-sequence dataset) An e-sequence dataset D is a set of n esequences {s1 , ..., sn }, where each e-sequence si is associated with an unique
identifier i.
Table 1 depicts an e-sequence dataset consisting of four e-sequences with
identifiers 1 to 4.
4

Table 1: Example of an e-sequence dataset
id Event
Label
A
B
1
C
E
A
C
2
E
F
A
B
3
C
E
A
B
4 D
C
E

Beginning
Time
8
18
24
25
1
6
8
8
6
6
14
16
4
5
5
16
18

Finishing
Time
28
21
28
27
14
14
11
11
22
14
20
18
24
10
12
22
20

Event Sequence
A
B

C
E

A
C
E
F
A
B

C
E
A

B
D
C
E

Problem Statement. Given an e-sequence dataset D, where each e-sequence
is associated with a class label, the problem is to construct a representation of D
such that common feature-based classifiers are able to classify previously unseen
e-sequences similar to those in D.

4

The FIBS Framework

In this section, we introduce the FIBS framework for classifying e-sequence
datasets. Many classification algorithms require data to be in a format reminiscent of a table, where rows represent instances (e-sequences) and columns
represent features (attributes). Since an e-sequence dataset does not follow this
format, we utilize FIBS to construct feature-based representations to enable
standard classification algorithms to build predictive models.
A feature-based representation of a dataset has three components: a class
label set, a feature set, and data instances. We first give a general definition of
a feature-based representation based on these components [17].
Definition 4. (Feature-based representation) A feature-based representation
K = (C, F, X) is defined as follows. Let C = {c1 , c2 , ..., ck } be a set of k class labels, F = {f1 , f2 , ..., fz } be a set of z features (or attributes), X = {x1 , x2 , ..., xn }
be a set of n instances, and let yi ∈ C denote the class label of instance xi ∈ X.
5

In supervised settings, the set of class labels C of the classes to which esequences belong is already known. Therefore, in order to form the feature-based
representation, FIBS extracts the feature set F and the instances X from dataset
D. To define the F and X components, we consider two alternative formulations
based on relative frequency and temporal relations among the events. These
formulations are explained in the following subsections.
4.1

Relative Frequency

Definition 5. (Relative frequency) The relative frequency R(s, l) of an event
label l ∈ Σ in an e-sequence s ∈ D, which is the duration-weighted frequency
of the occurrences of l in s, is defined as the accumulated durations of all event
intervals with event label l in s divided by the duration of s. Formally:
R(s, l) =

1
d(s)

X

d(e)

(1)

e∈s ∧ e.l=l

Suppose that we want to specify a feature-based representation of an esequence dataset D = {s1 , s2 , ..., sn } using relative frequency. Let every unique
event label l ∈ Σ found in D be used as a feature, i.e., let F = Σ. Also let
every e-sequence s ∈ D be used as the basis for defining an instance x ∈ X. The
feature-values of instance x are specified as a vector containing the relative frequencies of every event label l ∈ Σ in s. Formally, X = {x1 , x2 , ..., xn } ∈ Rn×|Σ| ,
xi = hR(si , l1 ), R(si , l2 ), ..., R(si , l|Σ| )i.
Example 1. Consider the feature-based representation that is constructed based
on the relative frequency of the event labels in the e-sequence dataset shown in
Table 1. Let the class label set C be {+, −} and the feature set F be {A, B, C,
D, E, F}. Assume that the class label of each of s1 , s3 , and s4 is + and the class
label of s2 is −. Table 2 shows the resulting feature-based representation.

Table 2: Feature-based representation constructed based on relative frequency
A
1.00
1.00
1.00
1.00

4.2

B
0.15
0
0.50
0.25

C D E F Class
0.20 0 0.10 0
+
0.62 0 0.23 0.23 −
0.38 0 0.13 0
+
0.30 0.35 0.10 0
+

Temporal Relations

Thirteen possible temporal relations between pairs of intervals were nicely categorized by Allen [18]. Table 3 illustrates Allen’s temporal relations. Ignoring the
6

“equals” relation, six of the relations are inverses of the other six. We emphasize
seven temporal relations, namely, equals (q), before (b), meets (m), overlaps (o),
contains (c), starts (s), and finished-by (f), which we call the primary temporal
relations. Let set U = T ∪ I represents the thirteen temporal relation labels,
where T = {q, b, m, o, c, s, f } is the set of labels for the primary temporal relations and I = {t−1 | t ∈ T − {q}} is the set of labels for the inverse temporal
relations.

Table 3: Allen’s temporal relations between two event intervals
Primary
Inverse
Temporal Relation Temporal Relation

α equals β

β equals α

α before β

β after α

Pictorial
Example
α
β
β

α
α
β

α meets β

β met-by α
α

α overlaps β
α contains β

β

β overlapped-by α
α
β

β during α
α

β
α starts β

β startted-by α
α
β

α finished-by β

β finishes α

Exactly one of these relations holds between any ordered pair of event intervals. Some event labels may not occur in an e-sequence and some may occur
multiple times. For simplicity, we assume the first occurrence of an event label in an e-sequence is more important than the remainder of its occurrences.
Therefore, when extracting temporal relations from an e-sequence, only the first
occurrence is considered
and the rest are ignored. With this assumption, there

are at most |Σ|
possible pairs of event labels in a dataset.
2
Based on Definition 4, we now define a second
 feature-based representation,
which relies on temporal relations. Let F = Σ2 be the set of all 2-combinations
of event labels from Σ. The feature-values of instance xi are specified as a vector
containing the labels corresponding to the temporal relations between every pair
|Σ|
that occurs in an e-sequence si . In other words, X = {x1 , x2 , ..., xn } ∈ U n×( 2 ) ,
where an instance xi ∈ X represents an e-sequence si .
7

Example 2. Following Example 1, Table 4 shows the feature-based representation that is constructed based on the temporal relations between the pairs of
event labels in the e-sequences given in Table 1. To increase readability, 0 is used
instead of ∅ to indicate that no temporal relation exists between the pair.

Table 4: Feature-based representation constructed based on temporal relations
A,B A,C A,D A,E A,F B,C B,D B,E B,F C,D C,E C,F D,E D,F E,F Class
c
f
0
c
0
b
0
b
0
0
c
0
0
0
0
+
0
f
0
c
c
0
0
0
0
0
c
c
0
0
q
−
s−1 c
0
c
0
b
0
b
0
0
c
0
0
0
0
+
c
c
c
c
0
b
s
b
0 b−1 c
0 b−1 0
0
+

4.3

Feature Selection

Feature selection for classification tasks aims to select a subset of features that
are highly discriminative and thus contribute substantially to increasing the
performance of the classification. Features with less discriminative power are
undesirable since they either have little impact on the accuracy of the classification or may even harm it. As well, reducing the number of features improves
the efficiency of many algorithms.
Based on their relevance to the targeted classes, features are divided by
John et al. [19] into three disjoint categories, namely, strongly relevant, weakly
relevant, and irrelevant features. Suppose fi ∈ F and f¯i = F −{fi }. Let P (C | F )
be the probability distribution of class labels in C given the values for the features
in F . The categories of feature relevance can be formalized as follows [20].
Definition 6. (Strong relevance) A feature fi is strongly relevant iff
P (C | fi , f¯i ) 6= P (C | f¯i )

(2)

Definition 7. (Weak relevance) A feature fi is weakly relevant iff
P (C | fi , f¯i ) = P (C | f¯i ) and
∃ gi ⊂ f¯i such that P (C | fi , gi ) 6= P (C | gi )

(3)

Corollary 1. (Irrelevance) A feature fi is irrelevant iff
∀ gi ⊆ f¯i , P (C | fi , gi ) = P (C | gi )

(4)

Strong relevance indicates that a feature is indispensable and it cannot be
removed without loss of prediction accuracy. Weak relevance implies that the
feature can sometimes contribute to prediction accuracy. Features are relevant if
8

they are either strongly or weakly relevant and are irrelevant otherwise. Irrelevant
features are dispensable and can never contribute to prediction accuracy.
Feature selection is beneficial to the quality of the temporal relation representation, especially when there are many distinct event labels in the dataset.
Although any feature selection method can be used to eliminate irrelevant features, some methods have advantages for particular representations. Filter-based
selection methods are generally efficient because they assess the relevance of features by examining intrinsic properties of the data prior to applying any classification method. We propose a simple and efficient filter-based method to avoid
producing irrelevant features for the temporal relation representation.
4.4

Filter-based Feature Selection Strategy

In this section, we propose a filter-based strategy for feature reduction that can
also be used in unsupervised settings. We apply this strategy to avoid producing
irrelevant features for the temporal relation representation.
Theorem 1. An event label l is an irrelevant feature of an e-sequence dataset
D if its relative frequencies are equal in every e-sequence in dataset D.
Proof. Suppose event label l occurs with equal relative frequencies in every
e-sequence in dataset D. We construct a feature-based representation K =
(C, F, X) based on the relative frequencies of the event labels as previously described. Therefore, there exists a feature fi ∈ F that has the constant value of
v for all instances x ∈ X. We have P (C | fi ) = P (C). Therefore, P (C | fi , gi ) =
P (C | gi ). According to Corollary 1, we conclude fi is an irrelevant feature.
We provide a definition for support that is applicable to relative frequency.
If we add up the relative frequencies of event label l in all e-sequences of dataset
D and then normalize the sum, we obtain the support of l in D. Formally:
1 X
sup(D, l) =
R(s, l)
(5)
n
s∈D

where n is the number of e-sequences in D.
The support of an event label can be used as the basis of dimensionality
reduction during pre-processing for a classification task. One can identify and
discard irrelevant features (event labels) based on their supports. We will now
show how the support is used to avoid extracting irrelevant features by the
following corollary, which is an immediate consequence of Theorem 1.
Corollary 2. An event label l whose support in dataset D is 0 or 1 is an
irrelevant feature.
Proof. As with the proof of Theorem 1, assume we construct a feature-based representation K based on the relative frequency of the event labels. If sup(l, D) = 0
then, there exists a mapping feature fi ∈ F that has equal relative frequencies
(values) of 0 for all instances x ∈ X. The same argument holds if sup(l, D) = 1.
According to Theorem 1, we conclude fi is an irrelevant feature.
9

In practice, situations where the support of a feature is exactly 0 or 1 do
not often happen. Hence, we propose a heuristic strategy that discards probably
irrelevant features based on a confidence interval defined with respect to an error
threshold .
Heuristic Strategy: If sup(D, l) is not in a confidence interval [0 + , 1 − ],
then event label l is presumably an irrelevant feature in D and can be discarded.
4.5

Comparison to Representation Based on Frequent Patterns

In frequent pattern mining, the support of temporal pattern p in a dataset is the
number of instances that contain p. A pattern is frequent if its support is no less
than a predefined threshold set by user. Once frequent patterns are discovered,
after computationally expensive operations, a subset of frequent patterns are
selected as features. The representation contains binary values such that if a
selected pattern occurs in an e-sequence the value of the corresponding feature
is 1, and 0 otherwise. Example 3 illustrates a limitation of classification of IBTSs
based on frequent pattern mining where frequent patterns are irrelevant to the
class labels.
Example 3. Consider Table 1 and its feature-based representation constructed
based on relative frequency, as shown in Example 1. In this example, the most
frequent pattern is A, which has a support of 1. However, according to Corollary
2, A is an irrelevant feature and can be discarded for the purpose of classification.
For this example, a better approach is to classify the e-sequences based on the
presence or absence of F such that the occurrence of F in an e-sequence means
the e-sequence belongs to the − class and the absence of F means it belongs to
the + class.
In practice, the large number of frequent patterns affects the performance of
the approach in both the pattern discovery step and the feature selection step.
Obviously, mining patterns that are later found to be irrelevant, is useless and
computationally costly.

5

Experiments

In our experiments, we evaluate the effectiveness of the FIBS framework on
the task of classifying interval-based temporal sequences using the well-known
random forest classification algorithm on eight real world datasets. We evaluate
performance of FIBS using classifiers implemented in R version 3.6.1. The FIBS
framework was also implemented in R. All experiments were conducted on a
laptop computer with a 2.2GHz Intel Core i5 CPU and 8GB memory. We obtain
overall classification accuracy using 10-fold cross-validation. We also compare
the results for FIBS against those for two well-known methods, STIFE [15]
and IBSM [14]. In order to see the effect of the feature selection strategy, the
FIBS framework was tested with it disabled (FIBS baseline) and with its error
threshold  set to various values.
10

5.1

Datasets

Eight real-world datasets from various application domains were used to evaluate
the FIBS framework. Statistics concerning these datasets are summarized in
Table 5. More details about the datasets are as follows:

• ASL-BU [5]. Event intervals correspond to facial or gestural expressions
(e.g., head tilt right, rapid head shake, eyebrow raise, etc.) obtained from
videos of American Sign Language expressions provided by Boston University. An e-sequence expresses an utterance using sign language that belongs
to one of nine classes, such as wh-word, wh-question, verb, or noun.
• ASL-BU2 [5]. ASL-BU2 is a newer version of the ASL-BU dataset with
improvements in annotation such that new e-sequences and additional event
labels have been introduced. As above, an e-sequence expresses an utterance.
• Auslan2 [4]. The e-sequences in the Australian Sign Language dataset contain event intervals that represent words like girl or right.
• Blocks [4]. Each event interval corresponds to a visual primitive obtained
from videos of a human hand stacking colored blocks and describes which
blocks are touched as well as the actions of the hand (e.g., contacts blue,
attached hand red, etc.). Each e-sequence represents one of eight scenarios,
such as assembling a tower.
• Context [4]. Each event interval was derived from categorical and numeric
data describing the context of a mobile device carried by a person in some
situation (e.g., walking inside/outside, using elevator, etc). Each e-sequence
represents one of five scenarios, such as being on a street or at a meeting.
• Hepatitis [2]. Each event interval represents the result of medical tests (e.g,
normal, below or above the normal range, etc) during an interval. Each esequence corresponds to a series of tests over a period of 10 years that a
patient who has either Hepatitis B or Hepatitis C undergoes.
• Pioneer [4]. Event intervals were derived from the Pioneer-1 dataset available in the UCI repository corresponding to the input provided by the robot
sensors (e.g, wheels velocity, distance to object, sonar depth reading, gripper
state, etc). Each e-sequence in the dataset describes one of three scenarios:
move, turn, or grip.
• Skating [4]. Each event interval describes muscle activity and leg positions
of one of six professional In-Line Speed Skaters during controlled tests at
seven different speeds on a treadmill. Each e-sequence represents a complete
movement cycle, which identifies one of the skaters.
11

Table 5: Statistical information about datasets
Dataset # e-sequences # Event Intervals e-sequence Size
min max avg
ASL-BU
873
18,250
4 41
18
ASL-BU2
1839
2,447
4 93
23
Auslan2
200
2,447
9 20
12
Blocks
210
1,207
3 12
6
Context
240
19,355
47 149 81
Hepatitis
498
53,692
15 592 108
Pioneer
160
8,949
36 89
56
Skating
530
23,202
27 143 44

5.2

|Σ| # Classes
216
254
12
8
54
63
92
41

9
7
10
8
5
2
3
6

Performance Evaluation

We assess the classification accuracy of FIBS on the set of datasets given in
Section 5.1, which is exactly the same set of datasets considered in work on
IBSM and STIFE [14,15]. For a fair comparison and following [15], in each case,
we apply the random forest algorithm using FIBS to perform classifications. We
adopt the classification results of the IBSM and STIFE methods, as reported in
Table 5 in [15].
Table 6 shows the mean classification accuracy on the datasets when using
FIBS baseline, FIBS with the error threshold  ranging from 0.01 to 0.03 (using
the feature selection strategy defined in Section 4.4), STIFE, and IBSM. The
best performance in each row is highlighted in bold.

Table 6: Mean classification accuracy of each framework on eight datasets. The
last two rows indicate the mean and median results of each method across all
datasets.
Dataset FIBS Baseline FIBS 0.01 FIBS 0.02 FIBS 0.03
ASL-BU
94.98
89.95
90.68
88.46
ASL-BU2
94.43
92.39
92.67
93.61
Auslan2
40.50
41.00
41.00
41.00
Blocks
100
100
100
100
Context
97.83
98.76
98.34
98.34
HEPATITIS
84.54
85.14
83.55
83.94
Pioneer
100
100
100
100
Skating
96.73
97.93
98.31
98.5
Mean
Median

88.63
95.86

88.15
95.16

88.07
95.49

87.98
95.98

STIFE
91.75
87.49
47.00
100
99.58
82.13
98.12
96.98

IBSM
89.29
76.92
37.50
100
96.25
77.52
95.00
96.79

87.88 83.66
94.37 92.15

According to the Wilcoxon signed ranks tests applied across the results from
the datasets given in Table 6, each of the FIBS models has significantly higher
12

accuracy than IBSM at significance level 0.05 (not shown). Compared with the
STIFE framework, each of the FIBS models outperforms on four datasets, loses
on three datasets, and ties on Blocks dataset at 100%. The Wilcoxon signed ranks
tests do not, however, confirm which method is significantly better. Overall, the
results suggest that FIBS is a strong competitor in terms of accuracy.
5.3

Effect of Feature Selection

The same experiments to classify the datasets using the random forest algorithm,
which were given in Section 5.2, were conducted to determine the computational
cost of FIBS with and without the feature selection strategy. Fig. 2 shows the
number of features produced by the frameworks and the execution time of applying the frameworks recorded on a log scale with base 10. The error threshold
 was varied from 0.00 (baseline) to 0.03 by 0.01.

(a) Number of features

(b) Execution time (s)

Fig. 2: Effect of feature selection strategy on the eight datasets based on different
error thresholds : (a) number of generated features on a log scale with base 10,
(b) execution time (s) on a log scale with base 10.
13

As shown in Fig. 2a, applying the feature selection strategy reduces the number of features, and consequently decreases the execution time in all datasets
(Fig. 2b). In particular, due to a significant reduction in the number of irrelevant features for ASL-BU and ASL-BU2, applying the FIBS framework with
the strategy achieves over an order of magnitude speedup compared to FIBS
without the strategy. As shown by the mean classification accuracy of the models in Table 6, applying the strategy also either improves the accuracy of the
classification or does not have a significant adverse effect on it in all datasets.
This result was confirmed by the Wilcoxon signed ranks tests at significance
level 0.05 (not shown). Overall, the above results suggest that incorporating the
feature selection strategy into FIBS is beneficial.

6

Conclusion

To date, most attempts to classify interval-based temporal sequences (IBTSs)
have been performed in frameworks based on frequent pattern mining. As a simpler alternative, we propose a feature-based framework, called FIBS, for classifying IBTSs. FIBS incorporates two possible representations for features extracted
from IBTSs, one based on the relative frequency of the occurrences of event labels and the other based on the temporal relations among the event intervals.
Due to the possibility of generating too many features when using the latter representation, we proposed a heuristic feature selection strategy based on the idea
of the support for the event labels. The experimental results demonstrated that
methods implemented in the FIBS framework can achieve significantly better or
similar performance in terms of accuracy when classifying IBTSs compared to
the state-of-the-art competitors. These results provide evidence that the FIBS
framework effectively represents IBTS data for classification algorithms.
When extracting temporal relations among multiple occurrences of events
with the same label in an e-sequence, FIBS considers only the first occurrences.
In the future, the impact of temporal relations among such events could be
studied under various assumptions.

References
[1] Sheetrit, E., Nissim, N., Klimov, D., Shahar, Y.: Temporal Probabilistic
Profiles for Sepsis Prediction in the ICU. In: Proceedings of the 25th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining,
ACM (2019) 2961–2969
[2] Patel, D., Hsu, W., Lee, M.L.: Mining Relationships Among Interval-based
Events for Classification. In: Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data. SIGMOD ’08, New York,
NY, USA, ACM (2008) 393–404
[3] Moskovitch, R., Shahar, Y.: Medical Temporal-Knowledge Discovery via
Temporal Abstraction. In: AMIA Annual Symposium Proceedings, American Medical Informatics Association (2009) 452–456
14

[4] Mörchen, F., Fradkin, D.: Robust mining of time intervals with semi-interval
partial order patterns. In: Proceedings of the 2010 SIAM International
Conference on Data Mining, SIAM (2010) 315–326
[5] Papapetrou, P., Kollios, G., Sclaroff, S., Gunopulos, D.: Mining Frequent
Arrangements of Temporal Intervals. Knowledge and Information Systems
21(2) (2009) 133
[6] Liu, Y., Nie, L., Liu, L., Rosenblum, D.S.: From action to activity: Sensorbased activity recognition. Neurocomputing 181 (2016) 108–115
[7] Lin, J., Keogh, E., Wei, L., Lonardi, S.: Experiencing SAX: a novel symbolic
representation of time series. Data Mining and Knowledge Discovery 15(2)
(2007) 107–144
[8] Mörchen, F., Ultsch, A.: Optimizing time series discretization for knowledge
discovery. In: Proceedings of the eleventh ACM SIGKDD International
Conference on Knowledge Discovery in Data Mining, ACM (2005) 660–665
[9] Rabiner, L.R.: A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE 77(2) (1989) 257–286
[10] Berndt, D.J., Clifford, J.: Using dynamic time warping to find patterns in
time series. In: KDD Workshop, Seattle, WA, AAAI Press (1994) 359–370
[11] Batal, I., Valizadegan, H., Cooper, G.F., Hauskrecht, M.: A temporal pattern mining approach for classifying electronic health record data. ACM
Transactions on Intelligent Systems and Technology (TIST) 4(4) (2013) 63
[12] Moskovitch, R., Shahar, Y.: Classification-driven temporal discretization
of multivariate time series. Data Mining and Knowledge Discovery 29(4)
(2015) 871–913
[13] Kostakis, O., Papapetrou, P., Hollmén, J.: Artemis: Assessing the Similarity
of Event-Interval Sequences. In: Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, Springer (2011) 229–244
[14] Kotsifakos, A., Papapetrou, P., Athitsos, V.: IBSM: Interval-Based Sequence Matching. In: Proceedings of the 2013 SIAM International Conference on Data Mining, SIAM (2013) 596–604
[15] Bornemann, L., Lecerf, J., Papapetrou, P.: STIFE: A Framework for
Feature-Based Classification of Sequences of Temporal Intervals. In: International Conference on Discovery Science, Springer (2016) 85–100
[16] Ye, L., Keogh, E.: Time series shapelets: A new primitive for data mining.
In: Proceedings of the 15th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, ACM (2009) 947–956
[17] Tang, J., Alelyani, S., Liu, H.: Feature selection for classification: A review.
Data classification: Algorithms and applications (2014) 37
[18] Allen, J.F.: Maintaining knowledge about temporal intervals. Communications of the ACM 26(11) (1983) 832–843
[19] John, G.H., Kohavi, R., Pfleger, K.: Irrelevant features and the subset
selection problem. In: Machine Learning Proceedings 1994. Elsevier (1994)
121–129
[20] Yu, L., Liu, H.: Redundancy based feature selection for microarray data.
In: Proceedings of the Tenth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, ACM (2004) 737–742

15

