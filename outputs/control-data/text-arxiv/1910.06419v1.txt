A unified view of likelihood ratio and reparameterization gradients
and an optimal importance sampling scheme

arXiv:1910.06419v1 [cs.LG] 14 Oct 2019

Paavo Parmas∗
Okinawa Institute of Science and Technology
paavo.parmas@oist.jp

Abstract
Reparameterization (RP) and likelihood ratio
(LR) gradient estimators are used throughout
machine and reinforcement learning; however,
they are usually explained as simple mathematical tricks without providing any insight
into their nature. We use a first principles
approach to explain LR and RP, and show
a connection between the two via the divergence theorem. The theory motivated us to
derive optimal importance sampling schemes
to reduce LR gradient variance. Our newly
derived distributions have analytic probability
densities and can be directly sampled from.
The improvement for Gaussian target distributions was modest, but for other distributions
such as a Beta distribution, our method could
lead to arbitrarily large improvements, and
was crucial to obtain competitive performance
in evolution strategies experiments.

1

Introduction

Both likelihood ratio (LR) gradients (Glynn, 1990;
Williams, 1992) and reparameterization (RP) gradients (Rezende et al., 2014; Kingma and Welling, 2013)
can be used to obtain unbiased estimates of the gradient
of an expectation w.r.t. the parameters of the distribution: ddθ Ex∼p(x;θ) [φ(x)]. This problem is fundamental
in machine learning (Mohamed et al., 2019), and the
gradients are used for optimization in a wide range
of tasks (Schulman et al., 2015a; Weber et al., 2019;
Parmas, 2018), e.g. reinforcement learning (Sutton and
Barto, 1998; Schulman et al., 2015b, 2017; Sutton et al.,
2000; Peters and Schaal, 2008), stochastic variational

*Work partially performed while interning at RIKEN.
Preliminary work.

Masashi Sugiyama
RIKEN and The University of Tokyo
sugi@k.u-tokyo.ac.jp

inference (Hoffman et al., 2013) and evolutionary algorithms (Wierstra et al., 2008; Salimans et al., 2017; Ha
and Schmidhuber, 2018; Conti et al., 2018).
The LR gradient is usually
R dp(x;θ) derived as
d
=
=
φ(x)dx
dθ Ex∼p(x;θ) [φ(x)]
dθ
dp(x;θ)
R
R
d log p(x;θ)
dθ
φ(x)dx =
p (x; θ) p(x;θ) φ(x)dx = p (x; θ) dθ
h
i
d log p(x;θ)
Ex∼p(x;θ) dθ
φ(x) . On the other hand, the RP
gradient is derived by defining a mapping g() = x,
where  is sampled from a fixed simple distribution,
but x ends up being sampled from the desired distribution. For example, if x is Gaussian x ∼ N (µ, σ),
then the required mapping is g() = µ + σ, where
 ∼ N (0, 1), and the RP gradient is derived
as ddθ Ex∼p(x;θ)
[φ(x)]
= ddθ E∼N
=
h
i
h (0,1) [φ (g())]
i
E∼N (0,1)
θ = [µ, σ],

dφ(g())
dφ(g())
= E∼N (0,1) dg
,
dθ
dθ dg
dφ(g())
dφ(x)
dg
dg
= dx .
dµ = 1, dσ =  and dg

where

What do these derivations mean, and what is the relationship between the methods? We give two possible
answers to this question in Secs. 2 and 3, then explain
that the LR gradient is the unique unbiased estimator
that weights the function values φ(x), and motivate importance sampling from a different distribution q(x) to
reduce LR gradient variance. Our optimal importance
sampling scheme is reminiscent of the optimal reward
baseline for reducing LR gradient variance (Weaver
and Tao, 2001) (App. A), but our result is orthogonal,
and can be combined with such prior methods.
Further background and related work: The variance of LR and RP gradients has been of central importance in their research. Typically, RP is said to be more
accurate and scale better with the sampling dimension (Rezende et al., 2014)—this claim is also backed
by theory (Xu et al., 2019; Nesterov and Spokoiny,
2017); however, there is no guarantee that RP
outperforms LR. In particular, for multimodal φ(x)
(Gal, 2016) or chaotic systems (Parmas et al., 2018),
LR can be arbitrarily better than RP (e.g., the latter
showed that LR can be 106 more accurate in practice).

A unified view of likelihood ratio and reparameterization gradients

Moreover, RP is not directly applicable to discrete
sampling spaces, but requires continuous relaxations
(Maddison et al., 2016; Jang et al., 2016; Tucker et al.,
2017). Differentiable RP is also not always possible,
but implicit RP gradients have increased the number
of usable distributions (Figurnov et al., 2018). Techniques for variance reduction have been extensively
studied, including control variates/baselines (Greensmith et al., 2004; Grathwohl et al., 2017; Tucker et al.,
2018; Gu et al., 2015; Geffner and Domke, 2018; Gu
et al., 2016) as well as Rao-Blackwellization (Titsias
and Lázaro-Gredilla, 2015; Ciosek and Whiteson, 2018;
Asadi et al., 2017). One can also combine the best of
both LR and RP gradients by dynamically reweighting
them (Parmas et al., 2018; Metz et al., 2019). Importance sampling for reducing LR gradient variance
was previously considered in variational inference (Ruiz
et al., 2016), but they proposed to sample from the
same distribution while tuning the variance, whereas
in our work we derive an optimal distribution. In reinforcement learning, importance sampling has been
studied for sample reuse via off-policy policy evaluation (Thomas and Brunskill, 2016; Jiang and Li, 2016;
Gu et al., 2017; Munos et al., 2016; Jie and Abbeel,
2010), but modifying the policy to improve gradient
accuracy has not been considered. The flow theory
in Sec. 3 was concurrently derived by Jankowiak and
Obermeyer (2018), but their work focused on deriving
new RP gradient estimators, and they do not discuss
the duality. Our derivation is also more visual.

2

A probability “boxes” view of LR
and RP gradients

Here we give the first of our two explanations of the link
between LR and RP gradients. The explanation relies
on a first principles thinking about the effect that changing the parameters of a probability distribution θ has on
infinitesimal “boxes” of probability mass
R (Fig. 1). Both
LR and RP are trying to estimate ddθ p (x; θ) φ(x)dx.
A typical finite explanation of Riemann integrals is
performed by discretizing the P
integrand into “boxes”
N
of size ∆x, and summing: ddθ i=1 p (xi ; θ) ∆xi φ(xi ).
Taking the limit as N → ∞ recovers the true integral.
In this equation p (xi ; θ) ∆xi is the amount of probability mass inside the “box”, and φ(xi ) is the function
value inside the “box”.
Such a view can be used to explain RP gradients. In
this case, the boundaries of the “box” are fixed with
reference to the shape of the probability distribution, i.e.
for each i we define the center of the box as xi = g(i ; θ),
and the boundaries as g(i ± ∆/2; θ), where i is the
reference position on a fixed simple distribution p ().
the amount of probability mass assigned to each “box”

stays fixed at ∆pi = p () ∆ ; however, the center of
the “box” moves, so the function value φ(xi ) inside each
“box” changes by δφi = φ (g(i ; θ + δθ)) − φ (g(i ; θ)) =
φ(xi + δxi ) − φ(xi ). The full derivative can then be
PN
1
expressed as ddθ Ex∼p(x;θ) [φ(x)] ≈ δθ
i=1 ∆pi δφi =
PN
δφi δxi
i=1 ∆pi δxi δθ . Taking the infinitesimal limit as N →
∞, and noting ∆pi = p (xi ; θ) ∆xi , we obtain the RP
R
dφ(x) dx
gradient estimator p (x; θ) dx
dθ dx. We see that
RP essentially estimates the gradient by keeping the
probability mass inside each “box” fixed, but estimating
how the function value φ inside the “box” changes as
the parameters θ are perturbed.
The LR gradient, on the other hand, keeps the boundaries of the “boxes” fixed, i.e. the centre of the box
is at xi , and the boundaries at xi ± ∆xi /2. Now,
as the boundaries are independent of θ, the function value φ(xi ) inside the box stays fixed, even
as θ is perturbed by δθ; however, the probability
mass inside the box changes, because the density
changes by δpi = p (xi ; θ + δθ) − p (xi ; θ). The full
derivative can be expressed as ddθ Ex∼p(x;θ) [φ(x)] ≈
PN
PN
δpi /δθ
1
i=1 p (xi ; θ) ∆xi p(xi ;θ) φ(xi ).
i=1 ∆xi δpi φ(xi ) =
δθ
Where we have multiplied and divided by p (xi ; θ).
Taking the infinitesimal limit recovers the
 LR gradi
dp(x;θ)
dp(x;θ)
R
dθ
dθ
ent p (x; θ) p(x;θ) φ(x) dx = Ex∼p(x;θ) p(x;θ) φ(x) .
dp(x;θ)

d log p(x;θ)
The transformation p (x; θ) dθ
p(x;θ) = p (x; θ) dθ
is known as the log-derivative trick, and it may appear to be the essence behind the LR gradient, but
actually the multiplication and division by p(x; θ) is
just a special case of the more general
R Monte Carlo
integration principle. Any integral f (x) dx can be
approximated by sampling from a distribution
h
i q(x) as
R
R
f (x)
f (x)
f (x) dx = q(x) q(x) dx = Ex∼q(x) q(x) . Rather
than thinking of the LR gradient in terms of the logderivative term, it may be better
to think of it as
R dp(x;θ)
simply estimating the integral dθ
φ(x) dx by applying the appropriate importance weights to samples
from p(x; θ). Thus, we see that in the discretized
case, the LR gradient picks q(x) = p (x; θ) (Jie and
Abbeel, 2010) and performs Monte Carlo integration
PN
1
to approximate δθ
i=1 ∆xi δpi φ(xi ) by sampling from
P (xi ) = ∆xi p (xi ; θ). To summarize: LR estimates the
gradient by keeping the boundaries of the boxes fixed,
measuring the change in probability mass in each box,
and weighting by the function value: φ(xi )δp.

Sometimes, the LR gradient is described as being “kind
of like a finite difference gradient” (Salimans et al., 2017;
Mania et al., 2018), but here we see that it is a different
concept, which does not rely on fitting a straight line
between differences of φ (App. A), but estimates how
probability mass is reallocated among different φ values

A unified view of likelihood ratio and reparameterization gradients

(a) RP probability “boxes”

(b) LR probability “boxes”

Figure 1: LR keeps the boundaries of the “boxes” fixed, while RP keeps the probability mass fixed.
via Monte Carlo integration by sampling from p (x; θ).

3

A unified probability flow view of
LR and RP gradients

Here we give another explanation of LR and RP. The
appeal of this theory is that both LR and RP come out
of the same derivation, thus showing a link between the
two. In particular, we define a virtual incompressible
flow of probability mass imposed by perturbing the
parameters θ of p (x; θ), which can be used to express
the derivative of the expectation as an integral over
this flow. LR and RP estimators correspond to duals
of this integral under the divergence theorem (App. B).
The main idea resembles RP, but in addition to sampling the x location, we sample a height h for each
point: h = h p (x; θ), where h ∼ unif(0, 1), i.e., the
sampling space is extended with an additional dimension for the height x̃ = [xT , h]T , and we are uniformly
sampling in the volume under p (x; θ). The definition
of g in the introduction is extended, s.t. g̃(x , h ) = x̃.
The expectation turns into:
d
dθ

Z

p (x; θ) φ(x)dx
Z Z
d
=
p (x ) p (h ) φ (g̃(x , h )) dx dh
dθ x h
Z
=
∇x̃ φ(x̃)∇θ g̃(x , h )dV.
V

(1)

In Eq. (1), V is the volume under the curve, and
φ([xT , h]T ) := φ(x) ignores the h-component. Each
column i of ∇θ g̃(x , h ) corresponds to a vector field
induced by perturbing the ith component of θ. The
red lines in Fig. 2 show the induced flow fields for a
Gaussian distribution as the mean and variance are
perturbed. The other member of the integral, ∇x̃ φ(x̃)
is the grad of the scalar field φ(x̃). As φ does not
depend on h, the grad will always be parallel to the x
axes with magnitude dφ
dx .
According to the divergence theorem, the volume integral in Eq. (1) can be turned into a surface integral
over the boundary S (dS is a shorthand for n̂dS, where
n̂ is the surface normal vector):
Z
V

∇ · FdV =

Z
FdS.

(2)

S

In Eq. (2), F is any vector field. A common corollary
arises by picking F = φv, where φ is a scalar field,
and v is a vector field. We choose v = ∇θ g̃(x , h )δθ,
where δθ is an arbitrary perturbation in θ, so that
F = φ(x̃)∇θ g̃(x , h )δθ, in which case ∇ · F = ∇ ·
(φ(x̃)∇θ g̃(x , h )δθ) = ∇x̃ φ(x̃)∇θ g̃(x , h )δθ+φ(x̃)∇x̃ ·
∇θ g̃(x , h )δθ. Note that the term ∇θ g̃(x , h )δθ corresponds to an incompressible flow (because the probability density does not change at any point in the
augmented space). As the div of an incompressible
flow is 0, then ∇x̃ · ∇θ g̃(x , h )δθ = 0, and the second term disappears. Noting that δθ can be canceled,

A unified view of likelihood ratio and reparameterization gradients

2.00

Probability flow
Surface normals

1.75

Probability density

Probability density

2.00

1.50
1.25
1.00
0.75
0.50
0.25
0.00
−1.0

0.0

−0.5

0.5

1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
−1.0

1.0

x

(a) µ flow lines

Probability flow
Surface normals

−0.5

0.0

0.5

1.0

x

(b) σ flow lines

Figure 2: Probability flow lines when µ and σ are perturbed.
because it is arbitrary, we are left with the equation:

Z

∇x̃ φ(x̃)∇θ g̃(x , h )dV =

V

Z
φ(x̃)∇θ g̃(x , h )dS.
S

(3)

Now we explain how the left-hand side of Eq. (3) gives
rise to the RP gradient estimator, while the right-hand
side corresponds to the LR gradient estimator.
RP estimator: Consider the ∇x̃ φ(x̃)∇θ g̃(x , h )
term. As the scalar field φ(x̃) is independent of the
height location h, the component of the grad in that
direction is 0, and φ(x̃) = φ(x). As the h-component
is 0, then the value of g̃ in the h-direction is multiplied by 0, and is irrelevant for the product, so
∇x̃ φ(x̃)∇θ g̃(x , h ) = ∇x φ(x)∇θ g(x ), which is just
the term used in the RP estimator. Hence, the lefthand side of Eq. (3) corresponds to the RP gradient.
LR estimator: R We will show that the LR estimator
tries to integrate S φ(x̃)∇θ g̃(x , h )dS. To do so, note
that dS = n̂dS. It is necessary to express the normalized surface vector n̂, and then perform the integral
over the surface. The derivation is in App. B.2, and
the final result is:

Z

Z
φ(x̃)∇θ g̃(x , h ) dS =

S

φ(x)
X

dp (x; θ)
dx.
dθ

(4)

We have already seen that a Monte Carlo integration
of the right-hand side of Eq. (4) using samples from
p (x; θ) yields the LR gradient estimator. Thus, the
RP and LR are duals under the divergence theorem.
To further strengthen this claim we prove that the LR
gradient estimator is the unique estimator that takes
weighted averages of the function values φ(x).
Theorem 1 (Uniqueness of LR estimator)
ψ(x) = p (x; θ) ddθlog p(x;θ) is the unique function ψ(x),
R
R
s.t. ψ(x)φ(x)dx = ddθ p (x; θ) φ(x)dx for any φ(x).
Proof.
Suppose that
R
R there exist ψ(x) and f (x), s.t.
φ(x)ψ(x) dx = φ(x)f
R (x) dx for any φ(x). Rearrange the equation into φ(x) (ψ(x) − f (x)) dx = 0,
then
pick φ(x) = ψ(x) − f (x) from which we get
R
2
(ψ(x) − f (x)) dx = 0. Therefore, ψ = f . Q.E.D.
We see that Eq. (4) was immediately clear without
having to go through the derivation. The same analysis does not work for RP (App. C). Indeed, there are
infinitely many RP gradients (Jankowiak and Obermeyer, 2018). Moreover, the analysis does not consider
coupled sampling of x (Walder et al., 2019).

4

Slice ratio importance sampling

As LR is the only unbiased gradient estimator that
weights samples of φ(x) (Sec. 3), what could be done
to reduce its variance? One underexplored option is to
keep the product p (x; θ) ddθlog p(x;θ) the same, but sam-

A unified view of likelihood ratio and reparameterization gradients

ple from a different distribution q(x) using importance
d log p(x;θ) 1
sampling q(x)p (x; θ) dθ
/q(x). How to pick q(x)?
Our first attempt (App. D, Fig. 3b) was suboptimal.
Optimal importance sampling for minimum gradient variance: We seek a distribution q(x), s.t. the
variance of dp(x;θ)
φ(x)/q(x) is minimized. The derivadθ
tion is analogous to the standard result for optimal
importance sampling in statistics (Owen, 2013). As
φ(x) is not known a priori, we minimize the variance of dp(x;θ)
/q(x). The omission is well-justified
dθ
in the multidimensional setting, as most of the variation in φ(x) is caused by the other dimensions and
can thus be viewed as noise. See also App. F.2 for
several other justifications. The variance can be ex
2
dp(x;θ)
R
dθ
dx. Adding in the conpressed as q(x)
q(x)
R
straint q(x) dx = 1 with a Lagrange multiplier λ,
and performing a variational optimization by setting
the derivative w.r.t. q to 0 we have:

−

dp(x;θ)
dθ

q(x)

!2
+ λ = 0 ⇒ q(x) =

dp (x; θ) √
/ λ.
dθ
(5)

Eq. (5) tells that the optimal importance sampling
distribution is proportional to the magnitude of the
gradient of the base distribution. How to normalize
this distribution, and how to sample from it?
For a Gaussian distribution, we can derive two possible distributions: one for µ (Fig. 3c) and one for
σ (Fig. 3d). The derivative w.r.t µ appears more
important, so we derive it first. Note that dp(x)
=
dµ
− dp(x)
, and that by sampling a height h and transdx
forming from the h-coordinate to the x-coordinate
via x = p−1 (h), the probability density is weighted:
dh =

dp(x)
dx

dx (App. D). This insight allows us to de-

rive the distribution and a sampling method (App. E.2).
Namely, to sample from the distribution: 1) sample
h ∼ unif(0, pmax ), where pmax = p (µ; µ, σ) is the peak
probability density, 2) compute the location of the edge
of the slice x = p−1 (h) (Fig. 3a). Putting these results
together, one obtains the pdf, a sampling method and
the LR gradient estimator:


|x − µ|
−(x − µ)2
pB (x; µ, σ) =
exp
,
2σ 2
2σ 2
p
x = µ ± σ −2 log(h ) where h ∼ unif(0, 1),


d
2
√
Ex∼p(x) [φ(x)] = Ex∼q(x) sgn(x − µ)
φ(x) .
dµ
σ 2π
(6)

We call the derived distribution the B-distribution
(Fig. 3c), and the resulting gradient estimator the slice
ratio gradient (SLRG). Notice that the B-distribution
is the Rayleigh distribution symmetrized about the
origin. The derivation for σ is similar (App. E.4), but
2
note, dp(x)
= σ ddxp(x)
for a Gaussian.
2
dσ
Slice ratio sampling for the symmetric Beta distribution: The slice ratio sampling method is crucial
in some situations. For example, consider a distribution, such as the symmetric Beta distribution:

pβ (x) =

xα−1 (1 − x)α−1
.
B(α, α)

(7)

When α tends to 1 from above, this distribution tends
to the uniform distribution between 0 and 1. Consider
a distribution with the same shape, but where the
mean is shifted, s.t. it is symmetric about a parameter
µ, instead of x = 1/2. In this case, as α tends to 1,
the variance of the gradient w.r.t. µ will tend to ∞,
dp (x)
because dµβ
is around 0 in most of the sampling
range, but very large at the edges of the distribution.
We derived the optimal pdf, sampling method and
gradient estimator (App. E.3):

α−1
(x − x2 )α−2 |1 − 2x|,
2 × 0.25α−1
q
1/(α−1)
x = 0.5 ± 0.5 1 − h
where h ∼ unif(0, 1),

pβR (x) =

d
Ex∼pβ (x) [φ(x)]
dµ


2 × 0.25α−1
φ(x) , for α > 1.
= Ex∼q(x) sgn(x − 0.5)
B(α, α)
(8)

For a shifted, stretched and centered distribution, replace x with k(x − 0.5) + µ, and the gradient estimator
needs to be√scaled down by k. To obtain a variance σ 2 ,
set k to 2σ 2α + 1.
Multidimensional case: For a Gaussian p (x; θ), as
the dimension increases, the optimal q(x) tends to the
original distribution (App. E.5). For this reason, we
propose to sample each dimension separately from the
B-distribution, potentially allowing for a bias, but while
reducing the variance of the gradient estimator (see
also App. F.1 for more justification). In general, we
believe that such a technique will be necessary for other
distributions as well if the dimension grows high. To
see this, consider the importance weighted likelihood
ratio gradient estimator for a factorized distribution

A unified view of likelihood ratio and reparameterization gradients

0.2

0.1

−4

−2

0

2

4

0.4

Probability density

0.3

0.0

(a) Slice ratio sampling

0.4

Probability density

Probability density

0.4

0.3

0.2

0.1

0.0

x

(b) L-distribution

−4

−2

0

2

4

x

(c) B-distribution

0.3

0.2

0.1

0.0

−4

−2

0

2

4

x

(d) W-distribution

Figure 3: Slice ratio sampling method Fig. 3a, and new importance sampling distributions to reduce LR gradient
variance (Figs. 3b,3c,3d). The shaded regions are histograms generated with the direct sampling methods, and
the solid lines are the analytic densities. Both methods match, demonstrating the correctness of the derivations.
d
B and W distributions are optimal for ddµ and dσ
respectively.
p (x; θ) =

Q

i

pi (xi ; θi ):

d
Ex∼p(x) [φ(x)]
dθi


p (x) d log p
= Ex∼q(x)
φ(x)
q(x) dθi
# (9)
"

p\i x\i pi (xi ) d log pi (xi )
φ(x) ,
= Ex∼q(x)
q\i (x\i )qi (xi ) dθi
Y
where p\i is
pj (xj ; θj ) .

The variance of qi may be larger than the variance
of pi , and this could manifest as a larger variance of
φ(x), which would act as additional noise on the other
dimensions j 6= i. Our proposed solution is to optimize
the reduction in gradient variance while constraining
the variance of q. Assuming the mean µ = 0, this can
be performed using a variational optimization
with an
R
additional Lagrange multiplier for q(x)x2 dx = kσ 2
analogously to Eq. (5). The general equation is

j6=i

While q can be modified to reduce the variance of
pi (xi ) d log pi (xi )
, this will increase the variance of the
qi (xi ) dθ
p\j (x\j )
q\j (x\j ) terms for j 6= i. If each qj is modified, then the
variance of these terms grows exponentially with the
dimension, and any decrease in variance from having
modified qi becomes negligible. Our proposed solution
p\j (x\j )
is to replace q\j (x\j ) with its expected value, which is 1.
Note that our technique is not just a convenience, but it
is a necessity. In practice, such a scheme may introduce
a small bias, but drastically reduce the variance. Next
we show some fairly general conditions under which
this method still gives an unbiased gradient estimator.
Sufficient conditions for an unbiased gradient
estimator in high dimensions with our scheme:
PD
1. If φ(x) = i=1 φi (xi ), then our estimation scheme
is unbiased.
2. If φ(x) is quadratic, then our estimation is scheme
is unbiased.
Both conditions are independently sufficient for unbiasedness (derivations in App. E.6).
Effect of greater variance of qi : Lastly, we point
toward another issue with modifying qi in Eq. (9).

q(x) =

dp (x; θ) p
/ λ1 + λ2 x2 .
dθ

(10)

For a Gaussian p (x; θ), this equation can be solved
(App. G). We call the result the truncated ratio gradient (TRRG). The pdf, sampling method and gradient
estimator are below:

ptr (x; c, µ, σ)
2

=

exp(− c2 )
1
|x − µ|
(x − µ)2
√ p
exp(−
)
1 − Φ(c) σ2 2π (x − µ)2 + σ 2 c2
2σ 2

where Φ(c) is the cdf of a unit normal distribution,
p
x = µ ± σ 2c − c2 where c ∼ truncG(c, ∞), and

truncG(a, b) is the unit normal truncated1 between [a, b]
d
Ex∼p(x) [φ(x)]
dµ
"
#
2c 1 − Φ(c)
= Ex∼q(x) sgn(x − µ)
φ(x) .
σ exp(− c22 )
(11)
This distribution interpolates between a Gaussian distribution and the B-distribution. The interpolation is
controlled by the c parameter: for c = 0 the distribu1
By truncated we mean that the probability density is
set to 0 outside these bounds, and the remaining probability distribution is renormalized. Such a distribution is
implemented e.g. in MATLAB and scipy (Jones et al., 01 ).

A unified view of likelihood ratio and reparameterization gradients

tion is Gaussian, and for c → ∞ the distribution tends
to the B-distribution. One half of the distribution is
plotted in Fig. 4b for several values of c, and Fig. 4a
shows how the accuracy of dp
dµ /q, and the variance of
the distribution scale with c (we name these functions
t(c) and v(c) respectively). These functions were computed analytically (App. G). How should one pick the
parameter c? A simple choice may be to pick c around
0.5, where the accuracy starts increasing slower than
the variance of the distribution. But is there a more
principled method based on the dimensionality?
We give a short analysis of the effect of the variance
and guidelines for picking c. For example, consider the
case when φ is linear with slope a in every dimension,
the dimensionality is D and the variance is scaled by
vc , then the variance of φ would increase by a factor
vc to a2 σ 2 Dvc . The noise from the other dimensions
would scale as roughly vc (D − 1)a2 σ 2 . However, the
increase in accuracy tc counteracts this increase in
noise, and the gradient variance of this noise scales as
(vc/tc )(D − 1)a2 σ 2 . Now if we assume that the gradient
signal has a variance around a2 σ 2 , and we want to guarantee that the additional gradient noise from the other
dimensions does not exceed the maximum decrease in
variance, then we could pick c s.t. (vc/tc −1)(D −1) ≈ 1.
In Tab. 1, we show several values of 1/(vc/tc −1) and the
expected increase in accuracy tc , which can be used as
a guideline for picking an appropriate c for the dimensionality of your problem. We could also estimate the
reduction in gradient signal variance as (1 − 1/tc )a2 σ 2
for a more conservative estimate of D, but in practice,
the reduction in gradient signal variance is greater than
1/tc because of structure in φ. In general, for deterministic problems it may be better to be conservative and
aim for a smaller increase in accuracy with a smaller c,
whereas if φ is stochastic, then the additional variance
from other dimensions may be negligible and higher c
values can be used.

5

Experiments to verify theory

We performed experiments on a quadratic φ(x) to verify the theory. In App. H we also evaluate our methods
in evolution strategies experiments in reinforcement
learning, but as our work proved that importance sampling for Gaussian base distributions can only lead to
modest gains at best, it is difficult to obtain statistically
significant results. On the other hand, our method was
crucial for obtaining competitive results using a Beta
distribution, as emphasized by our experiment here.

case when Gaussian noise σn2 = 1 is added on φ. We
vary the dimension between 1–1000, and plot the variance of the gradient estimators: GLR—LR gradient
with a Gaussian p(x); SLRG—slice ratio gradient with
a Gaussian p(x); TRRG—truncated ratio gradient with
c = 0.5; BRG—slice ratio gradient with a Beta p(x),
and α = 1.5, plotted in Fig. 4c; BLR—LR gradient
with a Beta p(x), and α = 1.5. The mean of the distributions was set to 0 and the variance was set
√ to 1 (the
Beta distributions were stretched by k = 2σ 2α + 1 to
achieve this). We used antithetic sampling, so that the
effect of any baseline could be ignored. The gradient
was estimated by averaging 100 samples, and this was
repeated for a large number of times to estimate the
variance of the gradient estimator. Bootstrapping was
used to obtain confidence intervals. The results are
plotted in Fig. 5.
Results and analysis: The main result is that using
the slice ratio method, the gradient accuracy for the
Beta distribution could be increased by 100–1000 times
(compare BRG to BLR), showing that our method
is necessary for some non-Gaussian distributions. In
general, the increase in gradient accuracy would tend to
∞ as the α parameter tends to 1 from above; however,
even for moderately curved cases, such as α = 1.5
(Fig. 4c) the improvement in accuracy can be drastic.
The results confirm our theoretical analysis: in the
deterministic case, the SLRG method outperforms the
standard GLR method, but as the dimensionality is
increased, this reverses; whereas in the high-noise case,
SLRG always outperforms GLR. In the noisy case, the
gradient variances at D = 1000 are GLR: 10.10 ± 0.05,
SLRG: 6.46±0.03, TRRG: 7.73±0.04, BRG: 4.14±0.02
(the errorbars correspond to 1 standard deviation).
The ratios 10.10/6.46 = 1.563 and 10.10/7.73 = 1.307
match the theoretical improvements in gradient accuracy for the SLRG gradient at large c in Fig. 4a and for
the TRRG gradient at c = 0.5 in Tab. 1. In the deterministic case, the gradient variances at D = 1000 are
GLR: 0.0803 ± 0.0004, SLRG: 0.1015 ± 0.0005, TRRG:
0.0815 ± 0.0004, BRG: 0.0864 ± 0.0004, showing that
TRRG is more robust than SLRG to problems arising from increasing the dimension, while it still allows
reducing the variance in the stochastic φ setting. Interestingly, BRG achieved a lower gradient variance than
SLRG in the deterministic setting, and was overall the
best in the stochastic setting even though the variances
of the base distributions were the same.

6
Setup: φ(x) is a quadratic (x − a)T Q(x − a), where
a = 1 and Q = ones(D, D)/D2 is a matrix of ones,
which is scaled, such that φ(x) remains constant at
x = 0. We evaluate a deterministic case, as well as a

Conclusions

We have introduced a new unified theory of LR and RP
gradients. The theory explained that the sampling distribution q(x) for LR gradients is a separate matter to

A unified view of likelihood ratio and reparameterization gradients
0.8

1.8
1.6
1.4
1.2
1.0

Increase in variance of distribution
Increase in gradient accuracy
0

1

2

3

4

0.6
0.5
0.4
0.3
0.2
0.1
0.0

5

5

0.7

Probability density

Probability density

Ratio to base value

2.0

0

1

2

Offset parameter c

3

4

5

Beta slice ratio distribution
Beta distribution

4
3
2
1
0
0.0

0.2

0.4

x

(a) t(c) in teal and v(c) in red

(b)

0.6

0.8

1.0

x

(c)

c ∈ [0.01, 0.1, 0.3, 0.5, 1.0, 2.0, 5.0]

α = 1.5

Figure 4: Scaling of truncated ratio gradient accuracy with the dimension (4a), truncated ratio distribution for
various c (4b) and the slice ratio distribution for the Beta distribution (4c).
Table 1: Guidelines for choosing the offset parameter c for the truncated ratio gradient.
0.1
4523
1.076

0.2
676
1.144

0.4
119
1.257

101
GLR
SLRG
TRRG
BRG
BLR

0

10−1
10−2

0.5
71
1.302

0.6
48
1.341

0.8
27
1.402

1.0
19
1.447

GLR
SLRG
TRRG
BRG
BLR

103

102

10

0.3
238
1.204

Variance of gradient

Variance of gradient

Suggested parameter c
Dimension (D − 1)
Exp. increase in accuracy t

102
101
100
10−1
10−2

10

0

10

1

10

2

10

3

# of dimensions
(a) Deterministic

100

101

102

103

# of dimensions
(b) Noisy

Figure 5: Scaling of gradient estimator variance on a quadratic problem with deterministic and noisy function
evaluations. The confidence intervals correspond to one standard deviation of the estimate.
the distribution p(x; θ) used to compute the objective
function Ex∼p(x;θ) [φ(x)], and motivated us to search for
the optimal importance sampling distribution q(x) to
reduce gradient variance. We derived these importance
sampling distributions together with sampling methods
for them for Gaussian and Beta objective distributions
p (x; θ) to reduce the variance of the gradient w.r.t. a
mean shifting parameter of the distribution. Optimal
sampling for other gradients is left for future work. We
further analyzed the scalability with the dimension of
the sampling space. Gaussian distributions are widely
used in the literature, and we found that our method is
able to provide a modest improvement in gradient accu-

racy. On the other hand, for distributions with a “flat
top”, which have found less use, our method can drastically improve the accuracy, and is crucial for obtaining
good results. Which objective distributions outperform
Gaussians in which situations is a substantial research
topic: e.g. clipped distributions (Fujita and Maeda,
2018), Beta distributions (Chou et al., 2017), exponential family distributions (Eisenach and Yang, 2019) or
normalizing flows (Tang and Agrawal, 2018; Mazoure
et al., 2019) have been considered, but they did not
importance sample from q(x). Our slice ratio gradients
will be essential to obtain a fair comparison between
different p (x; θ).

A unified view of likelihood ratio and reparameterization gradients

Acknowledgments
PP was supported by OIST Graduate School funding and by RIKEN. MS was supported by KAKENHI
17H00757.

References
Asadi, K., Allen, C., Roderick, M., Mohamed, A.-r.,
Konidaris, G., and Littman, M. (2017). Mean actor
critic. stat, 1050:1.
Chou, P.-W., Maturana, D., and Scherer, S. (2017).
Improving stochastic policy gradients in continuous
control with deep reinforcement learning using the
beta distribution. In International Conference on
Machine Learning, pages 834–843.
Ciosek, K. and Whiteson, S. (2018). Expected policy
gradients. In Thirty-Second AAAI Conference on
Artificial Intelligence.
Conti, E., Madhavan, V., Such, F. P., Lehman, J., Stanley, K., and Clune, J. (2018). Improving exploration
in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. In
Advances in Neural Information Processing Systems,
pages 5027–5038.
Corless, R. M., Gonnet, G. H., Hare, D. E., Jeffrey,
D. J., and Knuth, D. E. (1996). On the Lambert W
function. Advances in Computational mathematics,
5(1):329–359.
Eisenach, C. and Yang, Z. (2019). Natural policy
gradient for exponential families.
Figurnov, M., Mohamed, S., and Mnih, A. (2018).
Implicit reparameterization gradients. In Advances
in Neural Information Processing Systems, pages
441–452.
Fujita, Y. and Maeda, S.-i. (2018). Clipped action policy gradient. In International Conference on Machine
Learning, pages 1592–1601.
Gal, Y. (2016). Uncertainty in deep learning. PhD
thesis, PhD thesis, University of Cambridge.
Geffner, T. and Domke, J. (2018). Using large ensembles of control variates for variational inference. In
Advances in Neural Information Processing Systems,
pages 9960–9970.
Glynn, P. W. (1990). Likelihood ratio gradient estimation for stochastic systems. Communications of the
ACM, 33(10):75–84.
Grathwohl, W., Choi, D., Wu, Y., Roeder, G., and
Duvenaud, D. (2017). Backpropagation through the
void: Optimizing control variates for black-box gradient estimation. arXiv preprint arXiv:1711.00123.

Greensmith, E., Bartlett, P. L., and Baxter, J. (2004).
Variance reduction techniques for gradient estimates
in reinforcement learning. Journal of Machine Learning Research, 5(Nov):1471–1530.
Gu, S., Levine, S., Sutskever, I., and Mnih, A. (2015).
MuProp: Unbiased backpropagation for stochastic
neural networks. arXiv preprint arXiv:1511.05176.
Gu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E.,
and Levine, S. (2016). Q-prop: Sample-efficient policy gradient with an off-policy critic. arXiv preprint
arXiv:1611.02247.
Gu, S. S., Lillicrap, T., Turner, R. E., Ghahramani, Z.,
Schölkopf, B., and Levine, S. (2017). Interpolated
policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforcement learning. In
Advances in Neural Information Processing Systems,
pages 3846–3855.
Ha, D. (2017).
blog.otoro.net.

Evolving

stable

strategies.

Ha, D. and Schmidhuber, J. (2018). Recurrent world
models facilitate policy evolution. In Advances in
Neural Information Processing Systems, pages 2450–
2462.
Hoffman, M. D., Blei, D. M., Wang, C., and Paisley, J.
(2013). Stochastic variational inference. The Journal
of Machine Learning Research, 14(1):1303–1347.
Jang, E., Gu, S., and Poole, B. (2016). Categorical
reparameterization with Gumbel-Softmax. arXiv
preprint arXiv:1611.01144.
Jankowiak, M. and Obermeyer, F. (2018). Pathwise derivatives beyond the reparameterization trick.
In International Conference on Machine Learning,
pages 2240–2249.
Jiang, N. and Li, L. (2016). Doubly robust off-policy
value evaluation for reinforcement learning. In International Conference on Machine Learning, pages
652–661.
Jie, T. and Abbeel, P. (2010). On a connection between importance sampling and the likelihood ratio
policy gradient. In Advances in Neural Information
Processing Systems, pages 1000–1008.
Jones, E., Oliphant, T., Peterson, P., et al. (2001–
). SciPy: Open source scientific tools for Python.
[Online; accessed May, 2019].
Kingma, D. P. and Welling, M. (2013). Auto-encoding
variational Bayes. arXiv preprint arXiv:1312.6114.
Maddison, C. J., Mnih, A., and Teh, Y. W. (2016).
The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint
arXiv:1611.00712.

A unified view of likelihood ratio and reparameterization gradients

Mania, H., Guy, A., and Recht, B. (2018). Simple
random search of static linear policies is competitive
for reinforcement learning. In Advances in Neural
Information Processing Systems, pages 1800–1809.

Schulman, J., Heess, N., Weber, T., and Abbeel, P.
(2015a). Gradient estimation using stochastic computation graphs. In Advances in Neural Information
Processing Systems, pages 3528–3536.

Mazoure, B., Doan, T., Durand, A., Hjelm, R. D.,
and Pineau, J. (2019). Leveraging exploration in
off-policy algorithms via normalizing flows. arXiv
preprint arXiv:1905.06893.

Schulman, J., Levine, S., Abbeel, P., Jordan, M., and
Moritz, P. (2015b). Trust region policy optimization.
In International Conference on Machine Learning,
pages 1889–1897.

Metz, L., Maheswaranathan, N., Nixon, J., Freeman,
C. D., and Sohl-Dickstein, J. (2019). Understanding
and correcting pathologies in the training of learned
optimizers. In International Conference on Machine
Learning.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A.,
and Klimov, O. (2017). Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347.

Mohamed, S., Rosca, M., Figurnov, M., and Mnih, A.
(2019). Monte carlo gradient estimation in machine
learning. arXiv preprint arXiv:1906.10652.
Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M. (2016). Safe and efficient off-policy reinforcement learning. In Advances in Neural Information Processing Systems, pages 1054–1062.

Sutton, R. S. and Barto, A. G. (1998). Reinforcement
learning: An introduction, volume 1. MIT press
Cambridge.
Sutton, R. S., McAllester, D. A., Singh, S. P., and
Mansour, Y. (2000). Policy gradient methods for reinforcement learning with function approximation. In
Advances in neural information processing systems,
pages 1057–1063.

Neal, R. M. (2003). Slice sampling. The annals of
statistics, 31(3):705–767.

Tang, Y. and Agrawal, S. (2018). Boosting trust region policy optimization by normalizing flows policy.
arXiv preprint arXiv:1809.10326.

Nesterov, Y. and Spokoiny, V. (2017). Random
gradient-free minimization of convex functions. Foundations of Computational Mathematics, 17(2):527–
566.

Thomas, P. and Brunskill, E. (2016). Data-efficient offpolicy policy evaluation for reinforcement learning.
In International Conference on Machine Learning,
pages 2139–2148.

Owen, A. B. (2013). Monte Carlo theory, methods and
examples.

Titsias, M. K. and Lázaro-Gredilla, M. (2015). Local
expectation gradients for black box variational inference. In Advances in neural information processing
systems, pages 2638–2646.

Parmas, P. (2018). Total stochastic gradient algorithms
and applications in reinforcement learning. In Advances in Neural Information Processing Systems,
pages 10204–10214.
Parmas, P., Rasmussen, C. E., Peters, J., and Doya, K.
(2018). PIPPS: Flexible model-based policy search
robust to the curse of chaos. In International Conference on Machine Learning, pages 4062–4071.
Peters, J. and Schaal, S. (2008). Reinforcement learning
of motor skills with policy gradients. Neural networks,
21(4):682–697.
Rezende, D. J., Mohamed, S., and Wierstra, D. (2014).
Stochastic backpropagation and approximate inference in deep generative models. In International
Conference on Machine Learning, pages 1278–1286.
Ruiz, F., Titsias, M., and Blei, D. (2016). Overdispersed black-box variational inference. In 32nd Conference on Uncertainty in Artificial Intelligence 2016,
UAI 2016, pages 647–656.
Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever,
I. (2017). Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint
arXiv:1703.03864.

Tucker, G., Bhupatiraju, S., Gu, S., Turner, R.,
Ghahramani, Z., and Levine, S. (2018). The mirage of action-dependent baselines in reinforcement
learning. In International Conference on Machine
Learning, pages 5022–5031.
Tucker, G., Mnih, A., Maddison, C. J., Lawson, J., and
Sohl-Dickstein, J. (2017). REBAR: Low-variance,
unbiased gradient estimates for discrete latent variable models. In Advances in Neural Information
Processing Systems, pages 2627–2636.
Walder, C. J., Nock, R., Ong, C. S., and Sugiyama,
M. (2019). New tricks for estimating gradients of
expectations. arXiv preprint arXiv:1901.11311.
Weaver, L. and Tao, N. (2001). The optimal reward
baseline for gradient-based reinforcement learning. In
Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, pages 538–545. Morgan Kaufmann Publishers Inc.
Weber, T., Heess, N., Buesing, L., and Silver, D. (2019).
Credit assignment techniques in stochastic computation graphs. In The 22nd International Conference

A unified view of likelihood ratio and reparameterization gradients

on Artificial Intelligence and Statistics, pages 2650–
2660.
Wierstra, D., Schaul, T., Peters, J., and Schmidhuber,
J. (2008). Natural evolution strategies. In 2008 IEEE
Congress on Evolutionary Computation (IEEE World
Congress on Computational Intelligence), pages 3381–
3387. IEEE.
Williams, R. J. (1992). Simple statistical gradientfollowing algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229–256.
Xu, M., Quiroz, M., Kohn, R., and Sisson, S. A. (2019).
Variance reduction properties of the reparameterization trick. In International Conference on Artificial
Intelligence and Statistics.

A unified view of likelihood ratio and reparameterization gradients

Appendix A

Likelihood ratio gradient basics

The likelihood ratio (LR) gradient estimator is given by


d
d log p (x; θ)
Ex∼p(x;θ) [φ(x)] = Ex∼p(x;θ)
φ(x) .
dθ
dθ

(12)

1
(x − µ)2
log p (x; θ) = − log(2π) − log(σ) −
,
2
2σ 2

d log p (x; θ)
x−µ
= ,
=
dµ
σ2
σ
1
(x − µ)2
2
1
d log p (x; θ)
− =
=
− ,
3
dσ
σ
σ
σ
σ
where x = µ + σ and  ∼ N (0, 1).

(13)

For a Gaussian p (x; θ):

For a Beta p (x; θ):

p (x; θ) =

(x − x2 )α−1
for x ∈ [0, 1]
B(α, α)

log p (x; θ) = − log (B(α, α)) + (α − 1) log(x − x2 ),

d log p (x; θ)
α−1
=
(1 − 2x),
dx
x − x2
d log p (x; θ)
d log p (x; θ)
=−
,
dµ
dx
where µ is a shifting parameter for the mean.

In practice, we sample an β = x − 0.5 ⇒ x = β + 0.5, then the gradient estimator becomes:

(14)

2β (α−1) 1
,
0.25−2β k

where

the additional k factor comes if a stretching is applied: z = µ + kβ .

Baselines to reduce gradient variance: The LR gradient estimator on its own has a large variance, and
techniques have to be used to stabilize it. A common technique is to subtract a constant baseline b from the φ(x)
values, so that the gradient estimator becomes


d
d log p (x; θ)
Ex∼p(x;θ)
(φ(x) − b) .
dθ
dθ

(15)

In practice, using b = Ex∼p(x;θ) [φ(x)] works well, but one can also derive an optimal baseline (Weaver and Tao,
2001). We outline the derivation below. The gradient variance when a baseline is used can be expressed as


Vx∼p(x;θ)

"

2 #
d log p (x; θ)
d log p (x; θ)
(φ(x) − b) = Ex∼p(x;θ)
φ(x)
dθ
dθ
"
#
"
2
2 #
d log p (x; θ)
d log p (x; θ)
− 2Ex∼p(x;θ)
φ(x)b + Ex∼p(x;θ)
b
.
dθ
dθ
(16)

Taking the derivative of Eq. (16) w.r.t. b and setting to zero gives the optimal baseline as

A unified view of likelihood ratio and reparameterization gradients


Ex∼p(x;θ)
bopt =

d log p(x;θ)
dθ


Ex∼p(x;θ)

2

d log p(x;θ)
dθ


φ(x)
  .

(17)

2

In practice, for example if φ(x) is linear and p (x; θ) is Gaussian then bopt = Ex∼p(x;θ) [φ(x)], so the gain from
trying to use an optimal baseline is often small. What would happen to the optimal baseline derivation for our
importance sampling case (Sec. 4)? The sampling distribution has to be swapped with q(x), and ddθlog p(x;θ) has to
dp(x;θ)
be swapped with dθ
/q(x), giving the optimal baseline (Jie and Abbeel, 2010) as

Ex∼q(x)
bopt =

dp(x;θ)
dθ


Ex∼q(x)

Note that if the slice ratio distribution is used, then q =

2

/q(x)

dp(x;θ)
dθ


φ(x)
2  .

(18)

/q(x)

dp(x;θ)
dθ

√
/ λ, and bopt = Ex∼q(x) [φ(x)].

Antithetic sampling: An often used technique is to sample points x in pairs opposite to each other, s.t.
x+ = µ + σ and x− = µ − σ. This technique is particularly often used in evolution strategies’ research (Salimans
et al., 2017; Mania et al., 2018). We will explain that when this technique is used, then a baseline has no effect
because it cancels. Thus, using antithetic sampling allows us to disentangle any effect of the baselines from
the effect of the importance sampling, which is why we use it in all of our experiments. The derivation is easy
to see by considering that for a Gaussian: ddµlog p(x;θ) = σ , so ddµlog p(x+ ;θ) (φ(x+ ) − b) + ddµlog p(x− ;θ) (φ(x− ) − b) =

σ

(φ(x+ ) − b − (φ(x− ) − b)) =


σ

(φ(x+ ) − φ(x− )). In general, this result holds for any symmetric

dp(x;θ)
dθ

/q(x).

Relationship to finite difference methods: Finite difference methods also use the function values φ(x) to
estimate a derivative, so it may appear that the LR gradient estimator is a finite difference estimator. Finite
difference estimators work by estimating the slope of the function, by evaluating the change between two points,
i.e.
φ(x+ ) − φ(x− )
dφ(x)
≈
.
(19)
dx
∆x
In the antithetic sampling case, ∆x = 2σ, so the estimator is
dφ(x)
φ(x+ ) − φ(x− )
≈
.
dx
2σ

(20)

Clearly, this is different to the LR gradient estimator
 (φ(x+ ) − φ(x− ))
,
2σ

(21)

because the  is in the wrong place. In Sec. 2 we explain that the LR gradient estimator is a different concept to
finite differences, which is not trying to fit a linear function onto φ(x).

Appendix B

Derivations for the probability flow theory

Here we illustrate the background information in 3 dimensions, but it generalizes straightforwardly to higher
dimensions.
Notation:
F = [Fx (x, y, z), Fy (x, y, z), Fz (x, y, z)] is a vector field.
φ(x, y, z) is a scalar field (a scalar function)
∂Fy
∂Fz
x
Div operator: ∇ · F = ∂F
∂x + ∂y + ∂z .
∂φ ∂φ
Grad operator: ∇φ = [ ∂φ
∂x , ∂y , ∂z ].

A unified view of likelihood ratio and reparameterization gradients

Figure 6: Illustration of the divergence theorem.
B.1

Basic vector calculus and fluid mechanics

The vector field F could be for example thought of as a local flow velocity for some fluid. If F is the density flow
rate, then the div operator essentially measures how much the density is decreasing at a point. If the outflow is
larger than the inflow, the density would decrease and vice versa. The divergence theorem, illustrated in Fig. 6
illustrates how this change in density can be measured in two separate ways: one could integrate the divergence
across the volume, or one could integrate the in and and outflow across the surface. The divergence theorem
states:
Z
V

∇ · FdV =

Z
S

F · dS

(22)
∂F

y
x
To prove the claim, consider the infinitesimal box in Fig. 6. The divergence can be calculated as δxδy( ∂F
∂x + ∂y ).
On the other hand, to take the integral across the surface, note that the surface normals point outwards, and the
∂Fy
∂Fy
∂Fx
x
integral becomes δy (−Fx + Fx − ∂F
∂x δx) + δx(−Fy + Fy + ∂y δy) = δxδy( ∂x + ∂y ), which is the same as the
divergence. To generalize this to arbitrarily large volumes, notice that if one stacks the boxes next to each other,
then the surface integral across the area where the boxes meet cancels out, and only the integral across the outer
surface remains. For an incompressible flow, the density does not change, and the divergence must be zero.

B.2

Derivation of probability surface integral
R
We will show that the LR estimator tries to integrate S φ(x̃)∇θ g̃(x , h )dS. First, note that dS = n̂dS, and it is
necessary to express the normalized surface vector n̂. To do so, we first express the tangent vector t, then change
the height component of this vector to obtain a vector perpendicular to the tangent vector (this is exactly the
normal vector).
   T
dp
dp
A vector tangent and downhill to the surface is given by t = [− dp
]. The normal vector n is
dx , − dx
dx
   T    T
dp
dp
dp
− dp
h = 0 ⇒ h = 1. Finally, we normalize the
[− dp
dx ; h], such that t · n = 0. Therefore, dx
dx
dx
dx

A unified view of likelihood ratio and reparameterization gradients

vector:
dp

q

dp T
n̂ = [− dx ,1]/ ( dp
dx )( dx ) +1 .

(23)

Next, we perform a change of coordinates from the surface elements dS to cartesian coordinates dx. When
projecting a surface element dS with unit normal n̂ to a plane with unit normal m̂, the projected area is given by
dx = |n̂ · m̂| dS, therefore dx = dS

(

dp
dx

)(

dp
dx

)

T

+1

s
dS =

dS/ ( dp )( dp )T +1, from which we get
[− dp
dx
dx
dx , 1] · [0, 1] =
q

1
q



dp
dx



dp
dx

T
+ 1 dx.

(24)

Plugging Eqs. (23) and (24) into the right-hand side of Eq. (3) we get
s
   T
Z
dp
[− dx
, 1]
dp
dp
r
φ(x̃)∇θ g̃(x , h ) ·    
+ 1 dx =
T
dx
dx
X
dp
dp
+
1
dx
dx
Z
dp
φ(x̃)∇θ g̃(x , h ) · [− , 1] dx.
dx
X

(25)

Recall that the last element of g̃(x , h ) is h p (g(x ); θ), and that h at the boundary surface is 1, then the
dp
∇θ g̃(x , h ) · [− dp
dx , 1] term turns into −∇θ g(x ) · dx +

∂h p(g(x );θ)
∂θ

x =const,h =1

. The last term

∂p(g(x );θ)
∂θ

x =const

can be thought of as the rate of change of the probability density while following a point moving in the flow
x );θ)
induced by perturbing θ. This quantity can be expressed with the material derivative ∂p(g(
=
∂θ
x =const

dp(x;θ)
dθ

+ ∇θ g(x ) ·

dp
dx .

Finally, substituting into Eq. (25):
Z

Z
φ(x̃)∇θ g̃(x , h ) dS =

S

Appendix C

φ(x)
X

dp (x; θ)
dx.
dθ

(26)

Reparameterization gradients are not unique

What happens if we perform the same
R kind of analysis asR in Theorem 1 for the RP gradient? Similarly, suppose
that Rthere exist u(x) and v(x), s.t. ∇φ(x) · u(x) dx = ∇φ(x) · v(x) dx for any φ(x). Rearrange the equation
into ∇φ(x) · (u(x) − v(x)) dx = 0. Then, if we can pick ∇φ(x) = u(x) − v(x) it would lead to u = v, which
would show the uniqueness. However, it is not necessarily possible to pick such φ(x). In particular, the integral
of ∇φ(x) over any closed path is 0, but this is not necessarily the case for u − v. Therefore, the same kind of
analysis does not lead to a claim of uniqueness. Indeed, concurrent work (Jankowiak and Obermeyer, 2018)
showed that there are an infinite amount of possible reparameterization gradients, and the minimum variance2 is
achieved by the optimal transport flow.

Appendix D

Slice integral importance sampling

From Theorem 1 we saw that unlike the RP gradient case, the weighting ψ for function values φ(x) with
x ∼ p (x; θ) to obtain an unbiased estimator for the gradient ddθ E [φ(x)] is unique. The only option to reduce the
variance by changing the weighting would then be to sample from a different distribution q(x) via importance
sampling. Motivated by the resemblance of the “boxes” theory in Sec. 2 to the Riemann integral, we propose
to sample horizontal slices of probability mass resembling the Lebesgue integral. Such an approach appears
attractive, because if the location of the slice is moved by modifying the parameters of the distribution (e.g., by
changing the mean), then the derivative of the expected value of the integral over the slice will depend only on
the value at the edges of the slice (because the probability density in the middle would not change). To obtain
2
By minimum variance, we mean the minimum variance achievable without assuming knowledge of φ(x), or alternatively
that it is approximately linear in the sampling range, ∇φ(x) ≈ A. Their result holds for arbitrary dimensionality.

A unified view of likelihood ratio and reparameterization gradients

the gradient estimator, it will only be necessary to compute the probability density pL (x; θ). We derive such
a “slice integral” distribution corresponding to the Gaussian distribution. The method resembles the seminal
work by Neal (2003) on slice sampling in Markov chain Monte Carlo methods. We call our new distribution the
L-distribution, and it is plotted in Fig. 3b.
Derivation of the pdf of the L-distribution: One way to sample whole slices of a probability distribution
would be to sample a height h between 0 and pmax proportionally to the probability mass at that height. The
probability mass at a height h is just given by 2|x − µ| where x is such that p (x; µ, σ) = h, i.e., 2|x − µ| is the
distance between the edges of p (x; µ, σ). The probability mass corresponding to x is then given by 2|x − µ|dh.
Performing a change of coordinates to the x-domain, and splitting the mass between the two edges of the slice,
dx. This gives a closed-form normalized pdf for the L-distribution:
we get |x − µ|dh = |x − µ| dp(x;µ,σ)
dx
pL (x; µ, σ) = |x − µ|

dp (x; µ, σ)
|x − µ|
= |x − µ|p (x; µ, σ)
dx
σ2


2
|x − µ|
−(x − µ)2
= √
exp
.
2σ 2
2π σ 3

(27)

One can recognize that Eq. (27) is actually just a Maxwell-Boltzmann distribution reflected about the origin with
the probability mass split between the two sides.
Sampling from the L-distribution: To sample from this distribution, it is necessary to sample points
proportionally to the length of the slices. It suffices to sample uniformly in the area under the curve in the space
augmented with the height dimension h, then selecting the slice on which the sampled point lies. This can be
achieved with the three steps: 1) sample a point from the base distribution: xs ∼ p (x; µ, σ), 2) sample a height:
h ∼ unif (0, p (xs ; µ, σ)), 3) compute where the edge of slice is x = p−1 (h; µ, σ), where p−1 (h) inverts the pdf,
and computes the location x that gives a probability density h. For the L-distribution, this can be achieved by
sampling x ∼ N (0, 1) and h ∼ unif(0, 1) and transforming these by the equation:
p
x = µ ± σ −2 log(h ) + 2x .
(28)
Now it is straightfoward to obtain the LR gradient estimator:
" dp
#


1
d
dµ
Ex∼p(x;θ) [φ(x)] = Ex∼q(x;θ)
φ(x) = Ex∼q(x;θ)
φ(x)
dµ
q(x; θ)
x−µ
"
#
sgn(x − µ)
=E p
φ(x) .
σ −2 log(h ) + 2x

Appendix E
E.1

(29)

Slice ratio gradient derivations

Slice ratio gradients for general distributions

So far we have introduced slice ratio gradients for unimodal distributions. Here we explain that the technique
works for arbitrary distributions. The process is illustrated in Fig. 7. The curve p (x; θ) is projected onto the
vertical dimension. Then one samples uniformly in this projected area, and maps the sampled points back onto the
curve via x = p−1 (h). The probability density in the h-space is uniformly 1/H, where H is the total length of the
dp
vertical lines. Changing coordinates will give dh/H = | dx
|dx/H. Thus, this sampling method will always sample
dp
dp
dp
proportionally to | dx |. Because dx = − dµ for arbitrary distributions, this sampling method gives the desired
importance sampling distribution to minimize the variance of the gradient w.r.t. µ for arbitrary distributions.
The probability density becomes
1 dp
q(x) =
,
(30)
H dx
and the gradient estimator for one sample becomes
 
dp
dp
dµ
φ(x) = sgn
Hφ(x).
(31)
dp
dµ
dµ /H

A unified view of likelihood ratio and reparameterization gradients

Figure 7: Slice ratio sampling method for general multimodal distributions.

E.2

Slice ratio gradient for a Gaussian distribution

The pdf is
p (x) = √



1
(x − µ)2
exp −
.
2σ 2
2πσ

(32)

1
.
2πσ

(33)

The maximum probability density is at x = µ:
pmax = √

The probability density for the slice ratio distribution can be derived by performing a change in coordinates from
the h value to x. The probability mass at a slice dh split between two sides is dh/2pmax , so
1
1
dp
dh =
dx.
2pmax
2pmax dx

(34)



2πσ
1
(x − µ)2 −(x − µ)
√
q(x) =
exp −
2
2σ 2
σ2
2πσ


2
|x − µ|
(x − µ)
=
exp −
,
2
2σ
2σ 2

(35)

From this we get
√

which is the pdf in Eq. (6).

A unified view of likelihood ratio and reparameterization gradients

To derive the sampling method, first derive the inverse of the probability density p−1 (h) as


1
(x − µ)2
h= √
exp −
2σ 2
2πσ
1
(x − µ)2
log(h) = − log(2π) − log(σ) −
2 
2σ 2

1
(x − µ)2 = −2σ 2
log(2π) + log(σ) + log(h)
2
p
x = µ ± σ − log(2π) − 2 log(σ) − 2 log(h) .

(36)

Now, noting h = pmax h , where h ∼ unif(0, 1), we end up with the sampling method:
s
− log(2π) − 2 log(σ) − 2 log( √

x=µ±σ
=µ±σ
E.3

p

1
h )
2πσ

(37)

−2 log(h ) , where h ∼ unif(0, 1).

Slice ratio gradient for a symmetric Beta distribution

The pdf is
pβ (x) =

(x − x2 )α−1
xα−1 (1 − x)α−1
=
.
B(α, α)
B(α, α)

(38)

The maximum probability density is at x = 0.5:
pmax =

0.25α−1
B(α, α)

(39)
dp

Similarly to Eq. (35), the pdf of the slice ratio distribution is q(x) = | dx |/2pmax :
B(α, α)
(x − x2 )α−2
(α − 1)(1 − 2x)
2 × 0.25α−1
B(α, α)
α−1
=
(x − x2 )α−2 (1 − 2x) ,
2 × 0.25α−1

q(x) =

(40)

which is the pdf in Eq. (8).
To derive the sampling method, first derive the inverse of the probability density p−1 (h) as
(x − x2 )α−1
B(α, α)
(x − x2 )
=
B(α, α)1/(α−1)

h=
h /(α−1)
1

2

x − x + (hB(α, α))

(41)

1/(α−1)

=0
q
1 1
x= ±
1 − 4(hB(α, α))1/(α−1) .
2 2

Now, noting h = pmax h , where h ∼ unif(0, 1), we end up with the sampling method:
s

1/(α−1)
1 1
0.25α−1
x= ±
1 − 4 h
B(α, α)
2 2
B(α, α)
q
1 1
1/(α−1)
= ±
1 − h
, where h ∼ unif(0, 1).
2 2

(42)

A unified view of likelihood ratio and reparameterization gradients

Stretching factor k to achieve variance σ 2 : The variance of the Beta distribution is given by
√
1
1
2
2
4(2α+1) . We need k 4(2α+1) = σ ⇒ k = 2σ 2α + 1.
E.4

W-distribution for minimizing variance of

α2
(2α)2 (2α+1)

=

d
dσ

For completeness, for a Gaussian we also derive the optimal sampling distribution for the derivative w.r.t. σ.
d2 p(x)
First note that dp(x)
= σ dx
. This expression means that if we apply the same height sampling concept as used
2
dσ
for µ on the distribution proportional to
to

2

d p(x)
dx2

dp(x)
dx

, we would obtain samples with probability density proportional

, and would hence be sampling from the desired distribution. The required base distribution is just the

B-distribution (Eq. (6)), so we can perform the required derivation.
The result is given below:
√




e
−(x − µ)2
|x − µ|2
pW (x; µ, σ) =
exp
−1 ,
4σ
2σ 2
σ2
q
x = µ ± σ W (−2h /e) where h ∼ unif(0, 1),
#
"

 √
d
(x − µ)2
2 2
√ φ(x) .
Ex∼p(x) [φ(x)] = Ex∼pW (x) sgn
−1
dσ
σ2
σ eπ

(43)

In the above equation, W (x) is the Lambert W function (Corless et al., 1996)—a function s.t. z = W (zez ). The
solution for W is picked with equal probability from the −1 and 0 branches of W , and the ± is also sampled
randomly with equal probability. Efficient implementations of W are available in common numerical computation
packages, such as scipy (Jones et al., 01 ) or MATLAB. We call the result the W-distribution, and it is plotted in
Fig. 3d. To the best of our knowledge, this distribution does not exist in the literature.
Derivation of W-distribution: We first derive the probability density pW (x), then the sampling scheme.
The base distribution is pB (x), and we apply a transformation by which we sample the height h, and transorm
this to a point x by using the inverse x = p−1
B (h), and sampling uniformly between the x values that satisfy
the equation, e.g. for the B-distribution in Fig. 3c there are usually 4 points for each h value. Therefore
dpB (x)
dpB (x)
1
1
dh = 4max(p
dx and pW (x) = 4max(p
. The required derivative is given by
B ) dx
B ) dx
sgn(x − µ)
dpB (x)
=
exp
dx
2σ 2



−(x − µ)2
2σ 2


−

|x − µ|
exp
2σ 2



−(x − µ)2
2σ 2



(x − µ)
.
σ2

(44)

Setting the derivative to 0 gives the locations of the peaks at x = µ ± σ. Evaluating pB (x) at these locations in
Eq. (6) gives the peak value as
1
max(pB ) =
exp (−1/2) .
(45)
2σ
Combining these results gives the density in Eq. (43).
Deriving the sampling method, requires inverting pB (x):
|x − µ|
h=
exp
2σ 2



−(x − µ)2
2σ 2

t1/2
√ exp (−t)
σ 2
t
h2 =
exp (−2t)
2σ 2
−4σ 2 h2 = −2t exp (−2t)

W −4σ 2 h2 = −2t.


,

let t =

(x − µ)2
, then
2σ 2

h=

(46)

A unified view of likelihood ratio and reparameterization gradients

Now recalling that h = pmax h , h ∼ unif(0, 1), where pmax =
value of t, gives the sampling method in Eq. (43).

1
2σ

exp (−1/2) from Eq. (45), and plugging in the

dp
The gradient estimator can be found by computing dσ
/pW . The derivative is given by








(x − µ)2
d
(x − µ)2
1
1
(x − µ)2
(x − µ)2
1
√
√
√
exp
−
exp −
=
−
+
exp
−
dσ
2σ 2
2σ 2
2σ 2
σ3
2πσ
2πσ 2
2πσ




(x − µ)2
1
(x − µ)2
−1 .
=√
exp −
2
2σ
σ2
2πσ 2

(47)

Dividing this derivative with the density in Eq. (43) gives the gradient estimator in Eq. (43).
Experimental results for W-distribution: We performed experiments similar to the experiment in the
main section of the article by comparing the standard LR gradient with the estimator by sampling from the
W-distribution. The setup was such that φ(x) is a quadratic (x − a)T Q(x − a), where Q = ones(D, D)/D2 is
a matrix of ones, which is scaled, such that φ(x) remains constant at x = 0. We considered two options for a:
a = 1 or a = 0. We evaluate a deterministic case, as well as a case when Gaussian noise σn2 = 1 is added on φ.
We vary the dimension between 1–1000, and plot the variance of the gradient estimators: SLR—LR gradient with
a Gaussian p(x) and estimating the gradient w.r.t. σ; WRG—slice ratio gradient with a Gaussian p(x), and using
the W-distribution to importance sample and estimate the gradient w.r.t. σ. We used antithetic sampling, so
that the effect of any baseline could be ignored. The gradient was estimated by averaging 100 samples, and this
was repeated for a large number of times to estimate the variance of the gradient estimator. Bootstrapping was
used to obtain confidence intervals. The results (Fig. 8) confirm that the W-distribution increases the accuracy.
E.5

Multidimensional Gaussian Slice ratio gradient

In multiple dimensions the optimality equation in Eq. (5) is still valid, but the method to derive the normalized
distribution and sampling method have to be modified. For simplicity, we consider the case of optimal sampling
for the derivative w.r.t. µ for a spherical Gaussian. Motivated from the derivation for a single dimension, consider
a method which would sample a unit vector on a sphere for a direction r̂, as well as a height h, then invert the
distribution s.t. x = p−1 (h, r̂), where p−1 is a function s.t. p (x) = h and x = rr̂, i.e., it picks x in the direction r̂,
which gives the desired probability density. The conversion from the h-coordinate to the x-coordinate would still
give the desired dp(x)
term; however, due to the change in the surface area as the radius r is increased, there is
dx
an additional factor r−(D−1) , where D is the dimensionality. In other words, the sampling method has to be
modified to cancel out this new factor, and the required distribution must have the property: q(x) ∝ rD−1 dp(x)
.
dx
2

r
For a Gaussian base distribution we get q(x) ∝ rD exp(− 2σ
2 ). The required distribution is the chi distribution:

q(z; k) =

1
z2
k−1
z
exp(−
)
2
2(k/2−1) Γ(k/2)

where

r = σz

and k = D + 1.

(48)

In fact, the Rayleigh distribution is a special case of this distribution for D = 1, and the Maxwell-Boltzmann
distribution is the case for D = 2. Note that if one performs this sampling procedure, but while using D̃ = D − 1,
then the sample comes exactly from the original Gaussian distribution p (x; µ, Σ). This remark highlights that
there are diminishing returns to changing the sampling distribution as the dimensionality of the space is increased,
because the optimal sampling distribution tends to the original Gaussian distribution.
Derivation of the directional ratio gradient estimator (DRG): The chi distribution
q(z; k) with k
qP
k
degrees of freedom is a distribution, s.t. z is distributed according to the random variable z =
i=1 xi , where
xi are distributed according to a Gaussian distribution with mean 0 and standard deviation 1. In other words, z
is distributed according to the length of the distance from the origin, when sampling from a spherical Gaussian
with k dimensions, and to sample from a Gaussian distribution, it suffices to sample a direction r̂ on the unit
sphere, then sample the distance z according to the chi distribution, and add a factor σz to correct for the scaling
from the variance parameter. We can write the probability density of a Gaussian in spherical coordinates as

1
N x; 0, σ 2 I = q(z; D),
A

(49)

A unified view of likelihood ratio and reparameterization gradients

SLR
WRG

Variance of gradient

Variance of gradient

SLR
WRG

101

100

100

101

102

101

100

103

100

101

# of dimensions
(a) Deterministic, a = 1
SLR
WRG

SLR
WRG

Variance of gradient

Variance of gradient

103

(b) Noisy, a = 1

100
10−1
10−2
10−3
10−4

102

# of dimensions

10

1

100

10−1
10

0

10

1

10

2

# of dimensions
(c) Deterministic, a = 0

10

3

100

101

102

# of dimensions
(d) Noisy, a = 0

Figure 8: The confidence intervals correspond to one standard deviation of the estimate.

103

A unified view of likelihood ratio and reparameterization gradients

where A is the area of a D-dimensional hypersphere at radius σz given by A =

2π D/2
(σz)D−1 ,
Γ( 12 D )

and x = σzr̂,

where r̂ is a vector sampled on the unit sphere. In cartesian coordinates, the gradient w.r.t. x can be written as


dp (x; 0, Σ)
1
exp −xT Σ−1 x × −Σ−1 x = −Σ−1 xp(x; 0, Σ).
=
D/2
1/2
dx
(2π)
|Σ|

(50)

Translating this result to spherical coordinates, we have

and

dp (x; 0, Σ)
zr̂ 1
=−
q(z; D),
dx
σ A

(51)

dp (x; 0, Σ)
z 1
=
q(z; D).
dx
σA

(52)

If one applies the same directional sampling scheme, but instead of sampling from q(z; D), one samples from
a distribution proportional to zq(z; D), one would be sampling from the desired distribution. By inspecting
Eq. (48), it is clear that increasing the degrees of freedom by one adds the additional z factor, so the optimal
importance sampling distribution is
1
(53)
q(x) = q(z; D + 1),
A
where x = σzr̂. To obtain the gradient estimator, divide
zr̂ 1
q(z; D)
σ A



dp
dµ

dp
= − dx
given in Eq. (51), with q(x) in Eq. (53):

1
r̂ zq(z; D)
r̂ 21/2 Γ ((D + 1)/2)
q(z; D + 1) =
=
A
σ q(z; D + 1)
σ
Γ (D/2)

(54)

Derivation of the directional ratio gradient estimator while assuming a linear φ (DLRG): The
above derivation made our standard assumption that φ is ignored. Another option is to assume that φ varies
linearly. The derivation is easily modified. From Eq. (52), we saw that it was necessary to sample from a
distribution proportional to zq(z). In the new derivation, it will be necessary to sample proportionally to
zq(z)φ(z; D), which when φ is linear, is equivalent to sampling proportionally to z 2 q(z; D). Based on the same
argument as in Eq. (53), the necessary sampling can be done by increasing the degrees of freedom by 2, i.e., one
must sample from
1
q(x) = q(z; D + 2).
(55)
A
Similarly to Eq. (54), the gradient estimator can be derived:

zr̂ 1
1
r̂ zq(z; D)
2r̂ Γ ((D + 2)/2)
Dr̂
q(z; D)
q(z; D + 2) =
=
=
,
(56)
σ A
A
σ q(z; D + 2)
zσ
Γ (D/2)
zσ
where the last line follows from the property Γ(n + 1) = nΓ(n).
E.6

Sufficient conditions for an unbiased gradient estimator while ignoring importance weights
from other dimensions
PD
First we consider functions of the form φ(x) =
i=1 φi (xi ), and show that ignoring the importance
weights from dimension
j 6= i for the derivative iw.r.t. θi , still
estimator.
h
h gives an unbiased gradient
i
d log p(xi ;θi )
i ) d log p(xi ;θi )
Note that Exi ∼q(xi ) p(x
E
[φ
(x
)]
=
E
E
[φ
(x
)]
=
0,
because
j
j
j
j
xj ∼q(xj )
xi ∼p(xi ) dθi
xj ∼q(xj )
q(xi ) dθi
h
i
d log p(xi ;θi )
Exi ∼p(xi ) dθ
Y = ddθi E [Y ] = 0, for Y statistically independent from xi . This result means that if φ
i
has a structure, such that different dimensions affect φ independently, then the gradient estimator will still be
unbiased.
Next we show that even if the dimensions are not independent, in some cases the gradient estimator is unbiased.
Notably, for a quadratic function φ(x) = aT x + xT Qx + c, the gradient estimator will be unbiased. First
note that the diagonal terms in the quadratic function are independent, so the gradient of that portion of the
cost will be unbiased based on the previous example. Next consider the off-diagonal terms of xT Qx, which
are xi Qij xj . Note that the distributions we considered, namely the B, W, L and Beta distributions were

A unified view of likelihood ratio and reparameterization gradients

all symmetric about the mean value µj . Therefore Exj ∼q(xj ) [Qij xj ] = Exj ∼p(xj ) [Qij xj ], and the derivative
h
h
ii
p(xj )
d
E
x
E
Q
x
remains unchanged even if one ignores the p(xj )/q(xj ) importance weights.
i
ij
j
x
∼p(x
)
x
∼q(x
)
i
i
j
j
dθ
q(xj )
This result implies that if the variance of the distribution σ 2 is small, such that φ is roughly quadratic in the
range of the sampling distribution, then the gradient estimator will remain roughly unbiased.

Appendix F
F.1

Additional justifications for approximations in the derivations

Ignoring importance weights in multidimensional slice ratio sampling

Q
In Sec. 4 for the multidimensional case we considered factorized distributions p (x; θ) = i pi (xi ; θi ), and for
estimating the gradient w.r.t. θi , we chose to ignore the importance weights from the other dimensions j =
6 i. We
justified the omission by noting that the unbiased gradient estimator would be given by

p\i x\i pi (xi ) d log pi (xi )
φ(x),
(57)
q\i (x\i )qi (xi ) dθi
and that the variance would grow exponentially as the dimension increases, because of the growth of the variance
p\i (x\i )
of the q\i (x\i ) term. The assumption of factorized distributions may appear restrictive; however, note that this is
the most common scenario in practice, and finding a solution in this setting is important. Moreover, note that by
making the factorization assumption, we ended up with a worst case scenario, where as the dimension increases,
the optimal unbiased importance sampling distribution will tend to the original distribution, thus showing that
no gains are possible without adding in bias. Replacing the importance weights with their expected value is
not just a convenience, but a necessity. If the distribution does not factorize, then such an omission may not
be necessary, and good unbiased importance sampling distributions may exist, but our methods would not be
directly applicable, and this is a topic for future work.
F.2

Omission of φ in optimality of slice ratio sampling derivation theoretical reasons

φ(x)/q(x) was replaced
In the derivation of the slice ratio gradients, the optimization of the variance of dp(x;θ)
dθ
dp(x;θ)
/q(x). Here we explain the various reasons, which justify this omission,
with optimizing the variance of dθ
and show that in most realistic settings it is almost exactly the correct thing to do. We introduce three realistic
settings to which this omission corresponds: 1. the estimation of φ is very noisy, 2. φ is high dimensional, 3. φ
has high frequency variations at a length scale smaller than the range of the sampling distribution. In addition,
note that another reasonable assumption might be to assume that φ(x) is linear, but the L-distribution (App. D)
turns out to be optimal in this setting (in low dimensions).
φ is very noisy:

If φ is noise uncorrelated with x, then V

h

i

dp(x;θ)
φ(x)/q(x)
dθ

=V

h

i

dp(x;θ)
/q(x)
dθ

V [φ(x)], and

one can ignore φ in the optimization. The same reasoning holds if φ(x) = φ̂(x) + n , where n is random noise
with magnitude much larger than the variation of φ(x).
φ is high dimensional: Consider the independent multidimensional sampling scenario justified in App. F.1,
and estimating the variance of the gradient of one dimension i. The general gradient estimator is given by
"
#

Y
p\i x\i pi (xi ) d log pi (xi )
d
Ex∼p(x) [φ(x)] = Ex∼q(x)
φ(x) ,
where p\i is
pj (xj ; θj ) .
(58)
dθi
q\i (x\i )qi (xi ) dθi
j6=i

In the independent sampling case, we justified that

p\i (x\i )
q\i (x\i )

should be ignored if one hopes to make any gains in

(xi ) d log pi (xi )
terms of variance reduction. We are left with estimating the variance of pqii(x
φ(x). Note that φ(x) still
i ) dθi
contains all dimensions other than i, i.e. xj , where j 6= i still matter; however, they are statistically independent
of the gradient estimator, and thus the variation caused by xj acts as noise on the gradient signal. We call this,
3

3
Potentially other variance reduction techniques besides completely ignoring the weights may also work, e.g., clipping
the weights, but the analysis regarding omitting φ is not affected.

10

GLR
SLRG
TRRG
BRG
BLR
LRG
DRG
DLRG

2

101
10

0

10−1
10−2

105

Variance of gradient

Variance of gradient

A unified view of likelihood ratio and reparameterization gradients

10

GLR
SLRG
TRRG
BRG
BLR
LRG
DRG
DLRG

4

103
102
101
100
10−1
10−2

10

0

10

1

10

2

# of dimensions
(a) Deterministic

10

3

100

101

102

103

# of dimensions
(b) Noisy

Figure 9: yadayada. The confidence intervals correspond to one standard deviation of the estimate.

the sampling interference noise. If one assumes that the dimensionality is D, and that the variation of φ(x) is
roughly the same in each dimension, then roughly a fraction (D − 1)/D of φ can be considered as noise for each
gradient estimator. Thus, as the dimension increases, the variation in φ(x) rapidly approaches equivalence to
random noise, and rejecting the noise will be most important for reducing gradient variance.
φ has high frequency components: Consider φ(x) = φ̂(x) + a sin(ωx). If ω is large compared to the sampling
range, then sin(ωx) is almost statistically independent to x, and can be viewed as noise. Such high frequency
components occur when applying LR gradients to chaotic systems (Parmas et al., 2018), and correspond to
the situation when LR vastly outperforms RP. Thus, reducing the variance of LR gradients in this scenario is
important.
F.3

Additional experiments showing downsides of alternative approaches

In Fig. 9 we show experiments evaluating alternative optimal gradient estimators based on different assumptions,
and explain that our approach in the main paper is better. We evaluated importance sampling based on the
L-distribution (LRG), as well as the optimal importance sampling distribution in multiple dimensions without
ignoring the importance weights from the other dimensions. DRG stands for directional ratio gradient, which
omits φ in the derivation, but considers importance weights from all dimensions. DLRG assumes φ is roughly
linear. The other results are the same as in the main section of the article: GLR—LR gradient with a Gaussian
p(x); SLRG—slice ratio gradient with a Gaussian p(x); TRRG—truncated ratio gradient with c = 0.5; BRG—slice
ratio gradient with a Beta p(x), and α = 1.5, plotted in Fig. 4c; BLR—LR gradient with a Beta p(x), and α = 1.5.
Results: The alternative methods (LRG, DRG, DLRG) converge to GLR in the noisy setting for high dimensions,
and show no gain in terms of variance reduction, whereas our proposed methods, SLRG and TRRG are able
to show some advantage. In the deterministic case, at high dimensions LRG has higher variance than the
other methods, because it has a larger sampling variance, which increases the sampling interference noise from
the other dimensions, explained in Sec. F.2. A point which deserves discussion is that in the 1-dimensional
deterministic case, LRG and DLRG show extremely low variance. The gradient estimator in this situation is given
by √ sgn(x−µ) 2 φ(x) = sgn(x−µ)
φ(x). Note that in the antithetic sampling setting, this estimator just becomes
x
σ

−2 log(h )+x

a finite difference (φ(x+ ) − φ(x− ))/(2∆x), where the 2 comes from averaging two samples, and if φ is linear, then
the gradient estimator would be exact with just one sampled pair (unlike the standard finite difference estimator,
this estimator would be unbiased for non-linear φ as well). In the experiment, the curvature of φ was quite low,
so LRG and DLRG gave extremely low variance in the 1-dimensional setting, but this advantage does not hold
up, when the dimensionality is increased. In conclusion, the approximations we made are well justified.

A unified view of likelihood ratio and reparameterization gradients

Appendix G

Truncated ratio gradient derivations

Recall that the truncated ratio gradient probability density function, sampling method and gradient estimator
are given by the result below:
2

exp(− c2 )
(x − µ)2
1
|x − µ|
√ p
exp(−
ptr (x; c, µ, σ) =
)
1 − Φ(c) σ2 2π (x − µ)2 + σ 2 c2
2σ 2
where Φ(c) is the cdf of a unit normal distribution,
p
x = µ ± σ 2c − c2 where c ∼ truncG(c, ∞)

(59)

and truncG(a, b) is the unit normal truncated between a and b,
"
#
2c 1 − Φ(c)
d
Ex∼ptr (x) [φ(x)] = Ex∼q(x) sgn(x − µ)
φ(x) .
dµ
σ exp(− c22 )
The pdf ptr (x; c, µ, σ) satisfies the optimality Eq. (10), therefore, as long as it is a proper probability density, it is
correct. We will show that the proposed sampling method corresponds to this pdf.
Without loss of generality, let µ = 0. The pdf of c is given by
 2

1
1
, between c ∈ [c, ∞].
p (c ) = √ exp − c
2
1
−
Φ(c)
2π

(60)

Perform a change of coordinates from c to x and account for the stretching due to the Jacobian:
x=σ
note that σc =

√

p

c
2c − c2 ⇒ dx = σ p
dc ,
2
c − c2

(61)

x2 + σ 2 c2 , so
√

x/σ
dx = dc ,
x 2 + σ 2 c2

(62)

therefore
 2
1

1
x/σ
√
p (c ) dc = √ exp − c
dx
2
2
1
−
Φ(c)
2π
x + σ 2 c2


1
x2
c2
1
x/σ
√
= √ exp − 2 +
dx
2
2σ
2
1
−
Φ(c)
2π
x + σ 2 c2
 2


exp − c2
1
x
x2
√ √
=
exp − 2 dx.
1 − Φ(c) σ 2π x2 + σ 2 c2
2σ

(63)

This result is the desired probability distribution on the half-plane [0, ∞]. It is a normalized pdf by construction.
Symmetrizing the distribution about 0, and shifting by a mean parameter µ gives the desired result. The gradient
dp
estimator is easily derived by dθ /q.
Variance and gradient accuracy derivations: The variance is most easily derived by working with the
distribution on c in Eq. (60). Note that if we symmetrize the distribution about 0, then the mean will be 0, and
the variance can be estimated as the expectation of x2 when sampling from half the distribution:
Z
V [x] =

x2 p (c ) dc =

Z

σ 2 (2c − c2 )p (c ) dc =

Z

σ 2 2c p (c ) dc − σ 2 c2 .

(64)

A unified view of likelihood ratio and reparameterization gradients

 
2
So, we just need to find E 2c = V [c ] + E [c ] . Denote N (x) is the unit variance Gaussian distribution, and Φ(x)
is the cdf of the unit variance Gaussian, then the mean and variance of the 0 mean truncated Gaussian between
2

N (c)
cN (c)
N (c)
. Combining these two results:
[c, ∞] can be written as E [c ] = 1−Φ(c)
and V [c ] = 1 + 1−Φ(c)
− 1−Φ(c)
 
cN (c)
E 2c = 1 +
.
1 − Φ(c)

(65)



cN (c; 0, 1)
− c2 .
v(c) = V [x] = σ 2 1 +
1 − Φ(c)

(66)

Hence, the variance is

Next, we derive the variance of the gradient term
µ) 2σc 1−Φ(c)
2 ,
exp(− c2 )

dp
dµ /q.

so the variance is


dp
2c 1 − Φ(c)
V
/q = E 
dµ
σ exp(− c22 )




!2 

Note that this term is given in Eq. (59) as sgn(x −

!2
2 (1 − Φ(c))
σ exp(− c22 )


2
cN (c; 0, 1) 4 (1 − Φ(c))
= 1+
.
1 − Φ(c)
σ 2 exp(−c2 )

 
 = E 2c

Finally, note that the gradient accuracy t(c) is defined as 1/V

Appendix H

h

dp
dµ /q

i

(67)

.

Evolution strategies in reinforcement learning

Evolution strategies are a technique based on sampling in the parameter space of a problem p (w; θ), and applying
LR gradients to optimize the objective Ep(w;θ) [φ(w)]. For example w may be the parameters of a neural network
policy in reinforcement learning, and the objective is to find the distribution p (w; θ) over the parameters w,
which gives the behavior with the largest expected reward. In this case, φ(w) would be the return function for a
particular parameter set w. One would first sample parameters w, these would be kept fixed for one episode of
the agent’s behavior, the behavior would be evaluated based on a reward function, and the sum of the reward
would be returned to the algorithm as φ(w). LR gradients can be used to evaluate ddθ Ew∼p(w;θ) [φ(w)], and the
objective can be optimized directly using gradient ascent. We implemented our new importance sampling schemes
into David Ha’s Evolution Strategies code available from https://github.com/hardmaru/estool(Ha, 2017) (note
that our methods are not available from the link yet), and tested our methods on cart-pole swing-up and biped
walker tasks illustrated in Fig. 10.
H.1

Experiments

In all experiments we used spherical Gaussian base distributions p (w; θ) for the GLR, SLRG and TRRG methods,
while the sampling distributions q(w) varied based on the importance sampling scheme. For BRG, we used a
Beta base distribution, and applied the Beta slice ratio gradient method. We used antithetic sampling, i.e. we
always sampled w in pairs, which are located opposite of each other in the distribution. If such a scheme is used,
then any constant baseline b (Greensmith et al., 2004), which is subtracted from the φ(w) values will cancel out
from the opposite pairs, and the effect of such baselines can be ignored. We did not use a weight decay. For
TRRG, c = 0.5, and for BRG, α = 1.1 in all cases. We used a CPU cluster for our experiments. Biped tasks
were run on 33 cores, and cart-pole tasks were run on 5 cores. All tasks were run for 2000 policy improvement
iterations (gradient steps), and repeated for several different random number seeds (details in tables). Because
the samples from q(x) do not correspond to the objective Ep(x;θ) [φ(x)], we separately evaluated the performance
by sampling from p (x; θ) after every 10 iterations. Note that this was done only for evaluation purposes, and did
not have any effect on the learning.

A unified view of likelihood ratio and reparameterization gradients

(a) Cart-pole swing-up

(b) Biped walker

Figure 10: Environments used in evolution strategies experiments.
Cart-pole setup State dimension: 5; Action dimension: 1; Policy: neural network with one hidden layer with
10 neurons and tanh activations, total number of parameters w: 71; Optimizer: basic stochastic gradient ascent
with one learning rate parameter; Number of samples per iteration: 32; Std σ of Gaussian: 0.5. In addition to the
standard cart-pole task, we considered a setting where we artificially add noise onto the φ(w) values to simulate
a setting where the rewards can only be observed stochastically, and test how our importance sampling methods
cope with such noisy measurements. There are additional details in the table and figure captions.
Biped walker setup State dimension: 24; Action dimension: 4; Policy: neural network with two hidden layers
with 40 neurons each and tanh activations, total number of parameters w: 2804; Optimizer: Adam with β1 = 0.99
and β2 = 0.999; Number of samples per iteration: 256; Std σ of Gaussian: 0.04. We used reward normalization
(Mania et al., 2018), which is a technique to ensure that scale of the rewards stays roughly constant by normalizing
these with the standard deviation of the sampled returns φ. This appeared to perform better for GLR than rank
standardization as used in (Salimans et al., 2017).
Results The results are in the tables and figures. The errorbars in the tables correspond to the sample standard
deviation (so divide by the square root of the sample size to obtain a confidence interval), while the errorbars
in the figures are already the standard deviation of the mean. The results act as a sanity check and show that
our methods do work, while as expected the difference with standard GLR is small, because the improvement
in accuracy is modest. On the other hand, the cart-pole swing-up experiments show that using the slice ratio
gradient method allows the Beta base distribution to be competitive with Gaussian distributions. The experiments
also show that SLRG can indeed have trouble with systems with a low stochasticty, e.g. the cart-pole. Moreover,
the results confirm that our methods reduce gradient variance in stochastic settings. An important topic of future
work will be finding distributions, which outperform Gaussians, and importance sampling techniques like our
slice ratio gradient method will be crucial in such a pursuit.

A unified view of likelihood ratio and reparameterization gradients

Table 2: Cart-pole swing-up and balancing, no added noise, 32 samples per batch, SGD optimizer, average reward
over whole training run, 10 experimental runs for each setting
Learn. rate
GLR
SLRG
TRRG

0.001

0.003

0.005

0.008

0.01

492.7 ± 15.6
439.0 ± 12.2
464.1 ± 5.6

694.4 ± 43.2
580.3 ± 51.4
676.6 ± 52.4

716.9 ± 77.4
664.9 ± 55.2
771.2 ± 31.7

792.6 ± 31.9
763.2 ± 52.1
809.5 ± 21.7

782.9 ± 43.1
747.4 ± 72.7
747.9 ± 102.1

Table 3: Cart-pole swing-up and balancing, no added noise, 32 samples per batch, SGD optimizer, average reward
of last 100 parameter values, 10 experimental runs for each setting
Learn. rate
GLR
SLRG
TRRG

0.001

0.003

0.005

0.008

0.01

596.7 ± 39.7
548.8 ± 7.6
561.8 ± 16.0

881.2 ± 15.3
723.1 ± 133.8
867.1 ± 67.4

840.1 ± 103.2
845.4 ± 82.0
901.1 ± 3.5

904.4 ± 3.4
887.0 ± 57.0
905.8 ± 1.4

889.6 ± 47.4
895.8 ± 24.7
840.0 ± 136.0

Table 4: Cart-pole swing-up and balancing, 90 added noise standard deviation, 32 samples per batch, SGD
optimizer, average reward over whole training run, the number of experimental runs for GLR, SLRG, TRRG
were [10,10,50,50,10] for the learning rates from left to right respectively, and 20 runs for BRG in all cases.
Learn. rate

0.001

GLR
SLRG
TRRG
BRG

519.4
459.7
485.5
409.1

0.003
±
±
±
±

36.9
10.4
8.0
12.5

668.1
608.5
658.0
531.3

0.005
±
±
±
±

72.6
51.2
64.7
16.4

702.7
668.9
708.1
600.8

0.008
±
±
±
±

66.3
70.8
64.8
54.4

690.3
710.5
699.2
662.7

0.01
±
±
±
±

60.8
62.6
73.7
70.6

637.4
696.1
682.8
723.0

±
±
±
±

42.8
64.9
71.4
67.6

Table 5: Cart-pole swing-up and balancing, 90 added noise, 32 samples per batch, SGD optimizer, average reward
of last 100 parameter values, the number of experimental runs for GLR, SLRG, TRRG were [10,10,50,50,10] for
the learning rates from left to right respectively, and 20 runs for BRG in all cases.
Learn. rate

0.001

GLR
SLRG
TRRG
BRG

639.2
552.0
574.1
535.8

0.003
±
±
±
±

81.6
5.9
21.4
8.9

807.6
771.0
818.1
626.6

0.005
±
±
±
±

101.2
115.8
107.8
69.9

834.0
810.4
837.0
736.8

0.008
±
±
±
±

82.8
108.9
86.0
123.0

810.2
845.1
814.0
792.7

0.01
±
±
±
±

91.2
74.2
99.1
115.0

729.2
815.0
794.6
873.5

±
±
±
±

103.8
83.3
121.0
66.3

Table 6: Biped walker, 256 samples per batch, each parameter sample averaged over 4 episodes, Adam optimizer,
reward scaled by standard deviation of rewards, average reward of whole training run, the number of experimental
runs were [20,60,40,40,20] for the learning rates from left to right respectively
Learn. rate
GLR
TRRG

0.005

0.01

0.015

0.02

0.04

14.6 ± 30.0
31.6 ± 42.7

223.2 ± 62.1
230.0 ± 39.5

264.2 ± 45.7
250.2 ± 43.9

253.0 ± 51.6
257.4 ± 46.0

260.9 ± 56.6
251.2 ± 45.5

Table 7: Biped walker, 256 samples per batch, each parameter sample averaged over 4 episodes, Adam optimizer,
reward scaled by standard deviation of rewards, average reward of last 100 parameter values, the number of
experimental runs were [20,60,40,40,20] for the learning rates from left to right respectively
Learn. rate
GLR
TRRG

0.005

0.01

0.015

0.02

0.04

38.4 ± 92.2
104.1 ± 154.2

390.4 ± 61.5
394.0 ± 34.1

376.6 ± 48.6
363.3 ± 58.3

345.5 ± 63.4
353.8 ± 62.3

347.9 ± 63.0
352.5 ± 63.1

A unified view of likelihood ratio and reparameterization gradients

800
600
400
GLR
SLRG
TRRG

200
0
0

500

1000

1500

Gradient variance

Cumulative reward

200000

GLR
SLRG
TRRG

150000

100000

50000

0

2000

0

500

1000

Iteration

1500

2000

Iteration

(a) Learning performance

(b) Gradient variance

GLR
SLRG
TRRG
BRG

200000

800
600
400
GLR
SLRG
TRRG
BRG

200
0
0

500

1000

1500

Gradient variance

Cumulative reward

Figure 11: Cart-pole swingup, no noise, learning rate: 0.008 for all methods, errorbars show 1 standard deviation
of the mean, TRRG’s c = 0.5.

150000

100000

50000

2000

0

500

1000

Iteration

1500

2000

Iteration

(a) Learning performance

(b) Gradient variance

Figure 12: Cart-pole swingup; noise with std 90 added on cumulative reward; learning rates: GLR: 0.005, SLRG:
0.008, TRRG:0.005, BRG:0.01; errorbars show 1 standard deviation of the mean, BRG’s α = 1.1, TRRG’s c = 0.5.
400

300

200

100

GLR
TRRG

Gradient variance

GLR
TRRG

Cumulative reward

Cumulative reward

400

300

200

100

0

0
0

500

1000

1500

Iteration

(a) Learning rate: 0.01

2000

0

500

1000

1500

Iteration

(b) Learning rate: 0.015

2000

GLR
TRRG

1600
1400
1200
1000
800
600

0

500

1000

1500

2000

Iteration

(c) Learning rate: 0.01

Figure 13: Biped walker; errorbars show 1 standard deviation of the mean, each parameter sample averaged over
4 episodes, TRRG’s c = 0.5.

A unified view of likelihood ratio and reparameterization gradients

400

GLR
TRRG

Cumulative reward

Cumulative reward

400

300

200

100

0

GLR
TRRG

300
200
100
0

0

500

1000

Iteration
(a) Average results

1500

2000

0

500

1000

1500

2000

Iteration
(b) Raw data

Figure 14: Biped walker; learning rate: 0.01, errorbars show 1 standard deviation of the mean, each parameter
sample from 1 episode, 40 random number seeds, TRRG’s c = 0.5; the final reward was bimodal, and while
TRRG learned faster, more experiments converged to the lower local minimum.

