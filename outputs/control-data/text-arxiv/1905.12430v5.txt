Norm-based generalisation bounds for multi-class convolutional neural networks*
Antoine Ledent 1 , Waleed Mustafa 1 , Yunwen Lei 1,2 and Marius Kloft 1
1

arXiv:1905.12430v5 [cs.LG] 21 Feb 2021

2

Department of Computer Science, TU Kaiserslautern, 67653 Kaiserslautern, Germany
School of Computer Science, University of Birmingham, Birmingham B15 2TT, United Kingdom

Abstract
We show generalisation error bounds for deep learning with
two main improvements over the state of the art. (1) Our
bounds have no explicit dependence on the number of classes
except for logarithmic factors. This holds even when formulating the bounds in terms of the L2 -norm of the weight
matrices, where previous bounds exhibit at least a squareroot dependence on the number of classes. (2) We adapt the
classic Rademacher analysis of DNNs to incorporate weight
sharing—a task of fundamental theoretical importance which
was previously attempted only under very restrictive assumptions. In our results, each convolutional filter contributes only
once to the bound, regardless of how many times it is applied.
Further improvements exploiting pooling and sparse connections are provided. The presented bounds scale as the norms of
the parameter matrices, rather than the number of parameters.
In particular, contrary to bounds based on parameter counting, they are asymptotically tight (up to log factors) when
the weights approach initialisation, making them suitable as a
basic ingredient in bounds sensitive to the optimisation procedure. We also show how to adapt the recent technique of
loss function augmentation to our situation to replace spectral
norms by empirical analogues whilst maintaining the advantages of our approach.

Introduction
Deep learning has enjoyed an enormous amount of success
in a variety of engineering applications in the last decade (Krizhevsky, Sutskever, and Hinton 2012; He et al. 2016; Karras,
Laine, and Aila 2018; Silver et al. 2018). However, providing
a satisfying explanation to its sometimes surprising generalisation capabilities remains an elusive goal (Zhang et al.
2017; Du et al. 2019; Asadi, Abbe, and Verdu 2018; Goodfellow, Shlens, and Szegedy 2015). The statistical learning
theory of deep learning approaches this question by providing
a theoretical analysis of the generalisation performance of
* AL and MK acknowledge support by the German Research
Foundation (DFG) award KL 2698/2-1 and by the Federal Ministry of Science and Education (BMBF) awards 01IS18051A and
031B0770E. YL acknowledges support by the National Natural Science Foundation of China (Grant No 61806091) and the Alexander
von Humboldt Foundation.

deep neural networks (DNNs) through better understanding
of the complexity of the function class corresponding to a
given architecture or training procedure.
This field of research has enjoyed a revival since 2017
with the advent of learning guarantees for DNNs expressed
in terms of various norms of the weight matrices and classification margins (Neyshabur, Bhojanapalli, and Srebro 2018;
Bartlett, Foster, and Telgarsky 2017; Zhang, Lei, and Dhillon
2018; Li et al. 2019; Allen-Zhu, Li, and Liang 2019). Many
improvements have surfaced to make bounds non-vacuous at
realistic scales, including better depth dependence, bounds
that apply to ResNets (He, Liu, and Tao 2019), and PACBayesian bounds using network compression (Zhou et al.
2019), data-dependent Bayesian priors (Dziugaite and Roy
2018), fast rates (Suzuki 2018), and reduced dependence
on the product of spectral norms via data-dependent localisation (Wei and Ma 2019; Nagarajan and Kolter 2019). A
particularly interesting new branch of research combines
norm-based generalisation bounds with the study of how the
optimisation procedure (stochastic gradient descent) implicitly restricts the function class (Cao and Gu 2019; Du et al.
2019; Arora et al. 2019; Zou et al. 2018; Jacot, Gabriel, and
Hongler 2018; Frankle and Carbin 2019). One idea at the
core of many of these works is that the weights stay relatively
close to initialisation throughout training, reinforcing lucky
guesses from the initialised network rather than constructing
a solution from scratch. Thus, in this branch of research, it
is critical that the bound is negligible when the network approaches initialisation, i.e., the number of weights involved
is not as important as their size. This observation was first
made as early as in (Bartlett 1998).
Despite progress in so many new directions, we note that
some basic questions of fundamental theoretical importance
have remain unsolved. (1) How can we remove or decrease
the dependence of bounds on the number of classes? (2) How
can we account for weight sharing in convolutional neural
networks (CNNs)? In the present paper, we contribute to an
understanding of both questions.
Question (1) is of central importance in extreme classification (Prabhu and Varma 2014), where we deal with an
extremely high number of classes (e.g. millions). (Bartlett,
Foster, and Telgarsky 2017) showed a bound with no explicit

class dependence (except for log terms). However, this bound
is formulated in terms of the L2,1 norms of the network’s
weight matrices. If we convert the occurring L2,1 norms into
the more commonly used L2 norms, we obtain a square-root
dependence on the number of classes.
Regarding (2), (Li et al. 2019) showed a bound that accounts for weight sharing. However, this bound is valid
only under the assumption of orthonormality of the weight
matrices. The assumption of unit norm weights—which is
violated by typical convolutional architectures (GoogLeNet,
VGG, Inception, etc.)—makes it difficult to leverage the generalisation gains from small weights, and it is a fortiori not
easy to see how the bounds could be expressed in terms of
distance to initialisation.
In this paper, we provide, up to only logarithmic terms,
a complete solution to both of the above questions. First,
our bound relies only the L2 norm at the last layer, yet it
has no explicit (non-logarithmic) dependence on the number
of classes.1 In deep learning, no generalization bound other
than ours has ever achieved a lack of non-logarithmic class
dependency with L2 norms. Second, our bound accounts for
weight sharing in the following way. The Frobenius norm
of the weight matrix of each convolutional filter contributes
only once to the bound, regardless of how many times it is applied. Furthermore, our results have several more properties
of interest: (i) We exploit the L∞ -continuity of nonlinearities
such as pooling and ReLu to further significantly reduce the
explicit width dependence in the above bounds. (ii) We show
how to adapt the recent technique of loss function augmentation to our setting to replace the dependence on the spectral
norms by an empirical Lipschitz constant with respect to
well chosen norms. (iii) Our bounds also have very little explicit dependence on architectural choices and rely instead
on norms of the weight matrices expressed as distance to
initialisation, affording a high degree of architecture robustness compared to parameter-space bounds. In particular, our
bounds are negligible as the weights approach initialisation.
In parallel to our efforts, (Long and Sedghi 2020) recently
made progress on question (2), providing a remedy to the
weight-sharing problem. Their work, which is scheduled to
appear in the proceedings of ICLR 2020, is independent of
ours. This can be observed from the fact that their work and
ours were first preprinted on arXiv on the very same day.
Their approach is completely different from ours, and both
approaches have their merits and disadvantages. We provide
an extensive discussion and comparison in the sections below
and in Appendix H.

Related Work
In this section, we discuss related work on the statistical
learning theory (SLT) of DNNs. The SLT of neural networks
can be dated back to 1970s, based on the concepts of VC
dimension, fat-shattering dimension (Anthony and Bartlett
2002), and Rademacher complexities (Bartlett and Mendelson 2002). Here, we focus on recent work in the era of deep

learning.
Let (x1 , y1 ), . . . , (xn , yn ) be training examples independently drawn from a probability measure defined on the
sample space Z = X × {1, . . . , K}, where X ⊂ Rd , d
is the input dimension, and K is the number of classes.
We consider DNNs parameterized by weight matrices A =
{A1 , . . . , AL }, so that the prediction function
 can be written
FA (x) = AL σL−1 AL−1 σL−2 · · · A1 x , where L is the
depth of the DNN, Al ∈ RWl ×Wl−1 , W0 = d, WL = K, and
σi : RWi 7→ RWi is the non linearity (including any pooling
and activation functions), which we assume to be 1-Lipschitz.
When providing PAC guarantees for DNNs, a critical
quantity is the Rademacher complexity of the network obtained after appending any loss function. The first work in
this area (Neyshabur, Tomioka, and Srebro 2015) therefore
focused on bounding the Rademacher complexity of networks satisfying certain norm conditions, where the last layer
is one-dimensional. They apply the concentration lemma
and a peeling technique to get a bound on the Rademacher

QL
i
2L
complexity of the order O √
i=1 kA kFr , where kAkFr
n
denotes the Frobenius norm of a matrix A. (Golowich,
Rakhlin, and Shamir 2018) showed that this exponential
dependency on the depth can be avoided by an elegant
use of the
√ contraction
√ QL lemma to obtain bounds of the order O ( L/ n) i=1 kAi kFr .2 The most related work to
ours is the spectrally-normalized margin bound by (Bartlett,
Foster, and Telgarsky 2017) for multi-class classification.
Writing kAkσ for the spectral norm is, and M i for initialised
weights, the result is of order Õ(M/γ) with

 32
2
L
L
i>
i> 3
X
Y
kA − M k2,1
1
 , (1)
kAi kσ 
M=√
2
n i=1
ik 3
kA
i=1
σ
P

As explained
√ below, this corresponds to an implicit dependence
of the order C if the classifying vectors have comparable norms.
Our result is in line with the state of the art in shallow learning.

P

These results provide solid theoretical guarantees for
DNNs. However, they take very little architectural information into account. In particular, if the above bounds are
applied to a CNN, when calculating the squared Frobenius
norms kAi k2Fr , the matrix Ai is the matrix representing the
linear operation performed by the convolution, which implies that the weights of each filter will be summed as many
times as it is applied. This effectively adds a dependence
on the square root of the size of the corresponding activation map at each term of the sum. A notable exception
2

1

q 1

p p q
where kAkp,q =
is the (p, q)-norm,
j
i |Aij |
and γ denotes the classification margin.
At the same time as the above result appeared, the authors
in (Neyshabur, Bhojanapalli, and Srebro 2018) used a PAC
Bayesian approach to prove an analogous result 3 , where
W = max{W0 , W1 , . . . , WL } is the width:
 √
! L
! 12 
L
i
i 2
Y
X
kA + M kFr
L W
 . (2)
Õ  √
kAi kσ
kAi k2σ
γ n i=1
i=1

Note that both of these works require the output node to be one
dimensional and thus are not multiclass
3
Note that the result using formula (2) can also be derived by
expressing (1) in terms of L2 norms and using Jensen’s inequality

would be the bound in Theorem 2 of (Golowich, Rakhlin,
and
Shamir 2018), which
applies to DNN’s and scales like
√
QL
√ 
e
O
d( l=1 M (l))/ n where M (l) is an upper bound for
the l1 norm of the rows of the matrix Ãl . In this case, there
is also a lack of explicit dependence on the number of times
each filter is applied. However, the implicit dependence on
other architectural parameters such as the size of the patches
and the depth is stronger. Also, the activations are applied
element-wise, which rules out pooling and multi-class losses.
Note also that the L2 version (2) of the above bound (1)
includes a dependence on the square root of the number
of classes through the maximum width W of the network.
This square-root dependence is not favorable when the number of classes is very large. Although many efforts have
been performed to improve the class-size dependency in
the shallow learning literature (Lauer 2018; Guermeur 2002,
2007; Koltchinskii and Panchenko 2002; Guermeur 2017;
Musayeva, Lauer, and Guermeur 2019; Mohri, Rostamizadeh, and Talwalkar 2018; Lei et al. 2019), extensions of those
results to deep learning are missing so far.
In late 2017 and 2018, there was a spur of research effort on the question of fine-tuning the analyses that provided
the above bounds, with improved dependence on depth (Golowich, Rakhlin, and Shamir 2018), and some bounds for
recurrent neural networks (Chen, Li, and Zhao 2019; Zhang,
Lei, and Dhillon 2018)). Notably, in (Li et al. 2019), the authors provided an analogue of (1) for convolutional networks,
but only under some very specific assumptions, including
orthonormal filters.
Independently of our work, (Long and Sedghi 2020, to appear at ICLR 2020) address the weight-sharing problem using
a parameter-space approach. Their bounds scale roughly as
the square root of the number of parameters in the model. In
contrast to ours, their employed proof technique is more similar to (Li et al. 2019): it focuses on computing the Lipschitz
constant of the functions with respect to the parameters. The
result by (Long and Sedghi 2020) and ours, which we contrast in detail below, both have their merits. In nutshell, the
bound by (Long and Sedghi 2020) remarkably comes along
without dependence on the product of spectral norms (up
to log terms), thus effectively removing the exponential dependence on depth. Our result on the other hand comes along
without an explicit dependence on the number of parameters,
which can be very large in deep learning. As already noted in
(Bartlett 1998), this property is crucial when the weights are
small or close to the initialisation.
Lastly, we would like to point out that, over the course
of the past year, several techniques have been introduced
to replace the dependence on the product of spectral norms
by an empirical version of it, at the cost of either assuming
smoothness of the activation functions (Wei and Ma 2019)
or a factor of the inverse minimum preactivation (Nagarajan
and Kolter 2019). Slightly earlier, a similar bound to that
in (Long and Sedghi 2020) (with explicit dependence on
the number of parameters) had already been proved for an
unsupervised data compression task (which does not apply
to our supervised setting) in (Lee and Raginsky 2019). Recently, another paper addressing the weight sharing problem

appeared on arXiv (Lin and Zhang 2019). In this paper, which
was preprinted several months after (Long and Sedghi 2020)
and ours, the authors provided another solution to the weight
sharing problem, which incorporates elements from both our
approach and that of (Long and Sedghi 2020): they bound
the L2 -covering numbers at each layer independently, but use
parameter counting at each layer, yielding both an unwanted
dependence on the number of parameters in each layer (from
the parameter counting) and a dependence on the spectral
norms from the chaining of the layers.
Further related work includes the following. (Du et al.
2018) showed size-free bounds for CNNs in terms of the number of parameters for two-layer networks. In (Sedghi, Gupta,
and Long 2019), the authors provided an ingenious way of
computing the spectral norms of convolutional layers, and
showed that regularising the network to make them approach
1 for each layer is both feasible and beneficial to accuracy.
Other than the above mentioned work, several researchers
have provided interesting insights into DNNs from different
perspectives, including through model compression (Neyshabur, Bhojanapalli, and Srebro 2018), capacity control by
VC dimensions (Harvey, Liaw, and Mehrabian 2017), and
the implicit restriction on the function class imposed by the
optimisation procedure (Arora et al. 2018; Zhou et al. 2019;
Neyshabur et al. 2019, to appear; Suzuki 2018; Du et al. 2019;
Jacot, Gabriel, and Hongler 2018; Arora et al. 2019).

Contributions in a Nutshell
In this section, we state the simpler versions of our main
results for specific examples of neural networks. The general
results are described in in more technical detail in Section A.

Fully Connected Neural Networks
In the fully connected case, the bound is particularly simple:
Theorem 1 (Multi-class, fully connected). Assume that we
are given some fixed reference matrices M 1 , M 2 , . . . , M L
representing the initialised values of the weights of the
bγ (FA ) = (1/n)(#(i : F (xi )y < γ +
network. Set R
i
maxj6=yi F (xi )j )) With probability at least 1 − δ, every network FA with weight matrices A = (A1 , A2 , . . . , AL ) and
every margin γ > 0 satisfy:
bγ (FA )+
P(arg max(FA (x)j ) 6= y) ≤ R

(3)

j

e
O

maxni=1 kxi kFr RA
√
log(W̄ ) +
γ n

r

log(1/δ)
n

!
,

(4)

where W = W̄ = maxL
i=1 Wi is the maximum width of the
network, and
!
L−1
Y
L
i
RA := L max kAi, . kFr
kA kσ
(5)
i

2/3
(kAi − M i k2,1
2/3
kAi kσ
i=1

L−1
X

i=1

! 32

2/3

+

kAL kFr

2/3

maxi kAL
i, . kFr

.

(6)

Note that the last term of the sum does not explicitly
contain architectural information, and assuming bounded

L2 norms of the weights, the bound only implicitly depends
on Wi for i ≤ L − 1 (through kAi − M i k2,1 ≤
p
Wi−1 kAi −M i kFr ), but not on WL (the number of classes).
This means the above is a class-size free generalisation
bound (up to a logarithmic factor) with L2 norms of the
last layer weight matrix. This improves on the earlier L2,1
norm result in (Bartlett, Foster, and Telgarsky 2017). To
see this, let us consider a standard situation where the rows
of the matrix AL have approximately the same L2 norm,
i.e., kAL
i, . k2  a. (In Section I in the Appendix, we show
that this condition holds except on a subset of weight space
of asymptotically vanishing lebesgue measure and further
discuss possible behaviour of
√ the norms.) In this case, our
bound involves kAL kFr  WL a, which incurs a squareroot dependency on the number of classes. As a comparison,
the bound in (Bartlett, Foster, and Telgarsky 2017) involves
k(AL )> k2,1  WL a, which incurs a linear dependency on
the number of classes. If we further impose an L2 -constraint
on the last layer as kAL kFr ≤ a as in the SVM case for a
constant a (Lei et al. 2019), then our bound would enjoy a
logarithmic dependency while the bound in (Bartlett, Foster,
and Telgarsky 2017) enjoys a square-root dependency. This
cannot be improved without also changing the dependence
on n. Indeed, if it could, we would be able to get good guarantees for classifiers working on fewer examples than classes.
Furthermore, in the above bound, the dependence on the spectral norm of AL in the other terms of the sum is reduced to a
dependence on maxi kAL
i, . k2 .Both improvements are based
on using the L∞ -continuity of margin-based losses.

Convolutional Neural Networks
Our main contribution relates to CNNs. For the convenience
of the reader, we first present a simple versions of our results.
Two-layers The topic of the present paper is often notationally cumbersome, which imposes an undue burden on the
reviewers and readers. Therefore, we first present a particular
case of our bound for a two-layer network composed of a
convolutional layer and a fully connected layer with a single
input channel, with explicit pre chosen norm constraints4 .
Note that the restrictions are purely based on notational and
reader convenience: more general results are presented later
and in the supplementary material.
2-layer Notation: Consider a two-layer network with a
convolutional layer and a fully connected layer. Write d, C
for the dimensions of the input space and the number of
classes respectively. We write w for the spacial dimension of
the hidden layer after pooling5 Write A1 , A2 for the weight
matrices of the first and second layer, with the weights appearing only once in the convolutional case (thus, the matrix Ã1 representing the convolution operation presents the
weights of the matrix A1 repeated as many times as the filters
are applied). For any input x ∈ Rd , we write |x|0 for the
maximum L2 norm of a single convolutional patch of x. The
4

It is common practice to leave the post hoc step to the reader in
this way. Cf.,e.g., (Long and Sedghi 2020))
5
This is less than the number of convolutional patches in the
input and is not influenced by the number of filters applied.

network is represented by the function
F (x) = A2 σ(Ã1 x),
where σ denotes the non linearities (including both pooling
and activation functions). As above, M 1 , M 2 are the initialised weights.
Theorem 2. Let a1 , a2 , a∗ , b0 , b1 > 0. Suppose that the distribution over inputs is such that |x|0 ≤ b0 a.s. With probability > 1−δ over the draw of the training set, for every network
A = (A1 , A2 ) with weights satisfying k(A1 − M 1 )> k2,1 ≤
a1 , kA2 − M 2 kFr ≤ a2 and supc≤C kA2c, . k2 ≤ a∗ , if
supi≤n kÃ1 xn kFr ≤ b1 , then


P arg max(FA (x)j ) 6= y
(7)
j

s
bγ (FA ) + 3
≤R


1
log( 2δ )
C
+ √ R log2 (n2 D) 2 log(n),
2n
n

where C is an absolute constant,


2/3 
2/3
√
1
wa∗
b1 a2
2/3
R
= b0 a1 max
,
+
, (8)
b1
γ
γ
and the quantity in the log term is D
=
max(b0 a1 W̄ a∗ /b1 , b1 a2 C/γ) where W̄ is the number
of hidden neurons before pooling.
Remarks:
1. Just as in the fully connected case, the implicit dependence
on the number of classes is only through an L2 norm of
the full last layer matrix. b1 is a an upper bound on the L2
norms of hidden activations.
2. a1 is the norm of the filter matrix A1 , which counts each
filter only once regardless of how many times it is applied.
This means our bound enjoys only logarithmic dependence
on input size for a given stride.
3. As explained in more detail at the end of Appendix H, there
is also no explicit dependence on the size of the filters and
the bound is stable through up-resolution. In fact, there is
no explicit non logarithmic dependence on architectural
parameters, and the bounds converges to 0 as a1 , a2 tend to
zero (in contrast to parameter space bounds such as (Long
and Sedghi 2020)).
4. a∗ replaces the spectral norm of A2 , and is only equal to
the maximum L2 norm of the second layer weight vectors corresponding to each class. This improvement,comes
from better exploiting the continuity of margin based losses
with respect to the L∞ norm.
5. The spectral norm of the first layer matrix Ã1 is not neccessary and is absorbed into an empirical estimate of the
hidden layer norms. The first term in the max relates to
the estimation of the risk of a test point presenting with a
hidden layer norm higher than (a multiple of) b1 .
6. b0 refers to the maximum L2 norm of a single convolutional patch over all inputs and patches.

A result for the multi-layer case We assume we are given training and testing points
(x, y), (x1 , y1 ), (x2 , y2 ), . . . , (xn , yn ) drawn iid from
any probability distribution over Rd × {1, 2, . . . , C}. We
suppose we have a convolutional architecture so that for each
filter matrix Al ∈ Rml ×dl from layer l − 1 to layer l, we can
construct a larger matrix Ãl representing the corresponding
(linear) convolutional operation. The 0th layer is the input,
whist the Lth layer is the output/loss function. We write wl
for the spacial width at layer l, Wl for the total width at layer
l (including channels), and W for maxl Wl . For simplicity
of presentation, we assume that the activation functions are
composed only of ReLu and max pooling.
Theorem 3. With probability ≥ 1−δ, every network FA with
fliter matrices A = {A1 , A2 , . . . , AL } and every margin
γ > 0 satisfy:


P arg max(FA (x)j ) 6= y
j

bγ (FA ) + O
e
≤R

RA
√ log(W̄ ) +
n

r

log(1/δ)
n

Empirical spectral norms; Lipschitz augmentation

!
,

(9)

where W̄ is the maximum number of neurons in a single layer
(before pooling) and
2/3

RA =

L
X
(Tl )2/3
l=1

for where Tl =
√
Bl−1 (X)k(Al − M l )> k2,1 wl max

QU

kÃu kσ0
BU (X)

u=l+1

U ≤L

if l ≤ L − 1 and for l = L, Tl =
BL−1 (X) L
kA − M L kFr .
γ
Here, wl is the spacial width at layer l after pooling. By
convention, bL = γ, and for any layer l1 , Bl1 (X) :=
maxi F 0→ll (xi ) l denotes the maximum l2 norm of any con1
volutional patch of the layer l1 activations, over all inputs.
For l ≤ L − 1, kÃl kσ0 ≤ kÃl k denotes the maximum spectral
norm of any matrix obtained by deleting, for each pooling
window, all but one of the corresponding rows of Ã. In particL
ular, for l = L, kÃL kσ0 = ρL maxi kAL
i, . k2 .Here Ai, . denotes
the i’th row of AL , and k . k2 denotes the Frobenius norm6 .
Similarly to the two-layer case above, a notable property
of the above bounds is that the norm involved is that of the
matrix Al (the filter) instead of Ãl (the matrix representing
the full convolutional operation), which means we are only
adding the norms of each filter once, regardless of how many
patches it is applied to. As a comparison, although the genrealization bound in (Bartlett, Foster, and Telgarsky 2017)
6

also applies to CNNs, the resulting bound would involve the
e ignoring the structure of CNNs, yielding an
whole matrix A
√
extra factor of Ol−1 instead of Ol , where Ol denotes the
number of convolutional patches in layer l:p
Through exploiting weight sharing, we remove a factor of Ol−1 in the lth
term of the sum compared to a standard the result in (Bartlett,
Foster,
p and Telgarsky 2017), and we remove another factor
of Ol−1 /wl through exploitation of the L∞ continuity of
max pooling and our use of L∞ covering numbers.
A further significant improvement is in replacing the factor
Ql−1
kXk2,2 i=1 kÃi kσ from the classic bound by Bl−1 (X),
which is the maximum L2 norm of a single convolutional
p
patch. This implicitly removes another factor of Ol−1 , this
time from the local connection structure of convolutions.
We note that it is possible to obtain more simple bounds
without a maximum in the definition of Tl by using the spectral norms to estimate the norms at the intermediary layers.

NB: A simplified
version of the above Theorem can be obtained
Q
√
where Tl = i6=l kÃi kσ k(Al − M l )> k2,1 wl /γ for l ≤ L − 1
QL−1
and TL = i=1 kÃi kσ kAL − M L kFr . See Appendix E and in
particular equation (55).

A commonly mentioned weakness of norm-based bounds is
the dependence on the product of spectral norms from above.
In the case of fully connected networks, there has been a
lot of progress last year on how to tackle this problem. In
particular, it was shown in (Nagarajan and Kolter 2019) and
in (Wei and Ma 2019) that the products of spectral norms can
be replaced by empirical equivalents, at the cost of either a
factor of the minimum preactivation in the Relu case (Nagarajan and Kolter 2019), or Lipschitz constant of the derivative
of the activation functions if one makes stronger assumptions (Wei and Ma 2019). In the appendix, we adapt some
of those techniques to our convolutional, ReLu situation and
find that the quantity ρA
l can be replaced in our case by:

A,x

ρA
l = max maxi maxl̃≥l

i
ρl →l
1
2
Bl2 (X) , maxi

A,x

maxl̃≥l

θl →li
1
2
El2 (X)

where El (X) denotes the minimum preactivation (or distance to the max/second max in max pooling) at layer l
A,xi
i
for over every input, ρA,x
l1 →l2 (resp. θl1 →l2 ) is the Lipschitz
l1 →l2
constant of gradient of F
with respect to the norms
| . |∞,l1 and | . |l2 (resp. | . |∞,l1 and | . |∞ ). These quantities
can be computed explicitly: if M = ∇F 0→l1 (xi ) F l1 →l2
so that locally around F 0→l1 (xi ), F l1 →l2 (x) = M x, then
i
i
0
θlA,x
= kM > k1,∞ and ρA,x
l1 →l2 = maxM kM k1,2 where
1 →l2
0
M runs over all sub matrices of M obtained by keeping only
the rows corresponding to a single patch of layer l2 .
Note that an alternative approach is to obtain tighter
bounds on the worst-case Lipschitz constant. Theorem 7
in the Appendix is a variation of Theorem 3 involving the
explicit worst case Lipschitz constants across layers instead
of spectral norms. These quantities can then be bounded, or
made small via regularisation using recent techniques (cf, e.g.
Fazlyab et al. (2019); Latorre, Rolland, and Cevher (2020)).

General proof strategy
Some key aspects of our proofs and general results rely on
using the correct norms in activation spaces. On each activation space, we use the norm | . |∞ to refer to the maximum

absolute value of each neuron in the layer, the norm | . |l to
refer to the the maximum l2 norm of a single convolutional
patch (at layer l) and | . |∞,l for the maximum l2 norm of a
single pixel viewed as a vector over channels. Using these
norms, we can for each pair of layers l1 , l2 define the quantity
ρl1 →l2 as the Lipschitz constant of the subnetwork F l1 →l2
with respect to the norms | . |∞,l1 and | . |l2 . Using those norms
we can formulate a cleaner extention of Theorem 3 where the
quantity RA can be replaced by
2/3
 L−1
X
ρl→l̃
l
l
Bl−1 (X)kA − M k2,1 max
l̃>l Bl̃ (X)
l=1

2/3 3/2
BL−1 (X) L
+
kA − M L kFr
,
γ
where for any layer l1 , Bl1 (X) := maxi F 0→ll (xi ) l1 denotes
the maximum l2 norm of any conv. patch of the layer l1 , over
all inputs. BL (X) = γ. Our proofs derive this result, and the
previous Theorems follow. See Section A, Theorem 77 .
In the rest of this Section, we sketch the general strategy
of the proof, focusing on the (crucial) one-layer step. At this
point, we need to introduce notation w.r.t. the convolutional
channels: we will collect the data matrix of the previous layer
in the form of a tensor X ∈ Rn×U ×d consisting of all the
convolutional patch stacked together: if we fix the first index
(sample i.d.) and the second index (patch i.d.), we obtain
a convolutional patch of the corresponding sample. For a
set of weights A ∈ Rd×m , the result of the convolutional
operation is written XA where is defined by (XA)u,i,j =
Pd
o=1 Xu,i,o Ao,j for all u, i, j.
A first step in bounding the capacity of NN’s is to provide
a bound on the covering numbers of individual layers.
Definition 1 (Covering number). Let V ⊂ Rn and k · k
be a norm in Rn . The covering number w.r.t. k · k, denoted by N (V, , k · k), is the minimum cardinality m
of a collection of vectors v1 , . . . , vm ∈ Rn such that
supv∈V minj=1,...,m kv−vj k ≤ . In particular, if F ⊂ RX
n
is a function class and X =
√ (x1 , x2 , . . . , xn ) ∈ X are
data points, N (F(X), , (1/ n)k · k2 ) is the minimum cardinality m of a collection of functions F 3 f 1 , . . . , f m :
X → R such that for any f ∈ F, there exists j ≤ m
Pn
2
such that i=1 (1/n) f j (xi ) − f (xi ) ≤ 2 . Similarly,
N (F(X), , k · k∞ ) is the minimum cardinality m of a
collection of functions F 3 f 1 , . . . , f m : X → R such
that for any f ∈ F, there exists j ≤ m such that i ≤
n, f j (xi ) − f (xi ) ≤ .

patch, sample, output channel)" combination is mapped into
a higher dimensional space to be viewed as a single data
point. A further reduction in dependence on architectural
parameters is achieved by leveraging the L∞ -continuity of
margin-based loss functions and pooling. We will need the
following result from (Zhang 2002) (Theorem 4, page 537).
Proposition 4. Let n, d ∈ N, a, b > 0. Suppose we are given
n data points collected as the rows of a matrix X ∈Rn×d ,
with kXi, . k2 ≤ b, ∀i = 1, . . . , n. For Ua,b (X) = Xα :
kαk2 ≤ a, α ∈ Rd , we have
36a2 b2
log N (Ua,b (X), , k . k∞ ) ≤
log2
2

Our boundedness assumptions on worst-case Lipschitz constants remove some of the interactions between layers, yielding
simpler results than (Wei and Ma 2019; Nagarajan and Kolter 2019)


8abn
+ 6n + 1 .


Note this proposition is stronger than Lemma 3.2 in (Bartlett, Foster, and Telgarsky 2017). In the latter, the cover can
be chosen independently of the data set, and the metric used
in the covering is an L2 average over inputs. In Proposition 4,
the covering metric is a maximum over all inputs, and the
data set must be chosen in advance, though the size of the
cover only depends (logarithmically) on the sample size8 .
Using the above on the auxiliary problem based on (input,
convolutional patch, ouput channel) triplets, we can prove the
following covering number bounds for the one-layer case:
Proposition 5. Let positive reals (a, b, ) and positive integer
m be given. Let the tensor X ∈ Rn×U ×d be given with
∀i ∈ {1, 2, . . . , n}, ∀u ∈ {1, 2, . . . , U }, kXi,u, . k2 ≤ b.
For any choice of reference matrix M , we have

log N {XA : A ∈ Rd×m , kA − M kFr ≤ a}, , k . k∞



8ab
36a2 b2
log2
+ 7 mnU ,
≤
2

where the norm k . k∞ is over the space Rn×U ×m .
Sketch of proof: By translation invariance, it is clear
that we can suppose M = 0. We consider the problem of
bounding
the L∞ covering number of {(vi> X j )i≤I,j≤J :
P
2
2
j
d×n
for all j) with only
i≤I kvi k2 ≤ a } (where X ∈ R
logarithmic dependence on n, I, J. Here, I plays the role of
the number of output channels, while J plays the role of the
number of convolutional patches. We now apply the above
Proposition 4 on the nIJ × dI matrix constructed as follows:
























If we apply classical results on linear classifiers as is done
in (Bartlett, Foster, and Telgarsky 2017) (where results on L2
covering numbers are used) by viewing a convolutional layer
as a linear map directly, we cannot take advantage of weight
sharing. In this work, we circumvent this difficulty by applying results on the L∞ covering numbers of classes of linear
classifiers to a different problem where each "(convolutional
7



8

X1
0
...
0
X2
0
...
0
X3
...
XJ
0
...
0

0
X1
...
0
0
X2
...
0
0
...
0
XJ
...
0

...
...
...
...
...
...
...
...
...
...
...
...
...
...

0
0
...
X1
0
...
...
X2
0
...
0
0
...
XJ

>











 ,











We note that the proof is also much more obscure, although it is
far more approachable to prove an analogous result with a squared
log term instead, by going via the shattering dimension.

with the corresponding vectors being constructed as
(v1 , v2 , . . . , vI ) ∈ RdI .
If we compose the linear map on Rn×d represented by
(v1 , v2 , . . . , vI )> with k real-valued functions with L∞
Lipschitz constant 1, the above argument yields comparable
. k2 covering number of the composition, losbounds on the k√
ing a factor of k only (for the last layer, k = 1, and for
convolutional layers, k is the number of neurons in the layer
left after pooling).
The proposition above is only enough to deal with a purely
l2 version of our bounds. To prove Theorem 3, which involves
k . k2,1 norms, we must show the following extension:
Proposition 6. Let positive reals (a, b, ) and positive integer
m be given. Let the tensor X ∈ Rn×U ×d be given with
∀i ∈ {1, 2, . . . , n}, ∀u ∈ {1, 2, . . . , U }, kXi,u, . k2 ≤ b. For
any fixed M :

log N {XA : A ∈ Rd×m , kA − M k2,1 ≤ a}, , k . k∗



8ab
64a2 b2
log
≤
+
7
mnU
,
2
2

where the norm k . k∗ over the space Rn×U ×m is defined by
kY k∗ = maxi≤n maxj≤U

Pm

k=1

2
Yi,j,k

1
2

.

Sketch of proof: We first assume fixed bounds on the L2
.
norms kAi, k2 = ai of each filter, and use Proposition 5 with
m = 1 for each output channel with a different granularity
i . We then optimize over the choice ofPi , and make the
result apply to the case where only a = i ai ≥ kAk2,1 is
fixed in advance by l1 covering the set of possible choices for
(a1 , a2 , . . . , am ) for each a, picking a cover for each such
choice and taking the union. We accumulate a factor of 2
because of this approach, but to our knowledge,
it is not
√
possible to rescale the inputs by factors of ai as was done
in (Bartlett, Foster, and Telgarsky 2017), as the input samples
in an L∞ covering number bound must be chosen in advance.
We can now sketch the proof of the Theorem 2 : we use
the loss function

l(xi , yi ) = max λb1 (kσ(Ã1 xi )k2 − b1 ),

λγ max(A2 σ(Ã1 xi ))j − (A2 σ(Ã1 xi ))yi ,
j6=y

where for any θ > 0 the ramp loss λθ is defined by
λθ = 1 + min(max(x, −θ), 0)/θ. This loss incorporates
the following two failure scenarios: (1) the L2 norm of the
hidden activations exceed a multiple of b1 (2) The activations behave normally but the network still outputs a wrong
prediction. Since pooling is continuous w.r.t. the infty norm,
the above results for the one layer case applied to a layer
yields an  cover of hidden layer w.r.t to the L∞ norm. The
contributions to the error source (1) therefore follows directly
from the first layer case. The contribution of the 1st layer
cover error to (2) must be multiplied 1/γ and the Lipschitz
constant of√A2 with respect to the L∞ norms, which we estimate by wa∗ since the Euclidean norm of the√
deviation
from the cover at the hidden layer is bounded by w times
the deviation in ||∞,1 norm 9 .
9
This norm is a supremum over the spacial locations of the L2
norms over the channel directions.

Remarks and comparison to concurrent work
We have addressed the main problems of weight sharing and
dependence on the number of classes. As mentioned earlier,
(Long and Sedghi 2020) have recently studied the former
problem independently of us. It is interesting to provide a
comparison of their and our main results, which we do briefly
here and in more detail in the Appendix.
The bound in (Long and Sedghi 2020) scales like
q
P
s −log(γ))+log(1/δ)
W( L
l=1 l
,
n

where sl is an upper bound on
the spectral norm of the matrix corresponding to the lth layer,
γ is the margin, and W is the number of parameters, taking weight sharing into account by counting each parameter
of convolutional filters only once. The idea of the proof is
to bound the Lipschitz constant of the map from the set of
weights to the set of functions represented by the network,
and use dimension-dependent results on covering numbers of
finite dimensional function classes. Remarkably, this doesn’t
require chaining the layers, which results in a lack of a non
logarithmic dependence
PL on the product of spectral norms.
Note that the
term
comes from a log term via the
l=1 sl P
Q
inequality (1 + si ) ≤ exp( si ).
On the other hand, the bound scales at least as the square
root of the number of parameters, even if the weights are
arbitrarily close
p to initialisation. In contrast, our bound (3)
scales like O( 1/n) up to log terms when the weights approach initialisation. Furthermore, if we fix an explicit upper
bound on the relevant norms (cf.Theorem 11) 10 , the bound
then converges to zero as the bounds on the norms go to
zero. In a refined treatment via the NTK literature (cf. (Arora
et al. 2019)), explicit bounds would be provided for those
quantities via other tools.
Finally, note that the main advantages and disadvantages
of our bounds compared to (Long and Sedghi 2020) are connected through a tradeoff in the proof where one can decide
which quantities go inside or outside the log. In particular, it
is not possible to combine the advantages of both. We refer
the reader to Appendix H for a more detailed explanation.
C

Conclusion
We have proved norm-based generalisation bounds for deep
neural networks with significantly reduced dependence on
certain parameters and architectural choices. On the issue
of class dependency, we have completely bridged the gap
between the states of the art in shallow methods and in deep
learning. Furthermore, we have, simultaneously with (Long
and Sedghi 2020), provided the first satisfactory answer to
the weight sharing problem in the Rademacher analysis of
neural networks. Contrary to independent work, our bounds
are norm-based and are negligible at initialisation.

References
Allen-Zhu, Z.; Li, Y.; and Liang, Y. 2019. Learning and
Generalization in Overparameterized Neural Networks, Going Beyond Two Layers. In Wallach, H.; Larochelle, H.;
Beygelzimer, A.; d Alché-Buc, F.; Fox, E.; and Garnett, R.,
10
The bounds in (Long and Sedghi 2020) and other works deal
only with this case, leaving the post hoc case to the reader

eds., Advances in Neural Information Processing Systems 32,
6155–6166. Curran Associates, Inc.
Anthony, M.; and Bartlett, P. 2002. Neural Network Learning:
Theoretical Foundations. ISBN 978-0-521-57353-5. doi:
10.1017/CBO9780511624216.
Arora, S.; Du, S. S.; Hu, W.; Li, Z.; and Wang, R. 2019.
Fine-Grained Analysis of Optimization and Generalization
for Overparameterized Two-Layer Neural Networks. arXiv
e-prints arXiv:1901.08584.
Arora, S.; Ge, R.; Neyshabur, B.; and Zhang, Y. 2018.
Stronger Generalization Bounds for Deep Nets via a Compression Approach. In Dy, J.; and Krause, A., eds., Proceedings of the 35th International Conference on Machine
Learning, volume 80 of Proceedings of Machine Learning
Research, 254–263. Stockholm, Sweden: PMLR.
Asadi, A.; Abbe, E.; and Verdu, S. 2018. Chaining Mutual Information and Tightening Generalization Bounds. In Bengio,
S.; Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-Bianchi,
N.; and Garnett, R., eds., Advances in Neural Information
Processing Systems 31, 7234–7243. Curran Associates, Inc.
Bartlett, P.; and Shawe-taylor, J. 1998. Generalization Performance of Support Vector Machines and Other Pattern
Classifiers.
Bartlett, P. L. 1997. For Valid Generalization the Size of the
Weights is More Important than the Size of the Network. In
Mozer, M. C.; Jordan, M. I.; and Petsche, T., eds., Advances
in Neural Information Processing Systems 9, 134–140. MIT
Press.
Bartlett, P. L. 1998. The sample complexity of pattern classification with neural networks: the size of the weights is more
important than the size of the network. IEEE Transactions on
Information Theory 44(2): 525–536. doi:10.1109/18.661502.
Bartlett, P. L.; Foster, D. J.; and Telgarsky, M. J. 2017.
Spectrally-normalized margin bounds for neural networks.
6240–6249. Curran Associates, Inc.
Bartlett, P. L.; and Mendelson, S. 2002. Rademacher and
Gaussian complexities: Risk bounds and structural results.
Journal of Machine Learning Research 3(Nov): 463–482.
Brutzkus, A.; Globerson, A.; Malach, E.; and Shalev-Shwartz,
S. 2018. SGD Learns Over-parameterized Networks that
Provably Generalize on Linearly Separable Data. In International Conference on Learning Representations.

Du, S. S.; Zhai, X.; Poczos, B.; and Singh, A. 2019. Gradient Descent Provably Optimizes Over-parameterized Neural
Networks. In International Conference on Learning Representations.
Dziugaite, G.; and Roy, D. 2018. Data-dependent PAC-Bayes
priors via differential privacy .
Fazlyab, M.; Robey, A.; Hassani, H.; Morari, M.; and Pappas,
G. J. 2019. Efficient and Accurate Estimation of Lipschitz
Constants for Deep Neural Networks. CoRR abs/1906.04893.
Frankle, J.; and Carbin, M. 2019. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. In
International Conference on Learning Representations.
Giné, E.; and Guillou, A. 2001. On consistency of kernel
density estimators for randomly censored data: Rates holding
uniformly over adaptive intervals. Annales de l’Institut Henri
Poincare (B) Probability and Statistics 37: 503–522. doi:
10.1016/S0246-0203(01)01081-0.
Golowich, N.; Rakhlin, A.; and Shamir, O. 2018. SizeIndependent Sample Complexity of Neural Networks. In
Bubeck, S.; Perchet, V.; and Rigollet, P., eds., Proceedings of
the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, 297–299. PMLR.
Goodfellow, I.; Shlens, J.; and Szegedy, C. 2015. Explaining and Harnessing Adversarial Examples. In International
Conference on Learning Representations.
Guermeur, Y. 2002. Combining Discriminant Models
with New Multi-Class SVMs. Pattern Analysis & Applications 5(2): 168–179. ISSN 1433-7541. doi:10.1007/
s100440200015.
Guermeur, Y. 2007. VC Theory of Large Margin MultiCategory Classifiers. Journal of Machine Learning Research
8: 2551–2594.
Guermeur, Y. 2017. Lp-norm Sauer–Shelah lemma for margin multi-category classifiers. Journal of Computer and
System Sciences 89: 450 – 473. ISSN 0022-0000. doi:
https://doi.org/10.1016/j.jcss.2017.06.003.
Harvey, N.; Liaw, C.; and Mehrabian, A. 2017. Nearly-tight
VC-dimension bounds for piecewise linear neural networks.
In Kale, S.; and Shamir, O., eds., Proceedings of the 2017
Conference on Learning Theory, volume 65 of Proceedings of
Machine Learning Research, 1064–1068. Amsterdam, Netherlands: PMLR.

Cao, Y.; and Gu, Q. 2019. Generalization Bounds of
Stochastic Gradient Descent for Wide and Deep Neural Networks. arXiv e-prints arXiv:1905.13210.

He, F.; Liu, T.; and Tao, D. 2019. Why ResNet Works?
Residuals Generalize. arXiv e-prints arXiv:1904.01367.

Chen, M.; Li, X.; and Zhao, T. 2019. On Generalization
Bounds of a Family of Recurrent Neural Networks.

He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual
Learning for Image Recognition. In CVPR, 770–778. IEEE
Computer Society.

Du, S. S.; Wang, Y.; Zhai, X.; Balakrishnan, S.;
Salakhutdinov, R. R.; and Singh, A. 2018. How Many
Samples are Needed to Estimate a Convolutional Neural
Network? In Bengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.; and Garnett, R., eds., Advances in
Neural Information Processing Systems 31, 373–383. Curran
Associates, Inc.

Jacot, A.; Gabriel, F.; and Hongler, C. 2018. Neural Tangent
Kernel: Convergence and Generalization in Neural Networks.
CoRR abs/1806.07572.
Karras, T.; Laine, S.; and Aila, T. 2018. A Style-Based
Generator Architecture for Generative Adversarial Networks.
CoRR abs/1812.04948.

Koltchinskii, V.; and Panchenko, D. 2002. Empirical Margin Distributions and Bounding the Generalization Error
of Combined Classifiers. Ann. Statist. 30(1): 1–50. doi:
10.1214/aos/1015362183.

Neyshabur, B.; Li, Z.; Bhojanapalli, S.; LeCun, Y.; and
Srebro, N. 2019, to appear. The role of over-parametrization
in generalization of neural networks. In International Conference on Learning Representations.

Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. ImageNet Classification with Deep Convolutional Neural Networks. In Pereira, F.; Burges, C. J. C.; Bottou, L.; and
Weinberger, K. Q., eds., Advances in Neural Information
Processing Systems 25, 1097–1105. Curran Associates, Inc.

Neyshabur, B.; Tomioka, R.; and Srebro, N. 2015. NormBased Capacity Control in Neural Networks. In GrÃŒnwald,
P.; Hazan, E.; and Kale, S., eds., Proceedings of The 28th
Conference on Learning Theory, volume 40 of Proceedings
of Machine Learning Research, 1376–1401. Paris, France:
PMLR.

Latorre, F.; Rolland, P.; and Cevher, V. 2020. Lipschitz constant estimation of Neural Networks via sparse polynomial
optimization. In International Conference on Learning Representations. URL https://openreview.net/forum?id=rJe4_
xSFDB.
Lauer, F. 2018. Error bounds with almost radical dependence
on the number of components for multi-category classification, vector quantization and switching regression. In Conférence sur l’Apprentissage automatique (CAp) - French Conference on Machine Learning (FCML), Proc. of the French
Conference on Machine Learning (CAp/FCML). Rouen,
France.
Lee, J.; and Raginsky, M. 2019. Learning Finite-Dimensional
Coding Schemes with Nonlinear Reconstruction Maps. SIAM
Journal on Mathematics of Data Science 1: 617–642. doi:
10.1137/18M1234461.
Lei, Y.; Dogan, Ü.; Zhou, D.; and Kloft, M. 2019. DataDependent Generalization Bounds for Multi-Class Classification. IEEE Trans. Information Theory 65(5): 2995–3021.
doi:10.1109/TIT.2019.2893916.
Li, X.; Lu, J.; Wang, Z.; Haupt, J.; and Zhao, T. 2019. On
Tighter Generalization Bounds for Deep Neural Networks:
CNNs, ResNets, and Beyond.
Lin, S.; and Zhang, J. 2019. Generalization Bounds for
Convolutional Neural Networks.
Long, P. M.; and Sedghi, H. 2020. Size-free generalization
bounds for convolutional neural networks. In International
Conference on Learning Representations.
Mohri, M.; Rostamizadeh, A.; and Talwalkar, A. 2018. Foundations of Machine Learning. Adaptive Computation and Machine Learning. Cambridge, MA: MIT Press, 2 edition. ISBN
978-0-262-03940-6.
Musayeva, K.; Lauer, F.; and Guermeur, Y. 2019.
Rademacher complexity and generalization performance of
multi-category margin classifiers. Neurocomputing 342: 6 –
15. ISSN 0925-2312. doi:https://doi.org/10.1016/j.neucom.
2018.11.096. Advances in artificial neural networks, machine
learning and computational intelligence.
Nagarajan, V.; and Kolter, J. Z. 2019. Deterministic PACBayesian generalization bounds for deep networks via generalizing noise-resilience. CoRR abs/1905.13344.
Neyshabur, B.; Bhojanapalli, S.; and Srebro, N. 2018. A
PAC-Bayesian Approach to Spectrally-Normalized Margin
Bounds for Neural Networks. In International Conference
on Learning Representations. openreview.net.

Pisier, G. 1980-1981. Remarques sur un résultat non publié
de B. Maurey. Séminaire Analyse fonctionnelle (dit "MaureySchwartz") Talk:5.
Platen, E. 1986. Pollard, D.:Convergence of stochastic processes. (Springer series in statistics). Springer-Verlag, New
York - Berlin - Heidelberg - Tokyo 1984, 216 pp., 36 illustr., DM 82. Biometrical Journal 28(5): 644–644. doi:
10.1002/bimj.4710280516.
Prabhu, Y.; and Varma, M. 2014. FastXML: A Fast, Accurate
and Stable Tree-classifier for Extreme Multi-label Learning.
In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14,
263–272. New York, NY, USA: ACM. ISBN 978-1-45032956-9. doi:10.1145/2623330.2623651.
Scott, C. 2014. Rademacher Complexity. Lecture Notes
Statistical Learning Theory.
Sedghi, H.; Gupta, V.; and Long, P. M. 2019. The Singular
Values of Convolutional Layers. In International Conference
on Learning Representations.
Silver, D.; Hubert, T.; Schrittwieser, J.; Antonoglou, I.; Lai,
M.; Guez, A.; Lanctot, M.; Sifre, L.; Kumaran, D.; Graepel,
T.; Lillicrap, T.; Simonyan, K.; and Hassabis, D. 2018. A
general reinforcement learning algorithm that masters chess,
shogi, and Go through self-play. Science 362(6419): 1140–
1144. ISSN 0036-8075. doi:10.1126/science.aar6404.
Suzuki, T. 2018. Fast generalization error bound of deep
learning from a kernel perspective. In Storkey, A.; and
Perez-Cruz, F., eds., Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics,
volume 84 of Proceedings of Machine Learning Research,
1397–1406. Playa Blanca, Lanzarote, Canary Islands: PMLR.
Talagrand, M. 1994. Sharper Bounds for Gaussian and Empirical Processes. The Annals of Probability 22(1): 28–76.
ISSN 00911798.
Talagrand, M. 1996. New concentration inequalities in
product spaces. Inventiones mathematicae 126(3): 505–563.
ISSN 1432-1297. doi:10.1007/s002220050108.
Wei, C.; and Ma, T. 2019. Data-dependent Sample Complexity of Deep Neural Networks via Lipschitz Augmentation. In
Wallach, H.; Larochelle, H.; Beygelzimer, A.; d'Alché-Buc,
F.; Fox, E.; and Garnett, R., eds., Advances in Neural Information Processing Systems 32, 9725–9736. Curran Associates,
Inc.

Zhang, C.; Bengio, S.; Hardt, M.; Recht, B.; and Vinyals,
O. 2017. Understanding deep learning requires rethinking
generalization.
Zhang, J.; Lei, Q.; and Dhillon, I. S. 2018. Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization. In ICML, volume 80 of Proceedings of Machine
Learning Research, 5801–5809. PMLR.
Zhang, T. 2002. Covering Number Bounds of Certain Regularized Linear Function Classes. J. Mach. Learn. Res. 2: 527–
550. ISSN 1532-4435. doi:10.1162/153244302760200713.
Zhou, W.; Veitch, V.; Austern, M.; Adams, R. P.; and Orbanz, P. 2019. Non-vacuous Generalization Bounds at the
ImageNet Scale: a PAC-Bayesian Compression Approach.
In International Conference on Learning Representations.
openreview.net.
Zou, D.; Cao, Y.; Zhou, D.; and Gu, Q. 2018. Stochastic
Gradient Descent Optimizes Over-parameterized Deep ReLU
Networks. CoRR abs/1811.08888.

Figure 1: Illustration of architecture for one layer

A

Notation and general results

Notation
We use the following notation to represent linear layers with weight sharing such as convolution. Let x ∈ RU ×w , A ∈ Rm×d and
S 1 , S 2 , . . . , S O be O ordered subsets of ({1, 2, . . . , w} × {1, 2, . . . , U }) each of cardinality d11 , where we will denote by Sio the
Pd
ith element of S o . We will denote by ΛA (x) the element of Rm×O such that ΛA (x)j,o = i=1 XSio Aj,i . In a typical example
the sets S 1 , S 2 , . . . , S O represent the image patches where the convolutional filters are applied, and Λ would be represented
via the "tf.nn.conv2d" function in Tensorflow. We will also write Ãl for the matrix in R(Ul−1 wl−1 )×(Ol−1 ml ) that represents the
convolution operation ΛAl .
To represent a full network, we suppose that we are given a number L ∈ N of layers, 7L + 2 numbers
m1 , m2 , . . . , mL , d1 , d2 , . . . , dL , ρ1 , ρ2 , . . . , ρL ,
w0 , w 1 , . . . , w L , U 0 , U 1 , . . . , U L , O 1 , O 2 , . . . , O L ,
PL
l,o
⊂ {1, 2, . . . , Ul } × {1, 2, . . . , wl } (for l ≤ L, o ≤ Ol ), and L − 1
and k1 , k2 , . . . , kL , as well as
l=0 Ol ordered sets S

functions Gl : Rml ×Ol−1 → RUl ×wl (for l = 1, 2, . . . , L) which are ρl -Lipschitz with respect to the l∞ norm.
The architecture above can help us represent a feedforward neural network involving possible (intra-layer) weight sharing as
FA1 ,A2 ,...,AL : RU0 ×w0 → RUL ×wl : x 7→
(GL ◦ ΛAL ◦ GL−1 ◦ ΛAL−1 ◦ . . . G1 ◦ ΛA1 )(x),

where for each l ≤ L, the weight Al is a matrix in Rml ×dl . We will also write F l1 →l2 for the subnetwork that computes the
function from the l1th layer activations to the l2th layer activations. As usual, offset terms can be accounted for by adding a dummy
dimension of constants at each layer (this dimension must belong to S l,o for each o). We provide a quick table of notations in
Figure 1.
Some key aspects of our proofs and general results rely on using the correct norms in activation spaces. On each activation
space Rwl ×Ul , we will make use of the following three norms:
1. The l∞ norm: |x|∞ = supu∈{1,...,wl }×{1,...,Ul } |xu |
qP
2
2
2. The | . |l norm: |x|l = supo≤Ol
i≤Ol xi , i.e. the maximum l norm of a single convolutional patch.
qP
2
2
3. The | . |∞,l norm: |x|∞,l = supo≤wl
k≤Ul xo,k , the maximum l norm of a single pixel viewed as a vector over channels.
Remark 1. In covering number arguments, we will use the same notation to refer to the norms on (activation,input) space
induced by the above norms after taking a supremum over inputs.

Main result with Global Lipschitz bounds
Both of the results in the Section "contributions in a Nutshell" follow from the following result.
11
We suppose for notational simplicity that all convolutional filters at a given layer are of the same size. It is clear that the proof applies to
the general case as well.

Notation

Meaning

Gl
Al
ΛAl
Ãl
Ol
ml

Activation + pooling at layer l
Filter matrix at layer l
Convolution operation relative to filter matrix Al
Matrix representing ΛAl (Has repeated weights in conv. net)
Number of convolutional patches at layer l
# of channels at layer l before nonlinearity
(=# of output channels at layer l − 1)
Classification margin
oth convolutional patch at layer l
Number of spatial dimensions at layer l (No pooling =⇒ wl = Ol−1 )
Number of channels after nonlinearity
Lipschitz constant of Gl
Width (after pooling) at layer l
Maximum network width (after any pooling)
Maximum network width (before any pooling)
Total number of parameters
Size of convolutional patches corresponding to the operation ΛAl
Max l2 norm of a pixel across channels
2
Max l norm of a convolutional patch (across channels)

γ
S l,o
wl
Ul
ρl
Wl = Ul wl
W = maxl Wl
W̄ = maxl Ol−1 ml
W
dl
| . |∞,l
| . |l

Table 1: Table of notations for quick reference
Theorem 7. Assume that pooling does not occur over different channels, with probability ≥ 1 − δ, every network FA with
weight matrices A = {A1 , A2 , . . . , AL } and every margin γ > 0 satisfy:


bγ (FA )
P arg max(FA (x)j ) 6= y ≤ R
j

(RA + L)
√
log(W̄ ) +
n

e
+O

r

log(1/δ)
n

!
,

(10)

where W is the maximum number of neurons in a single layer (after pooling) and RA :=
 L−1
2/3 
2/3 3/2
X
ρl→l̃
BL−1 (X) L
l
l
L
Bl−1 (X)kA − M k2,1 max
+
kA − M kFr
,
γ
l̃>l Bl̃ (X)
l=1

where for any layer l1 , Bl1 (X) := maxi F 0→ll (xi ) l1 denotes the maximum l2 norm of any convolutional patch of the layer l1
activations, over all inputs. BL (X) = γ.
Here ρl1 →l2 is the Lipschitz constant of the map F l1 →l2 with respect to the norms | . |∞,l1 and | . |l2

B

The one layer case

A key aspect of the proof is that we can use proposition 4 to obtain an L∞ -covering of the map represented by a convolutional
layer. Indeed, by viewing each (sample, convolutional patch, output channel) trio as an individual data point, we can, for each ,
find N filters f1 , . . . , fN with kfi kF r ≤ a ∀i such for any convolutional map represented by the filter f (with kf kF r ≤ a),
there exists a uf ∈ {1, 2, . . . , N } such that for any input xi , any convolutional patch S, and any output channel j, the outputs of
f and fuf corresponding to this (input, patch, channel) combination differ by less than . More precisely, we can now prove
Proposition 5
Proof of proposition 5. This follows immediately from Lemma 4 applied to the nmU data points in Rd×m (considered as a
simple vector space with the Hadamard product used as the scalar product) defined by, for all δ ∈ {1, 2, . . . , m} × {1, 2, . . . , d},
(xu,i,j )δ = (Xju )δ2 for δ1 = i and (xu,i,j )δ = 0 otherwise, and the function class
{FA : Rd×m → R : x 7→ x
where

denotes the Hadamard product.

Before we proceed, we will need the following Proposition:

A; kAk2 ≤ a},

Proposition 8 (cf. (Anthony and Bartlett 2002; Bartlett and Shawe-taylor 1998; Pisier 1980-1981)). Let Bβ denote the ball of
radius β in Rd with respect to the L1 norm. We have
 2
β
.
log (N (Bβ , , k k2 )) ≤ 2 log(2d)
(11)

Proof. Wlog, β = 1. Let e1 , . . . , ed denote the standard basis in Rd , we will show that for any integer k ∈ N and any
P
Pd
a = (a1 , a2 , . . . , ad ) ∈ Rd+ with i ai ≤ 1, there exists (k1 , k2 , . . . , kd ) with ki ∈ N such that κ := i=1 ei kki satisfies
kκ − ak22 ≤

1
.
k

Let (W1 , . . . , Wk ) be k iid random variables with P(W1 = ei ) = ai . Define W =
Pd
i=1 ai ei = a. Thus we have

E(kW − ak22 ) =

k
X

1 
E
k2
i=1

1
k

Pk

i=1

Wi . We have E(W ) = E(W1 ) =


X
ka − Wi k2 +
ha − Wi , a − Wj i

=

k
X
1
E
ka − Wi k2
k2
i=1

≤

1
1
E(kW1 k2 ) = .
k
k

i6=j

!
=

 1

1
E ka − W1 k2 =
E(kW1 k2 ) − kak2
k
k
(12)

By the probabilistic method, it follows that there is a choice (k1 , k2 , . . . , kd ) such that kκ − ak22 ≤ k1 , as expected.
It follows that one can find a cover of the ball Bβ with
 size N , where
 N is the number of choices of (k1 , k2 , . . . , kd ) with
Pd
d
+
k
−
1
β2
ki ∈ Z and i=1 |ki | = k and k = d 2 e. There are 2d
≤ (2d)k such choices. The result follows.
d−1
With this in our toolkit, we can prove the extention of the one layer case to the L2,1 norm with an extra covering umber
argument:
Proof of Proposition 6. First, note that the case m = 1 follows from Proposition 5 (also with m = 1).
Thus any set of m positive real numbers a = (a1 , a2 , . . . , am ) and any (1 , 2 , . . . , m ) with i ≤  for all i, we can
find covers Ci ⊂ {A ∈ Rd : kAk2 ≤ ai } such that for all A1 , A2 , . . . , Am ∈ Rd such that kAi k2 ≤ ai ∀i ≤ m,
there exist Ā1 , Ā2 , . . . , Ām ∈ Rd such that kĀi k2 ≤ ai ∀i ≤ m and kXAi − X Āi k∞ ≤ 1 , and for all i, log(#Ci ) ≤



36a2i b2
log2 8ai b + 6 nU + 1 (since i ≤ ).
2i
Pm
Writing a = i=1 ai , the cardinality of C(a1 ,a2 ,...,am ) = C1 × C2 , . . . × Cm is bounded above by

X

m
a2i
8ab
2
+ 6 nU + 1
.
(13)
36b log2

2
i=1 i
Pm
and the product cover is an  cover of {XA : A ∈ Rd×m , kA . ,i k2 ≤ ai ∀i} with respect to the k . k∗ norm, where2 =  i=1 2i .
−2a2i
Applying the above to 3/4 and calculating the Lagrangian to optimize over the i ’s, we obtain the condition
∝ 2i ,
3
√
√
a
i
which yields i = (3/4) P ai =  ai /a, which pluggind back into formula (13), yields,



a2 b2
8ab
log(#(C1 × C2 , . . . , Cm )) ≤ 36
log
+
7
nU
(14)
2
(3/4)2 2




a 2 b2
8ab
≤ 64 2 log2
+ 7 nU
(15)


Pm
Of course, we do not know in advance the choice of (a1 , . . . , am ) such that i=1 ai = a, so we must take extra steps
Ptommake the
cover post hoc with respect to this choice. To do this we can choose the setD to be an /4b cover of (a1 , . . . , am ) : i=1 ai = a
with respect to the L1 norm. By Proposition 8, we can ensure #(D) ≤ 16a2 b2 /2 log(2m) ≤ 64a2 b2 /2 log(m) (since we
can assume m ≥ 2 and wlog 16a2 b2 /2 ≥ 1).

Clearly, the cardinality of the union ∪a∈D C1 × . . . × Cm is bounded by



8ab
64a2 b2
log2
+ 7 mnU ,
2

so we only need to show that it constitutes an  cover of {XA : A ∈ Rd×m , kAk2,1 ≤ a}. To see this, let A ∈ Rd×m be given
with kAk2,1 ≤ a. Pick (a1 , a2 , . . . , am ) to be the closest element of D to (kA . ,1 k2 , kA . ,2 k2 , . . . , kA . ,m k2 ) in terms of the
L∞ norm. Then pick the element Ã of C(a1 ,a2 ,...,am ) closest to Ā ∈ Rd×m defined by, ∀i, Ā . ,i = A . ,i if kA . ,i k2 ≤ ai and
Ā . ,i = kAa. ,ii k2 A . ,i otherwise. Clearly we have kXA − X Ãk∗ ≤ kXA − X Āk∗ + kX Ā − X Ãk∗ ≤ /5 + 4/5 = , which
completes the proof.
As a corollary of the above, we are now in a position to prove the one layer case.
w
Corollary 9. Let n, O, m, w, U be natural numbers, let G : RO × Rm → Rq
× RU be ρ-Lipschitz with respect to the | . |∗ norms
qP
P
m
U
O
m
2
2
defined by |x|∗ = supi∈{1,2...,O}
and x ∈ Rw × RU
j=1 xij and |x|∗ = supi∈{1,2...,w}
j=1 xij for x ∈ R × R
respectively (for instance, a combination of purely spacial pooling and elementwise relu satisfies this condition with ρ = 1). We
also write | . |∗ for the norm supi≤n |Xi, . |∗ for X ∈ Rn × Rw × RU or X ∈ Rn × RO × Rm . For any X ∈ Rn×O×d such that
.
kX i,o, k22 ≤ b2 (∀i, o), we have for any fixed reference matrix M :



 64a2 b2
8abnmO
d×m
.
log N G(XA) : A ∈ R
, kA − M k2,1 ≤ a , , | |∗ ≤ 2 2 log2
+ 7Omn
(16)
 ρ
ρ

C

Generalisation bound for fixed norm constraints

Once the one layer case is taken care of, we will now need to chain the covering number bounds of each layer, taking care to
control the excess | . | norms at each intermediary layer. To this effect, we have the following Proposition.
Proposition 10. Let L be a natural number and a1 , . . . , aL > 0 be real numbers. Let V0 , V1 , . . . , VL be L+1 vector spaces each
endowed with two norms | . |∞,l and | . |l for 0 ≤ l ≤ L. Let B1 , B2 , . . . , BL be L vector spaces with norms k . k1 , k . k2 , . . . , k . kL
and B1 , B2 , . . . , BL be the balls of radii a1 , a2 , . . . , aL in the spaces B1 , B2 , . . . , BL with the norms k . k1 , k . k2 , . . . , k . kL
respectively12 .
Suppose also that for each l ∈ {1, 2, . . . , L} we are given an operator F l : Vl−1 × Bl → Vl : (x, A) → FAl (x), continuous
with respect to the norms k . kl and | . |l . For each l1 , l2 with l2 > l1 and each Al1 ,l2 = (Al1 +1 , . . . , Al2 ) ∈ B l1 ,l2 :=
Bl1 +1 × Bl1 +2 × . . . Bl2 , let us define
l1 →l2
l1 →l2
l2
l1
FA
l1 ,l2 : Vl1 → Vl2 : x → FAl1 ,l2 (x) = FAl2 ◦ . . . ◦ FAl1 (x),
l
0→l
L
for all l, FA
and FA = FA
.
L = FAL
L
1
2
L
For each A = (A , A , . . . , A ) ∈ B L := B1 × B2 × . . . , BL , and for each l ∈ {1, 2, . . . , L}, the (worst case) Lipschitz
constant of F l1 →l2 with respect to the norms k . k∞,l1 and | . |l2 is denoted by ρA
l1 →l2 .
We suppose the following conditions are satisfied:
For all l ∈ {1, 2, . . . , L}, all b > 0, all Z1, Z2 , . . . , Zn ∈ Vl−1 such that |Zi |l−1 ≤ b ∀i and all  > 0, there exists a subset
Cl (b, Z, ) ⊂ Bl such that

log(# (Cl (b, Z, ))) ≤

Cl,,n a2l b2
,
2

(17)

where Cl,,n is some function of l, , n, and, for all A ∈ Bl , there exists an Ā ∈ Cl (b, ) such that
FAl (Z) − FĀl (Z)

∞,l

≤

∀i.

(18)

For any 0 <  < 1, any b = (b0 , b1 , b2 , . . . , bL ) with bl ≥ 1 ∀l and bL = 1, any set of positive number ρl+ (for l ≤ L) and
for any x1 , . . . , xn ∈ V0 such that |xi |0 ≤ b0 ∀i, there exists a subset C,b,n of B L such that for all A = (A1 , A2 , . . . , AL ) ∈
l
B := B L such that ρA
∀l2 ≥ l1 13 , there exists a Ā ∈ C,b,X such that, for any i such that FA
(xi ) l ≤ bl ∀l,
l1 →l2 ≤ ρl1 bl2
we have
12
The proof works with B1 , B2 , . . . , BL being arbitrary sets, but we formulate the problem as above to aid the intuitive comparison with the
areas of application of the Proposition.
13
Note that ρl→l is not necessarily 1, as the norms k . k∞,l and k . kl are different.

0→l
0→l
FA
(xi ) − FĀ
(xi ) l ≤ bl
l
l

(∀l < L, ∀i) and

l
(X) l ≤ 2bl .
FĀ

(19)

Furthermore, we have

 1
 23 3
 1
2
L
L
2
2
2 X
X
Cl, al bl−1 ρl
Cl,
al bl−1 ρl
L



  ≤4

 .
log #(C,b,X ) ≤ 4 

2

l=1

(20)

l=1

PL
l
Proof. For l = 1, . . . , L, let l = α
l=1 αl = 1.
ρl , where the αl > 0 will be determined later satisfying
For any X = (x1 , . . . , xn ), we define the covers Dl ⊂ Bl for l ≤ L by induction by Dl =
0→l−1
∪A∈D1 ×...×Dl−1 C(2bl , {FA
(xi ) : i ≤ n}, l ). Let us write also D := D1 × D2 . . . DL , and let us write dl for the
cardinality of Dl . We prove the equations (59) by induction. The case l = 1 follows directly from the definition of D1 and the
assumption on the xi ’s. For the induction case, let us assume the inequalities hold for each u ≤ l − 1. We have for any j ≤ n,

0→l
0→l
FA
(xj ) − FĀ
(xj ) l ≤

L
X

F(0→l
(x ) − F(0→l
(x )
Ā1 ,Ā2 ,...,Āi−1 ,Ai ,...,Al ) j
Ā1 ,Ā2 ,...,Āi ,Ai+1 ,...,Ai ) j

i=1

≤

l
X

0→i
0→i
ρA
i→l F(Ā1 ,Ā2 ,...,Āi−1 ,Ai ) (xj ) − F(Ā ,Ā
1

2 ,...,Āi

i=1

≤

l
X

) (xj )

l

i

l

ρA
i→l

i=1

X
αi
αi
≤
≤ bl ,
ρA
i→l A
ρi
ρi→l /bl
i=1

(21)

where at the first line, we have used the triangle inequality, at the second line, the definition of ρi→l , and at the last line, the fact
that ρA
∀l2 ≥ l1 . Now, by the triangle inequality, we obtain:
l1 →l2 ≤ ρl1
0→l
0→l
FĀ
(xj ) l ≤ FA
(xj ) l + bl ≤ 2bl ,

(22)

which concludes the proof by induction.
To finish the proof of the proposition, we just need to calculate the bound on the caridinality of D:

log(#(D)) =

L
X

log(#(Dl )) =

l=1

≤

L
X

log(dl )

l=1

L
L
X
X
Cl, a2l b2l−1
Cl, a2l b2l−1
4
=
4
2l
( αρll )2
l=1

l=1

L
4 X Cl, a2l b2l−1 ρ2l
= 2
.

αl2
l=1

Optimizing over the αl ’s subject to

PL

l=1

αl = 1, we find the Lagrangian condition

L
2Cl, a2l ρ2l
−
∝ (1)L
l=1 ,
αl3
l=1

yielding
p
2
( Cl, al ρl ) 3
αl = PL √
.
2
3
i=1 ( Ci ai ρi )
Substituting back into equation (23), we obtain

(23)

"
log # ({FA (X) : A ∈ D}) ≤ 4


L √
X
Ci ai bi−1 ρi 3
2

L
X



i=1

l=1

"

=

#2

L
 23
4 X p
C
a
b
ρ
i
i
i−1
i
2 i=1

!2−4/3
p
Cl, al bl−1 ρl


#3
,

as expected. The last inequality follows from Jensen’s inequality.
The next step is to use the above, together with the classic Rademacher theorem 25 and Dudley’s Entropy integral, to obtain a
result about large margin multi-class classifiers.
Theorem 11. Suppose we have a K class classification problem and are given n i.i.d. observations
(x1 , y1 ), (x2 , y2 ), . . . , (xn , yn ) ∈ RU0 ×w0 ⊗ {1, 2, . . . , K} drawn from our ground truth distribution (X, Y ), as well
as a fixed architecture as described in Section A, where we assume the last layer is fully connected and has width K and
corresponds to scores for each class. Suppose also that with probability one |x|0 ≤ b0 . Suppose we are given numbers
a1 , a2 , . . . , aL , β = (b0 , b1 , . . . , bL = γ) and ρl1 →l2 (for l1 , l2 ≤ L). For any δ > 0, with probability > 1 − δ over the draw of
the training set, for any network A = (A1 , A2 , . . . , AL ) satisfying ∀l : k(Al )> k2,1 ≤ al ∧ ρA
l1 →l2 ≤ ρl1 →l2 , we have
!
P

arg max (FL (x))j 6= y
j∈{1,2,...,K}

s
1

log( 2δ )
8
1536 
n − #(Iβ,γ )
+ + √ R log2 (32Γn2 + 7W̄ n) 2 log(n) + 3
,
≤
n
n
2n
n


I = i ≤ n : (f (xi ))yi − max((f (xi ))j ) > γ ∧ ∀l ≤ L : F 0→l (xi ) l ≤ bl ,

where

(24)

j6=yi

R2/3 :=

L
X

(al bl−1 ρl+ )

2/3

,

and

l=1
L

Γ := max (bl−1 al Ol−1 ml ρl+ ) ,
l=1

ρl+ = maxL
i=l

(25)

ρl→i
bi .

Proof. As explained in the sketch in the main text, we apply the Rademacher theorem to the loss function:




L−1
0→l
l(xi , yi ) = max sup λBl F
(xi ) l − 2Bl , λγ max(FA (x))j − FA (x)y
j6=y

l=1

(26)

Writing H for the function class defined by l(x, F A (x)) for F A satisfying the conditions of the Theorem, since Y 7→
maxj6=y (Y )j − Yy is 2 Lipschitz with respect to the l∞ norm, and l(x, y) = 1 for any x such that there exists l such that
|F 0→l (x)|l ≥ 2bl , Propositions 10 and 9 guanrantee that the covering number of H satisfies
" L 

2/3 #3
X al (2bl−1 )
al (2bl−1 )nml Ol−1
2
log(N (H, )) ≤ 4 × 64 × 2
ρl+ log2 8
+ 7Ol−1 ml n

(/ρl+ )
l=1

12

2

≤ 2 R log2 (32Γn/ + 7W̄ n)
Applying the Rademacher Theorem 25, we now obtain

(27)

!
P

arg max (FL (x))j 6= y

≤ E (l(x, y))

j∈{1,2,...,K}

s
log( 2δ )
l(x
,
y
)
i i
≤ i=1
+3
+ 2R̂n (l(x, y))
n
2n
s
log( 2δ )
n − #(I)
+3
+ 2R̂n (l(x, y)).
≤
n
2n
Pn

(28)

Now, by Dudley’s Entropy integral 73 with α =

1
n,

we have
Z 1q
12
R̂n (l(x, y)) ≤ 4α + √
log N (F|S, , k . kp )
n α
Z 1p
4 √ √ 12 12
log2 (32Γn/ + 7W̄ n)
≤ + R 2 √
n

n n1
q
4 √ 768
≤ + R√
log2 (32Γn2 + 7W̄ n) log(n)
n
n
(29)

Plugging this back into equation 28, we obtain the desired result.

D

Proofs of post hoc result and asymptotic results

The next step from Theorem 11 to Theorem 7 is now mostly a question of applying classical techniques, namely, splitting the
space of possible choices of parameters (a1 , . . . , aL , b0 , . . . , bL , γ, ρ0→1 , . . . , ) into different regions and using a union bound.
The following Lemma summarizes the techniques in question:
Lemma 12. Let RN denote a random variable indexed some finite dimensional vector N 14 . Let
γ1 (N ), . . . , γl (N ), N1 (N ), . . . , NL (N ) be some (positive) statistics of N . Suppose there exists a function F : Rl×L
→R
+
which is monotonically decreasing in γi for all i ≤ l and monotonically increasing in Ni for i ≤ L, such that the following
statement holds:
For any γ1 , . . . , γl , N1 , . . . , NL , and for any δ > 0, with probability > 1 − δ, we have that for any N such that γi (N ) ≤ γi
for all i and Ni (N ) ≤ Ni for all i,
s
log(1/δ)
RN ≤ f (γ1 , . . . , γl , N1 , . . . , NL ) + C1
,
(30)
C2
for some constants C1 , C2 .
For any fixed choice of β1 , . . . , βL and κ1 , . . . , κl ∈ N, we have for any δ > 0 that with probability greater than 1 − δ, for
any N ,
1
1
RN ≤ f (min(γ1 (N )/2, ), . . . , min(γl (N )/2, ), N1 (N ) + β1 , . . . , NL (N ) + βL )
κ1
κl
v
u


l
L
X
X
C1 u
Ni (N )
t
+√
log(1/δ) +
log(2κi (N )/γi (N )) + 2
log 2 +
.
βi
C2
i=1
i=1

(31)

Proof. For any j1 , . . . , jl , n1 , . . . , nL ∈ N, define
δj1 ,...,jl ,n1 ,...,nL = Ql

i=1

Define γiji =

κi
2ji

2ji

δ
QL

i=1

ni (ni + 1)

.

and Nini = ni βi . By applying equation (30), we obtain that with probability ≥ 1 − δj1 ,...,jl ,n1 ,...,nL ,
s
RN ≤ f (γ1j1 , . . . , γljl , N1n1 , . . . , NLnL ) + C1

log(1/δj1 ,...,jl ,n1 ,...,nL )
,
C2

(32)

P
Note that j1 ,...,jl ,n1 ,...,nL δj1 ,...,jl ,n1 ,...,nL = δ.
Thus, by a union bound, we obtain that with probability ≥ 1 − δ, for any choice of j1 , . . . , jl , n1 , . . . , nL ∈ N, we have
s
log(1/δj1 ,...,jl ,n1 ,...,nL )
RN ≤ f (γ1j1 , . . . , γljl , N1n1 , . . . , NLnL ) + C1
.
(33)
C2
14

We assume that the map from N to RN is sufficiently well behaved for the random variables RN to be jointly defined on the same
probability space for all values of N , as is the case where N represents the parameters of a neural network and RN is the misclassification
probability on a test point

For any network N , we can apply this for the choice of j1 , . . . , jl which are smallest whist still guaranteeing
1
2ji
1
≤ ji =
,
γi (N )
κi
γi
and
Ni (N ) ≤ ni βi .
For this choice, we have

γiji

≥ min(γ(N )/2, κ1i )) for all i ≤ l and Nini ≤ Ni (N ) + βi , thus by the properties of f ,

f (γ1j1 , . . . , γljl , N1n1 , . . . , NLnL )
1
1
≤ f (min(γ1 (N )/2, ), . . . , min(γl (N )/2, ), N1 (N ) + β1 , . . . , NL (N ) + βL ).
(34)
κ1
κl

 


Ni (N )
Ni (N )
)
2
+
≤
2
+
, and thus
Furthermore, we also have 2ji ≤ 2κi /γi (N ) and ni (ni + 1) ≤ 1 + Niβ(N
βi
βi
i
s
C1

v
u


l
L
X
X
log(1/δj1 ,...,jl ,n1 ,...,nL )
C1 u
Ni (N )
≤ √ tlog(1/δ) +
.
log(2κi (N )/γi (N )) + 2
log 2 +
C2
βi
C2
i=1
i=1

(35)

Plugging equations (34) and (35) back into equation (33) yields the desired result.
Using this, we obtain the following:
Theorem 13. Suppose we have a K class classification problem and are given n i.i.d. observations
(x1 , y1 ), (x2 , y2 ), . . . , (xn , yn ) ∈ RU0 ×w0 ⊗ {1, 2, . . . , K} drawn from our ground truth distribution (X, Y ), as well
as a fixed architecture as described in Section A, where we assume the last layer is fully connected and has width K and
corresponds to scores for each class. For any δ > 0, with probability > 1 − δ over the draw of the training set, for any network
A = (A1 , A2 , . . . , AL ) we have
!
arg max (FL (x))j 6= y

P

j∈{1,2,...,K}

s
1


log( 2δ )
n − #(Iβ,γ )
8
1536
≤
+ + √ RA log2 (32Γn2 + 7W̄ n) 2 log(n) + 3
n
n
2n
n
v
!
u L




A
>k
u1 X
ρ
B
(X)
k(A
−
M
)
l−1
2,1
l+
log 2 +
,
+ log 2 +
+ log 2 +
+ 3t
n
L
L
L

(36)

(37)

l=1

where

I=


i ≤ n : (f (xi ))yi − max((f (xi ))j ) > γ ∧ ∀l ≤ L : F 0→l (xi ) l ≤ Bl (X) ,
j6=yi

L 
X

2/3
1
1
1
A
:=
(k(A − M ) k2,1 + )(Bl−1 (X) + ))(ρl+ + )
, and
L
L
L
l=1


1
1
1
L
Γ := max (Bl−1 (X) + )(k(A − M )> k2,1 + )Ol−1 ml (ρA
+
)
,
l+
l=1
L
L
L

2/3
RA

>

ρA

(38)

L
l→i
ρA
l+ = maxi=l Bi (X) .
Here, X denotes the design matrix containing the sample points, BL = γ > 0 is arbitrary and Bl (X) for l ≤ L − 1 can
A
be chosen arbitrarily so that Bl (X) ≥ 1in a way that depends on X, with the choice Bl (X) = max maxi≤n F0→l
(xi ) l , 1
yielding



I=

i ≤ n : (f (xi ))yi − max((f (xi ))j ) > γ .
j6=yi

Proof. We split the space using Lemma 12 for the parameters B0 , B1 , . . . , BL−1 , ρA
l+ . Note that the bound is increasing in all of
those parameters, se we can treat them all as a "Ni "s from Lemma 12. Setting all the ki ’s to L yields the result. Note that the
dependence on γ is hidden in the definition of ρA
l+ . The bound doesn’t go to zero as γ as a result of the need to estimate the risk
of intermediary activations being too large.
As explained in the main text, if one is willing to forgo the gains obtained from the sparsity of the connections inside the
definition of Bl , then one can obtain bounds that scale like 1/γ.
√A ), and
Proof of Theorem 7. This is simply a question of reducing to the Õ notation. Note that log(n2 ), √1n , Γ are all Õ( R
n
2
2
asymptotically, log2 (32Γn + 7W̄ n) ≤ log2 (32Γn ) + log2 (7W̄ n), thus we only have to take care of the last line.√For this,
√A ), yielding the result (with a factor of
note that each of the concerned log terms inside the square root are also Õ( R
R from
n
the sum).

Similarly, in the case where the spectral norms are well controlled, we have the following result:
Theorem 14. Suppose we have a K class classification problem and are given n i.i.d. observations
(x1 , y1 ), (x2 , y2 ), . . . , (xn , yn ) ∈ RU0 ×w0 ⊗ {1, 2, . . . , K} drawn from our ground truth distribution (X, Y ), as well
as a fixed architecture as described in Section A, where we assume the last layer is fully connected and has width K and
corresponds to scores for each class. For any δ > 0, with probability > 1 − δ over the draw of the training set, for any network
A = (A1 , A2 , . . . , AL ) we have
!
arg max (FL (x))j 6= y

P

j∈{1,2,...,K}

s
s  
2
1


log(
)
n − #(Iβ,γ )
8
3072
2
δ
≤
+ + √ RA log2 (64Γn2 + 7W̄ n) 2 log(n) + 3
+ log
n
n
2n
γn
n
v
u L


u1 X
log (2 + LBl−1 (X)) + log (2 + k(A − M )> k2,1 ) + log 2 + LkÃkσ0 ,
+ 3t
n

(39)

(40)

l=1

where


i ≤ n : (f (xi ))yi − max((f (xi ))j ) > γ ∧ ∀l ≤ L : F

I=

2/3
RA

:=

0→l

j6=yi

L
X
l=1

1
1
(k(A − M ) k2,1 + )(Bl−1 (X) + )) max
l0
L
L
>

Ql0

i=l+1 (kÃkσ

0

+

1
L)

!2/3

Bl0 (X)
Ql 0

1
1
Γ := max (Bl−1 (X) + )(k(A − M )> k2,1 + )Ol−1 ml max
l=1
l0
L
L
L


(xi ) l ≤ Bl (X) ,

i=l+1 (kÃkσ

Bl0 (X)

,
0

+

1
L)

and
!
.

(41)

Here, X denotes the design matrix containing the sample points, BL = γ > 0 is arbitrary and Bl (X) for l ≤ L − 1 can be
A
chosen arbitrarily so that Bl (X) ≥ 1 in a way that depends on X, with the choice Bl (X) = max maxi≤n F0→l
(xi ) l , 1
yielding


I=

i ≤ n : (f (xi ))yi − max((f (xi ))j ) > γ .
j6=yi

Proof. We apply theorems 11 and Lemma 12 for the parameters γ, B0 , . . . , BL , a1 , . . . , aL , s1 , s2 , . . . , sL where si is a bound
Ql2
A
on kÃi kσ0 : note first that if kÃi kσ0 ≤ si for all i, then ρA
l1 →l2 ≤
l=l1 +1 ρl si for all l1 ≤ l2 , and if furthermore γ ≥ γ̄ and
Ql2
√
ρl si
1 +1
Bl (X) ≥ B̄l , for all l ≤ L − 1, we have ρA
l1 maxl2 l=lB̄
, with B̄L = γ̄. Thus we can apply Lemma 1215 with
l1 + ≤
l
2

the βi = L1 and γ being treated as a decreasing variable with k = n, to obtain the required result (γ̄ ≥ γ A /2 and B̄l ≥ Bl /2,
1
furthermore, the case γ ≤ 2n
is trivial since the RHS is ≥ 1 ).
15
Technically, we are applying a slight variation where f can have factors that are either increasing or decreasing in the same variable Ni ,
and the term f (min(γ1 (N )/2, κ11 ), . . . , min(γl (N )/2, κ1l ), N1 (N ) + β1 , . . . , NL (N ) + βL ) is replaced by an evaluation of f where each
factor involving Ni chooses whichever of (Ni , Ni + βi ) maximises it

We can now proceed to the proof of theorem 3:
Proof. The only difference between this proof and that of Theorem 7 is in the treatment
of the sum of log terms at the last line.
√
>
>
>
>
For this, note that k(A − M ) k2,1 ≤ k(A) k2,1 + k(M ) k2,1 ≤ k(M )k2,1 + W̄ k(A)> k2 ≤ k(M )> k2,1 + W̄ kAkσ0 ≤
k(M )> k2,1 + W̄ 3/2 kAkσ0 ), thus
q P
QL
L
R
1
>
√A
0
l=1 log (2 + Lk(A − M ) k2,1 ) is Õ( l=1 kÃkσ ρl ), which is Õ( n ) as expected.
n

E

Simpler results with explicit norms

In this Section, we show slight variations of our bounds sticking closer to (Bartlett, Foster, and Telgarsky 2017) by only using
norms involved at each individual layer or pair of layer. Theorem 3 follows from the theorems below. Whilst the results don’t
seem to follow directly from the above, the treatment is extremely similar. Suppose we are given some norms | . |Ll on the
activation spaces and some norms | . |L∗l on the weight spaces such that | . |Ll ≤ | . |l and the Lipschitz constant of Λl (A) with
√
respect to | . |Ll−1 and | . |Ll is bounded by |A|L∗ and | . |∞,l ≤ kl | . |Ll
Note that for any s, a simple argument on internal vs. external covering numbers shows that Theorem 9 can be adapted to
yield a cover such that kÃkσ0 ≤ s for any A in the cover, at the cost of a factor of 2 in .
We have the following simplified variation of Theorem 10:
Proposition 15. Let L be a natural number and a1 , . . . , aL > 0 be real numbers. Let V0 , V1 , . . . , VL be L+1 vector spaces, with
arbitrary norms | . |0 , | . |1 , . . . , | . |L , let B1 , B2 , . . . , BL be L vector spaces with norms k . k1 , k . k2 , . . . , k . kL and B1 , B2 , . . . , BL
be the balls of radii a1 , a2 , . . . , aL in the spaces B1 , B2 , . . . , BL with the norms k . k1 , k . k2 , . . . , k . kL respectively. Suppose also
that for each l ∈ {1, 2, . . . , L} we are given an operator F l : Vl−1 × Bl → Vl : (x, A) → FAl (x). Suppose also that there exist
real numbers ρ1 , ρ2 , . . . , ρL > 0 such that the following properties are satisfied.
1. For all l ∈ {1, 2, . . . , L} and for all A ∈ Bl , the Lipschitz constant of the operator FAl with respect to the norms | . |l−1 and
| . |l is less than ρl .
2. For all l ∈ {1, 2, . . . , L}, all b > 0, and all  > 0, there exists a subset Cl (b, ) ⊂ Bl such that
log(# (Cl (b, ))) ≤

Cl, a2l b2
,
2

(42)

where Cl, is some function of l,  and, and, for all A ∈ Bl and all X ∈ Vl−1 such that |X|i−1 ≤ b, there exists an Ā ∈ Cl (b, )
such that
FAl (X) − FĀl (X) l ≤ .

(43)

For each l and each Al = (A1 , A2 , . . . , Al ) ∈ B l := B1 × B2 × . . . , Bl , let us define
l
l
l
2
1
FA
l : V0 → VL : x → FAl (x) = FAl ◦ . . . ◦ FA2 ◦ FA1 ,
L
L
and FA = FA
such that for all A = (A1 , A2 , . . . , AL ) ∈ B := B L , there exists
L . For each  > 0, there exists a subset C of B
an Ā ∈ C such that the following two conditions are satisfied.

l
l
FA
≤ QL
l (X) − FĀl (X)
l



j=l+1

log #(C) ≤

|X|21
2

ρj

(∀l ≤ L),

(44)

and


 1  23 3
 1 2
L
L
L
2
2 Y
X
X
C
a
C2 a
l
|X|


1
2
 l,   ≤ L2
 l, l  .
ρ2i 
ρ
i
ρl
2 i=1
ρl
i=1
L
Y

l=1

l=1

In particular, for any X ∈ V0 and any  > 0, the following bound on the (, | . |L )-covering number of {FA (X) : A ∈ B L }
holds.

log N ({FA (X) : A ∈ B}, , | . |L ) ≤

|X|2
L2 2 0


L
Y
i=1

ρ2i

L
X




i=1

1
2
Cl,
al

ρi

2
 .

(45)

PL
, where the αl > 0 will be determined later satisfying l=1 αl = 1.


Ql−1
Using the second assumption, let us pick for each l the subset Cl = Cl |X|0 i=1 ρi , l satisfying the assumption. Let us
define also the set C := C1 × C2 × . . . × CL ⊂ B.
Claim 1
For all A ∈ B, there exists a Ā ∈ C such that for all l ≤ L,
Proof. For l = 1, . . . , L, let l =

QLαl

ρi

i=l+1



l
l
FA
(X) − FĀ
(X) l ≤ QL

j=l+1

ρj

.

(46)

Proof of Claim 1
To show this, observe first that for any 1 ≤ l ≤ L and for any A1 , A2 , . . . , Al ,
F

l−1

2

1

◦ . . . ◦ F ◦ F (X) l ≤ |X|0

l−1
Y

ρi ,

(47)

i=1

and therefore, by definition of Cl , we have that for any A1 , A2 , . . . , Al−1 , {FA1 ,A2 ,...,Al−1 ,Al (X) : Al ∈ Cl } is an (l , | . |l ) cover
of {FA1 ,A2 ,...,Al−1 ,Al (X) : Al ∈ Bl }.
Let us now fix A1 , A2 , . . . , AL and define Āl ∈ Cl inductively so that FĀl l (FĀ1 ,Ā2 ,...,Āl−1 (X)) is an element of
{FAl (FĀ1 ,Ā2 ,...,Āl−1 (X)) : A ∈ Cl } minimising the distance to FĀ1 ,Ā2 ,...,Āl−1 ,Al (X) in terms of the | . |l norm.
We now have for all l ≤ L:

|FA (X) − FĀ (X)|l ≤

l
X

F(Ā1 ,Ā2 ,...,Āi−1 ,Ai ,...,Al ) (X) − F(Ā1 ,Ā2 ,...,Āi ,Ai+1 ,...,Al ) (X)

i=1

≤

l
X

l
Y

ρj F(Ā1 ,Ā2 ,...,Āi−1 ,Ai ) (X) − F(Ā1 ,Ā2 ,...,Āi ) (X)

i=1 j=i+1

≤

l
l
X
Y
i=1 j=i+1

l
X

1

ρj i = QL

j=l+1

ρj

i=1

αi ≤ QL



j=l+1

ρj

l

l

,

(48)

as expected.
This concludes the proof of the claim.
To prove the proposition, we now simply need to calculate the cardinality of C:

log N ({FA (X) : A ∈ B}, , | . |L ) ≤ log(#(C)) ≤

L
X

log(#(Cl ))

l=1

=


Ql−1 2
L C a2 |X|
X
l, l
0
i=1 ρi
l=1

2l


2
Ql−1 2 QL
L C a2 |X|
ρ
ρ
X
l,
0
i
i
l
i=1
i=l+1
1
≤ 2

αl2
l=1

QL
L
|X|20 i=1 ρ2i X Cl, a2l
=
.
2
ρ2l αl2

(49)

l=1

Optimizing over the αl ’s subject to

PL

l=1

αl = 1, we find the Lagrangian condition


2Cl, a2l /ρ2l
−
αl3

L

∝ (1)L
l=1 ,

l=1

yielding
p
2
( Cl, al /ρl ) 3
αl = PL √
.
2
3
i=1 ( Ci ai /ρi )

Substituting back into equation (23), we obtain
" L √
!2−4/3
p
 23 #2 X
L
2 X
C
a
ρ
C
a
l,
l
i
i
i=1 i
log N ({FA (X) : A ∈ B}, , | . |L ) ≤
2
ρi
ρl
i=1
l=1


!2/3 3
p
QL
L
Cl, al
|X|20 i=1 ρ2i X
 ,
≤
2
ρl
|X|20

QL

l=1

as expected. The second inequality follows by Jensen’s inequality.
Using this, we obtain similarly:
Theorem 16. Suppose we have a K class classification problem and are given n i.i.d. observations
(x1 , y1 ), (x2 , y2 ), . . . , (xn , yn ) ∈ RU0 ×w0 ⊗ {1, 2, . . . , K} drawn from our ground truth distribution (X, Y ), as well
as a fixed architecture as described in Section A, where we assume the last layer is fully connected and has width K and
corresponds to scores for each class. For any δ > 0, with probability > 1 − δ over the draw of the training set, for any network
A = (A1 , A2 , . . . , AL )
!
arg max (FL (x))j 6= y

P

j∈{1,2,...,K}

s
s  
1


log( 2δ )
n − #(Iβ,γ )
8
1536
2
2
2
≤
+ + √ RA log2 (32Γn + 7W̄ n) log(n) + 3
+ log
n
n
2n
γn
n
v
u L






u1 X
kAkL∗l
k(A − M )> k2,1
t
+3
log 2 + sup kxi kL0 + log 2 +
+ log 2 +
,
n
L
L
i

(50)

(51)

l=1

where

I=



RA :=

1
L sup kxi kL0
γ
i


Γ :=

i

j6=yi



 1/2
 X

 2
L
Y
p
1  

(k(A − M )> kFr + 1 ) kl
+1 
kAkL∗i +
 ,
L
L
l=1



and

i6=l






Y
1
1 
L
+ 1 max Ol−1 ml (k(A − M )> k2,1 + )
,
kAkL∗i +
l=1
L
L


sup kxi kL0


i ≤ n : (f (xi ))yi − max((f (xi ))j ) > γ ,

(52)

i6=l

where

I=


i ≤ n : (f (xi ))yi − max((f (xi ))j ) > γ .
j6=yi

Note that there can be pooling over channels in this case, with the constant kl being determined after pooling.
√
Furthermore, if κl denotes instead the constant such that | . |l,∞ ≤ κl | . |l , the quantity RA in the above equation can be
replaced by


RA :=

sup kxi kL0
i



 3/2
 X

 2/3
L
Y
1  

(k(A − M )> k2,1 + 1 )√κl
+1 
kAkL∗i +
 ,
L
L
l=1

i6=l

(53)
After passing to the asymptotic regime (taking the choice | . |LL = | . |∞ so that kÃL kL∗L = maxi kAL
i, . k2 ):

Theorem 17. For training and testing points (x, y), (x1 , y1 ), (x2 , y2 ), . . . , (xn , yn ) as usual drawn iid from any probability distribution over Rd ×{1, 2, . . . , K}, with probability at least 1−δ, every network FA with weight matrices A = {A1 , A2 , . . . , AL }
and every margin γ > 0 satisfy:
!
r


R
log(1/δ)
A
bγ (FA ) + O
e
√ log(W̄ ) +
,
(54)
P arg max(FA (x)j ) 6= y ≤ R
n
γ n
j
where W is the maximum number of neurons in a single layer (after pooling) and
! 21
! L−1
L−1
Y
X kl k(Al − M l )> k2
kAL k22
1
L
l
Fr
+
,
(55)
RA := L ρL max kAi, . k2
ρl kÃ kLl∗
2
i
γ
maxi kAL
kÃl k2Ll∗
i, . k2
l=1
l=1
√ .
L
.
κl | |l , the quantity RA in
AL
i, . denotes the i’th row of A . Furthermore, if κl denotes instead the constant such that | |l,∞ ≤
the above equation can be replaced by
1
RA :=
γ

ρL max kAL
i, . k2
i

L−1
Y

!
ρl kÃl kLl∗

L−1
X

l=1

1/3

κl

2/3

k(Al − M l )> k2,1
2/3

kÃl kLl∗

l=1

! 32

2/3

+

kAL k2

2/3

maxi kAL
i, . k2

,

(56)

In particular, if

F

Localised analysis with loss function augmentation

Proposition 18. Let L be a natural number and a1 , . . . , aL > 0 be real numbers. Let V0 , V1 , . . . , VL be L + 1 finte dimensional
vector spaces each endowed with two norms, k . k∞ (the natural ∞ norm) and | . |l for 0 ≤ l ≤ L. Let B1 , B2 , . . . , BL be L
finite dimensional vector spaces with norms k . k1 , k . k2 , . . . , k . kL and B1 , B2 , . . . , BL be the balls of radii a1 , a2 , . . . , aL in the
spaces B1 , B2 , . . . , BL with the norms k . k1 , k . k2 , . . . , k . kL respectively16 .
Suppose also that for each l ∈ {1, 2, . . . , L} we are given an operator F l : Vl−1 × Bl → Vl : (x, A) → FAl (x), which is just
composed of a linear map F−l : Vl−1 × Bl → Vl− followed by max and Relu operations incorporated in the activation function
Gl : Vl− → Vl . For each l1 , l2 with l2 > l1 and each Al1 ,l2 = (Al1 +1 , . . . , Al2 ) ∈ B l1 ,l2 := Bl1 +1 × Bl1 +2 × . . . Bl2 , let us
define
l1 →l2
l1 →l2
l2
l1
FA
l1 ,l2 : Vl1 → Vl2 : x → FAl1 ,l2 (x) = FAl2 ◦ . . . ◦ FAl1 (x),
L
0→L
and FA = FA
. Write similarly F−0→l (X) for the preactivations at layer l. We assume that an extra component, with
L = FAL
index 0, of F 1→l , computes the minimum distance to a threshold (in the case where there is no pooling, this is the maximum
absolute value of any prectivation), so that

(F 0→l (X))0 = min

(i,j)∈Rl

F−0→l (X)i − F 0→l (X)j ,

where Rl represents the set of pairs of components such that Gl (x)j potentially depends on the ith component of x. We also
write El (xi ) for (F 0→l (xi ))0 .
For each AL = (A1 , A2 , . . . , AL ) ∈ B L := B1 × B2 × . . . , BL , and for each l1 , l2 ∈ {1, 2, . . . , L}, and for each x ∈ V0
l1 →l
such that FA
(x)0 > 0, the Lipschitz constant of the gradient of F l1 →l2 evaluated at F 0→l1 (x), with respect to the norms
.
.
k . k∞ and | . |l2 is denoted by ρA,x
l1 →l2 . The corresponding Lipschitz constant with respect to the norms k k∞ and k k∞ is denoted
A,x
by θl1 →l2 .
We suppose the following conditions are satisfied: For all l ∈ {1, 2, . . . , L}, all b > 0, all z1 , z2 , . . . , zn ∈ Vl−1 such that
|zi |l−1 ≤ b ∀i and all  > 0, there exists a subset Cl (Z, ) ⊂ Bl such that
Cl,,n a2l b2
,
2
where Cl,,n is some function of l, , n and, for all A ∈ Bl , there exists an Ā ∈ Cl (b, ) such that for all i ≤ n,
log(# (Cl (Z, ))) ≤

FAl (zi ) − FĀl (zi )

∞,l

≤ .

(57)

(58)

For any 0 <  < 1, b = (b0 , b1 , b2 , . . . , bL ) such that bl ≥ 1 ∀l and bL = 1, any set of positive numbers ρ1 , ρ2 , . . . , ρL−1 ,
any E1 , E2 , . . . , EL , and for any X ∈ V0 such that |X|0 ≤ b0 , there exists a subset C,b,X of B L such that for all A =
(A1 , A2 , . . . , AL ) ∈ B := B L there exists a Ā ∈ C,b,X such that for all i ≤ n such that the following conditions are satisfied:
16
The proof works with B1 , B2 , . . . , BL being arbitrary sets, but we formulate the problem as above to aid the intuitive comparison with the
areas of application of the Proposition.

1. |F l (xi )|l ≤ bl for all l
i
2. For all l1 ≤ l2 ≤ L, ρA,x
l1 →l2 ≤ ρl1 bl2 .
3. El (xi ) ≥ 2El ∀l
i
≤ El2 ρl1 , where as usual El2 (xi ) = F 0→l2 (xi )0 denotes the maximum preactivation at layer l2
4. For all l1 ≤ l2 ≤ L, θlA,x
1 →l2
for input xi ,
one has

l
l
FA
≤
l (xi ) − FĀl (xi )
l
l
FĀ
(xi ) l

≤ 2bl

(∀l ≤ L ∀i ≤ n)
∀l < L

ElĀ (xi ) ≥ El
ElA (xi ) − ElĀ (xi ) ≤ El

and

i
ΘĀ,x
l1 →l2 ≤ El2 ρl1

and

i
ρĀ,x
l1 →l2 ≤ ρl1 bl2

∀l1 , l2 .

(59)

Furthermore, we have

 1
 23 3
 1
2
L
L
2
2
al bl−1 ρl
L2 X  Cl,
X  Cl, al bl−1 ρl  
 .
log #(C,b,X ) ≤ 4 
 ≤4 2



l=1

(60)

l=1

l
Proof. As in the proof of Proposition 10, for l = 1, . . . , L, let l = α
ρl , where the αl > 0 will be determined later satisfying
PL
l=1 αl = 1. And again, for any X = (x1 , . . . , xn ), we define the covers Dl ⊂ Bl for l ≤ L by induction by Dl =
0→l−1
∪A∈D1 ×...×Dl−1 C(2bl , {FA
(xi ) : i ≤ n}, l ). Let us write also D := D1 × D2 . . . × DL , and write dl for the cardinality of
Dl . The key is to show that none of the thresholds such as Relu or max change value between Ā and A, which can be seen from
the equations above and by induction: let us suppose that the first four of the five inequalities above hold for layers before l − 1,
and that no threshold phenomenon has occured.
Since no threshold has occured, we have that for all l1 ≤ l − 1, (and for all j ≤ n),

(Ā1 ,...,Āi−1 ,Ai ,...,Al )
F(0→l
(x ) − F(0→l
(x ) ≤ i ρi→l
= i ρA
i→l ≤ ρi i bl ,
Ā1 ,...,Āi−1 ,Ai ,...,Al ) j
Ā1 ,...,Āi ,Ai+1 ,...,Al ) j
l
and
F(0→l
(x ) − F(0→l
(x )
Ā1 ,Ā2 ,...,Āi−1 ,Ai ,...,Al ) j
Ā1 ,Ā2 ,...,Āi ,Ai+1 ,...,Al ) j
Using this, we obtain as before:
0→l
0→l
FA
(xj ) − FĀ
(xj ) l ≤

L
X

∞

≤ ρi i El .

(x ) − F(0→l
(x )
F(0→l
Ā1 ,Ā2 ,...,Āi−1 ,Ai ,...,Al ) j
Ā1 ,Ā2 ,...,Āi ,Ai+1 ,...,Ai ) j

i=1

≤

l
X

ρi bl i ≤ bl ,

l

(61)

i=1

and similarly
ElA (xj ) − ElĀ (xj ) ≤
l

L
X

F(0→l
(x ) − F(0→l
(x )
Ā1 ,Ā2 ,...,Āi−1 ,Ai ,...,Al ) j
Ā1 ,Ā2 ,...,Āi ,Ai+1 ,...,Ai ) j

i=1

≤

l
X

ρi El i ≤ El .

l

(62)

i=1

From this, since ElA (xj ) ≥ 2El by assumption, we conclude that no threshold is crossed at layer l, and by the triangle inequality
l
ElĀ (xj ) ≥ 2El . The second equation FĀ
(xi ) l ≤ 2bl also follows by the triangle inequality.
By induction, we have proved that no Relu or max pooling threshold was crossed at any layer and the first four inequalities
hold. The last inequalities follow from the assumption and the fact that no treshold occurs.

Theorem 19. let b = (b0 , b1 , b2 , . . . , bL ) such that bl ≥ 1 ∀l and bL = 1, s ρ1 , ρ2 , . . . , ρL−1 , any E1 , E2 , . . . , EL > 0,
a1 , . . . , aL > 0 be given. For any δ > 0, with probability > 1 − δ, every network satisfying k(Al − M l )> k2,1 ≤ al for all l
satisfies
!
P

arg max (FL (x))j 6= y
j∈{1,2,...,K}

1
n − #(I)
8
1536 
≤
+ + √ R log2 (32Γn2 + 7W̄ n) 2 log(n) + 3
n
n
n

s

log( 2δ )
,
2n

(63)

where
R2/3 :=

L
X

(al bl−1 ρl+ )

2/3

,

and

l=1
L

Γ := max (bl−1 al Ol−1 ml ρl+ ) ,
l=1

(64)

I is the set of i ∈ {1, 2, . . . , n} such that:
1. |F l (xi )|l ≤ bl for all l
i
2. For all l1 ≤ l2 ≤ L, ρA,x
l1 →l2 ≤ ρl1 bl2 .
3. El (xi ) ≥ 3El ∀l
i
≤ El2 ρl1 , where as usual El2 (xi ) = F 0→l2 (xi )0 denotes the maximum preactivation at layer l2
4. For all l1 ≤ l2 ≤ L, θlA,x
1 →l2
for input xi ,
5. F (xi )yi − maxj F (x)j ≥ γ,
and
L
X
2/3
R2/3 :=
(al bl−1 ρl ) , and
l=1
L

Γ := max (bl−1 al Ol−1 ml ρl ) .
l=1

(65)

Proof. We apply the Rademacher theorem to the loss function:

L−1
l(x, y) = max sup λBl




F 0→l (x) l − 2Bl , λγ max(FA (x))j − FA (x)y ,
j6=y
l=1



A,xi
i
1 ∃l1 , l2 : θlA,x
>
E
ρ
∨
ρ
>
ρ
b
,
max
(λ
(2E
−
E
(x)))
l2 l1
l1 l2
El
l
l
l1 →l2
1 →l2
l

(66)

Writing H for the function class defined by l(x, F A (x)) for F A satisfying the conditions of the Theorem, since Y 7→
maxj6=y (Y )j − Yy is 2 Lipschitz with respect to the l∞ norm, and l(x, y) = 1 for any x such that there exists l such that
|F 0→l (x)|l ≥ 2bl or for any x that doesnt satisfy the conditions in Theorem 18, Propositions 18 and 9 guanrantee that the
covering number of H satisfies
" L 
2/3 #3

X al (2bl−1 )
al (2bl−1 )nml Ol−1
2
log(N (H, )) ≤ 4 × 64 × 2
ρl+ log2 8
+ 7Ol−1 ml n

(/ρl+ )
l=1

≤ 212 R2 log2 (32Γn/ + 7W̄ n)
Applying the Rademacher Theorem 25, we now obtain

(67)

!
P

arg max (FL (x))j 6= y

≤ E (l(x, y))

j∈{1,2,...,K}

s
log( 2δ )
l(x
,
y
)
i i
≤ i=1
+3
+ 2R̂n (l(x, y))
n
2n
s
log( 2δ )
n − #(I)
+ +3
+ 2R̂n (l(x, y)).
≤
n
2n
Pn

(68)

Similarly to the previous proofs, plugging inequality (67) into (68) and using Dudley’s entropy formula yields the desired
result.
Again, by using Lemma 12, we can turn this result into:
Theorem 20. Suppose we have a K class classification problem and are given n i.i.d. observations
(x1 , y1 ), (x2 , y2 ), . . . , (xn , yn ) ∈ RU0 ×w0 ⊗ {1, 2, . . . , K} drawn from our ground truth distribution (X, Y ), as well
as a fixed architecture as described in Section A, where we assume the last layer is fully connected and has width K and
corresponds to scores for each class. For any δ > 0, with probability > 1 − δ over the draw of the training set, for any network
A = (A1 , A2 , . . . , AL )we have
!
arg max (FL (x))j 6= y

P

j∈{1,2,...,K}

s
1


log( 2δ )
8
1536
n − #(Iβ,γ )
+ + √ RA log2 (32Γn2 + 7W̄ n) 2 log(n) + 3
≤
n
n
2n
n
v
u L






u1 X
ρA
Bl−1 (X)
k(A − M )> k2,1
+ 3t
log 2 +
+ log 2 +
+ log 2 + l ,
n
L
L
L

(69)

l=1

where ρA
l , El and Bl (X) ≥ 1 can be chosen in any way that depends on both A and X, and I is then defined as the set of
indices i ≤ n such that
1. |F l (xi )|l ≤ bl for all l
i
2. For all l1 ≤ l2 ≤ L, ρA,x
l1 →l2 ≤ ρl1 bl2 .

3. El (xi ) ≥ 3El

∀l

i
≤ El2 ρl1 , where as usual El2 (xi ) = F 0→l2 (xi )0 denotes the maximum preactivation at layer l2
4. For all l1 ≤ l2 ≤ L, θlA,x
1 →l2
for input xi ,
5. F (xi )yi − maxj F (x)j ≥ γ.

A
The particular choice El = 31 maxi E 0→l (x), Bl (X) = max maxi≤n F0→l
(xi ) l , 1 and


A,xi
A,x
ρl →l
θ →li
1
2
1
2
ρA
, maxi maxl̃≥l lE
yields
l = max maxi maxl̃≥l bl
l
2

2


I=


i ≤ n : (f (xi ))yi − max((f (xi ))j ) > γ .
j6=yi

In the above formula,
2/3
L 
X
1
1
1
A
>
, and
:=
(k(A − M ) k2,1 + )(Bl−1 (X) + ))(ρl+ + )
L
L
L
l=1


1
1
1
L
>
A
Γ := max (Bl−1 (X) + )(k(A − M ) k2,1 + )Ol−1 ml (ρl + ) .
l=1
L
L
L

2/3
RA

(70)

After reducing to the Õ notation, we obtain:
Theorem 21. For training and testing points (x, y), (x1 , y1 ), (x2 , y2 ), . . . , (xn , yn ) as usual drawn iid from any probability distribution over Rd ×{1, 2, . . . , K}, with probability at least 1−δ, every network FA with weight matrices A = {A1 , A2 , . . . , AL }
and every margin γ > 0 satisfy:
!
r


n − #(I)
(RA + L)
log(1/δ)
e
√
P arg max(FA (x)j ) 6= y ≤
+O
log(W̄ ) +
,
(71)
n
n
n
j
where W is the maximum number of neurons in a single layer (after pooling) and
"L−1

2/3 #3/2
X

B
(X)
2/3
L−1
RA :=
Bl−1 (X)kAl − M l k2,1 ρA
+
kAL − M L kFr
,
l
γ
l=1

(72)

where for any layer l1 , Bl1 (X) := maxi F 0→ll (xi ) l1 denotes the maximum l2 norm of any convolutional patch of the layer l1

A
activations, over all inputs. BL (X) = γ, El = 13 maxi E 0→l (x), Bl (X) = max maxi≤n F0→l
(xi ) l , 1 and


A,xi
A,x
ρl →l
θl →li
1
2
1
2
ρA
=
max
max
max
,
max
max
, and
i
i
l
l̃≥l bl2
l̃≥l El2


I = i ≤ n : (f (xi ))yi − max((f (xi ))j ) > γ .
j6=yi

G

Dudley’s entropy formula

For completeness, we include a proof of (a variant of) the classic Dudley’s entropy formula. To enable a comparison with the
results used in (Bartlett, Foster, and Telgarsky 2017), we write the result with arbitrary Lp norms. We will, however, only use the
L∞ version.
Proposition 22. Let F be a real-valued function class taking values in [0, 1], and assume that 0 ∈ F. Let S be a finite sample of
size n. For any 2 ≤ p ≤ ∞, we have the following relationship between the Rademacher complexity R(F|S ) and the covering
number N (F|S, , k . kp ).


Z 1q
12
R(F|S ) ≤ inf 4α + √
log N (F|S, , k . kp ) ,
α>0
n α
Pm
1
m
p
where the norm k . kp on R is defined by kxkp = n ( i=1 |xi |p ).
Proof. Let N ∈ N be arbitrary and let i = 2−(i−1) for i = 1, 2, . . . , N . For each i, let Vi denote the cover achieving
N (F|S, i , k . kp ), so that
! p1
n
1X
p
(f (xt ) − vt )
∀f ∈ F ∃v ∈ Vi
≤ i ,
(73)
n t=1
and #(Vi ) = N (F|S, i , k . kp ). For each f ∈ F,let v i [f ] denote the nearest element to k in Vi . Then we have, where
σ1 , σ2 , . . . , σn are n i.i.d. Rademacher random variables,
n

1X
σt f (xt )
f ∈F n t=1
#
" n
−1
n
n
 NX
 1X
1X
1X
i+1
N
i
1
σt ft (xt ) − vt [f ] −
σt vt [f ] − vt [f ] +
σt vt [f ]
= Eσ sup
n t=1
n t=1
f ∈F n t=1
i=1
" n
# N −1
" n
#
X
X


1X
1
≤ Eσ sup
σt ft (xt ) − vtN [f ] +
Eσ sup
σt vti [f ] − vti+1 [f ]
n
f ∈F n t=1
f
∈F
t=1
i=1
#
" n
1X
1
σt vt [f ] .
+ Eσ sup
f ∈F n t=1

Eσ sup

For the third term, pick V1 = {0}, so that
"

#
n
1X
1
Eσ sup
σt vt [f ] = 0.
f ∈F n t=1
For the first term, we use Hölder’s inequality to obtain, where q is the conjugate of p,
" n
#
! q1
! p1
N
−1
n
n
X
X
X

1
1X
1
p
Eσ sup
σt ft (xt ) − vtN [f ] ≤ Eσ
|σt |q
ft (xt ) − vtN [f ]
n
n
n
f
∈F
t=1
t=1
t=1
i=1
≤ N .
i

Next, for the remaining terms, we define Wi = {v [f ]−v i+1 [f ]|f ∈ F}. Then note that we have |Wi | ≤ |Vi ||Vi+1 | ≤ |Vi+1 |2 ,
and then
" n
#
" n
#

1X
1X
i+1
i
Eσ sup
σt vt [f ] − vt [f ] ≤ Eσ sup
σt wt .
w∈Wi n t=1
f ∈F n t=1

Next,
v
u n
u1 X
sup t
w2 = sup v i [f ] − v i+1 [f ]
n t=1 t
w∈Wi
f ∈F
≤ sup v i [f ] − (f (x1 ), . . . , f (xn ))
f ∈F

2

i

≤ sup v [f ] − (f (x1 ), . . . , f (xn ))
f ∈F

p

2

+ sup (f (x1 ), . . . , f (xn )) − v i+1 [f ]
f ∈F

+ sup (f (x1 ), . . . , f (xn )) − v

i+1

[f ]

f ∈F

2
p

≤ i + i+1 = 3i+1 ,
where at the third line, we have used the fact that p ≥ 2. Using this, as well as Massart’s lemma, we obtain
"
Eσ sup
w∈Wi

v
#
u
n
n
p
1 u
1X
3i+1 p
1X 2
6
2 log |Wi | ≤ √ i+1 log |Vi+1 |.
σt wt ≤ √ t2 sup
wt log |Wi | ≤ √
n t=1
n
n
n
w∈Wi n
t=1

Collecting all the terms, we have
Eσ sup

f ∈F

N −1
n
q
1X
6 X
σt f (xt ) ≤ N + √
i+1 log N (FS , i+1 , k . kp )
n t=1
n i=1
N
q
12 X
(i − i+1 ) log N (FS , i , k . kp )
≤ N + √
n i=1
Z 1 q
12
≤ N + √
log N (FS , , k . kp )d.
n N +1

Finally, select any α > 0 and take N to be the largest integer such that N +1 > α. Then N = 4N +2 ≤ 4α, and therefore
Z 1 q
Z 1q
12
12
N + √
log N (FS , , k . kp )d ≤ 4α + √
log N (F|S , , k . kp )d,
n N +1
n α
as expected.

H

Detailed comparison to other works

At the same time as the first version of this paper appeared on ArXiv, a different solution to the weight sharing problem (but not
to the multiclass problem) was posted on arXiv (Long and Sedghi 2020). The bound, which relies on computing the Lipschitz
constant of the map from parameter space to function space and applying known results about classifiers of a given number
b
of parameters, states that for some constant C and for large enough n, the generalisation gap E(l(ĝ)) − E(l(ĝ))
satisfies with
probability ≥ 1 − δ, assuming each input has unit l2 norm,
s
b
E(l(ĝ)) − E(l(ĝ))
≤ CB

W(

PL

l=1 sl

− log(γ)) + log(1/δ)
,
n

(74)

where sl is an upper bound on the spectral norm of the matrix corresponding to the lth layer, γ is the margin, and W is the
number of parameters, taking weight sharing into account by counting each parameter of convolutional filters only once. We
note that the method to obtain the bound is radically different from ours, and closer to (Li et al. 2019). Indeed, it relies on the
following general lemma mostly composed of known results, which bounds the complexity of function classes with a given
number of parameters:
Proposition 23. (Long and Sedghi 2020; Mohri, Rostamizadeh, and Talwalkar 2018; Giné and Guillou 2001; Platen 1986;
Talagrand 1994, 1996) Let G be a set of functions from a domain Z to [0, M ] such that for some B > 5 and for some d ∈ N and
for some norm k . k1 on Rd , there exists a map from Rd to G which is B-Lipschitz with respect to the norms k . k1 and k . k∞ . For
large enough n and for any distribution P over Z, if S is sampled n times independently form P , for any δ > 0, we have with
probability ≥ 1 − δ that for all g ∈ G,
r
d log(B) + log(1/δ)
Ez∼P (g(z)) ≤ ÊS (g) + CM
,
n
where C is some constant.

The proof of inequality (74) then boils down to explicitly bounding the Lipschitz constant of the map from parameter space to
PL
function space assuming some fixed norm constraints on the weights. Note that the term l=1 sl comes from a logarithm of
QL
i=1 si .
Norm-based bounds such as ours and those in (Bartlett, Foster, and Telgarsky 2017) require more details to work in activation
space directly, thereby replacing the explicit parameter dependence by a dependence on the norms of the weight matrices.
Furthermore, one notable advantage of Norm-based bounds is their suitability to be incorporated in further analyses that take
distance from initialisation into account, as do the approaches of the SDE branch of the litterature ((Du et al. 2019; Arora et al.
2019; Cao and Gu 2019; Jacot, Gabriel, and Hongler 2018; Neyshabur et al. 2019, to appear; Zou et al. 2018)). Indeed, note
that the bound (74) from (Long and Sedghi 2020) is still large, and still scales as the number of parameters, even if the weight
matrices Al are arbitrarily
to the initialised matrices M l . In contrast, the capacity estimate in our bounds converges toa
q closeq
constant times either n1 (3) or L
n (7) when the weights approach initialisation.
In what follows, we illustrate this fact by comparing our bounds with those of (Bartlett, Foster, and Telgarsky 2017; Long and
Sedghi 2020) both in the general case and in a simple illustrative particular case.
Comparison
Here we use the same notation as in the rest of the paper, assume the lipschitz constants ρl are 1, and set fixed norm constraints.
Below, B denotes an upper bound on the L2 norms of input data points. Recall Ol is the number of convolutional patches in
layer l, ml is the number of filters in layer l, al is an upper bound on the L2 norm of the filter matrix, and sl is an upper bound
on the spectral norm of the corresponding full convolution operation.
For a completely general feed forward Q
convolutional neural network, we have the following comparison, where C is an
unspecified constant, Γ = maxL
of comparison,
l=1 Wl Bal
i6=l si , and dl is the size of convolutional filters at layer l. For ease
q
8
we compare only with the forms of our theorems involving explicit spectral norms. Here, we write Ω = n + 3 log(1/δ)
. Note
2n
General bound
Prev. work
Simult. work
Our bounds

i1

2
PL Ol−1
ml a2l 2
72B log(2w) log(n) QL
√
L i=1 si
+Ω
l=1
s2l
nγ
q PL
PL
[ l=1 dl−1 Ol−1 ml ][ l=1 sl −log(γ)]+log(1/δ)
C
n
hP
1
L−1 wl Ul a2l
211 BL log(n) log2 (32Γn2 /γ+7W̄ n) 2 QL
√
s
i
i=1
l=1
s2l
nγ

h

+

a2L
s2L

i 12

+Ω

that at the term of the sum
corresponds to q
layer l, our bound is better than the bound in (Bartlett, Foster, and Telgarsky 2017)
q that
q
2
Ol−1
ml
Ol−1 ml
W̄l
∞
roughly by a factor of
.
A
factor
of
=
wl
Wl
Ul wl is removed by exploiting the L -continuity of the activation
p
functions and the pooling operation, while a further factor of Ol−1 is removed by exploiting weight sharing.
The main advantage of the bound in (Long and Sedghi 2020) compared to ours is the lack of a product of spectral norms
QL
i=1 si as a factor inside the square root. This is a significant difference as it arguably removes implicit exponential depth
dependence, and illustrate the difference between the methods. However, as explained in Section F andSubsection , this problem
can be tackled independently.
Theh main disadantage of
i the bound in (Long and Sedghi 2020) compared to ours is that it exhibits an explicit factor of
PL
W=
l=1 dl−1 Ol−1 ml , the total number of parameters in the network. Note also that the factor appears in a term where it is
not multiplied by any norm quantities.This has important implications. If the trained network has a large number of very small
weights (or weights very close to initialisation), the corresponding contribution is small. This suggests the superior potential of
refined norm-based bounds to explain the generalisation capabilities of DNN’s at the overparametrised regime (Bartlett 1997;
Brutzkus et al. 2018; Neyshabur et al. q
2019, to appear; Du et al. 2019). More crucially, the spectral norm version of our bound (3)
converges to a very small number Õ( n1 ) when the weights approach the initialised values M , whilst the bound in (Long and
p
Sedghi 2020) still scales like Õ( W/n) in that case, making our bound better suited to incorporation in bounds that take the
optimisation procedure into account.
Furthermore, whilst the bound does not directly involve the input space dimension, it does explicitly depend on the size of
the convolutional filters, including at the input (0th ) layer. On the other hand, our bound depends instead on the post pooling
width at layers l for l = 1, 2, . . . L, which is the maximum possible number of active neurons (after pooling) at layer l ≥ 1
(which excludes the input layer). Furthermore, norm-based bounds such as ours exhibit some degree of architecture robustness.
If weights are pruned, our bound is the same as it would be if we had started with the smaller architecture, whilst the bound
in (Long and Sedghi 2020) still involves the original number of parameters.
Furthermore, we believe the meaningful estimate of complexity lies in the term k(A> − M > )k2,1 , whilst the product of spectral
norms is more of a technicality: networks constrained to have all spectral norms equal to 1 form a rich function class of high

relevance to the original problem, and it has been shown in (Sedghi, Gupta, and Long 2019) that this class can be approached
through regularisation, and indeed that doing so even improves the accuracy.
Remark on the proof techniques: In fact, it is interesting to note that the main advantages and disadvantages of our bound
compared to that in (Long and Sedghi 2020) are intimately related via a tradeoff that appears in the proof choice: it is possible to
bound the covering number of a function class depending on a parameter θ in different ways depending on which of (1) the
dimension of parameter space (2) the norm and architectural constraints on the parameters, is the most restrictive. When the
dimension d of parameter space is moderate and assuming the Lipschitz constant is known, it is straightforward to bound the
covering number by a quantity of the form ( C )d , directly for an arbitrary function class of which we know nothing except the
number of parameters and the relevant Lipzitsch constant. When the norm constraints are stronger and the dimension is large
or possibly infinite, it is necessary to use different tools such as the Maurey sparsification lemma and L∞ versions of it which
apply directly only to linear classifiers and thus require more use of the architectural assumption and chaining arguments to
generalise to DNN. To better understand the trade-off between the product of spectral norm and the architecture robustness, it is
best to think of the simple example of a linear classifier in the setting of L2 covering numbers: consider the Maurey sparsification
lemma A.6 from (Bartlett, Foster, and Telgarsky 2017). The quantity k scales like the reciprocal of granularity  of the cover. The
Pd
covering
numberbehaves like the number of choices of integers (k1 , . . . , kd ) such that i=1 kd = k. This quantity is equal

k+d−1
to
. Depending on whether d is large or small compared to k, this can be approximated by k d or dk . The first
d−1
choice yields explicit dependence on the number of parameters, and the second choice yields dependence on the norms of the
input and weight matrices, which in the case of a neural network eventually translates into a product of spectral norms.
On input size independence/robustness to downsampling. Note that contrary to ours, the bound in (Long and Sedghi 2020)
depends explicitly on d (d0 in the general case, the size of the first input layer’s convolutional patches).
We argue that this implies our bound exhibits an even stronger form of input size-independence. We consider an idealised scenario where a downsampled version of each image contains the same information as the original image: suppose that each input xi ,
0
0
of size 2a×2b, satisfies (xi )2j,2j = (xi )2j,2j+1 = (xi )2j+1,2j = (xi )2j+1,2j+1 for any j, where (x
√i )r,r denotes the (r, r ) pixel
of image xi . If we create a downsampled version x̃i , of size a×b, of the q
input such that (x̃i )j1 ,j2 = 4(xi )2j1 ,2j2 , and similarly re2
2
2
place the first layer convolutional weights wu1 ,u2 of size 2c1 × 2c2 by w̃2u1 ,2u2 + w̃2u
+ w̃2u
,
+ w̃2u
1 +1,2u2 +1
1 ,2u2 +1
1 +1,2u2
of size c1 × c2 , then assuming the stride is also divided by two in the downsampled case, all activation at the next layers (from
layer 1 onwards) are the same in both the original and the downsampled version. Thus there is a natural bijection F between
solutions to the first problem and the second, and it is not hard to convinve onesef that the image by F of the solution of SGD on
one problem is the solution to SGD on the other, with the generalisation gap also staying exactly the same.However, in the case
of the bound in (Long and Sedghi 2020), the bound is smaller in the case of the downsampled version due to the decrease in the
number of parameters. Our bound, on the other hand, stays the same. Indeed, the maximum L2 norm B̃ of convolutional patches
stays the same, as does the L2 norm of every convolutional filter w, despite the change in the number of parameters.

I

On class dependency and working with L2 norms

As mentioned after Theorem 1, the main advantages of our bounds in terms of class dependency compared to the work of
Bartlett (Bartlett, Foster, and Telgarsky 2017) is to replace the capacity contribution of the last layer k(AL − ML )> k2,1 by
kAL − ML kFr . As explained before, in the case where the norms of the classifying vectors (AL )c, . for c ≤ C are within a
constant
factor of each other, the bound in Bartlett has an implicit dependence of C, whilst our bound has an implicit dependence
√
of C (ignoring logarithmic terms). In this section, we show that for large C, the region of weight space where this condition
does not hold has vanishingly small Lebesgue (or Gaussian) measure, further confirming the theoretical importance of our
improvements.
Proposition 24. Let X ∈ Rn be a random variable which is either spherically symmetric or has i.i.d. component, and satisfies
E(|X1 |2 ) < ∞. For all 0 <  < 13 with (E(X12 ))−1 ≤ 12 and (E(|X1 |))−1 ≤ 12 and for all n, we have, with probability
2
≥ 1 − 5e−2 n ,
√
C(1 − U )kXk1 ≤ nkXk2 ≤ C(1 + U )kXk1 ,
(75)
√
E((X )2 )


with C = E(|X11|) and U = 4 E(X
2 ) + 4 E(|X |) + 
1
1

Proof. Since the multivariate Gaussian is spherically symmetric and the inequality 75 is radially symmetric, we only need to
prove the case with i.i.d. components. We begin by picking Rn large enough to ensure
P (∃i ≤ n : |Xi | ≥ Rn ) ≤

n
X

P (|Xi | ≥ Rn )

i=1
2

= nP (|Xi | ≥ Rn ) ≤ exp(− n),

(76)


E (X1 )2 |X1 ≤ Rn > 2E(X12 )/3,

E |X1 | X1 ≤ Rn > 2E (|X1 |) /3,
and

E |X1 | X1 ≤ Rn
E(|X1 |)
≤p
(1 − ) p
E((X1 )2 )
E ((X1 )2 |X1 ≤ Rn )
E(|X1 |)
≤ (1 + ) p
E((X1 )2 )

(77)

which can be done by the assumption that E(|X1 |2 )) < ∞. Next, let Y be the random variable X conditioned on |Xi | ≤ Rn
for all i and let X̃ = Y Rn−1 .
By applying Hoeffding’s lemma, we note that we have



P kX̃k22 − nE(X̃12 ) ≥ n ≤ 2 exp −2n2 ,
(78)
and

P



kX̃k1 − nE(|X̃1 |) ≥ n ≤ 2 exp −2n2 .

(79)

Note that kX̃k22 − nE(X̃12 ) < n implies
kX̃k2 −

√

n
q
√
kX̃k2 + n E(X̃12 )
√
 n
.
≤q
E(X̃12 )

q
n E(X̃12 ) ≤

(80)

2

Hence, by inequalities 78, 77 and 79, we have with probability > 1 − 4e−2 n ,
q
√
n E(X̃12 ) + √ n 2
nkX̃k2
E(X̃1 )
≤
kX̃k1
nE(|X̃1 |) − n
q

E(X̃12 ) 1 + E(X̃ 2 )
1
≤

E(|X̃1 |) 1 − E(|X̃1 |)
q


E(X̃12 ) 


≤
1+
1+2
E(|X̃1 |)
E(X̃12 )
E(|X̃1 |)
q


E(X̃12 )


2
≤
1+
+2
+
E(|X̃1 |)
E(X̃12 )
E(|X̃1 | E(|X1 |)E(X̃12 )
q

E(X̃12 ) 


≤
+
2
1+2
E(|X̃1 |)
E(X̃12 )
E(|X̃1 |)
p


2
E(X1 )


≤
1+3
+3
(1 + )
E(|X1 |)
E(X12 )
E(|X1 |)
p


E(X12 )


≤
1+4
+
4
+

,
E(|X1 |)
E(X12 )
E(|X1 |)
as expected. The proof of the other inequality is similar.

J

Rademacher Theorem

Recall the definition of the Rademacher complexity of a function class F:

(81)

Definition 2. Let F be a class of real-valued functions with range X. Let also S = (x1 , x2 , . . . , xn ) ∈ X be n samples from the
domain of the functions in F. The empirical Rademacher complexity RS (F) of F with respect to x1 , x2 , . . . , xn is defined by
n

RS (F) := Eδ sup

f ∈F

1X
δi f (xi ),
n i=1

(82)

where δ = (δ1 , δ2 , . . . , δn ) ∈ {±1}n is a set of n iid Rademacher random variables (which take values 1 or −1 with probability
0.5 each).
Recall the following classic theorem (Scott 2014):
Theorem 25. Let Z, Z1 , . . . , Zn be iid random variables taking values in a set Z. Consider a set of functions F ∈ [0, 1]Z .
∀δ > 0, we have with probability ≥ 1 − δ over the draw of the sample S that
r
n
1X
log(2/δ)
∀f ∈ F, E(f (Z)) ≤
f (zi ) + 2 RS (F) + 3
.
n i=1
2n

K

Experiments

Although the main aim of this paper is purely theoretical, we provide two simple experiment strands here to illustrate the
behaviour of our bound on data.

Augmented MNIST
22
Our first experiment studies how the proposed bound changes
ln(R )
ln(M)
with input dimension. The goal is to generate a sequence of
21
datasets with increasing input sizes that have a small effect on the
problem complexity and the convolutional architectural design.
20
To that end, for each s ∈ {2, 4, . . . , 10}, we generate data points
19
of size 28s × 28s as follows. First, an image I of the MNIST
dataset is randomly sampled along with its label `. Then we
18
embed s/2 non-intersecting copies of I into a large black image
17
of the size 28s × 28s at random locations. For each input size
s ∈ {2, 4, 6, 8, 10}, we generate training set of 50000 data points.
16
The model used is a convolutional network with 4 convolutional layers, followed by a fully-connected layer with 10 outputs
15
2
3
4
5
6
7
8
9
10
for the classes. The filters at each convolutional layer are of size
Input Size (s)
3 × 3, applied with strides of 2 and the numbers of channels from
input to output are respectively as follows: 64, 128, 128, and 64.
We train all the models using the cross-entropy loss and weight decay with an ADAM optimizer until they achieve 99%
training accuracy. For each dataset, we select the margin to be the largest margin to achieve 96% training accuracy.
For each value of s, we compute the main term RA in our bound (see (9)) and the term M/γ from equation (1). In the graph
shown we plot the two bounds vs the dataset size s in log-scale.

Synthetic data
In this experiment, each data point is a sequence of length L digits from the set {0, 1, 2, 3}. We fix 20 "signature" sequences
s1 , s2 , . . . , s20 of length 15, the first 10 of which (i.e. {s1 , s2 , . . . , s10 }) are associated with label 0, and the last 10 (i.e.
{s11 , s12 , . . . , s20 }) of which are associated with label 1. Each data point is created by inserting 5 of the signatures into
an originally uniformly random sequence of length L at a uniformly random position. Optionally, we repeat each inserted
subsequence a total of iter times, where iter is a parameter (duplicate signatures need not appear consecutively). The label is
determined by a majority vote of the signatures present. For instance, if signatures s1 , s2 and s11 are present, the label is 0.
We use one-hot encoding and employ a two-layer neural network composed of one convolutional layer without any padding,
and one fully connected layer. We do not use any offset terms. We use 50 filters, and pooling is over the whole spacial region,
so that the total number of hidden neurons is also 50.
we compute for each
 Using a variation of our theorems from Section E, 
√

q P
K

e n) k
input the normalised margins γ(xi )/R where R = (B/

i=1

fi2 supi≤C Fi2 +

PK

i=1

fi2

PC

i=1

Fi2 , and the Fi ’s (resp.

fi ’s) are upper bounds on the L2 norms of 1st (resp. 2nd) layer filters.
We run the model for both N = 350 and N = 20000, and for L = 1000, 4000. The parameter iter, which we vary
proportionately to the total length appears required for optimisation purposes. Of course, it also has some influence on
generalisation, but bridging the data dependency gap is beyond the scope of this work, where we focus on generalisation bounds
valid on the whole of weight space.

L=1000, acc=94.29%
L=4000, acc=97.43%
N=350

L=1000, acc=94.29%
L=4000, acc=97.43%
N=350

0.0
1.0
2.0
3.0
0.0
2.0
4.0
6.0
102 Margin normalised with R 105 Margin normalised with M

L=1000, acc=99.94%
L=4000, acc=99.95%

L=1000, acc=99.94%
L=4000, acc=99.95%

N=20000

N=20000

0.0 1.0 2.0 3.0 4.0
102 Margin normalised with R

0.0 2.0 4.0 6.0 8.0
105 Margin normalised with M

Figure 2: Distribution of normalised margins for different values of N and L

We illustrate experimental results in Figure 2. Besides the margins normalised with R being several orders of magnitude larger
than the ones normalised with M , a point of interest is that in both data regimes the value of L has a strong influence on the
classically normalised margins, but a mild to moderate influence on both our normalised margins and two subjective measures of
data insufficiency: the test accuracy and the distribution of the margins. For N = 20000 and all values of L, the margins are
clearly divided into three sets depending on how many inserted signatures in the datapoint are associated with the same label 17 .
For N = 350 (all values of L), the three groups are still identifiable, but are less well separated, which shows the problem is in a
similarly borderline insufficient data regime. In conclusion, classification problems of similar difficulty but different data size
lead to similar normalised margins using our formula but very different normalised margins when using M from equation (1).

References
Allen-Zhu, Z.; Li, Y.; and Liang, Y. 2019. Learning and Generalization in Overparameterized Neural Networks, Going Beyond
Two Layers. In Wallach, H.; Larochelle, H.; Beygelzimer, A.; d Alché-Buc, F.; Fox, E.; and Garnett, R., eds., Advances in Neural
Information Processing Systems 32, 6155–6166. Curran Associates, Inc.
Anthony, M.; and Bartlett, P. 2002. Neural Network Learning: Theoretical Foundations. ISBN 978-0-521-57353-5. doi:
10.1017/CBO9780511624216.
Arora, S.; Du, S. S.; Hu, W.; Li, Z.; and Wang, R. 2019. Fine-Grained Analysis of Optimization and Generalization for
Overparameterized Two-Layer Neural Networks. arXiv e-prints arXiv:1901.08584.
Arora, S.; Ge, R.; Neyshabur, B.; and Zhang, Y. 2018. Stronger Generalization Bounds for Deep Nets via a Compression
Approach. In Dy, J.; and Krause, A., eds., Proceedings of the 35th International Conference on Machine Learning, volume 80 of
Proceedings of Machine Learning Research, 254–263. Stockholm, Sweden: PMLR.
Asadi, A.; Abbe, E.; and Verdu, S. 2018. Chaining Mutual Information and Tightening Generalization Bounds. In Bengio, S.;
Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.; and Garnett, R., eds., Advances in Neural Information Processing
Systems 31, 7234–7243. Curran Associates, Inc.
Bartlett, P.; and Shawe-taylor, J. 1998. Generalization Performance of Support Vector Machines and Other Pattern Classifiers.
Bartlett, P. L. 1997. For Valid Generalization the Size of the Weights is More Important than the Size of the Network. In Mozer,
M. C.; Jordan, M. I.; and Petsche, T., eds., Advances in Neural Information Processing Systems 9, 134–140. MIT Press.
Bartlett, P. L. 1998. The sample complexity of pattern classification with neural networks: the size of the weights is more
important than the size of the network. IEEE Transactions on Information Theory 44(2): 525–536. doi:10.1109/18.661502.
Bartlett, P. L.; Foster, D. J.; and Telgarsky, M. J. 2017. Spectrally-normalized margin bounds for neural networks. 6240–6249.
Curran Associates, Inc.
Bartlett, P. L.; and Mendelson, S. 2002. Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of
Machine Learning Research 3(Nov): 463–482.
Brutzkus, A.; Globerson, A.; Malach, E.; and Shalev-Shwartz, S. 2018. SGD Learns Over-parameterized Networks that Provably
Generalize on Linearly Separable Data. In International Conference on Learning Representations.
Cao, Y.; and Gu, Q. 2019. Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks. arXiv
e-prints arXiv:1905.13210.
Chen, M.; Li, X.; and Zhao, T. 2019. On Generalization Bounds of a Family of Recurrent Neural Networks.
17

{3, 2} is frequent and difficult to classify, {4, 1} is easier and rarer, {5, 0} is even easier and very rare

Du, S. S.; Wang, Y.; Zhai, X.; Balakrishnan, S.; Salakhutdinov, R. R.; and Singh, A. 2018. How Many Samples are Needed to
Estimate a Convolutional Neural Network? In Bengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.; and
Garnett, R., eds., Advances in Neural Information Processing Systems 31, 373–383. Curran Associates, Inc.
Du, S. S.; Zhai, X.; Poczos, B.; and Singh, A. 2019. Gradient Descent Provably Optimizes Over-parameterized Neural Networks.
In International Conference on Learning Representations.
Dziugaite, G.; and Roy, D. 2018. Data-dependent PAC-Bayes priors via differential privacy .
Fazlyab, M.; Robey, A.; Hassani, H.; Morari, M.; and Pappas, G. J. 2019. Efficient and Accurate Estimation of Lipschitz
Constants for Deep Neural Networks. CoRR abs/1906.04893.
Frankle, J.; and Carbin, M. 2019. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. In International
Conference on Learning Representations.
Giné, E.; and Guillou, A. 2001. On consistency of kernel density estimators for randomly censored data: Rates holding
uniformly over adaptive intervals. Annales de l’Institut Henri Poincare (B) Probability and Statistics 37: 503–522. doi:
10.1016/S0246-0203(01)01081-0.
Golowich, N.; Rakhlin, A.; and Shamir, O. 2018. Size-Independent Sample Complexity of Neural Networks. In Bubeck, S.;
Perchet, V.; and Rigollet, P., eds., Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine
Learning Research, 297–299. PMLR.
Goodfellow, I.; Shlens, J.; and Szegedy, C. 2015. Explaining and Harnessing Adversarial Examples. In International Conference
on Learning Representations.
Guermeur, Y. 2002. Combining Discriminant Models with New Multi-Class SVMs. Pattern Analysis & Applications 5(2):
168–179. ISSN 1433-7541. doi:10.1007/s100440200015.
Guermeur, Y. 2007. VC Theory of Large Margin Multi-Category Classifiers. Journal of Machine Learning Research 8:
2551–2594.
Guermeur, Y. 2017. Lp-norm Sauer–Shelah lemma for margin multi-category classifiers. Journal of Computer and System
Sciences 89: 450 – 473. ISSN 0022-0000. doi:https://doi.org/10.1016/j.jcss.2017.06.003.
Harvey, N.; Liaw, C.; and Mehrabian, A. 2017. Nearly-tight VC-dimension bounds for piecewise linear neural networks. In
Kale, S.; and Shamir, O., eds., Proceedings of the 2017 Conference on Learning Theory, volume 65 of Proceedings of Machine
Learning Research, 1064–1068. Amsterdam, Netherlands: PMLR.
He, F.; Liu, T.; and Tao, D. 2019. Why ResNet Works? Residuals Generalize. arXiv e-prints arXiv:1904.01367.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual Learning for Image Recognition. In CVPR, 770–778. IEEE
Computer Society.
Jacot, A.; Gabriel, F.; and Hongler, C. 2018. Neural Tangent Kernel: Convergence and Generalization in Neural Networks. CoRR
abs/1806.07572.
Karras, T.; Laine, S.; and Aila, T. 2018. A Style-Based Generator Architecture for Generative Adversarial Networks. CoRR
abs/1812.04948.
Koltchinskii, V.; and Panchenko, D. 2002. Empirical Margin Distributions and Bounding the Generalization Error of Combined
Classifiers. Ann. Statist. 30(1): 1–50. doi:10.1214/aos/1015362183.
Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. ImageNet Classification with Deep Convolutional Neural Networks. In
Pereira, F.; Burges, C. J. C.; Bottou, L.; and Weinberger, K. Q., eds., Advances in Neural Information Processing Systems 25,
1097–1105. Curran Associates, Inc.
Latorre, F.; Rolland, P.; and Cevher, V. 2020. Lipschitz constant estimation of Neural Networks via sparse polynomial
optimization. In International Conference on Learning Representations. URL https://openreview.net/forum?id=rJe4_xSFDB.
Lauer, F. 2018. Error bounds with almost radical dependence on the number of components for multi-category classification,
vector quantization and switching regression. In Conférence sur l’Apprentissage automatique (CAp) - French Conference on
Machine Learning (FCML), Proc. of the French Conference on Machine Learning (CAp/FCML). Rouen, France.
Lee, J.; and Raginsky, M. 2019. Learning Finite-Dimensional Coding Schemes with Nonlinear Reconstruction Maps. SIAM
Journal on Mathematics of Data Science 1: 617–642. doi:10.1137/18M1234461.
Lei, Y.; Dogan, Ü.; Zhou, D.; and Kloft, M. 2019. Data-Dependent Generalization Bounds for Multi-Class Classification. IEEE
Trans. Information Theory 65(5): 2995–3021. doi:10.1109/TIT.2019.2893916.
Li, X.; Lu, J.; Wang, Z.; Haupt, J.; and Zhao, T. 2019. On Tighter Generalization Bounds for Deep Neural Networks: CNNs,
ResNets, and Beyond.
Lin, S.; and Zhang, J. 2019. Generalization Bounds for Convolutional Neural Networks.

Long, P. M.; and Sedghi, H. 2020. Size-free generalization bounds for convolutional neural networks. In International Conference
on Learning Representations.
Mohri, M.; Rostamizadeh, A.; and Talwalkar, A. 2018. Foundations of Machine Learning. Adaptive Computation and Machine
Learning. Cambridge, MA: MIT Press, 2 edition. ISBN 978-0-262-03940-6.
Musayeva, K.; Lauer, F.; and Guermeur, Y. 2019. Rademacher complexity and generalization performance of multi-category
margin classifiers. Neurocomputing 342: 6 – 15. ISSN 0925-2312. doi:https://doi.org/10.1016/j.neucom.2018.11.096. Advances
in artificial neural networks, machine learning and computational intelligence.
Nagarajan, V.; and Kolter, J. Z. 2019. Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing
noise-resilience. CoRR abs/1905.13344.
Neyshabur, B.; Bhojanapalli, S.; and Srebro, N. 2018. A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for
Neural Networks. In International Conference on Learning Representations. openreview.net.
Neyshabur, B.; Li, Z.; Bhojanapalli, S.; LeCun, Y.; and Srebro, N. 2019, to appear. The role of over-parametrization in
generalization of neural networks. In International Conference on Learning Representations.
Neyshabur, B.; Tomioka, R.; and Srebro, N. 2015. Norm-Based Capacity Control in Neural Networks. In GrÃŒnwald, P.;
Hazan, E.; and Kale, S., eds., Proceedings of The 28th Conference on Learning Theory, volume 40 of Proceedings of Machine
Learning Research, 1376–1401. Paris, France: PMLR.
Pisier, G. 1980-1981. Remarques sur un résultat non publié de B. Maurey. Séminaire Analyse fonctionnelle (dit "MaureySchwartz") Talk:5.
Platen, E. 1986. Pollard, D.:Convergence of stochastic processes. (Springer series in statistics). Springer-Verlag, New York - Berlin
- Heidelberg - Tokyo 1984, 216 pp., 36 illustr., DM 82. Biometrical Journal 28(5): 644–644. doi:10.1002/bimj.4710280516.
Prabhu, Y.; and Varma, M. 2014. FastXML: A Fast, Accurate and Stable Tree-classifier for Extreme Multi-label Learning. In
Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14, 263–272.
New York, NY, USA: ACM. ISBN 978-1-4503-2956-9. doi:10.1145/2623330.2623651.
Scott, C. 2014. Rademacher Complexity. Lecture Notes Statistical Learning Theory.
Sedghi, H.; Gupta, V.; and Long, P. M. 2019. The Singular Values of Convolutional Layers. In International Conference on
Learning Representations.
Silver, D.; Hubert, T.; Schrittwieser, J.; Antonoglou, I.; Lai, M.; Guez, A.; Lanctot, M.; Sifre, L.; Kumaran, D.; Graepel, T.;
Lillicrap, T.; Simonyan, K.; and Hassabis, D. 2018. A general reinforcement learning algorithm that masters chess, shogi, and
Go through self-play. Science 362(6419): 1140–1144. ISSN 0036-8075. doi:10.1126/science.aar6404.
Suzuki, T. 2018. Fast generalization error bound of deep learning from a kernel perspective. In Storkey, A.; and Perez-Cruz, F.,
eds., Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings
of Machine Learning Research, 1397–1406. Playa Blanca, Lanzarote, Canary Islands: PMLR.
Talagrand, M. 1994. Sharper Bounds for Gaussian and Empirical Processes. The Annals of Probability 22(1): 28–76. ISSN
00911798.
Talagrand, M. 1996. New concentration inequalities in product spaces. Inventiones mathematicae 126(3): 505–563. ISSN
1432-1297. doi:10.1007/s002220050108.
Wei, C.; and Ma, T. 2019. Data-dependent Sample Complexity of Deep Neural Networks via Lipschitz Augmentation. In
Wallach, H.; Larochelle, H.; Beygelzimer, A.; d'Alché-Buc, F.; Fox, E.; and Garnett, R., eds., Advances in Neural Information
Processing Systems 32, 9725–9736. Curran Associates, Inc.
Zhang, C.; Bengio, S.; Hardt, M.; Recht, B.; and Vinyals, O. 2017. Understanding deep learning requires rethinking generalization.
Zhang, J.; Lei, Q.; and Dhillon, I. S. 2018. Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization.
In ICML, volume 80 of Proceedings of Machine Learning Research, 5801–5809. PMLR.
Zhang, T. 2002. Covering Number Bounds of Certain Regularized Linear Function Classes. J. Mach. Learn. Res. 2: 527–550.
ISSN 1532-4435. doi:10.1162/153244302760200713.
Zhou, W.; Veitch, V.; Austern, M.; Adams, R. P.; and Orbanz, P. 2019. Non-vacuous Generalization Bounds at the ImageNet
Scale: a PAC-Bayesian Compression Approach. In International Conference on Learning Representations. openreview.net.
Zou, D.; Cao, Y.; Zhou, D.; and Gu, Q. 2018. Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks.
CoRR abs/1811.08888.

