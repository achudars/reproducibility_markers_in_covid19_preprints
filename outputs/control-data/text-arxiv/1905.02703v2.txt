arXiv:1905.02703v2 [eess.SP] 21 Nov 2019

Fusion of Deep Neural Networks for Activity
Recognition: A Regular Vine Copula Based
Approach
Shan Zhang, Baocheng Geng and Pramod K. Varshney

Muralidhar Rangaswamy

Department of Electrical Engineering and Computer Science
Syracuse University, Syracuse, NY 13244
{szhang60, bageng, varshney}@syr.edu

Air Force Research Labs
Wright-Patterson Air Force Base, OH 45433
Muralidhar.Rangaswamy@us.af.mil

Abstract—In this paper, we propose regular vine copula based
fusion of multiple deep neural network classifiers for the problem
of multi-sensor based human activity recognition. We take the
cross-modal dependence into account by employing regular vine
copulas that are extremely flexible and powerful graphical models
to characterize complex dependence among multiple modalities.
Multiple deep neural networks are used to extract high-level
features from multi-sensing modalities, with each deep neural
network processing the data collected from a single sensor. The
extracted high-level features are then combined using a regular
vine copula model. Numerical experiments are conducted to
demonstrate the effectiveness of our approach.

I. I NTRODUCTION
The recent development of small-size, low-power, multifunctional sensors and revolutionary advances in communication and computing technologies have resulted in many applications of sensor/information fusion. One such application is
human activity recognition (HAR) that can be used for health
care, personal fitness, and border surveillance, etc. [1]–[3].
The task of HAR is to detect and recognize human actions
from the data provided by multiple sensors. HAR is naturally
a classification task. Combining multiple sensing modalities
can boost the classification performance. However, since each
sensor carries a unique physical trait, sensor heterogeneity or
incommensurability is the first critical challenge for multimodal fusion. Also, multiple sensor modalities tend to be
dependent due to non-linear cross-modal interactions. In this
paper, we study a classification problem by fusing multiple
classifiers, one for each modality, for the recognition of human
actions. Thus, the data compatibility issues among multiple
modalities can be avoided [4], [5]. Moreover, the non-linear
dependence is also addressed.
Most of the current multi-classifier fusion solutions for HAR
rely on shallow classifiers, such as Support Vector Machines
(SVMs), Random Forests and Decision Trees, which employ
handcrafted statistical features extracted from each modality.
The typical strategy for the fusion of these features is to
combine the outputs obtained from multiple classifiers, where
The work was supported by Air Force Office of Scientific Research under
Grant FA9550-17-1-0313 under the DDDAS program.

each classifier only takes the features of one modality [6]–[11].
Since the multivariate sensor observations are dependent, the
outputs from the multiple classifiers are inherently dependent.
This dependence can be non-linear. The design of an optimal
fusion rule that can address this non-linear dependence becomes the key issue. In the aforementioned literature [6]–[11],
simple fusion rules, such as majority voting, naive Bayesian
fusion, weighted average fusion, and Dempster-Shafer fusion,
were used. None of these existing fusion rules in [6]–[11]
exploit the non-linear dependence among multiple classifiers.
Moreover, designing and selecting robust features heavily
relies on human experience and is time consuming. Also,
only shallow features, such as mean, variance and amplitude,
can be learned according to human expertise, which can be
insufficient for more complex activities [12].
Deep learning has had breakthroughs in a wide variety
of unimodal applications, such as visual object recognition,
natural language processing and object detection [13]. Compared to the shallow classifiers, the deep classifiers can learn
many more high-level features directly from raw data (or
lightly processed data) and avoid the need for the design of
handcrafted features. Recently, single deep learning models for
HAR have been discussed in detail in the survey paper [14].
Very recently, multimodal deep learning methodologies for
HAR have attracted some attention [15], [16]. Similar fusion
strategies as discussed for multiple shallow classifiers can be
applied to deep neural networks, based on the level where
the fusion is performed: intermediate fusion with higher-level
representations, referred to as high-level features, late fusion
with decisions or probability scores. In [15], [16], intermediate
fusion strategies using Deep Neural Networks (DNNs) and
Convolutional Neural Networks (CNNs) were studied, where
a fully connected fusion layer was used to combine multiple
DNNs or CNNs. As mentioned earlier, the data from multiple sensing modalities are non-linearly dependent. The fully
connected fusion layer can learn this dependence in some
manner. However, understanding and analyzing this non-linear
dependence using the fully connected layer or another deep
neural network is yet to be solved.
Copula-based dependence modeling [17] provides a flexible

parametric characterization of the joint distribution of multiple
sensor observations. It allows the separation of modeling
different univariate marginals from modeling the multivariate
(dependence) structure. The separated multivariate structure
is referred to as a multivariate copula that can characterize
non-linear or even more complex dependence. It has been
shown that copula-based fusion of multiple sensing modalities
can significantly improve the inference performance [3], [18]–
[21]. However, for high-dimensional dependence structures,
the underlying dependence can be more complex and the
well defined multivariate copula functions, e.g., multivariate
Gaussian copulas, multivariate Student-t copulas, may lack
the ability to characterize the potential complex dependence
[22]. In this paper, we adopt a more general copula model,
regular vine (R-Vine) copulas [23]–[25] that are constructed by
decomposing a multivariate copula into a cascade of bivariate
copulas.
In this paper, we propose to leverage the DNNs and
the R-Vine copula based dependence modeling for sensorbased recognition of human actions. More specifically, we use
multiple DNNs to extract high-level features from multiple
sensing modalities, where each DNN only takes the data from
a single sensor. Different from the fusion strategy (using a
fully connected fusion layer) in [15], [16], we propose a
probabilistic fusion methodology, R-Vine copula based fusion
rule, that combines the extracted high-level features and characterizes the cross-modal dependence. Moreover, our proposed
model is designed to improve the classification performance
compared to the neural network based fusion method and
adds interpretability in the sense that it explicitly explains the
dependence structure of the extracted features from different
modalities.

II. C OPULA T HEORY
A copula is a multivariate distribution with uniform
marginal distributions. The unique correspondence between a
multivariate copula and any multivariate distribution is stated
in Sklar’s theorem [26] which is a fundamental theorem that
forms the basis of copula theory. Also, for high-dimensional
dependence structures, more flexible R-Vine copulas are constructed by decomposing a multivariate copula into a set of
bivariate copulas, which will be discussed in Section IV.
Theorem 1 (Sklar’s Theorem): The joint distribution
function F of random variables x1 , . . . , xd can be cast as
F (x1 , x2 , . . . , xd ) = C(F1 (x1 ), F2 (x2 ), . . . , Fd (xd )),

(1)

where F1 , . . . , Fd are marginal distribution functions for
x1 , . . . , xd . If Fm , m = 1, . . . , d are continuous, C is a
unique d-dimensional copula. Conversely, given a copula
C and univariate Cumulative Distribution Functions (CDFs)
F1 , . . . , Fd , F in (1) is a valid multivariate CDF with
marginals F1 , . . . , Fd .

For continuous distributions F and F1 , . . . , Fd , the joint
Probability Density Function (PDF) of random variables
x1 , . . . , xd is obtained by differentiating both sides of (1):
f (x1 , . . . , xd ) =

d
Y


fm (xm ) c(F1 (x1 ), . . . , Fd (xd )), (2)

m=1

where f1 , . . . , fd are the marginal densities and c is referred
to as the density of the multivariate copula C that is given by
c(u) =

∂ L (C(u1 , . . . , ud ))
,
∂u1 , . . . , ∂ud

(3)

where um = Fm (xm ) and u = [u1 , . . . , ud ]. Note that
C(·) is a valid CDF and c(·) is a valid PDF for uniformly
distributed random variables um , m = 1, 2, . . . , d. Since the
random variable um represents the CDF of xm , the CDF of
um naturally follows a uniform distribution over [0, 1].
Various families of multivariate copula functions are presented in [26], such as elliptical and Archimedean copulas.
Since different copula functions model different types of dependence, selection of copula functions to fit the given data is
a key problem. Moreover, the dependence parameter denoted
by φ, contained in a copula function, is used to characterize the
amount of dependence among d random variables. Typically,
φ is unknown a priori and needs to be estimated, e.g., using
Maximum Likelihood Estimation (MLE) or Kendall’s τ [27].
Note that in general, φ may be a scalar, a vector or a matrix.
III. P ROBLEM S TATEMENT
Consider a supervised classification problem with G classes.
Let Ω = {w1 , w2 , . . . , wG } be the set of class labels. L sensors
make a set of observations regarding the object/event at time
instant n, {x1n , x2n , . . . , xLn , yn }, where n = 1, 2, . . . and
1
2
xln ∈ Rdl ×dl , d1l , d2l ∈ N, N = [1, 2, . . .] is the observation of
sensor l at time n. yn ∈ Ω is the class label. We assume that
the sensor observations are continuous random variables that
are conditionally independent and identically distributed (i.i.d.)
over time. L independent pre-trained DNN classifiers are used
to extract high-level features from each sensing modality. A
typical DNN is shown in Fig. 1. Compared to the traditional
artificial neural networks, DNN is more capable of learning
informative features from large amount of data. We use hln ∈
R1×rl , rl ∈ N to represent the nth high-level feature vector
extracted from sensor l. These high-level features are then
combined using the R-Vine copula based fusion rule. We show
the classification system to be studied in Fig. 2.
Remark 1: Note that for the sensor-based HAR, we use
the DNNs to extract high-level features instead of CNNs.
There are two main reasons. The first one is that compared
to DNNs, CNNs are computationally more intensive. The
second one is that the high-level features extracted from
CNNs are generally high dimensional. Also, among these high
dimensional features, a large number of features are irrelevant
and redundant. Fusing all the features based on R-Vine copula
models is computationally inefficient.
Our aim is to determine the class label by combining the
extracted high-level features. Assume that we have a training

dependence structure that is fully characterized by the copula
(see (2)). Therefore, we have

f (h|wi ) =

N Y
K
Y
n=1

Fig. 1: A typical Deep Neural Network structure [15].

Sensor 𝟏𝟏

DNN

High-level 𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅 𝒉𝒉𝟏𝟏

Sensor 𝒍𝒍

DNN

High−level 𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅 𝒉𝒉𝒍𝒍

Sensor 𝑳𝑳

DNN

High−level 𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅𝐅 𝒉𝒉𝑳𝑳

⋮
⋮

R-Vine Copula
Based Fusion

Classification
Decision

Fig. 2: R-Vine Copula Based Multi-modal DNN.

set with a total of N feature vectors and the joint feature vector
is
1×(r1 +r2 +...+rL )
hn = [h1n h2n . . . hL
, n = [N ],
n] ∈ R

f (h|wi )P (wi )
∝ f (h|wi )P (wi ),
f (h)

(5)

where h = [h1 , . . . , hN ], f (h|wi ) is the joint likelihood
function and P (wi ) is the prior probability for class wi . If
the class prior probabilities are not known, it is commonly
assumed that the classes are equally likely. The class label
w0 is determined by choosing the label with highest posterior
probability, which is given by
w0 = arg max P (wi |h).
wi ∈Ω

(6)

Since f (h) is a constant for all the classes, the main problem
is how to model and maximize f (h|wi ) under unknown
multivariate dependence. In the following section, we will use
R-Vine copulas to model the joint likelihood function.
IV. R-V INE C OPULA BASED F USION OF M ULTIPLE D EEP
N EURAL N ETWORKS
In this section, we present the R-Vine copula based fusion
rule. Our goal now is to find the joint PDFs of feature vectors
h under each class. According to Sklar’s theorem (Section
II), the joint PDF can be separated into its marginals and the

(7)

k=1

where fk (hkn |wi ), k = 1, . . . , K are the marginal PDFs and
i
Fi (hn ) = [F1i (h1n ), . . . , FK
(hKn )] denotes all the marginal
CDFs at time instant n under class wi , wi ∈ Ω. Moreover,
ci is the copula density function for class wi and φi is the
corresponding parameter set.
Since we have no knowledge of the joint distributions of
the extracted high-level features, the marginal PDFs, marginal
CDFs, copula density functions and their corresponding parameters need to be estimated using the training dataset. The
estimation of the marginal distributions and optimal copula
density functions for all the classes is similar. Therefore, the
class index i will be omitted for now to simplify notations.
The marginal PDFs can be estimated non-parametrically
using kernel density estimators [28] that provide a smoothed
estimate of true density by choosing the optimal bandwidth
so that an accurate estimate is achieved. Further, the marginal
CDFs can be determined by the Empirical Probability Integral
Transforms (EPIT) [18]. The estimate of Fk (·) is given as

(4)

where [N ] = [1, 2, . . . , N ]. In the following, for notational
simplicity, we omit the superscripts of the feature vectors in
(4) and let hn = [h1n , h2n , . . . , hKn ], K = r1 + r2 + . . . + rL .
Using Bayes’ theorem, the posterior probability of class wi
given the joint high-level feature vectors is given as:
P (wi |h) =

 

fk (hkn |wi ) ci Fi (hn )|φi ,

F̂k (·) =

N
1 X
1h <{·} ,
N n=1 kn

(8)

where 1{·} is the indicator function.
Next, we discuss how to construct and find the optimal
multivariate copula c∗ using R-Vine copula models, which was
introduced by Bedford and Cooke in [23], [24].

A. R-Vine copula Models
Before we give the formal definition of the R-Vine copula,
we first introduce R-Vine. A d dimensional R-Vine V =
(T1 , . . . , Td−1 ) is a nest of d − 1 trees, where the edges of
the tree Ti are the nodes of the tree Ti+1 , and if two edges
in tree Ti share a common node, they are connected in tree
Ti+1 . By specifying bivariate copulas on each of the edges,
we obtain R-Vine copula and it is defined as follows.
Definition 1 (R-Vine Copula): An R-Vine copula consists
of three parts, denoted by (F, V, B), where
1) F = [F1 , F2 , . . . , Fd ]T ∈ [0, 1]d is a vector with uniform
marginals.
2) V is a d-dimensional R-Vine.
3) B = {CCe,a ,Ce,b |De | e = {a, b} ∈ Ei , i = 1, 2, . . . , d −
1; φ} is a set of bivariate copulas with a set of parameters
φ, where Ei is the edge set for tree Ti , Ce,a , Ce,b
are the conditioned nodes of the edge e and De is the
conditioning node set of the edge e.

𝒄𝒄𝟏𝟏,𝟐𝟐 (�; 𝜱𝜱𝟏𝟏,𝟐𝟐 )

1

f (zk )

Y

×

(9)
3,4|2

cCe,a ,Ce,b |De (FCe,a |De (zCe,a |zDe ), FCe,b |De (zCe,b |zDe ); φ),
where e = {a, b} and zDe = {zj |j ∈ De }, f (zk ) is the
marginal PDF of the observation of variable k, k = 1, . . . , K.
Moreover, the conditional distribution FCe,a |De (zCe,a |zDe ) can
be obtained recursively tree by tree using the following equation [29]:


∂CCa,a1,Ca,a2|Da FCa,a1|Da(zCa,a1|zDa), FCa,a2|Da(zCa,a2 |zDa)
,
∂FCa,a2|Da(zCa,a2|zDa)
(10)
where e = {a, b} ∈ Ei , a = {a1 , a2 } and b = {b1 , b2 } are
the edges that connect Ce,a and Ce,b given the conditioning
variables De . Similarly, we can obtain FCe,b |De (zCe,b |zDe ).
An illustrative 5-dimensional R-Vine copula example is
shown in Fig. 3. For variables z1 , z2 , z3 , z4 , z5 and if they
share the same dependence structure as in Fig. 3, according
to (9), their joint PDF can be expressed as
" 5
#
Y
f (z1 ,z2 ,z3 ,z4 ,z5 ) =
f (zl ) ·c1,2 ·c2,3 ·c2,4 ·c3,5 ·c1,3|2

2,3

𝒄𝒄𝟐𝟐,𝟓𝟓|𝟑𝟑 (�; 𝜱𝜱𝟐𝟐,𝟓𝟓|𝟑𝟑 )

𝑻𝑻𝟐𝟐

3,5

1,3|2

𝒄𝒄𝟏𝟏,𝟓𝟓|𝟐𝟐𝟐𝟐 (�; 𝜱𝜱𝟏𝟏,𝟓𝟓|𝟐𝟐𝟐𝟐 )

𝒄𝒄𝟒𝟒,𝟓𝟓|𝟏𝟏𝟏𝟏𝟏𝟏 (�; 𝜱𝜱𝟒𝟒,𝟓𝟓|𝟏𝟏𝟏𝟏𝟏𝟏 )

2,5|3

𝑻𝑻𝟑𝟑

𝑻𝑻𝟒𝟒

1,5|23

omit the conditioning elements for conditional marginal CDFs.
Note that the conditional marginal CDFs can be obtained
recursively using (10).
The best copula c∗ is selected from the copula library C
using the Akaike Information Criterion (AIC) [31] as the
criterion, which is given as
AICm = −

N
X

log cm (F̂k1 (hk1 n ), F̂k2 (hk2 n )|φ̂m ) + 2q(K),

n=1

(12)
where q(K) is the number of parameters in the mth copula model. Also, the conditioning elements for conditional
marginal CDFs are omitted.
The best copula c∗ is
c∗ = arg min AICm .

(13)

V. N UMERICAL R ESULTS

B. Estimation of Optimal R-Vine copula
The estimation of optimal R-Vine copula model for the
joint feature vector h requires the selection of the R-Vine
tree structure V, the choice of copula families for the bivariate copula set B and the estimation of their corresponding
parameters φ. To select the optimal R-Vine tree structure,
we adopt the sequential maximum spanning tree algorithm
in [30]. This sequential method is based on Kendall’s τ . The
sequential method starts with the selection of the first tree
T1 and continues tree by tree up to the last tree TK−1 . The
trees are selected in a way that the chosen bivariate copula
models the strongest pair-wise dependencies present which
are characterized by Kendall’s τ . After the optimal R-Vine
tree structure is selected, we need to define a bivariate copula
family and estimate the optimal bivariate copulas that best
characterizes the pair-wise dependencies.
Consider a library of copula, C = {cm : m = 1, 2, . . . , M }
Before estimating the optimal bivariate copula, the copula
parameter set φ is first obtained using MLE, which is given
by
log c(F̂k1 (hk1 n ), F̂k2 (hk2 n )|φ),

𝑻𝑻𝟏𝟏

cm ∈C

where the inputs for the bivariate copulas are omitted.

φ

5

Fig. 3: An example R-Vine copula for five variables.

·c3,4|2 ·c2,5|3 ·c1,4|23 ·c1,5|23 ·c4,5|123 ,

N
X

𝒄𝒄𝟏𝟏,𝟒𝟒|𝟐𝟐𝟐𝟐 (�; 𝜱𝜱𝟏𝟏,𝟒𝟒|𝟐𝟐𝟐𝟐 )
1,4|23

l=1

φ̂ = arg max

𝒄𝒄𝟑𝟑,𝟓𝟓 (�; 𝜱𝜱𝟑𝟑,𝟓𝟓 )

3

2,4

i=1 e∈Ei

k=1

𝒄𝒄𝟏𝟏,𝟑𝟑|𝟐𝟐 (�; 𝜱𝜱𝟏𝟏,𝟑𝟑|𝟐𝟐 )

𝒄𝒄𝟑𝟑,𝟒𝟒|𝟐𝟐 (�; 𝜱𝜱𝟑𝟑,𝟒𝟒|𝟐𝟐 )

f (z|V, B, φ) =

K−1
Y

𝒄𝒄𝟐𝟐,𝟑𝟑 (�; 𝜱𝜱𝟐𝟐,𝟑𝟑 )

4

1,2

K
Y

2
𝒄𝒄𝟐𝟐,𝟒𝟒 (�; 𝜱𝜱𝟐𝟐,𝟒𝟒 )

Using the R-Vine copula model and Sklar’s theorem,
the joint PDF of a K-dimensional observation vector z =
[z1 , . . . , zK ] is given by

(11)

n=1

where (k1 , k2 ), k1 , k2 ∈ [1, 2, . . . , K] is a connected pair in the
selected R-Vine tree V and for simplification of notation, we

In this section, we demonstrate the efficacy of our proposed
R-Vine copula based methodology for the fusion of multiple
DNNs. To show the superiority of our proposed fusion scheme,
we also compare our result with the classification performance
obtained by using the following schemes:
• Single modality without fusion: Feed the raw data into a
DNN classifier.
• Data-level fusion: Concatenate all the raw data from
different modalities into one input vector and feed it into
a DNN classifier.
• Fully connected layer fusion: Concatenate the extracted
features into one feature vector and use a fully connected
fusion layer to achieve a final classification decision.
TABLE I: STISEN: F1 scores for Watch-DNN, Phone-DNN, Fullyconnected layer fusion, Data-level fusion, R-Vine copula fusion.
Model
Watch-DNN
Phone-DNN
Fully-connected layer fusion
Data-level fusion
R-Vine copula fusion

F1 score
71.4%
70.2%
78.0%
79.3%
88.6%

A. Datasets
We select two publicly available datasets that contain multimodality sensor readings for the recognition of human activities.

TABLE II: ANGUITA: F1 scores for Accelerometer-DNN,
Gyroscope-DNN, Fully-connected layer fusion, Data-level fusion, RVine copula fusion.
Model
Accelerometer-DNN
Gyroscope-DNN
Fully-connected layer fusion
Data-level fusion
R-Vine copula fusion

F1 score
87.8%
72.9%
91.9%
88.3%
92.8%

STISEN Heterogeneity Activity Recognition Dataset, collected by Stisen et al. [32], contains the sensor readings
from two modalities: smartphone and smart watch. Each
modality is equipped with two motion sensors, accelerometer
and gyroscope. There are 6 classes (‘Sit’, ‘Stand’, ‘Walk’,
‘Stairsup’, ‘Stairsdown’, ‘Bike’) to be classified. We focus
on the fusion of phone and watch modalities. Each of the
two motion sensors produces a three-dimensional data vector,
making each data sample contain 6 attributes in total. We select
the data captured by Samsung Galaxy S3 mini phone and
Samsung Galaxy Gear watch, where the data samples were
sampled at a rate of 100 Hz. 9000 samples from each modality
are used to train and test DNN models for feature selection,
and another 9000 samples are used to train and test the R-Vine
copula based fusion methodology.
ANGUITA Human Activity Recognition Using Smartphone
Dataset, collected by Anguita et al. [33], contains accelerometer and gyroscope three-dimensional sensor data. It was
collected from 30 volunteers who performed six different activities (‘Walking’, ‘Walking-upstairs’, ‘Walking-downstairs’,
‘Siting’, ‘Standing’, ‘Laying’). We focus on the fusion of
accelerometer and gyroscope modalities. These sensor data
were sampled at a rate of 50 Hz, and were separated into
windows of 128 values. Each window has 50% overlap with
the previous window. The 128-real value vector in each
window stands for one sample for each activity.
B. Classification Accuracy
We use F1 score as the classification performance metric,
which is given by
2 X precisionw × recallw
,
(14)
F1 =
|Ω| w precisionw + recallw
P
TP
where precision = T PT+F
P and recall = T P +F N . Here, TP,
FP and FN denote true positive, false positive and false negative, respectively. F1 score is robust to unbalanced distributions
of data samples across classes.
Table I and Table II show the F1 scores comparing the
five classification schemes: two single modalities based DNN
classifiers and three multi-modal fusion based DNN classifiers
for the STISEN and ANGUITA datasets, respectively. As
we can see, fusion based schemes perform better than single
modality based schemes. Also, our proposed R-Vine copula
based fusion methodology performs better than using the datalevel fusion scheme and fully connected fusion layer scheme.
Our proposed methodology achieves an overall 88.6% and

92.8% F1 scores for the STISEN and ANGUITA datasets,
respectively. Moreover, we can see that for phone and watch
modalities, the R-Vine copula based fusion scheme achieves
higher performance improvement compared to accelerometer
and gyroscope modalities. This is because of the fact that
the accelerometer and gyroscope are less dependent while the
phone and watch are highly dependent.
It should be noted that the training of R-Vine copula models
requires less number of training samples compared to the
training of a fully connected fusion layer or another DNN used
for fusion. In Table I and Table II, the performance of our RVine copula scheme is obtained by using a total of N = 1200
feature samples. However, the fully connected fusion layer
based scheme requires a total of N = 6000 feature samples.
In Fig. 4, we show the first level dependence structure (first
tree of the R-Vine copula model; see Fig. 3 as an example) of
the extracted features using our proposed R-Vine copula based
fusion method for activity ‘Walking-upstairs’ in ANGUITA
dataset. Here, features h1 , h2 , h3 , h4 are from the accelerometer while the features h5 , h6 , h7 are from the gyroscope. As
we can see that, features h3 and h6 are highly correlated and
the two modalities accelerometer and gyroscope are correlated
mainly via these two features. Using the knowledge of intramodal and cross-modal feature dependencies, we can trace
back and find where these dependent features originated from,
which would yield the reduction of training data needed in
DNNs. Furthermore, we are able to understand the correlation
among the raw data from different modalities. The R-Vine
copula based fusion method adds interpretability of the model
which explicitly provides the dependence structure for features
from different modalities, compared to the neural network
based fusion which is totally a ‘black-box’ model.
𝒉𝒉𝟐𝟐
𝒉𝒉𝟏𝟏

𝒉𝒉𝟒𝟒

𝒉𝒉𝟑𝟑

𝒉𝒉𝟔𝟔

𝒉𝒉𝟕𝟕

𝒉𝒉𝟓𝟓

Fig. 4: First level dependence structure for activity ‘Walking-upstairs’.

Table III and Table IV show the confusion matrices using
the R-Vine copula based fusion scheme for the STISEN and
ANGUITA datasets, respectively. As we observe, the fusion
of phone and watch modalities achieves perfect classification
for static activities (‘Sit’ and ‘Stand’). Also, the fusion of the
accelerometer and gyroscope from the smartphone achieves
significantly accurate classification for moving activities (e.g.,
‘Walking’, ‘Laying’).
VI. C ONCLUSION
In this paper, an R-Vine copula based feature fusion approach was presented to perform activity recognition using
multi-modal sensor observations. The features of each modality were extracted via a DNN and afterwards, an R-Vine
copula model was constructed to capture the dependencies
of intra-modal and cross-modal features. The procedures of

TABLE III: STISEN: Confusion matrix for R-Vine copula based fusion.
Sit
Stand
Walk
Stairsup
Stairsdown
Bike
Precision

Sit
498
0
0
0
0
0
100.0%

Stand
0
454
0
0
0
0
100.0%

Walk
0
0
402
24
39
8
85.0%

Stairsup
0
0
43
408
53
4
80.3%

Stairsdown
2
46
17
50
408
2
77.7%

Bike
0
0
38
0
0
486
89.7%

Recall
99.6%
90.8%
80.4%
81.6%
81.6%
97.2%
88.6%

TABLE IV: ANGUITA: Confusion matrix for R-Vine copula based fusion.
Walking
Walking-upstairs
Waking-downstairs
Sitting
Standing
Laying
Precision

W
276
6
8
3
3
2
92.6%

WU
0
259
0
6
3
0
96.6%

WD
17
0
211
1
2
0
91.3%

model construction involve selecting the optimal R-Vine tree
structure, obtaining the copula parameter set φ, and choosing
the best copula c∗ . Experiments on two human activity datasets
demonstrated the efficiency of our proposed method compared
to neural network based data/feature fusion, in terms of high
prediction accuracy, less number of training samples required
and dependence interpretability. In the future, we aim to
address the problem of feature selection while constructing
copula fusion model to achieve the balance of computation
efforts and classification performance.
R EFERENCES
[1] Pang Wu, Huan-Kai Peng, Jiang Zhu, and Ying Zhang, “Senscare: Semiautomatic activity summarization system for elderly care,” in International Conference on Mobile Computing, Applications, and Services.
Springer, 2011, pp. 1–19.
[2] Pang Wu, Jiang Zhu, and Joy Ying Zhang, “Mobisens: A versatile
mobile sensing platform for real-world applications,” Mobile Networks
and Applications, vol. 18, no. 1, pp. 60–80, 2013.
[3] Satish G Iyengar, Pramod K Varshney, and Thyagaraju Damarla, “On
the detection of footsteps based on acoustic and seismic sensing,” in
2007 41st Asilomar Conference on Signals, Systems and Computers,
2007.
[4] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee,
and Andrew Y Ng, “Multimodal deep learning,” in Proceedings of the
28th international conference on machine learning (ICML-11), 2011,
pp. 689–696.
[5] Dana Lahat, Tülay Adali, and Christian Jutten, “Multimodal data fusion:
an overview of methods, challenges, and prospects,” Proceedings of the
IEEE, vol. 103, no. 9, pp. 1449–1477, 2015.
[6] Piero Zappi, Thomas Stiefmeier, Elisabetta Farella, Daniel Roggen, Luca
Benini, and Gerhard Troster, “Activity recognition from on-body sensors
by classifier fusion: sensor scalability and robustness,” in Intelligent
Sensors, Sensor Networks and Information, 2007. ISSNIP 2007. 3rd
International Conference on. IEEE, 2007, pp. 281–286.
[7] Maja Stikic, Tâm Huynh, Kristof Van Laerhoven, and Bernt Schiele,
“Adl recognition based on the combination of rfid and accelerometer
sensing,” in Pervasive Computing Technologies for Healthcare, 2008.
PervasiveHealth 2008. Second International Conference on. IEEE, 2008,
pp. 258–263.
[8] Ming Li, Viktor Rozgić, Gautam Thatte, Sangwon Lee, Adar Emken,
Murali Annavaram, Urbashi Mitra, Donna Spruijt-Metz, and Shrikanth
Narayanan, “Multimodal physical activity recognition by fusing temporal and cepstral information,” IEEE transactions on neural systems
and rehabilitation engineering: a publication of the IEEE Engineering
in Medicine and Biology Society, vol. 18, no. 4, pp. 369, 2010.

Si
3
3
0
248
26
0
88.6%

St
0
3
1
33
298
6
87.4%

L
0
0
0
0
0
329
100.0%

Recall
93.2%
95.6%.
95.9%
85.2%
87.4%
97.63%
92.8%

[9] Oresti Banos, Miguel Damas, Hector Pomares, Fernando Rojas, Blanca
Delgado-Marquez, and Olga Valenzuela, “Human activity recognition
based on a sensor weighting hierarchical classifier,” Soft Computing,
vol. 17, no. 2, pp. 333–343, 2013.
[10] Haodong Guo, Ling Chen, Liangying Peng, and Gencai Chen, “Wearable
sensor based multimodal human activity recognition exploiting the
diversity of classifier ensemble,” in Proceedings of the 2016 ACM
International Joint Conference on Pervasive and Ubiquitous Computing.
ACM, 2016, pp. 1112–1123.
[11] Henry Friday Nweke, Ying Wah Teh, Ghulam Mujtaba, and Mohammed Ali Al-garadi, “Data fusion and multiple classifier systems
for human activity detection and health monitoring: Review and open
research directions,” Information Fusion, vol. 46, pp. 147–170, 2019.
[12] Jianbo Yang, Minh Nhut Nguyen, Phyo Phyo San, Xiaoli Li, and Shonali
Krishnaswamy, “Deep convolutional neural networks on multichannel
time series for human activity recognition.,” in Ijcai, 2015, vol. 15, pp.
3995–4001.
[13] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, “Deep learning,”
nature, vol. 521, no. 7553, pp. 436, 2015.
[14] Jindong Wang, Yiqiang Chen, Shuji Hao, Xiaohui Peng, and Lisha Hu,
“Deep learning for sensor-based activity recognition: A survey,” Pattern
Recognition Letters, 2018.
[15] Valentin Radu, Catherine Tong, Sourav Bhattacharya, Nicholas D Lane,
Cecilia Mascolo, Mahesh K Marina, and Fahim Kawsar, “Multimodal
deep learning for activity and context recognition,” Proceedings of the
ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies,
vol. 1, no. 4, pp. 157, 2018.
[16] Fernando Moya Rueda, René Grzeszick, Gernot A Fink, Sascha Feldhorst, and Michael ten Hompel, “Convolutional neural networks for
human activity recognition using body-worn sensors,” in Informatics.
Multidisciplinary Digital Publishing Institute, 2018, vol. 5, p. 26.
[17] Harry Joe, Dependence modeling with copulas, CRC Press, 2014.
[18] Hao He, Arun Subramanian, Pramod K Varshney, and Thyagaraju
Damarla, “Fusing heterogeneous data for detection under non-stationary
dependence,” in 2012 15th International Conference on Information
Fusion (FUSION). IEEE, 2012, pp. 1792–1799.
[19] Ashok Sundaresan and Pramod K Varshney, “Location estimation of a
random signal source based on correlated sensor observations,” IEEE
Transactions on Signal Processing, vol. 59, no. 2, pp. 787–799, 2011.
[20] Satish G Iyengar, Pramod K Varshney, and Thyagaraju Damarla, “Biometric authentication: a copula based approach,” Multibiometrics for
Human Identification, pp. 95–119, 2011.
[21] Hao He, Arun Subramanian, Sora Choi, Pramod K Varshney, and Thyagaraju Damarla, “Social media data assisted inference with application
to stock prediction,” in 2015 49th Asilomar Conference on Signals,
Systems and Computers. IEEE, 2015, pp. 1801–1805.
[22] Shan Zhang, Lakshmi Narasimhan Theagarajan, Sora Choi, and
Pramod K Varshney, “Fusion of correlated decisions using regular vine
copulas,” IEEE Transactions on Signal Processing, 2019.

[23] Tim Bedford and Roger M Cooke, “Probability density decomposition
for conditionally dependent random variables modeled by vines,” Annals
of Mathematics and Artificial intelligence, vol. 32, no. 1-4, pp. 245–268,
2001.
[24] Tim Bedford and Roger M Cooke, “Vines: A new graphical model for
dependent random variables,” Annals of Statistics, pp. 1031–1068, 2002.
[25] Kjersti Aas, Claudia Czado, Arnoldo Frigessi, and Henrik Bakken, “Paircopula constructions of multiple dependence,” Insurance: Mathematics
and economics, vol. 44, no. 2, pp. 182–198, 2009.
[26] Roger B Nelsen, An introduction to copulas, vol. 139, Springer Science
& Business Media, 2013.
[27] Hao He, Heterogeneous sensor signal processing for inference with
nonlinear dependence, Ph.D. thesis, Syracuse University, 2015.
[28] L Wassermann, “All of nonparametric statistics,” 2006.
[29] Harry Joe, “Families of m-variate distributions with given margins and
m (m-1)/2 bivariate dependence parameters,” Lecture Notes-Monograph
Series, pp. 120–141, 1996.
[30] Jeffrey Dissmann, Eike C Brechmann, Claudia Czado, and Dorota
Kurowicka, “Selecting and estimating regular vine copulae and application to financial returns,” Computational Statistics & Data Analysis,
vol. 59, pp. 52–69, 2013.
[31] Hirotogu Akaike, BN Petrov, and F Csaki, “Information theory and an
extension of the maximum likelihood principle,” 1973.
[32] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow,
Mikkel Baun Kjærgaard, Anind Dey, Tobias Sonne, and Mads Møller
Jensen, “Smart devices are different: Assessing and mitigatingmobile
sensing heterogeneities for activity recognition,” in Proceedings of the
13th ACM Conference on Embedded Networked Sensor Systems. ACM,
2015, pp. 127–140.
[33] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and
Jorge Luis Reyes-Ortiz, “A public domain dataset for human activity
recognition using smartphones.,” in Esann, 2013.

