SUBMITTED TO IEEE SIGNAL PROCESSING LETTERS

1

Fast High-Dimensional Kernel Filtering

arXiv:1901.06112v1 [cs.CV] 18 Jan 2019

Pravin Nair, Student Member, IEEE and Kunal N. Chaudhury, Senior Member, IEEE

Abstract—The bilateral and nonlocal means filters are instances of kernel-based filters that are popularly used in image
processing. It was recently shown that fast and accurate bilateral
filtering of grayscale images can be performed using a low-rank
approximation of the kernel matrix. More specifically, based on
the eigendecomposition of the kernel matrix, the overall filtering
was approximated using spatial convolutions, for which efficient
algorithms are available. Unfortunately, this technique cannot be
scaled to high-dimensional data such as color and hyperspectral
images. This is simply because one needs to compute/store a large
matrix and perform its eigendecomposition in this case. We show
how this problem can be solved using the Nyström method, which
is generally used for approximating the eigendecomposition of
large matrices. The resulting algorithm can also be used for
nonlocal means filtering. We demonstrate the effectiveness of our
proposal for bilateral and nonlocal means filtering of color and
hyperspectral images. In particular, our method is shown to be
competitive with state-of-the-art fast algorithms, and moreover it
comes with a theoretical guarantee on the approximation error.
Index Terms—Kernel Filter, Nyström Method, Approximation,
Fast Algorithm, Error Bound.

I. I NTRODUCTION
The bilateral and nonlocal means filters [1], [2] are widely
used for edge-preserving smoothing and denoising of images
[3], [4]. These are instances of kernel filters, where the similarity (affinity) between pixels is measured using a symmetric
kernel. We refer the reader to [4] for an excellent review of
kernel filters. While they have proven to be useful in practice,
a flip side of kernel filtering, including bilateral filtering (BLF)
and nonlocal means (NLM), is their computational complexity
[3]. Nevertheless, several fast algorithms have been proposed,
e.g. [5]–[26], which can speed up BLF and NLM, without
compromising their filtering quality. See [11], [21], [26] for
a survey of these algorithms. Unfortunately, most algorithms
only work with grayscale images, and cannot be extended to
color, multispectral, and hyperspectral images.
Algorithms for fast BLF of color images have been proposed
in [12], [21], [27]–[29]. However, to the best of our knowledge, these methods have not been extended for multispectral
and hyperspectral images. Fast algorithms for generic highdimensional BLF and NLM have been proposed in [18]–[20],
[30]. A common feature of these algorithms is that they use
data clustering or tessellation in high-dimensions. The stateof-the-art fast algorithms for color BLF are [19], [21], and for
color NLM is [20].
More recently, it was shown in [15], [17] that fast BLF of
grayscale images can be performed using the partial eigendecomposition of the kernel matrix. In fact, the interpretation
of BLF (and NLM) as kernel filters goes back to [31]–[33].
While the Nyström method has widely been used in machine
learning [34]–[36], it appears that [31] is the first to apply this

for image filtering. Note that, unlike [15], [17], the spatial and
range kernel are treated as a single kernel in [31]–[33].
The differences between our and related approaches are:
• As explained in detail in §II, it is difficult to scale [15],
[17] for filtering high-dimensional (even color) images, since
one needs to populate a huge kernel matrix and compute its
eigendecomposition. We propose to use the Nyström method
to solve this problem. As a result, we are able to perform BLF
and NLM of color and hyperspectral images.
• The first difference with [31]–[33] is that we use clustering
instead of uniform sampling for the Nyström approximation.
A significant improvement in filtering accuracy is achieved as
a result. The other difference is that if a spatial kernel has
to incorporated in [31]–[33], then the Nyström approximation
needs to be performed in the spatio-range space. However,
we handle the spatial and range components differently—fast
convolutions are used for the spatial component and Nyström
approximation is used for the range component. As a result,
we require lesser samples for the Nyström approximation.
• In [28], [29], clustering is used to compute “intermediate”
images, which are interpolated to get the final output. On the
other hand, clustering is used in our method just to obtain the
“landmark points” for the Nyström approximation.
• Compared to [18]–[21], our algorithm is conceptually
simple and easy to implement. Moreover, we are able to derive
a bound on the filtering error incurred by the approximation.
Such a guarantee is not offered by [18]–[21].
The rest of the paper is organized as follows. In §II, we
introduce the notion of kernel filtering, and explain the core
problem in relation to the spectral approximations in [15], [17].
We use the Nyström method in §III to overcome this problem.
Numerical results are reported in §IV and we conclude in §V.
II. BACKGROUND
We begin by formulating BLF and NLM as kernel filters [4].
Suppose the input image is f : Ω → [0, R]n , where Ω ⊂ Zd is
the spatial domain, [0, R]n is the range space, and d (resp. n) is
the dimension of the domain (resp. range). Let p : Ω → [0, R]ρ
be the guide image, which is used to control the filtering. For
standard BLF, f and p are identical, and n = ρ = 1 and 3 for
grayscale and color images. However, f and p (also n and ρ)
can be different for joint BLF [3]. For NLM, ρ is generally
larger than n, where ρ is the number of pixels in a patch [2].
Let κ : Rρ × Rρ → R be the range kernel. The filtered output
g : Ω → [0, R]n is given by

P
y∈Wx ω(x − y)κ p(x), p(y) f (y)
 ,
(1)
g(x) = P
y∈Wx ω(x − y)κ p(x), p(y)
where Wx is a square window around x ∈ Ω consisting of
(2S + 1)d pixels, with S being the window radius. The spatial

SUBMITTED TO IEEE SIGNAL PROCESSING LETTERS

2

kernel ω : Zd → R controls the weighting of the neighboring
pixels involved in the averaging. At this point, we just assume
that κ is symmetric, i.e., κ(t, s) = κ(s, t) for t, s ∈ Rρ . For
example, κ(t, s) = exp(−θks−tk2 ), θ > 0, for standard BLF
and NLM, where k·k is the Euclidean norm.
It was shown in [15], [17] that the non-linear operations in
(1) can be computed using convolutions by approximating κ.
For convenience, we will describe this using our notations. Let
the actual range of p be

R = p(x) : x ∈ Ω .
(2)
We emphasize that R is a list and not a set, i.e., we allow repetition of elements in R. In particular, let R = {r 1 , r 2 , ...., r m }
be some ordering of the elements in R, where m is the number
of elements. This means that, given ` ∈ [1, m], r ` = p(x) for
some x ∈ Ω. We track this correspondence using the index
map ι : Ω → [1, m], where
ι(x) = `

if r` = p(x).

(3)

We next define the kernel matrix K ∈ Rm×m given by
K(i, j) = κ(r i , r j ).
In terms of (4), we can write (1) as

P
y∈Wx ω(x − y)K ι(x), ι(y) f (y)

g(x) = P
y∈Wx ω(x − y)K ι(x), ι(y)

(4)

m
X

λk uk u>
k,

(5)

(6)

k=1

where λ1 , . . . , λm ∈ R are its eigenvalues, and u1 , . . . , um ∈
Rm are the corresponding eigenvectors. Substituting (6) in (5),
we can write its numerator as
(m
)
X
X


ω(x − y)
λk uk ι(x) uk ι(y) f (y).
y∈Wx

k=1

On switching the sums, this becomes
m
X


λk uk ι(x) (ω ∗ hk )(x),

Originally, the Nyström method was used for approximating
the solution of functional eigenvalue problems [39], [40]. The
method has found useful applications in machine learning and
computer vision for approximating the eigendecomposition of
large matrices [31], [34], [35]. In the present context, the goal
is to approximate (6) using a decomposition of the form
b =
K

m0
X

αk v k v >
k,

(8)

k=1

b is at most
where αk ∈ R and v k ∈ Rm . Clearly, the rank of K
b
m0 . Thus, for small m0 , K is a low-rank approximation of
K. A large m0 results in a better approximation, but at higher
computational cost. In practice, a good tradeoff is required.
The original kernel K is defined on R. In the Nyström
method [39], [40], we first construct a smaller kernel A, compute its eigendecomposition, and then “extrapolate” the eigenvectors of A to approximate those of K. More precisely, we
pick few landmarks points from R, say, R0 = {µ1 , . . . , µm0 },
and define a kernel A ∈ Rm0 ×m0 on R0 :

A(i, j) = κ(µi , µj )
i, j ∈ [1, m0 ] .
(9)
Clearly, A is symmetric, and its size is much smaller than K.
Thus, we can efficiently compute its eigendecomposition:

It is clear from (4) that K is symmetric. In particular, let the
eigendecomposition of K be
K=

III. P ROPOSED M ETHOD

(7)

k=1

where ω∗ hk denotes the convolution of the image hk (x) =
uk ι(x) f (x) with ω. An identical argument applies for the
denominator. In summary, we can compute (5) using convolutions, for which several efficient algorithms are available [37],
[38]. Moreover, by considering just the largest eigenvalues,
fast and accurate approximations can be obtained [15], [17].
Unfortunately, computing the full kernel and its eigendecomposition becomes prohibitively expensive when ρ is large.
Just as an example, consider an 8-bit color image for which
R = 255 and ρ = 3. Even if we assume that m is just 10% of
the maximum range cardinality (= 2563 ), we will still need
to populate a 3 million × 3 million matrix, and compute its
eigenvalues. The situation is worse for hyperspectral images,
where ρ is of the order of tens, or even hundreds.

A=

m0
X

αk wk w>
k,

(10)

k=1

where αk ∈ R and wk ∈ Rm0 . We next construct B ∈ Rm0 ×m
on R0 × R given by
B(i, j) = κ(µi , r j ),

(11)

where i ∈ [1, m0 ] and j ∈ [1, m]. This captures the kernel
values between the points in R and the landmark points. This
matrix is used to extrapolate wk as follows:

1 >
B wk
k ∈ [1, m0 ] .
(12)
vk =
αk
This completes the specification of αk and v k in (8). We refer
the reader to [35] for the intuition behind the approximation.
The effective speedup of replacing (6) by (8) is O(m/m0 )3 .
This is because the complexity of eigendecomposition of a k×
k matrix is O(k 3 ) [41]. In particular, the speedup is significant
since m0  m. As will be evident shortly, we just need to
b explicitly.
compute (αk ) and (v k ); we will not use K
Following [36], we select the landmark points by clustering
R. More specifically, we partition R into m0 disjoint sets
using k-means clustering, and take the centroids to be the
landmarks. Note that, though R0 is not guaranteed to be a
subset of R, we can still apply the above approximation.
It was shown in [36] that the kernel error can be bounded
b F be
by the quantization error. More specifically, let kK − Kk
the kernel error (k·kF is the Frobenius norm), and let
m
X
e=
kr i − µc(i) k2
i=1

be the quantization error, where c(i) is the minimizer of kr i −
µj k over j ∈ [1, m0 ]. Then the following bound holds [36].

SUBMITTED TO IEEE SIGNAL PROCESSING LETTERS

3

Proposition 1: Suppose there exists some L > 0 such that,
for w, x, y, z ∈ R,
2
2
2
κ(x, y) − κ(w, z) ≤ L k(x − w)k + k(y − z)k .
Then the approximation error can be bounded as
√
b F ≤ c1 e + c2 e,
kK − Kk

(13)

where the positive constants c1 and c2 do not depend on e. In
particular, (13) holds when κ is a Gaussian.
Proposition 1 suggests that we can reduce the kernel error
by making e small. However, e measures how well Θ is
represented by the landmark points. Following this observation, k-means clustering was used in [36] for determining the
landmarks. It was empirically shown in [36] that clustering
indeed results in smaller error over uniform sampling [31],
[35]. We will see that this is also true for our algorithm.
b It is
We arrive at a fast algorithm by replacing K by K.
clear from (7) that the resulting approximation is given by
ĝ(x) =

m0

1 X
αk v k ι(x) (ω ∗ hk )(x),
η̂(x)

Algorithm 1: Fast Kernel Filtering.
Input: f : Ω → Rn and p : Ω → Rρ , kernels ω and κ;
Parameter: Number of landmarks m0 ;
Output: Approximation in (14);
Form R in (2) and index map ι in (3);
{µk } ← partition R into m0 clusters using k-means;
Construct A and B in (9) and (11) using κ and p;
Compute the eigendecomposition of A in (10);
Initialize ζ : Ω → Rn and η : Ω → R with zeros;
for k = 1, . . . , m0 do
v k = (1/αk )B>wk ;
for x ∈ Ω do
dk (x) = v k (ι(x));
hk (x) = dk (x)f (x);
end

ζ ← ζ ⊕ αk · dk ⊗ (ω ∗ hk ) ;
η ← η ⊕ αk · dk ⊗ (ω ∗ dk ) ;
end
ĝ ← ζ η.

(14)

k=1

η̂(x) =

m0
X


αk v k ι(x) (ω ∗ dk )(x),

(15)

is guaranteed to increase with m0 (Figure 4 in the supplement).
Deriving a similar bound is difficult for [18]–[21].

k=1

where dk : Ω → R and hk : Ω → Rn are defined as dk (x) =
v k (ι(x)) and hk (x) = dk (x)f (x).
The computation of (14) and (15) involves (n + 1)m0 convolutions, since for each k ∈ [1, m0 ], there are n convolutions
in (14) and one in (15). The main point is that we have been
able to express the non-linear kernel filter using convolutions,
for which efficient algorithms are available. In particular, (14)
and (15) can be performed using O(1) operations (w.r.t. the
size of the spatial kernel), when ω is a box or Gaussian [37],
[38], [42]. The overall algorithm is described in Algorithm 1
(source code in [43]), where the symbols ⊕, ⊗ and are used
to denote pixelwise addition, multiplication, and division. The
complexity of k-means clustering and the eigendecomposition
of A are O(|Ω|m0 ρ) [44] and O(m0 3 ) [41]. On the other
hand, the complexity of the convolutions in (14) and (15)
is O(|Ω|m0 (n + ρ)), where |Ω| is the number of pixels.
Since the complexity  of the brute-force implementation is
O |Ω|(2S+1)d (n+ρ) [3], and convolutions are the dominant
operations in our algorithm, we obtain an effective speedup of
(2S + 1)d /m0 . This is significant as S is typically large [3].
We now comment on the filtering accuracy, namely, how
well is (1) approximated by (14). Intuitively, we expect the
b ≈ K. In fact, since the
approximation to be accurate if K
b F is controlled by the quantization error
difference kK − Kk
(Proposition 1), we have the following result.
Theorem 2: Suppose ω and κ are positive, and κ satisfies
the property in Proposition 1. Then
√
kĝ − gk∞ = max kĝ(x) − g(x)k ≤ C1 e + C2 e, (16)
x∈Ω

where C1 , C2 > 0 do not depend on e.
The main steps of the derivation are given in the supplement.
Theorem 2 is true for BLF and NLM, where κ is a Gaussian. A
practical implication of this result is that the filtering accuracy

IV. R ESULTS
We demonstrate the effectiveness of our algorithm for BLF
and NLM of high-dimensional images by comparing it with
state-of-the-art algorithms. Instead of standard NLM [2], we
have used PCA-NLM [45], where the denoising performance
of the former is improved by applying PCA on the collection
of patches. As for the dataset, we have used the color images
from [46] and the hyperspectral images from [47]. Experiments were performed using Matlab on a 3.4 GHz quad-core
machine with 32 GB memory. The spatial kernel ω for BLF is
a Gaussian (covariance σ 2 I and S = 3σ), while it is a box in
PCA-NLM. The range kernel κ is Gaussian (covariance θ2 I)
for both BLF and PCA-NLM. We have used the fast O(1)
algorithm in [37] when ω is a Gaussian, and the Matlab routine
“imfilter” when ω is a box. Note that we can also use other
fast Gaussian filters [38], [42] if higher accuracy is desired.
Color BLF. The state-of-the-art fast algorithms for color
BLF are Adaptive Manifolds (AM) [20], Permutohedral Lattice (PL) [19], and Global Color Sparseness (GCS) [21].
We have compared with them in Figure 2. The number
of manifolds is set automatically in AM, whereas we have
used 15 clusters in GCS and for the Nyström approximation.
Following [20], [21], we used PSNR to measure the error
between the brute-force and fast implementations. In Figure 2,
notice that while our PSNR marginally exceeds that of GCS,
it is however much better than PL and AM. Also notice the
significant acceleration over the brute-force implementation
obtained using our algorithm. We have also provided a table
comparing the different methods on the Kodak dataset [46] in
the supplement. The table shows that our method is better than
GCS and PL when θ > 40. As claimed in the introduction,
we can see from the table that clustering provides a significant
boost in filtering accuracy (10-20 dB) over uniform sampling.

SUBMITTED TO IEEE SIGNAL PROCESSING LETTERS

(a) Clean/Noisy (20 dB).

4

(b) (420, 30.2, 0.88).

(c) Ours (2.6, 30.1, 0.88).

(d) AM (5.8, 29.4, 0.87).

(e) BM3D (5, 33.01, 0.93).

Fig. 1. Gaussian denoising (noise level 25/255) of a color image using (b) PCA-NLM, its fast approximations (c) and (d), and (e) BM3D. The respective
(Timing (sec), PSNR (dB), SSIM) is shown in the caption.

(a) Input (256 × 256).

(b) Ours (108, 48.4).

(c) [21] (107, 46.5).

(a) Clean/Noisy.
(d) Brute-force, 4 min.

(e) [20], (212, 38.7).

(b) Ours (37 sec, 31.2 dB, 0.87).

(f) [19], (44.5).

Fig. 2. Visual comparison for fast BLF at σ = 5, θ = 50, and |Ω0 | =
(6σ+1)2 . The (timing, PSNR) are mentioned, where timing is in milliseconds
and PSNR is in dB. Timing is not mentioned for [19] which is implemented in
C++. The breakup of timing for the proposed method is as follows: clustering
(11 ms), eigendecomposition (1 ms), and convolutions (96 ms). Note that the
overall timing is dominated by the convolutions.

Color NLM. AM is the state-of-the-art fast algorithm for
color NLM (and PCA-NLM). In NLM, ρ = 3(2r + 1)2 , where
r is the patch radius [2]. On the other hand, ρ is reduced to
a smaller value in PCA-NLM using PCA. Following [45], we
set θ to be three times the noise level for all experiments.
Denoising results are shown in Figure 1, where S = 10 and
r = 3. For (b), (c), and (d), PCA was used to reduce the
range dimension from 3 × 72 to 25. We used 31 clusters
(resp. manifolds) for the Nyström approximation (resp. AM).
Following [50], we measured the denoising performance using
PSNR and SSIM (between the clean and denoised images).
Note that we are superior to AM both in terms of accuracy
and timing. Importantly, our PSNR is close to PCA-NLM (the
method being approximated), but we are about 160× faster. In
comparison with BM3D [51], our PSNR is 3 dB less. However,
our timing is about half that of BM3D, since our complexity is
much less than that of BM3D. Additional visual comparisons
and accuracy analysis is provided in the supplement.
Hyperspectral BLF. Finally, we present a denoising result
for a hyperspectral image of size (610 × 340) × 103 bands
using BLF (σ = 3, θ = 100). We have also compared with
state-of-the-art methods for hyperspectral denoising [48], [49],
whose parameters have been tuned accordingly. The results are
shown in Figure 3. We have used m0 = 32 landmarks for the

(c) [48] (3 min, 30.54 dB, 0.89). (d) [49] (20 min, 30.09 dB, 0.74).
Fig. 3. Hyperspectral denoising of a natural image corrupted with Gaussian
noise of level 25/255. (Timing, PSNR, SSIM) are shown for all methods.

Nyström approximation. As a standard practice, the PSNR and
SSIM values are averaged over the spectral bands. Notice that
our method can restore details better, which results in higher
PSNR/SSIM values. In particular, the color is not satisfactorily
restored in [48] and grains can be seen in [49]. Being a oneshot method, we are much faster than [48], [49].
V. C ONCLUSION
We showed that fast bilateral and nonlocal means filtering of
high-dimensional images can be performed using the Nyström
approximation. The proposed algorithm can significantly accelerate the brute-force implementation of these filters, without
compromising the visual quality. In particular, our algorithm
is often competitive with state-of-the-art fast algorithms, and
comes with provable guarantee on the filtering accuracy.

SUBMITTED TO IEEE SIGNAL PROCESSING LETTERS

R EFERENCES
[1] C. Tomasi and R. Manduchi, “Bilateral filtering for gray and color
images,” Proc. IEEE International Conference on Computer Vision, pp.
839–846, 1998.
[2] A. Buades, B. Coll, and J.-M. Morel, “A non-local algorithm for image
denoising,” Proc. IEEE Conference on Computer Vision and Pattern
Recognition, vol. 2, pp. 60–65, 2005.
[3] S. Paris, P. Kornprobst, J. Tumblin, and F. Durand, “Bilateral filtering:
Theory and Applications,” Foundations and Trends R in Computer
Graphics and Vision, vol. 4, no. 1, pp. 1–73, 2009.
[4] P. Milanfar, “A tour of modern image filtering: New insights and methods, both practical and theoretical,” IEEE Signal Processing Magazine,
vol. 30, no. 1, pp. 106–128, 2013.
[5] F. Durand and J. Dorsey, “Fast bilateral filtering for the display of highdynamic-range images,” ACM Transactions on Graphics, vol. 21, no. 3,
pp. 257–266, 2002.
[6] S. Paris and F. Durand, “A fast approximation of the bilateral filter using
a signal processing approach,” Proc. European Conference on Computer
Vision, pp. 568–580, 2006.
[7] J. Chen, S. Paris, and F. Durand, “Real-time edge-aware image processing with the bilateral grid,” ACM Transactions on Graphics, vol. 26,
no. 3, p. 103, 2007.
[8] F. Porikli, “Constant time O(1) bilateral filtering,” Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–8, 2008.
[9] Q. Yang, K. H. Tan, and N. Ahuja, “Real-time O(1) bilateral filtering,”
Proc. IEEE Conference on Computer Vision and Pattern Recognition,
pp. 557–564, 2009.
[10] K. N. Chaudhury, D. Sage, and M. Unser, “Fast O(1) bilateral filtering
using trigonometric range kernels,” IEEE Transactions on Image Processing, vol. 20, no. 12, pp. 3376–3382, 2011.
[11] K. Sugimoto and S. I. Kamata, “Compressive bilateral filtering,” IEEE
Transactions on Image Processing, vol. 24, no. 11, pp. 3357–3369, 2015.
[12] Q. Yang, N. Ahuja, and K.-H. Tan, “Constant time median and bilateral
filtering,” International Journal of Computer Vision, vol. 112, no. 3, pp.
307–318, 2015.
[13] K. N. Chaudhury and S. D. Dabhade, “Fast and provably accurate
bilateral filtering,” IEEE Transactions on Image Processing, vol. 25,
no. 6, pp. 2519–2528, 2016.
[14] S. Ghosh and K. N. Chaudhury, “On fast bilateral filtering using Fourier
kernels,” IEEE Signal Processing Letters, vol. 23, no. 5, pp. 570–573,
2016.
[15] K. Sugimoto, T. Breckon, and S. I. Kamata, “Constant-time bilateral filter using spectral decomposition,” Proc. IEEE International Conference
on Image Processing, pp. 3319–3323, 2016.
[16] P. Nair, A. Popli, and K. N. Chaudhury, “A fast approximation of the
bilateral filter using the discrete Fourier transform,” Image Processing
On Line, vol. 7, pp. 115–130, 2017.
[17] G. Papari, N. Idowu, and T. Varslot, “Fast bilateral filtering for denoising
large 3D images,” IEEE Transactions on Image Processing, vol. 26,
no. 1, pp. 251–261, 2017.
[18] A. Adams, N. Gelfand, J. Dolson, and M. Levoy, “Gaussian KD-trees for
fast high-dimensional filtering,” ACM Transactions on Graphics, vol. 28,
no. 3, p. 21, 2009.
[19] A. Adams, J. Baek, and M. A. Davis, “Fast high-dimensional filtering
using the permutohedral lattice,” Computer Graphics Forum, vol. 29,
no. 2, pp. 753–762, 2010.
[20] E. S. Gastal and M. M. Oliveira, “Adaptive manifolds for real-time highdimensional filtering,” ACM Transactions on Graphics, vol. 31, no. 4,
pp. 33:1–33:13, 2012.
[21] M. G. Mozerov and J. Van De Weijer, “Global color sparseness and a
local statistics prior for fast bilateral filtering,” IEEE Transactions on
Image Processing, vol. 24, no. 12, pp. 5842–5853, 2015.
[22] P. Nair and K. N. Chaudhury, “Fast high-dimensional filtering using
clustering,” Proc. IEEE International Conference on Image Processing,
pp. 240–244, 2017.
[23] M. Mahmoudi and G. Sapiro, “Fast image and video denoising via
nonlocal means of similar neighborhoods,” IEEE Signal Processing
Letters, vol. 12, no. 12, pp. 839–842, 2005.
[24] J. Wang, Y. Guo, Y. Ying, Y. Liu, and Q. Peng, “Fast non-local algorithm
for image denoising,” Proc. IEEE International Conference on Image
Processing, pp. 1429–1432, 2006.
[25] J. Darbon, A. Cunha, T. F. Chan, S. Osher, and G. J. Jensen, “Fast
nonlocal filtering applied to electron cryomicroscopy,” Proc. IEEE
International Symposium on Biomedical Imaging, pp. 1331–1334, 2008.
[26] S. Ghosh and K. N. Chaudhury, “Fast separable non-local means,”
Journal of Electronic Imaging, vol. 25, no. 2, p. 023026, 2016.

5

[27] ——, “Fast bilateral filtering of vector-valued images,” Proc. IEEE
International Conference on Image Processing, pp. 1823–1827, 2016.
[28] W.-C. Tu, Y.-A. Lai, and S.-Y. Chien, “Constant time bilateral filtering
for color images,” Proc. IEEE International Conference on Image
Processing, pp. 3309–3313, 2016.
[29] K. Sugimoto, N. Fukushima, and S. I. Kamata, “Fast bilateral filter
for multichannel images via soft-assignment coding,” Proc. Signal and
Information Processing Association Annual Summit and Conference, pp.
1–4, 2016.
[30] C. Karam and K. Hirakawa, “Monte-Carlo acceleration of bilateral filter
and non-local means,” IEEE Transactions on Image Processing, vol. 27,
no. 3, pp. 1462–1474, 2018.
[31] H. Talebi and P. Milanfar, “Global image denoising,” IEEE Transactions
on Image Processing, vol. 23, no. 2, pp. 755–768, 2014.
[32] ——, “Nonlocal image editing,” IEEE Transactions on Image Processing, vol. 23, no. 10, pp. 4460–4473, 2014.
[33] ——, “Asymptotic performance of global denoising,” SIAM Journal on
Imaging Sciences, vol. 9, no. 2, pp. 665–683, 2016.
[34] C. Williams and M. Seeger, “Using the Nyström method to speed up
kernel machines,” Proc. Neural Information Processing Systems, pp.
682–688, 2001.
[35] C. Fowlkes, S. Belongie, F. Chung, and J. Malik, “Spectral grouping
using the Nyström method,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 26, no. 2, pp. 214–225, 2004.
[36] K. Zhang, I. W. Tsang, and J. T. Kwok, “Improved Nyström low-rank
approximation and error analysis,” Proc. International Conference on
Machine Learning, pp. 1232–1239, 2008.
[37] I. T. Young and L. J. Van Vliet, “Recursive implementation of the
gaussian filter,” Signal Processing, vol. 44, no. 2, pp. 139–151, 1995.
[38] K. Sugimoto and S. I. Kamata, “Fast gaussian filter with second-order
shift property of DCT-5,” Proc. IEEE International Conference on Image
Processing, pp. 514–518, 2013.
[39] E. J. Nyström, “Über die praktische auflösung von integralgleichungen
mit anwendungen auf randwertaufgaben,” Acta Mathematica, vol. 54,
no. 1, pp. 185–204, 1930.
[40] C. T. Baker, “The numerical treatment of integral equations,” Clarendon
Press, 1977.
[41] V. Y. Pan and Z. Q. Chen, “The complexity of the matrix eigenproblem,”
Proc. ACM Symposium on Theory of Computing, pp. 507–516, 1999.
[42] R. Deriche, “Recursively implementating the gaussian and its derivatives,” Research Report, no. RR-1893, 1993.
[43] “Matlab code.” [Online]. Available: https://github.com/pravin1390/
FastHDNystrom
[44] P.-N. Tan, M. Steinbach, and V. Kumar, Introduction to Data Mining.
Addison-Wesley Longman Publishing Co., Inc., 2005.
[45] T. Tasdizen, “Principal neighborhood dictionaries for nonlocal means
image denoising,” IEEE Transactions on Image Processing, vol. 18,
no. 12, pp. 2649–2660, 2009.
[46] “BM3D image database.” [Online]. Available: http://r0k.us/graphics/
kodak/index.html
[47] “Hyperspectral image database.” [Online]. Available: http://lesun.
weebly.com/hyperspectral-data-set.html
[48] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.
[49] Y.-Q. Zhao and J. Yang, “Hyperspectral image denoising via sparse representation and low-rank constraint,” IEEE Transactions on Geoscience
and Remote Sensing, vol. 53, no. 1, pp. 296–308, 2015.
[50] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image
quality assessment: from error visibility to structural similarity,” IEEE
Transactions on Image Processing, vol. 13, no. 4, pp. 600–612, 2004.
[51] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, “Image denoising
with block-matching and 3D filtering,” Proc. SPIE Electronic Imaging,
vol. 6064, no. 30, pp. 1–12, 2006.

