Using Interlinear Glosses as Pivot in Low-Resource
Multilingual Machine Translation
Zhong Zhou

Lori Levin

David R. Mortensen

Alex Waibel

Carnegie Mellon University

Carnegie Mellon University

Carnegie Mellon University

Carnegie Mellon University

zhongzhou@cmu.edu

lsl@cs.cmu.edu

dmortens@cs.cmu.edu Karlsruhe Institute of Technology
alex@waibel.com

arXiv:1911.02709v3 [cs.CL] 3 Mar 2020

Abstract
We demonstrate a new approach to Neural Machine Translation (NMT) for lowresource languages using a ubiquitous linguistic resource, Interlinear Glossed Text
(IGT). IGT represents a non-English sentence as a sequence of English lemmas
and morpheme labels. As such, it can
serve as a pivot or interlingua for NMT.
Our contribution is four-fold. Firstly, we
pool IGT for 1,497 languages in ODIN
(54,545 glosses) and 70,918 glosses in Arapaho and train a gloss-to-target NMT system from IGT to English, with a BLEU
score of 25.94. We introduce a multilingual NMT model that tags all glossed
text with gloss-source language tags and
train a universal system with shared attention across 1,497 languages. Secondly, we
use the IGT gloss-to-target translation as
a key step in an English-Turkish MT system trained on only 865 lines from ODIN.
Thirdly, we we present five metrics for evaluating extremely low-resource translation
when BLEU is no longer sufficient and evaluate the Turkish low-resource system using
BLEU and also using accuracy of matching nouns, verbs, agreement, tense, and
spurious repetition, showing large improvements.

1

Introduction

Machine polyglotism, training a universal NMT
system with a shared attention through implicit parameter sharing, is very helpful lowc 2020 The authors. This article is licensed under a Creative
Commons 3.0 licence, no derivative works, attribution, CCBY-ND.

Figure 1: Multilingual NMT.

Figure 2: Using interlinear glosses as pivot to translate in
multilingual NMT in Hmong, Chinese, and German.

resource settings (Firat et al.(2016)Firat, Cho,
and Bengio; Zoph and Knight(2016); Dong
et al.(2015)Dong, Wu, He, Yu, and Wang; Gillick
et al.(2016)Gillick, Brunk, Vinyals, and Subramanya; Al-Rfou et al.(2013)Al-Rfou, Perozzi,
and Skiena; Zhou et al.(2018)Zhou, Sperber, and
Waibel; Tsvetkov et al.(2016)Tsvetkov, Sitaram,
Faruqui, Lample, Littell, Mortensen, Black, Levin,
and Dyer). However, there is still a large disparity
between translation quality in high-resource and
low-resource settings, even when the model is welltuned (Koehn and Knowles(2017); Sennrich and
Zhang(2019); Nordhoff et al.(2013)Nordhoff, Hammarström, Forkel, and Haspelmath). Indeed, there
is room for creativity in low-resource scenarios.
Morphological analysis is useful in reducing
word sparsity in low-resource languages (Habash
and Sadat(2006); Lee(2004); Hajič(2000)). Toward that end, we leverage a linguistic resource,
Interlinear Glossed Text (IGT) (Lehmann and
Croft(2015)) as shown in Table 2 (Samardzic
et al.(2015)Samardzic, Schikowski, and Stoll;

Data

Example

Source language (German)
Interlinear Gloss w/ target-lemma
Target language (English)

Ich sah ihm den Film gefallen.
I saw he.DAT the film.ACC like.
I saw him like the film.

Source language (Hmong)
Interlinear gloss w/ target-lemma
Target language (English)

Nwg yeej qhuas nwg.
3SG always praise 3SG.
He always praises himself.

Source language (Arapaho)
Interlinear gloss w/ target-lemma

Niine’etii3i’ teesiihi’ coo’oteyou’uHohootino’ nenee3i’ neeyeicii.
live(at)-3PL on/over-ADV IC.hill(y)-0.PLtree-NA.PL IC.it is-3PL
timber.
Trees make the woods.

Target language (English)

Table 1: Examples of interlinear glosses in different source languages.

Moeller and Hulden(2018)). We propose to use
interlinear gloss as a pivot to address the harder
problem of morphological complexity and source of
data sparsity in multilingual NMT. We combine the
benefits of both multilingual NMT and linguistic information through the use of interlinear glosses as a
pivot representation. Our contribution is four-fold.
1. We present our multilingual model using a
single attention in translating from interlinear
glosses into a target language. Our best multilingual NMT result achieves a BLEU score
of 25.94, +5.65 above a single-source singletarget NMT baseline.
2. We present two linguistic datasets that we
normalized, cleaned and filtered: the cleaned
ODIN dataset includes 54,545 lines of IGT in
1,496 languages (Lewis and Xia(2010); Xia
et al.(2014)Xia, Lewis, Goodman, Crowgey,
and Bender), and the cleaned Arapaho dataset
includes 70,918 lines of IGT (Cowell and
O’Gorman(2012); Wagner et al.(2016)Wagner,
Cowell, and Hwang).
3. We present a three-step approach for extremely low-resource translation. We demonstrate it by training only on 865 lines of data.
(a) We use a morphological analyzer to automatically generate interlinear glosses
with source lemma.
(b) We translate the source lemma into target lemma in through alignments trained
from parallel data.
(c) We translate from interlinear glosses with
target lemma to target language by using
the gloss-to-target multilingual NMT developed in 1 presented above.
4. We present five metrics for evaluating ex-

tremely low-resource translation when BLEU
no longer suffices. Our system using interlinear glosses achieves an improvement of
+44.44% in Noun-Verb Agreements, raising
fluency.
We present our cleaned data followed by gloss-totarget models and our three-step Turkish-English
NMT in Section 3 and 4. We evaluate in Section 5.

2
2.1

Related Works
Multilingual Neural Machine Translation

Multilingual NMT’s objective is to translate
from any of N input languages to any of M
output languages (Firat et al.(2016)Firat, Cho,
and Bengio; Zoph and Knight(2016); Dong
et al.(2015)Dong, Wu, He, Yu, and Wang; Gillick
et al.(2016)Gillick, Brunk, Vinyals, and Subramanya; Al-Rfou et al.(2013)Al-Rfou, Perozzi, and
Skiena; Tsvetkov et al.(2016)Tsvetkov, Sitaram,
Faruqui, Lample, Littell, Mortensen, Black, Levin,
and Dyer). Many multilingual NMT systems work
on a universal model with a shared attention mechanism with Byte-Pair Encoding (BPE) (Johnson
et al.(2017)Johnson, Schuster, Le, Krikun, Wu,
Chen, Thorat, Viégas, Wattenberg, Corrado et al.;
Ha et al.(2016)Ha, Niehues, and Waibel; Zhou
et al.(2018)Zhou, Sperber, and Waibel). Its simplicity and implicit parameter sharing helps with
low-resource translation and zero-shot translation
(Johnson et al.(2017)Johnson, Schuster, Le, Krikun,
Wu, Chen, Thorat, Viégas, Wattenberg, Corrado
et al.; Firat et al.(2016)Firat, Cho, and Bengio).
2.2

Morpheme-Level Machine Translation

To
build
robustness
et al.(2018)Chaudhary, Zhou,

(Chaudhary
Levin, Neu-

Data

Example

1.
2.
3.
4.

Source language (Turkish)
Interlinear gloss with source-lemma
Interlinear gloss with target-lemma
Target language (English)

Kadin dans ediyor.
Kadin.NOM dance ediyor-AOR.3.SG.
Woman.NOM dance do-AOR.3.SG.
The woman dances.

1.
2.
3.
4.

Source language (Turkish)
Interlinear gloss with source-lemma
Interlinear gloss with target-lemma
Target language (English)

Adam kadin-i gör-dü.
Adam.NOM kadin-ACC gör-AOR.3.SG.
Man.NOM woman-ACC see-PST.3.SG.
The man saw the woman.

Table 2: Examples of the translation sequence using interlinear glosses.

Notation Meaning in translation sequence
1
2
3
4

Source language (Turkish) text
Interlinear gloss with source-lemma
Interlinear gloss with target-lemma
Target language (English) text
Table 3: Notation used in the translation sequence.

big, Mortensen, and Carbonell; Cotterell and
Schütze(2015); Wu et al.(2016)Wu, Schuster,
Chen, Le, Norouzi, Macherey, Krikun, Cao, Gao,
Macherey et al.), researchers work on characterlevel, byte-level (Gillick et al.(2016)Gillick,
Brunk, Vinyals, and Subramanya;
Ling
et al.(2015)Ling, Trancoso, Dyer, and Black;
Chung et al.(2016)Chung, Cho, and Bengio;
Tiedemann(2012)), and BPE-level (Sennrich
et al.(2016)Sennrich, Haddow, and Birch; Burlot
et al.(2017)Burlot, Garcia-Martinez, Barrault,
Bougares, and Yvon) translation. Morpheme-level
translation allows words to share embedding while
allowing variation in meanings (Cotterell and
Schütze(2015); Chaudhary et al.(2018)Chaudhary,
Zhou, Levin, Neubig, Mortensen, and Carbonell;
Renduchintala et al.(2019)Renduchintala, Shapiro,
Duh, and Koehn; Passban et al.(2018)Passban, Liu,
and Way; Dalvi et al.(2017)Dalvi, Durrani, Sajjad,
Belinkov, and Vogel), shrinks the vocabulary
size introduces smoothing (Goldwater and McClosky(2005)), and makes fine-grained correction
(Stroppa et al.(2006)Stroppa, Groves, Way, and
Sarasola; Matthews et al.(2018)Matthews, Neubig,
and Dyer).
2.3

Factored Machine Translation

Factored models translate a composition of
annotations including word, lemma, part-of-speech,
morphology, and word class into the target

language (Koehn and Hoang(2007); Yeniterzi and
Oflazer(2010)). In the era of NMT, morphological
information and grammatical decomposition that
are produced by a morphological analyzer are
employed (García-Martínez et al.(2016)GarcíaMartínez, Barrault, and Bougares; Burlot
et al.(2017)Burlot, Garcia-Martinez, Barrault,
Bougares, and Yvon; Hokamp(2017)).
2.4

Interlinear Gloss Generation

Interlinear gloss is a linguistic representation of
morphosyntactic categories and cross-linguistic lexical relations (Samardzic et al.(2015)Samardzic,
Schikowski, and Stoll; Moeller and Hulden(2018)).
IGT is used in linguistic publications and field notes
to communicate technical facts about languages
that the reader might not speak, or to convey a particular linguistic analysis to the reader. Typically,
there are three lines to an IGT. The first line consists of text segments in an object language, which
we call the source language in this paper. The
third line is a fluent translation in the metalanguage,
which we call the target language. In our work, the
target language (metalanguage) is always English.
The source (object) languages are the 1,496 languages of the ODIN (Lewis and Xia(2010); Xia
et al.(2014)Xia, Lewis, Goodman, Crowgey, and
Bender) database plus Arapaho, for which a large
collection of field notes has been published (Cowell and O’Gorman(2012)). In the middle (interlinear) line of an IGT, each object language word is
represented as an English (metalanguage) lemma
and labels for the non-English morphemes. For
our work, we add a fourth line: a source-language
lemma with morpheme labels. To illustrate with an
example from Arapaho, an endangered language
that is spoken in the United States by less than
200 people. (Cowell and O’Gorman(2012)), “ho-

Normalized Gloss

Meaning of the Abbreviations

Glosses in the Turkish Odin Data

NMLZ
PRS
PST

Nominalizer
Present tense
Past tense

ABL
ADV
RPRT

Ablative
Adverb(ial)
Reported Past tense

NML, NOMZ, FNom, NOML
PRES, PR, pres, Pres, PRESENT
PA, Pst, PST, Past, pst, PAST, PT, PTS, REPPAST, PST1S, past
Abl, Abli, abl, ABL
ADVL, Adv
ReportedPast, REPPAST

Table 4: Examples of the normalization mapping created for the Turkish ODIN data.

Tags

Meaning of the Abbreviations

Sets of Normalized Glosses Included

P1pl
A1sg
Reflex
NarrPart
AorPart
PresPart

1st person plural possessive
1st person singular
Reflexive Pronoun
Evidential participle
Aorist participle
Present participle

1, PL, POSS
1, SG
REFL
EVID, PTCP
AOR, PTCP
PRS, PTCP

Table 5: Examples of the normalization mapping created for the outputs from the Turkish morphological analyzer.

hoot nii3eihit" means “a tree is nice". The interlinear gloss with source-language lemmas is “hohoot
nii3eihit-3.S" and the interlinear gloss with targetlanguage lemmas is “Tree good-3.S".
A benefit of IGT is that there is a one-to-one mapping between each segment of the source sentence
to the gloss (Samardzic et al.(2015)Samardzic,
Schikowski, and Stoll). Researchers have tried to
generate interlinear glosses automatically by using supervised POS tagging, word disambiguation
and a dictionary (Samardzic et al.(2015)Samardzic,
Schikowski, and Stoll), and using conditional
random fields and active learning (Moeller and
Hulden(2018)).

3
3.1

Data
Newly Cleaned Datasets

We present two linguistic datasets that we have
cleaned and partially normalized: the partially normalized ODIN dataset that includes 54,545 lines
of IGT in 1,496 languages (Lewis and Xia(2010);
Xia et al.(2014)Xia, Lewis, Goodman, Crowgey,
and Bender), and the cleaned Arapaho dataset
that includes 70,918 lines of IGT (Cowell and
O’Gorman(2012); Wagner et al.(2016)Wagner,
Cowell, and Hwang).
ODIN is a unique multilingual database of IGT
that was scraped from the Web. The IGTs in ODIN
come from many different publications with differ-

ent standards for morpheme labels. A consequence
of the diversity is that morpheme labels in ODIN
are not standardized. For example, “singular” may
be "S", "SG", or "SING", and when combined with
"3" for third person, there may or may not be a delimiter, resulting such diverse labels as "3S", "3.s",
"3SG", and "3.sing".
In order to reduce the sparsity and diversity of
morpheme labels, we normalized them according
to the Leizig conventions (Lehmann(1982);
Croft(2002)) (preferred), and the Unimorph
conventions (Sylak-Glassman(2016);
Kirov
et al.(2016)Kirov, Sylak-Glassman, Que, and
Yarowsky; Sylak-Glassman et al.(2015)SylakGlassman, Kirov, Yarowsky, and Que) for labels
not covered by the Leipzig conventions. For the
work presented here, we normalized only those
morpheme labels that were found in ODIN’s 1,081
lines of Turkish IGT. However, we normalized
those morpheme labels throughout the entire ODIN
database. We show a few normalized examples in
Table 6.
The Arapaho dataset was originally created in
ToolBox (Buseman(2020)). Cleaning of this dataset
consisted of running an in-house script that found
lingering formatting errors. Some of the errors were
corrected by Andy Cowell, and others remain in
a default format created by our script and will be
corrected in the future.

Interlinear Gloss w/ Target-lemma

Example

Before normalization
After normalization

Ahmet self-3.sg-ACC very admire-Progr.-Rep.Past.
Ahmet self-3.SG-ACC very admire-PROG-Rep.PST.

Before normalization
After normalization

Woman.NOM dance do-AOR.3SG.
Woman.NOM dance do-AOR.SG.3.

Before normalization
After normalization

Man.NOM woman-ACC see-PAST.3SG.
Man.NOM woman-ACC see-PST.SG.3.

Table 6: Examples of the normalization of glosses from the Turkish ODIN data.

Analyzer Outputs

Example

Before normalization
After normalization

Kadi+A3sg+Pnon+Nom dans+A3sg+Pnon+Nom et+Prog1+A3sg.
Kadin.3.SG.NPOSS.NOM dans.3.SG.NPOSS.NOM ediyor-PROG.3.SG.

Before normalization
After normalization

Adam+A3pl+Pnon+Nom kadi+A3sg+Pnon+Acc gör+Past+A3sg.
Adam.3.SG.NPOSS.NOM kadi.3.SG.NPOSS.ACC gör-PST.3.SG.

Before normalization

Ali+A3sg+Pnon+Nom
hakkinda+A3sg+P3sg+Loc
met+Prop+A3sg+Pnon+Nom ne düünüyor+A3sg+Pnon+Nom?
Ali.3.SG.NPOSS.NOM
hakkinda.3.SG.POSS.LOC
met.3.SG.NPOSS.NOM ne düünüyor.3.SG.NPOSS.NOM?

After normalization

AhAh-

Table 7: Examples of the normalization process for the output of the Turkish morphological analyzer.

3.2

Data Preparation: Turkish-English NMT

The 1,081 Turkish-English glosses are split into
training, validation, and test sets with the ratio of
0.8,0.1,0.1. Our training data only contains 865
lines. We choose Turkish because it is morphologically rich (Matthews et al.(2018)Matthews, Neubig, and Dyer; Botha and Blunsom(2014)), agglutinative, and has words that cannot be translated as a single word in other languages (Clifton
and Sarkar(2011); El-Kahlout and Oflazer(2006);
Bisazza and Federico(2009)).

4

Models

For convenience, we use 1, 2, 3, 4 to denote each
line of the translation sequence as shown in Table 3.
4.1

An Extension to Multilingual NMT:
Gloss-to-Target Translation

In Figure 1, we show a simple setup for multilingual translation on the top. Each source sentence
is tagged with the source and language tags and is
added to the training data. In Figure 2, we also show
our model of gloss-to-target translation, which introduces a new extension to multilingual NMT.
In our gloss-to-target translation, we train a multilingual NMT system on 57,608 lines across 1,497
languages. Our source sentences are the interlinear

glosses with target-lemma (3), and our target sentences are the target translations (4). We tag each
glossed text with the gloss-source language tag,
for example, we tag Hmong glosses with “blu".
As such, our training data on the source side contains “blu 1SG be_thirsty water” (Hmong), “cmn
1SG be_thirsty” (Chinese) and “deu 1SG.ACC
be_thirsty” (German), and our training data on the
target side is “I am thirsty” in English. We proceed
to train using a unified attention mechanism. Even
though the gloss only contains the target-lemma,
our system is informed of the source language.
In our multilingual NMT translation, we use a
minibatch size of 64, a dropout rate of 0.3, 4 RNN
layers of size 1000, a word vector size of 600, number of epochs of 13, a learning rate of 0.8 that decays at the rate of 0.7 if the validation score is not
improving or it is past epoch 9. Our code is built
on OpenNMT (Klein et al.(2017)Klein, Kim, Deng,
Senellart, and Rush) and we evaluate our models
using BLEU scores (Papineni et al.(2002)Papineni,
Roukos, Ward, and Zhu), and qualitative evaluation.
We train a baseline attentional NMT model without adding the source language tags, as well as other
system variation, some of which include Arapaho
and ODIN in Section 5.

Analyzer Outputs

Example

Before normalization
After normalization
After using a dictionary
Reference in ODIN

Kadi+A3sg+Pnon+Nom dans+A3sg+Pnon+Nom et+Prog1+A3sg.
Kadin.3.SG.NPOSS.NOM dans.3.SG.NPOSS.NOM ediyor-PROG.3.SG.
Woman.3.SG.NPOSS.NOM dance.3.SG.NPOSS.NOM be-PROG.3.SG.
Woman.NOM dance do-AOR.3.SG.

Before normalization
After normalization
After using a dictionary
Reference in ODIN

Adam+A3pl+Pnon+Nom kadi+A3sg+Pnon+Acc gör+Past+A3sg.
Adam.3.SG.NPOSS.NOM kadi.3.SG.NPOSS.ACC gör-PST.3.SG.
Man.3.SG.NPOSS.NOM woman.3.SG.NPOSS.ACC see-PST.3.SG.
Man.NOM woman-ACC see-PST.3.SG.

Table 8: Examples of interlinear gloss generation (1→2→3) from the output from the Turkish morphological analyzer. Notation
of the translation sequence follows from Table 3.

Interlinear Gloss w/ Target-lemma

NMT Result in Target Lan- Reference Target Sentence
guage

Peter and Mary that/those not came3SG/3PL
PERF.AV-buy NOM-man ERG-fish
DAT-store
AGR-do-make-ASP that waterpot AGRfall-ASP

Peter and Mary , he didn’t
come.
The man bought fish at the
store.
The girl made that waterpot
fall.

Peter and Mary, they didn’t
come.
The man bought fish at thestore’.
The girl made the waterpot fall.

Table 9: Examples of Gloss-to-Target (3→4 in Table 3) NMT translation results. The source is the interlinear gloss with
target(English)-lemma and the target is the fluent English. Notation of the translation sequence follows from Table 3. Note that
the ODIN dataset is not clean, and the second example above is a case where two words are concatenated together without space
followed by a unnecessary punctuation symbol. This example serves to show that our NMT output automatically correct typos in
producing fluent target(English) sentence.

4.2

Case Study: Turkish-English Translation

We use our gloss-to-target model above as the third
step in our Turkish-English Translation pipeline.
We present a case study of Turkish-English translation using 865 lines of training data.
4.2.1 1→2: Generation of Interlinear Gloss
with Source-Lemma
We use a morphological analyzer (Oflazer(1994))
to generate morphological tags and a root for each
word token in the source text. After normalizing
the morpheme labels, we produce an interlinear
gloss with source-lemma. In Table 8, we show the
interlinear gloss with source-lemma in every second
line.
4.2.2 2→3: Generation of Interlinear Gloss
with Target-Lemma
We use a dictionary produced by aligning parallel
corpora to construct interlinear gloss with target
English tokens from source Turkish tokens (Dyer
et al.(2013)Dyer, Chahuneau, and Smith). In order
to produce a higher quality dictionary, instead of
choosing the 865 lines training data to construct

alignments, we use an additional parallel corpus
with 57,608 lines. This data, which is only used
to created a dictionary, would be unnecessary if
a high-quality dictionary already existed. Using
the dictionary, we generate interlinear glosses with
target English tokens as shown by every third line
in Table 8.
4.2.3 3→4: Training NMT system for
Gloss-to-Target Translation
We use our multilingual gloss-to-target NMT
model trained above to translate from glosses with
target-lemma (3) into the target language (4).

5
5.1

Results
Multilingual Gloss-to-Target NMT

In the description of our results, we will use a number of labels. In Table 10, we use Turkish to denote
the baseline translation system (taking all the interlinear glosses with target lemma (3) in our 865
lines of Turkish data and their target translations (4)
and training a single-source single-target translation system). We use ODIN to denote the baseline

Model
Type

Turkish
single

ODIN
single

ODIN_multi
multilingual

Arapaho
single

ODIN+Arapaho
single

ODIN+Arapaho_multi
multilingual

Data
BLEU

865
0.0

54,545
20.29

54,545
23.05

70,918
22.68

125,463
25.94

125,463
25.85

Table 10: BLEU scores in gloss-to-target translation using ODIN and the Arapaho dataset. If "Type" is "single", it is trained
using a single-source single-target NMT; if it is "multilingual", it is trained using a multilingual NMT.
Model
Translation Sequence

Baseline1
1→4

Baseline2
1→4*

IGT_src
1→2→4

IGT_tgt
1→2→3→4

Data used
Supplemental resources

865
-

58473
-

865
Analyzer

865
Analyzer, alignments

Noun-match accuracy
Verb-match accuracy
Subject-verb agreement accuracy
Tense-match accuracy
Non-repetition metric
4-gram BLEU
1-gram BLEU

15.96
4.63
45.37
16.67
89.24
3.05
21.30

5.07
7.87
64.81
40.74
96.95
2.39
12.3

20.50
9.72
37.96
22.22
92.46
5.08
28.0

(+4.54)
(+5.09)
(-7.41)
(+5.55)
(+3.22)
(+2.03)
(+6.70)

36.11
20.37
89.81
43.52
99.27
4.74
21.50

(+20.15)
(+15.74)
(+44.44)
(+26.85)
(+10.03)
(+1.66)
(+0.20)

Table 11: Evaluation of different translation sequences with notations from Table 3. Baseline2 uses additional 57,608 lines of
Turkish-English parallel data.

translation system in which we take all the interlinear glosses with target lemma (3) in ODIN and
their target translations (4) and train a single-source
single-target translation system. We also use Arapaho to denote the baseline translation system of
taking all the interlinear glosses with target lemma
(3) in Arapaho and their target translations (4) and
train a single-source single-target translation system. We use ODIN+Arapaho to denote the singlesource single-target NMT model trained on both the
ODIN and Arapaho datasets. We use ODIN_multi
to denote the multilingual NMT model trained for
gloss-to-target translation by tagging each gloss
with its source language labels, for example “blu”
for Hmong. We use ODIN+Arapaho_multi to denote the multilingual NMT model produced by tagging each gloss with its source language labels combining both the ODIN and Arapaho datasets.

set. After adding the Arapaho data to ODIN,
our ODIN+Arapaho raises the BLEU score to
25.94, an increase of +5.65 over that of ODIN. After adding the Arapaho data to ODIN_multi, our
ODIN+Arapaho_multi has a BLEU score of 25.85,
a increase of +2.80 over that of ODIN_multi. However, the BLEU score of ODIN+Arapaho_multi
is lower than that of ODIN+Arapaho by -0.09,
although it is a small difference. We think a
contributing factor to the similar performance of
ODIN+Arapaho and ODIN+Arapaho_multi is because the Arapaho dataset, though it is relatively
large, is monolingual. Even though we train in
a multilingual fashion in ODIN+Arapaho_multi,
most of the training data is skewed towards the
monolingual Arapaho data, therefore its performance is similar to that of ODIN+Arapaho.

We see that ODIN_multi raises the BLEU score
to 23.05, an increase of +2.76 compared to the
baseline ODIN as shown in Table 10 and Table 9.
We notice that the translation from the interlinear
gloss to target language is very good, with high
accuracy in named entities translated as well as
the verbs. In our second example, our model is
able to predict “store” when there is a typo in the
gold translation. In other words, our translation
sometimes beats the gold translation.

We use a few labels in the descriptions of our experiments. In Table 11 and Table 12, Baseline1 denotes
an attentional NMT system that trains on the 865
lines of Turkish-English parallel data without using
any information from the the interlinear gloss; Baseline2 denotes attentional NMT system that trains on
an additional 57,608 lines of Turkish-English parallel data; Generation denotes the interlinear gloss
with target-lemma generation step (1→2→3).
We use IGT_src to denote our translation through
gloss with source-lemma as a pivot into target language (1→2→4). We use IGT_tgt to denote our
translation through gloss with target-lemma as a

Since we have a large number of interlinear
glosses in the Arapaho dataset, we also consider the case where it is added to the training

5.2

Case Study: Turkish-English NMT

Source Sentence
Sequence

Gold

Problemi
çöz-mek
zor-dur.

Baseline1

Baseline2

IGT_src

Generation

IGT_tgt

1→4

1→4*

1→2→4

1→2→3

1→2→3→4

To solve the
problem is
difficult.

Ali read the
book.

As for this
book , it is
known that it
is known as a
result.

As for the
book , the
book .

Solv-3.SG.ACC the is
difficult3.SG.COP.PRS.

The fact that
it is difficult
for that.

Fatma
bu
kitabkimin
yazd
gn
sanyor.

Who does
Fatma think
wrote this
book.

As for Fatma
knows that I
one.

As for Fatma
, I have a
house.

As for Fatma ,
Fatma knows
that I left .

Fatma-3.SG.NOM
this-DET kitabkimin
ATATURK-3.SG.NOM
wrote-3.SG.NOM
it-3.SG.NOM.

As for Fatma,
it is possible
that he wrote
this.

Adam
cocuga top
verdi.

The man
gave the
child a ball.

As for the book,
the book, the
book, the book.

Ahmet read
the book.

the girl that
the book .

Man1.3.SG.NOM.POSS
child-3.SG.DAT
ball-3.SG.NOM.

The man’s
child is the
ball.

Table 12: Qualitative Evaluation. All experiments except the starred Baseline2 use 865 lines of training data. Baseline2 uses
additional 57608 lines of parallel data. Notation of the translation sequence follows from Table 3.

pivot into target language (1→2→3→4).

order.

For evaluation, the BLEU score does not suffice
for evaluating our translation using only 865 lines
of data, especially when our translating goal is to
improve meaningful translation and improve finegrained translation performance.

The model baseline2 performs better than baseline1 on verb-match accuracy, subject-verb agreement accuracy, and tense-match accuracy, but is
worse off in BLEU scores as well as the metric of
matching nouns. This is interesting because we
expect that our performance will increase with increased amount of data. However, it is worth noting
that linguistic gloss data is very domain specific.
The Turkish ODIN dataset has a relatively narrow
domain which may not be covered by the parallel
data that is injected. Therefore, adding more data
may not help with the translation.

We present five metrics that we have designed for
our evaluating purpose on top of both 4-gram and
1-gram BLEU scores. They are: noun-match accuracy, verb-match accuracy, subject-object agreement accuracy, and tense-match accuracy. The
noun-match accuracy is the percentage correctly
predicted string-matched nouns; and the verbmatch accuracy is the percentage correctly preOur model IGT_src beats baseline1 and basedicted string-matched verbs. The subject-verb line2 in all metrics excluding subject-verb agreeagreement accuracy is the percentage of correctly ment. The reason that the subject-verb agreepredicted subject-verb agreement, for example, “he” ment does not perform well in IGT_src is because
is matched with “talks” or “talked” rather than IGT_src is a factored model created by combin“talk”. The tense-match accuracy is the percentage ing source text with morphological tags. Though
of correctly predicted tense. The metric of non- source lemmas are tagged with subject-verb agreerepetition is designed especially for NMT outputs. ment information, the model finds it hard to learn
A lot of NMT models achieve high BLEU scores by about the target lemmas. IGT_tgt addresses this
creating spurious repetitions. To take care of this issue. The metric of non-repetition performs very
blind spot of BLEU evaluation, we introduce a non- well, beating all baselines. Our model IGT_tgt
repetition metric, the percentage of unique words raises the metric of matching nouns by +20.15,
in the sentence. All metrics are averaged over the raises the metric of matching verbs by +15.74,
entire test data and are presented as percentages in increases the metric of noun-verb agreements by
Table 11. These metrics were applied through hand +44.44, raises the metric of matching tense by
evaluation to minimize noise; however, the process +26.85, raises the 1-gram BLEU by +0.20, and
can be automated (using, e.g., POS tagging). We raises the 4-gram BLEU by +1.66 comparing with
also evaluate using both 4-gram and 1-gram BLEU, baseline1. It also beats baseline2 in all metrics. The
where 1-gram BLEU helps us to evaluate our trans- reason that the noun-verb agreement performs very
lation without considering the correct grammatical well in IGT_tgt is that the model is actively learning

information of the target lemma. For example, if
“he-3.SG walks-3.SG” is present in training, then
the model learns the noun-verb agreement in the
space of target lemma very well. The metric of nonrepetition performs very well showing significant
improvement over all baselines.
Our five metrics only evaluate individual sentences, but there are corpus-level patterns that are
also worthy of comment. For example, in baseline2,
“the book” is repeated across all translations even
when the source sentence is totally unrelated.
In extreme low-resource scenarios like ours, qualitative evaluation is more important than quantitative evaluation. In Table 12, we see clearly that the
two baseline NMT systems are hallucinating. The
baseline translations have nothing in common with
the source sentence, except fluency. The model
IGT_tgt also hallucinates as it is exposed to very
little information regarding the target lemmas during training. However, our model IGT_tgt produce
meaningful translations that preserves the content
of the source sentence to a certain extent while also
achieving fluency through a good gloss-to-target
NMT system.

6

Conclusion

We present the cleaned and normalized Arapaho
and the ODIN datasets and our multilingual model
in translating from interlinear glosses to fluent target language. In addition, we present a three-step
solution to extremely low-resource translation training only on 865 lines of data with linguistic information as a case study. Finally, we present five metrics
for evaluating extremely low-resource translation
and show that our NMT system performs well in
noun-verb agreements.
We would benefit from a more detailed gloss
normalization process. We also would like to explore disambiguation in a morphological analyzer
(Shen et al.(2016)Shen, Clothiaux, Tagtow, Littell,
and Dyer) and more detailed morpheme segmentation. Furthermore, IGT is ubiquitous in linguistics
publications and lecture notes. Future work could
increase the size of ODIN by including IGTs from
newly available publications.

References
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena. 2013.
Polyglot: Distributed word representations for multilingual nlp. In Proceedings of the 17th Conference on Computational Natural Language Learn-

ing, pages 183–192, Sofia, Bulgaria. Association for
Computational Linguistics.
Arianna Bisazza and Marcello Federico. 2009. Morphological pre-processing for turkish to english statistical machine translation. In IWSLT, pages 129–
135.
Jan Botha and Phil Blunsom. 2014. Compositional
morphology for word representations and language
modelling. In International Conference on Machine
Learning, pages 1899–1907.
Franck Burlot, Mercedes Garcia-Martinez, Loïc Barrault, Fethi Bougares, and François Yvon. 2017.
Word representations in factored neural machine
translation. In Conference on Machine Translation,
volume 1, pages 43–55.
Alan Buseman. 2020.

Field Linguists’ ToolKit.
[Online; accessed 13-Feb-2020].
https://software.sil.org/toolbox/.

Aditi Chaudhary, Chunting Zhou, Lori Levin, Graham
Neubig, David R Mortensen, and Jaime G Carbonell.
2018. Adapting word embeddings to new languages
with morphological and phonological subword representions. arXiv preprint arXiv:1808.09500.
Junyoung Chung, Kyunghyun Cho, and Yoshua Bengio.
2016. A character-level decoder without explicit segmentation for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1693–
1703.
Ann Clifton and Anoop Sarkar. 2011.
Combining morpheme-based machine translation with postprocessing morpheme prediction. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 32–42. Association
for Computational Linguistics.
Ryan Cotterell and Hinrich Schütze. 2015. Morphological word-embeddings. In Proceedings of the 2015
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 1287–1292.
Andrew Cowell and Timothy O’Gorman. 2012.
Speech-genre effects on statistical measurements of
arapaho language competency. Algonquian PapersArchive, 44:22–36.
William Croft. 2002. Typology and universals. Cambridge University Press.
Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan
Belinkov, and Stephan Vogel. 2017. Understanding
and improving morphological learning in the neural machine translation decoder. In Proceedings of
the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),
pages 142–151.

Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and
Haifeng Wang. 2015. Multi-task learning for multiple language translation. In Proceedings of the
53rd Annual Meeting of the Association for Computational Linguistics, pages 1723–1732.
Chris Dyer, Victor Chahuneau, and Noah A Smith.
2013. A simple, fast, and effective reparameterization of ibm model 2. In Proceedings of the 12th Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technologies, pages 644–648.
Ilknur Durgar El-Kahlout and Kemal Oflazer. 2006. Initial explorations in english to turkish statistical machine translation. In Proceedings of the Workshop
on Statistical Machine Translation, pages 7–14. Association for Computational Linguistics.
Orhan Firat, Kyunghyun Cho, and Yoshua Bengio.
2016.
Multi-way, multilingual neural machine
translation with a shared attention mechanism. In
Proceedings of the 15th Conference of the North
American Chapter of the Association for Computational Linguistics on Human Language Technologies,
pages 866–875.
Mercedes García-Martínez, Loïc Barrault, and Fethi
Bougares. 2016. Factored neural machine translation. arXiv preprint arXiv:1609.04621.
Dan Gillick, Cliff Brunk, Oriol Vinyals, and Amarnag
Subramanya. 2016. Multilingual language processing from bytes. In Proceedings of the 15th Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technologies, pages 1296–1306.
Sharon Goldwater and David McClosky. 2005. Improving statistical mt through morphological analysis. In
Proceedings of the conference on human language
technology and empirical methods in natural language processing, pages 676–683. Association for
Computational Linguistics.
Thanh-Le Ha, Jan Niehues, and Alexander Waibel.
2016. Toward multilingual neural machine translation with universal encoder and decoder. arXiv
preprint arXiv:1611.04798.
Nizar Habash and Fatiha Sadat. 2006. Arabic preprocessing schemes for statistical machine translation.
In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume:
Short Papers, pages 49–52. Association for Computational Linguistics.
Jan Hajič. 2000. Machine translation of very close languages. In Sixth Applied Natural Language Processing Conference.
Chris Hokamp. 2017.
Ensembling factored neural machine translation models for automatic postediting and quality estimation.
arXiv preprint
arXiv:1706.05083.

Melvin Johnson, Mike Schuster, Quoc V Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Viégas, Martin Wattenberg, Greg Corrado,
et al. 2017. Google’s multilingual neural machine
translation system: Enabling zero-shot translation.
Transactions of the Association for Computational
Linguistics, 5:339–351.
Christo Kirov, John Sylak-Glassman, Roger Que, and
David Yarowsky. 2016.
Very-large scale parsing and normalization of wiktionary morphological
paradigms. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 3121–3126.
Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. 2017. Opennmt: Opensource toolkit for neural machine translation. Proceedings of the 55th annual meeting of the Association for Computational Linguistics, System Demonstrations, pages 67–72.
Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL).
Philipp Koehn and Rebecca Knowles. 2017. Six challenges for neural machine translation. arXiv preprint
arXiv:1706.03872.
Young-Suk Lee. 2004. Morphological analysis for statistical machine translation. In Proceedings of HLTNAACL 2004: Short Papers, pages 57–60. Association for Computational Linguistics.
Christian Lehmann. 1982. Directions for interlinear
morphemic translations. Folia linguistica, 16(14):199–224.
Christian Lehmann and William Croft. 2015. Leipzig
glossing convention.
https://www.eva.mpg.
de/lingua/resources/glossing-rules.php.
[Online; accessed 19-Jan-2019].
William D Lewis and Fei Xia. 2010. Developing odin:
A multilingual repository of annotated language data
for hundreds of the world’s languages. Literary and
Linguistic Computing, 25(3):303–319.
Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W
Black. 2015. Character-based neural machine translation. arXiv preprint arXiv:1511.04586.
Austin Matthews, Graham Neubig, and Chris Dyer.
2018. Using morphological knowledge in openvocabulary neural language models. In Proceedings
of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Papers), pages 1435–1445.

Sarah Moeller and Mans Hulden. 2018. Automatic
glossing in a low-resource setting for language documentation. In Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages,
pages 84–93.
Sebastian Nordhoff, Harald Hammarström, Robert
Forkel, and Martin Haspelmath. 2013. Glottolog 2.0.
Kemal Oflazer. 1994. Two-level description of turkish morphology. Literary and linguistic computing,
9(2):137–148.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for computational linguistics, pages 311–318. Association for
Computational Linguistics.
Peyman Passban, Qun Liu, and Andy Way. 2018. Improving character-based decoding using target-side
morphological information for neural machine translation. arXiv preprint arXiv:1804.06506.
Adithya Renduchintala, Pamela Shapiro, Kevin Duh,
and Philipp Koehn. 2019. Character-aware decoder
for translation into morphologically rich languages.
In Proceedings of Machine Translation Summit XVII
Volume 1: Research Track, pages 244–255.
Tanja Samardzic, Robert Schikowski, and Sabine Stoll.
2015. Automatic interlinear glossing as two-level
sequence classification. In Proceedings of the 9th
SIGHUM Workshop on Language Technology for
Cultural Heritage, Social Sciences, and Humanities
(LaTeCH), pages 68–72.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics, pages 1715–1725.
Rico Sennrich and Biao Zhang. 2019. Revisiting lowresource neural machine translation: A case study.
arXiv preprint arXiv:1905.11901.

John Sylak-Glassman, Christo Kirov, David Yarowsky,
and Roger Que. 2015. A language-independent feature schema for inflectional morphology. In Proceedings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 674–680.
Jörg Tiedemann. 2012. Character-based pivot translation for under-resourced languages and domains. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguistics, pages 141–151. Association for Computational Linguistics.
Yulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui,
Guillaume Lample, Patrick Littell, David Mortensen,
Alan W Black, Lori Levin, and Chris Dyer. 2016.
Polyglot neural language models: A case study in
cross-lingual phonetic representation learning. In
Proceedings of the 15th Conference of the North
American Chapter of the Association for Computational Linguistics on Human Language Technologies,
pages 1357–1366.
Irina Wagner, Andrew Cowell, and Jena D Hwang.
2016. Applying universal dependency to the arapaho language. In Proceedings of the 10th Linguistic
Annotation Workshop held in conjunction with ACL
2016 (LAW-X 2016), pages 171–179.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le,
Mohammad Norouzi, Wolfgang Macherey, Maxim
Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.
2016. Google’s neural machine translation system:
Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.
Fei Xia, William D Lewis, Michael Wayne Goodman,
Joshua Crowgey, and Emily M Bender. 2014. Enriching odin. In LREC, pages 3151–3157.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-tomorphology mapping in factored phrase-based statistical machine translation from english to turkish. In
Proceedings of the 48th annual meeting of the association for computational linguistics, pages 454–464.
Association for Computational Linguistics.

Qinlan Shen, Daniel Clothiaux, Emily Tagtow, Patrick
Littell, and Chris Dyer. 2016. The role of context in
neural morphological disambiguation. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 181–191.

Zhong Zhou, Matthias Sperber, and Alex Waibel. 2018.
Massively parallel cross-lingual learning in lowresource target language translation. In Proceedings of the 3rd conference on Machine Translation
Worshop of the 23rd Conference on Empirical Methods in Natural Language Processing. Association for
Computational Linguistics.

Nicolas Stroppa, Declan Groves, Andy Way, and Kepa
Sarasola. 2006. Example-based machine translation
of the basque language. In Proceedings of the 7th
Conference of the Association for Machine Translation in the Americas, pages 232–241. The Association for Machine Translation in the Americas.

Barret Zoph and Kevin Knight. 2016. Multi-source neural translation. In Proceedings of the 15th Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technologies, pages 30–34.

John Sylak-Glassman. 2016. Unimorph. http://
unimorph.org/. [Online; accessed 19-Jan-2019].

