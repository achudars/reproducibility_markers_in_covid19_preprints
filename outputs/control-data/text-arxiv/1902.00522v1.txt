arXiv:1902.00522v1 [astro-ph.IM] 1 Feb 2019

Deep Learning for Multi-Messenger Astrophysics
A Gateway for Discovery in the Big Data Era
Gabrielle Allen,1, 2 Igor Andreoni,3 Etienne Bachelet,4 G. Bruce Berriman,5 Federica B. Bianco,6, 7, 8, 9 Rahul
Biswas,10 Matias Carrasco Kind,1, 2 Kyle Chard,11 Minsik Cho,12 Philip S. Cowperthwaite,13 Zachariah B.
Etienne,14, 15 Daniel George,1, 2 Tom Gibbs,16 Matthew Graham,3, 17 William Gropp,1, 18 Anushri Gupta,1, 2
Roland Haas,1 E. A. Huerta,1, 2, ∗ Elise Jennings,19 Daniel S. Katz,1, 18, 20, 21 Asad Khan,1, 22 Volodymyr
Kindratenko,1, 20 William T. C. Kramer,1, 18 Xin Liu,2, 1 Ashish Mahabal,3, 17 Kenton McHenry,1, 18 J. M.
Miller,23, 24, 25 M. S. Neubauer,1, 22 Steve Oberlin,16 Alexander R. Olivas Jr,26 Shawn Rosofsky,1, 22
Milton Ruiz,22 Aaron Saxton,1 Bernard Schutz,27 Alex Schwing,18, 20, 28 Ed Seidel,1, 2, 22 Stuart L.
Shapiro,1, 2, 22 Hongyu Shen,1, 20 Yue Shen,2 Brigitta M. Sipőcz,29 Lunan Sun,22 John Towns,1, 30 Antonios
Tsokaros,22 Wei Wei,1, 22 Jack Wells,31 Timothy J. Williams,32 Jinjun Xiong,33 and Zhizhen Zhao1, 20, 28
1

NCSA, University of Illinois at Urbana-Champaign, Urbana, Illinois 61801, USA
Department of Astronomy, University of Illinois at Urbana-Champaign, Urbana, Illinois 61801, USA
3
California Institute of Technology, 1200 E California Blvd, Pasadena, California 91125, USA
4
Las Cumbres Observatory, 6740 Cortona Drive, Suite 102, Goleta, California 93117 USA
5
Caltech/IPAC-NExScI, 1200 E California Blvd, Pasadena, California 91125, USA
6
Department of Physics and Astronomy, University of Delaware, Newark, Delaware, 19716, USA
7
Joseph R. Biden Jr,. School of Public Policy and Administration, University of Delaware, Newark, Delaware, 19716, USA
8
Data Science Institute, University of Delaware, Newark, Delaware, 19716, USA
9
Center for Urban Science and Progress, New York University, 370 Jay St, Brooklyn, NY 11201, USA
10
The Oskar Klein Centre for Cosmoparticle Physics, Stockholm University, AlbaNova, Stockholm SE-106 91, Sweden
11
The University of Chicago, Chicago, Illinois 60605, USA
12
IBM Systems, Austin, Texas 78717, USA
13
The Observatories of the Carnegie Institution for Science, 813 Santa Barbara St., Pasadena, California 91101, USA
14
Department of Mathematics, West Virginia University, Morgantown, West Virginia 26506, USA
15
Center for Gravitational Waves and Cosmology, West Virginia University,
Chestnut Ridge Research Building, Morgantown, West Virginia 26505, USA
16
NVIDIA, 2788 San Tomas Expressway Santa Clara, California, 95050
17
Center for Data Driven Discovery, Caltech, Pasadena, California 91125, USA
18
Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, Illinois 61801, USA
19
Argonne National Laboratory, Leadership Computing Facility, Lemont, Illinois 60439, USA
20
Department of Electrical & Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, Illinois 61801, USA
21
School of Information Sciences, University of Illinois at Urbana-Champaign, Urbana, Illinois 61801, USA
22
Department of Physics, University of Illinois at Urbana-Champaign, Urbana, Illinois 61801, USA
23
Computational Physics and Methods, Los Alamos National Laboratory, Los Alamos, New Mexico 87545, USA
24
Center for Theoretical Astrophysics, Los Alamos National Laboratory, Los Alamos, New Mexico 87545, USA
25
Center for Nonlinear Studies, Los Alamos National Laboratory, Los Alamos, New Mexico 87545, USA
26
University of Maryland, College Park, Maryland 80742, USA
27
Cardiff University, Cardiff CF24 3AA, United Kingdom
28
Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, Urbana, Illinois 61801, USA
29
DIRAC Institute, Department of Astronomy, University of
Washington, 3910 15th Avenue NE, Seattle, Washington 98195, USA
30
Office of the CIO, University of Illinois at Urbana-Champaign, Urbana, Illinois 61801, USA
31
National Center for Computational Sciences, Oak Ridge National Laboratory, Tennessee, USA
32
Argonne National Laboratory, Lemont, Illinois 60439, USA
33
IBM T.J. Watson Research Center, Yorktown Heights, New York 10598, USA
(Dated: February 5, 2019)
2

This report provides an overview of recent work that harnesses the Big Data Revolution and Large
Scale Computing to address grand computational challenges in Multi-Messenger Astrophysics, with
a particular emphasis on real-time discovery campaigns. Acknowledging the transdisciplinary nature of Multi-Messenger Astrophysics, this document has been prepared by members of the physics,
astronomy, computer science, data science, software and cyberinfrastructure communities who attended the NSF-, DOE- and NVIDIA-funded “Deep Learning for Multi-Messenger Astrophysics:
Real-time Discovery at Scale” workshop, hosted at the National Center for Supercomputing Applications, October 17-19, 2018. Highlights of this report include unanimous agreement that it is
critical to accelerate the development and deployment of novel, signal-processing algorithms that
use the synergy between artificial intelligence (AI) and high performance computing to maximize
the potential for scientific discovery with Multi-Messenger Astrophysics. We discuss key aspects to
realize this endeavor, namely (i) the design and exploitation of scalable and computationally efficient

2
AI algorithms for Multi-Messenger Astrophysics; (ii) cyberinfrastructure requirements to numerically simulate astrophysical sources, and to process and interpret Multi-Messenger Astrophysics
data; (iii) management of gravitational wave detections and triggers to enable electromagnetic and
astro-particle follow-ups; (iv) a vision to harness future developments of machine and deep learning
and cyberinfrastructure resources to cope with the scale of discovery in the Big Data Era; (v) and
the need to build a community that brings domain experts together with data scientists on equal
footing to maximize and accelerate discovery in the nascent field of Multi-Messenger Astrophysics.

I.

INTRODUCTION

The detection of multiple gravitational wave (GW) signals, consistent with the collision of binary black holes (BBHs),
has verified Einstein’s theory of general relativity with exquisite detail in the most extreme astrophysical environments [1–6]. Furthermore, the observation of two colliding neutron stars (NSs) in GWs and light has initiated the
era of Multi-Messenger Astrophysics (MMA), defined as contemporaneous observations of astrophysical phenomena
using GWs, electromagnetic radiation, neutrinos and cosmic rays [7, 8], which provide complementary insights about
the astrophysical processes and environments of MMA sources. Another recent MMA breakthrough was enabled by
observations of high-energy gamma rays and cosmic neutrinos from the blazar TXS 0506+056, which provided the
first observational evidence that BHs are proton accelerators, and that blazars are central engines of cosmic neutrinos
and cosmic rays [9–11].
A thorough analysis of the data gathered by the LIGO and Virgo detectors during the two previous observing runs
indicates that one GW source was detected for every 15 days of analyzed data [6]. Furthermore, new detections have
refined previous calculations for merger rates, which now stand at 9.7 − 101Gpc−3 yr−1 and 110 − 3840Gpc−3 yr−1 for
BBH mergers, and NS collisions, at 90% confidence level. For the yet undetected neutron star-black hole (NSBH)
merger scenario, a merger rate upper limit at the 90% confidence level is estimated to be 610Gpc−3 yr−1 [6, 12]. As
the worldwide network of kilometer-scale, GW detectors continues to expand, and each detector gradually reaches
design sensitivity, their volumetric sensitivity will continue to grow, thereby increasing these merger rates in upcoming
observing runs [6].
The combination of GW observations with large scale astronomical surveys has also enabled GW standard-siren measurement of the Hubble constant with and without electromagnetic counterparts [13–15], as envisioned by Schutz [16].
Once LIGO and its international counterparts observe O(100) GW170814-type BBH mergers, and these observations
are combined with Dark Energy Survey-type photometric redshift galaxy catalogs, it will be possible to conduct
statistical measurements of the Hubble constant with ∼ 5% statistical precision [15]. The availability of deeper,
more accurate and more numerous photometric-redshift information with next-generation electromagnetic surveys,
such as the Large Synoptic Survey Telescope (LSST) [17], combined with the ability of GW detectors to localize
signals within ten square degrees by the next decade, implies that Gravitational Wave Cosmology will soon become
a standard cosmological probe, as accurate as quasar time-delay strong lensing [15, 18].
As the number of MMA observations continues to grow in years to come, it will be possible to conduct statistical
analyses to infer the currently unknown matter composition of NSs, the origin of long gamma ray bursts, and the
mass distribution of BHs and NSs. We will also learn about mechanisms that may trigger electromagnetic radiation
when BHs and NSs collide, and will pinpoint the location of these events with unprecedented accuracy, shedding light
on the astrophysical environments where compact objects form and coalesce as a function of cosmic time [19–21].
In order to fully realize these science goals, it is necessary to address outstanding computational challenges that
currently limit the scope of existing MMA searches. In the context of GW discovery campaigns, template matching
algorithms used for low-latency analysis can only probe a 4-D signal manifold out of the 9-D parameter space that is
available to GW detectors. It is reported in the literature that expanding the existing parameter space into higherdimensional signal manifolds that are astrophysically motivated is computationally infeasible [22–24]. On the other
hand, template agnostic algorithms are tailored for burst-like GW sources [25], and they may miss >
∼ O(second-long)
GW signals with moderate signal-to-noise ratios [2].
Similarly, signal-processing algorithms used to identify electromagnetic transients in telescope images are still
based on rudimentary machine learning (ML) techniques that do not adequately handle TB-size datasets in real-time
analyses. Addressing this limitation is of paramount importance for at least two reasons. First, the construction of
LSST will enable the most detailed survey of the southern hemisphere, covering 10 square degrees with every single
snapshot taken by the telescope. Every night, LSST will generate over 15 TB of data [17], producing thousands of

∗

Corresponding author; elihu@illinois.edu

3
triggers per second that will encode information about electromagnetic transients and other astrophysical sources in
the nearby Universe. Second, by the time the LSST starts producing data, the existing network of ground-based
GW detectors will be able to localize MMA sources within ten square degrees. This means that a single LSST
observation may contain optical counterparts of GW signals, and it is likely that both LSST and GW detectors will
perform simultaneous MMA observations. Therefore, processing both LSST and GW data in real-time will require a
different approach to process simultaneously both time-series data and telescope images, leading to the identification
of GW sources and their optical counterparts within seconds. For instance, LIGO and Virgo observed the collision
of two neutron stars (GW170817) in GWs, and less than two seconds later, the Fermi satellite observed gamma ray
emission from this same system, which underscores the speediness with which several windows of observation need
to be coordinated to capture key astrophysical information from these objects. It is worth noting that MMA not
only encompasses real-time observations, since radio emission from this event peaked more than 160 days after its
observation in GWs, and X-ray emission may be observed until 1000 days after the neutron star collision [26]. In this
document, however, we will concentrate on low latency MMA observations.
In summary, to fully realize the MMA science program we need to address two specific problems: (i) the depth and
speed of GW searches are insufficient, so we need to develop algorithms that significantly increase their speed and
efficacy; and (ii) to effectively search for optical counterparts of GWs, we need to develop new algorithms to process
a large number of triggers generated by LSST-type surveys, and then search for an electromagnetic counterpart in
telescope images that span ∼ 10 sq. degrees.
Similarly, understanding the physics of MMA sources is of paramount importance to inform the development of
algorithms to extract as much information as possible from real-time observations, and to define the scope and length
of broadband electromagnetic and astro-particle follow-up observations. GW170817 made evident the need to further
strengthen community efforts to accelerate the development of numerical relativity software stacks that are essential
both for LIGO data analyses searches, and to obtain insights about the nature of the post-merger remnant from
electromagnetic follow-up studies.
In order to start addressing these pressing issues, synergistic activities between domain experts and members of
the data science, high performance computing (HPC), and cyberinfrastructure communities must be organized and
pursued in earnest. This document provides a vision to achieve this goal, building upon ongoing activities that are
successfully harnessing the Big Data Revolution to address computational challenges in GW astrophysics, large scale
astronomical surveys and neutrino observations. This document is organized as follows. Section II provides a brief
status update of numerical methods used to simulate MMA sources. Section III describes key improvements that are
needed in existing numerical relativity software stacks to gain a better understanding of the physical processes that lead
to GW, electromagnetic and astro-particle production in the merger of compact binary systems. Section IV outlines
existing challenges in optical follow-ups of GW sources, and recent developments on the use of machine and deep
learning to optimize these searches. Section V presents a summary of machine and deep learning applications for GW
data analysis. Section VI provides a vision to build upon this work to increase the depth and speed of GW searches in
the context of MMA discovery campaigns. The cyberinfrastruture requirements to achieve convergence between deep
learning, large scale computing and MMA is described in Section VII. Section VIII identifies key milestones, both
technical and sociological, that need to be accomplished to maximize the scientific return of MMA. Acknowledging
the transdisciplinary nature of the MMA science program, Section IX provides a vision to build an MMA community
that enables domain experts from disparate fields of research to contribute their expertise on an equal footing, and
the need to create synergies with industry partners that in addition to co-funding data science activities at the heart
of MMA, may also provide avenues to diversify the career paths of MMA researchers. We summarize the findings of
this report in Section X.

II.

NUMERICAL RELATIVITY SIMULATIONS OF MULTI-MESSENGER SOURCES

Modeling an MMA source requires a pipeline rather than a single simulation. The expected messengers from a
compact binary merger are, in order: GW emission from the inspiral and merger of a compact binary system, gamma
ray emission from the central post-merger engine, and an optical and infrared afterglow from the radioactive decay
of r-process elements in ejected material. Each of these processes requires one, if not many simulations. Accurately
modeling the GW emission requires capturing the general relativistic effects. Modeling the formation of the jet and
ejected material requires magnetic fields. Modeling the r-process elements requires neutrino transport and nuclear
reaction networks.
The inspiral and merger of two compact objects can be modeled via numerical relativity (for a review of numerical
relativity see [27–30] and references therein). This calculation provides the GW signal and the morphology and
composition of the dynamical ejecta. Calculations with general relativistic magnetohydrodynamics (GRMHD) are
now routine. For instance, the GRMHD simulation of merging NSBH and NSNS systems reportedly in [31, 32] showed

4
for the first time that these systems can launch jets and be the progenitors of short gamma ray bursts (sGRBs). In
the BHNS scenario, it was found that a jet can be launched if a) the NS is endowed with a magnetic field that extends
from the stellar interior into the exterior, as in a radio pulsar, b) the tilt angle between the magnetic moment and the
total angular momentum of the system is small, and c) the initial BH spin is a/MBH >
∼ 0.4 [33]. In the NSNS scenario,
the GRMHD studies reported in [32, 34], in which the NS is modeled as an irrotational Γ = 2 polytrope, showed that
NSNS systems may launch an incipient jet whether or not the seeded poloidal magnetic field is confined or not to the
NS interior, as long as the binary forms a hypermassive NS that undergoes delayed collapse to a BH. The lifetime of
the jets [∆t ∼ 100(MNS/1.625M⊙)ms] and the outgoing electromagnetic luminosities [Ljet ∼ 1051 erg/s] in the above
cases turn out to be consistent with sGRBs [35–37]. Based on the GRMHD simulations in [38], it has been argued
that in order to have a sGRB as in GW170817 the merger remnant is likely a hypermassive NS that undergoes delayed
collapse. Furthermore, the GW170817 data can be combined with causality arguments to establish a NS upper mass
limit in the range 2.16M⊙ − 2.28M⊙.
The inclusion of more sophisticated neutrino and microphysics, however, is in its infancy. The most common
approach to ignore neutrinos or use neutrino leakage and the state of the art is a moment scheme [39–43]—although
neutrinos have been modeled more accurately with Monte Carlo methods in post-processing [44, 45]. After the merger,
the remnant-disk-wind system must be modeled to accurately capture the formation of the jet and the composition
and morphology of the gravitationally unbound material driven off of the disk. This requires general relativity in a
fixed, rather than dynamic, background, magnetic fields, and neutrino transport. Recently there has been a significant
effort to model this system in three dimensions [43, 46–48]. The launching and propagation of the jet itself requires
additional specialized modeling as well (see [49, 50] and references therein for a review).
Rapid, or r-process, nucleosynthesis drives the formation of heavy elements in the ejected material. The r-process
is generally modeled in post-processing via codes such as [51]. These nucleosynthetic yields are then fed into radiative
transport models to extract the light curve for the kilonova, yielding results such as those presented in [52] and [53].
While NSNS mergers have been confirmed as progenitors of GW and EM emission, and NSBH mergers are expected
to also fall into this category, there have been just a handful of studies that explore the feasibility of observing EM
emission from BBH mergers [54, 55]. For instance, recent GRMDH simulations suggest that a circumbinary disk
accretion onto non-spinning, stellar-mass black hole binaries may launch magnetically driven jets (or a collimated,
mildly relativistic outflow which is at least partially magnetically dominated) whose Poynting luminosity is of order
0.1% of the accretion power, i.e., BBH systems could serve both as radio, X- and gamma-ray engines. Future MMA
observations will confirm or rule out these predictions.

III.

FUTURE DEVELOPMENTS OF NUMERICAL RELATIVITY SIMULATIONS OF MMA SOURCES

Moving towards a more realistic treatment of MMA sources requires a number of improvements in existing software
stacks. For instance, a realistic nuclear equation of the state with density, temperature and composition dependence to
better capture not only the inspiral and merger effects, but more importantly to study the remnant with its subsequent
BH-disk evolution. In the late merger phase, neutrino transport and realistic magnetic fields are important elements
to simulate the sGRB and kilonova scenarios reliably [56], as well as the implementation of electromagnetic radiative
transport to obtain realistic estimates of the disk geometry, the mass accretion rate and electromagnetic luminosity
and spectra.
Astrophysics modeling occurs on many disparate length and energy scales, ranging from the very small to the very
large. In many cases, the relevant scales can be modeled via, e.g., mesh refinement [57]. Unfortunately, if the small
scales are relevant everywhere within a simulation domain, global simulations can be prohibitively expensive. One
such example system is magnetically driven turbulence in accretion disks. It is well known that the magneto-rotational
instability can drive turbulence and momentum transport within a disk [58, 59]. This transport can be modeled as an
effective “alpha” viscosity [60] that contains only local shear stresses. This viscosity can be matched to high-resolution
models of turbulence within the disk [61].
This approach has proven to be extraordinarily powerful. However, these methods fail to capture many relevant
effects of the disk-wind system, such as jets, magnetically driven wind, and magnetic arrestment. One potential avenue
for improvement is to adapt the more sophisticated models developed by the engineering community, such as large
eddy simulations and Reynolds-averaged Navier-Stokes models. These models incorporate many more locally-defined
viscous stress and transport terms, but require more sophisticated tuning [62] .
The engineering community has recently begun exploring the possibility of using tuned subgrid models for turbulence
to train neural networks for turbulence and the field has exploded [63–74]. Although these techniques are in their
infancy, they have the potential to enable accurate subgrid modeling in regimes that were previously intractable.
One promising application for ML in simulating MMA sources is the use of ML to model subgrid physics that
cannot currently be modeled from first principles at acceptable computational costs [75–77]. For example fully

5
resolved simulations that capture turbulent motion during supernovae or neutron star collision simulations [78–80]
are too expensive to employ for parameter study simulations. Instead the use of subgrid models tuned via a small
number of high resolution simulations has become increasingly popular in astrophysical simulations [81]. These type
of models have been used for a long time in computational fluid dynamics where recently the use of ML to model
closure parameters of the system has seen increasing use.
These methods thus require access to advanced cyberinfrastructure platforms to handle both the few high resolution,
parallel simulations used to tune the subgroup model as well as infrastructure to construct the model and eventually
infrastructure for a large set of simulations to study MMA sources. Full, high resolution, parallel simulations to tune
subgrid models will require tens of millions of node hours or more, which can only be delivered by high-performance
resources on par or better than current NSF leadership compute resources like Blue Waters or the future Frontera
system.

IV.

NEEDS FOR LOW-LATENCY IDENTIFICATION OF THE OPTICAL COUNTERPARTS OF MMA
SOURCES

The optical counterparts of GWs emitted during the merger of NSNS or NSBH systems are known as kilonovae
or macronovae [82–87]. This emission spans the ultra violet (UV), optical and the near infrared (NIR) bands, and
encodes key insights regarding ejected material that is powered by radioactive decays of r-process nuclei [47, 88]. As
had been expected, the landmark discovery GW170817 demonstrated that such sources can power sGRBs structured
jets [26]. Future MMA observations of these sources will provide new and detailed insights about the astrophysical
scenarios that lead to the emission or suppression of GRB jets.
The optical counterpart is one of the crucial messengers of MMA observations, since detection in the UV/Optical/NIR
is the best, and perhaps, the only method of localization of the source sufficient for unambiguously identifying the
host galaxy [89]. The detection is aided by a suitable pipeline of transient detection, e.g., [90], including automated
classification of astrophysical vs algorithmic artifacts, e.g., [91]. Such an unambiguous identification allows an easy
determination of the redshift, and therefore their use in a distance-redshift relationship to infer cosmological parameters in conjunction with the luminosity distance determined from the standard siren property of GWs [16, 92].
This is currently anticipated with much excitement as an independent method of inferring the Hubble constant in
the local universe, since a tension of the order of three sigma exists in the inferred value from high-redshift cosmic
microwave background data, and distance ladder measurements [93–95]. This science requires the construction of
complete galaxy catalogs, an activity in which machine and deep learning (DL) is in earnest exploration [96–98].
Such a precise localization is also crucial for triggering deep multi-wavelength follow-ups and spectroscopy studies,
since relevant telescopes have small fields of view. Such monitoring is essential to extract physical properties of the
merger, and detailed modeling of the sources such as the physics of the jet. The location of the source within its host
galaxy can provide useful information about the evolution of the progenitor system, and even its dynamics in the
host galaxy [99]. Multi-wavelength follow-up enables our understanding of physical processes that take place during
MMA events. For example, those include the production of heavy elements and diagnostics regarding the mass and
composition of the ejected material, properties of the circumstellar medium, and whether jets are generated during
the event [100].
High confidence identification of the optical counterparts of GWs is a challenging task. These transients have peculiar optical properties, namely, they are fast, dim, and rare. Furthermore, the area within which LIGO-type detectors
can localize MMA triggers includes many other unrelated optical transients (including artifacts and supernovae)
whose properties will be consistent with optical counterparts of GW events. Due to their fast decay rate (the initial
optical emission of the counterpart of GW170817 faded more than 1 mag/day, and was followed by a longer-lived
red transient [101]) and to the particular usefulness of early observations in constraining the physical properties of
the transient, they need to be located within a few hours of the compact binary merger, and then followed up using
relatively competitive spectroscopic resources, and observations on the entire electromagnetic (EM) spectrum (for an
actual detection scenario, see Figure 2 in [102], which itemizes the timeline of GW and broadband EM observations
of GW170817). In practice, this requires prompt response to the trigger to initiate a multi-filters discovery imaging
campaign that will cover large sky areas using large field-of-view, deep-imaging telescopes, or campaigns imaging
preselected galaxies (selected for their morphological properties and distance) with deep-imaging telescopes. In both
cases, the campaigns are conducted using fully automated discovery pipelines that perform image subtraction. ML
is already a standard component of such pipelines [103, 104], and plays a critical role to tell apart real astrophysical
sources from noise artifacts. Rapid integration with archive data will be valuable in pinpointing the optical counterparts. In addition to post-processing applications, neural networks are now being used as key discovery methods in
new pipelines, in the optical [105] as well as at other frequencies. In fact, image-subtraction based campaigns require
the existence of adequate templates over the entire are of sky to be searched, as inadequate or missing templates would

6
lead to an overwhelming amount of false positive artifacts, and DL techniques can be faster and more accurate than
image-subtraction methods for transient discovery, especially when the amount of data to be processed grows at TB
scales. Such techniques are being tested to be fully implemented in future large surveys and MMA follow-up programs.
Finally, aside from using these techniques for identifying a real astrophysical source, and then using spectroscopy to
make the positive identification as a kilonova, future analyses may also utilize photometric classification methods for
identification. Such possibilities have been studied in quite some detail for other transients like supernovae [106–108]
or methods applied to real data [109, 110]. The possibility of identifying kilonovae using this approach from wide
field surveys like LSST using complete light curves in a serendipitous survey have been discussed in [111–114]. Graph
neural networks have also been used to improve neutrino detection with the IceCube observatory. This detection scenario combines two challenging problems, namely, the sparse nature of the neutrino signals, the irregular geometry of
the detectors, and the large asymmetry between positive and negative events, in which case the pattern classification
problem demands false positive rates of order 10−6 [115].

V.

STATE-OF-THE-ART DEEP LEARNING ALGORITHMS FOR GRAVITATIONAL WAVE
DETECTION

While DL algorithms have been used for classification studies of time-series signals [116], the first demonstration that
these algorithms could be used both for classification and regression of time-series data was presented in [117], with an
application for the detection and parameter estimation of BBH mergers whose GWs are embedded in Gaussian noise.
Follow-up studies demonstrated that this methodology is also applicable for the detection, parameter estimation and
denoising of BBH mergers, detected by the LIGO and Virgo detectors, whose GWs are embedded in non-Gaussian
and non-stationary LIGO data [118–122]. These studies have sparked the interest of the community, leading to several
developments on ML and DL for GW data analysis and source modeling [123–129].
Central developments in this field require the use of Bayesian neural networks to enable parameter estimation
analyses endowed with statistical errors [130–132]. In order to probe a higher-dimensional signal manifold, beyond
the 4-D parameter space covered with traditional detection algorithms, it will be essential to combine DL and HPC
to use distributed training at scale. Such an approach is needed both to conduct large scale parameter sweeps to
determine the optimal architecture of neural network models. Once a neural network model is chosen, the use of
HPC platforms is vital to have the flexibility to train neural network models using TB-size waveform template banks
to achieve state-of-the-art accuracy for detection and characterization of GW signals in actual detection scenarios.
Reducing the training stage from weeks to minutes using distributed training in HPC platforms not only enables
the design and training of robust neural network models, and detailed uncertainty quantification studies to assess
the robustness of the model, but also guarantees that once a model is fully trained, one can use information about
the sensitivity of the GW detectors, during an ongoing detection search, to enhance the accuracy of neural network
models for low-latency GW detection and parameter estimation studies.

VI.

FUTURE DEVELOPMENTS OF DEEP LEARNING ALGORITHMS FOR GRAVITATIONAL
WAVE DETECTION

Existing DL-based signal processing tools have only been explored in the context of BBH mergers, assuming
O(second-long) GWs. There is a pressing need to expand the scope of these methods to cover the 9-D signal manifold
that is available to existing GW detectors.
Furthermore, it is critical to design neural network models adequate for the detection and characterization of NSNS
and NSBH mergers. Obvious challenges related to this work involve the fact that these neural network models will
have to be trained with significantly longer waveform signals. Since existing DL algorithms [118] are capable of
processing over 4,000 seconds of data within a second, one expects that even if DL algorithms for NSNS observations
are ∼ 1, 000x slower than those used for BBH detection, one may still be able to extract NSNS and NSBH signals
from GW data in low latency.
Other developments that involve MMA observations concern GW observations of supernovae, and other burstlike GW sources. A study of this nature is a straightforward extension of the algorithms already developed for
GW detection, denoising and unsupervised clustering that have been developed for the GW observation of BBH
mergers [122, 133].

7
VII.

CYBERINFRASTRUCTURE REQUIREMENTS FOR THE DEVELOPMENT OF DEEP
LEARNING ALGORITHMS FOR MMA SEARCHES

As pointed out in the previous sections, DL is steadily being implemented in all aspects of the MMA analysis pipeline.
Key features of DL algorithms that promote them as tools to address existing grand computational challenges in MMA
include their scalability, and the fact that once these algorithms are fully trained, they require minimal computational
resources for inference, or in different words, in an actual MMA discovery campaign.
With packages like TensorFlow [134] and PyTorch [135], much of the GPU/CPU optimization has been done for
us. However MMA will continue to push the computation and data limits of any cyberinfrastructure. Scaling these
limits can broadly be described in two ways: horizontal and vertical scaling. Vertical scaling is generally perceived as
“make the CPU/GPU processing faster”, and horizontal scaling as “give me more CPU/GPUs.” Different stages of
a DL pipeline have varying success taking advantage of these two scaling strategies. The major components of a DL
pipeline are data preparation, model training, model inference, and results analysis. For this part of the discussion
we ignore data preparation and results analysis as they are typically either done offline or are integrated into the DL
model and thus fall into the category of training or inference.
Training is by far the most computationally intensive step. It requires a significant amount of interprocess/thread
communication and sequential calculations. Most mature and well studied models can take advantage of horizontal
scaling by using large training batch sizes. However, new models that are being developed and studied can suffer from
poor generalization with horizontal scaling and they require careful data-specific scaling strategies. Training requires
a rounded balance of horizontal and vertical scaling. Inference is commonly an embarrassingly parallel process. Each
input can be processed completely independently of any other input. Depending on the vertical scale of computation,
the bottleneck can often be data retrieval and I/O latency and bandwidth.
In a production grade cyberinfrastructure, source and revision control is an integrated feature. Studying DL models
for MMA revealed many best practices that should be carried forward into an eventual production cyberinfrastructure.
With the most recent DL programing models, e.g., TensorFlow and PyTorch, a DL model is made up of code plus
numeric weights. TensorFlow popularized the concept of a model graph and created file formats to store such objects,
but a DL model is still created and maintained as traditional developer code. As the project matures, errors and
improvements on models will be discovered. The best practices are to treat the code as part of a model as if it was
any other software project with source/revision control such as github and a gitflow style development cycle. The
tricky part is traceability of model weights. Storing the literal model weight is always a valid approach. However,
the weights by themselves give very little diagnostic insight and so is a poor candidate to source control. From an
information theoretic perspective, the training data, training method, and hyper parameters, and the information on
DL software framework are equivalent to the raw model weights. This report has emphasized the need for a common
public datastore. We propose source controlling the metadata needed to fetch training data from the eventual common
public datastore.
Cyberinfrastructure for MMA analytics needs to support large collections of disparate datasets, and the ability to
apply complex and compute-intensive analytics algorithms on large portions of the data. This type of cyberinfrastructure can be satisfied with an low interconnect latency HPC-like system, with adequately provisioned data and
compute resources. On the other side, the infrastructure must support interactive access to the data, and the ability
to apply analytics tools to subsets of the data in real-time for a large number of users.
At the time of this writing (January 2019) the resources available from the NSF via XSEDE offer a decreasing
number of resources appropriate to support MMA efforts. At present, XSEDE compute resources provide 2.8 PFlop/s
of single precision GPU compute through the Bridges and Comet clusters [136, 137]. With the impending retirement
of several resources there will be a continued decline in the availability of GPU resources. A current solicitation will
bring new resources online in the 2020 time frame, but it is unknown as to what the configuration of these resources
will be (as they will be selected based on proposals that have not yet been written) and whether they will effectively
support the computing modalities needed by MMA.
The NSF-funded Frontera supercomputer focuses on CPU performance providing 35-40 PFlop/s through its Intel
Xeon CPUs, and only 8 PF/s of 32-bit precision compute through a small GPU section of the system [138]. While
explicitly designed with ML in mind, MMA ML applications will easily consume more compute resources than available
through Frontera’s GPU section. Other NSF-funded investments include the DL project at NCSA, which consists
of a computer system dedicated to supporting DL research at the University of Illinois at Urbana-Champaign. The
system consists of 16 IBM Power9 servers with four NVIDIA V100 GPUs, or 64 GPUs in total. The nodes are
interconnected with dual-channel EDR IB. The system supports TensorFlow and other DL frameworks in a containerbased environment.
In summary, the landscape of existing and planned NSF-funded HPC cyberinfrastructure indicates that there is a
pressing need to expand these resources to meet MMA cyberinfrastructure needs [139–142]. In addition, it is unclear
as to whether there are sufficient technical staff available with whom to partner and gain the necessary expertise to

8
further develop these techniques. Given the much broader applicability of ML/DL/AI approaches, the agencies are
advised to invest in the resources and services necessary to enable the success of MMA and other community efforts
reliant on these approaches. This goes beyond simply provisioning GPU systems and the technical staff required to
support them. Appropriate storage and data sharing environments that can support multidisciplinary collaborations
along with adequate networking infrastructure to support both bulk data transport and rapid notifications is required.
There must also be exploration of newer technologies, e.g., tensor processing units, quantum information systems,
and of their applicability to these challenges.
On the other hand, Department of Energy (DoE) HPC and leadership computing centers provide access to stateof-the-art computing platforms such as Summit and Titan at Oak Ridge National Laboratory which are equipped
with over 27k NVIDIA Volta V100 GPUs, and over 18k K20X GPUs, respectively. Theta at Argonne National
Laboratory, though not providing GPUs, has the infrastructure in place to conduct DL research at scale. In 2018, the
DoE’s Innovative and Novel Computational Impact on Theory and Experiment (INCITE) and Argonne Leadership
Computing Facility Data Science Program (ADSP) provided computing resources for data science projects with an
emphasis on machine and DL research at scale, filling in a critical void in the US HPC landscape for this emergent
area of research, which are essential for the realization of the MMA science program.

VIII.

FUTURE NEEDS OF MULTI-MESSENGER ASTROPHYSICS

To maximize the scientific return of MMA in the near future, progress on several fronts has to be made. It is of
primary importance to obtain alerts from GW detectors as soon as possible, but also from high energy surveys such
as Fermi, to the optical follow up community to select the most promising targets. A list of potential candidates is
essential to enable every team to rank the targets by priority, given their observing capabilities and science goals.
A common tool for galaxy cross matching is highly desirable, as currently this task is duplicated by each team. In
addition, research into a ML classifier for MMA is highly desirable for an early classification.
For observational programs, it grows more and more important that all steps from target selection, to the planning
and execution of observations be done programmatically. This avoids, in principle, any human biases and mistakes
and could resolve some potential conflicts. The two first steps can be achieved with new generation of observing
software, the so-called Target and Observation Manager (TOM) systems [143]. These ensure an efficient way to
manage and submit observations at large scale. These tools are specially designed to have the flexibility to interact
with any instrument/telescopes in order to maximize the efficiency of the observing process. The systems provide an
efficient platform for communication between observers.
There needs to be discussion of the needs of the future generation of telescopes that will be used for MMA follow-up.
With the upcoming new generation of large telescopes (LSST, Thirty Meter Telescope, Extremely Large Telescope,
etc.), it is crucial to design future instruments to accurately characterize GW events. Moreover, the amount of
telescope time that will be dedicated to MMA has to be decided. The observing strategy, mechanism and software
infrastructure should be flexible enough to allow a rapid reaction to any potential targets of primary importance.
Ideally, a common, public database would be build to centralize all information available for any targets. This
will minimize the duplication of common tasks (such as data collection, galaxy cross-matching, ranking, etc.) among
follow-up teams, therefore allow their focus to be dedicated to producing the best science. This can be coupled with
classification algorithms and catalogs cross-matching. The infrastructure of the LSST data management system and
the alert brokers under development are good examples.
The development and the generalization of TOM tools in parallel to the robotization of telescopes is a important
step that has to be explored to maximize the science return of future MMA events. The architecture and operational
practices of primary detection instrument should be examined for improvements for MMA use cases. For example,
while primarily a general-purpose astronomy instrument, LSST is able to respond promptly to GW detections. Access
to these observations for the general community is currently seen as implemented thought community event brokers.
The match of this architecture to MMA use case should be validated.
Finally, it is crucial that funding agencies explore the best way to manage competitive proposals that try to achieve
the same scientific goals. Given the scientific value of these future events, several teams are likely to simultaneously
request the same observations at the same facilities, creating inefficient duplication and potential for conflict. With
increasing rates of target discovery this inefficiency could become a significant drain on limited observing resources.
While DL/ML software can manage the technical issues relevant to these problems, the political aspects remain to
be addressed.

9
IX.

COMMUNITY BUILDING

MMA, like many other fields, is inherently both collaborative and competitive. Competition can be both constructive and destructive. Destructive competition inhibits collaboration and teamwork and in the burgeoning MMA
field, is counterproductive. However, because MMA is transdisciplinary in nature, and its science goals will not be
realized by any one individual or small group, similar to how it necessarily requires large groups several years to put
multiple satellites in orbit, build vast surface arrays, or instrument a volume of Antarctic ice, both collaboration and
constructive competition must be encouraged and developed. In this section, we discuss several aspects of the future
MMA community that promote such collaboration and constructive competition, including software and services,
workforce development, and social and technical mechanisms for community building.
A.

Community software and services

While web-based services will remain useful, the predominant mechanism for accessing and analyzing data in the
immediate future will likely be through the command-line in a Python environment. Technologies such Jupyter
Notebooks are enabling the sharing and understanding of complex workflows. We encourage the MMA community
to develop Python interfaces to all software packages. In addition the MMA community should be asked to share
their workflows via Notebooks. In so far as is possible, we encourage software teams to license their software with a
permissive license to optimize sharing and re-use of code. Housing the code in a common repository (unless forbidden
for legal reasons) will support discovery and use of the data. Policies for use of the repository will require care and
attention, but it will be understood that the software providers own the code they contribute. Successful projects
such as Astropy [144] and yt [145] provide examples of how community projects can be managed, how credit and
reward can be assigned, and how conflict can be managed. And in some cases, MMA work can be merged into these
existing packages.
Astronomy archives will be developing data access services aimed at maximizing science return from MMA. The
NASA Extragalactic Database (NED) is actively developing an EM-GW follow-up service for the advanced LIGOVirgo third observing run in Spring 2019, to optimize fast EM follow-ups to GW events. Its design has been informed
by NEDs support for follow-up to the GW170817 event. The MMA community should be active in identifying
follow-up services, their requirements and their performance, and in evaluating the services.
The reproducibility of results is one of the fundamental tenets of the scientific method. When it comes to complex
software stacks, it is important to keep in mind that reproducibility within systematic and statistical errors is what is
important, as opposed to bitwise reproducibility, which is the standard in software engineering. IceCube has designed
a system, soon to be released as open source [146, 147], that will detect systematic deviations due to improvements,
bug fixes, or optimizations in the underlying algorithms but will be robust to purely statistical deviations. This
system has the potential to detect changes in continuous distributions beyond those due purely to statistics and below
the level of human observations. This was originally designed to detect small changes in large, complex simulation
chains, but can be adapted to analysis chains as well. It will be critical that this software be easy to use correctly and
difficult to use incorrectly, to facilitate early adoption. Near the end of an analysis, there is often little enthusiasm
for validation and verification and less care is given to this component of the process. This can often make results
difficult for others to reproduce, even within the same experiment. Investing in validation and verification reduces
the amount of time it takes for new people to get started. Reproducing previous results can be a great way for a new
person to gain the necessary confidence to adapt and extend the analysis, resulting in better science quicker for the
entire MMA community.
B.

Workforce development

Current large scientific collaborations are composed of two largely separate communities: scientists and engineers.
Though scientists have the technical background and aptitude to communicate needs and requirements to engineers,
they often lack the mindset, skills, and training to develop and design as engineers do. In large projects, a significant
fraction of the codebase is designed, developed, and maintained by students and postdocs who have very little formal
training in software engineering best practices. Their software is often integrated into complex processing stacks,
intended to work for years and even decades. Critical to the long-term performance of complex processing chains is
the training of the workforce largely responsible for design, development, and maintenance.
Modern ML/DL courses offered by Udacity, for example, tend to focus on industrial applications like facial recognition and natural language processing. The overlap with scientific applications can range from significant, e.g., facial
recognition and astronomical image categorization, to orthogonal, e.g., natural language processing. Issues such as

10
error propagation through DL architectures, critical to scientific applications, are largely ignored in industry. An
initial proposal for a DL curriculum, which could serve as an extension to Software Carpentry [148], should include
software engineering best practices, and several programming languages. It is well known that there are far fewer
academic positions than degrees earned and many colleagues therefore end up in industry. There is an opportunity
to engage and compel our industrial partners to provide support in training their future workforce. It is both cost
effective and mutually beneficial.
A variety of activities may be pursued to transfer skills and to on-board new members, e.g., in-person bootcamps,
either separate or in conjunction with major conferences. NCSA would be an ideal venue, but the community should
be open to other hosts. The Software Carpentry project is one successful example. Engaging such projects to provide
training needed for MMA is likely to be the optimum approach. Engaging such projects to provide training needed
for MMA is likely to be the optimum approach. It is important to engage industry partners in these activities. More
astronomers and physicists leave academia for industry, so it’s in industries’ best interest to invest in their workforce
training early.

C.

Towards a cohesive community

Developing a cohesive community depends on many complex sociological factors, implemented in policies and
practices as well as culture. Individuals first need to feel a sense of community, a group that is working together
at some level, and to feel that they are or want to be part of that community. Next, in order for a community to
thrive, individuals need to find it rewarding (i.e., to have incentives, whether intrinsic or extrinsic) to become or to
remain part of the community, to contribute back, and to urge fellow colleagues to join. Though modern technology
has made it relatively inexpensive to provide the tools to build and maintain global online communities, these tools
are just tools; they are not the community. The community must form and persists, and it is important to recognize
the value that face-to-face meetings, including conferences, workshops, bootcamps, can bring to community identity.
Funding for these events must be sought from NSF, DOE, and industry partners.
Defining teams as early as possible and providing the tools, e.g., on-boarding, to easily get up and running, will
provide individuals with the opportunity for optimal career advancement, while also optimally advancing the goals of
the community. As different groups work together, driving scientific and technological innovation for MMA, it will be
essential to implement reward mechanisms to attract and retain talent through career advancement. Furthermore, as
individuals and teams engage with this community, it is essential to establish transparent mechanisms to enable them
to achieve their goals, while contributing back to the team, thereby sustaining and encouraging team structures.
Many of the challenges described in this article, such as a building and sustaining cohesive community, workforce
training, development and advancement; access and delivery of large volume data, low-latency data analysis, and
employing ML/DL/AI techniques with the associated cyberinfrastructure required to minimize time-to-insight, are
shared with other data-intensive scientific domains such as high-energy physics (HEP). Through a process involving
18 workshops over two years, key national and international partners from HEP, computer science, industry, and
data science were brought together to generate over eight community position papers, including a software institute
Strategic Plan [149] and a Community Whitepaper [150] as a roadmap for HEP software and computing R&D over
the next decade. The MMA community should ascertain what can be learned from community processes such as this,
and seek out shared software and cyberinfrastructure solutions to challenges in common to maximize the scientific
return-on-investment from MMA data. This process is already in motion with the NSF-funded project “Community
planning for Scalable Cyberinfrastructure to support Multi-Messenger Astrophysics” [151].

X.

CONCLUSIONS

Representatives from the astronomy, physics, computer science, HPC, software, and data communities gathered
together at NCSA to discuss recent accomplishments of the MMA science program, and to identify grand computational and theoretical challenges that need to be addressed to maximize the potential for discovery of future MMA
discovery campaigns.
This community expressed unanimous agreement that there is a pressing need to accelerate the development of
ML and DL algorithms for the GW, EM, and neutrino detection of MMA sources in real-time and at scale. MMA
observations of GW170817 have been transformative, and have identified three pillars that require immediate attention
for the full realization of the MMA science program: 1) increase the speed, accuracy and robustness of numerical
relativity simulations of MMA sources; 2) increase the speed and depth of signal processing algorithms for real-time
detection of GWs and their counterparts in light and astro-particles; and 3) identify cyberinfrastructure resources to
simulate and search for MMA sources in ever increasing and disparate datasets.

11
There is concern regarding the availability of future NSF-funded cyberinfrastructure facilities for MMA data analytics. The convergence of DL with HPC to train deeper and more accurate models with TB-size training data sets is
critical to ensuring that neural network models cover as deep a parameter space as possible. Once these models are
fully trained, they can be used for inference searches using minimal computational resources. We suggest reaching out
to DOE HPC platforms to obtain computational resources for DL at scale, given that DOE labs such as Oak Ridge
and Argonne provide state-of-the-art resources for ML/DL research.
Beyond scientific development, this team recognizes the need for policy making regarding data acquisition and
data sharing at astronomical observatories and methods for fast interaction between the GW and the astronomy
communities to maximize scientific discovery. We also emphasize the need to create and nurture an MMA community
that designs mechanisms to reward its members, and to grow in an organic manner, providing career paths for its
members beyond academia. There are successful examples in astronomy (e.g., Astropy [144], yt [145] and the LSST
Dark Energy Science Collaboration [152]) that can act as models for building a community. We suggest creating
synergies with industry partners that, in addition to co-funding data science workshops and bootcamps for MMA
researchers, may also recruit members of this community.

XI.

ACKNOWLEDGEMENTS

We gratefully acknowledge support from NVIDIA, Argonne Computing Leadership Facility and the National Science
Foundation through grant NSF-1848815.

[1] B. P. Abbott, R. Abbott, T. D. Abbott, M. R. Abernathy, F. Acernese, K. Ackley, C. Adams, T. Adams, P. Addesso,
R. X. Adhikari, and et al., Physical Review Letters 116, 061102 (2016), arXiv:1602.03837 [gr-qc].
[2] B. P. Abbott, R. Abbott, T. D. Abbott, M. R. Abernathy, F. Acernese, K. Ackley, C. Adams, T. Adams, P. Addesso,
R. X. Adhikari, and et al., Physical Review Letters 116, 241103 (2016), arXiv:1606.04855 [gr-qc].
[3] B. P. Abbott, R. Abbott, T. D. Abbott, M. R. Abernathy, F. Acernese, K. Ackley, C. Adams, T. Adams, P. Addesso,
R. X. Adhikari, et al., Physical Review Letters 118, 221101 (2017).
[4] B. P. Abbott, R. Abbott, T. D. Abbott, F. Acernese, K. Ackley, C. Adams, T. Adams, P. Addesso, R. X. Adhikari, V. B.
Adya, and et al., Physical Review Letters 119, 141101 (2017), arXiv:1709.09660 [gr-qc].
[5] B. P. Abbott, R. Abbott, T. D. Abbott, F. Acernese, K. Ackley, C. Adams, T. Adams, P. Addesso, R. X. Adhikari, V. B.
Adya, and et al., Astrophys. J. Lett 851, L35 (2017), arXiv:1711.05578 [astro-ph.HE].
[6] The LIGO Scientific Collaboration, the Virgo Collaboration, et al., arXiv e-prints , arXiv:1811.12907 (2018),
arXiv:1811.12907 [astro-ph.HE].
[7] B. P. Abbott, R. Abbott, T. D. Abbott, F. Acernese, K. Ackley, C. Adams, T. Adams, P. Addesso, R. X. Adhikari, V. B.
Adya, and et al., Physical Review Letters 119, 161101 (2017), arXiv:1710.05832 [gr-qc].
[8] The LIGO Scientific Collaboration, the Virgo Collaboration, B. P. Abbott, R. Abbott, T. D. Abbott, F. Acernese,
K. Ackley, C. Adams, T. Adams, P. Addesso, and et al., ArXiv e-prints (2017), arXiv:1710.05836 [astro-ph.HE],
arXiv:1710.05836 [astro-ph.HE].
[9] National Science Foundation, “Neutrino observation points to one source of high-energy cosmic rays,” (2018),
https://nsf.gov/news/news_summ.jsp?cntn_id=295955.
[10] IceCube Collaboration, Science 361, 147 (2018).
[11] IceCube Collaboration, M. G. Aartsen, M. Ackermann, J. Adams, J. A. Aguilar, M. Ahlers, M. Ahrens, I. Al Samarai,
D. Altmann, K. Andeen, and et al., Science 361, eaat1378 (2018), arXiv:1807.08816 [astro-ph.HE].
[12] The LIGO Scientific Collaboration and the Virgo Collaboration, arXiv e-prints , arXiv:1811.12940 (2018),
arXiv:1811.12940 [astro-ph.HE].
[13] B. P. Abbott et al., Nature (London) 551, 85 (2017).
[14] M. Fishbach, R. Gray, I. Magaña Hernandez, H. Qi, A. Sur, members of the LIGO Scientific Collaboration, and the
Virgo Collaboration, ArXiv e-prints , arXiv:1807.05667 (2018), arXiv:1807.05667 [astro-ph.CO].
[15] The DES Collaboration, the LIGO Scientific Collaboration,
and the Virgo Collaboration, arXiv e-prints ,
arXiv:1901.01540 (2019), arXiv:1901.01540 [astro-ph.CO].
[16] B. F. Schutz, Nature (London) 323, 310 (1986).
[17] LSST Science Collaboration, P. A. Abell, J. Allison, S. F. Anderson, J. R. Andrew, J. R. P. Angel, L. Armus, D. Arnett,
S. J. Asztalos, T. S. Axelrod, and et al., ArXiv e-prints (2009), arXiv:0912.0201 [astro-ph.IM].
[18] S. Birrer, T. Treu, C. E. Rusu, V. Bonvin, C. D. Fassnacht, J. H. H. Chan, A. Agnello, A. J. Shajib, G. C. F. Chen,
M. Auger, F. Courbin, S. Hilbert, D. Sluse, S. H. Suyu, K. C. Wong, P. Marshall, B. C. Lemaux, and G. Meylan, arXiv
e-prints , arXiv:1809.01274 (2018), arXiv:1809.01274 [astro-ph.CO].
[19] L. P. Singer, L. R. Price, B. Farr, A. L. Urban, C. Pankow, S. Vitale, J. Veitch, W. M. Farr, C. Hanna, K. Cannon, T. Downes, P. Graff, C.-J. Haster, I. Mandel, T. Sidery, and A. Vecchio, Astrophys. J. 795, 105 (2014),

12
arXiv:1404.5623 [astro-ph.HE].
[20] M. Branchesi, Journal of Physics: Conference Series 718, 022004 (2016).
[21] I. Bartos, L. S. Collaboration, and V. Collaboration, Journal of Physics: Conference Series 888, 012001 (2017).
[22] E. A. Huerta, P. Kumar, B. Agarwal, D. George, H.-Y. Schive, H. P. Pfeiffer, R. Haas, W. Ren, T. Chu, M. Boyle, D. A.
Hemberger, L. E. Kidder, M. A. Scheel, and B. Szilagyi, Phys. Rev. D 95, 024038 (2017), arXiv:1609.05933 [gr-qc].
[23] E. A. Huerta, C. J. Moore, P. Kumar, D. George, A. J. K. Chua, R. Haas, E. Wessel, D. Johnson, D. Glennon, A. Rebei,
A. M. Holgado, J. R. Gair, and H. P. Pfeiffer, Phys. Rev. D 97, 024031 (2018), arXiv:1711.06276 [gr-qc].
[24] I. Harry, S. Privitera, A. Bohé, and A. Buonanno, Phys. Rev. D 94, 024012 (2016), arXiv:1603.02444 [gr-qc].
[25] S. Klimenko, G. Vedovato, M. Drago, F. Salemi, V. Tiwari, G. A. Prodi, C. Lazzaro, K. Ackley, S. Tiwari, C. F. Da Silva,
and G. Mitselmakher, Phys. Rev. D 93, 042004 (2016), arXiv:1511.05999 [gr-qc].
[26] K. D. Alexander, R. Margutti, P. K. Blanchard, W. Fong, E. Berger, A. Hajela, T. Eftekhari, R. Chornock, P. S.
Cowperthwaite, D. Giannios, C. Guidorzi, A. Kathirgamaraju, A. MacFadyen, B. D. Metzger, M. Nicholl, L. Sironi,
V. A. Villar, P. K. G. Williams, X. Xie, and J. Zrake, Astrophys. J. 863, L18 (2018), arXiv:1805.02870 [astro-ph.HE].
[27] M. Shibata and K. Taniguchi, Living Reviews in Relativity 14, 6 (2011).
[28] J. A. Faber and F. A. Rasio, Living Reviews in Relativity 15, 8 (2012).
[29] B. Brügmann, Science 361, 366 (2018), http://science.sciencemag.org/content/361/6400/366.full.pdf.
[30] M. D. Duez and Y. Zlochower, Reports on Progress in Physics 82, 016902 (2019), arXiv:1808.06011 [gr-qc].
[31] V. Paschalidis, M. Ruiz, and S. L. Shapiro, Astrophys. J. 806, L14 (2015), arXiv:1410.7392 [astro-ph.HE].
[32] M. Ruiz, R. N. Lang, V. Paschalidis, and S. L. Shapiro, Astrophys. J. 824, L6 (2016), arXiv:1604.02455 [astro-ph.HE].
[33] M. Ruiz, S. L. Shapiro, and A. Tsokaros, Phys. Rev. D 98, 123017 (2018), arXiv:1810.08618 [astro-ph.HE].
[34] M. Ruiz and S. L. Shapiro, Phys. Rev. D96, 084063 (2017), arXiv:1709.00414 [astro-ph.HE].
[35] P. N. Bhat et al., Astrophys. J. Suppl. 223, 28 (2016), arXiv:1603.07612 [astro-ph.HE].
[36] A. Lien et al., Astrophys. J. 829, 7 (2016), arXiv:1606.01956 [astro-ph.HE].
[37] D. S. Svinkin, D. D. Frederiks, R. L. Aptekar, S. V. Golenetskii, V. D. Pal’shin, P. P. Oleynik, A. E. Tsvetkova, M. V.
Ulanov, T. L. Cline, and K. Hurley, Astrophys. J. Suppl. 224, 10 (2016), arXiv:1603.06832 [astro-ph.HE].
[38] M. Ruiz, S. L. Shapiro, and A. Tsokaros, Phys. Rev. D97, 021501 (2018), arXiv:1711.00473 [astro-ph.HE].
[39] M. B. Deaton, M. D. Duez, F. Foucart, E. O’Connor, C. D. Ott, L. E. Kidder, C. D. Muhlberger, M. A. Scheel, and
B. Szilagyi, The Astrophysical Journal 776, 47 (2013).
[40] D. Neilsen, S. L. Liebling, M. Anderson, L. Lehner, E. O’Connor, and C. Palenzuela, Phys. Rev. D 89, 104029 (2014).
[41] S.
Wanajo,
Y.
Sekiguchi,
N.
Nishimura,
K.
Kiuchi,
K.
Kyutoku,
and
M.
Shibata,
The Astrophysical Journal 789, L39 (2014).
[42] D. Radice,
F. Galeazzi,
J. Lippuner,
L. F. Roberts,
C. D. Ott,
and L. Rezzolla,
Monthly Notices of the Royal Astronomical Society 460, 3255 (2016).
[43] F. Foucart et al., Phys. Rev. D 91, 124021 (2015).
[44] S. Richers, D. Kasen, E. O’Connor, R. Fernández, and C. D. Ott, The Astrophysical Journal 813, 38 (2015).
[45] F. Foucart, Monthly Notices of the Royal Astronomical Society 475, 4186 (2018).
[46] F. Hossein Nouri, M. D. Duez, F. Foucart, M. B. Deaton, R. Haas, M. Haddadi, L. E. Kidder, C. D. Ott, H. P. Pfeiffer,
M. A. Scheel, and B. Szilagyi, Phys. Rev. D 97, 083014 (2018).
[47] D. M. Siegel and B. D. Metzger, The Astrophysical Journal 858, 52 (2018).
[48] R. Fernández et al., ArXiv e-prints (2018), arXiv:1808.00461 [astro-ph.HE].
[49] L. Baiotti and L. Rezzolla, Reports on Progress in Physics 80, 096901 (2017).
[50] P. Kumar and B. Zhang, Physics Reports 561, 1 (2015), the physics of gamma-ray bursts and relativistic jets.
[51] J. Lippuner and L. F. Roberts, The Astrophysical Journal Supplement Series 233, 18 (2017).
[52] D. Kasen, N. R. Badnell, and J. Barnes, ApJ 774, 25 (2013), arXiv:1303.5788 [astro-ph.HE].
[53] N. R. Tanvir et al., The Astrophysical Journal Letters 848, L27 (2017).
[54] V. Connaughton et al., The Astrophysical Journal Letters 826, L6 (2016).
[55] F. Verrecchia, M. Tavani, A. Ursi, A. Argan, C. Pittori, I. Donnarumma, A. Bulgarelli, F. Fuschino, C. Labanti, and
et al., The Astrophysical Journal Letters 847, L20 (2017).
[56] D. Radice, A. Perego, K. Hotokezaka, S. A. Fromm, S. Bernuzzi, and L. F. Roberts, Astrophys. J. 869, 130 (2018),
arXiv:1809.11161 [astro-ph.HE].
[57] M. Berger and P. Colella, Journal of Computational Physics 82, 64 (1989).
[58] S. A. Balbus and J. F. Hawley, Astrophys. J. 376, 214 (1991).
[59] S. A. Balbus and J. F. Hawley, Reviews of Modern Physics 70, 1 (1998).
[60] N. I. Shakura and R. A. Sunyaev, A&A 24, 337 (1973).
[61] J. F. Hawley, C. F. Gammie, and S. A. Balbus, Astrophys. J. 440, 742 (1995).
[62] Y. Zhiyin, Chinese Journal of Aeronautics 28, 11 (2015).
[63] K. Duraisamy, Z. J. Zhang, and A. P. Singh, in 53rd AIAA Aerospace Sciences Meeting (2015) p. 1284.
[64] E. J. Parish and K. Duraisamy, Journal of Computational Physics 305, 758 (2016).
[65] J.-X. Wang, J.-L. Wu, and H. Xiao, Phys. Rev. Fluids 2, 034603 (2017).
[66] R. King, P. Graf, and M. Chertkov, in APS Meeting Abstracts (2017) p. A31.008.
[67] R. Maulik and O. San, Journal of Fluid Mechanics 831, 151 to 181 (2017).
[68] T. P. Miyanawala and R. K. Jaiman, arXiv e-prints , arXiv:1710.09099 (2017), arXiv:1710.09099 [physics.flu-dyn].
[69] A. Barati Farimani, J. Gomes, and V. S. Pande, arXiv e-prints , arXiv:1709.02432 (2017), arXiv:1709.02432 [cs.LG].
[70] O. Hennigh, arXiv e-prints , arXiv:1705.09036 (2017), arXiv:1705.09036 [stat.ML].

13
[71] Y. Xie, E. Franz, M. Chu, and N. Thuerey, arXiv e-prints , arXiv:1801.09710 (2018), arXiv:1801.09710 [cs.LG].
[72] A. T. Mohan and D. V. Gaitonde, arXiv e-prints , arXiv:1804.09269 (2018), arXiv:1804.09269 [physics.comp-ph].
[73] R. King, O. Hennigh, A. Mohan,
and M. Chertkov, arXiv e-prints , arXiv:1810.07785 (2018),
arXiv:1810.07785 [physics.flu-dyn].
[74] S. Wiewel, M. Becher, and N. Thuerey, arXiv e-prints , arXiv:1802.10123 (2018), arXiv:1802.10123 [cs.LG].
[75] E. Weinan, J. Han, and A. Jentzen, Communications in Mathematics and Statistics 5, 349 (2017).
[76] J. Berg and K. Nyström, Neurocomputing 317, 28 (2018).
[77] T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud, arXiv preprint arXiv:1806.07366 (2018).
[78] P. MÃ¶sta, C. D. Ott, D. Radice, L. F. Roberts, E. Schnetter,
and R. Haas, Nature 528, 376 (2015),
arXiv:1512.00838 [astro-ph.HE].
[79] K. Kiuchi, K. Kyutoku, Y. Sekiguchi, and M. Shibata, Phys. Rev. D97, 124039 (2018), arXiv:1710.01311 [astro-ph.HE].
[80] D. Radice, C. D. Ott, E. Abdikamalov, S. M. Couch, R. Haas, and E. Schnetter, Astrophys. J. 820, 76 (2016),
arXiv:1510.05022 [astro-ph.HE].
[81] B. Giacomazzo, J. Zrake, P. Duffell, A. I. MacFadyen,
and R. Perna, Astrophys. J. 809, 39 (2015),
arXiv:1410.0013 [astro-ph.HE].
[82] B. D. Metzger and E. Berger, Astrophys. J. 746, 48 (2012), arXiv:1108.6056 [astro-ph.HE].
[83] J. Barnes and D. Kasen, Astrophys. J. 775, 18 (2013), arXiv:1303.5787 [astro-ph.HE].
[84] D. Kasen, N. R. Badnell, and J. Barnes, Astrophys. J. 774, 25 (2013), arXiv:1303.5788 [astro-ph.HE].
[85] S. Rosswog, International Journal of Modern Physics D 24, 1530012-52 (2015), arXiv:1501.02081 [astro-ph.HE].
[86] V. A. Villar, J. Guillochon, E. Berger, B. D. Metzger, P. S. Cowperthwaite, M. Nicholl, K. D. Alexander, P. K. Blanchard, R. Chornock, T. Eftekhari, W. Fong, R. Margutti, and P. K. G. Williams, Astrophys. J. 851, L21 (2017),
arXiv:1710.11576 [astro-ph.HE].
[87] B. D. Metzger, arXiv e-prints (2017), arXiv:1710.05931 [astro-ph.HE].
[88] M. Tanaka, D. Kato, G. Gaigalas, P. Rynkun, L. Radžiūtė, S. Wanajo, Y. Sekiguchi, N. Nakamura, H. Tanuma, I. Murakami, and H. A. Sakaue, The Astrophysical Journal 852, 109 (2018).
[89] P. S. Cowperthwaite et al., The Astrophysical Journal Letters 848, L17 (2017).
[90] R. Kessler, J. Marriner, M. Childress, R. Covarrubias, C. B. D’Andrea, D. A. Finley, J. Fischer, R. J. Foley, D. Goldstein, R. R. Gupta, K. Kuehn, M. Marcha, R. C. Nichol, A. Papadopoulos, M. Sako, D. Scolnic, M. Smith, M. Sullivan, W. Wester, F. Yuan, T. Abbott, F. B. Abdalla, S. Allam, A. Benoit-Lévy, G. M. Bernstein, E. Bertin,
D. Brooks, A. Carnero Rosell, M. Carrasco Kind, F. J. Castander, M. Crocce, L. N. da Costa, S. Desai, H. T.
Diehl, T. F. Eifler, A. Fausti Neto, B. Flaugher, J. Frieman, D. W. Gerdes, D. Gruen, R. A. Gruendl, K. Honscheid, D. J. James, N. Kuropatkin, T. S. Li, M. A. G. Maia, J. L. Marshall, P. Martini, C. J. Miller, R. Miquel,
B. Nord, R. Ogando, A. A. Plazas, K. Reil, A. K. Romer, A. Roodman, E. Sanchez, I. Sevilla- Noarbe, R. C. Smith,
M. Soares-Santos, F. Sobreira, G. Tarle, J. Thaler, R. C. Thomas, D. Tucker, A. R. Walker, and DES Collaboration,
The Astrophysical Journal 150, 172 (2015), arXiv:1507.05137 [astro-ph.IM].
[91] D. A. Goldstein, C. B. D’Andrea, J. A. Fischer, R. J. Foley, R. R. Gupta, R. Kessler, A. G. Kim, R. C. Nichol, P. E.
Nugent, A. Papadopoulos, M. Sako, M. Smith, M. Sullivan, R. C. Thomas, W. Wester, R. C. Wolf, F. B. Abdalla,
M. Banerji, A. Benoit-Lévy, E. Bertin, D. Brooks, A. Carnero Rosell, F. J. Castander, L. N. da Costa, R. Covarrubias,
D. L. DePoy, S. Desai, H. T. Diehl, P. Doel, T. F. Eifler, A. Fausti Neto, D. A. Finley, B. Flaugher, P. Fosalba, J. Frieman,
D. Gerdes, D. Gruen, R. A. Gruendl, D. James, K. Kuehn, N. Kuropatkin, O. Lahav, T. S. Li, M. A. G. Maia, M. Makler,
M. March, J. L. Marshall, P. Martini, K. W. Merritt, R. Miquel, B. Nord, R. Ogando, A. A. Plazas, A. K. Romer,
A. Roodman, E. Sanchez, V. Scarpine, M. Schubnell, I. Sevilla-Noarbe, R. C. Smith, M. Soares-Santos, F. Sobreira,
E. Suchyta, M. E. C. Swanson, G. Tarle, J. Thaler, and A. R. Walker, The Astrophysical Journal 150, 82 (2015),
arXiv:1504.02936 [astro-ph.IM].
[92] S. A. Hughes and D. E. Holz, Classical and Quantum Gravity 20, S65 (2003), arXiv:astro-ph/0212218 [astro-ph].
[93] S. M. Feeney, H. V. Peiris, A. R. Williamson, S. M. Nissanke, D. J. Mortlock, J. Alsing, and D. Scolnic, arXiv e-prints
(2018), arXiv:1802.03404.
[94] H.-Y. Chen, M. Fishbach, and D. E. Holz, Nature (London) 562, 545 (2018), arXiv:1712.06531 [astro-ph.CO].
[95] D. J. Mortlock, S. M. Feeney, H. V. Peiris, A. R. Williamson, and S. M. Nissanke, arXiv e-prints , arXiv:1811.11723
(2018), arXiv:1811.11723 [astro-ph.CO].
[96] A. Khan, E. A. Huerta, S. Wang,
and R. Gruendl, arXiv e-prints , arXiv:1812.02183 (2018),
arXiv:1812.02183 [astro-ph.IM].
[97] H. Domı́nguez Sánchez, M. Huertas-Company, M. Bernardi, D. Tuccillo, and J. L. Fischer, MNRAS 476, 3661 (2018).
[98] H.
Domı́nguez
Sánchez,
Huertas-Company,
et al.,
ArXiv
e-prints
,
arXiv:1807.00807
(2018),
arXiv:1807.00807 [astro-ph.GA].
[99] A. J. Levan et al., The Astrophysical Journal Letters 848, L28 (2017).
[100] K. P. Mooley, E. Nakar, K. Hotokezaka, G. Hallinan, A. Corsi, D. A. Frail, A. Horesh, T. Murphy, E. Lenc, D. L.
Kaplan, K. de, D. Dobie, P. Chandra, A. Deller, O. Gottlieb, M. M. Kasliwal, S. R. Kulkarni, S. T. Myers, S. Nissanke,
T. Piran, C. Lynch, V. Bhalerao, S. Bourke, K. W. Bannister, and L. P. Singer, Nature (London) 554, 207 (2018),
arXiv:1711.11573 [astro-ph.HE].
[101] M. R. Drout, A. L. Piro, B. J. Shappee, C. D. Kilpatrick, J. D. Simon, C. Contreras, D. A. Coulter, R. J. Foley,
M. R. Siebert, N. Morrell, K. Boutsia, F. Di Mille, T. W.-S. Holoien, D. Kasen, J. A. Kollmeier, B. F. Madore, A. J.
Monson, A. Murguia-Berthier, Y.-C. Pan, J. X. Prochaska, E. Ramirez-Ruiz, A. Rest, C. Adams, K. Alatalo, E. Bañados,

14

[102]
[103]
[104]

[105]
[106]

[107]
[108]
[109]

[110]

[111]
[112]

[113]
[114]
[115]
[116]
[117]
[118]
[119]
[120]
[121]
[122]
[123]
[124]
[125]
[126]

J. Baughman, T. C. Beers, R. A. Bernstein, T. Bitsakis, A. Campillay, T. T. Hansen, C. R. Higgs, A. P. Ji, G. Maravelias,
J. L. Marshall, C. Moni Bidin, J. L. Prieto, K. C. Rasmussen, C. Rojas-Bravo, A. L. Strom, N. Ulloa, J. Vargas-González,
Z. Wan, and D. D. Whitten, Science 358, 1570 (2017), arXiv:1710.05443 [astro-ph.HE].
B. P. Abbott, R. Abbott, T. D. Abbott, F. Acernese, K. Ackley, C. Adams, T. Adams, P. Addesso, R. X. Adhikari, V. B.
Adya, and et al., Astrophys. J. Lett 848, L12 (2017), arXiv:1710.05833 [astro-ph.HE].
I.
Andreoni,
C.
Jacobs,
S.
Hegarty,
T.
Pritchard,
J.
Cooke,
and
S.
Ryder,
Publications of the Astronomical Society of Australia 34, e037 (2017), arXiv:1708.04629 [astro-ph.IM].
D. A. Goldstein, C. B. D’Andrea, J. A. Fischer, R. J. Foley, R. R. Gupta, R. Kessler, A. G. Kim, R. C. Nichol, P. E.
Nugent, A. Papadopoulos, M. Sako, M. Smith, M. Sullivan, R. C. Thomas, W. Wester, R. C. Wolf, F. B. Abdalla,
M. Banerji, A. Benoit-Lévy, E. Bertin, D. Brooks, A. Carnero Rosell, F. J. Castander, L. N. da Costa, R. Covarrubias,
D. L. DePoy, S. Desai, H. T. Diehl, P. Doel, T. F. Eifler, A. Fausti Neto, D. A. Finley, B. Flaugher, P. Fosalba, J. Frieman,
D. Gerdes, D. Gruen, R. A. Gruendl, D. James, K. Kuehn, N. Kuropatkin, O. Lahav, T. S. Li, M. A. G. Maia, M. Makler,
M. March, J. L. Marshall, P. Martini, K. W. Merritt, R. Miquel, B. Nord, R. Ogando, A. A. Plazas, A. K. Romer,
A. Roodman, E. Sanchez, V. Scarpine, M. Schubnell, I. Sevilla-Noarbe, R. C. Smith, M. Soares-Santos, F. Sobreira,
E. Suchyta, M. E. C. Swanson, G. Tarle, J. Thaler, and A. R. Walker, The Astronomical Journal 150, 82 (2015),
arXiv:1504.02936 [astro-ph.IM].
N. Sedaghat and A. Mahabal, MNRAS 476, 5365 (2018), arXiv:1710.01422 [astro-ph.IM].
R. Kessler, B. Bassett, P. Belov, V. Bhatnagar, H. Campbell, A. Conley, J. A. Frieman, A. Glazov, S. GonzálezGaitán, R. Hlozek, S. Jha, S. Kuhlmann, M. Kunz, H. Lampeitl, A. Mahabal, J. Newling, R. C. Nichol, D. Parkinson,
N. Sajeeth Philip, D. Poznanski, J. W. Richards, S. A. Rodney, M. Sako, D. P. Schneider, M. Smith, M. Stritzinger, and
M. Varughese, Publications of the Astronomical Society of the Pacific 122, 1415 (2010), arXiv:1008.1024 [astro-ph.CO].
M.
Lochner,
J.
D.
McEwen,
H.
V.
Peiris,
O.
Lahav,
and
M.
K.
Winter,
The Astrophysical Journal Supplement Series 225, 31 (2016), arXiv:1603.00882 [astro-ph.IM].
T. Charnock and A. Moss, Astrophys. J. Lett 837, L28 (2017), arXiv:1606.07442 [astro-ph.IM].
H. Campbell, C. B. D’Andrea, R. C. Nichol, M. Sako, M. Smith, H. Lampeitl, M. D. Olmstead, B. Bassett, R. Biswas,
P. Brown, D. Cinabro, K. S. Dawson, B. Dilday, R. J. Foley, J. A. Frieman, P. Garnavich, R. Hlozek, S. W. Jha,
S. Kuhlmann, M. Kunz, J. Marriner, R. Miquel, M. Richmond, A. Riess, D. P. Schneider, J. Sollerman, M. Taylor, and
G.-B. Zhao, Astrophys. J. 763, 88 (2013), arXiv:1211.4480 [astro-ph.CO].
D. O. Jones, D. M. Scolnic, A. G. Riess, A. Rest, R. P. Kirshner, E. Berger, R. Kessler, Y.-C. Pan, R. J. Foley, R. Chornock,
C. A. Ortega, P. J. Challis, W. S. Burgett, K. C. Chambers, P. W. Draper, H. Flewelling, M. E. Huber, N. Kaiser, R.-P.
Kudritzki, N. Metcalfe, J. Tonry, R. J. Wainscoat, C. Waters, E. E. E. Gall, R. Kotak, M. McCrum, S. J. Smartt, and
K. W. Smith, Astrophys. J. 857, 51 (2018), arXiv:1710.00846.
S. Rosswog, U. Feindt, O. Korobkin, M.-R. Wu, J. Sollerman, A. Goobar,
and G. Martinez-Pinedo,
Classical and Quantum Gravity 34, 104001 (2017), arXiv:1611.09822 [astro-ph.HE].
D. Scolnic, R. Kessler, D. Brout, P. S. Cowperthwaite, M. Soares-Santos, J. Annis, K. Herner, H.-Y. Chen, M. Sako,
Z. Doctor, R. E. Butler, A. Palmese, H. T. Diehl, J. Frieman, D. E. Holz, E. Berger, R. Chornock, V. A. Villar,
M. Nicholl, R. Biswas, R. Hounsell, R. J. Foley, J. Metzger, A. Rest, J. Garcı́a-Bellido, A. Möller, P. Nugent, T. M. C.
Abbott, F. B. Abdalla, S. Allam, K. Bechtol, A. Benoit-Lévy, E. Bertin, D. Brooks, E. Buckley-Geer, A. Carnero
Rosell, M. Carrasco Kind, J. Carretero, F. J. Castander, C. E. Cunha, C. B. D’Andrea, L. N. da Costa, C. Davis,
P. Doel, A. Drlica-Wagner, T. F. Eifler, B. Flaugher, P. Fosalba, E. Gaztanaga, D. W. Gerdes, D. Gruen, R. A. Gruendl,
J. Gschwend, G. Gutierrez, W. G. Hartley, K. Honscheid, D. J. James, M. W. G. Johnson, M. D. Johnson, E. Krause,
K. Kuehn, S. Kuhlmann, O. Lahav, T. S. Li, M. Lima, M. A. G. Maia, M. March, J. L. Marshall, F. Menanteau,
R. Miquel, E. Neilsen, A. A. Plazas, E. Sanchez, V. Scarpine, M. Schubnell, I. Sevilla-Noarbe, M. Smith, R. C. Smith,
F. Sobreira, E. Suchyta, M. E. C. Swanson, G. Tarle, R. C. Thomas, D. L. Tucker, A. R. Walker, and DES Collaboration,
Astrophys. J. Lett 852, L3 (2018), arXiv:1710.05845 [astro-ph.IM].
P. S. Cowperthwaite, V. A. Villar, D. M. Scolnic, and E. Berger, arXiv e-prints (2018), arXiv:1811.03098 [astro-ph.HE].
C. N. Setzer, R. Biswas, H. V. Peiris, S. Rosswog, O. Korobkin, and R. T. Wollaeger, arXiv e-prints , arXiv:1812.10492
(2018), arXiv:1812.10492 [astro-ph.IM].
N. Choma, F. Monti, L. Gerhardt, T. Palczewski, Z. Ronaghi, W. Bhimji, M. M. Bronstein, S. R. Klein, J. Bruna, et al.,
arXiv preprint arXiv:1809.06166 (2018).
H. Ismail Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller, arXiv e-prints , arXiv:1809.04356 (2018),
arXiv:1809.04356 [cs.LG].
D. George and E. A. Huerta, Phys. Rev. D 97, 044039 (2018), arXiv:1701.00008 [astro-ph.IM].
D. George and E. A. Huerta, Physics Letters B 778, 64 (2018), arXiv:1711.03121 [gr-qc].
H. Shen, D. George, E. A. Huerta, and Z. Zhao, ArXiv e-prints (2017), arXiv:1711.09919 [gr-qc].
W. Wei and E. A. Huerta, arXiv e-prints , arXiv:1901.00869 (2019), arXiv:1901.00869 [gr-qc].
A. Rebei, E. A. Huerta, S. Wang, S. Habib, R. Haas, D. Johnson, and D. George, arXiv e-prints , arXiv:1807.09787
(2018), arXiv:1807.09787 [gr-qc].
D. George, H. Shen, and E. A. Huerta, Phys. Rev. D 97, 101501 (2018).
A. J. K. Chua, C. R. Galley, and M. Vallisneri, ArXiv e-prints (2018), arXiv:1811.05491 [astro-ph.IM].
H. Gabbard, M. Williams, F. Hayes,
and C. Messenger, Physical Review Letters 120, 141103 (2018),
arXiv:1712.06041 [astro-ph.IM].
X. Fan, J. Li, X. Li, Y. Zhong, and J. Cao, ArXiv e-prints (2018), arXiv:1811.01380 [astro-ph.IM].
J. A. González and F. S. Guzmán, Phys. Rev. D 97, 063001 (2018), arXiv:1803.06060 [astro-ph.HE].

15
[127]
[128]
[129]
[130]

[131]
[132]
[133]
[134]
[135]
[136]
[137]
[138]
[139]
[140]
[141]
[142]
[143]

[144]
[145]
[146]
[147]
[148]
[149]
[150]
[151]

[152]

Y. Fujimoto, K. Fukushima, and K. Murase, Phys. Rev. D 98, 023019 (2018), arXiv:1711.06748 [nucl-th].
X. Li, W. Yu, and X. Fan, ArXiv e-prints (2017), arXiv:1712.00356 [astro-ph.IM].
H. Nakano et al., ArXiv e-prints (2018), arXiv:1811.06443 [gr-qc].
J. T. Springenberg, A. Klein, S. Falkner, and F. Hutter, in Advances in Neural Information Processing Systems 29 ,
edited by D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (Curran Associates, Inc., 2016) pp.
4134–4142.
Y. Gal and Z. Ghahramani, in international conference on machine learning (2016) pp. 1050–1059.
Y. Gal and Z. Ghahramani, in Proc. ICLR workshop track (2015).
D. George, H. Shen, and E. A. Huerta, ArXiv e-prints (2017), arXiv:1711.07468 [astro-ph.IM].
M. Abadi, A. Agarwal, et al., ArXiv e-prints (2016), arXiv:1603.04467 [cs.DC].
A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, in
NIPS-W (2017).
Comet User Guide, https://portal.xsede.org/sdsc-comet.
Bridges User Guide, https://www.psc.edu/bridges/user-guide/gpu-use.
Frontera, https://www.tacc.utexas.edu/systems/frontera.
E. A. Huerta, R. Haas, S. Jha, M. Neubauer, and D. S. Katz, ArXiv e-prints (2018), arXiv:1810.03056 [cs.DC].
E. A. Huerta, R. Haas, E. Fajardo, D. S. Katz, S. Anderson, P. Couvares, J. Willis, T. Bouvet, J. Enos, W. T. C.
Kramer, H. W. Leong, and D. Wheeler, in 2017 IEEE 13th International Conference on e-Science (e-Science) (2017)
pp. 335–344.
D. Weitzel, B. Bockelman, D. A. Brown, P. Couvares, F. Würthwein, and E. Fajardo Hernandez, arXiv e-prints ,
arXiv:1705.06202 (2017), arXiv:1705.06202 [cs.DC].
M. Belkin, R. Haas, G. W. Arnold, H. W. Leong, E. A. Huerta, D. Lesny,
and M. Neubauer, in
Proceedings of the Practice and Experience on Advanced Research Computing , PEARC ’18 (ACM, New York, NY, USA,
2018) pp. 43:1–43:8.
R. A. Street, M. Bowman, E. S. Saunders, and T. Boroson, in Software and Cyberinfrastructure for Astronomy V ,
Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series, Vol. 10707 (2018) p. 1070711,
arXiv:1806.09557 [astro-ph.IM].
Astropy, “The astropy project,” (2018), http://www.astropy.org/.
yt Community, “yt project,” (2018), http://www.astropy.org/.
IceCube, “Icecube open source software,” (2018), https://github.com/IceCubeOpenSource.
F. C. Porter, arXiv e-prints , arXiv:0804.0380 (2008), arXiv:0804.0380 [physics.data-an].
Software Carpentry, “Teaching basic lab skills for research computing,” (2018), https://software-carpentry.org.
P. Elmer, M. Neubauer, and M. D. Sokoloff, arXiv e-prints , arXiv:1712.06592 (2017), arXiv:1712.06592 [physics.comp-ph].
J. Albrecht et al., arXiv e-prints , arXiv:1712.06982 (2017), arXiv:1712.06982 [physics.comp-ph].
G. Allen, W. Anderson, E. Blaufuss, J. S. Bloom, P. Brady, S. Burke-Spolaor, S. B. Cenko, A. Connolly, P. Couvares,
D. Fox, A. Gal-Yam, S. Gezari, A. Goodman, D. Grant, P. Groot, J. Guillochon, C. Hanna, D. W. Hogg, K. HolleyBockelmann, D. A. Howell, D. Kaplan, E. Katsavounidis, M. Kowalski, L. Lehner, D. Muthukrishna, G. Narayan, J. E. G.
Peek, A. Saha, P. Shawhan, and I. Taboada, arXiv e-prints , arXiv:1807.04780 (2018), arXiv:1807.04780 [astro-ph.IM].
DESC, “Dark energy science collaboration,” (2018), http://lsst-desc.org/.

