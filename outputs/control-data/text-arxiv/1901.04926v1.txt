Unicast-Uniprior Index Coding Problems:
Minrank and Criticality
Niranjana Ambadi

arXiv:1901.04926v1 [cs.IT] 15 Jan 2019

Department of Electrical Communication Engineering,
Indian Institute of Science, Bangalore, India 560012.
Email: ambadi@iisc.ac.in.

Abstract—An index coding problem is called unicast-uniprior
when each receiver demands a unique subset of messages while
knowing another unique subset of messages apriori as sideinformation. In this work, we give an algorithm to compute
the minrank of a unicast-uniprior problem. The proposed
algorithm greatly simplifies the computation of minrank for
unicast-uniprior problem settings, over the existing method whose
complexity is exponential in the number of messages. First, we
establish some properties that are exclusive to the fitting matrix
of a unicast-uniprior problem. Identification of the critical side
information bits follows as a result of this exercise. Further, these
properties are used to lay down the algorithm that computes the
minrank.
Index Terms—Index Coding, Minrank, Unicast, Uniprior.

effect, the notion of a binary matrix called fitting matrix was
introduced. A matrix A = [aij ]n×n fits G if:


1,
aij = 0,


x,

if i = j; ,
there is no edge from vi to vj in G,
x ∈ {0, 1}, otherwise.

(1)

Let κ(i) denote the out-degree of the vertex vi in G. Any
n
P

κ(i)

of the z = 2i=1
matrices defined according to (1) can
be regarded as a fitting matrix (see Example 1). Minrank is
defined as the minimum among the ranks of all such fitting
matrices.

I. I NTRODUCTION
A. Background
The problem of index coding with side information was
introduced by Birk et al. [1], [2]. It was motivated by applications in which a server broadcasts a set of messages to
a set of clients such as audio and video-on-demand, daily
newspaper delivery etc. In practical scenarios, the clients fail
to receive all the transmissions broadcast by the server. This
can be due to various reasons like erraneous forward channel,
limited storage at the clients or path loss effects in the case of a
wireless broadcast channel. Through a slow backward channel,
the clients communicate to the server the current messages
in their caches. The server then exploits this information and
encodes the pending demands with the objective of minimizing
the number of further transmissions.
Index coding is a special case of the well known network
coding problem [3], [4]. Rouayheb et al. [5] established
a reduction that maps any instance of the network coding
problem to a corresponding instance of the index coding
problem.
Very often, an index coding problem where the source
has n messages and each message is desired by exactly one
receiver is specified by a simple directed graph on n nodes,
called the side-information graph, G(V, E). Each node vi ∈ V
corresponds to a demanded message xi and there exists an
edge eij ∈ E from vi → vj if and only if the receiver
Dj knows the message xi as side-information (see Example
1). Further, a graph functional called minrank was shown to
characterize the minimum length of index codes [2]. To this

minrank , min{rk2 (A); A fits G},
where rk2 (A) denotes the rank over Galois Field of two
elements, GF (2).
Finding the minrank requires computation of ranks of each
of the z fitting matrices over GF (2). Using the traditional
method of Gaussian elimination to compute ranks, the complexity [8], [9] of computing the minrank becomes zO(n2 ).
This was shown to be NP-hard [15], [16].
However, for some very specific problem settings minrank
can be computed in polynomial time [7], [10]. In [10], the
notion of critical graphs in index coding was introduced and
it was stated that certain side information bits can be called
critical, if removing the corresponding edges in the graph G
strictly reduces the capacity region (see Section II-A) of the
index coding problem. Critical graphs are defined as minimal
graphs that support a given set of rates for a given index
coding problem [10]. This minimality is defined in terms of
the containment of the edge set.
In many broadcast scenarios, the network topology plays a
crucial role in determining the side-information at receivers
[18]. One might often be able to predict the side-information
pattern at each receiver based on the network topology. Whenever the receivers have limited storage, it becomes important
to store only those side-information bits which are critical so
that index coding can be enabled that do not compromise
on optimal receiver rates. Thus, identifying critical sideinformation is useful in computing the minrank of an index
coding problem.

B. System Model
Ong and Ho [17] did a categorization of index coding
problems (refer Fig. 1) and we use it as the reference for
problem formulation.
There is a server with n messages, M = {x1 , x2 , . . . , xn }
that are to be transmitted to N receivers, D1 , D2 , . . . , DN .
Each receiver Di is identified by the ordered pair (Wi , Ai ),
where Wi ⊆ M is the set of messages demanded by Di and
Ai ⊂ M is the set of messages known apriori to Di as sideinformation.
The index coding problem is said to be unicast if Wi ∩Wj =
φ; ∀i 6= j, i, j ∈ [N ] 1 . The problem is called uniprior when
Ai ∩ Aj = φ; ∀i 6= j, i, j ∈ [N ]. Any problem that is both
unicast and uniprior is called a unicast-uniprior problem.
side information at clients
multiprior (general case)

uniprior

single
unicast
unicast

multicast (general case)

information flow from sender

single
uniprior

Fig. 1: Categorization of index coding problems; the shaded
part denotes the class of unicast-uniprior problems that are
dealt with in this paper.
C. Contributions
In this work, the unicast-uniprior problem is studied.
• The notion of a critical fitting matrix for an index coding
problem, is introduced.
• A new graph structure called the side-information supergraph is introduced in order to study the special
properties associated with the side-information graph of
a unicast-uniprior problem.
• The properties of fitting matrix of a unicast-uniprior
problem are stated and proved through a series of lemmas.
• The critical side information for a unicast-uniprior problem is identified using critical graphs and the corresponding fitting matrices.
• A new algorithm to compute the minrank is given. This
algorithm greatly reduces the number of computations
over the existing brute force technique.
1 For

any positive integer K, the set {1, 2, . . . , K} is denoted by [K].

D. Organization
The remainder of the paper is organized as follows: The
necessary background and terminologies are explained briefly
in Section II. The notions of a critical fitting matrix and a sideinformation supergraph are introduced in Sections III-A and
III-B respectively. Section III-C establishes certain properties
of a fitting matrix of a unicast-uniprior problem. Section
III-D contains results concerning the critical graphs and sideinformation of a unicast-uniprior problem. Finally, the polynomial time algorithm to compute minrank is presented in
Section IV. Some illustrative examples for minrank computation are given in Section V. Concluding remarks constitute
Section VI.
II. BACKGROUND
This section provides an overview of some existing results
and terminologies. These would be useful in establishing the
results in sections to follow.
A. Linear Index Code, Capacity Region and
Symmetric Capacity
Definition 1 (Scalar Linear Index Code [13]). When S is
a finite field, an (S, l, R) index code is scalar linear if, for
the source with n messages, M = {x1 , x2 , . . . , xn }, the
transmitted symbol sequence is given by,
n
X
Vj xj .
Sl =
j=1

The l × 1 vector Vj is referred to as the precoding vector (or
beamforming vector) for the message xj .
In general, the message xi is a random variable uniformly
lR
distributed over the set {1, 2, . . . , |S| i }, where Ri is the rate
for message xi , ∀i ∈ [n]. If the message xi consists of Pi
symbols, i.e., xi ∈ S Pi , the code is said to achieve a rate
Ri = Pli for receiver Di . The rate Ri of a receiver Di is the
normalized amount of information transmitted to it.
Identifying the set of all achievable rate tuples, i.e., the
capacity region, still remains an open problem. However,
for several special classes of index coding, for example the
symmetric cases when all the rates are equal or when G
has certain structure [2], the capacity region has been fully
characterized.
Definition 2 (Symmetric Rate [13]). Let R be the rate tuple
(R1 , R2 , . . . , Rn ). When all messages have the same rate, say
R, then R is called the Symmetric Rate.
An index code can be scalar (Pi = 1; for all i ∈ [n]), vector
(Pi > 1 for at least one i ∈ [n]), linear (when the encoding
function is linear) or non-linear (when the encoding function
is non-linear).
In this paper, we deal only with scalar linear index coding,
which means that each message consists of only one symbol,
i.e., Pi = 1; ∀i ∈ [n]. This implies R = ( 1l , 1l , . . . , 1l ) and
the encoding function is linear, with S being a finite field.
Clearly, the best rate (symmetric capacity) is achieved when
l = minrank.

B. Critical Graphs and Index Coding
Definition 3 (Critical Graphs [10]). Given a fixed set of rates,
let G denote the set of all graphs that support the rates. A
graph is said to be critical (or edge critical) if
1) it belongs to G, and
2) deletion of any edge from the graph makes it to fall
outside G.
Definition 4 (Strongly Connected Graph [12]). A directed
graph is strongly connected if there is a path between all pairs
of vertices [12].
Definition 5 (Union of Two Disjoint Graphs [11]). The union
of G = (V, E) and G′ = (V ′ , E ′ ) is defined as G ∪ G′ =
(V ∪ V ′ , E ∪ E ′ ).
Definition 6 (Union of strongly connected subgraphs [11]).
Graph G is the union of strongly connected subgraphs if there
exists a set of disjoint graphs G1 , G2 , · · · , Gk such that
1) Gi isSstrongly connected, and
2) G = Gi .
i

Definition 7 (Hamiltonian Cycle [12]). A Hamiltonian path
(or traceable path) is a path in a directed/undirected graph
that visits each vertex exactly once. A Hamiltonian cycle is a
Hamiltonian path that is a cycle.

the receivers and thus can be excluded while constructing an
index code.
Note that the rows of A correspond to receiver indices and
columns correspond to message indices.
A. Critical Fitting Matrix
Matrix theory says that if any one element of a 0 − 1 matrix
is flipped, one of the following happens to its rank i) increases
by one, ii) decreases by one, or iii) remains the same.
Let A′ be the matrix obtained by changing any non-diagonal
1 of the fitting matrix A whose rank equals minrank. By
definition, A′ is also a fitting matrix. Because the rank of
A is the minimum among all the fitting matrices, that rank of
A′ cannot be lesser than the rank of A. Thus,
rank(A′ ) ∈ {rank(A), rank(A) + 1}.
A subset of these fitting matrices whose rank equals minrank
shall be called as critical fitting matrices.
Definition 10 (Critical Fitting Matrix). Given a fitting matrix
A = [aij ]n×n whose rank is equal to minrank. The fitting
matrix A is called a critical fitting matrix if and only if
changing any aij = 1; i 6= j to 0 results in a matrix A′ whose
rank is strictly greater than the rank of A.
B. Side-information supergraph

Definition 8 (Unicycle [11]). A graph G(V, E) is referred to
as a unicycle if the set of edges E of the graph is a Hamiltonian
cycle of G.

In order to fully characterize the unicast-uniprior problem
a new graph structure called the side-information supergraph
is introduced.

Definition 9 (Vertex Induced Subgraph [12]). A vertexinduced subgraph is a subset of the vertices of a graph G
together with any edges whose endpoints are both in this
subset.

Definition 11 (Side-information supergraph). The sideinformation supergraph G(V, E) of a unicast-uniprior problem
is defined as follows:
1) A vertex vi ∈ V corresponds to the destination demanding the message xi for all i ∈ [n].
2) The vertices derived by splitting a demand set Wi of the
original unicast-uniprior problem are grouped into one
supernode Si for all i ∈ [N ].
3) There exists a directed edge from the supernode Sj ,
j ∈ [N ] to the vertex vi , i ∈ [n] iff the receiver Dj ,
(Wi , Ai ) knows the message xi as side-information.

III. R ESULTS
This section discusses the problem of finding the minrank of
a unicast-uniprior problem in detail. The discussions presented
in this section lead to a polynomial time algorithm (Section
IV) for computing minrank.
Consider a unicast-uniprior problem with N receivers. We
convert this problem to its single unicast equivalent by splitting
the receivers [6].
Let An×n be a fitting matrix of this single unicast problem
with n receivers and n messages, in its general form (with ‘x’
denoting side-information bits). Further,
n=

N
X
i=1

|Wi | =

N
X

|Ai |.

(2)

i=1

This is because:(i) If |Wi | >

N
P

|Ai | there exist de-

1) Properties of the side-information supergraph: We note
the following characteristics of the side-information supergraph G(V, E) of a unicast-uniprior problem:
• Every vertex vi ∈ V , i ∈ [n] has at least one incoming
edge from a supernode. This is because in the unicast uniprior problem, we have total number of messages, n =
N
N
P
P
|Ai |. So each message xi demanded by a
|Wi | =
i=1

i=1

mands which are not present as side-information at any of
the receivers. These messages must be sent as independent
transmissions and thus can be excluded while constructing
N
P
|Ai |, there exist as sidean index code, (ii) If |Wi | <
i=1

information, messages that are no longer demanded by any of

•

i=1

receiver Dk ; k ∈ [N ] is known apriori to some receiver
Dj ; j 6= k. Hence, there is one incoming edge to any vi
from some supernode Sj .
Every vi ∈ V , i ∈ [n] has only one incoming edge from
a supernode. This is because of the uniprior nature of
the problem. More than one incoming edge to a vertex
vk (k ∈ [n]), implies more than one receiver Di ; i ∈ [N ]

knows the message xk which is not possible as Ai ∩Aj =
φ, ∀i 6= j; i, j ∈ [N ].
Example 1. A sample unicast-uniprior problem is described
in Table I.
Wi
Ai

x1 , x2
x4

x3 , x4
x5 , x1

x5
x2 , x3

The single-unicast equivalent of this problem obtained by
splitting the N = 3 receivers into n = 5 is shown in Table II.
x1
x4

x2
x4

x3
x5 , x1

x4
x5 , x1

Proof. The proof is given in Appendix B.



Definition 12. A set of linearly dependent rows is said to be
minimally dependent if every proper subset of it is linearly
independent.

TABLE I: Sample unicast-uniprior problem

Wi
Ai

Corollary 1. The side-information graph G of a single unicast problem that is derived from a unicast-uniprior problem
cannot have cliques with three or more vertices.

x5
x2 , x3

Proposition 2. Delete every pair of identical rows from An×n
to obtain A′m×n . Suppose that the rank of A′ is r′ . Then there
are at least m − r′ minimally dependent sets of rows in A′
and at least n − r minimally dependent sets of rows in A.
Proof. The proof is given in Appendix C.



TABLE II: Single-unicast equivalent of the unicast- uniprior
problem in Table I

Note that the receiver demanding the message xj corresponds to the j th row rj of A, where j ∈ [n].

The side-information graph G for this problem is shown in
Fig. 2.

Proposition 3. The message demanded by the j th receiver, xj
is present as side-information in one and only one RWi where
i 6= j.

1

3

2

4

Proof. The proof directly follows from the uniprior nature of
the problem.

Lemma 1. Given a set L of l rows of the matrix A. Let it
be comprised of ci rows from RWi , i ∈ [N ]. Suppose that
in some fitting matrix these l rows are minimally dependent.
Then, there always exists a subset S of these rows that are
minimally dependent with at most one row from any RWi , for
a different choice of “x”s in these rows.

5

Proof. The proof is given in Appendix D.
Fig. 2: Side-information graph G, problem in Table II
The matrix A in Fig. 3 is one among the z = 28 = 256
fitting matrices of this problem.
The side-information supergraph G for this problem is
shown in Fig. 4.
Notice that the side-information supergraph G translates to
the side-information graph G when each outgoing edge from
any supernode Si is replaced by |Wi | outgoing edges from the
nodes within Si .
C. On fitting matrices of a unicast-uniprior problem
Let the subset of rows of A that correspond to receivers
obtained by splitting the ith receiver of the original unicastuniprior problem be denoted by RWi . Thus, the n rows of A
are partitioned into N sets of rows {RW1 , RW2 , . . . , RWN }.
Henceforth, an arbitrary fitting matrix shall be denoted by
A, while a fitting matrix in its general form by A.
Proposition 1. Any fitting matrix of a single unicast problem
that was derived from a unicast-uniprior problem cannot have
more than two identical rows.
Proof. The proof is given in Appendix A.





Lemma 2. The vertex induced subgraph Q of G, whose
vertices correspond to the rows of the set S in Lemma 1 forms
a unicycle.
Proof. The proof is given in Appendix E.



D. Identifying the critical side-information
For an index coding problem, the rank of a fitting matrix
that has x = 1 for all the critical side-information bits and
x = 0 for all the non-critical side-information bits, equals its
minrank. When one is interested in computing the minrank,
knowledge of critical graphs naturally helps in narrowing
down the number (z) of fitting matrices whose rank must be
computed.
The following theorem in [11] helps in identifying the
critcial edges in a side-information graph.
Theorem 1. An edge e in the side-information graph G is
critical if it belongs to a vertex induced subgraph of G which
is a unicycle [11].
For the unicast-uniprior problem, identifying critical sideinformation is as good as identifying the unicycles in the
side-information graph G. This is discussed in the following
section.





A=



1
0
x
x
0

0 0
1 0
0 1
0 0
x x



x 0

x 0 



0 x , A = 


1 x 
0 1

1
0
0
0
0

0
1
0
0
0

0
0
1
0
1

1
1
0
1
0

0
0
0
1
1









 , Ac = 





1
0
0
1
0

0
1
0
0
0

0
0
1
0
1

1
0
0
1
0

0
0
1
0
1





.



Fig. 3: For the problem in Example 1, A is the fitting matrix in its general form; A is a fitting matrix; Ac is a critical fitting
matrix.

1

3

S1

S2
2

4

S3
5

Fig. 4: Side-information supergraph G, problem in Table I

E. Translating critical graphs into fitting matrices for a
unicast-uniprior problem
Next, we combine the results in Sections III-C and III-D into
the context of a unicast-uniprior problem and subsequently
develop the algorithm for finding minrank in Section IV.
Using Proposition 1, any fitting matrix of a unicast-uniprior
problem cannot have more than two identical rows. Using
Proposition 3, each of these identical rows belong to RWi
and RWj with i 6= j; i, j ∈ [N ].
Using Proposition 2, it is clear that a fitting matrix has at
least m − r minimally dependent rows. For any critical fitting
matrix r = minrank and hence it is guaranteed to have at
least m − r minimally dependent sets of rows (which is the
guaranteed maximum according to Proposition 2).
The following theorem [10] regarding critical graphs becomes important in identifying the critical fitting matrices for
a unicast-uniprior problem.
Theorem 2. Every critical graph for linear index coding is a
union of strongly connected subgraphs. In particular, removing
the edges not lying on a directed cycle does not change the
capacity region in these cases [10].
IV. A LGORITHM
This section discusses an efficient algorithm to compute the
minrank of a unicast-uniprior problem in polynomial time.
A. Algorithm to compute minrank
1) Let Wmax , max | Wi |.
i=[N ]

2) We know that the (i, i)th element of a fitting matrix
corresponds to the ith wanted message xi . We form an

N × Wmax table T such that the k th row of the table
consists of messages in Wk ; k ∈ [N ]. Clearly, there are

N 
Y
Wmax
|Wi |!
(3)
β=
|Wi |
i=1
ways of filling this table. Let each instance of T be
denoted by Ti ; i ∈ [β].
3) Consider each column of the table as a single unicast
uniprior problem. The wanted message is the table entry
and the side-information is same as the side-information
derived from the unicast-uniprior problem. The minrank
of a single unicast uniprior index coding problem can
be computed in linear time as discussed in [7].
4) Let the minranks along each column in Ti be denoted by
rki1 , rki2 , . . . , rkiWmax . We define the sum of minranks
of the Wmax single unicast uniprior problems as follows:
For i ∈ [β],

Si ,

W
max
X

rkij

(4)

j=1

5) The minrank of the unicast-uniprior problem is given
by:
minrank = min Si .

(5)

B. Proof of correctness
Lemma 3. Let Q be a unicycle in the side-information graph
G. Let the vertices of Q be part of a bigger directed cycle C.
Let SQ and SC denote the sets of rows in a fitting matrix A
that correspond to the vertices of Q and C, respectively. If the
rows in SQ are minimally dependent in A, then the rows in
SC are not minimally dependent and vice versa.
Proof. The proof directly follows from the definition of a
minimally dependent set (Definition 12).

Lemma 4. Let GC be subgraph of G induced by the vertices
of a directed cycle C. The subgraph GC has one or more
disjoint vertex induced subgraphs GQ1 , GQ2 , . . . , GQm that
are unicycles.
Proof. Consider Example 1. The vertices C = (v1 , v4 , v5 , v3 )
form a directed cycle in G. However, there exists two vertex
induced subgraphs of vertices in C which are unicycles, viz.,
Q1 = (v1 , v4 ) and Q2 = (v3 , v5 ).


Theorem 2 states that a critical graph is always a union
of strongly connected subgraphs and removing the edges not
lying on a directed cycle in a critical graph do not change the
capacity region.
Theorem 1 says that an edge is critical if it belongs to a
unicycle. Thus, by keeping just the edges that form unicycles
in G, a critical graph could be constructed, followed by a
critical fitting matrix.
In any fitting matrix of rank r there are necessarily n − r
sets of minimally dependent rows (Proposition 2). Note that
n−r is maximum when r = minrank. Lemma 1 and Lemma
2 together imply that given a minimally dependent set of rows,
there always exist a unicycle Q in G with at most one vertex
(row) from RWi ; i ∈ [N ]. Further, if that minimally dependent
set corresponds to a directed cycle C in G, Lemma 3 implies
that in any fitting matrix where SQ is minimally dependent,
SC is not. This justifies Step 2 of the algorithm where each
column consists of at most one demand from any Wi ; i ∈ [N ].
One unicycle is equivalent to one minimally dependent set
of rows and hence reduces the rank by one. Step 3 of the algorithm computes minrank of single unicast uniprior problems,
using the method in [7], which prunes the corresponding sideinformation graph to obtain the maximum number of disjoint
unicycles.
This justifies Step 4 of the algorithm where summing the
minranks (along each column of the table) is analogous to
taking the count of unicycles after one round of pruning G.
Note that, minimizing the ranks of every column for one Ti
constitute one round of pruning G.
Finally, minimizing the sum of minranks over all Ti ; i ∈ [β]
gives the minrank of the unicast-uniprior problem. Minrank is
obtained when we have pruned G to get the maximum possible
number of disjoint unicycles.
C. Identifying critical side-information from the algorithm
Suppose that Si = minrank. The critical side-information
for the unicast uniprior problem can be characterized using
table Ti . Let C1 , C2 , . . . , CWmax denote the columns of Ti .
1) Along every column Ci ; i ∈ [Wmax ], there is a single
unicast uniprior problem whose rank can be computed
in polynomial time. Let the fitting matrix of minimum
rank for each of these Wmax problems be denoted by
A1 , A2 , . . . , AWmax .
2) Every minimally dependent set of rows in these matrices form unicycles in the side-information graph G.
Construct G′ from G by deleting all the edges other
than those that constitute those unicycles. G′ represents a
critical graph for the concerned unicast-uniprior problem
and the side-information bits corresponding to the edges
in G′ are critical.

Wi
Ai

Example 2. Consider the following unicast-uniprior index
coding problem: There are N = 5 receivers and n = 10
messages. The receiver demands and side-information are
given in Table III.

x3 , x5
x1 , x4

x4 , x6
x5 , x9 , x8

x9
x10 , x7

x7 , x8 , x10
x2 , x6

TABLE III: Unicast-uniprior problem in Example 2
We illustrate the step-by-step execution of the algorithm and
compute the minrank for this problem.
1) Wmax = max |Wi | = 3 .
i=[5]

2) We split the given unicast-uniprior problem into
Wmax = 3 single-unicast uniprior problems. There are
β ways of doing this, where

5 
Y
3
|Wi |! = 1944.
β=
|Wi |
i=1
All of these 1944 ways of splitting can be captured
by filling a 5 × 3 table. First instance out of the 1944
possible splitting is given below:
I
x1
x3
x4
x9
x7

II
x2
x5
x6

III

x10

x8

3) The three single-unicast uniprior problems that are obtained from the above table are as follows:
W1
x1
x3
x4
x9
x7

Problem I
A1
x3
x1 , x4
x5 , x9 , x8
x10 , x7
x2 , x6

Problem II
W2
A2
x2
x3
x5
x1 , x4
x6
x5 , x9 , x8

Problem III
W3
A3

x10

x8

x2 , x6

x2 , x6

4) We evaluate the minranks of the three problems using
the method given in [7]. rk11 = 4, rk12 = 4, rk13 = 1.
S1 = rk11 + rk12 + rk13 = 9
5) Similarly, we find Si values for all z = 1944 ways
of splitting the original problem. After that we get the
minrank as follows:
minrank , min Si = 7.
i=[1944]

A. Analysis for reduction in complexity
We know that calculating minrank by brute force requires
n
P

|κi |

evaluating the F2 rank of 2
matrices and finding the
minimum among them. If Gaussian elimination is used to
compute rank, this method has a computational complexity of
i=1

n
P

|κi |

O(n2 ). In this example instead of evaluating the ranks
2
10
of 2 = 1024 matrices each of order n = 10, we evaluate the
ranks of just 1944 matrices, each of order N = 5 to arrive at
minrank. Further the brute force technique employs gaussian
elimination whose computational complexity is O(n2 ) per
matrix. The proposed technique computes ranks of each of the
i=1

V. A LGORITHM I N ACTION

x1 , x2
x3

1944 matrices in linear polynomial number of computations
the complexity of which is O(N ).
Thus we see huge computational savings with the proposed
method of finding minrank. As n increases, the new method
shall offer a greater computational advantage over the brute
force technique.
Example 3. Consider the problem described in Table IV.
Wi
Ai

x1 , x6 , x10
x2 , x4 , x9

x2 , x4 , x7 , x9
x3 , x5 , x8 , x10

x3 , x5 , x8
x1 , x6 , x7

TABLE IV: Unicast-uniprior problem in Example 3.
The minrank for this problem is 7. If we compute minrank
by brute force, we need to compute the ranks of 2(9+16+9) =
234 ≈ 1.717 × 1010 matrices, each of order 10 × 10. Notice
that the rank over F2 is found by gaussian elimination. When
we employ our algorithm we compute the minranks of β =
13824 single unicast uniprior problems. The ranks are found
in number of computations that is linear in the number of
messages. Simulation result that computed minrank using our
algorithm was found to agree with that obtained from brute
force computations.
Example 4. Consider the problem described in Table V.
Wi
Ai

x1 , x2 , x3 , x4
x5 , x9 , x10 , x11 , x12

x5 , x6 , x7
x1 , x2 , x8

x8 , x9 , x10 , x11 , x12
x3 , x4 , x6 , x7

TABLE V: Unicast-uniprior problem in Example 4.
The minrank for this problem is 8. Brute force computation
of minrank needs computing the ranks of 2(20+9+20) =
249 ≈ 5.629 × 1014 matrices (each of order 12 × 12) by
Gaussian elimination. When our algorithm is employed, we
compute the minranks of β = 864000 single unicast uniprior
problems. Simulation result that computed minrank using our
algorithm was found to agree with that obtained from brute
force computations.
VI. C ONCLUSION
This work solves the unicast-uniprior index coding problem.
Novel ideas like critical fitting matrix and side-information
supergraph were employed to prove the results. A discussion
on the properties of the fitting matrix led to identifying
the critical side-information bits. Also, a novel method to
compute the minrank is provided. The proposed technique
greatly simplifies the existing brute force method of computing
minrank.
A PPENDIX
A. Proof of Proposition 1
Consider a fitting matrix A. Suppose that p > 2 rows of
A are identical. We prove by contradiction that this cannot be
true.
Two or more rows from a given RWi have identical sideinformation and thus have “x”s in the same columns. However

each row has a 1 as its diagonal entry corresponding to the
wanted message.
Suppose p = 3 rows of A are identical. For this to be
true, each row must be taken from a different RWi . Let these
be RWα1 , RWα2 , RWα3 . As the problem is uniprior, Aαm ∩
Aαn = φ whenever αm 6= αn . The columns having “x”s
for one row will always be different from the columns that
have “x”s in another. Due to the unicast-uniprior nature of
the problem, the message demanded by one receiver cannot
be present as side-information at more than one RWi s. So if
we need 3 identical rows, the “x”s in these rows must be
0s. But then, we already have “1”s in the diagonal entries of
all the 3 rows corresponding to their wanted messages. This
contradicts the assumption that p = 3 rows of A are identical.
This implies that for any fitting matrix A, p > 2 rows can
never be identical.
B. Proof of Corollary 1
Assume that the side-information graph has a clique with
p > 2 vertices. Consider the p rows of A that correspond to
these vertices. Let them be rα1 , rα2 , . . . , rαp . These rows will
have 1s in their diagonal entries corresponding to the wanted
messages. In particular, we choose the “x”s in these rows such
that x = 1 in the (αi , j)th entry only if the directed edge
ej,αi is part of the clique; x = 0 otherwise. We know that in a
clique one vertex has incoming edges from the remaining p−1
vertices. The entries corresponding to all other incoming edges
are chosen to be 0s. Thus we get a set of p identical rows (they
have 1s in columns indexed by the p vertices of the clique and
0s elsewhere). This implies the existence of a fitting matrix
with more than two identical rows, contradicting Proposition
1. Hence, our assumption that the side-information graph has
a clique with p > 2 vertices is wrong. Hence proved.
C. Proof of Proposition 2
Given that A′m×n is the matrix obtained after removing
all pairs of identical rows from An×n . When A′ is of rank
r′ , there are r′ linearly independent rows and each of the
remaining m − r′ rows is a linear combination of these
r′ rows. Let the rows of the matrix A′ be re-arranged to
get the matrix B ′ such that the first r′ rows of Bm×n are
linearly independent. Let the rows of Bm×n be represented
by r1 , r2 , . . . , rm . Each of the rows rr′ +1 , rr′ +2 , . . . , rm is some
linear combination of the first r′ rows, r1 , r2 , . . . , rr′ . Thus we
have,
T

r
r
P
P
r
c
r
c
r
r
·
·
·
r
·
·
·
B=
ji i
ji i
1
2
r
i=1

′

i=1

where cji ∈ {0, 1}; i ∈ [r ].
The row rj ; j ∈ {r′ +1, . . . , m} along with the set of linearly
independent rows that add to give rj (the set of rows rj ∪
{ri ; i ∈ [r′ ] where cji = 1}), form a minimally dependent set
of rows. Clearly, we have m − r′ minimally dependent sets
of rows in A′ . For the matrix A, each pair of identical rows
represents one minnimally dependent set. So, A has at least
n − r sets of minimally dependent rows.

D. Proof of Lemma 1
Consider a set of minimally dependent rows R =
{r1 , r2 , . . . , rl } taken from a fitting matrix A. Let them come
from Ω ≤ N different demand sets of the unicast-uniprior
problem. Let these rows form an l × n matrix L. Each of
these rows has 1 in the column corresponding to the message
demanded (diagonal entries of the fitting matrix). Also, each
column shall have an even number of 1s, because if not, the
rows cease to be minimally dependent.
In any row ri , in the ith column, apart from the 1 in the
(i, i)th position (denoting the message demanded), there is
always an odd number of 1s. This is because, if not, the l
rows cease to be minimally dependent. Also, because of the
uniprior nature of the problem these odd number of 1s in any
given column occur in one and only one RWi (Proposition 3).
In those columns whose indices do not correspond to the
messages demanded in the l rows, there is always an even
number of 1s and as the problem is uniprior, these 1s occur in
the rows derived from the same demand set (some RWi , out of
the Ω different ones that constitute the minimally dependent
set of rows). We choose to ignore these columns and focus
on the remaining columns. Thus, we form a subgraph G ′ of
the side-information supergraph G, keeping only those l nodes
corresponding to the messages demanded in the l rows.
In G ′ , locate the vertex that corresponds to the message
demanded in the first row r1 , let it be A1 . WLOG, assume that
it is part of the supernode S1 . There will be an odd number of
1s in the same column as the 1 corresponding to the message
demanded in r1 . This means there is an incoming edge from a
supernode Sj , j ∈ [N ], j 6= 1, to the node A1 . Pick one node
(say A2 ) in Sj . Let the row coresponding to A2 be denoted
by r2 . Since R is minimally dependent, there will be an odd
number of 1s in the same column as the 1 corresponding to the
message demanded in r2 . This means, there is some supernode
Si that has an outgoing edge to node A2 . This Si could be
one among the Ω − 1 supernodes in G ′ other than S2 .
If it is S1 , we have found the subset of rows as r1 , r2 with
x = 1 in r1 in the column corresponding to the message
demanded in r2 and x = 1 in r2 in the column corresponding
to the message demanded in r1 .
If not S1 , it could be some other supernode Sj , j 6= 1, 2.
Again, we can pick one node say A3 in Sj and find that
supernode Si which has an outgoing edge to A3 . But there
are only a finite number of supernodes (S), and we know that
the number of minimally dependent rows (or the number of
vertices in G) l > Ω as we have more than one row from
at least one RWi . Hence, as we proceed, there will be an
incoming edge from some supernode Si which contains a node
that was previously traversed, say Ai , to the node in hand, say
Af .
When this happens, we can start with the supernode Si of
the final node to which there was an incoming edge (Af ),
and traverse in the reverse direction along the path previously
traversed, getting back to Af , forming a cycle. Now, pick the
rows corresponding to the vertices in this cycle. For each row
in this cycle, if we put x = 1 in the column corresponding to

the message demanded by the node where an edge ends, we
get a set of minimally dependent rows with at most one from
any RWi .
Example 5. This example simplifies the understanding of
Lemma 1 and its proof. Consider the unicast-uniprior problem
as given in Table VI. Clearly, N = 5, n = 15.
Wi

x1 , x2
x3 , x4
x5 , x8 ,
x11 , x15

Ki

x5 , x6 ,
x7
x1 , x2 ,
x12 , x14

x8 , x9 ,
x10
x3 , x6

x11 , x12 ,
x13 , x14
x4 , x7

x15
x9 , x10 , x13

TABLE VI: Unicast-uniprior problem; Example 5.
A set L of minimally dependent rows is given below
with c1 = 4, c2 = 2, c3 = 1, c4 = 2, c5 = 1.
Thus, l = 10, Ω = 5. The matrix L represents the rows
{r1 , r2 , r3 , r4 , r5 , r7 , r8 , r11 , r13 , r15 } from an arbitrary fitting
matrix.
1
















L=

2

1
0
0
0
1
0
0
0
0
0

3

0
1
0
0
1
0
0
0
0
0

4

0
0
1
0
0
0
1
0
0
0

1

0
0
0
1
0
0
0
0
1
0

5
1
1
1
0
1
0
0
0
0
0

6

7

0
0
0
0
0
0
0
0
0
0

8

0
0
0
0
0
1
0
1
0
0

9

0
1
0
0
0
0
1
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

2

0
0
0
1
0
0
0
1
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
1
1

0
0
0
0
0
0
0
0
0
0

0
0
1
0
0
0
0
0
0
1

















5

S1
3

10 11 12 13 14 15

S2
7

4

6

9
S5

8
S3

15

10

S4

11
12

13
14

Fig. 5: G for Example 5.
G ′ obtained from the side-information supergraph G (Fig. 5)
by keeping just the 10 vertices corresponding to the messages
demanded, and their supernodes is shown in Fig. 6.
WLOG we start from v1 . There is an incoming edge from
the supernode S2 to v1 . Now pick any one vertex from S2 ,
say v5 . There is an incoming edge from S1 to v5 . Hence we
get a subset of two minimally dependent rows, with at most
one from any RWi , as follows:
r1
r5

1
1

0 0
0 0

0 1
0 1

0
0

0 0
0 0

0 0
0 0

0 0
0 0

0 0
0 0

0
0

1

2

5

S1
3

S2
4

7

S5
S3

15

8

11
S4
13

Fig. 6: G ′ for Example 5.
Alternately, start from v1 . There is an incoming edge to
v1 from S2 . Pick one vertex from S2 , say v7 . There is an
incoming edge to v7 from S4 . Pick one vertex from S4 , say
v11 . There is an incoming edge to v11 from S1 . Note that S1
is the supernode of the vertex we started with, i.e., v1 . Hence
we get a subset of three minimally dependent rows, with at
most one from any RWi , as follows:
r1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0
r7 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 .
r11 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0
E. Proof of Lemma 2
Using the proof of Lemma 1 in Appendix D, we know that
the vertices corresponding to the set of rows, S form a cycle.
The set S consists of at most one row from any RWi . Using
Proposition 3, in the vertex induced subgraph Q each vertex
can have at most one outgoing edge. Hence, Q is a unicycle
(see Definition 8).
R EFERENCES
[1] Birk, Yitzhak and Tomer Kol. “Informed-Source Coding-on-Demand
(ISCOD) over Broadcast Channels. INFOCOM (1998).

[2] Z. Bar-Yossef, Y. Birk, T. S. Jayram and T. Kol, “Index Coding With
Side Information,” in IEEE Transactions on Information Theory, vol. 57,
no. 3, pp. 1479-1494, March 2011.
[3] R. Ahlswede, Ning Cai, S. -. R. Li and R. W. Yeung, “Network
information flow,” in IEEE Transactions on Information Theory, vol. 46,
no. 4, pp. 1204-1216, July 2000.
[4] R. Bassoli, H. Marques, J. Rodriguez, K.W. Shum, R. Tafazolli, “Network
coding theory: A survey,” IEEE Comm. Sur. and Tut., 15 (2013), p. 29
[5] S. E. Rouayheb, A. Sprintson, and C. Georghiades, “On the index coding
problem and its relation to network coding and matroid theory, IEEE
Trans. on Inf. Theory, vol. 56, no. 7, pp. 31873195, July 2010.
[6] R. Kavitha and B. S. Rajan, “On the number of optimal index codes, in
IEEE Int. Symp. on Inf. Theory (ISIT), June 2015, pp. 10441048.
[7] R. Kavitha, N. Ambadi and B. S. Rajan, “On the number of optimal
linear index codes for unicast index coding problems,” IEEE Wireless
Communications and Networking Conference, Doha, 2016, pp. 1-7
[8] Bunch, James R., and John E. Hopcroft. “Triangular Factorization and
Inversion by Fast Matrix Multiplication. Mathematics of Computation,
vol. 28, no. 125, 1974, pp. 231236. JSTOR.
[9] O. H. Ibarra, S. Moran, and R. Hui, “A generalization of the fast LUP
matrix decomposition algorithm and applications, J. Algorithms, vol. 3,
no. 1, pp. 4556, 1982.
[10] M. Tahmasbi, A. Shahrasbi, and A. Gohari, “Critical graphs in index
coding, IEEE J. Sel. Areas Commun., vol. 33, no. 2, pp. 225235, 2015.
[11] F. Arbabjolfaei, Y. H. Kim, “On Critical Index Coding Problems”, in
Proc. IEEE Information Theory Workshop(ITW), Oct 2015, pp. 9-13.
[12] Reinhard Deistel, Graph Theory, 3rd edition, Springer 2006.
[13] H. Maleki, V. Cadambe and S. Jafar, “Index coding: An interference
alignment perspective,” IEEE Int. Symp. on Inf. Theory, Cambridge, MA,
2012, pp. 2236-2240.
[14] Y. Birk and T. Kol, “Coding on demand by an informed source (iscod)
for efficient broadcast of different supplemental data to caching clients,
IEEE Trans. on Inf. Theory, vol. 52, no. 6, pp. 28252830, June 2006.
[15] Z. Bar-Yossef, Y. Birk, T. S. Jayram, and T. Kol, “Index coding with side
information, IEEE Trans. on Inf. Theory, vol. 57, no. 3, pp. 14791494,
March 2011.
[16] R. Peeters, “Orthogonal representations over finite fields and the chromatic number of graphs,” Combinatorica, 16(3):417-431, 1996.
[17] L. Ong and C. K. Ho, “Optimal index codes for a class of multicast networks with receiver side information, in IEEE International Conference
on Communications (ICC), June 2012, pp. 22132218.
[18] S. A. Jafar, “Topological Interference Management Through Index
Coding,” in IEEE Transactions on Information Theory, vol. 60, no. 1,
pp. 529-568, Jan. 2014.
[19] Bunch, James R., and John E. Hopcroft. “Triangular Factorization and
Inversion by Fast Matrix Multiplication.” Mathematics of Computation
28, no. 125 (1974): 231-36.
[20] M. A. R. Chaudhry, Z. Asad, A. Sprintson and M. Langberg, “On the
complementary Index Coding problem,” IEEE International Symposium
on Information Theory Proceedings, St. Petersburg, 2011, pp. 244-248.

