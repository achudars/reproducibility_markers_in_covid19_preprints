Directed flow of information in chimera states

arXiv:1904.11553v1 [nlin.AO] 25 Apr 2019

Nicolás Deschle,1, 2, ∗ Andreas Daffertshofer,1, † Demian Battaglia,3, ‡ and Erik A. Martens4, 5, §
1
Faculty of Behavioural and Movement Sciences,
Amsterdam Movement Sciences & Institute for Brain and Behavior Amsterdam,
Vrije Universiteit Amsterdam, van der Boechorststraat 9, Amsterdam 1081 BT, The Netherlands
2
Institute for Complex Systems and Mathematical Biology, University of Aberdeen,
King’s College, Old Aberdeen AB24 3UE, United Kingdom
3
Institute for Systems Neuroscience, University Aix-Marseille,
Boulevard Jean Moulin 27, 13005 Marseille, France
4
Department of Applied Mathematics and Computer Science,
Technical University of Denmark, 2800 Kgs. Lyngby, Denmark
5
Department of Biomedical Sciences, University of Copenhagen, Blegdamsvej 3, 2200 Copenhagen, Denmark
(Dated: April 29, 2019)

We investigated interactions within chimera states in a phase oscillator network with two coupled
subpopulations. To quantify interactions within and between these subpopulations, we estimated
the corresponding (delayed) mutual information that – in general – quantifies the capacity or the
maximum rate at which information can be transferred to recover a sender’s information at the
receiver with a vanishingly low error probability. After verifying their equivalence with estimates
based on the continuous phase data, we determined the mutual information using the time points
at which the individual phases passed through their respective Poincaré sections. This stroboscopic
view on the dynamics may resemble, e.g., neural spike times, that are common observables in the
study of neuronal information transfer. This discretization also increased processing speed significantly, rendering it particularly suitable for a fine-grained analysis of the effects of experimental
and model parameters. In our model, the delayed mutual information within each subpopulation
peaked at zero delay, whereas between the subpopulations it was always maximal at non-zero delay,
irrespective of parameter choices. We observed that the delayed mutual information of the desynchronized subpopulation preceded the synchronized subpopulation. Put differently, the oscillators of
the desynchronized subpopulation were ’driving’ the ones in the synchronized subpopulation. These
findings were also observed when estimating mutual information of the full phase trajectories. We
can thus conclude that the delayed mutual information of discrete time points allows for inferring a
functional directed flow of information between subpopulations of coupled phase oscillators.
Keywords: Chimeras, phase oscillators, coupled networks, mutual information, information flow

I.

INTRODUCTION

Oscillatory units are found in a spectacular variety of
systems in nature and technology. Examples in biology
include flashing fireflies [1], cardiac pacemaker cells [2–6],
and neurons [7–11]; in physics one may think of Josephson junctions [12–14], electric power grids [15–21], and, of
course, pendulum clocks [22]. Synchronization plays an
important role in the collective behavior of and the communication between individual units [23–25]. In the last
two decades or so, many studies addressed the problem
of synchronization in networks with complex structure,
such as networks of networks, hierarchical networks and
multilayer networks [26–29]. Alongside efforts studying
synchronization on networks, a new symmetry breaking
regime coined chimera state has been observed. In a
chimera state an oscillator population ‘splits’ into two
parts, one being synchronized and the other being desynchronized [30–32], or more generally, different levels of

∗
†
‡
§

n.deschle@vu.nl
a.daffertshofer@vu.nl
demian.battaglia@univ-amu.fr
eama@dtu.dk

synchronization [33]. This state is a striking manifestation of symmetry breaking, as it may occur even if oscillators are identical and coupled symmetrically; see [34, 35]
for recent reviews. Chimera states have spurred much
interest resulting in many theoretical investigations, but
they have also been demonstrated in experimental settings using, e.g., mechanical and (electro-)chemical oscillators or lasers [36–39], and electronic circuits implementing FitzHugh-Nagumo neurons [40].
Chimera states can be considered patterns of localized
synchronization. As such they may contribute to the
coding of information in a network. This is particularly
interesting since systems with chimera states typically
display multi-stability, i.e., different chimera configurations may co-exist for identical parameters [41–44]. Such
a network may hence be able to encode different stimuli through different chimera states without the need for
adjusting parameters or altering network structure. It
is even possible to dynamically switch between different
synchronization patterns, thus allowing for dynamic coding of information [45, 46].
The mere existence of different levels of synchronization in the same network can also facilitate the transfer of
information across a network, especially between subpopulations. Coherent oscillations between neurons or neural

2
populations have been hypothesized to provide a communication mechanism while full neural synchronization
is usually considered pathological [24, 47, 48]. A recent
study reported chimera-like states in neuronal networks
in humans, more specifically in electro-encephalographic
patterns during epileptic seizures [49]. Admittedly, our
understanding of these mechanism is still in its infancy
also because the (interpretations of) experimental studies often lack mathematical rigor, both regarding the description of synchronization processes and the resulting
implications for the network’s capacity for information
processing. What is the information transfer within and
between synchronized and desynchronized populations?
We investigated the communication channels between
two subpopulations in a system of coupled phase oscillators. Since we designed that system to exhibit chimera
states, we expected non-trivial interactions between the
subpopulations. To tackle this, we employed the delayed
version of mutual information which measures the rate
at which information can be sent and recovered with
vanishingly low probability of error [50]. Furthermore,
assuming symmetry of our configuration, any directionality found in the system should be regarded of functional nature. With this we sought not only to answer
the aforementioned question but to contribute to a more
general understanding of how information can be transferred between subpopulations of oscillators. In view of
potential applicability in other (experimental) studies,


dφj,µ (t) = ωj,µ +

Nµ
M X
X

we also investigated whether we could gain sufficient insight into this communication by looking at the passage
times through the oscillators’ Poincaré sections rather
than evaluating the corresponding continuous-time data.
Such data may resemble, e.g., spike trains in neurophysiological assessments.
Our paper is structured as follows. First, we introduce
our model in the study of chimera states [32, 51–53]. It
consists of two subpopulations that either can be synchronized or desynchronized. We generalize this model
by including distributed coupling strengths among the oscillatory units as well as additive Gaussian white noise.
We briefly sketch the conditions under which chimera
states can exist. Second, we outline the concept of delayed mutual information and detail how to estimate this
for ‘event-based’ data where we define events via Poincaré
sections of the phase oscillators’ trajectories. With this
tool at hand, we finally investigate the flow of information
within and between the two subpopulations, and characterize its dependency on the essential model parameters.

II.

MODEL

We build on a variant of the noisy Kuramoto-Sakaguchi
model [54], generalized to M subpopulations of phase oscillators [55]. The phase φj,µ (t) of oscillator j = 1, . . . , N
in population µ = 1, . . . , M evolves according to


Ckj,µν sin (φk,ν (t) − φj,µ (t) − α) dt + dWj,µ (t) ,

(1)

ν=1 k=1

where ωj,µ is the natural frequency of the oscillator
j in subpopulation µ. Throughout our study ωj,µ is
drawn from a zero-centered Gaussian distribution with
variance σω2 . Oscillator j in population µ and oscillator k in population ν interact sinusoidally with coupling strength Ckj,µν and a phase lag α. The phase lag
α varies the interaction function between more cosine
(α → π/2) or more sine like-behavior (α → 0) and can
be interpreted as a (small) transmission delay between
units [34]. The additive noise term dWj,µ (t) represents
mean-centered Gaussian noise with variance (strength)
2
2
σW
, i.e., hdWj,µ (t)dWk,ν (t0 )it = σW
δjk δµν δ(t − t0 ).
For the sake of legibility, we restrict this model to the
case of M = 2 subpopulations of equal size N1 = N2 =
N . In the absence of noise and in the case of identical oscillators, uniform phase lag and homogeneous coupling,
i.e., for σW = 0, σω = 0, αµν := α, and σC = 0, respectively, the aforementioned studies establishes corresponding bifurcation diagrams [32, 51, 52].
To generalize the model towards more realistic, experimental setting, we included distributed coupling

strengths via a non-uniform, (statistically) symmetric
coupling between subpopulations. This can be cast in
the following coupling matrix:


1 Cζ Cη
C=
∈ R2N ×2N
(2)
N Cη Cζ
with block matrices Cζ , Cη ∈ RN ×N . The self-coupling
within the subpopulation and the neighbor coupling between subpopulations are represented by Cζ and Cη , respectively. Each block Cζ (or Cη ) is composed of random
numbers drawn from a normal distribution with mean ζ
2
(or η) and variance σC
. By this, we can tune the heterogeneity in the network. We would like to note that, in
general, the chosen coupling is only symmetric for σC > 0
in the sense of a statistical average, and that the expected coupling values ζ and η in each block only can
be retrieved by the mean in the limit of large numbers.
Although the symmetry between the two subpopulations
1 ↔ 2 is broken and only preserved in a statistical sense
in the limit of large numbers, we verified that chimera

3
a)

b)

1

2

1

0
0

d)

c)
S

D

PDF

states appeared in both configurations. That is, subpopulation 1 is synchronized and subpopulation 2 is desynchronized (SD) and subpopulation 2 is synchronized and
subpopulation 1 is desynchronized (DS), for a particular
choice of parameters using numerical simulations. Another consequence of distributed coupling strengths is
that only a very few oscillators may have experienced
negative (inhibitory) coupling, which can be neglected.
Following [51], we parametrize the relation between
strengths by A = ζ − η with ζ + η = 1 and the phase lag
αµ,ν by βµ,ν = π2 − αµ,ν which here can be kept homogeneous for the entire network, i.e., βµ,ν := β. By this, we
may recover the results reported in [32, 51] in the limit
σC → 0, under the proviso of σW = σω = 0. In brief, increasing A from 0 while fixing α (or β := π/2 − α) yields
the following bifurcation scheme: a “stable chimera” is
born through a saddle-node bifurcation and becomes unstable in a supercritical Hopf bifurcation with a stable limit cycle corresponding to a “breathing chimera”,
which eventually is destroyed in a homoclinic bifurcation. For all parameter values, the fully synchronized
state exists and is a stable attractor; see also Fig. 4 in
Appendix A. Subsequent studies demonstrated robustness of chimera states against non-uniformity of delays
(αµν 6= α) [33], heterogeneity of oscillator frequencies
(σω > 0) [56], and additive noise [57]. Although nonuniform phase lags lead to less degenerate dynamics and
give room for more complex dynamics [33, 53], we restrict
ourselves to the case of uniform phase lags without compromising the essential phenomenology of chimera states.
The macroscopic behavior, i.e. the synchronization of
the two populations, may be characterized by complexvalued order parameters, which are either defined on the
PN
population level, i.e., Zµ := N −1 j=1 eiθµ,j , or globPM PN
ally, i.e., Z = (2N )−1 µ=1 j=1 eiθµ,j . As common
in the studies of coupled phase oscillators, the level of
synchrony can be given by Rµ := |Zµ |. Thus, Rµ = 1 implies that oscillators in population µ are perfectly phase
synchronized (S), while for Rµ < 1 the oscillators are imperfectly synchronized or de-synchronized (D). For our
two-subpopulation case, full synchrony (SS) hence occurs when R1 / 1 and R2 / 1, and chimera states are
present if R1 < 1 and R2 / 1 or vice versa. The angular order parameter, Φµ := arg Zµ keeps track of the
average phase of the (sub)population. Fluctuations inherent to the model may affect the order parameter as
illustrated in Fig. 1(b) (please refer to section III A for
the numerical specifications). We therefore always considered averages over time, hRµ it , when discussing the
stability of a state. In fact, in our model chimera states
remain stable for relatively large coupling heterogeneity,
σC > 0 presuming σω > 0 and σW > 0, as is evidenced
by numerical simulations; see also Appendix A, Fig. 4.
The perfect synchronization manifold with R = 1 cannot
be achieved; see Fig. 1(b). Further aspects of these noisy
dynamics will be presented elsewhere.
Adding noise and heterogeneity to the system may alter its dynamics. In the present work we concentrated

0

index

5

100

200

300

400

500

490
480

S

D

470
460

index

FIG. 1. Panel (a): Chimera state in a network with M = 2
oscillator subpopulations and non-uniform coupling, simulated using Eq. (1). Panel (b): The time evolution of the
order parameter, |Zµ (t)|, µ = 1, 2, reflects the fluctuations
driven by the coupling and the external field. Despite temporary deviations off the synchronization manifold |Z1 | = 1,
the system remains near the chimera state attractor. That is,
chimera states appear robust to both coupling heterogeneity
and additive noise. Considering the time-average of the order
parameters, the state may be classified as a stable chimera.
Panel (d): Times t0j,µ correspond to the Poincaré sections of
the phases i.e., t0j,µ such that φi (t0j,µ ) = 0. The time period of
collective oscillation is defined as T := 1/Ω and Ω := hh dφ
ii
dt t j
where h·it and h·ij denote averages over time and oscillators, respectively. Parameters are: A = 0.275, β = 0.125,
2
= 0.001.
σC = 0.01, σω = 0.01, σW

on parameter regions characterized by the occurrence of
dynamic states that did resemble stable chimeras, i.e.,
hR1 it > hR2 it , where h·it denotes the average over a duration of T = 9·106 time steps (after removing a transient
of T = 105 time steps). Fig. 4 in Appendix A provides
an overview and explanation of how parameter points A
and β were selected.
III.

IMPLEMENTATION AND ANALYSIS
A.

Simulations

For the numerical implementation we employed a
Euler-Maruyama scheme with ∆t = 10−2 for N = 128
phase oscillators per subpopulation that were evolved for
T = 106 [54]. We varied the coupling parameter A and
the phase lag parameter β, while we fixed the width
(standard deviation) of the natural frequency distribution to σω = 0.01 and of the coupling distribution to
2
σC = 0.01. The additive noise had variance σW
= 0.001.
B.

Mutual Information

Mutual information I(X; Y ), first introduced by Shannon [58], is meant to assess the dependence between two
random variables X and Y . It measures the amount of

4
information that X contains about Y . In terms of communication theory, Y can be regarded as output of a communication channel which depends probabilistically on
its input, X. By construction, I(X; Y ) is non-negative;
it is equal to zero if and only if X and Y are independent.
Moreover, I(X; Y ) is symmetric, i.e., I(X; Y ) = I(Y ; X)
implying that it is not directional. The mutual information may also be considered as a measure of the reduction in the uncertainty of X(Y) due to the knowledge of
Y (X) or, in terms of communication, the rate at which
information is being shared between the two [50]. The
maximum rate at which one can send information over
the channel and recover it at the output with a vanishingly low probability of error is called channel capacity,
c = maxp(x) I(X; Y ). In networks with oscillatory nodes,
the random variables are the degree of synchronization
of different subpopulations, and the rate of information
shared will depend on the state of the system for each of
the subpopulations.
Mutual information can be defined via the KullbackLeibler divergence or in terms of entropies,
I(X; Y ) = H(X) − H(X|Y )
= H(X) + H(Y ) − H(X, Y ).

(3)

where H(X) is the
R entropy of the random variable X,
i.e., H(X) = − dx pX (x) log pX (x) with pX (x) being
the corresponding probability density.
C.

Delayed mutual information

For time-dependent random variables, one may generalize this definition to that of a delayed mutual information. This variant dates back to Fraser and Swinney
[59], who used the delayed mutual information for estimating the embedding delay in chaotic systems — in
that context the delayed auto-mutual information is often considered a ‘generalization’ of the auto-correlation
function [60]. Applications range from the study of coupled map lattices [61] via spatiotemporal [62] and general
dependencies between time series [63] to the detection of
synchronization [64]. With the notation IXY := I(X; Y )
and HX := H(X), the delayed mutual information for a
delay τ may read
IXY (τ ) = I(X(t); Y (t + τ ))
= HX + HY − HXY (τ )

(4)

which has the symmetry IXY (τ ) = IY X (−τ ) [65]. With
this definition, one can measure the rate of information
shared between X and Y as a function of time delay τ .
In fact, we are not particularly interested in the specific
value of the mutual information but rather focus here
on the time delay at which the mutual information is
maximal. Hence, we define τmax := arg maxτ IXY (τ ). A
positive (negative) value of τmax implies that Y shares
more information with a delayed (advanced) X. This
means there is an information flow from X(Y) to Y (X).

D.

Delayed mutual information between
subpopulations

a. Estimates using continuous-time data. When the
time-dependent random variables are continuous time series uµ (t) and vν (t) associated to populations µ and ν,
the delayed mutual information can be estimated from
Eq. (4) for X(t) = uµ (t) and Y (t) = uν (t),
I˜µν (τ ) = I˜ (uµ (t); uν (t + τ )) .

(5)

We determined the probability densities using kernel density estimators with Epanetchnikov kernels with bandwidths given through a uniform maximum likelihood
cross-validation search. For our parameter settings (254
oscillators and 106 samples), the resulting bandwidths
ranged from about 0.10 rad to 0.18 rad. This software
implementation is part of the KDE-toolbox; cf. [66] and
[67] for alternative schemes.
b. Estimates using event-based time data. For the
aforementioned event signals in the subpopulations 1
and 2, i.e., discrete time points are defined as passing moments through the respective Poincaré sections.
The probability densities to incorporate when estimating the mutual information are densities of events, or
densities of times. We implemented the probability estimates as follows. Let Sµ be a set of event times, i.e.,
(m)
(m0 )
(m)
Sµ = {tµ,1 , . . . , tµ,Nµ } where tµ,i stands for the time of
the m-th event of oscillator i in subpopulation µ. Then,
the probability density for an event to happen at time t
in subpopulation µ is pµ (t) = P (t ∈ Sµ ) and the probability of an event to happen at time t in subpopulation
µ and time t + τ in subpopulation ν is
pµν (t, t + τ ) = P ({t ∈ Sµ } ∩ {t + τ ∈ Sν })
= P (t ∈ Sµ ) + P (t + τ ∈ Sν )
− P ({t ∈ Sµ } ∪ {t + τ ∈ Sν }) . (6)
The delayed mutual information can be given as
Z
pµν (t, t + τ )
Iµν (τ ) = dt pµν (t, t + τ ) log
pµ (t)pν (t)
= Hµ + Hν − Hµν (τ ) .

(7)

We again computed the probability densities using kernel density estimators [66] but now involving Gaussian
kernels. We also adjusted the bandwidth selection to a
spherical, local maximum likelihood cross-validation due
to the sparsity of the data and the resulting bandwidths
ranged from about 25 to 35 time units. These results
appeared robust when using the aforementioned uniform
search; again we employed the KDE-toolbox.
E.

Events defined via Poincaré sections

We analyzed the times t1,µ , t2,µ , . . . tN,µ at which the
individual phases φ(t)j,µ passed through their respective

5
Poincaré sections. The latter were defined as tj,µ ∈
R+
0 : φj,µ (tj,µ )/2π ∈ Z. As mentioned above, every
subpopulation µ generated an ‘event sequence’ Sµ =
(m)
(m0 )
{tµ,1 , . . . , tµ,Nµ }; which, as already said, may be considered reminiscent of spike trains; cf. Fig. 1(c).

IV.

RESULTS

To determine the directionality of the information flow
in the network we computed the time lagged mutual information within the subpopulations, I11 (τ ) and I22 (τ )
and between them, I12 (τ ) = I21 (−τ ).
(a)

(b)

4.5
4
3.5
3
-10

(c)

-5

0

5

10

(d)

1
0.8
0.6
0.4
0.2
0
-0.2
-0.4
-1

0

1

FIG. 2. Panel (a): Mutual informations Iµν as a function of
τ between and within the two subpopulations µ and ν. Panel
(c): The range-zoomed curve reveals a directionality in the
mutual information I12 (τ ) between the two subpopulations 1
and 2, with a maximum peak Imax := I12 (τmax ) located at
τmax < 0. Panels (b) and (d): comparison of the average mutual information obtained from continuous time data (solid),
I˜12 , versus event time data (dotted), I12 . Obviously, locations
of the peaks agree very well. Panels (b-d) display the average mutual information (I − hIit )/ maxt (I − hIit ) with hIit
denoting the average over time. Parameters across panels are
2
A = 0.275, β = 0.125, σω = 0.01, σC = 0.01, σW
= 0.001.

The first results for the event data outlined in Section III E are shown in Fig. 2(a). Since the recurrence of
events mimicked the mean frequency of the phase oscillators, the mutual information turned out periodic. As
expected, we observed (average) values of the mutual information that differed between I11 (red), I22 (blue), I12
(black). This relates to the difference in entropy of the
subpopulations, with the less synchronized one (µ = 2)
being more disordered. The latter, hence, contained more
entropy. However, since we were not interested in the
explicit values of Iµν (τ ), we could rescale the mutual information to maximum value which allowed for a comparative view when zooming in to the neighborhood of

τ ≈ 0; see panel (c) of Fig. 2. The off-zero peak of
Imax := I12 (τmax ) in τ = τmax clearly indicated a directed
information flow, i.e. there is a functional directionality
despite the structural symmetry of our model.
When comparing the estimates for the mutual information obtained from the event data with those from the
continuous-time we found little to no difference in peak
location. That is, the positions τmax of the maximum
peaks, Imax and I˜max , were nearly identical using either
method as shown in panels (b) and (d) Fig. 2. Thus, to
study the effects of varying model parameters on Iµν (τ ),
our subsequent analysis may solely rely on the event data
approach.
Varying the values of A and β revealed a strong relation between the location of the peak of the delayed mutual information τmax and the relative phase arg (Z1 /Z2 )
between the two subpopulations; see Fig. 3(a). This convincingly shows that our approach to analyze the eventbased data reveals important information about the (otherwise continuous) dynamics of the subpopulations, here
given by the phases (arguments) of the local order parameters. By contrast, the relative strength of local synchronization |Z1 /Z2 | had little to no effect on τmax ; see
Fig. 3(b). These dependencies were inverted when lookSS
. Coning at the value of mutual information, Imax /Imax
sequently, the value of mutual information was affected
by relative strength of local synchronization |Z1 /Z2 | —
after all the more a subpopulation is synchronized, the
lower the corresponding entropy. However, effects were
small and probably negligible when transferring to (selected) experimental data. More details about the norSS
are discussed in Appendix B.
malization factor Imax

V.

DISCUSSION

The delayed mutual information from event data, here,
the passing times through the oscillators’ Poincaré sections, agreed with that of the continuous data. This is an
important finding as it shows that studying discrete event
time series allows for inferring information-related properties of continuous dynamics. This offers the possibility
to bring our approach to experimental studies, be that
in neuroscience, where spike trains are a traditional measure of continuous neuronal activity, or economics, where
stroboscopic assessments of stocks are common practice.
Here, we used this approach to explore the information
flow within and between subpopulations of a network of
coupled phase oscillators. This information flow turned
out to be directed.
Mutual information is the traditional measure of
shared information between the two systems [58]. Our
findings relied on the introduction of a time delay τ ,
which readily allows for identifying a direction of flow of
information. This concept is much related to the widely
used transfer entropy [68, 69]. In fact, transfer entropy
is the delayed conditional mutual information [70, 71]. It
therefore differs from our approach primarily by a nor-

6
(a)

(b)
-0.1

-0.1

-0.15

-0.15

-0.2

-0.2

-0.25

-0.25

-0.25

-0.2

-0.15

-0.1

(c)

1.3

1.4

1.5

(d)
0.94

0.94

0.93

0.93

0.92

0.92

0.91

0.91

0.9

0.9

as stated repeatedly, conventional outcome parameters
in many experimental settings. This is particularly true
for studies in neuroscience, for which the quest on information processing is often central. There, networks are
typically complex and modular. The complex collective
dynamics switches at multiple scales, rendering neuronal
networks especially exciting when it comes to information routing [72]. As of yet, our approach does not allow
for unraveling dynamics information routing. This will
require extending the (time-lagged) mutual information
to a time-dependent version, e.g., by windowing the data
under study. We plan to incorporate this in future studies.

VI.
0.89

0.89
-0.25

-0.2

-0.15

-0.1

1.3

1.4

1.5

14
0.
17
12
0.
33
10
0.
5
08
0.
67
06
0.
83
04
0.
03

0.

8
28
0. 75
2
0. 3
26
0. 0
25
0. 38
2
0. 25
2
0. 13
2
0.

FIG. 3. Dependency of (Imax , νmax ) on the synchronization
level and on the parameters β and A. Panel (a): The maximizing time delay τmax has a nearly linear dependency on
the angular phase difference of order parameters arg (Z1 /Z2 ).
Panel (b): τmax appears largely independent of the ratio of
the magnitude of order parameters |Z1 /Z2 |, though results
are weakly dependent on β. Panels (c) and (d): The normalSS
ized maximum peak of mutual information Imax /Imax
shows a
weak dependency on the ratio of order parameters. All other
2
= 0.001.
parameters are fixed at σω = 0.01, σC = 0.01, σW

CONCLUSION

Estimating the delayed mutual information based
on time points at which the individual phases passed
through their respective Poincaré sections allows for identifying the information flow between subpopulations of
networks. If the network displays chimera states, the information flow turns out to be directed. In our model
of coupled phase oscillators, the flow of information was
directed from the less synchronized subpopulation to the
fully synchronized one since the first preceded the latter. Our approach is a first step to study information
transfer between spike trains. It can be readily adopted
to static well-defined modular networks and needs to be
upgraded to a time dependent version to be applied to
real, biological data.

FUNDING

malization factor. Since here we were not interested in
the absolute amount of information (flow), we could simplify assessments and focus on delayed mutual information — adding the conditional part jeopardizes the approximation of probabilities, i.e., estimating transfer entropy in a reliable manner typically requires more data
than our delayed mutual information. A detailed discussion of these differences is beyond the scope of the current
study.
For our model we found that the delayed mutual information between the synchronized subpopulation and the
less synchronized one peaked at a finite delay τmax 6= 0.
Hence, there was a directed flow of information between
the two subpopulations. Since we found that τmax < 0 for
I12 (τ ), the direction of this flow was from the less synchronized subpopulation to the fully synchronized one,
i.e. 2 → 1 or D → S. In fact, the delay largely resembled
the relative phase between the corresponding local order
parameters, as shown in panel (a) of Fig. 3. We could
not only readily identify the relative phase by encountering event data only, but our approach also allowed for
attaching a meaningful interpretation in an information
theoretic sense. This is promising since event data are,

This study received funding from the European
Union’s Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant agreement
#642563 (COSMOS).

AUTHOR CONTRIBUTIONS

ND conducted the simulations and analysis and wrote
the manuscript; AD set up the study, incorporated the
kernel density estimation and wrote the manuscript, DB
contributed to the first concepts of the study and wrote
the manuscript; EAM set up the study, conducted simulations and wrote the manuscript.

ACKNOWLEDGEMENTS

ND and AD want to thank to Rok Cestnik and Bastian
Pietras for fruitful discussions.

7
Appendix A: Stability diagram for chimera states

For the study of mutual information, we restricted
ourselves to dynamic states most similar to the ’stable
chimera states’ reported in [51]. Due to the various
noise sources inherent to the model, the system dynamics
undergoes fluctuations, as described in Section II. This
poses additional challenges that we addressed by examining specific time averages of the complex-valued order
parameter in each subpopulation. Since fluctuations may
cause the system to temporarily drift off the synchronized
manifold, we may not simply consider |Z1 | = 1, |Z2 | < 1
to identify chimera states. Instead we require the order to
be sufficiently different between the two subpopulations,
i.e., we threshold h|Z2 |it /h|Z1 |it , defining the region R1
shown in Fig. 4(a).

(a)

(b)
1
0.95

0.4

0.25
0.4

0.2

0.9
0.3

0.85

0.2

0.8
0.213
0.225
0.238
0.250
0.263
0.275
0.288

0.1
0.05

0.1 0.15 0.2

0.75
0.7
0.65

0.3

0.15

0.2

0.213
0.225
0.238
0.250
0.263
0.275
0.288

0.1
0.05

0.1 0.15

0.1

small. To that end, we thresholded hmax(|Z2 |)it −
hmin(|Z2 |)it , defining the region R2 shown in Fig. 4(b).
Here, h·it denotes the average defined over the maxima
computed over time windows of length T = 103 time
steps, over a duration T = 9 · 105 after removing a transient of T = 105 time steps. Finally, chimera states with
parameter values (β, A) ∈ R1 ∩ R2 were selected for further analysis of mutual information.

SS
Appendix B: Normalization factor Imax

Our model displays a baseline dependency of the maxSS
imum peak of mutual information Imax
on the parameters β and A, even when the system is in the fully synchronized state SS, see Fig. 5. We therefore studied the
dependency of the maximum peak of the delayed muSS
tual information for chimera states normalized with Imax
,
SS
Imax /Imax
. For every set of parameters we computed
the delayed mutual information on two different solution
that have been forced by changing the initial conditions;
the maxima of each of these solutions (SS and SD) are
SD
SS := SS
(A, β). These are
Imax (A, β) and Imax := Imax
Imax
the quantities displayed in panels (c) and (d) of Fig. 3.

0.05

0.2
6.8

0.4

FIG. 4.
Stability diagram based on analyzing complexvalued order parameters. Regions R1 (a) and R2 (b) in green
outline parameter values (β, A) for which h|Z2 |it /h|Z1 |it < 0.8
and hmax(|Z2 |)it − hmin(|Z2 |)it < 0.1, respectively. The
states that are most similar to ’stable chimera states’ [51]
were identified within the intersection R1 ∩ R2 (outlined in
red). The outlined regions are bounded by parameter regions
known to exhibit stable chimeras in the limit of σω = σC =
2
= 0 [51]; saddle node and Hopf bifurcation curves are
σW
shown as black solid and dashed curves, respectively. The
other parameters have been fixed at σω = 0.01, σC = 0.01,
2
= 0.001.
and σW

6.4

0.3

6

0.2

5.6

0.1

5.2
0.05

0.1 0.15

0.2

FIG. 5.
The normalization factor given by the peak
value of mutual information in a fully synchronized state SS,
SS
:= I11 (τmax ) = I22 (τmax ) as a function of (β, A). The
Imax
other parameters are kept fixed at σω = 0.01, σC = 0.01, and
2
σW
= 0.001.

Furthermore, we presume amplitude variations to be

[1] G. B. Ermentrout and J. Rinzel, American Journal
of Physiology-Regulatory, Integrative and Comparative
Physiology 246, R102 (1984), pMID: 6696096.
[2] M. R. Guevara, L. Glass, and A. Shrier, Science 214,
1350 (1981).
[3] M. R. Guevara, F. Alonso, D. Jeandupeux, and A. C. G.
Van Ginneken, in Cell to Cell Signalling (Elsevier, 1989)
pp. 551–563.
[4] L. Glass and M. C. Mackey, From clocks to chaos: The
rhythms of life (Princeton University Press, 1988).
[5] W.-Z. Zeng, M. Courtemanche, L. Sehn, A. Shrier, and
L. Glass, Journal of Theoretical Biology 145, 225 (1990).

[6] L. Glass and A. Shrier, in Theory of Heart (Springer,
1991) pp. 289–312.
[7] W. Singer, Nature 397, 391 (1999).
[8] C. M. Gray, P. König, A. K. Engel, and W. Singer,
Nature 338, 334 (1989).
[9] W. A. MacKay, Trends in Cognitive Sciences 1, 176
(1997).
[10] W. Singer and C. M. Gray, Annual Review of Neuroscience 18, 555 (1995).
[11] E. A. Stern, D. Jaeger, and C. J. Wilson, Nature 394,
475 (1998).

8
[12] A. K. Jain, K. Likharev, J. Lukens, and J. Sauvageau,
Physics Reports 109, 309 (1984).
[13] K. Saitoh and T. Nishino, Physical Review B 44, 7070
(1991).
[14] T. Valkering, C. Hooijer, and M. Kroon, Physica D:
Nonlinear Phenomena 135, 137 (2000).
[15] P. J. Menck, J. Heitzig, J. Kurths, and H. J. Schellnhuber, Nature Communications 5, 3969 (2014).
[16] P. H. Nardelli, N. Rubido, C. Wang, M. S. Baptista,
C. Pomalaza-Raez, P. Cardieri, and M. Latva-aho, The
European Physical Journal Special Topics 223, 2423
(2014).
[17] A. E. Motter, S. A. Myers, M. Anghel, and T. Nishikawa,
Nature Physics 9, 191 (2013).
[18] F. Dörfler, M. Chertkov, and F. Bullo, Proceedings
of the National Academy of Sciences 110, 2005 (2013),
https://www.pnas.org/content/110/6/2005.full.pdf.
[19] S. Lozano, L. Buzna, and A. Dı́az-Guilera, The European Physical Journal B 85, 231 (2012).
[20] Y. Susuki and I. Mezic, IEEE Transactions on Power
Systems 26, 1894 (2011).
[21] M. Rohden, A. Sorge, M. Timme, and D. Witthaut,
Physical Review Letters 109, 064101 (2012).
[22] C. Huygens, Horologium oscillatorium sive de motu pendulorum ad horologia aptato demonstrationes geometricae (F. Muguet, Paris, France, 1673).
[23] A. Pikovsky, M. Rosenblum, J. Kurths, and J. Kurths,
Synchronization: a universal concept in nonlinear sciences, Vol. 12 (Cambridge University Press, 2003).
[24] G. Buzsaki, Rhythms of the Brain (Oxford University
Press, 2006).
[25] S. H. Strogatz, Hyperion (Hachette UK, 2003).
[26] S. Boccaletti, G. Bianconi, R. Criado, C. I. Del Genio, J. Gómez-Gardenes, M. Romance, I. Sendina-Nadal,
Z. Wang, and M. Zanin, Physics Reports 544, 1 (2014).
[27] A. Arenas, A. Dı́az-Guilera, J. Kurths, Y. Moreno, and
C. Zhou, Physics Reports 469, 93 (2008).
[28] M. Kivelä, A. Arenas, M. Barthelemy, J. P. Gleeson,
Y. Moreno, and M. A. Porter, Journal of Complex Networks 2, 203 (2014).
[29] F. A. Rodrigues, T. K. D. Peron, P. Ji, and J. Kurths,
Physics Reports 610, 1 (2016).
[30] Y. Kuramoto and D. Battogtokh, Nonlinear Phenomena
in Complex Systems 4, 380 (2002).
[31] D. M. Abrams and S. H. Strogatz, Physical Review Letters 93, 174102 (2004).
[32] E. Montbrió, J. Kurths, and B. Blasius, Physical Review
E 70, 056125 (2004).
[33] E. A. Martens, C. Bick, and M. J. Panaggio, Chaos 26,
094819 (2016), arXiv:1606.01871.
[34] M. J. Panaggio and D. M. Abrams, Nonlinearity 28, R67
(2015).
[35] E. Schöll, The European Physical Journal Special Topics
225, 891 (2016).
[36] E. A. Martens, S. Thutupalli, A. Fourriere, and O. Hallatschek, Proceedings of the National Academy of Sciences 110, 10563 (2013).
[37] M. R. Tinsley, S. Nkomo, and K. Showalter, Nature
Physics 8, 1 (2012).
[38] A. M. Hagerstrom, T. E. Murphy, R. Roy, P. Hövel,
I. Omelchenko, and E. Schöll, Nature Physics 8, 1 (2012).
[39] M. Wickramasinghe and I. Z. Kiss, PloS one 8, e80586
(2013).

[40] L. V. Gambuzza, A. Buscarino, S. Chessari, L. Fortuna,
R. Meucci, and M. Frasca, Physical Review E 90, 032905
(2014).
[41] E. A. Martens, Physical Review E 82, 016216 (2010).
[42] E. A. Martens, Chaos: An Interdisciplinary Journal of
Nonlinear Science 20, 043122 (2010).
[43] M. Shanahan, Chaos: An Interdisciplinary Journal of
Nonlinear Science 20, 013108 (2010).
[44] C. Bick, Physical Review E 97, 050201 (2018).
[45] C. Bick and E. A. Martens, New Journal of Physics 17,
033030 (2015).
[46] E. A. Martens, M. J. Panaggio, and D. M. Abrams, New
Journal of Physics 18, 022002 (2016).
[47] P. Fries, Trends in cognitive sciences 9, 474 (2005).
[48] J. Fell and N. Axmacher, Nature reviews neuroscience
12, 105 (2011).
[49] R. G. Andrzejak, C. Rummel, F. Mormann,
and
K. Schindler, Scientific Reports 6, 23000 (2016).
[50] T. M. Cover and J. A. Thomas, Elements of information
theory (John Wiley & Sons, 2012).
[51] D. M. Abrams, R. E. Mirollo, S. H. Strogatz, and D. A.
Wiley, Physical Review Letters 101, 084103 (2008).
[52] M. J. Panaggio, D. M. Abrams, P. Ashwin, and C. R.
Laing, Physical Review E 93, 012218 (2016).
[53] C. Bick, M. J. Panaggio, and E. A. Martens, Chaos: An
Interdisciplinary Journal of Nonlinear Science 28, 071102
(2018).
[54] J. Acebrón, L. Bonilla, C. Pérez Vicente, et al., Reviews in Modern Physics 77 (2005), 10.1103/RevModPhys.77.137.
[55] C. Bick, M. Goodfellow, C. R. Laing, and E. A. Martens,
1902 (2019), arXiv:arXiv:1902.05307v1.
[56] C. R. Laing, Chaos: An Interdisciplinary Journal of Nonlinear Science 19, 013113 (2009).
[57] C. R. Laing, Chaos: An Interdisciplinary Journal of Nonlinear Science 22, 043104 (2012).
[58] C. E. Shannon and W. Weaver, The Bell System Technical Journal (University of Illinois Press, Urbana, 1949)
arXiv:9411012 [chao-dyn].
[59] A. M. Fraser and H. L. Swinney, Physical Review A
(1986), 10.1103/PhysRevA.33.1134, arXiv:1511.02086.
[60] H. Abarbanel, Analysis of observed chaotic data
(Springer Science & Business Media, 2012).
[61] K. Kaneko, Physica D: Nonlinear Phenomena 23D, 436
(1986).
[62] J. A. Vastano and H. L. Swinney, Physics Review Letters
60, 1773 (1988).
[63] M. L. Green and R. Savit, Physica D: Nonlinear Phenomena 50, 521 (1991).
[64] M. Paluš, Physics Letters A 235, 341 (1997).
[65] This follows because of: I˜XY (τ ) = I (X(t); Y (t + ν)) =
I˜ (X(t − ν); Y (t)) = I˜ (Y (t); X(t − ν)) = I˜Y X (−τ ).
[66] A.
Ihler
and
M.
Mandel,
available
via
http://www.ics.uci.edu/˜ihler/code (2003).
[67] R. D. Thomas, N. C. Moses, E. A. Semple, and
A. J. Strang, Journal of Mathematical Psychology 61,
45 (2014).
[68] T. Schreiber, Physical Review Letters 85, 461 (2000).
[69] M. Wibral, N. Pampu, V. Priesemann, F. Siebenhühner,
H. Seiwert, M. Lindner, J. T. Lizier, and R. Vicente,
PloS One 8, e55809 (2013).
[70] R. L. Dobrushin, Uspekhi Matematicheskikh Nauk 14, 3
(1959).

9
[71] A. D. Wyner, Information and Control 38, 51 (1978).

[72] C. Kirst, M. Timme, and D. Battaglia, Nature Communications 7, 11061 (2016).

