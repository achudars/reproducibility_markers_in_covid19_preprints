Localized Fourier Analysis for Graph Signal Processing
Basile de Loynes∗, Fabien Navarro†, Baptiste Olivier‡

arXiv:1906.04529v2 [eess.SP] 10 Jun 2020

June 11, 2020

Abstract
We propose a new point of view in the study of Fourier analysis on graphs, taking advantage of
localization in the Fourier domain. For a signal f on vertices of a weighted graph G with Laplacian
matrix L, standard Fourier analysis of f relies on the study of functions g(L)f for some filters g on
IL , the smallest interval containing the Laplacian spectrum sp(L) ⊂ IL . We show that for carefully
chosen partitions IL = t1≤k≤K Ik (Ik ⊂ IL ), there are many advantages in understanding the collection
(g(LIk )f )1≤k≤K instead of g(L)f directly, where LI is the projected matrix PI (L)L. First, the partition
provides a convenient modelling for the study of theoretical properties of Fourier analysis and allows for
new results in graph signal analysis (e.g. noise level estimation, Fourier support approximation). We
extend the study of spectral graph wavelets to wavelets localized in the Fourier domain, called LocLets,
and we show that well-known frames can be written in terms of LocLets. From a practical perspective,
we highlight the interest of the proposed localized Fourier analysis through many experiments that
show significant improvements in two different tasks on large graphs, noise level estimation and signal
denoising. Moreover, efficient strategies permit to compute sequence (g(LIk )f )1≤k≤K with the same
time complexity as for the computation of g(L)f .

Keywords: Nonparametric regression; Multiscale statistics; Variance estimation; Concentration inequalities; Graph signal processing; Spectral graph theory; Graph laplacian, Harmonic analysis on graphs

1

Introduction

Graphs provide a generic representation for modelling and processing data that reside on complex domains
such as transportation or social networks. Numerous works combining both concepts from algebraic and
spectral graphs with those from harmonic analysis (see for example [5, 6, 2] and references therein) have
allowed to generalize fundamental notions from signal processing to the context of graphs thus giving
rise to Graph Signal Processing (GSP). For an introduction to this emerging field and a review of recent
developments and results see [26] and [21]. In general, two types of problems can be distinguished
according to whether the underlying graph is known or unknown. The first case corresponds to the
setup of a sampled signal at certain irregularly spaced points (intersections of a transportation network,
nodes in a computer network, . . . ). In the second case, a graph is constructed from the data itself, it is
generally interpreted as a noisy realization of one or several distributions supported by a submanifold of
the Euclidean space. In this latter context, the theoretical submanifold is somehow approximated using
standard methods such as k-NN, ε-graph and their Gaussian weighted versions. In any of these cases, the
∗

Basile de Loynes
ENSAI, France, E-mail: basile.deloynes@ensai.fr
†
Fabien Navarro
CREST, ENSAI, France, E-mail: fabien.navarro@ensai.fr
‡
Baptiste Olivier
Orange Labs, France. E-mail: baptiste.olivier@orange.com

1

2

B. de Loynes, F. Navarro and B. Olivier

framework is actually similar: it consists of a graph (given by the application or by the data) and signals
are real-valued functions defined on the vertices of the graph.
Notions of graph Fourier analysis for signals on graphs were introduced and studied over the past
several years [26, 23, 27, 24]. The graph Fourier basis is given by the eigenbasis (χ` )` of the Laplacian
matrix L. The Graph Fourier Transform (GFT) consists in representing a signal f in the Fourier basis
(hf, χ` i)` , and by analogy with the standard case the eigenvalues of L play the role of frequencies. From
this definition, it follows that many filtering techniques are written in terms of vectors g(L)f , for some
filter functions g which act on the spectrum of L (scaling, selecting, ...). Fourier analysis on graphs has
been successfully applied to many different fields such as stationary signals on graphs [22], graph signal
energy study [14], convolutional neural networks on graphs [10].
Graph wavelets are an important application of graph Fourier analysis, and several definitions of graph
wavelets were proposed [7, 6, 13, 18, 31, 15]. When performing Fourier analysis of a signal, there is no
guarantee that localization of a signal in the frequency domain (a.k.a Fourier domain) implies localization
in the graph domain. This phenomenon is illustrated by the fact that the eigenvectors corresponding
to the upper part of Laplacian spectrum tend to be more oscillating than those from the bottom of the
spectrum (see for example [32, Fig. 1.6, p. 28] for an illustration). To overcome this problem, [16]
developed a fairly general construction of a frame enjoying the usual properties of standard wavelets:
each vector of the frame is defined as a function g(sL)δm (where δm is a signal with zero values at every
vertex except m) and is localized both in the graph domain and the spectral domain at fine scale s. The
transform associated with this frame is named Spectral Graph Wavelet Transform (SGWT), and it was
used in numerous subsequent works [30, 1, 15].
Signals which are sparse in the Fourier domain form an important class of graph signals. Indeed, there
is a tight relationship between sparsity in the Fourier domain and the notion of regularity of a signal f
on the vertices of a graph G which comes from the Laplacian matrix L of G. Intuitively, a smooth signal
will not vary much between two vertices that are close in the graph. This regularity property can be read
in the Fourier domain: a very smooth signal will be correctly represented in the Fourier domain with
a small number of eigenvectors associated with the lower spectral values; on the contrary, non-smooth
signals (i.e. highly oscillating) are represented with eigenvectors corresponding to the upper part of the
spectrum. Both the types of signal are said frequency sparse.
In this paper, we propose to exploit localization in the Fourier domain to improve graph Fourier
analysis. More precisely, we consider vectors of the form g(LIk )f instead of vectors g(L)f in graph
Fourier analysis, where LIk is defined as the matrix LPIk (L) and PIk (L) denotes the projection onto
the eigenspaces whose eigenvalue is contained in subset Ik . Localized Fourier analysis is motivated by
problems and properties defined on strict subsets of the spectrum sp(L) (e.g. any problem defined in
terms of frequency sparse graph signals). As a central application fo Fourier localization, we introduce
the Fourier localized counterpart of SGWT, that we call LocLets for Localized graph wavelets. We
prove that various frame constructions can be written in terms of LocLets, hence benefiting from all the
advantages of localization discussed in this paper.
Defining IL as the smallest interval containing the entire spectrum sp(L), the local Fourier analysis
consists in choosing a suitable partition IL = tk Ik into subintervals on which standard Fourier analysis
is performed. Such an analysis on disjoint intervals naturally benefits from several interesting properties.
In particular, when f is modeled by a Gaussian random vector with independent entries, the disjointness
of subintervals preserves these properties in the sense that random variables (g(LIk )f )k are still Gaussian
and independent. This simple observation has important consequences to study the graph problem at
stake. In this work, it allows us to propose some noise level estimator from the random variables sequence
(g(LIk )f )k , and to provide a theoretical analysis of the denoising problem. Disjointness of subsets (Ik )k
also provides simple strategies to parallelize Fourier analysis computations.
We also consider the general problem given by a noisy signal on a graph fe = f + ξ, where ξ is some
random Gaussian vector with noise level σ. We provide results for two important tasks: the estimation of
σ when the latter is unknown, and the denoising of noisy signal fe in order to recover signal f . We show

B. de Loynes, F. Navarro and B. Olivier

3

that for frequency sparse signals, localization allows to adapt to the unknown Fourier support of signal
f . Theoretical guarantees and practical experiments show that localized Fourier analysis can improve
state-of-the-art denoising techniques, not only in precision of the estimator fb of f , but also in time
computations.
We provide an efficient method to choose a partition IL = tk Ik for the Fourier localized vectors g(LIk )f
to be sufficiently informative. Using well-known techniques for efficient graph Fourier analysis (a.k.a
Chebyshev filters approximations), we propose scalable methods to perform localized Fourier analysis
with no computational overhead over standard fast Fourier graph analysis.
The paper is structured as follows. Section 2 presents the relevant notions and techniques necessary to
perform localized Fourier analysis. In Section 2.2, we introduce LocLets, the Fourier localized extension
of SGWT. Section 3 is devoted to the study of the denoising problem for signals on graphs. The section
provides results about noise level estimation, and signal denoising. Additional properties of LocLets,
such as computational aspects and relationships with known wavelet transforms, are further developed
in Section 4. In Section 5, we analyze the experiments made to support the interesting properties of
localized Fourier analysis highlighted in this paper. Finally, the proofs are gathered in Section 7.

2

Localized Fourier analysis for graph signals

In this section, we introduce the central notion studied in this paper: localization of graph Fourier analysis.
First, we recall the relevant notions of graph Fourier analysis. Then we introduce LocLets, an important
application of Fourier localization to SGWT.

2.1

Functional calculus and Fourier analysis for graph signals

Let G = (V, E) be an undirected weighted graph with V the set of vertices, E the set of edges, n = |V|
the number of nodes (the size of G), and (Wij )i,j≤n the weights
Pon edges. Let us introduce the diagonal
degree matrix whose diagonal coefficients are given by Dii = 1≤j≤n Wij for 1 ≤ i ≤ n. The resulting
non-normalized Laplacian matrix L of graph G is defined as L = D − W . The n non-negative eigenvalues
of L, counted without multiplicity, are denoted by λ1 , . . . , λn in the decreasing order. In the sequel, sp(L)
stands for the spectrum of L. The corresponding eigenvectors are denoted χ1 , . . . , χn .
Given a graph G, the GFT of a real-valued function f defined on the vertices of G is nothing but the
representation of f in the orthonormal basis of eigenvectors of L. Namely, for a signal f : G → R, the `-th
Fourier coefficient of f , denoted fb(`), is given by fb(`) = hf, χ` i. The Fourier support supp(fb) of signal f
is the set of indices ` such that fb(`) 6= 0. We will see in Section 2.2 that graph wavelets can be defined
in a similar manner.
Functional calculus is a powerful technique to study matrices which is the heart of GSP. For a function
g defined on some domain Dg , sp(L) ⊂ Dg , functional calculus reads as
X
g(L) =
g(λ` )hχ` , ·iχ` .
1≤`≤n

Interpreting the eigenvalues λ` , ` = 1, . . . , n, as the fundamental frequencies associated with a graph, the
linear map g(L) is generally seen as a filter operator in terms of signal analysis.
Also, spectral projections of matrix L can be explicited with the help of functional calculus setting
g = 1I . More precisely, for any subset I ⊂ IL , consider the map PI (L) given by:
X
X
PI (L) =
1I (λ` )hχ` , ·iχl =
hχ` , ·iχ` .
1≤`≤n

`: λ` ∈I

Then, PI (L) is nothing but the spectral projection on the linear subspace spanned by the eigevectors
associated with the eigenvalues belonging to I. In the sequel, nI = |I ∩ sp(L)| will stand for the number
of eigenvalues contained in subset I ∩ sp(L).

4

B. de Loynes, F. Navarro and B. Olivier

Spectral projections are a practical tool to focus on some part of the spectrum sp(L). More precisely,
let IL = t1≤k≤K Ik be a partition of interval IL into disjoint subsets (Ik )k . Since intervals Ik are disjoints,
functional analysis of L reduces to that of its projections LIk = LPIk (L) in the sense of the identity:
X
g(L) =
g(LIk ).
1≤k≤K

In this paper, one will study the extent to which Fourier analysis on large graphs is improved when
considering local Fourier analysis on each subset Ik instead of global Fourier analysis on IL .

2.2

LocLets: a localized version of SGWT

This section introduces an important application of the localized graph Fourier analysis, namely the
notion of localized SGWT.
2.2.1

Construction of a SGWT

Let f : G → R be a signal on the graph G. Let ϕ, ψ : R → R be respectively the scaling and kernel
functions (a.k.a. father and mother wavelet functions), and let sj > 0, 1 ≤ j ≤ J, be some scale values.
The discrete SGWT is defined in [16] as follows:
Wf = (ϕ(L)f T , ψ(s1 L)f T , . . . , ψ(sJ L)f T )T .
The adjoint matrix W ∗ of W is:
W ∗ (η0T , η1T , . . . , ηJT )T = ϕ(L)η0 +

J
X

ψ(sj L)ηj .

(1)

j=1

We also recall from [16] that a discrete transform reconstruction formula using SGWT coefficients (cj,m ) 0≤j≤J
1≤m≤n

is obtained by the formula
(W ∗ W)−1 W ∗ (cj,m )j,m ,
where (W ∗ W)−1 stands for a pseudo-inverse of matrix W ∗ W.
2.2.2

Definition of LocLets

Spectral graph wavelet functions are given by (ϕ(L)δm , ψ(sj L)δm )1≤j≤J,1≤m≤n . We define a LocLet
function to be the projection of a graph wavelet function onto a subset of eigenspaces of L.
Definition 1. Let (ϕ(L)δm , ψ(sj L)δm )1≤j≤J,1≤m≤n be the family functions induced by a SGWT. Then,
for any subset I ⊂ IL , 1 ≤ j ≤ J and 1 ≤ m ≤ n, set:
X
ϕm,I = ϕ(LI )δm =
ϕ(λ` )δbm (`)χ`
`:λ` ∈I

ψj,m,I = ψ(sj LI )δm =

X

ψ(sj λ` )δbm (`)χ` .

`:λ` ∈I

The functions (ϕm,I , ψj,m,I )1≤j≤J,1≤m≤n are called Localized waveLets functions (LocLets). The functions
ϕm,I , ψj,m,I are said to be localized at I.
Let IL = t1≤k≤K Ik be some partition. Then, the localized SGWT transfom of f with respect to
partition (Ik )1≤k≤K , denoted by W (Ik )k f , is defined as the family W (Ik )k f = (W Ik f )k where
W Ik f = (ϕ(LIk )f T , ψ(s1 LIk )f T , ...)T ,

1 ≤ k ≤ K.

B. de Loynes, F. Navarro and B. Olivier

5

Similarly to Equation (1), the adjoint transform is given by
W

Ik ∗

(η0T , η1T , . . . , ηJT )T

= ϕ(LIk )η0 +

J
X

ψ(sj LIk )ηj ,

1 ≤ k ≤ K.

j=1

As already observed, localized SGWT of a signal f contains more precise information about signal f than
its standard SGWT. The latter
P can easily be obtained from the former since subsets (Ik )k are pairwise
disjoint and formula g(sL) = 1≤k≤K g(sLIk ) holds for all filter g, and in particular for g = ϕ or g = ψ.
When the partition IL = t1≤k≤K Ik is carefully chosen, we show that the SGWT localization provides
interesting features such as independence of random variables in denoising modelling, or considerable
improvements in denoising tasks.
Remark 2. A different localization property is studied in [16]. The latter refers to the vanishing of
coefficients hψj,m , δm0 i when the geodesic distance in the graph between vertices m and m0 is sufficiently
large (see [16, Theorem 5.5]). This notion is a property observable in the graph domain (through impulse
functions δm and δm0 ), and is different from the notion of Fourier localization discussed in the current
paper. We refer to the localization property in [16] as graph domain localization.

3

Local Fourier analysis and graph functions denoising

The denoising problem is stated as follows: given an observed noisy signal fe of the form fe = f + ξ
where ξ is a n-dimensional Gaussian vector distributed as N (0, σ 2 Id), provide an estimator of the a priori
unknown signal f .
This section shows how localized Fourier analysis helps in estimating the noise level σ when it is
unknown, and in recovering the original signal f when the latter is sparse in the Fourier domain. In what
follows, we will focus on random variables of the form kPIk feIk k2 where fe is the noisy signal and Ik is a
subset in the partition IL = tk Ik . To keep the notations light, nk , fk , ξk and fek will stand for nIk , PIk f ,
PIk ξ and PIk fe respectively. In addition, the cumulative distribution function of a random variable X will
be denoted by ΦX .

3.1

Noise level estimation for frequency sparse signals

Since in real application the noise level σ remains unknown in general, new estimators σ
b based on localization properties in the spectrum are introduced in the sequel.
3.1.1

Noise level estimation from projections along sp(L)

For any filter g defined on IL and any subset I ⊂ IL , simple computation gives rises to
E(feT g(LI )fe) = f T g(LI )f + σ 2 Tr(g(LI )).

(2)

Since both feT g(LI )fe and Tr(g(LI )) are known, Equation (2) suggests building estimators from the exeT

eT

g(LI )f
Lf
pression fTr(g(L
. In [9], the noise level is estimated by fTr(L)
which can be seen as the graph analog of
I ))
the Von Neumann estimator from [33]. The main drawback of this estimator is its bias.
T g(L )f
I
Theoretically, without any assumption on the signal f , the bias term fTr(g(L
is minimized when
I ))
∗
g = 1{λ ∗ } where ` = argmin{|fb(`)| : λ` ∈ sp(L)}. The computation of such filters would require the
e

e

`

complete reduction of L which does not scale well with the size of the graph. Instead, these ideal filters
will be approximated by filters of the form g = 1Ik , for Ik a subset in the partition IL = tk Ik . It is worth
∗
noting that with k ∗ = argmink kfP
k k2 , the function g = 1Ik∗ achieves the minimal bias of the estimator
among all filters of the form g = k αk 1Ik .

6

B. de Loynes, F. Navarro and B. Olivier

Discarding some intervals Ik with nk = 0, it can be assumed without loss of generality that nk 6= 0
for all 1 ≤ k ≤ K. Also observe that the random variable kfek k22 can be decomposed as follows
kfek k22 = kfk k22 + kξk k22 + 2hfk , ξk i
where

kξk k22
σ2

and

hfk ,ξk i
σ

(3)

are random variables distributed as χ2 (nk ) and N (0, kfk k22 ) respectively.

Proposition 3. Let (ck )1≤k≤K be the sequence of non-negative random variables defined, for all k =
1, . . . , K, by ck = kfek k22 /nk . Then,
1. the random variables c1 , . . . , cK are independent;
2. for all k, k 0 such that fk = fk0 , ck and ck0 are identically distributed if and only if nk = nk0 ;
3. for k such that fk = 0, ck is distributed as
3.1.2

σ2
nk Γnk

where Γnk ∼ χ2 (nk ).

The case of frequency-sparse signals

When the signal f is sparse in the Fourier domain, the condition fk = 0 is met for most of the intervals
Ik ⊂ IL . Let us define If = tk:I ∩suppfb6=∅ Ik to be the union of subsets Ik intersecting the Fourier support
k
supp(fb) of f . Also, denote by If = IL \If its complement set. In order to take advantage of Fourier
sparsity, let us introduce the quantities σ
bmean and σ
bmed as follows:
σ
bmean (c)2 =

1

X

|{k : Ik ⊂ If }|

ck

and σ
bmed (c)2 = mediank:Ik ⊂If (ck ).

(4)

k:Ik ⊂If

The following concentration inequalities show that σ
bmean and σ
bmed are natural estimators of the noise
level σ.
Proposition
P 4. Let Kf = |{k : Ik ⊂ If }|, n0 = min{nk : k, Ik ⊂ If }, n∞ = max{nk : k, Ik ⊂ If },
Vf = 2σ 4 k:Ik ⊂If 1/nk and Bf = 2σ 2 /n0 . Then the following concentration inequalities hold:
1. for all t ≥ 0,


P σ
bmean (c)2 − σ 2 ≥ t ≤ exp −

Kf2 t2
q
Vf (1 + Bf + 1 +


2Bf Kf t
)
Vf

,

and for all 0 ≤ t ≤ σ 2 ,
Kf2 t2

2
2
P σ
bmean (c) − σ ≤ −t ≤ exp −
2Vf

!
;

2. for all t ≥ 0, with β = n0 /n∞ ,
P

2
σ
bmed

≥β

−1 2

2 −1

σ + 2σ β


t ≤ exp



i
Kf h +
+
ln 4p (t)(1 − p (t)) ,
2

and for all 0 ≤ t ≤ 1 such that p− (t) ≤ 1/2,
P

2
σ
bmed

2

2


≤ βσ − σ βt ≤ exp



i
Kf h −
−
ln 4p (t)(1 − p (t)) ,
2

where
p+ (t) = P(Γn∞ ≥ n∞ + 2n∞ t)

and

p− (t) = P(Γn0 ≤ n0 − n0 t).

B. de Loynes, F. Navarro and B. Olivier

7

Obviously, the Fourier support supp(fb) and the subset If remain generally unknown in applications
and have to be approximated. Let us recall that the main issue for estimating σ comes from the bias term
kfk k22
kfk k22
2
nk in Equation (2), and in particular when the value σ is negligible compared to nk . Therefore,
a suitable candidate to approximate If will be some subset Jf ⊂ IL for which the impact of larger
kf k2

values nkk 2 is minimized. This is made clear by Proposition 5 below. The latter involves the following
concentration bounds for Gaussian random variables: for all 0 < α < 1
r
α
P(|hfk , ξk i| ≥ tα,σ kfk k2 ) ≤ α where tα,σ = σ × −2 ln
.
(5)
4
Proposition 5. Let 0 < α < 1. Let tα,σ be defined by Equation (5). Assume that f` = 0 and that the
following inequality holds:


kfk k22 + 2tα,σ kfk k2
3α
−1
≥ Φ nk Γ −Γ
1−
.
nk
σ2
2
n` n`
Then, the quantities
bk =

kξk k22 + kfk k22 + 2hξk , fk i
nk

and

b` =

kξ` k22
n`

satisfy P(bk ≥ b` ) ≥ 1 − α.
By invariance under permutations, one may assume without loss of generality that the values ck are
ordered in the decreasing order. Proposition 5 quantifies the fact that the highest values of ck correspond
most likely to the indices k for which fk 6= 0. Consequently, setting Jf (r) = tk∈{r,r+1,...K−r} Ik for all
1 ≤ r ≤ K2 , the estimators introduced in Equation (4) may be rewritten replacing the unknown subset If
by its known approximation Jf (r). So we define the estimators
r
σ
bmean
(c)2 =

1
|{k : Ik ⊂ Jf (r)}|

X

ck

r
(c)2 = medk:Ik ⊂Jf (r) (ck ).
and σ
bmed

k:Ik ⊂Jf (r)

r
It is worth noting that from the symmetry of the subset J f (r), it follows that the value σ
bmed
actually
r
does not depend on parameter r, and one will write σ
bmed in place of σ
bmed .

3.2

Denoising Frequency Sparse Signals

Let us begin with a result that illustrates that localized Fourier analysis in IL provides strong benefits in
noise reduction tasks when the underlying is frequency sparse.
Proposition 6. Assume f = fI for some subset I ⊂ IL . Then
i
h
i
h
2
2
E f − feI 2 = E f − fe 2 − σ 2 I ∩ sp(L) .
In particular, denoising of fe boils down to denoising of feI = fI + ξI .
While Proposition 6 asserts a trivial denoising solution in the Fourier domain, i.e. simply destroying
the projection feI = ξI , this approach is no longer that immediate when considering the graph domain
observations since the Fourier support of f is unknown in practice and needs to be estimated. Based
on the χ2 -statistics, Algorithm 1 is designed for this purpose. To the best of our knowledge, previous
works that proposed method for Fourier support recovery for graph noisy signals [25] involve the complete
eigendecomposition of matrix L. The methodology suggested below makes use of projectors on eigenspaces
which can be approximating with Chebyshev polynomials as detailed in the next Section 4.

8

B. de Loynes, F. Navarro and B. Olivier

Algorithm 1: Support approximation in the Fourier domain for noisy signal
Data: noisy signal fe, a subdivision I1 , I2 , . . . , IK , estimated nk = |Ik ∩ spL|, k = 1, . . . , K,
threshold α ∈ (0, 1)
Result: feI = PI (L)fe, where I is an approximation of the Fourier support of fe
1 for k = 1, . . . , K
e2
2 Compute kfek k2
2 = kPIk (L)f k2 ;
3 Compute
pk = P(σ 2 Γnk > kfek k22 ) and Γnk ∼ χ2 (nk );
X
4 Compute feI =
PIk fe.
k: pk ≤α

Heuristically, if I contains the support of the Fourier transform of f , on the complementary subset
I we only observe pure white Gaussian noise so that kPI fek22 = kfeI k22 is distributed as σ 2 χ2 (nI ) with
nI = |I ∩ sp(L)|. On the other hand, on I the square
of a non-centered Gaussian
 of the Euclidean norm

vector is observed. Consequently, the quantity P χ2 (nI ) > σ −2 kPI fek22 is typically very close to zero


whereas P χ2 (n − nI ) > σ −2 kPI fek22 remains away from 0. To put it in a nutshell, sliding a window
along the spectrum of L, Algorithm 1 performs a series of χ2 -test.
With the objective to provide theoretical guarantees that χ2 -tests approach supp(fb) correctly, it is
important to turn the condition on the pk -value into a condition involving only the values kfk k2 and σ.
The next lemma shows that for sufficiently large values of the ratio kfσk k2 , the inequality pk ≤ α holds
so that the corresponding components supp(fbk ) of the Fourier domain are legitimately included in the
support estimate I.
Lemma 7. Let 0 < α < 1 and let Γnk , Γ0nk be two i.i.d χ2 (nk ) random variables. Assume that:



tα/2,σ
α
kfk k2 kfk k2
1
−
−2
≥ Φ−1
,
0
Γnk −Γn
σ
σ
σ
2
k
where tα,σ is defined by Equation (5). Then pk ≤ α.
In contrast to Lemma 7, the lemma below states that condition pk > α holds for sufficiently small
values of ratio σ −1 kfk k2 .
Lemma 8. Let 0 < α < 1 and let Γnk be a χ2 (nk ) random variable. For 0 < β < 1, set tβ,k =
σ 2 Φ−1
Γn (1 − β). Assume that
k

kfk k2 +
σ

!2
p
tβ,k

<

Φ−1
Γnk



α
1−
1−β


.

Then pk > α.
Compared to Proposition 6, the result below quantifies the error resulting by approximating the
support running Algorithm 1. Note that the requirement to have a constant sequence (nk )k is used for
statement clarity but similar assertions hold for the case nk 6= nk0 .
P
Proposition 9. Set fI = k:pk ≤α PIk f . Assume that nk = n1 for all 1 ≤ k ≤ K. Then,
1. the Fourier support approximation `2 -error satisfies
r
kf − fI k22 ≤ |{k, Ik ⊂ If , pk > α}| tα/2,σ +



t2α/2,σ + σΦ−1
Γn

0
1 −Γn1



α 2
1−
2

!2
.

(6)

B. de Loynes, F. Navarro and B. Olivier

9

2. the Noise `2 -error on Fourier support:
EkfI − feI k22 = |{k, pk ≤ α}|n1 σ 2 .

(7)

Lemma 8 asserts that the set {k, pk > α} is small when most of the values kfk k2 are large enough
compared to noise level σ for Ik ∩ suppfb 6= ∅. In such a case, Fourier support approximation `2 -error is
small. Regarding the noise `2 -error, the inclusion {k, pk ≤ α} ⊂ {k, Ik ∩ suppfb 6= ∅} holds by Lemma
8. Moreover, Lemma 7 asserts that the set {k, pk ≤ α} contains the entire set {k, Ik ∩ suppfb 6= ∅} for
sufficiently large values of σ −1 kfk k2 when Ik ∩ suppfb 6= ∅. For such favorable situations, the noise `2 -error
is exactly n1 σ|{k, Ik ⊂ If }|, the amount of noise on the extended support If .
Algorithm 2: LocLets thresholding estimation procedure
Data: fe, α, (Ik )k=1,...,K , estimated nk = |Ik ∩ sp(L)|, thresholds t1 , t2
Result: estimator fb of signal f
1 Apply Algorithm 1 with fe, α, (Ik )k=1,...K , estimated nk ; it outputs feI and feI ;
2 Apply soft-thresholding with threshold t1 to W I fe and t2 to W I fe;
3
4

Apply the inverse LocLet transform to the soft-thresholded coefficients to obtain fbI , fbIe;
Compute the estimator fb = fbI + fbI ;

The second step gives an estimate of the original signal using a thresholding procedure on each element
feI and feI . On the one hand, the methodology developed in [15] is prohibitive in terms of time and space
complexity as soon as the underlying graphs become moderately large. On the other hand, the fast SGWT
remains an approximating procedure. If a signal happens to be very frequency-sparse, then an even more
optimal strategy is possible: first, the support I in the frequency domain is approximated with the help
of Algorithm 1; then, the procedure of [15] is applied to PI f (the low-rank part) and LocLets on PI (L)f .
This idea is made precisely in Algorithm 3.
Algorithm 3: LocLets support approximation, and low-rank Parseval Frame thresholding procedure
Data: fe, α, (Ik )k=1,...,K , estimated nk = |Ik ∩ sp(L)|, thresholds t1 , t2
Result: estimator fb of signal f
1 Apply Algorithm 1 with fe, α, (Ik )k=1,...K , estimated nk ; it outputs feI and fe ;
I

2
3
4
5
6

Compute Parseval Frame for LI ;
Apply Parseval Frame thresholding with threshold t1 to feI ; it outputs fbI ;
Apply soft-thresholding with threshold t2 to W I fe;
Apply the inverse LocLet transform to the soft-thresholded coefficients to obtain fbIe;
Compute the estimator fb = fbI + fbI ;

Estimator fb produced in Algorithm 3 satisfies a tighter oracle bound inequality than the one given in
[15, Theorem 3]. This theoretical guarantee is widely supported by our experiments described in Section
5. Following notations from [15, Equation (21)], we denote by OB(fI ) the oracle bound obtained from
an oracle estimator of fI from a noisy feI exploiting some knowledge about the unknown signal fI . We
refer to [15] for precise details.
Theorem 10. Let I, fb be respectively the support approximation and the estimator of f obtained from
Algorithms 1 and 3 with threshold value t2 = 0. Then we have
Ekf − fbk22 ≤ Ekf − fI k22 + (2 log(nI ) + 1)(σ 2 + OB(fI )).

10

B. de Loynes, F. Navarro and B. Olivier

The right-hand side in the inequality of Theorem 10 has a more explicit expression in terms of α, σ
using Proposition 9. Up to the error made by approximating the support with Algorithm 1, the `2 -risk is
essentially bounded by the `2 -risk of the Parseval frame procedure from [15] on the low-rank projection
fI of f , that is
Ekf − fbk22 . (2 log(nI ) + 1)(σ 2 + OB(fI )).
To conclude, Theorem 10 provides a theoretical guarantee that the support approximation improves the
denoising performances obtained from [15].

4

Properties of LocLets

In this section, we highlight important properties for the application of Fourier localization in practice.
First we discuss computational analysis, and methods to apply our techniques to large graphs. Then we
study the relationships of LocLets with well-known graph wavelet constructions.

4.1

Fast LocLet Transform and Computational Analysis

In the case of large graphs, GSP requires a special care for being efficient since functional calculus relies
a priori on the complete reduction of the Laplacian. Actually, several efficient methods were designed
to retrieve only partial information from the eigendecomposition as matrix reduction techniques (see for
instance [20, 30]) or polynomial approximations [16, 28, 11]. In this paper, the widely adopted latter
approach with Chebyshev polynomials approximation is preferred and briefly recalled below (we refer the
reader to [28, Section III.C.] for a brief but more detailed description of Chebyshev approximation).
4.1.1

Chebyshev approximations

Roughly speaking, the idea is to approximate the function g with its Chebyshev expansion gN at order
N . More precisely, the Chebyshev polynomials of the first kind (Ti )i≥0 are defined from the second order
recursion
T0 (x) = 1, T1 (x) = x, |x| ≤ 1, and Ti (x) = xTi−1 (x) − Ti−2 (x),
for i ≥ 2. Then, the matrix L is normalized as Le =

2
λ1 L

e ⊂ [−1, 1]. This gives rise
− In so that sp(L)
e In fact, ge(x) = g( λ1 (x + 1)) for all
to some function ge : [−1, 1] → R with the property g(L) = ge(L).
2
x ∈ [−1, 1]. Then g(L) has the following truncated Chebyshev expansion g(L) ≈ gN (L):
gN (L) =

X

e
ai (e
g )Ti (L),

0≤i≤N

where N is the maximal degree of polynomials Ti used in the expansion, and ai (e
g ) is the i-th coefficient
in the N -th order Chebyshev expansion of function ge. Following [16], for any filter g on sp(L) and any
signal f on graph G, the approximation gN (L) provides a vector value close to g(L)f with time complexity
O(|E|N ).
The object presented in the sequel involves in particular the spectral projection PI (L)f of a signal
f for any subset I ⊂ IL which can be derived from the Chebyshev expansion of the indicator function
g = 1I . This observation actually appears in several recent works [11, 12]. More importantly for our study,
this efficient estimation is part of the Hutchinson stochastic trace estimator technique [17], providing us
with an effective method to estimate nI = Tr(LI ). Finally, the present paper focuses on the computation
of a sequence g(LIk )1≤k≤K (or its vector counterpart g(LIk )f ) instead of a single g(L) (resp. g(L)f ).
While a naive estimation would suggest that the computational complexity is then multiplied by a factor
K compared to the complexity of the computation of g(L), we argue in the following that there is in fact
no significant computational overhead.

B. de Loynes, F. Navarro and B. Olivier

4.1.2

11

Sharing Chebyshev polynomial matrices among filters

Let us assume that it is needed to compute the estimated values of gk (L)f for a given signal f for several
filters gk , k = 0, . . . , K. Then the following
two-step strategy can be adopted: (1) pre-compute ChebyP
shev expansions gek (x) ≈ gek,N (x) =
0≤i≤N ai (gek )Ti (x) for all k = 0, . . . , K; independently, compute
e
Chebyshev approximation vectors Ti (L)f for all 0 ≤ i ≤ N ; (2) combine the previous results to compute
the Chebyshev approximation gk,N (L)f of gk (L)f :
X
e
gk,N (L)f =
ai (e
gk )Ti (L)f.
0≤i≤N

The complexity of the first step is dominated by the N matrix-vector multiplications required to
e . So the first step has complexity O(|E|N ). The second step adds N weighted matrices
obtain Ti (L)f
e
ai (e
gk )Ti (L) together, which is an operation of complexity O(N n2 ) at most. As an important matter of
fact, the overall complexity for this procedure is bounded by O(|E|N + N n2 ), which is independent of the
number of filters gk , and the same as for the computation of g(L).
Sharing matrices among filters has several examples of applications in the current paper:
1. Computation of g(LIk )f for all 1 ≤ k ≤ K: the equation g(LIk )f = g(1Ik (L)L)f holds so that we
can consider filters gk (x) = g(1Ik (x)x).
2. Computation of g(sL)f for several scale values s: consider filters of the form gs (x) = g(sx).
3. Computation of nIk for all 1 ≤ k ≤ K: Hutchinson’s stochastic estimation computes averages of
fiT PIk (L)fi for some random vectors fi (i ≤ nH ) whose computational complexity is dominated by
the approximation of vectors PIk fi . Considering filters gk (x) = 1Ik (x), and sharing random vectors
(fi )i among all approximations of nk , we end up with a complexity of O(nH N |E|), independent of
value K.
In particular, Algorithm 1 has complexity O(nH N |E| + N n2 ). Indeed, its efficiency is calibrated on
the computations of sequences (kfek k2 )1≤k≤K and (nk )1≤k≤K whose computational analysis was discussed
previously. It is worth observing that values nk do not depend on signal f and should be estimated only
once in the case where several signals fe1 , fe2 , . . . are to be denoised.
4.1.3

Optimizing storage of LocLets coefficients

The storage of wavelet coefficients (W Ik f )1≤k≤K requires a priori K times the storage cost associated
with the original transform Wf . When matrix reduction techniques are used to compute wavelets transform [30], one may reduce the storage consumption of the localized SGWT by suitably choosing the
k ) of the subimpulse functions (δm )m . For instance, assume that for each subset Ik a Lanczos basis (vm
m
k
k
space spanned by {χ` , ` ∈ Ik } is given. Then the size of sequences (vm )m and (vm )m,k are respectively of
k ) in place of δ for transform
order O(|Ik ∩ sp(L)|) = O(nk ) and O(n). Thus, with impulse functions (vm
n
m
W Ik , the storage requirements of localized transform (W Ik f )1≤k≤K and the original one Wf are of the
same order O(Jn).

4.2

Connections with Well-know Frames

A family F = {ri }i∈I of vectors of RV is a frame if there exist A, B > 0 satisfying for all f ∈ RV
X
Akf k22 ≤
|hf, ri i|2 ≤ Bkf k22 .
i∈I

A frame is said to be tight if A = B. This section gives two examples of frames introduced in the
literature which can be realized as a LocLets representation and thus benefit from the advantages given
by localization in the spectrum.

12

4.2.1

B. de Loynes, F. Navarro and B. Olivier

Parseval frames

Parseval frames are powerful representations to design wavelets with nice reconstruction properties [18, 15].
In this section, we investigate the extent to which Parseval frames can be obtained from some LocLet
representation. We show that for a particular choice of partition IL = tk Ik , there exist frames which are
Parseval frames and composed only of LocLets functions.
A finite collection (ψj )j=0,...,J is a finite partition of unity on the compact [0, λ1 ] if
ψj : [0, λ1 ] → [0, 1]

for all

j≤J

and ∀λ ∈ [0, λ1 ],

J
X

ψj (λ) = 1.

(8)

j=0

Given a finite partition of unity (ψj )j=0,...,J , the Parseval identity implies that the following set of vectors
is a tight frame:
o
np
ψj (L)δi , j = 0, . . . , J, i ∈ V .
F=
Some constructions of partition of unity involve functions (ψj )j that have almost pairwise disjoint supports
i.e. supp(ψj ) ∩ supp(ψj 0 ) = ∅ as soon as |j − j 0 | > 1. For such partition of unity, set I0 = supp(ψ0 ),
IJ = I0 = supp(ψJ ) and Ij = supp(ψj ) ∩ supp(ψj+1 ) for all 1 ≤ j ≤ J − 1. Then, the sequence (Ij )0≤j≤J
defines a finite partition of [0, λ1 ], [0, λ1 ] = t0≤j≤J Ij , such that:
ψ0 1I0 = 1I0 ,

(ψj + ψj+1 )1Ij = 1Ij ,

0 < j < J,

and ψJ 1IJ = 1IJ .

(9)

An alternative tight frame can be constructed using a LocLet representation as shown in the following
proposition.
√
ψ0 (LIk )δn ,
Proposition
11.
Assume
Equations
(8)
and
(9)
hold
and
set,
for
all
1
≤
k
≤
J,
ϕ
=
n,k
√
√
ψ1,n,k = ψk (LIk )δn and ψ2,n,k = ψk (LIk+1 )δn for all 1 ≤ k ≤ J. Then (ϕn,k , ψj,n,k )1≤j≤2, 1≤m≤n, 1≤k≤J
is a tight frame.
The resulting tight frame of Proposition 11 is actually frame of LocLets if additionally the functions ψj
is of the form ψj = ψ1 (sj .) for some scale parameter sj , 1 ≤ j ≤ J. This is typically the case for the frames
introduced in [18, 15]. In these papers, the partition of unity is defined as follows: let ω : R+ → [0, 1] be
some function with support in [0, 1], satisfying ω ≡ 1 on [0, b−1 ] and set ψ0 (·) = ω(·) and for j = 1, . . . , J


log λ1
−j
−j+1
ψj (·) = ω(b ·) − ω(b
·) with J =
+ 2.
log b
In particular, the functions ψk have supports in intervals Jk = [bk−2 , bk ]. Thus, one may define disjoint
intervals (Ik )k as follows: Ik = [bk−1 , bk ]. We have Jk = Ik ∪ Ik+1 , so that Equations (9) hold whereas the
scaling property ψj = ψ1 (b−1 .) is straightforward. By Proposition 11, the set of vectors
np
o
p
p
ψ0 (LIk )δn , ψ1 (sk LIk )δn , ψ1 (sk LIk+1 )δn , n, k
is a tight frame of LocLets. Observe that the transform (W Ik )Ik each component W Ik of the LocLet
transform (W Ik )Ik only admit two scale parameters sk , sk−1 .
4.2.2

Spectrum-adapted tight frames

Let us consider another family of tight frames tailored to the distribution of the Laplacian L eigenvalues
proposed in [1]. As shown below, these frames can be written in terms of a warped version of LocLets, and
up to some approximation, in terms of (non-warped) LocLets. First, let us briefly recall the construction
from [1].

B. de Loynes, F. Navarro and B. Olivier

13

The notion of warped SGWT is introduced in [1] to adapt the kernel to the spectral distribution.
Given a warping function ω : IL → R, the warped SGWT is defined as:
W ω f = (ϕ(ω(L))f T , ψ(s1 ω(L))f T , . . . , ψ(sJ ω(L))f T )T .
As for our spectral localization, the objective of warping is to take benefits from the distribution of sp(L)
along interval IL . While the two techniques show similarities (e.g. estimation of sp(L) distribution), they
are meant to answer different problems: warped SGWT is a technique to adapt the whole spectrum to
some task (e.g. producing a tight frame), whereas localized SGWT is designed to answer problems related
to localized subsets in the spectrum (e.g. denoising a frequency sparse signal). Here we show that the
advantages of both LocLets and warped SGWT are obtained when the two methods are combined in a
warped LocLet representation.
Let ω be some warping function on IL chosen in the form ω(·) = log(Cω0 (·)) where ω0 stands for
the cumulative spectral distribution of L and C is some normalization constant as shown in [1]. Then,
let γ > 0 be an upper bound on sp(L) and let R, J be two integers such that 2 ≤ R ≤ J. Setting
γ
ωγ,J,R = J+1+R
, Corollary 2 in [1] asserts that the family (gm,j )m,j of functions defined below is a tight
frame
X
gm,j =
gbj (λ` )δbm (`)χ` ,
(10)
`

where functions gbj arise from some kernel gb as


Cω(λ)
gbj (λ) = gb(ω(λ) − jωγ,J,R ) = gb log jωγ,J,R .
e
Typically in [1], the kernel gb takes the form

gb(λ) = 

X





aj cos 2πj cos

0≤j≤J

1
λ
+
Rωγ,J,R 2




 1[−Rω

γ,J,R ,0]

(λ).

P
for some sequence (aj )j satisfying j (−1)j aj = 0.
The following proposition states that Equation (10) admits an alternative form involving only (warped)
LocLets functions.
Proposition 12. Setting ψ(λ) = gb(log(Cλ)) for λ > 0, consider the family of warped LocLets defined for
all 0 ≤ k ≤ R − 1, 1 ≤ m ≤ n and 1 ≤ j ≤ J by
"
#
(k−R)ωγ,J,R e(k−R+1)ωγ,J,R
X
e
ψj,m,Ik =
ψ(sj ω0 (λ` ))δbm (`)χ` with Ik =
,
.
C
C
`∈Ik

Then, the following identity holds for all j = 1, . . . , J and all m = 1, . . . , n
gm,j =

X

ψj,m,Ik .

1≤k≤R−1

5

Experiments on some large matrices suite

This section details experiments made on large graphs to validate the Fourier localization techniques
introduced in that paper. After describing the experimental settings, we describe the outcomes of several
experiments showing strong advantages in the use of Fourier localization in practice.

14

5.1

B. de Loynes, F. Navarro and B. Olivier

Choice of spectral partition IL = tk Ik

In order to keep the problem combinatorially tractable, it is necessary to reduce the choice of possible
partitions of IL into subintervals Ik . That is why, the partitions considered in the sequel are regular in the
sense that all intervals have the same length λ1 /K for some integer K ≥ 1. Thereafter, the parameter K
is chosen so that the eigenvalues are distributed as evenly as possible in each interval Ik . Without prior
information, it is indeed natural not to favor one part of the spectrum over another. Most importantly,
in the view of the concentration property of the median around the noise level σ 2 of Proposition 4, it is
essential to keep the parameter β as close to one as possible.
In order to implement the ideas above, it is necessary to estimate the spectral measure of L which
can be described by the so-called spectral density function:
n

ϕL (λ) =

1X
δ(λ − λ` )
n

for all

λ ∈ IL .

`=1

There are several techniques for such an approximation among which the Kernel Polynomial Method
(see, e.g. [29, 34]). The latter approximates the spectral density ϕL with the help of matrix Chebyshev
expansion (ϕN
L )N (see [19] for a detailed presentation).
Now, let (Ik )1≤k≤K be some regular partition of IL and (nk )1≤k≤K be the corresponding numbers of
eigenvalues in each Ik . Choosing the parameter K ≥ 1 so that the entropy defined by
n 
X nk
k
E(K) = −
log
n
n
1≤k≤K

is maximal ensures that the eigenvalues are as equally distributed in each interval as possible. In application, the Kernel Polynomial Method provides an approximation nN
k of nk and the corresponding empirical
entropy EN (K) is used as a proxy for the theoretical one.
Empirically, the entropy increases logarithmically and then stabilizes from a certain elbow value Kelbow
as illustrated in Figure 1. This elbow value is displayed in dashed lines in Figure 1. In the experiments,
we choose this value Kelbow motivated by two reasons. First, as the intervals become shorter it is more
difficult to obtain a uniform repartition of the eigenvalues into those intervals. The second reason is
related to the quality of the estimate nN
k of nk as the sample size decreases. To illustrate this fact, we
consider the Mean Relative Error (MRE) defined by
P
N
1≤k≤K |nk − nk |
MREN (K) =
.
n
As highlighted by Figure 1, the empirical entropy actually stabilizes when the Chebyshev approximation,
in terms of MRE, is no longer sharp enough.

5.2

The experimental settings

Following [12], we propose to validate our techniques on an extended suite of large matrices extracted
from the Sparse Matrix Collection in [8]. Most of these matrices have an interpretation as the Laplacian
matrix of a large graph. We define matrix L from the following matrices of the suite:si2 (n = 769),
minnesota (n = 2642), cage9 (n = 3534), saylr4 (n = 3564) and net25 (n = 9520). We extend this graph
collection with the well-studied swissroll graph Laplacian matrix (n = 1000).
We sample randomly signals whose support are sparse in the Fourier domain. We will use the notation fi−j for normalized signals supported on a sub-interval of IL containing exactly the eigenvalues
λi , λi+1 , . . . , λj . As an example, fn−n is a constant signal while f1−2 is a highly non-smooth signal supported on the eigenspaces of large eigenvalues λ1 , λ2 . For experiments, the signals were calculated from
the knowledge of sp(L), and relevant projections of random functions on the graph.

B. de Loynes, F. Navarro and B. Olivier

3.0
2.5
2.0
1.5
1.0
0.5
0.0

Kelbow = 22
EN(K)
MREN(K)

10

20

30

40
K

50

(a) Si2 graph

60

70

3.0
2.5
2.0
1.5
1.0
0.5
0.0

15

Kelbow = 22
EN(K)
MREN(K)

10

20

30

40
K

50

60

70

(b) minnesota graph

Figure 1: Variations of EN (K) and MREN (K) with parameter K.
We have compared the performances of Algorithms 2 and 3 against the thresholding procedure described in [15]. As the denoising method in [15] requires the computation of the whole spectral decomposition of the Laplacian, it does not scale to large graphs. We stress here that we provide a fair comparison
with [15], only in terms of denoising performance, and with no computational considerations. Moreover,
we choose for LocLets to use the most naive thresholding procedure by considering a global and scale
independent threshold level.
For all the experiments below, the SGWT and LocLets are built upon the scale and kernel functions
giving rise to the Parseval
√ frame of [15],
√ whose construction is recalled in Section 4.2.1. More precisely,
set respectively ϕ = ζ0 and ψ = ζ1 for the scale and kernel functions with ζ0 (x) = ω(x), and
ζ1 (x) = ω(b−1 x) − ω(x), where we choose b = 2 and ω is piecewise linear, vanishes on [1, ∞) and is
constant equal to one on (−∞, b]. The scales are of the form sj = b−j+1 for j = 1, . . . , J where J is chosen
similarly to [15].
In what follows, ‘PF’ stands for Parseval Frame and refers to the estimator of [15]; the estimators
implemented by Algorithm 2 and Algorithm 3 are referred to as ‘LLet’ and ‘LLet+PF’ respectively. The
notation ‘SNRin ’ refers to the trivial model releasing the noisy signal fe, corresponding to the classical
input noise level measurement, and serves as a worst-case baseline for other models. Below, the latter
methodology is shown to outperform all the others for very frequency-sparse signals. It is also worth
recalling that ‘LLet+PF’ benefits from the dimension reduction property of LocLets. More precisely,
whereas the whole eigendecomposition of L is required to apply ‘PF’, for Parseval frame denoising in the
context of ‘LLet+PF’, only a low-rank spectral decomposition is needed, namely the decomposition of LI
for I the estimate of suppfb.
For all our experiments, we set α = 0.001 for Algorithm 1. For the denoising experiment, we compute
the best SNR result rD over a large grid of values (t1 , t2 ), and for each denoising method D with D ∈
{‘SNRin ’,‘PF’,‘LLet’,‘LLet+PF’}. Then, we calculate two metrics: the maximum MD and average value
µD of the values rD over 10 random perturbations of the signal f . We recall that a good quality in
denoising is reflected by a large value of the SNR metric.

5.3
5.3.1

Analysis of our experiments
Noise level estimation

r
We have evaluated the performances of estimators σ
bmean
and σ
bmed in the estimation of the unknown
e
noise level σ from 10 realizations of the noisy signal f = f + ξ for a given noise level σ. Figure 2 (resp.
Figure 3 ) shows the best performances of each estimator on the minnesota (resp. net25 ) graph for the
non-regular but frequency sparse signal f1392−1343 (resp. f = f4971−5020 ), when parameter K ranges in

16

B. de Loynes, F. Navarro and B. Olivier

0.0110
0.0108
0.0106
0.0104
0.0102
0.0100
0.0098

5

10

20

K

30

40

50

0.0110
0.0108
0.0106
0.0104
0.0102
0.0100
0.0098

5

10

20

K

30

40

50

Figure 2: Performances of estimators σ
bmean (left) and σ
bmed (right) for minnesota graph, signal f1392−1343
and σ = 0.01.

0.00105
0.00100
0.00095
0.00090
0.00085
0.00080

0.00105
0.00100
0.00095
0.00090
0.00085
0.00080
5

10

20

K

30

40

50

5

10

20

K

30

40

50

r
r
(right) for net25 graph, signal f4971−5020 and
(left) and σ
bmed
Figure 3: Performances of estimators σ
bmean
σ = 0.001.

{5, 10, 20, 30, 40, 50} and for level of noise σ = 0.01 (resp. σ = 0.001).
r
Figure 2 illustrates that both estimators σ
bmean
and σ
bmed can provide good estimates of σ. Best
performances are obtained for values of parameter K below the elbow value Kelbow (minnesota) = 22
introduced in Section 5.1. We observe that performances drop considerably if almost no localization is
used (for instance, for parameter values K = 1 or K = 2, σ
b ∼ 0.021 in the experiment of Figure 2, far
from the performances for K ≥ 5 for estimating σ = 0.01).
Figure 3 shows that localization is necessary, namely K ≥ 10 or even K ≥ 20, in order to reach the best
performances for the large net25 graph. Contrary to experiments for the minnesota graph, estimators
r
σ
bmean
and σ
bmed underestimate the value of σ. Also, best values of K range between 10 and 30 for net25
graph, compared to best values K = 5 and K = 10 for minnesota graph (see Figure 2). This illustrates the
idea that noise level estimation strongly depends on the underlying graph structure. As a consequence, a
parameter K selection has to take graph and signal information into account to be relevant. Interestingly,
the elbow values Kelbow (minnesota) = 22 and Kelbow (net25) = 22 provide performances which are not
optimal, but close to the best possible ones.
In Figure 4, performances for various values of parameter r are displayed for a fixed parameter K =
r
r , it happens only for very
Kelbow (minnesota). While it is true that σ
bmean
can perform better than σ
bmed
specific values of r, which a priori depend on the signal regularity. Without any further parameter
selection, these observations suggest using the most robust estimator σ
bmed in practice.
5.3.2

Sparse signal denoising

As a first denoising experiment, we have compared the performances of ‘LLet’, ‘PF’ and ‘LLet+PF’ for
a fixed value K = Kelbow given by the rule of thumb described in Section 5.1. For each matrix in the
Extended Matrices Suite, we have experimented the denoising task on two frequency-sparse signals, one

B. de Loynes, F. Navarro and B. Olivier

0.1010
0.1005
0.1000
0.0995
0.0990
0.0985

mean
med

0

2

4

r

6

8

10

0.022
0.020
0.018
0.016
0.014
0.012
0.010

17

mean
med

0

2

4

r

6

8

Figure 4: Dependence on parameter r for minnesota graph, signal f1392−1343 , K = 22 and σ = 0.1 (left),
σ = 0.01 (right).
regular and the other non-regular. Several values of noise level σ were used, corresponding to values of
SNRin ranging in [4, 18]. Results from experiments are displayed in Tables 5.3.2 and 2. The first obvious
observation is that ‘LLet+PF’ performs better than its competitors in almost all situations. The gain is
sometimes considerable since we observed a gap of 5dB in µD -metric between ‘LLet+PF’ and its closest
concurrent ‘PF’ in some cases, and up to 7dB in MD -metric. These experiments confirm the theoretical
guarantees obtained in Theorem 10. The benefits of localization are reduced for graph net25 : Table 2
shows that the more conservative choice K = 5 is better than K = 25. It appears that for net25, the
spectrum sp(L) is localized at a small number of distinct eigenvalues, hence diminishing the advantages
of localizing with our methods.
Table 1: SNR performance for Swissroll (n = 1000, K = 22).
signal
f951−1000
f501−550
f951−1000
f501−550
f951−1000
f501−550

σ

SNRin

MPF

MLLet

MLLet+PF

µPF

µLLet

µLLet+PF

0.005
0.005
0.01
0.01
0.015
0.015

16.195
15.859
10.267
10.178
6.763
6.362

17.557
18.298
12.183
13.121
9.430
9.898

20.580
8.244
15.564
7.879
12.661
7.611

20.528
20.821
15.701
16.204
13.129
14.159

17.361
18.044
10.433
11.165
8.961
9.540

20.035
8.140
13.652
7.646
12.127
7.481

19.974
20.245
13.760
14.518
12.388
13.398

Another interesting observation is that ‘LLet’ may outperform ‘PF’ in some specific signal and noise
level configurations, as shown in Table 5.3.2. This is a very favorable result for localized Fourier analysis,
since ‘LLet’ appears to be a technique which is more accurate and more efficient as well compared to ‘PF’
in some situations. However in many cases, ‘LLet’ performances drop down compared to the more stable
thresholding techniques ‘PF’ and ‘LLet+PF’, which use thresholds adapted to the wavelet basis.
We also provide experimental results to understand the extent to which our results depend on the
partition size parameter K. A few remarks are suggested by Figure 5:
• The best performances are not obtained for the elbow value Kelbow , suggesting searching for a more
task-adapted size of partition K.
• Good performances persist for values of K much larger than Kelbow , and in particular for regular
signals.
• For large values of K, there is a severe drop in performances. As explained before, the error

18

B. de Loynes, F. Navarro and B. Olivier

Table 2: SNR performances for denoising task.
matrix
Si2
(n = 762, K = 22)

Minnesota
(n = 2642, K = 22)

Cage9
(n = 3534, K = 22)

Saylr4
(n = 3564, K = 22)

Net25
(n = 9520, K = 5)

Net25
(n = 9520, K = 25)

signal

σ

SNRin

MPF

MLLet+PF

µPF

µLLet+PF

f720−769
f370−419
f720−769
f370−419
f720−769
f370−419

0.005
0.005
0.01
0.01
0.02
0.02
0.004
0.004
0.005
0.005
0.01
0.01
0.003
0.003
0.005
0.005
0.009
0.009
0.003
0.003
0.005
0.005
0.009
0.009
0.006
0.006
0.007
0.007
0.008
0.008
0.006
0.006
0.007
0.007
0.008
0.008

17.104
16.820
11.408
11.175
4.826
5.047
13.599
13.741
11.681
11.916
5.911
5.843
15.016
15.014
10.503
10.507
5.423
5.395
14.871
15.069
10.478
10.635
5.420
5.480
4.682
4.577
3.282
3.406
2.208
2.153
4.495
4.574
3.307
3.256
2.111
2.150

22.344
18.034
16.821
12.444
11.476
7.354
17.839
15.999
16.086
14.459
10.875
9.660
20.477
15.876
17.290
12.118
13.002
8.329
23.108
21.365
19.135
17.016
15.277
12.055
5.171
5.811
5.287
5.267
5.171
4.450
5.319
5.849
5.264
5.132
4.913
4.345

26.973
21.778
22.501
19.740
15.695
13.542
20.717
20.388
19.342
18.392
14.143
11.762
9.700
16.945
18.410
13.032
13.264
10.468
24.516
23.903
20.966
19.610
17.070
14.802
5.319
6.035
5.416
5.451
5.268
4.594
6.004
4.752
5.765
4.182
5.249
3.670

21.849
17.813
16.572
12.151
11.104
6.925
17.672
15.822
15.830
14.298
10.605
9.409
20.216
15.798
16.772
12.035
12.763
8.168
23.040
21.010
18.943
16.662
14.791
11.830
5.094
5.714
5.127
5.104
4.928
4.319
5.190
5.714
4.951
5.014
4.756
4.251

25.170
20.673
20.558
17.121
14.711
12.677
20.035
19.234
18.417
18.029
13.556
10.952
9.664
16.799
18.185
12.898
12.898
9.827
24.117
23.412
20.268
18.732
16.567
14.031
5.205
5.933
5.251
5.266
5.034
4.471
5.834
4.579
5.447
4.102
5.128
3.554

f2593−2642
f1343−1392
f2593−2642
f1343−1392
f2593−2642
f1343−1392
f3485−3534
f1785−1834
f3485−3534
f1785−1834
f3485−3534
f1785−1834
f3515−3564
f2015−2064
f3515−3564
f2015−2064
f3515−3564
f2015−2064
f9471−9520
f4971−5020
f9471−9520
f4971−5020
f9471−9520
f4971−5020
f9471−9520
f4971−5020
f9471−9520
f4971−5020
f9471−9520
f4971−5020

B. de Loynes, F. Navarro and B. Olivier

18

SNR(f, f)

SNR(f, f)

20
16
14
12

5

10

20

K

30

40

50

17.5
15.0
12.5
10.0
7.5
5.0
2.5

5

10

19

20

K

30

40

50

Figure 5: SNR performance depending on parameter K for minnesota graph, σ = 0.01, a regular signal
f2593−2642 (left) and a non-regular signal f1343−1392 (right), over 10 realizations of noise.
generated by Chebyshev’s approximation grows with the number of intervals in the partition, which
makes the approximation of the support more difficult.

6

Conclusion and future works

We have introduced a novel technique to perform efficiently graph Fourier analysis. This technique uses
functional calculus to perform Fourier analysis on different subsets of the graph Laplacian spectrum. In
this paper, we have demonstrated that localization in the spectrum provides interesting improvements in
theoretical results for some graph signal analysis tasks. New estimators of the noise level were introduced
taking advantage of the convenient modelling of the denoising problem given by localization, and for
which concentration results were proved. Localization allows also to study theoretically the denoising
procedure with wavelets, and fits with the design of many well-known techniques (e.g. tight frames for
graph analysis). Through many experiments, we have validated that localization techniques introduced
in this paper improve on state-of-the-art methods for several standard tasks.
Although we provide a rule of thumb to choose a partition IL = t1≤k≤K Ik for which denoising results
show good performances, experiments suggest that our elbow rule is not optimal in most cases. There
is certainly an interesting topic in searching for a suitable partition IL = t1≤k≤K Ik that would be more
adapted to a specific task (e.g. denoising). To extend the current work, it would also be interesting to
consider other common tasks in GSP, such as de-convolution or in-painting.

7

Proofs

P
b
b
Proof of Proposition 3.
1. We have f˜k = fk +ξk , where ξk = `:λ` ∈Ik ξ(`)χ
` . Random variables (ξ(`))`
2
are all distributed as N (0, σ ) and independent by orthogonality of the eigenbasis (χ` )` . In particular
b over disjoint
for k 6= k 0 , vectors ξk and ξk0 are independent as expressions involving variables ξ(`)
subsets Ik and Ik0 . Thus the random variables (ck )1≤k≤K are also independent.
2. When nk = nk0 , ξk and ξk0 are identically distributed and the result follows from Equality (3).
When nk 6= nk0 , we have E(ck ) 6= E(ck0 ) as the following equality holds for all 1 ≤ k ≤ K:
E(ck ) =

kfk k22
+ σ2.
nk

b ` are independent normal variables N (0, σ 2 ), the statement is clear from the expression
3. Since (ξ(`))
1 P
b 2.
ck = nk `:λ` ∈Ik |ξ(`)|

20

B. de Loynes, F. Navarro and B. Olivier

The following lemma is useful for the proof of Proposition 4.
Lemma 13. Let Z ∼ B(n, p) for some parameters n ≥ 1 and p ≤ 1/2. Then
n

P(Z ≥ dn/2e) ≤ exp
ln(4p(1 − p)) .
2
Proof. A simple consequence of [4, Theorem 1] implies that for all n ≥ 1 and all a ≥ p

P(Z ≥ na) ≤

(1 − p)1−a
1−a
1
2

Now the result follows since na = dn/2e implies
(1 − p)1−a
1−a



≤a≤



1
2

1−a
p
a

+

1
n

a n
.

so that


p
1 − a a (1 − a)a−1 p
p(1
−
p)
≤
4p(1 − p).
p ≤
a
aa

2

2

Proof of Proposition 4.
1. For k such that Ik ⊂ If , we have ck = nσk Γnk , which follows the Γ( n2k , 2σ
nk )
distribution. Then concentration inequalities for σ
bmean (c)2 are a direct consequence of Theorem
2
2.57 in [3], applied with ak = n2k and bk = 2σ
nk .
2. For all k = 1, . . . , Kf , we define
γk− = Φ−1
Γn ◦ ΦΓnk
0

n

k
ck
σ2



and γk+ = Φ−1
Γn∞ ◦ ΦΓnk

n

k
ck
σ2



.

As a matter of fact, (γ − )k=1,...,Kf and (γk+ )k=1,...,Kf are two sequences of i.i.d. random variables
with γ1− ∼ χ2 (n0 ) and γ1+ ∼ χ2 (n∞ ) such that
∀k = 1, . . . , Kf ,

γk− ≤

nk
ck ≤ γk+
σ2

almost surely.

Then, for all t > 0,
2
P σ
bmed





Kf
X

K
f 
≥ β −1 σ 2 + 2σ 2 β −1 t = P 
1{ck ≥β −1 σ2 +2σ2 β −1 t} ≥
2
k=1




Kf
X
Kf 
≤ P
1{γ + ≥n∞ +2n∞ t} ≥
.
k
2

(11)

k=1

Similarly, for all t ∈ (0, 1),
2
P σ
bmed





Kf
X

Kf 
≤ βσ 2 − σ 2 βt ≤ P 
1{γ − ≤n0 −n0 t} ≥
.
k
2
k=1

To conclude, apply Lemma 13 to Inequalities (11) and (12) to obtain our result.

(12)

B. de Loynes, F. Navarro and B. Olivier

21

Proof of Proposition 5. The concentration bound of Equation (5) implies that





−2 nk
2
2
−2
2
P(bk ≥ b` ) = P(nk bk ≥ nk b` ) = P σ
kξ` k2 − kξk k2 ≤ σ
kfk k2 + 2hfk , ξk i
n`





α
−2 nk
2
2
−2
2
kξ` k2 − kξk k2 ≤ σ
kfk k2 + 2tα,σ kfk k2 .
≥ +P σ
2
n`
Since subsets Ik and I` are disjoint, random variables kξk k22 and kξ` k22 are independent. Thus, nnk` kξ` k22 −
kξk k22 is distributed as nnk` Γn` −Γnk where Γnk and Γn` are independent random variables with Γnk ∼ χ2 (nk )
and Γn` ∼ χ2 (n` ). Therefore, the statement of Proposition 5 follows.
Proof of Proposition 6. First, the following equalities hold:
f − fe = f − feI + feI − fe = (f − fe)I + feI .
As (f − fe)I and feI are orthogonal vectors, it follows that
f − fe

2
2

= f − feI

2
2

2

+ feI

2

.

It remains to notice that E(||feI ||2 ) = σ 2 |I ∩ sp(L)|.
Proof of Lemma 7. By Equation (3) and the concentration bound of Equation (5), it follows that
pk = P(σ 2 Γnk > kξk k22 + kfk k22 + 2hfk , ξk i)

α
≤ + P σ 2 Γnk > kξk k22 + kfk k22 − 2kfk k2 tα/2,σ
2

α
= + P σ 2 (Γnk − Γ0nk ) > kfk k22 − 2kfk k2 tα/2,σ
2
α
= + 1 − ΦΓnk −Γ0n (θ(fk , α, σ)),
k
2
where θ(fk , α, σ) = σ −2 (kfk k2 − 2tα/2,σ )kfk k2 . Consequently, 1 − ΦΓnk −Γ0n (θ(fk , α, σ)) ≤ α/2 and pk ≤
k
α.
Proof of Lemma 8. Using an estimate on the χ2 (nk ) tail distribution and independence of Γnk and Γ0nk =
σ −2 kξk k22 , it follows

pk ≥ P σ 2 Γnk > kξk k22 + kfk k22 + 2hξk , fk i, kξk k22 ≤ tβ,k

≥ P σ 2 Γnk > kξk k22 + kfk k22 + 2kξk k2 kfk k2 , kξk k22 ≤ tβ,k

p
= P σ 2 Γnk > (kfk k2 + tβ,k )2 , kξk k22 ≤ tβ,k

p
= P σ 2 Γnk > (kfk k2 + tβ,k )2 , σ 2 Γ0nk ≤ tβ,k

p
≥ P σ 2 Γnk > (kfk k2 + tβ,k )2 (1 − β)
α
× (1 − β) = α.
≥
1−β

Proof of Proposition 9.

1. To prove Inequality (6), first observe that f =
f − fI =

X
k:Ik ⊂If

fk −

X
k:pk ≤α

fk .

P

k:Ik ⊂If

fk so that

22

B. de Loynes, F. Navarro and B. Olivier

The summands which are not present in both terms are exactly those satisfying either Ik ⊂ If and
pk > α or Ik ∩ If = ∅ and pk ≤ α. Noting that fk = 0 when Ik ∩ If = ∅, it comes
X
kf − fI k22 =
kfk k22 .
k:Ik ⊂If ,pk >α

Applying Lemma 7 for all indices 1 ≤ k ≤ K satisfying pk > α, one deduce
r

α 2
(1
−
kfk k2 < tα/2,σ + t2α/2,σ + σΦ−1
) .
Γnk −Γ0n
2
k
from which, since nk = n1 for all k, Inequality (6) follows.
2. Since σ −2 ||ξk ||22 is distributed as a χ2 (n1 ) random variable, the second Inequality (7) follows
X
EkfI − feI k22 =
Ekξk k22 = |{k, pk ≤ α}|n1 σ 2 .
k:pk ≤α

Proof of Theorem 10. Since threshold value is t2 = 0 on I, fb = fbI . Then, clearly fI (fI − fbI ) = 0 almost
surely so that
Ekf − fbk22 = Ekf − fbI k22 = Ekf − fI + fI − fbI k22 = Ekf − fI k22 + kfI − fbI k22 .
Applying Theorem 3 from [15] to EkfI − fbI k22 yields our statement.
Proof of Proposition 11. Recalling that, for any function g defined on sp(L) and any subset I ⊂ IL ,
X √
√
|h g(LI )δn , f i|2 = k g(LI )f k22 = hg(LI )f, f i
n

it follows by Equations (8) and (9).
X

|hϕn,k , f i|2 + |hψ1,n,k , f i|2 + |hψ2,n,k , f i|2

n,k

=

X

hψ0 (LIk )f, f i + hψk (LIk )f, f i + hψk (LIk+1 )f, f i

k

= hψ0 (L)f, f i +

X

hψk (L)f, f i = kf k22

k

Proof of Proposition 12. Remarking that gbj (λ) = ψ(sj ω0 (λ)) with sj = e−jωγ,J,R , Equation (10) implies
that
X
gm,j =
ψ(sj ω0 (λl ))δbm (l)χl .
(13)
l

[C −1 e(j−R)ωγ,J,R , C −1 ejωγ,J,R ]

Setting Jj =
and recalling that supp(b
g ) = [−Rωγ,J,R , 0], it follows that
λ ∈ supp(b
gj ) if and only if ω0 (λ) ∈ Jj if and only if sj ω0 (λ) ∈ J0 . Moreover, J0 = tk Ik and Jj = s−1
j J0
−1
−1
−1
0
0
yield Jj = tk sj Ik with sj Ik ∩ sj 0 Ik0 excepted when j = j ad k = k . Consequently, Equation (13) can
be reformulated as
X X
gm,j =
ψ(sj ω0 (λ` ))δbm (`)χ` .
1≤k≤R−1 `∈Ik

B. de Loynes, F. Navarro and B. Olivier

23

References
[1] Hamid Behjat, Ulrike Richter, Dimitri Van De Ville, and Leif Sörnmo. Signal-adapted tight frames
on graphs. IEEE Trans. Signal Process., 64(22):6017–6029, 2016.
[2] Mikhail Belkin and Partha Niyogi. Towards a theoretical foundation for laplacian-based manifold
methods. J. Comput. Syst. Sci., 74(8):1289–1308, 2008.
[3] Bernard Bercu, Bernard Delyon, and Emmanuel Rio. Concentration inequalities for sums and martingales. SpringerBriefs in Mathematics. Springer, Cham, 2015.
[4] Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of
observations. Ann. Math. Statistics, 23:493–507, 1952.
[5] Fan RK Chung and Fan Chung Graham. Spectral graph theory. Number 92. American Mathematical
Soc., 1997.
[6] Ronald R Coifman and Mauro Maggioni. Diffusion wavelets. Appl. Comput. Harmon. Anal., 21(1):53–
94, 2006.
[7] Mark Crovella and Eric Kolaczyk. Graph wavelets for spatial traffic analysis. In IEEE INFOCOM
2003. Twenty-second Annual Joint Conference of the IEEE Computer and Communications Societies
(IEEE Cat. No. 03CH37428), volume 3, pages 1848–1857. IEEE, 2003.
[8] Timothy A. Davis and Yifan Hu. The University of Florida sparse matrix collection. ACM Trans.
Math. Software, 38(1):Art. 1, 25, 2011.
[9] Basile de Loynes, Fabien Navarro, and Baptiste Olivier. Data-driven thresholding in denoising with
spectral graph wavelet transform. arXiv preprint arXiv:1906.01882, 2019.
[10] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In Advances in neural information processing systems,
pages 3844–3852, 2016.
[11] Edoardo Di Napoli, Eric Polizzi, and Yousef Saad. Efficient estimation of eigenvalue counts in an
interval. Numer. Linear Algebra Appl., 23(4):674–692, 2016.
[12] Li Fan, David I Shuman, Shashanka Ubaru, and Yousef Saad. Spectrum-adapted polynomial approximation for matrix functions. In ICASSP 2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages 4933–4937. IEEE, 2019.
[13] Matan Gavish, Boaz Nadler, and Ronald R Coifman. Multiscale wavelets on trees, graphs and high
dimensional data: theory and applications to semi supervised learning. In Proceedings of the 27th
International Conference on International Conference on Machine Learning, pages 367–374, 2010.
[14] Benjamin Girault, Antonio Ortega, and Shrikanth S Narayanan. Irregularity-aware graph fourier
transforms. IEEE Trans. Signal Process., 66(21):5746–5761, 2018.
[15] Franziska Göbel, Gilles Blanchard, and Ulrike von Luxburg. Construction of tight frames on graphs
and application to denoising. In Handbook of Big Data Analytics, pages 503–522. Springer, 2018.
[16] David K Hammond, Pierre Vandergheynst, and Rémi Gribonval. Wavelets on graphs via spectral
graph theory. Appl. Comput. Harmon. Anal., 30(2):129–150, 2011.
[17] M. F. Hutchinson. A stochastic estimator of the trace of the influence matrix for Laplacian smoothing
splines. Comm. Statist. Simulation Comput., 19(2):433–450, 1990.

24

B. de Loynes, F. Navarro and B. Olivier

[18] Nora Leonardi and Dimitri Van De Ville. Tight wavelet frames on multislice graphs. IEEE Trans.
Signal Process., 61(13):3357–3367, 2013.
[19] Lin Lin, Yousef Saad, and Chao Yang. Approximating spectral densities of large matrices. SIAM
review, 58(1):34–65, 2016.
[20] Sridhar Mahadevan. Fast spectral learning using lanczos eigenspace projections. In Proc. AAAI
Conference on Artificial Intelligence, 2008, pages 1472–1475, 2008.
[21] Antonio Ortega, Pascal Frossard, Jelena Kovačević, José MF Moura, and Pierre Vandergheynst.
Graph signal processing: Overview, challenges, and applications. Proceedings of the IEEE,
106(5):808–828, 2018.
[22] Nathanaël Perraudin and Pierre Vandergheynst. Stationary signal processing on graphs. IEEE Trans.
Signal Process., 65(13):3462–3477, 2017.
[23] Aliaksei Sandryhaila and Jose MF Moura. Discrete signal processing on graphs: Frequency analysis.
IEEE Trans. Signal Process., 62(12):3042–3054, 2014.
[24] Stefania Sardellitti, Sergio Barbarossa, and Paolo Di Lorenzo. On the graph fourier transform for
directed graphs. IEEE J. Sel. Top. Signal Process., 11(6):796–811, 2017.
[25] Santiago Segarra, Antonio G Marques, Geert Leus, and Alejandro Ribeiro. Aggregation sampling of
graph signals in the presence of noise. In 2015 IEEE 6th International Workshop on Computational
Advances in Multi-Sensor Adaptive Processing (CAMSAP), pages 101–104. IEEE, 2015.
[26] David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The
emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks
and other irregular domains. IEEE Signal Process. Mag., 30(3):83–98, 2013.
[27] David I Shuman, Benjamin Ricaud, and Pierre Vandergheynst. Vertex-frequency analysis on graphs.
Appl. Comput. Harmon. Anal., 40(2):260–291, 2016.
[28] David I Shuman, Pierre Vandergheynst, and Pascal Frossard. Chebyshev polynomial approximation
for distributed signal processing. In 2011 International Conference on Distributed Computing in
Sensor Systems and Workshops (DCOSS), pages 1–8. IEEE, 2011.
[29] RN Silver and H Röder. Densities of states of mega-dimensional hamiltonian matrices. Int. J. Mod.
Phys. C, 5(04):735–753, 1994.
[30] Ana Susnjara, Nathanael Perraudin, Daniel Kressner, and Pierre Vandergheynst. Accelerated filtering
on graphs using lanczos method. arXiv preprint arXiv:1509.04537, 2015.
[31] Yuichi Tanaka and Akie Sakiyama. m-channel oversampled graph filter banks. IEEE Trans. Signal
Process., 62(14):3578–3590, 2014.
[32] Nicolas Tremblay. Networks and signal : signal processing tools for network analysis. Theses, Ecole
normale supérieure de lyon - ENS LYON, October 2014.
[33] John von Neumann. Distribution of the ratio of the mean square successive difference to the variance.
Ann. Math. Statistics, 12:367–395, 1941.
[34] Lin-Wang Wang. Calculating the density of states and optical-absorption spectra of large quantum
systems by the plane-wave moments method. Phys. Rev. B, 49(15):10154, 1994.

