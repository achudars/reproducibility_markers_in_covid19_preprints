Inference and Uncertainty Quantification for
Noisy Matrix Completion
Yuxin Chen∗

Jianqing Fan†

arXiv:1906.04159v2 [stat.ML] 14 Nov 2019

June 2019;

Cong Ma†

Yuling Yan†

Revised: October 2019

Abstract
Noisy matrix completion aims at estimating a low-rank matrix given only partial and corrupted
entries. Despite substantial progress in designing efficient estimation algorithms, it remains largely
unclear how to assess the uncertainty of the obtained estimates and how to perform statistical inference
on the unknown matrix (e.g. constructing a valid and short confidence interval for an unseen entry).
This paper takes a step towards inference and uncertainty quantification for noisy matrix completion.
We develop a simple procedure to compensate for the bias of the widely used convex and nonconvex
estimators. The resulting de-biased estimators admit nearly precise non-asymptotic distributional characterizations, which in turn enable optimal construction of confidence intervals / regions for, say, the
missing entries and the low-rank factors. Our inferential procedures do not rely on sample splitting,
thus avoiding unnecessary loss of data efficiency. As a byproduct, we obtain a sharp characterization of
the estimation accuracy of our de-biased estimators, which, to the best of our knowledge, are the first
tractable algorithms that provably achieve full statistical efficiency (including the preconstant). The analysis herein is built upon the intimate link between convex and nonconvex optimization — an appealing
feature recently discovered by [CCF+ 19].

Keywords: matrix completion, statistical inference, confidence intervals, uncertainty quantification, convex
relaxation, nonconvex optimization

Contents
1 Introduction
1.1 Motivation: inference and uncertainty quantification? . . . . . . . . . . . . . . . . . . . . . . .
1.2 A glimpse of our contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2
2
3

2 Models and notation

4

3 Inferential procedures and main results
3.1 Background: convex and nonconvex estimators . . . .
3.2 Constructing de-biased estimators . . . . . . . . . . .
3.3 Main results: distributional guarantees . . . . . . . .
3.4 Lower bounds and optimality for inference . . . . . . .
3.5 Back to estimation: the de-biased estimator is optimal
3.6 Numerical experiments . . . . . . . . . . . . . . . . . .
3.7 A bit of intuition . . . . . . . . . . . . . . . . . . . . .
3.8 Inference based on spectral estimates? . . . . . . . . .
4 Prior art

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

4
4
6
7
10
11
12
14
15
16

Author names are sorted alphabetically.
of Electrical Engineering, Princeton University, Princeton, NJ 08544, USA; Email: yuxin.chen@princeton.edu.
† Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08544, USA; Email:
{jqfan, congm, yulingy}@princeton.edu.
∗ Department

1

5 Architecture of the proof
5.1 Near equivalence between convex and nonconvex estimators .
5.2 A precise characterization of the de-shrunken low-rank factors
5.3 Taking global rotation into account . . . . . . . . . . . . . . .
5.4 Key lemmas for establishing Theorem 5 . . . . . . . . . . . .
5.5 From low-rank factors to matrix entries (Proof of Theorem 6)

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

6 Discussion

18
19
20
21
21
23
24

A Preliminaries
25
A.1 Algorithmic details of nonconvex optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
A.2 Properties of approximate nonconvex solutions . . . . . . . . . . . . . . . . . . . . . . . . . . 26
B Summary of the proposed estimators

28

C Proof of Lemma 3
28
C.1 Proof of the inequality (5.4a) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
C.2 Proof of the inequality (5.4b) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
C.3 Proof of the inequality (5.5) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
D Analysis of the low-rank
D.1 Proof of Lemma 4 . .
D.2 Proof of Lemma 5 . .
D.3 Proof of Lemma 6 . .
D.4 Proof of Lemma 7 . .
D.5 Proof of Lemma 8 . .
D.6 Proof of Lemma 9 . .

factors
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

33
33
34
37
41
45
45

E Analysis of the entries of the matrix
48
E.1 Proof of Lemma 10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
E.2 Proof of Lemma 11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
F Proof of Corollary 1

50

G Proof of Theorem 3

53

H Proof of lower bounds
56
H.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
H.2 Proof of Lemma 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
I

Proofs in Section A
57
I.1 Proof of the inequalities (A.13) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
I.2 Proof of the inequalities (A.16) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59

J Technical lemmas

1
1.1

61

Introduction
Motivation: inference and uncertainty quantification?

Low-rank matrix completion is concerned with recovering a low-rank matrix, when only a small fraction
of its entries are revealed to us [Sre04, CR09, KMO10a]. Tackling this problem in large-scale applications
is computationally challenging, due to the intrinsic nonconvexity incurred by the low-rank structure. To
further complicate matters, another inevitable challenge stems from the imperfectness of data acquisition
mechanisms, wherein the acquired samples are contaminated by a certain amount of noise.

2

Fortunately, if the entries of the unknown matrix are sufficiently de-localized and randomly revealed, this
problem may not be as hard as it seems. Substantial progress has been made over the past several years
in designing computationally tractable algorithms — including both convex and nonconvex approaches —
that allow to fill in unseen entries faithfully given only partial noisy samples [CP10,NW12,KLT11,KMO10b,
CW15, MWCC17, CCF+ 19]. Nevertheless, modern decision making would often require one step further. It
not merely anticipates a faithful estimate, but also seeks to quantify the uncertainty or “confidence” of the
provided estimate, ideally in a reasonably accurate fashion. For instance, given an estimate returned by the
convex approach, how to use it to compute a short interval that is likely to contain a missing entry?
Conducting effective uncertainty quantification for noisy matrix completion is, however, far from straightforward. For the most part, the state-of-the-art matrix completion algorithms require solving highly complex
optimization problems, which often do not admit closed-form solutions. Of necessity, it is generally very
challenging to pin down the distributions of the estimates returned by these algorithms. The lack of distributional characterizations presents a major roadblock to performing valid, yet efficient, statistical inference
on the unknown matrix of interest.
It is worth noting that a number of recent papers have been dedicated to inference and uncertainty
quantification for various high-dimensional problems in high-dimensional statistics, including Lasso [ZZ14,
vdGBRD14, JM14a], generalized linear models [vdGBRD14, NL17, BFL+ 18], graphical models [JVDG15,
RSZZ15,MLL17]), amongst others. Very little work, however, has looked into noisy matrix completion along
this direction. While non-asymptotic statistical guarantees for noisy matrix completion have been derived
in prior theory, most, if not all, of the estimation error bounds are supplied only at an order-wise level. Such
order-wise error bounds either lose a significant factor relative to the optimal guarantees, or come with an
unspecified (but often enormous) pre-constant. Viewed in this light, a confidence region constructed directly
based on such results is bound to be overly conservative, resulting in substantial over-coverage.

1.2

A glimpse of our contributions

This paper takes a substantial step towards efficient inference and uncertainty quantification for noisy matrix
completion. Specifically, we develop a simple procedure to compensate for the bias of the commonly used
convex and nonconvex estimators. The resulting de-biased estimators admit nearly accurate non-asymptotic
distributional guarantees. Such distributional characterizations in turn allow us to reason about the uncertainty of the obtained estimates vis-à-vis the unknown matrix. While details of our main findings are
postponed to Section 3, we would like to immediately single out a few important merits of the proposed
inferential procedures and theory:
1. Our results enable two types of uncertainty assessment, namely, we can construct (i) confidence intervals
for each entry — either observed or missing — of the unknown matrix; (ii) confidence regions for the
low-rank factors of interest (modulo some unavoidable global ambiguity).
2. Despite the complicated statistical dependency, our procedure and theory do not rely on sample splitting,
thus avoiding the unnecessary widening of confidence intervals / regions due to insufficient data usage.
3. The confidence intervals / regions constructed based on the proposed procedures are, in some sense,
optimal.
4. We present a unified approach that accommodates both convex and nonconvex estimators seamlessly.
5. As a byproduct, we characterize the Euclidean estimation errors of the proposed de-biased estimators.
Such error bounds are sharp and match an oracle lower bound precisely (including the pre-constant).
To the best of our knowledge, this is the first theory that demonstrates that a computationally feasible
algorithm can achieve the statistical limit including the pre-constant.
All of this is built upon the intimate link between convex and nonconvex estimators [CCF+ 19], as well as
the recent advances in analyzing the stability of nonconvex optimization against random noise [MWCC17].

3

2

Models and notation

To cast the noisy matrix completion problem in concrete statistical settings, we adopt a model commonly
studied in the literature [CR09]. We also introduce some useful notation.
Ground truth. Denote by M ? ∈ Rn×n the unknown rank-r matrix of interest,1 whose (compact) singular
value decomposition (SVD) is given by M ? = U ? Σ? V ?> . We set
σmax , σ1 (M ? ),

σmin , σr (M ? ),

and κ , σmax /σmin ,

(2.1)

where σi (A) denotes the ith largest singular value of a matrix A. Further, we let X ? , U ? Σ?1/2 ∈ Rn×r
and Y ? , V ? Σ?1/2 ∈ Rn×r stand for the balanced low-rank factors of M ? , which obey
X ?> X ? = Y ?> Y ? = Σ?

and

M ? = X ? Y ?> .

(2.2)

Observation models. What we observe is a random subset of noisy entries of M ? ; more specifically, we
observe
i.i.d.
?
Mij = Mij
+ Eij ,
Eij ∼ N (0, σ 2 ),
for all (i, j) ∈ Ω,
(2.3)
where Ω ⊆ {1, · · · , n} × {1, · · · , n} is a subset of indices, and Eij denotes independently generated noise
at the location (i, j). From now on, we assume the random sampling model where each index (i, j) is
included in Ω independently with probability p (i.e. data are missing uniformly at random). We shall use
PΩ (·) : Rn×n 7→ Rn×n to represent the orthogonal projection onto the subspace of matrices that vanish
outside the index set Ω.
Incoherence conditions. Clearly, not all matrices can be reliably estimated from a highly incomplete set
of measurements. To address this issue, we impose a standard incoherence condition [CR09, Che15] on the
singular subspaces of M ? (i.e. U ? and V ? ):
p
(2.4)
max{kU ? k2,∞ , kV ? k2,∞ } ≤ µr/n,
where µ is termed the incoherence parameter and kAk2,∞ denotes the largest `2 norm of all rows in A. A
small µ implies that the energy of U ? and V ? are reasonably spread out across all of their rows.
Asymptotic notation. Here, f (n) . h(n) (or f (n) = O(h(n))) means |f (n)| ≤ c1 |h(n)| for some constant
c1 > 0, f (n) & h(n) means |f (n)| ≥ c2 |h(n)| for some constant c2 > 0, f (n)  h(n) means c2 |h(n)| ≤
|f (n)| ≤ c1 |h(n)| for some constants c1 , c2 > 0, and f (n) = o(h(n)) means limn→∞ f (n)/h(n) = 0. We write
f (n)  h(n) to indicate that |f (n)| ≤ c1 |h(n)| for some small constant c1 > 0 (much smaller than 1), and
use f (n)  h(n) to indicate that |f (n)| ≥ c2 |h(n)| for some large constant c2 > 0 (much larger than 1).

3

Inferential procedures and main results

The proposed inferential procedure lays its basis on two of the most popular estimation paradigms — convex
relaxation and nonconvex optimization — designed for noisy matrix completion. Recognizing the complicated
bias of these two highly nonlinear estimators, we shall first illustrate how to perform bias correction, followed
by a distributional theory that establishes the near-Gaussianity and optimality of the proposed de-biased
estimators.

3.1

Background: convex and nonconvex estimators

We first review in passing two tractable estimation algorithms that are arguably the most widely used in
practice. They serve as the starting point for us to design inferential procedures for noisy low-rank matrix
completion. The readers familiar with this literature can proceed directly to Section 3.2.
1 We

restrict our attention to squared matrices for simplicity of presentation. Most findings extend immediately to the more
general rectangular case M ? ∈ Rn1 ×n2 with different n1 and n2 .

4

Algorithm 1 Gradient descent for solving the nonconvex problem (3.4).
Suitable initialization: X 0 , Y 0
Gradient updates: for t = 0, 1, . . . , t0 − 1 do

η
PΩ (X t Y t> − M )Y t + λX t ,
p

η
=Y t − [PΩ (X t Y t> − M )]> X t + λY t ,
p

X t+1 =X t −

(3.3a)

Y t+1

(3.3b)

where η > 0 determines the step size or the learning rate.

Convex relaxation. Recall that the rank function rank(·) is highly nonconvex, which often prevents us
from computing a rank-constrained estimator in polynomial time. For the sake of computational feasibility,
prior works suggest relaxing the rank function into its convex surrogate [Faz02, RFP10]; for example, one
can consider the following penalized least-squares convex program
minimize
n×n
Z∈R

1 X
2
(Zij − Mij ) + λkZk∗ ,
2

(3.1)

(i,j)∈Ω

or using our notation PΩ ,
minimize
Z∈Rn×n


1
PΩ Z − M
2

2
F

+ λkZk∗ .

(3.2)

Here, k · k∗ is the nuclear norm (the sum of singular values, which is a convex surrogate of the rank function), and λ > 0 is some regularization parameter. Under mild conditions, the solution to the convex
program (3.1) provably attains near-optimal estimation accuracy (in an order-wise sense), provided that a
proper regularization parameter λ is adopted [CCF+ 19].
Nonconvex optimization. It is recognized that the convex approach, which typically relies on solving a
semidefinite program, is still computationally expensive and not scalable to large dimensions. This motivates
an alternative route, which represents the matrix variable via two low-rank factors X, Y ∈ Rn×r and
attempts solving the following nonconvex program directly
minimize

X,Y ∈Rn×r


1
PΩ XY > − M
2

2
F

+

λ
λ
kXk2F + kY k2F .
2
2

(3.4)

Here, we choose a regularizer of the form 0.5λ(kXk2F + kY k2F ) primarily to mimic the nuclear norm λkZk∗
(see [SS05, MHT10]). A variety of optimization algorithms have been proposed to tackle the nonconvex
program (3.4) or its variants [SL16, CW15, MWCC17]; the readers are referred to [CLC19] for a recent
overview. As a prominent example, a two-stage algorithm — gradient descent following suitable initialization — provably enjoys fast convergence and order-wise optimal statistical guarantees for a wide range of
scenarios [MWCC17, CCF+ 19, CLL19]. The current paper focuses on this simple yet powerful algorithm, as
documented in Algorithm 1 and detailed in Appendix A.1.
Intimate connections between convex and nonconvex estimates. Denote by Z cvx any minimizer
of the convex program (3.1), and denote by (X ncvx , Y ncvx ) the estimate returned by Algorithm 1 aimed at
solving (3.4). As was recently shown in [CCF+ 19], when the regularization parameter λ is properly chosen,
these two estimates obey (see (A.12) in Appendix A.2 for a precise statement)
X ncvx Y ncvx> ≈ Z cvx ≈ Z cvx,r .

(3.5)

Here, Z cvx,r , Prank-r (Z cvx ) is the best rank-r approximation of the convex estimate Z cvx , where Prank-r (B) ,
arg minA:rank(A)≤r kA − BkF . In truth, the three matrices of interest in (3.5) are exceedingly close to, if
not identical with, each other. This salient feature paves the way for a unified treatment of convex and
nonconvex approaches: most inferential procedures and guarantees developed for the nonconvex estimate
can be readily transferred to perform inference for the convex one, and vice versa.
5

3.2

Constructing de-biased estimators

We are now well equipped to describe how to construct new estimators based on the convex estimate Z cvx
and the nonconvex estimate (X ncvx , Y ncvx ), so as to enable statistical inference. Motivated by the proximity
of the convex and nonconvex estimates and for the sake of conciseness, we shall abuse notation by using
the shorthand Z, X, Y for both convex and nonconvex estimates; see Table 1 and Appendix B for precise
definitions. This allows us to unify the presentation for both convex and nonconvex estimators.
Given that both (3.1) and (3.4) are regularized least-squares problems, they behave effectively like shrinkage estimators, indicating that the provided estimates necessarily suffer from non-negligible bias. In order
to enable desired statistical inference, it is natural to first correct the estimation bias.
A de-biased estimator for the matrix. A natural de-biasing strategy that immediately comes to mind
is the following simple linear transformation (recall the notation in Table 1):
 1
 1

1
Z 0 , Z − PΩ Z − M = PΩ M ? + PΩ E + Z −
p
p
p
| {z } | {z }
|
mean: M ?

mean: 0


1
PΩ Z ,
p
{z
}

(3.6)

mean: 0 (heuristically)

where we identify PΩ (M ) with PΩ (M ? ) + PΩ (E). Heuristically, if Ω and Z are statistically independent,
then Z 0 serves as an unbiased estimator of M ? , i.e. E[Z 0 ] = M ? ; this arises since the noise E has zero mean
and E[PΩ ] = pI under the uniform random sampling model, with I the identity operator. Despite its (near)
unbiasedness nature at a heuristic level, however, the matrix Z 0 is typically full-rank, with non-negligible
energy spread across its entire spectrum. This results in dramatically increased variability in the estimate,
which is undesirable for inferential purposes.
To remedy this issue, we propose to further project Z 0 onto the set of rank-r matrices, leading to the
following de-biased estimator
i
h
1
M d , Prank-r Z − PΩ (Z − M ) ,
p

(3.7)

where Prank-r (B) = arg minA:rank(A)≤r kA−BkF , and Z can again be found in Table 1. This projection step
effectively suppresses the variability outside the r-dimensional principal subspace. As we shall see shortly,
the proposed estimator (3.7) properly de-biases the provided estimate Z, while optimally controlling the
extent of uncertainty.
Remark 1. The estimator (3.7) can be viewed as performing one iteration of singular value projection
(SVP) [MJD09, DC18] on the current estimate Z.
Remark 2. The estimator (3.7) also bears a similarity to the de-biased estimator proposed by [Xia18] for
low-rank trace regression; the disparity between them shall be discussed in Section 4.
An equivalent form: a de-shrunken estimator for the low-rank factors. It turns out that the debiased estimator (3.7) admits another almost equivalent representation that offers further insights. Specifi-

Table 1: Notation used to unify the convex estimate Z cvx and the nonconvex estimate (X ncvx , Y ncvx ). Here,
Z cvx,r = Prank-r (Z cvx ) is the best rank-r approximation of Z cvx . See Appendix B for a complete summary.
Z ∈ Rn×n
X, Y ∈ Rn×r

M d ∈ Rn×n
X d , Y d ∈ Rn×r

either Z cvx or X ncvx Y ncvx> .
for the nonconvex case, we take X = X ncvx and Y = Y ncvx ; for the convex case, let
X = X cvx and Y = Y cvx , which are the balanced low-rank factors of Z cvx,r obeying
Z cvx,r = X cvx Y cvx> and X cvx> X cvx = Y cvx> Y cvx .
the proposed de-biased estimator as in (3.7).
the proposed de-shrunken estimator as in (3.8).

6

cally, we consider the following de-shrunken estimator for the low-rank factors

−1 1/2
λ
X d , X Ir +
X >X
p

and


−1 1/2
λ
Y >Y
Y d , Y Ir +
,
p

(3.8)

where we recall the definition of X and Y in Table 1. To develop some intuition regarding why this is
called a de-shrunken estimator, let us look at a simple scenario where U ΣV > is the SVD of XY > and
X = U Σ1/2 , Y = V Σ1/2 . It is then self-evident that
1/2


λ 1/2
λ
= U Σ + Ir
X d = U Σ1/2 Ir + Σ−1
p
p

and


λ 1/2
Y d = V Σ + Ir
.
p

In words, X d and Y d are obtained by de-shrinking the spectrum of X and Y properly.
As we shall formalize in Section 5.1, the de-shrunken estimator (3.8) for the low-rank factors is nearly
equivalent to the de-biased estimator (3.7) for the whole matrix, in the sense that
M d ≈ X d Y d> .

(3.9)

Therefore, M d can be viewed as some sort of de-shrunken estimator as well.

3.3

Main results: distributional guarantees

The proposed estimators admit tractable distributional characterizations in the large-n regime, which facilitates the construction of confidence regions for many quantities of interest. In particular, this paper centers
around two types of inferential problems:
1. Each entry of the matrix M ? : the entry can be either missing (i.e. predicting an unseen entry) or
observed (i.e. de-noising an observed entry). For example, in the Netflix challenge, one would like to
infer a user’s preference about any movie, given partially revealed ratings [CR09]. Mathematically, this
seeks to determine the distribution of
d
?
Mij
− Mij
,

for all 1 ≤ i, j ≤ n.

(3.10)

2. The low-rank factors X ? , Y ? ∈ Rn×r : the low-rank factors often reveal critical information about the
applications of interest (e.g. community memberships of each individual in the community detection
problem [AFWZ17], or angles between each object and a global reference point in the angular synchronization problem [Sin11]). Recognizing the global rotational ambiguity issue,2 we aim to pin down the
distributions of X d and Y d up to global rotational ambiguity. More precisely, we intend to characterize
the distributions of
X dH d − X ?
and
Y dH d − Y ?
(3.11)
for the global rotation matrix H d ∈ Rr×r that best “aligns” (X d , Y d ) and (X ? , Y ? ), i.e.
H d , arg min

R∈O r×r

X dR − X ?

2
F

+ Y dR − Y ?

2
F

.

(3.12)

Here and below, Or×r denotes the set of orthonormal matrices in Rr×r .
Clearly, the above two inferential problems are tightly related: an accurate distributional characterization
for the low-rank factors (3.11) often results in a distributional guarantee for the entries (3.10). As such, we
shall begin by presenting our distributional characterizations of the low-rank factors. Here and throughout,
ei represents the ith standard basis vector in Rn .
Theorem 1 (Distributional guarantees w.r.t. low-rank factors). Suppose that the sample size and the noise
obey
q
(3.13)
np & κ8 µ3 r2 log3 n
and
σ/σmin . p/(κ8 µn log2 n).
2 For

any r × r rotation matrix H, we cannot distinguish (X ? , Y ? ) from (X ? H, Y ? H), if only pairwise measurements are
available.

7

Then one has the following decomposition
X d H d − X ? = ZX + ΨX ,

(3.14a)

Y d H d − Y ? = ZY + ΨY .

(3.14b)

with (X ? , Y ? ) defined in (2.2), (X d , Y d ) defined in Table 1, and H d defined in (3.12). Here, the rows of
ZX ∈ Rn×r (resp. ZY ∈ Rn×r ) are independent and obey

 σ2
i.i.d.
−1
>
(Σ? )
,
ZX
ej ∼ N 0,
p
 σ2

i.i.d.
−1
ZY> ej ∼ N 0,
(Σ? )
,
p

for

1 ≤ j ≤ n;

(3.15a)

for

1 ≤ j ≤ n.

(3.15b)

In addition, the residual matrices ΨX , ΨY ∈ Rn×r satisfy, with probability at least 1 − O(n−3 ), that
 √ 

σ r
max kΨX k2,∞ , kΨY k2,∞ = o √
.
(3.16)
pσmax
Remark 3. A more complete version can be found in Theorem 5.
Remark 4. Another interesting feature — which we shall make precise in the proof of this theorem — is
>
that: for any given 1 ≤ i, j ≤ n, the two random vectors ZX
ei and ZY> ej are nearly statistically independent.
This is crucial for deriving inferential guarantees for the entries of the matrix.
Theorem 1 is a non-asymptotic result. In words, Theorem 1 decomposes the estimation error X d H d −X ?
(resp. Y d H d − Y ? ) into a Gaussian component ZX (resp. ZY ) and a residual term ΨX (resp. ΨY ). If
the sample size is sufficiently large and the noise size is sufficiently small, then the residual terms are much
smaller in size compared to ZX and ZY . To see this, it is helpful to leverage the Gaussianity (3.15a) and
compute that: for each 1 ≤ j ≤ n, the jth row of ZX obeys
h
E

>
ZX
ej

2
2

i

= Tr

 σ2
p

−1

(Σ? )



≥

σ2 r
;
pσmax

p
in other words, the typical size of the jth row of ZX is no smaller than p
the order of σ r/(pσmax ). In
comparison, the size of each row of ΨX (see (3.16)) is much smaller than σ r/(pσmax ) (and hence smaller
than the size of the corresponding row of ZX ) with high probability, provided that (3.13) is satisfied.
Equipped with the above master decompositions of the low-rank factors and Remark 4, we are ready to
d
?
present a similar decomposition for the entry Mij
− Mij
.
?
Theorem 2 (Distributional guarantees w.r.t. matrix entries). For each 1 ≤ i, j ≤ n, define the variance vij
as
?
vij
,

σ2  ?
Ui,·
p

2
2

?
+ Vj,·

2
2



,

(3.17)

?
?
where Ui,·
(resp. Vj,·
) denotes the ith (resp. jth) row of U ? (resp. V ? ). Suppose that

np & κ8 µ3 r3 log3 n,
?
Ui,·

2

?
+ Vj,·

2

q
σ (κ8 µrn log2 n)/p . σmin
s
r
r σ
κ6 µ2 rn log3 n
&
.
n σmin
p

and

(3.18a)
(3.18b)

Then the matrix M d defined in Table 1 satisfies
d
?
Mij
− Mij
= gij + ∆ij ,
p ?
?
where gij ∼ N (0, vij
) and the residual obeys |∆ij | = o( vij
) with probability exceeding 1 − O(n−3 ).

8

(3.19)

Remark 5 (The symmetric case). In the symmetric case where the noise E, the truth M ? , and the sampling
>
?
pattern are all symmetric (i.e. PΩ (E) = PΩ (E)
and M ? = M ?> ), the variance vii
(cf. (3.17)) for the
diagonal entries has a different formula; more specifically, it is straightforward to extend our theory to show
that

2σ 2  ? 2
4σ 2
? 2
? 2
?
=
+
V
for the symmetric case.
Ui,·
U
vii
=
i,·
i,·
2
2
2
p
p
>
This additional multiplicative factor of 2 arises since ZX
ei and ZY> ei are identical (and hence not indepen?
dent) in this symmetric case. The variance formula for any vij
(i =
6 j) remains unchanged.
?
Several remarks are in order. To begin with, we develop some intuition regarding where the variance vij
comes from. By virtue of Theorem 1, one has the following Gaussian approximation

X d H d − X ? ≈ ZX

and

Y d H d − Y ? ≈ ZY .

Assuming that the first-order expansion is reasonably tight, one has
i
h
>
 ?>

d
?
d
d
?
?
d
d
? >
≈ e>
Mij
− Mij
= X d H d Y d H d − X ? Y ?>
ej + e>
ej
i X H −X Y
i X Y H −Y
ij

≈

?>
e>
ej
i ZX Y

+

? >
e>
i X ZY ej .

(3.20)

>
According to Remark 4, ZX
ei and ZY> ej are nearly independent. It is thus straightforward to compute the
variance of (3.20) as

 (i)


d
?
?>
? >
Var Mij
− Mij
≈ Var e>
ej + Var e>
i ZX Y
i X ZY ej
o (iii) σ 2 
2 n
(ii) σ
?
? −1
?
? −1
?
e>
Y ?> ej + e>
X ?> ei =
Ui,·
=
j Y (Σ )
i X (Σ )
p
p

2
2

?
+ Vj,·

2
2



?
= vij
.

>
Here, (i) relies on (3.20) and the near independence between ZX
ei and ZY> ej ; (ii) uses the variance formula in
?
?
Theorem 1; (iii) arises from the definitions of X and Y (cf. (2.2)). This computation explains (heuristically)
?
the variance formula vij
.
Given that Theorem 2 reveals the tightness of Gaussian approximation under conditions (3.18), it in
?
turn allows us to construct nearly accurate confidence intervals for each matrix entry Mij
. This is formally
summarized in the following corollary, the proof of which is deferred to Appendix F. Here and throughout,
we use [a ± b] to denote the interval [a − b, a + b].
?
Corollary 1 (Confidence intervals for the entries {Mij
}). Let X d , Y d and M d be as defined in Table 1. For
any given 1 ≤ i, j ≤ n, suppose that (3.18a) holds and that
s
r
r
σ
κ10 µ2 rn log3 n
?
?
.
(3.21)
Ui,·
+
V
&
j,·
2
2
n σmin
p

Denote by Φ(t) the CDF of a standard Gaussian random variable and by Φ−1 (·) its inverse function. Let
vij ,


−1
−1
σ2  d
d >
d
d >
Xi,· X d> X d
(Xi,·
) + Yj,·
Y d> Y d
(Yj,·
)
p

(3.22)

?
be the empirical estimate of the theoretical variance vij
. Then one has

sup
0<α<1

n
 d
√ o
?
P Mij
∈ Mij
± Φ−1 (1 − α/2) vij − (1 − α) = o(1).

In words, Corollary 1 tells us that for any fixed significance level 0 < α < 1, the interval
 d
√ 
Mij ± Φ−1 (1 − α/2) vij
?
is a nearly accurate two-sided (1 − α) confidence interval of Mij
.

9

(3.23)

?
?
In addition, we remark that when kUi,·
k2 = kVj,·
k2 = 0 (and hence Vij? = 0), the above Gaussian
approximation is completely off. In this case, one can still leverage Theorem 1 to show that
d
?
d
Mij
− Mij
= Mij
≈ u> v,

(3.24)

where u, v ∈ Rr are independent and identically distributed according to N (0, σ 2 (Σ? )−1 /p). However,
?
?
it is nontrivial to determine whether kUi,·
k2 + kVj,·
k2 is vanishingly small or not based on the observed
data, which makes it challenging to conduct efficient inference for entries with small (but a priori unknown)
?
?
k2 .
k2 + kVj,·
kUi,·
Last but not least, the careful readers might wonder how to interpret our conditions on the sample
complexity and the signal-to-noise ratio. Take the case with r, µ, κ = O(1) for example: our conditions read
q
(3.25)
n2 p & n log3 n;
σ (n log2 n)/p . σmin .
The first condition matches the minimal sample complexity limit (up to some logarithmic factor), while the
second one coincides with the regime (up to log factor) in which popular algorithms (like spectral methods
or nonconvex algorithms) work better than a random guess [KMO10b, CW15, MWCC17]. The take-away
message is this: once we are able to compute a reasonable estimate in an overall `2 sense, then we can reinforce
it to conduct entrywise inference in a statistically efficient fashion. The discussion of the dependency on r
and κ is deferred to Section 6.

3.4

Lower bounds and optimality for inference

It is natural to ask how well our inferential procedures perform compared to other algorithms. Encouragingly,
the de-biased estimator is optimal in some sense; for instance, it nearly attains the minimum covariance
among all unbiased estimators. To formalize this claim, we shall
1. Quantify the performance of two ideal estimators with the assistance of an oracle;
2. Demonstrate that the performance of our de-biased estimators is arbitrarily close to that of the ideal
estimators.
?
?
In what follows, we denote by Xi,·
(resp. Yi,·
) the ith row of X ? (resp. Y ? ).
?
An ideal estimator for Xi,·
(1 ≤ i ≤ n). Suppose that there is an oracle informing us of Y ? , and that
we observe the same set of data as in (2.3). Under such an idealistic setting and for any given 1 ≤ i ≤ n, the
following least-squares estimator achieves the minimum covariance among all unbiased estimators for the ith
?
row Xi,·
of X ? (see e.g. [Sha03, Theorem 3.7])
X h
 i2
ideal
? >
Xi,·
, arg min
M
−
u
Y
.
(3.26)
ik
k,·
1×r
u∈R

k:(i,k)∈Ω

?
In other words, for any unbiased estimator u of Xi,·
(conditional on Ω), one has



ideal
?
Cov u Ω  Cov Xi,·
Ω =: CRLB Xi,·
|Ω ,
(3.27)

ideal
Ω is precisely the Cramér-Rao lower bound (conditional on Ω) under this ideal setting.
where Cov Xi,·
As it turns out, with high probability, this lower bound concentrates around σ 2 (Σ? )−1 /p, as stated in the
following lemma. The proof is postponed to Appendix H.1.

Lemma 1. Fix an arbitrarily small constant ε > 0. Suppose that n2 p ≥ C0 ε−2 κ4 µrn for some sufficiently
large constant C0 > 0 independent of n. Then with probability at least 1 − O(n−10 ), one has

σ2
−1
?
(Σ? ) .
CRLB Xi,·
| Ω  (1 − ε)
p
Given that ε can be an arbitrarily small constant, Lemma 1 uncovers that the covariance of the ded
ideal
shrunken estimator Xi,·
(cf. Theorem 1) matches that of the ideal estimator Xi,·
, thus achieving the
d
Cramér-Rao lower bound with high probability. The same conclusion applies to Yj,· as well.
10

?
An ideal estimator for Mij
(1 ≤ i, j ≤ n). Suppose that there is another oracle informing us of
?
?
?
?
{Xk,· }k:k6=i and {Yk,· }k:k6=j ; that is, everything about X ? except Xi,·
and everything about Y ? except Yj,·
.
3
In addition, we observe the same set of data as in (2.3), except that we do not get to see Mij . Under this
? >
?
?
) can
(Yj,·
= Xi,·
idealistic model, the Cramér-Rao lower bound [Sha03, Theorem 3.3] for estimating Mij
be computed as

?
CRLB Mij
|Ω
i
−1

−1
X
X
σ2 h ?  1
? > ?
? >
? 1
? >
?
? >
,
· Yj,·
(Yk,·
(Xk,·
) Yk,·
(Yj,·
) + Xi,·
) Xk,·
(Xi,·
) . (3.28)
p
p
p
k:k6=j,(i,k)∈Ω

k:k6=i,(k,j)∈Ω

?
?
This means that any unbiased estimator of Mij
must have variance no smaller than CRLB(Mij
| Ω). This
quantity admits a much simpler lower bound as follows, whose proof can be found in Appendix H.2.

Lemma 2. Fix an arbitrarily small constant ε > 0. Suppose that n2 p ≥ C0 ε−2 κ4 µrn log n for some sufficiently large constant C0 > 0 independent of n. Then with probability at least 1 − O(n−10 ),

?
?
CRLB Mij
| Ω ≥ (1 − ε) vij
,
?
where vij
is defined in Theorem 2.
d
Similar to Lemma 1, Lemma 2 reveals that the variance of our de-biased estimator Mij
(cf. Theorem 2)
— which certainly does not have access to the side information provided by the oracle — is arbitrarily close
to the Cramér-Rao lower bound aided by an oracle.

All in all, the above lower bounds demonstrate that the degrees of uncertainty underlying our de-shrunken
low-rank factors and de-biased matrix are, in some sense, statistically minimal.

3.5

Back to estimation: the de-biased estimator is optimal

While the emphasis of the current paper is on inference, we would nevertheless like to single out an important consequence that informs the estimation step. To be specific, the decompositions and distributional
guarantees derived in Theorem 1 and Theorem 2 allow us to track the estimation accuracy of M d , as stated
in the following theorem. The proof of this result is postponed to Appendix G.
Theorem 3 (Estimation accuracy of M d ). Let M d be the de-biased estimator as defined in Table 1. Instate
the conditions in (3.18a). Then with probability at least 1 − O(n−3 ), one has
Md − M?

2
F

=

(2 + o(1))nrσ 2
.
p

(3.29)

In stark contrast to prior statistical estimation guarantees (e.g. [CP10, NW12, KLT11, CCF+ 19]), Theorem 3 pins down the estimation error of the proposed de-biased estimator in a sharp manner (namely,
even the pre-constant is fully determined). Encouragingly, there is a sense in which the proposed de-biased
estimator achieves the best possible statistical estimation accuracy, as revealed by the following result.
Theorem 4 (An oracle lower bound on `2 estimation errors). Fix an arbitrarily small constant ε > 0. Suppose
that n2 p & µrn log2 n, and that r = o(n). Then with probability exceeding 1 − O(n−10 ), any unbiased
c of M ? obeys
estimator M
h
i
2
c − M ? 2 | Ω ≥ (1 − ε)2nrσ .
E M
(3.30)
F
p
Proof. Intuitively, the term 2nr reflects approximately the underlying degrees of freedom in the true subspace
T ? of interest (i.e. the tangent space of the rank-r matrices at the truth M ? ), whereas the factor 1/p captures
the effect due to sub-sampling. This result has already been established in [CP10, Section III.B] (together
with [CR09, Theorem 4.1]). We thus omit the proof for conciseness. The key idea is to consider an oracle
informing us of the true tangent space T ? .
3 The

exclusion of Mij is merely for ease of presentation. One can consider the model where all Mij with (i, j) ∈ Ω are
observed with a slightly more complicated argument.

11

3

3

4

3

2

2

2
1
1
1
0
0

0
-1
-1

-1
-2
-2
-2

-3

-3

-3

-4
-3

-2

-1

0

1

2

3

-4
-3

-2

(a)

-1

0

1

2

3

(b)

-3

-2

-1

0

1

2

3

(c)

Figure 1: Q-Q (quantile-quantile) plots of T12 , T13 and T14 vs. the standard normal distribution in (a), (b)
and (c), respectively. The results are reported over 200 independent trials for r = 5, p = 0.4 and σ = 10−3 .
The implication of the above two theorems is remarkable: the de-biasing step not merely facilitates
uncertainty assessment, but also proves crucial in minimizing the estimation errors. It achieves optimal
statistical efficiency in terms of both the rate and the pre-constant. As far as we know, this is the first
theory about a polynomial time algorithm that matches the statistical limit in terms of the pre-constant.
This intriguing finding is further corroborated by numerical experiments; see Section 3.6 for details (in
particular, Figure 3).

3.6

Numerical experiments

We conduct numerical experiments on synthetic data to verify the distributional characterizations provided
in Theorem 1 and Theorem 2. Note that our main results hold for the de-biased estimators built upon Z cvx
and X ncvx Y ncvx> . As we will formalize shortly in Section 5.1, these two de-biased estimators are extremely
close to each other; see also Figure 4 for experimental evidence. Therefore, in order to save space, we use
the de-biased estimator built upon the convex estimate Z cvx throughout the experiments.
√
Fix the dimension n = 1000 and the regularization parameter λ = 2.5σ np throughout the experiments.
?
? ?>
?
?
n×r
We generate a rank-r matrix M = X Y , where X , Y ∈ R
are random orthonormal matrices and
apply the proximal gradient method [PB14] to solve the convex program (3.1).
We begin by checking the validity of Theorem 1. Suppose that one is interested in estimating the inner
?
?>
product e>
ej between X ?> ei and X ?> ej (i 6= j). In the Netflix challenge, this might correspond
i X X
to the similarity between the ith user and the jth one. As a straightforward consequence of Theorem 1, the
normalized estimation error

1
d
d>
?
?>
Tij , √
e>
ej − e>
ej
i X X
i X X
ρij

(3.31)

?
?>
Table 2: Empirical coverage rates of e>
ej for different (r, p, σ)’s over 200 Monte Carlo trials.
i X X

(r, p, σ)
(2, 0.2, 10−6 )
(2, 0.2, 10−3 )
(2, 0.4, 10−6 )
(2, 0.4, 10−3 )
(5, 0.2, 10−6 )
(5, 0.2, 10−3 )
(5, 0.4, 10−6 )
(5, 0.4, 10−3 )

d L)
Mean(Cov
0.9387
0.9400
0.9459
0.9460
0.9227
0.9273
0.9411
0.9418

12

d L)
Std(Cov
0.0197
0.0193
0.0161
0.0162
0.0244
0.0226
0.0173
0.0171

4

3

3

4

2

3

1

2

0

1

-1

0

-2

-1

-3

-2

2

1

0

-1

-2

-3

-4

-4
-3

-2

-1

0

1

2

3

-3
-3

-2

(a)

-1

0

1

2

3

-3

-2

(b)

-1

0

1

2

3

(c)

Figure 2: Q-Q (quantile-quantile) plot of S11 , S12 and S13 vs. the standard normal distribution in (a), (b)
and (c) respectively. The results are reported over 200 independent trials for r = 5, p = 0.4 and σ = 10−3 .
is extremely close to a standard Gaussian random variable. Here, similar to (3.22), we let
ρij ,

σ 2  > d d> d −1 d>
d
d>
ei X (X X ) X ei + e>
X d )−1 X d> ej
j X (X
p

(3.32)

? 2
? 2
be the empirical estimate of the theoretically predicted variance σ 2 (kUi,·
k2 + kUj,·
k2 )/p. As a result, a 95%
√
>
?
?>
>
d
d>
d L,(i,j)
confidence interval of ei X X ej would be [ei X X ej ± 1.96 ρij ]. For each (i, j), we define Cov
>
?
?>
to be the empirical coverage rate of ei X X ej over 200 Monte Carlo simulations. Correspondingly,
d L ) (resp. Std(Cov
d L )) the average (resp. the standard deviation) of Cov
d L,(i,j) over indices
denote by Mean(Cov
1 ≤ i < j ≤ n. Table 2 collects the simulation results for different values of (r, p, σ). As can be seen,
the reported empirical coverage rates are reasonably close to the nominal level 95%. In addition, Figure 1
depicts the Q-Q (quantile-quantile) plots of T12 , T13 and T14 vs. the standard Gaussian random variable over
200 Monte Carlo simulations for r = 5, p = 0.4 and σ = 10−3 . It is clearly seen that all of these are well
approximated by a standard Gaussian random variable.
Next, we turn to Theorem 2, namely the distributional guarantee for the entries of the matrix. Denote


1
d
?
Sij , √
Mij
− Mij
,
vij

(3.33)

where vij is the empirical variance defined in (3.22). In view of the 95% confidence interval predicted by
d E,(i,j)
Corollary 1, and similar to what have done for the low-rank components, for each (i, j), we define Cov
?
to be the empirical coverage rate of Mij over 200 Monte Carlo simulations. Correspondingly, denote by
d E ) (resp. Std(Cov
d E )) the average (resp. the standard deviation) of Cov
d E,(i,j) over indices 1 ≤ i, j ≤ n.
Mean(Cov
?
As before, Table 3 gathers the empirical coverage rates for Mij and Figure 2 displays the Q-Q (quantilequantile) plots of S11 , S12 and S13 vs. the standard Gaussian random variable over 200 Monte Carlo trials

?
Table 3: Empirical coverage rates of Mij
for different (r, p, σ)’s over 200 Monte Carlo trials.

(r, p, σ)
(2, 0.2, 10−6 )
(2, 0.2, 10−3 )
(2, 0.4, 10−6 )
(2, 0.4, 10−3 )
(5, 0.2, 10−6 )
(5, 0.2, 10−3 )
(5, 0.4, 10−6 )
(5, 0.4, 10−3 )

d E)
Mean(Cov
0.9380
0.9392
0.9455
0.9456
0.9226
0.9271
0.9410
0.9417

13

d E)
Std(Cov
0.0200
0.0196
0.0164
0.0164
0.0247
0.0228
0.0173
0.0172

10 0

10 0

10 -1

10 -1

10 -2

10 -2

10 -3

10 -3
Original convex estimator
De-biased convex estimator
Oracle lower bound

10

Original convex estimator
De-biased convex estimator

-4

10 -6

10 -5

10 -4

10

10 -3

(a)

-4

10 -6

10 -5

10 -4

10 -3

(b)

Figure 3: (a) Estimation error of Z cvx vs. M d measured in the Frobenius norm. (b) Estimation error of Z cvx
vs. M d measured in the `∞ norm. The results are averaged over 20 independent trials for r = 5, p = 0.2
and n = 1000.
for r = 5, p = 0.4 and σ = 10−3 . It is evident that the distribution of Sij matches that of N (0, 1) reasonably
well.
In addition to the tractable distributional guarantees, the de-biased estimator M d also exhibits superior
estimation accuracy compared to the original estimator Z cvx (cf. Theorem 3). Figure 3 reports the estimation
error of M d vs. Z cvx measured in both the Frobenius norm and in the `∞ norm across difference noise levels.
The results are averaged over 20 Monte Carlo simulations for r = 5, p = 0.2. It can be seen that the errors
of the de-biased estimator are uniformly smaller than that of the original estimator and are much closer to
the oracle lower bound. As a result, we recommend using M d even for the purpose of estimation.
We conclude this section with experiments on real data. Similar to [CP10], we use the daily temperature
data [NCD19] for 1400 stations across the world in 2018, which results in a 1400×365 data matrix. Inspection
on the singular values reveals that the data matrix is nearly low-rank. We vary the observation probability
p from 0.5 to 0.9 and randomly subsample the data accordingly. Based on the observed temperatures, we
then apply the proposed methodology to obtain 95% confidence intervals for all the entries. Table 4 reports
the empirical coverage probabilities, the average length of the confidence intervals as well as the estimation
error of both Z cvx and M d over 20 independent experiments. It can be seen that the average coverage
probabilities are reasonably close to 95% and the confidence intervals are also quite short. In addition, the
estimation error of M d is smaller than that of Z cvx , which corroborates our theoretical prediction. The
discrepancy between the nominal coverage probability and the actual one might arise from the facts that (1)
the underlying true temperature matrix is only approximately low-rank, and (2) the noise in the temperature
might not be independent.

3.7

A bit of intuition

We pause to develop some intuition behind the distributional guarantees for the proposed estimators. Bearing
in mind the intimate link between convex and nonconvex optimization (cf. (3.5)), it suffices to concentrate

Table 4: Empirical coverage rates, average lengths of the confidence intervals of the entries as well as the
estimation error vs. observation probability p. The results are averaged over 20 Monte Carlo trials.
p
0.5
0.6
0.7
0.8
0.9

Coverage
Mean
Std
0.8265
0.0016
0.8268
0.0011
0.8431
0.0006
0.8725
0.0003
0.9093
0.0003

CI Length
Mean
Std
3.6698
0.0209
2.8774
0.0098
2.3426
0.0054
2.0234
0.0052
1.8296
0.0072

14

b − M ? kF /kM ? kF
kZ
Convex Z cvx
Debiased M d
0.029
0.028
0.025
0.023
0.022
0.019
0.020
0.015
0.018
0.011

on the nonconvex problem (3.4). For the sake of clarity, we further restrict attention to the rank-1 positive
semidefinite case where M ? = x? x?> and set λ = 0, where one can focus on
minimize
n

f (x) ,

x∈R


1
PΩ xx> − M
2

2
.
F

(3.34)

b of (3.34) would necessarily satisfy the first-order optimality condition
Any optimizer x

bx
b> − M x
b = 0.
PΩ x

(3.35)

b is a reasonably reliable estimate obeying x
b ≈ x? .
We shall also assume that x
We begin with the no-missing-data case (i.e. p = 1), which already conveys the key insight. The condition (3.35) simplifies to
bx
b> x
b − x? x?> x
b = Ex
b,
x
(3.36)
which, through a little manipulation, leads to an equivalent decomposition:
kb
xk22 (b
x − x? ) =

b
Ex
|{z}

+

approximately Gaussian

>

>

b) x?
x? (x? − x
{z
}
|

negligible first-order term

b) (b
+ x? (x? − x
x − x? ).
|
{z
}

(3.37)

second-order term

Then: (1) the third term of (3.37), which can be viewed as a second-order term (in the sense that it is a
b − x? ), becomes vanishingly small when x
b ≈ x? ; (2) while the second term of (3.37) looks
quadratic term of x
?
b − x is sufficiently random and hence (b
like a first-order term, it is natural to conjecture that x
x − x? )> x? 
?
?
?
kb
x − x k2 kx k2 (i.e. the estimation error is not aligned with x ), meaning that this term is also expected to
be negligible compared to a typical first-order term (e.g. the term on the left-hand side of 3.37). In summary,
these non-rigorous arguments suggest that
b.
kb
xk22 (b
x − x? ) ≈ E x

(3.38)

b are only weakly dependent, then this means
If one can be convinced that E and x
b − x? ≈
x



1
1
σ2
?
b
E
x
≈
Ex
∼
N
0,
I
.
n
kb
xk22
kx? k22
kx? k22

(3.39)

Returning to the missing data scenario with p < 1, everything is based on the following approximation


bx
b> − x? x?> x
b≈p x
bx
b> − x? x?> x
b;
PΩ x
b are
this is certainly expected — using standard concentration arguments — if we “pretend” that PΩ and x
statistically independent. With this approximation in mind, one can translate (3.35) into

bx
b> x
b − x? x?> x
b ≈ PΩ (E) x
b.
p x
(3.40)
Repeating the above argument then immediately yields
b − x? ≈
x

1
1
1
b ≈
· PΩ (E) x
· PΩ (E) x?
kb
xk22 p
pkx? k22

approx.

∼


N 0,


σ2
I
n .
pkx? k22

(3.41)

The case with λ > 0 can be intuitively understood in a very similar way by first de-shrinking the estimate;
we omit it here for brevity. We note that these hand-waving arguments can all be made rigorous, which is
the main content of the proof.

3.8

Inference based on spectral estimates?

One would naturally be curious about whether there are other estimation procedures that also enable reasonable statistical inference. While this is beyond the scope of the current paper, we take a moment to
discuss one alternative: the spectral method, as pioneered by [KMO10a, KMO10b] in the matrix completion problem. In a nutshell, this approach consists in computing a rank-r approximation to PΩ (M )/p,
15

which is precisely the spectral initialization widely used in a two-stage nonconvex algorithm (cf. Algorithm 1) [KMO10b, SL16, CW15, CCF18, MWCC17]. While inference has not been, as far as we know, the
focus of prior work on spectral methods,4 the recent papers [AFWZ17, MWCC17] hinted at the possibility
of characterizing the distribution of the spectral estimate. Take a simple symmetric rank-1 case for example
(i.e. M ? = x? x?> with kx? k2 = 1): the leading eigenvector uspectral of PΩ (M )/p often admits the following
approximation (up to a global sign)
uspectral ≈

1
PΩ (M )x? .
p

Expanding PΩ (M ) = px? x?> + PΩ (x? x?> ) − px? x?> + PΩ (E), we arrive at

1
1
uspectral ≈ x? x?> x? +
PΩ (x? x?> ) − x? x?> x? + PΩ (E)x? ,
p
p
which is equivalent to
uspectral − x? ≈


1
1
PΩ (E)x? +
PΩ (x? x?> ) − x? x?> x? .
p
p
|
{z
} |
{z
}
noise effect

(3.42)

effect of random sub-sampling

In words, two major factors dictate the uncertainty of the spectral estimate: (1) the additive Gaussian
noise (cf. the 1st term on the right-hand side of (3.42)), and (2) random sub-sampling (in particular, the
randomness incurred by employing the sub-sampled PΩ (x? x?> )/p to approximate the truth x? x?> ). Given
that the random sub-sampling effect cannot be ignored at all, the spectral estimates often suffer from a
much larger estimation error (and hence a higher degree of uncertainty) compared to either the convex or
the nonconvex estimates. In truth, this random sub-sampling effect does not go away even when the noise
vanishes. Consequently, uncertainty quantification based on the spectral estimates may not be the most
desirable option.

4

Prior art

Matrix completion. Low-rank matrix completion, or more broadly, low-rank matrix recovery, is a fundamental task that permeates through a wide spectrum of applications in science, engineering, and finance
(e.g. [RS05, SY07, CC14, FSZZ18, CCG15, ZPL15, BN06, CC18b, FWZ19, KS11, CZ16, KX15, FLM13, DR17,
CDDD19, DPVW14, SZ12, FS11]). A paper of this length is unable to review all papers motivating and contributing to this enormous subject; interested readers are referred to [DR16,CC18a] for extensive discussions
of motivating applications as well as the exciting recent development.
Numerous algorithms have been proposed to solve this problem efficiently, with two paradigms being
arguably the most widely used: convex relaxation and nonconvex optimization. We briefly review the
literature contributing to these two paradigms.
• Convex relaxation was largely popularized by the seminal works [Faz02, RFP10, CR09]. In the absence
of noise, it has been shown that nuclear norm minimization, which can be solved by semidefinite programming, achieves minimal sample complexity under mild conditions [Gro11,Rec11,Che15]. When the
observed entries are further corrupted by noise, Candès and Plan [CP10] provided the first theoretical
guarantee regarding the estimation accuracy of perhaps the most natural convex relaxation algorithm.
While the theory might be tight for certain adversarial scenarios (as shown by the recent work [KS19]),
it is loose by some large factor under the natural random noise model. This statistical guarantee has
been partially improved later on by two papers [NW12, KLT11] under proper modifications to the convex program (e.g. enforcing an additional spikiness constraint [NW12, Klo14], or modifying the squared
loss [KLT11]). Nevertheless, the error bounds provided in these papers (and their follow-ups) remain
suboptimal, unless the typical size of the noise is sufficiently large. Our recent work [CCF+ 19] establishes near-optimal statistical guarantees — when the estimation errors are measured by the Frobenius
4 We

note that inference from spectral estimates has been investigated in other context beyond matrix completion (e.g. the
model without missing data [Xia19, FFHL19]).

16

norm, the spectral norm, and the `2,∞ norm — for a wide range of noise levels when r = O(1). All of
these estimation guarantees, however, come with a hidden and likely large pre-constant, which do not
serve the inferential purpose well.
• Nonconvex optimization algorithms, as pioneered by [KMO10a, Sre04], become increasingly more popular for solving various low-rank factorization problems, due to their appealing computational complexities [JNS13, CLS15, CC17, TBS+ 16, SL16, ZL16, CCFM19, WZG16, CLL19]. For instance, the gradientbased nonconvex methods have been analyzed for noisy matrix completion [KMO10b,CW15,MWCC17,
CCF+ 19], which are shown to achieve near-optimal statistical accuracy and linear-time convergence
guarantees all at once. Going beyond gradient methods, we note that other nonconvex methods
(e.g. [RS05,JMD10,WYZ12,JNS13,FRW11,Van13,LXY13,Har14,JKN16,RT11,WCCL16,DC18,ZWL15,
ZWYG18, MSL19, CCD+ 19]) and landscape properties [GLM16, CL17, GJZ17, ZSL19, ZJSL18, SXZ19]
have been largely explored as well. The interested readers are referred to [CLC19] for an in-depth discussion. One limitation, however, is that the theoretical guarantees provided for nonconvex algorithms
often exhibit sub-optimal dependency in the rank r of the unknown matrix; for instance, most theory
requires a sample complexity of at least nr2 (in fact, often much larger than nr2 ). This is outperformed
by the convex relaxation approach.
Despite these recent developments, very little work has investigated statistical inference for noisy matrix
completion. While [CKLN18, CKL16, CN15, CEGN15] discussed the construction of “honest” confidence
regions, the volume of these regions is dependent on some (possibly huge) hidden constants, thus resulting
in over-coverage. Perhaps the closest to our paper is the recent work [Xia18], which investigated inference
for low-rank trace regression. Employing a closely related de-biased estimator with sample splitting, the
paper [Xia18] established asymptotic normality of a certain projected distance between the estimate and the
truth. The result therein, however, requires a sampling mechanism obeying the restricted isometry property
(e.g. i.i.d. Gaussian designs), which fails to hold for matrix completion. Also, our approach does not require
sample splitting — a technique that is convenient for analysis but conservative in constructing confidence
regions. Another work by Cai et al. [CLR16] developed a unified approach to provide inference guarantees
for linear inverse problems including low-rank matrix estimation. Their results, however, require the sample
size to exceed the total dimension n2 even under the Gaussian design. Finally, a recent line of work [MX17]
explored uncertainty quantification under the Bayesian setting, hypothesizing on a special prior regarding
the true matrix. This departs drastically from the scenario considered herein.
Inference in high-dimensional problems. Inference in high-dimensional sparse regression has received
much attention in the last few years [WR09, ZZ14, BCH11, vdGBRD14, JM14b, DBMM15, CG17, NL17,
NNLL18, LSST16, LTTT14, MMB09, DBZ17, ZC17, BFL+ 18]. Our inferential approach is partly inspired
by the recent developments on this topic, particularly with regard to the de-biased / de-sparsified estimators
proposed for Lasso. More specifically, recognizing the non-negligible bias of the Lasso estimate
1
βb , arg min ky − Xβk22 + λkβk1 ,
β
2

(4.1)

A line of work [ZZ14, vdGBRD14, JM14a] came up with a linear transformation of βb of the form

β d , βb + LX > y − X βb ,

(4.2)


where L is some matrix to be designed, and X > y − X βb corresponds to the negative gradient of the
b or equivalently, the (scaled) sub-gradient of the `1 norm at β.
b If L is properly chosen,
squared loss at β,
then β d is able to correct the bias of this nonlinear estimator β, while controlling the degree of uncertainty.
Many follow-up papers have investigated the design of L as well as the resulting inferential guarantees [ZZ14,
vdGBRD14, JM14a, JM15].
Interestingly, our de-biased estimator (3.7) for matrix completion admits a very similar form as (4.2). To
see this, recall that our de-biased estimator is given by


M d = Prank-r Z − p1 PΩ Z + p1 PΩ M ,
17

where Z can be either Z cvx or X ncvx Y ncvx> (see Table 1). Let T be the tangent space of the set of rankr matrices at Z cvx,r (resp. X ncvx Y ncvx> ) in the convex (resp. nonconvex) case, and PT be the projection
operator onto T . Somewhat surprisingly, replacing Prank-r by PT does not affect the de-biased estimator by
much, in the sense that


(4.3)
M d ≈ PT Z − p1 PΩ Z + p1 PΩ M .
In addition, recognizing that Z almost lies within the tangent space T ,5 one can rewrite

M d ≈ Z − p1 PT PΩ Z − M ,

(4.4)

a fact to be made precise in Section 5.1. This
bears a striking resemblance to the de-biasing approach


developed for Lasso — the term PΩ Z − M ? represents the gradient of the squared loss 0.5kPΩ Z − M k2F
(or equivalently, the negative sub-gradient of the nuclear norm) at Z, and PT is the linear operator we pick.
To the best of our knowledge, no de-biasing approach — with rigorous theoretical guarantees and without
sample splitting — has been proposed and analyzed for matrix completion in prior literature. In addition,
we note that our de-biased estimator for matrix completion achieves full statistical efficiency in terms of
both the rates and the pre-constant; in comparison, the commonly used de-biased estimators for sparse
linear regression typically fall short of achieving the best possible estimation accuracy, unless additional
thresholding procedures are enforced.
Finally, de-biased estimators have been put forward to tackle other high-dimensional problems, including
but not limited to generalized linear models [vdGBRD14,NL17], graphical models [JVDG15,RSZZ15,MLL17,
JvdG17], sparse PCA [JvdG18], treatment effects estimation [CCD+ 18,AIW18]. These are beyond the scope
of the current paper.

5

Architecture of the proof

This section outlines the main steps for establishing Theorem 1 and Theorem 2. Before starting, we introduce
some useful notation. For convenience of presentation, we insert the factor 1/p into (3.4) and redefine the
nonconvex loss function as
 2
λ
λ
1
2
2
PΩ XY > − M F +
kXkF +
kY kF .
(5.1)
f (X, Y ) ,
2p
2p
2p
In addition, for each 1 ≤ j, k ≤ n, we define the indicator δjk , 1{(j, k) ∈ Ω}, which is a Bernoulli random
variable with mean p.
We also note that Theorem 1 (resp. Theorem 2) is subsumed by Theorem 5 (resp. Theorem 6). As a
result, we shall focus on establishing Theorem 5 (resp. Theorem 6) when it comes to estimating low-rank
factors (resp. the entries of the matrix).
Theorem 5. Suppose that the samplepcomplexity meets n2 p ≥ Cκ4 µ2 r2 n log3 n for some sufficiently large
constant C > 0 and the noise obeys σ (κ4 µrn log n)/p ≤ cσmin for some sufficiently small constant c > 0.
Then the decomposition in Theorem 1 remains valid, except that the residual matrices ΨX , ΨY ∈ Rn×r
satisfy, with probability at least 1 − O(n−3 ), that


s
s
7 µrn log n
7 µ3 r 3 log2 n

σ
σ
κ
κ

.
(5.2)
max kΨX k2,∞ , kΨY k2,∞ . √
+
pσmin σmin
p
np
?
Theorem 6. Instate the assumptions of Theorem 5. Recall the definition of vij
in (3.17). Then one has the
following decomposition
d
?
Mij
− Mij
= gij + ∆ij ,
(5.3)
?
where gij ∼ N (0, vij
) and the residual obeys — with probability exceeding 1 − O(n−10 ) — that


s
s
s
!2

 σ
8 µrn log n
8 µ3 r 3 log2 n
3 µr log n
σ
σ
κ
κ
κ
?
?
+ √
|∆ij | . Ui,· 2 + Vj,· 2 √ 
+
.
p σmin
p
np
σmin
p
5 More

precisely, if Z = X ncvx Y ncvx> , then Z ∈ T ; if Z = Z cvx , one has PT (Z) ≈ Z.

18

5.1

Near equivalence between convex and nonconvex estimators

Note that Theorem 5 and Theorem 6 are concerned with the de-biased estimators built upon both convex and
nonconvex estimates. At first glance, one needs to establish theoretical guarantees for each of them separately.
Fortunately, as alluded to previously (cf. (3.5)), the convex and nonconvex estimates are extremely close —
a fact that has been established in [CCF+ 19]. The proximity of these two estimates naturally extends to
the de-biased estimators constructed based on them. As a result, it suffices to concentrate on proving the
theorems for any of these estimators; the claims for the other one follow immediately.
The following key lemma formalizes this argument, which will be established in Appendix C (see also Figure 4 for numerical evidence). Before continuing, we remind the readers of the key notation (see Appendix B
for precise definitions):
• (X ncvx , Y ncvx ): an approximate solution to the nonconvex problem (3.4) (see Appendix A.1);
• M cvx,d , X cvx,d , Y cvx,d : the de-biased estimators built upon the convex optimizer Z cvx ;
• M ncvx,d , X ncvx,d , Y ncvx,d : the de-biased estimators built upon the nonconvex estimate (X ncvx , Y ncvx ).
Our proximity result is this:
3
2
4 2 2
Lemma 3. Suppose that the sample
p size obeys n p ≥ Cκ µ r n log n for some sufficiently large constant
4
C > 0 and the noise satisfies σ (κ µnr log n)/p ≤ cσmin for some sufficiently small constant c > 0. Set
√
λ = Cλ σ np with some large enough constant Cλ > 0.

1. With probability at least 1 − O(n−10 ), one has
max



r
1
n
M
−X
Y
, M
−X
Y
. 4 ·σ
,
F
F
n
p
r
q
1
n
σ
2
2
min
kX cvx,d R − X ncvx,d kF + kY cvx,d R − Y ncvx,d kF . 3 · √
,
r×r
n
σmin p
R∈O
cvx,d

ncvx,d

ncvx,d>

ncvx,d

ncvx,d

ncvx,d>

(5.4a)
(5.4b)

where Or×r is the set of r × r rotation matrices.
2. With probability exceeding 1 − O(n−10 ), one has


M ncvx,d − X ncvx Y ncvx> − p−1 PT PΩ X ncvx Y ncvx> − M

F

.

1
·σ
n4

r

n
,
p

(5.5)

where T is the tangent space of the set of rank-r matrices at X ncvx Y ncvx> . The same holds true if we
replace X ncvx Y ncvx> with Z cvx and replace T with the tangent space at Z cvx,r = Prank-r (Z cvx ) .
In short, the first part of Lemma 3 tells us that
M cvx,d ≈ X ncvx,d Y ncvx,d> ≈ M ncvx,d ,


X cvx,d , Y cvx,d ≈ X ncvx,d , Y ncvx,d
(up to global rotation),

(5.6a)
(5.6b)

whereas the second part of Lemma 3 justifies that the proposed de-biased estimator is closely approximated
by a linearized version (cf. (4.4)). Note that this linearized form bears a resemblance to the de-biased
estimators developed for sparse linear regression [ZZ14, vdGBRD14, JM14a].
With Lemma 3 in place, we shall, from now on, focus on proving the main theorems for the nonconvex
estimators, viz.
1. establishing Theorem 5 for the de-shrunken low-rank factors (X ncvx,d , Y ncvx,d );
2. establishing Theorem 6 for the de-biased matrix estimator X ncvx,d Y ncvx,d> .
To simplify the presentation hereafter, we shall use the following notation throughout the rest of this section:
• (X, Y ): the nonconvex estimate (X ncvx , Y ncvx );
• (X d , Y d ): the de-shrunken estimate defined in (3.8) based on (X, Y ) = (X ncvx , Y ncvx );
• M d , X d Y d> .
19

10 0

10 -2

10 -4

cvx d
ncvx d
cvx d

ncvx d

ncvx d

10

-6

10 -8
10 -6

dist

ncvx d

cvx d

cvx d

10 -5

ncvx d
ncvx d
ncvx d

ncvx d

10 -4

10 -3

Figure 4: The relative estimation errors of M cvx,d and M ncvx,d and related quantities in Lemma 3 vs. the
standard deviation σ of the noise. Here, dist((X cvx,d , Y cvx,d ), (X ncvx,d , Y ncvx,d )) is defined to be the left-hand
side of (5.4b). The results, which are averaged over 20 trials, are reported for n = 1000, r = 5, p = 0.2,
√
and λ = 5σ np. As can be seen, the difference between M cvx,d , M ncvx,d and X ncvx,d Y ncvx,d> , as well as the
distance dist((X cvx,d , Y cvx,d ), (X ncvx,d , Y ncvx,d )), are all significantly smaller than the estimation errors.

5.2

A precise characterization of the de-shrunken low-rank factors

We start with a precise characterization of the de-shrunken low-rank factors X d and Y d , which paves the
way for demonstrating both Theorem 5 and Theorem 6.
Lemma 4 (Decompositions of low-rank factors). Denote
A,



1
PΩ XY > − X ? Y ?> − XY > − X ? Y ?> .
p

(5.7)

One has the following decompositions for X d and Y d
−1
−1
−1
1
PΩ (E) Y d Y d> Y d
+ X ? Y ?> Y d Y d> Y d
− AY d Y d> Y d
p

−1 1/2
−1
λ
Y >Y
Y d> Y d
+ X∆balancing ;
+ ∇X f (X, Y ) Ir +
p
−1
−1
−1
1
>
Y d = [PΩ (E)] X d X d> X d
+ Y ? X ?> X d X d> X d
− A> X d X d> X d
p

−1 1/2
−1
λ
+ ∇Y f (X, Y ) Ir +
X >X
X d> X d
− Y ∆balancing .
p

Xd =

(5.8a)

(5.8b)

Here, we denote

−1 1/2 
−1 1/2
λ
λ
∆balancing , Ir +
X >X
− Ir +
Y >Y
,
p
p

(5.9)

which measures the imbalance between the low-rank factors X and Y .
Proof. The claims follow from straightforward algebraic manipulations; see Appendix D.1.
We make a few observations regarding Lemma 4. Take the decomposition of X d (5.8a) as an example:
• First, the term AY d (Y d> Y d )−1 vanishes when we have full observations, i.e. p = 1. Second, the terms
involving ∇X f (X, Y ) and ∆balancing are both zero if (X, Y ) is an exact stationary point of f (·, ·); to
see this, it is not hard to verify that any stationary point of f (·, ·) necessarily satisfies X > X = Y > Y ,
which in turn implies ∆balancing = 0. Consequently, the last three terms in (5.8a) are expected to be
small when p is sufficiently large and (X, Y ) is near a stationary point.

20

• Turning to the first two terms in (5.8a), we note that the second term of (5.8a) is close to X ?
(up to rotation) if Y d is a nearly accurate approximation to Y ? . In comparison, the first term
PΩ (E)Y d (Y d> Y d )−1 /p has to do with a collection of Gaussian random variables, which accounts for
the main uncertainty term.
We shall make precise these arguments in subsequent subsections.

5.3

Taking global rotation into account

In order to invoke the decompositions of X d and Y d (cf. Lemma 4) to characterize the estimation errors,
we still need to incorporate the (unrecoverable) rotation matrix. From now on, we shall focus primarily on
the factor X d . The claims on the other factor Y d can be easily obtained via symmetry.
Denote
d
d
X , X dH d
and
Y , Y dH d,
(5.10)
where we recall that H d is the rotation matrix that best aligns (X d , Y d ) and (X ? , Y ? ) (see (3.12)). Substituting the identity
Y d Y d> Y d

−1

H d = Y d H d H d> Y d> Y d H d

−1

=Y

d

Y

d>

Y

d −1

into the decomposition (5.8a), we arrive at
h
i
1
d
d> d −1
d
d> d −1
d
d> d −1
PΩ (E) Y Y Y
+ X ? Y ?> Y Y Y
− Ir − AY Y Y
p

−1 1/2
−1 d
λ
+ ∇X f (X, Y ) Ir +
Y >Y
Y d> Y d
H + X∆balancing H d
p
−1
1
= PΩ (E) Y ? Y ?> Y ?
+ ΦX .
(5.11)
p

X dH d − X ? =

Here, the term ΦX ∈ Rn×r is defined to be
ΦX ,

h d d> d 
h
i
−1 i
1
d
d> d −1
−1
PΩ (E) Y Y Y
− Y ? Y ?> Y ?
+ X ? Y ?> Y Y Y
− Ir
p
{z
} |
{z
}
|
:=Φ1

−AY
|

d

d>

Y Y
{z

:=Φ3

:=Φ2

λ
Y >Y
+ ∇X f (X, Y ) Ir +
p
} |

d −1



−1 1/2
{z

:=Φ4

Y d> Y d

−1

H d + X∆balancing H d ,
}
(5.12)

where A is defined in (5.7). To establish Theorem 5, it remains to (1) demonstrate that ΦX has small `2,∞
norm, and (2) show that PΩ (E)Y ? (Y ?> Y ? )−1 /p is approximately a Gaussian random matrix. These two
steps constitute the main content of the next subsection.

5.4

Key lemmas for establishing Theorem 5

We now state five key lemmas. Taking these collectively and substituting them into (5.11) immediately
establish Theorem 5.
We shall start by controlling the term Φ1 as defined in (5.12).
Lemma 5 (Negligibility of Φ1 ). Suppose that the sample complexity
obeys n2 p ≥ Cκ4 µ2 r2 n log3 n for some
p
4
sufficiently large constant C > 0 and the noise satisfies σ (κ µrn log n)/p ≤ cσmin for some sufficiently
small constant c < 0. Then with probability at least 1 − O(n−10 ), we have
s
σ
σ
κ3 µrn log n
kΦ1 k2,∞ . √
·
.
pσmin σmin
p
21

d

Proof. Fix any 1 ≤ j ≤ n. If the de-shrunken estimate Y were independent of the randomness in the jth
>
row of the matrix, i.e. e>
j PΩ (E), then kej Φ1 k2 would be well controlled. This hypothesis is certainly false,
d

as Y clearly depends on e>
j PΩ (E). Nevertheless, by exploiting the leave-one-out technique recently used
in [EKBB+ 13, EK15, AFWZ17, MWCC17, CFMW19, CCF+ 19, CLL19, DC18], one can properly decouple the
dependency and establish the desired bound. See Appendix D.2.
The next lemma controls the size of kΦ2 k2,∞ . In essence, the term Φ2 measures the difference between
d

the estimate Y and the true signal Y ? ; the closer these two are, the smaller kΦ2 k2,∞ should be. See
Appendix D.3 for the proof of the following result.
Lemma 6 (Negligibility of Φ2 ). Suppose that the sample complexity
obeys n2 p ≥ Cκ4 µ2 r2 n log3 n for some
p
sufficiently large constant C > 0 and the noise satisfies σ (κ4 µrn log n)/p ≤ cσmin for some sufficiently
small constant c < 0. Then with probability exceeding 1 − O(n−10 ), one has
s
s
!
σ
κ7 µrn
κ7 µ3 r3 log n
σ
κ
.
+
kΦ2 k2,∞ . √
pσmin
σmin
p
np
Moving on to Φ3 and Φ4 , one has the following lemmas.
Lemma 7 (Negligibility of Φ3 ). Suppose that the sample complexity
obeys n2 p ≥ Cκ4 µ2 r2 n log3 n for some
p
sufficiently large constant C > 0 and the noise satisfies σ (κ4 µrn log n)/p ≤ cσmin for some sufficiently
small constant c < 0. Then with probability exceeding 1 − O(n−10 ), we have
s
σ
κ5 µ3 r3 log2 n
kΦ3 k2,∞ . √
.
pσmin
np
Proof. It is straightforward to check that when p = 1, one has kΦ3 k2,∞ = kAk = 0, where we recall the
definition of A in (5.7). Therefore, one expects kΦ3 k2,∞ to be small when p is sufficiently large. See
Appendix D.4.
Lemma 8 (Negligibility of Φ4 ). Suppose that the sample complexity
obeys n2 p ≥ Cκ4 µ2 r2 n log3 n for some
p
sufficiently large constant C > 0 and the noise satisfies σ (κ4 µrn log n)/p ≤ cσmin for some sufficiently
small constant c < 0. Then with probability at least 1 − O(n−10 ), one has
kΦ4 k2,∞ . √

σ
1
.
·
pσmin n4

Proof. It is easily seen that the size of Φ4 depends on how close (X, Y ) is to a stationary point of f (·, ·).
For instance, in the extreme case where (X, Y ) is an exact stationary point, then one would have Φ4 = 0.
See Appendix D.5.
The last lemma asserts that PΩ (E)Y ? (Y ?> Y ? )−1 /p is, in some sense, close to a zero-mean Gaussian
random matrix with the desired covariance.
Lemma 9 (Approximate Gaussianity of PΩ (E)Y ? (Y ?> Y ? )−1 /p). Suppose that the sample size obeys n2 p ≥
Cκ2 µrn log3 n for some sufficiently large constant C > 0. Then one has the decomposition
−1
1
PΩ (E) Y ? Y ?> Y ?
= ZX + ∆X ,
p
where each row of ZX ∈ Rn×r is independent and identically distributed according to
 σ2

i.i.d
−1
>
ZX
ej ∼ N 0,
(Σ? )
,
for 1 ≤ j ≤ n.
p
In addition, with probability at least 1 − O(n−10 ), the remaining term ∆X ∈ Rn×r obeys
s
σ
κ2 µr2 log2 n
k∆X k2,∞ . √
.
pσmin
np
22

?
?> ? −1
Proof. Fix any 1 ≤ j ≤ n. The jth row, namely e>
Y ) /p] is conditionally Gaussian in
j [PΩ (E)Y (Y
the sense that


n
 σ2  1 X



−1
−1
> 1
?
?> ? −1
? >
?
PΩ (E) Y Y Y
Ω ∼ N 0,
δjk (Σ? )
ej
Yk,·
Yk,·
(Σ? )
,
p
p p
k=1

where we recall that δjk = 1{(j, k) ∈ Ω}. Recognize that the conditional covariance matrix concentrates
>
sharply around its expectation, i.e. σ 2 (Σ? )−1 /p, which is the covariance matrix of ZX
ej that we are after.
?
?> ? −1
Hence, one can expect that PΩ (E)Y (Y Y ) /p is, marginally, not too far from a Gaussian random
matrix. This argument can be carried out formally; see Appendix D.6.

5.5

From low-rank factors to matrix entries (Proof of Theorem 6)

We now turn attention to inference on the matrix entries, by establishing Theorem 6. Towards this, we first
make the following observation: for any 1 ≤ i, j ≤ n,
d

d
?
Mij
− Mij
= e>
i X Y

= e>
i

d>

? ?>
ej − e>
ej
i X Y

>
 d
>
d
d
d
?
?>
>
?
X − X Y ej + ei X ? Y − Y ? ej + e>
Y − Y ? ej .
i X −X

(5.13)

One can readily apply the decompositions in Theorem 5 to obtain
 ?>
d
?
?>
?>
ej = e>
ej + e>
ej ,
e>
i ZX Y
i ΨX Y
i X −X Y

d
>
?
? >
>
? >
>
? >
ei X Y − Y
ej = ei X ZY ej + ei X ΨY ej .

(5.14)
(5.15)

Take the preceding three identities collectively to reach
d
?
Mij
− Mij
= e>
ZX Y ?> ej + e>
X ? ZY> ej
{z i
}
|i
:=Θij

 d
>
d
?>
? >
>
?
+ e>
ej + e>
Y − Y ? ej .
i ΨX Y
i X ΨY ej + ei X − X
{z
}
|
:=Λij

?>
? >
Following the same route as in Section 5.4, one can verify that Θij = e>
ej + e>
i ZX Y
i X ZY ej is approximately Gaussian, whereas the residual term Λij is small in magnitude. These claims are formally stated in
the next two lemmas, with the proofs deferred to Appendix E.

Lemma 10 (Negligibility of Λij ). Suppose that the sample complexity
obeys n2 p ≥ Cκ4 µ2 r2 n log3 n for some
p
sufficiently large constant C > 0 and the noise satisfies σ (κ4 µrn log n)/p ≤ cσmin for some sufficiently
small constant c < 0. Then with probability exceeding 1 − O(n−10 ), one has


s
s
s
!2

 σ
8 µrn log n
8 µ3 r 3 log2 n
σ
κ
κ
σ
κ3 µr log n
?
?


|Λij | . Ui,· 2 + Vj,· 2 √
+
+ √
.
p σmin
p
np
σmin
p
Lemma 11 (Approximate Gaussianity of Θij ). Suppose that np ≥ Cκ2 µr2 log2 n for some sufficiently large
constant C > 0. Then we have the decomposition
?>
? >
Θij = e>
ej + e>
i ZX Y
i X ZY ej = gij + θij ,
?
where gij ∼ N (0, vij
) and the remaining term θij satisfies — with probability exceeding 1 − O(n−10 ) — that

σ
|θij | . √
p

s

n
κ2 µr log n
?
min Ui,·
np

2

?
, Vj,·

o
2

.

Putting the above two lemmas together immediately establishes Theorem 6 and hence Theorem 2.
23

6

Discussion

The present paper makes progress towards inference and uncertainty quantification for noisy matrix completion, by developing simple de-biased estimators that admit tractable and accurate distributional characterizations. While we have achieved some early success in accomplishing this, our results are likely sub-optimal
in the following aspects:
• Dependency on the rank and the condition number. To enable valid inference, our sample complexity
(cf. (3.13) and (3.18a)) scales sub-optimally with the rank r and the condition number κ. The suboptimality can be understood through comparisons with the sample size requirement O(nr log2 n) in
the noise-free settings, which is independent of κ and matches the information limit (up to some log
factor). Improving such dependency calls for more refined analysis techniques.
?
• Detection of the size of the entries. On one hand, when the size of the entry Mij
is moderately large
(cf. (3.18b)), Corollary 1 allows us to construct a valid confidence interval for it. On the other hand,
?
d
?
?
is better
− Mij
k2 vanishes, Theorem 5 tells us that the estimation error Mij
k2 + kVj,·
when kUi,·
approximated by the inner product of two independent Gaussian random vectors. It remains to be seen
?
?
how to determine whether kUi,·
k2 + kVj,·
k2 is too small.

• Low signal-to-noise (SNR) regime. Our theory operates under the moderate-to-high SNR regime, where
2
σmin
/σ 2 (which is proportional to the SNR) is required to exceed the order of n/p; see the conditions in
Theorem 5. It is unclear whether the connection between the convex and the nonconvex estimators hold
in the low SNR regime. How to conduct inference in such a scenario is an important future direction.
In addition, our investigation has been dedicated to a natural random model, which by no means covers
the most general settings of practical interest. There are numerous possible extensions that merit future
investigation:
• Approximate low-rank structure. Our current theory is built upon the exact low-rank structure of M ? .
Realistically, the matrix of interest is often only approximately low-rank. It is of great interest to study
how to carry out statistical inference under such imperfect structural assumptions.
• More general sampling patterns. This paper operates under the uniform random sampling assumption,
which might sometimes be off in practical situations. It would be interesting to investigate whether our
results in this paper can extend to more general non-uniform sampling patterns (e.g. [NW12]).
• Extensions to robust PCA, sparse PCA, and 1-bit matrix completion. A variety of important extensions
of matrix completion have been explored in prior literature, including but not limited to the case
with sparse outliers (i.e. robust PCA [CLMW11, CSPW11]), the case where the matrix of interest
is simultaneously sparse and low-rank (i.e. sparse PCA [ZHT06, CMW13]), and the case where only
finite-bit observations are available (i.e. 1-bit matrix completion [DPVW14, CZ13]). Performing valid
uncertainty assessment for these scenarios requires non-trivial extensions of the link between convex and
nonconvex optimization.
• Other loss functions. In the estimation stage, one might sometimes prefer other loss functions beyond the
penalized squared loss. This might arise from either statistical considerations (e.g. employing a penalized
Poisson log-likelihood to accommodate Poisson noise [CX16]), or computational concerns (e.g. adopting
a non-smooth loss to improve convergence [CCD+ 19]). It would be of fundamental importance to
develop a unified inferential framework that covers a broader family of loss functions.

Acknowledgements
Y. Chen is supported in part by the AFOSR YIP award FA9550-19-1-0030, by the ONR grant N0001419-1-2120, by the ARO grant W911NF-18-1-0303, by the NSF grants CCF-1907661 and IIS-1900140, and
by the Princeton SEAS innovation award. J. Fan is supported in part by NSF grants DMS-1662139 and
DMS-1712591, ONR grant N00014-19-1-2120, and NIH grant 2R01-GM072611-13. C. Ma is supported in
part by Hudson River Trading AI Labs (HAIL) Fellowship. This work was done in part while Y. Chen was
24

visiting the Kavli Institute for Theoretical Physics (supported in part by the NSF grant PHY-1748958). We
thank Weijie Su for helpful discussions.

A

Preliminaries

In this section, we gather several notation and preliminary facts that are useful throughout the analysis. All
the proofs, if needed, are deferred to Appendix I.

A.1

Algorithmic details of nonconvex optimization

To begin with, we make precise the algorithm used to minimize the nonconvex loss function (5.1). Specifically,
we describe the following details that are crucial for us to implement Algorithm 1:
• Set the initial point to be (X 0 , Y 0 ) = (X ? , Y ? ) or the spectral initialization as in [MWCC17, CLL19];
• Set the stepsize η  1/(n6 κ3 σmax );
• Set the maximum number of iterations to be t0  n23 ;
• The returned estimate is (X ncvx , Y ncvx ) , (X t? , Y t? ), where


t? , min 0 ≤ t ≤ t0 ∇f X t , Y t

F

≤


1 λ√
σ
.
min
n5 p

(A.1)

In words, we run gradient descent in Algorithm 1 until we reach a point whose gradient is exceedingly
small.
Remark 6 (Spectral initialization). Many of the preliminary facts below were established for the case
(X 0 , Y 0 ) = (X ? , Y ? ) [CCF+ 19], which is certainly not implementable in practice, however, it serves as
a good proxy for studying the convex estimator. Fortunately, the same theoretical guarantees stated in Appendix A.2 can be readily established for spectral initialization using almost the same arguments adopted
in [MWCC17, CLL19, CCF+ 19]. We omit this part mainly for the sake of brevity.
To facilitate analysis, we introduce a set of auxiliary nonconvex loss functions. For any 1 ≤ j ≤ n, define
f (j) (X, Y ) ,


1
PΩ−j ,· XY > − M
2p

2
F

+


1
Pj,· XY > − M
2

2
F

+

λ
λ
2
2
kXkF +
kY kF ,
2p
2p

(A.2)

where PΩ−j,· : Rn×n 7→ Rn×n (resp. Pj,· (·)) denotes the orthogonal projection onto the subspace of matrices
that vanish outside of {(i, k) ∈ Ω | i 6= j} (resp. {(i, k) | i = j}). Let
(X (j) , Y (j) ) = (X t? ,(j) , Y t? ,(j) )

(A.3)

be the nonconvex estimate returned by this auxiliary algorithm (i.e. Algorithm 2), which serves as an
approximate solution to (A.2).
Algorithm 2 Gradient descent for solving the auxiliary nonconvex problem (A.2).
Suitable initialization: (X 0,(j) , Y 0,(j) ) = (X ? , Y ? )
Gradient updates: for t = 0, 1, . . . , t? − 1 do

X t+1,(j) =X t,(j) − η∇X f (j) X t,(j) , Y t,(j) ;

Y t+1,(j) =Y t,(j) − η∇Y f (j) X t,(j) , Y t,(j) .

25

(A.4a)
(A.4b)

A.2

Properties of approximate nonconvex solutions

This subsection gathers the properties of the (approximate) nonconvex solutions. Throughout this subsection,
we use the shorthand
(X, Y ) = (X ncvx , Y ncvx )
(A.5)
and recall the definition of (X (j) , Y (j) ) in (A.3). The regularization parameter is chosen to satisfy
√
λ  σ np.

(A.6)

To further simplify the presentation, we introduce F ? , F , F d , F d,(j) ∈ R2n×r as follows








X?
X
Xd
X d,(j)
?
d
d,(j)
F ,
; F ,
; F ,
; F
,
,
Y?
Y
Yd
Y d,(j)

(A.7)

and define
n
o
2
2
kXR − X ? kF + kY R − Y ? kF ,
R∈O r×r
n
o
2
2
2
F (j) R − F ? F = arg min
X (j) R − X ? F + Y (j) R − Y ? F ,
R∈O r×r
n
o
2
2
2
X (j) R − XH F + Y (j) R − Y H F ,
F (j) R − F H F = arg min
R∈O r×r
n
o
2
2
2
X d,(j) R − X ? F + Y d,(j) R − Y ? F .
F d,(j) R − F ? F = arg min
2

H , arg min kF R − F ? kF = arg min
R∈O r×r

H (j) , arg min

R∈O r×r

R(j) , arg min

R∈O r×r

H d,(j) , arg min

R∈O r×r

R∈O r×r

(A.8a)
(A.8b)
(A.8c)
(A.8d)

The claims stated below hold under the sample complexity and the noise condition presumed in [CCF+ 19,
Theorem 1] (see also Theorem 5 in the current manuscript)
r
n
σmin
3
2
4 2 2
p
.
n p  κ µ r n log n and σ
4
p
κ µr log n
1. The first set of facts is related to (X, Y ). In view of [CCF+ 19], F is a faithful estimate6 of F ? , in the
sense that
r
n
σ
kF H − F ? kF .
kX ? kF ,
(A.9a)
σmin p
r
σ
n
kF H − F ? k .
kX ? k ,
(A.9b)
σmin p
s
σ
n log n
kF H − F ? k2,∞ . κ
kF ? k2,∞
(A.9c)
σmin
p
hold with probability exceeding 1 − O(n−10 ). In addition, on the same high-probability event, one has
1 λ√
σmin ;
n5 p
r
1 σ
n
1
≤ 5
σmax ≤ 5 σmax ;
n σmin p
n

k∇f (X, Y )kF ≤
X >X − Y >Y
max



Z cvx − XY >

, Z cvx,r − XY >
F

F

F

.

κ2 λ
.
n5 p

(A.10)
(A.11)
(A.12)

In words, the first claim ensures that (X, Y ) is an approximate stationary point of f (·, ·); the second
bound tells us that (X, Y ) is nearly balanced, in the sense that X > X ≈ Y > Y ; the last one formalizes
the proximity between the convex solution and the nonconvex one; see also (3.5).
6 Technically,

the statements in [CCF+ 19, Lemma 5] are for η  1/(nκ3 σmax ) and t0  n18 . Nevertheless, inspecting their
proofs reveals that the claims continue to hold for our choices η  1/(n6 κ3 σmax ) and t0  n23 .

26

2. We move on to the properties of the de-shrunken estimator (X d , Y d ), which is defined in (3.8). Specifically, we can show that (see Appendix I)
r
σ
n
kX ? k ,
(A.13a)
F dH − F ? .
σmin p
r
σ
n
F dH d − F ? . κ
kX ? k ,
(A.13b)
σmin p
r
σ
n
kX ? kF ,
(A.13c)
F dH d − F ? F .
σmin p
s
σ
n log n
d
d
?
F H − F 2,∞ . κ
kF ? k2,∞ ,
(A.13d)
σmin
p
r
κ σ
n
d>
d
d> d
. 5
σmax
(A.13e)
X X −Y Y
n σmin p
hold with probability at least 1 − O(n−10 ).
3. As has been shown in [CCF+ 19], the leave-one-out auxiliary point (X (j) , Y (j) ) satisfies
s
σ
n log n
F (j) R(j) − F H F .
kF ? k2,∞ ,
σmin
p
s
σ
n log n
(j)
(j)
F H − FH F . κ
kF ? k2,∞ ,
σmin
p
r
σ
n
(j)
(j)
?
F H −F .
kX ? k ,
σmin p
s
σ
n log n
kF ? k2,∞
F (j) R(j) − F ? 2,∞ . κ
σmin
p

(A.14a)

(A.14b)
(A.14c)
(A.14d)

with probability exceeding 1 − O(n−10 ).
4. Parallel to the transition from (X, Y ) to (X d , Y d ), we set


−1 1/2
−1 1/2
λ
λ
X d,(j) , X (j) Ir +
X (j)> X (j)
and Y d,(j) , Y (j) Ir +
Y (j)> Y (j)
p
p

(A.15)

to be the de-shrunken estimators of X (j) and Y (j) , respectively. We shall demonstrate in Appendix I
that, with probability at least 1 − O(n−10 ),
r
σ
n
kX ? k ,
(A.16a)
F d,(j) H d,(j) − F ? . κ
σmin p
s
σ
n log n
d,(j)
d,(j)
?
F
H
− F 2,∞ . κ
kF ? k2,∞ ,
(A.16b)
σmin
p
s
σ
n log n
F d,(j) H d,(j) − F d H d . κ
kF ? k2,∞ .
(A.16c)
σmin
p
In addition to these four sets of claims, we have the following immediate consequence of the incoherence
condition (2.4)
p

kF ? k2,∞ = max kX ? k2,∞ , kY ? k2,∞ ≤ µrσmax /n.
(A.17)


Moreover, recall that A = (1/p) · PΩ XY > − X ? Y ?> − XY > − X ? Y ?> (cf. (5.7)). We obtain from
the proof of [CCF+ 19, Lemma 8] that
s
r
n
κ4 µ2 r2 log n
kAk . σ
·
.
(A.18)
p
np
27

Last but not least, we list a few simple but useful results: the nonconvex solution F satisfies
√
σr (F ) ≥ 0.5 σmin , kF k ≤ 2 kX ? k , kF kF ≤ 2 kX ? kF , kF k2,∞ ≤ 2 kF ? k2,∞ .

(A.19)

The same holds true if we replace F by either F d , F (j) F d,(j) or their corresponding low-rank factors. Here
j can vary from 1 to n.

B

Summary of the proposed estimators

Let Z cvx be the minimizer of the convex program (3.1), and let (X ncvx , Y ncvx ) be the solution returned by
the Algorithm 1 (with algorithmic details specified in Appendix A.1). Recall that Z cvx,r is the best rank-r
approximation of Z cvx , viz.
Z cvx,r = arg min kB − Z cvx kF .
B: rank(B)≤r

In addition, we let the matrix estimate obtained by the nonconvex algorithm be Z ncvx , X ncvx Y ncvx> .
We further denote by (X cvx , Y cvx ) the estimate of low-rank factors obtained by convex relaxation; more
specifically, we set (X cvx , Y cvx ) to be the balanced rank-r factorization of Z cvx,r obeying X cvx Y cvx> = Z cvx,r
and X cvx> X cvx = Y cvx> Y cvx . With these notations in place, our de-biased and de-shrunken estimators can
be summarized as follows.
• De-biased matrix estimators:
h
i
1
M cvx,d , Prank-r Z cvx − PΩ Z cvx − M ,
p
h
i
1
M ncvx,d , Prank-r X ncvx Y ncvx> − PΩ X ncvx Y ncvx> − M .
p
• De-shrunken estimators for low-rank factors:

−1 1/2
λ
X ncvx,d , X ncvx Ir +
X ncvx> X ncvx
,
p

−1 1/2
λ
Y ncvx> Y ncvx
Y ncvx,d , Y ncvx Ir +
,
p

−1 1/2
λ
X cvx> X cvx
X cvx,d , X cvx Ir +
,
p

−1 1/2
λ
Y cvx> Y cvx
.
Y cvx,d , Y cvx Ir +
p

C

(B.1a)
(B.1b)

(B.2a)
(B.2b)
(B.2c)
(B.2d)

Proof of Lemma 3

Throughout this section, let U ΣV > be the rank-r SVD of the nonconvex estimate X ncvx Y ncvx> and T the
tangent space of the set of rank-r matrices at X ncvx Y ncvx> . Correspondingly, we denote by PT the projection
operator onto the tangent space T , and let PT ⊥ = I − PT , where I is the identity operator.

C.1

Proof of the inequality (5.4a)

In essence, we intend to justify that M cvx,d , M ncvx,d and X ncvx,d Y ncvx,d> are all very close to U (Σ+ λp Ir )V > .
Recall from the definition of the de-biased estimator M cvx,d (cf. (B.1a)) that
h
i
1
M cvx,d = Prank-r Z cvx − PΩ (Z cvx − M ) .
(C.1)
p
Replacing Z cvx by X ncvx Y ncvx> results in

1
1
Z cvx − PΩ (Z cvx − M ) = X ncvx Y ncvx> − PΩ X ncvx Y ncvx> − M + ∆Z ,
p
p
28

(C.2)

where we denote

 1

∆Z , Z cvx − X ncvx Y ncvx> + PΩ X ncvx Y ncvx> − Z cvx .
p

Apply the proximity bound (A.12) to obtain (recall that in (A.12), one has (X, Y ) = (X ncvx , Y ncvx ))
k∆Z kF ≤ Z cvx − X ncvx Y ncvx>
≤

F

2
Z cvx − X ncvx Y ncvx>
p

+
F

1
Z cvx − X ncvx Y ncvx>
p
κ2 λ
λ
. 5
≤
,
n pp
8p

F

(C.3)

as long as n5 p  κ2 . In addition, in view of [CCF+ 19, Claim 2], one has the decomposition

PΩ X ncvx Y ncvx> − M = −λU V > + R,

(C.4)

where R ∈ Rn×n is a residual matrix obeying
kPT (R)kF . κ √

λ
κ
p
k∇f (X, Y )kF . 5 λ ≤
σmin
n
8

and

kPT ⊥ (R)k ≤

λ
2

(C.5)

with probability exceeding 1 − O(n−10 ). Here we utilize the small-gradient condition k∇f (X, Y )kF ≤
1 λ√
n5 p σmin (cf. (A.10)). Take (C.1), (C.2) and (C.4) collectively to reach


1
λ
>
cvx,d
ncvx ncvx>
M
= Prank-r X
Y
+ U V − R + ∆Z
p
p

 
λ  >
1
= Prank-r U Σ + Ir V + ∆Z − R
p
p

h 

1 
λ  >
1 i
= Prank-r U Σ + Ir V + PT ⊥ ∆Z − R + PT ∆Z − R ,
(C.6)
p
p
p
|
{z
} |
{z
}
:=C

:=∆

where the middle line follows since U ΣV > is defined to be the SVD of X ncvx Y ncvx> .
We view ∆ as a perturbation and intend to apply Lemma 14 to control kM cvx,d − U (Σ + (λ/p)Ir )V > kF .
First, notice that the rth largest singular value obeys σr (U (Σ + λp Ir )V > ) ≥ λp , and that

1
5λ
1 
,
PT ⊥ ∆Z − R ≤ k∆Z kF + kPT ⊥ (R)kF ≤
p
p
8p

(C.7)

where the last inequality results from (C.3) and (C.5). Combining the above two bounds with the fact
that U (Σ + λp Ir )V > and PT (∆Z − p1 R) are orthogonal to each other, we arrive at the conclusion that
U (Σ + λp Ir )V > is the top-r SVD of C and
 

λ 
σi (C) = σi U Σ + Ir V > ,
p

1 
σr+1 (C) = PT ⊥ ∆Z − R .
p

for 1 ≤ i ≤ r;

(C.8a)
(C.8b)

Second, let Û Σ̂V̂ > be the top-r SVD of C + ∆. By definition, one has Û Σ̂V̂ > = M cvx,d . We are left
with checking the two conditions in Lemma 14. To begin with, the perturbation term ∆ obeys
k∆kF ≤ k∆Z kF +

(i) κ2 λ
1
κ λ (ii) 1 λ
kPT (R)kF . 5
+ 5 ≤
,
p
n pp
n p
2n4 p

(C.9)

where (i) comes from (C.3) and (C.5) and the last inequality (ii) arises since np  κ2 . Clearly, the size of
the perturbation is much smaller than λ/p and hence kCk (cf. (C.8a)). In addition,

1 
σr+1 (C + ∆) ≤ σr+1 (C) + k∆k = PT ⊥ ∆Z − R + k∆kF
p
29

≤

1 λ
3λ
5λ
+ 4 ≤
,
8p
2n p
4p

where the equality depends on (C.8b) and the last line results from (C.7) and (C.9). Consequently,
 

λ  >
3λ
λ
σmin
σr (C) − σr+1 (C + ∆) ≥ σr U Σ + Ir V
−
≥ σr (Σ) +
≥
.
p
4p
4p
2
Here the first relation arises from (C.8a) and the second holds since σr (Σ) ≥ σmin /2, a simple consequence
of (A.19). We are now ready to apply Lemma 14 to obtain



12 kΣ + (λ/p)Ir k
λ 
M cvx,d − U Σ + Ir V > ≤
+ 1 k∆kF . κ k∆kF ,
p
σmin /2
F
where we have used the fact that kΣ + (λ/p)Ir k . σmax , which also can be derived from (A.19). The above
bound combined with (C.9) yields

λ 
M cvx,d − U Σ + Ir V >
p

F

.

κ3 λ κ2 λ
1 λ
+ 5 ≤ 4
n5 p p
n p
2n p

as long as np  κ3 . We remark that by setting ∆Z = 0, one also obtains the bound on M ncvx,d , i.e.

λ 
1 λ
M ncvx,d − U Σ + Ir V > ≤ 4 .
(C.10)
p
2n p
F
We move on to investigating kX ncvx,d Y ncvx,d> − U (Σ + λp Ir )V > k, for which we have the following claim.
Claim 1. One has


λ 
1 λ
X ncvx,d Y ncvx,d> − U Σ + Ir V > ≤ 4 .
(C.11)
p
2n p
√
Taking the above three bounds collectively and recognizing that λ . σ np yield the advertised bound (5.4a).

Proof of Claim 1. Utilize [CCF+ 19, Claim 3] to see that
X ncvx = U Σ1/2 Q

and

Y ncvx = V Σ1/2 Q−>

(C.12)

hold for some invertible matrix Q ∈ Rr×r with SVD UQ ΣQ VQ> obeying
ΣQ − Σ−1
Q

F

√
p
∇f (X ncvx , Y ncvx )
≤8 κ √
λ σmin

F

≤

√
8 κ
.
n5

(C.13)

The last inequality is the small-gradient condition (see (A.10), in which (X, Y ) = (X ncvx , Y ncvx )). Employ
the definitions for X ncvx,d and Y ncvx,d (cf. (B.2a) and (B.2b)) to see that

−1 1/2 
−1 1/2 ncvx>
λ
λ
X ncvx,d Y ncvx,d> = X ncvx Ir +
X ncvx> X ncvx
Ir +
Y ncvx> Y ncvx
Y
p
p

−1 1/2 
−1 1/2 ncvx>
λ
λ
= X ncvx Ir +
X ncvx> X ncvx
Ir +
X ncvx> X ncvx
Y
p
p

−1 
λ
− X ncvx Ir +
X ncvx> X ncvx
∆balancing Y ncvx>
p


−1  ncvx>
−1 
λ
λ
= X ncvx Ir +
X ncvx> X ncvx
Y
− X ncvx Ir +
X ncvx> X ncvx
∆balancing Y ncvx> .
p
p
|
{z
} |
{z
}
:=A1

:=A2

(C.14)
Here we denote

−1 1/2 
−1 1/2
λ
λ
∆balancing , Ir +
X ncvx> X ncvx
− Ir +
Y ncvx> Y ncvx
.
p
p
30

It then boils down to showing that (i) A1 is very close to U (Σ + λp Ir )V > , and (ii) A2 is small in size.
First, recall that X ncvx Y ncvx> = U ΣV > , which combined with (C.12) gives

−1 ncvx>
λ 
λ
X ncvx X ncvx> X ncvx
Y
− UV >
A 1 − U Σ + Ir V > =
p
p
λ
=
U Σ−1/2 Q−> Q−1 Σ1/2 V > − U V >
p


√ λ
λ
Σ−1/2 Q−> Q−1 − I r Σ1/2 ≤ κ Q−> Q−1 − I r
=
p
p
√ λ
√
λ
= κ Σ−2
· Σ−1
κ Σ−1
Q − ΣQ F
Q − Ir ≤
Q
p
p
λ 1
.κ
.
p n5

(C.15)

Here, the last inequality comes from (C.13) and its immediate consequence that kΣQ k  kΣ−1
Q k  1.
Second, apply the perturbation bound for matrix square roots (cf. Lemma 13) to obtain
−1
−1
− Y ncvx> Y ncvx
X ncvx> X ncvx


k∆balancing k .
1/2 
1/2 
−1
−1
λ
λ
ncvx>
ncvx
ncvx>
ncvx
λmin Ir + p (X
X
)
Y
)
+ λmin Ir + p (Y
λ
p

(i)

λ
p
λ
≤
p
.

(ii)

.

X ncvx> X ncvx
X ncvx> X ncvx

−1

−1

− Y ncvx> Y ncvx

−1

X ncvx> X ncvx − Y ncvx> Y ncvx

F

Y ncvx> Y ncvx

−1

1 λ κ
.
n5 p σmin

Here, the inequality (i) depends on the facts that
"
1/2 #

λ
ncvx>
ncvx −1
λmin
Ir +
X
X
≥1
p

(C.16)

"
and

−1
λ
Y ncvx> Y ncvx
Ir +
p

λmin

1/2 #
≥ 1,

whereas the inequality (ii) holds because of the facts that k(X ncvx> X ncvx )−1 k . 1/σmin , k(Y ncvx> Y ncvx )−1 k .
1/σmin and the balancedness condition (A.11)
X ncvx> X ncvx − Y ncvx> Y ncvx

F

≤

1
σmax .
n5

As a result, the operator norm of A2 is bounded by

−1 
λ
kA2 k ≤ X ncvx Ir +
X ncvx> X ncvx
k∆balancing k kY ncvx k
p
√
√
1 λ κ
λ κ2
. σmax · 5
· σmax 
.
n p σmin
p n5
Take (C.14), (C.15) and (C.17) collectively to arrive at


λ 
λ 
λ κ2
1 λ
X ncvx,d Y ncvx,d> − U Σ + Ir V > ≤ A1 − U Σ + Ir V > + kA2 k .
≤ 4 ,
p
p
p n5
2n p
provided that n  κ2 .

31

(C.17)

C.2

Proof of the inequality (5.4b)

Next, we switch attention to the low-rank factors. Our goal is to demonstrate that (X cvx,d , Y cvx,d ) and
(X ncvx,d , Y ncvx,d ) are both extremely close to (U (Σ + λp Ir )1/2 , V (Σ + λp Ir )1/2 ) modulo some global rotation,
which will be established in (C.19) and (C.20) shortly.
We start by justifying the proximity between (X ncvx,d , Y ncvx,d ) and (U (Σ + λp Ir )1/2 , V (Σ + λp Ir )1/2 ). In
view of (C.12), we know that
X ncvx − U Σ1/2 UQ VQ> = U Σ1/2 UQ ΣQ VQ> − U Σ1/2 UQ VQ> ≤ Σ1/2 kΣQ − Ir k
(i)

.
(ii)

.

√

σmax

√

1
σmin

σmax

X >X − Y >Y

σ
1
σmax
5
σmin n
σmin
1

r

F

n (ii) 1 σ
≤ 4
p
n σmin

r

n √
· σmax .
p

(C.18)

>
>
+
Here, (i) depends on the fact that kΣQ − Ir k . kΣQ − Σ−1
Q kF . kX X − Y Y kF /σmin (see [CCF 19,
Lemma 20]), (ii) makes use of the balancedness assumption (A.11), whereas (iii) holds if n  κ. Denoting
X̃ , U Σ1/2 UQ VQ> , one invokes the triangle inequality to reach

1/2

λ  > −1
X̃ X̃
X ncvx,d − X̃ Ir +
p

−1 1/2
λ
Ir +
≤ X ncvx − X̃
X ncvx> X ncvx
p



 1/2
−1 1/2
λ
λ
ncvx>
ncvx −1
>
X
X
− Ir +
X̃ X̃
+ X̃
Ir +
p
p
r
n √
1 σ
· σmax .
≤ 4
n σmin p
Here the last line arises from (C.18) and the facts kIr + λp (X ncvx> X ncvx )−1 k  1, kX̃k .


Ir +

−1 1/2 
−1 1/2
1 σ
λ
λ
X ncvx> X ncvx
X̃ > X̃
. 4
− Ir +
p
p
n σmin

r

√

σmax and

n
.
p

The latter bound follows from similar derivations as in (C.16). A similar bound holds for Y ncvx,d . Recognizing
that



−1 1/2
λ
λ 1/2
>
X̃ Ir +
X̃ X̃
= U Σ + Ir
UQ VQ> ,
p
p
we have
s
min
r×r

R∈O


λ 1/2
X ncvx,d R − U Σ + Ir
p

s


X ncvx,d − U Σ +

≤
√

λ
Ir
p

1/2

2


λ 1/2
+ Y ncvx,d R − V Σ + Ir
p
F
2

UQ VQ>

+ Y ncvx,d − V Σ +
F

s


λ 1/2
X ncvx,d − U Σ + Ir
UQ VQ>
p
√
r
r σ
n √
. 4
· σmax .
n σmin p

≤



r

2

λ
Ir
p

1/2

2
F
2

UQ VQ>
F


λ 1/2
+ Y ncvx,d − V Σ + Ir
UQ VQ>
p

2

(C.19)

Next, we establish the connection between (X cvx,d , Y cvx,d ) and (U (Σ + λp Ir )1/2 , V (Σ + λp Ir )1/2 ). To
accomplish this, we first study the relationship between (X cvx , Y cvx ) and (U Σ1/2 , V Σ1/2 ). Recall that
32

X cvx and Y cvx constitute a balanced factorization of Z cvx,r , while (U Σ1/2 , V Σ1/2 ) is a balanced one of
X ncvx Y ncvx> = U ΣV > . Hence one can view Z cvx,r as a perturbation of X ncvx Y ncvx> = U ΣV > and
investigate the perturbation bounds on the balanced factorizations. Going through the same derivations as
in [MWCC17, Appendix B.7] and [CLL19, Appendix B.2.1], one reaches
q
√
κ2
cvx R − U Σ1/2 2 + Y cvx R − V Σ1/2 2 .
X
Z cvx,r − X ncvx Y ncvx> F
r
·
min
√
F
F
σmin
R∈O r×r
√
κ4
1 λ
· 5 .
. r· √
σmin n p
Here the last relation follows from the proximity of the convex estimator and the nonconvex estimator;
see (A.12). Repeating the same argument as above to translate the bound between (X cvx , Y cvx ) and
(U Σ1/2 , V Σ1/2 ) to that of (X cvx,d , Y cvx,d ) and (U (Σ + λp Ir )1/2 , V (Σ + λp Ir )1/2 ), we conclude that
s
min
r×r

R∈O


λ 1/2
X cvx,d R − U Σ + Ir
p

2


λ 1/2
+ Y cvx,d R − V Σ + Ir
p
F

2

.

√

r·

F

κ4 σ
n5 σmin

r

n √
· σmin .
p
(C.20)

This together with (C.19) and the assumption n  κ4 concludes the proof.

C.3

Proof of the inequality (5.5)

We shall focus on proving the claim for the nonconvex estimator M ncvx,d and X ncvx Y ncvx> ; the claim for the
convex estimator Z cvx can be treated similarly.
Recall from (C.10) that

λ 
1 λ
M ncvx,d − U Σ + Ir V > ≤ 4 .
p
2n p
F
It then suffices to prove that


1
λ 
X ncvx Y ncvx> − PT PΩ X ncvx Y ncvx> − M − U Σ + Ir V >
p
p

≤
F

1 λ
.
2n4 p

To see this, it has been established in Appendix C.1 that


1
1
X ncvx Y ncvx> − PT PΩ X ncvx Y ncvx> − M = U ΣV > − PT −λU V > + R
p
p
λ
1
= U ΣV > + U V > − PT (R)
p
p

λ  > 1
= U Σ + Ir V − PT (R) .
p
p
This together with the fact kPT (R)kF ≤
the proof.

D
D.1

72κ
n5 λ

(cf. (C.5)) and the assumption n  κ immediately completes

Analysis of the low-rank factors
Proof of Lemma 4

We concentrate on the factor X d ; the other factor Y d can be treated similarly. By definition of the gradient,
one has

λ
1
(D.1)
∇X f (X, Y ) = PΩ XY > − M Y + X.
p
p
Making use of the decomposition

1
1
PΩ XY > − M = XY > − X ? Y ?> + A − PΩ (E)
p
p
33

(D.2)

with A defined in (5.7), we can rearrange (D.1) as follows

λ 
1
X Y > Y + Ir = X ? Y ?> Y + PΩ (E) Y − AY + ∇X f (X, Y ) .
p
p

(D.3)

By construction, the de-shrunken estimator Y d satisfies the following property

−1 1/2

−1 1/2
λ
λ
Y d> Y d = I r +
Y >Y
Y >Y I r +
Y >Y
p
p
1/2

1


−1 1/2
1 
λ
λ
−1
= Ir +
Y >Y
Y >Y 2 Y >Y 2 I r +
Y >Y
p
p
λ
= Y >Y + I r ,
p

(D.4)

where the last identity follows since (Y > Y )1/2 and (Ir + λp (Y > Y )−1 )1/2 commute. Combining (D.3) with
the identity (D.4) gives

1
X Y d> Y d = X ? Y ?> Y + PΩ (E) Y − AY + ∇X f (X, Y ) .
p

(D.5)

Multiplying both sides of (D.5) by (Ir + λp (Y > Y )−1 )1/2 and recalling the definition of Y d in (3.8), we have
−1 1/2
λ
Y >Y
p

−1 1/2
λ
1
Y >Y
.
= X ? Y ?> Y d + PΩ (E) Y d − AY d + ∇X f (X, Y ) I r +
p
p

X Y d> Y d



Ir +

(D.6)

Since Y d> Y d and (Ir + λp (Y > Y )−1 )1/2 also commute, we have
X Y d> Y d



Ir +


−1 1/2
−1 1/2

λ
λ
Y >Y
Y >Y
= X Ir +
Y d> Y d
p
p

−1 1/2


λ
X >X
Y d> Y d − X∆balancing Y d> Y d
= X Ir +
p


= X d Y d> Y d − X∆balancing Y d> Y d ,
(D.7)

where the last relation uses the definition of X d (see (3.8)).
Substituting the identity (D.7) back into (D.6) and making a few elementary algebraic manipulations
yield the desired decomposition (5.8a).

D.2

Proof of Lemma 5
d

Recall that Y = Y d H d and similarly define
Y

d,(j)

, Y d,(j) H d,(j) .

The triangle inequality tells us that for any fixed 1 ≤ j ≤ n,
e>
j Φ1

2

h d,(j) d,(j)> d,(j) 
 i
1
−1
?
?> ? −1
−
Y
Y
Y
≤ e>
P
(E)
Y
Y
Y
Ω
j
p
|
{z
}2
:=α1

h d d> d 
i
1
d,(j)
d,(j)> d,(j) −1
−1
+ e>
P
(E)
Y
Y
Y
−
Y
Y
Y
.
Ω
j
p
2
|
{z
}
:=α2

In what follows, we shall control α1 and α2 separately.
34

1. To begin with, denoting ∆(j) , Y

d,(j)

(Y

d,(j)>

Y

d,(j) −1

1
α1 = e>
PΩ (E) ∆(j)
j
p

)

− Y ? (Y ?> Y ? )−1 results in

=

1X
(j)
Ejk δjk ∆k,·
p

n

2

k=1

.

(D.8)

2

Before proceeding, we gather a few useful facts regarding ∆(j) , as summarized in the following claim.
Claim 2. With probability at least 1 − O(n−11 ), we have
∆
∆(j)

(j)

2,∞

1
σ
.√
·
σmin σmin

s

κ3 n
,
p

1
σ
.√
·
σmin σmin

s

κ5 µr log n
.
p

With the bounds on k∆(j) k and k∆(j) k2,∞ in place, we are ready to control α1 . By construction, ∆(j) is
Pn
(j)
1
independent of e>
j PΩ (E). Therefore, the vector on the right-hand side of (D.8), p
k=1 Ejk δjk ∆k,· , is a
sum of conditionally independent random vectors. In particular, conditional on ∆(j) and {δjk }k:1≤k≤n ,
one has
n
n
 σ2 X

1X
(j)
(j)> (j)
Ejk δjk ∆k,· ∆(j) , {δjk }k:1≤k≤n ∼ N 0, 2
δjk ∆k,· ∆k,· .
(D.9)
p
p
k=1
k=1
|
{z
}
:=Σ̂

Invoke the concentration inequality for Gaussian random vectors [HKZ12, Proposition 1.1] to see that
q
q
√
√

α1 ≤ Tr Σ̂ + 2 t Σ̂ F + 2 Σ̂ t ≤ r Σ̂ + 2 rt Σ̂ + 2 Σ̂ t
q
√
√
Σ̂
.
r+ t
(D.10)
with probability at least 1 − e−t . It remains to control kΣ̂k, which we state in the following claim.
Claim 3. Suppose that n2 p  κ2 µrn log2 n. Then with probability exceeding 1 − O(n−11 ),
σ2
Σ̂ .
p

1
σ
√
σmin σmin

s

κ3 n
p

!2
.

Combine the upper bound on kΣ̂k with (D.10) and choose t  log n to arrive at
s
q
√

p
1
σ
σ
κ3 rn log n
α1 .
Σ̂
r + log n . √ √
p σmin σmin
p
with probability exceeding 1 − O(n−11 ).
2. We move on to bounding α2 , for which we have
1
d
d> d −1
d,(j)
d,(j)> d,(j) −1
kPΩ (E)k Y Y Y
−Y
Y
Y
p
r
(i)
n 1
d
d,(j)
.σ
Y −Y
p σmin
s
r
(ii)
n 1
σ
n log n
kY ? k2,∞
. σ
κ
p σmin σmin
p

α2 ≤

35

(D.11)

r
.σ

n 1
σ
√
p σmin σmin

s

κ3 µr log n
.
p

√
Here (i) uses the fact that kPΩ (E)k . σ np (see [CCF+ 19, Lemma 3]), the perturbation bounds
for pseudo-inverses (see Lemma
q 12) and (A.19); the penultimate inequality (ii) comes from the fact
d

d,(j)

n log n
σ
k . κ σmin
kY ? k2,∞ (see (A.16c)) and last one uses the incoherence condition
that kY − Y
p
p
kY ? k2,∞ ≤ µrσmax /n (cf. (A.17)).

Combine the bounds on α1 and α2 to reach
e>
j Φ1

2

s
s
r
σ
1
σ
κ3 rn log n
n 1
σ
κ3 µr log n
.√ √
+σ
√
p σmin σmin
p
p σmin σmin
p
s
σ
κ3 µrn log n
σ
.√
·
.
pσmin σmin
p

(D.12)

Taking the maximum over 1 ≤ j ≤ n establishes our bound on kΦ1 k2,∞ .
Proof of Claim 2. Apply the perturbation bound for pseudo-inverses (see Lemma 12) to obtain
n
o
−1 2
d,(j)
d,(j)> d,(j) −1 2
d,(j)
, Y
Y
Y
Y
−Y?
∆(j) . max Y ? Y ?> Y ?
s
r
σ
1
σ
1
n
κ3 n
?
κ
kX k  √
,
.
σmin σmin p
σmin σmin
p
Here we have utilized the facts that kY

d,(j)

σ
− Y ? k . κ σmin

q

n
?
p kX k

(D.13)

(see (A.16a)) and a simple consequence

of (A.19), viz.
max

n

Y ? Y ?> Y ?

−1

2

, Y

d,(j)

Y

d,(j)>

Y

Moreover, the triangle inequality tells us that
h d,(j)> d,(j) 
−1 i
d,(j)
−1
Y
Y
− Y ?> Y ?
∆(j) 2,∞ ≤ Y
≤ Y
.

d,(j)
2,∞

1
σmin

κ2

σ

d,(j) −1

?>

+

Y

2,∞

? −1

o

d,(j)

d,(j)

.

Y

− Y

1
σmin

−Y?

−Y
Y
+ Y
s
n
1
σ
κ2 n log n
kF ? k2,∞ +
kF ? k2,∞
p
σmin σmin
p

Y
r

d,(j)>

d,(j) −1 2



.

Y ?> Y ?

?
2,∞

Y

−1

?>

Y

2,∞

? −1

σmin
s
1
σ
κ5 µr log n
.√
,
σmin σmin
p

whereq
the penultimate inequality follows from the facts that kY
n
σ
κ σmin n log
kF ? k2,∞ (see (A.16b)) and that
p
Y

d,(j)>

Y

d,(j) −1

− Y ?> Y ?

−1

≤

Y

d,(j)>

Y

d,(j) −1

1

d,(j)

Y

d,(j)>

F d,(j) H d,(j) − F ?
r
1 2 σ
n
.
κ
.
σmin σmin p

.

2
σmin

k2,∞ ≤ 2kF ? k2,∞ , kY

Y

d,(j)

− Y ?> Y ?

d,(j)

Y ?> Y ?

F?

Here the penultimate inequality follows from (A.19). The proof of the claim is then complete.

36

− Y ? k2,∞ .

−1

Proof of Claim 3. Conditional on ∆(j) , using Bernstein’s inequality and the fact that ∆(j) and {δjk }k:1≤k≤n
are independent, we arrive at that with probability exceeding 1 − O(n−11 ),
Σ̂ −


σ 2 (j)> (j)
σ 2 p
V log n + B log n ,
. 2
∆
∆
p
p

where
B , max

1≤k≤n

V ,

n
X

(j)>

(j)

(j)>

(j)

(δjk − p) ∆k,· ∆k,· ≤ ∆(j)
2

(j)>

(j)

2
,
2,∞

E (δjk − p) ∆k,· ∆k,· ∆k,· ∆k,· ≤ p ∆(j)

2
2,∞

∆(j)

2

.

k=1

As a result, with probability at least 1 − O(n−11 ), we have
Σ̂ −

σ 2 (j)> (j)
σ2
. 2 ∆(j)
∆
∆
p
p
.

σ2
∆(j)
p2


p
(j)
(j)
+
∆
log
n
p
log
n
∆
2,∞
2,∞
s
s
!
p
1
κ3 n
1
κ5 µr log n
σ
σ
p log n √
+√
log n
·
·
2,∞
σmin σmin
p
σmin σmin
p

.

σ2
∆(j)
p2

2,∞

√

1
σ p 3
κ n log n,
σmin σmin

as long as np  κ2 µr log2 n. Here the middle inequality uses Claim 2. In view of the triangle inequality,
 2

σ 2 (j)> (j)
σ
1
σ p 3
(j)
+O
Σ̂ ≤
∆
∆
∆
κ
n
log
n
√
2,∞ σ
p
p2
min σmin


2
1
1
σ
σ p 3
(j) 2
(j)
∆
+
∆
.
κ n log n
√
2,∞ σ
p
p
min σmin


1
σ2
1
σ p 3
(j)
(j) 2
∆
κ n log n
.
∆
+
√
2,∞ σ
p
p
min σmin
s
!2
σ2
1
σ
κ3 n
.
,
√
p
σmin σmin
p
with the proviso that n2 p  κ2 µrn log2 n. Again, the last line makes use of Claim 2. This concludes the
proof of the claim.

D.3

Proof of Lemma 6
d

Recall that Y = Y d H d . The sub-multiplicativity of the operator norm gives that for any 1 ≤ j ≤ n,
h
i
d> d −1
d> d
d> d −1
>
?
?> d
e>
Φ
=
−
e
X
Y
Y
Y
Y
Y
Y
Y
Y
2 2
j
j
2
d > d
d> d −1
>
?
?
≤ ej X 2 Y − Y
Y
Y Y
(D.14)
r
µrσmax 1
d > d
.
Y?−Y
Y
n
σmin
r
κµr 1
d > d

Y?−Y
Y ,
(D.15)
√
n
σmin
p
?
?
where the second inequality follows from the incoherence assumption that ke>
µrσmax /n
j X k2 ≤ kX k2,∞ ≤
(cf. (A.17)) and the fact that k(Y

d>

d

Y )−1 k . 1/σmin , a simple consequence of (A.19).

37

d

d

d

It remains to control k(Y − Y ? )> Y k. To simplify notation hereafter, define ∆X , X − X ? and
d
∆Y , Y − Y ? . First, observe that
d > d
?
>
Y?−Y
Y = ∆>
(D.16)
Y Y + ∆Y ∆Y .
Second, in view of the decomposition of Y d given in (5.8b), we have
1
d
d>
d −1
d
d>
d −1
>
[PΩ (E)] X X X
− A> X X X
p
1/2

λ
+ ∇Y f (X, Y ) Ir + (X > X)−1
(X d> X d )−1 H d − Y ∆balancing H d .
p

d

d

Y = Y ? X ?> X X

d>

X

d −1

+

(D.17)

As a result, one obtains


 1
 d> d −1
 d> d −1 >
d
d>
d −1
d
d
>
?
?
?>
>
X
Y?
∆>
Y
=
Y
X
X
X
−
I
+
X
X
X
−
A
X
X X
[P
(E)]
r
Ω
Y
p

>

1/2
λ
>
−1
d>
d −1
d
d
+ ∇Y f (X, Y ) Ir + (X X)
Y?
(X X ) H − Y ∆balancing H
p
d>
d −1
d>
d>
d −1
d> 1
d>
d −1
d>
=− X X
X ∆X Σ? + X X
X
PΩ (E) Y ? − X X
X AY ?
p

>

1/2
λ
>
−1
d>
d −1
d
d
+ ∇Y f (X, Y ) Ir + (X X)
(X X ) H − Y ∆balancing H
Y?
p
d>
d −1
=− X X
X ?> ∆X Σ? + S,
(D.18)
where we have used
d

X ?> X X

d>

X

d −1

d

− Ir = X ?> X X

d>

X

d −1

−X

d>

d

X X

d>

X

d −1

d

= −∆>
XX X

d>

X

d −1

.

Here, we define S to be
S,− X

d>

X



d −1

?
∆>
X ∆X Σ + X

d>

X

d −1

X

d> 1

p

PΩ (E) Y ? − X

d>

X

d −1

1/2
λ
+ ∇Y f (X, Y ) Ir + (X > X)−1
(X d> X d )−1 H d − Y ∆balancing H d
p


X

d>

>

AY ?

Y ?.

(D.19)

?
?>
The following claim connects ∆>
∆X .
Y Y with X

Claim 4. The following identity holds true:
?
?>
∆>
∆X =
YY −X

 1 d> d> d

1
>
∆>
Y Y − X d> X d H d .
X ∆X − ∆Y ∆Y + H
2
|2
{z
}
:=∆dXY

This relation together with (D.18) yields



1
d>
d −1
d
>
?
>
?
>
>
∆Y Y = − X X
∆Y Y −
∆X ∆X − ∆Y ∆Y − ∆XY Σ? + S.
2
A little algebraic manipulation then gives
X

d>

d

?
>
? ?
X ∆>
Y Y + ∆Y Y Σ = X

d>

d

X S+
d>

 ?
1
d
?
>
∆>
X ∆X − ∆Y ∆Y Σ + ∆XY Σ .
2
d

It is easy to check from (A.19) that 0.25σmin Ir  X X , Σ?  4σmax Ir . Hence one can invoke Lemma 15
d>
d
d>
d
d
?
?
>
?
?
with X = ∆>
X and C = X X S + 0.5(∆>
YY , A = Σ , B = X
X ∆X − ∆Y ∆Y )Σ + ∆XY Σ to
obtain
 ?
1
1
d>
d
d
?
?
>
∆>
.
X X S+
∆>
YY
X ∆X − ∆Y ∆Y Σ + ∆XY Σ
σmin
2
38

1

≤

X

d> 1

PΩ (E) Y ? − X

d>

 ?
1
>
∆>
X ∆X + ∆Y ∆Y Σ
2

AY ? −

σmin
p

1/2
λ
1
d>
d
>
+
H d> Ir + (X > X)−1
[∇Y f (X, Y )] Y ? + X X H d> ∆balancing Y > Y ? + ∆dXY Σ? ,
σmin
p

where we have plugged in the definition of S (see (D.19)) and used the identity X
H d> . Combine the above inequality with (D.16) to obtain
d

Y −Y?
.

>

1

X

σmin

Y

d

+

σmin

d

X H d> (X d> X d )−1 =

?
+ ∆>
≤ ∆>
Y ∆Y
YY

d> 1

|
1

d>


1
d>
>
PΩ (E) Y ? +
X AY ? +κ ∆>
X ∆X + ∆Y ∆Y
p
σmin |
{z
}
|
{z
}
{z
}
:=α2
:=α3
:=α1


1/2
λ
d>
d
>
H d> Ir + (X > X)−1
[∇Y f (X, Y )] Y ? − X X H d> ∆balancing Y > Y ? + ∆dXY Σ? .
p
{z
}
|
:=α4

(D.20)
It then boils down to controlling the above terms α1 , α2 , α3 and α4 .
1. First, the term α4 can be upper bounded by
1/2
λ
d>
d
(X > X)−1
k∇Y f (X, Y )kF kY ? k + X X k∆balancing kkY > Y ? k + k∆dXY kkΣ? k
p
r
√
1 λ√
1 λ κ
κ σ
n
2
σmax
. 5
σmin σmax + σmax 5
+ σmax 5
n p
n p σmin
n σmin p
r
κ σ
n 2
 5
σ
.
n σmin p max

α4 ≤



Ir +

Here, the second line utilizes the facts that k(Ir +λ(X > X)−1 /p)1/2 k . 1, kX
and the results in (A.10), (A.13e) and (C.16).

d>

2. Moving on to α3 , we recall from (A.13b) that
max {k∆X k , k∆Y k} . κ
Therefore one arrives at

α3 .

κ

σ

r

σmin

n
kX ? k .
p

r 2
n
σmax .
σmin p
σ

3. Regarding the term α2 , we have
α2 . X

d

kAk kX ? k . σmax σ

r

n
·
p

s

κ4 µ2 r2 log n
,
np

where we utilize the bound in (A.18)
r
kAk . σ

n
·
p

s

κ4 µ2 r2 log n
.
np

4. Finally, for the term α1 , by the triangle inequality one has
1
1
?
α1 ≤ X ?> PΩ (E) Y ? + ∆>
.
X PΩ (E) Y
p
p
39

d

X k  kY > Y ? k . σmax

Note that
1
1
X ?> PΩ (E) Y ? ≤ X ?> PΩ (E) Y ?
p
p

F

v
uX
r
2
u r X

? > 1 P (E) Y ?
=t
X·,i
Ω
·,j ,
p
i=1 j=1

(D.21)

Observe that conditional on {δjk }1≤j,k≤n one has

? >
X·,i

1
PΩ (E) Y ?·,j =
p





> 
1
∼N
E, PΩ X ?·,i Y ?·,j
p

0, σ

2


> 
1
PΩ X ?·,i Y ?·,j
p

2

!

F

As a result, we obtain that with probability at least 1 − O(n−10 ),
?
X·,i


> 
> 1
1
PΩ (E) Y ?·,j . σ
PΩ X ?·,i Y ?·,j
p
p
s
>
log n
X ?·,i Y ?·,j
.σ
p

p

log n

F

.

(D.22)

F

Here, the second relation uses the fact that

 
1
?
? >
√ PΩ X ·,i Y ·,j
p

 X ?·,i Y ?·,j

>
F

F

with probability at least 1 − O(n−10 ) as long as n2 p  µrn log n, which follows from [MWCC17, Lemma
38] or [CR09, Section 4.2] by observing that X ?·,i (Y ?·,j )> lies in the tangent space of M ? . Take (D.21)
and (D.22) collectively to reach
v
s
s
s
uX
r X
r
> 2
log n u
log
n
log n
?
?
?> 1
?
?
?
t
≤σ
X
.σ
X ·,i Y ·,j
PΩ (E) Y
kX kF kY kF . σ
rσmax .
p
p
p
p
F
i=1 j=1
In addition, we have
1
σ
1
?
≤ k∆X k PΩ (E) kY ? k . κ
∆>
X PΩ (E) Y
p
p
σmin

r

n
kX ? k σ
p

r

n
kY ? k .
p


κσ

r 2
n
.
p

Combine these two bounds to reach
s
α1 . σ

 r 2
r2 log n
n
σmax + κσ
.
p
p

Substituting the bounds on α1 , α2 , α3 and α4 back to (D.20) results in
d

Y −Y?

>

Y

d

1
1
1
α1 +
α2 + κα3 +
α4
σmin
σmin
σmin
s
s
!
 r 2
r
1
r2 log n
n
n
κ4 µ2 r2 log n
.
σ
σmax + κσ
+ σmax σ
·
σmin
p
p
p
np

r 2
r
1 κ σ
σ
n
n 2
σmax +
σ
+κ κ
σmin p
σmin n5 σmin p max
s

r 2
r
σ
n
σ
n
κ4 µ2 r2 log n
 κσmax κ
+ σmax
·
,
σmin p
σmin p
np
.

40

which together with (D.15) yields
?
e>
j X

h

Y

?>

Y

d

Y

d>

Y

d −1

− Ir

i
2

σ
.√
pσmin

κ

σ
σmin

s

κ7 µrn
+
p

s

κ7 µ3 r3 log n
np

!
.

Taking the maximum over 1 ≤ j ≤ n leads to the desired result.
Finally, we are left with proving Claim 4.
Proof of Claim 4. First, by X ?> X ? = Y ?> Y ? , one can obtain
>

?
?>
∆>
∆X = Y d H d − Y ? Y ? − X ?> X d H d − X ?
YY −X
>

= Y d H d Y ? − X ?> X d H d
>


Y ? − Y d H d + H d> Y d> Y d H − X ?> X d H d
= Y dH d
>


d
d
= − Y d H d ∆Y + ∆>
+ H d> Y d> Y d − X d> X d H d .
X X H
We can further decompose it as

d>
?
?>
?
>
?>
∆>
∆X = ∆>
∆Y − ∆>
Y d> Y d − X d> X d H d .
YY −X
X X + ∆X ∆X − Y
Y ∆Y + H

(D.23)

Second, since H d is the best rotation matrix to align (X d , Y d ) and (X ? , Y ? ), we know from [MWCC17,
Lemma 35] that
>
>
X d H d X ? + Y d H d Y ?  0,
which implies
X dH d − X ?

>

X ? + Y dH d − Y ?

>

?
>
?
Y ? = ∆>
X X + ∆Y Y

is a symmetric matrix, i.e.
?
>
?
?>
∆>
∆X + Y ?> ∆Y .
X X + ∆Y Y = X

This is equivalent to
?
?>
?
∆>
∆X = Y ?> ∆Y − ∆>
YY −X
XX .

(D.24)

Combine (D.23) and (D.24) to arrive at

d>
?
>
?>
?
∆>
∆Y − ∆>
Y d> Y d − X d> X d H d = Y ?> ∆Y − ∆>
X X + ∆X ∆X − Y
Y ∆Y + H
XX ,
which results in
?
?>
?
∆>
∆X = Y ?> ∆Y − ∆>
YY −X
XX =


 1 d>  d> d
1
>
∆>
Y Y − X d> X d H d .
X ∆X − ∆Y ∆Y + H
2
2

This completes the proof of the claim.

D.4

Proof of Lemma 7

Recall that
A=



1
PΩ XY > − M ? − XY > − M ?
p

and

Φ3 = −AY

d

Y

d>

Y

d

with Y = Y d H d . For any 1 ≤ j ≤ n, we have
e>
j AY

d

Y

d>

Y

d −1
2

≤ e>
j AY

d
2

Y

d>

Y

d −1

(i)

d> d −1
d
= e>
Y Y
j AY
2

−1 1/2
λ
(ii)
>
= e>
AY
I
+
Y
Y
r
j
p

41

Y
2

d>

Y

d −1

d −1

≤ e>
j AY
(iii)

.


2

Ir +

1
e> AY
σmin j

−1 1/2
λ
Y >Y
p

(iv)

=

2

1
e> AY H
σmin j

Y

2

d>

Y

d −1

.

Here (i) and (iv) rely on the unitary invariance of the operator norm, (ii) uses the definition of Y d (see (3.8))
√
and (iii) follows from the choice λ . σ np and immediate consequences of (A.19)


Ir +

−1 1/2
λ
1
Y >Y
p

and

Y

d>

Y

d −1

.

1
.
σmin

Therefore, it suffices to control ke>
j AY Hk2 . To this end, we have the following decomposition




>
> 1
>
?
>
?
PΩ XY − M − XY − M
ej AY H = ej
YH
p


 

(j) (j)>
?
(j) (j)>
?
> 1
Y (j) H (j) + ∆2 ,
−M
PΩ X Y
−M − X Y
= ej
p

(D.25)

where we define





1
>
?
>
?
∆2 ,
PΩ XY − M − XY − M
YH
p




(j) (j)>
?
(j) (j)>
?
> 1
PΩ X Y
−M − X Y
−M
Y (j) H (j) .
− ej
p
e>
j

Denoting

(j) (j)>
v = [v1, · · · , vn ] , e>
Y
− M? ,
j X
we can rewrite the first term of (D.25) as
e>
j




n
h
i


1X
1
PΩ X (j) Y (j)> − M ? − X (j) Y (j)> − M ? Y (j) H (j) =
(δjk − p) vk Y (j) H (j)
.
p
p
k,·
k=1

Since (X (j) , Y (j) ) is independent of {δjk }1≤k≤n , the right hand side of the above equation can be viewed as
a sum of independent random vectors, conditional on (X (j) , Y (j) ). Invoke Bernstein’s inequality to see that
n



1X
(δjk − p) vk Y (j) H (j) k,·
p
k=1

.
2


1 p
V log n + B log n
p

holds with probability at least 1 − O(n−10 ). Here, we denote
h
i 
Xn
 
>
2
2
V ,
E (δjk − p) vk2 Y (j) H (j) k,· Y (j) H (j) k,· ≤ p kvk∞ Y (j)
k=1


B , max (δjk − p) vk Y (j) H (j) k,· ≤ kvk∞ Y (j) 2,∞ .
1≤k≤n

2
,
F

2

As a result, we obtain
n

h
i
1X
(δjk − p) vk Y (j) H (j)
p
k,·
k=1


1 p
p log n kvk∞ Y (j) F + kvk∞ Y (j) 2,∞ log n
p
r


kvk∞ p
µr
2
.
prσmax log n +
σmax log n
p
n
s
r log n
 kvk∞
σmax
p
.

2

42

√
(j)
(j)
with
p the proviso that np  µ log n. Here the middle line depends on kY kF . rσmax and kY k2,∞ .
µrσmax /n. Additionally,
>



≤ X (j) R(j) − X ? R(j)> Y (j)> + X ? Y (j) R(j) − Y ?
kvk∞ ≤ X (j) Y (j)> − M ?
∞

≤ X
.κ

(j)

σ
σmin

(j)

R
s

−X

?

∞

Y

?

(j)
2,∞

2,∞

n log n µr
σmax . κ2 σ
p
n

s

+ kX k2,∞ Y

(j)

R

(j)

−Y

?
2,∞

µ2 r2 log n
.
np

p
Here the penultimate inequality uses (A.14d) and the bound kY (j) k2,∞ . µrσmax /n. We arrive at the
conclusion that: with probability exceeding 1 − O(n−10 ),
s
r
n

1X
σ
κ4 µ2 r3 log2 n
max
(δjk − p) vk X (j) H (j) k,· . σ
·
.
p
p
np
k=1

2

Next, we move on to the second term ∆2 of (D.25), which can be further decomposed as follows




 
1
>
?
>
?
P
XY
−
M
−
XY
−
M
Y H − Y (j) H (j)
∆2 = e>
Ω
j
p
|
{z
}
:=θ1



 

1
>
(j) (j)>
>
(j) (j)>
+ e>
P
XY
−
X
Y
−
XY
−
X
Y
Y (j) H (j) .
Ω
j
p
{z
}
|


:=θ2

In what follows, we bound θ1 and θ2 sequentially.
1. Regarding θ1 , using the definition of A we obtain
s
s
n
κ4 µ2 r2 log n
σ
µr log n √
σmax
·
·κ
kθ1 k2 ≤ kAk Y H − Y H
.σ
p
np
σmin
p
F
s
s
r
σmax
κ4 µ2 r2 log n
σ
κ2 µrn log n
σ
·
·
,
p
np
σmin
p
q q 4 2 2
r log n
(cf. (A.18)).
where the second relation holds due to (A.14b) and the fact that kAk . σ np κ µ np
(j)

(j)

r

2. Moving on to θ2 , we can utilize the identity

>
>
XY > − X (j) Y (j)> = XH − X (j) H (j) Y (j) H (j) + XH Y H − Y (j) H (j)
to deduce that


h

 (j) (j) > i 

1
(j)
(j)
(j)
(j)
(j)
(j) >
kθ2 k2 ≤ e>
P
XH
−
X
H
Y
H
−
XH
−
X
H
Y
H
Y (j) H (j)
Ω
j
p



> 

> 
1
(j)
(j)
(j)
(j)
+ e>
P
XH
Y
H
−
Y
H
−
XH
Y
H
−
Y
H
Y (j) H (j)
Ω
j
p
2
XH − X (j) H (j)

=
|

n
 1X
>

(δjk − p) Y (j) H (j) k,· Y (j) H (j) k,·
j,· p
k=1
{z
}2
:=α1

n

> 

 1X
+ XH j,·
(δjk − p) Y H − Y (j) H (j)
Y (j) H (j)
.
p
k,·
k,·
k=1
2
|
{z
}
:=α2

43

2

With regards to α1 , we have by Bernstein’s inequality and (A.14b) that
n

α1 ≤ XH − X (j) H (j)

.κ

s

σ
σmin

n log n
p

r

F


> 

1X
Y (j) H (j)
(δjk − p) Y (j) H (j)
p
k,·
k,·
k=1


µr
1 p
V2 log n + B2 log n
σmax ·
n
p

holds with probability exceeding 1 − O(n−10 ). Here, we define
V2 ,

n
X

2

E (δjk − p)

Y (j) H (j)

>
k,·

Y (j) H (j)

k,·

Y (j) H (j)

k,·

≤ Y (j)



>
k,·

Y (j) H (j)


k,·

k=1

≤ p Y (j)
B2 , max

1≤k≤n

2
2,∞

Y (j)> Y (j) ,

(δjk − p) Y (j) H (j)

>
k,·

Y (j) H (j)



As a result, we can obtain
s

µr log n
1 p
σ
α1 . κ
pσmax log n Y (j)
σmax ·
σmin
p
p
s
r
σmax
κ4 µ2 r2 log2 n
.σ
·
,
p
np

2
.
2,∞

+ Y
2,∞

(j)



2

log n
2,∞

p
√
provided that np  µr log n. Here we apply the bounds kY (j) k . σmax and kY (j) k2,∞ . µrσmax /n
(see (A.19) and the following remarks). In the end, we turn to the term α2 , which obeys
n
X


1
|δjk − p| Y H − Y (j) H (j) k·
Y (j) H (j) k· 2
kXk2,∞
p
2
k=1
v
v
u n
u n
r
uX
uX
 2

1 µr √
2
≤
σmax · t
(δjk − p) · t
Y H − Y (j) H (j) k· 2 Y (j) H (j) k·
p n
k=1
k=1
r
1 µr √
√
.
σmax · np · Y H − Y (j) H (j)
Y (j) 2,∞
p n
F
s
s
r
r
µr √
σ
n log n µr
σmax
κ4 µ3 r3 log n
σmax · κ
σmax  σ
·
,
.
p
σmin
p
n
p
np

α2 ≤

2
2

where the second line arises from the Cauchy-Schwarz inequality.
Take the previous bounds collectively to arrive at
s

s
s
s
r
σmax  κ4 µ2 r2 log n
σ
κ2 µrn log n
κ4 µ2 r2 log2 n
κ4 µ3 r3 log n 
k∆2 k2 . σ
·
+
+

p 
np
σmin
p
np
np
s
r
σmax
κ4 µ3 r3 log2 n
.σ
·
p
np
q
κ2 n log n
σ
as long as σmin
 1. Finally, we conclude that
p
e>
j AY

d

Y

d>

Y

d −1
2

.

1
e> AY H
σmin j

2

44




s
s
r
4 µ2 r 3 log2 n
4 µ3 r 3 log2 n
σ
κ
σ
κ
max
max
σ

.
+σ
σmin
p
np
p
np
s
σ
κ5 µ3 r3 log2 n
√
·
,
pσmin
np
1

r

(D.26)

thus concluding the proof.

D.5

Proof of Lemma 8

First, it is straightforward to verify that

−1 1/2
−1 d
λ
∇X f (X, Y ) Ir +
Y >Y
Y d> Y d
H
p
2,∞

1/2
−1

λ
−1
Y d> Y d
≤ k∇X f (X, Y )kF Ir +
Y >Y
p
1 λ√
1
σ
1
. 5
σmin ·
.√
·
,
n p
σmin
pσmin n4
√
where the last line arises from (A.10), the choice λ . σ np (cf. (A.6)), and the bounds


Ir +

−1 1/2
λ
Y >Y
1
p

and

Y d> Y d

−1

.

1
σmin

(D.27)

.

Here the latter two are immediate consequences of (A.19). Second, with regards to the term involving
∆balancing , we have
X∆balancing H d

2,∞

≤ kXk2,∞ k∆balancing k
r

−1 1/2 
−1 1/2
µr
λ
λ
.
σmax Ir +
X >X
− Ir +
Y >Y
n
p
p
r
r
3
1 λ κ
σ
µr
κ µr
σmax · 5
√
,
.
n
n p σmin
pσmin
n10

(D.28)

where the middle line uses (A.19) and the last one follows from (C.16).
Combine (D.27), (D.28) and the triangle inequality to establish the advertised result, with the proviso
that n2  κ3 µr.

D.6

Proof of Lemma 9

We invoke the identity Y ? (Y ?> Y ? )−1 = V ? (Σ? )−1/2 (since Y ? = V ? (Σ? )1/2 ) to see that for any 1 ≤ i ≤ n,

>

>
n
X

1
1
1
−1/2
−1/2
? >
PΩ (E) Y ? (Y ?> Y ? )−1
ei =
PΩ (E) V ? (Σ? )
ei =
Eik δik (Σ? )
Vk,·
(D.29)
p
p
p
k=1

consists of a sum of independent random vectors, where we recall that δik = 1{(i, k) ∈ Ω}. In addition, the
right-hand side of the above formula is conditionally Gaussian, namely,
!
n
n
X


1
σ2 X
? −1/2
? >
? > ?
? −1/2
? −1/2
Eik δik (Σ )
Vk,·
{δik }k:1≤k≤n ∼ N 0, 2
δik (Σ )
Vk,· Vk,· (Σ )
.
p
p
k=1
k=1
|
{z
}
:=S

?

Note that S depends on the index i through {δik }k:1≤k≤n . Denote by S the expectation of S, that is,
 
−1
S ? , E S = p−1 σ 2 (Σ? )  σ 2 /(pσmax ) · Ir ,
45

and introduce the following event
(
E,

σ2
kS − S ? k .
pσmin

s

µr log n
np

)
.

Clearly, when np  κ2 µr log n, one has S  0 on the event E and hence S −1/2 is well-defined. As a result,
on the event E, we have
(S ? )

1/2

S −1/2

n
X
1
k=1

p

−1/2

Eik δik (Σ? )

?
Vk,·

>

{δik }k:1≤k≤n ∼ N (0, S ? ) .

In view of this relation, we can define the ith row of ZX ∈ Rn×r to be
(
1/2
1 >
e PΩ (E) Y ? (Y ?> Y ? )−1 S −1/2 (S ? ) , on the event E,
>
ei ZX , p>i
e i GX ,
on the event E c ,

(D.30)

(D.31)

where GX ∈ Rn×r is an independently generated random matrix satisfying


σ2
i.i.d.
>
? −1
GX ei ∼ N 0,
(Σ )
for 1 ≤ i ≤ n.
p
As can be easily seen from (D.29) and (D.30), each row of ZX follows the Gaussian distribution


σ2
i.i.d.
? −1
>
(Σ )
for 1 ≤ i ≤ n.
ZX ei ∼ N 0,
p
It remains to show that, with high probability,
∆X ,

1
1
−1/2
PΩ (E) Y ? (Y ?> Y ? )−1 − ZX = PΩ (E) V ? (Σ? )
− ZX
p
p

is small when measured by the `2,∞ norm. To this end, observe that on the event E,
1 >
1
−1/2
−1/2 −1/2
1/2
e PΩ (E) V ? (Σ? )
− e>
PΩ (E) V ? (Σ? )
S
(S ? )
p i
p i
i
h
1
1/2
?
? −1/2
= e>
Ir − S −1/2 (S ? )
,
i PΩ (E) V (Σ )
p

e>
i ∆X =

and therefore, we have
e>
i ∆X

2

1 >
−1/2
1/2
e PΩ (E) V ?
(Σ? )
Ir − S −1/2 (S ? )
p i
2
X
1
1/2
?
= √
Eik δik Vk,·
Ir − S −1/2 (S ? )
.
k
p σmin
2

≤

In what follows, we shall bound the two terms on the right-hand side of the above display sequentially.
Pn
?
1. First, observe that k=1 Eik δik Vk,·
involves a sum of independent random vectors with
?
Eik δik Vk,·

2 ψ
1

?
≤ Vk,·

2

kEjk δjk kψ1 . σ

p

µr/n,

where k · kψ1 denotes the sub-exponential norm [Ver17]. One can then apply the matrix Bernstein
inequality [Kol11, Theorem 2.7] to conclude that with probability at least 1 − O(n−20 ),
X
k

?
Eik δik Vk,·

2

.

p

V1 log n + max

1≤k≤n

46

?
Eik δik Vk,·

2 ψ
1

log2 n,

where we denote
V1 , E
As a result, we arrive at
X
k

hXn
k=1

?
Eik δik Vk,·

2

2 2
?
?
Ejk
δjk Vk,·
Vk,·

.

> i

p
σ 2 pr log n + σ

2

= σ 2 p kV ? kF = σ 2 pr.

r

p
µr
log2 n . σ pr log n
n

(D.32)

as long as np  µ log3 n.
2. Next, we move on to kIr − S −1/2 (S ? )1/2 k. Recall that on the event E, one has
s
σ2
µr log n
?
kS − S k .
.
pσmin
np
This together with the fact that σ 2 /(pσmax ) ≤ λmin (S ? ) ≤ λmax (S ? ) ≤ σ 2 /(pσmin ) gives
s
s


σ2
2σ 2
σ2
2σ 2
≤ λmin (S) ≤ λmax (S) ≤
,
≤ λmin S 1/2 ≤ λmax S 1/2 ≤
, (D.33)
2pσmax
pσmin
2pσmax
pσmin
with the proviso that np  κ2 µr log n. Therefore, straightforward calculations yield
1/2

Ir − S −1/2 (S ? )

1/2

≤ S −1/2 · S 1/2 − (S ? )
1
≤ S −1/2
S − S?

1/2 
1/2
λmin S
+ λmin (S ? )
s
s
r
pσmax
1
σ2
µr log n
κ2 µr log n
q
.
·
·

.
σ2
pσmin
np
np
σ2
pσmax

Here the second relation is the perturbation bound for the matrix square roots (see Lemma 13).
Combine the above two bounds to conclude that
h
i
1 >
?
? −1/2
−1/2
? 1/2
e>
∆
=
e
P
(E)
V
(Σ
)
I
−
S
(S
)
X 2
Ω
r
i
p i
2
s
s
1 p
σ
1
κ2 µr log n
κ2 µr2 log2 n
. · σ pr log n · √
√
.
·
·
p
σmin
np
pσmin
np
Finally, we are left with demonstrating that P(E c ) = O(n−10 ). To see this, by definition one has
kS − S ? k =

σ2
p

n


1X
−1/2
−1/2
−1
? > ?
δik (Σ? )
Vk,·
Vk,· (Σ? )
− (Σ? )
p
k=1

X

σ2
? > ?
≤ 2
δik Vk,·
Vk,· − pIr
k
p σmin

σ 2 p
V2 log n + B2 log n
. 2
p σmin
with probability at least 1 − O(n−10 ). Here the last line utilizes the matrix Bernstein inequality, where

µr
? >
?
B2 , max (δjk − p) Vk,·
Vk,·
≤
,
1≤k≤n
n
hX
i


µr
µrp
2
? > ?
? > ?
V2 , E
(δjk − p) Vk,·
Vk,· Vk,·
Vk,· ≤ p
V ?> V ? =
.
k
n
n
Consequently with probability exceeding 1 − O(n−10 ) one has
s
!
r
2
2
σ
µrp
log
n
µr
σ
µr log n
kS − S ? k . 2
+
log n 
p σmin
n
n
pσmin
np
as long as np & µr log n. This means that P(E c ) = O(n−10 ) and taking the union bounds over 1 ≤ i ≤ n
concludes the proof.
47

E
E.1

Analysis of the entries of the matrix
Proof of Lemma 10

The term Λij can be naturally split into two terms, namely
?>
? >
e>
ej + e>
i ΨX Y
i X ΨY e j

 d
>
d
?
Y − Y ? ej .
e>
i X −X

and

In what follows, we shall bound each term individually.
1. Regarding the first term, one sees from Theorem 5 that with probability exceeding 1 − O(n−10 )


s
s
2
7
7
3
3

κ µrn log n
κ µ r log n 
σ
 σ
.
+
max kΨX k2,∞ , kΨY k2,∞ . √
pσmin σmin
p
np
As a result, we obtain
?>
? >
?
e>
ej + e>
i ΨX Y
i X ΨY ej ≤ kΨX k2,∞ Yj,·

.



?
Ui,·

2

?
where the last line follows since kXi,·
k2 ≤

?
+ Vj,·

√

2

?
+ Xi,·
kΨY k2,∞
2

s
s
 σ
8 µrn log n
8 µ3 r 3 log2 n
κ
κ
σ
,
+
√ 
2
p σmin
p
np

?
?
σmax kUi,·
k2 and kYj,·
k2 ≤

√

?
σmax kVj,·
k2 .

2. Turning to the second term, we have by the Cauchy-Schwarz inequality that
e>
i

d

X −X

?



d

Y −Y


? >

d

ej ≤ X − X

.

√

σ
σmin

d

?

s

2,∞

Y −Y

κ3 µr log n
p

?
2,∞

.

κ

σ
σmin

s

n log n
kF ? k2,∞
p

!2

!2
,

where the penultimate
inequality uses (A.13d) and the last one depends on the incoherence assumption
p
that kF ? k2,∞ ≤ µrσmax /n (see (A.17)).
Take collectively the above two bounds to complete the proof.

E.2

Proof of Lemma 11

>
If ZX
ei and ZY> ej were independent, then clearly one would have


?>
? >
?
e>
ej + e>
i ZX Y
i X ZY ej ∼ N 0, vij .
>
As such, the main ingredient of the proof boils down to demonstrating that ZX
ei and ZY> ej are nearly
independent.
>
To begin with, we remind the readers of the way we construct e>
i ZX and ej ZY in Appendix D.6: there
c
c
−10
e
e
exist events E and E with P(E ∪ E ) . n
such that

1 >
1/2
e PΩ (E) Y ? (Y ?> Y ? )−1 S −1/2 (S ? )
p i
>
1
1/2
, e>
PΩ (E) X ? (X ?> X ? )−1 S̃ −1/2 (S ? )
p j

e>
i ZX ,

on the event E

e>
j ZY

on the event Ee

where the randomness of S only comes from {δik }k:1≤k≤n , and the randomness of S̃ only comes from
{δkj }k:1≤k≤n . In addition, the events E and Ee depend only on {δik }k:1≤k≤n and {δkj }k:1≤k≤n , respectively.
>
As a result, ZX
ei depends only on {δik , Eik }k:1≤k≤n and ZY> ej relies only on {δkj , Ekj }k:1≤k≤n . This tells
>
us that: the only common randomness underlying ZX
ei and ZY> ej lies in δij and Eij .
Fortunately, this weak dependency can be easily decoupled, for which we have the following claim.
48

Claim 5. Suppose that np  κ2 µr2 log2 n. One has the decomposition
>
e > ei + ∆i ,
ZX
ei = Z
X

e > ei ∼ N (0, σ 2 (Σ? )−1 /p) and is independent of {δkj , Ek j}k:1≤k≤n and hence of Z > ej . In addition,
where Z
Y
X
with probability at least 1 − O(n−10 ) one has
s
κµr log n
σ
k∆i k2 . √
.
pσmin
np
The desired result follows immediately from Claim 5, since
?>
? >
> e
?>
?>
e>
ej + e>
ej + e>
X ? ZY> ej + ∆>
ej ,
i ZX Y
i X ZY ej = ei ZX Y
i Y
{z i
}
|
?
∼ N (0,vij
)

where
?>
∆>
ej
i Y

≤

?
k∆i k2 kYj,·
k2,∞

σ
.√
pσmin

s

κµr log n √
σ
?
σmax kVj,·
k2,∞  √
np
p

s

κ2 µr log n
?
kVj,·
k2,∞ .
np

?>
? >
Similarly, repeating the same argument above, we canalso show that e>
ej + e>
i ZX Y
i X ZY ej can
?
be decomposed as a Gaussian random variable N 0, vij as well as a residual term bounded above by
√ p
?
k2,∞ with high probability. These together finish the proof.
(σ/ p) (κ2 µr log n)/(np)kUi,·

Proof of Claim 5. Instate the notation used in Appendix D.6. Recall that
(

Pn
1/2
−1/2
? >
(S ? ) S −1/2 k=1 p1 Eik δik (Σ? )
Vk,·
, on the event E,
>
ZX ei =
>
GX ei ,
on the event E c .
>
eX as follows
To remove the effect of δij , Eij on ZX
ei , we construct an auxiliary random matrix Z
(

P
1
? −1/2
? >
? 1/2 −1/2
E
δ
(Σ
)
V
, on the event E−j ,
(S
)
S
ik
ik
>
−j
k,·
k:k6
=
j
p
e ei =
Z
X
>
c
GX ei ,
on the event E−j
,
−1

where S ? = p−1 σ 2 (Σ? )
S−j

,
(


σ2 X
−1/2
−1/2
? > ?
δik (Σ? )
Vk,·
Vk,· (Σ? )
, 2
p

E−j ,

and

kS−j

k:k6=j

σ2
− S?k .
pσmin

s

µr log n
np

)
.

e > ei ∼ N (0, σ 2 (Σ? )−1 /p); more importantly Z
e > ei is independent of {δkj , Ekj }1≤k≤n
It is easily seen that Z
X
X
and hence of ZY> ej .
e > ei and Z > ei . Towards this, we first repeat the proof in
We still need to verify the closeness between Z
X
X
−10
Appendix D.6 to obtain P(E−j ) ≥ 1 − O(n ). Therefore on the high probability event E ∩ E−j , one has
e > ei − Z > ei
Z
X
X

−1/2

1/2

2

S −1/2 (Σ? )

≤ (S ? )

n
X
1
k=1

p

?
Eik δik Vk,·

>

−1/2

− S−j

−1/2

(Σ? )

k:k6=j

which together with the triangle inequality and the fact kS ? k = σ 2 /(pσmin ) yields
r

pσmin e >
>
ZX ei − ZX
ei
σ2

−1/2

2

≤ S −1/2 − S−j

−1/2

(Σ? )

X 1
?
Eik δik Vk,·
p

k:k6=j

+ S −1/2

(Σ? )

X 1

? >
Eik δik Vk,·
,
p

−1/2

49

1
?
Eij δij Vj,·
p

2

2

. S

−1/2

−

−1/2
S−j

σ
√
σmin

s

r log n
+
p

r

r
√
pσmax 1 σ log n µr
.
√
σ2
σmin
p
n
−1/2

Here we have used the results in (D.32) and (D.33). We are left with bounding kS −1/2 − S−j k, for which
we have

σ 2 µr
σ2
−1/2
−1/2
? >
?
≤ 2
Vj,·
Vj,·
(Σ? )
.
kS − S−j k = 2 δij (Σ? )
p
p σmin n
Take the above bound collectively with (D.33) to yield
−1/2
S−j

r
.

pσmax
,
σ2

as long as np  κµr. As a result, we have
−1/2

≤ S −1/2

S −1/2 − S−j

1/2

S 1/2 − S−j

−1/2

S−j

pσmax
1
·
kS − S−j k
2
1/2
σ
λmin (S 1/2 ) + λmin (S−j )
r
1
σ 2 µr
κµr pσmax
pσmax
q
·
·

,
.
σ2
p2 σmin n
np
σ2
σ2
.

pσmax

where the middle line relies on the perturbation of matrix square roots; see Lemma 13. Combining all, we
arrive at
s
s
r
p
pσmin e >
κµr
log
n
κµr log n
κµr
>
· κr log n +

,
ZX ei − ZX
ei .
2
σ
np
np
np
2
with the proviso that np  κ2 µr2 log2 n. This finishes the proof.

F

Proof of Corollary 1

This section is dedicated to establishing the following result, which subsumes Corollary 1 as a special case.
Corollary 2. Suppose that the conditions (3.18) hold, and recall the notation in Corollary 1. Then one has
n
 d
√ o
?
sup P Mij
∈ Mij
± Φ−1 (1 − α/2) vij − (1 − α)
0<α<1
s
s
s
−1 r r σ
σ
κ8 µrn log n
κ8 µ3 r3 log2 n  ?
κ10 µ2 rn log2 n
?
.
+
+ Ui,· 2 + Vj,· 2
.
σmin
p
np
n σmin
p
Before entering the main proof of Corollary 2, we make a simple observation that
s
r
√
n
o
σmax  ?
σ
n log n µrσmax
d
d
?
?
max X i,· − Xi,· 2 , Y j,· − Yj,· 2 . κ
≤
Ui,·
σmin
p
n
κ2
d

2

?
+ Vj,·


2

, (F.1)

d

where we recall that X = X d H d and Y = Y d H d . Here, the first inequality arises from (A.13d) and the
?
?
second one uses the assumption on kUi,·
k2 + kVj,·
k2 (i.e. (3.18b)). A simple consequence of (F.1) is that
n
o


√
d
d
?
?
max X i,· 2 , Y j,· 2 ≤ 2 σmax Ui,·
+
V
(F.2)
j,· 2 .
2
Turning to the main proof, we define
∆V ,

d
?
d
Mij
− Mij
Mij
− M?
p ? ij ,
−
√
vij
vij

50

(F.3)

which in conjunction with Theorem 6 yields the following decomposition
?
d
d
− M?
− Mij
Mij
Mij
g
∆
p ? ij + ∆V = pij? + p ij? + ∆V .
=
√
vij
vij
vij
vij

With this decomposition at hand, we have that for any ε > 0,
!
!
d
?
Mij
− Mij
∆ij
gij
P
≤ t − Φ (t) = P p ? + p ? + ∆V ≤ t − Φ (t)
√
vij
vij
vij
!
!
|∆ij |
gij
≤ P p ? ≤ t + ε + P p ? + |∆V | ≥ ε − Φ (t)
vij
vij

q
q 
(i)
? ≥ ε v?
= Φ (t + ε) − Φ (t) + P |∆ij | + |∆V | vij
ij

q
q 
? ≥ ε v? ,
≤ ε + P |∆ij | + |∆V | vij
ij
?
where Φ(·) is the CDF of N (0, 1). Here, the relation (i) uses the fact that gij ∼ N (0, vij
). It then suffices
p ?
p ?
to upper bound the right-hand side ε + P(|∆ij | + |∆V | vij ≥ ε vij ). Our goal is to demonstrate that for
a particular choice of ε > 0, this quantity is well controlled. In view of Theorem 6, we know that |∆ij | is
small with high probability. We are still in need of a high probability bound on the term |∆V |, which we
obtain through the following claim.

Claim 6. With probability exceeding 1 − O(n−10 ), the term ∆V obeys
s
r

−1 σ
κ10 µ2 rn log2 n r
?
?
.
|∆V | . Ui,· 2 + Vj,· 2
σmin
p
n
With Claim 6 at hand, we are ready to take
s
s
σ
κ8 µrn log n
κ8 µ3 r3 log2 n  ?
ε
+
+ Ui,·
σmin
p
np

2

+

?
Vj,·
2

−1

σ
σmin

s

κ10 µ2 rn log2 n
p

r

r
n

and arrive at the upper bound
P

d
?
Mij
− Mij
≤t
√
vij

!
− Φ (t) ≤ ε + n−3 .

√
d
?
A similar argument yields the lower bound on P(Mij
− Mij
≤ t vij ) − Φ(t). As a result, one has
!
d
?
Mij
− Mij
P
≤ t − Φ (t) . ε + n−3
√
vij
s
s
s
r
−1 σ
κ8 µrn log n
κ8 µ3 r3 log2 n  ?
κ10 µ2 rn log2 n r
σ
?
+
+ Ui,· 2 + Vj,· 2

σmin
p
np
σmin
p
n
for any t. This immediately establishes Corollary 2.
Proof of Claim 6. Recall that
d
?
∆V = Mij
− Mij

h

−1/2

(vij )

?
− vij

−1/2 i

?
 vij
− vij
1
d
?
p ?√ p ? √ .
= Mij
− Mij
vij vij vij + vij

?
?
Suppose for the moment that |vij − vij
| ≤ cvij
for some c ≤ 1/2. Then it follows immediately that

|∆V | . c

d
Mij
− M?
p ? ij .
vij

d
?
?
Therefore if suffices to control |Mij
− Mij
| and |vij
− vij | (i.e. obtaining the quantity c).

51

d
?
• First, expand Mij
and Mij
to see
d
d >
d
d
?
?
? >
?
= X i,· Y j,· − Xi,·
Mij
− Mij
(Yj,·
) ≤ X i,· − Xi,·
s
r

σ
n log n µr
?
?
.κ
+ Vj,·
σmax Ui,·
2
σmin
p
n
q
p
?,
. κ2 µr log n vij

d

Y j,·

2

2

d

?
+ Xi,·

2

?
Y j,· − Yj,·

2


2

?
where the middle line depends on (F.1) and (F.2), and the last inequality arises since σ(kUi,·
k2 +
p ?
√
?
kVj,· k2 )/ p . vij .
?
• Now we move on to |vij
− vij |. By the definition of vij , one has
?
vij
− vij ≤

−1
−1
σ2
?
? >
d
d >
Xi,·
X ?> X ?
(Xi,·
) − Xi,·
X d> X d
(Xi,·
)
p
−1
−1
σ2
?
? >
d
d >
+
Yj,·
Y ?> Y ?
(Yj,·
) − Yj,·
Y d> Y d
(Yj,·
) .
p

Focusing on the X factor, we have — with probability at least 1 − O(n−10 ) — that
−1
−1
?
? >
d
d >
Xi,·
X ?> X ?
(Xi,·
) − Xi,·
X d> X d
(Xi,·
)


−1
d
d>
d
d
−1
?
? >
= Xi,·
X ?> X ?
(Xi,·
) − X i,· X X
(X i,· )>
−1
−1
d
d>
d −1
?
?
?
X ?> X ?
− X X
Xi,·
− X i,· 2 + Xi,·
≤ Xi,·
X ?> X ?
2
2
d
d>
d −1
d
?
− X i,· 2 X X
+ Xi,·
X i,· 2 .
d

d
d >
Here, the first relation comes from the identity Xi,·
(X d> X d )−1 (Xi,·
) = X i,· (X
d>

d>

d

d

X i,·

2

(F.4)
d

X )−1 (X i,· )> , and

d

the inequality arises from the triangle inequality. Notice that k(X X )−1 k . 1/σmin and that
−1
−1
d>
d −1
d>
d
d>
d −1
X ?> X ?
− X X
≤ X ?> X ?
X ?> X ? − X X
X X
r
n
1
1
σ
d
X − X ? kX ? k . κ2
·
. 2
,
σmin
σmin p σmin
where the last inequality follows from (A.13b). Using the bounds (F.1) and (F.2), we continue the upper
bound in (F.4) as follows
−1
−1
?
? >
d
d >
Xi,·
X ?> X ?
(Xi,·
) − Xi,·
X d> X d
(Xi,·
)
s
r
σ
n log n µrσmax
1
?
Ui,· 2 · κ
.√
σmin
σmin
p
n
r


√
√
n 1
?
2 σ
?
?
+ Ui,·
σ
·
κ
·
σ
U
+
V
max
max
i,· 2
j,· 2
2
σmin p σmin
s
r


√
1
σ
n log n µrσmax
?
?
·
· σmax Ui,·
+
+κ
V
j,· 2
2
σmin
p
n
σmin
s
r

n log n µr  ?
3 σ
?
.κ
Ui,· 2 + Vj,·
.
2
σmin
p
n
A similar bound holds for the factor Y . Therefore, with high probability we have
s
r

2
σ
σ
n log n µr  ?
?
?
3
κ
Ui,· 2 + Vj,·
vij − vij .
2
p
σmin
p
n
52

.



?
Ui,·
2

+

?
Vj,·
2

−1

3

κ

σ
σmin

s

n log n
p

r

µr ?
1 ?
Vij ≤ vij
,
n
2

?
?
where the last relation results from the condition on kUi,·
k2 + kVj,·
k2 (cf. (3.18b)).
d
?
?
Combine the bounds on |Mij
− Mij
| and |vij
− vij | to see that with probability exceeding 1 − O(n−10 ),
s
r
−1

n log n µr 2 p
?
3 σ
?
κ
· κ µr log n
|∆V | . Ui,· 2 + Vj,· 2
σmin
p
n
s
r
−1 σ

κ10 nµ2 r log2 n r
?
?
+
V
. Ui,·
.
j,· 2
2
σmin
p
n

This establishes the desired upper bound on |∆V |.

G

Proof of Theorem 3
d

As we have argued in Section 5.1, it suffices to prove the claim for M d = X d X d> = X Y
of notation, we define
d
d
ΓX , X − X ?
and
ΓY , Y − Y ? .

d>

. For simplicity

Apply the decompositions in Theorem 5 to obtain
d

Md − M? = X Y

d>

− X ? Y ?>

>
= ΓX Y ?> + X ? Γ>
Y + ΓX ΓY

= ZX Y ?> + X ? ZY> + ΨX Y ?> + X ? Ψ>
+ ΓX Γ>
Y,
{z Y
}
|

(G.1)

,Θ

where ΨX and ΨY are defined in Theorem 5. Further, expand kM d − M ? k2F to obtain
Md − M?

2
F

= ZX Y ?>

2
F

+ X ? ZY>

2
F

+ rem,

where we define the remainder term as



2
rem , 2Tr ZX Y ?> ZY X ?> + kΘkF + 2Tr ZX Y ?> Θ> + 2Tr X ? ZY> Θ> .
In what follows, we aim to demonstrate that kZX Y ? k2F + kX ? ZY> k2F , which can be shown to sharply
concentrate around its mean, is the dominant term, and the remainder term rem is much smaller in magnitude
with high probability.
• We begin with the term kZX Y ? k2F + kX ? ZY> k2F . We shall focus on bounding kZX Y ? k2F since the other
term kX ? ZY> k2F can be treated analogously. To this end, we first have the identity
p
ZX Y ?>
σ2

2
F

n

=


 X
p
p
>
>
Tr ZX Y ?> Y ? ZX
= 2 Tr ZX Σ? ZX
=
2
σ
σ
i=1

√

i.i.d.

p
1/2
>
(Σ? ) ZX
ei
σ

2

,
2

>
where we use the fact that Y ?> Y ? = Σ? . Theorem 5 tells us that ZX
ei ∼ N (0, σ 2 (Σ? )−1 /p), which
further implies
√
p
i.i.d.
1/2
>
(Σ? ) ZX
ei ∼ N (0, Ir ) .
σ
Therefore, the quantity pkZX Y ? k2F /σ 2 follows the chi-squared distribution with nr degrees of freedom.
Standard concentration inequalities [Wai19, Equation (2.19)] reveals that with probability at least 1 −
O(n−10 ),
p
p
2
ZX Y ?> F − nr . nr log n.
2
σ

53

Repeating the above argument for kX ? ZY> k2F , we conclude that with probability at least 1 − O(n−10 ),
 2

σ 2 nr
σ p
σ 2 nr
2
2
ZX Y ?> F + X ? ZY> F = 2
+O
nr log n = (2 + o(1))
.
p
p
p
• Now we turn to the term rem, for which we have the following two claims.
Claim 7. With probability at least 1 − O(n−10 ), one has
2
kΘkF

+ 2Tr ZX Y

?>

Θ

>



+ 2Tr X

?

ZY> Θ>





=o

σ 2 nr
p


.

Claim 8. With probability exceeding 1 − O(n−10 ), we have
Tr ZX Y ?> ZY X ?>




=o

σ 2 nr
p


.

Combine all of the above bounds to yield the desired result.
Proof of Claim 7. Use triangle inequality and the bound |Tr(AB)| ≤ kAkF kBkF to obtain


2
2
kΘkF + 2Tr ZX Y ?> Θ> + 2Tr X ? ZY> Θ> ≤ kΘkF + 2 ZX Y ?>

kΘkF + 2 X ? ZY> F kΘkF

+ 2 X ? ZY> F kΘkF . (G.2)
F

F

= kΘkF + 2 ZX Y ?>

Plug in the definition of Θ (cf. (G.1)) and invoke the triangle inequality again to see that
>
kΘkF ≤ ΨX Y ?> F + X ? Ψ>
Y F + ΓX ΓY F
√
√
≤ kΨX kF σmax + σmax kΨY kF + kΓX kF kΓY kF
√
≤ nσmax (kΨX k2,∞ + kΨY k2,∞ ) + kΓX kF kΓY kF .

p
Combine Theorem 5 and the fact max{kΓX kF , kΓY kF } . (σ/σmin ) n/pkX ? kF (see (A.13c)) to conclude
that with probability at least 1 − O(n−3 )


s
s
2

r
2
7
7
3
3
√
σ
κ µrn log n
κ µ r log n 
n√
σ
 σ
rσmax
kΘkF . nσmax √
+
+
pσmin σmin
p
np
σmin p
 r 
nr
=o σ
.
p
Here the last relation depends on the assumption (3.18a). Second, we have already established in this section
that
p
kZX Y ? kF + kX ? ZY> kF = O(σ nr/p)
with probability exceeding 1 − O(n−10 ). Substitute the above two facts into (G.2) to arrive at
 2 
r


nr
σ nr
2
?> >
? > >
kΘkF + 2Tr ZX Y Θ + 2Tr X ZY Θ
.σ
kΘkF = o
.
p
p
This concludes the proof.
Proof of Claim 8. According to Lemma 9, one can write
ZX =

−1
1
− ∆X ,
PΩ (E) Y ? Y ?> Y ?
p
|
{z
}

ZY =

,ZX,E

−1
1
>
− ∆Y ,
[PΩ (E)] X ? X ?> X ?
p
|
{z
}
,ZY ,E

54


where max k∆X k2,∞ , k∆Y k2,∞ .

max {k∆X kF , k∆Y kF } .

√ σ
pσmin

√

q

κ2 µr 2 log2 n
np

and hence

σ
n max {k∆X k2,∞ , k∆Y k2,∞ } . √
pσmin

s

κ2 µr2 log2 n
.
p

(G.3)

Consequently, use the triangle inequality and Cauchy-Schwarz to verify that


Tr ZX Y ?> ZY X ?> − Tr ZX,E Y ?> ZY ,E X ?>



≤ Tr ∆X Y ?> ZY X ?> + Tr ZX Y ?> ∆Y X ?> + Tr ∆X Y ?> ∆Y X ?>
≤ k∆X kF kY ? kkZY X ?> kF + k∆Y kF kX ? kkZX Y ?> kF + kX ? kkY ? kk∆X kF k∆Y kF
(i)

≤ k∆X kF kY ? kkZY Σ?1/2 kF + k∆Y kF kX ? kkZX Σ?1/2 kF + kX ? kkY ? kk∆X kF k∆Y kF
s
s
(ii) σ
κ3 µr2 log2 n
κ3 µr2 log2 n
σ
σ 2 κ3 µr2 log2 n
kZY Σ?1/2 kF + √
kZY Σ?1/2 kF +
·
,
≤ √
p
p
p
p
p
p

(G.4)
(G.5)

where (i) follows since X ? = U ? Σ?1/2 and kU ? k = 1, and (ii) makes use of (G.3) as well as the facts
√
kY ? k, kX ? k = σmax . In addition, invoke Lemma 9 to see that ZX Σ?1/2 and ZY Σ?1/2 are both Gaussian
matrices with i.i.d. N (0, σ 2 /p) entries, which together with standard concentration results implies that
p
p
kZX Σ?1/2 kF = (1 + o(1))σ nr/p;
kZY Σ?1/2 kF = (1 + o(1))σ nr/p.
Substituting it into (G.5) gives
Tr ZX Y

?>

ZY X

?>



− Tr ZX,E Y

?>

ZY ,E X

?>



σ2
.
p

s

κ3 µnr3 log2 n σ 2 κ3 µr2 log2 n
+
o
p
p
p



σ 2 nr
p


,

with the proviso that np & κ3 µr log3 n. This means that, with high probability,



Tr ZX Y ?> ZY X ?> = Tr ZX,E Y ?> ZY ,E X ?> + o σ 2 nr/p .
(G.6)

Everything then boils down to controlling Tr ZX,E Y ?> ZY ,E X ?> . Towards this end, we first note that
ZX,E Y ?> = p−1 PΩ (E) Y ? Y ?> Y ?

−1

Y ?> = p−1 PΩ (E) V ? V ?> .

Similarly, ZY ,E X ?> = [PΩ (E)]> U ? U ?> /p. These identities allow us to derive


1 
>
Tr ZX,E Y ?> ZY ,E X ?> = 2 Tr PΩ (E) V ? V ?> [PΩ (E)] U ? U ?>
p

1 
>
= 2 Tr U ?> PΩ (E) V ? V ?> [PΩ (E)] U ?
p
2
1
= U ?> PΩ (E) V ? .
p
F
Apply the same arguments in controlling (D.21) to obtain that with probability at least 1 − O(n−10 ),
 2 
2
σ 2 r2 log n
σ nr
? 2
?> 1
?
2 log n
? 2
PΩ (E) V
kU kF kV kF 
=o
,
U
.σ
p
p
p
p
F
as long as n & r log2 n. This combined with (G.6) yields the desired claim.

55

H
H.1

Proof of lower bounds
Proof of Lemma 1

?
Fix any ε > 0. It suffices to prove that the matrix CRLB(Xi,·
| Ω) defined in (3.27) satisfies

ε
p
−1
?
≤
CRLB(Xi,·
| Ω) − (Σ? )
2
σ
σmax

(H.1)

with probability at least 1 − O(n−10 ), provided that np ≥ C0 ε−2 κ4 µr. Towards this end, we first compute
?
CRLB(Xi,·

| Ω) = σ

2



X

? > ?
(Yk,·
) Yk,·
k:(i,k)∈Ω

−1

n

σ2
=
p

1X
? > ?
δik (Yk,·
) Yk,·
p
k=1
{z
}
|

!−1
,

:=A

where we recall that δik = 1{(i, k) ∈ Ω}. Next, define the following event
s


µr log n
?
A−Σ ≤C
E,
σmax ,
np
where C > 0 is some large absolute constant. On the event E, in view of the fact σmin Ir  Σ?  σmax Ir ,
one has
0.5σmin Ir  A  2σmax Ir ,
with the proviso that np ≥ 4C 2 κ2 µr log n. This further implies that
p
−1
−1
−1
?
= A−1 − (Σ? )
≤ kA − Σ? k · kA−1 k · (Σ? )
CRLB(Xi,·
| Ω) − (Σ? )
σ2
s
κ2 µr log n
2C
≤
σmin
np
on the event E. Clearly, the requirement (H.1) holds true if np ≥ C0 ε−2 κ4 µr log n with C0 = 4C 2 .
To finish up, we are left with proving that E occurs with probability at least 1 − O(n−10 ). Invoke the
matrix Bernstein inequality to show that
A − Σ? =

n

1 X
1 p
? > ?
(δik − p) (Yk,·
) Yk,· .
V log n + B log n
p
p
k=1

holds with probability at least 1 − O(n−10 ), where we define
B , max
V ,

1≤k≤n
n
X

2

? > ?
(δik − p) (Yk,·
) Yk,· ≤ kY ? k2,∞ ≤ µrσmax /n,

n
h
i
X
2
? > ?
? > ?
? > ?
? > ?
E (δik − p) (Yk,·
) Yk,· (Yk,·
) Yk,· ≤ p
(Yk,·
) Yk,· (Yk,·
) Yk,·

k=1

≤ p kY

k=1
? 2
k2,∞

Y

?>

Y

?

≤

2
µrpσmax
/n.

Here we have used the incoherence condition (A.17). Consequently, one reaches the conclusion that with
probability exceeding 1 − O(n−10 ),
! s
r
2
µrpσmax
µrσmax
µr log n
1
?
log n +
log n 
σmax
A−Σ .
p
n
n
np
as long as np  µr log n, thus concluding the proof.

56

H.2

Proof of Lemma 2

The proof strategy is similar to the one used in proving Lemma 1 (cf. Appendix H.1). Fix any ε > 0. It is
sufficient to establish the following inequality
p ?
p
?
?
CRLB(Mij
| Ω) − vij
≤ ε 2 vij
,
σ2
σ

(H.2)

?
?
where the scalar CRLB(Mij
| Ω) is defined in (3.28) and vij
is defined in Theorem 2. Expand the left-hand
side to reach
1
−1
X
p
−1
?
?
?
? >
?
? >
? > ?
Y
≤
CRLB(M
|
Ω)
−
v
(Yj,·
) − Yj,·
(Σ? ) (Yj,·
)
(Y
)
Y
j,·
ij
ij
k,·
k,·
σ2
p
k:k6=j,(i,k)∈Ω
{z
}
|
:=AY

?
+ Xi,·

1

X

? >
?
(Xk,·
) Xk,·

p
k:k6=i,(k,j)∈Ω
|
{z

2
2

−1

?
σmax A−1
Y − (Σ )

?
+ Ui,·

?
where the last line follows from the observations that kYj,·
k2 ≤
Define the following event

E2 ,

√

max



?

AY − Σ

−1

2
2

−1

?

, AX − Σ

?
σmax A−1
X − (Σ )

?
?
σmax kVj,·
k2 and kXi,·
k2 ≤

s



? >
?
(Xi,·
) − Xj,·
(Σ? )

? >
(Xj,·
)

}

:=AX

?
≤ Vj,·

−1

≤C

√

,
?
σmax kUi,·
k2 .


µr log n
σmax ,
np

where C > 0 is some large universal constant. Two observations are sufficient to derive the desired the
result (H.2). First, the event E2 happens with probability at least 1 − O(n−10 ) — an easy consequence of
the proof of Lemma 1 (cf. Appendix H.1). Second, on the event E2 , repeating the same proof of Lemma 1
(cf. Appendix H.1), one can deduce that
s


κ2 µr log n
p
2C
2
2
?
?
?
?
.
(H.3)
CRLB(M
|
Ω)
−
v
≤
U
+
V
σ
·
max
ij
ij
i,·
j,·
2
2
σ2
σmin
np
Comparing (H.2) and (H.3), one arrives at the desired result as long as np ≥ 4C 2 ε−2 κ4 µr log n.

I
I.1

Proofs in Section A
Proof of the inequalities (A.13)

We start with (A.13a). Invoke the triangle inequality to get
d

F H −F

?

d

?

d

≤ F H − F H + kF H − F k = F − F + O



σ
σmin

r


n
?
kX k ,
p

(I.1)

where the last relation depends on the unitary invariance of the operator norm and (A.9b). It then boils
down to controlling kF d − F k. Notice that



−1 1/2
−1 1/2 
−1 1/2
λ
λ
λ
d
>
>
>
F − F ≤ F Ir +
X X
−F + Y
Ir +
Y Y
− Ir +
X X
p
p
p


−1 1/2
−1 1/2 
−1 1/2
λ
λ
λ
≤ kF k Ir +
X >X
− Ir + kY k Ir +
Y >Y
− Ir +
X >X
p
p
p


r

1/2

λ
σ
n
−1
≤ kF k Ir +
X >X
− Ir + O
kX ? k ,
p
σmin p
57

√
where the last inequality uses kY k ≤ kF k ≤ 2kX ? k (cf. (A.19)), the fact that λ . σ np (see (A.6)),
the bound (C.16) and the condition n5  κ. Apply the perturbation bound for matrix square roots (see
Lemma 13) to obtain that


Ir +

−1 1/2
λ
− Ir ≤
X >X
p

λ/p
λmin (Ir ) + λmin

(i)

λ (ii) σ
.
.
pσmin
σmin



r

Ir +

−1

λ
p

(X > X)

X >X

1/2 

−1

n
.
p

Here, (i) uses the facts that k(X > X)−1 k . 1/σmin and that λmin [(Ir + λ/p(X > X)−1 )1/2 ] ≥ 1, and (ii)
√
follows from the condition that λ . σ np (see (A.6)). Combine the above two bounds with kF k ≤ 2kX ? k
(cf. (A.19)) to reach
r
n
σ
Fd − F .
kX ? k .
(I.2)
σmin p
Substitution into (I.1) gives
d

F H −F

?

.

σ

r

σmin

n
kX ? k .
p

(I.3)

Analogous arguments yield
F dH d − F ?

F

≤ F dH − F ?

F

.

r

σ
σmin

n
kX ? kF ,
p

which is the claim in (A.13c).
Moving on to (A.13b), we apply the triangle inequality and (I.3) to see that
F dH d − F ? ≤ F dH d − F dH + F dH − F ? ≤ F d

Hd − H + O



σ
σmin

r


n
kX ? k .
p

In order to control kH d − Hk, we leverage [MWCC17, Lemma 36] to get
r
1
σ
1
n
d> ?
> ?
d
?
F
F
−
F
F
.
F
−
F
kF
k
.
κ
,
(I.4)
σmin (F > F ? )
σmin
σmin p
√
where the last relation uses (I.2) and kF ? k  kX ? k  σmax . Taking these bounds collectively yields
r
σ
n
F dH d − F ? . κ
kX ? k .
σmin p
Hd − H ≤

Now we turn attention to (A.13d). Observe that
F dH d − F ?

2,∞

≤ F dH d − F dH
≤ Fd

2,∞

+ kF H − F ? k2,∞
s

 σ
n log n
kF ? k2,∞ ,
H d − H + F d − F 2,∞ + O κ
σmin
p
2,∞

+ F dH − F H

2,∞

(I.5)

where the last bound arises from (A.9c). Going through the same calculation as in bounding kF d − F k, we
arrive at
r
σ
n
F d − F 2,∞ .
kF ? k2,∞
and
F d 2,∞ ≤ 2 kF ? k2,∞
σmin p
p
as long as σ n/p  σmin . We can thus continue the upper bound in (I.5) to derive
s
σ
n log n
F d H d − F ? 2,∞ . kF ? k2,∞ H d − H + κ
kF ? k2,∞
σmin
p
58

.κ
κ

σ

r

n
σ
kF ? k2,∞ + κ
p
σmin

s

n log n
kF ? k2,∞ .
p

σmin
σ
σmin

s

n log n
kF ? k2,∞
p

Here, the second line results from (I.4).
Finally, we deal with (A.13e). From the definition of the de-shrunken estimator (3.8), we have

−1 1/2 > 
−1 1/2
λ
λ
X d> X d − Y d> Y d = Ir +
X >X
X X Ir +
X >X
p
p
1/2



−1 1/2
λ
λ
−1
− Ir +
Y >Y
Y > Y Ir +
Y >Y
.
p
p
This combined with the triangle inequality reveals that
X d> X d − Y d> Y d


−1 1/2
−1 1/2
λ
λ
X >X
X >X − Y >Y
X >X
Ir +
≤ Ir +
p
p





1/2
1/2


λ
λ
−1
−1
X >X
Y >Y
Ir +
+ Ir +
− Ir +
Y >Y
p
p


−1 1/2
−1 1/2 
λ
λ
+ Ir +
Y >Y
Y >Y
Ir +
X >X
− Ir +
p
p

−1 1/2
λ
X >X
p
−1 1/2
λ
Y >Y
.
p

Making use of (A.11) and (C.16) allows us to establish the claim.

I.2

Proof of the inequalities (A.16)

The proofs of (A.16a) and (A.16b) are the same as those of (A.13b) and (A.13d), and are hence omitted for
conciseness. We are left with (A.16c). Denoting
F0 , F ? ,

F1 , F d H

and

F2 , F d,(j) R(j) ,

one has
kF1 − F0 k kF0 k = F d H − F ? kF ? k .
as long as σ

σ
σmin

r

n
σ 2 (F0 )
σmax ≤ σmin = r
,
p
2

p
n/p  σmin /κ. Here the first inequality follows from (A.13a). In addition, we have

kF1 − F2 k kF0 k = F d H − F d,(j) R(j) kF ? k


−1 1/2
λ
λ  (j)> (j) −1 1/2 (j)
≤ F Ir +
Y >Y
H − F (j) Ir +
Y
Y
R
kF ? k + θ,
p
p


−1 1/2
λ
λ  (j)> (j)> (j) (j) −1 1/2
= F H Ir +
H >Y >Y H
− F (j) R(j) Ir +
R
Y
Y R
kF ? k + θ,
p
p
(I.6)
where θ is defined to be


−1 1/2 
−1 1/2
λ
λ
>
>
θ , X Ir +
Y Y
− Ir +
X X
kF ? k
p
p


λ  (j)> (j) −1 1/2 
λ  (j)> (j) −1 1/2
(j)
Y
Y
− Ir +
X
X
kF ? k .
+ X
Ir +
p
p

59

Regarding θ, one can apply the bound (C.16) for (X, Y ) and a similar bound for (X (j) , Y (j) ) to obtain
r
n
κ σ
θ . σmax · 5
.
n σmin p
Returning to (I.6), one has by the triangle inequality that


−1 1/2
λ  (j)> (j)> (j) (j) −1 1/2
λ
R
Y
Y R
F H Ir +
H >Y >Y H
− F (j) R(j) Ir +
p
p


−1 1/2
λ
≤ F H − F (j) R(j) Ir +
H >Y >Y H
p


−1 1/2 
λ
λ  (j)> (j)> (j) (j) −1 1/2
(j) (j)
> >
Ir +
+ F R
R
Y
Y R
H Y YH
− Ir +
p
p

−1 1/2
λ
Ir +
≤ F H − F (j) R(j)
H >Y >Y H
p
F

−1 1/2 
λ
λ  (j)> (j)> (j) (j) −1 1/2
Ir +
+ F (j) R(j)
.
− Ir +
H >Y >Y H
R
Y
Y R
p
p
Recognizing that


−1 1/2
λ
> >
λmin Ir +
H Y YH
≥1
p

λmin

and




λ  (j)> (j)> (j) (j) −1 1/2
R
Y
Y R
≥ 1,
Ir +
p

we can apply the perturbation bound for matrix square roots (see Lemma 13) to obtain
−1 1/2 
λ  (j)> (j)> (j) (j) −1 1/2
λ
H >Y >Y H
R
Y
Y R
− Ir +
p
p
−1  (j)> (j)> (j) (j) −1
λ
.
H >Y >Y H
− R
Y
Y R
p

−1
−1
λ
.
H >Y >Y H
H > Y > Y H − R(j)> Y (j)> Y (j) R(j)
R(j)> Y (j)> Y (j) R(j)
p
λ 1
λ 1 √
.
H > Y > Y H − R(j)> Y (j)> Y (j) R(j) .
σmax F H − F (j) R(j) F
2
2
p σmin
p σmin
r √
σ
n σmax
F H − F (j) R(j) F .
.
σmin p σmin



Ir +

Collect the pieces to arrive at

√
kF1 − F2 k kF0 k . σmax
F H − F (j) R(j)
√

2,∞





r

n
+κ
F H − F (j) R(j)
F
σmin p
r
κ σ
n
+ σmax · 5
F
n σmin p

σmax F H − F (j) R(j)
s
√
σ
n log n
. σmax
F?
σmin
p
.

σ

F

κ σ
+ σmax · 5
n σmin

σr2 (F0 )
,
4

p
where the penultimate relation uses (A.14a) as well as the fact that kF ? k2,∞ ≥ σmin r/n.
With the above bound in place, we are ready to invoke [CCF+ 19, Lemma 22] to obtain
F d H d − F d,(j) H d,(j) . κ F d H − F d,(j) R(j) . κ F H − F (j) R(j)
s
n log n
σ
F ? 2,∞ ,
.κ
σmin
p
where the last line comes from (A.14a). This concludes the proof.
60

F

r

n
p

J

Technical lemmas

This section collects a few useful matrix perturbation bounds. The first one is concerned with the perturbation of pseudo-inverses.
Lemma 12 (Perturbation of pseudo-inverses). Let A† (resp. B † ) be the pseudo-inverse (i.e. Moore–Penrose
inverse) of A (resp. B). Then we have

kB † − A† k ≤ 3 max kA† k2 , kB † k2 kB − Ak .
Proof. See [Ste77, Theorem 3.3].
The next lemma focuses on the perturbation bound for matrix square roots.
Lemma 13 (Perturbation of matrix square roots). Consider two symmetric matrices obeying A1  µ1 I and
A2  µ2 I for some µ1 , µ2 > 0. Let R1  0 (resp. R2  0) be the (principal) matrix square root of A1
(resp. A2 ). Then one has
1
kR1 − R2 k ≤ √
√ kA1 − A2 k .
µ1 + µ2
Proof. See [Sch92, Lemma 2.1].
The following lemma concerns the perturbation of top-r components of matrices.
Lemma 14 (Perturbation of top-r components). Consider two matrices M , M + E ∈ Rn×n . Suppose
that kEk ≤ kM k and σr (M ) > σr+1 (M + E). Let U ΣV > (resp. Û Σ̂V̂ > ) be the rank-r SVD of M
(resp. M + E). Then one has


12 kΣk
>
>
U ΣV − Û Σ̂V̂ F ≤
+ 1 kEkF .
σr (M ) − σr+1 (M + E)
Proof. From Wedin’s sin Θ theorem [Wed72], there exist orthonormal matrices R1 , R2 ∈ Or×r such that
n
o
max kÛ R1 − U kF , kV̂ R2 − V kF ≤

2
kEkF .
σr (M ) − σr+1 (M + E)

(J.1)

In addition, Weyl’s inequality tells us that
Σ − Σ̂ ≤ E

Σ̂ ≤ 2 Σ .

and hence

(J.2)

Here, the second inequality follows from the triangle inequality and the assumption that kEk ≤ kM k = kΣk.
Expand U ΣV > − Û Σ̂V̂ > and apply the triangle inequality to obtain
U ΣV > − Û Σ̂V̂ >

F

= U ΣV > − Û R1 R1> Σ̂R2 R2> V̂ > F


≤ U − Û R1 ΣV > F + Û R1 Σ − R1> Σ̂R2 V >
>
+ Û R1 R1> Σ̂R2 V − V̂ R2
,
F

F

which further implies that
U ΣV > − Û Σ̂V̂ >

F

≤ U − Û R1

F

Σ + Σ − R1> Σ̂R2

F

+ Σ̂

V − V̂ R2

6 kΣk
kEkF + Σ − R1> Σ̂R2
≤
σr (M ) − σr+1 (M + E)

F

.

F

(J.3)

Here, the last line arises from (J.1) and (J.2). It then boils down to controlling kΣ−R1> Σ̂R2 kF . Recognizing
that Σ = U > M V and Σ̂ = Û > (M + E)V̂ , we obtain
Σ − R1> Σ̂R2

F

= U > M V − R1> Û > (M + E) V̂ R2
61

F

≤

U − Û R1

>

≤ U − Û R1

MV

F

+ R1> Û > EV

kΣk + kEkF + Σ̂

F

F

+ R1> Û > (M + E) V − V̂ R2

V − V̂ R2

F


F

.

Once again, employ (J.1) and (J.2) to arrive at
Σ − R1> Σ̂R2

F

≤

6 kΣk
kEkF + kEkF .
σr (M ) − σr+1 (M + E)

(J.4)

Combining (J.3) and (J.4), we reach
U ΣV

>

− Û Σ̂V̂



>
F

≤


12 kΣk
+ 1 kEkF
σr (M ) − σr+1 (M + E)

as claimed.
The last bound centers around the well-known Sylvester equation XA + BX = C.
Lemma 15 (The Sylvester equation). Suppose X ∈ Rr×r satisfies the matrix equation XA + BX = C for
some matrices A ∈ Rr×r ,B ∈ Rr×r and C ∈ Rr×r . Then one has
kXk ≤ (2λmin )−1 kCk ,
as long as λmin Ir  A  λmax Ir and λmin Ir  B  λmax Ir for some λmax ≥ λmin > 0.
Proof. To begin with, we intend to show that under the condition λmin Ir  A, B  λmax Ir for some
λmax ≥ λmin > 0, there is a unique solution to the matrix equation XA + BX = C. Use the notation of
Kronecker product to obtain an equivalent form of XA + BX = C as follows

vec (XA + BX) = A> ⊗ Ir + Ir ⊗ B · vec (X) = vec (C) ,
where ⊗ denotes the Kronecker product and vec(A) stands for the vectorization of the matrix A. Given
that A  0 and B  0, it is straightforward to see that A> ⊗ Ir + Ir ⊗ B is invertible, thus justifying the
uniqueness of X.
The next step is to characterize X explicitly. The argument herein is adapted from [Smi68] and [Sch92].
Specifically, it has been shown in [Smi68] that the equation XA + BX = C is equivalent to
X − U XV = W ,
where U = (qIr + B)−1 (qIr − B), V = (qIr − A)(qIr + A)−1 and W = 2q(qIr + B)−1 C(qIr + A)−1 , for
any q > 0. In particular, when q > λmin , the matrix
∞
X

X=

U k−1 W V k−1

(J.5)

k=1

is the unique solution to X − U XV = W and hence to XA + BX = C. To show this, it suffices to verify
that the matrix series is convergent. Note that when q > λmin , one has
−1

kU k ≤ k (qIr + B)

k kqIr − Bk ≤

q − λmin
< 1,
q + λmin

and similarly kV k ≤ (q − λmin )/(q + λmax ) < 1. These two bounds taken together immediately establish the
convergence of the matrix series (J.5).
In the end, the explicit representation (J.5) allows us to upper bound kXk. A little algebra reveals that
kXk ≤

∞
X
k=1

U k−1 W V k−1 ≤ kW k

∞
X
k=1

62

kU k

k−1

kV k

k−1

≤

kW k
,
1 − kU k kV k

where we make use of the fact kU kkV k < 1. In addition, from the definition of W we know that
−1

kW k ≤ 2q (qIr + B)

kCk (qIr + A)

−1

≤ kCk

2q

2,

(q + λmin )

provided that q > 0. Combine this with the bounds on kU k and kV k to reach
kCk (q+λ2q )2
2q kCk
kCk
min
kXk ≤

2 =
2
2 = 2λ
min
(q + λmin ) − (q − λmin )
min
1 − q−λ
q+λmin
as claimed.

References
[AFWZ17]

Emmanuel Abbe, Jianqing Fan, Kaizheng Wang, and Yiqiao Zhong. Entrywise eigenvector
analysis of random matrices with low expected rank. arXiv:1709.09565, 2017.

[AIW18]

Susan Athey, Guido W Imbens, and Stefan Wager. Approximate residual balancing: debiased
inference of average treatment effects in high dimensions. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 80(4):597–623, 2018.

[BCH11]

Alexandre Belloni, Victor Chernozhukov, and Christian Hansen. Inference for high-dimensional
sparse econometric models. arXiv preprint arXiv:1201.0220, 2011.

[BFL+ 18]

Heather Battey, Jianqing Fan, Han Liu, Junwei Lu, and Ziwei Zhu. Distributed testing and
estimation under sparse high dimensional models. Annals of statistics, 46(3):1352, 2018.

[BN06]

Jushan Bai and Serena Ng. Confidence intervals for diffusion index forecasts and inference for
factor-augmented regressions. Econometrica, 74(4):1133–1150, 2006.

[CC14]

Yuxin Chen and Yuejie Chi. Robust spectral compressed sensing via structured matrix completion. IEEE Transactions on Information Theory, 60(10):6576 – 6601, 2014.

[CC17]

Yuxin Chen and Emmanuel J. Candès. Solving random quadratic systems of equations is
nearly as easy as solving linear systems. Comm. Pure Appl. Math., 70(5):822–883, 2017.

[CC18a]

Yudong Chen and Yuejie Chi. Harnessing structures in big data via guaranteed low-rank
matrix estimation: Recent theory and fast algorithms via convex and nonconvex optimization.
IEEE Signal Processing Magazine, 35(4):14–31, July 2018.

[CC18b]

Yuxin Chen and Emmanuel Candès. The projected power method: An efficient algorithm for
joint alignment from pairwise differences. Communications on Pure and Applied Mathematics,
71(8):1648–1714, 2018.

[CCD+ 18]

Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen,
Whitney Newey, and James Robins. Double/debiased machine learning for treatment and
structural parameters, 2018.

[CCD+ 19]

Vasileios Charisopoulos, Yudong Chen, Damek Davis, Mateo Díaz, Lijun Ding, and Dmitriy
Drusvyatskiy. Low-rank matrix recovery with composite optimization: good conditioning and
rapid convergence. arXiv preprint arXiv:1904.10020, 2019.

[CCF18]

Yuxin Chen, Chen Cheng, and Jianqing Fan. Asymmetry helps: Eigenvalue and eigenvector
analyses of asymmetrically perturbed low-rank matrices. arXiv preprint arXiv:1811.12804,
2018.

63

[CCF+ 19]

Yuxin Chen, Yuejie Chi, Jianqing Fan, Cong Ma, and Yuling Yan. Noisy matrix completion: Understanding statistical guarantees for convex relaxation via nonconvex optimization.
arXiv:1902.07698, 2019.

[CCFM19]

Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma. Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval. Mathematical Programming,
176(1-2):5–37, July 2019.

[CCG15]

Y. Chen, Y. Chi, and A. J. Goldsmith. Exact and stable covariance estimation from quadratic
sampling via convex programming. IEEE Transactions on Information Theory, 61(7):4034–
4059, 2015.

[CDDD19]

Vasileios Charisopoulos, Damek Davis, Mateo Díaz, and Dmitriy Drusvyatskiy. Composite
optimization for robust blind deconvolution. arXiv preprint arXiv:1901.01624, 2019.

[CEGN15]

Alexandra Carpentier, Jens Eisert, David Gross, and Richard Nickl. Uncertainty quantification for matrix compressed sensing and quantum tomography problems. arXiv preprint
arXiv:1504.03234, 2015.

[CFMW19]

Yuxin Chen, Jianqing Fan, Cong Ma, and Kaizheng Wang. Spectral method and regularized
MLE are both optimal for top-K ranking. Annals of Statistics, 47(4):2204–2235, August 2019.

[CG17]

T Tony Cai and Zijian Guo. Confidence intervals for high-dimensional linear regression: Minimax rates and adaptivity. The Annals of statistics, 45(2):615–646, 2017.

[Che15]

Yudong Chen. Incoherence-optimal matrix completion. IEEE Transactions on Information
Theory, 61(5):2909–2923, 2015.

[CKL16]

Alexandra Carpentier, Olga Klopp, and Matthias Löffler. Constructing confidence sets for the
matrix completion problem. In Conference of the International Society for Non-Parametric
Statistics, pages 103–118. Springer, 2016.

[CKLN18]

Alexandra Carpentier, Olga Klopp, Matthias Löffler, and Richard Nickl. Adaptive confidence
sets for matrix completion. Bernoulli, 24(4A):2429–2460, 2018.

[CL17]

Ji Chen and Xiaodong Li. Memory-efficient kernel PCA via partial matrix sampling and
nonconvex optimization: a model-free analysis of local minima. arXiv:1711.01742, 2017.

[CLC19]

Yuejie Chi, Yue M Lu, and Yuxin Chen. Nonconvex optimization meets low-rank matrix
factorization: An overview. IEEE Transactions on Signal Processing, 67(20):5239 – 5269,
October 2019.

[CLL19]

Ji Chen, Dekai Liu, and Xiaodong Li. Nonconvex rectangular matrix completion via gradient
descent without `2,∞ regularization. arXiv:1901.06116v1, 2019.

[CLMW11]

Emmanuel Candès, Xiaodong Li, Yi Ma, and John Wright. Robust principal component
analysis? Journal of ACM, 58(3):11:1–11:37, Jun 2011.

[CLR16]

T Tony Cai, Tengyuan Liang, and Alexander Rakhlin. Geometric inference for general highdimensional linear inverse problems. The Annals of Statistics, 44(4):1536–1563, 2016.

[CLS15]

E. Candès, X. Li, and M. Soltanolkotabi. Phase retrieval via Wirtinger flow: Theory and
algorithms. IEEE Transactions on Information Theory, 61(4):1985–2007, April 2015.

[CMW13]

T Tony Cai, Zongming Ma, and Yihong Wu. Sparse PCA: Optimal rates and adaptive estimation. The Annals of Statistics, 41(6):3074–3110, 2013.

[CN15]

Alexandra Carpentier and Richard Nickl. On signal detection and confidence sets for low rank
inference problems. Electronic Journal of Statistics, 9(2):2675–2688, 2015.

64

[CP10]

Emmanuel Candès and Yaniv Plan. Matrix completion with noise. Proceedings of the IEEE,
98(6):925 –936, June 2010.

[CR09]

Emmanuel Candès and Benjamin Recht. Exact matrix completion via convex optimization.
Foundations of Computational Mathematics, 9(6):717–772, April 2009.

[CSPW11]

Venkat Chandrasekaran, Sujay Sanghavi, Pablo A Parrilo, and Alan S Willsky. Rank-sparsity
incoherence for matrix decomposition. SIAM Journal on Optimization, 21(2):572–596, 2011.

[CW15]

Yudong Chen and Martin J Wainwright. Fast low-rank estimation by projected gradient
descent: General statistical and algorithmic guarantees. arXiv:1509.03025, 2015.

[CX16]

Yang Cao and Yao Xie. Poisson matrix recovery and completion. IEEE Transactions on Signal
Processing, 64(6):1609–1620, 2016.

[CZ13]

Tony Cai and Wen-Xin Zhou. A max-norm constrained minimization approach to 1-bit matrix
completion. The Journal of Machine Learning Research, 14(1):3619–3647, 2013.

[CZ16]

T Tony Cai and Wen-Xin Zhou. Matrix completion via max-norm constrained optimization.
Electronic Journal of Statistics, 10(1):1493–1525, 2016.

[DBMM15]

R. Dezeure, P. Bühlmann, L. Meier, and N. Meinshausen. High-dimensional inference: Confidence intervals, p-values and r-software hdi. Statistical science, pages 533–558, 2015.

[DBZ17]

Ruben Dezeure, Peter Bühlmann, and Cun-Hui Zhang. High-dimensional simultaneous inference with the bootstrap. Test, 26(4):685–719, 2017.

[DC18]

Lijun Ding and Yudong Chen. The leave-one-out approach for matrix completion: Primal and
dual analysis. arXiv preprint arXiv:1803.07554, 2018.

[DPVW14]

Mark A Davenport, Yaniv Plan, Ewout Van Den Berg, and Mary Wootters. 1-bit matrix
completion. Information and Inference, 3(3):189–223, 2014.

[DR16]

Mark A Davenport and Justin Romberg. An overview of low-rank matrix recovery from
incomplete observations. IEEE Journal of Selected Topics in Signal Processing, 10(4):608–
622, 2016.

[DR17]

John C Duchi and Feng Ruan. Solving (most) of a set of quadratic equalities: Composite
optimization for robust phase retrieval. arXiv:1705.02356, Information and Inference, 2017.

[EK15]

Noureddine El Karoui. On the impact of predictor geometry on the performance on highdimensional ridge-regularized generalized robust regression estimators. Probability Theory and
Related Fields, pages 1–81, 2015.

[EKBB+ 13]

Noureddine El Karoui, Derek Bean, Peter J Bickel, Chinghway Lim, and Bin Yu. On robust
regression with high-dimensional predictors. Proceedings of the National Academy of Sciences,
110(36):14557–14562, 2013.

[Faz02]

Maryam Fazel. Matrix rank minimization with applications. PhD thesis, 2002.

[FFHL19]

Jianqing Fan, Yingying Fan, Xiao Han, and Jinchi Lv. Asymptotic theory of eigenvectors for
large random matrices. arXiv preprint arXiv:1902.06846, 2019.

[FLM13]

J. Fan, Y. Liao, and M. Mincheva. Large covariance estimation by thresholding principal
orthogonal complements. Journal of the Royal Stat. Society: Series B, 75(4):603–680, 2013.

[FRW11]

Massimo Fornasier, Holger Rauhut, and Rachel Ward. Low-rank matrix recovery via iteratively
reweighted least squares minimization. SIAM Journal on Optimization, 21(4):1614–1640, 2011.

[FS11]

Rina Foygel and Nathan Srebro. Concentration-based guarantees for low-rank matrix reconstruction. In Conference on Learning Theory, pages 315–340, 2011.
65

[FSZZ18]

Jianqing Fan, Qiang Sun, Wen-Xin Zhou, and Ziwei Zhu. Principal component analysis for
big data. arXiv preprint arXiv:1801.01602, 2018.

[FWZ19]

Jianqing Fan, Weichen Wang, and Yiqiao Zhong. Robust covariance estimation for approximate
factor models. Journal of econometrics, 208(1):5–22, 2019.

[GJZ17]

Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems:
A unified geometric analysis. arXiv preprint arXiv:1704.00708, 2017.

[GLM16]

Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum.
In Advances in Neural Information Processing Systems, pages 2973–2981, 2016.

[Gro11]

David Gross. Recovering low-rank matrices from few coefficients in any basis. IEEE Transactions on Information Theory, 57(3):1548–1566, March 2011.

[Har14]

Moritz Hardt. Understanding alternating minimization for matrix completion. In Foundations
of Computer Science (FOCS), pages 651–660, 2014.

[HKZ12]

Daniel Hsu, Sham M. Kakade, and Tong Zhang. A tail inequality for quadratic forms of
subgaussian random vectors. Electron. Commun. Probab., 17:no. 52, 6, 2012.

[JKN16]

Chi Jin, Sham M Kakade, and Praneeth Netrapalli. Provable efficient online matrix completion
via non-convex stochastic gradient descent. In NIPS, pages 4520–4528, 2016.

[JM14a]

Adel Javanmard and Andrea Montanari. Confidence intervals and hypothesis testing for highdimensional regression. The Journal of Machine Learning Research, 15(1):2869–2909, 2014.

[JM14b]

Adel Javanmard and Andrea Montanari. Hypothesis testing in high-dimensional regression under the gaussian random design model: Asymptotic theory. IEEE Transactions on Information
Theory, 60(10):6522–6554, 2014.

[JM15]

Adel Javanmard and Andrea Montanari. De-biasing the lasso: Optimal sample size for Gaussian designs. arXiv preprint arXiv:1508.02757, 2015.

[JMD10]

Prateek Jain, Raghu Meka, and Inderjit S Dhillon. Guaranteed rank minimization via singular
value projection. In Advances in Neural Information Processing Systems, pages 937–945, 2010.

[JNS13]

P. Jain, P. Netrapalli, and S. Sanghavi. Low-rank matrix completion using alternating minimization. In ACM symposium on Theory of computing, pages 665–674, 2013.

[JVDG15]

Jana Jankova and Sara Van De Geer. Confidence intervals for high-dimensional inverse covariance estimation. Electronic Journal of Statistics, 9(1):1205–1229, 2015.

[JvdG17]

Jana Janková and Sara van de Geer. Honest confidence regions and optimality in highdimensional precision matrix estimation. Test, 26(1):143–162, 2017.

[JvdG18]

Jana Janková and Sara van de Geer. De-biased sparse pca: Inference and testing for eigenstructure of large covariance matrices. arXiv preprint arXiv:1801.10567, 2018.

[Klo14]

Olga Klopp. Noisy low-rank matrix completion with general sampling distribution. Bernoulli,
20(1):282–303, 2014.

[KLT11]

Vladimir Koltchinskii, Karim Lounici, and Alexandre B. Tsybakov. Nuclear-norm penalization
and optimal rates for noisy low-rank matrix completion. Ann. Statist., 39(5):2302–2329, 2011.

[KMO10a]

R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries. IEEE
Transactions on Information Theory, 56(6):2980 –2998, June 2010.

[KMO10b]

Raghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from
noisy entries. J. Mach. Learn. Res., 11:2057–2078, 2010.

66

[Kol11]

Vladimir Koltchinskii. Oracle inequalities in empirical risk minimization and sparse recovery
problems, volume 2033 of Lecture Notes in Mathematics. Springer, Heidelberg, 2011.

[KS11]

Alois Kneip and Pascal Sarda. Factor models and variable selection in high-dimensional regression analysis. The Annals of Statistics, 39(5):2410–2447, 2011.

[KS19]

Felix Krahmer and Dominik Stöger. On the convex geometry of blind deconvolution and matrix
completion. arXiv preprint arXiv:1902.11156, 2019.

[KX15]

Vladimir Koltchinskii and Dong Xia. Optimal estimation of low rank density matrices. Journal
of Machine Learning Research, 16:1757–1792, 2015.

[LSST16]

Jason D Lee, Dennis L Sun, Yuekai Sun, and Jonathan E Taylor. Exact post-selection inference,
with application to the lasso. The Annals of Statistics, 44(3):907–927, 2016.

[LTTT14]

Richard Lockhart, Jonathan Taylor, Ryan J Tibshirani, and Robert Tibshirani. A significance
test for the lasso. Annals of statistics, 42(2):413, 2014.

[LXY13]

M. Lai, Y. Xu, and W. Yin. Improved iteratively reweighted least squares for unconstrained
smoothed `q minimization. SIAM Journal on Numerical Analysis, 51(2):927–957, 2013.

[MHT10]

R. Mazumder, T. Hastie, and R. Tibshirani. Spectral regularization algorithms for learning
large incomplete matrices. Journal of machine learning research, 11(Aug):2287–2322, 2010.

[MJD09]

Raghu Meka, Prateek Jain, and Inderjit S. Dhillon. Guaranteed rank minimization via singular
value projection. preprint, 2009.

[MLL17]

Cong Ma, Junwei Lu, and Han Liu. Inter-subject analysis: Inferring sparse interactions with
dense intra-graphs. arXiv preprint arXiv:1709.07036, 2017.

[MMB09]

Nicolai Meinshausen, Lukas Meier, and Peter Bühlmann. P-values for high-dimensional regression. Journal of the American Statistical Association, 104(488):1671–1681, 2009.

[MSL19]

Igor Molybog, Somayeh Sojoudi, and Javad Lavaei. No spurious solutions in non-convex matrix
sensing: Structure compensates for isometry. 2019.

[MWCC17]

Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex
statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion and blind deconvolution. arXiv preprint arXiv:1711.10467, accepted to Foundations of
Computational Mathematics, 2017.

[MX17]

Simon Mak and Yao Xie. Active matrix completion with uncertainty quantification. arXiv
preprint arXiv:1706.08037, 2017.

[NCD19]

National climatic data center. https://www.ncdc.noaa.gov/, 2019. Accessed: 2019-08-31.

[NL17]

Yang Ning and Han Liu. A general theory of hypothesis tests and confidence regions for sparse
high dimensional models. The Annals of Statistics, 45(1):158–195, 2017.

[NNLL18]

Matey Neykov, Yang Ning, Jun S Liu, and Han Liu. A unified theory of confidence regions and
testing for high-dimensional estimating equations. Statistical Science, 33(3):427–443, 2018.

[NW12]

S. Negahban and M.J. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. Journal of Machine Learning Research, pages 1665–1697,
May 2012.

[PB14]

Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends R in Optimization, 1(3):127–239, 2014.

[Rec11]

Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning
Research, 12(Dec):3413–3430, 2011.
67

[RFP10]

B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization. SIAM Review, 52(3):471–501, 2010.

[RS05]

Jasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative prediction. International conference on Machine learning, pages 713–719, 2005.

[RSZZ15]

Z. Ren, T. Sun, C. Zhang, and H. Zhou. Asymptotic normality and optimalities in estimation
of large gaussian graphical models. The Annals of Statistics, 43(3):991–1026, 2015.

[RT11]

Angelika Rohde and Alexandre B Tsybakov. Estimation of high-dimensional low-rank matrices.
The Annals of Statistics, 39(2):887–930, 2011.

[Sch92]

Bernhard A. Schmitt. Perturbation bounds for matrix square roots and Pythagorean sums.
Linear Algebra Appl., 174:215–227, 1992.

[Sha03]

J. Shao. Mathematical Statistics. Springer Texts in Statistics. Springer, 2003.

[Sin11]

Amit Singer. Angular synchronization by eigenvectors and semidefinite programming. Applied
and computational harmonic analysis, 30(1):20–36, 2011.

[SL16]

Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization.
IEEE Transactions on Information Theory, 62(11):6535–6579, 2016.

[Smi68]

RA Smith. Matrix equation XA + BX = C.
16(1):198–201, 1968.

[Sre04]

Nathan Srebro. Learning with matrix factorizations. Ph. D. thesis, 2004.

[SS05]

Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In International Conference on Computational Learning Theory, pages 545–560. Springer, 2005.

[Ste77]

Gilbert W Stewart. On the perturbation of pseudo-inverses, projections and linear least squares
problems. SIAM review, 19(4):634–662, 1977.

[SXZ19]

Alexander Shapiro, Yao Xie, and Rui Zhang. Matrix completion with deterministic pattern:
A geometric perspective. IEEE Transactions on Signal Processing, 67(4):1088–1103, 2019.

[SY07]

Anthony Man-Cho So and Yinyu Ye. Theory of semidefinite programming for sensor network
localization. Mathematical Programming, 109(2-3):367–384, 2007.

[SZ12]

Tingni Sun and Cun-Hui Zhang. Calibrated elastic regularization in matrix completion. In
Advances in Neural Information Processing Systems, pages 863–871, 2012.

[TBS+ 16]

S. Tu, R. Boczar, M. Simchowitz, M. Soltanolkotabi, and B. Recht. Low-rank solutions of
linear matrix equations via procrustes flow. ICML, pages 964–973, 2016.

[Van13]

Bart Vandereycken. Low-rank matrix completion by riemannian optimization. SIAM Journal
on Optimization, 23(2):1214–1236, 2013.

SIAM Journal on Applied Mathematics,

[vdGBRD14] Sara van de Geer, Peter Bühlmann, Ya’acov Ritov, and Ruben Dezeure. On asymptotically
optimal confidence regions and tests for high-dimensional models. The Annals of Statistics,
42(3):1166–1202, 2014.
[Ver17]

Roman Vershynin. High dimensional probability, 2017.

[Wai19]

M.J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge
Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.

[WCCL16]

K. Wei, J.F. Cai, T. Chan, and S. Leung. Guarantees of Riemannian optimization for low rank
matrix recovery. SIAM Journal on Matrix Analysis and Applications, 37(3):1198–1222, 2016.

68

[Wed72]

Per-Åke Wedin. Perturbation bounds in connection with singular value decomposition. BIT
Numerical Mathematics, 12(1):99–111, 1972.

[WR09]

Larry Wasserman and Kathryn Roeder. High dimensional variable selection. Annals of statistics, 37(5A):2178, 2009.

[WYZ12]

Zaiwen Wen, Wotao Yin, and Yin Zhang. Solving a low-rank factorization model for matrix
completion by a nonlinear successive over-relaxation algorithm. Mathematical Programming
Computation, 4(4):333–361, 2012.

[WZG16]

Lingxiao Wang, Xiao Zhang, and Quanquan Gu. A unified computational and statistical
framework for nonconvex low-rank matrix estimation. arXiv preprint arXiv:1610.05275, 2016.

[Xia18]

Dong Xia. Confidence interval of singular vectors for high-dimensional and low-rank matrix
regression. arXiv preprint arXiv:1805.09871, 2018.

[Xia19]

Dong Xia. Data-dependent confidence regions of singular subspaces.
arXiv:1901.00304, 2019.

[ZC17]

Xianyang Zhang and Guang Cheng. Simultaneous inference for high-dimensional linear models.
Journal of the American Statistical Association, 112(518):757–768, 2017.

[ZHT06]

Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis. Journal
of computational and graphical statistics, 15(2):265–286, 2006.

[ZJSL18]

Richard Zhang, Cédric Josz, Somayeh Sojoudi, and Javad Lavaei. How much restricted isometry is needed in nonconvex matrix recovery? pages 5586–5597, 2018.

[ZL16]

Qinqing Zheng and John Lafferty. Convergence analysis for rectangular matrix completion
using Burer-Monteiro factorization and gradient descent. arXiv:1605.07051, 2016.

[ZPL15]

Tao Zhang, John M Pauly, and Ives R Levesque. Accelerating parameter mapping with a
locally low rank constraint. Magnetic resonance in medicine, 73(2):655–661, 2015.

[ZSL19]

R. Zhang, S. Sojoudi, and J. Lavaei. Sharp restricted isometry bounds for the inexistence of
spurious local minima in nonconvex matrix recovery. arXiv:1901.01631, 2019.

[ZWL15]

Tuo Zhao, Zhaoran Wang, and Han Liu. A nonconvex optimization framework for low rank
matrix estimation. In NIPS, pages 559–567, 2015.

[ZWYG18]

Xiao Zhang, Lingxiao Wang, Yaodong Yu, and Quanquan Gu. A primal-dual analysis of global
optimality in nonconvex low-rank matrix recovery. In International conference on machine
learning, pages 5857–5866, 2018.

[ZZ14]

Cun-Hui Zhang and Stephanie S Zhang. Confidence intervals for low dimensional parameters in
high dimensional linear models. Journal of the Royal Statistical Society: Series B, 76(1):217–
242, 2014.

69

arXiv preprint

