Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff

Yochai Blau 1 Tomer Michaeli 1

arXiv:1901.07821v4 [cs.LG] 30 Jul 2019

Abstract
Lossy compression algorithms are typically designed and analyzed through the lens of Shannon’s rate-distortion theory, where the goal is to
achieve the lowest possible distortion (e.g., low
MSE or high SSIM) at any given bit rate. However, in recent years, it has become increasingly
accepted that “low distortion” is not a synonym
for “high perceptual quality”, and in fact optimization of one often comes at the expense of the
other. In light of this understanding, it is natural to
seek for a generalization of rate-distortion theory
which takes perceptual quality into account. In
this paper, we adopt the mathematical definition
of perceptual quality recently proposed by Blau &
Michaeli (2018), and use it to study the three-way
tradeoff between rate, distortion, and perception.
We show that restricting the perceptual quality
to be high, generally leads to an elevation of the
rate-distortion curve, thus necessitating a sacrifice in either rate or distortion. We prove several
fundamental properties of this triple-tradeoff, calculate it in closed form for a Bernoulli source, and
illustrate it visually on a toy MNIST example.

1. Introduction
Lossy compression techniques are ubiquitous in the modernday digital world, and are regularly used for communicating
and storing images, video and audio. In recent years, lossy
compression is seeing a surge of research, due in part to the
advancements in deep learning and their application in this
domain (Toderici et al., 2016; 2017; Ballé et al., 2016; 2017;
2018; Agustsson et al., 2017; 2018; Rippel & Bourdev,
2017; Minnen et al., 2018; Li et al., 2018; Mentzer et al.,
2018; Johnston et al., 2018; Galteri et al., 2017; Tschannen
et al., 2018; Santurkar et al., 2018; Rott Shaham & Michaeli,
2018). The theoretical foundations of lossy compression
are rooted in Shannon’s seminal work on rate-distortion
1
Technion–Israel Institute of Technology, Haifa, Israel. Correspondence to: Yochai Blau <yochai@campus.technion.ac.il>,
Tomer Michaeli <tomer.m@ee.technion.ac.il>.

Published in the Proceedings of the 36 th International Conference
on Machine Learning, Long Beach, California, PMLR 97, 2019.

theory (Shannon, 1959), which analyzes the fundamental
tradeoff between the bit rate used for representing data, and
the distortion incurred when reconstructing the data from
its compressed representation (Cover & Thomas, 2012).
The premise in rate-distortion theory is that reduced distortion is a desired property. However, recent works demonstrate that minimizing distortion alone does not necessarily
drive the decoded signals to have good perceptual quality. For example, incorporating generative adversarial type
losses has been shown to lead to significantly better perceptual quality, but at the cost of increased distortion (Tschannen et al., 2018; Agustsson et al., 2018; Santurkar et al.,
2018). This behavior has also been studied in the context
of signal restoration (Blau & Michaeli, 2018), where it was
shown that minimizing distortion causes the distribution
of restored signals to deviate from that of the ground-truth
signals (indicating worse perceptual quality). In light of this
understanding, it is natural to seek for a generalized ratedistortion theory, which also accounts for perception. In
particular, it is of key importance to understand how the best
achievable rate depends not only on the distortion, but also
on the perceptual quality of the algorithm. A preliminary
attempt to incorporate perceptual quality into rate-distortion
theory was briefly reported in (Matsumoto, 2018a;b). Yet,
no theoretical characterization nor practical demonstration
of its effect on the rate-distortion tradeoff was presented.
In this paper, we adopt the mathematical definition of perceptual quality used in (Blau & Michaeli, 2018), and prove
that there is a triple tradeoff between rate, distortion and
perception. Our key observation is that the rate-distortion
function elevates as the perceptual quality is enforced to be
higher (see Fig. 1). In other words, to obtain good perceptual quality, it is necessary to make a sacrifice in either the
distortion or the rate of the algorithm.
Our analysis is based on the definition of a rate-distortionperception function R(D, P ), which characterizes the minimal achievable rate R for any given distortion D and perception index P . We begin by deriving a closed form for this
function in the classical case study of a Bernoulli source,
a simple example which nonetheless nicely illustrates the
typical behavior of the tradeoff. We then prove several general properties of R(D, P ), showing that it is monotone
and convex for any full-reference distortion measure (under

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff

Rate

Shannon’s R-D curve (unconstrained)
Medium perceptual quality constraint
Perfect perceptual quality constraint

Distortion
Figure 1. The rate-distortion function under a perceptual quality constraint. When perceptual quality is unconstrained, the
tradeoff is characterized by Shannon’s classic rate-distortion function (black line). However, as the constraint on perceptual quality
is tightened to ensure perceptually pleasing reconstructions, the
function elevates (colored lines). Thus, the improvement in perceptual quality comes at the cost of a higher rate and/or distortion.

minor assumptions), and that there is a range of P values
for which it necessarily does not coincide with the traditional rate-distortion function. For the specific case of the
squared-error distortion, we also provide an upper bound on
the increase in distortion that has to be incurred in order to
achieve perfect perceptual quality, at any given rate.
Our observations have important implications for the design
and evaluation of practical compression methods. In particular, they suggest that comparing between algorithms only
in terms of their rate-distortion curves can be misleading.
We demonstrate this in the context of image compression
using a toy MNIST example, by systematically exploring
the visual effect of improvement in each of the three properties (rate, distortion, perception) on the expense of the
others. We do this by training an encoder-decoder net utilizing a generative model, similarly to (Tschannen et al., 2018;
Agustsson et al., 2018). As we show, the phenomena we
discuss are dominant at low bit rates, where the classical
approach of optimizing distortion alone leads to unacceptable perceptual quality. This is perhaps not surprising when
using the MSE distortion, which is known to be inconsistent
with human perception. But our theory shows that every
distortion measure (excluding pathological cases) must have
a tradeoff with perceptual quality. This includes e.g., the
popular SSIM/MS-SSIM (Wang et al., 2003; 2004), the
L2 distance between deep features (Johnson et al., 2016),
and any other full reference criterion. To illustrate this, we
repeat our toy experiment with the distortion measure of
(Johnson et al., 2016), which has been used as a means
for enhancing perceptual quality in low-level vision tasks
(Ledig et al., 2017). As we show, minimizing this distortion
does not lead to good perceptual quality at low bit rates, just
like our theory predicts. Moreover, when enforcing high
perceptual quality, this distortion rather increases.

2. Background
2.1. Rate-Distortion Theory
Rate-distortion theory analyzes the fundamental tradeoff
between the rate (bits per sample) used for representing
samples from a data source X ∼ pX , and the expected
distortion incurred in decoding those samples from their
compressed representations. Formally, the relation between
the input X and output X̂ of an encoder-decoder pair, is a
(possibly stochastic) mapping defined by some conditional
distribution pX̂|X , as visualized in Fig. 2. The expected
distortion of the decoded signals is thus defined as
E[∆(X, X̂)],

(1)

where the expectation is with respect to the joint distribution
pX,X̂ = pX̂|X pX , and ∆ : X × X̂ → R+ is any fullreference distortion measure such that ∆(x, x̂) = 0 if and
only if x = x̂ (e.g., squared error, L2 distance between deep
features (Johnson et al., 2016; Zhang et al., 2018), SSIM/
MS-SSIM1 (Wang et al., 2003; 2004), PESQ (Rix et al.,
2001), etc.).
A key result in rate-distortion theory states that for an iid
source X, if the expected distortion is bounded by D, then
the lowest achievable rate R is characterized by the (information) rate-distortion function
R(D) = min I(X, X̂) s.t.
pX̂|X

E[∆(X, X̂)] ≤ D,

(2)

where I denotes mutual information (Cover & Thomas,
2012). Closed form expressions for the rate-distortion function R(D) are known for only a few source distributions
and under quite simple distortion measures (e.g., squared
error or Hamming distance). However several general properties of this function are known, including that it is always
monotonically non-increasing and convex.
2.2. Perceptual Quality
The perceptual quality of an output sample x̂ refers to the
extent to which it is perceived by humans as a valid (natural)
sample, regardless of its similarity to the input x. In various
domains, perceptual quality has been associated with the
deviation of the distribution pX̂ of output signals from the
distribution pX of natural signals, which, as discussed in
(Blau & Michaeli, 2018), is linked to the common practice
of quantifying perceptual quality via real-vs.-fake user studies (Isola et al., 2017; Salimans et al., 2016; Zhang et al.,
2016; Denton et al., 2015). In particular, deviation from
natural scene statistics is the basis for many no-reference
image quality measures (Mittal et al., 2013; 2012; Wang
1

Measures like SSIM, which quantify similarity rather than
dissimilarity and are not necessarily positive, need to be negated
and shifted to qualify as valid distortion measures.

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff

Encoder

Rate:

3. The Rate-Distortion-Perception Tradeoff

Distortion:

Since both perceptual quality and distortion are typically
important, here we extend the rate-distortion function (2) to
take into account the perception index3 (3).

Decoder

Perception:

Definition 1 The (information) rate-distortion-perception
function is defined as

Figure 2. Lossy compression. A source signal X ∼ pX is mapped
into a coded sequence by an encoder and back into an estimated
signal X̂ by the decoder. Three desired properties are: (i) the coded
sequence be compact (low bit rate); (ii) the reconstruction X̂ be
similar to the source X on average (low distortion); (iii) the distribution pX̂ be similar to pX , so that decoded signals are perceived
as genuine source signals (good perceptual quality).

& Simoncelli, 2005), which have been shown to correlate
well with human opinion scores. It is also the principle
underlying GAN-based image restoration schemes, which
achieve enhanced perceptual quality by directly minimizing some divergence d(pX , pX̂ ) (Ledig et al., 2017; Pathak
et al., 2016; Isola et al., 2017; Wang et al., 2018). Based
on these works, and following (Blau & Michaeli, 2018), we
define the perceptual quality index (lower is better) of an
algorithm as
d(pX , pX̂ ),

(3)

R(D, P ) = min I(X, X̂)
pX̂|X

s.t. E[∆(X, X̂)] ≤ D, d(pX , pX̂ ) ≤ P. (4)
Unfortunately, closed form solutions for (4) are even harder
to obtain than for (2). Yet, one notable exception is the
classical case study of a binary source, as we show next.
While of limited applicability, this example illustrates the
typical behavior of (4), which we analyze in Sec. 3.2.
3.1. Bernoulli Source
Consider the problem of encoding a binary source X ∼
Bern(p), where the decoder’s output X̂ is also constrained
to be binary. Let us take the distortion measure ∆(·, ·) to
be the Hamming distance, and the perception index4 to be
the total-variation (TV) distance dTV (·, ·). Without loss of
generality, we assume that p ≤ 21 . When perception is not
constrained (i.e., P = ∞), the solution to (4) reduces to
the rate-distortion function (2) of a binary source, which is
known to be given by
(
Hb (p) − Hb (D)
D ∈ [0, p)
R(D, ∞) =
(5)
0
D ∈ [p, ∞)

where d(·, ·) is some divergence between distributions2 (e.g.,
Kulback-Leibler, Wasserstein, etc.). Note that the divergence function d(·, ·) which best relates to human perception is a subject of ongoing research. Yet, our results below
hold for (nearly) any divergence.

where Hb (α) is the entropy of a Bernoulli random variable
with probability α (Cover & Thomas, 2012).

Obviously, perceptual quality, as defined above, is very
different from distortion. In particular, minimizing the perceptual quality index does not necessarily lead to low distortion. For example, if the decoder disregards the input,
and outputs random samples from the source distribution
pX , it will achieve perfect perceptual quality but very poor
distortion. It turns out that this is true also in the other direction. That is, minimizing distortion does not necessarily
lead to good perceptual quality. This observation has been
studied in (Blau & Michaeli, 2018) in the specific context
of signal restoration (e.g. denoising, super-resolution). In
particular, perception and distortion are fundamentally at
odds with each other (for non-invertible degradations), in
the sense that optimizing one always comes on the expense
of the other. This behavior, coined the perception-distortion
tradeoff, was shown to hold true for any distortion measure.

R(D, P ) =
(6)


D ∈ S1
Hb (p) − Hb (D)
D+P
2Hb (p)+Hb (p−P )−Ht ( D−P
,
p)−H
(
,
q)
D ∈ S2
t
2
2


0
D ∈ S3

2

We assume that d(p, q) ≥ 0, d(p, q) = 0 ⇔ p = q.

In the Supplementary Material, we derive the solution for
arbitrary P . It turns out that as long as the perceptual quality
constraint is sufficiently loose, the solution remains the
same. However, when P ≤ p, the perception constraint
in (4) becomes active whenever the distortion constraint is
loose enough, from which point the function R(·, P ) departs
from R(·, ∞). Specifically, for P ≤ p, we have

3

Similarly to (2), R(D, P ) in (4) lower bounds the best achievable rate for an iid source (see Supplementary Material). We do not
prove achievability of R(D, P ) in general. However for the MSE
distortion, we show an achievable upper bound (see Theorem 2).
4
The term “perception” is somewhat inappropriate for a
Bernoulli source, as it is not perceived by humans (contrary to
images, audio). Yet, we keep this terminology here for consistency.

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff

the tradeoff between distortion and perception in this regime.
Figure 4(b) provides an additional viewpoint, by showing
perception-distortion curves for different bit rates. Notice
again that the tradeoff between distortion and perceptual
quality becomes stronger at low bit-rates. Finally, Fig. 4(c)
shows the somewhat counter-intuitive tradeoff between rate
and perceptual quality as a function of distortion. Specifically, we see that at every constant distortion level, the
perceptual quality can be improved by increasing the rate.

0.4
0.3
0.2
0.1
0

0

0.05

0.1

0.15

Figure 3. Perception constrained rate-distortion curves for a
Bernoulli source. Shannon’s rate-distortion function (dashed
curve) characterizes the best achievable rate under any prescribed
distortion level, yet does not ensure good perceptual quality. When
constraining the perceptual quality index dTV (pX , pX̂ ) to be low
(good quality), the rate-distortion function elevates (solid curves).
This indicates that good perceptual quality must come at the cost
1
of a higher rate and/or a higher distortion. Here X ∼ Bern( 10
).

3.2. Theoretical Properties
For general source distributions, it is usually impossible to
solve (4) analytically. However, it turns out that the behavior
we saw for a Bernoulli source is quite typical. We next prove
several general properties of the function (4), which hold
under rather mild assumptions. Specifically, we assume:
A1 The divergence d(·, ·) in (4) is convex in its second
argument. That is, for any λ ∈ [0, 1] and for any three
distributions p0 , q1 , q2 ,
d(p0 , λq1 +(1−λ)q2 ) ≤ λd(p0 , q1 )+(1−λ)d(p0 , q2 ). (7)

where q = 1 − p and Ht (α, β) denotes the entropy of a
ternary random variable with probabilities α, β, 1 − α − β.
Here, S1 = [0, D1 ), S2 = [D1 , D2 ), and S3 = [D2 , ∞),
P
where D1 = 1−2(p−P
) and D2 = 2pq − (q − p)P .
Figure 3 plots R(D, P ) as a function of D for several values of P . As can be seen, at D = 0, all the curves merge.
This is because at this point X̂ = X (lossless compression), so that pX̂ = pX , and thus the perceptual quality is
perfect. Yet, as the allowed distortion D grows larger, the
curves depart. This illustrates that achieving the classical
rate-distortion curve (black dashed line) does not generally
lead to good perceptual quality. The more stringent our prescribed perceptual quality constraint (lower P ), the more the
rate-distortion curve elevates (colored curves). In particular,
the tradeoff becomes severe at the low bit rate regime, where
good perceptual quality comes at the cost of a significantly
higher distortion and/or bit rate. Notice that it is possible to
achieve perfect perceptual quality at every rate (blue curve)
by compromising the distortion to some extent. In Sec. 3.2
we provide an upper-bound on the increase in distortion
required for obtaining perfect perceptual quality.
While Fig. 3 displays cross-sections of R(D, P ) along ratedistortion planes, in Fig. 4 we plot R(D, P ) as a surface in
3 dimensions, as well as its cross-sections along the other
planes. The equi-rate level sets shown on the surface in
Fig. 4(a), provide another visualization for the phenomenon
described above. That is, at high bit-rates, it is possible to
achieve good perceptual quality (low P ) without a significant sacrifice in the distortion D. However, as the bit-rate
becomes lower, the equi-rate level sets substantially curve
towards the low P values, illuminating the exacerbation in

A2 The function k(z) = EX∼pX [∆(X, z)] is not constant5
over the entire support of pX .
Assumption A1 is not very limiting. For instance, any f divergence (e.g. KL, TV, Hellinger, X 2 ) as well as the
Renyi divergence, satisfies this assumption (Csiszár et al.,
2004; Van Erven & Harremos, 2014). Assumption A2 holds
in any setting where the mean distance between a “valid”
signal z and all other “valid” signals is not constant6 . In
particular, it holds for any distortion function ∆(·, ·) with
a unique minimizer, such as the squared-error distortion
and the SSIM index (under some assumptions) (Brunet,
2012). Using these assumptions, we are able to qualitatively
characterize the general shape of the function R(D, P ).
Theorem 1 The rate-distortion-perception function (4):
1. is monotonically non-increasing in D and P ;
2. is convex if A1 holds;
3. satisfies R(·, 0) 6= R(·, ∞) if A2 holds.
The proof of Theorem 1 can be found in the Supplementary
Material. Note that when assumption A2 holds, properties 1 and 3 indicate that there exists some D0 for which
R(D0 , 0) > R(D0 , ∞), showing that the rate-distortion
curve necessarily elevates when constraining for perfect perceptual quality. In any case, assumption A2 is a sufficient
condition for property 3, so that even if it does not hold, this
does not necessarily imply that R(·, 0) = R(·, ∞).
5
In fact, we only need the weaker condition that k(z) do not
attain its minimum over the entire support of pX .
6
A valid signal is any x : pX (x) > 0. Also, we use “distance”
here for clarity, although ∆(·, ·) is not necessarily a metric.

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff
0.3

0.4

0.08
0.4
0.3

0.2

0.2

0.06

0.2

0.04
0.1

0
0
0.05

0.1

0.1
0.15

0.1
0.15

0.02
0

0.05
0

0

0.05

(a)

0.1

0

0

(b)

0.05

0.1

(c)

Figure 4. The rate-distortion-perception function of a Bernoulli source. (a) Equi-rate level sets depicted on the rate-distortionperception function R(D, P ). At low bit-rates, the equi-rate lines curve substantially when approaching P = 0, displaying the increasing
tradeoff between distortion and perceptual quality. (b) Cross sections of R(D, P ) along perception-distortion planes. Notice the tradeoff
between perceptual quality and distortion, which becomes stronger at low bit-rates. (c) Cross sections of R(D, P ) along rate-perception
planes. Note that at constant distortion, the perceptual quality can be improved by increasing the rate.

image restoration scenarios, such a 2-fold increase in the
MSE (3dB decrease in PSNR) has been shown to enable a
substantial improvement in perceptual quality by practical
algorithms (Blau et al., 2018; Ledig et al., 2017). Note that
this bound is generally not tight. Thus, in some settings, perfect perceptual quality can be obtained with an even smaller
increase in distortion.

2

4. Experimental Illustration
Figure 5. Illustration of Theorem 2. When using the MSE distortion, the rate-distortion curve for compression with perfect perceptual quality (blue) is higher than Shannon’s rate-distortion function
(black dashed line) but is necessarily lower than the 2× scaled
version of Shannon’s function (dotted line).

How much does the rate-distortion curve elevate when constraining for perfect perceptual quality? The next theorem
upper-bounds this elevation for the MSE distortion (see
proof in the Supplementary Material).
Theorem 2 When using the squared-error distortion, the
function R(·, 0) (rate-distortion at perfect perceptual quality) is bounded by
R(D, 0) ≤ R( 12 D, ∞).

(8)

Theorem 2 shows that it is possible to attain perfect perceptual quality without increasing the rate, by sacrificing
no more than a 2-fold increase in the mean squared-error
(MSE). More specifically, attaining perfect perceptual quality at distortion D does not require a higher bit rate than
that necessary for compression at distortion 12 D with no
perceptual quality constraint. This is illustrated in Fig. 5,
where the perfect-quality curve R(·, 0) shown in blue is
bounded by the scaled version of Shannon’s unconstrained
quality curve R(·, ∞) shown as a black dashed line. In

We now turn to demonstrate the visual implications of the
rate-distortion-perception tradeoff in lossy image compression on a toy MNIST example. We make no attempt to
propose a new state-of-the-art compression method. Our
sole goal is to systematically explore the effect of the balance between rate, distortion, and perception. To this end,
we utilize a net-based encoder-decoder pair trained in an
end-to-end fashion, similarly to recent works. By tuning the
influence of each of the different terms of the loss, we can
easily control the balance between these three quantities.
More concretely, we use an encoder f and a decoder g, both
parametrized by deep neural nets (DNNs). The encoder
maps the input x into a latent feature vector f (x), whose
entries are then uniformly quantized to L levels to obtain the
representation fˆ(x). The decoder outputs a reconstruction
x̂ = g(fˆ(x)). To enable back-propagation through the quantizer, we use the differentiable relaxation of (Mentzer et al.,
2018). Note that this relaxation affects only the gradient
computation through the quantizer during back-propagation,
but not the forward-pass “hard” quantization.
As in recent perceptual-quality driven lossy compression
schemes (Tschannen et al., 2018; Agustsson et al., 2018),
the rate is controlled by the dimension dim of the encoder’s
output f (x), and the number of levels L used for quantizing
each of its entries, such that R ≤ dim × log2 (L). Note that

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff

Shannon’s

12 bits
8 bits
4 bits
2 bits

Input

Decoded

Figure 6. Perceptual lossy compression of MNIST digits. Left: Shannon’s rate-distortion curve (black) describes the lowest possible
rate (bits per digit) as a function of distortion, but leads to low perceptual quality (high dW values), especially at low rates. When
constraining the perceptual quality to be good (low P values), the rate-distortion curve elevates, indicating that this comes at the cost of a
higher rate and/or distortion. Right: Encoder-decoder outputs along Shannon’s rate-distortion curve and along two equi-perceptual-quality
curves. As the rate decreases, the perceptual quality along Shannon’s curve degrades significantly. This is avoided when constraining the
perceptual quality, which results in visually pleasing reconstructions, even at extremely low bit-rates. Notice that increased perceptually
quality does not imply increased accuracy, as most reconstructions fail to preserve the digits’ identities at a 2-bit rate.

this only upper-bounds the best achievable rate, as lossless
compression of fˆ(x) would potentially further reduce the
representation’s size. However, it significantly simplifies
the scheme, and was found to be only slightly sub-optimal
(Agustsson et al., 2018).
For any fixed rate, we train the encoder-decoder to minimize
a loss comprising a weighted combination of the expected
distortion and the perception index,
E[∆(X, g(fˆ(X)))] + λdW (pX , pX̂ ).

(9)

Here, X̂ = g(fˆ(X)), dW (·, ·) is the Wasserstein distance,
and λ is a tuning parameter, which we use to control the
balance between perception and distortion. The perceptual quality term can be optimized with the framework of
generative adversarial networks (GAN) (Goodfellow et al.,
2014) by introducing an additional (discriminator) DNN
h : X → R and minimizing


E[∆(X, g(fˆ(X)))] + λ max E[h(X)] − E[h(g(fˆ(X)))]
h∈F

(10)
where in our specific case of a Wasserstein GAN (Arjovsky
et al., 2017), F denotes the class of bounded 1-Lipschitz
functions. As usual, all expectations are replaced by sample
means, the constraint h ∈ F is replaced by a gradient
penalty (Gulrajani et al., 2017), and the loss is minimized by
alternating between minimization w.r.t. f, g while holding
h fixed and maximization w.r.t. h while holding f, g fixed.

To achieve good perceptual quality, especially at low rates,
it is essential that the decoder be stochastic (Tschannen
et al., 2018). This is commonly carried out by an additional
random noise input. Yet, deep generative models in the
conditional setting tend to ignore this type of stochasticity
(Zhu et al., 2017a;b; Mathieu et al., 2016). Tschannen et
al. (2018) remedy this by applying a two-stage training
scheme, which indeed promotes the use of stochasticity
within the decoder, but can lead to sub-optimal results. Here,
instead of concatenating a noise vector n to the encoder’s
output fˆ(x), we add it, so that the decoder in fact operators
on the noisy representation fˆ(x) + n. This does not lead to
loss of information, as the noise n is drawn from a uniform
distribution U (− α2 , α2 ), with α smaller than the quantization
bin size. Thus, different coded representations fˆ(x) do not
“mix-up”, and can always be distinguished from one another.
This scheme urges the decoder to utilize the stochastic input,
while allowing end-to-end training in a one-step manner.
4.1. Squared-Error Distortion
We begin by experimenting with the squared-error distortion
∆(x, x̂) = kx − x̂k2 . We train 98 encoder-decoder pairs on
the MNIST handwritten digit dataset (LeCun et al., 1998),
while varying the encoder’s output dimension dim and number of quantization levels L to control the rate R, and the
tuning coefficient λ to achieve different balances between
distortion and perceptual quality. A list of all combinations

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff

0.8
0.6
0.4
0.2

0.04

0.06

(a)

0.08

(b)

0.1

0.12

16

16

14

14

12

12

10

10

8

8

6

6

4

4

2

2

0.12
0.1
0.08
0.06
0.04

0.2

0.4

0.6

0.8

(c)

Figure 7. The rate-distortion-perception function of MNIST images. (a) Equi-rate lines plotted on R(D, P ) highlight the tradeoff
between distortion and perceptual quality at any constant rate. (b) Cross sections of R(D, P ) along perception-distortion planes show that
this tradeoff becomes stronger at low bit-rates. (c) Cross-sections of R(D, P ) along rate-perception planes highlght that at any constant
distortion, the perceptual quality can be improved by increasing the rate.

of (dim, L, λ) used, along with all other training details can
be found in the Supplementary Material.
The left side of Fig. 6 plots the 98 trained encoder-decoder
pairs on the rate-distortion plane, with the perceptual quality indicated by color coding and rate measured in bits per
digit. The perceptual quality is quantified by the final discriminator loss7 , which approximates the Wasserstein distance dW (pX , pX̂ ). We plot an approximation of Shannon’s
rate-distortion function (obtained with λ = 0), and two
additional rate-distortion curves with (approximately) constant perceptual quality8 . As can be seen, the rate-distortion
curve elevates when constraining the perceptual quality to
be good. This demonstrates once again that we can improve
the perceptual quality w.r.t. that obtained on Shannon’s ratedistortion curve, yet this must come at the cost of a higher
rate and/or distortion. Notice that the perception index is
not constant along Shannon’s function; it increases (worse
quality) towards lower bit-rates.
On the right side of Fig. 6, we depict the outputs of encoderdecoder pairs along Shannon’s rate-distortion function, and
along the two equi-perception curves shown on the left.
It can be seen that as the rate decreases, the perceptual
quality of the reconstructions along Shannon’s function degrades. However, this is avoided when constraining the
perceptual quality, which results in visually pleasing reconstructions even at extremely low bit-rates. Notice that this
increased perceptual quality does not imply increased accuracy, as at low bit rates (e.g., 2 bits), most reconstructions
fail to preserve even the identity of the digit. Yet, while the
encoder-decoder pairs on Shannon’s rate-distortion curve
7

Ldis

=

2
N

P

N/2
i=1

h(xi ) −

PN

i=N/2+1


h(g(fˆ(xi ))) ,

where {xi }N
i=1 are the test samples.
8
We plot a smoothing spline calculated over the set of points
which satisfy the constraint dW (pX , pX̂ ) ≤ P and have the minimal distortion among all points with the same rate.

are more accurate on average, no doubt that the perceptuallyconstrained encoder-decoder pairs are favorable in terms
of perceptual quality. Also, notice that at a rate of 2 bits,
the outputs of the perceptually-constrained encoder-decoder
pairs are all distinct, even though there are only 4 code
words (as can be seen for Shannon’s encoder-decoder). This
shows that the decoder effectively utilizes the noise.
Figure 7 depicts the function R(D, P ) in 3-dimensions,
as well as its cross sections along the other axis aligned
planes. In Fig. 7(a), the curved equi-rate lines show the
tradeoff between distortion and perceptual quality. This is
also apparent in Fig. 7(b), which shows cross sections along
perception-distortion planes at different rates. As can be
seen, the tradeoff becomes stronger at low bit-rates. Figure
7(c) shows the counter-intuitive tradeoff between rate and
perception. That is, at constant distortion, the perceptual
quality can be improved by increasing the rate.
4.2. Advanced Distortion Measures
The peak-signal-to-noise ratio (PSNR), which is a rescaling
of the MSE, is still the most common quality measure in
image compression. Yet, it is well-known to be inadequate
for quantifying distortion as perceived by humans (Wang
& Bovik, 2009). Over the past decades, there has been a
constant search for better distortion criteria, ranging from
the simple SSIM/MS-SSIM (Wang et al., 2003; 2004) to
the recently popular deep-feature based distortion (Johnson
et al., 2016; Zhang et al., 2018). Interestingly, the perceptual
quality along Shannon’s classical rate-distortion function is
not perfect for nearly any distortion measure (see property 3
in Theorem 1). This implies that perfect perceptual quality
cannot be achieved by merely switching to more advanced
distortion criteria, but rather requires directly optimizing
the perception index (e.g. using GAN-based schemes). This
is not to say that the function R(D, P ) is the same for
all distortion measures. The strength of the tradeoff can

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff

distortion measures does not eliminate the tradeoff. From
the decoded outputs, however, it is evident that the tradeoff
here is somewhat weaker, as minimizing distortion alone
(Shannon’s) appears a bit more visually pleasing compared
to Fig. 6 (though still with reduced variability and more blur
than the perception constrained reconstruction).
4.3. Related Work

Shannon’s

2 bits

Input

Decoded

Figure 8. Replacing the MSE with the deep feature based distortion. We repeat the experiment of Fig. 6, while replacing the
squared-error distortion with the deep-feature based distortion of
Johnson et al. (2016). Top: The rate-distortion curves elevate
when constraining the perceptual quality, demonstrating that the
use of this advanced distortion measure does not eliminate the
tradeoff. Bottom: Even with this popular advanced distortion, is
it still beneficial to compromise distortion and constrain for improved perceptual quality. Nevertheless, the tradeoff does appear
to be a bit weaker than in Fig. 6, as minimizing distortion alone
(Shannon’s curve) is now somewhat more pleasing visually.

certainly decrease for distortion criteria which capture more
semantic similarities (Blau & Michaeli, 2018).
We demonstrate this by repeating the experiment of Sec. 4.1,
while replacing the squared-error distortion by the deepfeature based distortion of (Ledig et al., 2017), i.e.,
∆(x, x̂) = kx − x̂k2 + αkΨ(x) − Ψ(x̂)k2 ,

(11)

where Ψ(x) is the output of an intermediate DNN layer for
input x. Here we take the second conv-layer output of a
4-layer DNN, which we pre-trained to achieve over 99%
classification accuracy on the MNIST test set. All training
details appear in the Supplementary Material.
Figure 8 plots 98 encoder-decoder pairs on the ratedistortion plane, trained exactly as in Fig. 6, but this time
with the loss (11) instead of MSE. As can be seen, here
too the rate-distortion curves elevate when constraining the
perceptual quality, demonstrating that the use of advanced

Our theoretical analysis and experimental validation help
explain some of the observations reported in the recent literature. Specifically, a lot of research efforts have been
devoted to optimizing the rate-distortion function (2) using deep nets (Toderici et al., 2016; 2017; Agustsson et al.,
2017; Ballé et al., 2017; Minnen et al., 2018; Li et al., 2018).
Some papers explicitly targeted high perceptual quality. One
line of works did so by choosing the distortion criterion to
be some advanced full-reference measure, like SSIM/MSSSIM (Ballé et al., 2018; Mentzer et al., 2018; Johnston
et al., 2018), normalized Laplacian pyramid (Ballé et al.,
2016) and deformation-aware sum of squared differences
(DASSD) (Rott Shaham & Michaeli, 2018). While beneficial, these methods could not demonstrate high perceptual quality at very low bit rates, which aligns with our
theory. Another line of works incorporated generative models, which explicitly encourage the distribution of outputs
to be similar to that of natural images (decreasing the divergence in (3)). This was done on an image patch level
(Rippel & Bourdev, 2017), on reduced-size (thumbnail) images (Tschannen et al., 2018; Santurkar et al., 2018), on
a full-image scale (Agustsson et al., 2018), and as a postprocessing step (Galteri et al., 2017). In particular, Tschannen et al. (2018) propose a practical method for distributionpreserving compression (P = 0 in our terminology). These
methods managed to obtain impressive perceptual quality at
very low bit rates, but not without a substantial sacrifice in
distortion, as predicted by our theory. Finally, we note that
rate-distortion analysis (with a specific distortion) has also
been used in the context of generative models (Alemi et al.,
2018), which target pX̂ = pX (i.e., P = 0). Our results
hold for arbitrary distortions and arbitrary P .

5. Conclusion
We proved that in lossy compression, perceptual quality is
at odds with rate and distortion. Specifically, any attempt
to keep the statistics of decoded signals similar to that of
source signals, will result in a higher distortion or rate. We
characterized the triple tradeoff between rate, distortion and
perception, and empirically illustrated its manifestation in
image compression. Our observations suggest that comparing methods based on their rate-distortion curves alone may
be misleading. A more informative evaluation must also
include some (no-reference) perceptual quality measure.

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff

Acknowledgements
This research was supported in part by the Israel Science
Foundation (grant no. 852/17) and by the Ollendorf Foundation.

References
Agustsson, E., Mentzer, F., Tschannen, M., Cavigelli, L.,
Timofte, R., Benini, L., and Gool, L. V. Soft-to-hard
vector quantization for end-to-end learning compressible
representations. In Proceedings of the Conference on
Neural Information Processing Systems (NIPS), 2017.
Agustsson, E., Tschannen, M., Mentzer, F., Timofte, R.,
and Van Gool, L. Generative adversarial networks for
extreme learned image compression. arXiv preprint
arXiv:1804.02958, 2018.
Alemi, A., Poole, B., Fischer, I., Dillon, J., Saurous, R. A.,
and Murphy, K. Fixing a broken elbo. In Proceedings
of the International Conference on Machine Learning
(ICML), 2018.
Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein
generative adversarial networks. In Proceedings of the
International Conference on Machine Learning (ICML),
2017.
Ballé, J., Laparra, V., and Simoncelli, E. P. End-to-end
optimization of nonlinear transform codes for perceptual
quality. In Proceedings of the Picture Coding Symposium
(PCS), 2016.
Ballé, J., Laparra, V., and Simoncelli, E. P. End-to-end
optimized image compression. In Proceedings of the
International Conference on Learning Representations
(ICLR), 2017.
Ballé, J., Minnen, D., Singh, S., Hwang, S. J., and Johnston,
N. Variational image compression with a scale hyperprior. In Proceedings of the International Conference on
Learning Representations (ICLR), 2018.
Blau, Y. and Michaeli, T. The perception-distortion tradeoff.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2018.
Blau, Y., Mechrez, R., Timofte, R., Michaeli, T., and ZelnikManor, L. The 2018 PIRM challenge on perceptual image
super-resolution. In Proceedings of the European Conference on Computer Vision Workshops (ECCVW), 2018.
Brunet, D. A study of the structural similarity image quality
measure with applications to image processing. 2012.
Cover, T. M. and Thomas, J. A. Elements of information
theory. John Wiley & Sons, 2012.

Csiszár, I., Shields, P. C., et al. Information theory and statistics: A tutorial. Foundations and Trends R in Communications and Information Theory, 1(4):417–528, 2004.
Denton, E. L., Chintala, S., Szlam, A., and Fergus, R. Deep
generative image models using a laplacian pyramid of
adversarial networks. In Proceedings of the Conference
on Neural Information Processing Systems (NIPS), 2015.
Galteri, L., Seidenari, L., Bertini, M., and Del Bimbo, A.
Deep generative adversarial compression artifact removal.
In Proceedings of the International Conference on Computer Vision (ICCV), 2017.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A., and Bengio,
Y. Generative adversarial nets. In Proceedings of the
Conference on Neural Information Processing Systems
(NIPS), 2014.
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and
Courville, A. C. Improved training of wasserstein gans.
In Proceedings of the Conference on Neural Information
Processing Systems (NIPS), 2017.
Isola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. Image-toimage translation with conditional adversarial networks.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2017.
Johnson, J., Alahi, A., and Fei-Fei, L. Perceptual losses for
real-time style transfer and super-resolution. In Proceedings of the European Conference on Computer Vision
(ECCV), 2016.
Johnston, N., Vincent, D., Minnen, D., Covell, M., Singh,
S., Chinen, T., Hwang, S. J., Shor, J., and Toderici, G.
Improved lossy image compression with priming and
spatially adaptive bit rates for recurrent networks. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2018.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
Ledig, C., Theis, L., Huszár, F., Caballero, J., Cunningham, A., Acosta, A., Aitken, A. P., Tejani, A., Totz, J.,
Wang, Z., and Shi, W. Photo-realistic single image superresolution using a generative adversarial network. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2017.
Li, M., Zuo, W., Gu, S., Zhao, D., and Zhang, D. Learning convolutional networks for content-weighted image
compression. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2018.

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff

Liu, G., Reda, F. A., Shih, K. J., Wang, T.-C., Tao, A., and
Catanzaro, B. Image inpainting for irregular holes using
partial convolutions. In Proceedings of the European
Conference on Computer Vision (ECCV), 2018.
Mathieu, M., Couprie, C., and LeCun, Y. Deep multi-scale
video prediction beyond mean square error. In Proceedings of the International Conference on Learning Representation (ICLR), 2016.
Matsumoto, R. Introducing the perception-distortion tradeoff into the rate-distortion theory of general information
sources. IEICE Communications Express, 7(11):427–431,
2018a.
Matsumoto, R. Rate-distortion-perception tradeoff of
variable-length source coding for general information
sources. IEICE Communications Express, 2018b.
Mechrez, R., Talmi, I., Shama, F., and Zelnik-Manor, L.
Maintaining natural image statistics with the contextual
loss. In Proceedings of the Asian Conference on Computer Vision (ACCV), 2018a.
Mechrez, R., Talmi, I., and Zelnik-Manor, L. The contextual
loss for image transformation with non-aligned data. In
Proceedings of the European Conference on Computer
Vision (ECCV), 2018b.
Mentzer, F., Agustsson, E., Tschannen, M., Timofte, R.,
and Van Gool, L. Conditional probability models for
deep image compression. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2018.
Minnen, D., Ballé, J., and Toderici, G. D. Joint autoregressive and hierarchical priors for learned image compression. In Proceedings of the Conference on Neural
Information Processing Systems (NIPS), 2018.
Mittal, A., Moorthy, A. K., and Bovik, A. C. No-reference
image quality assessment in the spatial domain. IEEE
Transactions on Image Processing (TIP), 21(12):4695–
4708, 2012.

Rix, A. W., Beerends, J. G., Hollier, M. P., and Hekstra, A. P.
Perceptual evaluation of speech quality (PESQ)-a new
method for speech quality assessment of telephone networks and codecs. In Proceedings of the IEEE Conference
on Acoustics, Speech, and Signal Processing (ICASSP),
2001.
Rott Shaham, T. and Michaeli, T. Deformation aware image
compression. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2018.
Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V.,
Radford, A., and Chen, X. Improved techniques for
training gans. In Proceedings of the Conference on Neural
Information Processing Systems (NIPS), 2016.
Santurkar, S., Budden, D., and Shavit, N. Generative compression. In Proceedings of the Picture Coding Symposium (PCS), 2018.
Shama, F., Mechrez, R., Shoshan, A., and ZelnikManor, L. Adversarial feedback loop. arXiv preprint
arXiv:1811.08126, 2018.
Shannon, C. E. Coding theorems for a discrete source with
a fidelity criterion. IRE Nat. Conv. Rec, 4(142-163):1,
1959.
Shoshan, A., Mechrez, R., and Zelnik-Manor, L. DynamicNet: Tuning the objective without re-training. arXiv
preprint arXiv:1811.08760, 2018.
Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.
Toderici, G., O’Malley, S. M., Hwang, S. J., Vincent, D.,
Minnen, D., Baluja, S., Covell, M., and Sukthankar, R.
Variable rate image compression with recurrent neural
networks. In Proceedings of the International Conference
on Learning Representations (ICLR), 2016.

Mittal, A., Soundararajan, R., and Bovik, A. C. Making a
completely blind image quality analyzer. IEEE Signal
Processing Letters, 20(3):209–212, 2013.

Toderici, G., Vincent, D., Johnston, N., Jin Hwang, S., Minnen, D., Shor, J., and Covell, M. Full resolution image
compression with recurrent neural networks. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017.

Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., and
Efros, A. A. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2016.

Tschannen, M., Agustsson, E., and Lucic, M. Deep generative models for distribution-preserving lossy compression.
In Proceedings of the Conference on Neural Information
Processing Systems (NIPS), 2018.

Rippel, O. and Bourdev, L. Real-time adaptive image compression. In Proceedings of the International Conference
on Machine Learning (ICML), 2017.

Van Erven, T. and Harremos, P. Rényi divergence and
kullback-leibler divergence. IEEE Transactions on Information Theory, 60(7):3797–3820, 2014.

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff

Wang, X., Yu, K., Wu, S., Gu, J., Liu, Y., Dong, C., Loy,
C. C., Qiao, Y., and Tang, X. ESRGAN: Enhanced superresolution generative adversarial networks. In Proceedings of the European Conference on Computer Vision
Workshops (ECCVW), 2018.
Wang, Z. and Bovik, A. C. Mean squared error: Love it or
leave it? a new look at signal fidelity measures. IEEE
Signal Processing Magazine, 26(1):98–117, 2009.
Wang, Z. and Simoncelli, E. P. Reduced-reference image
quality assessment using a wavelet-domain natural image
statistic model. In Human Vision and Electronic Imaging
X, volume 5666, pp. 149–160, 2005.
Wang, Z., Simoncelli, E. P., and Bovik, A. C. Multiscale
structural similarity for image quality assessment. In
Proceedings of the Thrity-Seventh Asilomar Conference
on Signals, Systems & Computers, 2003.
Wang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli,
E. P. Image quality assessment: from error visibility
to structural similarity. IEEE Transactions on Image
Processing (TIP), 13(4):600–612, 2004.
Zhang, R., Isola, P., and Efros, A. A. Colorful image colorization. In Proceedings of the European Conference on
Computer Vision (ECCV), 2016.
Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang,
O. The unreasonable effectiveness of deep features as a
perceptual metric. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR),
2018.
Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A. Unpaired
image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV), 2017a.
Zhu, J.-Y., Zhang, R., Pathak, D., Darrell, T., Efros, A. A.,
Wang, O., and Shechtman, E. Toward multimodal imageto-image translation. In Proceedings of the Conference on
Neural Information Processing Systems (NIPS), 2017b.

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff
Supplementary Material

In this supplemental, we first provide proofs for Theorems 1 and 2. We then prove the relation between the rate of an
encoder-decoder pair and the rate-distortion-perception function in (4) for a memoryless stationary source. Next, we derive
the rate-distortion-perception function R(D, P ) of a Bernoulli random variable (see Sec. 3.1), which appears in (6). In the
following section, we specify all training and architecture details for the experiments in Sec. 4. Finally, we include details
on the choice of the perceptual loss in the experiment of Sec. 4.2.

A. Proof of Theorem 1
The proof of this theorem follows closely that of its rate-distortion analogue (Cover & Thomas (2012), 2nd ed., p. 316).

Monotonicity

The value R(P, D) is the minimal mutual information I(X, X̂) over a constraint set whose size increases

with D and P . This implies that the function R(D, P ) is non-increasing in D and P .

Convexity

Here, we assume that A1 holds. That is, the divergence d(p, q) in (4) is convex in its second argument, so that

for any λ ∈ [0, 1],
d(p, λq1 + (1 − λ)q2 ) ≤ λd(p, q1 ) + (1 − λ)d(p, q2 ).

(12)

To prove the convexity of R(D, P ), we will show that
λR(D1 , P1 ) + (1 − λ)R(D2 , P2 ) ≥ R(λD1 + (1 − λ)D2 , λP1 + (1 − λ)P2 ),

(13)

for all λ ∈ [0, 1]. First, by definition, the left hand side of (13) can be written as
λI(X, X̂1 ) + (1 − λ)I(X, X̂2 ),

(14)

where X̂1 and X̂2 are defined by
pX̂1 |X = arg min I(X, X̂) s.t.

E[∆(X, X̂)] ≤ D1 , d(pX , pX̂ ) ≤ P1 ,

(15)

pX̂2 |X = arg min I(X, X̂) s.t.

E[∆(X, X̂)] ≤ D2 , d(pX , pX̂ ) ≤ P2 .

(16)

pX̂|X

pX̂|X

Since I(X, X̂) is convex in pX̂|X for a fixed pX (Cover & Thomas (2012), 2nd ed., p. 33),
λI(X, X̂1 ) + (1 − λ)I(X, X̂2 ) ≥ I(X, X̂λ ),

(17)

pX̂λ |X = λpX̂1 |X + (1 − λ) pX̂2 |X .

(18)

where X̂λ is defined by

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff

Denoting Dλ = E[∆(X, X̂λ )] and Pλ = d(pX , pX̂λ ), we have that
n
o
I(X, X̂λ ) ≥ min I(X, X̂) : E[∆(X, X̂)] ≤ Dλ , d(pX , pX̂λ ) ≤ Pλ = R(Dλ , Pλ ),
pX̂|X

(19)

because X̂λ is in the constraint set. The divergence d(p, q) is assumed to be convex in the second argument, thus
Pλ = d(pX , pX̂λ )
≤ λd(pX , pX1 ) + (1 − λ)d(pX , pX2 )
≤ λP1 + (1 − λ)P2 .

(20)

Similarly,
h
i
Dλ = E ∆(X, Xˆλ )
h h
ii
(a)
= E E ∆(X, Xˆλ )|X
h h
i
h
ii
(b)
= E λE ∆(X, X̂1 )|X + (1 − λ)E ∆(X, X̂2 )|X
(c)

= λE[∆(X, X̂1 )] + (1 − λ) E[∆(X, X̂2 )]

≤ λD1 + (1 − λ)D2 ,

(21)

where (a) and (c) are according to the law of total expectation, and (b) is by (18). Therefore, since R(D, P ) is non-increasing
in D and P , we have from (20) and (21) that
R(Dλ , Pλ ) ≥ R(λD1 + (1 − λ)D2 , λP1 + (1 − λ)P2 ).

(22)

Combining (14), (17), (19) and (22) proves (13), thus proving that R(D, P ) is convex.

Dependence on the perceptual quality

Here, we assume that A2 holds. In particular, this implies that the function

k(z) = EX∼pX [∆(X, z)] does not attain its minimum over the entire support of pX . To prove that R(·, 0) 6= R(·, ∞), let
us assume to the contrary that R(·, 0) = R(·, ∞). This implies that for any distortion level, minimizing the rate without
a constraint on perception (P = ∞), leads to perfect perceptual quality, pX̂ = pX , just like with a perfect perception
constraint P = 0. Let us examine the solution specifically at the distortion level D∗ defined by
D∗ = min E(X,X̂)∼p
pX̂|X

[∆(X, X̂)]
X,X̂

s.t. I(X, X̂) = 0.

(23)

Notice that since I(X, X̂) = 0 in this case, X and X̂ are independent, so that pX̂|X = pX̂ . Therefore
D∗ = min E(X,X̂)∼pX p [∆(X, X̂)]
pX̂

X̂

= min EX̂∼p [EX∼pX [∆(X, X̂)]]
pX̂

X̂

= min EX̂∼p [k(X̂)].
pX̂

X̂

(24)

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff

Clearly, the pX̂ which minimizes (24) cannot assign positive probability outside the set where k(z) attains its minimal value.
Namely, the support of pX̂ must be contained in the set S defined by
S = {z ∈ arg min k(z̃)}.

(25)

z̃

But since our encoder-decoder pair achieves perfect perceptual quality, i.e. pX̂ = pX , this implies that support{pX } ⊂ S,
contradicting Assumption A2.

B. Proof of Theorem 2
Assume the MSE distortion, and consider any (R, D) pair on Shannon’s classic rate-distortion function (corresponding to
R(D, ∞) on the rate-distortion-perception function), and the encoder-decoder mapping pX̂|X which achieves this (R, D)
pair. We will prove the theorem by explicitly constructing a modified encoder-decoder, which achieves perfect perceptual
quality and has only twice the distortion. To do this, we concatenate a post-processing mapping pX̃|X̂ to produce a new
decoded output X̃ by drawing from the posterior distribution pX|X̂ . That is,
pX̃|X̂ (x̃|x̂) = pX|X̂ (x̃|x̂) =

pX̂|X (x̂|x̃)pX (x̃)
pX̂ (x̂)

.

(26)

The distribution pX̃ of this new output X̃ is identical to the distribution of the source signal pX , as
Z
pX̃ (z) =

Z
pX̃|X̂ (z|x̂)pX̂ (x̂)dx̂ =

pX|X̂ (z|x̂)pX̂ (x̂)dx̂ = pX (z).

(27)

Therefore, d(pX , pX̃ ) = 0, showing that it achieves perfect perceptual quality. The MSE distortion of the modified
encoder-decoder is given by
D̃ = E[kX − X̃k2 ]
= E[kXk2 ] − 2E[X T X̃] + E[kX̃k2 ]
(a)

= E[kXk2 ] − 2E[E[X T X̃|X̂]] + E[E[kX̃k2 |X̂]]

(b)

= E[kXk2 ] − 2E[kE[X|X̂]k2 ] + E[E[kXk2 |X̂]]

= E[kXk2 ] − 2E[kE[X|X̂]k2 ]) + E[kXk2 |]
= 2(E[kXk2 ] − E[kE[X|X̂]k2 ])
= 2 E[kX − E[X|X̂]k2 ]
(c)

= 2 E[kX − X̂k2 ] = 2D,

(28)

where in (a) we used the law of total expectation, in (b) we used the fact that X and X̃ are independent given X̂ and both
have distribution pX , and in (c) we used that fact that X̂ = E[X|X̂] as otherwise it would not have lied on the rate-distortion
curve (replacing X̂ by E[X|X̂] would lead to a lower MSE without increasing the rate).
The mutual information between the source X and the modified decoded signal X̃ satisfies
I(X; X̃) ≤ I(X; X̂),
due to the data processing inequality for the Markov chain X → X̂ → X̃.

(29)

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff

Putting it together, we get
R(D, ∞) = min {I(X, X̂) : E[∆(X, X̂)] ≤ D}
pX̂|X

(f)

≥ I(X, X̃)
(g)

≥ min {I(X, X̂) : E[∆(X, X̂)] ≤ D̃, d(pX , pX̂ ) ≤ 0}
pX̂|X

= R(D̃, 0)
(h)

≥ R(2D, 0),

(30)

where (f) is due to (29), (g) is since pX̃|X is in the constraint set, and (h) is justified by (28) and the fact that R(D, P ) is
non-increasing in D (see Theorem 1). This proves that R(D, 0) ≤ R( 12 D, ∞).

C. Perception aware lossy compression of a memoryless stationary source
We now prove that when compressing a memoryless stationary source with average distortion D and average perception
index P , the rate is lower bounded by R(D, P ). This proof follows closely that of its rate-distortion analogue (Cover &
Thomas (2012), 2nd ed., p. 316).
Assume a memoryless stationary source. Given a source sequence X n comprising i.i.d. variables X1 , . . . , Xn with
distribution pX , the encoder fn constructs an encoded representation with rate R as fn : X n → {1, 2, . . . , 2nR }. The
decoder gn outputs an estimate X̂ n of X n as gn : {1, 2, . . . , 2nR } → X̂ n . We are interested in the the average distortion of
Pn
Pn
the reconstructions, n1 i=1 ∆(Xi , X̂i ), and in their average perceptual quality, n1 i=1 d(pXi , pX̂i ). Assume that
n

1X
∆(Xi , X̂i ) ≤ D,
n i=1

n

1X
d(pXi , pX̂i ) ≤ P.
n i=1

Then
(a)

nR ≥ H(fn (X n ))
(b)

≥ H(fn (X n )) − H(fn (X n )|X n )

= I(X n ; fn (X n ))
(c)

≥ I(X n , X̂ n )

= H(X n ) − H(X n |X̂ n )
(d)

=

n
X

H(Xi ) − H(X n |X̂ n )

i=1
n
n
X
(e) X
=
H(Xi ) −
H(Xi |X̂ n , Xi−1 , . . . , X1 )
(f)

≥
=

i=1

i=1

n
X

n
X

i=1
n
X
i=1

H(Xi ) −

i=1

I(Xi , X̂i )

H(Xi |X̂i )

(31)

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff
(g)

≥

n
X



R E[∆(Xi , X̂i )], d(pXi , pX̂i )

i=1

=n

n

1X 
R E[∆(Xi , X̂i )], d(pXi , pX̂i )
n i=1

(h)

≥ nR

!

!
n
n
1X
1X
E[∆(Xi , X̂i )],
d(pXi , pX̂i )
n i=1
n i=1

(i)

≥ n R(D, P ),

(32)

where (a) is since the size of the range of fn is 2nR , (b) is since H(fn (X n )|X n ) > 0, (c) is from the data-processing
inequality, (d) is since Xi are independent, (e) is from the chain rule of entropy, (f) is since conditioning reduces entropy, (g)
is from the definition of R(D, P ) in (4), (h) is from the convexity of R(D, P ) (see Theorem 1) and Jensen’s inequality, and
(i) is from (31) and the fact that R(D, P ) is non-increasing in D, P (see Theorem 1). This proves that the rate of any encoderPn
Pn
decoder pair having average distortion n1 i=1 ∆(Xi , X̂i ) = D and average perceptual quality n1 i=1 d(pXi , pX̂i ) = P ,
is lower-bounded by R(D, P ), the rate-distortion-perception function evaluated at D, P .
To prove that the rate-distortion-perception function describes the optimal rate at distortion level D and perceptual quality
P , we would also have to prove that R(D, P ) is achievable, which we leave for future work. Yet, the proof that R(D, P )
lower-bounds the rate is sufficient for concluding that a tradeoff between rate, distortion and perception necessarily exists.
Specifically, in Theorem 1 we prove that (subject to assumptions) the rate-distortion curve elevates when constraining for
perceptual quality, i.e. R(·, 0) > R(·, ∞). Now, Shannon’s rate-distortion curve R(·, ∞) is known to be achievable (Cover
& Thomas (2012), 2nd ed., p. 318) and thus describes the optimal rate RS when not constraining the perceptual quality.
As shown above, R(·, 0) lower-bounds the rate RP when constraining for perfect perceptual quality. Combining these, we
get that RP > RS , indicating that constraining for perceptual quality necessarily leads to an increase in rate (for constant
distortion level), thus illustrating the rate-distortion-perception tradeoff.

D. Derivation of the rate-distortion-perception function R(D, P ) of a Bernoulli source
Assume that X ∼ Bern(p) with p ≤ 12 . We seek a conditional distribution pX̂|X , which we parameterize by a, b as
P (X̂ = 0|X = 0) = a,

(33)

P (X̂ = 0|X = 1) = b,

(34)

that solves the rate-distortion-perception problem
R(D, P ) = min I(X, X̂) s.t. E[∆(X, X̂)] ≤ D, d(pX , pX̂ ) ≤ P.
a,b

(35)

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff

Here we concentrate on the case where ∆(·, ·) is the Hamming distance, and d(·, ·) is the total-variation (TV) divergence.
The mutual information term I(X, X̂) is given by
X

I(X, X̂) =

P (X = x, X̂ = x̂) log

P (X = x, X̂ = x̂)

!

P (X = x)P (X̂ = x̂)



1−b
b
p
= − a(1 − p) log (1 − p) + p − (1 − a)(1 − p) log (1 − p) +
a
1−a


a

1−a
− bp log
(1 − p) + p − (1 − b)p log
(1 − p) + p ,
b
1−b
x,x̂∈{0,1}



(36)

the Hamming distance term is given by
dH (X, X̂) = P (X = 0, X̂ = 1) + P (X = 1, X̂ = 0) = (1 − a)(1 − p) + bp,

(37)

and the TV divergence term is given by
dTV (pX , pX̂ ) =

1
2

X

|pX (z) − pX̂ (z)|

z∈0,1

= 21 (|P (X = 0) − P (X̂ = 0)| + |P (X = 1) − P (X̂ = 1)|)
= |(1 − a)(1 − p) − bp|.
Solution for P = ∞ (Shannon’s rate-distortion problem)

(38)
The function R(D, ∞) for Shannon’s classic rate-distortion

problem is given by (see (Cover & Thomas, 2012), 2nd ed., p. 308)

H (p) − H (D)
b
b
R(D, ∞) =
0

0 ≤ D ≤ p,

(39)

D > p,

where Hb denotes the binary entropy Hb (z) = −z log(z) − (1 − z) log(1 − z). This optimal solution is obtained by setting
the parameters a, b to

aS (D) =


 (1−D)(1−p−D)

D ≤ p,

(1−p)(1−2D)

1

Solution for finite P and I(X, X̂) > 0

bS (D) =

D > p,


 D(1−p−D)

D ≤ p,

1

D > p.

p(1−2D)

(40)

We now move on to incorporate the additional perception constraint d(pX , pX̂ ) ≤

P . First, notice that the distortion constraint E[∆(X, X̂)] ≤ D is always active when I(X, X̂) > 0, since I(X, X̂) = 0 is
achievable for any P when D is not an active constraint9 . From (37), the fact that dH (X, X̂) = D implies that
b=
9

D − (1 − a)(1 − p)
.
p

(41)

We can always set pX̂|X (X̂ = x̂|X = x) = pX (x̂) (i.e. a random draw from pX disregarding the given input x), which satisfies

dTV (X, X̂) = 0 and leads to I(X, X̂) = 0 since X and X̂ are independent in this case. Only a constraint on the distortion can prevent
this solution from being viable.

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff

Substituting (41) into (38), we get that
dTV (pX , pX̂ ) = |2(1 − a)(1 − p) − D|.

(42)

Therefore, the constraint dTV (pX , pX̂ ) ≤ P is satisfied when
− P ≤ 2(1 − a)(1 − p) − D ≤ P

⇒

1−

D−P
D+P
≤a≤1−
.
2(1 − p)
2(1 − p)

(43)

Below, we show that the lower constraint of (43) is never active (see J1). The upper constraint is obviously active only when
aS (D) of (40) does not satisfy the upper bound in (43), which happens when
1−

D−P
(1 − D)(1 − p − D)
<
2(1 − p)
(1 − p)(1 − 2D)

⇒

D>

P
, D1 .
1 + 2P − 2p

(44)

Therefore, when D ≤ D1 the solution is independent of P and is given by (39). When D > D1 , the constraint
dTV (pX , pX̂ ) ≤ P is active, the upper constraint of (43) is active, and thus
a=1−

D−P
,
2(1 − p)

(45)

and by substituting into (41) we also get
b=

D+P
.
2p

(46)

Note that in (44) we assumed D ≤ 21 , below we will justify that this is always the case in this region (see J2).
Now, substituting a, b from (45), (46) back into (36) we get
!
!
D−P
1 − p − D−P
D−P
2
2
+(
) log
(1 − p)(1 − p + P )
2
(1 − p)(p − P )
!
!
D+P
D+P
p− 2
D+P
D+P
2
+(
) log
+ (p −
) log
2
p(1 − p + P )
2
p(p − P )




q−α
α
=(q − α) log
+ α log
q(q + P )
q(p − P )




β
p−β
+ β log
+ (p − β) log
p(q + P )
p(p − P )

D−P
) log
I(X, X̂) =(1 − p −
2

where q = 1 − p, α =

D−P
2

and β =

D+P
2

(47)

. This can be further simplified to obtain

I(X, X̂) = 2Hb (p) + Hb (p − P ) − Ht (α, p) − Ht (β, q),

(48)

where Ht (p1 , p2 ) is the entropy of a ternary random variable (taking values in a three element alphabet) with probabilities
p1 , p2 , 1 − p1 − p2 .

Solution for finite P and I(X, X̂) = 0 The function R(D, P ) is non-increasing in D (see Theorem 1), and will reach
R(D, P ) = 0 for a = b since in this case X̂ and X are independent. From (45) and (46), this happens when
1−

D+P
D−P
=
2(1 − p)
2p

⇒

D = 2p(1 − p) + (2p − 1)P = 2pq + (p − q)P , D2 ,

(49)

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff

where for D = D2 we get
a = b = (1 − p) + P.

(50)

From this point onward, the solution is fixed, as mutual information I(X, X̂) is non-negative and we cannot further decrease
the objective of (35).
Overall solution Putting all the pieces together, the overall solution for P < p is



Hb (p) − Hb (D)

D+P
R(D, P ) = 2Hb (p) + Hb (p − P ) − Ht ( D−P
2 , p) − Ht ( 2 , q)



0

D ≤ D1
(51)

D1 < D ≤ D2
D2 < D

where D1 and D2 are defined in (44) and (49), respectively. For P ≥ p, the solution is independent of P and is given by the
solution to Shannon’s classic rate-distortion curve for a Bernoulli source in (39) (see justification in J3 below).
Additional justifications
J1

The solution aS (D) in (40) does not satisfy this lower constraint of (43) when
1−

When P <
D>

1−2p
2

P
2p+2P −1 .

this happens for D <

However, since

(1 − D)(1 − p − D)
D+P
>
.
2(1 − p)
(1 − p)(1 − 2D)

P
2p+2P −1

P
2p+2P −1

< 0, which never occurs as D ∈ [0, 1]. When P ≥

> D1 =

P
1+2P −2p

for all p ≤

1
2

(52)
1−2p
2

this happens for

(which is our assumption), the upper constraint

of (43) will always become active before the lower constraint.
J2

Taking the derivative of D2 = 2p(1 − p) + (2p − 1)P with respect to p we obtain
∂D2
= 2 − 4p + 2P
∂p

(53)

which is non-negative since p ≤ 12 . Thus, D2 is increasing in p for all P > 0, and its largest value in the range p ∈ [0, 12 ],
which is D2 = 21 , is obtained at p = 12 . Thus, in the region where D1 < D ≤ D2 , it is ensured that D ≤ 12 .
J3

Taking the derivative of D1 =

P
1+2P −2p

with respect to P we obtain
1 − 2p
∂D1
=
∂P
(1 + 2P − 2p)2

(54)

which is non-negative for p ≤ 12 , thus D1 is non-decreasing in P . It is easy to see from (49) that D2 in non-increasing
in P (for p ≤

1
2 ).

Thus, D1 (P ) = D2 (P ) for a single P , which is P = p. For any P ≥ p, there are no D satisfying

D1 < D ≤ D2 .

E. Architecture and training parameters for the experiments in Sec. 4
The architecture of the encoder, decoder and discriminator nets used for compressing (and decompressing) the MNIST
images in Sec. 4 is detailed in Table 1. The optimization objective is given in (10), where ∆(x, x̂) is the squared-error
distortion in Sec. 4.1, and a combination of the squared-error and the “perceptual loss” of Johnson et al. (2016) in Sec. 4.2.

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff
Table 1. Encoder, decoder, and discriminator architectures. FC is a fully-connected layer, Conv/ConvT is a convolutional/transposedconvolutional layer with “st” denoting stride, BN is a batch-norm layer, and l-ReLU is a leaky-ReLU activation.
Encoder
Size
Layer
28 × 28 × 1
Input
784
Flatten
512
FC, BN, l-ReLU
256
FC, BN, l-ReLU
128
FC, BN, l-ReLU
128
FC, BN, l-ReLU
dim
FC, BN, Tanh
dim
Quantize

Decoder
Size
dim
128
512
4 × 4 × 32
11 × 11 × 64
25 × 25 × 128
28 × 28 × 1

Layer
Input
FC, BN, l-ReLU
FC, BN, l-ReLU
Unflatten
ConvT (st=2), BN, l-ReLU
ConvT (st=2), BN, l-ReLU
ConvT (st=1), Sigmoid

Discriminator
Size
Layer
28 × 28 × 1
Input
14 × 14 × 64 Conv (st=2), l-ReLU
7 × 7 × 128
Conv (st=2), l-ReLU
4 × 4 × 256
Conv (st=2), l-ReLU
4096
Flatten
1
FC

Table 2. Encoder output dimension dim, quantization levels L, and tradeoff coefficients λ used for training the encoder-decoder pairs in
the experiments of of Sec. 4
dim
2
3
4
4
4
4
4
4
4

L
2
2
2
3
4
6
8
11
16

λ
0, 2, 2.5, 3, 3.5, 4, 4.3, 4.6, 5, 5.5, 6, 8, 10, 15, 20
0, 2, 2.5, 3, 3.5, 4, 4.3, 4.6, 5, 6, 8, 9, 10
0, 2, 2.5, 3, 3.5, 4, 4.3, 4.6, 4.8, 4.9, 5, 6, 8, 10
0, 2, 2.5, 3, 3.5, 4, 4.3, 4.6, 5, 6, 8, 10
0, 2, 2.5, 3, 3.5, 4, 4.3, 4.6, 5, 6, 8, 10
0, 2, 2.5, 3, 3.5, 4, 5, 10
0, 2, 2.5, 3, 4, 5, 6, 10
0, 2, 2.5, 3, 4, 5, 6, 10
0, 2, 2.5, 3, 4, 5, 6, 10

The encoder output dimension dim, the number of quantization levels L, and values of the tradeoff coefficient λ in (10)
used for training the 98 encoder-decoder pairs appear in Table 2. The distortion term in (10) was also multiplied by a
constant factor of 10−3 for the MSE term (in Sec. 4.1 and Sec. 4.2) and factor of 5 × 10−5 for the perceptual loss (in
Sec. 4.2). For each dim and L, an encodoer-decoder with λ = 0 (only distortion, no adversarial loss) was trained for
25 epochs. The other encoder-decoder pairs with λ > 0 continued training from this point for another 25 epochs. The
ADAM optimizer was used with β1 = 0.5, β2 = 0.9. Batch size was 64. Initial learning rates were 10−2 /2 × 10−4 for the
encoder-decoder/discriminator updates in Sec. 4.1, and 5 × 10−3 /2 × 10−4 for the encoder-decoder/discriminator updates in
Sec. 4.2. These learning rates decreased by

1
5

after 20 epochs. The convolutional/transposed-convolutional layers filter size

(in the decoder and discriminator) was always 5, except for the last convolutional layer in the decoder where the filter size
was 4. No padding was used in the decoder, and a padding of 2 was used in each convolutional layer of the discriminator.
The quantization layer (last encoder layer) follows Mentzer et al. (2018). Here, the bin centers C = {c1 , . . . , cL } are fixed
and evenly spaced in the interval [−1, 1]. Denoting by zi the output of the encoder unit i before quantization (after the Tanh
activation), the encoder output ẑi in the forward pass is given by nearest-neighbor assignment, i.e. ẑi = arg mincj kzi − cj k.
To compute the gradients in the backward pass, we use a differential “soft” assignment
z̃i =

L
X
j=1

exp(−σkzi − cj k1 )
cj ,
PL
l=1 exp(−σkzi − cl k1 )

(55)

where we use σ = 2/L. Uniformly distributed noise U(− a2 , a2 ) is added to the encoder output before it is passed on to the
decoder, with a = 2/(L − 1).

Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff
Table 3. Architecture of the pre-trained MNIST digit classification net used with the perceptual loss of Johnson et al. (2016) in the
experiment in Sec. 4.2. FC is a fully-connected layer, Conv is a convolutional layer with k denoting the kernel size, MP is a max-pooling
layer with w denoting the window size over which the maximum is taken.
Size
28 × 28 × 1
12 × 12 × 10
4 × 4 × 20
320
50
10

Layer
Input
Conv (k=5), MP (w=2), l-ReLU
Conv (k=5), Dropout, MP (w=2), l-ReLU
Flatten
FC, ReLU, Dropout
FC, Softmax

F. The perceptual loss in the experiment in Sec. 4.2
In the experiment of Sec. 4.2, the distortion term of the optimization objective (10) is taken as a combination of the
squared-error and the perceptual loss of Johnson et al. (2016). We use this combination since minimizing the perceptual loss
alone does not lead to pleasing results (Fig. 9), and is commonly used in combination with an additional distortion term,
e.g. `2 /`1 /contextual loss (Ledig et al., 2017; Mechrez et al., 2018a;b; Liu et al., 2018; Wang et al., 2018; Shama et al.,
2018; Shoshan et al., 2018). As shown in (11), this perceptual loss is in essence the squared-error in the deep-feature space
of a pre-trained convolutional net. The standard pre-trained net used with the perceptual loss is the VGG net (Simonyan &
Zisserman, 2014), which is trained on natural images from the ImageNet dataset, and is not appropriate for assessing the
similarity between MNIST digit images. We therefore pre-train a simple net for classifying MNIST digit images, which
achieves over 99% accuracy. The architecture of this pre-trained net is presented in Table 3. The perceptual loss in our
experiment is taken as the MSE on the outputs of the second convolutional layer, as this leads to the best perceptual quality
(see Fig. 10). We trained with stochastic gradient descent for 30 epochs with a batch size of 30. The learning rate was
initialized to 10−2 and decreased by

1
5

after 20 epochs.

(a)

(b)

(c)

Figure 9. Minimizing the perceptual loss alone (without an additional MSE term). The perceptual loss is evaluated on the outputs of:
(a) the first convolutional layer, (b) the second convolutional layer, and (c) the first fully-connected layer.

(a)

(b)

(c)

(d)

Figure 10. Visual comparison of assessing the perceptual loss on the outputs of different layers. (a) Minimizing the MSE alone.
(b)-(d) Minimizing a combination of the MSE and percpetual loss, where the perceptual loss is evaluated on the outputs of: (b) the first
convolutional layer, (c) the second convolutional layer, and (d) the first fully-connected layer. The weights of each term (MSE, perceptual)
in the loss were optimized for visual quality.

