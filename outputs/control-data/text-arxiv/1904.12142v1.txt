Guarantees on Nearest-Neighbor Condensation heuristics∗

arXiv:1904.12142v1 [cs.CG] 27 Apr 2019

Alejandro Flores-Velazco†

Abstract The problem of nearest-neighbor (NN) condensation aims to reduce the size of a training set
of a nearest-neighbor classifier while maintaining its
classification accuracy. Although many condensation
techniques have been proposed, few bounds have been
proved on the amount of reduction achieved. In this
paper, we present one of the first theoretical results for
practical NN condensation algorithms. We propose two
condensation algorithms, called RSS and VSS, along
with provable upper-bounds on the size of their selected
subsets. Additionally, we shed light on the selection size
of two other state-of-the-art algorithms, called MSS and
FCNN, and compare them to the new algorithms.
1

Introduction

In machine learning, classification involves a training set
P ⊂ Rd of n labeled points in Euclidean space. The label
l(p) of each point p ∈ P indicates the class to which the
point belongs to, partitioning of P into a finite set of
classes. Given an unlabeled query point q ∈ Rd the goal
of a classifier is to predict q’s label using P .
The nearest-neighbor (NN) rule is among the bestknown classification techniques [6]. It classifies a query
point q with the label of its closest point in P , according to some metric. Throughout, we will assume the
Euclidean `2 metric. Despite its simplicity, the NN rule
exhibits good classification accuracy both experimentally and theoretically [13, 4, 5]. However, it’s often
criticized due to its high space and time complexities.
This raises the question of whether it is possible to replace P with a significantly smaller subset without affecting the classification accuracy under the NN rule.
This problem is called nearest-neighbor condensation.
In this paper we propose two new NN condensation algorithms and analyze their worst-case performance.
1.1

Preliminaries

For any point p ∈ P , define an enemy of p to be any
point in P of different class than p. The nearest enemy
(NE) of p, denoted ne(p), is the closest such point, and
its distance from p, called the NE distance, is denoted as
dne (p) = d(p, ne(p)). Similarly, denote the NN distance
as dnn (p) = d(p, nn(p)). Define the NE ball of p to be
∗ Research

supported by NSF grant CCF–1618866.
of Maryland, College Park, afloresv@cs.umd.edu
‡ University of Maryland, College Park, mount@umd.edu
† University

David Mount‡

the ball centered at p with radius dne (p). Let κ denote
the number of distinct NE points of P .
A point p ∈ P is called a border point if it is incident
to an edge of the Delaunay triangulation of P whose
opposite endpoint is an enemy of p. Otherwise, p is
called an internal point. By definition, the border points
of P completely characterize the portion of the Voronoi
diagram that separates Voronoi cells of different classes.
Let k denote the number of border points of P .
1.2

Related work

A subset R ⊆ P is said to be consistent if for all p ∈ P
its nearest neighbor in R is of the same class as p. Intuitively, R is consistent if and only if every point of P is
correctly classified using the NN rule over R. Formally,
nearest-neighbor condensation involves finding an (ideally small) consistent subset of P [9].
Other criteria for condensation have been studied in
the literature. One such criterion is known as selectivity [12]. A subset R ⊆ P is said to be selective if and
only if for all p ∈ P , its nearest neighbor in R is closer
to p than its nearest enemy in P . Clearly selectivity implies consistency, as the NE distance in R of any point
of P is at least its NE distance in P . Note that neither
consistency or selectivity imply that every query point
of Rd is correctly classified, just those in P .
The strongest criteria, known as Voronoi condensation, consists of selecting all border points of P [16].
This guarantees the correct classification of any query
point in Rd . In contrast, a consistent subset only guarantees correct classification of P . For the case when
P ⊂ R2 , an output-sensitive algorithm was proposed [3]
for finding all border points of P in O(n log k) worst-case
time. Unfortunately, it is not known how to generalize
this algorithm to higher dimensions, and a straightforward approach suffers from the super-linear worst-case
size of the Delaunay triangulation.
In general, it has been shown that the problems
of computing consistent and selective subsets of minimum cardinality are both NP-complete [17, 18, 11].
Thus, most research has focused on practical heuristics. For comprehensive surveys, see [14, 15, 10].
CNN (Condensed Nearest-Neighbor) [9] was the first
algorithm proposed for computing consistent subsets.
Even though it has been widely cited in the literature,
CNN suffers from several drawbacks: its running time
is cubic in the worst-case, and the resulting subset is

● ●●
●
●●
● ●

●

●

●
●

●
●

●
● ●
●

● ● ●
●● ●

●
●●
●

● ●
●●

●
● ●
●

●

●●
●
●●

●●
●
●
●●
●

● ●●
●
●
● ●●
●●
●

●
●
●

●
●●

●
●
●
●●
●●
● ● ●
●
●
●●
●

●●

●

● ●
●

●

●

●

●

●

●●

●
●●

●●
●
●

●●
●
●
●

●
●● ●
●

●

●●
●

●

●

●
●●

●●
●
●
●

●
● ●
●
●●
●●

●

●

●
●

●
●

● ●
●●
●
●

●
●
●● ●
●●

●
●

●
● ●

●

● ● ●
●
●

●
●●●● ●
●●
●

●
●●

●

●
●
● ●

●
●

●

●

● ●
●

●

●●

●
●
●
●

●
●

●
● ● ●●●
●●
●
●

●
● ● ●
●●

●●
●
●

●●
● ●

●
●

●
●

●
●
●

●

●

●
●
●

●
●●
●
● ●
●
●

●
●
● ●
●●
●
●
●●
●
●

●

●●
●
●
●

●

●●
● ●

●
●●

●●
●●

●●
●●

●
●

●●

● ●
●●
●●
●
●●

●
●●
●

●
●

●●
●
● ●●
●

●
●
●●
●
●

●
● ●
● ●
●●

● ● ●
● ●
●

●
●

●
●

●
●●●● ●
●●
●

●
●

●
● ● ●
●
●

●
●

●
●
●
●

●●
●
●
●
●●
●

●

●●

●●
●

●
●●
●● ●

●
●

●●
●
● ●●
●

●

●●
●●
●
●
●

●
●

●
● ●
● ●
●●

●
● ● ●
●●
● ● ●
● ●
●

(d) RSS (233 pts)

●●
●

●●
●
●
●
●●
● ●
● ●

●

●

●

●

●

●●
●

●●
●
●

●

●●
●
●●

●●
●●

●●

●
●
●●
●●

●●

●●
●

●
●
●
●

●

(c) MSS (272 pts)

●

●

●●

●●
●

●●

●●
●

●
● ●
●●●● ●
●

●
● ●
●●

●●
●●
●●
●
●
●

●●
●
●
●

●

●
●

●●
●

●
● ●

●
●

●

●●
●

●
●

● ●
●
●
● ●

●●
●
●
●
●●
●

●●
●●

● ● ●
●
● ●
● ●
●

●●

●●
●●

●●
●
●
●●●
●
● ●●
●

●

(b) FCNN (222 pts)

●
● ●
●

●●
●

●
●
●●
●● ●
●
●
●
●●
●●
●
●

●
●

●
●●
●

● ●

● ●
●●●
●
●●

● ● ●
●● ●

●
●
●●

●●

●
●

●
●
●
●

● ●
● ●

●

●

●
●●

●●

●
●●
●
●

●● ●
●
● ●●
●

●

●
● ● ●
● ●

●

●
●
● ●
● ●
● ●

●
●

●

●●

●●
●
●
●
●●
●

●●
●
●●
●

●

●●

●●
●
●●
●●
●●
●

●●
●●

●
●

●●

●
● ●
●●

●●
●●
●●
●
●
●

●●
●
●
●

●●●
●

●
●●
●
●
●
●
●
●●

●●
●

●

●
● ●

●
●
●
●

●
●

●●

●●
●
●
●
●
●●
●
●

●
●

●

● ●

●●
●●

● ● ●
●
● ●
● ●
●

● ●
●
●
●

●
●●

●●

●
●
●●
●
●

●

●

● ●●

●●

●
●

●
●●
●

●●

●●●
●
●●
●
●●
●
●
●

●●
●
●
●
●
●●
●
●

● ● ●
●● ●

●
●●

● ●

●
● ●

●
●
●●

● ●
● ●

●
● ● ●
● ●

●

●

●

●
●●
●
● ●

●
● ●
●●
● ●

●

●
●

● ●
●●

●

(a) Set P (104 pts)

●
●
●●
● ● ●
●
●●●
●
●●
● ●
●

●
●
●●
● ●

●
● ●
●

●

● ●
● ● ●●●● ●●
● ●

●

●

●
●
●

●●

●●

●
● ● ●
●
● ●

●
●

●
●
●

●
●

●
●

●
●

●
●

●
●●●● ●
●●
●

(e) VSS (233 pts)

●
●

●

●
●
● ●
●
● ●●
●
●
● ● ● ●
● ● ●
●●
●
●
●● ●
● ●
● ●● ●
●
●●
●
●●
●●
●
● ●●
●
●
●
● ● ●● ● ●● ●
●● ●●
●
●
●
● ●●
●●● ●
●
●●
●
●● ●● ●
● ●●
●●●●
● ● ●●
●● ● ●
●
● ●●
●
●
● ●
●●
●
●●
●
●
● ●
● ● ●● ●●
●
●
●●
●●● ●●
●
●●● ● ● ● ● ●
●
● ●●
●
●
●
●● ●
●
● ●● ●
● ●
● ● ●●●● ●
●
● ●
● ●
●●
●●● ●
●
●●●
●
●●● ● ● ● ● ●
●
●
●●
● ● ●● ●
●
●●●●●
● ●●● ● ●●●●
● ●
●●●
●
●
●
●●●●●
● ● ●●
●
● ● ●
●
●●
●●●● ● ●
●●
● ●● ●
●● ●
●
● ● ● ●
● ●
● ●
●
●
●
●
●
●
● ●
●●
●
●
●
●
●
●
●
●● ●
●
●
●
●
● ● ●● ●
●
●
●
●
●
●
●
●●● ● ●
●
●● ●
●
●●●
●●●●
●
●●
● ●●
●
●●● ●
●
● ●
● ●● ●● ●
● ● ●●
●
●
●
●
● ●
●
●
●●
●
●
●
●
●
● ●
●
●
●
●
●
●
●●● ● ● ●
●●
●●● ●● ●●●
● ●
●●● ●
● ●●
● ● ●● ●● ●
●● ●
●
●●
●●
●
●●
● ●●
●
●●
●
●●● ● ●● ●
●
●
●
●
●
●
●
● ● ● ●●●
●
●
●
●
●● ●●
● ●
●●●
●●
●
●●
●
●
● ●●●
●
● ●
●
●●
●
● ●●
●
●
●
●●
●
●
●
●●
●
● ●
● ● ●●
●●
●● ●● ●●● ●●
●●
●
● ● ● ● ●● ●
●
●●
● ● ●
●●
●●
●
●
●
●
● ●●●
●●
● ●
● ●
● ●
●●
●
●
●●● ●
●
● ● ● ●●
● ●● ●
● ● ●● ● ●
● ●●
●
● ● ● ●●●● ● ● ● ●●
●●●●● ●●
●●
●
●
●
●●
● ●
● ●●
●●
●●
● ●
●
●
●
●
●
● ●●
●
●● ●
●
● ●
●
●
● ● ●
●●
●
●
●
●
●
●
●
●● ●
● ●● ●
●
●
●
●
●
●
●●●● ● ●
●●
● ●
● ●●●
● ●
●
●
●
●●●
●●●● ● ● ● ●
● ●
●
● ●● ●
● ●● ●●●● ● ●
●
●
●
● ● ●● ●
●
●
● ●
●
●
● ● ●
●
●
●
●
●●
●
●●● ●
●
●
●
● ●●
●●●
●
●
●●
●
● ● ●● ●
●
●
●
●●
●
●●
●●
● ●●● ● ● ●
●
●● ●
●● ● ●
●●
●
●●●
●●
●
● ● ●●● ●●● ● ●
●●
●
● ●●●● ●
●
●
● ● ●●
●
● ● ●●
●
● ●
● ●●● ●
●●● ● ●
● ●
●
●
● ●
●●
●● ● ● ●
● ●
● ●
●● ●● ●
●
●● ● ● ●
● ●● ●
●●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
● ●● ● ● ● ●
●
●●● ●●
●●
● ●
● ●
●
● ● ●●
● ●●
●
●
●● ● ●
●
●●
●
●● ●
●● ●
●●●
●
●
●
●
●
●
●●
●●
●
●
●●
●
●●
●
●
●●●
● ●
●
● ●
●●
●●
● ● ●
● ●●
●
●●
●●● ● ●●●● ●
●
●
●
●●● ● ●
●
●
●●
●
●● ●●
●● ●
● ●
●
●●●
●
● ● ●
● ●●
●●
●●
●● ● ●
●
●
● ●
●
●
●
●

●

●

●

●

●
● ●
●
●
● ●●
●
● ● ● ●
● ● ●
●●
●
●
● ●
●● ●
● ●● ●
●●
●
●●
●●●
●
● ●●
●
●
●
●
● ● ●● ● ●● ●
●● ●●
●
●● ●
●
●
● ●●
●●● ●
● ●
●●
●●● ●
●
●
●
●
● ●
●●●●
●
●
●●
●
● ●●
●● ●● ●●
●
●●●
●●●
●
● ●
● ● ● ●●● ●
● ●● ●●
●●
●
●
●●
●
● ●
●
●
●
●
● ●●
●●●
●
●
●● ●
●●
● ●●●●
●
●
●
●
● ●
●
●●
●
●
●
● ●
●●
●●● ● ●●
●
●●●
● ●●●
● ●
●
●
●● ●
●
●
● ●●
● ● ● ●● ●
● ● ● ●●●
●●●
● ●●● ●● ●●●●
●
● ●
●●
●●●●●
● ● ●● ● ●
●
● ● ●
●
●●
●
● ●
● ●● ●
●●
●
●● ●
● ●
●●● ●
●
● ●
●
●
●
●
●●
●
●
●●
● ● ●
●
●
●
●
●
●
●● ●
●
●●
●
●●
●● ●
●●
●
●
●
● ●
●
●
●● ●
●
●
● ● ●●
●●
●
●●●
●●●●
● ●
●
●
●
●● ●
●
●
● ●● ●
●●●
●● ●
● ●●
●
●● ●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
● ●
●
●
●●● ● ● ●
●
●●
●
●● ● ●● ●●●●
●
●
●●
●●
● ●●●
●
●● ● ●
●● ●●● ●
●●●
●
●
●●
● ●
● ●●
●●
●
● ●●●
●
●
●● ●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●● ●
● ●●
●
●●●
● ● ● ●●●
●
●
●
●
● ●●●
●
● ●
●
●● ●
●
●
●●
● ●
●
●
●● ● ●
●●
●
●●●
●●●
●
●
●
● ●●
●● ●●● ●● ●
●
●
● ●
●
●●●
●
● ●
●
●
●●
●
● ●●
●
●
●
●
●
●
● ●●●
●●●
●●
● ●●
●
●
●●
●
●●●
●
●
● ● ●●● ●
●
●
●
●
●●●● ● ●●
● ● ● ●● ● ●
● ●●
●
● ● ● ●●●● ● ● ● ●●
●●●●● ●●●
●●
●
●
●
●
●
●
● ●
● ●●
●
●
●● ●
●●●
●
●
● ●●
●
●●
●
● ●●
● ●
●
● ●
● ●
● ● ●
●●
●
●
●●
●
●
●
●●●
● ●●
●
●
●
●
●
● ● ●
●
●●
●●
●
●●
●● ●
●
●
●
●●●●
●● ●
●
●
●
● ●●
●●●
●●●● ● ●●
● ●
●
● ●● ● ●
●●
● ●● ●●●●
●
● ● ●
●
●
●
●
●
●● ●
●
● ● ●
●
●
●
●
●●
●●●● ●●
●
●
●
●● ●
●●
●●●
●●
●
●
●
●●
●● ●
●
●
●●
●●
●●
●
● ● ●●
●● ●●
●
●● ● ●
●●
●
●●
●
●
●●●
●●
●
●● ●
●
● ● ●●● ●●●
●●
● ●●
●●
●●
●●● ●●
●
●●● ● ●
●
●
● ● ●●
● ●
● ●●● ●
●
●
●
● ●
●
●●
●● ● ●
● ●
● ●
●
●●● ●
● ●● ● ● ●
● ● ● ●●
●●
●●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
● ●
● ● ●
● ● ●
●
●
●
●●
●
● ●●
●●
●●
●
●●
●
●
● ● ●● ● ●●●●
● ●●
● ●●
●●
●
●●
●
●●
●● ● ●
●
●
●
●
●●
●
●
● ●●●
●
●
●
●
● ●●
●
●
●
●
●
●
●●
●●
●●
●
●
●
●●●
●● ●
●
●
● ●
●●
● ● ●
● ●●
●
●●●● ●
●●
●●
●●● ●
●
●
●●● ● ●
●
●
●
●●
●
● ●●●●●
●
●●
●
●
●
●
●
●●●
●
● ●●
● ●
●●
●
●●
●● ● ●
●
●
●
●
● ●
●
●
● ●
●
●

●
●●

●

●
●

●

●

●

●

●
●
●
● ●
●
● ●●
● ● ● ●
● ● ●
●●
●
●
● ●
●● ●
● ●● ●
●●
●
●●
●●●
●
● ●●
●
●
●
●
●
●
●
●● ●●
●
● ● ●● ● ●
●
●
●
●●● ●
● ● ● ●
●
●●
●
●● ●● ●
● ●
●●●●
●
●●
●● ●● ●
●
● ●●
●
●
●●●
●●●
●
●●
●
● ●
● ● ● ●●
●
●
●●
●
●● ●
●
●
●●● ● ●● ●
●
● ●●
● ●●
●
●● ●
●●
● ●●●●
●
●
●
● ●
●
●●
●
● ●
●●
●●● ● ● ●
●
●●●
● ●●●
●●
●
●● ●
●
●
●●
● ● ● ●● ●
●
●●
●●●●
● ●●● ●● ●●●●
●
●●●
●
●●●●●
● ● ●
●
● ● ●
●
●●
● ●●●●● ● ●
●●
●● ●
● ●● ●
●● ●
●
● ●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●● ●
●
●
●●
●
●● ●
●●
●
●
●
●
●
●●● ●
●
●●
●● ●
●
●●●
●●●●
●
●
● ●●
●●● ●
●
●
● ●
● ●● ●● ●
● ●●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
● ●
●
●
●●● ● ● ●
●
●●
●● ● ●● ●●●●
●
●●● ●
● ●●
●
●● ● ●
●● ●
●
●
●●
●
●●
● ●● ●
●
● ●●●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
● ●
●
●
●● ●
● ●
●●●
●●
●
●●
●
●
● ●●●
●
● ●
●
●●
●●
● ●●
●
●
●
●
●●
●●
●
●●
●●
●
●
● ● ●●
●
●
●● ●● ●●● ●
●
●
●●●
● ● ●● ●●
●
● ● ●
●●
●●
●
●
●
●●
● ●●●
●●
● ●●
●
● ●
●●
●
●
● ● ● ●● ● ● ●
●
●
● ●● ●
● ● ●● ● ●
●
● ●●
●
● ● ● ●●●● ● ● ● ●●
●●●●● ●●●
●●
●
●
●
●
●
● ●
● ●●
●
●●
● ●
●
●
●
●
●
● ●●
●
●● ●
●
● ●
● ●
● ● ●
●●
●
●
●
●
●
●
●● ●
● ●● ●
●
●
●
●
●
●
●●●● ● ●
●●
● ●
● ●●●
● ●
●
●
●
●●●
●●●● ● ● ●
● ●
●
● ●● ●
● ●● ●●●● ● ●
● ● ●
●
●
●
●
●● ●
●
●
● ● ●
●
●
●
●
●●
●
●●● ●
●
●
●● ●
●
●
●
●●
●
●
●●
●●
●
●
●
●
●●
●
●●
●● ●●
●● ●
●● ● ●
●●
●
●●
●
●
●●●
●●
●● ●
●
● ● ●●● ●●●
●●
● ●●
● ●●
●
●●● ●●
●
●
●
● ●●
● ●
● ●●● ●
●●● ●
●
●
●
● ●
●● ● ●
● ●
● ●
●● ●● ● ● ●●
●
● ● ● ●●
● ●● ●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ● ●
● ● ●
●
●
●
●
●●
●
●●
●●
●
●●
●
●
● ● ●● ● ●●●
● ●●
●
●● ● ●
●
●●
●
●● ●
●● ●
●●●
●
●
●
●
●
●●
●●
●
●
●
●●
●
●●
●
●●
●
●●●
● ●
●
● ●
●●
● ● ●
● ●●
●
●●●● ●
●
●●
●●
●
●
●
●●● ● ●
●
●●
●●
●
●● ●●
●
●
●
●●●
●
● ●●
●
● ●
●●
●● ● ●
●
●
● ●
●
●
●
●
●

●

●

●

●

●
●
● ●
●
● ●●
●
● ● ● ●
● ● ●
●●
●
●
● ●
●● ●
● ●● ●
●●
●
●●
●●●
●
● ●●
●
●
●
●
●
●
●
●● ●●
●
● ● ●● ● ●
●
●
●
●●● ●
● ● ● ●
●
●●
●
●● ●● ●
● ●
●●●●
●
●
●●
●
● ●●
●●
● ●● ●●
●
●●●
●●●
●
● ●
●
● ● ● ●●
●
●
●●
●
●● ●
●
●
● ● ● ●● ●
●
● ●●
● ●●
●
●● ●
●●
● ●●●●
●
●
●
● ●
●
●●
●
● ●
●●
●●● ● ● ●
●
●●●
● ●●●
●●
●
●● ●
●
●
●●
● ● ● ●● ●
●
●●
●●●●
● ●●● ●● ●●●●
●
●●●
●
●●●●●
● ● ●
●
● ● ●
●
●●
● ●●●●● ● ●
●●
●● ●
● ●● ●
●● ●
●
● ●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●● ●
●
●
●●
●
●● ●
●●
●
●
●
●
●
●
●●
●
●
●●
●● ●
●
●●●
●●●●
●
●
● ●●
●●● ●
●
●
●
● ●
● ●● ●● ●
● ●●
●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
● ●
●
●
●●● ● ● ●
●
●●
●● ● ●● ●●●●
●
●●● ●
● ●●
●
●● ● ●
●● ●
●
●
●●
●
●●
● ●● ●
●
● ●●●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
● ●
●
●
●● ●
●●
●●●
●●
●
●●
●
●
● ●●●
●
● ●
●
●●
●●
● ●●
● ●
● ●
●
●
●●
●●
●
●●
●●
●
●
●
●
● ●●
●● ●● ●●● ●
●
●
●●●
● ● ●● ●●
●●
● ● ●
●●
●●
●
●
●
●
●●
● ●●●
●●
● ●●
●
● ●
●●
●
●
●
● ● ● ●● ● ● ●
●
● ●● ●
● ● ●● ● ●
● ●●
●
● ● ● ●●●● ● ● ● ●●
●●●●● ●●●
●●
●
●
●
●
●
● ●
● ●●
●
●●
● ●
●
●
●
●
●
● ●●
●
●● ●
●
● ●
● ●
● ● ●
●●
●
●
●
●
●
●
●●●
● ●● ●
●
●
●
●
●
●
●●●● ● ●
●●
● ●
●●●●
● ●
●
●
●
●●●
●●●● ● ●
● ●
●
● ●● ●
●
● ●● ●●●● ● ●
● ● ●
●
●
●
●
●● ●
●
●
● ● ●
●
●
●
●●
●
●●● ●
●
●
●● ●
●●●
●
●
●●
●
● ● ●● ●
●
●
●●
●●
●●
●● ●●
●● ●
●● ● ●
●●
●
●●
●
●
●●●
●●
●● ●
●
● ● ●●● ●●●
●●
● ●●
● ●●
●
●●● ●●
●
●
●
● ●●
● ●
● ●●● ●
●●● ●
●
●
●
● ●
●
●● ● ●
● ●
● ●
●
●
● ●● ● ● ●
● ● ● ●●
● ●● ●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ● ●
● ● ●
●
●
●
●●
●
●●
●●
●
●
●●
●
●
● ● ●● ● ●●●●
● ●●
●● ● ●
●
●●
●
●● ●
●● ●
●
●
●
● ●●●
●
●
●
●●
●
●
●
●●
●●
●●
●●
●
●●●
● ●
●
● ●
●●
● ● ●
● ●●
●
●●●● ●
●
●●
●●
●
●
●
●●● ● ●
●
●●
●●
●
●● ●●
●
●
●
●●●
●
● ●●
●
● ●
●●
●● ● ●
●
●
● ●
●
●
●
●
●
●

●
●●

●

●
●●

●

●
●

●

●

●

●

●

●

●
●

●

●

●

●
●

●
●

●

●

●

(f) Set P (5300 pts)

(g) FCNN (1046 pts)

●

●

(h) MSS (1136 pts)

●

(i) RSS (1025 pts)

(j) VSS (1027 pts)

Figure 1: Examples of the subsets selected by FCNN, MSS, RSS, and VSS, on two different training sets. Training
set (a) is a set of uniformly distributed points in R2 of two classes: red points lying inside a disk, and blue points
lying outside. Training set (f) is a well-known benchmark from the UCI Machine Learning repository, called Banana,
consisting of points in R2 of two classes, red and blue.
order-dependent, meaning that the result is determined
by the order in which points are considered by the algorithm. Alternatives include FCNN (Fast CNN) [1]
and MSS (Modified Selective Subset) [2], which produce consistent and selective subsets respectively. Both
algorithms run in O(n2 ) worst-case time, and are orderindependent. These algorithms are considered the stateof-the-art for the NN condensation problem, subject to
achieving these properties. While such heuristics have
been extensively studied experimentally [10, 7], theoretical results are scarce. Unfortunately, to the best of
our knowledge, no bounds are known for the size of the
subsets generated by any of these heuristics.
More recently, an approximation algorithm called
NET [8] was proposed, along with almost matching
hardness lower bounds for the problem. The idea is to
compute a γ-net of P , with γ equal to the minimum NE
distance in P , implying that the resulting subset is consistent. Unfortunately, while NET has provable worstcase performance, this approach allows little room for
condensation, and in practice, the resulting subset can
be too large. For example, on the training set in Figure 1a, NET selects a subset of over 90% of the points,
while other algorithms select only 3% of the points.
1.3

Our contributions

In this paper, we propose two new NN condensation algorithms, called RSS and VSS. We will establish asymp-

totically tight upper-bounds on the sizes of their selected subsets. Moreover, we prove that these algorithms have similar complexity to the popular stateof-the-art algorithms FCNN and MSS. Additionally, we
also analyze the selection size of FCNN and MSS. To
the best of our knowledge, these represent the first theoretical results on practical NN condensation algorithms.
The following is a summary of our contributions.

2

Algorithm

Selection size

RSS
VSS
MSS [2]
FCNN [1]

O(κ cd−1 )
≤k
Ω(1/ε) w.r.t. κ and k
Ω(k)

Results on condensation size

One of the most significant shortcomings in research
on practical NN condensation algorithms is the lack of
theoretical results on the sizes of the selected subsets.
Typically, the performance of these heuristics has been
established experimentally.
We establish bounds with respect to the size of two
well-known and structured subsets of points: (a) the set
of all NE points of P of size κ, and (b) the set of border
points of P of size k. Throughout the paper, we refer
equally to the algorithms and their selected subsets.

2.1

The state-of-the-art

Let’s begin our analysis with a state-of-the-art algorithm for the problem: MSS or Modified Selective Subset (see Algorithm 1). The selection process of the algorithm can be simply described as follows: for every
p ∈ P , MSS selects the point with smaller NE distance
contained inside the NE ball of p.
Clearly, this approach computes a selective subset of
P , which by definition, is order-independent. MSS can
be implemented in O(n2 ) worst-case time. Unfortunately, the selection criteria of MSS can be too strict,
requiring one particular point to be added for each point
p ∈ P . Note that any point inside the NE ball of p suffices for achieving selectiveness. In practice, this can
lead to much larger subsets than needed.
Algorithm 1: Modified Selective Subset

1

2
3
4
5
6
7
8
9
10
11
12

Input: Initial training set P
Output: Condensed training set MSS ⊆ P
n
Let {pi }i=1 be the points of P sorted in increasing
order of NE distance dne (pi )
MSS ← ∅
S←P
foreach pi ∈ P , where i = 1 . . . n do
add ← false
foreach pj ∈ P , where j = i . . . n do
if pj ∈ S ∧ d(pj , pi ) < dne (pj ) then
S ← S \ {pj }
add ← true
if add then
MSS ← MSS ∪ {pi }

has only 4 NE points and 4 border points, corresponding
to r1 , r2 , b1 and b3/ε .

(a) Initial training set of collinear points, where both the
number of NE points and the number of border points equal
to 4. That is, κ = k = 4.

(b) Subset of points computed by MSS from the original
training set (fully colored points belong to the subset, while
faded points do not). The size of the subset is Ω(1/ε).

Figure 2: Unbounded example for MSS w.r.t. κ and k.
Let’s discuss which points are added by MSS for each
point in P (see Figure 2b). For points r1 and r2 , the
only points inside their NE balls are themselves, so both
r1 and r2 belong to the subset selected by MSS. For
points bi with i ≤ 2/ε, the point with minimum NE
distance contained inside their NE ball is b1 , which is
also added to the subset. Now, consider the points bi
with 2/ε < i < 5/2ε. Let j = i − 2/ε, it is easy to
prove that the point with minimum NE distance inside
the NE ball of bi is b2j+1 (see Figure 2b). Therefore,
this implies that the number of points selected by MSS
equals 5/2ε − 2/ε = 1/2ε = Ω(1/ε).


return MSS
2.2

This intuition is formalized in the following theorem.
Here we show that the subset selected by MSS can select
a subset of unbounded size as a function of κ or k.
Theorem 1 Given 0 < ε < 1, there exists a training
set P ⊂ Rd with a constant number of NE and border
points such that MSS selects Ω(1/ε) points.
Proof. Recall that for each point in P , the MSS algorithm selects the point inside its NE ball with minimum
NE distance. Given a parameter 0 < ε < 1, we construct a training set in d-dimensional Euclidean space,
as illustrated in Figure 2a.
Create two points r1 and r2 , and assign them to
the class of red points. Without lost of generality,
the distance between these two points is 1. Let ~u be
the unit vector from r1 to r2 , create additional points
bi = r1 + iε
u for i = {1, 2, . . . , 3/ε}. Assign all bi points
4~
to the class of blue points. The set of all these points
constitute the training set P . It is easy to prove that P

A better approach

We propose a new algorithm, called RSS or Relaxed Selective Subset, with the idea relaxing the selection process of MSS, while still computing a selective subset.
For a given point p ∈ P , while MSS requires to add the
point with smallest NE distance inside the NE ball of
p, in RSS any point inside the NE ball p suffices.
Algorithm 2: Relaxed Selective Subset

5

Input: Initial training set P
Output: Condensed training set RSS ⊆ P
RSS ← ∅
n
Let {pi }i=1 be the points of P sorted in increasing
order of NE distance dne (pi )
foreach pi ∈ P , where i = 1 . . . n do
if dnn (pi , RSS) ≥ dne (pi ) then
RSS ← RSS ∪ {pi }

6

return RSS

1
2

3
4

The idea is rather simple (see Algorithm 2). Points of
P are examined in increasing order with respect to their
NE distance, and we add any point whose NE ball contains no point previously added by the algorithm. This
tends to select points close to the decision boundary of
P (see Figure 1d), as points far from the boundary are
examined later in the selection process, and are more
likely to already contain points inside their NE ball.
Theorem 2 RSS is order-independent and computes a
selective subset of P in O(n2 ) worst-case time.
Proof. By construction, every point in P was either
added into RSS, or has a point in RSS inside its NE ball.
Therefore, RSS is selective. The order-independence follows from the initial sorting step.
Let’s analyze the time complexity of RSS. The initial step requires O(n2 ) time for computing the NE distances of each point in P , plus additional O(n log n)
time for sorting the points according to such distances.
The main loop iterates through each point in P , and
searches their nearest neighbor in the current subset,
incurring into additional O(n2 ) time. Finally, the worstcase time complexity of the algorithm is O(n2 ).

Theorem 3 RSS selects at most O(κ (3/π)d−1 ) points.
Proof. The proof follows by a charging argument on
each NE point of P . Consider a NE point p ∈ P , and
let RSSp be the set of points selected by RSS such that
p is their NE. Let pi , pj ∈ RSSp be two such points, and
without loss of generality say that dne (pi ) ≤ dne (pj ).
By construction of the algorithm, we also know that
d(pi , pj ) ≥ dne (pj ). Now, consider the triangle 4ppi pj .
Clearly, the side ppi is the larger of such triangle, and
therefore, the angle ∠pi ppj ≥ π/3. Meaning that the
angle between any two points in RSSp with respect to
p is at least π/3.
By a standard packing argument, this implies that
d−1
|RSS
we obtain that |RSS| =
P p | = O((3/π) ). Finally,
d−1
|RSS
|
=
κ
O((3/π)
).

p
p
For constant dimension d, the size of RSS is O(κ).
Therefore, the following result implies that the upperbound on RSS is tight up to constant factors. Furthermore, it implies that this is the best upper-bound we
can hope to achieve in terms of κ.
Theorem 4 (Lower-bound) There exists a training
set P ⊂ Rd with κ NE points, for which any consistent
subset contains Ω(κ cd−1 ) points, for some constant c.
Proof. We construct a training set P in d-dimensional
Euclidean space, which contains points of two classes:
red and blue. Consider the following arrangement of
points: create a red point p, and take every point at
distance 1 from p as a blue point. Simply, the points on
the surface of a unit ball centered at p.

Take any consistent subset of this training set, and
consider some point p0 in such subset, and the bisector
between p and p0 . The intersection between this bisector
and the unit ball centered at p describes a cap of such
ball of height 1/2. Any point located inside this cap
is closer to p0 than p, and hence, correctly classified.
Clearly, by definition of consistency, all points in the
ball must be covered by at least one cap. By a simple
packing argument, we know such covering needs Ω(cd−1 )
points, for some constant c.
So far the training set constructed has only two nearest enemy points; the red point p, and one blue point
closest to p (assuming general position). Then, we can
repeat this arrangement κ/2 times, using sufficiently
separated center points. This generates a training set P
with a number of NE points equal to κ, for which any
consistent subset has size Ω(κ cd−1 ).

Different parameters from κ can be used to bound
the selection size of condensation algorithms. Let’s consider k, the number of border points in the training set
P . From the example illustrated in Figure 3, we know
that RSS can select more points than k (see Figure 3b).
Repeating such arrangement forces RSS to select Ω(k)
points. Yet, the question remains, at most, how many
more points than k can this algorithm select?

(a) Point arrangement.

(b) RSS selection outlined.

Figure 3: Example where RSS selects k + 1 points.

Lemma 5 The nearest enemy point of any point in P
is a also a border point of P .
Proof. Take any point p ∈ P . Consider the empty
ball of maximum radius, tangent to point ne(p), and
with center in the line segment between p and ne(p).
Being maximal, this ball is tangent to another point
p∗ ∈ P (see Figure 4a). Clearly, p∗ is inside the NE ball
of p, which implies that p and p∗ belong to the same
class, making p∗ and ne(p) enemies. By the empty ball
property, this means that both p∗ and ne(p) are border
points of P .

From Lemma 5, we know that in Euclidean space,
the number of NE points of P is at most the number of
border points of P . That is, κ ≤ k. While this implies

(a)

(b)

Figure 4: (a) The largest empty ball tangent to ne(p)
and center in p ne(p), is also tangent to some point p∗ ,
making p∗ and ne(p) border points. (b) Computing the
radius of a ball with center in the line segment between
p and ne(p), and tangent to both ne(p) and p0 .

finds a border point inside its NE ball. Without loss of
generality implement VSS to compute the point p∗ that
minimizes the radius of an empty ball tangent to both
ne(p) and p∗ , and center in the line segment between
p and ne(p). For any given point p0 inside the NE ball
of p, denote r(p, p0 ) to be the radius of the ball tangent
to p0 and ne(p) and center in the line segment between
p and ne(p). As illustrated in Figure 4b, let vectors
p−ne(p)
~u = kp−ne(p)k
and ~v = p0 − ne(p), the radius of this ball
can be derived from the formula r(p, p0 ) = k~v +r(p, p0 )~uk
~
v ·~
v
as r(p, p0 ) = 2~
u·~
v.
∗
As p is defined as the point that minimizes such radius, a simple scan over the points of P suffices to identify the corresponding p∗ for any point p ∈ P . Therefore, this implies that VSS can be computed in O(n2 )
worst-case time.

2.3

an easy extension of the bound for RSS, now in terms
of k, it is unclear if the other factors can be improved.
Alternatively, this opens another idea for condensation. In order to prove Lemma 5, we show that there
exist at least one border point inside the NE ball of
any point p ∈ P . Therefore, any algorithm that only
selects such border points, can guarantee to compute
a selective subset of size at most k. Consider then a
modification of RSS, where for each point pi ∈ P , if no
other point lying inside the NE ball of p has been added
yet, instead of adding pi as RSS does, we add a border
point inside NE ball of p. We call this new algorithm
VSS or Voronoi Selective Subset (see Algorithm 3).
Algorithm 3: Voronoi Selective Subset

1
2

3
4
5

6

Input: Initial training set P
Output: Condensed training set VSS ⊆ P
VSS ← ∅
n
Let {pi }i=1 be the points of P sorted in increasing
order of NE distance dne (pi )
foreach pi ∈ P , where i = 1 . . . n do
if dnn (pi , RSS) ≥ dne (pi ) then
Find a border point that lies inside the NE
ball of pi and add it to VSS
return VSS

FCNN or Fast Condensed Nearest-Neighbor is yet another popular state-of-the-art algorithm for the NN condensation problem. In contrast with MSS, which finds
selective subsets, FCNN selects consistent subsets of P .
Let’s now describe the selection process of FCNN (see
Algorithm 4). Essentially, FCNN maintains a subset of
P , which is updated in each iteration, by adding points
that are incorrectly classified using the current subset.
The iterations stop when all points of P are correctly
classified by the current subset, that is, when FCNN
is consistent. Starting with the centroids of each class,
set S contains some misclassified points from P \ FCNN
that will be added in the next iterartion. How does the
algorithm decide which points to include in S? Define
voren(p, FCNN, P ) as the set of enemy points of p in
P , whose NN in FCNN is p, that is, the set {q ∈ P |
l(q) 6= l(p) ∧ ne(q, FCNN) = p}. Then, for each point
p ∈ FCNN, the algorithm selects one representative out
of its corresponding voren(p, FCNN, P ), which is usually
defined as the NN to p.
Algorithm 4: Fast Condensed Nearest-Neighbor

7

Input: Initial training set P
Output: Condensed training set FCNN ⊆ P
FCNN ← ∅
S ← centroids(P )
while S 6= ∅ do
FCNN ← FCNN ∪ S
S←∅
foreach r ∈ FCNN do
S ← S ∪ {rep(p, voren(p, FCNN, P ))}

8

return FCNN

1
2
3
4

Theorem 6 VSS computes a selective subset of P of
size at most k in O(n2 ) worst-case time.
Proof. By construction, for any point in p ∈ P \ VSS
the algorithm selected one border point inside the NE
ball of p. This implies that the resulting subset is selective, and contains no more than k points.
Now, we describe an efficient implementation of VSS.
Recall that for every point p ∈ P \ VSS, the algorithm

What about FCNN?

5
6

Theorem 7 There exists a training set P ⊂ Rd with k

(b) Middle arrangement.

(c) Side arrangement.

(a) Entire arrangement of points.

Figure 5: Example of a training set P ⊂ R2 for which FCNN selects Ω(k) points.
border points, for which FCNN selects Ω(k) points.
Proof. Consider the arrangement in Figure 5b (left),
consisting of points of 4 classes. The centroids of the
blue, yellow, and red classes are the only points labeled as such. By placing a sufficient number of black
points far at the top of this arrangement, we avoid their
centroid to be any of the three black points in the arrangement. Beginning with the centroids, the first iteration of FCNN would have added the points outlined
in Figure 5b (right). Now each of these points have
one black point inside their Voronoi cells, and therefore,
these black points will be the representatives added in
the second iteration. This small example, with k = 5,
shows how to force FCNN to add all the border points
plus two internal points. Out of these two internal black
points, one is the centroid added in the initial step. The
remaining internal black point, however, was added by
the algorithm during the iterative process. This scheme
can be extended to larger values of k, without increasing
the number of classes.
The previous is the first building block of the entire
training set, shown in Figure 5a. To this “middle” arrangement, we append “side” arrangements of points,
as the one illustrated in Figure 5c, which will have similar behavior to the middle arrangement. This particular side arrangement will be appended to the right of
the middle one, such that the distance between the red
points is greater than the distance from the yellow to
the red point. Every time we append a new side arrangement, its blue and red labels are swapped. The
arrangements appended to the left side are simply a
horizontal flip of the right arrangement. Now, the behavior of FCNN in such a setup is illustrated with the
arrows in Figure 5a. The extreme point of the previous arrangement adds the yellow point at the center of
the current arrangement, which then adds the red point
next to the blue point, as is closer than the other red
point. Next, this red point adds the blue point, and the

yellow point adds the remaining red point. Finally, the
Voronoi cells of these points will look as shown in Figure 5c (right), and in the next iteration, the tree black
points will be added.
After adding side arrangements as needed (same number of the left and right), it is easy to show that the centroids are still the tree points in the middle arrangement
and the black point at the top (by adding a sufficient
number of black points in the top cluster). This implies
than FCNN will be forced to select Ω(k) points.

While this example sheds light on the selection behavior of FCNN, an upper-bound is still missing. Based on
the following lemma, we conjecture that FCNN selects
at most O(κ cd−1 ) points, for some constant c.
Lemma 8 Consider a point p selected by FCNN. Then,
the number of representatives of p selected throughout
the algorithm does not exceed O((3/π)d−1 ) points.
Proof. This proof follows from similar arguments to
the ones described in the proof of Theorem 3. Consider p1 , p2 ∈ FCNN to be two points added to by
the algorithm as representatives of p. Without loss
of generality, p1 was added before p2 , implying that
d(p, p1 ) ≤ d(p, p2 ). By construction, if p2 was added
as a representative of p, and not of p1 , we also know
that d(p, p2 ) ≤ d(p1 , p2 ). From this, consider the triangle 4pp1 p2 and the angle ∠p1 pp2 . This is the largest
angle of the triangle, meaning that ∠p1 pp2 ≥ π/3. Finally, by a standard packing argument, there are at most
O((3/π)d−1 ) such points.

3

Open problems

A few key questions remain unsolved:
• In terms of k, our best upper-bound on the selection
size of RSS is not tight. Can it be improved?
• Is it possible to prove an upper-bound on the selection size of FCNN in terms of either κ or k?

References
[1] F. Angiulli. Fast nearest neighbor condensation for large
data sets classification. IEEE Transactions on Knowledge
and Data Engineering, 19(11):1450–1464, 2007.
[2] R. Barandela, F. J. Ferri, and J. S. Sánchez. Decision boundary preserving prototype selection for nearest neighbor classification. International Journal of Pattern Recognition and
Artificial Intelligence, 19(06):787–806, 2005.
[3] D. Bremner, E. Demaine, J. Erickson, J. Iacono, S. Langerman, P. Morin, and G. Toussaint. Output-sensitive algorithms for computing nearest-neighbour decision boundaries. In F. Dehne, J.-R. Sack, and M. Smid, editors, Algorithms and Data Structures: 8th International Workshop,
WADS 2003, Ottawa, Ontario, Canada, July 30 - August 1,
2003. Proceedings, pages 451–461, Berlin, Heidelberg, 2003.
Springer Berlin Heidelberg.
[4] T. Cover and P. Hart. Nearest neighbor pattern classification. IEEE Trans. Inf. Theor., 13(1):21–27, Jan. 1967.
[5] L. Devroye. On the inequality of cover and hart in nearest neighbor discrimination. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, (1):75–78, 1981.
[6] E. Fix and J. L. Hodges. Discriminatory analysis, nonparametric discrimination: Consistency properties. US Air Force
School of Aviation Medicine, Technical Report 4(3):477+,
Jan. 1951.
[7] S. Garcia, J. Derrac, J. Cano, and F. Herrera. Prototype
selection for nearest neighbor classification: Taxonomy and
empirical study. IEEE Trans. Pattern Anal. Mach. Intell.,
34(3):417–435, Mar. 2012.
[8] L.-A. Gottlieb, A. Kontorovich, and P. Nisnevitch. Nearoptimal sample compression for nearest neighbors. In Advances in Neural Information Processing Systems, pages
370–378, 2014.
[9] P. Hart. The condensed nearest neighbor rule (corresp.).
IEEE Trans. Inf. Theor., 14(3):515–516, Sept. 1968.
[10] N. Jankowski and M. Grochowski. Comparison of instances
selection algorithms I. Algorithms survey. In Artificial Intelligence and Soft Computing-ICAISC 2004, pages 598–603.
Springer, 2004.
[11] K. Khodamoradi, R. Krishnamurti, and B. Roy. Consistent
subset problem with two labels. In Conference on Algorithms
and Discrete Applied Mathematics, pages 131–142. Springer,
2018.
[12] G. Ritter, H. Woodruff, S. Lowry, and T. Isenhour. An algorithm for a selective nearest neighbor decision rule. IEEE
Transactions on Information Theory, 21(6):665–669, 1975.
[13] C. J. Stone. Consistent nonparametric regression. The annals of statistics, pages 595–620, 1977.
[14] G. Toussaint. Open problems in geometric methods for
instance-based learning. In J. Akiyama and M. Kano, editors, JCDCG, volume 2866 of Lecture Notes in Computer
Science, pages 273–283. Springer, 2002.
[15] G. Toussaint. Proximity graphs for nearest neighbor decision
rules: Recent progress. In Progress, Proceedings of the 34th
Symposium on the INTERFACE, pages 17–20, 2002.
[16] G. T. Toussaint, B. K. Bhattacharya, and R. S. Poulsen.
The application of Voronoi diagrams to non-parametric decision rules. Proc. 16th Symposium on Computer Science
and Statistics: The Interface, pages 97–108, 1984.
[17] G. Wilfong. Nearest neighbor problems. In Proceedings of
the Seventh Annual Symposium on Computational Geometry, SCG ’91, pages 224–233, New York, NY, USA, 1991.
ACM.

[18] A. V. Zukhba. NP-completeness of the problem of prototype
selection in the nearest neighbor method. Pattern Recog.
Image Anal., 20(4):484–494, Dec. 2010.

