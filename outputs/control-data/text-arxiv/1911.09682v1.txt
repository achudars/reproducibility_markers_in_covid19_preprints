Quantum Observables for continuous control of the Quantum
Approximate Optimization Algorithm via Reinforcement Learning
Artur Garcia-Saez∗
Barcelona Supercomputing Center (BSC), Barcelona, Spain. and
Qilimanjaro Quantum Tech, Barcelona, Spain.

Jordi Riu

arXiv:1911.09682v1 [quant-ph] 21 Nov 2019

Barcelona Supercomputing Center (BSC), Barcelona, Spain.
We present a classical control mechanism for Quantum devices using Reinforcement Learning.
Our strategy is applied to the Quantum Approximate Optimization Algorithm (QAOA) in order
to optimize an objective function that encodes a solution to a hard combinatorial problem. This
method provides optimal control of the Quantum device following a reformulation of QAOA as an
environment where an autonomous classical agent interacts and performs actions to achieve higher
rewards. This formulation allows a hybrid classical-Quantum device to train itself from previous
executions using a continuous formulation of deep Q-learning to control the continuous degrees of
freedom of QAOA. Our approach makes a selective use of Quantum measurements to complete
the observations of the Quantum state available to the agent. We run tests of this approach on
MAXCUT instances of size up to N = 21 obtaining optimal results. We show how this formulation
can be used to transfer the knowledge from shorter training episodes to reach a regime of longer
executions where QAOA delivers higher results.

I.

INTRODUCTION

Optimization strategies performed by classical machines may help to overcome current limitations of NISQ
devices [1] –namely noise and decoherence time– in order
to solve hard computational tasks. Following this line of
research, new algorithms such as the Variational Quantum Eigensolver [2] and the Quantum Approximate Optimization Algorithm [3] search for the optimal operations
to be performed under experimental constraints. In this
scenario, hybrid algorithmic proposals offer the possibility of using conventional optimization tools to optimally
tune the parameter set controlling the Quantum device.
This interface allows efficient state preparation of Quantum states which are solution to Quantum or classical
problems formulated as a minimization task, delegating
the optimization to an efficient classical algorithm.
The Quantum Approximate Optimization Algorithm
(QAOA) [3–9] provides a powerful circuit description [10–
12] that transforms a trivial Quantum state into one
encoding the solution to a hard combinatorial problem.
The circuit used by QAOA is constructed from the problem description and the solution is found as the ground
state of a Quantum Hamiltonian adjusting gate operation
by a set of real parameters. Finding these parameters
through optimization provides a solution to the combinatorial problem, but at a large classical computational
effort. A number of optimization techniques have been
extensively used to attack QAOA classical optimization
[6, 13–17].
We approach the optimization of hybrid algorithms as

∗ Corresponding

author: artur.garcia@bsc.es

an interaction process between an active agent –with access to classical resources– controlling the operation of a
Quantum device executing a Quantum circuit, a scenario
typically solved in Machine Learning with Reinforcement
Learning techniques [18]. The agent learns from previous actions according to the rewards obtained and the
effects that such actions had over the environment. After
a training process, the agent exploits a learned strategy
to maximize the reward received from the environment.
The interaction is completely described by the reward
function, the actions available to the agent, and the observations the agent may perform over the Quantum environment.
In this work we propose an approach to QAOA optimization based on continuous Reinforcement Learning.
To complete this task we reformulate QAOA as an agentenvironment interaction. A classical agent performs actions over a Quantum environment using a set of operations equivalent to the circuit formulation of QAOA. At
each episode the agent chooses an action that modifies
the Quantum state. The Quantum device performs the
selected action, and returns a description of the Quantum state and a reward value. Measurement strategies
are necessary to maximise the available information to
the agent about the Quantum state. Together with the
use of delayed rewards, this is the key contribution of
our work. These observations are efficiently used by the
agent to construct the optimal policy, and to find optimal
reward values for large systems and circuits with numerically stability. Recent Reinforcement learning techniques
based on Q-learning have shown great success in agent
control over discrete actions [19–21]. Our approach uses
a formulation of Reinforcement Learning developed for a
continuous action space [20, 21]. Using these techniques,
we find optimal values of the objective function of the

II

THE QUANTUM APPROXIMATE OPTIMIZATION ALGORITHM AS A QUANTUM ENVIRONMENT

original QAOA problem after training, and we are capable to reach a regime of large circuit depth out of reach to
global optimizers. Applications of Reinforcement Learning to Quantum systems have been explored extensively
before in [22–24]. Recently, approaches of Reinforcement
Learning optimization of QAOA based on sampling [25]
or variations of the reward function [26] have been applied to small Quantum systems.
We structure the present paper starting with the formulation of the QAOA in section II. Section III presents
the tool set used for solving reinforcement Learning problems in a continuous action space. In section IV we plot
results of our method that are summarized on section V.
FIG. 1: Two equivalent formulations of the QAOA circuit.
Upper panel: Block depiction of QAOA. At each step of the
algorithm two continuous parameters {γi , βi } are used to perform an operation over a Quantum state initialized as |si. The
final number of parameters {γ, β} required to reach the final
state |Ψi is 2p for a given number of QAOA steps p. The
objective function is evaluated after the last step. Bottom
panel: Transformation of the QAOA algorithm into an interactive operational environment. At each step i, the initial
internal state of the system |Ψi i is transformed by an action
described with 2 continuous parameters {γ, β}, and returns
a set of measurements {hOi} = {Xi , Zi , . . .} over the final
state |Ψi+1 i as an observation. The reward is obtained from
Eq.1 at the final step. We recover the original formulation of
QAOA after p steps and proper initialization on the first step
to |si.

II. THE QUANTUM APPROXIMATE
OPTIMIZATION ALGORITHM AS A QUANTUM
ENVIRONMENT

We study solutions to the MAXCUT problem, i.e.
finding an optimal graph bi-partition maximizing the
number of edges that connect the two partitions. The
MAXCUT problem on a graph of size N can be formulated in Hamiltonian form over a set of N qubits as
C=

X1
X
(1 − σiz σjz ) =
Cij
2

hi,ji

(1)

hi,ji

with m terms Cij accounting for adjacent edges in the
original graph problem. The circuit implementation of
QAOA [3] requires a set of two operators U (C, γ) and
U (B, β), constructed from the edge operator C and the
n
P
local operator B =
σjx :

We propose a revision of the QAOA optimization as
an agent-environment interaction. A classical agent performs a sequence of actions over a Quantum device, and
the combination of these actions is equivalent to the execution of QAOA. The actions transform the Quantum
environment such that the final state is the result of the
QAOA computation. Our approach is graphically expressed in Fig.1. A complete computation of QAOA is
decomposed in a series of p steps. A collection of p steps,
starting with the |si state, forms an episode. At each
step, an agent selects an action described by two real
parameters {γ, β}, and receives a reward value and an
observation of the current state of the Quantum system
. This information is processed by the agent in order
to construct a successful strategy that improves the final
reward of the episode. The agent obtains information of
the Quantum environment by a set of Quantum measurements {hOi} = {Xi , Zi , . . .}. At the end of each episode
the objective function Eq.1 is evaluated and used as the
final reward. However, different strategies to reward the
agent along the episode can be designed, along the set of
observations available at each step. This choice of observations and rewards will have deep impact in the strategies of the agent, and are a key ingredient in a successful
Reinforcement Learning optimization.

j=1

U (C, γ) = e−iγC =
U (B, β) =

e−iβB =

m
Y
α=1
n
Y

e−iγCα
(2)
x

e−iβσj

j=1

with γ ∈ [0, π) and β ∈ [0, 2π). With m terms in
Eq.1, the algorithm requires a Quantum circuit depth
mp + p. Operators U (C, γ) and U (B, β) are alternatively
applied (see Fig.1) to a Quantum state initialized to
the uniformP
superposition of computational basis states
|si = √12n
|zi. For an integer p ≥ 1 and a set of 2p
z

angles {γ1 , β1 . . . γp , βp } ≡ {γ, β}, QAOA produces the
Quantum state
|γ, βip = U (B, βp ) U (C, γp ) · · · U (B, β1 ) U (C, γ1 ) |si
(3)
which delivers a MAXCUT solution for large values of p
[3, 4]:


lim max hγ, β|p C |γ, βip = max C.
(4)
p→∞

γ,β

2

IV
III.

RESULTS

REINFORCEMENT LEARNING IN A
CONTINUOUS ACTION SPACE

The general task of Reinforcement Learning [18] is
to optimize the actions of an agent interacting with an
environment in a Quantum state of which we have a
collection of measurements {hOi i}. In our scenario, at
each step the agent performs an action {γ, β} that alters the state in the Quantum device |Ψi i. The final
goal is to maximize the total reward function Rt =
PT i−t
r({hOi i}, {γ, β}) according to the rewards rei=t d
ceived r({hOi i}, {γ, β}) computed using Eq.1, and a discount factor d. At each time step i in [1, p] the agent
chooses an action {γ, β} according to its current policy
π({γ, β}|{hOi i}) and performs measurements to the current state and receives a reward.
We use an off-policy model-free Reinforcement Learning method presented in [20] based on Q-learning. We
define a policy-dependent Qπ function,
Qπ ({hOi i}, {γ, β}) = E(Rp |{hOi i}, {γ, β}),

(5)

such that the optimized policy is the greedy policy
π = δ({γ, β} = arg max (Qπ ({hOi i}, {γ, β})).
{hOi+1 i}

(6)

To obtain the Qπ function from deep neural networks
in a continuous action space, we use a decomposition of
the network expression of Qπ as the composition of an
action term AθA ≡ A({hOi i}, {γ, β}|θA ) and a value term
VθV ≡ V ({hOi i}|θV )
Qπ ({hOi i}, {γ, β}|θQ ) = AθA + VθV

FIG. 2: Top: Reward computed by Eq.1 at the end of each
training episode for MAXCUT problems of size N = 13.
Mean rewards are averaged over several executions (included
as a color line to guide the eye). An horizontal line marks
the global optimal value. Episodes with a larger number of
steps p result in higher rewards, as expected from QAOA formulation. Bottom: Comparison of the relative error for each
p with the result of a global optimizer (BFGS) on the same
instance. Consistent results are obtained with both methods
after a similar number of evaluations.

(7)

Actions selected according to the greedy policy include
a correlated random term modeled by an OrnsteinUhlenbeck distribution in order to allow the agent a selective exploration of the action space.
IV.

RESULTS

We run simulations of a classical-Quantum system running the Q-learning algorithm, and report the reward at
the end of each episode. We study the episode reward
along a training session using an implementation of the
Normalized Advantage Functions Q-learning algorithm
[20] with a noise contribution modeled by an OrnsteinUhlenbeck distribution with (θ = 0.01, µ = 0, σ = 0.01).
Values of A and V are computed and stored in neural
networks with up to 20 dense layers and a maximum of
256 neurons at each layer. The agent receives a reward
only at the final step of the episode, and the observation
is restricted to a set of local operators {Xi , Zi }. The
learning rate is kept constant (lr = 10−4 ) along all executions. We have programmed the QAOA as a GymAI
environment [27], using Tensorflow [28, 29] to implement
the NAF method, and QUPY [30] to simulate the Quantum circuit.

We pick an instance of MAXCUT defined by a random 3−connected graph of size N = 13 and show in
Fig.2 the reward at the end of each episode along the
training. We observe that initially the reward obtained
is consistent with an agent taking random actions, but
evolves towards a set of parameters {γ, β} delivering a
solution the MAXCUT problem as the training proceeds.
We show results for increasing values of p. The method
exhibits a stable behaviour along a large number of training episodes, and according to QAOA condition
max hγ, β|p0 C |γ, βip0 ≥ max hγ, β|p C |γ, βip
γ,β

γ,β

(8)

for p0 > p the method delivers better results for increasing values of p. In Fig.2(bottom) we compare results
for N = 13, p = 5, 10, 15 to a global optimization per3

B

Extending episode length to p0 > p

IV

formed by a quasi-Newton method (BFGS) running during a comparable number of steps. Similar results are
obtained for a collection of tests performed over 2− and
3−connected graphs, for different values of p.
A.

in slow converging rates for large N and p, and may require fine adjustment of parameters along the training.
Moreover, in this regime one may require the evaluation
of larger neural networks where a cold start may complicate the optimization. We explore here a strategy based
on incremental training to reach in a stable manner the
regime of large p. An agent is initially trained in a plength environment (i.e. all episodes have length p). After the training is completed to a converged reward, the
agent is trained on longer episodes p0 > p. The setup in
the p0 -training keeps the learned values from episodes of
length p, i.e. the actions are computed from the same A
and V functions. The agent improves the strategy with
the p0 −p extra steps, potentially reaching higher rewards.
This process is iterated to reach longer episode duration,
a regime out of reach for global optimization techniques
in our experiments.

Environment observations through local
operators and reward strategies

Reward functions play a critical role in the design of
Reinforcement Learning strategies. At each step one can
use the reward function to value the current step actions.
A cumulative sum of partial rewards can be used as the
final episode reward. However, our goal is the global optimization of the Hamiltonian in Eq.1. A reward value is
returned only once the episode is finished, skipping limitations of local rewards that may lead to local minima
of the global optimization. For the optimization of the
QAOA we use delayed rewards motivated by the observation that global maximum of QAOA is not a combination
of local solutions for smaller optimizations [6]. In addition, a delayed reward strategy minimizes the evaluation
of the reward functions at each step, reducing dramatically the number of executions for large p.
We have reported here results where an agent has access to the Quantum state through a set of local measurements in different basis. At the end of each step
these observations are used together with the reward received (if any) and the previous action taken by the agent
throughout the learning phase. The set of observables define the observation space, and this can be complemented
with additional information (e.g. the current number of
steps). The observation available to the agent is limited
to Quantum observables (i.e. access to measurements
on a computational basis) along with limitations on the
number of experimental repetitions of the experiment.
Our particular choice of observations is inspired by the
objective function Eq.1 and the adiabatic principle: at
the start ofPQAOA execution the initial state is fixed to
|si = √12n
|zi, an Eigenstate of the global X operator.

FIG. 3: Our strategy for using QAOA in a regime with a large
number of steps p reuse the training performed by the agent
at a fixed episode length p to start the training at episodes
with p0 > p. We plot here the results of successive training of
a system of N = 21 initially trained with p = 10. After completion, data collected in this training phase is reused to solve
the new episodes with p = 15, 20, progressively improving the
results of the previous phase. We iterate again to reach training episodes with p = 25. An horizontal line marks the global
optimal value. Inset: evolution of the best reward obtained
for each value of p.

z

The final state is an Eigenstate of a diagonal Hamiltonian C in the computational basis. To describe the state
along QAOA evolution we choose the local observables
hXii and hZii , ∀i : 1 ≤ i ≤ N . The results shown
in Fig.2 include an evaluation of each of the terms Cij ,
which has positive benefits for convergence. However,
tests performed without these terms will deliver optimal
results with a reduced number of measurements. This
compromise between measurements and convergence has
to be explored for each setting.
B.

RESULTS

We report in Fig.3 results for a MAXCUT problem
with N = 21 and up to p = 25. After an initial training
with p = 10, the agent is exposed to longer episodes with
p = 15, 20 and p = 25. At the transition between training chapters where the value of p is changed, instabilities appear initially as the agent optimizes the actions to
the new setup. However, these instabilities are soon corrected and the agent reaches consistently larger rewards.
Reaching larger values of p is essential to a deep understanding of the computational capabilities of QAOA,
as higher rewards are returned and eventually one may
reach the exact solution. We show the maximum reward
returned for values of p = 5, 10, 15, 20, 15 in the inset of

Extending episode length to p0 > p

From the QAOA formulation we observe that reaching
larger values of p is a key element to obtain larger final
rewards. Results reported in Fig.2 are obtained after fixing p at the start of the training process. This translates
4

B

Extending episode length to p0 > p

V

years, and successfully applied to a wide range of problems in robotics and optimal control. Numerical stability
allows us to reach a regime of large p where QAOA delivers increasingly higher rewards. The tools developed
here contribute to understand how QAOA performs in a
large range of p values.
A fundamental difference in the execution of our
method would appear running the Reinforcement Learning algorithm in a real Quantum device. For a given
intermediate step k of an episode, the Quantum state is
repeatedly prepared to obtain the set of observed quantities. This requires a memory expression of the previous
steps in terms of classical parameters, i.e. {γ, β}i , ∀i < k.
This little overhead results in a longer state preparation
for later steps on each episode. However, this overhead
can be reduced by grouping more actions inside each
episode, or equivalently reducing the number of Quantum observations.

Fig.3.

V.

SUMMARY

SUMMARY

We have formulated the QAOA as an agentenvironment interaction where a classical agent controls
a Quantum device. Applying Reinforcement Learning
strategies to QAOA we show how to interactively drive
the execution of Quantum algorithms in order to solve
hard combinatorial problems. Our agent controls the
optimization having access to a selection of information about the performance of the algorithm available
through Quantum measurements, a collection of data
that extends the capabilities of global optimizators relying solely on the value of the objective function. Our results suggest that this information is a valuable resource
in running-time optimization, and that Reinforcement
Learning methods used in our work are able to exploit
the advantage provided by these observations.
We have explored the QAOA parameter space by
means of Reinforcement Learning methods formulated
for a continuous action space. Our work allows a general revision of Quantum algorithms using the corpus of
Reinforcement Learning techniques developed in the last

Acknowledgement

We acknowledge funding from project FIS2017-89860P (MINECO/AEI/FEDER, UE) and support of NVIDIA
Corporation for this research.

[1] Quantum Computing in the NISQ era and beyond. John
Preskill. Quantum 2, 79 (2018)
[2] The theory of variational hybrid Quantum-classical algorithms. Jarrod R McClean et al. New J. Phys. 18 023023
(2016)
[3] A Quantum Approximate Optimization Algorithm.
Edward Farhi, Jeffrey Goldstone, Sam Gutmann.
arXiv:1411.4028 (2014)
[4] A Quantum Approximate Optimization Algorithm Applied to a Bounded Occurrence Constraint Problem. Edward Farhi, Jeffrey Goldstone, Sam Gutmann.
arXiv:1412.6062 (2014)
[5] The Quantum Approximation Optimization Algorithm
for MaxCut: A Fermionic View. Zhihui Wang, Stuart
Hadfield, Zhang Jiang, Eleanor G. Rieffel. Phys. Rev. A
97, 022304 (2018)
[6] Quantum Approximate Optimization Algorithm:
Performance, Mechanism, and Implementation on
Near-Term Devices. Leo Zhou, Sheng-Tao Wang,
Soonwon Choi, Hannes Pichler, Mikhail D. Lukin.
https://arxiv.org/abs/1812.01041 (2018)
[7] A Quantum Approximate Optimization Algorithm
for continuous problems. Guillaume Verdon, Juan
Miguel Arrazola, Kamil Brádler, Nathan Killoran.
arXiv:1902.00409 (2019)
[8] Quantum Approximate Optimization with a Trapped-Ion
Quantum Simulator. G. Pagano, A. Bapat, P. Becker, K.
S. Collins, A. De, P. W. Hess, H. B. Kaplan, A. Kyprianidis, W. L. Tan, C. Baldwin, L. T. Brady, A. Deshpande, F. Liu, S. Jordan, A. V. Gorshkov, C. Monroe.
arXiv:1906.02700 (2019)
[9] Analysis of Quantum Approximate Optimization Algo-

[10]

[11]
[12]

[13]

[14]

[15]

[16]

[17]

[18]

5

rithm under Realistic Noise in Superconducting Qubits.
Mahabubul Alam, Abdullah Ash-Saki, Swaroop Ghosh.
arXiv:1907.09631 (2019)
Quantum Supremacy through the Quantum Approximate Optimization Algorithm. Edward Farhi, Aram W
Harrow. arXiv:1602.07674 (2016)
Quantum approximate optimization is computationally
universal. Seth Lloyd. arXiv:1812.11075 (2018)
On the Universality of the Quantum Approximate Optimization Algorithm. Mauro E. S. Morales, Jacob Biamonte, Zoltán Zimborás. arXiv:1909.03123 (2019)
Performance of the Quantum Approximate Optimization Algorithm on the Maximum Cut Problem, Gavin
E. Crooks, arXiv:1811.08419 (2018)
Optimizing Quantum optimization algorithms via faster
Quantum gradient computation, András Gilyén, Srinivasan Arunachalam, Nathan Wiebe. In Proceedings of
the 30th ACM-SIAM Symposium on Discrete Algorithms
(SODA 2019), pp. 1425-1444
Comparison of QAOA with Quantum and Simulated Annealing. Michael Streif, Martin Leib. arXiv:1901.01903
(2019)
Learning to learn with Quantum neural networks via
classical neural networks. Guillaume Verdon, Michael
Broughton, Jarrod R. McClean, Kevin J. Sung, Ryan
Babbush, Zhang Jiang, Hartmut Neven, Masoud
Mohseni. arXiv:1907.05415 (2019)
Optimizing QAOA: Success Probability and Runtime Dependence on Circuit Depth. Murphy Yuezhen Niu, Sirui
Lu, Isaac L. Chuang.
Richard S. Sutton, and Andy G. Barto. Reinforcement
learning: An introduction. Vol. 2. Cambridge: MIT press

V
(2017)
[19] Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex
Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg
Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik,
Ioannis Antonoglou, Helen King, Dharshan Kumaran,
Daan Wierstra, Shane Legg, Demis Hassabis. Nature volume 518, pages 529-533 (2015)
[20] Continuous Deep Q-Learning with Model-based Acceleration, Shixiang Gu, Timothy Lillicrap, Ilya Sutskever,
Sergey Levine, arXiv:1603.00748 (2016)
[21] Continuous control with deep reinforcement learning.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander
Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David
Silver, Daan Wierstra. arXiv:1509.02971 (2015)
[22] Fidelity-based Probabilistic Q-learning for Control of
Quantum Systems. Chunlin Chen, Daoyi Dong, HanXiong Li, Jian Chu, Tzyh-Jong Tarn. IEEE Transactions on Neural Networks and Learning Systems, VOL.
25, NO. 5, pp.920-933 (2014)
[23] Reinforcement Learning in Different Phases of Quantum
Control. Marin Bukov, Alexandre G.R. Day, Dries Sels,
Phillip Weinberg, Anatoli Polkovnikov, Pankaj Mehta.
Phys. Rev. X 8, 031086 (2018)

SUMMARY

[24] Universal Quantum Control through Deep Reinforcement
Learning. Murphy Yuezhen Niu, Sergio Boixo, Vadim
Smelyanskiy, Hartmut Neven. arXiv:1803.01857 (2018)
[25] Automated Quantum programming via reinforcement
learning for combinatorial optimization, Keri A. McKiernan, Erik Davis, M. Sohaib Alam, Chad Rigetti.
https://arxiv.org/abs/1908.08054 (2019)
[26] Reinforcement-Learning-Based
Variational
Quantum Circuits Optimization for Combinatorial Problems. Sami Khairy, Ruslan Shaydulin, Lukasz
Cincio,
Yuri Alexeev,
Prasanna Balaprakash.
https//arXiv.org/abs/1911.04574 (2019)
[27] Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman
and Jie Tang and Wojciech Zaremba, OpenAI Gym,
arXiv:1606.01540 (2016)
[28] Abadi et al. TensorFlow: Large-scale machine learning on
heterogeneous systems, (2015). Software available from
tensorflow.org.
[29] Matthias Plappert, keras-rl, https://github.com/
keras-rl/keras-rl
[30] Ken M. Nakanishi, QUPY: A Quantum circuit simulator for both CPU and GPU, https://github.com/
ken-nakanishi/qupy

6

