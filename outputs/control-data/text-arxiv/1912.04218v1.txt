Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.
Digital Object Identifier 10.1109/ACCESS.2017.DOI

arXiv:1912.04218v1 [eess.SP] 14 Nov 2019

A Neural Network Based on the Johnson
SU Translation System and Related
Application to Electromyogram
Classification
HIDEAKI HAYASHI1 , (Member, IEEE), TARO SHIBANOKI2 , (Member, IEEE), AND TOSHIO
TSUJI3 , (Member, IEEE)
1
2
3

Department of Advanced Information Technology, Kyushu University, Fukuoka, Japan (e-mail: hayashi@ait.kyushu-u.ac.jp)
College of Engineering, Ibaraki University, Hitachi, Japan
Department of System Cybernetics, Graduate School of Engineering, Hiroshima University, Higashi-hiroshima, Japan

Corresponding author: Hideaki Hayashi (e-mail: hayashi@ait.kyushu-u.ac.jp).
This work was supported in part by JSPS KAKENHI Grant Number JP17K12752.

ABSTRACT Electromyogram (EMG) classification is a key technique in EMG-based control systems.
The existing EMG classification methods do not consider the characteristics of EMG features that the
distribution has skewness and kurtosis, causing drawbacks such as the requirement of hyperparameter
tuning. In this paper, we propose a neural network based on the Johnson SU translation system that is
capable of representing distributions with skewness and kurtosis. The Johnson system is a normalizing
translation that transforms non-normal data to a normal distribution, thereby enabling the representation
of a wide range of distributions. In this study, a discriminative model based on the multivariate Johnson
SU translation system is transformed into a linear combination of coefficients and input vectors using loglinearization. This is then incorporated into a neural network structure, thereby allowing the calculation of
the posterior probability of the input vectors for each class and the determination of model parameters as
weight coefficients of the network. The uniqueness of convergence of the network learning is theoretically
guaranteed. In the experiments, the suitability of the proposed network for distributions including skewness
and kurtosis is evaluated using artificially generated data. Its applicability for real biological data is also
evaluated via an EMG classification experiment. The results show that the proposed network achieves high
classification performance without the need for hyperparameter optimization.
INDEX TERMS Biomedical signal processing, electromyography, Johnson distribution, neural networks,
pattern recognition

I. INTRODUCTION

Biosignals such as electroencephalograms (EEGs), electrocardiograms (ECGs), and electromyograms (EMGs) strongly
reflect a human’s internal state and intentions, and have therefore been applied to human–machine interfaces and diagnosis
[1]–[4]. In particular, EMG-based control systems have been
widely studied, because EMGs can be voluntarily controlled.
Many practical applications have been developed, typified by
myoelectric prosthetics, which are prosthetic hands that can
be controlled using surface EMGs [5], [6].
According to Oskoei and Hu [7], EMG-based control
systems include four main stages: data segmentation, feature
extraction, classification, and control. The raw data are first
VOLUME 4, 2016

segmented, and then converted into feature vectors. These
vectors are classified into predefined categories, before the
controller generates output commands for the instruments
based on the classification results.
To realize highly intuitive and dexterous control, it is
particularly important to achieve a high level of classification
performance, both in terms of accuracy and speed of training
and prediction. Classifiers such as the support vector machine
(SVM) [8], multilayer perceptron (MLP) with back propagation learning [9], and k-nearest neighbors algorithm (k-NN)
[10] have been widely used. These popular techniques, however, are not always the most suitable for EMG classification
in spite of their high classification abilities. For example,
1

Hayashi et al.: A Neural Network Based on the Johnson SU Translation System and Related Application to Electromyogram Classification

SVMs are computationally expensive for hyperparameter
optimization, and the MLP requires a very long training time.
Similarly, it is difficult to use k-NN in real-time applications
because of the large computational cost of prediction.
To improve the classification ability for a certain purpose,
stochastic models can be incorporated into the structure of
the classifier if there is some prior knowledge of the input
signals [11]–[14]. For instance, Tsuji et al. [14] proposed
a Gaussian mixture model-based neural network, known as
the log-linearized Gaussian mixture network (LLGMN), by
assuming that the input signals obey a Gaussian mixture
model.
The authors now consider the following assumptions to
be the prior knowledge of the feature vectors obtained from
EMG signals:
• The distribution of input signals for each class is unimodal
• The distribution has skewness and kurtosis
The derivation of these assumptions are explained in Section
III.
To satisfy the above assumptions, we can use a flexible
distribution known as the Johnson distribution [15], which
represents the mean, variance, skewness, and kurtosis using
four parameters. Its extension to higher dimensions is enabled by the multivariate Johnson translation system [16]–
[18]. If we could construct a classifier that incorporates the
multivariate Johnson distribution in its structure, it would be
applicable to EMG classification and EMG-based systems.
This paper proposes a neural network (NN) based on the
Johnson SU translation system. The proposed NN represents
a flexible distribution by including a discriminative model
based on the multivariate Johnson SU translation system,
thereby supporting the accurate classification of data with
skewness and kurtosis. The parameters of the model can be
determined as weight coefficients of the proposed NN via
learning.
This paper is related to our previous workshop paper [19].
Our previous work was preliminary, and has the following
drawbacks:
• The network structure does not correctly represent the
Johnson distribution due to the lack of the Jacobian.
• The training algorithm does not guarantee the uniqueness of convergence.
• The dataset variation and comparison are limited in the
EMG classification experiments.
In this paper, the above problems have been solved.
The rest of this paper is organized as follows: Related
studies and their characteristic comparisons are described
in Section II. Section III explains the derivation of the
above assumptions, then describes a discriminative model
based on the multivariate Johnson translation system and its
transformation to linear combinations of weight coefficients
and input vectors via log-linearization. The structure and
learning algorithms of the proposed NN are presented in
Section IV, and the results of a simulation experiment using
2

artificial data are described in Section V. Section VI outlines
the application potential for biosignal classification based
on an EMG classification experiment. Finally, Section VII
concludes the paper.
II. RELATED WORK

This section summarizes popular algorithms for EMG classification, and compares each of their characteristics. The
algorithms compared in this section are:
• SVM [8]
• LLGMN [14]
• MLP [9]
• Linear logistic regression (LLR) [20]
• k-NN [10]
• Random forest
These algorithms are compared in terms of the following
significant factors for EMG classification and EMG-based
control systems: (non-)requirement of hyperparameter optimization, speed of training, uniqueness of solutions, speed
of prediction, nonlinearity, and computability of posterior
probabilities. The first three factors are associated with the
effort needed to construct the classifier. Hyperparameter optimization is conducted before training, and typically takes a
very long time; hence, the usability of the system is enhanced
if this step is not required. Fast training and a unique solution
are also desirable to avoid effort and uncertainty in the
training of the classifier. Systems that are to be deployed in
an online manner require fast prediction, while the nonlinearity and computability of posterior probabilities is related
to the accuracy of classification. Although EMG classification problems are unlikely to be linear, their nonlinearity
is not overly complex, because each class of EMG signals
can be clustered to some extent. Calculating the posterior
probabilities has some powerful merits, such as minimizing
risk, rejecting options, compensating for class priors, and
combining models [20].
Table 1 summarizes the characteristics of the classification
algorithms. An SVM is distinguished classifier that realizes
fast training and a unique solution. Its problem, however, is
that two hyperparameters must be optimized. Additionally,
because an SVM was originally developed as binary classifier, multi-class classification can take a long time.
The LLGMN is a discriminative model that incorporates
Gaussian mixture models into an NN structure, allowing
the posterior probability to be accurately calculated. The
number of components (how many Gaussian distributions
are summed in the model) should be carefully determined,
because the classification ability of the LLGMN for data
following a non-Gaussian distribution decreases when there
are few components.
The MLP is generally more compact than an SVM, and
hence gives faster predictions, although training has a large
computational cost. The number of layers and units should
be determined as hyperparameters.
The LLR is a probabilistic discriminative model that can
be trained using Newton’s method. The structural limit of the
VOLUME 4, 2016

Hayashi et al.: A Neural Network Based on the Johnson SU Translation System and Related Application to Electromyogram Classification

TABLE 1. Characteristics of classification algorithms

Unique solution
X
×
×
X
n/a

After EMG signals have been acquired, we extract significant
features. Although the raw EMG signals can be considered to
obey a Gaussian distribution with zero mean [21], the distribution of the extracted features may exhibit some skewness
and kurtosis. We describe how the features obtain skewness
and kurtosis in the process of feature extraction. Although
many feature extraction methods have been proposed [22]–
[24], we focus on the method of Fukuda et al. [6] because of
its simplicity and universality.
Fukuda’s method consists of two main parts: rectification
and smoothing based on a Butterworth low-pass filter. Rectification takes the absolute value of the raw EMG signals,
converting negative EMG values to positive values. Rectification is also used in methods such as integrated EMG
(IEMG), mean absolute value (MAV), and modified mean
absolute value (MMAV) [25], and is strongly related to the
occurrence of skewness and kurtosis. Let x be a raw EMG
signal that obeys a Gaussian distribution with a mean of 0
and a standard deviation of σ. The skewness and kurtosis of
x are both 0. Fig. 1 (a) shows an example raw EMG signal x
and its histogram.
The probability density function of the rectified EMG
signal y = |x| is represented as
h
i
(
y2
√ 2
exp
−
(0 ≤ y < ∞)
2
2
2σ
2πσ
. (1)
p(y) =
0
(−∞ < y < 0)
The mean My and the variance Vy of y is then calculated as
r
Z ∞
2
My =
yp(y)dy =
σ,
(2)
π
−∞
Z ∞
2
Vy =
(y − My )2 p(y)dy = (1 − )σ 2 .
(3)
π
−∞
VOLUME 4, 2016

Nonlinearity
X
X
X
×
X

Posterior probability
×
X
×
X
×

180
Frequency

0.4
0

−0.4
0

1.0

Time [s]

0
−0.4

0
x [mV]

0.4

(a) Raw EMG signal x
200
0.4
Frequency

III. MODEL STRUCTURE
A. SKEWNESS AND KURTOSIS IN PROCESSED EMG
SIGNALS

Fast prediction
×
X
X
X
×

0
0

Time [s]

1.0
0
0

y [mV]

0.4

z

0.08

(b) Rectified EMG signal y
250
0.08
Frequency

LLR is its inability to solve nonlinear separation problems.
The k-NN is a very simple algorithm that does not require
training. However, predictions entail large computational
expense, as k-NN compares the distance between the input
vector and every training vector. The constant k, the number
of nearest neighbors used in the voting, is a user-defined
constant, and is selected by heuristic techniques in general.
The proposed NN is designed to optimize the above characteristics as much as possible. In particular, there is no
need to optimize the hyperparameters, and the uniqueness of
solutions and computability of posterior probabilities can be
theoretically explained.

x [mV]

Fast training
X
×
×
X
n/a

y [mV]

Hyperparameter-free
×
×
×
X
×

z

Algorithm
SVM
LLGMN
MLP
LLR
k-NN

0
0

Time [s]

1.0
0
0.06

(c) Smoothed EMG signal z
FIGURE 1. Examples of the time-series signal and histogram of (a) raw EMG
x, (b) rectified EMG y, and (c) smoothed EMG z. (a) raw EMG x obeys a
Gaussian distribution with zero mean (this is also discussed in Hogan and
Mann [21]). The histograms of (b) rectified EMG y and (c) smoothed EMG z,
however, become asymmetric and include skewness and kurtosis.

In the rectified signal y, the skewness Sy and the kurtosis Ky
are no longer 0. They can be calculated as follows [26]:
q
R∞
4
2
3
(
−
1)
(y
−
M
)
p(y)dy
y
π
π
Sy = −∞
=
6= 0, (4)
3
3
(1 − π2 ) 2
Vy2
R∞
(y − My )4 p(y)dy
Ky = −∞
−3
Vy2
=

(3 − π4 − π122 )
− 3 6= 0.
(1 − π2 )2

(5)

The influence of rectification is also visible in the smoothed
signal z (see Fig. 1 (c)).
3

Because the extracted features include skewness and kurtosis, conventional Gaussian-based models cannot readily
model the data. In the next subsection, we therefore adopt
the multivariate Johnson translation system [16], which is
suitable for data with skewness and kurtosis.

Probability density

Hayashi et al.: A Neural Network Based on the Johnson SU Translation System and Related Application to Electromyogram Classification

0.9

0
−2.0

B. MULTIVARIATE JOHNSON TRANSLATION SYSTEM

The Jonson translation system is a family of transformations
of a non-normal variate to a standard normal variate proposed
by N. L. Johnson in 1949 [15]. Based on this translation,
Johnson derived a system of distributions that is suitable
for representing distributions with skewness and kurtosis. Its
multivariate extension is also proposed [16].
Consider a d-dimensional continuous random vector x ∈
Rd with skewness and kurtosis. The multivariate Johnson
translation system [16] involves the normalizing translation:
z = γ + δg[λ−1 (x − ξ)] ∼ N (0, Σ),

(6)

where z is a random vector obeying a normal distribution
T
with mean 0 and variance Σ, γ ≡ [γ1 , . . . , γd ] and δ ≡
diag[δ1 , . . . , δd ] are shape parameters, λ ≡ diag[λ1 , . . . , λd ]
T
is a scale parameter, ξ ≡ [ξ1 , . . . , ξd ] is a location paramT
eter, and g(·) ≡ [g1 (·), . . . , gd (·)] denotes the transformation function that determines the family of a system. gi (·)
(i = 1, . . . , d) is defined by the following four functions:

ln h(y)


i for SL (lognormal)
p


2
ln y + y + 1 for SU (unbounded)
.
gi (y) =

ln
[y/(1
−
y)]
for
S
(bounded)

B


y
for SN (normal)
(7)
The domains of xi for SL , SU , SB , and SN are (ξ, +∞),
(−∞, +∞), (ξ, ξ + λ), and (−∞, +∞), respectively. In (6),
parameters λ and ξ affects the location and scale of the
distribution of x, respectively. The combination of γ and δ
are associated with skewness and kurtosis, and g[·] decides
the shape of distribution tails, i.e. whether the distribution
tails have boundary or go to infinity.
Since EMG can be seen as a random process, the systems
with bounds are not suitable for EMG classification because
probabilities cannot be calculated if an observation is out
of the domain. In unbounded systems, SU is expected to
be an extension of the normal distribution in particular, and
have enough flexibility to fit data from arbitrary unimodal
distribution. This paper therefore focuses on SU as the form
of the function gi (y). Fig. 2 shows an example of the Johnson
SU distribution, which can be calculated from the Johnson
SU translation system (d = 1). This asymmetric distribution
represents skewness and kurtosis, and seems to be adaptable
to the histogram of smoothed EMG shown in Fig. 1 (c).
C. POSTERIOR PROBABILITY ESTIMATION

To classify the vector x ∈ Rd into one of the given C
classes, we must examine the posterior probability P (c|x)
(c = 1, . . . , C). First, x is translated into a vector z (c) using
(6), which has the four parameters γ (c) , δ (c) , λ(c) , and ξ (c) .
4

2.0

x

FIGURE 2. Probability density function of the Johnson SU distribution (d = 1,
γ1 = −1.4, δ1 = 1.0, λ1 = 0.3, ξ1 = −1.5). This distribution represents
skewness and kurtosis, hence the shape becomes asymmetric.

Assuming that the translated vector obeys a normal distribution, the posterior probability of x for class c is calculated
as
P (c)P (x|c)
,
(8)
P (c|x) = PC
c=1 P (c)P (x|c)


|J(c) |
1 (c) T (c) −1 (c)
P (x|c) =
exp
−
z
Σ
z
,(9)
1
d
2
(2π) 2 |Σ(c) | 2
where P (c) is the prior probability of c, Σ(c) is the variance
matrix of z (c) , and J(c) is the d × d Jacobian matrix, whose
(i, j)th element is given by
(
(c)
(c) (c) −1 0 (c) −1
(c)
∂zi
g [λi
(xi − ξi )] (i = j) ,(10)
= δi λ i
∂xj
0
(i 6= j)
where

1/y


 p 2
1/ y + 1
gi0 (y) =
 1/[y(1 − y)]


1

for SL (lognormal)
for SU (unbounded)
. (11)
for SB (bounded)
for SN (normal)

The determinant of J(c) is therefore calculated as
"
#)
(
d
d
(c)
(c)
(c)
Y
Y
∂zi
δi 0 (xi − ξi )
(c)
|J | =
g
. (12)
=
(c)
∂xi i=1 λ(c)
λi
i=1
i
D. LOG-LINEARIZATION

To incorporate the probabilistic model described above into
a network structure, we transform the calculation of the
Johnson translation and posterior probability estimation to
linear combinations of coefficient matrices and input vectors.
First, let y (c) be a calculation in the function g(·) of (6).
(c)
y is then transformed as follows:
y (c) = λ(c)
=λ


−1

(c) −1

x − λ(c)

−1 (c)
−λ(c) 1 ξ1

..
.


=

=

(x − ξ (c) )
ξ

(c) −1
λ1



..

−1 (c)
−λ(c) d ξd
(1)
(c) T

W

−1 (c)

X.

0

.

0  
 1
(c) −1
λd

 x

(13)

(c)

Hence, y is expressed by multiplying the coefficient matrix (1) W(c) ∈ R(d+1)×d and the augmented input vector
X ∈ Rd+1 .
VOLUME 4, 2016

Hayashi et al.: A Neural Network Based on the Johnson SU Translation System and Related Application to Electromyogram Classification

d
Y

1 (c)
(c)
− δi,j )si,j , . . .,− sd,d ]Z(c)
2

(c)

g 0 (yi )

i=1
T

= (3) W(c) Z(c) +

d
X

(c)

log g 0 (yi ),

(16)

i=1
(c)

(c)

where s1,1 , . . . , sd,d are elements of the inverse matrix
(c) −1

Σ
, and δi,j is the Kronecker delta (1 if i = j, 0
d
otherwise). Note that (2π)− 2 in (9) is omitted because it
is canceled out in (8). Additionally, Z(c) ∈ RH (H =
1 + d(d+1)
) is defined as
2
(c) 2

Z(c) = [1, z1
(c) 2

z2

(c) (c)

(c) (c)

, z1 z2 , . . . , z1 zd ,
(c) 2

(c) (c)

, z2 z3 , . . . , zd

].

(17)

Taking the exponent of (16), ζc (i.e., the numerator of (8)) is
ultimately expressed by
!
d
X
T
(c)
ζc = exp (3) W(c) Z(c) +
log g 0 (yi )
(18)
i=1

As outlined above, the Johnson translation and posterior
probability estimation are calculated as linear combinations
of coefficient matrices and nonlinearly transformed input
vectors. If these coefficients are appropriately determined,
the parameters and structure of the model can be defined, and
therefore the posterior probability of the input vectors can be
calculated for each class.
The next section describes how the NN weight coefficients
(1)
W(c) , (2) W(c) , and (3) W(c) are determined via learning.
VOLUME 4, 2016

O1

(5)

Oc

(5)

OC

࣭࣭࣭

+log

1 (c)
1
− log |Σ(c) |, − s1,1 ,
2
2

1

w(c)
h

(5)

࣭࣭࣭

2

(c)
λi

࣭࣭࣭

i=1

1
(c)
−s1,2 , . . . , − (2

δi

1

w(c)
j ,k

࣭࣭࣭

(c)

log

(2)

Oc,h

࣭࣭࣭

d
X

w

(c)
i, j

࣭࣭࣭

log ζc = [log P (c)+

(1)

࣭࣭࣭

and taking the log-linearization of ζc gives

Oc,k

࣭࣭࣭

(15)

( 4)

( 3)

(3)

࣭࣭࣭

ζc = P (c)P (x|c)

x2

࣭࣭࣭

where (2) W(c) ∈ R(d+1)×d is a coefficient matrix and
Y(c) ∈ Rd+1 is determined by a nonlinear transformation
of y (c) .
Finally, setting

x1

1

Oc, j

࣭࣭࣭

(14)

1( 2)

Oi

࣭࣭࣭

T

= (2) W(c) Y(c) ,

1 (1)

࣭࣭࣭

Second, the translated vector z (c) is also transformed and
expressed as the product of a coefficient matrix and an
augmented vector as follows:


z (c) = γ (c) + δ (c) g y (c)


(c)
(c)
γ1
δ1


0
 .

1 
..

.
=
.
 .
 g y (c)
(c)
(c)
δd
γd
0

xd
FIGURE 3. Structure of the proposed NN. This network is constructed by
incorporating the posterior probability calculation based on the Johnson SU
translation system into the network structure, and consequently consists of five
layers. Symbols , , and ⊗ denote a summation unit, identity unit, and
multiplication unit, respectively. The weight coefficients between the
first/second layers and the second/third layers correspond to the parameters of
the Johnson translation system. The weight coefficients between the
fourth/fifth layers correspond to the probabilistic parameters such as the prior
probability and the variance matrix. Because of this structure, the output
(5)
Oc of this network estimates the posterior probability of each class c given
x, P (c|x).

IV. PROPOSED NEURAL NETWORK
A. NETWORK STRUCTURE

Fig. 3 shows the structure of the proposed NN. This is a fivelayer feedforward network with weight coefficients (1) W(c) ,
(2)
W(c) , and (3) W(c) between the first/second, second/third,
and fourth/fifth layers, respectively. Symbols , , and ⊗
denote a summation unit, identity unit, and multiplication
unit, respectively. Because of this structure, the output (5) Oc
of this network estimates the posterior probability of each
class c given x, P (c|x).
The first layer consists of d + 1 units corresponding to the
dimensions of the input data x. The relationship between the
input and the output is defined as

1 (i = 0)
(1)
Ii =
,
(19)
xi (i = 1, . . . , d)
(1)
(1)

Oi = (1) Ii ,

(20)

(1)

where Ii and Oi are the input and output of the ith unit,
respectively. This layer corresponds to the construction of X
in (13).
The second layer is composed of C(d + 1) units, each
receiving the output of the first layer weighted by the co(c)
efficient (1) wi,j . The relationship between the input (2) Ic,j
and the output (2) Oc,j of unit {c, j} (c = 1, . . . , C, j =
1, . . . , d + 1) is described as
(2)

Ic,j =

d
X

(1)

(c)

wi,j (1) Oi ,

(21)

i=0


(j = 0)
 1
(2)
g((2) Ic,j )
(j = 1, . . . , d) ,(22)
Oc,j =
 Pd
0 (2)
0
Ic,j ) (j = d + 1)
j 0 =1 log g (
5

Hayashi et al.: A Neural Network Based on the Johnson SU Translation System and Related Application to Electromyogram Classification

(c)

where the weight coefficient (1) wi,j is an element of the
matrix (1) W(c) , which is given as:


(1)

W

(c)

(1)

(c)

w0,1
 (1) (c)
w1,1

=



(1)

...
..

(c)

w0,d



0



.



.
(1)

0

(c)

(23)

wd,d

This layer is equal to the multiplication of (1) W(c) and X
in (13), the construction of Y(c) in (14), and non-coefficient
part of Jacobian in (12).
The third layer is comprised of Cd units. The relationship
between the input (3) Ic,k and the output (3) Oc,k is defined as
(3)

d
X

Ic,k =

(2)

(c)

wj,k (2) Oc,j ,

(24)

j=0
(3)

Oc,k = (3) Ic,k ,

(25)

B. LEARNING ALGORITHM

This subsection describes a learning algorithm that can acquire a unique optimal solution without any hyperparameters.
The learning algorithm consists of two steps.
In the first step, we estimate (1) W(c) and (2) W(c) , which
contain the parameters of the Johnson translation system.
Although various parameter estimation algorithms have been
proposed for the Johnson translation system, this paper
adopts the percentile method [27], because it analytically estimates the parameters with a certain degree of accuracy. The
percentile method calculates Johnson system parameters by
comparing distances in the tails with distances in the central
portion of the distribution. Using the percentile method, the
parameters for the Johnson translation system are determined
as follows:
2z
(c)
 
 ,
(31)
δi =
(c)
(c)
mi
ni
cosh−1 12
+
(c)
(c)
pi
pi


n

(2) (c)
wj,k

where the weight coefficient
is an element of the
(2)
(c)
matrix W , which can be written as:

(c) 
(2) (c)
w0,1 . . . (2) w0,d

 (2) (c)
w1,1

0 
(2)
.
(26)
W(c) = 
..


.


(2) (c)
wd,d
0
This layer corresponds to the multiplication of (2) W(c) and
Y(c) in (14).
) units. The
The fourth layer has CH (H = 1 + d(d+1)
2
relationship between the input (4) Ic,h and the output (4) Oc,h
of units {c, h} (h = 1, . . . , H) is defined as

 1 (h = 1)
(4)
, (27)
Ic,h = (3) Oc,k (3) Oc,k0

(h = k 0 − 12 k 2 +(d+ 12 )k−d+1)
(4)

Oc,h = (4) Ic,h ,

(28)

where k ≤ k 0 (k 0 = 1, . . ., d), and (27) corresponds to the
nonlinear conversion shown in (17).
Finally, the fifth layer consists of C units, and its input
(5)
Ic and output (5) Oc are
(5)

Ic =

H
X

(3)

(c)
wh (4) Oc,h

+

(2)

Oc,d+1 ,

(29)

h=1
(5)

exp

Oc = PC

c0 =1

(5)

exp

Ic



(5) I 0
c

.

(30)

The output (5) Oc corresponds to the posterior probability for
class c, P (c|x). Here, the posterior probability P (c|x) can
be calculated if the NN coefficients (1) W(c) , (2) W(c) , and
(3)
W(c) have been appropriately established.
6

(c)

γi

(c)

m

(c)

i
i


(c) −
(c)
pi
pi

(c)
−1 
= δi sinh  
,
1
2 


(c) (c)
m i ni
2
(c) 2 − 1

(32)

(pi )

(c)

λi

(c)

ξi

(c)
2pi



(c)

(c)

m i ni
(c)

(pi )2

 12
−1


 12 , (33)
(c)
(c)
mi
ni
(c) +
(c) − 2
(c) +
(c) + 2
pi
pi
pi
pi


(c)
(c)
ni
mi
(c)
pi
(c)
(c)
(c) −
(c)
xz,i + x−z,i
pi
pi
,
(34)
=
+  (c)
(c)
2
mi
ni
2
(c) +
(c) − 2

=

(c)

(c)

mi

ni

pi

pi

where z > 0 is chosen depending on the number of data
points, and i = 1, . . . , d, c = 1, . . . , C are indices corresponding to the dimension and class, respectively. The
(c)
variable xζ,i (ζ = −3z, −z, z, 3z) is the Pζ th percentile
of the ith dimension of training data for class c, where
Pζ is the percentage of the area in the normal distribution
(c)
(c) (c)
corresponding to ζ. Using percentiles, mi , ni , pi are
calculated as:
(c)

mi

(c)
ni
(c)
pi

(c)

(c)

= x3z,i − xz,i ,
=
=

(c)
(c)
x−z,i − x−3z,i ,
(c)
(c)
xz,i − x−z,i .

(35)
(36)
(37)

For more detail, refer to [27]. (1) W(c) and (2) W(c) can then
(c)
(c)
(c)
(c)
be determined by substituting δi , γi , λi , and ξi in (13)
and (14).
The second step concerns the discriminative learning of
the remaining weight (3) W(c) , which includes probabilistic
parameters such as the prior probability P (c) and covariance
matrix Σ(c) . A set of vectors x(n) (n = 1, . . . , N ) is given
(n)
(n)
for training, with the teacher vector T(n) = [T1 , . . . , Tc ,
(n)
(3)
. . . , TC ] for the nth input. The training process of W(c)
VOLUME 4, 2016

Hayashi et al.: A Neural Network Based on the Johnson SU Translation System and Related Application to Electromyogram Classification

TABLE 2. Parameters for data generation in the simulation experiment

ξ
0.15
0.7
0.5
0.55

Class 1
Class 2

λ
0.04
0.05
0.05
0.01

δ
0.9
0.8
0.8
0.5

γ
-0.9
0.5
0.5
-0.5

1.0

Σ−1
0.6
0.9

involves minimizing the energy function E, which is defined
as
N
N X
C
X
X
E=
En = −
Tc(n) log (5) Oc(n) ,
(38)
n=1

x2 0.5

n=1 c=1
(n)

to maximize the log-likelihood. Here, (5) Oc is the output
(c)
for an input vector x(n) . The weight modification for (3) wh
based on Newton’s method is defined as
(3)

Wnew = (3) Wold − H−1 ∇E,

(39)

where (3) Wold and (3) Wnew are the weight coefficients
before and after weight modification, which have (3) W(c) in
the cth block. ∇En is the gradient vector whose hth element
in the cth block can be calculated as
∂E
(c)
∂ (3) wh

=

=

=

N
X

∂En

(3) w (c)
n=1 ∂
h
N X
C
X

∂En

(n)

(n)

∂ (5) Oc0 ∂ (5) Ic

(5) O (n) ∂ (5) I (n) ∂ (3) w (c)
c
n=1 c0 =1 ∂
c0
h
N
X
(n)
((5) Oc(n) − Tc(n) )(4) Oc,h .
n=1

(40)

H is the Hessian matrix comprised of H × H blocks, where
the (h, l) element of block (c, k) is
N
X

∂ 2 En
(c)

n=1

=

(k)

∂ (3) wh ∂ (3) wl

N
X

(5)

(n)

(n)

(n)

Ok (δc,k − (5) Oc(n) )(4) Oc,h (4) Ok,l . (41)

n=1

Note that H is positive semi-definite (see Appendix A). It
follows that E is a convex function of (3) W(c) , and hence
has a unique minimum.
Using this algorithm, the process of training the network
converges to a unique solution without the need for any
hyperparameters.
V. SIMULATION EXPERIMENT
A. METHOD

To verify that the proposed network can properly calculate the posterior probability for data with skewness and
kurtosis, we performed a simulation experiment using twodimensional (d = 2) two-class (C = 2) data. The data were
artificially generated using the inverse of the multivariate
Johnson SU translation system [16]. Table 2 lists the parameters used for each class in this generation. An example of a
dataset used in the experiments is shown in Fig. 4.
VOLUME 4, 2016

Class 1
Class 2
0

0.5

1.0

x1
FIGURE 4. Scattergram of a dataset used in the simulation experiment. Each
class has different skewness and kurtosis, and they should be considered to
accurately calculate the posterior probabilities.

Each class has different skewness and kurtosis qualities, as
well as a different mean and variance.
In the experiment, 100 samples were treated as training
samples for each class. The function gi (y) was of type SU
(unbounded). After training, the proposed NN was tested
using inputs in the range 0 ≤ x1 ≤ 1 and 0 ≤ x2 ≤ 1. The
corresponding posterior probabilities were compared with
those given by LLR and LLGMN [14]. The LLR was trained
using Newton’s method [20], and LLGMN was trained by
terminal learning [28] with an ideal convergence time of
1.0 and a learning sampling time of 0.001. The number of
components Mc in the LLGMN was varied from 1 to 10.
B. RESULTS AND DISCUSSION

Fig. 5 shows the posterior probability of class 1 (P (c = 1|x))
given by the proposed NN, LLR, and LLGMN. The probability of class 2 (P (c = 2|x)) is clear from this graph, because
it can be calculated as P (c = 2|x) = 1 − P (c = 1|x).
From Fig. 5 (a), it is clear that the posterior probability
given by the proposed NN resembles the distribution shape
of the experimental data of class 1 (Fig. 4). The probability
given by LLR (Fig. 5 (b)) is totally different from the experimental data distribution. Although LLGMN with Mc = 1
(Fig. 5 (c)) is also different from the experimental data
distribution, this classifier produces probability that becomes
closer to the experimental data as the number of components
increases.
It can be inferred that the proposed NN is capable of appropriately dealing with data including skewness and kurtosis,
because it is based on the Johnson translation system. In
contrast, LLR cannot be adapted to data with skewness and
7

Hayashi et al.: A Neural Network Based on the Johnson SU Translation System and Related Application to Electromyogram Classification

1.0

1.0

x2 0.5

x2 0.5

0

0.5

x1

1.0

1.0

x2 0.5

x2 0.5

x1

1.0

1.0

x2 0.5

x2 0.5

x1

(e) LLGMN (Mc = 3)

x1

1.0

0.5

x1

1.0

(d) LLGMN (Mc = 2)

1.0

0.5

0.5

0

(c) LLGMN (Mc = 1)

0

Dataset
I
II
III
IV
V
VI

(b) Linear logistic regression

1.0

0.5

1.0

0

(a) Proposed NN

0

TABLE 3. Summary of dataset for the EMG classification experiment

Posterior probability

0

1.0

0

0.5

x1

1.0

(f) LLGMN (Mc = 10)

FIGURE 5. Posterior probability of class 1 (P (c = 1|x)) given by (a) the
proposed NN, (b) LLR, and (c) LLGMN (Mc = 1), (d) LLGMN (Mc = 2), (e)
LLGMN (Mc = 3), and (f) LLGMN (Mc = 10), where Mc is the number of
components for a Gaussian mixture model used in the LLGMN. The probability
of class 2 (P (c = 2|x)) is the reverse of that of class 1 with respect to black
and white.

kurtosis, because it is a linear classifier. LLGMN is capable
of handling data with skewness and kurtosis when the number of components is sufficiently large, but does not represent
the data distribution well with few components. The above
results demonstrate that the proposed NN can handle data
including skewness and kurtosis without hyperparameters,
although conventional methods require a hyperparameter optimization step.
VI. EMG CLASSIFICATION EXPERIMENT
A. METHOD

To evaluate the suitability of the proposed network for real
biological data, a classification experiment was conducted
using EMG data. Details of the data acquisition are described
in the next subsection. Table 3 shows the characteristics of six
8

# Motions
6
14
15
16
17
6

# Electrodes
6
8
8
13
12
2

# Sub.
1
8
8
1
9
5

# Trials
10
4
3
15
6
30

# Samples
20000
36000
72000
10000
12000
2000

datasets prepared for this experiment in terms of the number
of motions that the subjects performed, number of electrodes,
number of subjects, number of trials for each subject, and
number of samples for each trial. The number of motions
and the number of electrodes correspond to the number of
classes and the number of input dimensions, respectively. The
training samples were randomly chosen from the available
samples for each trial, with the remaining samples used
for testing. Because it is difficult to procure many training
samples in real-world applications, only 1% of the available
samples were selected for training to evaluate the validity of
the proposed NN for learning with limited training data.
We compared the performance of the proposed NN with
that of ν-SVM [29] with a one-vs-one classifier, LLGMN
[14], MLP, LLR, and k-NN. The hyperparameters of ν-SVM
(γ and ν) were optimized by 10-fold cross-validation (CV)
and a 10 × 10 grid search (γ ranging from log10 5.0 to
log10 1.0−5 , and ν ranging from νmax to log10 1.0−5 at even
intervals in logarithmic space, where νmax is dependent on
the ratio of labels in the training data). LLGMN was trained
by terminal learning [28] with an ideal convergence time of
1.0 and a learning sampling time of 0.001. The number of
components (from 1 to 5) in the LLGMN was determined
using 10-fold CV. The number of nodes (from d to d + 10)
in the hidden layer of MLP was also determined using 10fold CV, and MLP was trained using the back propagation
algorithm with a learning rate of 0.1. The LLR was trained
using Newton’s method, and the value of k in the k-NN
algorithm was chosen in the range 1 to 10 using 10-fold
CV. All algorithms were programmed using C++ and the dlib
C++ Library [30]. The experiments were run on a computer
with an Intel Core(TM) i7-3770K (3.5 GHz) processor and
16.0 GB RAM for Datasets I–V, and an Intel Core(TM) i77700K (4.2 GHz) processor and 16.0 GB RAM for Datasets
VI.
To evaluate the usefulness of a classifier for real-world
applications, it is necessary to measure not only the classification accuracy, but also the training/preparation time and
the prediction time. We therefore compared the performance
of the above algorithms through four metrics: accuracy, CV
time, training time, and prediction time. Accuracy is defined
as 100 × Ncorrect /Ntotal , where Ncorrect is the number
of correctly classified test samples and Ntotal is the total
number of test samples. CV time is the total time taken for
hyperparameter optimization based on CV, and training time
is the time required for training. The sum of CV time and
training time can be considered as the time until the classifier
VOLUME 4, 2016

Hayashi et al.: A Neural Network Based on the Johnson SU Translation System and Related Application to Electromyogram Classification

Reference Ch. 3

Ch. 6

Dorsum

Palm

Ch. 2 Ch. 1

Ch. 5

Ch. 4

FIGURE 6. Locations of electrodes for dataset I

becomes available. Prediction time is the total time taken to
classify all test samples. These metrics were measured for
each trial and each subject, and the average value was then
calculated.
B. DATA ACQUISITION

Dataset I contains six-channel (d = 6) EMG data recorded
by the authors. The six pairs of electrodes were located as
follows: Ch. 1: extensor carpi ulnaris; Ch. 2: flexor digitorum
profundus; Ch. 3: extensor digitorum; Ch. 4: flexor carpi
ulnaris; Ch. 5: triceps brachii; Ch. 6: biceps brachii (see Fig.
6).
The healthy 22-year-old male subject performed six successive motions in a relaxed state (C = 6; M1: hand opening;
M2: hand grasping; M3: wrist extension; M4: wrist flexion;
M5: pronation; M6: supination). EMG signals were recorded
at 1 kHz and digitalized using a 16-bit A/D converter. Feature
extraction was then conducted according to the method of
[6]. The signals were rectified and smoothed using a secondorder Butterworth low-pass filter with a cut-off frequency
of 1 Hz. These features were defined as EM Gi (n) (i =
1, . . . , d, n = 1, . . . , N : N is the number of data) and
normalized as follows:
(n)

xi

=

EM Gi (n) − EM Gst
i
,
d 

X
st
EM Gi0 (n) − EM Gi0

(42)

i0 =1

where EM Gst
i is the mean of EM Gi (n) in a state of
(n)
muscular relaxation. xi was then used as the input for the
network.
Datasets II and III are those used in [31] and [32], respectively1 . Dataset II contains measurements from eight
subjects (aged from 20–35 years old) while seated on an
armchair, putting their hands on a steering wheel attached
to a desk and performing twelve classes of finger pressures
and two classes of finger pointing (i.e., a total of 14 classes
(C = 14)). For dataset III, eight subjects (aged from 20–
35 years old) performed fifteen classes of finger and hand
movements while seated on an armchair with their arm
supported and fixed in one position. In both these datasets,
EMG signals were recorded using eight-channel electrodes at
4 kHz and digitalized using a 12-bit A/D converter. Feature
extraction for these datasets was conducted by rectification
and smoothing, as for dataset I.
1 These
datasets are available at Dr. Khushaba’s webpage:
http://www.rami-khushaba.com/electromyogram-emg-repository.html
VOLUME 4, 2016

Dataset IV contains measurements from a healthy 23-yearold male performing sixteen forearm motions (C = 16).
EMG signals were recorded using thirteen pairs of electrodes
(d = 13) at 1 kHz with a 60-Hz notch filter and a bandpass filter of 0.1–200 Hz. Details of the experimental conditions are
described in [33]. Additionally, to evaluate the performance
of the proposed NN for more difficult classification problems,
we also confirmed the change of accuracy according to the
decrease of the number of electrodes. The elimination of
electrodes was conducted in order from the ones having
the largest channel numbers, and then average classification
accuracy and standard deviation were calculated for all the
trials.
Dataset V is the Ninapro Database 3 exercise 1 [34],
which is available on Ninaweb2 . EMG signals were recorded
from 11 trans-radial amputated subjects using 12 electrodes
(d = 12) at 2 kHz while conducting 17 movements (C = 17).
Each movement lasted five seconds and was repeated six
times with a rest interval of three seconds. Because some
electrodes were missed for two subjects, nine out of the
11 subjects were used to uniform the number of channels
in the classification experiment. Feature extraction for these
datasets was conducted by rectification and smoothing, as for
other datasets.
Dataset VI is the sEMG for Basic Hand movements Data
Set provided by Sapsanis et al. [35]. Five healthy subjects
(two males and three females) were asked to perform six
grasping movements (C = 6): holding a cylindrical tool,
supporting a heavy load, holding a small tool, grasping with
palm facing the object, holding a spherical tool, and holding
a thin and flat object. Each movement lasted six seconds and
was repeated 30 times. EMG signals were collected from two
forearm surface EMG electrodes (d = 2) at a sampling rate
of 500 Hz. Feature extraction was conducted in the same way
as for other datasets, and the initial 1,000 samples for each
movement were then discarded to remove transition states.
For this dataset, classification accuracy is calculated based
on 5 × 2 CV approach referring to the original paper that
provides this dataset [35]. The number of training data is
limited also in this dataset by randomly sampling 1% of the
training set in each fold.
C. RESULTS

Table 4 summarizes the results of EMG classification. Values
are the average and standard deviation of scores measured for
each trial of each subject, and are presented as “average value
± standard deviation” or “average value” if the standard
deviation was 0. “∗∗” in the accuracy column denotes a significant difference, based on the Holm method, between that
algorithm and the proposed NN (p < 0.01). The absence of
“∗∗” in accuracy denotes no significant statistical difference.
Fig. 7 shows the confusion matrix of the classification results
for the dataset VI using the proposed NN.
Fig. 8 shows accuracy for each number of electrodes, while
2 http://ninapro.hevs.ch/node/131

9

Hayashi et al.: A Neural Network Based on the Johnson SU Translation System and Related Application to Electromyogram Classification

TABLE 4. Results of EMG classification

Dataset
I

II

III

IV

V

VI

Algorithm
JohnsonNN
SVM
LLGMN
MLP
LLR
k-NN
RandomForest
JohnsonNN
SVM
LLGMN
MLP
LLR
k-NN
RandomForest
JohnsonNN
SVM
LLGMN
MLP
LLR
k-NN
RandomForest
JohnsonNN
SVM
LLGMN
MLP
LLR
k-NN
RandomForest
JohnsonNN
SVM
LLGMN
MLP
LLR
k-NN
RandomForest
JohnsonNN
SVM
LLGMN
MLP
LLR
k-NN
RandomForest

Accuracy [%]
100
100
100
100
99.504 ± 1.489
100
100
100
99.999 ± 0.005
99.198 ± 0.695 **
96.689 ± 5.300 **
95.908 ± 6.812 **
99.999 ± 0.002
98.862 ± 1.643 **
99.973 ± 0.107
99.726 ± 0.633
96.131 ± 3.286 **
83.631 ± 8.659 **
90.590 ± 8.986 **
99.997 ± 0.005
97.252 ± 3.783
100
100
98.386 ± 2.022 **
85.038 ± 11.362 **
97.906 ± 1.863 **
100
100
39.122 ± 7.568
39.169 ± 7.699
34.782 ± 7.297
36.622 ± 6.492
33.848 ± 8.373
37.798 ± 7.455
33.109 ± 8.264
68.190 ± 10.014
67.544 ± 9.073
65.852 ± 6.457
53.117 ± 7.198 **
70.992 ± 8.646
68.039 ± 8.658
69.388 ± 9.645

CV time [s]
0
23.390 ± 0.548
197.497 ± 1.187
207.315 ± 7.259
0
1.347 ± 0.037
4.084 ± 0.079
0
522.975 ± 8.864
3074.678 ± 137.516
1609.333 ± 34.906
0
26.992 ± 0.779
43.713 ± 0.674
0
2454.33 ± 49.024
6844.277 ± 168.153
3483.782 ± 47.591
0
144.351 ± 6.757
105.956 ± 2.710
0
70.399 ± 1.417
2638.634 ± 48.720
786.566 ± 9.134
0
3.214 ± 0.087
31.526 ± 0.394
0
124.734 ± 1.403
2965.718 ± 76.741
960.015 ± 6.262
0
4.974 ± 0.295
39.456 ± 0.677
0
31.037 ± 0.603
92.514 ± 2.071
171.2 ± 2.574
0
2.766 ± 0.115
1.321 ± 0.073

Training time [s]
0.370 ± 0.015
0.115 ± 0.017
1.209 ± 0.011
36.034 ± 0.58
0.029 ± 0.007
0
0.206 ± 0.006
29.025 ± 4.891
0.447 ± 0.349
40.3 ± 24.863
161.952 ± 3.923
1.145 ± 0.619
0
1.379 ± 0.024
99.295 ± 39.217
0.922 ± 0.614
105.441 ± 55.835
356.209 ± 7.832
2.329 ± 1.951
0
3.357 ± 0.074
66.226 ± 6.118
0.144 ± 0.071
26.279 ± 18.556
55.727 ± 1.386
0.542 ± 0.527
0
0.389 ± 0.017
218.288 ± 19.043
0.065 ± 0.014
40.293 ± 19.812
73.066 ± 1.389
0.369 ± 0.098
0
0.569 ± 0.032
0.049 ± 0.032
0.064 ± 0.041
1.609 ± 0.637
56.152 ± 0.988
0.02 ± 0.006
0
0.445 ± 0.042

Prediction time [s]
0.525 ± 0.004
29.221 ± 7.102
0.318 ± 0.022
0.07 ± 0.004
0.303 ± 0.037
15.231 ± 0.149
2.562 ± 0.146
6.536 ± 0.104
131.309 ± 119.874
4.392 ± 1.166
0.418 ± 0.034
2.792 ± 0.131
319.459 ± 11.172
19.971 ± 1.645
15.235 ± 0.324
254.447 ± 183.636
10.436 ± 2.686
1.021 ± 0.119
6.904 ± 0.382
1740.607 ± 62.525
48.156 ± 3.105
4.149 ± 0.037
66.870 ± 66.167
2.080 ± 0.850
0.181 ± 0.014
1.042 ± 0.096
36.481 ± 0.357
5.136 ± 0.168
6.511 ± 0.056
23.799 ± 5.279
3.025 ± 0.937
0.274 ± 0.030
1.291 ± 0.044
58.331 ± 1.936
8.446 ± 0.767
0.501 ± 0.017
7.821 ± 5.088
0.505 ± 0.066
0.115 ± 0.013
0.454 ± 0.025
29.957 ± 0.669
5.349 ± 0.568

**: significant difference with the proposed NN (p < 0.01)

decreasing the electrodes for dataset IV. For comparison, the
accuracies of ν-SVM are also plotted. Significant differences
between the proposed NN and ν-SVM were confirmed when
the number of electrodes was d = 2 and d = 3 (p < 0.05).
Fig. 9 shows the relationship between accuracy and preparation time (CV time + training time), representing the time
until the classifier becomes available, and between accuracy
and prediction time for each classification method. Proximity
to the upper-left corner indicates superior performance.
D. DISCUSSION

In terms of accuracy, the proposed NN, ν-SVM, and kNN achieve the same level of performance, demonstrating
a suitability for EMG signal classification. The proposed
NN involves Johnson distribution in its structure based on
prior knowledge of EMG signals for appropriate modeling
of EMG distribution in the network. ν-SVM showed strong
generalization ability derived from margin maximization. kNN can be used to express arbitrary complex decision bound10

aries based on determination of the parameter k, ensuring
a fit to the skewness and kurtosis of EMG data. On the
other hand, the accuracies of LLGMN, MLP, and LLR were
notably low in some cases. This is because LLGMN and
MLP require many parameters to fit data with skewness
and kurtosis, which results in over-fitting, and LLR cannot
solve nonlinear classification problems because it is a linear
classifier. For dataset V, accuracies were relatively low in all
of the algorithms. This is because this dataset is recorded
from amputated subjects, and therefore EMG signals were
unstable and the reproducibility of motions was low compared with the data recorded form intact subjects.
In Fig. 7, confusion between class 3 and class 6 is relatively frequent compared with other classes. This is reasonable since the motions of these classes are similar (class 3:
holding a small tool, class 6: holding a thin and flat object).
However, there was no extreme bias toward a certain class;
therefore the proposed NN worked properly for multi-class
classification.
VOLUME 4, 2016

Hayashi et al.: A Neural Network Based on the Johnson SU Translation System and Related Application to Electromyogram Classification

FIGURE 7. Confusion matrix of the classification results for the dataset VI.
Values are normalized by the number of test samples for each class.

*

*

2

3

*: p < 0.05

Proposed NN

ν -SVM

Accuracy [%]

100

90

0

4

5

6

7

8

9

10

11

12

13

Number of electrodes d

FIGURE 8. Classification accuracy for each number of electrodes (Dataset
IV).

n -SVM

LLGMN
100
90
80
70
60
50
40
30
20
10
0

MLP

LLR

k-NN

RF

•
•

Accuracy [%]

Accuracy [%]

Proposed NN
100
90
80
70
60
50
40
30
20
10
0

0

0.2

0.4

0.6

0.8

Normalized preparation time

1.0

•

High accuracy for classification of EMG signals
Relatively short time until the classifier becomes available
Shorter prediction time than ν-SVM and k-NN

VII. CONCLUSION
0

0.2

0.4

0.6

0.8

1.0

Normalized prediction time

FIGURE 9. Relationships between (left) accuracy and preparation time (CV
time + Training time) and between (right) accuracy and prediction time by
using each classification method for six different datasets. The preparation
time and the prediction time are normalized by the maximum value for each
dataset. Proximity to the upper-left corner indicates superior performance.

In Fig. 8, the accuracies of the proposed NN and ν-SVM
both decreased according to the decrease of the number of
electrodes. This is because the decrease of electrodes yielded
the loss of information enough to classify the motions,
making the classification problem difficult. In particular,
the accuracies were sharply reduced when the number of
channels was reduced d = 3 to d = 2, although the proposed
NN exceeded ν-SVM. One possible explanation is that the
VOLUME 4, 2016

substantial reduction of the input dimensions yielded the
overlap of distribution for each class, and thus the proposed
NN could not model the data distribution precisely.
With respect to CV time, LLGMN and MLP took particularly long. In contrast, the proposed NN and LLR had CV
times of 0 because they have a unique solution of learning and
therefore do not require hyperparameters such as a learning
rate.
The training time for the proposed NN was relatively
short for dataset I. In datasets II, III, IV, and V, however,
significant training time was required. This is because the
cost of calculating the Hessian matrix and finding its inverse
(see (39) and (41)) increases with the number of classes
and input dimensions. Although the overall time until the
classifier becomes available is relatively short (because the
CV time is 0), there is room for improvement by making the
numerical calculations more efficient.
Regarding the prediction time, ν-SVM and k-NN took
particularly long. This is because ν-SVM was originally a
binary classifier, and thus solves multi-class classification
problems by calculating all combinations of two-class classification. k-NN also has a long computation time, because it
calculates the distance between the input sample and every
training sample. The prediction time of the proposed NN
is relatively short because it realizes a compact model for
EMG classification by incorporating prior knowledge of the
processed EMG characteristics.
Overall performance is summarized in Fig. 9. The plots of
the proposed NN are concentrated toward the upper-left corner, demonstrating well-balanced performance for accuracy
and computation cost.
Finally, the performance of the proposed NN can be summarized as follows:

In this paper, we proposed a NN based on the Johnson
SU translation system. The NN includes a discriminative
model based on the multivariate Johnson SU translation
system, with the model transformed into linear combinations
of weight coefficients and nonlinearly transformed input
vectors. This enables the representation of more flexible
distributions for data with skewness and kurtosis. Parameters
describing the shape of the distribution can be determined
as network coefficients via network learning. The proposed
NN can be trained without hyperparameter optimization, and
the training converges to a unique solution. In addition, the
posterior probability of input vectors for each class can be
calculated as the output of the NN.
In a simulation experiment, the proposed network was
shown to be more suitable than a conventional GMM-based
network and linear logistic regression for data with skewness
11

Hayashi et al.: A Neural Network Based on the Johnson SU Translation System and Related Application to Electromyogram Classification

and kurtosis. The applicability of the proposed NN to biosignal classification was also demonstrated by the results of an
EMG classification experiment.
In future research, we plan to construct an expanded model
of the proposed NN. As the function gi (y), which determines
the shape of the distribution, was only examined in relation to
SU in this study, future work will investigate other functions.
Despite of the assumption of SU distribution, EMG data are
occasionally distributed like a different type of distribution
such as SB ; thus in such situation SU distribution is used
as an approximation. Although SU distribution worked well
even in such situation according to the classification accuracy, more detailed comparison with other types of function
and development of the selection criteria for the distribution
type are needed. Using a different type of function for each
dimension will also enable the classification of multivariate
biosignals, such as the combination of EMG and EEG. Furthermore, the learning algorithm will be improved in future
work, and the training time will be shortened by contriving
numerical calculations for the Hessian matrix. Complete
discriminative learning for (1) W (c) and (2) W (c) will also be
developed using backpropagation-based learning.
.

where

APPENDIX A POSITIVE DEFINITENESS OF THE
HESSIAN MATRIX

REFERENCES

This appendix shows that the Hessian matrix described in
(39) is positive semi-definite. As described in (41), the
(h, l)th element of the (c, k)th block of the matrix is given
as
N
X

∂ 2 En
(c)

(k)

∂ (3) wh ∂ (3) wl

n=1

N
X

=

(5)

(n)

(n)

(n)

Ok (δc,k − (5) Oc(n) )(4) Oc,h (4) Ok,l . (43)

n=1

For simplification, we can consider just one term in the
summation over n, because the sum of positive semi-definite
matrices is also positive semi-definite.
Consider an arbitrary vector u ∈ C × H with elements
uc,h . Then,
uT Hu
C X
H
X
=
uc,h (5) Ok (δc,k − (5) Oc )(4) Oc,h (4) Ok,l uk,l
c,k h,l

=

C
X

bc (5) Ok (δc,k − (5) Oc )bk

c,k

=

C
X

bc bk

(5)

Ok δc,k −

=

k
12

bc (5) Oc bk (5) Ok

c,k

c,k
C
X

C
X

b2k (5) Ok − (

C
X
k

bk (5) Ok )2 ,

(44)

bc =

H
X

bk =

uc,h (4) Oc,h ,

h
H
X

(45)

uk,l (4) Ok,l .

(46)

l

Here, (5) Ok is P
the posterior probability satisfying 0 ≤
(5)
Ok ≤ 1 and k (5) Ok = 1. Furthermore, the function
f (bc ) = b2c is a convex function. Hence, we can apply
Jensen’s inequality [36] to give
C
X

b2k (5) Ok

=

k

C
X

f (bk )(5) Ok

k

≥

C
C
X
X
f(
bk (5) Ok ) = (
bk (5) Ok )2(.47)
k

k

Therefore,
uT Hu ≥ 0.

(48)

From the definition of definiteness, the Hessian matrix H is
positive semi-definite.

[1] N. Mammone, C. Ieracitano, H. Adeli, A. Bramanti, and F. C. Morabito,
“Permutation jaccard distance-based hierarchical clustering to estimate
EEG network density modifications in MCI subjects,” IEEE Trans. Neural
Netw. Learn. Syst., 2018.
[2] S. Sakhavi, C. Guan, and S. Yan, “Learning temporal information for
brain-computer interface using convolutional neural networks,” IEEE
Trans. Neural Netw. Learn. Syst., 2018.
[3] H. Zeng and A. Song, “Optimizing single-trial EEG classification by
stationary matrix logistic regression in brain–computer interface,” IEEE
Trans. Neural Netw. Learn. Syst., vol. 27, no. 11, pp. 2301–2313, 2016.
[4] Y. Zhang, G. Zhou, J. Jin, Q. Zhao, X. Wang, and A. Cichocki, “Sparse
bayesian classification of EEG for brain–computer interface,” IEEE Trans.
Neural Netw. Learn. Syst., vol. 27, no. 11, pp. 2256–2267, 2016.
[5] W. Tigra, B. Navarro, A. Cherubini, X. Gorron, A. Gélis, C. Fattal,
D. Guiraud, and C. Azevedo-Coste, “A novel EMG interface for individuals with tetraplegia to pilot robot hand grasping,” IEEE Trans. Neural Syst.
Rehabili. Eng., vol. 26, no. 2, pp. 291–298, 2016.
[6] O. Fukuda, T. Tsuji, M. Kaneko, and A. Otsuka, “A human-assisting
manipulator teleoperated by EMG signals and arm motions,” IEEE Trans.
Robot. Autom., vol. 19, no. 2, pp. 210–222, 2003.
[7] M. A. Oskoei and H. Hu, “Myoelectric control systems–a survey,” Biomed.
Signal Process. and Control, vol. 2, no. 4, pp. 275–294, 2007.
[8] C. Cortes and V. Vapnik, “Support-vector networks,” Mach. learning,
vol. 20, no. 3, pp. 273–297, 1995.
[9] D. E. Rumelhart, G. E. Hintont, and R. J. Williams, “Learning representations by back-propagating errors,” Nature, vol. 323, no. 9, pp. 533–536,
1986.
[10] T. Cover and P. Hart, “Nearest neighbor pattern classification,” IEEE
Trans. Inf. Theory, vol. 13, no. 1, pp. 21–27, 1967.
[11] Q. Wang, P. Li, and L. Zhang, “G2denet: Global gaussian distribution
embedding network and its application to visual recognition,” in Proc.
IEEE Conf. Comput. Vision and Pattern Recognition, 2017, pp. 2730–
2739.
[12] H. Hayashi, T. Shibanoki, K. Shima, Y. Kurita, and T. Tsuji, “A recurrent
probabilistic neural network with dimensionality reduction based on timeseries discriminant component analysis,” IEEE Trans. Neural Netw. Learn.
Syst., vol. 26, no. 12, pp. 3021–3033, 2015.
[13] T. Tsuji, N. Bu, O. Fukuda, and M. Kaneko, “A recurrent log-linearized
gaussian mixture network,” IEEE Trans. Neural Netw., vol. 14, no. 2, pp.
304–316, 2003.
VOLUME 4, 2016

Hayashi et al.: A Neural Network Based on the Johnson SU Translation System and Related Application to Electromyogram Classification

[14] T. Tsuji, O. Fukuda, H. Ichinobe, and M. Kaneko, “A log-linearized gaussian mixture network and its application to EEG pattern classification,”
IEEE Trans. Syst. Man Cybern. C, Appl. Rev., vol. 29, no. 1, pp. 60–72,
1999.
[15] N. L. Johnson, “Systems of frequency curves generated by methods of
translation,” Biometrika, vol. 36, no. 1/2, pp. 149–176, 1949.
[16] P. M. Stanfield, J. R. Wilson, G. A. Mirka, N. F. Glasscock, J. P. Psihogios,
and J. R. Davis, “Multivariate input modeling with johnson distributions,”
in Proc. 28th Conf. Winter Simulation. IEEE Computer Society, 1996,
pp. 1457–1464.
[17] N. L. Johnson, “Bivariate distributions based on simple translation systems,” Biometrika, vol. 36, no. 3/4, pp. 297–304, 1949.
[18] N. L. Johnson and S. Kotz, Distributions in statistics, continuous multivariate distributions. Wiley, 1972.
[19] H. Hayashi, Y. Kurita, and T. Tsuji, “A non-gaussian approach for biosignal classification based on the johnson su translation system,” in Proc.
IEEE 8th Int. Workshop on Comput. Intell. and Appl. (IWCIA). IEEE,
2015, pp. 115–120.
[20] C. M. Bishop and N. M. Nasrabadi, Pattern Recognition and Machine
Learning. Springer New York, 2006, vol. 1.
[21] N. Hogan and R. W. Mann, “Myoelectric signal processing: Optimal
estimation applied to electromyography-part i: Derivation of the optimal
myoprocessor,” IEEE Trans. Biomed. Eng., vol. 27, no. 7, pp. 382–395,
1980.
[22] R. N. Khushaba, A. H. Al-Timemy, A. Al-Ani, and A. Al-Jumaily, “A
framework of temporal-spatial descriptors-based feature extraction for
improved myoelectric pattern recognition,” IEEE Trans. Neural Syst.
Rehabil. Eng., vol. 25, no. 10, pp. 1821–1831, 2017.
[23] N. Nazmi, M. A. Abdul Rahman, S.-I. Yamamoto, S. A. Ahmad, H. Zamzuri, and S. A. Mazlan, “A review of classification techniques of EMG
signals during isotonic and isometric contractions,” Sensors, vol. 16, no. 8,
p. 1304, 2016.
[24] C. Altın and O. Er, “Comparison of different time and frequency domain
feature extraction methods on elbow gesture’s EMG,” EJIS European J.
Interdisciplinary Stud. Articles, vol. 5, 2016.
[25] E. J. Rechy-Ramirez and H. Hu, “Stages for developing control systems
using EMG and EEG signals: A survey,” Technical Report: CES-513 in
School of Comput. Sci. and Electron. Eng., University of Essex, United
Kingdom, Tech. Rep., 2011.
[26] T.-H. Kim and H. White, “On more robust estimation of skewness and
kurtosis,” Finance Research Lett., vol. 1, no. 1, pp. 56–73, 2004.
[27] J. F. Slifker and S. S. Shapiro, “The Johnson system: selection and
parameter estimation,” Technometrics, vol. 22, no. 2, pp. 239–246, 1980.
[28] M. Zak, “Terminal attractors in neural networks,” Neural Networks, vol. 2,
no. 4, pp. 259–274, 1989.
[29] C. C. Chang and C. J. Lin, “Training ν -support vector classifiers: theory
and algorithms,” Neural computation, vol. 13, no. 9, pp. 2119–2147, 2001.
[30] D. E. King, “Dlib-ml: A machine learning toolkit,” The J. Mach. Learning
Research, vol. 10, pp. 1755–1758, 2009.
[31] R. N. Khushaba, S. Kodagoda, D. Liu, and G. Dissanayake, “Muscle
computer interfaces for driver distraction reduction,” Comput. Methods
and Programs in Biomedicine, vol. 110, no. 2, pp. 137–149, 2013.
[32] R. N. Khushaba and S. Kodagoda, “Electromyogram (EMG) feature
reduction using mutual components analysis for multifunction prosthetic
fingers control,” in Proc. 12th Int. Conf. Control Automat. Robot & Vision
(ICARCV), 2012, pp. 1534–1539.
[33] T. Shibanoki, K. Shima, T. Tsuji, A. Otsuka, and T. Chin, “A quasioptimal channel selection method for bioelectric signal classification using
a partial kullback–leibler information measure,” IEEE Trans. Biomed.
Eng., vol. 60, no. 3, pp. 853–861, 2013.
[34] M. Atzori, A. Gijsberts, C. Castellini, B. Caputo, A.-G. M. Hager, S. Elsig,
G. Giatsidis, F. Bassetto, and H. Müller, “Electromyography data for noninvasive naturally-controlled robotic hand prostheses,” Scientific Data,
vol. 1, 2014.
[35] C. Sapsanis, G. Georgoulas, A. Tzes, and D. Lymberopoulos, “Improving
EMG based classification of basic hand movements using EMD,” in
Engineering in Medicine and Biology Society (EMBC), 2013 35th Annual
International Conference of the IEEE. IEEE, 2013, pp. 5754–5757.
[36] J. L. W. V. Jensen, “Sur les fonctions convexes et les inégalités entre les
valeurs moyennes,” Acta Mathematica, vol. 30, no. 1, pp. 175–193, 1906.

VOLUME 4, 2016

13

