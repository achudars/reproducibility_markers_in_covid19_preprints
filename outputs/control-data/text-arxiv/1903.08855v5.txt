Linguistic Knowledge and Transferability of Contextual Representations
Nelson F. Liu♠♥∗ Matt Gardner♣ Yonatan Belinkov♦
Matthew E. Peters♣ Noah A. Smith♠♣
♠
Paul G. Allen School of Computer Science & Engineering,
University of Washington, Seattle, WA, USA
♥
Department of Linguistics, University of Washington, Seattle, WA, USA
♣
Allen Institute for Artificial Intelligence, Seattle, WA, USA
♦
Harvard John A. Paulson School of Engineering and Applied Sciences and
MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA, USA
{nfliu,nasmith}@cs.washington.edu
{mattg,matthewp}@allenai.org,
belinkov@seas.harvard.edu
Abstract

arXiv:1903.08855v5 [cs.CL] 25 Apr 2019

Contextual word representations derived from
large-scale neural language models are successful across a diverse set of NLP tasks,
suggesting that they encode useful and transferable features of language. To shed light
on the linguistic knowledge they capture, we
study the representations produced by several recent pretrained contextualizers (variants
of ELMo, the OpenAI transformer language
model, and BERT) with a suite of seventeen
diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art
task-specific models in many cases, but fail on
tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word
representations, we quantify differences in the
transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more taskspecific, while transformer layers do not exhibit the same monotonic trend. In addition, to
better understand what makes contextual word
representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task,
pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language
model pretraining on more data gives the best
results.

1

Introduction

Pretrained word representations (Mikolov et al.,
2013; Pennington et al., 2014) are a key component of state-of-the-art neural NLP models. Traditionally, these word vectors are static—a single
*
Work done while at the Allen Institute for Artificial Intelligence.

Figure 1: An illustration of the probing model setup
used to study the linguistic knowledge within contextual word representations.

vector is assigned to each word. Recent work has
explored contextual word representations (henceforth: CWRs), which assign each word a vector
that is a function of the entire input sequence; this
enables them to model the use of words in context.
CWR s are typically the outputs of a neural network (which we call a contextualizer) trained on
tasks with large datasets, such as machine translation (McCann et al., 2017) and language modeling (Peters et al., 2018a). CWRs are extraordinarily effective—using them in place of traditional
static word vectors within the latest models leads
to large gains across a variety of NLP tasks.
The broad success of CWRs indicates that they
encode useful, transferable features of language.
However, their linguistic knowledge and transferability are not yet well understood.
Recent work has explored the linguistic knowledge captured by language models and neural machine translation systems, but these studies often
focus on a single phenomenon, e.g., knowledge of
hierarchical syntax (Blevins et al., 2018) or morphology (Belinkov et al., 2017a). We extend prior
work by studying CWRs with a diverse set of seventeen probing tasks designed to assess a wide array of phenomena, such as coreference, knowledge of semantic relations, and entity information,

among others. The result is a broader view of the
linguistic knowledge encoded within CWRs.
With respect to transferability, pretraining contextualizers on the language modeling task has
had the most empirical success, but we can also
consider pretraining contextualizers with other supervised objectives and probing their linguistic
knowledge. We examine how the pretraining task
affects the linguistic knowledge learned, considering twelve pretraining tasks and assessing transferability to nine target tasks.
Better understanding the linguistic knowledge
and transferability of CWRs is necessary for
their principled enhancement through new encoder architectures and pretraining tasks that build
upon their strengths or alleviate their weaknesses
(Linzen, 2018). This paper asks and answers:
1. What features of language do these vectors
capture, and what do they miss? (§4)
2. How and why does transferability vary across
representation layers in contextualizers? (§5)
3. How does the choice of pretraining task affect
the vectors’ learned linguistic knowledge and
transferability? (§6)
We use probing models1 (Shi et al., 2016b;
Adi et al., 2017; Hupkes et al., 2018; Belinkov
and Glass, 2019) to analyze the linguistic information within CWRs. Concretely, we generate
features for words from pretrained contextualizers and train a model to make predictions from
those features alone (Figure 1). If a simple model
can be trained to predict linguistic information
about a word (e.g., its part-of-speech tag) or a pair
of words (e.g., their semantic relation) from the
CWR (s) alone, we can reasonably conclude that the
CWR (s) encode this information.
Our analysis reveals interesting insights such as:
1. Linear models trained on top of frozen CWRs
are competitive with state-of-the-art taskspecific models in many cases, but fail on
tasks requiring fine-grained linguistic knowledge. In these cases, we show that tasktrained contextual features greatly help with
encoding the requisite knowledge.
2. The first layer output of long short-term
memory (LSTM) recurrent neural networks
is consistently the most transferable, whereas
it is the middle layers for transformers.
1

Sometimes called auxiliary or diagnostic classifiers.

3. Higher layers in LSTMs are more taskspecific (and thus less general), while the
transformer layers do not exhibit this same
monotonic increase in task-specificity.
4. Language model pretraining yields representations that are more transferable in general
than eleven other candidate pretraining tasks,
though pretraining on related tasks yields the
strongest results for individual end tasks.

2

Probing Tasks

We construct a suite of seventeen diverse English
probing tasks and use it to better understand the
linguistic knowledge contained within CWRs. In
contrast to previous studies that analyze the properties and task performance of sentence embeddings (Adi et al., 2017; Conneau et al., 2018, inter alia), we specifically focus on understanding
the CWRs of individual or pairs of words. We release this analysis toolkit to support future work
in probing the contents of representations.2 See
Appendix A for details about task setup.
2.1

Token Labeling

The majority of past work in probing the internal representations of neural models has examined various token labeling tasks, where a decision
is made independently for each token (Belinkov
et al., 2017a,b; Blevins et al., 2018, inter alia). We
synthesize these disparate studies and build upon
them by proposing additional probing tasks.
The part-of-speech tagging (POS) task assesses whether CWRs capture basic syntax. We
experiment with two standard datasets: the Penn
Treebank (PTB; Marcus et al., 1993) and the Universal Dependencies English Web Treebank (UDEWT; Silveira et al., 2014).
The CCG supertagging (CCG) task assesses
the vectors’ fine-grained information about the
syntactic roles of words in context. It is considered “almost parsing” (Bangalore and Joshi,
1999), since a sequence of supertags maps a sentence to a small set of possible parses. We use
CCGbank (Hockenmaier and Steedman, 2007), a
conversion of the PTB into CCG derivations.
The syntactic constituency ancestor tagging
tasks are designed to probe the vectors’ knowledge
of hierarchical syntax. For a given word, the probing model is trained to predict the constituent la2
http://nelsonliu.me/papers/
contextual-repr-analysis

where a prediction is made only for tokens corresponding to events (rather than every token in a
sequence). Performance is measured using Pearson correlation (r); we report (r × 100) so metrics
for all tasks fall between 0 and 100.
Figure 2: Annotated sentences from the STREUSLE
4.0 corpus, used in the preposition supersense disambiguation task. Prepositions are marked by boldface,
immediately followed by their labeled function. If applicable, ; precedes the preposition’s labeled role.
Figure reproduced from Schneider et al. (2018).

bel of its parent (Parent), grandparent (GParent),
or great-grandparent (GGParent) in the phrasestructure tree (from the PTB).
In the semantic tagging task (ST), tokens are
assigned labels that reflect their semantic role in
context. These semantic tags assess lexical semantics, and they abstract over redundant POS distinctions and disambiguate useful cases within POS
tags. We use the dataset of Bjerva et al. (2016);
the tagset has since been developed as part of the
Parallel Meaning Bank (Abzianidze et al., 2017).
Preposition supersense disambiguation is the
task of classifying a preposition’s lexical semantic contribution (the function; PS-fxn) and the semantic role or relation it mediates (the role; PSrole). This task is a specialized kind of word sense
disambiguation, and examines one facet of lexical
semantic knowledge. In contrast to the tagging
tasks above, the model is trained and evaluated
on single-token prepositions (rather than making
a decision for every token in a sequence). We
use the STREUSLE 4.0 corpus (Schneider et al.,
2018); example sentences appear in Figure 2.
The event factuality (EF) task involves labeling phrases with the factuality of the events
they describe (Saurı́ and Pustejovsky, 2009, 2012;
de Marneffe et al., 2012). For instance, in the following example reproduced from Rudinger et al.
(2018), “(1a) conveys that the leaving didn’t happen, while the superficially similar (1b) does not”.
(1)

a.
b.

Jo didn’t remember to leave.
Jo didn’t remember leaving.

We use the Universal Decompositional Semantics
It Happened v2 dataset (Rudinger et al., 2018), and
the model is trained to predict a (non)factuality
value in the range [−3, 3]. Unlike the tagging tasks
above, this task is treated as a regression problem,

2.2

Segmentation

Several of our probing tasks involve segmentation
using BIO or IO tags. Here the model is trained to
predict labels from only a single word’s CWR.
Syntactic chunking (Chunk) tests whether
CWR s contain notions of spans and boundaries;
the task is to segment text into shallow constituent
chunks. We use the CoNLL 2000 shared task
dataset (Tjong Kim Sang and Buchholz, 2000).
Named entity recognition (NER) examines
whether CWRs encode information about entity
types. We use the CoNLL 2003 shared task dataset
(Tjong Kim Sang and De Meulder, 2003).
Grammatical error detection (GED) is the
task of identifying tokens which need to be edited
in order to produce a grammatically correct sentence. Given that CWRs are extracted from models
trained on large amounts of grammatical text, this
task assesses whether embeddings encode features
that indicate anomalies in their input (in this case,
ungrammaticality). We use the First Certificate in
English (Yannakoudakis et al., 2011) dataset, converted into sequence-labeling format by Rei and
Yannakoudakis (2016).
The conjunct identification (Conj) task challenges the model to identify the tokens that comprise the conjuncts in a coordination construction.
Doing so requires highly specific syntactic knowledge. The data comes from the coordinationannotated PTB of Ficler and Goldberg (2016).
2.3

Pairwise Relations

We also design probing tasks that examine
whether relationships between words are encoded
in CWRs. In these tasks, given a word pair w1 , w2 ,
we input [w1 , w2 , w1 w2 ] into the probing model;
it is trained to predict information about the relation between the tokens (Belinkov, 2018).
We distinguish between arc prediction and arc
classification tasks. Arc prediction is a binary
classification task, where the model is trained to
identify whether a relation exists between two tokens. Arc classification is a multiclass classification task, where the model is provided with two
tokens that are linked via some relationship and
trained to identify how they are related.

For example, in the syntactic dependency arc
prediction task, the model is given the representations of two tokens (wa , wb ) and trained to predict
whether the sentence’s syntactic dependency parse
contains a dependency arc with wa as the head and
wb as the modifier. The syntactic dependency arc
classification task presents the model with the representations of two tokens (whead , wmod ), where
wmod is the modifier of whead , and the model is
trained to predict the type of syntactic relation that
link them (the label on that dependency arc). We
use the PTB (converted to UD) and the UD-EWT.
Similarly, semantic dependency arc prediction trains the model to predict whether two tokens are connected by a semantic dependency arc,
while the semantic dependency arc classification task trains models to classify the semantic relations between tokens. We use the dataset from
the SemEval 2015 shared task (Oepen et al., 2015)
with the DELPH-IN MRS-Derived Semantic Dependencies (DM) target representation.
The syntactic and semantic dependency arc prediction and classification tasks are closely related
to state-of-the-art models for semantic and syntactic dependency parsing, which score pairs of CWRs
to make head attachment and arc labeling decisions (Dozat and Manning, 2016, 2018).
To generate negative examples for the dependency arc prediction tasks, we take each positive
example (whead , wmod ) and generate a new negative example (wrand , wmod ). wrand is a random
token in the sentence that is not the head of wmod .
Thus, the datasets used in these tasks are balanced.
We also consider a coreference arc prediction
task, where the model is trained to predict whether
two entities corefer from their CWRs. We use the
dataset from the CoNLL 2012 shared task (Pradhan et al., 2012). To generate negative examples, we follow a similar procedure as the dependency arc prediction tasks: given a positive example (wa , wb ), where wb occurs after wa and the
two tokens share a coreference cluster, we create
a negative example (wrandom entity , wb ), where
wrandom entity is a token that occurs before wb and
belongs to a different coreference cluster.

3

Models

Probing Model We use a linear model as our
probing model; limiting its capacity enables us to
focus on what information can be easily extracted
from CWRs. See Appendix B for probing model

training hyperparameters and other details.
Contextualizers We study six publiclyavailable models for contextualized word
representation in English.
ELMo (Peters et al., 2018a) concatenates
the output of two contextualizers independently
trained on the bidirectional language modeling
(biLM) task. ELMo (original) uses a 2-layer
LSTM for contextualization. We also study two
variations from Peters et al. (2018b): ELMo (4layer) uses a 4-layer LSTM, and ELMo (transformer) uses a 6-layer transformer (Vaswani et al.,
2017). Each of these models is trained on 800M
tokens of sentence-shuffled newswire text (the 1
Billion Word Benchmark; Chelba et al., 2014).
The OpenAI transformer (Radford et al.,
2018) is a left-to-right 12-layer transformer language model trained on 800M tokens of contiguous text from over 7,000 unique unpublished
books (BookCorpus; Zhu et al., 2015).
BERT (Devlin et al., 2018) uses a bidirectional
transformer jointly trained on a masked language
modeling task and a next sentence prediction task.
The model is trained on BookCorpus and the English Wikipedia, a total of approximately 3300M
tokens. We study BERT (base, cased), which
uses a 12-layer transformer, and BERT (large,
cased), which uses a 24-layer transformer.

4

Pretrained Contextualizer Comparison

To better understand the linguistic knowledge captured by pretrained contextualizers, we analyze
each of their layers with our set of probing tasks.
These contextualizers differ in many respects, and
it is outside the scope of this work to control for
all differences between them. We focus on probing the models that are available to us, leaving a
more systematic comparison of training regimes
and model architectures to future work.
4.1

Experimental Setup

Our probing models are trained on the representations produced by the individual layers of each
contextualizer. We also compare to a linear probing model trained on noncontextual vectors (300dimensional GloVe trained on the cased Common
Crawl; Pennington et al., 2014) to assess the gains
from contextualization.

POS

Pretrained Representation

Supersense ID

Avg.

CCG

PTB

EWT Chunk

NER

ST

ELMo (original) best layer
ELMo (4-layer) best layer
ELMo (transformer) best layer
OpenAI transformer best layer
BERT (base, cased) best layer
BERT (large, cased) best layer

81.58
81.58
80.97
75.01
84.09
85.07

93.31
93.81
92.68
82.69
93.67
94.28

97.26
97.31
97.09
93.82
96.95
96.73

95.61
95.60
95.13
91.28
95.21
95.80

90.04
89.78
93.06
86.06
92.64
93.64

82.85
82.06
81.21
58.14
82.71
84.44

93.82
94.18
93.78
87.81
93.72
93.83

29.37
29.24
30.80
33.10
43.30
46.46

75.44
74.78
72.81
66.23
79.61
79.17

84.87
85.96
82.24
76.97
87.94
90.13

73.20
73.03
70.88
74.03
75.11
76.25

GloVe (840B.300d)

59.94 71.58 90.49 83.93

62.28

53.22 80.92 14.94

40.79

51.54

49.70

Previous state of the art
(without pretraining)

83.44 94.7

95.77

91.38 95.15 39.83

66.89

78.29

77.10

97.96 95.82

GED PS-Role PS-Fxn

EF

Table 1: Performance of the best layerwise linear probing model for each contextualizer compared against a
GloVe-based linear probing baseline and the previous state of the art. The best contextualizer for each task is
bolded. Results for all layers on all tasks, and papers describing the prior state of the art, are given in Appendix D.

4.2

Results and Discussion

Table 1 compares each contextualizer’s bestperforming probing model with the GloVe baseline and the previous state of the art for the task
(excluding methods that use pretrained CWRs).3,4
With just a linear model, we can readily extract
much of the information needed for high performance on various NLP tasks. In all cases, CWRs
perform significantly better than the noncontextual baseline. Indeed, we often see probing models rivaling or exceeding the performance of (often carefully tuned and task-specific) state-of-theart models. In particular, the linear probing model
surpasses the published state of the art for grammatical error detection and preposition supersense
identification (both role and function).
Comparing the ELMo-based contextualizers,
we see that ELMo (4-layer) and ELMo (original)
are essentially even, though both recurrent models outperform ELMo (transformer). We also see
that the OpenAI transformer significantly underperforms the ELMo models and BERT. Given that
it is also the only model trained in a unidirectional
(left-to-right) fashion, this reaffirms that bidirectionality is a crucial component for the highestquality contextualizers (Devlin et al., 2018). In addition, the OpenAI transformer is the only model
trained on lowercased text, which hinders its performance on tasks like NER. BERT significantly
improves over the ELMo and OpenAI models.
Our probing task results indicate that current
methods for CWR do not capture much transfer3

See Appendix C for references to the previous state of
the art (without pretraining).
4
For brevity, in this section we omit probing tasks that
cannot be compared to prior work. See Appendix D for pretrained contextualizer performance for all layers and all tasks.

able information about entities and coreference
phenomena in their input (e.g., the NER results
in Table 1 and the coreference arc prediction results in Appendix D). To alleviate this weakness,
future work could augment pretrained contextualizers with explicit entity representations (Ji et al.,
2017; Yang et al., 2017; Bosselut et al., 2017).
Probing Failures While probing models are at
or near state-of-the-art performance across a number of tasks, they also do not perform as well on
several others, including NER, grammatical error
detection, and conjunct identification. This may
occur because (1) the CWR simply does not encode the pertinent information or any predictive
correlates, or (2) the probing model does not have
the capacity necessary to extract the information
or predictive correlates from the vector. In the
former case, learning task-specific contextual features might be necessary for encoding the requisite
task-specific information into the CWRs. Learning
task-specific contextual features with a contextual
probing model also helps with (2), but we would
expect the results to be comparable to increasing
the probing model’s capacity.
To better understand the failures of our probing
model, we experiment with (1) a contextual probing model that uses a task-trained LSTM (unidirectional, 200 hidden units) before the linear output layer (thus adding task-specific contextualization) or (2) replacing the linear probing model
with a multilayer perceptron (MLP; adding more
parameters to the probing model: a single 1024d
hidden layer activated by ReLU). These alternate
probing models have nearly the same number of
parameters (LSTM + linear has slightly fewer).
We also compare to a full-featured model to

Probing Model

NER

GED

Conj GGParent

Linear
82.85 29.37 38.72
MLP (1024d)
87.19 47.45 55.09
LSTM (200d) + Linear 88.08 48.90 78.21

67.50
78.80
84.96

BiLSTM (512d)
+ MLP (1024d)

90.38

90.05 48.34 87.07

Table 2: Comparison of different probing models
trained on ELMo (original); best-performing probing
model is bolded. Results for each probing model are
from the highest-performing contextualizer layer. Enabling probing models to learn task-specific contextual
features (with LSTMs) yields outsized benefits in tasks
requiring highly specific information.

estimate an upper bound on performance for our
probing setup. In this model, the CWRs are inputs to a 2-layer BiLSTM with 512 hidden units,
and the output is fed into a MLP with a single 1024-dimensional hidden layer activated by a
ReLU to predict a label. A similar model, augmented with a conditional random field (CRF;
Lafferty et al., 2001), achieved state-of-the-art results on the CoNLL 2003 NER dataset (Peters
et al., 2018a). We remove the CRF, since other
probing models have no global context.
For this experiment, we focus on the ELMo
(original) pretrained contextualizer.
Table 2
presents the performance of the best layer within
each alternative probing model on the two tasks
with the largest gap between the linear probing
model and state-of-the-art methods: NER and
grammatical error detection. We also include
great-grandparent prediction and conjunct identification, two tasks that require highly specific
syntactic knowledge. In all cases, we see that
adding more parameters (either by replacing the
linear model with a MLP, or using a contextual
probing model) leads to significant gains over the
linear probing model. On NER and grammatical error detection, we observe very similar performance between the MLP and LSTM + Linear models—this indicates that the probing model
simply needed more capacity to extract the necessary information from the CWRs. On conjunct identification and great-grandparent prediction, two tasks that probe syntactic knowledge unlikely to be encoded in CWRs, adding parameters
as a task-trained component of our probing model
leads to large gains over simply adding parameters
to the probing model. This indicates that the pretrained contextualizers do not capture the informa-

tion necessary for the task, since such information
is learnable by a task-specific contextualizer.
This analysis also reveals insights about contextualizer fine-tuning, which seeks to specialize
the CWRs for an end task (Howard and Ruder,
2018; Radford et al., 2018; Devlin et al., 2018).
Our results confirm that task-trained contextualization is important when the end task requires
specific information that may not be captured by
the pretraining task (§4). However, such end-task–
specific contextualization can come from either
fine-tuning CWRs or using fixed output features
as inputs to a task-trained contextualizer; Peters
et al. (2019) begins to explore when each approach
should be applied.

5

Analyzing Layerwise Transferability

We quantify the transferability of CWRs by how
well they can do on the range of probing tasks—
representations that are more transferable will perform better than alternatives across tasks. When
analyzing the representations produced by each
layer of pretrained contextualizers, we observe
marked patterns in layerwise transferability (Figure 3). The first layer of contextualization in recurrent models (original and 4-layer ELMo) is consistently the most transferable, even outperforming a scalar mix of layers on most tasks (see Appendix D for scalar mix results). Schuster et al.
(2019) see the same trend in English dependency
parsing. By contrast, transformer-based contextualizers have no single most-transferable layer; the
best performing layer for each task varies, and is
usually near the middle. Accordingly, a scalar mix
of transformer layers outperforms the best individual layer on most tasks (see Appendix D).
Pretraining encourages the model to encode
pretraining-task–specific information; they learn
transferable features incidentally. We hypothesize
that this is an inherent trade-off—since these models used fixed-sized vector representations, taskspecificity comes at the cost of generality and
transferability. To investigate the task-specificity
of the representations generated by each contextualizer layer, we assess how informative each layer
of representation is for the pretraining task, essentially treating it as a probe.
5.1

Experimental Setup

We focus on the ELMo-based models, since the
authors have released code for training their con-

(a) ELMo (original)
Layer 0
Layer 2

(b) ELMo (4-layer)
Layer 0
Layer 4

(c) ELMo (transformer)
Layer 0
Layer 6

(d) OpenAI transformer
Layer 0
Layer 12

(e) BERT (base, cased)
Layer 0
Layer 12

(f) BERT (large, cased)
Layer 0
Layer 24

Lower Performance

Higher Performance

Figure 3: A visualization of layerwise patterns in task
performance. Each column represents a probing task,
and each row represents a contextualizer layer.

textualizers. Furthermore, the ELMo-based models facilitate a controlled comparison—they only
differ in the contextualizer architecture used.
We evaluate how well CWR features perform
the pretraining task—bidirectional language modeling. Specifically, we take the pretrained representations for each layer and relearn the language
model softmax classifiers used to predict the next
and previous token. The ELMo models are trained
on the Billion Word Benchmark, so we retrain
the softmax classifier on similar data to mitigate
any possible effects from domain shift. We split
the held-out portion of the Billion Word Benchmark into train (80%, 6.2M tokens) and evaluation (20%, 1.6M tokens) sets and use this data to
retrain and evaluate the softmax classifiers. We
expect that biLM perplexity will be lower when
training the softmax classifiers on representations
from layers that capture more information about
the pretraining task.
5.2

Results and Discussion

Figure 4 presents the performance of softmax classifiers trained to perform the bidirectional language modeling task, given just the CWRs as input. We notice that higher layers in recurrent models consistently achieve lower perplexities. Inter-

estingly, we see that layers 1 and 2 in the 4-layer
ELMo model have very similar performance—this
warrants further exploration. On the other hand,
the layers of the ELMo (transformer) model do not
exhibit such a monotonic increase. While the topmost layer is best (which we expected, since this
is the vector originally fed into a softmax classifier
during pretraining), the middle layers show varying performance. Across all models, the representations that are better-suited for language modeling are also those that exhibit worse probing task
performance (Figure 3), indicating that contextualizer layers trade off between encoding general
and task-specific features.
These results also reveal a difference in the
layerwise behavior of LSTMs and transformers;
moving up the LSTM layers yields more taskspecific representations, but the same does not
hold for transformers. Better understanding the
differences between transformers and LSTMs is
an active area of research (Chen et al., 2018; Tang
et al., 2018), and we leave further exploration of
these observations to future work.
These observations motivate the gradual unfreezing method of Howard and Ruder (2018),
where the model layers are progressively unfrozen
(starting from the final layer) during the finetuning process. Given our observation that higherlevel LSTM layers are less general (and more pretraining task-specific), they likely have to be finetuned a bit more in order to make them appropriately task specific. Meanwhile, the base layer of
the LSTM already learns highly transferable features, and may not benefit from fine-tuning.

6

Transferring Between Tasks

Successful pretrained contextualizers have used
self-supervised tasks such as bidirectional language modeling (Peters et al., 2018a) and next sentence prediction (Devlin et al., 2018), which enable the use of large, unannotated text corpora.
However, contextualizers can also be pretrained
on explicitly supervised objectives, as done in
pretrained sentence embedding methods (Conneau et al., 2017). To better understand how
the choice of pretraining task affects the linguistic knowledge within and transferability of CWRs,
we compare pretraining on a range of different
explicitly-supervised tasks with bidirectional language model pretraining.

(b) ELMo (4-layer)

7026

7000
6000
5000
4000
3000
2000
1000
0

4000

920
0

1

Perplexity

Perplexity

(a) ELMo (original)

235

3000

Perplexity

500
400
300
200
100
0

1013

1000
0 0

2

195
1

2

3

4

ELMo (4-layer) Layer #

(c) ELMo (transformer)
448
295

523
374

314
91

0

1

2

3

4

ELMo (transformer) Layer #

5

6

Figure 4: Bidirectional language modeling as a probe:
average of forward and backward perplexity (lower is
better) of each ELMo contextualizer layer. We see a
monotonic decrease in BiLM perplexity when trained
on the outputs of higher LSTM layers, but transformer
layers do not exhibit the same pattern.

6.1

Experimental Setup

To ensure a controlled comparison of different pretraining tasks, we fix the contextualizer’s architecture and pretraining dataset. All of our contextualizers use the ELMo (original) architecture,
and the training data from each of the pretraining
tasks is taken from the PTB. Each of the (identical) models thus see the same tokens, but the supervision signal differs.5 We compare to (1) a noncontextual baseline (GloVe) to assess the effect of
contextualization, (2) a randomly-initialized, untrained ELMo (original) baseline to measure the
effect of pretraining, and (3) the ELMo (original)
model pretrained on the Billion Word Benchmark
to examine the effect of training the bidirectional
language model on more data.
6.2

Results and Discussion

Table 3 presents the average target task performance of each layer in contextualizers pretrained
on twelve different tasks (biLM and the eleven
tasks from §2 with PTB annotations). Bidirectional language modeling pretraining is the most
effective on average. However, the settings that
achieve the highest performance for individual
target tasks often involve transferring between
related tasks (not shown in Table 3; see Appendix E). For example, when probing CWRs on
5

Pretraining Task

2398 2363

2000

ELMo (original) Layer #

546

4204

We omit the OpenAI transformer and BERT from this
comparison, since code for pretraining these contextualizers
is not publicly available.

CCG
Chunk
POS
Parent
GParent
GGParent
Syn. Arc Prediction
Syn. Arc Classification
Sem. Arc Prediction
Sem. Arc Classification
Conj
BiLM

Layer Average
Target Task Performance
0

1

2

Mix

56.70
54.27
56.21
54.57
55.50
54.83
53.63
56.15
53.19
56.28
50.24
66.53

64.45
62.69
63.86
62.46
62.94
61.10
59.94
64.41
54.69
62.41
49.93
65.91

63.71
63.25
64.15
61.67
62.91
59.84
58.62
63.60
53.04
61.47
48.42
65.82

66.06
63.96
65.13
64.31
64.96
63.81
62.43
66.07
59.84
64.67
56.92
66.49

GloVe (840B.300d)
60.55
Untrained ELMo (original) 52.14 39.26 39.39 54.42
ELMo (original)
(BiLM on 1B Benchmark)

64.40 79.05 77.72 78.90

Table 3: Performance (averaged across target tasks) of
contextualizers pretrained on a variety of tasks.

the syntactic dependency arc classification (EWT)
task, we see the largest gains from pretraining on
the task itself, but with a different dataset (PTB).
However, pretraining on syntactic dependency arc
prediction (PTB), CCG supertagging, chunking,
the ancestor prediction tasks, and semantic dependency arc classification all give better performance
than bidirectional language model pretraining.
Although related task transfer is beneficial, we
naturally see stronger results from training on
more data (the ELMo original BiLM trained on the
Billion Word Benchmark). This indicates that the
transferability of pretrained CWRs relies on pretraining on large corpora, emphasizing the utility
and importance of self-supervised pretraining.
Furthermore, layer 0 of the BiLM is the highestperforming single layer among PTB-pretrained
contextualizers. This observation suggests that
lexical information is the source of the language
model’s initial generalizability, since layer 0 is the
output of a character-level convolutional neural
network with no token-level contextual information.

7 Related Work
Methodologically, our work is most similar to
Shi et al. (2016b), Adi et al. (2017), and Hupkes
et al. (2018), who use the internal representations
of neural models to predict properties of interest.
Conneau et al. (2018) construct probing tasks to
study the linguistic properties of sentence embed-

ding methods. We focus on contextual word representations, which have achieved state-of-the-art
results on a variety of tasks, and examine a broader
range of linguistic knowledge.
In contemporaneous work, Tenney et al. (2019)
evaluate CoVe (McCann et al., 2017), ELMo (Peters et al., 2018a), the OpenAI Transformer (Radford et al., 2018), and BERT (Devlin et al., 2018)
on a variety of sub-sentence linguistic analysis
tasks. Their results also suggest that the aforementioned pretrained models for contextualized word
representation encode stronger notions of syntax
than higher-level semantics. They also find that
using a scalar mix of output layers is particularly effective in deep transformer-based models,
aligned with our own probing results and our observation that transformers tend to encode transferable features in their intermediate layers. Furthermore, they find that ELMo’s performance cannot be explained by a model with access to only
local context, indicating that ELMo encodes linguistic features from distant tokens.
Several other papers have examined how architecture design and choice of pretraining task affect the quality of learned CWRs. Peters et al.
(2018b) study how the choice of neural architecture influences the end-task performance and qualitative properties of CWRs derived from bidirectional language models (ELMo). Bowman et al.
(2018) compare a variety of pretraining tasks and
explore the the impact of multitask learning.
Prior work has employed a variety of other
methods to study the learned representations in
neural models, such as directly examining the activations of individual neurons (Karpathy et al.,
2015; Li et al., 2015; Shi et al., 2016a, inter alia), ablating components of the model and
dataset (Kuncoro et al., 2017; Gaddy et al., 2018;
Khandelwal et al., 2018), or interpreting attention
mechanisms (Bahdanau et al., 2015); see Belinkov
and Glass (2019) for a recent survey. One particularly relevant line of work involves the construction of synthetic tasks that a model can only solve
if it captures a particular phenomenon (Linzen
et al., 2016; Jumelet and Hupkes, 2018; Wilcox
et al., 2018; Futrell and Levy, 2019, inter alia).
Zhang and Bowman (2018) compare the syntactic knowledge of language models and neural machine translation systems. We widen the range of
pretraining tasks and target probing model tasks to
gain a more complete picture. We also focus on a

stronger contextualizer architecture, ELMo (original), that has produced state-of-the-art results.
Several studies have sought to intrinsically
evaluate noncontextual word representations with
word similarity tasks, such as analogies (Mikolov
et al., 2013). These methods differ from our approach in that they require no extra parameters and
directly assess the vectors, while our probing models must be trained. In this regard, our method is
similar to QVEC (Tsvetkov et al., 2015).

8

Conclusion

We study the linguistic knowledge and transferability of contextualized word representations
with a suite of seventeen diverse probing tasks.
The features generated by pretrained contextualizers are sufficient for high performance on a broad
set of tasks. For tasks that require specific information not captured by the contextual word representation, we show that learning task-specific contextual features helps encode the requisite knowledge. In addition, our analysis of patterns in the
transferability of contextualizer layers shows that
the lowest layer of LSTMs encodes the most transferable features, while transformers’ middle layers
are most transferable. We find that higher layers in
LSTMs are more task-specific (and thus less general), while transformer layers do not exhibit this
same monotonic increase in task-specificity. Prior
work has suggested that higher-level contextualizer layers may be expressly encoding higher-level
semantic information. Instead, it seems likely that
certain high-level semantic phenomena are incidentally useful for the contextualizer’s pretraining task, leading to their presence in higher layers. Lastly, we find that bidirectional language
model pretraining yields representations that are
more transferable in general than eleven other candidate pretraining tasks.

Acknowledgments
We thank Johannes Bjerva for sharing the semantic tagging dataset used in Bjerva et al. (2016). We
also thank the members of the Noah’s ARK group
at the University of Washington, the researchers at
the Allen Institute for Artificial Intelligence, and
the anonymous reviewers for their valuable feedback. NL is supported by a Washington Research
Foundation Fellowship and a Barry M. Goldwater Scholarship. YB is supported by the Harvard
Mind, Brain, and Behavior Initiative.

References
Lasha Abzianidze, Johannes Bjerva, Kilian Evang,
Hessel Haagsma, Rik van Noord, Pierre Ludmann,
Duc-Duy Nguyen, and Johan Bos. 2017. The parallel meaning bank: Towards a multilingual corpus of
translations annotated with compositional meaning
representations. In Proc. of EACL.
Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer
Lavi, and Yoav Goldberg. 2017. Fine-grained analysis of sentence embeddings using auxiliary prediction tasks. In Proc. of ICLR.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly
learning to align and translate. In Proc. of ICLR.
Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237–265.
Yonatan Belinkov. 2018. On Internal Language Representations in Deep Learning: An Analysis of Machine Translation and Speech Recognition. Ph.D.
thesis, Massachusetts Institute of Technology.
Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James R. Glass. 2017a. What do
neural machine translation models learn about morphology? In Proc. of ACL.
Yonatan Belinkov and James Glass. 2019. Analysis
methods in neural language processing: A survey.
Transactions of the Association for Computational
Linguistics.
Yonatan Belinkov, Lluı́s Màrquez, Hassan Sajjad,
Nadir Durrani, Fahim Dalvi, and James R. Glass.
2017b. Evaluating layers of representation in neural
machine translation on part-of-speech and semantic
tagging tasks. In Proc. of IJCNLP.
Johannes Bjerva, Barbara Plank, and Johan Bos. 2016.
Semantic tagging with deep residual networks. In
Proc. of COLING.
Terra Blevins, Omer Levy, and Luke Zettlemoyer.
2018. Deep RNNs encode soft hierarchical syntax.
In Proc. of ACL.
Bernd Bohnet, Ryan T. McDonald, Gonalo Simões,
Daniel Andor, Emily Pitler, and Joshua Maynez.
2018.
Morphosyntactic tagging with a metaBiLSTM model over context sensitive token encodings. In Proc. of ACL.

Yinghui Huang, Katherin Yu, Shuning Jin, and
Berlin Chen. 2018. Looking for ELMo’s friends:
Sentence-level pretraining beyond language modeling. ArXiv:1812.10860.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, and Phillipp Koehn. 2014. One
billion word benchmark for measuring progress in
statistical language modeling. In Proc. of INTERSPEECH.
Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin
Johnson, Wolfgang Macherey, George Foster, Llion
Jones, Niki Parmar, Mike Schuster, Zhifeng Chen,
Yonghui Wu, and Macduff Hughes. 2018. The best
of both worlds: Combining recent advances in neural machine translation. In Proc. of ACL.
Alexis Conneau, Douwe Kiela, Holger Schwenk,
Loı̈c Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations
from natural language inference data. In Proc. of
EMNLP.
Alexis Conneau, Germán Kruszewski, Guillaume
Lample, Loı̈c Barrault, and Marco Baroni. 2018.
What you can cram into a single vector: Probing
sentence embeddings for linguistic properties. In
Proc. of ACL.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: Pre-training of
deep bidirectional transformers for language understanding. In Proc. of NAACL.
Timothy Dozat and Christopher D. Manning. 2016.
Deep biaffine attention for neural dependency parsing. In Proc. of ICLR.
Timothy Dozat and Christopher D. Manning. 2018.
Simpler but more accurate semantic dependency
parsing. In Proc. of ACL.
Jessica Ficler and Yoav Goldberg. 2016. Coordination
annotation extension in the Penn Treebank. In Proc.
of ACL.
Richard Futrell and Roger P. Levy. 2019. Do RNNs
learn human-like abstract word order preferences?
In Proc. of SCiL.
David Gaddy, Mitchell Stern, and Dan Klein. 2018.
What’s going on in neural constituency parsers? An
analysis. In Proc. of NAACL.

Antoine Bosselut, Omer Levy, Ari Holtzman, Corin
Ennis, Dieter Fox, and Yejin Choi. 2017. Simulating action dynamics with neural process networks.
In Proc. of ICLR.

Matt Gardner, Joel Grus, Mark Neumann, Oyvind
Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew E.
Peters, Michael Schmitz, and Luke Zettlemoyer.
2018. AllenNLP: A deep semantic natural language
processing platform. In Proc. of NLP-OSS.

Samuel R. Bowman, Ellie Pavlick, Edouard Grave,
Benjamin Van Durme, Alex Wang, Jan Hula,
Patrick Xia, Raghavendra Pappagari, R. Thomas
McCoy, Roma Patel, Najoung Kim, Ian Tenney,

Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. 2017. A joint many-task
model: Growing a neural network for multiple nlp
tasks. In Proc. of EMNLP.

Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.

Marie-Catherine de Marneffe, Christopher D. Manning, and Christopher Potts. 2012. Did it happen? The pragmatic complexity of veridicality assessment. Computational Linguistics, 38:301–333.

Jeremy Howard and Sebastian Ruder. 2018. Universal
language model fine-tuning for text classification. In
Proc. of ACL.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Contextualized word vectors. In Proc. of NeurIPS.

Dieuwke Hupkes, Sara Veldhoen, and Willem
Zuidema. 2018. Visualisation and ‘diagnostic classifiers’ reveal how recurrent and recursive neural
networks process hierarchical structure. In Proc. of
IJCAI.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Proc. of NeurIPS.

Yangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin
Choi, and Noah A. Smith. 2017. Dynamic entity
representations in neural language models. In Proc.
of EMNLP.
Jaap Jumelet and Dieuwke Hupkes. 2018. Do language models understand anything? On the ability
of LSTMs to understand negative polarity items. In
Proc. of BlackboxNLP.
Andrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015.
Visualizing and understanding recurrent networks.
In Proc. of ICLR (Workshop).
Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. 2018. Sharp nearby, fuzzy far away: How neural language models use context. In Proc. of ACL.
Diederik P. Kingma and Jimmy Lei Ba. 2015. Adam:
A method for stochastic optimization. In Proc. of
ICLR.
Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng
Kong, Chris Dyer, Graham Neubig, and Noah A
Smith. 2017. What do recurrent neural network
grammars learn about syntax? In Proc. of EACL.
John D. Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of ICML.
Mike Lewis, Kenton Lee, and Luke Zettlemoyer. 2016.
LSTM CCG parsing. In Proc. of NAACL.
Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.
2015. Visualizing and understanding neural models
in nlp. In Proc. of NAACL.
Tal Linzen. 2018. What can linguistics and deep learning contribute to each other? Language.
Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.
2016. Assessing the ability of LSTMs to learn
syntax-sensitive dependencies. Transactions of the
Association for Computational Linguistics, 4:521–
535.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of English: The Penn treebank. Computational linguistics, 19(2):313–330.

Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Silvie Cinková, Dan Flickinger, Jan
Hajic, and Zdenka Uresova. 2015. SemEval 2015
task 18: Broad-coverage semantic dependency parsing. In Proc. of SemEval 2015.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global vectors for
word representation. In Proc. of EMNLP.
Matthew Peters, Sebastian Ruder, and Noah A Smith.
2019. To tune or not to tune? Adapting pretrained
representations to diverse tasks. ArXiv:1903.05987.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018a. Deep contextualized word representations. In Proc. of NAACL.
Matthew E. Peters, Mark Neumann, Luke Zettlemoyer,
and Wen-tau Yih. 2018b. Dissecting contextual
word embeddings: Architecture and representation.
In Proc. of EMNLP.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes. In Proc. of
CoNLL.
Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language understanding by generative pre-training. Technical report, OpenAI.
Marek Rei and Anders Sogaard. 2019. Jointly learning
to label sentences and tokens. In Proc. of AAAI.
Marek Rei and Helen Yannakoudakis. 2016. Compositional sequence labeling models for error detection
in learner writing. In Proc. of ACL.
Rachel Rudinger, Aaron Steven White, and Benjamin Van Durme. 2018. Neural models of factuality. In Proc. of NAACL.
Roser Saurı́ and James Pustejovsky. 2009. Factbank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43:227–268.
Roser Saurı́ and James Pustejovsky. 2012. Are you
sure that this happened? Assessing the factuality degree of events in text. Computational Linguistics,
38:261–299.

Nathan Schneider, Jena D. Hwang, Vivek Srikumar,
Jakob Prange, Austin Blodgett, Sarah R. Moeller,
Aviram Stern, Adi Bitan, and Omri Abend. 2018.
Comprehensive supersense disambiguation of English prepositions and possessives. In Proc. of ACL.
Tal Schuster, Ori Ram, Regina Barzilay, and Amir
Globerson. 2019. Cross-lingual alignment of contextual word embeddings, with applications to zeroshot dependency parsing. In Proc. of NAACL.
Xing Shi, Kevin Knight, and Deniz Yuret. 2016a. Why
neural translations are the right length. In Proc. of
EMNLP.
Xing Shi, Inkit Padhi, and Kevin Knight. 2016b. Does
string-based neural MT learn source syntax? In
Proc. of EMNLP.
Natalia Silveira, Timothy Dozat, Marie-Catherine
de Marneffe, Samuel Bowman, Miriam Connor,
John Bauer, and Christopher D. Manning. 2014. A
gold standard dependency corpus for English. In
Proc. of LREC.
Gongbo Tang, Mathias Müller, Annette Rios, and Rico
Sennrich. 2018. Why self-attention? A targeted
evaluation of neural machine translation architectures. In Proc. of EMNLP.
Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang,
Adam Poliak, R. Thomas McCoy, Najoung Kim,
Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. 2019. What do you learn
from context? Probing for sentence structure in contextualized word representations. In Proc. of ICLR.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared task:
Chunking. In Proc. of LLL and CoNLL.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Proc. of CoNLL.
Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guillaume Lample, and Chris Dyer. 2015. Evaluation of
word vector representations by subspace alignment.
In Proc. of EMNLP.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proc. of NeurIPS.
Ethan Wilcox, Roger Levy, Takashi Morita, and
Richard Futrell. 2018. What do RNN language
models learn about filler-gap dependencies?
In
Proc. of BlackboxNLP.
Zichao Yang, Phil Blunsom, Chris Dyer, and Wang
Ling. 2017. Reference-aware language models. In
Proc. of EMNLP.

Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In Proc. of ACL.
Michihiro Yasunaga, Jungo Kasai, and Dragomir R.
Radev. 2018. Robust multilingual part-of-speech
tagging via adversarial training. In Proc. of NAACL.
Kelly W. Zhang and Samuel R. Bowman. 2018. Language modeling teaches you more syntax than translation does: Lessons learned through auxiliary task
analysis. In Proc. of BlackboxNLP.
Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan
Salakhutdinov, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Aligning books and movies:
Towards story-like visual explanations by watching
movies and reading books. In Proc. of ICCV.

Appendices
A

Probing Task Setup Details

Syntactic Constituency Ancestor Tagging We
remove the top-level ROOT node in each sentence.
For words that do not have a parent, grandparent,
or great-grandparent, we set the label to ”None”.
The example is then treated as any other, and the
probing model is required to predict this ”None”
label during training and evaluation.
Preposition
Supersense
Disambiguation
Since we focus on the linguistic knowledge
within individual or pairs of CWRs, we train and
evaluate our probing models on only single-word
adpositions.
Conjunct Identification Our probing models
are only trained and evaluated on sentences with
a coordination construction in them.

B

Probing Model Training Details

Our probing models are trained with Adam
(Kingma and Ba, 2015), using a learning rate of
0.001. We train for 50 epochs, using early stopping with a patience of 3. Our models are implemented in the AllenNLP framework (Gardner
et al., 2018).
For contextualizers that use subword representations (e.g., the OpenAI transformer and BERT),
we aggregate subword representations into token
representations by taking a token’s representation
to be the representation of its final subword.

C

References to State-of-the-Art
Task-Specific Models (Without
Pretraining)

Task

Previous state of the art
(without pretraining)

CCG
POS (PTB)
POS (EWT)
Chunk
NER
ST
GED
PS-Role
PS-Fxn
EG

94.7 (Lewis et al., 2016)
97.96 (Bohnet et al., 2018)
95.82 (Yasunaga et al., 2018)
95.77 (Hashimoto et al., 2017)
91.38 (Hashimoto et al., 2017)
95.15 (Bjerva et al., 2016)
39.83 (Rei and Sogaard, 2019)
66.89 (Schneider et al., 2018)
78.29 (Schneider et al., 2018)
77.10 (Rudinger et al., 2018)

Table 4: Performance of prior state of the art models
(without pretraining) for each task.

Note that the performance reported in this paper
for the preposition supersense identification models of Schneider et al. (2018) differs from their
published result. Their published result is the accuracy on all adpositions; since we only train and
evaluate our model on single-word adpositions,
the number we report in this paper is the performance of the Schneider et al. (2018) model on
only single-word adpositions.

D
D.1

Performance of Pretrained Contextualizers on All Tasks
Token Labeling (ELMo and OpenAI Transformer)
POS

Pretrained Representation

Supersense ID

CCG

PTB

EWT Parent GParent GGParent

ELMo Original, Layer 0
ELMo Original, Layer 1
ELMo Original, Layer 2
ELMo Original, Scalar Mix

73.43
93.31
91.23
92.96

93.31
97.26
96.45
97.19

89.71
95.61
94.52
95.09

85.23
95.56
94.35
95.56

54.58
81.61
76.22
81.56

41.57
67.50
62.32
67.42

83.99
93.82
92.41
93.86

41.45
74.12
75.44
74.56

52.41
84.87
83.11
84.65

52.49
73.20
72.11
72.96

ELMo (4-layer), Layer 0
ELMo (4-layer), Layer 1
ELMo (4-layer), Layer 2
ELMo (4-layer), Layer 3
ELMo (4-layer), Layer 4
ELMo (4-layer), Scalar Mix

73.41
93.81
92.47
91.56
90.67
93.23

93.42
97.31
97.09
96.82
96.44
97.34

89.30
95.60
95.08
94.56
93.99
95.14

85.45
95.70
95.01
94.65
94.24
95.55

55.40
81.57
77.08
75.58
75.70
81.36

42.22
67.66
63.04
61.04
61.45
67.47

83.95
94.18
93.43
92.82
91.90
94.05

40.13
74.78
74.12
74.12
73.46
76.10

55.26
85.96
85.53
83.55
83.77
84.65

53.58
73.03
70.97
70.66
72.59
72.70

ELMo (transformer), Layer 0
ELMo (transformer), Layer 1
ELMo (transformer), Layer 2
ELMo (transformer), Layer 3
ELMo (transformer), Layer 4
ELMo (transformer), Layer 5
ELMo (transformer), Layer 6
ELMo (transformer), Scalar Mix

73.06
91.66
92.68
92.82
91.86
91.06
90.19
93.66

93.27
97.09
96.93
96.97
96.71
96.24
96.33
97.35

89.42
94.78
95.13
94.74
94.41
93.85
93.62
94.59

85.59
94.43
95.15
95.28
94.97
94.30
93.98
95.16

55.03
77.28
81.37
82.16
81.48
79.65
77.40
83.38

41.38
62.69
67.39
68.06
67.33
64.92
63.49
69.29

83.81
93.78
93.71
93.45
92.82
91.92
91.78
94.26

41.45
65.13
69.74
70.61
72.81
69.52
65.57
72.59

54.39
80.04
80.26
82.24
82.02
79.82
80.48
82.46

53.13
67.19
70.88
70.24
69.97
70.21
70.82
71.94

OpenAI transformer, Layer 0
OpenAI transformer, Layer 1
OpenAI transformer, Layer 2
OpenAI transformer, Layer 3
OpenAI transformer, Layer 4
OpenAI transformer, Layer 5
OpenAI transformer, Layer 6
OpenAI transformer, Layer 7
OpenAI transformer, Layer 8
OpenAI transformer, Layer 9
OpenAI transformer, Layer 10
OpenAI transformer, Layer 11
OpenAI transformer, Layer 12
OpenAI transformer, Scalar Mix

71.58
78.08
78.19
79.53
80.95
82.03
82.38
82.61
81.43
81.73
81.73
81.97
82.69
83.94

89.54
93.32
92.71
93.43
93.82
93.82
93.45
93.25
92.10
91.99
92.05
91.64
92.18
94.63

87.44
89.93
85.27
89.67
91.28
91.11
88.09
86.50
86.66
86.60
86.37
86.62
90.87
92.60

84.50
88.75
88.22
88.73
90.07
90.51
90.32
90.71
91.00
90.84
90.74
90.43
90.89
92.08

56.24
63.59
65.85
67.34
69.34
71.41
72.10
72.60
72.66
72.34
71.41
70.48
69.14
73.11

46.31
53.28
56.34
58.10
60.74
62.69
63.68
63.69
64.01
63.72
62.45
60.84
58.74
64.64

81.18
85.73
85.54
86.17
87.34
87.81
87.46
86.49
86.65
86.19
86.22
85.91
87.43
88.73

37.72
43.64
52.41
53.51
58.55
60.75
64.04
65.13
66.23
66.01
63.38
63.16
63.60
64.69

48.90
61.40
66.45
70.18
71.27
73.46
74.12
76.32
76.97
76.54
75.88
76.97
75.66
79.17

55.03
63.13
65.69
68.39
69.82
70.92
72.08
73.87
73.86
74.03
73.30
71.99
71.34
74.25

GloVe (840B.300d)

71.58 90.49 83.93

81.77

54.01

41.21

80.92

40.79

51.54

49.70

Previous state of the art

94.7

-

-

-

95.15

66.89

78.29

77.10

97.96 96.73

ST

PS-Role PS-Fxn

EF

Table 5: Token labeling task performance of a linear probing model trained on top of the ELMo and OpenAI
contextualizers, compared against a GloVe-based probing baseline and the previous state of the art.

D.2

Token Labeling (BERT)
POS

Pretrained Representation

Supersense ID

CCG

PTB

EWT Parent GParent GGParent

ST

PS-Role PS-Fxn

EF

BERT (base, cased), Layer 0
BERT (base, cased), Layer 1
BERT (base, cased), Layer 2
BERT (base, cased), Layer 3
BERT (base, cased), Layer 4
BERT (base, cased), Layer 5
BERT (base, cased), Layer 6
BERT (base, cased), Layer 7
BERT (base, cased), Layer 8
BERT (base, cased), Layer 9
BERT (base, cased), Layer 10
BERT (base, cased), Layer 11
BERT (base, cased), Layer 12
BERT (base, cased), Scalar Mix

71.45
81.67
88.43
89.77
91.41
92.22
93.14
93.51
93.67
93.51
93.25
92.75
92.21
93.78

89.99
93.80
95.76
96.08
96.57
96.68
96.95
96.92
96.80
96.68
96.54
96.40
96.09
97.02

86.77
90.58
93.72
94.30
94.58
94.93
95.15
95.12
95.21
94.94
94.51
94.31
93.86
95.63

84.41
89.47
92.98
93.92
94.67
95.10
95.46
95.70
95.60
95.64
95.26
95.00
94.55
95.83

55.92
62.92
71.73
73.24
76.09
77.79
79.75
80.38
81.04
80.70
79.60
78.50
76.95
81.67

46.07
50.93
57.84
58.57
61.17
63.56
65.36
65.96
66.66
66.53
65.49
64.34
62.87
67.48

82.25
88.89
92.23
92.85
93.38
93.47
93.72
93.62
93.37
93.18
92.90
92.64
92.34
93.85

42.11
50.88
63.60
64.69
66.23
68.20
76.10
77.85
79.61
79.39
79.17
77.41
78.07
78.51

54.82
67.76
75.00
78.95
79.17
82.89
84.65
86.40
87.94
86.84
86.18
85.53
84.65
85.96

52.70
59.83
64.91
65.58
67.55
69.08
71.26
71.54
73.49
75.11
74.70
75.11
73.77
74.88

BERT (large, cased), Layer 0
BERT (large, cased), Layer 1
BERT (large, cased), Layer 2
BERT (large, cased), Layer 3
BERT (large, cased), Layer 4
BERT (large, cased), Layer 5
BERT (large, cased), Layer 6
BERT (large, cased), Layer 7
BERT (large, cased), Layer 8
BERT (large, cased), Layer 9
BERT (large, cased), Layer 10
BERT (large, cased), Layer 11
BERT (large, cased), Layer 12
BERT (large, cased), Layer 13
BERT (large, cased), Layer 14
BERT (large, cased), Layer 15
BERT (large, cased), Layer 16
BERT (large, cased), Layer 17
BERT (large, cased), Layer 18
BERT (large, cased), Layer 19
BERT (large, cased), Layer 20
BERT (large, cased), Layer 21
BERT (large, cased), Layer 22
BERT (large, cased), Layer 23
BERT (large, cased), Layer 24
BERT (large, cased), Scalar Mix

71.06
79.49
83.30
83.32
88.51
89.69
90.91
91.72
91.56
91.76
91.71
92.01
92.82
93.48
93.85
94.21
94.28
94.13
93.76
93.36
93.06
91.83
89.66
88.70
87.65
94.48

89.84
92.58
94.03
94.09
95.61
95.95
96.14
96.30
96.36
96.31
96.27
96.26
96.48
96.73
96.73
96.72
96.67
96.53
96.38
96.25
96.10
95.38
93.88
93.02
92.60
97.17

86.81
89.45
91.70
91.92
93.36
94.15
94.35
94.64
94.80
94.86
94.89
94.96
95.27
95.56
95.54
95.80
95.62
95.55
95.45
95.30
94.96
94.05
92.30
91.90
90.84
96.05

84.28
88.50
90.48
90.76
93.26
93.94
94.47
94.55
94.61
94.70
94.88
95.10
95.31
95.72
95.98
96.10
96.05
95.92
95.57
95.38
95.20
94.16
92.62
92.36
91.81
96.27

55.84
60.96
64.91
64.99
70.99
72.62
75.59
76.35
76.40
75.95
75.84
77.01
78.66
80.51
81.89
82.46
82.78
82.56
81.47
80.47
79.32
76.84
74.73
73.33
71.98
83.51

46.17
49.88
51.94
52.26
56.22
57.58
60.80
60.98
61.93
61.60
61.44
62.79
64.51
65.85
67.02
67.53
67.90
67.74
67.11
66.08
64.86
62.43
60.76
59.27
57.95
68.90

82.31
87.16
89.47
89.67
92.58
93.05
93.35
93.55
93.50
93.44
93.42
93.39
93.61
93.83
93.81
93.76
93.61
93.45
93.21
93.01
92.78
91.65
89.42
88.92
88.26
93.96

38.38
53.51
58.55
58.33
65.35
62.06
62.72
67.98
66.89
66.89
68.64
70.83
74.34
76.54
78.95
79.17
78.73
79.17
79.17
76.10
78.29
74.12
73.90
69.08
69.74
78.95

54.61
65.13
71.93
72.81
78.29
76.97
78.51
81.36
80.26
82.02
79.39
81.80
84.21
85.75
87.94
89.25
90.13
87.06
87.06
85.96
87.72
82.89
82.02
80.70
78.73
87.06

52.81
59.49
62.49
62.52
65.06
65.79
67.00
66.42
68.56
69.12
69.37
71.12
72.44
72.91
72.72
72.79
74.27
75.52
75.95
76.25
75.92
75.16
74.28
73.54
72.65
76.13

Table 6: Token labeling task performance of a linear probing model trained on top of the BERT contextualizers.

D.3

Segmentation (ELMo and OpenAI Transformer)
Pretrained Representation

Chunk

NER

GED

Conj

ELMo Original, Layer 0
ELMo Original, Layer 1
ELMo Original, Layer 2
ELMo Original, Scalar Mix

70.68
90.04
86.47
89.29

64.39
82.85
82.80
82.90

18.49
29.37
26.08
27.54

15.59
38.72
29.08
39.57

ELMo (4-layer), Layer 0
ELMo (4-layer), Layer 1
ELMo (4-layer), Layer 2
ELMo (4-layer), Layer 3
ELMo (4-layer), Layer 4
ELMo (4-layer), Scalar Mix

70.57
89.78
87.18
86.20
85.07
86.67

63.96
81.04
80.19
81.56
82.06
82.37

8.46
28.07
29.24
28.51
23.85
30.46

15.15
36.37
31.44
28.57
26.31
28.42

ELMo (transformer), Layer 0
ELMo (transformer), Layer 1
ELMo (transformer), Layer 2
ELMo (transformer), Layer 3
ELMo (transformer), Layer 4
ELMo (transformer), Layer 5
ELMo (transformer), Layer 6
ELMo (transformer), Scalar Mix

71.01
91.75
92.18
92.14
91.32
89.18
87.96
92.08

64.23
78.51
80.92
80.80
80.47
81.21
79.77
81.68

13.25
25.29
28.63
29.16
29.71
30.80
27.20
26.56

15.69
26.56
34.99
38.23
38.52
35.49
29.17
38.45

OpenAI transformer, Layer 0
OpenAI transformer, Layer 1
OpenAI transformer, Layer 2
OpenAI transformer, Layer 3
OpenAI transformer, Layer 4
OpenAI transformer, Layer 5
OpenAI transformer, Layer 6
OpenAI transformer, Layer 7
OpenAI transformer, Layer 8
OpenAI transformer, Layer 9
OpenAI transformer, Layer 10
OpenAI transformer, Layer 11
OpenAI transformer, Layer 12
OpenAI transformer, Scalar Mix

66.59
77.87
79.67
80.78
82.95
84.67
85.46
86.06
85.75
85.40
84.52
83.00
82.44
87.44

46.29
48.88
52.13
52.40
54.62
56.25
56.46
57.73
56.50
57.31
57.32
56.94
58.14
59.39

14.78
19.72
21.59
22.58
25.61
29.69
30.69
33.10
32.17
31.90
32.08
30.22
30.81
34.54

16.84
17.59
20.72
22.36
23.04
25.53
27.25
30.68
33.06
32.65
30.27
26.60
25.19
31.65

GloVe (840B.300d)

62.28

53.22 14.94 10.53

Previous state of the art

95.77

91.38 34.76

-

Table 7: Segmentation task performance of a linear probing model trained on top of the ELMo and OpenAI
contextualizers, compared against a GloVe-based probing baseline and the previous state of the art.

D.4

Segmentation (BERT)
Pretrained Representation

Chunk

NER

GED

Conj

BERT (base, cased), Layer 0
BERT (base, cased), Layer 1
BERT (base, cased), Layer 2
BERT (base, cased), Layer 3
BERT (base, cased), Layer 4
BERT (base, cased), Layer 5
BERT (base, cased), Layer 6
BERT (base, cased), Layer 7
BERT (base, cased), Layer 8
BERT (base, cased), Layer 9
BERT (base, cased), Layer 10
BERT (base, cased), Layer 11
BERT (base, cased), Layer 12
BERT (base, cased), Scalar Mix

69.86
75.56
86.64
87.70
90.64
91.21
92.29
92.64
92.11
91.95
91.30
90.71
89.38
92.96

53.50
66.94
71.08
73.83
77.28
78.81
80.81
81.50
82.45
82.71
82.66
82.42
80.64
82.43

12.63
16.85
22.66
25.80
31.35
32.34
37.85
40.14
42.08
43.20
42.46
43.30
39.87
43.22

16.24
21.83
22.87
25.50
29.39
30.58
35.26
35.86
42.26
43.93
43.38
41.35
39.34
43.15

BERT (large, cased), Layer 0
BERT (large, cased), Layer 1
BERT (large, cased), Layer 2
BERT (large, cased), Layer 3
BERT (large, cased), Layer 4
BERT (large, cased), Layer 5
BERT (large, cased), Layer 6
BERT (large, cased), Layer 7
BERT (large, cased), Layer 8
BERT (large, cased), Layer 9
BERT (large, cased), Layer 10
BERT (large, cased), Layer 11
BERT (large, cased), Layer 12
BERT (large, cased), Layer 13
BERT (large, cased), Layer 14
BERT (large, cased), Layer 15
BERT (large, cased), Layer 16
BERT (large, cased), Layer 17
BERT (large, cased), Layer 18
BERT (large, cased), Layer 19
BERT (large, cased), Layer 20
BERT (large, cased), Layer 21
BERT (large, cased), Layer 22
BERT (large, cased), Layer 23
BERT (large, cased), Layer 24
BERT (large, cased), Scalar Mix

70.42
73.98
79.82
79.50
87.49
89.81
89.92
90.39
90.28
90.09
89.92
90.20
91.22
93.04
93.64
93.18
93.14
92.80
91.72
91.48
90.78
87.97
85.19
84.23
83.30
93.59

53.95
65.92
67.96
68.82
71.13
72.06
74.30
75.93
76.99
78.87
80.08
81.23
83.00
83.66
84.11
84.21
84.34
84.44
84.03
84.29
84.25
82.36
77.58
77.02
74.83
84.98

13.44
16.20
17.26
17.42
24.06
30.27
31.44
33.27
33.34
33.16
33.31
34.49
37.27
40.10
43.11
44.92
45.37
45.60
45.82
46.46
46.07
44.53
43.03
42.00
41.29
47.32

16.65
19.58
20.01
21.83
23.21
24.13
26.75
27.74
29.94
30.07
30.17
31.78
34.10
35.04
39.67
43.12
46.54
47.76
47.34
46.00
44.81
41.91
37.49
35.21
34.38
45.94

Table 8: Segmentation task performance of a linear probing model trained on top of the BERT contextualizers.

D.5

Pairwise Relations (ELMo and OpenAI Transformer)

Pretrained Representation

Syntactic Dep. Syntactic Dep.
Semantic Dep. Semantic Dep.
Coreference
Arc Prediction Arc Classification
Arc Prediction Arc Classification Arc Prediction
PTB EWT PTB
EWT

ELMo (original), Layer 0
ELMo (original), Layer 1
ELMo (original), Layer 2
ELMo (original), Scalar Mix

78.27
89.04
88.33
89.30

77.73
86.46
85.34
86.56

82.05
96.13
94.72
95.81

78.52
93.01
91.32
91.69

70.65
87.71
86.44
87.79

77.48
93.31
90.22
93.13

72.89
71.33
68.46
73.24

ELMo (4-layer), Layer 0
ELMo (4-layer), Layer 1
ELMo (4-layer), Layer 2
ELMo (4-layer), Layer 3
ELMo (4-layer), Layer 4
ELMo (4-layer), Scalar Mix

78.09
88.79
87.33
86.74
87.61
88.98

77.57
86.31
84.75
84.17
85.09
85.94

82.13
96.20
95.38
95.06
94.14
95.82

77.99
93.20
91.87
91.55
90.68
91.77

69.96
87.15
85.29
84.44
85.81
87.39

77.22
93.27
90.57
90.04
89.45
93.25

73.57
72.93
71.78
70.11
68.36
73.88

ELMo (transformer), Layer 0
ELMo (transformer), Layer 1
ELMo (transformer), Layer 2
ELMo (transformer), Layer 3
ELMo (transformer), Layer 4
ELMo (transformer), Layer 5
ELMo (transformer), Layer 6
ELMo (transformer), Scalar Mix

78.10
88.24
88.87
89.01
88.55
88.09
87.22
90.74

78.04
85.48
84.72
84.62
85.62
83.23
83.28
86.39

81.09
93.62
94.14
94.07
94.14
92.70
92.55
96.40

77.67
89.18
89.40
89.17
89.00
88.84
87.13
91.06

70.11
85.16
85.97
86.83
86.00
85.79
84.71
89.18

77.11
90.66
91.29
90.35
89.04
89.66
87.21
94.35

72.50
72.47
73.03
72.62
71.80
71.62
66.35
75.52

OpenAI transformer, Layer 0
OpenAI transformer, Layer 1
OpenAI transformer, Layer 2
OpenAI transformer, Layer 3
OpenAI transformer, Layer 4
OpenAI transformer, Layer 5
OpenAI transformer, Layer 6
OpenAI transformer, Layer 7
OpenAI transformer, Layer 8
OpenAI transformer, Layer 9
OpenAI transformer, Layer 10
OpenAI transformer, Layer 11
OpenAI transformer, Layer 12
OpenAI transformer, Scalar Mix

80.80
81.91
82.56
82.87
83.69
84.53
85.47
86.32
86.84
87.00
86.76
85.84
85.06
87.18

79.10
79.99
80.22
81.21
82.07
82.77
83.89
84.15
84.06
84.47
84.28
83.42
83.02
85.30

83.35
88.22
89.34
90.89
92.21
93.12
93.71
93.95
94.16
93.95
93.40
92.82
92.37
94.51

80.32
84.51
85.99
87.67
89.24
90.34
90.63
90.82
91.02
90.77
90.26
89.07
89.08
91.55

76.39
77.70
78.47
78.91
80.51
81.95
83.88
85.15
85.23
85.95
85.17
83.39
81.88
86.13

80.50
83.88
85.85
87.76
89.59
90.25
90.99
91.18
90.86
90.85
89.94
88.46
87.47
91.55

72.58
75.23
75.77
75.81
75.99
76.05
74.43
74.05
74.20
74.57
73.86
72.03
70.44
76.47

GloVe (840B.300d)

74.14

73.94

77.54

72.74

68.94

71.84

72.96

Table 9: Pairwise relation task performance of a linear probing model trained on top of the ELMo and OpenAI
contextualizers, compared against a GloVe-based probing baseline.

D.6

Pairwise Relations (BERT)

Pretrained Representation

Syntactic Dep. Syntactic Dep.
Semantic Dep. Semantic Dep.
Coreference
Arc Prediction Arc Classification
Arc Prediction Arc Classification Arc Prediction
PTB EWT PTB
EWT

BERT (base, cased), Layer 0
BERT (base, cased), Layer 1
BERT (base, cased), Layer 2
BERT (base, cased), Layer 3
BERT (base, cased), Layer 4
BERT (base, cased), Layer 5
BERT (base, cased), Layer 6
BERT (base, cased), Layer 7
BERT (base, cased), Layer 8
BERT (base, cased), Layer 9
BERT (base, cased), Layer 10
BERT (base, cased), Layer 11
BERT (base, cased), Layer 12
BERT (base, cased), Scalar Mix

83.00
83.66
84.00
84.12
85.50
86.67
87.98
88.24
88.64
88.76
88.16
87.74
85.93
89.06

80.36
81.69
82.66
82.86
84.07
84.69
85.91
86.30
86.49
86.17
85.86
85.40
83.99
86.58

83.47
86.92
91.90
92.80
93.91
94.87
95.57
95.65
95.90
95.84
95.42
95.09
94.79
95.91

79.15
82.62
88.51
89.49
91.02
92.01
93.01
93.31
93.39
93.32
92.82
92.37
91.70
93.10

80.26
80.81
79.34
79.05
81.37
83.41
85.73
85.96
86.59
86.74
86.29
85.83
82.71
87.10

80.35
82.69
87.45
88.41
90.20
91.34
92.47
92.75
93.18
92.68
91.79
91.07
90.10
93.38

74.93
75.35
75.19
75.83
76.14
76.35
75.95
75.37
76.39
76.62
76.84
76.88
76.78
77.88

BERT (large, cased), Layer 0
BERT (large, cased), Layer 1
BERT (large, cased), Layer 2
BERT (large, cased), Layer 3
BERT (large, cased), Layer 4
BERT (large, cased), Layer 5
BERT (large, cased), Layer 6
BERT (large, cased), Layer 7
BERT (large, cased), Layer 8
BERT (large, cased), Layer 9
BERT (large, cased), Layer 10
BERT (large, cased), Layer 11
BERT (large, cased), Layer 12
BERT (large, cased), Layer 13
BERT (large, cased), Layer 14
BERT (large, cased), Layer 15
BERT (large, cased), Layer 16
BERT (large, cased), Layer 17
BERT (large, cased), Layer 18
BERT (large, cased), Layer 19
BERT (large, cased), Layer 20
BERT (large, cased), Layer 21
BERT (large, cased), Layer 22
BERT (large, cased), Layer 23
BERT (large, cased), Layer 24
BERT (large, cased), Scalar Mix

82.22
81.65
81.84
81.66
83.56
84.24
85.05
85.43
85.41
85.35
85.51
85.91
86.80
87.64
88.62
88.87
89.36
89.62
89.41
88.78
88.24
86.48
85.42
84.69
83.24
90.09

79.92
80.04
80.09
80.35
82.17
82.94
83.50
84.03
83.92
83.76
83.92
83.88
85.13
86.00
86.50
86.95
87.25
87.47
87.00
86.60
85.87
84.21
83.24
82.81
81.48
87.51

83.57
85.23
87.39
87.36
91.44
92.33
93.75
94.06
94.18
94.11
94.09
94.48
95.03
95.54
95.94
96.02
96.18
96.01
95.82
95.59
95.12
94.21
92.94
92.28
91.07
96.15

79.32
80.95
83.80
83.74
88.45
89.62
91.02
91.65
91.66
91.10
91.17
91.73
92.37
93.02
93.62
93.66
93.86
93.88
93.47
92.98
92.47
91.12
90.02
89.47
87.88
93.61

79.04
77.97
77.17
76.92
78.43
79.28
80.18
80.64
80.64
80.64
81.51
82.05
83.99
84.91
85.91
86.49
87.79
88.14
87.77
87.16
86.45
83.94
82.01
81.07
78.24
88.49

81.25
81.36
82.44
82.91
87.32
88.85
90.14
90.69
90.82
90.62
90.43
91.13
92.08
92.74
93.51
93.86
93.83
93.41
93.00
92.27
91.33
89.42
88.17
87.32
85.98
94.25

73.75
73.99
73.89
73.62
72.99
73.34
74.02
74.55
75.92
76.00
76.19
75.86
75.13
74.63
75.16
75.58
75.15
75.93
77.85
80.47
80.94
81.14
80.36
79.64
79.35
81.16

Table 10: Pairwise relation task performance of a linear probing model trained on top of the BERT contextualizers.

E
E.1

Full Results for Transferring Between Pretraining Tasks
Token Labeling
Supersense ID

Pretrained Representation
POS (EWT)

ST

PS-Role PS-Fxn

EF

Untrained ELMo (original), Layer 0
Untrained ELMo (original), Layer 1
Untrained ELMo (original), Layer 2
Untrained ELMo (original), Scalar Mix

77.05
56.03
55.89
78.58

76.09
68.63
68.51
82.45

36.99
16.01
16.01
38.23

48.17
24.71
25.44
48.90

43.08
45.57
46.06
47.37

CCG, Layer 0
CCG, Layer 1
CCG, Layer 2
CCG, Scalar Mix

84.33
88.02
87.81
90.44

79.53
87.97
87.38
91.21

38.38
46.27
43.79
50.07

53.29
58.48
58.55
65.57

47.71
57.96
57.98
60.24

Chunk, Layer 0
Chunk, Layer 1
Chunk, Layer 2
Chunk, Scalar Mix

82.51
87.33
86.61
88.62

78.45
87.42
87.04
89.77

37.06
44.81
39.91
44.23

49.12
59.36
58.11
60.01

38.93
55.66
56.95
56.24

PTB (POS), Layer 0
PTB (POS), Layer 1
PTB (POS), Layer 2
PTB (POS), Scalar Mix

84.58
90.53
90.45
90.75

79.95
90.10
89.83
91.13

37.43
42.47
44.37
45.39

49.49
59.80
58.92
60.67

46.19
61.28
62.14
62.77

Parent, Layer 0
Parent, Layer 1
Parent, Layer 2
Parent, Scalar Mix

81.84
87.21
86.57
89.10

78.47
87.36
86.18
90.01

36.33
45.98
42.69
44.88

49.71
58.85
58.48
61.92

38.35
54.45
54.58
55.64

GParent, Layer 0
GParent, Layer 1
GParent, Layer 2
GParent, Scalar Mix

81.85
86.05
85.64
88.08

78.77
86.78
86.17
89.48

37.06
46.86
45.25
48.03

51.75
60.82
62.13
63.38

40.46
55.58
55.65
55.96

GGParent, Layer 0
GGParent, Layer 1
GGParent, Layer 2
GGParent, Scalar Mix

81.44
83.51
83.17
86.18

77.88
85.23
84.10
88.84

38.74
44.08
39.40
44.52

49.12
57.68
56.29
61.62

42.17
55.77
55.82
55.50

Syn. Arc Prediction (PTB), Layer 0
Syn. Arc Prediction (PTB), Layer 1
Syn. Arc Prediction (PTB), Layer 2
Syn. Arc Prediction (PTB), Scalar Mix

79.97
80.67
78.83
85.76

77.34
82.60
80.91
88.13

36.26
40.06
34.65
40.79

47.15
54.61
52.12
54.17

38.81
47.86
45.64
50.91

Syn. Arc Classification (PTB), Layer 0
Syn. Arc Classification (PTB), Layer 1
Syn. Arc Classification (PTB), Layer 2
Syn. Arc Classification (PTB), Scalar Mix

83.61
89.28
88.77
90.18

79.61
88.70
88.12
90.99

37.21
47.22
44.66
48.17

51.97
61.11
58.92
62.21

42.07
55.55
56.16
56.90

Sem. Arc Prediction, Layer 0
Sem. Arc Prediction, Layer 1
Sem. Arc Prediction, Layer 2
Sem. Arc Prediction, Scalar Mix

78.64
74.66
74.06
83.77

76.95
74.83
73.42
85.06

34.43
33.92
30.85
38.45

49.78
47.88
45.39
57.16

39.64
36.46
35.63
48.27

Sem. Arc Classification, Layer 0
Sem. Arc Classification, Layer 1
Sem. Arc Classification, Layer 2
Sem. Arc Classification, Scalar Mix

83.17
86.45
85.42
88.44

79.17
87.04
85.87
90.00

38.60
44.81
41.45
45.03

51.54
58.19
58.55
61.33

44.79
55.18
52.87
56.07

Conj, Layer 0
Conj, Layer 1
Conj, Layer 2
Conj, Scalar Mix

72.21
64.95
64.03
76.96

73.87
68.96
67.17
80.22

37.43
27.70
27.56
36.33

47.95
41.89
37.21
50.66

36.33
42.10
40.59
42.79

BiLM, Layer 0
BiLM, Layer 1
BiLM, Layer 2
BiLM, Scalar Mix

87.54
86.55
86.49
86.76

90.22
87.19
89.67
90.11

50.88
50.22
49.34
50.44

67.32
67.11
66.01
67.32

59.65
59.32
59.45
67.32

ELMo (original), Layer 0
ELMo (original), Layer 1
ELMo (original), Layer 2
ELMo (original), Scalar Mix

89.71
95.61
94.52
95.09

83.99
93.82
92.41
93.86

41.45
74.12
75.44
74.56

52.41
84.87
83.11
84.65

52.49
73.20
72.11
84.65

GloVe (840B.300d)

83.93

80.92

40.79

51.54

49.70

Table 11: Target token labeling task performance of contextualizers pretrained on a variety of different tasks. The
probing model used is linear, and the contextualizer architecture is ELMo (original).

E.2

Segmentation
Pretrained Representation

NER GED

Untrained ELMo (original), Layer 0
Untrained ELMo (original), Layer 1
Untrained ELMo (original), Layer 2
Untrained ELMo (original), Scalar Mix

24.71
0.00
0.00
34.28

0.00
0.00
0.00
1.81

CCG, Layer 0
CCG, Layer 1
CCG, Layer 2
CCG, Scalar Mix

32.30
44.01
42.45
49.07

8.89
22.68
25.15
4.52

Chunk, Layer 0
Chunk, Layer 1
Chunk, Layer 2
Chunk, Scalar Mix

23.47
45.44
43.59
46.83

5.80
5.46
24.11
4.30

PTB (POS), Layer 0
PTB (POS), Layer 1
PTB (POS), Layer 2
PTB (POS), Scalar Mix

32.64
52.03
52.04
53.51

7.87
5.80
9.76
3.19

Parent, Layer 0
Parent, Layer 1
Parent, Layer 2
Parent, Scalar Mix

25.11
42.76
42.49
47.06

6.66
6.22
8.33
3.01

GParent, Layer 0
GParent, Layer 1
GParent, Layer 2
GParent, Scalar Mix

30.39
47.67
47.87
50.06

4.58
6.20
10.34
1.71

GGParent, Layer 0
GGParent, Layer 1
GGParent, Layer 2
GGParent, Scalar Mix

28.57
46.21
45.34
48.19

2.25
4.32
3.74
1.54

Syn. Arc Prediction (PTB), Layer 0
Syn. Arc Prediction (PTB), Layer 1
Syn. Arc Prediction (PTB), Layer 2
Syn. Arc Prediction (PTB), Scalar Mix

26.77
43.93
41.83
46.58

1.82
5.94
14.50
1.47

Syn. Arc Classification (PTB), Layer 0
Syn. Arc Classification (PTB), Layer 1
Syn. Arc Classification (PTB), Layer 2
Syn. Arc Classification (PTB), Scalar Mix

33.10
50.76
49.64
53.00

3.51
3.92
5.77
1.27

Sem. Arc Prediction, Layer 0
Sem. Arc Prediction, Layer 1
Sem. Arc Prediction, Layer 2
Sem. Arc Prediction, Scalar Mix

24.47
34.47
31.30
36.97

1.05
10.78
10.77
0.32

Sem. Arc Classification, Layer 0
Sem. Arc Classification, Layer 1
Sem. Arc Classification, Layer 2
Sem. Arc Classification, Scalar Mix

34.00
48.07
46.67
50.80

5.08
5.39
6.24
1.75

Conj, Layer 0
Conj, Layer 1
Conj, Layer 2
Conj, Scalar Mix

17.15
37.61
34.78
40.97

3.99
0.87
2.38
0.33

BiLM, Layer 0
BiLM, Layer 1
BiLM, Layer 2
BiLM, Scalar Mix

56.05
57.19
57.05
58.50

3.99
1.22
1.03
1.29

ELMo (original), Layer 0
ELMo (original), Layer 1
ELMo (original), Layer 2
ELMo (original), Scalar Mix

64.39
82.85
82.80
82.90

18.49
29.37
26.08
27.54

GloVe (840B.300d)

53.22 14.94

Table 12: Target segmentation task performance of contextualizers pretrained on a variety of different tasks. The
probing model used is linear, and the contextualizer architecture is ELMo (original).

E.3

Pairwise Prediction

Pretrained Representation

Syn. Arc
Syn. Arc
Coreference
Prediction Classification
Arc Prediction
(EWT)
(EWT)

Untrained ELMo (original), Layer 0
Untrained ELMo (original), Layer 1
Untrained ELMo (original), Layer 2
Untrained ELMo (original), Scalar Mix

73.75
68.40
68.86
72.24

66.27
56.73
56.62
70.62

66.25
62.82
63.15
69.72

CCG, Layer 0
CCG, Layer 1
CCG, Layer 2
CCG, Scalar Mix

75.92
84.93
84.45
85.44

69.84
85.59
84.59
88.11

67.84
62.10
59.19
70.14

Chunk, Layer 0
Chunk, Layer 1
Chunk, Layer 2
Chunk, Scalar Mix

76.67
85.18
84.80
85.42

69.72
86.50
84.84
87.57

65.60
62.74
60.23
68.92

PTB (POS), Layer 0
PTB (POS), Layer 1
PTB (POS), Layer 2
PTB (POS), Scalar Mix

76.07
83.97
83.88
84.17

70.32
86.64
86.44
87.72

67.50
63.43
61.61
69.61

Parent, Layer 0
Parent, Layer 1
Parent, Layer 2
Parent, Scalar Mix

76.20
84.93
85.57
86.01

68.99
86.15
85.61
87.49

67.80
62.69
59.10
69.34

GParent, Layer 0
GParent, Layer 1
GParent, Layer 2
GParent, Scalar Mix

76.59
85.96
85.69
86.17

69.51
85.33
84.38
87.49

68.99
60.84
58.76
70.24

GGParent, Layer 0
GGParent, Layer 1
GGParent, Layer 2
GGParent, Scalar Mix

76.28
85.74
85.49
86.27

69.91
83.45
82.12
86.57

69.24
59.73
58.89
70.58

Syn. Arc Prediction (PTB), Layer 0
Syn. Arc Prediction (PTB), Layer 1
Syn. Arc Prediction (PTB), Layer 2
Syn. Arc Prediction (PTB), Scalar Mix

77.04
90.39
90.82
91.66

68.01
81.00
76.50
84.18

68.28
60.29
57.46
69.15

Syn. Arc Classification (PTB), Layer 0
Syn. Arc Classification (PTB), Layer 1
Syn. Arc Classification (PTB), Layer 2
Syn. Arc Classification (PTB), Scalar Mix

76.14
86.55
87.46
87.78

71.80
90.04
89.35
90.98

68.40
62.10
59.74
70.00

Sem. Arc Prediction, Layer 0
Sem. Arc Prediction, Layer 1
Sem. Arc Prediction, Layer 2
Sem. Arc Prediction, Scalar Mix

76.25
84.91
85.86
86.37

67.73
73.11
69.75
80.74

69.44
57.62
55.91
69.72

Sem. Arc Classification, Layer 0
Sem. Arc Classification, Layer 1
Sem. Arc Classification, Layer 2
Sem. Arc Classification, Scalar Mix

75.85
85.30
86.10
86.53

70.12
86.21
84.50
87.75

68.96
60.25
58.39
70.36

Conj, Layer 0
Conj, Layer 1
Conj, Layer 2
Conj, Scalar Mix

72.62
80.84
80.46
80.96

58.40
68.12
64.30
73.89

68.50
58.46
57.89
71.96

BiLM, Layer 0
BiLM, Layer 1
BiLM, Layer 2
BiLM, Scalar Mix

84.27
86.36
86.44
86.42

86.74
86.86
86.19
85.93

71.75
70.47
70.14
71.62

ELMo (original), Layer 0
ELMo (original), Layer 1
ELMo (original), Layer 2
ELMo (original), Scalar Mix

77.73
86.46
85.34
86.56

78.52
93.01
91.32
91.69

72.89
71.33
68.46
73.24

GloVe (840B.300d)

73.94

72.74

72.96

Table 13: Target pairwise prediction task performance of contextualizers pretrained on a variety of different tasks.
The probing model used is linear, and the contextualizer architecture is ELMo (original).

