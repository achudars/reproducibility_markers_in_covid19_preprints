1

Learn to Compress CSI and Allocate
Resources in Vehicular Networks
arXiv:1908.04685v1 [eess.SP] 12 Aug 2019

Liang Wang, Member, IEEE, Hao Ye, Student Member, IEEE,
Le Liang, Member, IEEE, and Geoffrey Ye Li, Fellow, IEEE

Abstract
Resource allocation has a direct and profound impact on the performance of vehicle-to-everything
(V2X) networks. In this paper, we develop a hybrid architecture consisting of centralized decision
making and distributed resource sharing (the C-Decision scheme) to maximize the long-term sum rate
of all vehicles. To reduce the network signaling overhead, each vehicle uses a deep neural network
to compress its observed information that is thereafter fed back to the centralized decision making
unit. The centralized decision unit employs a deep Q-network to allocate resources and then sends the
decision results to all vehicles. We further adopt a quantization layer for each vehicle that learns to
quantize the continuous feedback. In addition, we devise a mechanism to balance the transmission of
vehicle-to-vehicle (V2V) links and vehicle-to-infrastructure (V2I) links. To further facilitate distributed
spectrum sharing, we also propose a distributed decision making and spectrum sharing architecture
(the D-Decision scheme) for each V2V link. Through extensive simulation results, we demonstrate that
the proposed C-Decision and D-Decision schemes can both achieve near-optimal performance and are
robust to feedback interval variations, input noise, and feedback noise.

Index Terms
Vehicular networks, deep reinforcement learning, spectrum sharing, binary feedback.

I. I NTRODUCTION
Connecting vehicles on the road as a dynamic communication network, commonly known
as vehicle-to-everything (V2X) networks, is gradually becoming a reality to make our daily
Liang Wang is with the Key Laboratory of Modern Teaching Technology, Ministry of Education, Xi’an 710062, China, and
also with the School of Computer Science, Shaanxi Normal University, Xi’an 710119, China (e-mail: wangliang@snnu.edu.cn).
Hao Ye, Le Liang and Geoffrey Ye Li are with the School of Electrical and Computer Engineering, Georgia Institute of
Technology, Atlanta, GA, 30332 USA (e-mail: {yehao, lliang}@gatech.edu; liye@ece.gatech.edu).

2

experience on wheels safer and more convenient [1]. V2X enabled coordination among vehicles,
pedestrians, and other entities on the road can alleviate traffic congestion, improve road safety,
in addition to providing ubiquitous infotainment services [2]–[4]. Recently, the 3rd generation
partnership project (3GPP) begins to support V2X services in the long-term evolution (LTE) [5]
and further the fifth generation (5G) mobile communication networks [6]. Cross-industry alliance
has also been founded, such as the 5G automotive association (5GAA), to push development,
testing, and deployment of V2X technologies.
Due to high mobility of vehicles and complicated time-varying communication environments,
it is very challenging to guarantee the diverse quality-of-service (QoS) requirements in vehicular
networks, such as extremely large capacity, ultra reliability, and low latency [7]. To address such
issues, efficient resource allocation for spectrum sharing becomes necessary in the V2X scenario.
Existing works on spectrum sharing in vehicular networks can be mainly categorized into two
classes: centralized schemes [8]–[11] and distributed approaches [12], [13]. For the centralized
schemes, decisions are usually made centrally at a given node, such as the head in a cluster or the
base station (BS) in a given coverage area. Novel graph-based resource allocation schemes have
been proposed in [8] and [9] to maximize the vehicle-to-infrastructure (V2I) capacity, exploiting
the slow fading statistics of channel state information (CSI). In [10], an interference hyper-graph
based resource allocation scheme has been developed in the non-orthogonal multiple access
(NOMA)-integrated V2X scenario with the distance, channel gain, and interference known in
each vehicle-to-vehicle (V2V) and V2I group. In [11], a segmentation medium access control
(MAC) protocol has been proposed in large-scale V2X networks, where the location information
of vehicles is updated. In these schemes, the decision making node needs to acquire accurate
CSI, interference information of all the V2V links, and each V2V link’s transmit power to make
spectrum sharing decisions. However, reporting all such information from each V2V link to the
decision making node poses a heavy burden on the feedback links, and even becomes infeasible
in practice.
As for distributed schemes [12], [13], each V2V link makes its own decision with partial
or little knowledge of other V2V links. In [12], a distributed shuffling based Hopcroft-Karp
algorithm has been devised to handle the subchannel allocation in V2V communications with
one-bit CSI broadcasting. In [13], the spatio-temporal traffic pattern has been exploited for
distributed load-aware resource allocation for V2V communications with slowly varying channel
information. In these methods, V2V links may exchange partial or none channel information

3

with their neighbors before making a decision. However, each V2V link can only observe partial
information of its surrounding environment since it is geographically apart from other V2V links
in the V2X scenario. This may leave some channels overly congested while others underutilized,
leading to substantial performance degradation.
Notably, the above works usually rely on some levels of channel information, such as channel
gain, interference, locations and so on. This kind of channel information is usually hard to obtain
perfectly in practical wireless communication systems, which is even challenging in the V2X
scenario. Fortunately, machine learning enables wireless communications systems to learn their
surroundings and feed critical information back to the BS for resource allocation. In particular,
reinforcement learning (RL) can make decisions to maximize long-term return in the sequential
decision problems, which has gained great success in various applications, such as AlphaGo
[14]. Inspired by its remarkable performance, the wireless community is increasingly interested
in leveraging machine learning for the physical layer and resource allocation design [15]–[23].
In particular, machine learning for future vehicular networks has been discussed in [24] and [25].
In [26], each V2V link is treated as an agent to ensure the latency constraint is satisfied while
minimizing interference to V2I link transmission. In [27], a multi-agent RL-based spectrum
sharing scheme has been proposed to promote the payload delivery rate of V2V links while
improving the sum capacity of V2I links. A dynamic RL scheduling algorithm has been developed
to solve the network traffic and computation offloading problems in vehicular networks [28].
In order to fully exploit the advantages of both centralized and distributed schemes while
alleviating the requirement on CSI for spectrum sharing in vehicular networks, we propose an RLbased resource allocation scheme with learned feedback. In particular, we devise a distributed CSI
compression and centralized decision making architecture to maximize the sum rate of all V2V
links in the long run. In this architecture, each V2V link first observes the state of its surrounding
channels and adopts a deep neural network (DNN) to learn what to feed back to the decision
making unit, such as the BS, instead of sending all observed information directly. To maximize
the long-term sum rate of all links, the BS then adopts deep reinforcement learning to allocate
spectrum for all V2V links. To further reduce feedback overhead, we adopt a quantization layer
in each vehicle’s DNN and learn how to quantize the continuous feedback. Besides, to further
facilitate distributed spectrum sharing, we devise a distributed spectrum sharing architecture to
let each V2V link make its own decision locally. The contributions of this paper are summarized
as follows.

4

•

We leverage the power of DNN and RL to devise a centralized decision making and
distributed implementation architecture for vehicular spectrum sharing that maximizes the
long-term sum rate of all vehicles. We use a weighted sum rate reward to balance V2I and
V2V performance dynamically.

•

We exploit the DNN at each vehicle to compress local observations, which is further
augmented by a quantized layer, to reduce network signaling overhead while achieving
desirable performance.

•

We also develop a distributed decision making architecture that allows spectrum sharing
decisions to be made at each vehicle locally and binary feedback is designed for signaling
overhead reduction.

•

Based on extensive computer simulations, we demonstrate both of the proposed architectures
can achieve near-optimal performance and are robust to feedback interval variations, input
noise, and feedback noise. In addition, the optimal number of continuous feedback and
feedback bits for each V2V link are presented that strike a balance between signaling
overhead and performance loss.

The rest of this paper is organized as follows. The system model is presented in Section II.
Then, the BS aided spectrum sharing architecture, including distributed CSI compression and
feedback, centralized resource allocation and quantized feedback, is introduced in Section III.
The distributed decision making and spectrum sharing architecture is discussed in Section IV.
Simulation results are presented in Section V. Finally, conclusions are drawn in Section VI.
II. S YSTEM M ODEL
We consider a vehicular communication network with N cellular users (CUs) and K pairs of
coexisting device-to-device (D2D) users, where all devices are equipped with a single antenna.
Let K = {1, 2, ..., K} and N = {1, 2, ..., N} denote the sets of all D2D pairs and CUs, respectively. Each pair of D2D users exchange important and short messages, such as safety-related
information via establishing a V2V link while each CU uses a V2I link to support bandwidthintensive applications, such as social networking and video streaming. In order to ensure the
QoS of the CUs, we assume all V2I links are assigned orthogonal radio resources. Without loss
of generality, we assume that each CU occupies one channel for its uplink transmission. To
improve the spectrum utilization efficiency, all V2V links share the spectrum resource with V2I
links. Therefore, N is also referred to as the channel set.

5

Denote the channel power gain from the n-th CU to the BS on the n-th channel, i.e., the n-th
V2I link, by gn [n]. Let hk,B [n] represent the cross channel power gain from the transmitter of the
k-th V2V link to the BS on the n-th channel. The received signal-to-interference-plus-noise-ratio
(SINR) of the n-th V2I link can be expressed as
γnc [n] = PK

Pnc gn [n]

d
2
k=1 ρk [n]Pk hk,B [n] + σ

,

(1)

where Pnc and Pkd refer to the transmit powers of the n-th V2I link and the k-th D2D pair,
respectively, σ 2 represents the noise power, and ρk [n] ∈ {0, 1} is the channel allocation indicator
with ρk [n] = 1 if the k-th D2D user pair chooses the n-th channel and ρk [n] = 0 otherwise.
P
We assume each D2D pair only occupies one channel, i.e., N
n=1 ρk [n] ≤ 1. Then, the capacity
of the n-th V2I link on the n-th channel can be written as

Cnc [n] = B log2 (1 + γnc [n]) ,

(2)

where B denotes the channel bandwidth.
Similarly, hk [n] denotes the channel power gain of the k-th V2V link on the n-th channel.
Meanwhile, hl,k [n] denotes the cross channel power gain from the transmitter of the l-th D2D
pair to the receiver of the k-th D2D pair on the n-th channel. Denote the cross channel power
gain from the n-th CU to the receiver of the k-th D2D pair on the n-th channel by gn,k [n].
Then, the SINR of the k-th V2V link over the n-th channel can be written as
γkd [n] =

ρk [n]Pkd hk [n]
,
Ik [n] + σ 2

(3)

where the interference power for the k-th V2V link Ik [n] is
Ik [n] =

K
X

ρl [n]Pld hl,k [n] + Pnc gn,k [n] .

(4)

l6=k

In (4), the terms

PK

l6=k

ρl [n]Pld hl,k [n] and Pnc gn,k [n] refer to the interference of the other V2V

links and the V2I link on the n-th channel, respectively. Hence the capacity of the k-th V2V
link on the n-th channel can be written as

Ckd [n] = B log2 1 + γkd [n] .

(5)

In the V2X networks, a naive distributed approach will allow each V2V link to select a channel
independently such that its own data rate is maximized. However, local rate maximization often
leads to suboptimal global performance due to the interference among different V2V links. On

6

the other hand, the BS in the V2X scenario has enough computational and storage resources to
achieve efficient resource allocation. With the help of machine learning, we propose a centralized
decision making scheme based on compressed information learned by each individual V2V link
distributively.
In order to achieve this goal, each V2V link first learns to compress local observations,
including the channel gain, the observed interference from other V2V links and V2I link, transmit
power, etc., and then feeds the compressed information back to the BS. According to feedback
information from all V2V links, the BS will make optimal decisions for all V2V links using
RL. Then, the BS broadcasts the decision result to all V2V links.
III. BS D ECISION

BASED

S PECTRUM S HARING A RCHITECTURE

As shown in Fig. 1, we adopt the deep RL approach for resource allocation. In this section,
we first design the DNN architecture of each V2V link and the deep Q-network (DQN) for
centralized control at the BS, respectively. Then, we propose the centralized decision making
and distributed spectrum sharing architecture, termed C-Decision scheme. Finally, we introduce
the binary feedback design for information compression.
A. V2V DNN Design
Here, we discuss the DNN at each V2V link to compress local observation for feedback. As
shown in Fig. 1, each V2V link k first observes its surroundings, and obtains its transmission
power, the current channel gains and interference powers of all channels, which are denoted
as hk = (hk [1] , ..., hk [n] , ..., hk [N]) and Ik = (Ik [1] , ..., Ik [n] , ..., Ik [N]), respectively. Here,
Ik [n] refers to the aggregated interference powers at the k-th V2V link on the n-th channel as
shown in (4). To consider the impact of V2V links on V2I links, the observation of the k-th
V2V also needs to include the cross channel gain from the k-th V2V link to all V2I links, such
as hk,B [n] , ∀n ∈ N . Then, the observation of the k-th V2V can be written as

ok = hk , Ik , Pkd , hk,B ,

(6)

where hk,B = (hk,B [1] , ..., hk,B [n] , ..., hk,B [N]). Here, the channel information hk can be
accurately estimated by the receiver of the k-th V2V link and we assume it is also available
at the transmitter through delay-free feedback [29]. Similarly, the received interference power
over all channels Ik can be measured at the k-th V2V receiver. Each V2V transmitter knows its

7

VW99

VW'11

kWK99

kWK'11

KWK'11

%6'41

%6

'HFLVLRQ
)HHGEDFN

KWK99

Fig. 1. Neural network architecture for V2V links and the BS in the C-Decision scheme.

transmit power Pkd. Besides, the vector hk,B can be estimated at the BS and then broadcast to
all V2V links in its coverage, which incurs a small signaling overhead [27].
Then, the local observation, ok , is compressed using the DNN at each V2V link. The compressed information, bk , which is the output of the DNN, is fed back to the DQN at the BS. To
limit overhead on information feedback, each V2V link only reports the compressed information
vector, bk , instead of ok to the BS. Here, bk = {bk,j } is also known as the feedback vector of
the k-th V2V link and the term bk,j , ∀j ∈ {1, 2, ..., Nk } refers to the j-th feedback element of
the k-th V2V, where Nk denotes the number of feedback learned by the k-th V2V link. All V2V
links aim at maximizing their global sum rate in the long run while minimizing the feedback
information bk . Therefore, the parameters of the DNNs at all V2V links and those of the DQN
will be jointly determined to maximize the sum rate of the whole V2X network.
B. Deep Q-Network at the BS
To make a proper resource sharing decision, we introduce the deep RL architecture at the
BS as shown in Fig. 1. In order to maximize the long-term sum rate of all links, we resort
to the RL technique by treating the BS as the agent. In the RL, an agent interacts with its
surroundings, named as the environment, via taking actions, and then observes a corresponding
numerical reward from the environment. The agent’s goal is to find optimal actions so that the

8

expected sum of rewards is maximized. Mathematically, the RL can be modelled as a Markov
decision process (MDP). At each discrete time slot t, the agent observes the current state St of
the environment from the state space S and then chooses an action At from the action space A
and one time step later obtains a reward Rt+1 . Then, the environment evolves to the next state
St+1 , with the transition probability p (s′ , r|s, a) , Pr {St+1 = s′ , Rt+1 = r|St = s, At = a}.
The BS treats all the learned feedback as the current state s of the agent’s environment, which
can be expressed as:
s = {b1 , b2 , ..., bK } , ∀k ∈ K.

(7)

Then, the action of the BS is to determine the values of the channel indicators, ρk [n], for each
V2V link. Thus, we define the action a of the BS as
a = {ρ1 , ..., ρk , ..., ρK } , ∀k ∈ K,

(8)

where ρk = {ρk [n]} , ∀n ∈ N refers to the channel allocation vector for the k-th V2V link.
Finally, we design the reward for the BS, which is very crucial to the performance of RL.
To maximize the long-term sum rate of V2V links while ensuring the QoS of V2I links in the
V2X scenario, we need to devise a mechanism to consider the transmissions of V2V links and
V2I links simultaneously. As we know, the V2V links usually carry the safety-critical messages,
such as vehicle’s speed and emergency vehicle warning on the road, while the V2I links often
support the entertainment services [27]. Thus, we should guarantee the transmission of V2V
links as the primal target while making sure that the impact of V2V transmission on the V2I
links can be tolerable and adjustable to some specific applications. To this end, we model the
reward of the BS as
R = λc

N
X
n=1

where Ckd =

PN

n=1

Cnc

[n] + λd

K
X

Ckd ,

(9)

k=1

Ckd [n] refers to the capacity of the k-th V2V on all the channels. Besides,

λc and λd are nonnegative weights to balance the performance of V2I links and V2V links.
The solution of the RL problem is related to the concept of policy π (a, s), which defines the
probabilities of choosing each action in A when observing a state in S. The goal of learning is
to find an optimal policy π ∗ to maximize the expected return Gt from any initial state s0 . The
P
k
expected return is defined as Gt = ∞
k=0 γ Rt+k+1 , which is the cumulative discounted return
with a discount factor γ.

9

To solve this problem, we resort to the Q-learning [30], which is a well-known effective approach to tackle the RL problem, due to its model-free property where p (s′ , r|s, a) is not required
a priori. Q-learning is based on the idea of action-value function qπ (s, a) = Eπ [Gt |St = s, At = a]
for a given policy π, which means the expected return when the agent starts from the state s,
takes action a, and thereafter follows the policy π. The optimal action-value function q ∗ (s, a)
under the optimal policy π ∗ satisfies the well-known Bellman optimality equations [31], which
can be approached through an iterative update method:
h
i
Q (St , At ) ← Q (St , At ) + α Rt+1 + γ max Q (St+1 , a) − Q (St , At ) ,
a

(10)

where α is the step-size parameter. Besides, the choice of action At in state St follows some
exploratory policies, such as the ǫ-greedy policy. For better understanding, the ǫ-greedy policy
can be expressed as
A←


 arg max Q (s, a),
a

 a random action,

with probability 1 − ǫ;

(11)

with probability ǫ.

Here, ǫ is also known as the exploration rate in the RL literature. Furthermore, it has been shown
in [31] that with a variant of the stochastic approximation conditions on α and the assumption
that all the state-action pairs continue to be updated, Q converges with probability 1 to the
optimal action-value function q ∗ .
However, in many practical problems, the state and action space can be extremely large, which
prevents storing all action-value functions in a tabular form. As a result, it is common to adopt
function approximation to estimate these action-value functions. Moreover, by doing so, we can
generalize action-value functions from limited seen state-action pairs to to a much larger space.
In [32], a DNN parameterized by θ is employed to represent the action-value function, thus
called as DQN. DQN adopts the ǫ-greedy policy to explore the state space and store the transition
tuple (St , At , Rt+1 , St+1 ) in a replay memory (also known as the replay buffer) at each time step.
The replay memory accumulates agent’s experiences over many episodes of the MDP. At each
time step, a mini-batch of experiences D are uniformly sampled from the replay memory, called
experience replay, to update the network parameters θ with variants of stochastic gradient descent
method to minimize the squared errors shown as follows:
i2
Xh

Rt+1 + γ max Q St+1 , a; θ− − Q (St , At ; θ) ,
t∈D

a

(12)

10

where θ − is the parameter set of a target Q-network, which is duplicated from the training Qnetwork parameter set θ, and fixed for a couple of updates with the aim of further improving the
stability of DQN. Besides, experience replay improves sample efficiency via repeatedly sampling
experiences from the replay memory and also breaks correlation in successive updates, which
also stabilizes the learning process.
C. Centralized Control and Distributed Transmission Architecture
In this part, the architecture for the C-Decision scheme is shown in Fig. 1. Each V2V link first
observes its local environment and then adopts a DNN to compress the observed information into
several real numbers, which are finally fed back to the BS for centralized decision making. The
BS takes the feedback information of all V2V links as the input, utilizes the DQN to perform
Q-learning to decide the channel allocation for all V2V links, and broadcasts its decision. Finally,
each V2V link chooses the BS-allocated channel for its transmission.
Details of the training framework for the C-Decision scheme are provided in Algorithm 1.
We define Ot = {otk } , ∀k ∈ K as the observations of all V2Vs at the time step t ∈ {1, 2, ..., T },
where otk refers to the observation of the k-th V2V at the time step t. Then, we can express the
estimation of the return also known as the approximate target value [32] as

(13)
yt = Rt+1 + γ max Q Ot+1 , a; θ − ,
a

where Rt+1 and Q Ot+1 , a; θ − represent the reward of all links and the Q function of the

target DQN with parameters θ − under the next observation Ot+1 and the action a, respectively.

Then, the updating process for the BS DQN can be written as [32], [33]:
X ∂Q (Ot , at ; θ)
[yt − Q (Ot , at ; θ)] ,
θ ←θ+β
∂θ
t∈D

(14)

where β is the step size in one gradient iteration.
As for the testing phase, at each time step t, each V2V adopts its observation otk as the input
of the trained DNN to obtain its learned feedback btk , and then sends it to the BS. After that, the
BS takes {btk } as the input of its trained DQN to generate the decision result at , and broadcasts
at to all V2Vs. Finally, each V2V chooses the specific channel indicated by at to transmit.
D. Spectrum Sharing with Binary Feedback
In order to further reduce feedback overhead, we propose a framework to quantize the V2V
links’ real-valued feedback into several binary digits. In other words, we try to constrain bk,j ∈

11

Algorithm 1 Training algorithm for the C-Decision scheme
Input: the DNN model for each V2V, the DQN model for the BS, the V2X environment
simulator
Output: the DNN for each V2V, the optimal control policy π ∗ represented by a DQN Q with
parameters θ
1:

Initialize all DNNs and DQN models respectively

2:

for episode l = 1, ..., Ltrain do

3:

Start the V2X environment simulator, generate vehicles, V2V links and V2I links

4:

Initialize the beginning observations O0

5:

Initialize the policy π randomly

6:

for time-step t = 1, ..., T do

7:

Each V2V adopts the current observation otk as the input of its DNN to learn the
feedback btk , and sends it to BS

8:

BS takes st = {btk } as the input of its DQN Q

9:

BS chooses at according to st using some policy π derived from Q, e.g., ǫ-greedy
strategy as in (11)

10:

BS broadcasts the action at to every V2V

11:

Each V2V takes action based on at , and gets the reward Rt+1 and the next observation
ot+1
k

12:

Save the data {Ot , at , Rt+1 , Ot+1 } into the replay buffer B

13:

Sample a mini-batch of data D from B uniformly

14:

Use the data in D to train all V2Vs’ DNNs and BS’s DQN together as in (14)

15:

Each V2V updates its observation otk ← ot+1
k

16:

Update the target network: θ − ← θ every Nu steps

17:
18:

end for
end for

12

{−1, 1} , ∀k ∈ K, ∀j ∈ {1, 2, ..., Nk }. The binarization procedure can help force the neural
networks to learn efficient representations of the feedback information compared to the standard
floating-point layer. In other words, a binary layer can make each V2V compress its observation
more efficiently.
The binary quantization process consists of two steps [34]. The first step is to generate the
required number of continuous feedback values in the continuous interval [−1, 1], which is also
equal to the desired number of the binary feedback. Then, the second step takes the outputs of
the first step as its input to produce the desired number of discrete feedback in the set {−1, 1}
for each output real-valued feedback of the first step.
For the first step, we adopt a fully-connected layer with tanh activations, defined as tanh (x) =
2
1+e−2x

− 1, where we term this layer as the pre-binary layer. Here, the input of this pre-binary

layer connects the outputs of each V2V’s DNN. Then, in order to binarize the continuous output
of the first step, we adopt the traditional sign function method in the second step. To be specific,
we take the sign of the input value as the output of this layer, which is shown as below:

 1, x ≥ 0;
(15)
b (x) =

−1, x < 0.

However, the gradient of this function is not continuous, challenging the back propagation
procedure for DNN training. As a remedy to this, we adopt the identity function in the backward
pass, which is known as the straight-through estimator [35]. Combining these two steps together,
we can express the full binary feedback function as
B (x) = b (tanh (W0 x + b0 )) ,

(16)

where W0 and b0 denote the linear weights and bias of the pre-binary layer that transform the
activations from the previous layer in the neural network respectively. Here, we term this layer
as the binary layer.
Finally, to implement the C-Decision scheme with binary feedback, we add the full binary
feedback function in (16), which consists of the pre-binary layer in the first step and the binary
layer in the second step, to the output of each V2V link’s DNN. Besides, in response to the
change in the number of feedback bits at each V2V link’s new DNN, the number of inputs in
the DQN of BS should change correspondingly.

13

IV. D ISTRIBUTED D ECISION M AKING

AND

S PECTRUM S HARING A RCHITECTURE

In order to further facilitate distributed spectrum sharing and reduce the computational complexity, we propose the distributed decision making and spectrum sharing architecture (named
as the D-Decision scheme) shown in Fig. 2 to let each V2V link make its own spectrum sharing
decision. In this section, we first devise the neural network architecture for each V2V link to
compress CSI and make decision, respectively, and then design the neural network for the BS
to aggregate feedback from all V2V links. Then, we propose the hybrid information aggregation
and distributed control architecture. Finally, we propose the D-Decision scheme with the binary
aggregated information.
A. DNN Design at V2V and BS
To enable distributed decision making, each V2V contains one DNN to compress local
observations for feedback, termed the Compression DNN and another DQN for distributed
spectrum sharing decision making, termed Decision DQN. Here, we employ the same DNN
architecture for each V2V as that in Part A of Section III since they share the same functionality.
The BS aggregates the feedback from all V2Vs via its DNN, termed as the Aggregation DNN,
and then broadcasts the aggregated global information (AGI) φ to all V2Vs. Here, the AGI can
be expressed as φ = {φj } , ∀j ∈ {1, 2, ..., Ng }, where Ng refers to the number of AGI values
and also equals the number of outputs of BS Aggregation DNN. Finally, each V2V combines
its local observation and the AGI as the input of its Decision DQN to decide which channel to
transmit.
B. Hybrid Information Aggregation and Distributed Control Architecture
Each V2V link first observes its local environment to obtain ok , and then adopts its Compression DNN to compress ok into several real numbers bk , and finally feeds this compressed
information back to the BS. After that, the BS takes the feedback values of all V2V links {bk }
as the input of its Aggregation DNN to aggregate the compressed observations of all V2V links
and further compress this information into the AGI φ. Finally, each V2V link combines the
received AGI φ and its local observation ok as the input of its Decision DQN, and performs the
Q-learning algorithm to decide which channel to transmit.
Details of the training framework for the D-Decision scheme are provided in Algorithm 2.
Here, we define at = {atk } , ∀k ∈ K as the actions of all V2V links at the time step t ∈

14

VW&RPSUHVVLRQ'11

VW99

VW'HFLVLRQ'41

%6

%6$JJUHJDWLRQ'11

KWK'HFLVLRQ'41

)HHGEDFN
$*,
'HFLVLRQ

KWK99

KWK&RPSUHVVLRQ'11

Fig. 2. Neural network architecture for V2V links and the BS in the D-Decision scheme.

{1, 2, ..., T }, where atk = ρk refers to the action for k-th V2V. Besides, in the training process,
we take the observations of all V2V links Ot as the input and train all DNNs and DQNs in an
end-to-end manner. The training process can be implemented in a fully distributed manner.
As for the testing phase, at each time step t, each V2V link adopts its observation otk as the
input of its Compression DNN to learn the feedback btk , and sends it to the BS. Then, the BS
utilizes {btk } as the input of its Aggregation DNN to generate the AGI φt , and broadcasts φt
to all V2V links. Finally, each V2V link takes otk and φt as the input of its Decision DQN to
make decision, and then transmits on the chosen channel.
C. Distributed Spectrum Sharing with binary information
Similar to Section III-D, we can also quantize the continuous feedback and the AGI in the
D-Decision scheme into the binary data to further reduce signaling overhead. Then, both the
Compression DNN of each V2V link and the Aggregation DNN at the BS need to include the
binary function in (16).

15

Algorithm 2 Training algorithm for the D-Decision scheme
Input: the Compression DNN and Decision DQN for each V2V, the Aggregation DNN for the
BS, the V2X environment simulator
Output: the Compression DNN, the optimal policy πk∗ represented by the Decision DQN Qk
with parameters θ k for each V2V, the Aggregation DNN for the BS
1:

Initialize all DNNs and DQNs models respectively

2:

for episode l = 1, ..., Ltrain do

3:

Start the V2X environment simulator, generate vehicles, V2V links and V2I links

4:

Initialize the beginning observations O0

5:

Initialize the policy πk for each V2V randomly

6:

for time-step t = 1, ..., T do

7:

Each V2V adopts the current observation otk as the input of its Compression DNN
to learn the feedback btk , and sends it to BS

8:

BS takes {btk } as the input of its Aggregation DNN, and generates the AGI φt

9:

BS broadcasts the AGI φt to every V2V

10:

Each V2V takes stk = {otk , φt } as the input of its Decision DQN Qk

11:

Each V2V chooses atk according to stk using some policy πk derived from Qk , e.g.,
ǫ-greedy strategy as in (11)

12:

Each V2V takes action atk , and gets the reward Rt+1 and the next observation ot+1
k

13:

Save the data {Ot , at , Rt+1 , Ot+1 } into the buffer B

14:

Sample a mini-batch of data D from B uniformly

15:

Use the data in D to train all V2Vs’ Compression DNNs and Decision DQNs
and BS’s Aggregation DNN together as in (14)

16:

Each V2V updates its observation otk ← ot+1
k

17:

Each V2V updates its target network: θ −
k ← θ k every Nu steps

18:
19:

end for
end for

16

V. S IMULATION R ESULTS
In this section, we conduct extensive simulation to verify the performance of the proposed
schemes. In particular, we provide the simulation settings in Part A, and evaluate the training
performance of the C-Decision scheme in Part B. Then, we assess the testing performance
under the real-valued feedback and binary feedback in Parts C and D respectively. Besides, we
demonstrate the impacts of V2I and V2V links weights on the performance in Part E and the
robustness of the proposed scheme in Part F, respectively. Finally, we show the training and
testing performance of the D-Decision scheme in Part G.
A. Simulation Settings
The simulation scenario follows the urban case in Annex A of [5]. The simulation area size is
1, 299 m × 750 m, where the BS is located in the center of this area. For better understanding,
we provide related parameters and their corresponding settings in Table I. In addition, we list
the corresponding channel models for both V2V and V2I links respectively in Table II.
TABLE I
S IMULATION PARAMETERS
Parameters

Typical values

Number of V2I links N

4

Number of V2V links K

4

Carrier frequency

2 GHz

Normalized Channel Bandwidth

1

BS antenna height

25 m

BS antenna gain

8 dBi

BS receive noise figure

5 dB

Vehicle antenna height

1.5 m

Vehicle antenna gain

3 dBi

Vehicle receive noise figure

9 dB

Vehicle speed

randomly in [10, 15] km/h

Vehicle drop and mobility model
V2I transmit power

Pnc

V2V transmit power

Pkd

Urban case of A.1.2 in [5]
23 dBm
10 dBm

The specific architecture of DNNs and BS DQN under the C-Decision scheme are summarized
in Table III, where Nk to refers the number of feedback for each V2V link and FC denotes the

17

TABLE II
C HANNEL MODELS FOR V2I AND V2V LINKS
Parameter

V2I link

V2V link

Path loss model

128.1 + 37.6 log10 (d), d in km

LOS in WINNER + B1 Manhattan [36]

Shadowing distribution

Log-normal

Log-normal

Shadowing standard deviation

8 dB

3 dB

Decorrelation distance

50 m

10 m

Noise power σ

2

-114 dBm

-114 dBm

Fast fading

Rayleigh fading

Rayleigh fading

Fast fading update

Every 1 ms

Every 1 ms

TABLE III
A RCHITECTURE FOR DNN AND BS DQN IN THE C-D ECISION SCHEME

DNN

BS DQN

Input layer

13

K × Nk

Hidden layers

3 FC layers (16, 32, 16)

3 FC layers (1200, 800, 600)

Output layer

Nk

256

fully connected (FC) layer respectively. In addition, the number of neurons in the output layer
of the BS DQN is set as 256, which refers to all the possible channel allocations for all V2V
links under current simulation setting. Besides, the settings for the DNNs and DQNs under the
D-Decision scheme are listed in Table IV.
We use the rectified linear unit (ReLU) activation function for both DNN and DQNs, defined
as f (x) = max (0, x). Here, the activation function of output layers in DNNs and DQNs is
set as a linear function. Besides, the RMSProp optimizer [37] is adopted to update the network
parameters with a learning rate of 0.001. The loss function is set as the Huber loss [38].
We choose the weights λc = 0.1 and λd = 1 for V2I and V2V links, respectively. We train the

TABLE IV
A RCHITECTURE FOR DNN S AND DQN S IN THE D-D ECISION SCHEME

Compression DNN

Aggregation DNN

Decision DQN

Input layer

13

K × Nk

Ng

Hidden layers

3 FC layers (16, 32, 16)

3 FC layers (500, 400, 300)

3 FC layers (80, 40, 20)

Output layer

Nk

Ng

4

18

11000

0.008

Average Return per Episode

Training Loss

0.007
0.006
0.005
0.004
0.003
0.002
0.001
0

250

500 750 1000 1250 1500 1750 2000
Number of Training Episodes

10000
9000
8000
7000
6000
5000

0

(a) Training loss

250

500

750

1000

1250

1500

Number of Training Episodes

1750

2000

(b) Average return per episode

Fig. 3. Training performance evaluation for the C-Decision scheme.

whole neural network for 2, 000 episodes and the exploration rate ǫ is linearly annealed from 1
to 0.01 over the beginning 1, 600 episodes and keeps constant afterwards. The number of steps
in each episode is set as T = 1, 000. The update frequency Nu of the target Q-network is every
500 steps. The discount factor, γ, in the training is chosen as 0.05. The size of the replay buffer
B is set as 1, 000, 000 samples. Meanwhile, the mini-batch size D varies in different settings, to
be specified in each figure.
B. Training Performance Evaluation
Fig. 3 demonstrates the training performance of the proposed C-Decision scheme with a minibatch size D = 512 and the number of real-valued feedback Nk = 3. In Fig. 3 (a), the loss
function decreases quickly with the increasing number of training episodes Ltrain , and becomes
nearly unchanged with the further increasing Ltrain . On the other hand, the change of average
return per episode is displayed in Fig. 3 (b). Here, we evaluate the training process every 5
training episodes under 10 different random seeds with the exploration rate ǫ = 0, and plot the
average return per episode in Fig. 3 (b). The average return per episode first increases quickly
with increasing Ltrain , and gradually converges despite some small fluctuations due to the timevarying V2X scenario, which shows the stability of the training process. Thus, Fig. 3 (a) and
(b) demonstrate the desired convergence of the proposed training algorithm. Therefore, we set
Ltrain = 2, 000 for the C-Decision scheme afterwards.

19

100%

90%

90%

Optimal Scheme
C-Decision
Average of C-Decision
Random Action
Average of Random Action

80%
70%
60%
50%
40%

Average Return Percentage

Normalized Return per Episode

100%

80%

70%

60%

50%

0

250

500

750

1000

1250

1500

Number of Testing Episodes

1750

2000

(a) Normalized return comparison

Batchsize = 256
Batchsize = 512
Batchsize = 1000

40%
0

1

2

3

4

5

6

7

8

9

Number of Real-valued Feedback

(b) ARP performance with real-valued feedback

Fig. 4. Performance evaluation for the C-Decision scheme with real-valued feedback.

C. Performance of Real-Valued Feedback
Fig. 4 (a) shows the return variation under the real-valued feedback with the number of testing
episodes. Here, we choose the mini-batch size as D = 512, number of testing episodes as
Ltest = 2, 000, and the number of real-valued feedback as Nk = 3, respectively. For comparison,
we also display the performance of two benchmark schemes: the optimal and the random action
schemes, respectively. In the optimal scheme, we perform time-consuming brute-force search to
find the optimal spectrum allocation in each testing step. In the random action scheme, each
V2V link chooses the channel randomly. For better comparison, we depict the normalized return
of these three schemes in Fig. 4 (a), where we use the return of the optimal scheme to normalize
the return of the other two schemes in each testing episode. Besides, the average return of our
proposed scheme and the random action scheme are also depicted. In Fig. 4 (a), the performance
of the C-Decision approaches 100% in most episodes and its average performance is about 97%
of the optimal scheme while the average performance of random selection is about 55% of the
optimal performance. Thus, we conclude the proposed C-Decision scheme can achieve nearoptimal spectrum sharing.
Fig. 4 (b) shows the impacts of different mini-batch sizes D and different numbers of realvalued feedback Nk on the performance of the C-Decision scheme, which adopts the average
return percentage (ARP) as the metric. Here, the ARP metric is defined as: the return under
the C-Decision scheme is first averaged over 2, 000 testing episodes and then normalized by
the average return of the optimal scheme. In Fig. 4 (b), the number of real-valued feedback

20

equals 0 refers to the situation where each V2V link does not feed anything back to the BS
and therefore, each V2V link just randomly selects channel to transmit, which is known as the
random action scheme. From Fig. 4 (b), the ARP under the C-Decision scheme increases rapidly
with the increase of Nk , and reaches the maximal percentage nearly 98% at Nk = 3. Thereafter,
the ARP virtually keeps constant with increasing Nk . In other words, each V2V link only needs
to send 3 real-valued feedback to the BS to achieve near-optimal performance. Besides, different
mini-batch sizes can achieve very similar performance. Particularly, the mini-batch size D = 512
achieves the best performance, which is good enough considering the computational overhead
in the training process and the gained performance.
D. Performance of Binary Feedback
Fig. 5 demonstrates the change of the ARP performance with an increasing number of feedback
bits under different mini-batch sizes. Here, we fix the number of real-valued feedback as 3, and
quantize these real-valued feedback into different numbers of feedback bits. Similarly, the number
of feedback bits equals 0 in Fig. 5 refers to the situation where each V2V link does not feedback
anything to the BS and just adopts the random action scheme. The ARP first increases quickly
with the number of feedback bits, and then keeps nearly unchanged with the further increasing
of feedback bits after the number of feedback bits is larger than 21. The ARP under different
D has quite similar performance. Besides, the ARP can reach 94% with 36 feedback bits under
D = 512. Considering the performance and feedback overhead tradeoff, we choose 36 feedback
bits under D = 512 in the subsequent evaluation.
E. Impacts of V2I and V2V Weights
In this part, we evaluate the impacts of V2I links weight λc and V2V links weights λd on
the system performance. For better understanding, we fix λd = 1 and vary the values of λc .
Fig. 6 demonstrates the empirical cumulative distribution function (CDF) of V2I and V2V sum
rate. In Fig. 6, “Real FB” and “Binary FB” refer to the proposed C-Decision scheme with realvalued feedback and that with binary feedback respectively, and “Optimal” represents the optimal
scheme. In particular, two empirical CDFs of V2I sum rate under both real-valued feedback and
binary feedback in Fig. 6 (a) shift quickly to the right when the V2I weight λc = 0.1 increases
to 0.5, which shows our proposed scheme can ensure different QoS requirements of V2I links
via adjusting λc . Besides, the performance gap between the real-valued feedback and binary

21

100%

Average Return Percentage

90%

80%

70%

60%

50%

Batchsize = 256
Batchsize = 512
Batchsize = 1000

40%
0

5

10

15

20

25

30

35

40

45

Number of Feedback Bits

1.0

1.0

0.8

0.8
Empirical CDF

Empirical CDF

Fig. 5. The ARP performance for the C-Decision scheme with binary feedback.

0.6
Optimal λc = 0.1
Real FB λc = 0.1
Binary FB λc = 0.1
Optimal λc = 0.5
Real FB λc = 0.5
Binary FB λc = 0.5

0.4
0.2
0.0

10

20
30
40
50
V2I Sum Rate per Step/[bps/Hz]

(a) V2I sum rate comparison

0.6

0.7
0.6

0.4

0.5
0.4

0.2

60

0.0

0.3
2

0

10

4

6

8

10

20
30
40
50
60
V2V Sum Rate per Step/[bps/Hz]

12

70

(b) V2V sum rate comparison (same legend as Fig. 6 (a))

Fig. 6. Sum rate performance with different weights.

feedback decreases with the increase of λc . From Fig. 6 (b), the empirical CDFs of V2V sum
rate under the real-valued feedback and binary feedback are very close to each other and shift
slightly to the left with increasing λc , which implies the rate degradation of V2V links is quite
small. Besides, the CDFs of V2V sum rate under both feedback schemes are very close to that
under the optimal scheme, and slightly deviate from the optimal performance with the further
increase of λc . Thus, we can see that the proposed C-Decision scheme can ensure negligible
degradation of V2V links while adjusting the QoS of V2I links via choosing different values of
λc .

22

100%

Noramlized Average Return

90%

80%

70%

60%

50%
Real Feedback = 3
Binary Feedback = 36
40%
100

101

102

103

104

105

Feedback Interval/[time steps]

Fig. 7. Normalized average return with different feedback intervals.

F. Robustness Evaluation
Fig. 7 shows the impacts of different feedback intervals on the performance of both realvalued feedback and binary feedback, where the feedback interval is measured in the number of
testing steps. To investigate the impact of very large feedback intervals on the performance, we
set the number of testing steps T as 50, 000 and the number of testing episodes Ltest as 200.
The normalized average return under both feedback schemes decreases quite slowly with the
increasing feedback interval at the beginning, which shows that the proposed scheme is immune
to the feedback interval variations and then drops quickly with the very large feedback interval.
Please note where the average return is normalized by the average return under the scheme with
Nk = 3 since we set T = 50, 000 and it is very high computational demanding to find the return
under the optimal scheme.
Fig. 8 evaluates the impacts of different noise sources on the ARP performance. Specifically,
Fig. 8 (a) illustrates the impacts of noisy input on the performance of both real-valued feedback
and binary feedback. Here, the x-axis means the ratio of the strength of Gaussian white noise
with respect to the each observation (such as channel gain value) for V2V links. In Fig. 8 (a),
the ARP under both feedback schemes decreases very slowly at the beginning and then drops
very quickly, and finally keeps nearly unchanged with the very large input noise, which shows
the robustness of the proposed scheme. In addition, the proposed scheme can also gain nearly
60% of the optimal performance under both real-valued feedback and binary feedback even at
the very large input noise, which is still better than the random action scheme shown in Fig. 4

100%

100%

90%

90%

Average Return Percentage

Average Return Percentage

23

80%

70%

60%

50%

80%

70%

60%

50%
Real Feedback = 3
Binary Feedback = 36

40%
-20

-15

-10

-5

0

5

10

15

20

25

Real Feedback = 3
Binary Feedback = 36
30

40%
-20

Noise to Input value ratio /[dB]

(a) The ARP performance under the noisy input

-15

-10

-5

0

5

10

15

20

25

30

Noise to Feedback value ratio/[dB]

(b) The ARP performance under the noisy feedback

Fig. 8. Impact of noise on the ARP performance.

(a). Based on this observation, we remark the proposed scheme can learn the intrinsic structure
of the resource allocation in the V2X scenario.
Besides, Fig. 8 (b) displays the impacts of noisy feedback on the performance of both feedback
schemes. Here, noisy feedback refers to the situation where noise occurs when each V2V link
sends its learned feedback to the BS. Similarly, the x-axis means the ratio of the strength of the
Gaussian white noise with respect to each feedback. In Fig. 8 (b), the ARP of both feedback
schemes keeps nearly unchanged with the increasing feedback noise, which demonstrates the
robustness of the proposed scheme, and then decreases more quickly under the real-valued
feedback compared with that under the binary feedback with the further increasing feedback
noise. This is because there are only 3 real-valued feedback under the real-valued feedback
scheme while there exist 36 feedback bits under the binary feedback scheme. Finally, the ARP
of both feedback schemes becomes nearly constant with the very large feedback noise. Similarly,
the binary feedback scheme is more robust to the feedback noise compared with the real-valued
feedback scheme.
G. Performance Evaluation for the D-Decision Scheme
Fig. 9 evaluates the training process of the D-Decision scheme. Here, we choose D = 512,
Nk = 3 and Ngr = 16, respectively. In particular, the training loss for the 1st V2V in Fig.
9 (a) first decreases very slowly with some jitters with an increasing Ltrain , and then drops
almost linearly, and finally becomes nearly unchanged with the further increase of Ltrain . The

24

11000

Average Return per Episode

0.5

Training Loss

0.4
0.3
0.2
0.1
0

2000

4000
6000
8000
Number of Training Episodes

10000

10000
9000
8000
7000
6000
5000

0

(a) Training loss of the 1st V2V

2000

4000

6000

Number of Training Episodes

8000

10000

(b) Average return per episode

Fig. 9. Training performance evaluation for the D-Decision scheme.

average return per episode under the D-Decision scheme in Fig. 9 (b) first increases quickly
with the increase of Ltrain , and then increases slowly, and finally gradually converges despite
some fluctuations, which shows the stability of the training process. Besides, we observe that
Ltrain = 10, 000 under the D-Decision scheme is much bigger than Ltrain = 2, 000 under the
C-Decision scheme, which indicates that the D-Decision scheme converges more slowly than
the C-Decision scheme. To train the whole neural network well, we set Ltrain = 10, 000 under
the D-Decision scheme. Besides, the exploration rate ǫ is linearly annealed from 1 to 0.01 over
the beginning 8, 000 episodes and then keeps constant.
Then, the testing performance of the D-Decision scheme with the increasing number of AGI
values is shown in Fig. 10. In particular, Fig. 10 (a) illustrates the ARP performance with the
increasing number of real-valued AGI Ngr . Here, we set the number of real-valued feedback
which each V2V transmits to the BS as 3 as indicated by Fig. 4 (b). The APR first increases
with increasing Ngr , and then keeps nearly unchanged with the further increase of Ngr . Especially,
the ARP nearly achieves its maximal value 96% when Ngr = 16. In other words, the BS only
needs 16 real-valued AGI to represent the real-valued feedback of all V2V links to achieve 96%
of the optimal performance. Furthermore, even when Ngr = 2, the ARP can still reach 90%,
which is suitable for the bandwidth-constrained broadcast channel of the BS. Compared with
the C-Decision scheme, the D-Decision scheme only incurs 2% ARP degradation. However, it
can achieve the fully distributed decision making and spectrum sharing, which is very appealing
in the V2X scenario. In addition, the computational complexity for decision making under the

100%

100%

90%

90%

Average Return Percentage

Average Return Percentage

25

80%

70%

60%

50%

80%

70%

60%

50%
Batchsize = 512

Batchsize = 512

40%

40%
0

2

4

6

8

10

12

14

16

18

20

0

Number of Real-valued Aggregated Global Information

(a) ARP with real-valued aggregated global information

10

20

30

40

50

60

70

80

90

100

Number of Aggregated Global Information Bits

(b) ARP with binary aggregated global information

Fig. 10. The ARP performance with two kinds of aggregated global information.

D-Decision scheme is greatly reduced compared with that under the C-Decision scheme, which
can further facilitate the fully distributed spectrum sharing in the V2X scenario.
Besides, the testing performance of the D-Decision scheme with the binary AGI is evaluated
in Fig. 10 (b). Here, we choose the number of feedback bits as 36 for each V2V link and the
number of real-valued AGI Ngr = 16. In Fig. 10 (b), the ARP first increases with the increasing
number of AGI bits Ngb , and then becomes nearly unchanged with the further increase of Ngb .
In particular, the APR reaches 90% when Ngb = 80. Meanwhile, the APR is very close to 90%
even when Ngb = 36. Similarly, compared with the C-Decision scheme with binary feedback, the
D-Decision scheme with the binary feedback only incurs 4% ARP degradation, which, however,
can be implemented in a fully distributed manner.
VI. C ONCLUSION
In this paper, we proposed a novel C-Decision architecture to allow distributed V2V links to
share spectrum efficiently with the aid of the BS in V2X scenario and also devised an approach
to binarize the continuous feedback. To further facilitate distributed decision making, we have
developed a D-Decision scheme for each V2V link to make its own decision locally and also
designed the binary procedure for this scheme. Simulation results demonstrated that the number
of real-valued feedback can be quite small to achieve near-optimal performance. Meanwhile,
the D-Decision scheme can also gain near-optimal performance and enable a fully distributed
decision making, which is more appealing to the V2X networks. Besides, the quantization of

26

the feedback or AGI incurs small performance loss with an acceptable number of bits under
both schemes. Our proposed scheme is quite immune to the variation of feedback interval, input
noise, and feedback noise respectively, which validates the robustness of the proposed scheme.
In the future, we will investigate joint power control and spectrum sharing issue in this scenario.
R EFERENCES
[1] H. Seo, K. Lee, S. Yasukawa, Y. Peng, and P. Sartori, “LTE evolution for vehicle-to-everything services,” IEEE Commun.
Mag., vol. 54, no. 6, pp. 22–28, Jun. 2016.
[2] S. Chen, J. Hu, Y. Shi, Y. Peng, J. Fang, R. Zhao, and L. Zhao, “Vehicle-to-everything (V2X) services supported by
LTE-based systems and 5G,” IEEE Commun. Standards Mag., vol. 1, no. 2, pp. 70–76, 2017.
[3] L. Liang, H. Peng, G. Y. Li, and X. Shen, “Vehicular communications: A physical layer perspective,” IEEE Trans. Veh.
Technol., vol. 66, no. 12, pp. 10 647–10 659, Dec. 2017.
[4] H. Peng and L. Liang and X. Shen and G. Y. Li, “Vehicular communications: A network layer perspective,” IEEE Trans.
Veh. Technol., vol. 68, no. 2, pp. 1064–1078, Feb. 2019.
[5] 3rd Generation Partnership Project, “Technical spefication group radio access network: Study on LTE-based V2X services,”
3GPP, TR 36.885 V14.0.0, Jun. 2016.
[6] ——, “Study on enhancement of 3GPP support for 5G V2X services,” 3GPP, TR 22.886 V15.1.0, Mar. 2017.
[7] C. Guo, L. Liang, and G. Y. Li, “Resource allocation for low-latency vehicular communications: An effective capacity
perspective,” IEEE J. Sel. Areas Commun., vol. 37, no. 4, pp. 905–917, Apr. 2019.
[8] L. Liang, G. Y. Li, and W. Xu, “Resource allocation for D2D-enabled vehicular communications,” IEEE Trans. Commun.,
vol. 65, no. 7, pp. 3186–3197, Jul. 2017.
[9] L. Liang, S. Xie, G. Y. Li, Z. Ding, and X. Yu, “Graph-based resource sharing in vehicular communication,” IEEE Trans.
Wireless Commun., vol. 17, no. 7, pp. 4579–4592, Jul. 2018.
[10] C. Chen, B. Wang, and R. Zhang, “Interference hypergraph-based resource allocation (IHG-RA) for NOMA-integrated
V2X networks,” IEEE Internet Things J., vol. 6, no. 1, pp. 161–170, Feb. 2019.
[11] C. Han, M. Dianati, Y. Cao, F. Mccullough, and A. Mouzakitis, “Adaptive network segmentation and channel allocation
in large-scale V2X communication networks,” IEEE Trans. Commun., vol. 67, no. 1, pp. 405–416, Jan. 2019.
[12] B. Bai, W. Chen, K. B. Letaief, and Z. Cao, “Low complexity outage optimal distributed channel allocation for vehicleto-vehicle communications,” IEEE J. Sel. Areas Commun., vol. 29, no. 1, pp. 161–172, Jan. 2011.
[13] M. I. Ashraf, M. Bennis, C. Perfecto, and W. Saad, “Dynamic proximity-aware resource allocation in vehicle-to-vehicle
(V2V) communications,” in Proc. IEEE Globecom Workshops (GC Wkshps), Dec. 2016, pp. 1–6.
[14] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou,
V. Panneershelvam, M. Lanctot et al., “Mastering the game of Go with deep neural networks and tree search,” Nature,
vol. 529, no. 7587, p. 484, 2016.
[15] T. O’Shea and J. Hoydis, “An introduction to deep learning for the physical layer,” IEEE Trans. Cogn. Commun. Netw.,
vol. 3, no. 4, pp. 563–575, Dec. 2017.
[16] Z. Qin, H. Ye, G. Y. Li, and B. F. Juang, “Deep learning in physical layer communications,” IEEE Wireless Commun.,
vol. 26, no. 2, pp. 93–99, Apr. 2019.
[17] H. Ye, G. Y. Li, and B. Juang, “Power of deep learning for channel estimation and signal detection in OFDM systems,”
IEEE Wireless Commun. Lett., vol. 7, no. 1, pp. 114–117, Feb. 2018.

27

[18] F. A. Aoudia and J. Hoydis, “End-to-end learning of communications systems without a channel model,” arXiv preprint
arXiv:1804.02276, 2018.
[19] C. Jiang, H. Zhang, Y. Ren, Z. Han, K. Chen, and L. Hanzo, “Machine learning paradigms for next-generation wireless
networks,” IEEE Wireless Commun., vol. 24, no. 2, pp. 98–105, Apr. 2017.
[20] R. Li, Z. Zhao, X. Zhou, G. Ding, Y. Chen, Z. Wang, and H. Zhang, “Intelligent 5G: When cellular networks meet artificial
intelligence,” IEEE Wireless Commun., vol. 24, no. 5, pp. 175–183, Oct. 2017.
[21] S. Wang, H. Liu, P. H. Gomes, and B. Krishnamachari, “Deep reinforcement learning for dynamic multichannel access in
wireless networks,” IEEE Trans. Cogn. Commun. Netw., vol. 4, no. 2, pp. 257–265, Jun. 2018.
[22] Y. Sun, M. Peng, and S. Mao, “Deep reinforcement learning based mode selection and resource management for green
fog radio access networks,” IEEE Internet Things J., vol. 6, no. 2, pp. 1960–1971, Apr. 2019.
[23] L. Liang, H. Ye, G. Yu, and G. Y. Li, “Deep learning based wireless resource allocation with application to vehicular
networks,” arXiv preprint arXiv:1907.03289, 2019.
[24] H. Ye, L. Liang, G. Y. Li, J. Kim, L. Lu, and M. Wu, “Machine learning for vehicular networks: Recent advances and
application examples,” IEEE Veh. Technol. Mag., vol. 13, no. 2, pp. 94–101, Jun. 2018.
[25] L. Liang, H. Ye, and G. Y. Li, “Toward intelligent vehicular networks: A machine learning framework,” IEEE Internet
Things J., vol. 6, no. 1, pp. 124–135, Feb. 2019.
[26] H. Ye, G. Y. Li, and B. F. Juang, “Deep reinforcement learning based resource allocation for V2V communications,” IEEE
Trans. Veh. Technol., vol. 68, no. 4, pp. 3163–3173, Apr. 2019.
[27] L. Liang, H. Ye, and G. Y. Li, “Spectrum sharing in vehicular networks based on multi-agent reinforcement learning,” to
appear in IEEE J. Sel. Areas Commun., 2019.
[28] Y. Wang, K. Wang, H. Huang, T. Miyazaki, and S. Guo, “Traffic and computation co-offloading with reinforcement learning
in fog computing for industrial applications,” IEEE Trans. Ind. Informat., vol. 15, no. 2, pp. 976–986, Feb. 2019.
[29] Y. S. Nasir and D. Guo, “Deep reinforcement learning for distributed dynamic power allocation in wireless networks,”
arXiv preprint arXiv:1808.00490, 2018.
[30] C. J. Watkins and P. Dayan, “Q-learning,” Machine Learning, vol. 8, no. 3-4, pp. 279–292, Feb. 1992.
[31] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction.

Cambridge, MA, USA: MIT Press, 2018.

[32] V. Mnih et al., “Human-level control through deep reinforcement learning,” Nature, vol. 518, no. 7540, pp. 529–533, Feb.
2015.
[33] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with double Q-learning,” in Proc. 30th AAAI Conf.,
Feb. 2016, pp. 2094–2100.
[34] G. Toderici, S. M. O’Malley, S. J. Hwang, D. Vincent, D. Minnen, S. Baluja, M. Covell, and R. Sukthankar, “Variable
rate image compression with recurrent neural networks,” arXiv preprint arXiv:1511.06085, 2015.
[35] Y. Bengio, N. Léonard, and A. Courville, “Estimating or propagating gradients through stochastic neurons for conditional
computation,” arXiv preprint arXiv:1308.3432, 2013.
[36] Y. Bultitude and T. Rautiainen, “IST-4-027756 WINNER II d1. 1.2 v1. 2 WINNER II channel models.”
[37] S. Ruder, “An overview of gradient descent optimization algorithms,” arXiv preprint arXiv:1609.04747, 2016.
[38] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction.
New York, NY, USA: Springer Science & Business Media, 2009.

