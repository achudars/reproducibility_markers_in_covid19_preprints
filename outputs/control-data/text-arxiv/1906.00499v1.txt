Budgeted Policy Learning for Task-Oriented Dialogue Systems
Zhirui Zhang‚Ä† Xiujun Li‚Ä°¬ß Jianfeng Gao‚Ä° Enhong Chen‚Ä†
‚Ä†
University of Science and Technology of China
‚Ä°
¬ß
Microsoft Research AI
University of Washington
‚Ä†
‚Ä†
zrustc11@gmail.com
cheneh@ustc.edu.cn
‚Ä°
{xiul,jfgao}@microsoft.com

arXiv:1906.00499v1 [cs.CL] 2 Jun 2019

Abstract
This paper presents a new approach that extends Deep Dyna-Q (DDQ) by incorporating
a Budget-Conscious Scheduling (BCS) to best
utilize a fixed, small amount of user interactions (budget) for learning task-oriented dialogue agents. BCS consists of (1) a Poissonbased global scheduler to allocate budget over
different stages of training; (2) a controller to
decide at each training step whether the agent
is trained using real or simulated experiences;
(3) a user goal sampling module to generate
the experiences that are most effective for policy learning. Experiments on a movie-ticket
booking task with simulated and real users
show that our approach leads to significant improvements in success rate over the state-ofthe-art baselines given the fixed budget.

1

Introduction

There has been a growing interest in exploiting
reinforcement learning (RL) for dialogue policy
learning in task-oriented dialogue systems (Levin
et al., 1997; Williams, 2008; Young et al., 2013;
Fatemi et al., 2016; Zhao and EskeÃÅnazi, 2016; Su
et al., 2016; Li et al., 2017; Williams et al., 2017;
Dhingra et al., 2017; Budzianowski et al., 2017;
Chang et al., 2017; Liu and Lane, 2017; Liu et al.,
2018; Gao et al., 2019). This is a challenging
machine learning task because an RL learner requires real users to interact with a dialogue agent
constantly to provide feedback. The process incurs significant real-world cost for complex tasks,
such as movie-ticket booking and travel planning,
which require exploration in a large state-action
space.
In reality, we often need to develop a dialogue
agent with some fixed, limited budget due to limited project funding, conversational data, and development time. Specifically, in this study we
measure budget in terms of the number of real user

interactions. That is, we strive to optimize a dialogue agent via a fixed, small number of interactions with real users.
One common strategy is to leverage a user simulator built on human conversational data (Schatzmann et al., 2007; Li et al., 2016). However, due
to design bias and the limited amounts of publicly available human conversational data for training the simulator, there always exists discrepancies between the behaviors of real and simulated
users, which inevitably leads to a sub-optimal dialogue policy. Another strategy is to integrate planning into dialogue policy learning, as the Deep
Dyna-Q (DDQ) framework (Peng et al., 2018),
which effectively leverages a small number of real
experiences to learn a dialogue policy efficiently.
In DDQ, the limited amounts of real user experiences are utilized for: (1) training a world model to
mimic real user behaviors and generate simulated
experiences; and (2) improving the dialogue policy using both real experiences via direct RL and
simulated experiences via indirect RL (planning).
Recently, some DDQ variants further incorporate
discriminators (Su et al., 2018) and active learning (Wu et al., 2019) into planning to obtain highquality simulated experiences.
DDQ and its variants face two challenges in the
fixed-budget setting. First, DDQ lacks any explicit
guidance on how to generate highly effective real
dialogue experiences. For example, the experiences in the state-action space that has not, or less,
been explored by the dialogue agent are usually
more desirable. Second, DDQ lacks a mechanism
of letting a human (teacher) play the role of the
agent to explicitly demonstrate how to drive the
dialogue (Barlier et al., 2018). This is useful in the
cases where the dialogue agent fails to respond to
users in conversations and the sparse negative rewards fail to help the agent improve its dialogue
policy. To this end, DDQ needs to be equipped

Direct Reinforcement Learning
Budget

Dialogue
Agent

Human
Conversational Data

Scheduler

Acting

Controller

BCS

User Goal
Sampling Module

User

Real
Experience

World
Model

Simulated
Experience

Planning

Figure 1: Proposed BCS-DDQ framework for dialogue policy learning. BCS represents the Budget-Conscious
Scheduling module, which consists of a scheduler, a controller and a user goal sampling module.

with the ability to decide whether to learn from
human demonstrations or from agent-user interactions where the user can be a real user or simulated
by the world model.
In this paper, we propose a new framework,
called Budget-Conscious Scheduling-based Deep
Dyna-Q (BCS-DDQ), to best utilize a fixed, small
number of human interactions (budget) for taskoriented dialogue policy learning. Our new framework extends DDQ by incorporating BudgetConscious Scheduling (BCS), which aims to control the budget and improve DDQ‚Äôs sample efficiency by leveraging active learning and human
teaching to handle the aforementioned issues. As
shown in Figure 1, the BCS module consists of
(1) a Poisson-based global scheduler to allocate
budget over the different stages of training; (2)
a user goal sampling module to select previously
failed or unexplored user goals to generate experiences that are effective for dialogue policy learning; (3) a controller which decides (based on the
pre-allocated budget and the agent‚Äôs performance
on the sampled user goals) whether to collect
human-human conversation, or to conduct humanagent interactions to obtain high-quality real experiences, or to generate simulated experiences
through interaction with the world model. During
dialogue policy learning, real experiences are used
to train the world model via supervised learning
(world model learning) and directly improve the
dialogue policy via direct RL, while simulated experiences are used to enhance the dialogue policy
via indirect RL (planning).
Experiments on the movie-ticket booking task
with simulated and real users show that our ap-

proach leads to significant improvements in success rate over the state-of-the-art baselines given a
fixed budget. Our main contributions are two-fold:
‚Ä¢ We propose a BCS-DDQ framework, to best utilize a fixed, small amount of user interactions
(budget) for task-oriented dialogue policy learning.
‚Ä¢ We empirically validate the effectiveness of
BCS-DDQ on a movie-ticket booking domain
with simulated and real users.

2

Budget-Conscious Scheduling-based
Deep Dyna-Q (BCS-DDQ)

As illustrated in Figure 2, the BCS-DDQ dialogue system consists of six modules: (1)
an LSTM-based natural language understanding
(NLU) module (Hakkani-TuÃàr et al., 2016) for
identifying user intents and extracting associated
slots; (2) a state tracker (Mrksic et al., 2017) for
tracking dialogue state; (3) a dialogue policy that
chooses the next action based on the current state
and database results; (4) a model-based natural
language generation (NLG) module for producing
a natural language response (Wen et al., 2015);
(5) a world model for generating simulated user
actions and simulated rewards; and (6) the BCS
module incorporating a global scheduler, a user
goal sampling module and a controller, to manage the budget and select the most effective way to
generate real or simulated experiences for learning
a dialogue policy.
To leverage BCS in dialogue policy learning,
we design a new iterative training algorithm,
called BCS-DDQ, as summarized in Algorithm 1.
It starts with an initial dialogue policy and an ini-

Dialogue Manager
2

1

Natural Language
Understanding (NLU)

Semantic Frame

Dialogue State Tracker
ùëú1
ùëú2
ùëúùë°

ùë†ùë°

5

User

World Model

State Representation

Human-Human

6

Backend
Database

Controller
User Goal
Sampling Module
Budget-Conscious
Scheduling (BCS)

Scheduler
Budget
3 Dialogue Policy Learning

ùë†2

ùë†1

‚Ä¶‚Ä¶

ùë†ùëõ

4

Natural Language
Generation (NLG)

System Action (Policy)

ùëé1

ùëé2

‚Ä¶

ùëéùëò

ùëé‚àó = max ùúã ùëé|ùë†
ùëé

Figure 2: Illustration of the proposed BCS-DDQ dialogue system.

tial world model, both trained with pre-collected
human conversational data. Given the total budget b and maximum number of training epochs
m, the scheduler allocates the available budget bk
at each training step. Then, the user goal sampling module actively selects a previously failed
or unexplored user goal gu . Based on the agent‚Äôs
performance and the current pre-allocated budget, the controller chooses the most effective way,
with cost cu = {0, 1, 2}, to generate real or simulated experiences B r /B s for this sampled user
goal. For convenience, the cost of different dialogue generation methods is defined as the number
of people involved:
‚Ä¢ cost cu = 2 for collecting human-human demonstrated conversational data.
‚Ä¢ cost cu = 1 for conducting the interactions between human and agent.
‚Ä¢ cost cu = 0 for performing the interactions between world model and agent.
The generated experiences are used to update the
dialogue policy and the world model. This process continues until all pre-allocated budget is exhausted. In the rest of this section, we detail
the components of BCS, and describe the learning methods of the dialogue agent and the world
model.
2.1

Budget-Conscious Scheduling (BCS)

As illustrated in Figure 2 and Algorithm 1, BSC
consists of a budget allocation algorithm for the
scheduler, an active sampling strategy for the user

Algorithm 1 BCS-DDQ for Dialogue Policy
Learning
Input: The total budget b, the maximum number
of training epochs m, the dialogue agent A and
the world model W (both pre-trained with precollected human conversational data);
1: procedure TRAINING PROCESS
2:
while k < m do
3:
bk ‚Üê Scheduler(b, m, k);
4:
repeat
5:
g u ‚Üê UserGoalSampler(A);
6:
B r , B s , cu ‚Üê Controller(g u , bk ,
A, W );
7:
bk ‚Üê bk ‚àí cu ;
8:
until bk ‚â§ 0
9:
Train the dialogue agent A on B r ‚à™B s
10:
Train world model W on B r
11:
end while
12: end procedure

goal sampling module, and a selection policy for
the controller.
2.1.1 Poisson-based Budget Allocation
The global scheduler is designed to allocate budget {b1 , . . . , bm } (where m is the final training
epoch) during training. The budget allocation process can be viewed as a series of random events,
where the allocated budget is a random variable.
In this manner, the whole allocation process essentially is a discrete stochastic process, which can
be modeled as a Poisson process. Specifically, at
each training step k, the probability distribution of
a random variable bk equaling n is given by:
P {bk = n} =

Œªnk ‚àíŒªk
m+1‚àík
e
, Œªk =
Œª (1)
n!
m

The global scheduling in BCS is based on a Decayed Possion Process, motivated by two considerations: (1) For simplicity, we assume that all
budget allocations are mutually-independent. The
Poisson process is suitable for this assumption.
(2) As the training process progresses, the dialogue agent tends to produce higher-quality dialogue experiences using the world model due to
the improving performance of both the agent and
the world model. As a result, the budget demand
for the agent decays during the course of training.
Thus, we linearly decay the parameter of the Poisson distribution so as to allocate more budget at
the beginning of training.

In addition, to ensure that the sum of the allocated budget does not exceed the total budget b,
we impose the following constraint:
m
X
k=1

E[bk ] =

m
X
m+1‚àík
k=1

m

Œª‚â§b

(2)

Using this formula, we can calculate the range of
2b
the parameter value: Œª ‚â§ m+1
. In our experi2b
ments, we set Œª = m+1 and sample bk from the
probability distribution defined in Equation 1.
2.1.2 Active Sampling Strategy
The active sampling strategy involves the definition of a user goal space and sampling algorithm.
In a typical task-oriented dialogue (Schatzmann
et al., 2007), the user begins a conversation with
a user goal g u which consists of multiple constraints. In fact, these constraints correspond to
attributes in the knowledge base. For example, in
the movie-ticket-booking scenario, the constraints
may be the name of the theater (theater), the
number of tickets to buy (numberofpeople) or
the name of the movie (moviename), and so on.
Given the knowledge base, we can generate large
amounts of user goals by traversing the combination of all the attributes, and then filtering unreasonable user goals which are not similar to real
user goals collected from human-human conversational data. We then group the user goals with
the same inform and request slots into a category.
Suppose there are altogether l different categories
of user goals. We design a Thompson-Samplinglike algorithm (Chapelle and Li, 2011; Eckles and
Kaptein, 2014; Russo et al., 2018) to actively select a previously failed or unexplored user goal in
two steps.
‚Ä¢ Draw a numberq
pi for each category follow-

ing pi ‚àº N (fi , l lnniN ), where N represents
the Gaussian distribution, fi denotes the failure rate of each category estimated on the validation set, ni is the number
P of samples for
each category and N = i ni .
‚Ä¢ Select the category with maximum pi , then
randomly sample a user goal gu in the category.
Using this method, user goals in the categories
with higher failure rates or less exploration are
more likely to be selected during training, which
encourages the real or simulated user to generate
dialogue experiences in the state-action space that
the agent has not fully explored.

2.1.3 Controller
Given a sampled user goal g u , based on the agent‚Äôs
performance on g u and pre-allocated budget bk ,
the controller decides whether to collect humanhuman dialogues, human-agent dialogues, or simulated dialogues between the agent and the world
model. We design a heuristic selection policy of
Equation 3 where dialogue experiences B are collected as follow: we first generate a set of simulated dialogues B s given g u , and record the success rate Sgu . If Sgu is higher than a threshold Œª1
(i.e. Œª1 = 2/3) or there is no budget left, we use
B s for training. If Sgu is lower than a threshold
Œª2 (i.e. Œª2 = 1/3) and there is still budget, we
resort to human agents and real users to generate
r . Otherwise, we collect real
real experiences Bhh
experiences generated by human users and the dir .
alogue agent Bha
Ô£±
if Sgu ‚â• Œª1 or bk = 0
Ô£≤ (B s , 0)
u
r , 2)
if Sgu ‚â§ Œª2 and bk ‚â• 2
(Bhh
(B, c ) =
Ô£≥
r , 1)
otherwise
(Bha
(3)
Combined with the active sampling strategy,
this selection policy makes fuller use of the budget to generate experiences that are most effective
for dialogue policy learning.
2.2

Direct Reinforcement Learning and
Planning

Policy learning in task-oriented dialogue using RL
can be cast as a Markov Decision Process which
consists of a sequence of <state, action, reward>
tuples. We can use the same Q-learning algorithm
to train the dialogue agent using either real or simulated experiences. Here we employ the Deep Qnetwork (DQN) (Mnih et al., 2015).
Specifically, at each step, the agent observes the
dialogue state s, then chooses an action a using an
-greedy policy that selects a random action with
probability , and otherwise follows the greedy
policy a = arg maxa0 Q(s, a0 ; Œ∏Q ). Q(s, a; Œ∏Q )
approximates the state-action value function with
a Multi-Layer Perceptron (MLP) parameterized by
Œ∏Q . Afterwards, the agent receives reward r, observes the next user or simulator response, and updates the state to s0 . The experience (s, a, r, au , s0 )
is then stored in a real experience buffer B r1 or
simulated experience buffer B s depending on the
source. Given these experiences, we optimize the
1

r
r
B r = {Bhh
, Bha
}

value function Q(s, a; Œ∏Q ) through mean-squared
loss:

Intent

L(Œ∏Q ) = E(s,a,r,s0 )‚àºB r ‚à™B s [(y ‚àí Q(s, a; Œ∏Q ))2 ]
Slot

y = r + Œ≥ max
Q0 (s0 , a0 ; Œ∏Q0 )
0
a

(4)
where Œ≥ ‚àà [0, 1] is a discount factor, and
is
the target value function that is updated only periodically (i.e., fixed-target). The updating of Q(¬∑)
thus is conducted through differentiating this objective function via mini-batch gradient descent.
Q0 (¬∑)

2.3

World Model Learning

We utilize the same design of the world model
in Peng et al. (2018), which is implemented as a
multi-task deep neural network. At each turn of
a dialogue, the world model takes the current dialogue state s and the last system action a from
the agent as input, and generates the corresponding user response au , reward r, and a binary termination signal t. The computation for each term
can be shown as below:
h = tanh(Wh [s, a] + bh )
r = Wr h + br
u

a

= softmax(Wa h + ba )

(5)

t = sigmoid(Wt h + bt )
where all W and b are weight matrices and bias
vectors respectively.

3

Experiments

We evaluate BCS-DDQ on a movie-ticket booking
task in three settings: simulation, human evaluation and human-in-the-loop training. All the experiments are conducted on the text level.
3.1

Setup

Dataset. The dialogue dataset used in this study
is a subset of the movie-ticket booking dialogue
dataset released in Microsoft Dialogue Challenge (Li et al., 2018). Our dataset consists of
280 dialogues, which have been manually labeled
based on the schema defined by domain experts, as
in Table 1. The average length of these dialogues
is 11 turns.
Dialogue Agents. We benchmark the BCSDDQ agent with several baseline agents:
‚Ä¢ The SL agent is learned by a variant of imitation learning (Lipton et al., 2018). At the beginning of training, the entire budget is used to col-

request, inform, deny, confirm question,
confirm answer, greeting, closing, not sure,
multiple choice, thanks, welcome
city, closing, date, distanceconstraints,
greeting, moviename, numberofpeople,
price, starttime, state, taskcomplete, theater,
theater chain, ticket, video format, zip

Table 1: The dialogue annotation schema

lect human-human dialogues, based on which
the dialogue agent is trained.
‚Ä¢ The DQN agent is learned by standard DQN
At each epoch of training, the budget is spent
on human-agent interactions, and the agent is
trained by direct RL.
‚Ä¢ The DDQ agent is learned by the DDQ method
(Peng et al., 2018). The training process is similar to that of the DQN agent, differing in that
DDQ integrates a jointly-trained world model
to generate simulated experience which can further improve the dialogue policy. At each epoch
of training, the budget is spent on human-agent
interactions.
‚Ä¢ The BCS-DDQ agent is learned by the proposed
BCS-DDQ approach. For a fair comparison, we
use the same number of training epochs m used
for the DQN and DDQ agents.
Hyper-parameter Settings. We use an MLP to
parameterize function Q(¬∑) in all the dialogue
agents (SL, DQN, DDQ and BCS-DDQ), with
hidden layer size set to 80. The -greedy policy is adopted for exploration. We set discount
factor Œ≥ = 0.9. The target value function Q0 (¬∑)
is updated at the end of each epoch. The world
model contains one shared hidden layer and three
task-specific hidden layers, all of size 80. The
number of planning steps is set to 5 for using
the world model to improve the agent‚Äôs policy in
DDQ and BCS-DDQ frameworks. Each dialogue
is allowed a maximum of 40 turns, and dialogues
exceeding this maximum are considered failures.
Other parameters used in BCS-DDQ are set as
l = 128, d = 10.
Training Details. The parameters of all neural networks are initialized using a normal distribution
with a mean of 0 and a variance of
p
6/(drow + dcol ), where drow and dcol are the
number of rows and columns in the structure (Glorot and Bengio, 2010). All models are optimized
by RMSProp (Tieleman and Hinton, 2012). The
mini-batch size is set to 16 and the initial learn-

90

SL

DQN

DDQ

BCS-DDQ

90

  

70

  

60

Success Rate(%)

80

50

  

40

  

 6 X F F H V V  5 D W H   

70

Success Rate(%)

80

30

60

20

50
10

40

  
  
  

0
0

50
  

30

  

20

 

 6 /
 ' 4 1
 ' ' 4
 % & 6  ' ' 4

100

 

  

   

   

 6 L P X O D W L R Q  ( S R F K

   

   

   

10
50

100

150

200

250

300

Budget
90

Figure 3: The success rates of different agents (SL,
DQN, DDQ, BCS-DDQ) given a fixed budget (b =
{50, 100, 150, 200, 250, 300}). Each number is averaged over 5 runs, each run tested on 50 dialogues.

ing rate is 5e-4. The buffer sizes of B r and B s
are set to 3000. In order to train the agents more
efficiently, we utilized a variant of imitation learning, Reply Buffer Spiking (Lipton et al., 2018), to
pre-train all agent variants at the starting stage.
3.2

Simulation Evaluation

In this setting, the dialogue agents are trained and
evaluated by interacting with the user simulators
(Li et al., 2016) instead of real users. In spite of
the discrepancy between simulated and real users,
this setting enables us to perform a detailed analysis of all agents without any real-world cost. During training, the simulator provides a simulated
user response on each turn and a reward signal
at the end of the dialogue. The dialogue is considered successful if and only if a movie ticket is
booked successfully and the information provided
by the agent satisfies all the user‚Äôs constraints (user
goal). When the dialogue is completed, the agent
receives a positive reward of 2 ‚àó L for success, or
a negative reward of ‚àíL for failure, where L is
the maximum number of turns allowed (40). To
encourage shorter dialogues, the agent receives a
reward of ‚àí1 on each turn.
In addition to the user simulator, the training of SL and BCS-DDQ agents requires a highperformance dialogue agent to play the role of the
human agent in collecting human-human conversational data. In the simulation setting, we leverage a well-trained DQN agent as the human agent.

SL

Figure
4: The
learning curves of different agents
DQN
DDQ(5)
Our method
(DQN, DDQ and BCS-DDQ) with budget b = 300.

Main Results. We evaluate the performance of
all agents (SL, DQN, DDQ, BCS-DDQ) given a
fixed budget (b = {50, 100, 150, 200, 250, 300}).
As shown in Figure 3, the BCS-DDQ agent consistently outperforms other baseline agents by a
statistically significant margin. Specifically, when
the budget is small (b = 50), SL does better than
DQN and DDQ that haven‚Äôt been trained long
enough to obtain significant positive reward. BCSDDQ leverages human demonstrations to explicitly guide the agent‚Äôs learning when the agent‚Äôs
performance is very bad. In this way, BCS-DDQ
not only takes advantage of imitation learning,
but also further improves the performance via exploration and RL. As the budget increases, DDQ
can leverage real experiences to learn a good policy. Our method achieves better performance than
DDQ, demonstrating that the BCS module can
better utilize the budget by directing exploration to
parts of the state-action space that have been less
explored.

Learning Curves. We also investigate the training process of different agents. Figure 4 shows
the learning curves of different agents with a fixed
budget (b = 300). At the beginning of training,
similar to a very small budget situation, the performance of the BCS-DDQ agent improves faster
thanks to its combination of imitation learning
and reinforcement learning. After that, BCS-DDQ
consistently outperforms DQN and DDQ as training progresses. This proves that the BCS module
can generate higher quality dialogue experiences
for training dialogue policy.

Agent
DQN
DDQ
BCS-DDQ

Success
0.3032
0.4204
0.7542

Epoch=100
Reward
-18.77
-2.24
43.80

Turns
32.31
27.34
15.42

Success
0.4675
0.5467
0.7870

Epoch=150
Reward
2.07
15.46
47.38

Turns
30.07
22.26
16.13

Success
0.5401
0.6694
0.7629

Epoch=200
Reward
18.94
32.00
44.45

Turns
26.59
18.66
16.20

Success Rate

Table 2: The performance of different agents at training epoch = {100, 150, 200} in the human-in-the-loop experiments. The differences between the results of all agent pairs evaluated at the same epoch are statistically significant
(p < 0.05). (Success: success rate)

90

0.4

80

0.3

  

0

72
SL

34.39

78
74

30

  
  
  
  
  

70

 
0.8

0
DQN

DDQ

Success Rate

SL

0.7

 ' 4 1
 ' ' 4
 % & 6  ' ' 4

  

0.9

10

 

  

   

 ( S R F K

   

   

BCS-DDQ

0.6

Figure 5: The human evaluation results for SL, DQN,
DDQ and BCS-DDQ agents, the number of test dialogues indicated on each bar, and the p-values from a
two-sided permutation test. The differences between
the results of all agent pairs are statistically significant
(p < 0.05).

3.3

 6 X F F H V V  5 D W H   

0.1

54.05

50

20

  

0.2

64.12

60

40

  
81.94

70

Success Rate(%)

0.5

p=0.013

Human Evaluation

For human evaluation, real users interact with different agents without knowing which agent is behind the system. At the beginning of each dialogue session, we randomly pick one agent to converse with the user. The user is provided with a
randomly-sampled user goal, and the dialogue session can be terminated at any time, if the user believes that the dialogue is unlikely to succeed, or
if it lasts too long. In either case, the dialogue is
considered as failure. At the end of each dialogue,
the user is asked to give explicit feedback about
whether the conversation is successful.
Four agents (SL, DQN, DDQ and BCS-DDQ)
trained in simulation (with b = 300) are selected
for human evaluation. As illustrated in Figure 5,
the results are consistent with those in the simulation evaluation (the rightmost group with budget=300 in Figure 3). In addition, due to the discrepancy between simulated users and real users,
the success rates of all agents drop compared to the
simulation evaluation, but the performance degra-

Figure 6: Human-in-the-Loop learning curves of different agents with budget b = 200.

dation of BCS-DDQ is minimal. This indicates
that our approach is more robust and effective than
the others.
3.4

Human-in-the-Loop Training

We further verify the effectiveness of our method
in human-in-the-loop training experiments. In this
experiment, we replace the user simulator with
real users during training. Similar to the human
evaluation, based on a randomly-sampled user
goal, the user converses with a randomly-selected
agent and gives feedback as to whether the conversation is successful. In order to collect humanhuman conversations during the training of the
BCS-DDQ agent, human agents are interacting directly with real users through the dialogue system.
In a dialogue session, the human agent has access
to the conversation history, as well as the current
search results from the knowledge base, before selecting each dialogue action in response to the real
user. Each learning curve is trained with two runs,
with each run assigning a budget of 200 human
interactions.
The main results are presented in Table 2 and
Figure 6. We can see that the BCS-DDQ agent
consistently outperforms DQN and DDQ during
the course of training, confirming the conclusion

Table 3: Sample dialogue sessions by DDQ and BCSDDQ agents trained at epoch 200 (with total budget
b = 200) in the human-in-the-loop experiments: (agt:
agent, usr: user)

drawn from the simulation evaluation. Besides,
Table 3 shows example dialogues produced by two
dialogue agents (DDQ and BCS-DDQ) interacting
with human users respectively. We can see that
DDQ agent fails to respond to the user question
‚Äúwhich theater is available?‚Äù, which lead to the repeated inquiry of theater information. By introducing human demonstrations for agent training,

  
  
  

 6 X F F H V V  5 D W H   

Real User Dialogue Sample
movie-ticket booking user goal:
{
‚Äúconstraint slots‚Äù: {
‚Äúrequest slots‚Äù: {
‚Äúnumberofpeople‚Äù: ‚Äúfour‚Äù
‚Äúticket‚Äù:‚Äú?‚Äù
‚Äúmoviename‚Äù: ‚Äúcreed‚Äù
‚Äútheater‚Äù: ‚Äú?‚Äù
‚Äúcity‚Äù: ‚Äúregency‚Äù
}
‚Äúdate‚Äù: ‚Äútomorrow‚Äù
‚Äústarttime‚Äù: ‚Äúaround noon‚Äù
}
}
DDQ agent
1 usr: I want to watch creed at around noon.
2 agt: around noon is available.
3 usr: I want four tickets please!
4 agt: Which city would you like?
5 usr: I want to watch at regency.
6 agt: Which theater would you like?
7 usr: Which theater is available?
8 agt: What date would you like to watch it?
9 usr: I want to set it up tomorrow.
10 agt: Which theater would you like?
11 usr: Which theater is available?
12 agt: Which city would you like?
13 usr: I want to watch at regency.
14 agt: Which theater would you like?
15 usr: Which theater is available?
16 agt: What date would you like to watch it?
..
.
Agent does not book the right ticket within the
Failure:
max turns.
BCS-DDQ agent
1 usr: I want to watch creed at around noon.
2 agt: Which city would you like?
3 usr: I want to watch at regency.
4 agt: What date would you like to watch it?
5 usr: I want to set it up tomorrow.
6 agt: Which theater would you like?
7 usr: Which theater is available?
8 agt: century eastport 16 is available.
9 usr: Could you help me to book the tickets?
8 agt: How many tickets do you need?
9 usr: I want four tickets please!
Great - I was able to purchase 4 tickets for you
10 agt: to see creed tomorrow at century eastport 16
theater in regency at around noon.
11 usr: Thank you.
12 agt: Thank you.
Success

  
  
  
  
  

 % & 6  ' ' 4
 % & 6  ' ' 4  Y D U 
 % & 6  ' ' 4  Y D U 

  
 

 

  

   

   

 6 L P X O D W L R Q  ( S R F K

   

   

   

Figure 7: The learning curves of BCS-DDQ and its
variants agents with budget b = 300.

BCS-DDQ agent can successfully respond to the
available theater information.
3.5

Ablation Study

We investigate the relative contribution of the budget allocation algorithm and the active sampling
strategy in BCS-DDQ by implementing two variant BCS-DDQ agents:
‚Ä¢ The BCS-DDQ-var1 agent: Replacing the decayed Poisson process with a regular Poisson process in the budget allocation algorithm,
b
which means that the parameter Œª is set to m
during training.
‚Ä¢ The BCS-DDQ-var2 agent: Further replacing
the active sampling strategy with random sampling, based on the BCS-DDQ-var1 agent.
The results in Figure 7 shows that the budget allocation algorithm and active sampling strategy are
helpful for improving a dialogue policy in the limited budget setting. The active sampling strategy
is more important, without which the performance
drops significantly.

4

Conclusion

We presented a new framework BCS-DDQ for
task-oriented dialogue policy learning. Compared
to previous work, our approach can better utilize
the limited real user interactions in a more efficient
way in the fixed budget setting, and its effectiveness was demonstrated in the simulation evaluation, human evaluation, including human-in-theloop experiments.
In future, we plan to investigate the effectiveness of our method on more complex task-oriented
dialogue datasets. Another interesting direction

is to design a trainable budget scheduler. In this
paper, the budget scheduler was created independently of the dialogue policy training algorithm, so
a trainable budget scheduler may incur additional
cost. One possible solution is transfer learning, in
which we train the budget scheduler on some welldefined dialogue tasks, then leverage this scheduler to guide the policy learning on other complex
dialogue tasks.

5

Acknowledgments

Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural
networks. In AISTATS.
Dilek Z. Hakkani-TuÃàr, GoÃàkhan TuÃàr, Asli elikyilmaz,
Yun-Nung Chen, Jianfeng Gao, Li Deng, and YeYi Wang. 2016. Multi-domain joint semantic frame
parsing using bi-directional rnn-lstm. In INTERSPEECH.
Esther Levin, Roberto Pieraccini, and Wieland Eckert. 1997. Learning dialogue strategies within the
markov decision process framework. In ASRU 1997,
pages 72‚Äì79.

We appreciate Sungjin Lee, Jinchao Li, Jingjing
Liu, Xiaodong Liu, and Ricky Loynd for the fruitful discussions. We would like to thank the volunteers from Microsoft Research for helping us with
the human evaluation and the human-in-the-loop
experiments. We also thank the anonymous reviewers for their careful reading of our paper and
insightful comments. This work was done when
Zhirui Zhang was an intern at Microsoft Research.

Xiujun Li, Yun-Nung Chen, Lihong Li, and Jianfeng
Gao. 2017. End-to-end task-completion neural dialogue systems. In IJCNLP.

References

Zachary C Lipton, Jianfeng Gao, Lihong Li, Xiujun
Li, Faisal Ahmed, and Li Deng. 2018. Efficient exploration for dialogue policy learning with bbq networks & replay buffer spiking. AAAI.

Merwan Barlier, Romain Laroche, and Olivier
Pietquin. 2018. Training dialogue systems with human advice. In AAMAS.
Pawel Budzianowski, Stefan Ultes, Pei hao Su,
Nikola Mrksic, Tsung-Hsien Wen, InÃÉigo Casanueva,
Lina Maria Rojas-Barahona, and Milica Gasic.
2017. Sub-domain modelling for dialogue management with hierarchical reinforcement learning. In
SIGDIAL.
Cheng Chang, Runzhe Yang, Lu Chen, Xiang Zhou,
and Kai Yu. 2017. Affordable on-line dialogue policy learning. In EMNLP.
Olivier Chapelle and Lihong Li. 2011. An empirical
evaluation of thompson sampling. In NIPS.
Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng
Gao, Yun-Nung Chen, Faisal Ahmed, and Li Deng.
2017. End-to-end reinforcement learning of dialogue agents for information access. In ACL.
Dean Eckles and Maurits Kaptein. 2014. Thompson sampling with the online bootstrap. CoRR,
abs/1410.4009.
Mehdi Fatemi, Layla El Asri, Hannes Schulz, Jing He,
and Kaheer Suleman. 2016. Policy networks with
two-stage training for dialogue systems. In SIGDIAL Conference.
Jianfeng Gao, Michel Galley, and Lihong Li. 2019.
Neural approaches to conversational ai. Foundations and Trends R in Information Retrieval, 13(23):127‚Äì298.

Xiujun Li, Zachary C Lipton, Bhuwan Dhingra, Lihong
Li, Jianfeng Gao, and Yun-Nung Chen. 2016. A
user simulator for task-completion dialogues. arXiv
preprint arXiv:1612.05688.
Xiujun Li, Sarah Panda, Jingjing Liu, and Jianfeng
Gao. 2018. Microsoft dialogue challenge: Building
end-to-end task-completion dialogue systems. arXiv
preprint arXiv:1807.11125.

Bing Liu and Ian Lane. 2017. Iterative policy learning
in end-to-end trainable task-oriented neural dialog
models. ASRU, pages 482‚Äì489.
Bing Liu, GoÃàkhan TuÃàr, Dilek Z. Hakkani-TuÃàr, Pararth
Shah, and Larry P. Heck. 2018. Dialogue learning with human teaching and feedback in end-toend trainable task-oriented dialogue systems. In
NAACL-HLT.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A. Rusu, Joel Veness, Marc G. Bellemare,
Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King,
Dharshan Kumaran, Daan Wierstra, Shane Legg,
and Demis Hassabis. 2015. Human-level control through deep reinforcement learning. Nature,
518:529‚Äì533.
Nikola Mrksic, Diarmuid OÃÅ SeÃÅaghdha, Tsung-Hsien
Wen, Blaise Thomson, and Steve J. Young. 2017.
Neural belief tracker: Data-driven dialogue state
tracking. In ACL.
Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu,
and Kam-Fai Wong. 2018. Integrating planning for
task-completion dialogue policy learning. In ACL.
Daniel J Russo, Benjamin Van Roy, Abbas Kazerouni,
Ian Osband, Zheng Wen, et al. 2018. A tutorial on
thompson sampling. Foundations and Trends R in
Machine Learning, 11(1):1‚Äì96.

Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve J. Young. 2007. Agenda-based
user simulation for bootstrapping a pomdp dialogue
system. In HLT-NAACL.
Peihao Su, Milica Gasic, Nikola Mrksic, Lina Maria
Rojas-Barahona, Stefan Ultes, David Vandyke,
Tsung-Hsien Wen, and Steve J. Young. 2016. Continuously learning neural dialogue management.
CoRR, abs/1606.02689.
Shang-Yu Su, Xiujun Li, Jianfeng Gao, Jingjing Liu,
and Yun-Nung Chen. 2018. Discriminative deep
dyna-q: Robust planning for dialogue policy learning. In EMNLP.
Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture
6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural
networks for machine learning, 4(2):26‚Äì31.
Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei
hao Su, David Vandyke, and Steve J. Young. 2015.
Semantically conditioned lstm-based natural language generation for spoken dialogue systems. In
EMNLP.
Jason D Williams. 2008. The best of both worlds:
Unifying conventional dialog systems and pomdps.
In Ninth Annual Conference of the International
Speech Communication Association.
Jason D. Williams, Kavosh Asadi, and Geoffrey Zweig.
2017. Hybrid code networks: practical and efficient
end-to-end dialog control with supervised and reinforcement learning. In ACL.
Yuexin Wu, Xiujun Li, Jianfeng Gao, Jingjing Liu, and
Yiming Yang. 2019. Switch-based active deep dynaq: Efficient adaptive planning for task-completion
dialogue policy learning. In AAAI.
Steve J. Young, Milica Gasic, Blaise Thomson, and Jason D. Williams. 2013. Pomdp-based statistical spoken dialog systems: A review. Proceedings of the
IEEE, 101:1160‚Äì1179.
Tiancheng Zhao and Maxine EskeÃÅnazi. 2016. Towards
end-to-end learning for dialog state tracking and
management using deep reinforcement learning. In
SIGDIAL Conference.

