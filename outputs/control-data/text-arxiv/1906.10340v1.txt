Flows in Almost Linear Time via Adaptive Preconditioning

arXiv:1906.10340v1 [cs.DS] 25 Jun 2019

Rasmus Kyng∗†
Harvard

Richard Peng∗‡
Georgia Tech /
MSR Redmond

Sushant Sachdeva∗§
UToronto

Di Wang∗‡
Georgia Tech

June 26, 2019

Abstract
We present algorithms for solving a large class of flow and regression problems on unit
weighted graphs to (1 + 1/poly(n)) accuracy in almost-linear time. These problems include
ℓp -norm minimizing flow for p large (p ∈ [ω(1), o(log2/3 n)]), and their duals, ℓp -norm semisupervised learning for p close to 1.
As p tends to infinity, ℓp -norm flow and its dual tend to max-flow and min-cut respectively.
Using this connection and our algorithms, we give an alternate approach for approximating
undirected max-flow, and the first almost-linear time approximations of discretizations of total
variation minimization objectives.
This algorithm demonstrates that many tools previous viewed as limited to linear systems are
in fact applicable to a much wider range of convex objectives. It is based on the the routing-based
solver for Laplacian linear systems by Spielman and Teng (STOC ’04, SIMAX ’14), but require
several new tools: adaptive non-linear preconditioning, tree-routing based ultra-sparsification
for mixed ℓ2 and ℓp norm objectives, and decomposing graphs into uniform expanders.

∗

Emails: {rjkyng,richard.peng}@gmail.com, sachdeva@cs.toronto.edu, di.wang@cc.gatech.edu
Supported by ONR grant N00014-18-1-2562.
‡
Supported in part by the National Science Foundation under Grant No. 1718533.
§
Supported by the Natural Sciences and Engineering Research Council of Canada (NSERC), and a Connaught
New Researcher award.
†

Contents
1 Introduction
1.1 Main Results and Applications . . . . . . . . . . . . . .
1.1.1 ℓp -Norm Flows . . . . . . . . . . . . . . . . . . .
1.1.2 Semi-Supervised Learning on Graphs. . . . . . .
1.1.3 Use as Oracle in Conjunction with Multiplicative
1.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Overview . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4 Open Questions . . . . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.

1
2
2
3
3
4
5
6

.
.
.
.

7
7
7
8
9

3 Numerical Methods
3.1 Iterative Refinement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Vertex Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Recursive Preconditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10
10
12
13

4 Graph Theoretic Preconditioners
4.1 Tree-Portal Routing . . . . . . . . . . . . . . . . .
4.2 Partitioning Trees into Subtrees and Portals . . . .
4.3 Graph Sparsification . . . . . . . . . . . . . . . . .
4.4 Ultra-sparsification Algorithm and Error Analysis .

18
19
21
23
25

2 Preliminaries
2.1 Smoothed ℓp -norm functions . . . . . . . . .
2.2 Flow Problems and Approximation . . . . .
2.3 Approximating Smoothed p-norm Instances
2.4 Orthogonal Decompositions of Flows . . . .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

. . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . .
Weight Updates
. . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . .
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.

5 Decomposing into Uniform Expanders

35

Acknowledgements

38

A Deferred Proofs from Prelims, Section 2.2

43

B Deferred Proofs for Numerical Methods from Section 3

45

C Elimination of Low-Degree Vertices, and Loops

49

D Sparsifying Uniform Expanders

53

E Using Approximate Projections

63

F ℓp -norm Semi-Supervised Learning on Graphs.

64

1

Introduction

Graphs are among the most ubiquitous representations of data, and efficiently computing on graphs
is a task central to operations research, machine learning, and network science. Among graph
algorithms, network flows have been extensively studied [EK72, Kar73, ET75, GT88, GR98, Sch02,
Hoc08, CKM+ 11, HO13, Orl13, GT14], and have wide ranges of applications [KBR07, LSBG13,
PZZ13]. Over the past decade, the ‘Laplacian paradigm’ of designing graph algorithms spurred a
revolution in the best run-time upper bounds for many fundamental graph optimization problems.
Many of these new graph algorithms incorporated numerical primitives: even for the s-t shortest
path problem in graphs with negative edge weights, the current best running times [CMTV17] are
from invoking linear system solvers.
This incorporation of numerical routines [DS08, CKM+ 11] in turn led to a dependence on ǫ, the
approximation accuracy. While maximum flow and transshipment problems on undirected graphs
can now be approximated in nearly-linear time [KLOS14, She13, BKKL17, Pen16, She17b, She17a]
(and the distributed setting has also been studied [GKK+ 15, BKKL17]), these algorithms are
low accuracy in that their running times have factors of 1/ǫ or higher. This is in contrast to
high accuracy solvers for linear systems and convex programs, which with polylog(n) overhead
give 1/poly(n)-approximate solutions. Prior to our result, such high accuracy runtime bounds for
problems beyond linear systems all utilize second order methods [DS08, Mad13, LS14, CMTV17,
ALdOW17, BCLL18] from convex optimization.
The main contribution of this paper is giving almost-linear time, high accuracy solutions to
a significantly wider range of graph optimization problems that can be viewed as interpolations
between maximum flow, shortest paths, and graph-structured linear systems. Our unified formulation of these problems is based on the following unified formulation of flow/path problems as norm
minimization over a demand vector b ∈ RV≥0 :
min

flow f with residue b

kf k⊙ .

(1)

In particular, when k·k⊙ is the ℓ∞ -norm, this formulation is equivalent finding the flow of minimum
congestion, which is in turn equivalent to computing maximum flows and bipartite matchings in
unit capacitated graphs [Mad11]. Our main result is that for any p ≥ 2, given weights r ∈ RE
≥0 , a
⊤
E
V
“gradient” g ∈ R , and a demand vector b ∈ R (with b 1 = 0), we can solve
X
g e f e + r e f 2e + |f e |p ,
(2)
min
flow f with residue b
3/2

e

1+O( √1 )

p
. We will formally state this result as Theorem 1.1
to 1/poly(n) additive error in time 2O(p ) m
at the start of Section 1.1, and discuss several of its applications in flows, semi-supervised learning,
and total variation minimization.
We believe that our algorithm represents a new approach to designing high accuracy solvers
for graph-structured optimization problems. A brief survey of relevant works is in Section 1.2:
previous high accuracy algorithms treat linear systems as the separation between graph theoretic
and numerical components: the outer loop adjusts the numerics, while the inner loop quickly
solves the resulting linear systems using the underlying graph structures. Our result, in contrast,
directly invoke analogs of linear system solving primitives to the non-linear (but still convex)
objective functions, and no longer has this clear separation between graph theoretic and numerical
components.

1

We will overview key components of our approach, as well as how they are combined, in Section 1.3. Discussions of possible avenues for addressing shortcomings of our result, namely the
exponential dependence on p, the restriction to unweighted graphs, and gap between ℓ√log n -norm
flow and ℓ∞ are in Section 1.4.

1.1

Main Results and Applications

The formal formulation of our problem relies on the following objects defined on a graph G = (V, E)
with n vertices and m edges:
1. edge-vertex incidence matrix B ,
2. a vector b indicating the required residues on vertices (satisfying 1T b = 0), and
3. a norm p as finding a flow f with demands b that minimize a specified norm k · k.

The normed flow problem that we solve can then be formulated as:
X
min
g e f e + r e f 2e + |f e |p ,
B ⊤ f =b

Using kf k2,r =
k22,r

qP

2
e r ef e
kf kpp . Let

(2)

e

to denote the r -weighted 2-norm, the objective can also be viewed as

g ⊤ f + kf
+
val(f ) denote value of a flow f according to the above objective, and
let OPT denote value of the optimal solution to Problem (2). Our main technical result is the
following statement which we prove as corollary of our main technical theorem in Section 3.3.
Theorem 1.1 (Smoothed ℓp -norm flows). For any p ≥ 2, given weights r ∈ RE
≥0 , a “gradient” g ∈
⊤
(0)
E
V
R , a demand vector b ∈ R (with b 1 = 0), and an initial solution f
such that all parameters
poly(log
n)
e
are bounded by 2
, we can compute a flow f satisfying demands b, i.e., B G⊤ fe = b, such
that


1
1
val(f (0) ) − OPT +
val(fe) − OPT ≤
poly(m)
poly(m)
in 2O(p
1.1.1

3/2
)

m

1+O( √1p )

time, where m denotes the number of edges in G.

ℓp -Norm Flows

From this, we also get a (slightly simpler) statement about ℓp -norm flows.
Theorem 1.2 (ℓp -norm flows). For any p ≥ 2, given an unweighted graph G(V, E) and demands
b, using the routine pFlows(G, b) (Algorithm 2) we can compute a flow fe satisfying satisfying b,
i.e., B G⊤ fe = b, such that


p
1
e
min kf kpp .
f ≤ 1+
poly(m) f :B G⊤ f =b
p

in 2O(p

3/2
)

m

1+O( √1p )

, time, where m denotes the number of edges in G.

This corollary is also proven in Section 3.3.
3/2

1+O( √1 )

p
Picking g , r = 0 gives us an 2O(p ) m
time high-accuracy algorithm for
√ ℓp -norm minimizing flows on unit weighted undirected graphs (p ≥ 2). For large p, e.g. p = log n this is an
m1+o(1) time algorithm, and to our knowledge the first almost linear time high-accuracy algorithm
for a flow problem other than Laplacian solvers (ℓ2 ) or shortest-paths (ℓ1 ).

2

1.1.2

Semi-Supervised Learning on Graphs.

Semi-supervised learning on graphs in machine learning is often based on solving an optimization
problem where voltages (labels) are fixed at some vertices in a graph the voltages at remaning
nodes are chosen so that some overall objective is minimized (e.g. ℓp -norm of the vector of voltage
differences across edges) [AL11, KRSS15, EACR+ 16]. Formally, given a graph G = (V, E) and a
labelled subset of the nodes T ⊂ V with labels s T ∈ RT , we can write the problem as
X
|x u − x v |p .
(3)
min
x ∈ℜV |x T =s T

u∼v

By converting this problem to its dual, we get an almost linear time algorithm for solving it to high
accuracy, provided the initial voltage problem uses p close to 1: In this case, voltage solutions are
“cut-like”. Given p < 2, we get a solver that computes a (1 + 1/poly(m)) multiplicative accuracy
O((

1

√

3/2
)

)

1
m1+O( p−1) . For p = 1 + √log
, this is time is bounded by m1+o(1) .
solution in time 2 p−1
n
Converting the dual of Problem (3) into a form solvable by our algorithms requires a small
transformation, which we describe in Appendix F.

1.1.3

Use as Oracle in Conjunction with Multiplicative Weight Updates

The mixed ℓ22 and ℓpp objective in our Problem (2) is useful for building oracles to use in multiplicative
weight update algorithms based on flows, as they appear in [CKM+ 11, AKPS19]. Assume we are
looking to solve some problem to (1 + ǫ)-accuracy measured as multiplicative error, and let us
1
< ǫ < 0.5. Specifically we can solve for the following objective subject to certain
assume poly(m)
linear constraints.
X
ǫkr k1
r e f 2e +
|f e |p .
(4)
m
e
This gives an oracle for several problems. Algorithms based on oracle solutions to this type of
objective work by noting that any f with kf k∞ ≤ 1 gives an objective value at most
X
e

r e f 2e +

ǫkr k1
|f e |p ≤ (1 + ǫ)kr k1 .
m

Since such a flow must exist in the context where the oracle is applied, the optimum flow must also
meet this bound. Now, if we compute a (1 + 0.01ǫ) approximately optimal solution to this problem,
it must satisfy
X
ǫkr k1
|f e |p ≤ (1 + 1.1ǫ)kr k1 .
r e f 2e +
m
e
q
P
P
By Cauchy-Schwarz, we get e r e |f e | ≤ kr k1 e r e f 2e ≤ (1 + 1.1ǫ)kr k1 , which tells us the oracle
is “good-on-average” according to the weights r . The objective value also implies for every edge
that
ǫkr k1
|f e |p ≤ (1 + 1.1ǫ)kr k1 ≤ 2kr k1 ,
m
which simplifies to:
|f e | ≤ (m/ǫ)1/p ≤ mo(1)
(5)
3

when we set p = log0.1 n. This is the width of the oracle, and together these conditions demonstrate
that the oracle suffices for a multiplicative weights algorithm and bounds the number of calls to
the oracle by mo(1) poly(1/ǫ).
This oracle has multiple uses:
Approximate undirected maximum flow. Using the oracle, we can approximate maximum
flow using [CKM+ 11], giving an algorithm for undirected maximum flow that is not based
on oblivious routings unlike other fast algorithms for approximate maximum flow [She13,
KLOS14, Pen16]. Our algorithm obtains almost-linear time, albeit only for unit weighted
graphs.
Isotropic total variation denoising. Using our algorithm, we can give the first almost linear
time, low accuracy algorithm for total variation denoising on unit weighted graphs [ROF92,
ZWC10]. While there has been significant advances in image processing since the introduction
of this objective, it still remains a representative objective in pixel vision tasks. The total
variation objectives can be viewed as variants of semi-supervised learning on graphs: Given
a “signal” vector s which corresponds to noisy observations of pixels of an image, we want to
find a denoised version of s, which we refer to as x . The denoised output x should minimize an
objective that measures both the between pixels in x that are close to each other in the image
(which should be small), and the difference between x and s (which should also be small).
The most popular version of this problem, known as isotropic total variation denoising, allows
the input to specify a collection of groups of pixels with connections inside each group i given
by a set of edges Ei , and asks that 1) the denoised pixels are close in an ℓ2 sense to the
measured signal, 2) in each group, the standard deviation between denoised pixels is not too
high. These goals are expressed in the objective
X
XsX
2
(x u − x v )2 .
(x u − s u ) +
u

i

e∈Ei

The dual of this problem is grouped flows, which is finding f such that B ⊤ f = d and for
edge sets Ei ,
2
f Ei 2 ≤ 1.
Our oracle gives the first routine for approximate isotropic T V denoising that runs in almost
linear time. The previous best running time was about m4/3 [CMMP13].

1.2

Related Work

Network flow problems have a long history of motivating broader developments in algorithms, including the introduction of strongly polynomial time as a benchmark of algorithmic efficiency [Edm65,
EK72], the development of tree data structures [GN79, ST83, ST85], and randomized graph algorithms and graph approximations [KS96, BK96]. For general capacities, the best strongly
polynomial time algorithms run in about quadratic time due to the flow decomposition barrier [EK72, GN79, GT88, HO13, Orl13], which says that the there exists graphs where the path
decomposition of an optimum flow must have quadratic size.
The flow decomposition barrier suggest that sub-quadratic time algorithms for network flows
should decompose solutions numericallly, and this has indeed been the case in the development of
4

such algorithms [GR98, GT14]. These numerical approaches recently culminated in nearly-linear
time algorithms for undirected maximum flow and transshipment (the ℓ1 case of Problem (1)),
yielding nearly-linear time algorithms [CKM+ 11, She13, KLOS14, Pen16, She17b, She17a]. Much
of these progress were motivated by the development of nearly-linear time high-accuracy solvers for
Laplacian linear systems [ST14, KMP12, KOSZ13, LS13, KS16], whose duals, electrical flows are
the ℓ∞ case of Problem (1). Such solvers can in turn be used to give the current best high accuracy
flow algorithms. For graphs with polynomially bounded capacities, the current best running time
e √n) due to Lee and Sidford [LS14]. On sparse graphs, this bound still does not break
is O(m
the long-standing O(n1.5 ) barrier dating back to the early 70s [HK73, Kar73, ET75]. Recently
e 10/7 ) running
Madry [Mad13, Mad16] broke this barrier on unit capacitated graphs, obtaining O(m
time.
Our result has in common with all previous results on almost-linear time optimization problems on graphs [KLOS14, She13, BKKL17, Pen16, She17b, She17a] in that it is based on white-box
modifications of a linear system solver. In particular, our high level algorithmic strategy in creating edge and vertex reductions is identical to the first nearly-linear time solver by Spielman and
Teng [ST14]. Much of this similarity is due to the lack of understanding of more general versions
of key components: some possibilities for simplifying the result will be discussed in Section 1.4.
On the other hand, our algorithms differ from previous adaptations of solvers in that it obtains
high accuracy 1 . This requires us to tailor the scheme to the residual problems from the p-norm
iterative methods, and results in us taking a more numerical approach, instead of the more routing
and path embedding-based approaches utilized in similar adaptations of Spielman and Teng [ST14]
to cuts [Mad10], flows [She13, KLOS14], and shortest paths [BKKL17].
The development of high-accuracy algorithms for p-norm minimization that are faster than interior point methods (IPMs) [NN94] was pioneered by the recent work of Bubeck et al. [BCLL18]
which introduced the γ-functions that were also used in [AKPS19]. However, the methods in [BCLL18]
are conceptually similar to interior point methods (IPMs) [NN94] (as in they are homotopy methods). Their runtime for large p behaves essentially like IPMs, requiring about m3/2−o(1) time for
solving p-norm flow problems, whereas the limiting behavior of our result is about m1+o(1) .

1.3

Overview

At a high level, our approach can be viewed as solving a graph optimization problem as a linear
system. This is done by combining the numerical methods for ℓp -norms by Adil et al. [AKPS19]
with the recursive preconditioning of graph structured linear systems by Spielman and Teng [ST14].
Many conceptual obstacles arise in trying to realize this vision, preventing us from adopting later
Laplacian linear solvers that have greatly simplified the result of Spielman and Teng. The main one
is the lack of concentration theory for the smoothed p-norm objectives integral to our algorithms:
these concentration arguments are at the core of all subsequent improvements to Laplacian solver
algorithms [KMP11, KOSZ13, LPS15, KS16].
Our starting point is a recent result involving a subset of the authors [AKPS19] that significantly
generalized the phenomenon of high-accuracy numerical methods. In particular, this method is
applicable to general ℓp -norm optimization problems, for all p that are bounded away from 1 and
∞. It also opens up a major question: can we develop an appropriate notion of preconditioning, the
1

The nearly-linear time matrix scaling algorithm [CMTV17] has a linear dependence on the condition number κ,
while convex optimization methods for matrix scaling have dependencies of log κ instead.

5

other central ingredient of fast solvers for linear systems, applicable to ℓp -norms? We resolve this
question in the affirmative, and develop a theory of preconditioning that works for a wide class of
non-linear problems in Section 3. In particular, we show that the second and pth order terms from
the main formulation in Equation 2 form a class of functions that’s closed under taking residual
problems. We will formally define these as smoothed ℓp -norms in Section 2.1.
The crux of our problem then becomes designing preconditioners that interact well with these
smoothed ℓp -norms. Here it’s worth noting that earlier works on preconditioning for non-linear
(maximum) flow problems all relied on oblivious routing which gives rise to linear preconditioners.
Such an approach encounters a significant obstacle with ℓp norms: consider changing a single
coordinate from, say 1, to (1 + δ):
• If the update δ is much smaller than 1 in absolute value, the change in the objective from 1p
to (1 + δ)p is dominated by terms that scale as δ and δ2 .
• However, if the update is much larger than 1, the change is dominated by a δp term.
This means that good preconditioning across small and large updates is inherently highly dependent
on the current coordinate value.
This example captures the core difficulties of our preconditioned iterative methods for smoothed
ℓp -norm problems, which heavily rely on both the second and pth power terms the objective functions. It means our graph theoretic components must simultaneously control terms of different
degrees (namely scaling as δ, δ2 , and δp ) related to the flows on graphs. Here our key idea is that
unit-weighted graphs have “multi-objective low-stretch trees” that simultaneously preserve the δ2
and δp terms, while the linear (gradient) terms can be preserved exactly when routing along these
trees. Here a major difficulty is that the tree depends on the second order derivatives of the current
solution point, and thus must continuously change as the algorithm proceeds. Additionally, after
rerouting graph edges along the tree, we need to sparsify the graph according to weights defined
by the same second derivatives at the current solution, which makes the adaptive aspect of the
algorithm even more important. We defer the construction of our adaptive preconditioner to Section 4, after first formally defining our objective functions in Section 2, and introducing numerical
methods based on them in Section 3.

1.4

Open Questions

We expect that our algorithm can be greatly simplified and adapted to non-unit weight graphs in
ways similar to the sampling based solvers for Laplacian linear systems [KMP14, KOSZ13, KS16].
The current understanding of concentration theory for ℓp norms rely heavily on tools from functional
analysis [CP15]: generalizing these tools to smoothed ℓp -norm objectives is beyond the scope of
this paper.
A major limitation of our result is the restriction to unit capacitated graphs. We believe this limitation is inherent to our approach of constructing preconditioners from trees: for general weights,
there are cases where no tree can simultaneously have small stretch w.r.t. ℓ2 -norm and ℓp -norm
weights. We believe that by developing a more complete theory of elimination and sparsification for
these objectives, it will be possible to sparsify non-unit weight graphs, and develop solvers following
the patterns of non-tree based Laplacian solvers [PS14, LPS15, KS16].
We also believe that the overall algorithmic approach established here is applicable far beyond the class of objective functions studied in this paper. Here a direct question is whether the
6

dependency on p can be improved to handling ℓm flows, which in unit weighted graphs imply
maximum flows. The exponential dependence on p has already been shown to be improvable to
e 2 ) [Sac19]. For even larger values of p, a natural approach is to use homotopy methods
about O(p
that modify the p values gradually. Here it is also plausible that our techniques, or their possible
generalizations to weighted cases, can be used as algorithmic building blocks.

2
2.1

Preliminaries
Smoothed ℓp -norm functions

We consider p-norms smoothed by the addition of a quadratic term. First we define such a smoothed
pth -power on R.
Definition 2.1 (Smoothed pth -power). Given r, x ∈ R, r ≥ 0 define the r-smoothed s-weighted
pth -power of x to be
hp (r, s, x) = rx2 + s|x|p .
This definition can be naturally extended to vectors to obtained smoothed ℓp -norms.
Definition 2.2 (Smoothed ℓp -norm). Given vectors x ∈ Rm , r ∈ Rm
≥0 , and a positive scalar
s ∈ R≥0 , define the r -smooth s-weighted p-norm of x to be
hp (r , s, x ) =

m
X

hp (r i , s, x i ) =

i=1

i=1

2.2

m
X
(r i x 2i + s|x i |p ).

Flow Problems and Approximation

We will consider problems where we seek to find flows minimizing smoothed p-norms. We first
define these problem instances.
Definition 2.3 (Smoothed p-norm instance). A smoothed p-norm instance is a tuple G,
def

G = (V G , E G , g G , r G , sG ),
where V G is a set of vertices, E G is a set of undirected edges on V G , the edges are accompanied by
G
G
a gradient, specified by g G ∈ RE , the edges have ℓ22 -resistances given by r G ∈ RE
≥0 , and s ∈ R≥0
gives the p-norm scaling.
Definition 2.4 (Flows, residues, and circulations). Given a smoothed p-norm instance G, a vector
⊤
G
G
f ∈ RE is said to be a flow on G. A flow vector f satisfies residues b ∈ RV if B G f = b, where
⊤
G
G
B G ∈ RE ×V is the edge-vertex incidence matrix of the graph (V G , E G ), i.e., B G (u,v) = 1u − 1v .
A flow f with residue 0 is called a circulation on G.
Note that our underlying instance and the edges are undirected. However, for every undirected
edge e = (u, v) ∈ E, we assign an arbitrary fixed direction to the edge, say u → v, and interpret
f e ≥ 0 as flow in the direction of the edge from u to v, and f e < 0 as flow in the reverse direction.
For convenience, we assume that for any edge (u, v) ∈ E, we have f (u,v) = −f (v,u) .
7

Definition 2.5 (Objective, E G ). Given a smoothed p-norm instance G, and a flow f on G, the
associated objective function, or the energy, of f is given by
E G (f ) = g G

⊤

f − hp (r , s, f ).

Definition 2.6 (Smoothed p-norm flow / circulation problem). Given a smoothed p-norm instance
G
G
G and a residue vector b ∈ RE , the smoothed p-norm flow problem (G, b), finds a flow f ∈ RE
with residues b that maximizes E G (f ), i.e.,
max

f :(B G )⊤ f =b

E G (f ).

If b = 0, we call it a smoothed p-norm circulation problem.
Note that the optimal objective of a smoothed p-norm circulation problem is always nonnegative, whereas for a smoothed p-norm flow problem, it could be negative.

2.3

Approximating Smoothed p-norm Instances

Since we work with objective functions that are non-standard (and not even homogeneous), we
need to carefully define a new notion of approximation for these instances.
Definition 2.7 (H κ G). For two smoothed p-norm instances, G, H, we write H κ G if there is a
H
G
linear map MH→G : RE → RE such that for every flow f H on H, we have that f G = MH→G (f H )
is a flow on G such that
1. f G has the same residues as f H i.e., (B G )⊤ f G = (B H )⊤ f H , and
2. has energy bounded by:



1 H H
G 1 G
E f
f
≤E
.
κ
κ

For some of our transformations on graphs, we will be able to prove approximation guarantees
only for circulations. Thus, we define the following notion restricted to circulations.
Definition 2.8 (H cycle
G). For two smoothed p-norm instances, G, H, we write H cycle
G if there
κ
κ
G
H
H
H ⊤ H
E
E
→ R such that for any circulation f on H, i.e., (B ) f = 0,
is a linear map MH→G : R
the flow f G = MH→G (f H ) is a circulation, i.e., (B G )⊤ f G = 0, and satisfies


1 H H
G 1 G
E f
f
≤E
.
κ
κ

Observe that H κ G implies H cycle
G.
κ

These definitions satisfy most properties that we want from comparisons.
Lemma 2.9 (Reflexivity). For every smoothed p-norm instance G, and every κ ≥ 1, G κ G and
G cycle
G with the identity map.
κ
It behaves well under composition.
8

Lemma 2.10 (Composition). Given two smoothed p-norm instances, G1 , G2 , such that G1 κ1 G2
with the map MG1 →G2 and G2 κ2 G3 with the map MG2 →G3 , then G1 κ1 κ2 G3 with the map
MG1 →G3 = MG2 →G3 ◦ MG1 →G2 .
Similarly, for any G1 , G2 , if G1 cycle
G2 with the map MG1 →G2 and G2 cycle
G3 with the map
κ1
κ2
G
with
the
map
M
=
M
◦
M
.
MG2 →G3 , then G1 cycle
κ1 κ2 3
G1 →G3
G2 →G3
G1 →G2
The most important property of this is that this notion of approximation is also additive, i.e.,
it works well with graph decompositions.
Definition 2.11 (Union of two instances). Consider smoothed p-norm instances, G1 , G2 , with the
same set of vertices, i.e. V G1 = V G2 . Define G = G1 ∪ G2 as the instance on the same set of vertices
obtained by taking a disjoint union of the edges (potentially resulting in multi-edges). Formally,
G = (V G1 , E G1 ∪ E G2 , (g G1 , g G2 ), (r G1 , r G2 ), (s G1 , s G2 )).
Lemma 2.12 (κ under union). Consider four smoothed p-norm instances, G1 , G2 , H1 , H2 , on the
same set of vertices, i.e. V G1 = V G2 = V H1 = V H2 , such that for i = 1, 2, Hi κ Gi with the map
def
def
MHi →Gi . Let G = G1 ∪ G2 , and H = H1 ∪ H2 . Then, H κ G with the map
 def


MH→G f H = (f H1 , f H2 ) = MH1 →G1 f H1 , MH2 →G2 f H2 ,

where (f H1 , f H2 ) is the decomposition of f H onto the supports of H1 and H2 .

This notion of approximation also behaves nicely with scaling of ℓ2 and ℓp resistances.
Lemma 2.13. For all κ ≥ 1, and for all pairs of smoothed p-norm instances, G, H, on the same
underlying graphs, i.e., (V G , E G ) = (V H , E H ), such that,
1. the gradients are identical, g G = g H ,
2. the ℓ22 resistances are off by at most κ, i.e., r Ge ≤ κr H
e for all edges e, and
3. the p-norm scaling is off by at most κp−1 , i.e., sG ≤ κp−1 sH ,

then H κ G with the identity map.

2.4

Orthogonal Decompositions of Flows

At the core of our graph decomposition and sparsification procedures is a decomposition of the
gradient g of G into its cycle space and potential flow space. We denote such a splitting using
⊤

g G = gb G + B G ψ G , s.t. B G gb G = 0.

(6)

Here gb is a circulation, while Bψ gives a potential induced edge value. We will omit the superscripts
when the context is clear.
The following minimization based formulation of this splitting of g is critical to our method of
bounding the overall progress of our algorithm
Fact 2.14. The projection of g onto the cycle space is obtained by minimizing the energy added to
a potential flow to g . Specifically,
kb
g k22 = minkg + Bx k22 .
x

9

Lemma 2.15. Given a graph/gradient instance G, consider H formed from a subset of its edges.
The projections of g G and g H onto their respective cycle spaces, gb G and gb H satsify:

3

Numerical Methods

gb H

2
2

≤ gb G

2
2

≤ gG

2
.
2

The general idea of (preconditioned) numerical methods, which are at the core of solvers for graphstructured linear systems [ST14] is to repeatedly update a current solution in ways that multiplicative reduce the difference in objective value to optimum. In the setting of flows, suppose we
currently have some tentative solution f to the minimization problem
min kf kpp

B f =b

(7)

by performing the step
f ← f + δ,
with the goal of improving the objective value substantially.
The work of Adil et al. [AKPS19] proved that ℓp -norm minimization problems could be iteratively refined. While that result hinted at a much more general theory of numerical iterative
methods for minimizing convex objectives, this topic is very much under development. In this section, we will develop the tools necessary for preconditioning ℓp -norm based functions, and formalize
the requirements for preconditioners necessary for recursive preconditioning algorithms.

3.1

Iterative Refinement

The following key Lemma from [AKPS19] allows us to approximate the change in the smoothed
p-norm of x + δ relative to the norm of x , in terms of another smoothed p-norm of δ.
Lemma 3.1 ([AKPS19]). For all r , x , δ ∈ Rm , with r ∈ Rm
≥0 , and s ≥ 0, we have
2−p · hp (r + |x |p−2 , s, δ) ≤ hp (r , s, x + δ) − hp (r , s, x ) − δ ⊤ ∇x hp (r , s, x ) ≤ 22p · hp (r + |x |p−2 , s, δ).
The above lemma gives us the following theorem about iteratively refining smoothed ℓp -norm
minimization problems. While the lemma was essentially proven in [AKPS19], they used slightly
different definitions, and for completeness we prove the lemma in Appendix B.
The following theorem also essentially appeared in [AKPS19], but again as slightly different
definitions were used in that paper, we prove the theorem in Appendix B for completeness.
Theorem 3.2 ([AKPS19]). Given the following optimization problem,
maxx
s.t.

def

E1 (x ) = g ⊤ x − hp (r , s, x )
Ax = b

(P1)

and an initial feasible solution x 0 , we can construct the following residual problem:
maxδ
s.t.

def

E2 (δ) = (g ′ )⊤ δ − hp (r ′ , s, δ)
Aδ = 0,
10

(R1)

where g ′ = 2p (g − ∇x h(r , s, x )|x =x 0 ), and r ′ = r + s|x 0 |p−2 .
exists a feasible solution e
δ to the residual problem R1 that achieves an objective of
There

p
⋆
e
E2 δ ≥ 2 (E1 (x ) − E1 (x 0 )), where x ⋆ is an optimal solution to problem P1.
def

Moreover, given any feasible solution δ to Program R1, the vector x 1 = x 0 + 2−3p δ is a feasible
solution to the Program P1 and obtains the objective
E1 (x 1 ) ≥ E1 (x 0 ) + 2−4p E2 (δ).
Importantly, we can apply the above theorem to smoothed p-norm flow/circulation problems.

Corollary 3.3 (Iterative refinement for smoothed p-norm flow/circulation problems). Given any
smoothed p-norm flow problem (G, b) with optimal objective E ⋆ (G), and any initial circulation f 0 ,
we can build, in O( E G ) time, a smoothed p-norm circulation problem H with the same underlying
graph (V H , E H ) = (V G , E G ), such that E ⋆ (H) ≥ 2p (E ⋆ (G) − E G (f 0 )) and for any circulation f H on
def

H, the flow f 1 = f 0 + 2−3p f H satisfies residues b on G and has an objective
E G (f 1 ) ≥ E G (f 0 ) + 2−4p E H (f H ).

This means if we find even a crude approximate minimizer δe of this update problem, we can move
e so that the gap to the optimum in the original optimization problem (7)
to a new point f ′ = f + δ,
p
will decrease by a constant factor (depending only on p) from kf kpp − OPT to f ′ p − OPT ≤

(1 − 2−O(p) )(kf kpp − OPT). In other words, we have a kind of iterative refinement: crude solutions
to an update problem directly
P p give constant factor progress in the original objective.
p
Note that kf kp = i f i . This will help us understand the objective function of the update
problem coordinate-wise. Our update problem objective function is motivated by the following
observations. Our function differs slightly from the function used in [AKPS19], which in turn
was based on functions from [BCLL18], but our function still uses a few special properties of the
[BCLL18] functions. Suppose p ≥ 2 is an even integer (only to avoid writing absolute values), then
δ 2 + δ pi )
δ i + 2O(p) (f p−2
δ 2 + δ pi ) ≤ (f i + δ i )p ≤ f pi + pf p−1
δ i + 2−O(p) (f p−2
f pi + pf p−1
i
i
| i {zi
}
}
| i {zi
write as hp (f p−2
,δ i )
i

hp (f p−2
,δ i )
i

Of course, the exact expansion gives

(f i + δ i )p = f pi + pf p−1
δi +
i

p(p − 1) p−2 2 p(p − 1)(p − 2) p−3 3
f i δi +
f i δ i + . . . + δpi
2
6

(8)

So essentially we can approximate this expansion using only the zeroth, first, second, and last term
in the expansion. We use g(f ) to denote the vector with g i (f ) = pf p−1
i P(i.e. the gradient), and
let f p−2 denote the entrywise powered vector, and define hp (f p−2 , δ) = i hp (f p−2
, δ i ). Thus we
i
have
kf kpp + g (f )⊤ δ + 2−O(p) hp (f p−2 , δ) ≤ kf + δkpp ≤ kf kpp + g (f )⊤ δ + 2O(p) hp (f p−2 , δ)
Note that for any scalar 0 < λ < 1,
λp hp (f p−2 , δ) ≤ hp (f p−2 , λδ) ≤ λ2 hp (f p−2 , δ)
11

Together, these observations are enough to ensure that if we have δe which is a constant factor
approximate solution to the follow optimization problem, which we define as our update problem
min g (f )⊤ δ + hp (f p−2 , δ)

B δ=0

then we can find a λ s.t.

f + λδe

p
p

(9)

− OPT ≤ (1 − 2−O(p) )(kf kpp − OPT).

But what have we gained? Why is Problem (9) more tractable than the one we started with?
A key reason is that unlike the exact expansion of an update as given by Equation (8), all the
higher order terms in the objective function of (9) are coordinate-wise even functions, i.e. flipping
the sign of a coordinate does not change the value of the function. [AKPS19] used a different but
still even function instead of our hp . This symmetrization allowed them to develop a multiplicative
weights update algorithm motivated by [CKM+ 11] for their version of Problem (9), reducing the
problem to solving a sequence of linear equations.
For our choice of hp , it is particularly simple to show another very important property: Consider
solving Problem (9) by again applying iterative refinement to this problem. That is, at some
intermediate step with δ being the current solution, we aim to find an update δb s.t. B δb = 0 and
b + hp (f p−2 , δ + δ)
b is smaller than g (f )⊤ (δ) + hp (f p−2 , δ). Then by expanding the
g (f )⊤ (δ + δ)
b p and (δ i + δ)
b 2 , similar to Equation (8), we get
, δ i ), i.e. (δ i + δ)
two non-linear terms of hp (f p−2
i
a sequence of terms with powers of δ i ranging from 2 to p. If we approximate this sequence again
p
2
using only the δbi and δbi terms, we get another update problem. This update problem is an instance
of Problem (2). And in general, we can set up iterative refinement update problems for instances
of Problem (2), and get back another problem of the that class (after our approximation based
on dropping intermediate terms). Thus, the problem class (2) is closed under repeatedly creating
iterative update problems. This observation is central because it allows us to develop recursive
algorithms.

3.2

Vertex Elimination

Following the template of the Spielman-Teng Laplacian solver, we recursively solve a problem on
m edges by solving about κ problems on graphs with n − 1 + m/κ edges. These ultra-sparse graphs
allow us to eliminate degree 1 and 2 vertices and obtain a smaller problem. Because our update
problem (Problem (9)) corresponds to a flow-circulation problem with some objective, we are able
to understand elimination on these objectives in a relatively simple way: the flow on degree 1 and
2 vertices is easily related to flow in a smaller graph created by elimination. Unlike Spielman-Teng,
every recursive call must rely on a new graph sparsifier, because the “graph” we sparsify depends
heavily on the current solution that we are seeking to update: we have to simultaneously preserve
linear, quadratic and p-th order terms, whose weights depend on the current solution.
A critical component of this schema is the mapping of flows back and forth between the original
graph and the new graph so a good solution on a smaller graph can be transformed into a good
solution on the larger graph. These mappings are direct analogs of eliminating degrees 1 and 2
vertices. In Appendix C, we generalize these processes to smoothed ℓp -norm objectives, proving
the following statements:
Theorem 3.4 (Eliminating vertices with degree 1 and 2). Given a smoothed p-norm instance G,
the algorithm Eliminate(G) returns another smoothed p-norm instance G ′ , along with the map
′
′
MG ′ →G in O( V G + E G ) time, such that the graph G′ = (V G , E G ) is obtained from the graph
12

G = (V G , E G ) by first repeatedly removing vertices with non-selfloop degree2 1 in G, and then
replacing every path u
v in G where all internal path vertices have non-selfloop degree exactly 2
in G, with a new edge (u, v).
Moreover,
G cycle
G′,
G ′ cycle
1
1
n p−1

where n = V

G

, and the map MG ′ →G can be applied in O( V G + E G ) time.

Lemma 3.5 (Eliminating Self-loops). There is an algorithm RemoveLoops such that, given a
smoothed p-norm instance G with self-loops in E G , in O( V G + E G ) time, it returns instances
G1 , G2 , such that G = G1 ∪ G2 , where G1 is obtained from G by eliminating all self-loops from E G , and
G2 is an instance consisting of just the self-loops from G. Thus, any flow f G2 on G2 is a circulation.
Moreover, there is an algorithm SolveLoops that, given G2 , for any δ ≤ 1/p, in time O(|E G2 | log 1/δ ),
finds a circulation feG2 on G2 such that
E G2 (feG2 ) ≥ (1 − δ)

max

f G :(B )G f G =0

E G2 (f G2 ).

We remark that only the map from the smaller graph to the larger has to be constructive.

3.3

Recursive Preconditioning

We can now present our main recursive preconditioning algorithm, RecursivePreconditioning
(Algorithm 1). Earlier work on preconditioning for non-linear (maximum) flow problems all relied
on oblivious routing which gives rise to linear preconditioners. These inherently cannot work well
for high-accuracy iterative refinement, and the issue is not merely linearity: Consider Problem (9):
the optimal δ is highly dependent on the current f , and when a coordinate δ i is large compared
to the current |f i |, the function depends on it as δ pi , while when δ i is small compared to |f i |, it
behaves as δ 2i . Thus the behavior is highly dependent on the current solution. This necessitates
adaptive and non-linear preconditioners.
To develop adaptive preconditioners, we employ recursive chains of alternating calls to nonlinear iterative refinement and a new type of (ultra-)sparsification that is more general and stronger,
allowing us to simultanously preserve multiple different properties of our problem. And crucially,
every time our solution is updated, our preconditioners change. The central theorem governing the
combinatorial components of our algorithm, which is the main result proven in Section 4, is:
Theorem 3.6 (Ultra-Sparsification). Given any instance G = (V G , E G , g G , r G , sG ) with n nodes, m
edges, and parameters κ, δ where log 1δ and log r G ∞ are both O(logc n) for some constant c and κ <
e
m , UltraSparsify computes in O(m)
running time another instance H = (V H , E H , g H , r H , sH =
sG ) along with flow mapping functions MH→G , MG→H such that V H = V G , and with high probability
we have
1. E H consists of a spanning tree in the graph (V G , E G ), up to m − n + 1 self-loops and at most
e m ) other non self-loop edges.
O(
κ

2

By non-selfloop degree, we mean that self-loops do not count towards the degree of a vertex.

13

3/(p−1) ) for any flow f G of G we have
e
2. With κG→H = O(κm

EH (

MG→H (f G )
1
)≥
EG (f G ) − δ f G
κG→H
κG→H

2

gG

2

e 2/(p−1) ), for any flow solution f H of H we have
and with κH→G = O(m
EG (

MH→G (f H )
1
)≥
EH (f H ) − δ( f H
κH→G
κH→G

2

gG

2

,

+ fH

2
).
2

e
The flow mappings MH→G , MG→H preserve residue of flow, and can be applied in O(m)
time.

Algorithm 1 Recursive Preconditioning Algorithm for p-smoothed flow/circulation problem
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:

procedure RecursivePreconditioning(G, b, f (0) , κ, δ)
e
solve G using the algorithm from [AKPS19]
m ← E G . If m ≤ O(κ),
6
e 3p κm p−1 )
T ← O(2
for t = 0 to T do
Construct the residual smoothed p-norm circulation problem H1 for (G, b) with the
current solution f (t) , given by Corollary 3.3.
− p
δ′ ← min{1, g H1 p−1 } · δ/(4T m).
H2 , MH2 →H1 , κH2 →H1 ← UltraSparsify(H1 , κ, δ′ ) ⊲ H2 is an ultrasparsifier for H1
H3 , MH3 →H2 ← Eliminate(H2 ) ⊲ Gaussian elimination to remove degree 1,2 vertices
H4 , Hloops ← RemoveLoops(H3 )
⊲ Remove self-loops
∆Hloops ← SolveLoops(Hloops , 1/p)
⊲ Solve the self-loop instance
∆H4 ← RecursivePreconditioning(H4 , 0, 0, κ, δ/T ) ⊲ Recurse on smaller instance
∆H3 ← ∆H4 + ∆Hloops
⊲ Adding solution for Hloops to obtain solution for H3
−

1

∆H2 ← V H2 p−1 · MH3 →H2 (∆H3 ).
H2
∆H1 ← κ−1
H2 →H1 MH2 →H1 (∆ )
f (t+1) ← f (t) + 2−3p ∆H1
return f (T )

⊲ Undo elimination to map solution back to H2
⊲ Map it back to the residual problem
⊲ Update the current flow solution

Our key theorem about the performance of the algorithm is then:
Theorem 3.7 (Recursive Preconditioning). For all p ≥ 2, say we are given a smoothed p-norm instance G, residues b, initial solution f (0) , and δ ≤ 1 such that log 1/δ , log g G , log r G , log f (0) ≤
1

e
e √p−1 ) so that the procedure RecursivePreconditioning(G, b, f (0) , κ, δ)
O(1).
We can pick κ = Θ(m

runs in time 2O(p

3/2
)

m

1
1+O( √p−1
)

, and returns a flow f on G such that f satisfies residues b, and

1
E ⋆ (G) − E G (f (T ) ) ≤ (E ⋆ (G) − E G (f (0) )) + δsG .
2

Proof. (of Theorem 3.7) By scaling g G , r G , we can assume that sG = 1 without loss of generality.
Let us consider iteration t of the for loop in RecursivePreconditioning. First, let us prove
guarantees on the optimal solutions of all the relevant instances. By the guarantees of Corollary 3.3, we know that H1 is a smoothed p-norm circulation problem with the same underlying
graph (V H1 , E H1 ) = (V G , E G ), such that E ⋆ (H1 ) ≥ 2p (E ⋆ (G) − E G (f (t) )).
14

From Theorem 3.6, we know that UltraSparsify returns a smoothed p-norm circulation
H1
⋆
′
f ⋆H1 .
instance H2 on the same set of vertices such that E ⋆ (H2 ) ≥ κ−1
H1 →H2 E (H1 ) − δ g

From Theorem 3.4, we know that the instance H3 returned by Eliminate(H2 ) satisfies H2 cycle
1
H3 , and hence E ⋆ (H2 ) ≤ E ⋆ (H3 ). From Lemma 3.5, we know that E ⋆ (H4 ) + E ⋆ (Hloops ) = E ⋆ (H3 ).
Combining these guarantees, we obtain,
⋆
G
(t)
E ⋆ (H3 ) = E ⋆ (H4 ) + E ⋆ (Hloops ) ≥ 2p κ−1
)) − δ g H1
H1 →H2 (E (G) − E (f

f ⋆H1 .

Now, we analyze the approximation guarantee provided by the solutions to these instances.
From Lemma 3.5, SolveLoops(Hloops ) returns a ∆Hloops that satisfies E Hloops (∆Hloops ) ≥ 12 α⋆ (Hloops ).
By induction, RecursivePreconditioning(H4 , 0, κ, δT −1 ), upon starting with the initial solution 0, returns a ∆H4 that satisfies, E H4 (∆H4 ) ≥ 12 E ⋆ (H4 ) − δT −1 . Combining these guarantees,
we have,
1
E H3 (∆H3 ) = E H4 (∆H4 ) + E Hloops (∆Hloops ) ≥ E ⋆ (H3 ) − δT −1 .
2
H2
From Theorem 3.4, we also have H3 cycle
κelim H2 , for κelim = V

1
p−1

, and hence

H3
H2
H3
(∆H2 ).
(∆H3 ) ≤ E H2 (κ−1
κ−1
elim MH3 →H2 (∆ )) = E
elim E

Finally, from Theorem 3.6, we have,
−1
H2
H2
E H1 (∆H1 ) = E H1 (κ−1
(∆H2 ) − δ′ g H1
H2 →H1 MH2 →H1 (∆ )) ≥ κH2 →H1 E

Combining these guarantees, we obtain,

1
1
−1
−1
H1
H1
⋆
G
(t)
E (∆ ) ≥ κH2 →H1 κelim 2p κ−1
)) − δ′ g H1
H1 →H2 (E (G) − E (f
2
2
− δ′ g H1

∆H2 − δ′ ∆H2

f ⋆H1 ≤

− δT

−1

2



6

f ⋆H1 − δ′ g H1

6

∆H2 − δ′ ∆H2

e p κ−1 m− p−1 )(E ⋆ (G) − E G (f (t) )) − 2δT −1 ,
≥ Ω(2

√
m g H1

f

⋆H1

2

e p κ−1 m− p−1 )(E ⋆ (G) − E G (f (t) ))
≥ Ω(2
− δ′ g H1

∆H2 − δ′ ∆H2

2

− δT −1 .

1
p−1

p

p

, since E H1 (f ⋆H1 ) ≥ 0 implies that m1−p/2 f ⋆H1 2 ≤ f ⋆H1 p ≤ g H1 ⊤ f ⋆H1 ≤
1
√
√
. Similarly, ∆H1 ≤ m g H1 p−1 , and ∆H2 ≤ m ∆H1 , by the reverse tree

g H1 f ⋆H1
routing map.
Thus, by Theorem 3.3, f (t+1) satisfies

E ⋆ (G) − E G (f (t+1) ) ≤ E ⋆ (G) − E G (f (t) ) − 2−4p E H1 (∆H1 )


6
e −3p κ−1 m− p−1 ) (E ⋆ (G) − E G (f (t) )) + 2−4p · 2δT −1 .
≤ 1 − Ω(2
6

e 3p κm p−1 ) times gives us a solution f (T ) such that
Thus, repeating for loop O(2
1
E ⋆ (G) − E G (f (T ) ) ≤ (E ⋆ (G) − E G (f (0) )) + δ.
2
15

Now, we analyze the running time. In a single iteration, the total cost of all the operations
e
other than the recursive call to RecursivePreconditioning is O(m).
Note that H1 has m edges.
e
Theorem 3.6 tells us that H2 consists of a tree (n − 1 edges), at most O(m/κ)
non-selfloop edges,
plus many self-loops. After invoking Eliminate, and RemoveLoops, the instance H4 has no
self-loops left, and after dropping vertices with degree 0, only has vertices with degree at least
3. Every edge removed in Eliminate decreases the number of edges and vertices with non-zero
non-selfloop degree by 1. Suppose n′ is the number of vertices in H4 with non-zero degree. Then,
Eliminate must have removed n − n′ edges from H2 . Since H4 must have at least 32 n′ vertices, we
e
e
have n − 1 + O(m/κ)
− (n − n′ ) ≥ 23 n′ . Hence, n′ ≤ O(m/κ).
Thus H4 is an instance with at most
e
O(m/κ) vertices and edges.
Thus, the total running time recurrence is


6
e
e 3p κm p−1 ) T (m/κ) + O(m)
.
T (m) ≤ O(2
√1

e
p−1 ), we can fix the depth of
Note that κ is fixed throughout the recursion. By picking κ = Θ(m
√
the recursion to be O( p − 1). The total cost is dominated by the cost at the bottom level of the
3/2

1+O( √ 1

)

p−1 .
recursion, which adds up to a total running time of 2O(p ) m
The above discussion does not take into account the reduction in δ as we go down the recursion.
Observe that δ is lower bounded by
p

√

δ(mT maxkg k p−1 )−O(

p−1)

p

√

= δ(m2p max{{kf 0 k, g G , r G } p−1 })−O(

p−1)

.

e
Thus, we always satisfy log δ = O(1).

We can now prove the central collaries regardling smoothed ℓp -norm flows and ℓp -norm flows.

Theorem 1.1 (Smoothed ℓp -norm flows). For any p ≥ 2, given weights r ∈ RE
≥0 , a “gradient” g ∈
⊤
(0)
E
V
R , a demand vector b ∈ R (with b 1 = 0), and an initial solution f
such that all parameters
are bounded by 2poly(log n) , we can compute a flow fe satisfying demands b, i.e., B G⊤ fe = b, such
that


1
1
val(f (0) ) − OPT +
val(fe) − OPT ≤
poly(m)
poly(m)

in 2O(p

3/2
)

m

1+O( √1p )

time, where m denotes the number of edges in G.

Proof. First note that the Problem (2) is a smoothed p-norm instance after flipping the sign of g ,
and the sign of the objective function. We can solve this smoothed p-norm instance G by using
Theorem 3.7 to compute the desired approximate solution to the residual problems. We start with
f (0) as our initial solution. At iteration t, we invoke Theorem 3.7 using f (t−1) as the initial solution,


1
(t−1)
,
f (t) ← RecursivePreconditioning G, b, f
, κ,
poly(n)
where κ is given by Theorem 3.7.
We know that f (t) satisfies,
1
1
.
E ⋆ (G) − E G (f (t) ) ≤ (E ⋆ (G) − E G (f (t−1) )) +
2
poly(n)
16

Iterating O(log n) times, we obtain,
E ⋆ (G) − E G (f (T ) ) ≤

1
1
(E ⋆ (G) − E G (f (0) )) +
.
poly(n)
poly(n)

Finally, noting that we had flipped the sign of the objective function in Problem (2), we obtain our
claim.
Theorem 1.2 (ℓp -norm flows). For any p ≥ 2, given an unweighted graph G(V, E) and demands
b, using the routine pFlows(G, b) (Algorithm 2) we can compute a flow fe satisfying satisfying b,
i.e., B G⊤ fe = b, such that


p
1
e
min kf kpp .
f ≤ 1+
poly(m) f :B G⊤ f =b
p

in 2O(p

3/2
)

m

1+O( √1p )

, time, where m denotes the number of edges in G.

Proof. The pseudocode for our procedure pFlows(G, b) for this problem is given in Algorithm 2.
Our goal is to compute a flow fe satisfying B G⊤ fe = b, such that


p
1
fe ≤ 1 +
kf ⋆ kpp ,
poly(m)
p

where f ⋆ is the flow minimizing the ℓp -norm with residue b. For concreteness, we take this to
p
mean fe ≤ (1 + 3m−c )kf ⋆ kpp , for some constant c. We construct a smoothed p-norm instance
p

G = (V, E, 0, 0, 1). Note that the smoothed p-norm flow problem (G, b) finds a flow satisfying
residues b, and maximizing E G (f ) = −kf kpp . We can solve this smoothed p-norm instance by
iteratively refining using Corollary 3.3, and using Theorem 3.7 to compute the desired approximate
solution to the residual problems.
e
Formally, we use Laplacian solvers to compute in O(m)
time f (0) as a 2-approximation to
minf :B ⊤ f =b kf k. We have,
f (0)

p

≤ f (0)

1

2

1

≤ 2 min kf k2 ≤ 2kf ⋆ k2 ≤ 2m 2 − p kf ⋆ kp .
B G⊤ f =b

At each iteration t, we construct the residual smoothed p-norm circulation problem Ht for (G, b)
with the current solution f (t) , given by Corollary 3.3. We then invoke
∆(t) ← RecursivePreconditioning(Ht , 0, 0, κ,

1
· f (0)
p
2 mp/2 mc

1

p

),

e √p−1 ) is given by Theorem 3.7. Let ∆(t) be the flow returned. We know
where κ = Θ(m
E ⋆ (Ht ) − E Ht (∆(t) ) ≤

1
1 ⋆
E (Ht ) + p p/2 c f (0)
2
2 m m

17

p
p

We let f (t+1) ← f (t) + 2−3p ∆(t) . Thus, by Corollary 3.3, at every iteration, we have
− f (t+1)

p

= E G (f (t+1) ) ≥ E G (f (t) ) + 2−4p E Ht (∆(t) )


p
p
1
−4p 1 ⋆
(t)
(0)
+2
≥− f
E (Ht ) − p p/2 c f
2
p
p
2 m m


p
1
−4p 1 p ⋆
(t)
G
(t)
⋆ p
+2
≥− f
2 (E (G) − E (f )) − c kf kp
2
m
p


−4p
p
p
2
≥ − f (t) + 2−4p f (t) − kf ⋆ kpp −
kf ⋆ kpp .
p
p
mc

p

where we have used, f (0)
f

(t+1)

p
p

1

p

≤ 2m 2

−

kf ⋆ kpp

− p1

kf ⋆ kp ,
−4p

≤ (1 − 2


) f (t)

p
p

−

kf ⋆ kpp



+

2−4p ⋆ p
kf kp .
mc

Thus
f

(t)

p
p

−

kf ⋆ kpp

−4p


) f (t)

p

⋆ p
kp

− kf
≤ (1 − 2
p


−(4p+1)
≤ max (1 − 2
) f (t)



2−4p ⋆ p
kf kp
mc


p
⋆ p
−c
⋆ p
− kf kp , 3m kf kp .
+

p

Where firststep follows by rearranging
terms. To establish the second inequality, first
 consider the 

p
p
p
p
p
p
f (t) − kf ⋆ kp ≥ 2m−c kf ⋆ kp and hence f (t) −kf ⋆ kp ≤ (1−2−4p /2) f (t) − kf ⋆ kpp ,
case when
p
p
p


p
⋆ p
(t)
⋆ p
−c
− kf kp < 2m kf kp , the inequality is immediate.
f
meanwhile, when
p

Iterating T = Θ((c + p)24p log m) times gives us


p
⋆ p
−(4p+1) T
(T )
− kf kp ≤ max (1 − 2
)
f
f (0)
p

4

p
p

−

kf ⋆ kpp



, 3m

−c

kf ⋆ kpp



≤ 3m−c kf ⋆ kpp .

Graph Theoretic Preconditioners

In this section, we discuss at a high level of the construction of ultra-sparsifiers for a smooth ℓp norm instance. We start by restating the main theorem of our ultra-sparsifier. After establishing
the necessary tools, we prove this theorem at the end of this section.
Theorem 3.6 (Ultra-Sparsification). Given any instance G = (V G , E G , g G , r G , sG ) with n nodes, m
edges, and parameters κ, δ where log 1δ and log r G ∞ are both O(logc n) for some constant c and κ <
e
m , UltraSparsify computes in O(m)
running time another instance H = (V H , E H , g H , r H , sH =
sG ) along with flow mapping functions MH→G , MG→H such that V H = V G , and with high probability
we have
18

Algorithm 2 Computing p-norm minimizing flows. Given constant p and c, the routine computes
fe with residues b and p-norm that is within a factor (1 + 3m−c ) of the minimum p-norm achievable
for these residues.
1: procedure pFlows(G, b)
2:
Use Laplacian solvers to compute f (0) as a 2-approximation to minf :B ⊤ f =b kf k.


p
(0)
1
3:
δ ← min 1, 2p mp/2 mc · f
p

1

4:
5:
6:
7:
8:
9:
10:

e √p−1 )
κ ← Θ(m
T ← Θ((c + p)24p log m)
for t = 0 to T − 1 do
Construct the residual smoothed p-norm circulation problem Ht for (G, b) with the
current solution f (t) , given by Corollary 3.3.
∆(t) ← RecursivePreconditioning(Ht , 0, 0, κ, δ)
f (t+1) ← f (t) + 2−3p ∆(t)
return fe ← f (T )

1. E H consists of a spanning tree in the graph (V G , E G ), up to m − n + 1 self-loops and at most
e m ) other non self-loop edges.
O(
κ
3/(p−1) ) for any flow f G of G we have
e
2. With κG→H = O(κm

EH (

1
MG→H (f G )
)≥
EG (f G ) − δ f G
κG→H
κG→H

2

gG

2

e 2/(p−1) ), for any flow solution f H of H we have
and with κH→G = O(m
EG (

MH→G (f H )
1
)≥
EH (f H ) − δ( f H
κH→G
κH→G

2

gG

2

,

+ fH

2
).
2

e
The flow mappings MH→G , MG→H preserve residue of flow, and can be applied in O(m)
time.

Our high-level approach is the same as Spielman-Teng [ST14], where we utilize a low-stretch
spanning tree, and move off-tree edges to a small set of portal nodes. Once most off-tree edges are
only between a small set of portal nodes, we sparsify the graph over the portal nodes to reduce
the number of edges. As we need to map flow solutions between the original instance and the
sparsified instance, our main concern is to carry out these step without incurring too much error on
the objective function values. In our case we have ℓpp resistances and gradients on edges in addition
to ℓ22 resistances, and thus the main challenge is to simultaneously preserve their respective terms.

4.1

Tree-Portal Routing

Faced with a sparse graph or dense graph, we wish to move most edges onto a few vertices of a
tree, so that many of the remaining vertices are low degree and can be eliminated. Our high-level
approach is the same as Spielman-Teng [ST14], where we utilize a low-stretch spanning tree, and
move off-tree edges to a small set of portal nodes. However, when we reroute flow on the graph
where most edges are moved to be among a few vertices using the tree, we need to (approximately)
19

preserve three different properties
of the flow on the original graph, namely the inner product
P
P
between gradients and flows e g e f e , the 2-norm energy r e f 2e , and the ℓp -norm energy e f pe .
It turns out that we can move
P edges around on our graphs to produce a new graph while exactly
preserving the linear term e g e f e for flows mapped between one graph and the other. This means
any tree is acceptable from the point of preserving the linear term. To move edges around and
bound distortion of solutions w.r.t. the quadratic r e f 2e term, we use
stretch tree w.r.t. the r
Pa low
p
weights as resistances. This leaves us with little flexibility for the e fP
for large
e term. However,P
p, provided every p-th order term is weighted the same (i.e. we have s e f pe instead of e s e f pe ),
it turns out that, moving edges along any tree will result in bounded distortion of the solution,
provided we are careful about how we move those edges. Thus, we can move edges around carefully
to be among a small subset of the portal nodes while simultaneously controlling all linear, 2-nd order
and p-th order terms. But, this only works if all the p-th order terms are weighted the same. This
leads us to maintain uniform-weighted p-th order terms as an invariant throughout the algorithm.
The iterative refinement steps naturally weigh all p-th order terms the same provided the original
function does. However, our sparsification procedures do not immediately achieve this, but we show
we can enforce this uniform-weight invariant with only a manageable additional distortion of our
solutions. Elimination also does not naturally weigh all p-th order terms the same even in our case
when the original function does, but we can bound the distortion incurred by explicitly making the
weights uniform. Our tree-based edge re-routing naturally creates maps between solutions on the
old and new graphs.
We first formalize what we mean by moving off-tree edges. Suppose we have a spanning tree T
of a graph (V, E) and a subset set of nodes Vb ⊂ V designated as portal nodes, for any off-tree edge
e = {u, v} ∈ E \ T , there is a unique tree path PT (u, v) in T from u to v. We define a tree-portal
path PT,Vb (u, v), which is not necessarily a simple path.
Definition 4.1 (Tree-portal path and edge moving). Given spanning tree T and set of portal nodes
Vb , let e = {u, v} be any edge not in T , and PT (u, v) the unique tree path in T from u to v. We
define e’s tree-portal path PT,Vb (u, v) and e’s image under tree-portal edge moving as follows
1. If PT (u, v) doesn’t go through any portal vertex. In this case, we replace {u, v} with a distinct
self-loop of v. We let PT,Vb (u, v) be the path PT (u, v) followed by the self-loop at v.

2. If PT (u, v) goes through exactly one portal vertex vb. In this case, we replace {u, v} with a
distinct self-loop at vb. We let PT,Vb (u, v) be the tree path PT (u, vb) followed by the self-loop
at vb and then the tree path PT (b
v , v).
3. If Puv goes through at least two portal vertices. In this case, let u
b (and vb) be closest the
portal vertex to u (and v) on Puv , we replace {u, v} with a distinct edge3 {b
u, vb}. We let
PT,Vb (u, v) be the tree path PT (u, u
b) followed by the new edge from u
b to vb and then the tree
path PT (b
v , v).

This maps any off-tree edge e to a unique (edge or self-loop) eb given any T, Vb . We denote the
tree-portal edge moving with the map eb = MoveT,Vb (e).

Although we will get self-loops in tree-portal routing, to keep the discussion simple, we ignore
the possibility of getting self-loops. This still captures all the main ideas, and the algorithm/analysis
3

We will keep multi-edges explicitly between portal nodes.

20

extends to self-loops in a very straightforward but slightly tedious way. We discuss self-loops briefly
at the end of the section.
Tree-portal routing is a mapping from flow solutions on the original off-tree edges to a flow
solution (with the same residue) using the edges they are mapped to. Any flow along off-tree edge
(u, v) in the original graph is rerouted (again from u to v) using the tree-portal path PT,Vb (u, v)
instead. Rerouting the flow of any off-tree edge along its tree-portal path increases the congestion
on tree edges, which in turn incurs error in the ℓ22 and ℓpp terms in the objective function. We need
to pick the tree and portal nodes carefully to bound the error.
Definition 4.2. Given any graph (V, E), resistance r on edges, a spanning tree T , and a set of
portals Vb ⊂ V , for any e = {u, v} ∈ E, let eb = MoveT,Vb (e) and PT,Vb (u, v) be as specified above.
The stretch of e = {u, v} ∈ E \ T in the tree-portal routing is
def

StrT,Vb (e) =

1
re

X

r e′ ,

e′ ∈PT,Vb (e)\{b
e}

and the stretch of a tree edge e ∈ T is StrT,Vb (e) = 1. Note with our definition StrT,∅ (e) gives the
standard stretch.
The starting point is low stretch spanning trees [AN12], which provide good bounds on the total
ℓ22 stretch.
Lemma 4.3 (Low-Stretch Trees [AN12]). Given any graph G = (V, E) of m edges and n nodes,
as well as resistance r , LSST(r ) finds a spanning tree in O(m log n log log n) time such that
X
StrT,∅ (e) ≤ O(m log n log log n).
e∈E

We will construct a low-stretch spanning tree T of (V G , E G , r G ) using the above result. Still,
the error will be too large if we only use tree edges to reroute the flow of all the off-tree edges,
since the low average stretch doesn’t prevent one tree edge to be on the tree path for many off-tree
edges. Thus, we need to add portal nodes so we can shortcut between them to reduce the extra
congestion on tree edges.

4.2

Partitioning Trees into Subtrees and Portals

Next, we show how to find a small set of good portal nodes so that rerouting flow on off-tree edges
using their tree-portal paths incurs small error in the objective function. Pseudocode of this routine
is in Algorithm 3, and its guarantees are stated in Lemma 4.4 below.
Lemma 4.4. There is a linear-time routine FindPortals that given any graph G, a spanning
tree T , with m
b off-tree edges, and a portal count n
b ≤ m,
b returns a subset of Vb of n
b vertices so that
for all edges eb ∈ T , we have
X
10 X
StrT,∅ (e)
StrT,Vb (e) ≤
n
b e
e:b
e∈PT,Vb (e)

e : eb ∈ PT,Vb (e) ≤
21

10m
b
.
n
b

Algorithm 3 Find portal nodes for tree-portal routing
1: procedure FindPortal(T
n) P
 ,E,b

Str (e′ )
′
2:
∀e ∈ E : η(e) ← max StrT,∅ (e), e ∈E |E| T,∅
3:
Call decompose in [ST14] (page 881 of journal version) with (T, E, η, n
b).
4:
The subroutine breaks T into at most n
b edge-disjoint induced tree pieces to divide up the
η(e)’s roughly evenly so that the sum of η(e) for all e attached to each non-singleton piece is
not too big.
5:
The subroutine works by recursively
P cut off sub-trees from T whenever the sum of η(e) of
2 e η(e)
.
all e attached to a sub-tree is above
n
b
6:
Let Vb be the set of nodes where the tree pieces intersect.
This lemma will be a fairly straightforward using the tree decomposition subroutine (page 881
of journal version) from Spielman and Teng [ST14], which we include below for completeness.

Definition 4.5 ([ST14] Definition 10.2). Given a tree T that spans a set of S
vertices V , a T decomposition is a decomposition of V into sets W1 , . . . , Wh such that V =
Wi , Tthe graph
induced by T on each Wi is a tree, possibly with just one vertex, and for all i 6= j, |Wi Wj | ≤ 1.
Given an additional set of edges E on V , a (T, E)-decomposition is a pair ({W1 , . . . , Wh }, ρ)
where {W1 , . . . , Wh } is a T -decomposition and ρ is a map that sends each edge of E to a set or
pair of sets in {W1 , . . . , Wh } so that for each edge in (u, v) ∈ E,
1. if ρ(u, v) = {Wi }, then {u, v} ⊂ Wi , and
2. if ρ(u, v) = {Wi , Wj }, then either u ∈ Wi and v ∈ Wj or vice versa.
Theorem 4.6 ([ST14] Theorem 10.3). There exists a linear-time algorithm such that on input a
+
set of edges
P E on vertex set V , a spanning tree T on V , a function η : E → R , and an integer
1 < t ≤ e∈E η(e), outputs a (T, E)-decomposition ({W1 , . . . , Wh }, ρ), such that
1. h ≤ t

2. for all Wi such that |Wi | > 1,
X

e∈E:Wi ∈ρ(e)

η(e) ≤

4X
η(e)
t
e∈E

We can use the above theorem to show Lemma 4.4.
Proof of Lemma 4.4. We will apply Theorem 4.6 with t = n
b, and the function η will be
P

′ 
e′ ∈E StrT,∅ (e )
η(e) = max StrT,∅ (e),
m
b
P
P
Note by construction e η(e) ≤ 2 e′ ∈E StrT,∅ (e′ ). We get {W1 , . . . , Wnb } back, and let Ti be the
tree induced by T on Wi . Note the Ti ’s will be edge disjoint, and cover all tree edges of T . Our
set of portals will be the set of nodes that are in more than one of the Wi ’s, i.e. the nodes where
different Ti ’s overlap. Note the number of portals is at most the number of Ti ’s by an inductive
22

argument from any Ti that is a sub-tree in T . Such Ti have exactly one portal, and we can remove
Ti from T and continue the argument until all that remain in T is one sub-tree.
Consider any tree edge eb ∈ T , suppose it in Ti for some i. eb can only be on the tree-portal
routing for some edge {u, v} when Wi ∈ ρ(u, v). Note as Ti contains at least one tree edge, we
know |Wi | > 1, the second guarantee in Theorem 4.6 gives
!
P

′ 
X
X
4
e′ ∈E StrT,∅ (e )
′
2
max StrT,∅ (e),
≤
StrT,∅ (e )
m
b
t
′
e ∈E

e:Wi ∈ρ(e)

which directly gives the bounds we want in the lemma.

4.3

Graph Sparsification

Once we are able to move most of the edges onto a small subset of vertices, we wish to sparsify
the resulting dense graph over those vertices. This sparsification has to simultaneously preserve
properties of 1-st, 2-nd and p-th order terms, as well as the interactions between them, which turns
out to be challenging. We resort to expander decomposition which allows us to partition the vertex
set s.t. the edges internal to each subset form an expander and not too many edges cross the
partitions. Just having an expander graph is not enough to allow us to sample the graph due to the
need of preserving the linear terms. Thus, we also require that on each expander the orthogonal
projection of the gradient to the cycle space of the sub-graph has its maximum squared entry not
much larger than the the average squared entry. We refer to this as a uniform (projected) gradient.
We discuss how to obtain an expander decomposition that guarantees the projected gradients are
uniform in the expanders later in this overview. Given the uniform projected gradient condition,
we show that we can uniformly sample edges of these expanders to create sparsified versions of
them. We construct maps between the flows on an original expander and its sampled
P version that
work for any flow, not only a circulation. These maps preserve the linear term e g e f e exactly,
while bounding the cost of the 2-norm and ℓp -norm terms by relating them to the cost of the
optimal routing of a flow with the same demands and same gradient inner product, and showing
that optimal solutions are similar on the original expander and its sampled version. This strategy
resembles the flow maps developed in [KLOS14], and like their maps, we route demands using
electrical flows on individual expanders, but additionally we need create a flow in the cycle space
that depends on projection of the gradient onto the cycle space.
Tree-portal routing will give us an instance where all the off-tree edges are between portal nodes.
We can look at the sub-graph restricted to the portal nodes and the off-tree edges between them.
This graph has many fewer nodes comparing to the original graph but roughly the same number of
edges, and thus is much denser. We can then sparsify this graph to reduce the number of off-tree
edges similar to the construction of spectral sparsifiers. The main technical difficulty is that in the
sparsified graph, we still want the ℓpp terms in our objective function to have a same scalar s for
every edge, but similar to the case of how resistances are scaled in spectral sparsification, to preserve
the total value of the ℓpp terms, we would naturally want to scale s according to the probability we
sample an edge e. Thus, to get a same scalar s for all sampled edges, we are limited to uniform
sampling. We know uniform sampling works in expanders (c.f. [ST14, SS11] and [KLOS14, She13]
for ℓ2 and ℓ∞ respectively), so the natural approach is to first decompose the graph into expanders,
and sampling uniformly inside each expander. However, because of the presence of a gradient, we

23

need to be a bit more careful than even expanderdecomposition-based sparsification steps. Thus,
we work with uniform expanders.
Definition 4.7. A graph 4 G is a α-uniform φ-expander (or uniform expander when parameters
not spelled out explicitly) if
1. r on all edges are the same.
2. s on all edges are the same.
3. G has conductance5 at least φ.
4. The projection of g onto the cycle space of G, gb G = (I − BL† B ⊤ )g , is α-uniform (see next
definition), where B is the edge-vertex incidence matrix of G, and L = B ⊤ B is the Laplacian.

Definition 4.8. A vector y ∈ Rm is said to be α-uniform if
kyk2∞ ≤

α
ky k22 .
m

We abuse the notation to also let the all zero vector 0 be 1-uniform.
In Section 5 we show how to decompose the graph consisting of portals and the off-tree edges
between them into vertex disjoint uniform expanders such that more than half of the edges are
inside the expanders.6
Theorem 4.9 (Decomposition into Uniform Expanders). Given any graph/gradient/resistance
instance G with n vertices, m edges, unit resistances, and gradient g G , along with a parameter δ,
Decompose(G, δ) returns vertex disjoint subgraphs G1 , G2 , . . . in O(m log7 n log2 (n/δ)) time such
that at least m/2 edges are contained in these subgraphs, and each Gi satisfies (for some absolute
constant cpartition ):
1. The graph (V Gi , E Gi ) has conductance at least
φ=
and degrees at least φ ·

m
3n ,

1
,
cpartition · log3 n · log(n/δ)

where cpartition is an absolute constant.

2. The projection of its gradient g Gi into the cycle space of Gi , gb Gi satisfies one of:
(a) gb Gi is O(log8 n log3 (n/δ))-uniform,


gb Ge i

2

O log14 n log5 (n/δ)
≤
mi

Here mi is the number of edges in G Gi .
4



gb Gi

2
2

∀e ∈ E(Gi ).

We use an instance and its underlying graph interchangeably in our discussion.
r are uniform, so conductance is defined as in unweighted graphs. We use the standard definition of conductance.
δ(S)
For graph G = (V, E), the conductance of any ∅ =
6 S ( V is φ(S) = min(vol(S),vol(V
where δ(S) is the number
\S))
of edges on the cut (S, V \ S) and vol(S) is the sum of the degree of nodes in S. The conductance of a graph is
φG = minS6=∅,V φ(S).
6
Some of the expanders we find actually won’t satisfy the projected gradient being α-uniform constraint (case 3(b)
in Theorem 4.9). For those expanders, the projection of the gradient in the cycle space is tiny so we make it 0. This
leads to the additive error in Theorem 3.6.
5

24

(b) The ℓ22 norm of gb Gi is smaller by a factor of δ than the unprojected gradient:
gb Gi

2
2

≤ δ · gG

2
.
2

Moreover, the min degree of any node in the expanders is up to a polylog factor close to the
average degree. For the off-tree edges not included in these uniform expanders, we work on the
pre-image7 of them in the next iteration. That is, for any edge eb inside one of the expanders, we
remove its pre-image from the instance G, and work on the remaining off-tree edges in G in the
next iteration. This iterative process terminates when the number of remaining off-tree edges is
e G |/κ)). This takes O(log |E G |) iterations as a constant fraction of off-tree
small enough (i.e. O(|E
edges are moved to be inside the expanders each iteration.
Sparsify Uniform Expanders If we append a column containing the gradient of edges to the
edge-vertex incidence matrix B, the conditions of a α-uniform φ-expander is equivalent to each
−1
where n, m are number of nodes and edges. An
row of B having leverage score at most nαφ
m
underlying connection with the ℓp -norm row sampling result by Cohen and Peng [CP15] is that this
is also a setting under which ℓq -norm functionals are preserved under uniform sampling. We refrain
from developing a more complete picture of such machinery here, and will utilize ideas closer to
routing on expanders [KM09, KLOS14] to show a cruder approximation in Section D.
Theorem 4.10 (Sampling Uniform Expanders). Given an α-uniform φ-expander G = (V G E G , r G , sG , g G )
with m edges and vertex degrees at least dmin , for any sampling probability τ satisfying


1
α
+
,
τ ≥ csample · log n ·
m φ2 dmin
where csample is some absolute constant, SampleAndFixGradient(G, τ ) w.h.p. returns a partial
instance H = (H, r H , sH , g H ) and maps MG→H and MH→G . The graph H has the same vertex set
as G, and H has at most 2τ m edges. Furthermore, r H = τ · r G and sH = τ p · sG . The maps MG→H
and MH→G certify
H κ G and G κ H,
where κ = m1/(p−1) φ−9 log3 n.

4.4

Ultra-sparsification Algorithm and Error Analysis

Now we put all the pieces together. We need to show that adding together our individual sparsifiers
results in a sparsifier of the overall graph. This is fairly immediate given the strong guarantees
we established on the individual graphs. We also need to be able to repeatedly decompose and
sparsify enough times that the overall graph becomes sparse. To address this issue, we use ideas
from [KMP11] that suggest scaling up the tree from the tree routing section limits the error incurred
during sampling. Here it again becomes important that because we rely on [SW18], we know exactly
which edges belong to a sparsifier. This guarantee limits the interaction between sparsification of
different expanders.
7

By pre-image of eb we mean the original off-tree edge e that gets moved to eb in the tree-portal routing, i.e.
e = Move−1
e).
b (b
T,V

25

After constructing a low-stretch spanning tree T , we round each r Ge of off-tree edges e ∈ E G \ T
down to the nearest power of 2 (can be less than 1) if r Ge ≥ δ, and round to 0 otherwise. This gives
kr G k
of log δ ∞ bucket of edges with uniform resistances, and we work with one bucket of edges at
a time, since the edges in a uniform expander need to have uniform r e . If G ′ is the instance of G
after rounding the resistance of off-tree edges, it is easy to see the following error guarantee.
′

Lemma 4.11. G 1 G ′ with the identity mapping, , and for any flow solution f G of G ′ , again
using the identity mapping, we have
′
′
1
1 ′
EG ( f G ) ≥ EG ′ (f G ) − δ f G
2
2

2
2

.

To avoid using too many symbols, we reuse G to refer to the original instance after the resistance
rounding (i.e. the G ′ above). Denote E r the subset of edges in E G \ T containing edges with r e = r
kr G k
for some particular r, note there are at most log δ ∞ possible value of r. We work iteratively
on the set E r , starting with E0r = E r . In the i-th iteration, we use FindPortal(T, Eir , m/κ)
(Lemma 4.4) to find a set of m/κ portal vertices for the edges remaining in Eir , note the lowstretch spanning tree is fixed through the process, but each iteration we find a new set of portals
using FinalPortal as introduced in Section 4.2.
br to be the graph of the m/κ
We then move edges in Eir using the tree-portal routing. We let G
i
br is |E r | and the
portal nodes and the off-tree edges between them. Note the number of edges in G
i
i
number of nodes is m/κ.
br , and these values will depend
So far we haven’t specified the r , s, g values on the edges in G
i
br . For now we focus on discuss the
on the tree-portal routing as well as the average degree in G
i
br . We will come
edge set in our final sparsified instance, and assume we have r , s, g values for G
i
back to specify these quantities later.
br to compute a collection of vertex disjoint
We use Decompose (Theorem 4.9) on the graph G
i
r
r
b
b
br are inside these sub-graphs. We
sub-graphs {Gi,1 , Gi,2 , . . .}, and at least half of the edges in G
i
br to be the edges contained in these sub-graphs, and E
e r be the set of pre-images of edges in
let E
i
i
b r (in terms of tree-portal off-tree edge moving). We remove E
e r from E r and proceed to iteration
E
i
i
i
e
i + 1. If at the beginning of some iteration i, the size of Eir is at most O(m/κ),
we leave them as
r
as the set containing them. Note for any r, the iterative process
off-tree edges, and denote Elast
must finish in O(log κ) iterations as we start with |E r | < m edges. We do this for all r.
So far any edge in the original instance G we get either (1) a tree edge in T , or (2) an off-tree
br for some resistance value r, iteration i, and j-th expander computed in that iteration,
edge in a G
i,j
or (3) an off-tree edge remaining in Erlast for some resistance value r. There are n − 1 edges in
e
group (1), and O(m/κ)
edges in group (3), so we can keep all these edges in the ultra-sparsifier H.
br to get a sparsified
For the off-tree edges in group (2), we uniformly sample the edges in each G
i,j
r
br (i.e. case
graph H i,j . Technically our sampling result only applies to an α-uniform φ-expander G
i,j
r
b
3(a) in Theorem 4.9). If the Gi,j we get back from Decompose is in case 3(b) of, we perturb
the gradient on edges so that the projection of the gradient to the cycle space of the expander is
br after
0, i.e. project the gradient to the space orthogonal to the cycle space. Then we have G
i,j
perturbation is a 1-uniform φ-expander.
The edges in our final ultra-sparsifier H will be the tree edges in T , the off-tree edges in the
r
r
Elast ’s over all resistance bucket value r, and the off-tree edges in the H i,j ’s over all resistance r,
26

r

iteration i and expander j. We argued about the size of all but the edges in the H i,j ’s, which we
will do now.
br to get H ri,j
Sampling Probability We first specify the probability we sample each edge in G
i,j
which we denote by τr,i (same across all the expanders, i.e. j’s, for any resistance r and iteration
−2 + αm−1 ). Here
i). By Theorem 4.10 we need the probability to be at least csample log n(d−1
min φ
br from Theorem 4.9 allow us
csample is a fixed constant across all r, i, j’s, and the guarantees on G
i,j
−2
to use some fixed polylogn as φ and α across all r, i, j’s. The only parameter that varies across
br , which by Theorem 4.9 is
different r, i’s is dmin , a lower bound on the minimum vertex degree in G
i,j
br . As there are mr,i edges and m/κ nodes
within a (fixed) polylog factor of the average degree in G
i
br , the average degree is mr,i κ/m. Thus, we can write τr,i = c1 m logc2 n for some global constants
in G
i
κmr,i
c1 , c2 , and since both mr,i and κ is at most m, τr,i satisfies the requirement on τ in Theorem 4.10.
br and the
With this particular choice of τr,i we can use SampleAndFixGradient to sample G
i,j
the guarantees from Theorem 4.10. Now we can prove the statement about the number of off-tree
edges in H.
r
e m ) with high probability.
Lemma 4.12. The total number of edges over all H i,j ’s is O(
κ

br with uniform r ,
Proof. Pick any r, i, recall when we call Decompose in that iteration, we have G
i
r
mr,i = |Ei | edges and ni = m/κ nodes. From the previous discussion of the sampling probability,
br with probability
we know it is sufficient to call SampleAndFixGradient on G
i,j
τr,i =

c1 m logc2 n
κmr,i

r

for some constants c1 , c2 . By Theorem 4.10, the number of edges in H i,j ’s over all j is at most
br ’s contain Θ(mr,i ) edges.
e m ) with high probability since over all j the G
Θ(
i,j
κ
kr G k
Since for each r the number of iterations is at most i ≤ log κ, and there are log δ ∞ possible
r values, the final bound in the lemma follows from summing over all r, i. Note we can hide all
log factors as log n factors by our assumption in Theorem 3.6 that log r G ∞ and log 1δ are both
polylog in n.
Now we discuss the g , r , s values we put on the edges in all the steps. Note we need the
final instance H to have a uniform scalar sH for every |fe |p term, so we can recursively optimize
the instance. However, in the intermediate steps, we will divide the instance into sub-instances
br ’s, and later combine sub-instances induced by
induced by the different subsets of edges, e.g. G
i,j
r
the sampled sub-graphs H i,j ’s to get H. Each of these sub-instances will have its own scalar, e.g.
H
sG
r,i , sr,i , but in general they won’t necessarily have the same value across different sub-instances.
Notation-wise, in the following discussion, we assume each edge has its own scalar s e associated
with the term |fe |p in the intermediate instances. Eventually, the different scaling we do to s in
the intermediate steps will cancel so that in H we have the scalar sH . The input G has a uniform
scalar sG , and we will make sH = sG .
Now we specify the g , r and s values of the edges in the final instance H as well as in some of
the key intermediate sub-instances we consider.
27

Algorithm 4 Producing Ultra-Sparsifier H with unit sH = sG
1: procedure UltraSparsify(G, κ,δ)
2:
T ← LSST(r G) . (low-stretch spanning tree)
3:
Initiate H with T , and the identity flow mapping.
4:
Round r G down to nearest power of 2, or 0 if less than δ
5:
n
b ← m/κ (number of portal nodes per batch)
6:
for Each bucket of resistance value r do
7:
Let i ← 0, E r ← {e|e ∈ E G \ T, r Ge = r}
e
8:
while E r has more than O(m/κ)
off-tree edges do
9:
Let mr,i be the number of edges in E r .
10:
Find n
b portal nodes to short-cut tree routing:
11:
12:
13:

14:
15:
16:
17:

Vb ← FindPortal(T, E r , n
b).

Route edges in E r along T , using portal nodes to short-cut tree-portal routing:
br ← TreePortalRoute(E r , T, Vb ),,
G
i
Decompose the graph after tree-portal routing into uniform expanders:
n
o


br , G
br , . . . ← Decompose G
br , δ/m5 .
G
i,2
i,1
i
br , G
br , . . . from E r .
Remove the pre-image of edges in G
i,1
i,2
c1 m logc2 n
r
b
Set τr,i ← mr,i κ (for sampling Gi,j ’s in SampleAndFixGradient)
br do
for each G
i,j
Rescale the gradients and ℓpp scalar as
br

(10)

br

(11)

r Gi,j = rκ log2 n
−p
· sG
sGi,j = τr,i

18:
19:
20:
21:
22:
23:
24:



r
br , τr,i .
Let H i,j ← SampleAndFixGradient G
i,j
r

r

br and H
Add H i,j to H, and incorporate the flow mappings between G
i,j
i,j
r
b
(composed with the tree-portal routing between Gi,j and
its pre-image) to the mapping between G and H.
i←i+1
Add all remaining edges of E r to H with the identity flow mapping on them
3/(p−1) ).
e
return H, MH→G , and κH→G = O(κm

28

Algorithm 5 Tree-Portal Routing of Edges
b)
1: procedure TreePortalRoute(E,T ,V
b
2:
Initialize E ← ∅
3:
for each e = {u, v} ∈ E do
4:
Let eb ← MoveT,Vb (e), and PT,Vb (u, v) be its tree-portal path.
5:
(See Definition 4.1)
6:
Let r eb,s be be the same as r e and s e .
7:
Set g eb so that sending 1 unit of flow from u to v along PT,Vb (u, v) has the same flow
dot gradient as g e , i.e. the flow dot gradient of sending directly along e. Note all edges on
PT,Vb (u, v) other than eb have known gradients.
b with g eb, r eb, s eb as specified. Note E
b may contain multi-edges.
8:
Add eb to E
b
return E
1. e ∈ T : The gradient, resistance and s e on these edges in H remain the same as in G, that is
G
G
H
G
gH
e = g e , r e = r e , and s e = s .
r : These off-tree edges remain at the end for each bucket E r . We keep their gradient,
2. e ∈ Elast
resistance, and s e = sG as in the original instance.

br , the j-th expander computed in iteration i for resistance r: In the intermediate
3. eb ∈ G
i,j
br , we have r be = rκ log2 n, s be = τ −p sG . For the gradient on eb,
sub-instance induced by G
i,j
r,i
recall eb is the image of some off-tree edge e under the mapping Move b r where Vb r is the set
T,Vi

i

of portals in the i-th iteration for resistance r. Under the tree-portal routing, any flow along
e = (u, v) will be rerouted along the tree-portal path PT,Vb r (u, v). We want the linear term
i
(i.e. gradient times flow) in the objective function to remain the same under this rerouting,
so routing 1 unit of flow from u to v along PT,Vb r (u, v) should give the same dot product with
i

the gradients as routing 1 unit of flow from u to v along e in the original instance (i.e. g Ge ). As
the only off-tree edge on the tree-portal path is eb, and we are keeping the original gradients
on all the tree edges, this uniquely determines g eb.
r

4. e ∈ H i,j for some r, i, j: As specified in Theorem 4.10, if the edge e is sampled (with uniform
br scaled up by τr,i and τ p
probability τr,i ), and r e ,s e are their corresponding values in G
i,j
r,i
p
−p
respectively. In particular we get back s e = sG as the τr,i
scaling cancels the τr,i
scaling in
br .
G
i,j

Note all the edges in our H (i.e. group 1, 2, 4 above) end up with the same scalar sH = sG .
Now we bound the approximation error. For simplicity, we carry out the analysis ignoring the
additive errors in the bound, and defer the discussion of them to the end. In particular, additive
errors come in at two cases. The first is when we round an original resistance to 0 when it is less
than δ, and the second is in Decompose, we may get an expander whose projected gradient is not
e
O(1)-uniform
but has tiny norm (i.e. case 3(b)), and we zero out its projection to the cycle space
before sampling. For now we assume we don’t have these cases.
We summarize the notations in our algorithm and analysis in Table 1. We explicitly point out
whenever we change the gradient, resistance or s value on an edge. We will use instances and their
29

Table 1: Glossary of Notations in Algorithm and Analysis.
G
T
Er
Eir
br
G
i
mr,i
br
G
i,j

br
E
i
er
E
i
Erlast
τr,i
r
H i,j

G

Notations in Ultrasparsify

Input instance with V G , E G , g G , r G , sG .
Low stretch spanning tree of G (stretch with respect to r G ).
All in E G \ T whose resistance after rounding is r.
The remaining edges in E r at the i-th iteration of tree-portal routing E r .
The image of edges in Eir by the mapping MoveT,Vb , i.e. moving off-tree edges along
br are set to preserve the linear flow dot
tree-portal path. The gradients of edges in G
i
gradient term under tree-portal routing.
br ).
The number of edges in Eir (also the size of G
i
br . Edges keep their gradients from
The j-th expander we get from decomposing G
i
br , and r , s are scaled.
G
i
br (i.e. over all j’s).
The union of edges contained in the expanders G
i,j
r
b .
The pre-image of edges in E
i
The set of edges remaining in E r after the last iteration for r.
br .
The probability we use in SampleAndFixGradient to uniformly sample G
i,j
br computed by SampleAndFixGradient. g , r , s on
The sparsified graph of G
i,j
edges are computed by the subroutine.
Additional notations in the analysis
P r
P er
The instance with the same edge set as G. Note† E G = T + r Elast
+ r,i E
i . Edges
P er
in G has the same g , r , s as in G except for those in
E . For any resistance r
r,i

r

Gi
Grest
†

i

er has the same gradient as in G, but r e = rκ log2 n, se = τ −p sG
and iteration i, e ∈ E
i
r,i
are scaled.
er .
The instance G restricted to the set of edges in E
Pi r
The instance G restricted to the set of edges in r Elast .

We use addition on sets as union but signify that the sets are disjoint.

30

underlying graphs interchangeably, and when we refer to a subgraph as an instance, it will be clear
what are the g , r , s values for the instance.
e r (i.e. e will
First we let G be the instance on the same nodes and edges as G, but for any e ∈ E
i
br ), we rescale the resistance and s to be r e = rκ log2 n, and s e = τ −p sG
be mapped to some eb in G
i,j
r,i
. Note the gradient of e in G stays the same as in G. We first bound the approximation error
between G and this rescaled instance G.
Lemma 4.13. G O(m
e 1/(p−1) κ) G 1 G with the identity mapping in both directions.

p
G
G
G
Proof. For any edge e, we have g G
e = g e . As to the ℓp scalar, we have either s e = s e , or if e is
br then
eventually moved to some G
i,j

sG
e

=

−p G
τr,i
s

=



mr,i κ
c1 m logc2 n

p

sG

e
as mi,r ≥ Om/κ
or otherwise we would have stopped for resistance value r, we can assume mr,i κ ≥
c2
c1 m log n so
p/(p−1) p−1 G
) s e ≤ (m1/(p−1) κ)p−1 s Ge
s Ge ≤ s G
e ≤ (κ
where the second inequality is by mr,i ≤ m, and the third inequality is by κ < m. Similar calculation
2
G
gives r Ge ≤ r G
e ≤ κ log n · r e . Our result directly follow by Lemma 2.13.
r

Now we break G into sub-instances induced on the disjoint edge sets. Let E i be the instance
e r , T the instance restricted to the tree edges, and Grest the instance
of G restricted to edges in E
i
r ’s. When use addition as union on sets when the sets are
restricted to edges in any of the Elast
disjoint. The objective of the sum of two instances is simply the sum of the individual instances
objectives.
Lemma 4.14. For any resistance value r, round i, we have
r
br e 1/(p−1) T + E r
T + E i O(m
e 1/(p−1) ) T + Ei O(m
i
)

where the flow mapping is the tree-portal routing and its reverse.

Proof. Fix any resistance value r and iteration i, the set of remaining off-tree edges of resistance
r in iteration i is Eir , and these edges have a total stretch at most O(m log n log log n) with T by
Lemma 4.3, and Eir = mr,i . As we use FindPortal to get a set of m/κ portal nodes Vb in that
iteration, by Lemma 4.4, for any edge e′ on T , we have in T + Eir
def

We ′ =

X

e∈Eir :e′ ∈PT,Vb (e)

StrT,Vb (e) ≤

and

10 X
StrT,Vb (e) ≤ 10κ log n log log n ≤ κ log2 n
n
b
r
e∈Ei

def

Ke′ = e ∈ Eir : e′ ∈ PT,Vb (e) ≤

10κmr,i
10m
b
≤
n
b
m

r
br . Let f be the flow in T + E r , and fb be
We first look at the direction from from T + E i to T + E
i
i
the tree-portal routing of f . In the tree-portal routing, flow on tree edges is mapped to the same
r
flow, while any flow along an off-tree edge e = (u, v) ∈ E i is rerouted along the tree-portal path

31

br is the image
PT,Vb (u, v). This rerouting clearly preserves the residue between f , fb, and if eb ∈ E
i
r
b
of (u, v), its gradient g eb in Ei is by construction set to be the value which preserves the linear
term in the objective function for f and fb. The cost of ℓ22 and ℓpp terms for fe is the same as the
corresponding costs for fb , since eb is only used for the rerouting of e (so |fb | = |fe |), and they
e
b

eb

have the same r , s values. Thus, the contribution to the ℓ22 , ℓpp terms in objective function from the
off-tree edges are the same for f and fb. The only extra cost comes from the ℓ22 and ℓpp terms of tree
edges for fb since we put additional flow through them. First consider the sum of the ℓpp terms over
b r . Recall we don’t scale the s value for tree edges, so the scalar is still
all tree edges for fb in T + E
i


r

sG on tree edges, while for off-tree edges in E i , the value s is scaled to be

p
κmr,i
sG
c2
c1 m log n

p

X

e′ ∈T

sG fbe′

p

=

=

X

=

sG

fe

e′ ∈T

e:e′ ∈PT,Vb (e)

X

X

e′ ∈T

≤

X

X

e′ ∈T

X

e′ ∈T

sG Kep′
sG Kep′

e:e′ ∈PT,Vb (e)

X

e:e′ ∈PT,Vb (e)

sG Kep−1
′

X
|f e |p
≤
e

p

X

1
f
Ke′ e
1
|f |p
Ke′ e

(Using Jensen’s inequality)

|f e |p

e:e′ ∈PT,Vb (e)

X

sG Kep−1
′

e′ ∈PT,Vb (e)

X
≤
|f e |p m · sG Kep−1
′

(Tree- portal path’s length < m)

e

≤
ℓpp

X
(10c1 logc2 n)p−1 m · s e |f e |p
e

So the
term goes up by at most a factor (10c1 logc2 n)p−1 m. Similar calculation shows that
the ℓ22 term goes up by at most a constant factor by the tree-portal routing. Thus, we get
r
br
T + E i O(m
e 1/(p−1) ) T + Ei . The other direction is symmetric using the reverse tree-portal routing, and the calculation stays the same since the tree-portal routing in reverse incurs the same
load/congestion on tree edges.
r

If we put the Gi over all resistance r’s and round i’s together, we get
Lemma 4.15.
T+

X
r,i

r

Gi O(m
e 1/(p−1) ) T +

X
r,i

b r  e 1/(p−1) T +
E
i
O(m
)

X

r

Gi

r,i

The sum is over all possible resistance value r’s, and over all iterations i for r.
Proof. By Lemma 4.14 and Lemma 2.12 we have
[
[
[
r
r
bir  e 1/(p−1)
T + Gi
T +E
T + Gi O(m
e 1/(p−1) )
O(m
)
r,i

r,i

r,i

32

r

b r ’s) are disjoint for different resistance values or different iterations, thus
Note the Gi ’s (and the E
i
these edges contribution to the objective function value simply adds up. For the tree edges, since
there are at most log2 n different pairs of resistance and iteration pairs, we have
[
T + log2 n T
T 1
r,i

by considering the mapping that split flow on one tree edge to log2 n copies of it and the reverse
mapping of merging. Note |a1 |x + . . . + |a1 |x ≤ (|a1 | + . . . + |ak |)x ≤ k(|a1 |x + . . . + |a1 |x ). This
gives the final result we want.
P
r
Note that G is the disjoint union of T + r,i Gi + Grest, while H is the disjoint union of
P
r
T + r,i H i + Grest . Thus, we can show the following
Lemma 4.16. G O(m
e 2/(p−1) ) G.
e 2/(p−1) ) H O(m

br is the j-th uniform expander we
Proof. Recall for each resistance value r, in the i-th round, G
i,j
r
br .
find, and H i,j is the sparsified graph of G
i,j
G = T + Grest +

X

r

Gi

r,i

O(m
e 1/(p−1) ) T + Grest +
= T + Grest +

X
r,i,j

br
G
i,j

O(m
e 1/(p−1) ) T + Grest +
=H

(valid as the sets are disjoint)

X
r,i

X
r,i,j

bir
E

(Lemma 4.15)

br
H
i,j

(By Theorem 4.10, and sets being disjoint)

br is the disjoint union of G
br over all j)
(E
i
i,j

G O(m
e 2/(p−1) ) H follows by taking the composition of all the intermediate steps, and multiplying
the approximation error by Lemma 2.10. The other direction is similar.
Now we can prove the main ultra-sparsification theorem.
Proof of Theorem 3.6. Other than the additive error terms and the self-loops, everything in the
theorem statement follow directly from Lemma 4.12 (the number of off-tree edges), and composition
of Lemma 4.13 with Lemma 4.16 (the approximation error). We explicitly spell out the flow
mappings between G and H. We start with the G to H direction. We break the flow in G as the
er , specify the mapping from each piece to H, and
sum of flow on disjoint edge subsets T ,Grest , and E
i
later take the sum of the mappings. The mapping from T and Grest to H is just the identity. For
er , we get a flow on T + E
br by tree-portal routing. As E
b r is the sum of G
br ’s, for the flow
flow on E
i
i
i
i,j
br , we map it to a flow on H ri,j using the flow mapping in SampleAndFixGradient.
mapped to G
i,j
br to a flow on H, and
We add these mapping over all j’s to get a mapping from the flow on T + E
i
er to H. Summing over
take the composition with the tree-portal routing to get a mapping from E
i
all r, i (together with the identity on T and Grest gives the mapping from G to H. The mapping
33

b r to T + E
er we use the reverse of tree-portal
from H to G is symmetric, and in the part from E
i
i
routing.
All the subroutines take nearly linear time, and we have at most log n different r, and for each
e
r there are at most log m iterations, so the overall running time is O(m).
The flow mappings can
e
also be applied in O(m) time, and they are linear maps.
Now we look at the additive error terms. In particular, additive errors come in at two places.
The first is when we round an original resistance to 0 when it is less than δ, and we have Lemma 4.11
to bound the error (at that step). The second place is in Decompose (Algorithm 6), we may get
br whose projected gradient is not α-uniform but has tiny norm (i.e. case 3(b)), and
an expander G
i,j
we zero out its projection to the cycle space before sampling to make it 1-uniform. If we have a
br , the additive error is in the linear term, and is equal to the dot product
flow f on such an G
i,j
br ,G
br respectively,
of f with the removed gradient. We let g ri , g ri,j be the gradient on edges in G
i
i,j
br (and G
br ). We remove gb r
and gb ri , gb ri,j as the projection of g ri (and g ri,j ) to the cycle space of G
i,j
i
i,j
from the gradient g ri,j when gb ri,j ≤ δ′ gb ri for some parameter δ′ , so the additive error we introduce is
f T gb ri,j , which is at most kf k2 gb ri,j 2 , which is in turn at most δ′ kf k2 kg ri k2 as gb ri is a projection of
g ri . Now we look at how this additive error propagates in terms of the overall approximation error
between G and H. We will get an additional factor m when we combine the additive errors over all
the individual expanders where we carry outP
this perturbation.
Note
Pwe are not really introducing
P
√
kf
k
≥
k
more error here, but simply because mk i fi k ≥
i
i fi k when fi ’s have disjoint
i
support and total size m. The additive error is also amplified through the intermediate steps,
but since the multiplicative approximation errors are mO 1/p, we lose at most another polynomial
factor. Additional polynomial factor comes in because the norm of the gradient vector after treerouting can be off by a polynomial factor comparing to the norm of the original gradient. However,
overall the blowup is at most polynomial, and we use a polynomially smaller δ′ in Decompose to
accommodate these factors to get the additive error in our final result. The same argument applies
to the additive error introduced by resistance rounding (e.g. round to 0 when the gradient is at
most δ/mc for some large enough c).
We brief go over the case when tree-portal routing gives self-loops. We treat self-loops the same
way as the edges that are in the uniform expanders except they don’t go through the expander
decomposition and sampling steps. Once we get a self-loop eb from tree-portal routing of some
edge e ∈ E G , we add eb to H, where the gradient on eb is set (the same way as non self-loops) to
preserve the flow dot gradient term under tree-portal routing. We remove its pre-image e from Eir ,
but if in some iteration, more than half of the edges in Eir are mapped to self-loops by tree-portal
routing, we skip the decomposition and sampling steps also for other edges, as we don’t have a
dense enough graph between the portal nodes to sparsify. We still have the size of Eir drop by at
least 1/2 across each iteration as before. The final caveat is that since self-loops don’t go through
SampleAndFixGradient, and thus their s values are not scaled to be the same as the rest of
the edges in H. This is not an issue because we will remove them from the instance and optimize
them individually (see Lemma 3.5), so they won’t exist in the instance that we recursively solve,
so uniform s scalar is not required for them.

34

5

Decomposing into Uniform Expanders

In this section we prove our decomposition result necessary for finding large portions of edges that
can be sampled. This and the subsequent sampling step in Appendix D are critical for reducing the
number of edges between portal vertices, after they were routed there in Line 12 of UltraSparsify
(Algorithm 4). The main algorithmic guarantees can be summarized as below in Theorem 4.9.
Theorem 4.9 (Decomposition into Uniform Expanders). Given any graph/gradient/resistance
instance G with n vertices, m edges, unit resistances, and gradient g G , along with a parameter δ,
Decompose(G, δ) returns vertex disjoint subgraphs G1 , G2 , . . . in O(m log7 n log2 (n/δ)) time such
that at least m/2 edges are contained in these subgraphs, and each Gi satisfies (for some absolute
constant cpartition ):
1. The graph (V Gi , E Gi ) has conductance at least
φ=
and degrees at least φ ·

m
3n ,

1
,
cpartition · log3 n · log(n/δ)

where cpartition is an absolute constant.

2. The projection of its gradient g Gi into the cycle space of Gi , gb Gi satisfies one of:
(a) gb Gi is O(log8 n log3 (n/δ))-uniform,


gb Ge i

2

O log14 n log5 (n/δ)
≤
mi

Here mi is the number of edges in G Gi .



gb Gi

2
2

∀e ∈ E(Gi ).

(b) The ℓ22 norm of gb Gi is smaller by a factor of δ than the unprojected gradient:
gb Gi

2
2

≤ δ · gG

2
.
2

We will obtain the expansion properties via expander decompositions. Specifically we will invoke
the following result from [SW18] as a black box.
Lemma 5.1. There is a routine ExpanderDecompose that when given any graph G and any degrees d such that d u ≥ degG (u) for all u, along with a parameter 0 < φ < 1, ExpanderDecompose(G, d , φ)
returns a partition of the vertices of G into V1 , V2 , . . . in O(mφ−1 log4 n) time such that
P G[Vi ] has
conductance at least φ w.r.t. d u , and the number of edges between the Vi s is at most O( u d u φ log3 n).
Note that we explicitly introduce the d vector containing the degrees of the initial graph because
we will repeatedly invoke this partition routine. This is due to our other half of the routine, which
is to repeatedly project g among the remaining edges, and removing the ones that contribute to
too much of its ℓ22 -norm in order to ensure uniformity as given in Case 2a of Theorem 4.9. To see
that this process makes progress, we need the key observation from Lemma 2.15 that projections
can only decrease the ℓ22 norm of gb , the projection of the gradient.
35

Algorithm 6 Decomposition into Uniform Expanders
1: procedure Decompose(G, δ)
2:
Set φ ← cpartition log3 n log(1/δ) for some absolute constant cpartition .
m
3:
Iteratively remove all vertices with degree less than 10n
to form Glarge .
4:
Compute d , the degrees of Glarge
5:
Return RecursiveDecompose(Glarge , φ, 1, log(n/δ)).
Algorithm 7 Recursive Helper for Decomposition
Compute the projection of g G into its cycle space, gb G .
2: procedure DecomposeRecursive(G, d , φ, i, L)
1:

3:

4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

Form Gtrimmed by removing all edges e ∈ E G such that (b
g Ge )2 ≥

10L
mG

· gb G

Gtrimmed , E Gtrimmed ), d

2
2

.

(G1 , G2 , . . . , Gt ) ← ExpanderDecompose((V
V Gtrimmed , φ).
G
Initialize collection of results, P ← ∅.
for i = 1 . . . t do
Form Gi from the edges in G corresponding to Gi
Compute gb Gi , the projection of g (Gi ) onto its cycle space.
g G k22 and mGi ≥ mG /2) then
if i = L or (kb
g Gi k22 ≥ 12 kb
Add Gi to the results, P G ← P G + Gi .
else
Recurse on P G : P G ← P G + DecomposeRecursive(Gi , d , φ, i + 1, L).
Return P G .

This leads to an approach where we alternate between dropping the edges with high energy, and
repartitioning the remaining edges into expanders. Pseudocode of this routine is in Algorithm 6,
which calls a recursive routine, DecomposeRecursive shown in Algorithm 7 with a suitable value
of φ and number of layers. Note that we also need to trim the initial graph so that we only work
with large degree vertices.
We will also need the following result (Lemma 28 of [KLOS14], see also [KM11]).
Lemma 5.2. Suppose G is a unit weight graph with conductance φ. Then the projection operations
into cycle and potential flow spaces both have ℓ∞ norms bounded by O(φ−2 log n):
†
 ⊤
⊤
BG BG BG BG
≤ O(φ−2 log n)
∞

and

†
 ⊤
⊤
I − BG BG BG BG

∞

≤ O(φ−2 log n).

Proof. (of Theorem 4.9)
We start by bounding the qualities of the G pieces returned. As we only return pieces that are
the outputs of ExpanderDecompose, all of them have conductance at least φ by Lemma 5.1.
Also, since we only keep the non-trivial pieces containing edges, taking the singleton cuts gives that
the degrees in these pieces are at least
φ · du ≥

m
1
m
=
.
·
3
3
2n cpartition log n · log(n/δ)
10n log n log(n/δ)
36

Now consider the quality of each gb Gi : if it was returned due to i = L, then the energy of the
projected gradient must have been halved at least L − log n times, or by a factor of 2L−log n =
2log(1/δ) = 1/δ. Thus we would have
gb Gi

2
2

2

≤ δ gb G

2

2
.
2

≤ δ gG

Otherwise, we must have terminated because both the energy and edge count did not decrease
too much. An edge e was kept in the trimmed set only if


gb Ge

2

≤

10L G
gb
mG

2
2

≤

20L G
gb
mGi

Combining this with the termination requirement of gb Gi

the pre-projection gradient on Gi , gb GE Gi satisfies
gb GE Gi

2

∞

≤

40L Gi
gb
mGi

2
2

2
2

≥

2
2
1
2

.
gb G

2
2

gives that the ℓ∞ norm of

.

On the other hand, because Gi has expansion φ, doing an orthogonal cycle projection on it can only
increase the ℓ∞ -norm of a vector by a factor of O(φ−2 log n) by Lemma 5.2. Thus we have
gb

Gi

2
∞

=




†
 ⊤
G
G⊤
G
gb GE Gi
B
I −B B B
G


≤ O φ−4 log2 n · gb GE Gi

2

∞

2
∞

†
 ⊤
⊤
≤ I − BG BG BG BG


≤ O φ−4 log2 n · O



L
mGi



gb Gi

2
2

=

O log

14

2
∞
5

gb GE Gi

n log (n/δ)
mGi

2



∞

gb Gi

2
2

,

which is the desired (post-projection) uniformity bound.
We now bound the number of edges removed during all the recursive calls. The bound on L
means this recursion has at most O(log(n/δ)) levels. Lemma 5.1 gives that the number of edges
between the expander pieces is
!
X

d u φ log3 n · log(n/δ) = O mφ log3 n log(n/δ) ,
O
u

1
gives at most m/10 edges between the pieces for an
so the setting of φ = c
3
partition log n log(n/δ))
appropriate choice of cpartition .
Furthermore, as each edge’s contribution to gb is non-negative, the number of edges whose
m
relative contribution exceed 10L
m is at most 10L . Summing this over all levels gives at most m/10
edges removed from the trimming step on Line 3 of DecomposeRecursive in Algorithm 7.
Finally, the running time is dominated by the expander decomposition calls. As there are
O(log(n/δ)) levels of recursion and each level deals with edge-disjoint subsets, we obtain the total
running time by substituting the value of φ into the runtime of expander decompositions as given
in Lemma 5.1.

37

Acknowledgements
This project would not have been possible without Dan Spielman’s optimism about the existence
of analogs of numerical methods for ℓp -norms, which he has expressed to us on multiple occasions
over the past six years. We also thank Ainesh Bakshi, Jelani Nelson, Aaron Schild, and Junxing
Wang for comments and suggestions on earlier drafts and presentations of these ideas.
As with many recent works in optimization algorithms on graphs, this project has its large
share of influence by the late Michael B. Cohen. In fact, Michael’s first papers on recursive preconditioning [CKM+ 14] and ℓp -norm preserving sampling of matrices [CP15] directly influenced the
constructions of preconditioners (Section 4.4) and uniform expanders (Section 5 and Appendix D)
respectively. While our overall algorithm falls short of what Michael would consider ‘snazzy’, it’s
also striking how many aspects of it he predicted, including: the use of expander decompositions;
the p → ∞ case being different than the p → 1 case; and the large initial dependence on p that’s
also eventually fixable (see Section 1.4).
Richard regrets not being able to convince Michael to systematically investigate preconditioning
for ℓp -norm flows. He is deeply grateful to Aleksander Ma̧dry, Jon Kelner, Ian Munro, Tom Cohen,
Marie Cohen, Sebastian Bubeck, and Ilya Razenshteyn for many helpful conversations following
Michael’s passing.

References
[AKPS19]

Deeksha Adil, Rasmus Kyng, Richard Peng, and Sushant Sachdeva. Iterative refinement for ℓp -norm regression. In Proceedings of the Thirtieth Annual ACM-SIAM
Symposium on Discrete Algorithms, SODA 2019, 2019.

[AL11]

Morteza Alamgir and Ulrike V Luxburg. Phase transition in the family of presistances. In Advances in Neural Information Processing Systems, pages 379–387,
2011.

[ALdOW17] Zeyuan Allen-Zhu, Yuanzhi Li, Rafael Mendes de Oliveira, and Avi Wigderson. Much
faster algorithms for matrix scaling. In Symposium on Foundations of Computer Science (FOCS), pages 890–901, 2017. Available at: https://arxiv.org/abs/1704.02315.
[AN12]

Ittai Abraham and Ofer Neiman. Using petal-decompositions to build a low stretch
spanning tree. In STOC, 2012.

[BCLL18]

Sébastien Bubeck, Michael B. Cohen, Yin Tat Lee, and Yuanzhi Li. An homotopy
method for lp regression provably beyond self-concordance and in input-sparsity time.
In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2018, pages 1130–1137, New York, NY, USA, 2018. ACM.

[BK96]

András A. Benczúr and David R. Karger. Approximating s-t minimum cuts in Õ(n2 )
time. In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing, STOC ’96, pages 47–55, New York, NY, USA, 1996. ACM.

[BKKL17]

Ruben Becker, Andreas Karrenbauer, Sebastian Krinninger, and Christoph Lenzen.
Near-optimal approximate shortest paths and transshipment in distributed and

38

streaming models. In 31st International Symposium on Distributed Computing, DISC
2017, October 16-20, 2017, Vienna, Austria, pages 7:1–7:16, 2017. Available at:
https://arxiv.org/abs/1607.05127.
[CKM+ 11]

Paul Christiano, Jonathan A. Kelner, Aleksander Madry, Daniel A. Spielman, and
Shang-Hua Teng. Electrical flows, laplacian systems, and faster approximation of
maximum flow in undirected graphs. In Proceedings of the 43rd annual ACM symposium on Theory of computing, STOC ’11, pages 273–282, New York, NY, USA, 2011.
ACM. Available at http://arxiv.org/abs/1010.2921.

[CKM+ 14]

Michael B. Cohen, Rasmus Kyng, Gary L. Miller, Jakub W. Pachocki, Richard Peng,
Anup Rao, and Shen Chen Xu. Solving SDD linear systems in nearly m log1/2 n time.
In STOC, pages 343–352, 2014.

[CMMP13] Hui Han Chin, Aleksander Madry, Gary L. Miller, and Richard Peng. Runtime guarantees for regression problems. In Proceedings of the 4th conference on Innovations in
Theoretical Computer Science, ITCS ’13, pages 269–282, New York, NY, USA, 2013.
ACM. Available at http://arxiv.org/abs/1110.1358.
[CMTV17]

Michael B. Cohen, Aleksander Madry, Dimitris Tsipras, and Adrian Vladu. Matrix
scaling and balancing via box constrained newton’s method and interior point methods. In 58th IEEE Annual Symposium on Foundations of Computer Science, FOCS
2017, Berkeley, CA, USA, October 15-17, 2017, pages 902–913, 2017. Available at:
https://arxiv.org/abs/1704.02310.

[CP15]

Michael B. Cohen and Richard Peng. ℓp row sampling by Lewis weights. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC ’15, pages 183–192, New York, NY, USA, 2015. ACM. Available at
http://arxiv.org/abs/1412.0588.

[DS08]

Samuel I. Daitch and Daniel A. Spielman. Faster approximate lossy generalized flow
via interior point algorithms. In Proceedings of the 40th annual ACM symposium on
Theory of computing, STOC ’08, pages 451–460, New York, NY, USA, 2008. ACM.
Available at http://arxiv.org/abs/0803.0988.

[EACR+ 16] Ahmed El Alaoui, Xiang Cheng, Aaditya Ramdas, Martin J Wainwright, and Michael I
Jordan. Asymptotic behavior of ℓp -based Laplacian regularization in semi-supervised
learning. In Conference on Learning Theory, pages 879–906, 2016.
[Edm65]

Jack Edmonds. Paths, trees, and flowers. Canadian Journal of mathematics,
17(3):449–467, 1965. Available at: https://cms.math.ca/10.4153/CJM-1965-045-4.

[EK72]

Jack Edmonds and Richard M. Karp. Theoretical improvements in algorithmic efficiency for network flow problems. J. ACM, 19(2):248–264, 1972.

[ET75]

Shimon Even and Robert Endre Tarjan. Network flow and testing graph connectivity.
SIAM J. Comput., 4(4):507–518, 1975.

39

[GKK+ 15]

Mohsen Ghaffari, Andreas Karrenbauer, Fabian Kuhn, Christoph Lenzen, and Boaz
Patt-Shamir. Near-optimal distributed maximum flow: Extended abstract. In Proceedings of the 2015 ACM Symposium on Principles of Distributed Computing, PODC
2015, Donostia-San Sebastián, Spain, July 21 - 23, 2015, pages 81–90, 2015. Available
at: https://arxiv.org/abs/1508.04747.

[GN79]

Zvi Galil and Amnon Naamad. Network flow and generalized path compression. In
Proceedings of the 11h Annual ACM Symposium on Theory of Computing, April 30 May 2, 1979, Atlanta, Georgia, USA, pages 13–26, 1979.

[GR98]

Andrew V. Goldberg and Satish Rao. Beyond the flow decomposition barrier. J.
ACM, 45(5):783–797, 1998.

[GT88]

Andrew V. Goldberg and Robert Endre Tarjan. A new approach to the maximum-flow
problem. J. ACM, 35(4):921–940, 1988.

[GT14]

Andrew V. Goldberg and Robert Endre Tarjan. Efficient maximum flow algorithms.
Commun. ACM, 57(8):82–89, 2014.

[HK73]

John E. Hopcroft and Richard M. Karp. An n5/2 algorithm for maximum matchings
in bipartite graphs. SIAM J. Comput., 2(4):225–231, 1973.

[HO13]

Dorit S. Hochbaum and James B. Orlin. Simplifications and speedups of the pseudoflow algorithm. Networks, 61(1):40–57, 2013.

[Hoc08]

Dorit S Hochbaum. The pseudoflow algorithm: A new algorithm for the maximumflow problem. Operations research, 56(4):992–1009, 2008.

[Kar73]

Alexander V. Karzanov. O nakhozhdenii maksimalńogo potoka v setyakh spetsialńogo
vida i nekotorykh prilozheniyakh. Matematicheskie Voprosy Upravleniya Proizvodstvom, 5:81–94, 1973. In Russian, title translation: on finding maximum flows in
networks with special structure and some applications.

[KBR07]

Vladimir Kolmogorov, Yuri Boykov, and Carsten Rother. Applications of parametric
maxflow in computer vision. In Computer Vision, 2007. ICCV 2007. IEEE 11th
International Conference on, pages 1–8. IEEE, 2007.

[KLOS14]

Jonathan A. Kelner, Yin Tat Lee, Lorenzo Orecchia, and Aaron Sidford. An almostlinear-time algorithm for approximate max flow in undirected graphs, and its multicommodity generalizations. In Proceedings of the Twenty-Fifth Annual ACM-SIAM
Symposium on Discrete Algorithms, SODA 2014, Portland, Oregon, USA, January
5-7, 2014, pages 217–226, 2014. Available at http://arxiv.org/abs/1304.2338.

[KM09]

J.A. Kelner and A. Madry. Faster generation of random spanning trees. In FOCS,
2009.

[KM11]

Jonathan A. Kelner and Petar Maymounkov. Electric routing and concurrent
flow cutting.
Theor. Comput. Sci., 412(32):4123–4135, 2011.
Available at:
https://arxiv.org/abs/0909.2859.

40

[KMP11]

Ioannis Koutis, Gary L. Miller, and Richard Peng. A nearly-m log n time solver for
SDD linear systems. In Proceedings of the 2011 IEEE 52nd Annual Symposium on
Foundations of Computer Science, FOCS ’11, pages 590–598, Washington, DC, USA,
2011. IEEE Computer Society. Available at http://arxiv.org/abs/1102.4842.

[KMP12]

Ioannis Koutis, Gary L. Miller, and Richard Peng. A fast solver for a class of linear systems. Communications of the ACM, 55(10):99–107, October 2012. Available at https://cacm.acm.org/magazines/2012/10/155538-a-fast-solver-for-a-class-oflinear-systems/fulltext.

[KMP14]

I. Koutis, G. Miller, and R. Peng. Approaching optimality for solving sdd linear systems. SIAM Journal on Computing, 43(1):337–354, 2014. Available at
http://arxiv.org/abs/1003.2958.

[KOSZ13]

J. A. Kelner, L. Orecchia, A. Sidford, and Z. A. Zhu. A simple, combinatorial algorithm for solving sdd systems in nearly-linear time. In STOC, 2013.

[KRSS15]

R. Kyng, A. B. Rao, S. Sachdeva, and D. A Spielman. Algorithms for lipschitz learning
on graphs. In COLT, 2015.

[KS96]

David R. Karger and Clifford Stein. A new approach to the minimum cut problem.
J. ACM, 43(4):601–640, 1996.

[KS16]

Rasmus Kyng and Sushant Sachdeva. Approximate gaussian elimination for laplacians
- fast, sparse, and simple. In FOCS, pages 573–582. IEEE Computer Society, 2016.
Available at http://arxiv.org/abs/1605.02353.

[LPS15]

Y. T. Lee, R. Peng, and D. A. Spielman. Sparsified cholesky solvers for SDD linear
systems. CoRR, abs/1506.08204, 2015.

[LS13]

Yin Tat Lee and Aaron Sidford. Efficient accelerated coordinate descent methods and
faster algorithms for solving linear systems. In Proceedings of the 2013 IEEE 54th
Annual Symposium on Foundations of Computer Science, FOCS ’13, pages 147–156,
Washington, DC, USA, 2013. IEEE Computer Society.

[LS14]

Y. T. Lee and A. Sidford. Path finding methods for linear programming: Solving
linear programs in Õ(vrank) iterations and faster algorithms for maximum flow. In
FOCS, 2014.

[LSBG13]

Bingdong Li, Jeff Springer, George Bebis, and Mehmet Hadi Gunes. A survey of
network flow applications. Journal of Network and Computer Applications, 36(2):567–
581, 2013.

[Mad10]

Aleksander Madry.
Fast approximation algorithms for cut-based problems
in undirected graphs.
In Foundations of Computer Science (FOCS), 2010
51st Annual IEEE Symposium on, pages 245–254. IEEE, 2010. Available at
http://arxiv.org/abs/1008.1975.

[Mad11]

Aleksander Madry. From graphs to matrices, and back: new techniques for graph
algorithms. PhD thesis, Massachusetts Institute of Technology, 2011.
41

[Mad13]

A. Madry. Navigating central path with electrical flows: From flows to matchings,
and back. In FOCS, 2013.

[Mad16]

Aleksander Madry. Computing maximum flow with augmenting electrical flows. In
IEEE 57th Annual Symposium on Foundations of Computer Science, FOCS 2016, 911 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA, pages 593–602,
2016. Available at: https://arxiv.org/abs/1608.06016.

[NN94]

Y. Nesterov and A. Nemirovskii. Interior-Point Polynomial Algorithms in Convex
Programming. Society for Industrial and Applied Mathematics, 1994.

[Orl13]

James B. Orlin. Max flows in o(nm) time, or better. In Symposium on Theory of
Computing Conference, STOC’13, Palo Alto, CA, USA, June 1-4, 2013, pages 765–
774, 2013.

[Pen16]

Richard Peng. Approximate undirected maximum flows in O(m polylog(n)) time. In
Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1862–1867. SIAM, 2016. Available at http://arxiv.org/abs/1411.7631.

[PS14]

Richard Peng and Daniel A. Spielman. An efficient parallel solver for SDD linear
systems. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing, STOC ’14, pages 333–342, New York, NY, USA, 2014. ACM. Available at
http://arxiv.org/abs/1311.3286.

[PZZ13]

Bo Peng, Lei Zhang, and David Zhang. A survey of graph theoretical approaches to
image segmentation. Pattern Recognition, 46(3):1020–1038, 2013.

[ROF92]

Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based
noise removal algorithms. Physica D: nonlinear phenomena, 60(1-4):259–268, 1992.

[Sac19]

Sushant Sachdeva. Private Communication, 2019.

[Sch02]

Alexander Schrijver. On the history of the transportation and maximum flow problems. Math. Program., 91(3):437–445, 2002.

[She13]

Jonah Sherman. Nearly maximum flows in nearly linear time. In 54th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2013, 2629 October, 2013, Berkeley, CA, USA, pages 263–269, 2013.
Available at
http://arxiv.org/abs/1304.2077.

[She17a]

Jonah Sherman. Area-convexity, l∞ regularization, and undirected multicommodity
flow. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of
Computing, STOC 2017, Montreal, QC, Canada, June 19-23, 2017, pages 452–460,
2017.

[She17b]

Jonah Sherman.
Generalized preconditioning and undirected minimum-cost
flow.
In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium
on Discrete Algorithms, SODA ’17, pages 772–780, 2017.
Available at:
https://arxiv.org/abs/1606.07425.

42

[Spi18]

D. A. Spielman. Conductance, the Normalized Laplacian, and Cheegers Inequality.
http://www.cs.yale.edu/homes/spielman/561/lect11-18.pdf, 2018.

[SS11]

D. Spielman and N. Srivastava.
Graph sparsification by effective resistances.
SIAM Journal on Computing, 40(6):1913–1926, 2011.
Available at
http://arxiv.org/abs/0803.0929.

[ST83]

Daniel D Sleator and Robert Endre Tarjan. A data structure for dynamic trees. Journal of computer and system sciences, 26(3):362–391, 1983. Announced at STOC’81.

[ST85]

Daniel Dominic Sleator and Robert Endre Tarjan. Self-adjusting binary search trees.
J. ACM, 32(3):652–686, 1985.

[ST14]

D. Spielman and S. Teng. Nearly linear time algorithms for preconditioning and solving
symmetric, diagonally dominant linear systems. SIAM Journal on Matrix Analysis
and Applications, 35(3):835–885, 2014. Available at http://arxiv.org/abs/cs/0607105.

[SW18]

Thatchaphol Saranurak and Di Wang.
Expander decomposition and pruning:
Faster, stronger, and simpler, 2018.
To appear at SODA 2019.
https://dw236.github.io/papers/main_decomp.pdf.

[Tro12]

Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Found. Comput.
Math., 12(4):389–434, August 2012. Available at http://arxiv.org/abs/1004.4389.

[ZWC10]

Mingqiang Zhu, Stephen J Wright, and Tony F Chan. Duality-based algorithms
for total-variation-regularized image restoration. Computational Optimization and
Applications, 47(3):377–400, 2010.

A

Deferred Proofs from Prelims, Section 2.2

Lemma 2.9 (Reflexivity). For every smoothed p-norm instance G, and every κ ≥ 1, G κ G and
G cycle
G with the identity map.
κ
Proof. Consider the map MG→G such that for every flow f G on G, we have MG→G (f G ) = f G . Thus,


E G κ−1 MG→G (f G ) = E G κ−1 f G
⊤ −1 G 
= gG
κ f − hp (r , κ−1 f G )
⊤
(Using Lemma B.1)
≥ κ−1 g G f G − κ−2 hp (r , f G )

⊤
≥ κ−1 g G f G − κ−1 hp (r , f G ) = κ−1 E G (f G ).

Moreover (B G )⊤ MG→G (f G ) = B G f G . Thus, the claims follow.

Lemma 2.10 (Composition). Given two smoothed p-norm instances, G1 , G2 , such that G1 κ1 G2
with the map MG1 →G2 and G2 κ2 G3 with the map MG2 →G3 , then G1 κ1 κ2 G3 with the map
MG1 →G3 = MG2 →G3 ◦ MG1 →G2 .
Similarly, for any G1 , G2 , if G1 cycle
G2 with the map MG1 →G2 and G2 cycle
G3 with the map
κ1
κ2
cycle
MG2 →G3 , then G1 κ1 κ2 G3 with the map MG1 →G3 = MG2 →G3 ◦ MG1 →G2 .
43

Proof. It is easy to observe that the given mapping is linear. Given a flow f G1 on G1 , we have,


(B G3 )⊤ MG2 →G3 ◦ MG1 →G2 (f G1 ) = (B G3 )⊤ MG2 →G1 MG3 →G2 (f G1 )

= (B G2 )⊤ MG3 →G2 (f G1 ) = (B G1 )⊤ (f G1 ).
Moreover,



EG3 (κ1 κ2 )−1 MG2 →G3 MG1 →G2 (f G1 ) ≥ EG3 κ2 −1 MG2 →G3 κ1 −1 MG1 →G2 (f G1 )
(Using linearity)

≥ κ2 −1 EG2 κ1 −1 MG1 →G2 (f G1 )
(Using G2 κ2 G3 )

−1
G1
≥ (κ2 κ1 ) EG1 f
(Using G1 κ1 G2 )

The same proof works for cycle .

Lemma 2.12 (κ under union). Consider four smoothed p-norm instances, G1 , G2 , H1 , H2 , on the
same set of vertices, i.e. V G1 = V G2 = V H1 = V H2 , such that for i = 1, 2, Hi κ Gi with the map
def
def
MHi →Gi . Let G = G1 ∪ G2 , and H = H1 ∪ H2 . Then, H κ G with the map
 def


MH→G f H = (f H1 , f H2 ) = MH1 →G1 f H1 , MH2 →G2 f H2 ,
where (f H1 , f H2 ) is the decomposition of f H onto the supports of H1 and H2 .
def

Proof. Let f H be a flow on H. We write f H = (f H1 , f H2 ). Let f G = MH→G (f H ). If f Gi denotes
MHi →Gi (f Hi ) for i = 1, 2, then we know that f G = (f G1 , f G2 ). Thus, the objectives satisfy
E G (κ−1 f G ) = E G1 (κ−1 f G1 ) + E G2 (κ−1 f G2 )

≥ κ−1 E H1 (f H1 ) + κ−1 E H2 (f H2 ) = κ−1 E H (f H )

For the residues, we have,
(B G )⊤ (f G ) = (B G1 )⊤ (f G1 ) + (B G2 )⊤ (f G2 )
= (B H1 )⊤ (f H1 ) + (B H2 )⊤ (f H2 ) = (B H )⊤ (f H ).
Thus, H κ G.
Lemma 2.13. For all κ ≥ 1, and for all pairs of smoothed p-norm instances, G, H, on the same
underlying graphs, i.e., (V G , E G ) = (V H , E H ), such that,
1. the gradients are identical, g G = g H ,
2. the ℓ22 resistances are off by at most κ, i.e., r Ge ≤ κr H
e for all edges e, and
3. the p-norm scaling is off by at most κp−1 , i.e., sG ≤ κp−1 sH ,

then H κ G with the identity map.
Proof. Consider the map MH→G (f ) = f . Thus, since the underlying graphs are the same, we
immediately have (B G )⊤ f = (B H )⊤ f . For the objective, we have
X

κ−1 g Ge f e − κ−2 r Ge f 2e − κ−p sG |f e |p
E G (κ−1 f ) =
e

≥ κ−1

X
e

p
H 2
H
gH
= κ−1 E H (f ).
e f e − r e f e − s |f e |

44

B

Deferred Proofs for Numerical Methods from Section 3

The following simple lemma characterizes the change in smoothed ℓp -norms under rescaling of the
input vector.
Lemma B.1. For all x ∈ Rm , r ∈ Rm
≥0 , s ∈ R≥0 , and λ ∈ R, we have,
min{|λ|2 , |λ|p }hp (r , s, x ) ≤ hp (r , s, λx ) ≤ max{|λ|2 , |λ|p }hp (r , s, x ).
Proof. It suffices to prove the claim for x ∈ R, r ∈ R≥0 , s ∈ R≥0
hp (r, s, λx) = r(λx)2 + s|λx|p
= |λ|2 · rx2 + |λ|p · s|x|p
Since all terms are non-negative, we get,

and

hp (r, s, λx) ≥ min{|λ|2 , |λ|p } · (rx2 + s|x|p ),

hp (r, s, λx) ≤ max{|λ|2 , |λ|p } · (rx2 + s|x|p ).

Lemma 3.1 ([AKPS19]). For all r , x , δ ∈ Rm , with r ∈ Rm
≥0 , and s ≥ 0, we have
2−p · hp (r + |x |p−2 , s, δ) ≤ hp (r , s, x + δ) − hp (r , s, x ) − δ ⊤ ∇x hp (r , s, x ) ≤ 22p · hp (r + |x |p−2 , s, δ).
Proof. Note that all the terms are a sum over the coordinates. Thus, it suffices to prove the
inequality for x, δ ∈ R, and r, s ∈ R≥0 . We have,
hp (r, s, x + δ) − hp (r, s, x) − δ

∂
hp (r, s, x) = r(x + δ)2 + s|x + δ|p − rx2 − s|x|p − δ(2rx + ps|x|p−2 x)
∂x
= rδ2 + s|x + δ|p − s|x|p − psδ|x|p−2 x

p
= rδ2 + s|x|p 1 + δ′ − 1 − pδ′ ,

where δ′ = δ/x.
Lemma B.2, proved later, proves that for all δ′ , and p ≥ 2, we have,

p
1 + δ′ − 1 − pδ′ ≤ p2p−1 δ′2 + |δ′ |p .

Thus,

hp (r, s, x + δ) − hp (r, s, x) − δ


∂
hp (r, s, x) ≤ rδ2 + s|x|p p2p−1 δ′2 + |δ′ |p
∂x
= rδ2 + sp2p−1 |x|p−2 δ2 + sp2p−1 |δ|p
≤ p2p−1 ((r + s|x|p−2 )δ2 + s|δ|p )
= p2p−1 hp (r + s|x|p−2 , s, δ)
≤ 22p hp (r + s|x|p−2 , s, δ).
45

Lemma B.3, proved later, shows that for all δ′ , and p ≥ 2, we have,

p
1 + δ′ − 1 − pδ′ ≥ 2−p δ′2 + |δ′ |p .
hp (r, s, x + δ) − hp (r, s, x) − δ


∂
hp (r, s, x) ≥ rδ2 + s|x|p 2−p δ′2 + |δ′ |p
∂x
= rδ2 + 2−p s|x|p−2 δ2 + 2−p s|δ|p
≥ 2−p ((r + s|x|p−2 )δ2 + s|δ|p )
= 2−p hp (r + s|x|p−2 , s, δ).

Lemma B.2. For all δ ∈ R, p ≥ 1, we have,


|1 + δ|p − 1 − pδ ≤ p2p−1 δ2 + |δ|p .

Proof. The proof has to consider several cases.
δ ≥ 1.

Using mean-value theorem, we know there is some z ∈ [0, δ] such that
|1 + δ|p − 1 − pδ = (1 + δ)p − 1 − pδ


= pδ (1 + z)p−1 − 1

≤ pδ(1 + δ)p−1
≤ pδ(2δ)p−1 .

0 ≤ δ ≤ 1.

Using mean-value theorem, we know there is some z ∈ [0, δ] such that
|1 + δ|p − 1 − pδ = (1 + δ)p − 1 − pδ


= pδ (1 + z)p−1 − 1

≤ pδ (1 + δ)p−1 − 1 .

If p ≤ 2, we have (1 + δ)p−1 is a concave function, and hence (1 + δ)p−1 ≤ 1 + (p − 1)δ. If p ≥ 2, we
have (1 + δ)p−1 is a convex function, and hence for δ ∈ [0, 1], we have (1 + δ)p−1 ≤ 1 + (2p−1 − 1)δ.
Thus,
|1 + δ|p − 1 − pδ ≤ p max{(p − 1)δ2 , (2p−1 − 1)δ2 }.
−1 ≤ δ ≤ 0.

Using mean-value theorem, we know there is some z ∈ [−|δ|, 0] such that
|1 + δ|p − 1 − pδ = (1 + δ)p − 1 − pδ


= pδ (1 + z)p−1 − 1


≤ p|δ| 1 − (1 + δ)p−1 .

If p ≤ 2, we have (1+δ)p−1 is a concave function, and hence for δ ∈ [−1, 1], we have (1+δ)p−1 ≥ 1+δ.
If p ≥ 2, we have (1 + δ)p−1 is a convex function, and hence (1 + δ)p−1 ≥ 1 + (p − 1)δ. Thus,
|1 + δ|p − 1 − pδ ≤ p|δ| max{|δ|, (p − 1)|δ|}.
46

δ ≤ −1.

We have,
|1 + δ|p − 1 − pδ = (|δ| − 1)p − 1 + p|δ|
≤ |δ|p + p|δ|

≤ |δ|p + p|δ|2 ,
since |δ| ≥ 1.
Lemma B.3. For all δ ∈ R, p ≥ 2, we have
|1 + δ|p − 1 − pδ ≥ 2−p (δ2 + |δ|p ).
Proof. Let h(δ) denote the function
h(δ) = |1 + δ|p − 1 − pδ − 2−p (δ2 + |δ|p ).
Thus, h(0) = 0. As for the previous proof, we consider several cases:
δ ≥ 0.

We have h(δ) = (1 + δ)p − 1 − pδ − 2−p (δ2 + δp ). Thus,
h′ (δ) = p(1 + δ)p−1 − p − 2−p+1 δ − p2−p δp−1

h′′ (δ) = p(p − 1)(1 + δ)p−2 − 2−p+1 − p(p − 1)2−p δp−2
Observe that since p ≥ 2, we have (1 + δ)p−2 ≥ max{1, δp−2 } ≥ 2−1 (1 + δp−2 ), and p(p − 1) ≥ 2.
Thus,
h′′ (δ) ≥ 2−1 p(p − 1) + 2−1 p(p − 1)δp−2 − 2−p+1 − p(p − 1)2−p δp−2 ≥ 0.
Since h(0) = h′ (0) = 0, and h′′ (δ) ≥ 0, for all δ ≥ 0, we must have h(δ) ≥ 0 for all δ ≥ 0.
−1 ≤ δ ≤ 0.

We have,
h(δ) = (1 + δ)p − 1 − pδ − 2−p δ2 − 2−p |δ|p

h′ (δ) = p(1 + δ)p−1 − p − 2−p+1 δ + p2−p |δ|p−1 .
Since 0 ≤ 1 + δ ≤ 1, and p − 1 ≥ 1, we have (1 + δ)p−1 ≤ 1 + δ, and |δ|p−1 ≤ |δ|1 = −δ. Thus,
h′ (δ) ≤ p(1 + δ) − p − 2−p+1 δ + p2−p |δ|
= −p|δ| + (2 + p)2−p |δ|

≤ −p|δ| + 2−2 (p + 2)|δ| ≤ 0.

δ ≤ −1.

We have,
h(δ) = (−1 − δ)p − 1 − pδ − 2−p δ2 − 2−p |δ|p

h′ (δ) = −p(−1 − δ)p−1 − p − 2−p+1 δ + p2−p |δ|p−1 .

47

Since |δ| ≥ 1, and p − 1 ≥ 1, we have −δ = |δ| ≤ |δ|p−1 . Thus,
h′ (δ) ≤ −p(−1 − δ)p−1 − p + 2−p (2 + p)|δ|p−1


≤ −p (−1 − δ)p−1 + 1 − 2−p+1 |δ|p−1 .

Now, observe that since 0 ≤ −1 − δ, and p − 1 ≥ 1,

|δ|p−1 = (−δ)p−1 = (1 + (−1 − δ))p−1 ≤ 2p−1 (1p−1 + (−1 − δ)p−1 ),
Thus,

(−1 − δ)p−1 + 1 − 2−p+1 |δ|p−1 ≥ 0,

and hence h′ (δ) ≤ 0 for δ ≤ −1.

For the last two cases, since h(0) = 0, and h′ (δ) ≤ 0, for all δ ≤ 0. Thus, we must have h(δ) ≥ 0,
for δ ≤ 0.
Theorem 3.2 ([AKPS19]). Given the following optimization problem,
maxx
s.t.

def

E1 (x ) = g ⊤ x − hp (r , s, x )
Ax = b

(P1)

and an initial feasible solution x 0 , we can construct the following residual problem:
maxδ
s.t.

def

E2 (δ) = (g ′ )⊤ δ − hp (r ′ , s, δ)
Aδ = 0,

(R1)

where g ′ = 2p (g − ∇x h(r , s, x )|x =x 0 ), and r ′ = r + s|x 0 |p−2 .
exists a feasible solution e
δ to the residual problem R1 that achieves an objective of
There

p
⋆
E2 e
δ ≥ 2 (E1 (x ) − E1 (x 0 )), where x ⋆ is an optimal solution to problem P1.
def

Moreover, given any feasible solution δ to Program R1, the vector x 1 = x 0 + 2−3p δ is a feasible
solution to the Program P1 and obtains the objective
E1 (x 1 ) ≥ E1 (x 0 ) + 2−4p E2 (δ).

Proof. Let x ⋆ to be an optimal solution to Problem P1. Consider e
δ = x ⋆ − x 0 . Thus,
Ae
δ = Ax ⋆ − Ax 0 = b − b = 0.

Thus, e
δ is a feasible solution to Problem R1. Moreover, it satisfies,

e
E2 (e
δ) = (e
δ)⊤ 2p (g − ∇x h(r , s, x )|x =x 0 ) − hp (r + s|x 0 |p−2 , s, δ)


= 2p g ⊤ e
δ − 2p 2−p hp (r + s|x 0 |p−2 , s, e
δ) + (e
δ)⊤ ∇x h(r , s, x )|x =x 0


≥ 2p g ⊤ e
δ − 2p hp (r , s, x 0 + e
δ) − hp (r , s, x 0 )
= 2p g ⊤ (x ⋆ − x 0 ) − 2p (hp (r , s, x ⋆ ) − hp (r , s, x 0 ))

= 2p (E1 (x ⋆ ) − E1 (x 0 )).

48

(Using Lemma 3.1)

Now, given a feasible solution δ to Problem R1, we must have Aδ = 0. Thus, Ax 1 = Ax 0 +
2−3p Aδ = b, and x 1 is a feasible solution to Problem P1. Moreover,
E1 (x 1 ) = g ⊤ (x 0 + 2−3p δ) − hp (r , s, x 0 + 2−3p δ)

≥ g ⊤ x 0 + 2−3p g ⊤ δ − hp (r , s, x 0 ) − 2−3p δ⊤ ∇x hp (r , s, x )|x =x 0 − 22p hp (r + s|x 0 |p−2 , s, 2−3p δ)
(Using Lemma 3.1)

p−2

≥ E1 (x 0 ) + 2−4p δ⊤ g ′ − 22p · 2−6p hp (r + s|x 0 |
−4p

= E1 (x 0 ) + 2

C

, s, δ)

(Using Lemma B.1)

E2 (δ).

Elimination of Low-Degree Vertices, and Loops

In this section, we that the instance H returned by UltraSparsify can be reduced to a smaller
graph by repeatedly eliminating vertices of degree at most 2. This step is analogous to the partial
Cholesky factorization in the Laplacian solver of Spielman and Teng [ST14]. A slight technical
issue is that if we run into a cycle where at most 1 vertex on the cycle has edge(s) to the rest of
the graph, the elimination of the degree 2 nodes on the cycle essentially becomes an optimization
problem on only the cycle edges that can be solved independently from the rest of the graph.
Algorithm 8 Elimination of Degree 1 and 2 vertices and Self-loops
1: procedure Eliminate(H)
2:
Initiate H′ ← H
3:
repeat
4:
For every edge with non-selfloop degree 1, remove the only non-selfloop edge incident on
it
5:
until No vertex has non-selfloop degree 1
6:
for every maximal path with all internal nodes having non-selfloop degree 2 do
7:
Replace such a path with a single edge in H′ with the end points as the end points of
the path, and,
• resistance is the sum of the resistances of the edges on the path

• gradient is the sum of the gradients of the edges on the path
• s the same as before

• Flow on the new edge is mapped to a flow along the original path (or cycle) in H.
8:
9:

Move all self-loops from H′ to Hloop
return H′ , Hloop , M(H′ +Hloop )→H

Theorem 3.4 (Eliminating vertices with degree 1 and 2). Given a smoothed p-norm instance G,
the algorithm Eliminate(G) returns another smoothed p-norm instance G ′ , along with the map
′
′
MG ′ →G in O( V G + E G ) time, such that the graph G′ = (V G , E G ) is obtained from the graph
49

G = (V G , E G ) by first repeatedly removing vertices with non-selfloop degree8 1 in G, and then
replacing every path u
v in G where all internal path vertices have non-selfloop degree exactly 2
in G, with a new edge (u, v).
Moreover,
G cycle
G′,
G ′ cycle
1
1
n p−1

where n = V

G

, and the map MG ′ →G can be applied in O( V G + E G ) time.

Proof. We first observe that a self-loop e ∈ E G on a vertex v ∈ V G does not contribute to the
residue at any vertex, including v. Thus, the circulation constraint on a flow f G does not impose
any constraint on f Ge . Moreover, since the objective αG can be written as a sum over the edges,
for every self-loop e, the variable f Ge is independent of all other variables. Thus, we can ignore the
self-loops in remainder of the proof.
We first prove that we can repeatedly eliminate vertices of non-selfloop degree 1 in G while
preserving E exactly for a circulation. Consider one such vertex v ∈ V G , and let e = (v, u) ∈ E G be
the only non-selfloop edge incident on v (the argument for the reverse direction is identical). Given
any circulation f G , since the only non-selfloop edge incident on v is e, we must have f Ge = 0. Thus,
we can drop e entirely from the instance. Formally, we define
′

V G = V G,

′

E G = E G \ {e},

′

g G = g G |E G ′ ,

′

r G = r G |E G ′ ,

′

and sG = sG .

′

We let the mapping MG→G ′ to be just the projection on to E G . Thus, MG→G ′ (f G ) = f G |E G ′ . Since
′
G′.
f Ge = 0, we immediately get E G (MG→G ′ (f G )) = E G (f G ). Thus, G cycle
1
′
G
Now, consider the mapping MG ′ →G that pads a circulation f on G ′ with 0 on e, i.e.,
(


0
if e′ = e,
′
MG ′ →G (f G ) ′ =
′
e
f Ge′ otherwise.
′

′

′

G.
Again, it is immediate that E G (MG ′ →G (f G )) = E G (f G ). Thus, G ′ cycle
1
We can repeatedly apply the above transformation to eliminate all vertices of non-selfloop
degree 1 in G. For convenience, we let G ′ denote the final instance obtained. Thus, we have,
G′.
G cycle
G ′ cycle
1
1
Now, we will replace maximal paths with all internal vertices of non-selfloop degree 2 with single
edges. Consider such a path P. Formally, P is a path of length l in G, say P = (v0 , v1 , . . . , vl−1 , vl ),
with all of v1 , . . . , vl−1 having degree exactly 2, and v0 , vl have non-selfloop degree at least 3. For
convenience, we assume that all edges (vi−1 , vi ) are oriented in the same direction. Observe that
′
′
for a circulation f G , the flow on all the edges (vi−1 , vi ) must be the same, i.e., f G(vi−1 ,vi ) must all
be equal. Thus, we can replace P with a single edge eP while preserving the amount of flow and
the direction.
′
′
Formally, let P = {P1 , . . . , Pt } denote the set of all maximal paths in G′ = (V G , E G ) such that
all their internal vertices have non-selfloop degree exactly 2 in G′ . We replace each of these paths
8

By non-selfloop degree, we mean that self-loops do not count towards the degree of a vertex.

50

with a new edge connecting its endpoints. Let,
′′

′

′′

′

VG =VG ,
E G = E G ∪P ∈P {ep
( ′
gG
′′
g Ge = Pe
G′
e′ ∈P g e′
( ′
rG
′′
r Ge = Pe
G′
e′ ∈P r e′
′′

sG = sG .

= (v0 , vl )|P = (v0 , . . . , vl )} \ ∪P ∈P {e = (vi−1 , vi ) ∈ P |P = (v0 , . . . , vl )},
′

′′

′

′′

if e ∈ E G ∩ E G ,
if e = eP for P ∈ P,

if e ∈ E G ∩ E G ,
if e = eP for P ∈ P,

We define the mapping MG ′ →G ′′ as follows
( ′
′′
′


f Ge
if e ∈ E G ∩ E G ,
G′
MG ′ →G ′′ (f ) =
′
e
f G(v0 ,v1 ) if e = eP , where P = (v0 , . . . , vl ), P ∈ P.
We define MG ′′ →G ′ to be the inverse map of MG ′ →G ′′ .
( ′′
′′
′


f Ge
if e ∈ E G ∩ E G ,
′′
G
MG ′′ →G ′ (f ) =
′
e
f GP
if e = (vi−1 , vi ), i ∈ [l], where P = (v0 , . . . , vl ) ∈ P.
′

′′

′

It follows from the definitions that for every circulation f G , letting f G denote MG ′ →G ′′ (f G ),
′
′′
we have, MG ′′ →G ′ (f G ) = f G . Moreover,
 ′ ⊤ ′  ′ ⊤ ′′
gG
f G = gG
fG
 ′′ 2
 ′ 2
X
X
′′
′
=
r Ge f Ge
r Ge f Ge
e∈E G ′

sG

′

X

e∈E G ′

e∈E G ′′

f Ge

′

p

≥ sG

′′

X

e∈E G ′′

f Ge

′′

p

≥

1 G′ X G′ p
fe ,
s
n
G′
e∈E

where the last inequality follows since for every path of length l, the contribution to the ℓpp changes
by a factor of l−1 , and since the paths must be vertex-disjoint, l ≤ |V G | ≤ n. The above inequalities
imply,
′′
′′
′
′
E G (f G ) ≤ E G (f G ),

and hence G ′ cycle
G ′′ . Combined with G cycle
G ′ , we get G cycle
G ′′ . Moreover, we have, for
1
1
1
1

κ = n p−1 ,
 ′ 2
  ′ ⊤

X
X
′
′
′
′
′
′
f Ge
− κ−p sG
r Ge f Ge
κ−1 f G − κ−2
E G κ−1 f G = g G


e∈E G ′

e∈E G ′

p


 ′ 2 1 ′ X
 ′ ⊤ ′
X
p
′
′
− sG
= κ−1  g G
r Ge f Ge
f G − κ−1
f Ge 
n
e∈E G ′
e∈E G ′




 ′′ ⊤ ′′
X
X
p
2
′′
′′
′′
′′
′′
′′
f Ge  = κ−1 E G (f G ).
− sG
r Ge f Ge
≥ κ−1  g G
fG −
e∈E G ′′

e∈E G ′′

51

Thus, G ′′ cycle
G ′ . Combining with G ′ cycle
G, we obtain G ′′ cycle
G. The final instance returned
κ
κ
1
′′
is G , giving us our theorem.
Lemma 3.5 (Eliminating Self-loops). There is an algorithm RemoveLoops such that, given a
smoothed p-norm instance G with self-loops in E G , in O( V G + E G ) time, it returns instances
G1 , G2 , such that G = G1 ∪ G2 , where G1 is obtained from G by eliminating all self-loops from E G , and
G2 is an instance consisting of just the self-loops from G. Thus, any flow f G2 on G2 is a circulation.
Moreover, there is an algorithm SolveLoops that, given G2 , for any δ ≤ 1/p, in time O(|E G2 | log 1/δ ),
finds a circulation feG2 on G2 such that
E G2 (feG2 ) ≥ (1 − δ)

f

G

max

:(B )G f G =0

E G2 (f G2 ).

Proof. Let E ′ denote the set of all self-loops in E G . Then, we define G1 to be the instance obtained
by removing all edges in E ′ . Formally,
def

G1 = (V G , E G \ E ′ , g G |E G \E ′ , r G |E G \E ′ , sG ).
We define G2 be the instance G restricted to E ′ . Thus,
def

G2 = (V G , E ′ , g G |E ′ , r G |E ′ , sG ).
It is immediate that G = G1 ∪ G2 . Since G2 only has self-loops, we have that for every f G2 , we have
(B G2 )⊤ f G2 = 0. Thus, the constraint (B G2 )⊤ f G2 = 0 is vacuous.
Now, observe that in the absence of linear constraints on f G2 , the variables f Ge 2 are independent
for all e ∈ E G2 . Moreover, we have
X
EeG2 (f Ge 2 ).
E G2 (f G2 ) =
e∈E G2

Thus, we can solve for each f Ge 2 independently. Now, consider a fixed e ∈ E G2 . We write fe for f Ge 2 .
We wish to solve
max EeG2 (fe ) = max g Ge 2 fe − r Ge 2 fe2 − sG2 |fe |p .
fe

fe

Note that the objective function is concave. The gradient of EeG2 (fe ) with respect to fe is
′
d G2
Ee (fe ) = g Ge 2 − (2r Ge 2 + psG2 |fe |p−2 )fe .
EeG2 (fe ) =
dfe

First observe that if fe⋆ is the optimal solution, it must have the same sign as g Ge 2 . Without loss
G2

of generality, we assume that g Ge 2 ≥ 0. Observe that for fe ≥ g eG2 , we have dfde EeG2 ≤ 0. Thus,
2r e
1
 G2  p−1
G2
ge
, where fe⋆ is the optimal solution. Thus, if we define
fe⋆ ≤ g eG2 . Similarly, we have, fe⋆ ≤ ps
G2
2r e
z as
(
1 )
 G2  p−1
G2
g
g
def
e
e
,
z = min
,
2r Ge 2 psG2
then fe⋆ ≤ z.
52

Moreover, for fe ≤ 2z , we have,
′
2zr Ge 2
psG2 z p−1
g Ge 2
g Ge 2
G2
EeG2 (fe ) ≥ g Ge 2 −
−
−
≥
g
−
≥ 0.
e
2
2p−1
2
2p−1

Thus, fe⋆ ≥ z2 , and hence z gives a 2-approximation to f ⋆ that can be computed in O(1) time. Now,
applying binary search allows us to find fe ∈ [(1 − δ/p)fe⋆ , (1 + δ/p)fe⋆ ] in O(log 1/δ ) time. Now, we
show that such an estimate is good enough. Consider the point 43 z. We have
3
1 3 1 3p−1
1
3
−
) ≥ zg Ge .
max EeG2 (fe ) ≥ EeG2 ( z) ≥ g Ge z(1 −
p−1
fe
4
4
24 p4
4
Now,
EeG2 (fe ) − EeG2 (fe⋆ ) ≤ δfe⋆ max

n

o
′
′
EeG2 ((1 − δ)fe⋆ ) , EeG2 ((1 + δ)fe⋆ )

(Using mean-value theorem and concavity)
o
n
≤ δfe⋆ max g Ge , −g Ge + (1 + δ)2r Ge fe⋆ + (1 + δ)p−1 psG |fe⋆ |p−1

≤ δfe⋆ max g Ge , −g Ge + (1 + δ)g Ge + (1 + δ)p−1 g Ge
≤ 4δfe⋆ g Ge

≤

4δzg Ge

(Using δ ≤ 1/p)

≤ 16δ max EeG2 (fe )
fe

Rewriting, we get EeG2 (fe ) ≥ (1 − 16δ) max fe EeG2 (fe ). Rescaling δ, we obtain our claim.
We can compute such an estimate for all the edges in O( E G2 log 1/δ ) time.

D

Sparsifying Uniform Expanders

We now verify that sparsifying that sampling α-uniform expanders preserve the objectives of the
optimizations. Pseudocode of our routine and the flow maps constructed by it are in Algorithm 9.
We remark that the maps are identical to the ones used for flow sparsifiers by Kelner et al. [KLOS14].
Algorithm 9 Producing Sparsifier
1: procedure SampleAndFixGradient(G = (G, r G , sG , g G ), τ )
2:
Initialize H with V H = V G .
3:
Sample each edge of E G independently w. probability τ to form E H .
4:
Let r H ← τ · r G and sH = τ p · sG
5:
Compute the decomposition g G = gb G + B G ψ, s.t. gb G is the cycle-space projection of g G .
6:
Let ge H ← (b
g G ) , i.e ge H is the restriction of gb G to F .

 |F
†
7:
Let gb H ← I − B H B H⊤ B H B H⊤ ge H i.e. the cycle-space projection of ge H .
8:

9:

10:
11:

Let g H ← gb H + B H ψ
†
1
b H gb G⊤ f
Let MG→H be the map f → B H B H⊤ B H B G⊤ f + H
2g
kgb k2
†
Let MH→G be the map f → B G B G⊤ B G B H⊤ f + G1 2 gb G gb H⊤ f
kgb k2
return H = (V H , E H , r H , sH , g H ), MG→H , MH→G
53

†
Note that the decomposition in Line (5) can be found by first computing ψ = B G⊤ B G B G⊤ g G .
The only randomness in the Algorithm 9 is in the sampling in Line (3). In Lines (5), and (7)-(10),
when the pseudo-inverse of a Laplacian is applied, we can rely deterministically on a high-accuracy
approximation based on the fact that if the earlier sampling succeeded, both matrices are Laplacians of expanders, and hence well-conditioned. Alternatively, we can call a high-accuracy Laplacian
solver. This encurs another small failure probability. In either case, we can ensure that an implicit
representation of the operator is only computed once, and succeeds with high probability.
Theorem 4.10 (Sampling Uniform Expanders). Given an α-uniform φ-expander G = (V G E G , r G , sG , g G )
with m edges and vertex degrees at least dmin , for any sampling probability τ satisfying


1
α
+ 2
,
τ ≥ csample · log n ·
m φ dmin
where csample is some absolute constant, SampleAndFixGradient(G, τ ) w.h.p. returns a partial
instance H = (H, r H , sH , g H ) and maps MG→H and MH→G . The graph H has the same vertex set
as G, and H has at most 2τ m edges. Furthermore, r H = τ · r G and sH = τ p · sG . The maps MG→H
and MH→G certify
H κ G and G κ H,
where κ = m1/(p−1) φ−9 log3 n.

Remark D.1. If in Algorithm 9, the input gradient g G has zero cycle-space projection, i.e. gb G = 0,
then the cycle-space gradient terms in Lines (9) and (10) should be set to zero, so that MG→H
†
†
is the map f → B H B H⊤ B H B G⊤ f and MH→G is the map f → B G B G⊤ B G B H⊤ f . The
proof of this case is simpler, we omit all terms that deal with cycle-space projected gradients and
everything else stays the same as in the proof given in this section.
To prove this theorem, we first collect a number of observations that will help us. The most
basic of these is that Line (3) succeeds in producing a sparsifier in the spectral approximation
sense, and with edge set F satisfying 0.5τ m ≤ |F | ≤ 2τ m. This is a direct consequence of matrix
concentration bounds [Tro12].
Lemma D.2. Consider the edge-vertex incidence matrices with gradients (projected via ψ G ) appended as an extra column for both G and H, [B G , gb G ] and [B H , ge H ]. With high probability we have
that for any vector x
i 2
h
i 2
h
(12)
τ B G , gb G x ≈0.1 B H , ge H x
2

2

and the edge set F of H satisfies 0.5τ m ≤ |F | ≤ 2τ m.

Proof. The bounds on |F | follow from a scalar Chernoff bound. For the matrix approximation
bound, we will invoke matrix Chernoff bounds [Tro12], which give such a bound as long as the rows
of [B G , gb G ] are sampled with probaiblity exceeding csample log n times their leverage scores.
So it suffices to bound the leverage scores of the rows of this matrix. As B G and gb G are
orthogonal to each other, we can bound the leverage scores of the rows in these two matrices and
add them.
The fact that the graph (V G , E G ) has expansion φ means that its normalized Laplacian has
eigenvalue at least φ−2 . So the leverage score of a row of B G is at least φ−2 dmin . The leverage score
54

of gb Ge in gb on the other hand is at most α/m due to the α-uniform assumption. Thus, the sampling
probablity τ meets the requirements of matrix Chernoff bounds, and we get the approximation
with high probability.
Corollary D.3. Assuming Equation (12), the graphs underlying G and H (with resistances r G and
r H ) are spectral approximations of each other:
τ B G⊤ B G ≈0.1 B H⊤ B H

(13)

and the subset of gradient terms chosen after rescaling, ge H , has ℓ22 norm that’s bigger by a factor
of about τ :
τ gb G

2
2

≈0.1 ge H

2
2

.

(14)

Proof. The approximation of graphs follows from considering vectors x with 0 in the last coordinate.
The approximation of ℓ22 norms of vectors follow from considering the indicator vector with 1
in the last column and 0 everywhere else.
From this spectral approximation, we can also conclude that (V H , E H ) must be an expander,
as captured by the next corollary.
Corollary D.4. Assuming Equation (12), H has conductance at least 0.8φ.
Proof. Let CG (S) and CH (S) denote the number of edges of E G and E H respectively crossing a cut
S ⊆ V G = V H . Condition (13) implies that cuts are preserved between (V G , E G ) and (V H , E H ):
For all S ⊆ V τ CG (S) ≈0.1 CH (S), by computing the quadratic form in an indicator vector of S.
The degree of every vertex is also preserved, to up a scaling of τ and a multiplicative error 1±0.1,
i.e. τ degH (v) ≈0.1 degG (v). This follows from considering the quadratic form of Condition (13) in
the indicator vector of vertex v. This implies for any S,
P

CG (S)
CH (S)
≈0.2 P
,
v∈S degH (v)
v∈S degG (v)

from which we conclude the conductance is preserved up to a factor of 0.8.
We can also conclude from this that gb H is well-spread.

Corollary D.5. Assuming Equation (12), the projection of g H onto the cycle-space of H, gb H has
ℓ1 and ℓ2 norms that are close to τ times the corresponding terms in G:

gb H

gb H

1

2
2

≈0.2 τ gb G

2

(15)

2

≈O(αφ−6 log2 n) τ gb G

(16)

1

And gb H is O(αφ−6 log2 n)-well spread, i.e. (as |F | is the number of entries of gb H )
gb H

2

∞

≤

O(αφ−6 log2 n) H
gb
|F |
55

2
2

.

(17)

2

Proof. We first show gb H
 
0
. By Equation (12),
1
τ gb G
Thus τ gb G

2
2

≥ 0.9 gb H

2
2

2
2

2

≈0.5 τ gb G

=τ

h

2
2

i
B G , gb G x

2
2

≈0.1

h

≥ min
y

H

2

h

B H , ge H
2

= miny B G y + gb G

2

we get from the definition of gb , that
iy H 
h
2
H
H
H
= B , ge
gb
1
2
2

2

2
2

2

≈0.1 τ

h

2

G

2

= B G 0 + gb G

i
B H , ge H x

.

The definition of gb G ensures gb G

2

. Note that gb G

2

Now, consider x =

2
2

iy 
1

2
2

2

= gb H

2

.

. Letting y H ∈ arg miny B H y + ge H

B , gb

G

iy H 

1

i y
h
G
G
≥ τ min B , gb
y
1

2
2

,

2

2
2

= τ gb G

2

2
2

.

Thus we also have τ gb H ≥ 0.9 gb G , allowing us to conclude that Equation (15) is satisfied.
2
2
Next, observe that by Lemma 5.2, since H has conductance at least 0.8φ,




†

†

H
H
H⊤
H⊤ H
H
H⊤
H⊤ H
H
gb
=
B
ge
I −B B B
≤
B
ge H
I −B B B
∞

∞

≤ O(φ−3 log n) gb G

∞

∞

∞

,

where in the last step we also used ge H
≤ gb G , since the former vector consists of a subset
∞
∞
of the entries of the latter. Furthermore, by combining the above inequality with the assumption
that gb G is α-well-spread, and Equation (15) holds, we get
gb H

2

∞

≤ O(φ−6 log2 n)

α G
gb
m

2
2

≤

O(αφ−6 log2 n) H
gb
τm

2
2

.

As gb H has |F | ≤ 2τ m entries, this shows that it is O(αφ−6 log2 n)-well-spread, which establishes
Equation (17).
Next, to prove that Equation (16) holds, we first observe that for any γ-well-spread vector on
x with t coordinates, since kx k1 kx k∞ ≥ kx k22 ,
kx k1 ≥

kx k22
t
t1/2
≥ kx k∞ ≥
kx k2 .
kx k∞
γ
γ

1/2

So t γ kx k2 ≤ kx k1 ≤ t1/2 kx k2 . As gb G and gb H are α and O(αφ−6 log2 n)-well-spread respectively,
we then get
Ω(1)

|F |
(αφ−6 log2 n)2

m gb

G

gb H

2

2
2

≤

2

56

gb H
gb

G

2

1
2
1

≤ O(1)

|F | gb H
m
α2

gb

G

2

2
2 .
2

Combining this with gb H

2
2

≈0.2 τ gb G
Ω(1)

2
2

, and 0.5τ ≤

τ2

(αφ−6

log

2

n)2

≤

|F |
m

gb H
gb G

≤ 2τ , we get

2
1
2

≤ O(1)α2 τ 2 .

1

From this we can directly conclude that Equation (16) holds.
Also, we can show that for any b and θ, the optimum ℓ2 as well as ℓp energies are close to the
per degree lower bounds. We start with the lower bounds.
Lemma D.6. Consider any graph with degrees D, uniform r and s, gradient g decomposable into
g = gb + Bψ where gb is the cycle-space projection of g , and any θ, any flow f such that:
1. f has residues b: B ⊤ f = b, and

2. f has dot product θ + b ⊤ ψ with g , i.e. g ⊤ f = θ + b ⊤ ψ
must satisfy
X
e

r e f 2e + s e |f e |p ≥ Ω r · kbk2D −1

θ2
+ s · D −1 b
+r·
kb
g k22

p
∞

+s·



θ
kb
g k1

p !

.

Proof. First, note that because B ⊤ f = b, we have


gb ⊤ f = (g − B ψ)⊤ f = g ⊤ f − ψ ⊤ B ⊤ f = g ⊤ f − x ⊤ b = θ.

That is, the dot of f against gb must be θ.
The total energy is a sum of the ℓ22 and ℓpp terms. First, we will give two different lower bounds
on the ℓ22 , and can hence also conclude that the average of the two lower bounds is another lower
bound. We then do the same for the ℓpp terms and add the lower bounds together for a lower bound
on the overall objective. We will do so separately, by matching the ℓ22 terms to the electrical energy,
and the ℓpp terms to the minimum congestion.
1. kf k22 ≥ kbk2D −1 : here we use the fact that the minimum energy of the electrical flow is given
by
b ⊤ L† b,
and that the graph Laplacian is dominated by twice its diagonal
L  2D,
to get

kbk2L† ≥ kbk21 D −1 .
2

2. kf k22 ≥

θ2
kb
g k22

is by rearranging Cauchy-Schwarz inequality, which in its simplest form gives
kf k2 · kb
g k2 ≥ f ⊤ gb = |θ|.
57

p

3. kf kpp ≥ D −1 b ∞ is because if we have a residue of b u at some vertex, then some edge
incident to u must have flow at least
bu
du
on it. The p-th power of that lower bounds the overall p-norm energy.
4. kf kpp ≥ ( kbg|θ|k )p uses a similar lower bound on kf k∞ , except using Holder’s inequality on ℓ∞
1
and ℓ1 norms to obtain
kf k∞ kb
g k1 ≥ |θ|,
which rearranges to give
kf k∞ ≥

|θ|
.
kb
g k1

Before proving upper bounds on the energy required to route a flow in the graph, we state a
lemma that upper bounds the energy required to route the “electrical” component of the flow, i.e.
the projection of the flow orthogonal to the cycle space.
Lemma D.7. Consider a graph G with degrees D, conductance φ, and edge-vertex incidence matrix
†
P
B, and any demand b⊥1. Define the electrical flow f = B B ⊤ B b. Then e f 2e ≤ 2φ−2 kbk2D −1 .
The proof relies on first Cheeger’s Inequality (e.g. see [Spi18]):

Theorem D.8. (Cheeger’s Inequality) Consider a graph G with degrees D, conductance φ, and
adjancency matrix A. Then
x ⊤ (D − A)x
≤ 2φ
x ⊥D1
x ⊤ Dx

0.5φ2 ≤ min

We will also need the following helpful fact.
Fact D.9. Suppose M = X AX ⊤ where A is symmetric and X is non-singular, and that P is the
projection orthogonal to the kernel of M , i.e. P = M † M = M M † . Then M † = PX −⊤ A† X −1 P.
Proof of Lemma D.7. Note B ⊤ B = D − A = L, where A is the adjacency matrix of the graph,
†
†
†
P
and L is its Laplacian. Also e f 2e = (B B ⊤ B b)⊤ B B ⊤ B b = b ⊤ B ⊤ B b = b ⊤ L† b. By
Theorem D.8, we get that for x ⊥D1
x ⊤ Lx ≥ 0.5φ2 x ⊤ Dx .
Substituting y = D 1/2 x changes the constraint to D −1/2 y ⊥D1 i.e. y ⊥D 1/2 1. The inequality now
states
y ⊤ D −1/2 LD −1/2 y ≥ 0.5φ2 y ⊤ y .

If we let Q denote the projection orthogonal to D 1/2 1, we can summarize the inequality and
orthogonality constraint in one condition using the Loewner order as
QD −1/2 LD −1/2 Q  0.5φ2 Q.
58

Note that the null space of D −1/2 LD −1/2 is spanned by D 1/2 1, as 1 spans the null space of L. So
in fact QD −1/2 LD −1/2 Q = D −1/2 LD −1/2 , and we can conclude
D −1/2 LD −1/2  0.5φ2 Q.
From this we conclude that,
(D −1/2 LD −1/2 )†  2φ−2 Q †

as A  B implies A†  B † when A and B have the same null space. Hence by Fact D.9 and
Q = Q † , we then get
QD 1/2 L† D 1/2 Q  2φ−2 Q.

This we can rewrite as for all y⊥D 1/2 1.

y ⊤ D 1/2 L† D 1/2 y ≤ 2φ−2 y ⊤ y .
Substituting z = D 1/2 y changes the constraint to D −1/2 z ⊥D 1/2 1 i.e. z ⊥1. Thus we have that
for all z ⊥1.
z ⊤ L† z  2φ−2 z ⊤ D −1 z .
†
Taking z = b, we then get b ⊤ B ⊤ B b ≤ 2φ−2 kbk2D −1 .

Lemma D.10. Consider a graph G on n vertices with degrees D, conductance φ, and edge-vertex
†
incidence matrix B, and any demand b⊥1. Define the electrical flow f = B B ⊤ B b. Then
kf k∞ ≤ O(φ−3 log(n)) D −1 b ∞ .
Proof. We first note that if f ∗ is the optimal routing of b in G, then
D −1 b

∞

≤ kf ∗ k∞ ≤ φ−1 D −1 b

∞

,

as per Example 1.4 of [She13]. Secondly, we note that by Lemma 5.2, the electrical flow f E =
†
†
B B ⊤ B b = B B ⊤ B B ⊤ f ∗ satisfies
fE

∞


†
= B B ⊤B B ⊤f ∗

∞


†
≤ B B ⊤B B ⊤

∞→∞

kf ∗ k∞ ≤ O(φ−3 log n) D −1 b

∞

.

Lemma D.11. On an expander G with degrees D, conductance φ, and gradient g whose projection
into the cycle space of G, gb is α-well-spread, for any demand b⊥1 and dot θ with gb , the flow given
by

†
θ
f = B B ⊤B b +
gb
(18)
kb
g k22
satisfies
X
X
rf 2e +
s|f e |p ≤
e

e




|θ| 2
2
−2
+ s · m · φ−3 log n D −1 b
Op r · φ kbkD −1 +r ·
kb
g k2
59

∞

p

+s·m·

|θ|α1/2
kb
g k1

!p !

.

Proof. We first bound the quadratic term

P

2
e rf e .

†
Let us write f E = B B ⊤ B b and f C =

and note (in fact, appealing to orthogonality would save an additional factor of 2)
X
X
X
f 2e =
(f Ee + f Ce )2 ≤
2(f Ee )2 + 2(f Ce )2 .
e

e

Then we observe by Lemma D.7 that
X

θ
gb ,
kb
g k22

e

P

E 2
e (f e )

(f Ce )2

=f

≤ φ−2 kbk2D −1 . Furthermore,

C⊤ C

f =

e



|θ|
kb
g k2

2

.

Combining these equations gives
X
e

rf 2e

−2

≤ 2r · φ

kbk2D −1

We then bound the p-th power term,
X
X
X
p
s|f e |p =
s f Ee + f Ce ≤
s2p ( f Ee
e

e

e

Now by Lemma D.10, we have f E
α-well-spreadness of gb
fC

∞

=

∞

p

+ 2r ·



|θ|
kb
g k2

2

.

p

+ f Ce ) ≤ ms2p · ( f E

†
= B B ⊤B b

∞

p
∞

+ fC

≤ O(φ−3 log n) D −1 b

∞

p
)
∞

and by the

1/2
α1/2 |θ|
α1/2 |θ|
|θ|  α
2
≤
,
kb
g
k
≤
kb
g
k
≤
2
∞
kb
g k1
m1/2 kb
g k2
kb
g k22
kb
g k22 m
|θ|

where in the last step we used kb
g k1 ≤ m1/2 kb
g k2 .
Proof of Theorem 4.10. Refer to the pseudo-code in Algorithm 9. We first collect the facts that
we have established about the sampling procedure. In Line 3, E H is formed from E G by sampling
each edge independently with probability τ . It follows that the expected number of edges in E H is
τ m, and since τ > log n/m, a standard scalar Chernoff bound shows that E H ≤ 2τ m with high
probability. The parameters r H = τ · r G and sH = τ p · sG are set in Line 4. By Lemma D.2, with
high probability the sampling in Line 3 guarantees Equation (12). Note also that
• Equation (13) implies τ D H ≈0.1 D G , by considering the quadratic form in each of the standard basis vectors.
• By Corollary D.5, gb H is O(αφ−6 log2 n)-well-spread, and gb H
τ gb G

1

.

2
2

≈ τ gb G

2
2

and gb H

1

≈O(αφ−6 log2 n)

• As G has conductance at least φ, by Corollary D.4, H has conductance at least 0.8φ.

60

We can now establish G κ H. Suppose f G is a flow in G with B G f G = b and gb G⊤ f G = θ. Then
g G⊤ f G = θ + ψ ⊤ b. By Lemma D.6, we then get that
X
X
p
r G (f Ge )2 +
sG f Ge
e




≥ Ωr G · kbk2(D G )−1 + r G ·

Applying our flow map from G to H

θ2
gb G

2

p
∞

+ sG · (D G )−1 b

†

f H = MG→H (f G ) = B H B H⊤ B H B G⊤ f G +
1
gb

H

1
gb

H

gb G

1

p 
 
.

b H gb G⊤ f G

2g
2

b H θ.

2g
2

θ

+ sG · 

2

†

= B H B H⊤ B H b +

e



We note that by construction, we can readily verify B H f H = b, gb H⊤ f H = θ, and g H⊤ f H =
θ + ψ ⊤ b. So by applying Lemma D.11 to κ1 f H , we get

X
e

r

H

fH
e
κ

!2



+

X

s

H

e

≤ Op r H kbk2(D H )−1

!p

fH
e
κ



φ−1
κ

+sH · m · φ−2 (D H )−1 b


∞

1
≤ Op (τ r G ) · kbk2(D G )−1
τ
(τ p sG ) · τ −p · (D H )−1 b

2



p
∞



+ rH · 

p

2

|θ|
gb

H

2

 κ−2



κ−p + sH · m · 

|θ|(αφ−6
gb

log
H

2

n)1/2

1

p



 κ−p 


2
2
1
|θ|
φ−1
 κ−2 +
+ (τ r G ) · · 
G
κ
τ
gb
2

p
!p
!p 
2
1/p
−6
3/2
1/p
−2
|θ|  m (αφ log n)
m φ
.
+ (τ p sG ) · τ −p · 
κ
κ
gb G
1

Our goal is to ensure

X
1
g H⊤ ( f H )−
rH
κ
e

fH
e
κ

!2

+

X
e

s

H

fH
e
κ

!p 
 ≤ 1 g G⊤ f G −
κ

X
e

G

r (f

G 2
e)

+

X
e

s

G

p
f Ge

!!

Because the linear terms cancel out, we can use the upper and lower bounds established above to
say that this inequality holds provided the following conditions are satisfied (for a Cp which is a
constant greater than 1 that depends on p):
61

.

• Cp



φ−1
κ



m1/p (αφ−6 log2 n)3/2
κ

2

≤ 1/κ.

• Cp κ−2 ≤ 1/κ.
 1/p −2 p
≤ 1/κ.
• Cp m κφ
• Cp

p

≤ 1/κ.

Recalling that α is a constant, it follows that there exists a constant Cp′ (depending on p), s.t. all
of the above conditions are satisfied, provided


κ ≥ Cp′ max m1/(p−1) φ−2p/(p−1) , m1/(p−1) φ−9p/(p−1) (log n)3p/(p−1) , φ−2

And this in turn is implied by the stronger condition, κ ≥ Cp′ (m1/(p−1) φ−9 log3 n), which is hence
sufficient to ensure G κ H.
We can then show H κ G with a very similar calculation. We include it for completeness.
Suppose f H is a flow in H with B H f H = b and gb H⊤ f H = θ. Then g H⊤ f H = θ + ψ ⊤ b.
By Lemma D.6, we then get that
X
X
p
2
r H (f H
sH f H
≥
e ) +
e
e



θ2


Ωr H · kbk2(D H )−1 + r H ·

gb H




Ωr H · kbk2(D H )−1 + r H ·

Applying our flow map from H to G

2

+ sH · (D H )−1 b

p
∞

2

θ2

gb H

2

+ sH · (D H )−1 b



+ sH · 
p
∞

2


†
f G = MH→G (f H ) = B G B G⊤ B G B H⊤ f H +
†

= B G B G⊤ B G b +

1
gb

G

2

θ
gb H



+ sH · 
1

gb

G

b G θ.

2g

e

gb

1

p 
 
. ≥

θ
H

1

p 
 
.

b G gb H⊤ f H

2g
2

Again, by construction, we have B G f G = b, gb G⊤ f G = θ, and g G⊤ f G = θ + ψ ⊤ b. So by applying

62

Lemma D.11 to
X
e

rG

f Ge
κ

!2



1 G
κf ,

+

X

we get
sG

e

f Ge
κ


!p

φ−1

2



2

|θ|  −2
κ
+ rG · 
κ
gb G
2

p

−6 log 2 n)1/2

|θ|(αφ
p
 κ−p 
+sG · m · φ−2 (D G )−1 b ∞ κ−p + sG · m · 
G
gb
1
2


 −1 2
|θ|  −2
φ
κ +
+ (τ −1 r H ) · τ · 
≤ Op (τ −1 r H ) · τ kbk2(D H )−1
κ
gb H
p
 2
!p
!p 
1/p (αφ−6 log2 n)3/2
1/p
−2
m
|θ|
m
φ
p

.
(τ −p sH ) · τ p · (D H )−1 b ∞
+ (τ −p sH ) · τ p · 
κ
κ
gb H
≤ Op r G kbk2(D G )−1

1

Now, we want to guarantee

!2
X
X
f Ge
1
G
G
G⊤
r
+
sG
g ( f )−
κ
κ
e
e

f Ge
κ

!p 

 ≤ 1 g H⊤ f H −
κ

X
e

H

r (f

H 2
e )

+

X
e

s

H

f

H p
e

!!

Again the linear terms agree, and termwise verification shows that κ ≥ Cp′ (m1/(p−1) log3 (n)φ−9 ) is
sufficient to give H κ G.

E

Using Approximate Projections

Finally, we need to account for the errors in computing the cycle projections gb of the gradients g .
This error arise due to the use of iterative methods in Laplacian solvers used to evaluate (B ⊤ B)† .
As we only perform such projections on expanders, we can in fact use iterative methods. However,
a dependence of log(1/ǫ) in the error ǫ still remain.
We first formalize the exact form of this error. Kelner et al. [KOSZ13] showed that a Laplacian
solver can converge in error proportional to that of the electrical flow. That is, for a slightly higher
overhead of O(log(n/ǫ)), we can obtain a vector ge such that
kb
g − ge k2 ≤ ǫkb
g k2 ≤ ǫkg k2 .

This was also generalized to a black-box reduction between solvers for vertex solutions and flows
subsequently [CKM+ 14]. As a result, we will work this guarantee with errors relative to gb .
For the partitioning stage, this error occurs in two places: for computing the norm of the
projection, and for identifying edges with high contributions (aka. non-uniform) for removal.
For the former, a constant factor error in the norm of gb will only lead to a constant factor
increase in:
63

.

1. The uniformity of the true projected gradient,
2. The factor of decrease in the norm of the projected gradient from one step to next.
For both of these, such constant factor slowdowns can be absorbed by an increase in the thresholds,
which in turn result in a higher uniformity parameter in decompositions returned. As this uniformity parameter only affects the number of edges sampled in Theorem 4.10, they only accumulate
√
to a larger overhead in the mO(1/ p) term in the overall running time.
The other invocation of projections is in the sparsification of expanders in Algorithm 9. Here
the decomposition of g G into a circulation and potential flows is necessary for the construction of
the gradient of the sampled graph, H.
While an approximate energy minimizing circulation ge will not have g − ge being a potential
flow, we can instead perturb g slightly in this instance. Specifically, we can also compute a set of
e so that
approximate potentials ψ


e
g − ge + B ψ
≤ ǫkg k2 .
2

That is, we can perturb the initial g based on the result of this solve so that we have an exact
decomposition of it into a circulation and a potential flow. The error of this perturbation is then
incorporated in the same manner as terminating when kb
g k2 is too small in Case 2b of Theorem 4.9.
Specifically, the additive error of this goes into the additive trailing terms of the guarantees of the
ultra-sparsifier shown in Theorem 3.6.
Finally, the projection of the sampled gradient ge H into gb H also carries such an error term. By
picking ǫ to be in the 1/poly(n) range, we ensure that both the ℓ2 and ℓ1 norms of gb H is close
to their true terms. This in turn leads to constant factor errors in the lower and upper bounds
on objectives give in Lemmas D.6 and D.11, and thus a constant factor increase in the overall
approximation factors.
Therefore, it suffices to set ǫ in these approximate projection algorithms to be within poly(n)
factors of the δ by which UltraSparsify is invoked by the overall recursive preconditioning scheme.
e
The choice of parameters in Theorem 3.7 then gives that it suffices to have log(1/ǫ) ≤ O(1)
in all
projection steps. In other words, all the projections can be performed in time nearly-liner in the
sizes of the graphs.

F

ℓp -norm Semi-Supervised Learning on Graphs.

In this appendix, we briefly describe how to convert Problem (3), into a form that can be solved
using our algorithm for smoothed p-norm flows as stated in Theorem 1.1.
Recall that formally, given a graph G = (V, E) and a labelled subset of the nodes T ⊂ V with
labels s T ∈ RT , we can write the problem as
X
min
|x u − x v |p .
x ∈ℜV
s.t. x T =s T u∼v

Taking a Lagrange dual now results in the problem
X
g ⊤f −
|f uv |q .
max
⊤
f :(B f )V \T =0
u∼v
64

1
, and the gradient g is given by g = B :,T s T . We cannot directly solve this
where q = 1−1/p
formulation, since the net incoming flow at vertices in T is unknown. However, notice that the
flow is preserved at all other vertices, so summed across all of T , the net flow must be zero. Thus
if we merge all the vertices in T into one vertex, while turning edges in T × T into self-loops,
the problem is now a circulation. Note that the optimal flow on each self-loop can be computed
exactly. Now the resulting problem can be solved to high accuracy using Theorem 1.1. Meanwhile,
mapping the flow back to the original flow, it can be shown that the optimal flow arises as a simple
non-linear function of some voltages x : f e = (Bx )qe . This means that if we have f to high enough
accuracy, we can get an almost optimal set of voltages, e.g. by looking at flow along edges of a
tree to compute a set of voltages x that are a (1 + 1/poly(m)) multiplicative accuracy solution to
p
-flow problem, where
Problem (3). Since we call the algorithm of Theorem 1.1 using to solve a p−1
3/2
)

p
)
O(( p−1

p < 2, the running time will be on the order of 2
3/2
)

1
)
O(( p−1

simplified as 2

√
m1+O( p−1) ,

m

q
)
1+O( p−1
p

since p < 2. For p = 1 +

m1+o(1) .

65

. This in turn can be further

√1 ,
log n

this is time is bounded by

