LEARNING EATING ENVIRONMENTS THROUGH SCENE CLUSTERING
S. K. Yarlagadda1 , Sriram Baireddy1 , David GuÃàera1 , Carol J. Boushey2 , Deborah A. Kerr3 , Fengqing Zhu1

arXiv:1910.11367v2 [cs.CV] 10 Nov 2019

School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA1
University of Hawaii Cancer Center, Honolulu, HI, USA2
School of Public Health, Curtin University, Perth, Western Australia3
ABSTRACT
It is well known that dietary habits have a significant influence
on health. While many studies have been conducted to understand this relationship, little is known about the relationship
between eating environments and health. Yet researchers and
health agencies around the world have recognized the eating
environment as a promising context for improving diet and
health. In this paper, we propose an image clustering method
to automatically extract the eating environments from eating
occasion images captured during a community dwelling dietary study. Specifically, we are interested in learning how
many different environments an individual consumes food in.
Our method clusters images by extracting features at both
global and local scales using a deep neural network. The variation in the number of clusters and images captured by different individual makes this a very challenging problem. Experimental results show that our method performs significantly
better compared to several existing clustering approaches.
Index Terms‚Äî Image Clustering, Eating Environment,
Deep Learning
1. INTRODUCTION
In 2016, $7.5 trillion was spent on healthcare worldwide,
which is approximately 10% of the world GDP [1]. While
there are many factors that influence health, dietary habits
have a significant impact [2, 3]. To understand the complex relationship between dietary habits and health, nutrition
practitioners and researchers often conduct dietary studies
to subjectively assess dietary intake of children and adults.
Participants in these studies are asked to report the foods
and drinks they consumed on a daily basis for a period of
time. This data is then analyzed by researchers to understand
the impact of certain dietary behaviors on health. For example, studies have shown that frequent consumption of fast
food [4], skipping breakfast [5], and absence of home food [6]
contribute to the increasing risk of obesity and overweight.
While many studies have been conducted to understand
how diet affects health [7], fewer work has been done to study
the relationship between eating environments and health.
However, researchers [8, 9] and organizations such as the

(a) Scene 1

(b) Scene 2

Fig. 1: Here are images of two different eating environments,
captured by a single participant. The colored checkerboard in
all the images is the FM.

World Heath Organization and International Obesity Task
Force have recognized the vital role of eating environments
on health and diet. Studies have shown that some factors of
the environment such as screen viewing during meals, family mealtime [10], and meal preparation time [11] influence
health. For instance, family mealtime is shown to positively
affect nutrient intake and meal preparation time is inversely
related to Body Mass Index (BMI). While such patterns have
been found, the relationship between eating environments
and health is still poorly understood, partly due to the lack
of valid, reliable measures of environmental factors. In this
paper, we focus on understanding eating environments of an
individual using digital images captured during eating occasions. In particular, we are interested in learning how many
different environments an individual consumes food in.
Dietary Recall, 24-hr recall, and Food Frequency Questionnaire (FFQ) [12] are well-known dietary assessment
methods used in most dietary studies. These methods require
participants to manually enter details of their diet information
through a web interface, or an in-person or phone interview.
These methods are known to be time-consuming and prone to
errors because participants may not recall all foods and bev-

Fig. 2: Scatter plot of the number of images captured per participant versus the number of eating scene clusters per participant.

eral classical [20] and deep learning-based [21] methods have
been proposed. Classical clustering approaches are applied on
relevant features extracted from the data [22]. However, the
problem of clustering food images based on the eating environments captured has not been studied before, so the relevant
features are not defined. On the other hand, deep learningbased methods, given sufficient data, are capable of simultaneously learning to extract relevant features and cluster images [21]. A common assumption of such approaches is that
the number of clusters is known a priori [21]. However, in our
case, we do not know a priori the number of eating environments for each participant. In addition, the number of images
collected by each participant is not sufficient to apply deep
learning-based methods directly, as they usually require hundreds of images per cluster for training a good model. As both
classical and deep learning-based approaches have their shortcomings for clustering eating environments from images, new
techniques need to be developed to solve this problem.
3. METHOD

erages they consumed or cannot accurately estimate the food
portion [12]. To overcome these limitations, researchers have
leveraged advances in mobile technology to develop imagebased dietary assessment methods to record daily food intake.
Some examples of image based dietary assessment tools are
TADA‚Ñ¢ [13] and Food-Log [14].In these approaches, participants record their diet by capturing images of foods and
beverages consumed using mobile cameras. These images
can then be analyzed by trained analysts [15] or using image
analysis methods [16, 17, 18] to extract nutrient information.
In this paper, we leverage eating occasion images captured
using the TADA‚Ñ¢ system to cluster eating scenes based on
their visual appearance to help understand the relationship
between a person‚Äôs eating environment and dietary quality.
2. DATASET AND RELATED WORK
We used a dataset D that consists of 3137 images from
66 participants collected in a community dwelling dietary
study [19]. In this study, participants were asked to take
pictures of their foods and to place a colored checkerboard
with known dimensions, called the Fiducial Marker (FM), in
the scene. An example of two pairs of images belonging to
two different eating environments are shown in Fig. 1. The
FM serves two purposes: to aid in color correction, and more
importantly, to provide a reference scale for food portion estimation, so the nutrient content of the foods can be computed.
One of the challenges of this dataset is the large variance in
the number of images and the number of eating environments
for different participant, as shown in Fig. 2.
Our goal is to cluster food images of a participant based
on eating environments. Clustering is not a new concept. Sev-

In this section, we describe the details of the proposed
method. First, we introduce the notation used throughout
the paper. Pk denotes the k th participant in the dataset. The
k
ith image captured by participant Pk is denoted by xP
i . Our
goal is to cluster food images based on eating environment.
We do this by extracting features at the local and global level
k
from relevant pixels of xP
and then applying a clustering
i
method on said features.
3.1. Global Feature Extraction
k
The image xP
i contains many salient objects such as the food,
drinks, FM, and silverware. However, pixels belonging to the
salient objects do not contain any credible information regarding the eating environment because a person can eat different food with different plates in the same eating environment.
Pixels belonging to the FM are also not relevant because the
FM is common to all food images, irrespective of the eating
environment. Instead, we are interested in pixels belonging
to the non-salient regions. To extract these relevant pixels in
Pk
k
xP
i , we first extract its binary saliency map denoted by si
using a state-of-the-art saliency estimator R3NET [23]. The
Pk
k
salient pixels of xP
i have a value of 1 in si and the rest have
Pk
k
a value of 0. The relevant pixels of xi are captured by xÃÉP
i ,
and is defined as

Pk
Pk
k
xÃÉP
i = (1 ‚àí si ) ‚àó xi

(1)

k
Features are extracted from xÃÉP
i using a Convolutional Neural Network (CNN) VGG16 [24] pre-trained on the ImageNet
dataset [25]. We use a pre-trained VGG16 for feature extraction because these features are robust to artifacts such as
noise, lighting changes, and differing viewpoints.

VGG16

(

ùë•' )
R3NET

Global Feature Extraction

(
+
ùë•' )

Detect FM
and
(
+
Obtain ùë•' )

VGG16

(
ùë†' )

(

ùëî' )

Cropped Region
(
+
around FM from ùë•' )

(

ùëô' )

Local Feature Extraction

Fig. 3: Overview of feature extraction
The 2nd convolutional layer is chosen for feature extraction and the reasoning behind this is explained in Section 4.1.
Global Average Pooling (GAP) is applied to the output of the
2nd layer to spatially summarize the information into a 64dimensional vector. This vector is denoted as giPk and we
refer to it as the global feature.

3.2. Local Feature Extraction
Since the FM is always placed in close proximity to the foods,
we assume it is on the same surface as the foods. Examples
of such surfaces may include desk, dining table, and kitchen
counter, to name a few. These surfaces give us a lot of information about the eating environment because it is very likely
that a person uses the same surface during an eating event in a
particular eating environment. Therefore, we extract features
from this surface and they are denoted by liPk .
By identifying the FM in the image, we can extract information about the eating surface. The FM is detected by
finding the salient object with the highest number of interest
points. An interest point is a pixel that ORB (Oriented FAST
and Rotated BRIEF) [26] finds useful for image registration.
The FM has a lot of interest points because of the colored
checkerboard pattern and it is unlikely for other salient objects such as foods to have as many distinct interest points.
The salient objects of the image can be found by performing
k
connected component analysis on sP
i and treating each connected component as a salient object. Once we have located
Pk
k
the FM, we extract a region around it from xÃÉP
is obi . li
tained from this region using the pre-trained VGG16 in the
same way as done in 3.1. Fig. 3 illustrates the the global and
local feature extraction process.

3.3. Feature Fusion and Clustering
We fuse the local and global features using their distance matrices. The distance matrices GPk and LPk are defined as
follows
Pk
Pk
k
LP
(i,j) = ||li ‚àí lj ||2
Pk
Pk
k
GP
(i,j) = ||gi ‚àí gj ||2

(2)

The fused distance matrix DPk is defined as follows
DPk = Œ± ‚àó LPk + (1 ‚àí Œ±) ‚àó GPk

(3)

Here Œ± ‚àà [0, 1] and controls the relative importance of the
local and global features. A higher value of Œ± indicates local features are more important and vice versa. In our case ,
Œ± = 0.44. The reason behind this is explained in Section 4.1.
Affinity Propagation (AP) [27] is applied to DPk to obtain the
final clusters.

4. EXPERIMENTS
In this section we describe all the experiments conducted
and compare our method to four existing clustering methods,
namely: DBSCAN [28], MeanShift [29], OPTICS [30], and
AP [27]. We use the Adjusted Rand Index (ARI) and Normalized Mutual Information (NMI) to measure the accuracy
of our clusters. ARI ranges from ‚àí1.0 to 1.0 while NMI
ranges from 0 to 1.0. Higher values indicate better clustering
for both ARI and NMI. Accuracy over a dataset is reported as
the average accuracy among all participants.

ble 1, where our method achieves the best performance. It
is worth noting that although we use AP [27] for clustering
after feature fusion, our method performs significantly better
than AP [27] when meaningful features are not known. This
indicates that our feature extraction strategy is highly relevant
and very important to our problem.
Table 1: ARI and NMI scores for methods tested on Dtest .
The best results are reported in bold.

Fig. 4: ARI scores for different Œ± and convolutional layer m
on Dval

4.1. Hyperparameter Tuning
Our method has two hyperparameters: the weighting factor
for feature fusion Œ± and the convolutional layer of VGG16
for feature extraction m. Our dataset D is split into Dval and
Dtest . Dval consists of images from 10 participants and Dtest
contain images from 56 participants. Our method is evaluated
on Dval by varying Œ± and m. Œ± ranges from 0 to 1 in steps
of 0.01. To find the optimal m for our dataset, we extract
features from convolutional layers preceding a max-pooling
layer. There are five such layers in VGG16. ARI is used
to select the optimal values. Fig. 4 shows the ARI scores
for different values of Œ± and convolutional layer m, and the
optimal value for Œ± is 0.44 and for m is 2.
From equation 3, we can infer that as the value of Œ± increases, more weight is given to local features and vice-versa.
An optimal value of 0.44 for Œ± suggests that our method performs best when approximately equal weight is given to both
local and global features. This shows that using only one of
these features is less optimal. In Fig. 4, we can see that the
performance of our method degrades once the chosen feature
extraction layer gets very deep. Later layers, like conv-13, extract abstract features and completely lose information about
edges, colors, and textures. We suspect this loss of information is the reason for decrease in performance.

Methods

ARI

NMI

Ours
DBSCAN [28]
MeanShift [29]
OPTICS [30]
AP [27]

0.39
0.24
0.12
0.08
0.2

0.68
0.47
0.35
0.39
0.49

5. CONCLUSION
We proposed a method to cluster food images based on their
eating environment. Our method extracts features from a pretrained CNN at multiple levels. These features are fused using
their distance matrices and a clustering algorithm is applied
after feature fusion. Our method is evaluated on a dataset
containing 3137 eating scene images collected from a dietary
study with a total of 585 clusters. We compared our method
to state-of-the-art clustering methods and showed improved
performance.
6. REFERENCES
[1] World Health Organization et al., ‚ÄúPublic spending on
health: a closer look at global trends,‚Äù Tech. Rep., World
Health Organization, 2018.
[2] A. E. Mesas and et al., ‚ÄúSelected eating behaviours and
excess body weight: a systematic review,‚Äù Obes Rev,
vol. 13, no. 2, pp. 106‚Äì135, Feb 2012, 21955734[pmid].
[3] Karin NordstroÃàm, Christian Coff, HaÃäkan JoÃànsson,
Lennart Nordenfelt, and Ulf GoÃàrman, ‚ÄúFood and health:
individual, cultural, or scientific matters?,‚Äù Genes Nutr,
vol. 8, no. 4, pp. 357‚Äì363, Jul 2013, 23494484[pmid].

4.2. Testing

[4] Garcia Ginny and et al., ‚ÄúThe fast food and obesity link:
consumption patterns and severity of obesity,‚Äù Obes
Surg, vol. 22, no. 5, pp. 810‚Äì818, May 2012.

We evaluated the performance of our method on Dtest after
selecting the optimal values for Œ± and m using the validation set Dval . We choose four well-known clustering methods
for comparison, namely DBSCAN [28], MeanShift [29], OPTICS [30], and AP [27] using the eating scene image as the
input. ARI and NMI for the five methods are reported in Ta-

[5] Deshmukh-Taskar PR1 and et al., ‚ÄúThe relationship of
breakfast skipping and type of breakfast consumption
with nutrient intake and weight status in children and
adolescents: the National Health and Nutrition Examination Survey 1999-2006,‚Äù J Am Diet Assoc, vol. 110,
no. 6, pp. 869‚Äì878, Jun 2010.

[6] Amber J. Hammons and Barbara H. Fiese, ‚ÄúIs frequency
of shared family meals related to the nutritional health
of children and adolescents?,‚Äù Pediatrics, vol. 127, no.
6, pp. e1565‚Äì1574, Jun 2011.
[7] Weiqing Min and et al., ‚ÄúA survey on food computing,‚Äù ACM Comput. Surv., vol. 52, no. 5, pp. 92:1‚Äì92:36,
Sept. 2019.
[8] Sarah L. Booth and et al., ‚ÄúEnvironmental and societal factors affect food choice and physical activity: Rationale, influences, and leverage points,‚Äù Nutrition Reviews, vol. 59, no. 3, pp. S21‚ÄìS36, 2001.
[9] James O. Hill and et al., ‚ÄúObesity and the environment:
Where do we go from here?,‚Äù Science, vol. 299, no.
5608, pp. 853‚Äì855, 2003.

[19] Deborah A Kerr and et al., ‚ÄúThe connecting health
and technology study: a 6-month randomized controlled
trial to improve nutrition behaviours using a mobile food
record and text messaging support in young adults,‚Äù The
international journal of behavioral nutrition and physical activity., vol. 13, no. 1, 2016.
[20] Rui Xu and D. Wunsch, ‚ÄúSurvey of clustering algorithms,‚Äù IEEE Transactions on Neural Networks, vol.
16, no. 3, pp. 645‚Äì678, May 2005.
[21] E. Min and et al., ‚ÄúA survey of clustering with deep
learning: From the perspective of network architecture,‚Äù
IEEE Access, vol. 6, pp. 39501‚Äì39514, 2018.

[10] Brian Wansink and Ellen Van Kleef, ‚ÄúDinner rituals
that correlate with child and adult bmi,‚Äù Obesity (Silver Spring, Md.), vol. 22, 05 2014.

[22] Seema Wazarkar and Bettahally N Keshavamurthy, ‚ÄúA
survey on image data analysis through clustering techniques for real world applications,‚Äù Journal of Visual
Communication and Image Representation, vol. 55, pp.
596‚Äì626, 2018.

[11] Brad Appelhans and et al., ‚ÄúMeal preparation and
cleanup time and cardiometabolic risk over 14years in
the study of women‚Äôs health across the nation (swan),‚Äù
Preventive medicine, vol. 71, 12 2014.

[23] Zijun Deng and et al., ‚ÄúR3net: Recurrent residual refinement network for saliency detection,‚Äù International
Joint Conference on Artificial Intelligence, pp. 684‚Äì690,
July 2018.

[12] Jee-Seon Shim and et al., ‚ÄúDietary assessment methods
in epidemiologic studies,‚Äù Epidemiol Health, vol. 36,
pp. e2014009‚Äìe2014009, Jul 2014, 25078382[pmid].

[24] Karen Simonyan and Andrew Zisserman, ‚ÄúVery deep
convolutional networks for large-scale image recognition,‚Äù International Conference on Learning Representations, May 2015, San Diego, CA.

[13] Fengqing Zhu and et al., ‚ÄúThe use of mobile devices in
aiding dietary assessment and evaluation,‚Äù IEEE Journal of Selected Topics in Signal Processing, vol. 4, no.
4, pp. 756‚Äì766, Aug 2010.
[14] Kiyoharu Aizawa and Makoto Ogawa, ‚ÄúFoodlog: Multimedia tool for healthcare applications,‚Äù IEEE MultiMedia, vol. 22, no. 2, pp. 4‚Äì8, Apr 2015.
[15] Erica Howes and et al., ‚ÄúImage-Based Dietary Assessment Ability of Dietetics Students and Interns,‚Äù Nutrients, vol. 9, no. 2, pp. 114, feb 2017.
[16] Fengqing Zhu and et al., ‚ÄúMultiple Hypotheses Image Segmentation and Classification With Application
to Dietary Assessment,‚Äù IEEE Journal of Biomedical
and Health Informatics, vol. 19, no. 1, pp. 377‚Äì388, jan
2015.
[17] Fang, Shao, Kerr, Boushey, and Zhu, ‚ÄúAn End-to-End
Image-Based Automatic Food Energy Estimation Technique Based on Learned Energy Distribution Images:
Protocol and Methodology,‚Äù Nutrients, vol. 11, no. 4,
pp. 877, apr 2019.
[18] Yu Wang and et al., ‚ÄúContext based image analysis
with application in dietary assessment and evaluation,‚Äù
Multimedia Tools and Applications, vol. 77, no. 15, pp.
19769‚Äì19794, aug 2018.

[25] Jia Deng and et al., ‚ÄúImagenet: A large-scale hierarchical image database,‚Äù 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248‚Äì255, June
2009.
[26] Ethan Rublee and et al., ‚ÄúOrb: An efficient alternative
to sift or surf.,‚Äù in ICCV. Citeseer, 2011, vol. 11, p. 2.
[27] Brendan J Frey and Delbert Dueck, ‚ÄúClustering by passing messages between data points,‚Äù science, vol. 315,
no. 5814, pp. 972‚Äì976, 2007.
[28] Martin Ester and et al., ‚ÄúA density-based algorithm
for discovering clusters in large spatial databases with
noise.,‚Äù in Kdd, 1996, vol. 96, pp. 226‚Äì231.
[29] D. Comaniciu and P. Meer, ‚ÄúMean shift: a robust approach toward feature space analysis,‚Äù IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.
24, no. 5, pp. 603‚Äì619, May 2002.
[30] Mihael Ankerst and et al., ‚ÄúOptics: ordering points
to identify the clustering structure,‚Äù in ACM Sigmod
record. ACM, 1999, vol. 28, pp. 49‚Äì60.

