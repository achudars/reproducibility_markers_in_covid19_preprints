Finding Effective Geo-Social Group for Impromptu Activity
with Multiple Demands
Lu Chen† , Chengfei Liu† , Rui Zhou† , Jiajie Xu§ , Jianxin Li¶
†

arXiv:1912.08322v1 [cs.DB] 18 Dec 2019

†

Swinburne University of Technology, ¶ Soochow University, § Deakin University

{luchen,

cliu, rzhou}@swin.edu.au § xujj@suda.edu.cn ¶ jianxin.li@deakin.edu.au

ABSTRACT
s

Geo-social group search aims to find a group of people proximate to a location while socially related. One of the driven
applications for geo-social group search is organizing an impromptu activity. This is because the social cohesiveness
of a found geo-social group ensures a good communication
atmosphere and the spatial closeness of the geo-social group
reduces the preparation time for the activity. Most existing works treat geo-social group search as a problem that
finds a group satisfying a single social constraint while optimizing the spatial proximity. However, when an impromptu
activity has additional demands on attendees, e.g., the activity requires that the attendees have certain set of skills,
the existing works cannot find an effective geo-social group
efficiently. In this paper, we study how to find a group
that is most proximate to a query location while satisfying
multiple constraints. Specifically, the multiple constraints
on which we focus include social constraint, size constraint
and keyword constraint. We propose a novel search framework which first effectively narrows down the search space
with theoretical guarantees and then efficiently finds the optimum result. Although our model considers multiple constraints, novel techniques devised in this paper ensure that
search cost is equivalent to parameterized constant times of
one time social constraint checking on a vastly restricted
search space. We conduct extensive experiments on both
real and semi-synthetic datasets for demonstrating the efficiency of the proposed search algorithm. To evaluate the
effectiveness, we conduct two case studies on real datasets,
demonstrating the superiority of our proposed model.

i: k3 h: k2
g: k1
f: k3 s: k1 p: k2
d: k1 e: k2
a: k1 c: k3
t: k2
b: k2 j: k3

q: k4

m: k4

p
m

q
o

k

l

n

b a
c d fj
e i
h g

λ
t

n: k2 k: k2 r: k3

u

(a) Graph data
(b) Spatial distribution
Figure 1: Graph with location and keyword
to find a group that is socially cohesive while spatially closest to a location, i.e., the found group satisfies a single social
constraint while optimizing a distance objective function for
most works. This is different from most social-aware spatial
search works [21, 14, 1, 6] that consider various objectives
together as an aggregate objective function and find the result that is optimum w.r.t. the aggregate function. One of
the most motivating applications for geo-social group search
is instant organization of impromptu activities. This is because two nice proprieties of geo-social groups. Firstly, the
social cohesiveness of a geo-social group ensures the members are socially close within the group, which is key to ensure a good communication atmosphere for the activity. Secondly, subjecting to social cohesiveness, a geo-social group
is the one that is closest to the location of the activity, which
reduces the waiting time for the activity potentially. However, since most of the existing geo-social group studies only
focus on social constraint while optimizing the spatial closeness, they become less useful when an activity has more
demands, e.g., demanding attendees with certain skills and
demanding minimum number of attendees. Let us consider
one of the application scenarios below.
Online open-world game data: finding participants for a
real time quest. For online open-world game data, each
player is associated with a friend list, an attribute describing the role of a player, and location information showing
his/her location in the virtual world. Suppose there is a real
time quest requested in a randomly location with duration
of 15 minutes. The quest has a set of suggested roles and
suggests that each role shall have no less than 2 players for
accomplishing the quest. The gaming system would like to
formulate a group of participants who are adequate to carry
out the quest. Who shall be the players in the group?
To effectively find the desired geo-social group for the
above scenario, extra factors shall be considered thoroughly

PVLDB Reference Format:
. . PVLDB, (): xxxx-yyyy, .
DOI:

1.

l: k2 o: k1 u:k3

r

INTRODUCTION

As the geo-social networks become popular, finding geosocial groups has drawn great attention in recent years. In
general, geo-social group search problem [15, 2, 26, 16] aims
This work is licensed under the Creative Commons AttributionNonCommercial-NoDerivatives 4.0 International License. To view a copy
of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. For
any use beyond those covered by this license, obtain permission by emailing
info@vldb.org. Copyright is held by the owner/author(s). Publication rights
licensed to the VLDB Endowment.
Proceedings of the VLDB Endowment, Vol. , No.
ISSN 2150-8097.
DOI:

1

in addition to social and spatial closeness, i.e., the minimum number of players for each suggested role. If there are
more demands, the effort to coordinate them increases substantially. As such, it is imperative to devise efficient novel
techniques to alleviate the effort for planning or organising
activities with multiple demands. A specific motivating example is shown below.
Example 1. Figure 1 illustrates the gaming data, which
consists of graph data in Figure 1(a) and spatial data in
Figure 1(b), when a real time quest is happening at the location labelled as λ. The graph data contain friendships for
players and the current role of players in terms of keyword.
The spatial data contain the current location for each users.
Let the quest has a suggested role of {k1 , k2 , k3 } and has a
suggested minimum number of players, say, 2 for each suggested role. Below, we show the desired result and the results
found by the most related models.

substantially large, resulting poor performance.
Challenges. As discussed above, a general effective framework for searching geo-social group with multiple polynomial checkable constraints is required in urgency. This arises
challenges as follows. Firstly, can we have a search framework that can narrow the search space fast while preserving the correct result? Secondly, can we have a theoretical
bound for the size of the narrowed search space? Thirdly,
given the specific constraints in MKASG, can we reduce the
time complexity of multi-constraint checking approach to
constant times of single constraint checking?
Our approach. In this paper, we devise a novel search
framework for effectively finding geo-social group with multiple constraints. This search framework contains expanding and reducing stage. The expanding stage addresses
the first two challenges. It approaches to a search space
that is sufficient large to contain the optimum result at a
cost equivalent to constant times of the time complexity of
multi-constraint checking. The approached search space is
no greater than the size of the optimum search space with
a ratio of parameterized constant, which vastly restricts the
search space for the reducing stage. For the reducing stage,
we adapt the method proposed in [12]. For MKASG search
problem, within the proposed search framework, we further
devise novel techniques including keyword aware truss union
and keyword aware spanning forest, which reduce the overall
search complexity, including expanding and reducing stages,
to constant times of the social constraint checking. This addresses the third challenge. We also propose novel pruning
techniques that further improve the search performance as
much as possible.
Contribution. Our predominant contributions in this paper are summarised as follows.
• We study finding geo-social group with multiple constraints,
considering minimum keyword, social acquaintance and
size constraints while preserving its spatial proximity to
a specific query location.
(Section 2)
• We devise an effective search framework for multiple constraints geo-social group search problem, which first approaches to the region containing a group stratifying all
constraints and then reduces the group to MKASG to
guarantee the spatial proximity.
(Section 4)
• For the expanding stage, we propose a power law based
expanding strategy which ensures that the evaluated search
space of the expanding range is restricted. We further
propose effective techniques including search region lower
bound, and keyword aware truss union-find operation to
speed up this stage.
(Section 5)
• For the reducing stage, we propose novel keyword frequency aware spanning forest, which guarantees the total
cost of the reducing stage to its lower bound for MKASG
search.
(Section 6)
• We conduct extensive experiments on real datasets to
demonstrate the efficiency and effectiveness of the proposed algorithm and geo-social group model. (Section 7)

The desired group for accomplishing the quest is the subgraph enclosed by the dashed rectangle in Figure 1(a). The
players found within the group have strong relationship while
preserving spatial proximity to the query location λ. Simultaneously, the group contains players satisfying all roles
recommended by the quest and the group also has sufficient
players, i.e., two players for each suggested role. Considering a single social constraint (e.g., k-core [10]), geo-social
group works such as [26] tend to find the nearest group satisfying the social constraint, i.e., {a, c, b, j} induced subgraph
in Figure 1(a). Considering a social constraint and the exact group size constraint, existing works such as [16, 6] are
likely to find {a, b, c, j, l, t} induced subgraph in Figure 1(a).
None of them can find the group as the desired one since
they do not consider that the activity has multiple demands
as discussed above.
Geo-social group with multiple constraints. The example motivates us to study a novel type of geo-social group
search problems for impromptu activities with multiple demands, and propose efficient solutions. In particular, the
model which we study finds a group with multiple constraints induced by various demands of an activity while
preserving that the group is most spatially proximate to
the activity location, in which the spatial proximity is measured by the distance of the person in the group that is
most distant to an activity location. The multi-constraint
geo-social group search problem that we focus on is to find
MKASG - a Group of people with Minimum requirements of
Keyword cohesiveness, Acquaintance (social strength) and
size while preserving its Spatial-proximity to a given location. We name this problem as MKASG search problem.
Existing search framework. Most existing approaches [15,
2, 26] for finding a geo-social group mainly based on the
nearest neighbour search framework. This framework progressively adds vertices that potentially satisfy social constraint according to nearest neighbour order (w.r.t. the activity location), while checking the social constraint after
each vertex is added. It returns the optimum result when it
finds a subgraph satisfying the social constraint for the first
time. This framework is efficient when considering a single
social constraint. When coming to geo-social group with
multiple constraints, this framework becomes less attractive since some constraints, e.g., minimum size constraint
discussed above, may enlarge the size of desired geo-social
group. This makes the times of multi-constraint checking

2.

PROBLEM FORMULATION

In this section we formulate MKASG with social, keyword
and size constraints and MKASG search problem. Some of
other constraints on geo-social group that can be solved by
our proposed method will be discussed in Section 6.3.
Data. We model data with network structure, spatial attribute and textual attribute as an undirected graph G =
2

(V, E). G has a set of vertices (users) V and a set of edges
(friendships) E. For each vertex v ∈ V (G), v has a piece of
location information expressed as latitude and longitude denoted as (v.x, v.y), and has a keyword denoted as v.A that
describes the current role of v.
We formally define the query for searching MKASG.
Query for MKASG. We allow users to give a query Q consisting of a query location λ, a set of keywords ϕ that describe the roles of the desired group members, an integer
parameter ρ for defining minimum size of the group, and an
integer parameter c that defines social cohesiveness.
Multiple constraints for MKASG. Now we define the
multiple constraints of MKASG, given an MKASG query.
Social constraint. We consider minimum trussness to measure the social cohesiveness of an MKASG S ⊆ G. Trussness
is defined based on the number of triangles each edge is involved in a graph. In general, given a subgraph S ⊆ G, we
use 4uvw to denote a triangle consisting of vertices u, v, w ∈
V (S).
Support. The support of an edge e(u, v) ∈ E(S), denoted by sup(e, S), is the number of triangles containing
e, i.e., sup(e, S) = |{4uvw : w ∈ N (v, S) ∩ N (u, S)}|, where
N (v, S) and N (u, S) are the neighbours of u, v in S correspondingly.
Minimum subgraph trussness. The trussness for a subgraph S is defined as an integer c that is 2 plus the minimum
possible support for edges in E(S). That is, the minimum
subgraph trussness defines that for every edge e ∈ E(S), the
number of triangles in which e participates shall be no less
than c - 2.
Based on the definition of trussness, we define the c-truss
constraint of an MKASG S as follows:
Definition 1. c-truss constraint. An MKASG S satisfies c-truss constraint if the trussness of S is c, and S is
connected.

Definition 2. Minimum ρ keyword vertex constraint
Given an integer ρ, ϕ and S, S satisfies minimum ρ keyword
vertex constraint if: min{|Ski ||∀ki ∈ ϕ, Ski ⊆ S} ≥ ρ.
With the minimum ρ keyword vertex constraint, the size
of a group is no less than ρ × |ϕ|. In the following of this
paper, we call minimum ρ keyword vertex constraint as keyword vertex constraint.
Searching objective for MKASG search. Now, we formalize the spatial proximity measurement for MKASG and
the research problem studied in this paper.
Spatial proximity. Given a query location λ, we consider a
distance function to measure the closeness between λ and
an MKASG S as:
Definition 3. Distance measurement.
dist(λ, S) = max{kλ − vk|v ∈ V (S)},
where kλ − vk denotes Euclidean distance between v and λ.
Definition 4. (ρ, c, d)-truss. Given a Q = {λ, ρ, ϕ, c} and
a distance threshold d, a subgraph S ⊆ G is a (ρ, c, d)-truss,
if it satisfies all the constraints below.
• min{|V (Ski )||∀ki ∈ ϕ, V (Ski ) ⊆ V (S)} ≥ ρ.
• S satisfies c-truss constraint.
• dist(λ, S) ≤ d.
Research Problem. MKASG search. Given a query Q =
{λ, ρ, ϕ, c} and G, return (ρ, c, d)-truss S ∗ so that there is
no (ρ, c, d0 )-truss S 0 with d0 ≤ d.
Example. Come back to Example 1, and set a query for
MKASG search with λ, ϕ = {k1 , k2 , k3 }, ρ = 2, c = 4.
MKASG, denoted as S ∗ , is the subgraph in the doted area.
It is a 4-truss subgraph, and for every keyword in ϕ there
are no less than two members whose attributes match the
keyword. It is also the group closest to λ subject to the
social, keyword and size constraints.

Intuitively, if S satisfies c-truss constraint, the vertices of
an edge in S have at least c-2 common neighbours in the
group S, every vertex in S has no less than c-1 edges and
at least c-1 edges have to be deleted in order to make S
disconnected. An S with a large value c indicates strong
internal social relationships over vertices.
Example. For instance, in Figure 1(a), the whole graph is
a 4-truss. Every edge in this graph involves no less than 2
triangles.
Keyword constraint. We adopt the concept of collective keyword coverage to measure the keyword cohesiveness between
the keyword attributes of V (S) and query keywords ϕ.
Collective keyword coverage. Given a group S and the
query keywords ϕ, the attributes of V (S) collectively cover
ϕ if and only if ∪v∈V (S) v.A = ϕ.
Minimum size constraint. In real application, we could allow users to specify the minimum size of the group directly.
However, this is likely to result in that the attributes of the
found group members overemphasize on part of ϕ, which is
undesired. To mitigate such effect, we propose an alternative
approach defining the minimum size of the group together
with the keyword constraint. We introduce the definition of
minimum ρ keyword vertex constraint.
Given a set of keyword ϕ = {k1 , . . . , k|ϕ| }, a social group
S, and let V (Ski ) ⊆ V (S) be the set of vertices in V (S)
containing keywords ki ∈ ϕ, the minimum ρ keyword vertex
constraint is defined as follows.

3.

BASELINE SOLUTIONS

In this section, we discuss three baseline solutions that
find the exact result.
Incremental approach. Given a query, this approach progressively includes a vertex into a candidate set according
to nearest neighbour order w.r.t. the query location. Every
time a vertex is added into the candidate set, this approach
checks if there is a subgraph induced by vertices in the candidate set that satisfies all constraints. If there is one, the
approach stops and returns the subgraph as result. Otherwise, this approach keeps on exploring the vertices in order.
This method has a time complexity of O ( |V (G)| |E(G)|1.5 ).
The dominated cost is induced by repeatedly checking ctruss constraint and keyword vertex constraint.
Decremental approach. Borrowing the technique proposed in [12], a baseline with better time complexity can
be derived. This approach progressively deletes the vertex
most distant to λ. When a most distant vertex is deleted,
this approach further deletes edges that do not satisfy trussness constraint. This ensures that every time before deleting
the next most distant vertex, the remaining subgraphs are
still c-truss. To adapt this approach for our problem, after trussness checking, for the remaining truss subgraphs we
further check if there is connected c-truss satisfying both
size and keyword vertex constraint using depth-first search.
The decremental approach progressively deletes the most
distant vertex and performs the multi-constraint checking
3

Algorithm 1: searchMKASG(Q,H)
s

r
p

m
n k

q
o
l
d1
λ
t

b a
d f
c
e i j
d∗

h

i: k3 h: k2
g: k1
f: k3 s: k1 p: k2
d: k1 e: k2
a: k1 c: k3

1
2

l: k2 o: k1 u:k3

3

g

t: k2
b: k2 j: k3

q: k4

4

m: k4

5
6

n: k2 k: k2 r: k3

*/

0

7

u

8

(a) Spatial distribution
(b) H≤d1 and H≤d∗
Figure 2: Graphs for H≤d1 , H≤d∗

9

until there is no subgraph that satisfies all constraints simultaneously. The last subgraph that satisfies all constraints
becomes the result.
The time complexity of this approach is O (|V (G)||E(G)|
+ |E(G)|1.5 ). This approach can reduce the cost of truss
checking. But, it suffers from exploring large search space.
Binary search based approach. This approach progressively guesses a distance d via binary search. For a distance
d, this approach checks if there is a subgraph that satisfies
all constraints in the subgraph induced by vertices having
distance no greater than d to the query location λ. If there
is one, this approach reduces the d to d2 and continues. If
there is no such a subgraph, this approach increases d to
d0 −d
where d0 is the last evaluated distance and checks the
2
corresponding subgraphs. For any two consecutive evaluated d0 and d, if there is no vertices having distance to λ
between d0 and d, the search stops and the last subgraph
satisfying all constraints becomes the result. To support retrieve subgraphs based on d efficiently, we use R-tree index
in this method.
The time complexity of this approach is O (log2 (|V (G)|)
|E(G)| + log2 (|V (G)|)|E(G)|1.5 ). The major drawback of
this approach is that its search space is large even though it
can approach to the optimum result fast.
Discussion. The advantage of incremental approach is if
the result is near to the query location, the search space is
quite restricted. The advantage of the decremantal approach
is that it can reduce the cost of truss computation. The
advantage of binary search based approach is it can quickly
approach to optimum result in the worst case. Clearly, an
ideal search framework shall take all the advantages. This
motivate us to devise a novel framework that only explores
restricted area, approaches to the optimum result fast and
reduces multi-constraint checking as much as possible.

4.

Input: H, Q
Output: S ∗
d ← initial search distance for H≤d ;
d∗ ← ∞, S ∗ ← ∅ ;
/* Expanding stage
S ← ispcTrussIn(H≤d ) ;
while S is ∅ do
H≤d0 ← newRange(d);
S ← ispcTrussIn(H≤d0 );
S ∗ ← S, d ← d ;
/* Reducing stage
S ∗ ← redcuepcTruss(S ∗ );
return S ∗ ;

*/

bounded graph, denoted as H≤d , is the subgraph of H induced
by vertices of H with distance to λ no greater than d.
We would like to highlight a special instance of d radius
bounded graph, d∗ radius bounded graph (H≤d∗ ), which
is the d radius bounded graph just large enough to contain MKASG for a query, i.e., there is no H≤d0 such that
H≤d0 contains MKASG and d0 < d∗ . We refer H≤d∗ as optimum search space since it is just large enough to contain
MKASG for the query.
For instance, in Figure 2, H≤d1 and H≤d∗ are demonstrated. d1 and d∗ identified regions are displayed in Figure 2(a), i.e., cycles centred by λ with radius of d1 and d∗
respectively. The subgraphs are shown in Figure 2(b), i.e.,
H≤d1 is the subgraph in doted area and H≤d∗ is the subgraph in grey coloured area. H≤d∗ is the optimum search
space containing MKASG for the query in Example 1.
Next we show the search framework for MKASG. It firstly
approaches to a H≤d0 just sufficient large to constrain H≤d∗
quickly. Then it reduces H≤d0 to the optimum result.
The framework. As shown in Algorithm 1, MKASG search
framework consists of two stages: expanding stage (lines 3
to 7) and reducing stage (line 8). During the expanding
stage, Algorithm 1 intends to quickly identify H≤d that is
just sufficiently large to contain the optimum search space
H≤d∗ by exploring H≤d that progressively gets larger, in
which isptTrussIn is called to determine the existence of a
subgraph satisfying all constraints. For the reducing stage,
to get the optimum result, reducepcTruss attempts to
progressively remove the vertex that is the most distant to
λ in S ∗ . The last survived (ρ, c)-truss during the vertices
removing process is the optimum result.
In the following sections, we will discuss details of the
two stages. We will propose techniques that make expanding stage having the time complexity of one time calling
of isptTrussIn. For the reducing stage, we will propose
novel online index and combine the index with our proposed reducing strategy to efficiently check all constraints
of MKASG. Eventually, our proposed techniques can guarantee that Algorithm 1 has a time complexity of one time
truss computation.

SEARCH FRAMEWORK

Before showing the search framework, we firstly introduce
a pre-pruning technique and some definitions.
Maximal (ρ, c)-truss based pruning. A maximal (ρ, c)truss is a (ρ, c, d)-truss that cannot be extended by adding
either an edge or a vertex while considering d as ∞.
Given an MKASG query containing parameters ρ and c, it
is clear that MKASG for the query can only reside in a maximal (ρ, c)-truss if it exists. As such, given the MKASG query
and G, computing maximal (ρ, c)-truss subgraphs contained
in G would reduce the search space significantly. This can
be done by traversing maximal c-truss subgraph with the
state of the art truss technique [25].
Definition 5. d radius bounded graph. Given a query
location λ, a subgraph H and a distance threshold d, d radius

5.

EXPANDING STAGE

In this stage, we explore a set of d radius subgraphs, starting from a relatively small d radius subgraph and stopping
at the first d radius subgraph that is a super graph of H≤d∗ .
Challenges. Since expanding stage involves expensive constraint checking, our first challenge is how to devise an expanding strategy that can elegantly bound the overall computations tightly? On the other hand, if we can expand to
4

d∗ with less number of attempts, the performance will be improved. This can be achieved by starting the search from a
d radius graph with d that is close to but no greater than d∗ .
This arises the second challenge: can we identify such initial
search range efficiently? At last, when processing an H≤d
during the expanding stage, if we apply multi-constraint
checking just on some restricted subgraphs of H≤d that potentially contain a (ρ, c)-truss, the search performance can
be further boosted. This arises the third challenge on how
to quickly identify those potential subgraphs in H≤d ?
In the following sub-sections, we will address these three
challenges consecutively.

Algorithm 2: Finding lower bound search range

1
2
3
4
5
6
7
8
9
10
11

5.1

12

Expanding Strategy

13
14

In this part, we propose an expanding strategy which can
bound the total amount of subgraphs that will be evaluated.
We first define an expanding invariant as follows.
Definition 6. ∆ size invariant. Let {d1 , d2 , . . . , di } be
the series of radius for defining d radius graphs, for any two
consecutive d, d0 in the series, we define ∆ invariant as
∆=
in which ∆ > 1 must hold.
The strategy. The strategy applied for the expanding
stage is to maintain ∆ size invariant over any two consecutively evaluated H≤d , H≤d0 . Applying ∆ invariant for expanding stage guarantees two nice properties below.
Property 1. Nearest first search. Vertices accessed by
the expanding stage are in non-increasing order according to
their distance to λ on a batch basis.
Property 2. Power law expansion [3]. The sizes of
the set of d radius graphs follow power law expansion, i.e.,
{H≤d1 , . . . , H≤di } equals {|H≤d1 |∆0 , . . . , |H≤d1 |∆i−1 }.
The two properties help us introduce and prove a lemma
as follows.
Lemma 1. Let H≤di−1 , H≤di be the last two d radius subgraphs evaluated by the expanding stage, we have |E(H≤di−1 )|
< |E(H≤d∗ )| < |E(H≤di )|.
The correctness is clear. Firstly, when expanding, Properties 1 and 2 hold. Secondly, the expanding stage stops when
Hdi is the first d radius subgraph containing a (ρ, c)-truss.
Next, we establish precise relationship between |E(H≤d∗ )|
and |E(H≤di )| via the lemma below.
Lemma 2. Let H≤di be the last d radius subgraph evaluated
|E(H≤d )|
i

|E(H≤d∗ )|

5.2

such it equals to
1
1
1− ∆

1
1− ∆

*/

Initial Expanding Range

Intuitively, if the initial search range is close to d∗ , the
total amount of subgraphs that has to be evaluated to approaching H≤d∗ is less. This motivates us to study a lower
bound of d radius subgraph.
We define the lower bound d radius subgraph, denoted as
H≤d defined as follows.
Definition 7. H≤d . A subgraph H≤d of H is a lower bound
d radius subgraph of H≤d∗ if it satisfies conditions: 1) H≤d
is connected, 2) H≤d satisfies keyword vertex constraint and
3) there is no H 0 ⊆ H≤d such that H 0 satisfies the first two
constraints and dist(λ, H 0 ) < dist(λ, H≤d ).

< ∆ holds.

Now, let us show the tight bound that is guaranteed by
applying the proposed expanding strategy.
Lemma 3. Let (H≤d1 , . . . , H≤di ) be the set of d radius
subgraphsPevaluated in order by the expanding stage, the in∆
)|E(H≤di )| must hold.
equality ij=1 |E(H≤dj )| ≤ (1 + ∆−1
P
Proof sketch. Since we have ∆ invariant, ii=1 E(H≤dj )
is essentially the sum of a geometric progression with a
1
and a scale factor of |E(H≤di )|. As
common ratio of ∆
1 )i
1−( ∆

*/

Discussion. With Lemma 3, the correctness of the following statement is clear. The running time of lines 3 to 8 in
Algorithm 1 is proportional to (1 + ∆a + ∆a1−1 ) × the time
complexity of ispcTrussIn(H≤d∗ ), where a is determined
by the time complexity of ispcTrussIn(H≤d∗ ) (later on we
show a equals 1.5). This provides a tight bound for the expanding stage if we can access every H≤d locally during the
loop of lines 3 to 8. As such, we will introduce techniques
that ensure local explanation during the expanding stage.
Local exploration. We propose a structure aiding us to
retrieve H≤d for some d radius subgraph with time liner to
|E(H≤d )|. We firstly show lemma as follows.
Lemma 4. For any maximal connected (ρ, c)-truss H and
fixed query, there is a structure that takes O(|E(H)|) space,
that can be built in O(|V (H)| log(|V (H)|)) time, and that
retrieves E(H≤d ) in O(|E(H≤d )|) time.
The structure. The structure is an array of edges in E(H)
with non-decreasing order according to their distances to
query location, where the distance from λ to an edge (u, v) is
measured the same as Definition 3. To create the structure,
we firstly sort the vertices in H taking O (|V (H)| log2 (|V (H)|)).
And then arrange edges into appropriate position. For different maximal connected (ρ, c)-truss H, we sort them separately and then merge together to speed up the performance.
With the structure, for consecutive evaluated d and d0 ,
we can easily retrieve H≤d0 based on H≤d with time liner to
|E(H≤d0 ) \ E(H≤d )|.

|E(H≤d0 )|
,
|E(H≤d )|

by the expanding stage, the inequality

Input: H
Output: H≤d
/* W.o.l.g, DIST (λ, u) ≤ DIST (λ, v)
foreach (u, v) ∈ sorted edge list of H do
maintain adjacency list;
initialise set rooted as u and v if necessary;
ru ← find(u), rv ← find(v);
/* W.o.l.g, ru.rank ≤ rv.rank
if ru 6= rv then
standard union opertion;
f lag ← true;
foreach k ∈ ϕ do
ru.k ← ru.k + rv.k;
if ru.k < ρ then
f lag ← f alse;
if flag then
H≤d ← maintained adjacency list;
return H≤d ;

H≤d relaxes the structure constraint of MKASG. As such,
it can be computed efficiently, discussed below.
Finding lower bound d radius subgraph. Algorithm 2
demonstrates the major steps for finding H≤d . It is a refined union-find process [19]. We augment the union-find

|E(H≤di )| and is no greater than

∆
|E(H≤di )|, which can be expressed as (1+ ∆−1
)|E(H≤di )|.

5

data structure with keyword vertex frequency. Algorithm 2
progressively performs union operations on edges in nonincreasing order of their distance to λ. By union operations,
vertices that are connected are added into the same set.
Each set is attached with keyword vertex frequency for each
keyword. When an edge (u, v) is being evaluated, Algorithm 2 first finds if u and v are contained in the same set
in existing union-find structure (lines 3 to 5). If not, the
two sets containing u and v shall be connected via standard
union operation and keyword vertex frequency of the two
sets shall be aggregated (lines 8 to 9). Due to the space limitation, the discussion for union-find operations is omitted.
After a union operation, if there is a set satisfying keyword
vertex constraint, we find H≤d . Otherwise, Algorithm 2
continues.
Time complexity. The time complexity of Algorithm 2
is O(α(|V (H≤d∗ )|)|E(H≤d∗ )|), where α(|V (H≤d∗ )|) ≤ 5 is
the cost of one union-find operation [19] and there are at
most |E(H≤d∗ )| number union-find operations. Additionally, checking keyword vertex constraint can be considered
taking constant time assuming |ϕ| is small.
Example. Figure 3 shows the keyword-aware union-find
structure maintained by Algorithm 2 for the query in Example 1. Each of the sets in terms of trees in the keywordaware union-find structure indicates a connected component
the current subgraphs. After (f, h) is added, the tree rooted
by h becomes the first connected component satisfying the
keyword vertex constraint. The induced subgraphs of vertices in the trees are displayed in Figure 3(b).
Alternative initial bound. We may also relax the keyword vertex constraint to derive an alternative bound, i.e.,
considering the smallest H≤d containing a connected c-truss
as a lower bound. But, this bound is costly to compute.

{k1 : 2, k2 : 2, k3 : 2}

c

e g

i

d

t

b n

i: k3

h: k2

d: k1

e: k2

g: k1

u
k

f

a: k1

f: k3

c: k3

u: k3
t: k2

a

n: k2 k: k2

b: k2

(a)
Figure 3: Finding H≤d

(b)

ρ potential subgraph P≤d . A subgraph P≤d ⊆ H≤d is defined
as ρ potential subgraph if it is connected, satisfies keyword
vertex constraint and is maximal within H≤d .
The strategy. Since a (ρ, c)-truss should reside in P≤d , we
propose lazy (ρ, c)-truss checking strategy that applies (ρ, c)truss constraint checking on every P≤d in H≤d only instead
of the entire H≤d .
Identifying all P≤d can be done almost at no cost by using keyword aware union-find structure discussed in Algorithm 2. That is, when expanding H≤d to H≤d0 , vertices
in edges of H≤d0 are progressively added to keyword aware
union-find structure. As such, the ρ potential subgraphs
in H≤d0 can be retrieved easily since every set in keyword
aware union-find structure satisfying keyword vertex constraint identifies a ρ potential subgraph.
For instance, in Figure 3, after all edges in H≤d are retrieved, the ρ potential subgraph for the query in Example 1
is {h, f, e, g, i, d} induced subgraph. As such, according
to lazy (ρ, c)-truss checking strategy, we only apply (ρ, c)truss checking on this potential subgraph. In contrast, we
will not apply (ρ, c)-truss checking on subgraph induced by
{a, b, c, t, h, k, u}.
Next, we show how to address the second drawback. Please
be noted, the computation discussed below shall be performed on ρ potential subgraphs only. The size of these
subgraphs is vastly restricted compared to the size of H≤d .
Union with existing truss. To avoid graph traversing for
checking keyword vertex constraint and connectivity after
updating trussness, we propose a solution below. Firstly,
we maintain every maximal connected c truss subgraph in
every P≤d , each of which is attached with keyword vertex frequency. Secondly, after P≤d is expanded to P≤d0 ,
we update the maintained c-truss subgraphs if applicable.
Although this approach cannot update trussness for existing truss subgraphs precisely, it is sufficient and efficient to
check the existence of (ρ, c)-truss in P≤d0 . As such, keyword vertex constraint and connectivity checking for truss
subgraphs can be performed simultaneously and incrementally. We give formal explanations below and focus on truss
unions for expanding a P≤d to P≤d0 . Since all P≤d in H≤d
are disjoint, the truss union for expanding a P≤d to P≤d0
can be easily extended to truss unions for expanding H≤d
to H≤d0 .
Existing truss C≤d . We maintain connected c-truss subgraphs
C≤d ⊆ P≤d if they exist. For each C≤d ∈ C≤d , its keyword
vertex frequencies for every keyword in ϕ are recorded.
Truss potential subgraph. After expanding P≤d to P≤d0 . We
only compute maximal truss subgraphs in truss potential
subgraph defined below.

5.3

Checking (ρ, c)-truss in d Radius Subgraph
In this section, we show the detailed implementation of
checking (ρ, c)-truss in a d radius subgraph H≤d , i.e., the
procedure isptTruss in Algorithm 1.
To simplify the discussion, for any two consecutive H≤d
|H

{k1 : 1, k2 : 3, k3 : 1, k4 : 1}

h

0|

and H≤d0 with |H≤d | = ∆, let us introduce a new notation
≤d
Hd0 \d to denote the subgraph of H≤d0 induced by vertices
appearing in edges of E(H≤d0 ) \ E(H≤d ).
Baseline approaches. For checking whether there is any
(ρ, c)-truss in H≤d , one baseline approach is to compute the
trussness for the entire H≤d , and traverse c truss subgraphs
to further verify keyword vertex constraint and connectivity.
A better approach is for any two consecutive H≤d and H≤d0 ,
we update trussness for H≤d according to the difference between H≤d0 and H≤d and traverse the updated c-truss for
checking keyword vertex constraint and connectivity.
The two baseline approaches suffer from two drawbacks.
Firstly, trussness for the whole H≤d is computed/updated.
As such for the parts of H≤d that cannot contain MKASG,
the truss computation is wasted. Secondly, checking keyword vertex constraint and connectivity has to traverse the
whole H≤d . If we can perform the check incrementally, the
performance can be improved. We propose novel techniques
to address the two drawbacks.
To address the first drawback, we propose lazy (ρ, c)-truss
checking strategy as follows.
Lazy (ρ, c)-truss checking strategy. Given H≤d , we only
apply (ρ, c)-truss checking on any subgraph potentially containing (ρ, c)-truss, defined as ρ potential subgraph below.

Definition 8. Truss potential subgraph. Given two consecutive P≤d ⊆ H≤d and P≤d0 ⊆ H≤d0 with C≤d ⊆ P≤d , the
truss potential subgraph is defined as T Pd\d0 = H≤d0 (V 0 ),
where V 0 is the set of vertices appearing in E(P≤d0 ) \ E(C≤d ).
6

Algorithm 3: incIspcTrussIn(H≤d , d0 )
i: k3

h: k2

g: k1

i: k3
f: k3

e: k2

d: k1

{k1 : 2, k2 : 2, k3 : 1}

h

h: k2

g: k1

f: k3
e: k2

d: k1

{k1 : 2, k2 : 2, k3 : 2}

h

1
2

e g

i

d

f

e g

i

3

d

4

(a)
(b)
Figure 4: Truss union

5
6

The sufficiency of T Pd\d0 is clear since it contains all triangles in P≤d0 for edges that are not in C≤d but potentially
lead to (ρ, c)-truss.
Truss union. Based on Definition 8, for consecutive P≤d and
P≤d0 , we compute maximal truss subgraphs in T Pd\d0 and
then add them to C≤d via union operation, which forms C≤d0 .
Example. In Figure 4, we show an example for truss union
operation. In Figure 4(a), let {g, d, e, i, h} induced subgraph
be H≤d and its P≤d and C≤d are the same graph. Let the
whole graph in Figure 4(a) be H≤d0 . Then, P≤d0 is {g,
d, e, i, h, f } induced subgraph, and E(Pd0 ) \ E(C≤d ) is {
(f, e), (f, h), (f, i) }. Then T Pd0 \d is {i, h, f, e} induced
subgraph shown in Figure 4(b). Since there is a c-truss in
{i, h, f, e} induced subgraph, truss-union data structure in
Figure 4(a) (in terms of tree structure) is updated to the
one in Figure 4(b).
Next, we show the (ρ,c)-truss checking algorithm with the
proposed techniques.
The (ρ,c)-truss checking algorithm. The principal steps
of (ρ,c)-truss checking are shown in Algorithm 3.
Data structure. Since Algorithm 3 is called iteratively, it
works on progressively refined data structures including adjacency list of H≤d , the keyword aware union-find denoted as
U F≤d storing every ρ potential subgraph, the keyword-aware
truss union-find structure denoted T U F≤d storing maximal
connected k-truss with aggregated keyword frequency. All
those data structures are empty sets before the first time
when Algorithm 3 is called.
Principal steps. Algorithm 3 adds each edge in Hd0 \d to
H≤d0 and performs union operation on each edge to U F≤d0 ,
where the edges of Hd0 \d can be retrieved easily with the
sorted array proposed in Lemma 4. After that, Algorithm 3
computes maximal c-truss subgraphs in the truss potential subgraph defined in Definition 8 (line 5). More precisely, with U F≤d0 and T U F≤d0 , T Pd0 \d is H≤d0 (V 0 ), where
V 0 are the vertices appearing in E( ∪P≤d0 ∈U F≤d0 P≤d0 ) \
E(∪C≤d ∈T U F≤d0 C≤d ). Next, Algorithm 3 performs truss union
operations for the computed maximal c truss subgraphs. After the truss union, if there is a set in T U F≤d0 that satisfies
keyword vertex constraint, then there is a (ρ, c)-truss and
Algorithm 3 returns the (ρ, c)-truss S (line 9). Otherwise,
Algorithm 3 returns ∅.
The correctness of Algorithm 3 is clear according to the
techniques discussed above.
Time complexity. The time complexity of Algorithm 3
is O(|E(H≤d0 )|1.5 ). Computations between lines 2 to 4 are
dominated by keyword aware union-find operations that are
O(|E(Hd\d0 )|), and it is the same for lines 6 to 7. The
dominating part is line 5. In the worst case, T Pd0 \d could
be the same as H≤d0 . This results in O(|E(H≤d0 )|1.5 ) time
complexity for Algorithm 3.

7
8
9
10
11

Input: H≤d , d0
Output: S
/* U F≤d : keyword aware union find structure storing all ρ
potential graphs in H≤d
*/
/* U F≤d : keyword aware truss union find structure storing
all truss subgraphs in H≤d
*/
H≤d0 ← H≤d , U F≤d0 ← U F≤d , T U F≤d0 ← T U F≤d ;
foreach (u, v) ∈ Hd0 \d do
H≤d0 ← H≤d0 ∪ {{u, v}};
U F≤d0 ← U F≤d0 ∪ {(u, v)}; // union
/* Pd0 \d has been generated during updating U F≤d0
*/
H 0 ← compute c-truss in T Pd0 \d ;
foreach (u, v) ∈ H 0 do
T U F≤d0 ← T U F≤d0 ∪ {(u, v)}; // truss union
if T U F≤d0 contains a set satisfies keyword constraint then
return the set as S ;
else
return ∅;

To conclude the expanding stage, we show lemma below.
Lemma 5. The time complexity of expanding stage is O
((1 + ∆1.5 + ∆1.51 −1 ) × |E(H≤d∗ |1.5 ).
The correctness is clear based on the time complexity of
Algorithm 3 and Lemma 3. When ∆ = 2, the time complexity becomes the minimum that is O(|E(H≤d∗ )|1.5 ).

6.

REDUCING STAGE

For the reducing stage, we focus on searching MKASG in
the (ρ, c)-trusses found by the expanding stage, denoted as
S. We would like to revisit that the size of S is O(|H≤d∗ |).
Intuitively, this stage progressively removes the vertex in
S that is most distant to the query location till there is no
(ρ, c)-truss in the remaining S. The last survived (ρ, c)-truss
is MKASG.
Efficiently checking the existence of (ρ, c)-truss after deleting a vertex is challenging. This is because after a vertex
deletion, we have to deal with truss computation, verifying
keyword vertex constraint and checking connectivity. The
obvious time consuming part is truss computation, which
can be bounded nicely by taking the advantage of decremental truss computation. The pitfall when analyzing the
cost is ignoring the cost of keyword vertex constraint and
connectivity checking. Actually, a graph traversing-based
implementation for checking them can lead to complexity
of O (|V (H≤d∗ )| |E(H≤d∗ )|), which is worse than the time
complexity of truss computation and becomes the performance bottleneck of MKASG search.
We will propose efficient approach for checking multiple
constraints together.

6.1

Reducing Strategy

In this part, we show the reducing strategy.
The strategy. Algorithm 4 shows the major steps of the
strategy for the reducing stage. It progressively removes the
vertex that is most distant to λ (the query location) in S
and checks the existence of (ρ, c)-trusses in the remaining
of S after the deletion. If there exists one, Algorithm 4
continues to delete next most distant vertex in S. Otherwise,
Algorithm 4 returns the last (ρ, c)-truss as MKASG.
Clearly, the strategy can find MKASG in S correctly since
Algorithm 4 maintains an invariant that every time deleting
the most distant vertex in S, S contains set of (ρ, c)-trusses.
This invariant is ensured by our proposed pcTrussChecking in Algorithm 4. That is, after the most distant vertex
7

Algorithm 4: reducepcTruss(Q,S)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

18
19
20

{k1 : 2, k2 : 2, k3 : 2}

i

Input: S: (ρ, t) − truss
Output: S ∗
sort vertices in S according to their distance to λ in none
decreasing order;
foreach u ∈ V (S) do
S 0 ← pcTrussCecking(S, u);
if S 0 6= ∅ then
S ← S 0 ; // order preserved
else
return S as S ∗ ;
Procedure pcTrussChecking (S, u)
Q ← ∅;
foreach v ∈ N (u, S) do
Q ← Q ∪ {(u, v)};
while Q 6= ∅ do
(u, v) ← Q.pop();
foreach w ∈ N (u, S) ∩ N (v, S) do
update triangle numbers for (w, u), (w, v) ;
put (w, u), (w, v) into Q if they cannot be part of
c-truss;
remove (u, v) from S ;
/* Checking connectivity and keyword constraints
*/
if ckChecking((u, v)) then
return ∅;
return the remaining S;

i

h

fg

g
e

d

f
e

d

(a)

(b)

{k1 : 2, k2 : 2, k3 : 2}

{k1 : 2, k2 : 2, k3 : 1}

i

i

h

g

h

f g

f

1
d

e

d

e

(c)
(d)
Figure 5: keyword aware spanning forest
Subcase 1: cannot link the cut trees. In this subcase, we
cannot find a replacement edge from the remaining S to link
the two trees, which means the subgraph referred by the two
trees becomes two disjoint subgraphs. We update keyword
vertex frequency for each of the cut tree. After the update,
we safely prune the cut tree from the maintained spanning
forest if it does not satisfy keyword constraint since they
cannot contribute to MKASG.
Subcase 2: can link the cut trees. If we can find a replacement edge, the subgraph referred by two cut trees is
still connected. We link the two trees with the replacement
edge. Keyword vertex frequency remains the same.
It is clear that the above idea can correctly maintain all
connected subgraphs satisfying keyword vertex constraint if
they exist after deleting an edge from S. But, it is challenging to preform the maintenance efficiently since checking the
existence of a replacement edge could be costly.
To make the maintenance efficient, we borrow the idea
from [8]. Given S, every edge in E(S) is associated with
a level progressively increased as edges are deleted, which
is equivalent to progressively partitioning S hierarchically.
Edges with high level refer to a more restricted part of S.
In contrast, edges with low level refer to a more general
part of S (super graphs of the high level subgraphs). As
such when deleting an edge with a certain level, we do not
need to consider any edge with lower level as a replacement
edge, which elegantly reduces the search space for finding a
replacement edge.
We first use an example to demonstrate our method.
Example. Suppose we have the input graph as shown in
Figure 5(a), and we want remove vertex f . The minimum
spanning forest is shown in Figure 5(a) with edges in solid
lines and the edges not in the spanning forest are shown as
dashed lines. We do not show the level of an edge if its level
is 0. Removing f is equivalent to remove edges incident to
f . It is trivial to remove (i, f ) and (g, f ) since they are
not a part of the spanning forest. After that, supposing
that we remove (h, f ) shown as grey line in Figure 5(b),
the spanning tree becomes two trees where the tree with
vertices of {f, e} is the smaller tree and the level of the edge
in the tree is increased by 1. By checking edges incident to f
and e, we find a replacement edge (h, e). By connecting the
two trees, the spanning tree becomes the one in Figure 5(c).
Next, we remove (e, f ) shown in Figure 5(c) and the tree
with vertex only f is the smaller tree. In this case, we
cannot find any edge incident to f , leading to Figure 5(d).
We know the graph becomes separated and we also know
that there is a connected component in the remaining graph

is deleted, we further delete edges violating the minimum
trussness requirement. Meanwhile, for each edge deletion,
we immediately check whether the remaining subgraphs contain a connected subgraph satisfying keyword vertex constraint. If no, we stop edge deletions and return empty set
since no (ρ, c)-truss exists. If yes, we exclude all the other
subgraphs since they cannot lead to MKASG.
It is clear to see that the time complexity of Algorithm 4
consists of the trussness computation cost and keywordaware connectivity checking cost. The former is bounded
by O (|E(H≤d∗ )|1.5 ) since Algorithm 4 takes the advantage of decremantal truss computation and we have shown
that S returned by the expanding stage will be no greater
than O (|E(H≤d∗ )|). The later is dependent on the cost of
ckChecking called in pcTrussChecking, Algorithm 4.
In the following subsection, we focus on proposing techniques for devising efficient ckChecking (Algorithm 5),
which makes the total cost of keyword-aware connectivity
checking is less than O (|E(H≤d∗ )|1.5 ). As such, the proposed strategy embedding with elegant techniques devised
by us can bound the total cost of multi-constraint checking
in the reducing stage as O(|E(H≤d∗ )|1.5 ).

6.2

{k1 : 2, k2 : 2, k3 : 2}

h

Keyword-aware Connectivity Checking

In this section, we show how to efficiently check the existence of a connected subgraph satisfying keyword vertex
constraint after an edge is deleted induced by removing the
most distant vertex in Algorithm 4.
High level idea. We will maintain a minimum spanning
forest for S (input of Algorithm 4) augmented with aggregated keyword vertex frequency. Notice that initially, every spanning tree in the forest satisfies keyword vertex constraint. After an edge is deleted from S, one of the two cases
below may happen.
Case 1: the deleted edge is not in the forest. In this case, the
remaining subgraphs are still connected and each connected
subgraph still satisfies keyword vertex constraint.
Case 2: the deleted edge is in the forest. In this case, one of
the tree in the minimum spanning forest is cut into two
trees, which may lead to one of the following subcases.
8

Algorithm 5: ckChekcing((u, v))
1
2
3
4

5
6
7
8
9

10
11
12
13
14
15
16
17
18
19
20
21

22
23
24

25

26
27

into to subtrees, Algorithm 5 always increases the levels of
edges in the smaller tree by 1. As such, the worst case
is that every time a tree is split, the two trees are equal
size, leading to largest possible size of a tree at level l as
b |V 2(S)|
c. This invariant guarantees that the level of an edge
l
is no greater than log2 |V (S)|, which is the key for time
complexity analysis.
More detailed steps are given below.
Given that (u, v) with level l is to be deleted, Algorithm 5
firstly checks whether it is in the current forest or not.
Case 1: . If (u, v) is not in F≥0 , (u, v) is deleted (line 3), the
algorithm return true .
Case 2: . If (u, v) is in F≥0 , Algorithm 5 deletes it from the
tree containing (u, v) from level l which is the highest forest
it is in.
Performing tree cut (lines 10 to 11). The tree is cut into
two subtrees Tu and Tv , and levels of edges in the smaller
tree in terms of number of vertices are increased by 1. Next,
Algorithm 5 propagates the deletion from F≥l+1 to F≥0 so
that from the view at all the levels, the tree is split.
Searching a replacement edge (line 13 to 20). After performing tree cut, Algorithm 5 starts to search a replacement
edge of (u, v) that may connect Tu to Tv . This is achieved
by searching all edges incident to vertices appearing in Tu .
To maintain the minimum spanning forest property, Algorithm 5 searches an alternative edge from level l. If an edge
(v, w) is incident to Tu but v and w are in Tu then its level
is increased by 1.
Subcase 1: cannot link the cut trees (lines 24 to 26). If no
replacement edge is found, Tu induced subgraph is isolated.
Aggregated keyword frequencies are adjusted. If the there
is a tree violating the keyword vertex constraint after the
adjustment, it is pruned.
Subcase 2: can link the cut tress (lines 21 to 23). If a
replacement edge is found, the incident edge (v, w) shall
link Tu to Tv and this edge is inserted to F≥l to F≥0 so that
from the view of all the levels, the tree is linked.
Next we further discuss data structures used in our implementation which are useful for time complexity analysis.
Data structure. In Algorithm 5, to efficiently deal with
tree cut and tree link operations, we store spanning forest
as Euler tours and the Euler tours are stored as balanced
binary search tree [7]. As such, each operation of tree cut
and tree link can be performed in O(log2 (|V (S)|)).
Time complexity analysis. The time complexity of index initialisation is O(|E(S)|log2 (|V (S)|)). The time complexity of Algorithm 5 for deleting E(S) number of edges
is O(|E(S)|log22 (|V (S)|)). Lines 7 to 9 in the algorithm
have the time complexity of O(|E(S)|log2 (|V (S)|)). This
is because for each edge, its level is at most log2 (|V (S)|).
The dominating parts are lines 14 to 23 and lines 24 to 27
in the algorithm since they perform up to O(log2 (|V (S)|))
number of cut or link operations and each has a cost of
O(log2 (|V (S)|)), which results O(|E(S)|log22 (|V (S)|)).
Due to the space limitation, the discussion of obvious
prunings is omitted.

Input: (u, v), F
Output: True or False
l = (u, v).level;
if (u, v) ∈
/ F≥0 then
delete (u, v) directly;
return True;
/* adjust level of edges
*/
Tu , Tv ← delete (u, v) from F≥l ;
assume |V (Tu )| ≤ |V (Tu )|;
foreach e ∈ E(Tu ) do
e.l ← e.l + 1;
progressively calculate aggregated keyword vertex frequency
in Tu ;
for i ← l + 1 to 0 do
cut F≥i ;
/* search the replacement edge of (u, v)
*/
altE ← ∅;
for i ← l to 0 do
for v ∈ Tu do
/* v.adjGi is level-aware adjacency list
*/
for w ∈ v.adjGi do
if w ∈ V (Tu ) then
increase level of (v, w) by 1;
else
// find the replacement edge
altE ← (v, w), altE.l ← i;
Break ;
if altE 6= ∅ then
/* alternative edge is found and update the minimum
forest
*/
for i ← altE.l to 0 do
Link corresponding two subtrees in Fi via altE;
else
/* graph is split and keyword frequencies shall be
updated
*/
update aggregated keyword frequencies of Tv in F0
according to the aggregated keyword vertex frequencies of
Tu ;
prune trees in S do not satisfy keyword vertex constraint;
return True If at least one of Tv in F0 and Tu satisfies keyword
constraint else return False;

with keyword frequencies of {k1 :2, k2 :2, k3 :1}. Without
using the proposed method, we cannot simultaneously know
the keyword vertex frequency and the connectivity of the
subgraph after deleting f .
Now, let us describe the method formally. We first introduce the keyword aware spanning forest.
Keyword aware spanning forest. The minimum spanning tree for every connected (ρ, c)-truss in S from the expanding stage is computed and stored, in which each spanning tree is augmented with keyword vertex frequency. As
discussed, initially every spanning tree in this forest (F ) satisfies keyword vertex constraint and level for every edge in
S is assigned as 0. Below, we use F≥i to denote the forest
of edges with level at least i.
The algorithm. Algorithm 5 guarantees that after an edge
deletion, every remaining minimum spanning tree in the
keyword aware spanning forest satisfies keyword vertex constraint. It returns true if the keyword aware spanning forest
is not an empty set. Otherwise it return ∅. To efficiently
achieve that, Algorithm 5 maintains invariants as follows.
Invariant 1. F≥0 ⊇ F≥1 ⊇, . . . , ⊇ F≥lmax always holds.
This invariant ensures no duplicated trees are generated.
Invariant 2. F≥i is a minimum spanning forest for edges
with level at least i induced subgraphs. This invariant maximizes the possibility that a deleted edge is not in the maintained forest.
Invariant 3. The number of vertices in F≥l is always no
c. This is because when a tree is split
greater than b |V 2(S)|
l

6.3

Search Algorithm Wrap-Up

We formally claim the lemma as follows.
Lemma 6. The time complexity of MKASG is the maximum
of O (|E(H≤d∗ )|1.5 ) and O(|V (H)| log2 |V (H)|).
The correctness is clear given the discussion throughout
this paper. In practical, our proposed algorithm is much
9

Parameter
c
|ϕ|
ρ

Range
3, 4, 5, 6, 7, 8
1, 3, 5, 7, 9
1, 3, 5, 7, 9

Default value
6
3
3

MKASG-. This algorithm is a simplified version of MKASG.
For the expanding stage, it only applies power law expansion
and for the reducing stage it does not applies the proposed
online index. This is used to show the power of the search
framework used in this paper.
For all the baseline approaches, we apply (ρ, c)-truss based
prunings in prior.

Table 1: Parameter settings
Dataset
Gowalla
Brightkite
Foursquare
Weibo
Yelp
WoW

#vertices
196,591
58,228
4,899,219
1,019,055
257,532
278

#edges
950,327
214,078
28,484,755
32,981,833
957,711
752

#checkins
6,442,890
4,491,143
1,021,970
32,981,833
431,563
278

cmax
29
43
16
11
21
36

7.2

Table 2: Statistic information in datasets
faster since we propose many optimizations that prune search
spaces as much as possible. We shall evaluate those optimisations in experimental studies.
Below, we introduce some of other possible constraints
that can be solved efficiently by the proposed search framework and then establish the lower bound for the proposed
framework.
Alliterative keyword constraints. We can use Jaccard
similarity to measure the keyword similarity between the attributes of a vertex and the query keywords firstly and then
set minimum similarity threshold as the keyword constraint
for the desired geo-social group.
Alliterative size constraints. We can directly set a minimum size as the size constraint for a geo-social group. Or we
set the minimum vertex frequency for each of the keyword
vertex to define the size constraint.
Alliterative social constraints. Our proposed method
supports social constraint defined as k-core, or more generalized cohesive constraint (k, s)-nucleus.
Influential constraints. Beside keyword, size and social
constraints, we can consider member influence as a factor
for finding the geo-social group. Assuming each vertex in a
group has an influential score, we set minimum influential
score threshold as the influential constraint for the desired
geo-social group.
Given multiple polynomial checkable constraints, let us
use O(Cmax ) to denote the dominating time complexity for
checking all the constraints. We are ready to establish a general lower bound for geo-social group search problem with
multiple constraints using the proposed search framework.
Lemma 7. The lower bound of multi-constraint geo-social
group search is Ω(Cmax ) using the proposed search framework.

7.3

EXPERIMENTAL STUDY

In this section, we conduct experimental studies on real
datasets to evaluate the proposed model and algorithms.
We first discuss non-trivial baseline algorithms used in the
experimental study.

7.1

Efficiency Evaluation

Scalability. To verify the scalability of our algorithms, we
choose different sizes of sub-datasets by selecting different
percentages of vertices in each dataset. The results are displayed in Figures 6(a) and (e). Overall speaking, algorithms
using our proposed search framework (MKASG-, MKASG)
are more scalable compared to MKASGDec, MKASGInc, and
MKASGBinInd. This is because the proposed search framework has nice property that can limit the search region while
preserving optimum result. MKAS is the one most scalable
since it incorporates with the proposed techniques which
make the time complexity of MKASG optimal. On the other
hand, MKASGInc is the least scalable due to its high time
complexity. In large dataset Foursquare, it cannot get result
over 24 hours. For MKASGBinInd using R-tree, it is slower
than the algorithms based on our proposed search framework. It seems to counterintuitive since using index reduces
the search space explored. This is because the R-tree based
index can only locate vertex efficiently; however, to identify
the subgraph and trussness of the subgraph in a region, it
has to perform induced subgraph and truss computations

For the instance of multi-constraint geo-social group search,
MKASG search, our proposed techniques ensure that the
time complexity of the search matches this lower bound.

7.

Experiment Setups

Datasets. For efficiency evaluation, we conducted the experiments over five real social network datasets including
Gowalla, Brightkite, Foursquare, Weibo and Yelp. Each social user contains some check-in locations. Table 2 presents
the statistics for all datasets. Since we only need one checkin for each user, we select the latest check-in as the spatial
coordinate for a vertex, if the user has multiple check-ins.
The keyword attribute of each user is randomly assigned
for the first four datasets, which refers to the current main
interest. The Yelp contains real social relationships, checkins and textual information. For effectiveness evaluation, we
use WoW dataset. WoW is player data in World of Warcraft
game. The social network in WoW is friendships over players in game, the spatial information is location in the virtual
game world, and the keywords are players’ real classes and
roles in the game.
Parameter settings. The experiments are evaluated using
different settings of query parameters: c (the minimum truss
number), reasonable sets of keywords ϕ as well the keyword
constraint parameter ρ. The query locations are generated
randomly. The ranges of the parameters and their default
values are shown in Table 1, in which we select reasonable c
based on datasets. Furthermore, when we vary the value of
a parameter for evaluation, all the other parameters are set
as their default values.
All algorithms are implemented in C++, and the experiments are conducted on a PC with CPU of AMD 3900x (12
cores, 24 threads), memory of 128GB DDR4 3600HZ, and
Windows 10 (build 1803). All experiments are conducted
no less than 100 times and the average results are demonstrated.

Evaluated Algorithms

In the experiment we denote Algorithm 1 as MKASG, the
incremental approach as MKASGInc, the decremental approach as MKASGDec and the binary search based approach
with R-tree index as MKASGBinInd. Besides, we also consider a simplified Algorithm 1 as one of the baselines discussed below.
10

1

3

5
|φ|

7

10

0

10−1
1

9

104

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

103
102
101
100
3

4

5
6
Turssness c

7

100

102
101
100
1

3

5

ρ

7

9

(p) Gowalla

1

3

4

5
6
Turssness c

7

11

103

101

10−1
3

5

ρ

7

9

(q) Brightkite

7

1

3

11

10

1

4

5
6
Turssness c

7

3

101
100
20%

40%
60%
80% 100%
Percentage of vertices

8

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

1

3

5

ρ

7

9

11

(r) Foursquare
Figure 6: Efficiency evaluation

repeatedly and the total repeated computations are worse
than the search framework proposed in this paper.
Varying |ϕ|. Figures 6(f) to (j) demonstrate the running
times as |ϕ| varies for different datasets. As the number
of query keywords increases, the running time for MKASG,
MKASG-, MKASGBinInd and MKASGInc rises. This is because having more keywords indicates more data need to
be explored by those algorithms since they explore vertices
from the region near by the query location to the region
containing the optimum result. For MKASGDec, more keywords lead to less computations since less vertices need to
be deleted, which makes its running time decrease for all
datasets as |ϕ| increases. MKASG outperforms all the other
algorithms substantially. MKASG can find optimum result
within 1 second in most of the datasets while can still answer
a query for extreme large dataset in reasonable time, i.e., in
a few seconds. For most of the dataset, MKASG- has the second best performance given the evaluated parameters. This
shows the power of the proposed search framework. This is
because MKASG- can bound repeated computation nicely.
Varying c. We evaluate the performance for all the algorithms when varying the trussness c in Figures 6(k) and (o).
In general, as c increases, the running time for all the algorithms reduces. The reason is that the size of subgraph
with high c tends to be small, which makes search space
decrease as c rises. Noticeablely, MKASG outperforms other
algorithms in most of the datasets. Especially for Brightkite
and Twitter, it can get result in less than 1 second. Again,
this experiment also justifies the superiority of the proposed
search framework, i.e., MKASG- is the second faster for all
datasets. MKASGInc is slower than all the other algorithms
and it runs over 24 hours for Foursquare to get the result.

7

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

101
100

9

1

103

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

102
101
100
10−1

3

4

5
6
Turssness c

7

10

2

0

1

3

5

ρ

7

(s) Twitter

7

9

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

104
103
102
101
3

9

4

5
6
Turssness c

7

8

9

11

(o) Weibo

101
10

5
|φ|

105

8

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

103

3

(j) Weibo

(n) Twitter

102
101

5
|φ|

102

(i) Twitter

3

100

102

(e) Weibo

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

100

9

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

102
10

Running time (s)

101

(m) Foursquare

100

1

5
|φ|

3

8

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

102

3

104

(l) Brightkite

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

103

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

101

10

40%
60%
80% 100%
Percentage of vertices

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

103

103

(h) Foursquare

10−1
−2

20%

104

(d) Twitter

102

100

9

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

101

10

8

Running time (s)

Running time (s)

10

7

102

(k) Gowalla
4

5
|φ|

40%
60%
80% 100%
Percentage of vertices

103

(g) Brightkite
Running time (s)

Running time (s)

(f) Gowalla

3

100

Running time (s)

101

101

(c) Foursquare

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

102

10

2

Running time (s)

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

10−1

20%

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

103

Running time (s)

10

40%
60%
80% 100%
Percentage of vertices

Running time (s)

101
0

101

(b) Brightkite

102

Running time (s)

Running time (s)

(a) Gowalla

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

102

Running time (s)

10−2
20%

10

3

Running time (s)

10−1

104

Running time (s)

40%
60%
80% 100%
Percentage of vertices

100

Running time (s)

20%

101

Running time (s)

100

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

102

Running time (s)

101

103

Running time (s)

102

Running time (s)

Running time (s)

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

103

11

MKASG
MKASGMKASGDec
MKASGBinInd
MKASGInc

103
10

2

101
100

1

3

5

ρ

7

(t) Weibo

This is because its highly repeated computations, which
makes it worse than MKASGBinInd and MKASGDec. At
last, compared to other algorithms, MKASG is less sensitive
to the changes of c because of its optimality.
Varying ρ. In Figures 6(p) to (t), the running time for
the algorithms when we change ρ are shown for different
datasets. For all datasets, the running time of all algorithms increases as ρ increases except for MKASGInc. The
reason is similar to what has been explained when varying
|ϕ|. For both Gowalla and Weibo, MKASG is not very sensitive to the change of ρ. This is because for all these datasets
the proposed initial search bound can approach to optimum
result effectively, and the dominating computation is just
trussness verification. This set of experiments also demonstrate the power of the proposed search framework, i.e., both
MKASG and MKASG- outperform other algorithms clearly
for Gowalla, Brightkite, Twitter and Weibo. Compared to
MKASGBinInd, MKASGDec is much slower. This is because
two reasons: first MKASGDec takes extra cost for sorting
and secondly MKASGDec does not have social prunings.
Pruning effectiveness evaluation. We show pruning effectiveness in Table 3 in term of size ratio for corresponding
subgraphs evaluated by MKASG. The result is the average
of 200 randomly generated queries with default settings but
different query keywords for every dataset. As we can see,
the maximal (ρ, c)-truss based pruning can filter out 40% to
60% of vertices from the original graph. Using the power
law expanding, our algorithm only evaluates 20% to 35% of
maximal (ρ, c)-trusses for corresponding datasets. It is very
noticeable that, our proposed ρ potential and truss potential
subgraphs for different datasets are extremely small. This
further justifies the effectiveness of our proposed pruning

11

2
Hunte

r, 0.1

2
r, 0.1

Rog

ue,

0.1

5

.67
e, 0
mg
Da e, 1.42
Damg
Dam
Da ge, 1.27
m
ge,
1.3
3

0.93
ge,
Dam
Healer, 0.27
Heale
r,
He
ale 0.32
r, 1
.22

.72
er, 0
Heal
Healer, 0.8
Tank
, 0.3
9

2
0.1
nk,
Ta nk, 2.21
Ta
Tank
Tan , 1.34
k, 0
.77

.67
e, 0
mg
Da e, 1.42
Damg
Dam
Da ge, 1.27
m
ge,
1.3
3

2
0.1
nk,
Ta nk, 2.21
Ta
Tank
Tan , 1.34
k, 0
.77

.72
er, 0
Heal
Healer, 0.8
Tank
, 0.3
9

0.93
ge,
m
a
D
Healer, 0.27
Heale
r,
He
ale 0.32
r, 1
.22

(a) WoW
(b) WoW
Figure 7: Formulating small team

Effectiveness Evaluation

In this section, we report two case studies conducted to
justify the effectiveness of the proposed model on WoW
dataset.
Data collection. We collect friend list for players in a guild
(similar to a community) in world of warcraft. Each player
has two sets of attributes. The universes of the two sets are:
class : {W arrior, Hunter, Rogue, . . . , P riest, M age} and
role : {Damage, Healer, T ank}.
Methodology. In the virtual world, there are random missions requested in real time at a specific location. There
are two scenarios of popular missions. The first one needs a
team containing 5 players and the second one need a team
containing 15 players. We compare the team formed by
world of warcraft and team found by our algorithm.
Small team formulation. We use the mission location as
λ, ϕ = {P reiest, Rogue, Hunter, M age, W arrior}, ρ = 1,
c = 4, and the players data we collected. The team with 5
players found by MKASG is shown in Figure 7(a). First of
all, it ensures each suggested class for finishing the mission is
in the team. Secondly, the players are near to the location,
i.e, 0.21 at most. At last, the relationships between the
player are very close. In comparison, the team formed by
the system only ensures the class requirement and players
are close to the location. However, the friendships between
the players are loose.
Large team formulation. We use the mission location as
λ, ϕ = {Damage, Healer, T ank}, ρ = 5, c = 6, and the
players data we collected. The team with 15 players found
by our method and generated by the system are displayed
in Figures 8(a) and (b). Both of the teams containing team
members that are close to the mission and satisfy role requirement for the mission. However, the social relationships
of the team found by our method is substantially denser than
the social relationships of the team found by the system.

8.

Warrior, 0.1

, 0.19
Mage

techniques.

, 0.21
Mage

Table 3: Pruning evaluations

7.4

Hunte
Warrior, 0.1

8

5.2%
7.6%
3.7%
4.3%
2.2%

0.0

12.4%
17.42%
10.3%
8.7%
11.3%

st,

33.5%
27.8%
32.2%
21.2%
32.1%

Prie

58.4%
47.6%
39.5%
43.8%
57.2%

08

Gowalla
Brightkite
Foursquare
Weibo
Yelp

t, 0
.

|C≤d∗ |
|P≤d∗ |

Prie
s

|P≤d∗ |
|∆H≤d∗ |

.15

|∆H≤d∗ |
|H|

e, 0

|H|
|G|

Rog
u

Dataset

(a) WoW
(b) WoW
Figure 8: Formulating large team
networks. All these works considered loose social constraints
in the query but did not consider keyword cohesiveness.
Team formulation. Studies on the formation of teams
of socially close experts from a social network have drawn
additional research interest recently. However, these studies have mostly focused on minimizing some social metrics
in a team without consdering the spatial factor. Lappas et
al. [11] found a team that covers the required skills and minimizes the structure diameter of the team or the total edge
weight of the spanning tree within the team. In Kargar et
al. [9], the authors considered forming a team with minimized communication and team costs. However, only the
experts who are responsible for at least one required skill
are considered in the team cost, and thus cannot be directly
applied to our MKASG search problem. Shen et al. [17]
aimed to find a team that covers appropriate keywords and
is spatially close to a location, where the minimum social
acquaintance of the team member has not been considered.
Spatial-aware community search. In [23], they found
(k, r)-core community such that socially the vertices in (k, r)core is a k-core and from similarity perspective pairwise vertices similarity is more than a threshold r. Recently, Three
kinds of CS queries have been studied on geo-socialnetworks,
namely spatial-aware community search [5], radius-bounded
k-core search [20], and geo-social group queries with minimum acquaintance constraint [26, 16]. They all required
that the communities are structurally and spatially cohesive. But, they did not consider textual cohesiveness w.r.t.
a set of query keywords as our proposed approach did.

RELATED WORKS

Geo-social group discovery. Doytsher et al. [4] combined
spatial and social networks and proposed graph-based query
processing techniques. Liu et al. [15] proposed a circle-offriend query to find minimal-diameter social groups. Yang
et al. [22] considered a special socio-spatial group query
with the requirement of minimizing the total spatial distance. Armenatzoglou et al. [2] proposed a general framework for geo-social query processing, which separates the
social, geographical and query processing modules. Li et
al. [13] studied a geo-social query that retrieves a group of
socially connected users whose familiar regions collectively
cover a set of query points. Zhang et al. [24] proposed a geosocial location recommendation system based on personalized social and geographical influence modeling. Similarly,
Shi et al. [18] proposed to cluster and categorize locations
based on social and spatial density obtained from geo-social

9.

CONCLUSION

In this paper, we study geo-social group search with multiconstraint. We propose novel search framework making the
search towards optimum result fast. In addition, we propose
online data structures, keyword aware union-find structure
and keyword-aware forest, which lead the time complexity
of basic search framework to be optimal. We also propose
heuristics, and truss union operation to further speed up the
proposed search algorithm. Extensive experiments are conducted on both synthetic and real datasets, from which the
12

efficiency and the effectiveness are evaluated and justified.

10.

[15] W. Liu, W. Sun, C. Chen, Y. Huang, Y. Jing, and
K. Chen. Circle of friend query in geo-social networks.
In International Conference on Database Systems for
Advanced Applications, pages 126–137. Springer, 2012.
[16] C. Shen, D. Yang, L. Huang, W. Lee, and M. Chen.
Socio-spatial group queries for impromptu activity
planning. IEEE Transactions on Knowledge and Data
Engineering, 28(1):196–210, Jan 2016.
[17] C.-Y. Shen, D.-N. Yang, W.-C. Lee, and M.-S. Chen.
Spatial-proximity optimization for rapid task group
deployment. ACM Transactions on Knowledge
Discovery from Data, 10(4):47, 2016.
[18] J. Shi, N. Mamoulis, D. Wu, and D. W. Cheung.
Density-based place clustering in geo-social networks.
In Proceedings of the 2014 ACM SIGMOD
international conference on Management of data,
pages 99–110. ACM, 2014.
[19] R. E. Tarjan. Efficiency of a good but not linear set
union algorithm. Journal of the ACM, 22(2):215–225,
Apr. 1975.
[20] K. Wang, X. Cao, X. Lin, W. Zhang, and L. Qin.
Efficient computing of radius-bounded k-cores. In
2018 IEEE 34th International Conference on Data
Engineering, pages 233–244. IEEE, 2018.
[21] D. Wu, Y. Li, B. Choi, and J. Xu. Social-aware top-k
spatial keyword search. In 2014 IEEE 15th
International Conference on Mobile Data
Management, volume 1, pages 235–244. IEEE, 2014.
[22] D.-N. Yang, C.-Y. Shen, W.-C. Lee, and M.-S. Chen.
On socio-spatial group query for location-based social
networks. In Proceedings of the 18th ACM SIGKDD
international conference on Knowledge discovery and
data mining, pages 949–957. ACM, 2012.
[23] F. Zhang, Y. Zhang, L. Qin, W. Zhang, and X. Lin.
When engagement meets similarity: efficient (k,
r)-core computation on social networks. Proceedings of
the VLDB Endowment, 10(10):998–1009, 2017.
[24] J.-D. Zhang and C.-Y. Chow. iGSLR: Personalized
geo-social location recommendation: A kernel density
estimation approach. In Proceedings of the 18th
SIGSPATIAL International Conference on Advances
in Geographic Information Systems, pages 334–343,
New York, NY, USA, 2013. ACM.
[25] Y. Zhang and J. X. Yu. Unboundedness and efficiency
of truss maintenance in evolving graphs. In
Proceedings of the 2019 ACM SIGMOD international
conference on Management of data, pages 1024–1041,
2019.
[26] Q. Zhu, H. Hu, C. Xu, J. Xu, and W.-C. Lee.
Geo-social group queries with minimum acquaintance
constraints. The VLDB JournalThe International
Journal on Very Large Data Bases, 26(5):709–727,
2017.

REFERENCES

[1] R. Ahuja, N. Armenatzoglou, D. Papadias, and G. J.
Fakas. Geo-social keyword search. In International
Symposium on Spatial and Temporal Databases, pages
431–450. Springer, 2015.
[2] N. Armenatzoglou, S. Papadopoulos, and D. Papadias.
A general framework for geo-social query processing.
Proceedings of the VLDB Endowment, 6(10):913–924,
2013.
[3] F. Bi, L. Chang, X. Lin, and W. Zhang. An optimal
and progressive approach to online search of top-k
influential communities. Proceedings of the VLDB
Endowment, 11(9):1056–1068, 2018.
[4] Y. Doytsher, B. Galon, and Y. Kanza. Querying
geo-social data by bridging spatial networks and social
networks. In Proceedings of the 2nd ACM
SIGSPATIAL International Workshop on Location
Based Social Networks, pages 39–46. ACM, 2010.
[5] Y. Fang, R. Cheng, X. Li, S. Luo, and J. Hu. Effective
community search over large spatial graphs.
Proceedings of the VLDB Endowment, 10(6):709–720,
2017.
[6] B. Ghosh, M. E. Ali, F. M. Choudhury, S. H. Apon,
T. Sellis, and J. Li. The flexible socio spatial group
queries. Proceedings of the VLDB Endowment,
12(2):99–111, Oct. 2018.
[7] M. R. Henzinger, V. King, and V. King. Randomized
fully dynamic graph algorithms with polylogarithmic
time per operation. Journal of the ACM,
46(4):502–516, 1999.
[8] J. Holm, K. De Lichtenberg, M. Thorup, and
M. Thorup. Poly-logarithmic deterministic
fully-dynamic algorithms for connectivity, minimum
spanning tree, 2-edge, and biconnectivity. Journal of
the ACM, 48(4):723–760, 2001.
[9] M. Kargar, M. Zihayat, and A. An. Finding affordable
and collaborative teams from a network of experts. In
Proceedings of the 2013 SIAM International
Conference on Data Mining, pages 587–595. SIAM,
2013.
[10] W. Khaouid, M. Barsky, V. Srinivasan, and
A. Thomo. K-core decomposition of large networks on
a single pc. Proceedings of the VLDB Endowment,
9(1):13–23, 2015.
[11] T. Lappas, K. Liu, and E. Terzi. Finding a team of
experts in social networks. In Proceedings of the 15th
ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 467–476. ACM,
2009.
[12] R.-H. Li, L. Qin, J. X. Yu, and R. Mao. Influential
community search in large networks. Proceedings of
the VLDB Endowment, 8(5):509–520, 2015.
[13] Y. Li, R. Chen, J. Xu, Q. Huang, H. Hu, and B. Choi.
Geo-social k-cover group queries for collaborative
spatial computing. IEEE Transactions on Knowledge
and Data Engineering, 27(10):2729–2742, Oct 2015.
[14] Y. Li, D. Wu, J. Xu, B. Choi, and W. Su.
Spatial-aware interest group queries in location-based
social networks. Data & Knowledge Engineering,
92:20–38, 2014.
13

