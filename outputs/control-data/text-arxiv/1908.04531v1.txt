arXiv:1908.04531v1 [cs.CL] 13 Aug 2019

O FFENSIVE L ANGUAGE AND H ATE S PEECH D ETECTION
FOR DANISH

Leon Derczynski
Department of Computer Science
IT University of Copenhagen
Denmark 2300
ld@itu.dk

Gudbjartur Ingi Sigurbergsson
Department of Computer Science
IT University of Copenhagen
Denmark 2300
gusi@itu.dk

August 14, 2019

A BSTRACT
The presence of offensive language on social media platforms and the implications this poses is
becoming a major concern in modern society. Given the enormous amount of content created every
day, automatic methods are required to detect and deal with this type of content. Until now, most
of the research has focused on solving the problem for the English language, while the problem is
multilingual.
We construct a Danish dataset containing user-generated comments from Reddit and Facebook. It
contains user generated comments from various social media platforms, and to our knowledge, it is
the first of its kind. Our dataset is annotated to capture various types and target of offensive language.
We develop four automatic classification systems, each designed to work for both the English and
the Danish language. In the detection of offensive language in English, the best performing system
achieves a macro averaged F1-score of 0.74, and the best performing system for Danish achieves a
macro averaged F1-score of 0.70. In the detection of whether or not an offensive post is targeted,
the best performing system for English achieves a macro averaged F1-score of 0.62, while the best
performing system for Danish achieves a macro averaged F1-score of 0.73. Finally, in the detection
of the target type in a targeted offensive post, the best performing system for English achieves a
macro averaged F1-score of 0.56, and the best performing system for Danish achieves a macro
averaged F1-score of 0.63.
Our work for both the English and the Danish language captures the type and targets of offensive
language, and present automatic methods for detecting different kinds of offensive language such as
hate speech and cyberbullying.
Keywords offensive language · hate speech detection · natural language processing · Danish

1 Introduction
Offensive language in user-generated content on online platforms and its implications has been gaining attention over
the last couple of years. This interest is sparked by the fact that many of the online social media platforms have come
under scrutiny on how this type of content should be detected and dealt with. It is, however, far from trivial to deal
with this type of language directly due to the gigantic amount of user-generated content created every day. For this
reason, automatic methods are required, using natural language processing (NLP) and machine learning techniques.
Given the fact that the research on offensive language detection has to a large extent been focused on the English
language, we set out to explore the design of models that can successfully be used for both English and Danish.
To accomplish this, an appropriate dataset must be constructed, annotated with the guidelines described in [1]. We,
furthermore, set out to analyze the linguistic features that prove hard to detect by analyzing the patterns that prove
hard to detect.

O FFENSIVE L ANGUAGE

AND

H ATE S PEECH D ETECTION
FOR DANISH

2 Background
Offensive language varies greatly, ranging from simple profanity to much more severe types of language. One of the
more troublesome types of language is hate speech and the presence of hate speech on social media platforms has been
shown to be in correlation with hate crimes in real life settings [2]. It can be quite hard to distinguish between generally
offensive language and hate speech as few universal definitions exist [3]. There does, however, seem to be a general
consensus that hate speech can be defined as language that targets a group with the intent to be harmful or to cause
social chaos. This targeting is usually done on the basis of some characteristics such as race, color, ethnicity, gender,
sexual orientation, nationality or religion [4]. In section 2, hate speech is defined in more detail. Offensive language,
on the other hand, is a more general category containing any type of profanity or insult. Hate speech can, therefore,
be classified as a subset of offensive language. [1] propose guidelines for classifying offensive language as well as the
type and the target of offensive language. These guidelines capture the characteristics of generally offensive language,
hate speech and other types of targeted offensive language such as cyberbullying. However, despite offensive language
detection being a burgeoning field, no dataset yet exists for Danish [5] despite this phenomenon being present [6].
Many different sub-tasks have been considered in the literature on offensive and harmful language detection, ranging from the detection of general offensive language to more refined tasks such as hate speech detection [3], and
cyberbullying detection [7].
A key aspect in the research of automatic classification methods for language of any kind is having substantial amount
of high quality data that reflects the goal of the task at hand, and that also contains a decent amount of samples
belonging to each of the classes being considered. To approach this problem as a supervised classification task the
data needs to be annotated according to a well-defined annotation schema that clearly reflects the problem statement.
The quality of the data is of vital importance, since low quality data is unlikely to provide meaningful results.
Cyberbullying is commonly defined as targeted insults or threats against an individual [1]. Three factors are mentioned as indicators of cyberbullying [7]: intent to cause harm, repetitiveness, and an imbalance of power. This type
of online harassment most commonly occurs between children and teenagers, and cyberbullying acts are prohibited by
law in several countries, as well as many of the US states [8].
[9] focus on classifying cyberbullying events in Dutch. They define cyberbullying as textual content that is published
online by an individual and is aggressive or hurtful against a victim. The annotation-schema used consists of two steps.
In the first step, a three-point harmfulness score is assigned to each post as well as a category denoting the authors
role (i.e. harasser, victim, or bystander). In the second step a more refined categorization is applied, by annotating
the posts using the the following labels: Threat/Blackmail, Insult, Curse/Exclusion, Defamation, Sexual Talk, Defense,
and Encouragement to the harasser.
Hate Speech. As discussed in Section 1, hate speech is generally defined as language that is targeted towards a group,
with the intend to be harmful or cause social chaos. This targeting is usually based on characteristics such as race, color,
ethnicity, gender, sexual orientation, nationality or religion [4]. Hate speech is prohibited by law in many countries,
although the definitions may vary. In article 20 of the International Covenant on Civil and Political Rights (ICCPR)
it is stated that "Any advocacy of national, racial or religious hatred that constitutes incitement to discrimination,
hostility or violence shall be prohibited by law" [10]. In Denmark, hate speech is prohibited by law, and is formally
defined as public statements where a group is threatened, insulted, or degraded on the basis of characteristics such as
nationality, ethnicity, religion, or sexual orientation [11]. Hate speech is generally prohibited by law in the European
Union, where it is defined as public incitement to violence or hatred directed against a group defined on the basis of
characteristics such as race, religion, and national or ethnic origin [12]. Hate speech is, however, not prohibited by
law in the United States. This is due to the fact that hate speech is protected by the freedom of speech act in the First
Amendment of the U.S. Constitution [13].
[3] focus is on classifying hate speech by distinguishing between general offensive language and hate speech. They
define hate speech as "language that is used to express hatred towards a targeted group or is intended to be derogatory,
to humiliate, or to insult the members of the group". They argue that the high use of profanity on social media makes
it vitally important to be able to effectively distinguish between generally offensive language and the more severe hate
speech. The dataset is constructed by gathering data from Twitter, using a hate speech lexicon to query the data with
crowdsourced annotations.
Contradicting definitions. It becomes clear that one of the key challenges in doing meaningful research on the topic
are the differences in both the annotation-schemas and the definitions used, since it makes it difficult to effectively
compare results to existing work, as pointed out by several authors ([14], [4], [15], [1]). These issues become clear
when comparing the work of [7], where racist and sexist remarks are classified as a subset of insults, to the work
of [16], where similar remarks are split into two categories; hate speech and derogatory language. Another clear
2

O FFENSIVE L ANGUAGE

AND

H ATE S PEECH D ETECTION
FOR DANISH

example of conflicting definitions becomes visible when comparing [17], where hate speech is considered without any
consideration of overlaps with the more general type of offensive language, to [3] where a clear distinction is made
between the two, by classifying posts as either Hate speech, Offensive or Neither. This lack of consensus led [15] to
propose annotation guidelines and introduce a typology. [18] argue that these proposed guidelines do not effectively
capture both the type and target of the offensive language.

3 Dataset
In this section we give a comprehensive overview of the structure of the task and describe the dataset provided in [1].
Our work adopts this framing of the offensive language phenomenon.
3.1 Classification Structure
Offensive content is broken into three sub-tasks to be able to effectively identify both the type and the target of the
offensive posts. These three sub-tasks are chosen with the objective of being able to capture different types of offensive
language, such as hate speech and cyberbullying (section 2).
Sub-task A - Offensive language identification In sub-task A the goal is to classify posts as either offensive or not.
Offensive posts include insults and threats as well as any form of untargeted profanity [18]. Each sample is annotated
with one of the following labels:
Not Offensive (NOT). In English this could be a post such as #TheNunMovie was just as scary as I thought it would
be. Clearly the critics don’t think she is terrifyingly creepy. I like how it ties in with #TheConjuring series. In Danish
this could be a post such as Kim Larsen var god, men hans død blev alt for hyped.
Offensive (OFF) . In English this could be a post such as USER is a #pervert himself!. In Danish this could be a
post such as Kalle er faggot...
Sub-task B - Automatic categorization of offensive language types In sub-task B the goal is to classify the type
of offensive language by determining if the offensive language is targeted or not. Targeted offensive language contains
insults and threats to an individual, group, or others [18]. Untargeted posts contain general profanity while not clearly
targeting anyone [18]. Only posts labeled as offensive (OFF) in sub-task A are considered in this task. Each sample is
annotated with one of the following labels:
• Targeted Insult (TIN). In English this could be a post such as @USER Please ban this cheating scum. In
Danish this could be e.g. Hun skal da selv have 99 år, den smatso.
• Untargeted (UNT). In English this could be a post such as 2 weeks of resp done and I still don’t know shit my
ass still on vacation mode. In Danish this could e.g. Dumme svin...
Sub-task C - Offensive language target identification In sub-task C the goal is to classify the target of the offensive
language. Only posts labeled as targeted insults (TIN) in sub-task B are considered in this task [18]. Samples are
annotated with one of the following:
• Individual (IND): Posts targeting a named or unnamed person that is part of the conversation. In English this
could be a post such as @USER Is a FRAUD Female @USER group paid for and organized by @USER. In
Danish this could be a post such as USER du er sku da syg i hoved. These examples further demonstrate that
this category captures the characteristics of cyberbullying, as it is defined in section 2.
• Group (GRP): Posts targeting a group of people based on ethnicity, gender or sexual orientation, political
affiliation, religious belief, or other characteristics. In English this could be a post such as #Antifa are mentally
unstable cowards, pretending to be relevant. In Danish this could be e.g. Åh nej! Svensk lorteret!
• Other (OTH): The target of the offensive language does not fit the criteria of either of the previous two
categories. [18]. In English this could be a post such as And these entertainment agencies just gonna have to
be an ass about it.. In Danish this could be a post such as Netto er jo et tempel over lort.
One of the main concerns when it comes to collecting data for the task of offensive language detection is to find high
quality sources of user-generated content that represent each class in the annotation-schema to some extent. In our
exploration phase we considered various social media platforms such as Twitter, Facebook, and Reddit.

3

O FFENSIVE L ANGUAGE

AND

H ATE S PEECH D ETECTION
FOR DANISH

Table 1: The distribution of samples by sources in our final dataset. ‘w off. terms" represents that the samples were
retrieved using offensive words in the Danish hate speech lexicon as a filter.
Data Source
Facebook - Ekstra Bladet
Reddit; r/Denmark w off. term
Reddit; r/Denmark, no off. term
Reddit; r/DANMAG w off. term
Reddit; r/DANMAG

# Comments
800
200
1,200
32
1,368

% of all
22.2
5.6
33.3
0.9
38.0

Table 2: The distribution of labels in the annotated Danish dataset for both the train and test set.
Task A
OFF
OFF
OFF
OFF
NOT
ALL

Task B
TIN
TIN
TIN
UNT

Task C
IND
OTH
GRP

Train
77
30
98
147
2,527
2,879

Test
18
6
23
42
632
721

Total
95
36
121
189
3,159
3,600

We consider three social media sites as data.
Twitter. Twitter has been used extensively as a source of user-generated content and it was the first source considered
in our initial data collection phase. The platform provides excellent interface for developers making it easy to gather
substantial amounts of data with limited efforts. However, Twitter was not a suitable source of data for our task. This
is due to the fact that Twitter has limited usage in Denmark, resulting in low quality data with many classes of interest
unrepresented.
Facebook. We next considered Facebook, and the public page for the Danish media company Ekstra Bladet. We
looked at user-generated comments on articles posted by Ekstra Bladet, and initial analysis of these comments showed
great promise as they have a high degree of variation. The user behaviour on the page and the language used ranges
from neutral language to very aggressive, where some users pour out sexist, racist and generally hateful language.
We faced obstacles when collecting data from Facebook, due to the fact that Facebook recently made the decision
to shut down all access to public pages through their developer interface. This makes computational data collection
approaches impossible. We faced restrictions on scraping public pages with Facebook, and turned to manual collection
of randomly selected user-generated comments from Ekstra Bladet’s public page, yielding 800 comments of sufficient
quality.
Reddit. Given that language classification tasks in general require substantial amounts of data, our exploration for
suitable sources continued and our search next led us to Reddit. We scraped Reddit, collecting the top 500 posts from
the Danish sub-reddits r/DANMAG and r/Denmark, as well as the user comments contained within each post.
We published a survey on Reddit asking Danish speaking users to suggest offensive, sexist, and racist terms for a
lexicon. Language and user behaviour varies between platforms, so the goal is to capture platform-specific terms. This
gave 113 offensive and hateful terms which were used to find offensive comments. The remainder of comments in the
corpus were shuffled and a subset of this corpus was then used to fill the remainder of the final dataset. The resulting
dataset contains 3600 user-generated comments, 800 from Ekstra Bladet on Facebook, 1400 from r/DANMAG and
1400 from r/Denmark.
In light of the General Data Protection Regulations in Europe (GDPR) and the increased concern for online privacy,
we applied some necessary pre-processing steps on our dataset to ensure the privacy of the authors of the comments
that were used. Personally identifying content (such as the names of individuals, not including celebrity names)
was removed. This was handled by replacing each name of an individual (i.e. author or subject) with @USER, as
presented in both [1] and [3]. All comments containing any sensitive information were removed. We classify sensitive
information as any information that can be used to uniquely identify someone by the following characteristics; racial
or ethnic origin, political opinions, religious or philosophical beliefs, trade union membership, genetic data, and biometric data.
We base our annotation procedure on the guidelines and schemas presented in [1], discussed in detail in section 3.1.
As a warm-up procedure, the first 100 posts were annotated by two annotators (the author and the supervisor) and the
results compared. This was used as an opportunity to refine the mutual understanding of the task at hand and to discuss
the mismatches in these annotations for each sub-task.
4

O FFENSIVE L ANGUAGE

AND

H ATE S PEECH D ETECTION
FOR DANISH

We used a Jaccard index [19] to assess the similarity of our annotations. In sub-task A the Jaccard index of these
initial 100 posts was 41.9%, 39.1% for sub-task B , and 42.8% for sub-task C. After some analysis of these results and
the posts that we disagreed on it became obvious that to a large extent the disagreement was mainly caused by two
reasons:
1. Guesswork of the context where the post itself was too vague to make a decisive decision on whether it was
offensive or not without more context. An example of this is a post such as Skal de hjælpes hjem, næ nej de
skal sendes hjem, where one might conclude, given the current political climate, that this is an offensive post
targeted at immigrants. The context is, however, lacking so we cannot make a decisive decision. This post
should, therefore, be labeled as non-offensive, since the post does not contain any profanity or a clearly stated
group.
2. Failure to label posts containing some kind of profanity as offensive (typically when the posts themselves
were not aggressive, harmful, or hateful). An example could be a post like @USER sgu da ikke hans skyld at
hun ikke han finde ud af at koge fucking pasta, where the post itself is rather mild, but the presence of fucking
makes this an offensive post according to our definitions.
In light of these findings our internal guidelines were refined so that no post should be labeled as offensive by interpreting any context that is not directly visible in the post itself and that any post containing any form of profanity
should automatically be labeled as offensive. These stricter guidelines made the annotation procedure considerably
easier while ensuring consistency. The remainder of the annotation task was performed by the author, resulting in
3600 annotated samples.
3.2 Final Dataset
In Table 1 the distribution of samples by sources in our final dataset is presented. Although a useful tool, using the hate
speech lexicon as a filter only resulted in 232 comments. The remaining comments from Reddit were then randomly
sampled from the remaining corpus.
The fully annotated dataset was split into a train and test set, while maintaining the distribution of labels from the
original dataset. The training set contains 80% of the samples, and the test set contains 20%. Table 2 presents the
distribution of samples by label for both the train and test set. The dataset is skewed, with around 88% of the posts
labeled as not offensive (NOT). This is, however, generally the case when it comes to user-generated content on online
platforms, and any automatic detection system needs be able to handle the problem of imbalanced data in order to be
truly effective.

4 Features
One of the most important factors to consider when it comes to automatic classification tasks the the feature representation. This section discusses various representations used in the abusive language detection literature.
Top-level features. In [4] information comes from top-level features such as bag-of-words, uni-grams and more
complex n-grams, and the literature certainly supports this. In their work on cyberbullying detection, [9] use word ngrams, character n-grams, and bag-of-words. They report uni-gram bag-of-word features as most predictive, followed
by character tri-gram bag-of-words. Later work finds character n-grams are the most helpful features [16], underlying
the need for the modeling of un-normalized text. these simple top-level feature approaches are good but not without
their limitations, since they often have high recall but lead to high rate of false positives [3]. This is due to the fact
that the presence of certain terms can easily lead to misclassification when using these types of features. Many words,
however, do not clearly indicate which category the text sample belongs to, e.g. the word gay can be used in both
neutral and offensive contexts.
Linguistic Features [16] use a number of linguistic features, including the length of samples, average word lengths,
number of periods and question marks, number of capitalized letters, number of URLs, number of polite words,
number of unknown words (by using an English dictionary), and number of insults and hate speech words. Although
these features have not proven to provide much value on their own, they have been shown to be a good addition to the
overall feature space [16].
Word Representations. Top-level features often require the predictive words to occur in both the training set and
the test sets, as discussed in [4]. For this reason, some sort of word generalization is required. [16] explore three
types of embedding-derived features. First, they explore pre-trained embeddings derived from a large corpus of news
samples. Secondly, they use word2vec [20] to generate word embeddings using their own corpus of text samples. We
5

O FFENSIVE L ANGUAGE

AND

H ATE S PEECH D ETECTION
FOR DANISH

use both approaches. Both the pre-trained and word2vec models represent each word as a 200 dimensional distributed
real number vector. Lastly, they develop 100 dimensional comment2vec model, based on the work of [21]. Their
results show that the comment2vec and the word2vec models provide the most predictive features [16]. In [22] they
experiment with pre-trained GloVe embeddings [23], learned FastText embeddings [24], and randomly initialized
learned embeddings. Interestingly, the randomly initialized embeddings slightly outperform the others [22].
Sentiment Scores. Sentiment scores are a common addition to the feature space of classification systems dealing with
offensive and hateful speech. In our work we experiment with sentiment scores and some of our models rely on them
as a dimension in their feature space. To compute these sentiment score features our systems use two Python libraries:
VADER [25] and AFINN [26].Our models use the compound attribute, which gives a normalized sum of sentiment
scores over all words in the sample. The compound attribute ranges from −1 (extremely negative) to +1 (extremely
positive).
Reading Ease. As well as some of the top-level features mentioned so far, we also use Flesch-Kincaid Grade Level
and Flesch Reading Ease scores. The Flesch-Kincaid Grade Level is a metric assessing the level of reading ability
required to easily understand a sample of text.
Pre-trained Embeddings. The pre-trained FastText [24] embeddings are trained on data from the Common Crawl
project and Wikipedia, in 157 languages (including English and Danish). FastText also provides trained models
that can be used to predict word embeddings for out-of-vocabulary (OOV) words. This is a major advantage since
challenges can arise when using pre-trained word embeddings depending on how often words in the data are not found
in the pre-trained corpus.
Randomly Initialized Learned Embeddings. Some of our models use randomly initialized embeddings, that are
updated during training. In this case, the embedding matrix for the embedding layer is initialized using a uniform
distribution.

5 Models
We introduce a variety of models in our work to compare different approaches to the task at hand. First of all, we
introduce naive baselines that simply classify each sample as one of the categories of interest (based on [1]). Next,
we introduce a logistic regression model based on the work of [3], using the same set of features as introduced there.
Finally, we introduce three deep learning models: Learned-BiLSTM, Fast-BiLSTM, and AUX-Fast-BiLSTM. The
logistic regression model is built using Scikit Learn [27] and the deep learning models are built using Keras [28]. The
following sections describe these model architectures in detail, the algorithms they are based on, and the features they
use.
Baselines Following the work of [1], we create simple baseline prediction models that simply classify all samples
as the class containing the largest amount of samples. This allows us to investigate the properties and distribution
of the samples in the datasets, and to evaluate how well our classifiers are performing. The baseline models are the
following:
Logistic Regression One of our model architecture uses a Logistic Regression as the classification algorithm. Logistic regression predicts the probability of events by using a logit function. This logit function is usually a Sigmoid
function, mapping continues variables to discrete values. A logistic regression is computed by applying the Sigmoid
function to the linear regression. Here, y is the dependent variable, X1 , . . . , Xn are the explanatory variables, and
β0 , . . . , βn are the constants we are trying to estimate.
• Sub-Task A: All NOT for both languages.
• Sub-Task B: All TIN for both languages.
• Sub-Task C: All IND for English and All GRP for Danish.
Logistic Regression Classifier We base one of our models on [3], where the objective is to distinguish between
neutral, offensive and hateful language.
Learned-BiLSTM Classifier The Learned-BiLSTM model consists of four parts; a randomly initialized embedding
layer, a bi-directional long short memory (BiLSTM) layer, a fully connected hidden layer, and a fully connected output
layer. The BiLSTM layer consists of two parts; a forward and a backward LSTM, each of size 20. This vector is then
used as input to the fully connected hidden layer, which contains 16 hidden units. The output is a single node for
6

O FFENSIVE L ANGUAGE

AND

H ATE S PEECH D ETECTION
FOR DANISH

Table 3: Results from sub-task A in English. ε=epochs.
Data
OLID[1]
OLID
OLID
OLID
OLID+HSAOFL
OLID+HSAOFL
OLID+HSAOFL
OLID+HSAOFL

Model
All NOT
Logistic Regression
Learned-BiLSTM ε = 10
Fast-BiLSTM ε = 100
AUX-Fast-BiLSTM ε = 10
Logistic Regression
Learned-BiLSTM ε = 10
Fast-BiLSTM ε = 100
AUX-Fast-BiLSTM ε = 20

F1macro
0.419
0.724
0.707
0.735
0.692
0.728
0.704
0.688
0.712

Table 4: Results from sub-task A in Danish.
Model
All NOT
Logistic Regression
Learned-BiLSTM (10 Epochs)
Fast-BiLSTM (100 Epochs)
AUX-Fast-BiLSTM (50 Epochs)

Data
DA
DA
DA
DA

Macro F1
0.467
0.699
0.658
0.630
0.675

sub-tasks A and B and 3 nodes in sub-task C. The activation function used in the LSTM layers is tanh and ReLU is
used in the hidden layer. For sub-tasks A and B, the activation function for the output layer is Sigmoid, and Softmax is
used for sub-task C. Loss is calculated using Binary Crossentropy.
Fast-BiLSTM Classifier The Fast-BiLSTM model is built using the same layers and the same set of hyperparameters as the Learned-BiLSTM model. With this the embedding layer is initialized with the FastText embeddings.
These embeddings stay fixed and are not updated during the training of the model.
AUX-Fast-BiLSTM Classifier To experiment with a wider combination of features, we extend the Fast-BiLSTM
model to AUX-Fast-BiLSTM, which accepts auxiliary features, namely: sentiment scores, n-grams weighted by their
TF-IDF scores, n-gram POS-tags, counters for the number of characters, count of: syllables; words; Twitter hashtags;
URLs; Twitter mentions; and re-tweets, and Flesch reading ease and grade level.
Hyper-Parameter Tuning We perform Grid Search Cross Validation to determine the optimal dropout amount, the
batch size, the optimizer and the learning rate. The best set of hyper-parameters for all of our models are the following:
batch size of 128, Adam [29] as the optimization algorithm with a learning rate of 0.001, and a dropout rate of 0.2
between all layers. To tackle imbalance in our dataset we use class weights. Each class is given a weight equal to the
inverse of the number of samples it contains.

6 Results and Analysis
For each sub-task (A, B, and C, Section 3.1) we present results for all methods in each language.
A - Offensive language identification:
English. For English (Table 3) Fast-BiLSTM performs best, trained for 100 epochs, using the OLID dataset. The
model achieves a macro averaged F1-score of 0.735. This result is comparable to the BiLSTM based methods in
OffensEval.
Additional training data from HSAOFL [3] does not consistently improve results. For the models using word embeddings results are worse with additional training data. On the other hand, for models that use a range of additional
features (Logistic Regression and AUX-Fast-BiLSTM), the additional training data helps.

Table 5: Recall (R), precision (P), and F1 score by class for our best performing models in sub-task A.
Model
Fast BiLSTM EN
Logistic Regression DA

R NOT
0.835
0.913

R OFF
0.646
0.506

7

P NOT
0.859
0.929

P OFF
0.603
0.450

F1 NOT
0.847
0.921

F1 OFF
0.624
0.476

O FFENSIVE L ANGUAGE

AND

H ATE S PEECH D ETECTION
FOR DANISH

Table 6: Results from sub-task B in English.
Model
All TIN
Logistic Regression
Learned-BiLSTM (60 Epochs)
Fast-BiLSTM (10 Epochs)
AUX-Fast-BiLSTM (50 Epochs)

Data
OLID
OLID
OLID
OLID

Macro F1
0.470
0.593
0.619
0.567
0.595

Table 7: Results from sub-task B in Danish.
Model
All TIN
Logistic Regression
Learned-BiLSTM (40 Epochs)
Fast-BiLSTM (100 Epochs)
AUX-Fast-BiLSTM (100 Epochs)

Data
DA
DA
DA
DA

Macro F1
0.346
0.594
0.643
0.681
0.729

Danish. Results are in Table 4. Logistic Regression works best with an F1-score of 0.699. This is the second best
performing model for English, though the best performing model for English (Fast-BiLSTM) is worst for Danish.
Best results are given in Table 5. The low scores for Danish compared to English may be explained by the low amount
of data in the Danish dataset. The Danish training set contains 2, 879 samples (table 2) while the English training set
contains 13, 240 sample.Futher, in the English dataset around 33% of the samples are labeled offensive while in the
Danish set this rate is only at around 12%. The effect that this under represented class has on the Danish classification
task can be seen in more detail in Table 5.
B - Categorization of offensive language type
English. In Table 6 the results are presented for sub-task B on English. The Learned-BiLSTM model trained for 60
epochs performs the best, obtaining a macro F1-score of 0.619.
Recall and precision scores are lower for UNT than TIN (Table 5). One reason is skew in the data, with only around
14% of the posts labeled as UNT. The pre-trained embedding model, Fast-BiLSTM, performs the worst, with a macro
averaged F1-score of 0.567. This indicates this approach is not good for detecting subtle differences in offensive
samples in skewed data, while more complex feature models perform better.
Danish. Table 7 presents the results for sub-task B and the Danish language. The best performing system is the
AUX-Fast-BiLSTM model (section 5) trained for 100 epochs, which obtains an impressive macro F1-score of 0.729.
This suggests that models that only rely on pre-trained word embeddings may not be optimal for this task. This is be
considered alongside the indication in Section 3.2 that relying on lexicon-based selection also performs poorly.
The limiting factor seems to be recall for the UNT category (Table 8). As mentioned in Section 2, the best performing
system for sub-task B in OffensEval was a rule-based system, suggesting that more refined features, (e.g. lexica) may
improve performance on this task. The better performance of models for Danish over English can most likely be explained by the fact that the training set used for Danish is more balanced, with around 42% of the posts labeled as UNT.
C - Offensive language target identification
English. The results for sub-task C and the English language are presented in Table 9. The best performing system
is the Learned-BiLSTM model (section 5) trained for 10 epochs, obtaining a macro averaged F1-score of 0.557. This
is an improvement over the models introduced in [1], where the BiLSTM based model achieves a macro F1-score of
0.470.
The main limitations of our model seems to be in the classification of OTH samples, as seen in Table 11. This may
be explained by the imbalance in the training data. It is interesting to see that this imbalance does not effect the GRP
category as much, which only constitutes about 28% of the training samples. One cause for the differences in these,

Table 8: Recall (R), precision (P), and F1 score by class for our best performing models in sub-task B.
Model
Learned BiLSTM EN
AUX-Fast-BiLSTM DA

R UNT
0.370
0.690

R TIN
0.892
0.766

8

P UNT
0.303
0.725

P TIN
0.918
0.735

F1 UNT
0.333
0.707

F1 TIN
0.905
0.750

O FFENSIVE L ANGUAGE

AND

H ATE S PEECH D ETECTION
FOR DANISH

Table 9: Results for sub-task C in English.
Model
All IND
Logistic Regression
Learned-BiLSTM (10 Epochs)
Fast-BiLSTM (50 Epochs)
AUX-Fast-BiLSTM (40 Epochs)

Data
OLID
OLID
OLID
OLID

Macro F1
0.213
0.458
0.557
0.516
0.536

Table 10: Results from sub-task C in Danish.
Model
All GRP
Logistic Regression
Learned-BiLSTM (100 Epochs)
Fast-BiLSTM (60 epochs)
AUX-Fast-BiLSTM (100 Epochs)

Data
DA
DA
DA
DA

Macro F1
0.219
0.438
0.629
0.579
0.401

is the fact that the definitions of the OTH category are vague, capturing all samples that do not belong to the previous
two.
Danish. Table 10 presents the results for sub-task C and the Danish language. The best performing system is the same
as in English, the Learned-BiLSTM model (section 5), trained for 100 epochs, obtaining a macro averaged F1-score
of 0.629. Given that this is the same model as the one that performed the best for English, this further indicates that
task specific embeddings are helpful for more refined classification tasks.
It is interesting to see that both of the models using the additional set of features (Logistic Regression and AUXFast-BiLSTM) perform the worst. This indicates that these additional features are not beneficial for this more refined
sub-task in Danish. The amount of samples used in training for this sub-task is very low. Imbalance does have as much
effect for Danish as it does in English, as can be seen in Table 11. Only about 14% of the samples are labeled as OTH
in the data (table 2), but the recall and precision scores are closer than they are for English.

7 Analysis
We perform analysis of the misclassified samples in the evaluation of our best performing models. To accomplish this,
we compute the TF-IDF scores for a range of n-grams. We then take the top scoring n-grams in each category and
try to discover any patterns that might exist. We also perform some manual analysis of these misclassified samples.
The goal of this process is to try to get a clear idea of the areas our classifiers are lacking in. The following sections
describe this process for each of the sub-tasks.
A - Offensive language identification
The classifier struggles to identify obfuscated offensive terms. This includes words that are concatenated together,
such as barrrysoetorobullshit. The classifier also seems to associate she with offensiveness, and samples containing
she are misclassified as offensive in several samples while he is less often associated with offensive language.
There are several examples where our classifier labels profanity-bearing content as offensive that are labeled as nonoffensive in the test set. Posts such as Are you fucking serious? and Fuck I cried in this scene are labeled non-offensive
in the test set, but according to annotation guidelines should be classified as offensive.
The best classifier is inclined to classify longer sequences as offensive. The mean character length of misclassified
offensive samples is 204.7, while the mean character length of the samples misclassified not offensive is 107.9. This
may be due to any post containing any form of profanity being offensive in sub-task A, so more words increase the
likelihood of > 0 profane words.

Table 11: Recall (R), precision (P), and F1 score by class for our best performing models in sub-task C. Baselines also
included to get an idea of the class distribution.
Model
Learned-BiLSTM EN
Learned-BiLSTM DA

R IND
0.670
0.556

R GRP
0.667
0.696

R OTH
0.343
0.667

P IND
0.770
0.667

9

P GRP
0.634
0.640

P OTH
0.273
0.571

F1 IND
0.717
0.606

F1 GRP
0.650
0.667

F1 OTH
0.304
0.615

O FFENSIVE L ANGUAGE

AND

H ATE S PEECH D ETECTION
FOR DANISH

The classifier suffers from the same limitations as the classifier for English when it comes to obfuscated words, misclassifying samples such as Hahhaaha lær det biiiiiaaaatch as non-offensive. It also seems to associate the occurrence of
the word svensken with offensive language, and quite a few samples containing that word are misclassified as offensive.
This can be explained by the fact that offensive language towards Swedes is common in the training data, resulting in
this association. From this, we can conclude that the classifier relies too much on the presence of individual keywords,
ignoring the context of these keywords.
B - Categorization of offensive language type
Obfuscation prevails in sub-task B. Our classifier misses indicators of targeted insults such as WalkAwayFromAllDemocrats. It seems to rely too highly on the presence of profanity, misclassifying samples containing terms such as
bitch, fuck, shit, etc. as targeted insults.
The issue of the data quality is also concerning in this sub-task, as we discover samples containing clear targeted
insults such as HillaryForPrison being labeled as untargeted in the test set.
Our Danish classifier also seems to be missing obfuscated words such as kidsarefuckingstupid in the classification of
targeted insults. It relies to some extent to heavily on the presence of profanity such as pikfjæs, lorte and fucking, and
misclassifies untargeted posts containing these keywords as targeted insults.
C - Offensive language target identification Misclassification based on obfuscated terms as discussed earlier also
seems to be an issue for sub-task C. This problem of obfuscated terms could be tackled by introducing character-level
features such as character level n-grams.

8 Conclusion
Offensive language on online social media platforms is harmful. Due to the vast amount of user-generated content
on online platforms, automatic methods are required to detect this kind of harmful content. Until now, most of the
research on the topic has focused on solving the problem for English. We explored English and Danish hate speed
detection and categorization, finding that sharing information across languages and platforms leads to good models
for the task.
The resources and classifiers are available from the authors under CC-BY license, pending use in a shared task; a data
statement [30] is included in the appendix. Extended results and analysis are given in [31].

References
[1] Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. Predicting
the type and target of offensive posts in social media. In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1
(Long and Short Papers), pages 1415–1420, Minneapolis, Minnesota, June 2019. Association for Computational
Linguistics.
[2] Karsten Müller and Carlo Schwarz. Fanning the flames of hate: Social media and hate crime. Available at SSRN
3082972, 2018.
[3] Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. Automated hate speech detection and the
problem of offensive language. In Eleventh International AAAI Conference on Web and Social Media, 2017.
[4] Anna Schmidt and Michael Wiegand. A survey on hate speech detection using natural language processing. In
Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media, pages 1–10,
2017.
[5] Andreas Kirkedal, Barbara Plank, Leon Derczynski, and Natalie Schluter. The Lacunae of Danish Natural
Language Processing. In Proceedings of the Nordic Conference on Computational Linguistics (NODALIDA).
Northern European Association for Language Technology, 2019.
[6] Leon Derczynski et al. Kvinder nedgøres oftere end mænd i politiske debatter på sociale medier. TjekDet /
Mandag Morgen, 2019.
[7] Cynthia Van Hee, Ben Verhoeven, Els Lefever, Guy De Pauw, Véronique Hoste, and Walter Daelemans. Guidelines for the fine-grained analysis of cyberbullying. Technical report, Language and Translation Technology
Team, Ghent University, 2015.
[8] Trudy M Gregorie. Cyberstalking: Dangers on the information superhighway. National Center for Victims of
crime, 2001.
10

O FFENSIVE L ANGUAGE

AND

H ATE S PEECH D ETECTION
FOR DANISH

[9] Cynthia Van Hee, Els Lefever, Ben Verhoeven, Julie Mennes, Bart Desmet, Guy De Pauw, Walter Daelemans,
and Véronique Hoste. Detection and fine-grained classification of cyberbullying events. In Proceedings of the
international conference recent advances in natural language processing, pages 672–680, 2015.
[10] Sarah Joseph and Melissa Castan. The international covenant on civil and political rights: cases, materials, and
commentary. Oxford University Press, 2013.
[11] Straffeloven § 266 b. https://danskelove.dk/straffeloven/266b. Accessed: 2019-05-29.
[12] EU Council Framework Decision 2008/913/JHA. https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=LEGISSUM%3
Accessed: 2019-05-29.
[13] James Banks. Regulating hate speech online. International Review of Law, Computers & Technology, 24(3):233–
239, 2010.
[14] Chikashi Nobata, Joel Tetreault, Achint Thomas, Yashar Mehdad, and Yi Chang. Abusive language detection
in online user content. In Proceedings of the 25th international conference on world wide web, pages 145–153.
International World Wide Web Conferences Steering Committee, 2016.
[15] Zeerak Waseem, Thomas Davidson, Dana Warmsley, and Ingmar Weber. Understanding abuse: A typology of
abusive language detection subtasks. In Proceedings of the First Workshop on Abusive Language Online, pages
78–84, Vancouver, BC, Canada, August 2017. Association for Computational Linguistics.
[16] Chikashi Nobata, Joel Tetreault, Achint Thomas, Yashar Mehdad, and Yi Chang. Abusive language detection
in online user content. In Proceedings of the 25th international conference on world wide web, pages 145–153.
International World Wide Web Conferences Steering Committee, 2016.
[17] Zeerak Waseem and Dirk Hovy. Hateful symbols or hateful people? Predictive features for hate speech detection
on twitter. In Proceedings of the NAACL student research workshop, pages 88–93, 2016.
[18] Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. Semeval2019 task 6: Identifying and categorizing offensive language in social media (offenseval). In Proceedings of
SemEval, 2019.
[19] Lieve Hamers et al. Similarity measures in scientometric research: The jaccard index versus salton’s cosine
formula. Information Processing and Management, 25(3):315–18, 1989.
[20] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in
vector space. arXiv preprint arXiv:1301.3781, 2013.
[21] Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In International conference on machine learning, pages 1188–1196, 2014.
[22] Pinkesh Badjatiya, Shashank Gupta, Manish Gupta, and Vasudeva Varma. Deep learning for hate speech detection in tweets. In Proceedings of the 26th International Conference on World Wide Web Companion, pages
759–760, 2017.
[23] Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global Vectors for word representation.
In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages
1532–1543, 2014.
[24] Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. Advances in pretraining distributed word representations. In Proceedings of the International Conference on Language Resources
and Evaluation (LREC), 2018.
[25] Clayton J Hutto and Eric Gilbert. Vader: A parsimonious rule-based model for sentiment analysis of social media
text. In Proceedings of the international AAAI conference on weblogs and social media (ICWSM), 2014.
[26] Finn Årup Nielsen. A new anew: Evaluation of a word list for sentiment analysis in microblogs. arXiv preprint
arXiv:1103.2903, 2011.
[27] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12(Oct):2825–2830, 2011.
[28] François Chollet et al. Keras. https://keras.io, 2015.
[29] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
[30] Emily M. Bender and Batya Friedman. Data statements for natural language processing: Toward mitigating
system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587–
604, 2018.
11

O FFENSIVE L ANGUAGE

AND

H ATE S PEECH D ETECTION
FOR DANISH

[31] Guðbjartur Ingi Sigurbergsson. Offensive & hate speech detection. Master’s thesis, IT University of Copenhagen,
2019.

12

O FFENSIVE L ANGUAGE

A

AND

H ATE S PEECH D ETECTION
FOR DANISH

Data statement

Curation rationale Examples of offensive language and hate speech, in Danish
Language variety Danish, BCP-47: da-DK
Speaker demographic
•
•
•
•
•
•
•

Danish Reddit and Facebook users
Age: Unknown – mixed.
Gender: Unknown – mixed.
Race/ethnicity: Unknown – mixed.
Native language: Unknown; Danish speakers.
Socioeconomic status: Unknown – mixed.
Different speakers represented: Unknown; upper bound is the number of posts.

• Presence of disordered speech: Some presences.
Annotator demographic
•
•
•
•
•

Age: 25-40.
Gender: male.
Race/ethnicity: white northern European.
Native language: Icelandic, English.
Socioeconomic status: higher education student / university faculty.

Speech situation Discussions held in public on the Reddit or Facebook platform.
Text characteristics Danish colloquial web speech.
Provenance Originally taken from Reddit and Facebook, 2018; details given in Section 3.

13

