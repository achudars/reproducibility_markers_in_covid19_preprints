What Can ResNet Learn Efficiently, Going Beyond Kernels?
Zeyuan Allen-Zhu
zeyuan@csail.mit.edu
Microsoft Research AI

Yuanzhi Li
yuanzhil@stanford.edu
Stanford University

May 23, 2019

arXiv:1905.10337v3 [cs.LG] 1 Jun 2020

(version 3)‚àó

Abstract
How can neural networks such as ResNet efficiently learn CIFAR-10 with test accuracy more
than 96%, while other methods, especially kernel methods, fall relatively behind? Can we more
provide theoretical justifications for this gap?
Recently, there is an influential line of work relating neural networks to kernels in the overparameterized regime, proving they can learn certain concept class that is also learnable by
kernels with similar test error. Yet, can neural networks provably learn some concept class
better than kernels?
We answer this positively in the distribution-free setting. We prove neural networks can
efficiently learn a notable class of functions, including those defined by three-layer residual
networks with smooth activations, without any distributional assumption. At the same time,
we prove there are simple functions in this class such that with the same number of training
examples, the test error obtained by neural networks can be much smaller than any kernel
method, including neural tangent kernels (NTK).
The main intuition is that multi-layer neural networks can implicitly perform hierarchical
learning using different layers, which reduces the sample complexity comparing to ‚Äúone-shot‚Äù
learning algorithms such as kernel methods. In a follow-up work [2], this theory of hierarchical
learning is further strengthened to incorporate the ‚Äúbackward feature correction‚Äù process when
training deep networks.
In the end, we also prove a computation complexity advantage of ResNet with respect to
other learning methods including linear regression over arbitrary feature mappings.

‚àó

V1 appears on this date, V2 slightly improved the lower bound, V3 strengthens experiments and adds citation to
‚Äúbackward feature correction‚Äù which is an even stronger form of hierarchical learning [2]. We would like to thank Greg
Yang for many enlightening conversations as well as discussions on neural tangent kernels. A 45-min presentation of
this result at the UC Berkeley Simons Institute can be found at https://youtu.be/NNPCk2gvTnI.

1

Introduction

Frobenius norm

Neural network learning has become a key practical machine learning approach and has achieved
remarkable success in a wide range of real-world domains, such as computer vision, speech recognition, and game playing [19, 20, 23, 35]. On the other hand, from a theoretical standpoint, it is less
understood that how large-scale, non-convex, non-smooth neural networks can be optimized efficiently over the training data and generalize to the test data with relatively few training examples.
There has been a sequence of research trying to address this question, showing that under
certain conditions neural networks can be learned efficiently [3, 9‚Äì11, 16, 17, 22, 24, 25, 27‚Äì29, 36‚Äì
39, 41, 44]. These provable guarantees typically come with strong assumptions and the proofs
heavily rely on them. One common assumption from them is on the input distribution, usually
being random Gaussian or sufficiently close to Gaussian. While providing great insights to the
optimization side of neural networks, it is not clear whether these works emphasizing on Gaussian
inputs can coincide with the neural network learning process in practice. Indeed, in nearly all real
world data where deep learning is applied to, the input distributions are not close to Gaussians;
even worse, there may be no simple model to capture such distributions.
The difficulty of modeling real-world distributions brings us back to the traditional PAC-learning
language which is distribution-free. In this language, one of the most popular, provable learning
methods is the kernel methods, defined with respect to kernel functions K(x, x0 ) over pairs of data
(x, x0 ). The optimization task associated with kernel methods is convex, hence the convergence
rate and the generalization error bound are well-established in theory.
Recently, there is a line of work studying the convergence of neural networks in the PAClearning language, especially for over-parameterized neural networks [1, 4‚Äì8, 13‚Äì15, 21, 26, 45],
putting neural network theory back to the distribution-free setting. Most of these works rely on
the so-called Neural Tangent Kernel (NTK) technique [13, 21], by relating the training process of
sufficiently over-parameterized (or even infinite-width) neural networks to the learning process over
a kernel whose features are defined by the randomly initialized weights of the neural network. In
other words, on the same training data set, these works prove that neural networks can efficiently
learn a concept class with as good generalization as kernels, but nothing more is known.1
In contrast, in many practical tasks, neural networks
400
give much better generalization error compared to ker300
nels, although both methods can achieve zero training
200
error. For example, ResNet achieves 96% test accuracy
100
on the CIFAR-10 data set, but NTKs achieve 77% [7]
0
and random feature kernels achieve 85% [33]. This gap
0.004
0.016
0.063
0.25
1.
error
becomes larger on more complicated data sets.
we construct by hand in train/test error
To separate the generalization power of neural netbest found by SGD in train error
best found by SGD in test error
works from kernel methods, the recent work [40] tries to
identify conditions where the solutions found by neural Figure 1: d = 40, N = 5000, after exhaustive
networks provably generalize better than kernels. This
search in network size, learning rate,
weight decay, randomly initialized SGD
approach assumes that the optimization converges to
still cannot find solutions with Frobeminimal complexity solutions (i.e. the ones minimizing
nius norm comparable to what we conthe value of the regularizer, usually the sum of squared
struct by hand. Details and more exFrobenius norms of weight matrices) of the training obperiments in Section 8.2.
1

Technically speaking, the three-layer learning theorem of [4] is beyond NTK, because the learned weights across
different layers interact with each other, while in NTK the learned weights of each layer only interact with random
weights of other layers. However, there exist other kernels‚Äî such as recursive kernels [43] ‚Äî that can more or less
efficiently learn the same concept class proposed in [4].

1

jective. However, for most practical applications, it is unclear how, when training neural networks,
minimal complexity solutions can be found efficiently by local search algorithms such as stochastic
gradient descent. In fact, it is not true even for rather simple problems (see Figure 1).2 Towards
this end, the following fundamental question is largely unsolved:
Can neural networks efficiently and distribution-freely learn a concept class,
with better generalization than kernel methods?
In this paper, we give arguably the first positive answer to this question for neural networks with
ReLU activations. We show without any distributional assumption, a three-layer residual network
(ResNet) can (improperly) learn a concept class that includes three-layer ResNets of smaller size
and smooth activations. This learning process can be efficiently done by stochastic gradient descent
(SGD), and the generalization error is also small if polynomially many training examples are given.
More importantly, we give a provable separation between the generalization error obtained by
neural networks and arbitrary kernel methods. For some Œ¥ ‚àà (0, 1), with N = O(Œ¥ ‚àí2 ) training samples, we prove that neural networks can efficiently achieve generalization error Œ¥ for this concept
class over any distribution; in contrast, there exists rather simple distributions such that any ‚àö
kernel
method (including NTK, recursive kernel, etc) cannot have generalization error better than Œ¥ for
this class. To the best of our knowledge, this is the first work that gives provable, efficiently achievable separation between neural networks with ReLU activations and kernels in the distribution-free
setting. In the end, we also prove a computation complexity advantage of neural networks with
respect to linear regression over arbitrary feature mappings as well.
Roadmap. We present detailed overview of our positive and negative results in Section 2 and 3.
Then, we introduce notations in Section 4, formally define our concept class in Section 5, and give
proof overviews in Section 6 and 7.

2

Positive Result: The Learnability of Three-Layer ResNet

In this paper, we consider learner networks that are single-skip three-layer ResNet
with ReLU activation, defined as a function out : Rd ‚Üí Rk :
out(x) = A (œÉ (Wx + b1 ) + œÉ (UœÉ (Wx + b1 ) + b2 ))

(2.1)

ùë•

ùëä
ReLU

Here, œÉ is the ReLU function, W ‚àà Rm√ód and U ‚àà Rm√óm are the hidden weights,
A ‚àà Rk√óm is the output weight, and b1 , b2 ‚àà Rm are two bias vectors.
We wish to learn a concept class given by target functions that can be written
as
H(x) = F(x) + Œ±G (F(x))

(2.2)

ùëà
ReLU

ùê¥

where Œ± ‚àà [0, 1) and G : Rk ‚Üí Rk , F : Rd ‚Üí Rk are two functions that can be written as two-layer
networks with smooth activations (see Section 5 for the formal definition). Intuitively, the target
function is a mixture of two parts: the base signal F, which is simpler and contributes more to
the target, and the composite signal G (F), which is more complicated but contributes less. As an
analogy, F could capture the signal in which ‚Äú85%‚Äù examples in CIFAR-10 can be learned by kernel
2
Consider the class of degree-6
‚àö polynomials over 6 coordinates of the d-dimensional input. There exist twolayer networks with F-norm O( d) implementing this function (thus have near-zero training and testing error).
By Rademacher complexity, O(d) samples suffice to learn if we are able to find a minimal complexity solution.
Unfortunately, due to the non-convexity of the optimization landscape, two-layer networks can not be trained to
match this F-norm even with O(d2 ) samples, see Figure 1.

2

methods, and G (F) could capture the additional ‚Äú11%‚Äù examples that are more complicated. The
goal is to use three-layer ResNet (2.1) to improperly learn this concept class (2.2), meaning learning
‚Äúboth‚Äù the base and composite signals, with as few samples as possible. In this paper, we consider
a simple `2 regression task where the features x ‚àà Rd and labels y ‚àà Rk are sampled from some
unknown distribution D. Thus, given a network out(x), the population risk is
1
kout (x) ‚àí yk22 .
(x,y)‚àºD 2
E

To illustrate our result, we first assume for simplicity that y = H (x) for some H of the form (2.2)
(so the optimal target has zero regression error). Our main theorem can be sketched as follows.
Let CF and CG respectively be the individual ‚Äúcomplexity‚Äù of F and G, which at a high level,
capture the size and smoothness of F and G. This complexity notion shall be formally introduced
in Section 4, and is used by prior works such as [4, 8, 43].

Theorem 1 (ResNet, sketched). For any distribution over x, for every Œ¥ ‚àà (Œ±CG )4 , 1 , with
probability at least 0.99, SGD efficiently learns a network out(x) in the form (2.1) satisfying
 2
1
e CF samples
E
kout (x) ‚àí yk22 ‚â§ Œ¥ using N = O
Œ¥2
(x,y)‚àºD 2
The running time of SGD is polynomial in poly(CG , CF , Œ±‚àí1 ).
In other words, ResNet is capable of achieving population risk Œ±4 , or equivalently learning the
output H(x) up to Œ±2 error. In our full theorem, we also allow label y to be generated from H(x)
with error, thus our result also holds in the agnostic learning framework.

2.1

Our Contributions

Our main contribution is to obtain time and sample complexity in CF and CG without any dependency on the composed function G(F) as in prior work [4, 43]. We illustrate this crucial difference
with an example. Suppose
I/d), k = 2‚àöand F ‚àà Rd ‚Üí R2 consists of two linear function:
 x ‚àº N (0,
‚àó
‚àó
‚àó
F(x) = hw1 , xi, hw2 , xi with kw1 k2 , kw2‚àó k2 = d, and G is degree-10 polynomial with constant
‚àö
e
Theorem 1 implies
coefficient. As we shall see, CF = O( d) and CG = O(1).
e
e 2 ).
‚Ä¢ we need O(d)
samples to efficiently learn H = F + Œ±G(F) up to accuracy O(Œ±
‚àö 10
e
In contrast, the complexity of G(F) is O((
d) ), so
e 10 ) samples to efficiently learn H up to any accuracy o(Œ±),
‚Ä¢ prior works [4, 43] need ‚Ñ¶(d
even if G(x) is of some simple form such as hw1‚àó , xi10 ‚àí hw2‚àó , xi10 .3
Inductive Bias. Our network is over-parameterized, thus intuitively in the example above, with
only O(d) training examples, the learner network could over-fit to the training data since it has
to decide from a set of d10 many possible coefficients to learn the degree 10 polynomial G. This
is indeed the case if we learn the target function using kernels, or possibly even learn it with a
two-layer network. However, three-layer ResNet posts a completely different inductive bias, and
manages to avoid over-fitting to G(F) with the help from F.
Of course, if one knew a priori the form H(x) = hw1‚àó , xi10 ‚àí hw2‚àó , xi10 , one could also try to solve it directly by
minimizing objective (hw1‚àó , xi10 ‚àí hw2‚àó , xi10 + hw2 , xi10 ‚àí hw1 , xi10 )2 over w1 , w2 ‚àà Rd . Unfortunately, the underlying
optimization process is highly non-convex and it remains unclear how to minimize it efficiently. Using matrix
e 5 ).
sensing [29], one can efficiently learn such H(x) in sample complexity O(d
3

3

Implicit Hierarchical Learning using Forward Feature Learning. Since H(x) = F(x) +
Œ±G (F(x)), if we only learn F but not G (F), we will have regression error ‚âà (Œ±CG )2 . Thus, to get
to regression error (Œ±CG )4 , Theorem 1 shows that ResNet is also capable of learning G (F) up to
some good accuracy with relatively few training examples. This is also observed in practice, where
with this number of training examples, three-layer fully-connected networks and kernel methods
can indeed fail to learn G (F) up to any non-trivial accuracy, see Figure 2.
Intuitively, there is a hierarchy of the learning process: we would like to first learn F, and
then we could learn G(F) much easier with the help of F using the residual link. In our learner
network (2.1), the first hidden layer serves to learn F and the second hidden layer serves to learn
G with the help of F, which reduces the sample complexity. However, the important message is
that F and G are not given as separate data to the network, rather the learning algorithm has to
disentangle them from the ‚Äúcombined‚Äù function H = F + Œ±G(F) automatically during the training
process. Moreover, since we train both layers simultaneously, the learning algorithm also has to
distribute the learning task of F and G onto different layers automatically. We call this process
‚Äúforward feature learning‚Äù:
Hierarchical Learning in ResNet: The Forward Feature Learning
During the training process of a residual network, the lower-level layers automatically learn an
approximation of the lower-complexity features/signals in the target function. It then forward
these features to the higher-level layers in the network to further learn the higher-complexity
features/signals in the target function.
We point out forward feature learning is different from layer-wise training. For instance, our
result cannot be obtained by first training the hidden layer close to the input, and then fixing it and
training the hidden layer close to the output. Since it could be the case the first layer incurs some
Œ± error (since it cannot learn G(F) directly), then it could be really hard, or perhaps impossible,
for the second layer to fix it only using inputs of the form F(x) ¬± Œ±. In other words, it is crucial
that the two hidden layers are simultaneously trained. 4
A follow-up work. In a follow-up work [2], this theory of hierarchical learning is strengthened to
further incorporate the backward feature correction step when training deep neural networks.
In the language of this paper, when the two layers trained together, given enough samples, the
accuracy in the first layer can actually be improved from F ¬± Œ± to arbitrarily close to F during the
training process. As a consequence, the final training and generalization error can be arbitrarily
small as well, as opposite to Œ±2 (or equivalently population risk Œ±4 ) in this work. The new ‚Äúbackward
feature correction‚Äù is also critical to extend the hierarchical learning process from 3 layers to
arbitrarily number of layers.

3
3.1

Negative Results
Limitation of Kernel Methods

Given (Mercer) kernels K1 , . . . , Kk : Rd√ód ‚Üí R and training examples {(x(i) , y (i) )}i‚àà[N ] from D, a
kernel method tries to learn a function K : Rd ‚Üí Rk where each
P
Kj (x) = n‚àà[N ] Kj (x, x(n) ) ¬∑ wj,n
(3.1)
4
This does not mean that the error of the first layer can be reduced by its own, since it is still possible for the
first layer to learn F + Œ±R(x) ¬± Œ±2 and the second layer to learn G(F)(x) ‚àí R(x) ¬± Œ±, for an arbitrary (bounded)
function R.

4

is parameterized by a weight vector wj ‚àà RN . Usually, for the `2 regression task, a kernel method
finds the optimal weights w1 , . . . , wk ‚àà RN by solving the following convex minimization problem
P
(i) 2
1 PN P
(i) (n) )w
+ R(w)
(3.2)
j,n ‚àí yj
i=1
j‚àà[k]
n‚àà[N ] Kj (x , x
N
for some convex regularizer R(w).5 In this paper, however, we do not make assumptions about how
K(x) is found as the optimal solution of the training objective. Instead, we focus on any kernel
regression function that can be written in the form (3.1).
Most of the widely-used kernels are Mercer kernels.6 This includes
(1) Gaussian kernel K(x, y) =

2 /h
‚àíkx‚àíyk
2
e
; (2) arcsin kernel K(x, y) = arcsin hx, yi/(kxk2 kyk2 ) ; (3) recursive kernel with any recursive function [43]; (4) random feature kernel K(x, y) = Ew‚àºW œÜw (x)œÜw (y) for any function œÜw (¬∑)
and distribution W; (5) the conjugate kernel defined by the last hidden layer of random initialized neural networks [12]; (6) the neural tangent kernels (NTK) for fully-connected [21] networks,
convolutional networks [7, 42] or more generally for any architectures [42].
Our theorem can be sketched as follows:
Theorem 2 (kernel, sketched). For every constant k ‚â• 2, for every sufficiently large d ‚â• 2, there
exist concept classes consisting of functions H(x) = F(x) + Œ±G (F(x)) with complexities CF , CG
and Œ± ‚àà (0, C1G ) such that, letting
Nres be the sample complexity from Theorem 1 to achieve Œ±3.9 population risk,
then there exists simple distributions D over (x, H(x))
 such that, for at least 99% of the functions
k/2
H in this concept class, even given N = O (Nres )
training samples from D, any function K(x)
of the form (3.1) has to suffer population risk
E(x,y)‚àºD

1
2

kK(x) ‚àí yk22 > Œ±2

even if the label y = H(x) has no error.

Contribution and Intuition. Let us compare this to Theorem 1. While both algorithms are
efficient, neural networks (trained by SGD) achieve population risk Œ±3.9 using Nres samples for any
distribution over x, while kernel methods cannot achieve any population risk better than Œ±2 for some
simple distributions even with N = (Nres )k/2  Nres samples.7 Our two theorems together gives
a provable separation between the generalization error of the solutions found by neural networks
and kernel methods, in the efficiently computable regime.
More specifically, recall CF and CG only depend on individual complexity of G, F, but not on
G(F). In Theorem
‚àö 2, we will construct F as linear functions and G as degree-k polynomials. This
ensures CF = O( d) and CG = O(1) for k being constant, but the combined complexity of G(F) is
as high as ‚Ñ¶(dk/2 ). Since ResNet can perform hierarchical learning, it only needs sample complexity
Nres = O(d/Œ±8 ) instead of paying (square of) the combined complexity ‚Ñ¶(dk ).
In contrast, a kernel method is not hierarchical: rather than discovering F first and then learning
G(F) with the guidance of it, kernel method tries to learn everything in one shot. This unavoidably
requires the sample complexity to be at least ‚Ñ¶(dk ). Intuitively, as the kernel method tries to learn
G(F) from scratch, this means that it has to take into account all ‚Ñ¶(dk ) many possible choices
of G(F) (recall that G is a degree k polynomial over dimension d). On the other hand, a kernel
5

In many cases, R(w) = Œª ¬∑

P

j‚àà[k]

wj> Kj wj is the norm associated with the kernel, for matrix Kj ‚àà RN √óN defined

as [Kj ]i,n = Kj (x(i) , x(n) ).
6
Recall a Mercer kernel K : Rd√ód ‚Üí R can be written as K(x, y) = hœÜ(x), œÜ(y)i where œÜ : Rd ‚Üí V is a feature
mapping to some inner product space V.
7
It is necessary the negative result of kernel methods is distribution dependent, since for trivial distributions where
x is non-zero only on the first constantly many coordinates, both neural networks and kernel methods can learn it
with constantly many samples.

5

method with N samples only has N -degrees of freedom (for each output dimension). This means, if
N  o(dk ), kernel method simply does not have enough degrees of freedom to distinguish between
different G(F), so has to pay ‚Ñ¶(Œ±2 ) in population risk. Choosing for instance Œ± = d‚àí0.1 , we have
the desired negative result for all N ‚â§ O (Nres )k/2  o(dk ).

3.2

Limitation of Linear Regression Over Feature Mappings

Given an arbitrary feature mapping œÜ : Rd ‚Üí RD , one may consider learning a linear function over
œÜ. Namely, to learn a function F : Rd ‚Üí Rk where each
Fj (x) = wj> œÜ(x)

(3.3)

is parameterized by a weight vector wj ‚àà RD . Usually, these weights are determined by minimizing
the following regression objective:8

2

P
1 P
> œÜ x(i) ‚àí y (i)
w
+ R(w)
j
i‚àà[N ]
j‚àà[k]
j
N
for some regularizer R(w). In this paper, we do not make assumptions about how the weighted are
found. Instead, we focus on any linear function over such feature mapping in the form (3.3).
Theorem 3 (feature mapping, sketched). For sufficiently large integers d, k, there exist concept
classes consisting of functions H(x) = F(x) + Œ±G (F(x)) with complexities CF , CG and Œ± ‚àà (0, C1G )
such that, letting
Tres be the time complexity from Theorem 1 to achieve Œ±3.9 population risk,
then for at least 99% of the functions H in this concept class, even with arbitrary D = (Tres )2
dimensional feature mapping, any function F(x) of the form (3.3) has to suffer population risk
E(x,y)‚àºD

1
2

kF(x) ‚àí yk22 > Œ±2

even if the label y = H(x) has no error.

Interpretation. Since any algorithm that optimizes linear functions over D-dimensional feature
mapping has to run in time ‚Ñ¶(D), this proves a time complexity separation between neural networks
(say, for achieving population risk Œ±3.9 ) and linear regression over feature mappings (for achieving
even any population risk better than Œ±2  Œ±3.9 ). Usually, such an algorithm also has to suffer
from ‚Ñ¶(D) space complexity. If that happens, we also have a space complexity separation. Our
hard instance in proving Theorem 3 is the same as Theorem 2, and the proof is analogous.

4

Notations

We denote by kwk2 and kwk‚àû the Euclidean and infinity norms of vectors w, and kwk0 the number
of non-zeros of w. We also abbreviate kwk = kwk2 when it is clear from the context. We denote
the row `p norm for W ‚àà Rm√ód (for p ‚â• 1) as
P
def
p 1/p
kWk2,p =
.
i‚àà[m] kwi k2
By definition, kWk2,2 = kWkF is the Frobenius norm of W. We use kWk2 to denote the matrix
spectral norm. For a diagonal matrix D we use kDk0 to denote its sparsity. For a matrix W ‚àà
Rm√ód , we use Wi or wi to denote the i-th row of W.
We use N (¬µ, œÉ) to denote Gaussian distribution with mean ¬µ and variance œÉ; or N (¬µ, Œ£) to
denote Gaussian vector with mean ¬µ and covariance Œ£. We use 1event or 1[event] to denote the
8

If R(w) is the `2 regularizer, then this becomes a kernel method again since the minimizer can be written in the
form (3.1). For other regularizers, this may not be the case.

6

indicator function of whether event is true. We use œÉ(¬∑) to denote the ReLU function, namely
œÉ(x) = max{x, 0} = 1x‚â•0 ¬∑ x. Given univariate function f : R ‚Üí R, we also use f to denote the
same function over vectors: f (x) = (f (x1 ), . . . , f (xm )) if x ‚àà Rm .
For notation simplicity, throughout this paper ‚Äúwith high probability‚Äù (or w.h.p.) means with
2
e to hide polylog(m) factors.
probability 1 ‚àí e‚àíc log m for a sufficiently large constant c. We use O
Function complexity. The following notions introduced
the complexity of any
Pin [4] measure
i is its Taylor expansion. 9
c
z
infinite-order smooth function œÜ : R ‚Üí R. Suppose œÜ(z) = ‚àû
i=0 i
‚àö


log(1/Œµ) ‚àó i
def P
‚àó i
‚àö
CŒµ (œÜ) = CŒµ (œÜ, 1) = ‚àû
C
|ci |
i=0 (C ) +
i
P
def
Cs (œÜ) = Cs (œÜ, 1) = C ‚àó ‚àû
i=0 (i + 1)|ci |
where C ‚àó is a sufficiently large constant (e.g., 104 ).
Example 4.1. If œÜ(z) = ec¬∑z ‚àí 1, sin(c ¬∑ z), cos(c ¬∑ z) or degree-c polynomial for constant c, then
CŒµ (œÜ, 1) = o(1/Œµ) and Cs (œÜ, 1) = O(1). If œÜ(z) = sigmoid(z) or tanh(z), to get Œµ approximation we
can truncate their Taylor series at degree Œò(log 1Œµ ). One can verify that CŒµ (œÜ, 1) ‚â§ poly(1/Œµ) by the
fact that (log(1/Œµ)/i)i ‚â§ poly(Œµ‚àí1 ) for every i ‚â• 1, and Cs (œÜ, 1) ‚â§ O(1).

5

Concept Class

We consider learning some unknown distribution D of data points z = (x, y) ‚àà Rd √ó Rk , where
x ‚àà Rd is the input vector and y is the associated label. Let us consider target functions H : Rd ‚Üí Rk
coming from the following concept class.
Concept 1. H is given by two smooth functions F, G : Rk ‚Üí Rk and a value Œ± ‚àà R+ :
H(x) = F(x) + Œ±G (F(x)) ,
where for each output coordinate r,
X

‚àó
Fr (x) =
a‚àóF ,r,i ¬∑ Fr,i hwr,i
, xi

and

i‚àà[pF ]

Gr (h) =

(5.1)
X

‚àó
a‚àóG,r,i ¬∑ Gr,i hvr,i
, hi



(5.2)

i‚àà[pG ]

‚àó ‚àà Rd and v ‚àó ‚àà Rk . We assume for
for some parameters a‚àóF ,r,i , a‚àóG,r,i ‚àà [‚àí1, 1] and vectors wr,i
r,i
‚àö
‚àó k = kv ‚àó k = 1/ 2.10 For simplicity, we assume kxk = 1 and kF(x)k = 1 for
simplicity kwr,i
2
2
2
2
r,i
(x, y) ‚àº D and in Appendix A we state a more general Concept 2 without these assumptions.11

We denote by CŒµ (F) = maxr,i {CŒµ (Fr,i )} and Cs (F) = maxr,i {Cs (Fr,i )}. Intuitively, F and G
are both generated by two-layer neural networks with smooth activation functions Fr,i and Gr,i .
Borrowing the agnostic PAC-learning language, our concept class consists of all functions H(x)
in the form of Concept 1 with complexity bounded by tuple (pF , CF , pG , CG ). Let OPT be the
population risk achieved by the best target function in this concept class. Then, our goal is to learn
this concept class with population risk O(OPT)+Œµ using sample and time complexity polynomial in
pF , CF , pG , CG and 1/Œµ. In the remainder of this paper, to simplify notations, we do not explicitly
9
In [4, ver.5], they have used (i + 1)1.75 |ci | instead of (i + 1)|ci |. For the purpose of this paper we have tightened
this complexity measure.
10
‚àó
‚àó
For general kw1,i
k2 ‚â§ B, kw2,i
k2 ‚â§ B, |a‚àór,i | ‚â§ B, the scaling factor B can be absorbed into the activation
0
function œÜ (x) = œÜ(Bx). Our results then hold by replacing the complexity of œÜ with œÜ0 .
11
Since we use ReLU networks as learners, they are positive homogeneous so to learn general functions F, G which
may not be positive homogenous, it is in some sense necessary that the inputs are scaled properly.

7

define this concept class parameterized by (pF , CF , pG , CG ). Instead, we equivalently state our
theorem with respect to any (unknown) fixed target function H with with population risk OPT:


E(x,y)‚àºD 12 kH(x) ‚àí yk22 ‚â§ OPT .
In the analysis we adopt the following notations. For every (x, y) ‚àº D, it satisfies kF(x)k2 ‚â§ BF
and kG(F(x))k2 ‚â§ BF ‚ó¶G . We ‚àö
assume G(¬∑) is LG -Lipschitz
continuous. It is a simple ‚àö
exercise (see
‚àö
Fact A.3) to verify that LG ‚â§ kpG Cs (G), BF ‚â§ kpF Cs (F) and BF ‚ó¶G ‚â§ LG BF + kpG C(G) ‚â§
kpF Cs (F)pG Cs (G).

6

Overview of Theorem 1

We learn the unknown distribution D with three-layer ResNet with ReLU activation (2.1) as learners. For notation simplicity, we absorb the bias vector into weight matrix: that is, given W ‚àà Rm√ód
and bias b1 ‚àà Rm , we rewrite Wx + b as W(x, 1) for a new weight matrix W ‚àà Rm√ó(d+1) . We
also re-parameterize U as U = VA and we find this parameterization (similar to the ‚Äúbottleneck‚Äù
structure in ResNet) simplifies the proof and also works well empirically for our concept class. After
such notation simplification and re-parameterization, we can rewrite out(x) : Rd ‚Üí Rk as


out(W, V; x) = out(x) = out1 (x) + AœÉ (V(0) + V)(out1 (x), 1)
out1 (W, V; x) = out1 (x) = AœÉ(W(0) + W)(x, 1) .
Above, A ‚àà Rk√óm , V(0) ‚àà Rm√ó(k+1) , W(0) ‚àà Rm√ó(d+1) are weight matrices corresponding to random initialization, and W ‚àà Rm√ó(k+1) , W ‚àà Rm√ó(d+1) are the additional weights to be learned by
the algorithm. To prove the strongest result, we only train W, V and do not train A.12 We consider
random Gaussian initialization where the entries of A, W(0) , V(0) are independently generated as
follows:



1
2
Ai,j ‚àº N 0, m
[W(0) ]i,j ‚àº N 0, œÉw
[V(0) ]i,j ‚àº N 0, œÉv2 /m
In this paper we focus on the `2 loss function between H and out, given as:
1
Obj(W, V; (x, y)) = ky ‚àí out(W, V; x)k22
2
We consider the vanilla SGD algorithm given in Algorithm 1.13

(6.1)

Algorithm 1 SGD
1: Initially W0 , V0 = 0.
2: for t = 0, 1, ¬∑ ¬∑ ¬∑ , T ‚àí 1 do
3:
Sample (xt , yt ) ‚àº D.
4:
Define `2 objective Obj(W, V; (xt , yt )) = 21 kyt ‚àí out(W, V; xt )k22 .
t ,yt ))
5:
Update Wt+1 ‚Üê Wt ‚àí Œ∑w ‚àÇObj(W,V;(x
.
‚àÇW
W=Wt ,V=Vt
t ,yt ))
Update Vt+1 ‚Üê Vt ‚àí Œ∑v ‚àÇObj(W,V;(x
‚àÇV
7: end for

6:

W=Wt ,V=Vt

12

.

This can be more meaningful than training all the layers together, in which if one is not careful with parameter
choices, the training process can degenerate as if only the last layer is trained [12]. (That is a convex kernel method.)
Of course, as a simple corollary, our result also applies to training all the layers together, with appropriately chosen
random initialization and learning rate.
13
Performing SGD with respect to W(0) + W and V(0) + V is the same as that with respect to W and V; we
introduce W(0) , V(0) notation for analysis purpose. Note also, one can alternatively consider having a training set
and then performing SGD on this training set with multiple passes; similar results can be obtained.

8


1
e
Theorem 1. Under Concept 1 or Concept 2, for every Œ± ‚àà 0, Œò(
kpG Cs (G) ) and Œ¥ ‚â• OPT +

e Œ±4 (kpG Cs (G))4 (1 + BF )2 . There exist M = poly(CŒ± (F), CŒ± (G), pF , Œ±‚àí1 ) satisfying that for
Œò
every m ‚â• M , with high probability over A, W(0) , V(0) , for a wide range of random initialization
parameters œÉw , œÉv (see Table 1), choosing




2
Œ±pG Cs (G) 2
e
e
e (kpF Cs (F))
Œ∑
=
Œò
(min{1,
Œ¥})
Œ∑
=
Œ∑
¬∑
Œò
T =Œò
w
v
w
min{1, Œ¥ 2 }
pF Cs (F)
With high probability, the SGD algorithm satisfies
T ‚àí1
1 X
E kH(x) ‚àí out(Wt , Vt ; x)k22 ‚â§ O(Œ¥) .
T
(x,y)‚àºD
t=0

As a corollary, under Concept 1, we can archive population risk
T ‚àí1

1 X
e Œ±4 (kpG Cs (G))4
E kH(x) ‚àí out(Wt , Vt ; x)k22 ‚â§ O(OPT) + Œò
T
(x,y)‚àºD

using sample complexity T .

t=0

(6.2)
Remark 6.1. Our Theorem 1 is almost in the PAC-learning language, except that the final error
has an additive Œ±4 term that can not be arbitrarily small.

6.1

Proof Overview

In the analysis, let us define diagonal matrices
DV(0) ,W = diag{1V(0) (out1 (x),1)‚â•0 }

DW(0) = diag{1W(0) (x,1)‚â•0 }
DW = diag{1(W(0) +W)(x,1)‚â•0 }

DV,W = diag{1(V(0) +V)(out1 (x),1)‚â•0 }

which satisfy out1 (x) = ADW (W(0) + W)(x, 1) and out(x) = ADV,W (V(0) + V)(out1 (x), 1).
The proof of Theorem 1 can be divided into three simple steps with parameter choices in Table 1.
1
e
In this paper, we assume 0 < Œ± ‚â§ O(
kpG Cs (G) ) and choose parameters

and they satisfy

œÉw ‚àà [m‚àí1/2+0.01 , m‚àí0.01 ]

œÉv ‚àà [polylog(m), m3/8‚àí0.01 ]

def
e
œÑw = Œò(kp
F Cs (F)) ‚â• 1

def
e
œÑv = Œò(Œ±kp
G Cs (G)) ‚â§



1/4
œÑw ‚àà m1/8+0.001 œÉw , m1/8‚àí0.001 œÉw

1
polylog(m)


œÉv
3/8
œÑv ‚àà œÉv ¬∑ (k/m) ,
polylog(m)

Table 1: Three-layer ResNet parameter choices.


2
œÉw , œÉv : recall entries of W(0) and V(0) are from N 0, œÉw
and N 0, œÉv2 /m .
œÑw , œÑv : the proofs work with respect to kWk2 ‚â§ œÑw and kVk2 ‚â§ œÑv .

In the first step, we prove that for all weight matrices not very far from random initialization
(namely, all kWk2 ‚â§ œÑw and kVk2 ‚â§ œÑv ), many good ‚Äúcoupling properties‚Äù occur. This includes
upper bounds on the number of sign changes (i.e., on kDW(0) ‚àí DW k0 and DV(0) ,W ‚àí DV,W )
0

as well as vanishing properties such as ADW W(0) , ADV,W V(0) being negligible. We prove such
properties using techniques from prior works [4, 6]. Details are in Section C.1.
œÑv
In the second step, we prove the existence of W> , V> with kW> kF ‚â§ œÑ10w and kV> kF ‚â§ 10
satisfying ADW(0) W> (x, 1) ‚âà F(x) and ADV(0) ,W V> (out1 (x), 1) ‚âà Œ±G (out1 (x)). This existential
9

proof relies on an ‚Äúindicator to function‚Äù lemma from [4]; for the purpose of this paper we have to
revise it to include a trainable bias term (or equivalently, to support vectors of the form (x, 1)).
Combining it with the aforementioned vanishing properties, we derive (details are in Section C.2):
ADW W> (x, 1) ‚âà F(x)

and ADV,W V> (out1 (x), 1) ‚âà Œ±G (out1 (x)) .

(6.3)

In the third step, consider iteration t of SGD with sample (xt , yt ) ‚àº D. For simplicity we assume
OPT = 0 so yt = H(xt ). One can carefully write down gradient formula, and plug in (6.3) to derive
def

Œût = h‚àáW,V Obj(Wt , Vt ; (xt , yt )), (Wt ‚àí W> , Vt ‚àí V> ))i
‚â• 21 kH(xt ) ‚àí out(Wt , Vt ; xt )k22 ‚àí 2kErrt k22



e Œ±4 (kpG Cs (G))4 . This quantity Œût is quite famous in classical mirror
with E kErrt k22 ‚â§ Œò
descent analysis: for appropriately chosen learning rates, Œût must converge to zero.14 In other
words, by concentration, SGD is capable of finding solutions Wt , Vt so that the population risk
kH(xt ) ‚àí out(Wt , Vt ; xt )k22 is as small as E[kErrt k22 ]. This is why we can obtain population risk

e Œ±4 (kpG Cs (G))4 in (6.2). Details are in Section C.3 and C.4.
Œò

7

Overview of Theorem 2 and 3

 ¬±1 d
We construct the following hard instance. The input x ‚àà ‚àö
in on the (scaled) Boolean cube,
d


def
¬±1 d1
and is drawn from distribution x ‚àº D = U ‚àö
√óD2 . That is, the first d1 coordinates are drawn
d
‚àö d
uniformly at random from {¬±1/ d} 1 , and the last d ‚àí d1 coordinates are drawn from an arbitrary
distribution D2 . Our hard instance works for a wide range of d1 , including for example d1 = d
(uniform distribution over boolean cube) and d1 = o(d) (only a small subset of the coordinates are
uniform). We consider X = {x(1) , . . . , x(N ) } being N i.i.d. samples from D.
Consider the class of target functions H(x) = F(x) + Œ±G(F(x)), where

Q
F(x) = W‚àó x and G(y) =
(7.1)
j‚àà[k] yj i‚àà[k]
‚àö
where W‚àó = d(ei1 , ei2 , ¬∑ ¬∑ ¬∑ eik ) for
 i1 , i2 , . . . , ik ‚àà [d1 ] are distinct indices chosen from the first d1
coordinates. There are clearly dk1 many target functions in this class.
Intuitively, e1 , ¬∑ ¬∑ ¬∑ , ed1 represent the directions where the signal possibly lies, where usually
the inputs would have high variance; and ed1 +1 , . . . , ed represent the directions that can be view
as ‚Äúbackground noise‚Äù, where the distribution can be arbitrary. For example when d1 ‚â§ d/2,
such distribution D can be very different from Gaussian distribution or uniform distribution over
Boolean cube, yet kernel methods still suffer from high population risk when learning over these
distributions comparing to using neural networks.
We first state the population risk for the three-layer ResNet to learn this concept class: Our
Theorem 1 ‚àö
implies the following complexity on learning this concept class (after verifying that
Cs (F) = O( d), pF = 1, Cs (G) = 2O(k) , pG = 2k , see Section D.4).

1
Corollary 7.1. For every d ‚â• d1 ‚â• k ‚â• 2, for every Œ± ‚àà 0, e O(k)
, there exist M =
Œò(2

)

poly(d, 2k , Œ±‚àí1 ) satisfying that for every m ‚â• M , for every target functions H(x) in the class (7.1),
with probability at least 0.99 over A, W(0) , V(0) and X , given labels y (n) = H(x(n) ) for n ‚àà [N ],
P ‚àí1
Indeed, one can show Tt=0
Œût ‚â§ O(Œ∑w + Œ∑v ) ¬∑ T +
‚àö
O( T ) ignoring other factors.
14

kW> k2
F
Œ∑w

10

+

kV> k2
F
Œ∑v

, and thus the right hand side can be made

SGD finds a network out(x) with population risk
E kH(x) ‚àí

x‚àºD

7.1

out(x)k22

e 4 2O(k) )
‚â§ O(Œ±


e
using N = Œò

k2 d
Œ±8


samples .

Kernel Method

We restate Theorem 2 as follows.

d1
1
Theorem 2 (restated). For every integers k, d1 , d, N satisfying 2 ‚â§ k ‚â§ d1 ‚â§ d and N ‚â§ 1000
k ,
for every Œ± ‚àà (0, 1), for every X , for every (Mercer) kernels K1 , . . . , Kk : Rd√ód ‚Üí R, the following
holds for at least 99% of the target functions H(x) in the class (7.1). For all kernel regression
functions
P
Ki (x) = n‚àà[N ] Ki (x, x(n) ) ¬∑ wi,n for i ‚àà [k],
where weights wi,n ‚àà R can depend on Œ±, X , K and the training labels {y (1) , ¬∑ ¬∑ ¬∑ , y (N ) }, it must
suffer from population risk
E kH(x) ‚àí K(x)k22 > Œ±2 /16 .

x‚àºD

As an example, when k ‚â• 2 is constant, d = Œò(d1 ) is sufficiently large, and Œ± = Œò(d‚àí0.1 ),
‚Ä¢ Corollary 7.1 says that ResNet achieves regression error Œ±3.9 on the true distribution, with
e 1.8 ) samples to learn any function in (7.1);
Nres = O(d
‚Ä¢ Theorem 2 says that kernel methods cannot achieve Œ±2 /16 error even with N ‚â§ (Nres )k/2 
o(dk ) samples. Hence, to achieve generalization Œ±2 /16  Œ±3.9 , the sample complexity of any
kernel method is at least N ‚â• (Nres )k/2  Nres .
Proof Overview. Our proof of Theorem 2 is relatively
simple, and we illustrate the main idea

n
in the case of d = d1 . At a high level, given N  d samples, the kernel regression function only
has
 N -degrees of freedom (each with respect to a sample point). Now, since there are possibly
n
d many target functions, if the kernel regression learns most of these target functions to some
sufficient accuracy, then by some rank counting argument, the degree of freedom is not enough.

7.2

Linear Regression Over Feature Mappings

We restate Theorem 3 as follows.

d1
1
Theorem 3 (restated). For every integers k, d1 , d, D satisfying 2 ‚â§ k ‚â§ d1 ‚â§ d and D ‚â§ 1000
k ,
for every Œ± ‚àà (0, 1), for every feature mapping œÜ : Rd ‚Üí RD , the following holds for at least 99% of
the target functions H(x) in the class (7.1). For all linear regression functions
Fj (x) = wj> œÜ(x)

for j ‚àà [k],

where weights wj ‚àà RD can depend on Œ± and œÜ, it must suffer from population risk
E kH(x) ‚àí F(x)k22 > Œ±2 /16 .

x‚àºD

As an example, there exists sufficiently large constant c > 1 such that, for every k ‚â• 4c, for every
d1 ‚â• d/2, for every d ‚â• ‚Ñ¶(2k ), there exists choice Œ± = 2‚àíŒò(k) ¬∑ d‚àí0.001 such that
e 4 2O(k) ) ‚â§ Œ±3.9 in time Tres =
‚Ä¢ Corollary 7.1 says that ResNet achieves regression error O(Œ±
poly(d, 2k , Œ±‚àí1 ) ‚â§ dc to learn any function in (7.1);
11

‚Ä¢ Theorem 3 says that linear
over feature mapping cannot achieve regression error
 regression
d1
2
2c
Œ± /16 even if D = ‚Ñ¶ k ‚â• d .
In particular, this means linear regression over feature mappings cannot achieve regression error
Œ±2 /16 even if D = (Tres )2 . Since a linear regression over RD normally takes at least time/space D
to compute/store, this implies that ResNet is also more time/space efficient than linear regression
over feature mappings as well.
Theorem 3 can be proved in the same way as Theorem 2, using exactly the same hard instance,
since F(x) has exactly D-degrees of freedom.

8

Experiments

8.1

ResNet vs. Kernel Methods vs. Fully-Connected Networks

Consider synthetic data where the feature vectors x ‚àà {‚àí1, 1}30 that are uniformly sampled at
random, and labels are generated from a target function H(x) = F(x) + Œ±G(F(x)) ‚àà R15 satisfying
F(x) = (x1 x2 , . . . , x29 x30 ) and Gi (y) = (‚àí1)i y1 y2 y3 y4 for all i = 1, 2, . . . , 15. In other words, F is a
degree-2 parity function over 30 dimensions, and G is a degree-4 parity function over 15 dimensions.
Neural Networks Algorithms. Recall in our positive result on three-layer ResNet (see Theorem 1
and Footnote 12), to prove the strongest result, we only train hidden weights W and V but not the
output layer A. One can naturally extend this to show that Theorem 1 also holds when W, V, A
are jointly trained. For such reason, we implement both algorithms: 3resnet(hidden) for training
only W, V and 3resnet(all) for training all W, V, A. This is similar for two-layer and three-layer
fully-connected networks, where previously the strongest theoretical work is in terms of training
only hidden weights [4], so we implement both (all) and (hidden) for them.
Kernel Methods. We implement conjugate kernel, which corresponds to training only the last
(output) layer [12]; as well as neural tangent kernel (NTK), in which we train all the layers [21].
Setup. We choose the network width (i.e., parameter m) in the range m ‚àà {20, 50, 100, 200, . . . }
until the largest possible value m that fits into a 16GB GPU memory. We choose the popular
random initialization: entries of A, V, W (and their corresponding bias terms) are all i.i.d. from
1 15
N (1, m
). We use similar initializations for two and three-layer networks.
We use the default SGD optimizer of pytorch, with momentum 0.9, mini-batch size 50. We
carefully run each algorithm with respect to learning rates and weight decay parameters in the set
{10‚àík , 2 ¬∑ 10‚àík , 5 ¬∑ 10‚àík : k ‚àà Z}, and present the best one in terms of testing accuracy. In each
parameter setting, we run SGD for 800 epochs, and decrease the learning rate by 10 on epoch 400.
Experiment 1: Performance Comparison. Since it is unfair to compare neural network
training ‚Äúwith respect to hidden weights only‚Äù vs. ‚Äúwith respect to all weights‚Äù, we conduct two
experiments. The first experiment is on training all layers vs. kernel methods, see Figure 2(a); and
the second experiment is on training hidden layers vs. kernel methods, see Figure 2(b). We use
N = 500 training samples for the former case and N = 1000 samples for the latter case, because
training the last layer together gives more power to a neural network.
In both experiments, we choose Œ± = 0.3 and k = 15 so that test error kŒ±2 = 1.35 is a threshold
for detecting whether the trained model has successfully learned Œ±G(F(x)) or not. If the model
has not learned Œ±G(F(x)) to any non-trivial accuracy, then the error is Œ± per output coordinate,
totaling to kŒ±2 in regression error.
15

1
This corresponds to choosing the standard deviation as ‚àöfan in+1‚àöfan out . Some practitioners also use ‚àöfan
as
in
the standard deviation. We have included an experiment with respect to that choice in our V1/V2 of this paper.

12

1

0.1

0.01

3resnet(hidden)
3layer(hidden)
2layer(hidden)
3resnet(last)
3layer(last)
2layer(last)
3resnet(NTK)
3layer(NTK)
2layer(NTK)

5

Test error

Test error

25

3resnet(all)
3layer(all)
2layer(all)
3resnet(last)
3layer(last)
2layer(last)
3resnet(NTK)
3layer(NTK)
2layer(NTK)

10

1

0.2

0.04

m = number of hidden heurons

m = number of hidden heurons

(a) N = 500, train all layers vs. kernel methods

(b) N = 1000, train hidden layers vs. kernel methods

Figure 2: Performance comparison. 3resnet stands for our three-layer ResNet and 3layer/2layer stands for three
and two-layer fully connected networks. (all) stands for training all layers, (hidden) stands for training
only hidden layers, (last) stands for training only the last output layer, and (NTK) stands for training all
layers in the neural tangent kernel [21]. We emphasize that (last) is a kernel method and corresponds to
the conjugate kernel [12]. Experiment setup is in Section 8.1.

10

Test error

0.1
0.01

Test error

10

resnet(Œ≤=1,all)
resnet(Œ≤=0.5,all)
resnet(Œ≤=0.3,all)
resnet(Œ≤=0.2,all)
resnet(Œ≤=0,all)

1

resnet(Œ≤=1,hidden)
resnet(Œ≤=0.5,hidden)
resnet(Œ≤=0.3,hidden)
resnet(Œ≤=0.2,hidden)
resnet(Œ≤=0,hidden)

1

0.1

0.001
0.0001

0.01

m = number of hidden neurons

m = number of hidden neurons

(a) training all layers of 3resnet

(b) training hidden layers of 3resnet

Figure 3: Sensitivity test on Œ±. Using the same choice of F(x) and G(y) from Section 8.1, we choose target function
H(x) = Œ≤F(x) + Œ±G(F(x)) with Œ± = 0.3 and varying Œ≤ ‚àà [0, 1].

From Figure 2, it is clear that for our choice of N , training a three-layer ResNet is the only
method among the ones we compare that can learn Œ±G(F(x)) (even only non-trivially). All kernel
methods fall far behind even when the network width m is large.
Experiment 2: Sensitivity on Œ±. One key assumption of this paper is to have Œ± to be sufficiently
small, so that ResNet can perform hierarchical learning, by first learning the base signal F, which
is simpler and contributes more to the target, and then learning the composite signal Œ±G (F), which
is more complicated but contributes less.
In Figure 3, we verify that this assumption is indeed necessary. Instead of varying Œ± (which
will change the error magnitude), we define H(x) = Œ≤F(x) + Œ±G(F(x)) and let Œ≤ vary between 0
and 1. As shown in Figure 3, when Œ± . Œ≤, the base signal is larger than the composite signal, so
indeed ResNet can perform hierachical learning; in contrast, when Œ± & Œ≤, learning the composite
signal becomes practically impossible.
Other Findings. Although this paper proves theoretical separation between three-layer ResNet
and kernel methods (and it is verified by Figure 2), we do not yet have
‚Ä¢ theoretical separation between two/three-layer fully-connected networks and kernel methods;
13

‚Ä¢ theoretical separation between three-layer ResNet and two/three-layer networks.
It seems in practice such separations do exist (as observed in Figure 2). We leave these as future
research directions.

8.2

SGD Does Not Converge To Minimal Norm Solutions

Frobenius norm

Frobenius norm

400
300
200
100

0
0.004

0.016

0.063
0.25
1.
error
we construct by hand in train/test error
best found by SGD in train error
best found by SGD in test error

1200
800

400
0
0.004

0.063
0.25
1.
error
we construct by hand in train/test error
best found by SGD in train error
best found by SGD in test error

(a) d = 40, N = 5000

0.016

(b) d = 100, N = 50000

Figure 4: SGD cannot find solutions with Frobenius norm comparable to what we construct by hand.

We give a simple experiment to show that optimization methods (such as SGD) do not necessarily converge to minimal complexity solutions.
Consider two-layer neural networks F (W ; x) = a> œÉ(W x) where W ‚àà Rm√ód and a ‚àà { ‚àö¬±1
}m is
m
an arbitrary vector with exactly m/2 positive and m/2 negative values. For simplicity, we focus
on the case when x is of norm 1 and we only train W keeping a fixed.
¬±1
Consider a simple data distribution where each xi is independently drawn from { ‚àö
} for some
d
def

d ‚â• 6. Consider labels y ‚àà {‚àí1, +1} being generated from some target function y = F(x) =
d3 xi1 xi2 xi3 xi4 xi5 xi6 for some distinct indices i1 , i2 , i3 , i4 , i5 , i6 ‚àà [d].
It is a simple experimental exercise to verify that, for every even m ‚â• 200 and every d ‚â• 6,
there exist16
‚àö


‚Ä¢ W ‚àó ‚àà Rm√ód with kW ‚àó kF ‚âà 9.7 d satisfying E(x,y) |F (W ; x) ‚àí y|2 ‚â§ 0.12.
‚àö


‚Ä¢ W ‚àó ‚àà Rm√ód with kW ‚àó kF ‚âà 12.5 d satisfying E(x,y) |F (W ; x) ‚àí y|2 ‚â§ 0.037.
‚àö


‚Ä¢ W ‚àó ‚àà Rm√ód with kW ‚àó kF ‚âà 13.8 d satisfying E(x,y) |F (W ; x) ‚àí y|2 ‚â§ 0.011.
Using simple Rademacher complexity argument,
the above existential statement implies if we
‚àö
focus only on matrices W P
with kW kF ‚â§ 9.7 d, then given N training samples the Rademacher
‚àö2

kWj k2

k
‚àö F .17 This implies, for any m ‚â• 200 and d ‚â• 6,
complexity is at most m ‚àöN
‚â§ 2kW
N
if N = O(d) samples are given and if SGD
‚àö finds any close-to-minimal complexity solution (i.e.
with F-norm within some constant times d) that performs well on the training set, then it also
generalizes to give small test error (i.e. test error < 0.3).
Unfortunately, one can experimentally verify that, even for d = 40 and N = 5000, starting from
random initialization, even after searching learning rates and weight decay parameters in the set
{10‚àík , 2 ¬∑ 10‚àík , 5 ¬∑ 10‚àík : k ‚àà Z}, searching network size m in {200, 500, 1000, 2000, . . . , 100000}:
j‚àà[m]

16

This can be done by first considering m = 200 and d = 6. Experimentally one can easily use SGD to train such
two-layer networks to obtain some W ‚àó with such test errors. Then, for general d > 6, one can pad W ‚àó with d ‚àí 6
zero columns; and for general m > 200, one can duplicate the rows of W ‚àó and re-scale.
17
This can found for instance in [18, 32]. A cleaner one page proof can be found in the lecture notes [30].

14

‚Ä¢ SGD cannot find solution with test error better than 0.69 (see Figure 4(a)), and
‚Ä¢ SGD cannot find solution with small training error and small Frobenius norm (see Figure 4(a)).
Thus, SGD starting from random initialization fails to find the minimal complexity solution.
We also tried d = 100 and N = 50000 (where N is the same comparing to the standard CIFAR10/100 datasets), and this time we choose mini-batch 100 to speed up training. Even after
searching learning rates and weight decay parameters in the set {10‚àík , 2 ¬∑ 10‚àík , 5 ¬∑ 10‚àík : k ‚àà Z},
searching network size m in {200, 500, 1000, 2000, . . . , 50000}:
‚Ä¢ SGD cannot find solution with test error better than 0.98 (see Figure 4(b)), and
‚Ä¢ SGD cannot find solution with small training error and small Frobenius norm (see Figure 4(b)).

Appendix: Complete Proofs
In Appendix A we give some more information about our concept class and complexity measure.
In Appendix B we review some simple lemmas from probability theory.
In Appendix C we give our full proof to Theorem 1.
In Appendix D we give our full proof to Theorem 2.
In Appendix E we include a variant of the existential lemma from prior work, and include its
proof only for completeness‚Äô sake.

A

Complexity and Concept Class

In this section we introduce an alternative (but bigger) concept class.
Definition A.1. We say F : Rd ‚Üí Rk has general complexity (p, Cs (F), CŒµ (F)) if for each r ‚àà [k],
 ‚àó

p
X
hw1,i , (x, 1)i
‚àó
‚àó
Fr (x) =
ar,i ¬∑ Fr,i
¬∑ hw2,i
, (x, 1)i ,
k(x, 1)k2
i=1

a‚àór,i

‚àó , w ‚àó ‚àà Rd+1 has Euclidean norm 1, each F : R ‚Üí R is a
where each
‚àà [‚àí1, 1], each w1,i
r,i
2,i
smooth function with only zero-order and odd-order terms in its Taylor expansion at point zero,
and CŒµ (F) = maxr,i {CŒµ (Fr,i )} and Cs (F) = maxr,i {Cs (Fr,i )}.

Concept 2. H is given by two smooth functions F, G : Rk ‚Üí Rk and a value Œ± ‚àà R+ :
H(x) = F(x) + Œ±G (F(x)) ,

(A.1)

where where F and G respectively have general complexity (pF , Cs (F), CŒµ (G)) and (pG , Cs (G), CŒµ (G)).
We further assume kF(x)k2 ‚â§ BF for all (x, y) ‚àº D.
We have the following lemma which states that Concept 1 is a special case of Concept 2 (with
constant factor 2 blow up).
Lemma A.2. Under Concept 1, we can construct F 0 , G 0 satisfying Concept 2 with general complexity (2pF , Cs (F), CŒµ (G)) and (2pG , Cs (G), CŒµ (G)) and with BF = 1.

15

Proof of Lemma A.2. Lemma A.2 is a simple corollary of the following claim.
Given any F : Rd ‚Üí Rk where for each r ‚àà [k]:
 ‚àó

p
X
hwi , xi
‚àó
‚àö
Fr (x) =
ar,i ¬∑ Fr,i
,
2
i=1
where each a‚àór,i ‚àà [‚àí1, 1], each wi‚àó ‚àà Rd has Euclidean norm 1, each Fr,i : R ‚Üí R is a smooth
function. Then, there exists some F 0 : Rd ‚Üí Rk such that:
‚Ä¢ F(x) = F 0 (x) for all unit vectors x ‚àà Rd ; and
‚Ä¢ F 0 has general complexity (2p, Cs (F), CŒµ (F)) where CŒµ (F) = maxr,i {CŒµ (Fr,i )} and Cs (F) =
maxr,i {Cs (Fr,i )}.
P
i
Below we prove that the above claim holds. For each Fr,i (¬∑) suppose we have Fr,i (z) = ‚àû
i=0 ci z
as its Taylor expansion, then we can write
Ô£´
Ô£∂
Ô£´
Ô£∂
X
X
def
+
‚àí
ci z i Ô£∏ + z ¬∑ Ô£≠
ci z i‚àí1 Ô£∏ .
Fr,i (z) = Fr,i
(z) + z ¬∑ Fr,i
(z) = Ô£≠
i=0,1,3,5,7,...

i=2,4,6,8,...

+
‚àí
From this expansion we see that both Fr,i
and Fr,i
have only zero-order or odd-order terms in its
0
d
Taylor expansion at zero. We can define F : R ‚Üí Rk where






p
X
1 ‚àí h(wi‚àó , 0), (x, 1)i
h(wi‚àó , 0), (x, 1)i
+
‚àó
0
‚àó
~
¬∑ h(0, 1), (x, 1)i + ‚àö Fr,i
¬∑ h(wi , 0), (x, 1)i
Fr (x) =
ar,i ¬∑ Fr,i
k(x, 1)k2
k(x, 1)k2
2
i=1

It is a simple exercise to verify that F 0 (x) = F(x) for all unit vectors x.



We also state some simple properties regarding our complexity measure.
Fact A.3. If F : Rd‚àö‚Üí Rk has general complexity (p, Cs (F),‚àöCŒµ (F)), then for every x, y ‚àà Rd , it
satisfies kF(x)k2 ‚â§ kpCs (F) ¬∑ kxk2 and kF(x) ‚àí F(y)k2 ‚â§ kpCs (F) ¬∑ kx ‚àí yk2 .
Proof of Fact A.3. The boundedness
ofkF(x)k2 is trivial so we only focus on kF(x) ‚àí F(y)k2 . For
 hw‚àó ,(x,1)i
1,i
‚àó , (x, 1)i, denoting by w ‚àó as the first d coordinate of
each component g(x) = Fr,i k(x,1)k2
¬∑ hw2,i
1
‚àó , and by w ‚àó as the first d coordinates of w ‚àó , we have
w1,i
2,i
2,i
 ‚àó

hw1,i , (x, 1)i
g 0 (x) = Fr,i
¬∑ w2‚àó
k(x, 1)k2
 ‚àó

‚àó , (x, 1)i ¬∑ (x, 1)/k(x, 1)k2
hw1,i , (x, 1)i
w1‚àó ¬∑ k(x, 1)k2 ‚àí hw1,i
2
‚àó
0
+ hw2,i , (x, 1)i ¬∑ Fr,i
¬∑
k(x, 1)k2
k(x, 1)k22
This implies
0

kg (x)k2 ‚â§ Fr,i



‚àó , (x, 1)i
hw1,i
k(x, 1)k2


+2

0
Fr,i



‚àó , (x, 1)i
hw1,i
k(x, 1)k2

‚â§ 3Cs (Fr,i ) .



As a result, |Fr (x) ‚àí Fr (y)| ‚â§ 3pCs (Fr,i ).

B



Probability Theory Review

The following concentration of chi-square distribution is standard.

16

Proposition B.1 (chi-square concentration). If g ‚àº N (0, I) is m-dimensional, then for every t ‚â• 1
‚àö
Pr[kgk22 ‚àí m ‚â• 2 mt + 2t] ‚â§ e‚àít
The following norm bound on random Gaussian matrix is standard.
Proposition B.2. If M ‚àà Rn√óm is a random matrix where Mi,j are i.i.d. from N (0, 1). Then,
‚àö
‚àö
2
‚Ä¢ For any t ‚â• 1, with probability ‚â• 1 ‚àí e‚àí‚Ñ¶(t ) it satisfies kMk2 ‚â§ O( n + m) + t.

‚àö
2
‚Ä¢ If 1 ‚â§ s ‚â§ O logm2 m , then with probability ‚â• 1 ‚àí e‚àí‚Ñ¶(n+s log m) it satisfies kMvk2 ‚â§ O( n +
‚àö
s log m) ¬∑ kvk2 for all s-sparse vectors v ‚àà Rm .
Proof. The first statement can be found for instance in [34, Proposition 2.4]. As for the second
statement, it suffices for us to consider all m
s possible n √ó s sub-matrices of M, each applying the
first statement, and then taking a union bound.

The following concentration is proved for instance in [4].
Lemma B.3 (Gaussian indicator concentration). Let (n1 , Œ±1 , a1,1 , a2,1 ), ¬∑ ¬∑ ¬∑ , (nm , Œ±m , a1,m , a2,m ) be
m i.i.d. samples from some distribution, where within a 4-tuples:
‚Ä¢ the marginal distribution of a1,i and a2,i is standard Gaussian N (0, 1);
‚Ä¢ ni and Œ±i are not necessarily independent;
‚Ä¢ a1,i and a2,i are independent; and
‚Ä¢ ni and Œ±i are independent of a1,i and a2,i .
Suppose h : R ‚Üí [‚àíL, L] is a fixed function. Then, for every B ‚â• 1:
Ô£ÆÔ£´
Ô£∂
Ô£π
X
‚àö
2
Pr Ô£∞ Ô£≠
a1,i a2,i 1[ni ‚â• 0]h(Œ±i )Ô£∏ ‚â• BL( m + B)Ô£ª ‚â§ 4e‚àíB /8
i‚àà[m]

and
Ô£ÆÔ£´
Pr Ô£∞ Ô£≠

Ô£π
‚àö
2
a21,i 1[ni ‚â• 0]h(Œ±i )Ô£∏ ‚àí m E[a21,1 1[n1 ‚â• 0]h(Œ±1 )] ‚â• BL( m + B)Ô£ª ‚â§ 4e‚àíB /8 .
Ô£∂

X
i‚àà[m]

Proof of Lemma B.3. Let us consider a fixed n1 , Œ±1 , ¬∑ ¬∑ ¬∑ , nm , Œ±m , then since each |1[ni ‚â• 0]h(Œ±i )| ‚â§
L, by Gaussian chaos variables concentration bound (e.g., Example 2.15 in [31]) we have that
Ô£ÆÔ£´
Ô£∂
Ô£π
X
‚àö
2
a1,i a2,i 1[ni ‚â• 0]h(Œ±i )Ô£∏ ‚â• BL( m + B) {ni , Œ±i }i‚àà[m] Ô£ª ‚â§ 4e‚àíB /8 .
Pr Ô£∞ Ô£≠
i‚àà[m]

Since this holds for every choice of {ni , Œ±i }i‚àà[m] we can complete the proof. The second inequality
follows from sub-exponential concentration bounds.

The next proposition at least traces back to [5] and was stated for instance in [6].
I
Proposition B.4. Suppose Œ¥ ‚àà [0, 1] and g (0) ‚àà Rm is a random vector g (0) ‚àº N (0, m
). With
2/3
‚àí‚Ñ¶(mŒ¥
)
0
m
0
0
m√óm
probability at least 1 ‚àí e
, for all vectors g ‚àà R with kg k2 ‚â§ Œ¥, letting D ‚àà R
be the
diagonal matrix where (D0 )k,k = 1(g(0) +g0 )k ‚â•0 ‚àí 1(g(0) )k ‚â•0 for each k ‚àà [m], we have

kD0 k0 ‚â§ O(mŒ¥ 2/3 )

and

17

kD0 g (0) k2 ‚â§ kg 0 k2 .

Proof of Proposition B.4. Observe that (D0 )j,j is non-zero for some j ‚àà [m] only if
|gj0 | > |(g (0) )j | .

(B.1)

Therefore, denoting by x = D0 g (0) , for each j ‚àà [m] such that xj 6= 0, we must have |xj | = |(g (0) )j | ‚â§
|(g 0 )j | so we have kxk2 ‚â§ kg 0 k2 .
Let Œæ ‚â§ 2‚àö1m be a constant parameter to be chosen later.
‚Ä¢ We denote by S1 ‚äÜ [m] the index sets where j satisfies |(g (0) )j | ‚â§ Œæ. Since we know (g (0) )j ‚àº
‚àö
N (0, 1/m), we have Pr[|(g (0) )j | ‚â§ Œæ] ‚â§ O (Œæ m) for each j ‚àà [m]. Using Chernoff bound for
3/2
all j ‚àà [m], we have with probability at least 1 ‚àí e‚àí‚Ñ¶(m Œæ) ,
n
o
|S1 | = i ‚àà [m] : |(g (0) )j | ‚â§ Œæ ‚â§ O(Œæm3/2 ) .
‚Ä¢ We denote by S2 ‚äÜ [m] \ S1 the index set of all j ‚àà [m] \ S1 where (D0 )j,j 6= 0. Using (B.1),
kg 0 k2

we have for each j ‚àà S2 it satisfies |(g 0 )j | ‚â• |(g (0) )j | ‚â• Œæ . This means |S2 | ‚â§ Œæ12 2 .
2
Œ¥ 2/3
From above, we have kD0 k0 ‚â§ |S1 | + |S2 | ‚â§ O Œæm3/2 + Œ¥Œæ2 . Choosing Œæ = 2m
1/2 gives the desired
result.


C

Theorem 1 Proof Details

In the analysis, let us define a diagonal matrices
DW(0) = diag{1W(0) (x,1)‚â•0 }

DV(0) ,W = diag{1V(0) (out1 (x),1)‚â•0 }

DW = diag{1(W(0) +W)(x,1)‚â•0 }

DV,W = diag{1(V(0) +V)(out1 (x),1)‚â•0 }

which satisfy out1 (x) = ADW (W(0) + W)(x, 1) and out(x) = ADV,W (V(0) + V)(out1 (x), 1).
Throughout the proof, we assume m ‚â• poly(CŒ± (F), CŒ± (G), pG , pF , k, Œ±‚àí1 ).

C.1

Coupling

In this subsection we present our coupling lemma. It shows that for all weight matrices not very
far from random initialization (namely, all kWk2 ‚â§ œÑw and kVk2 ‚â§ œÑv ), many good properties
occur. This includes upper bounds on the number of sign changes (i.e., on kDW(0) ‚àí DW k0 and
DV(0) ,W ‚àí DV,W

0

) as well as vanishing properties such as ADW W(0) , ADV,W V(0) being neg-

ligible. We prove such properties using techniques from prior works [4, 6].


1/4 
Lemma C.1 (Coupling). Suppose œÑw ‚â• 1, œÑw ‚àà m1/8+0.001 œÉw , m1/8‚àí0.001 œÉw , and œÑv ‚àà œÉv ¬∑

(k/m)3/8 , œÉv . Then, for every fixed x, with high probability over A, W(0) , V(0) , we have that for
all W, V satisfying kWk2 ‚â§ œÑw and kVk2 ‚â§ œÑv , it holds that
(a) kDW(0) ‚àí DW k0 ‚â§ O((œÑw /œÉw )2/3 m2/3 )
(b)

ADW W(x, 1) ‚àí ADW ((W(0) + W)(x, 1))

(c) kout1 (x)k2 = ADW (W(0) + W)(x, 1)

2

2

e
‚â§O



œÑw (œÑw /œÉw )1/3
m1/6

‚â§ O(m‚àí0.001 )

‚â§ O (œÑw )

‚â§ O((œÑv /œÉv )2/3 m)

(d)

DV(0) ,W ‚àí DV,W

(e)

ADV,W V(out1 (x), 1) ‚àí ADV,W (V(0) + V)(out1 (x), 1)

0



18

2


e œÑv (œÑv /œÉv )1/3 ¬∑(kout1 (x)k2 +1)
‚â§O

(f )

ADV,W V(0)

(g)

ADV,W (V(0) + V)(out1 (x), 1)

2

e œÑv (œÑv /œÉv )1/3
‚â§O
2


e (œÑv (kout1 (x)k2 + 1))
‚â§O

Proof.
(a) Using basic probability argument (appropriately scaling and invoking Proposition B.4) we have


œÑw 2/3
‚àö
¬∑ m = O((œÑw /œÉw )2/3 m2/3 ) .
kDW ‚àí DW(0) k0 ‚â§ O
œÉw m
(b) We write
ADW W(x, 1) ‚àí ADW (W(0) + W)(x, 1) = ‚àíADW(0) W(0) (x, 1) + A(DW(0) ‚àí DW )W(0) (x, 1)
‚àö
For the first term, we have DW(0) W(0) (x, 1) 2 ‚â§ W(0) (x, 1) 2 ‚â§ O(œÉw m) with high
probability due to concentration of chi-square distribution, and then using the randomness of A
and applying concentration of chi-square distribution again, we have ADW(0) W(0) (x, 1) 2 ‚â§
‚àö
e kœÉw ) with high probability.
O(
For the second term, invoking Proposition B.4 again, we have
(DW ‚àí DW(0) )W(0) (x, 1)

2

‚â§ kW(x, 1)k2 ‚â§ œÑw
‚àö

e ‚àö s ) ¬∑ kyk2 with high probability
Recall for every s-sparse vectors y, it satisfies kAyk2 ‚â§ O(
m
(see Proposition B.2). This implies
‚àö
‚àö
s
s
(0)
e
e
A(DW ‚àí DW(0) )W (x, 1) ‚â§ O( ‚àö ) ¬∑ kW(x, 1)k2 ‚â§ O( ‚àö œÑw )
m
m
2



2/3
w
¬∑ m . Together, we have
for s = O œÉwœÑ‚àö
m
!
1/3
‚àö
œÑ
(œÑ
/œÉ
)
w w
w
e
ADW W(x, 1) ‚àí ADW ((W(0) + W)(x, 1)) ‚â§ O
.
+ kœÉw
2
m1/6
(c) We use Lemma C.1b together with kADW W(x, 1)k2 ‚â§ kAk2 kWk2 ‚â§ O(œÑw ), where the property kAk2 ‚â§ O(1) holds with high probability using Proposition B.2.
(d) Recall DV(0) ,W = diag{1V(0) (out1 (x),1)‚â•0 } and DV,W = diag{1(V(0) +V)(out1 (x),1)‚â•0 }. Let us
denote by z = (out1 (x), 1). We know that if z ‚àà Rk+1 is a fixed vector (as opposed to depending
on W(0) and W), then owing to Proposition B.4


œÑv 2/3
DV,W ‚àí DV(0) ,W ‚â§ O
¬∑m
(C.1)
œÉv
0
2/3

with probability at least 1 ‚àí e‚àí‚Ñ¶(m ) . This means, taking Œµ-net over all possible unit vectors
z ‚àà Rk+1 , we have (C.1) holds for all such unit vectors z, therefore also for all vectors z ‚àà
Rk+1 .18 In particular, choosing z = (out1 (x), 1) finishes the proof.
(e) We write
ADV,W V(out1 (x), 1) ‚àí ADV,W (V(0) + V)(out1 (x), 1)
= ‚àíADV(0) ,W V(0) (out1 (x), 1) + A(DV(0) ,W ‚àí DV,W )V(0) (out1 (x), 1)
18
More formally, this requires one to construct a set {z1 , z2 , . . . } ‚äÇ Rk+1 of Œµ‚àí‚Ñ¶(k) unit vectors so that each unit
vector is at most Œµ-close to some point in this set with Œµ = 1/poly(m). Then, one can derive that as long as Œµ is
2/3
sufficiently small, for each i, with probability at least 1 ‚àí e‚àí‚Ñ¶(m ) inequality (C.1) holds for all unit vectors z with
kz ‚àí zi k2 ‚â§ Œµ. Taking union bond over all zi in this set finishes the argument.

19

Let us denote by z = (out1 (x), 1). Again, suppose for now that z ‚àà Rk+1 is a fixed vector that
does not depend on W(0) or W.
Then, for the first term, we have ADV(0) ,W V(0) z = AœÉ(V(0) z) and by by concentration of
chi-square distribution we have kV(0) zk2 ‚â§ O(œÉv kzk2 ) with probability at least 1‚àíe‚àí‚Ñ¶(m) , and
then using the randomness of A and applying chi-square concentration again (see Proposition B.1),
2
we have with probability at least 1 ‚àí e‚àí‚Ñ¶(k log m) ,
‚àö ‚àö
e k/ m) ¬∑ O(œÉv kzk2 ) .
ADV(0) ,W V(0) z ‚â§ O(
2

For the second term, invoking Proposition B.4, we have
(DV,W ‚àí DV(0) ,W )V(0) z

2

‚â§ kVzk2 ‚â§ œÑv ¬∑ kzk2
‚àö

e ‚àö s ) ¬∑ kyk2 with probability at least
Recall for every s-sparse vectors y, it satisfies kAyk2 ‚â§ O(
m
1 ‚àí e‚àí‚Ñ¶(s) (see Proposition B.2). This implies
e

A(DV,W ‚àí DV(0) ,W )V
for s = O




œÑv 2/3
œÉv

(0)

z

2

‚àö
s
e
‚â§ O( ‚àö œÑv kzk2 )
m



¬∑ m . Combining the two bounds above, we have for every fixed z ‚àà Rk+1 ,

ADV,W V(0) z = ADV,W Vz ‚àí ADV,W (V(0) + V)z
2
2




‚àö
‚àö
1/3
1/3
e (œÑv (œÑv /œÉv ) + kœÉv / m)kzk2 ‚â§ O
e kzk2 œÑv (œÑv /œÉv )
.
‚â§O
2

with probability at least 1 ‚àí e‚àí‚Ñ¶(k log m) . Finally, because this confidence is sufficiently small,
one can take an Œµ-net over all possible vectors z ‚àà Rk+1 and derive the above bound for all
vectors z. In particular, choosing z = (out1 (x), 1) finishes the proof.
(f) This is a byproduct of the proof of Lemma C.1e.
(g) With high probability
kADV,W V(out1 (x), 1)k2 ‚â§ kAk2 ¬∑ kVk2 ¬∑ (1 + kout1 (x)k2 ) ‚â§ O(œÑv ) ¬∑ (1 + kout1 (x)k2 )
Combining this with Lemma C.1e gives the proof.


C.2

Existantial

œÑv
In this subsection, we prove the existence of matrices W> , V> with kW> kF ‚â§ œÑ10w and kV> kF ‚â§ 10
satisfying ADW(0) W> (x, 1) ‚âà F(x) and ADV(0) ,W V> (out1 (x), 1) ‚âà Œ±G (out1 (x)).
This existential proof relies on an ‚Äúindicator to function‚Äù lemma that was used in prior work [4];
however, for the purpose of this paper we have to revise it to include a trainable bias term (or
equivalently, to support vectors of the form (x, 1)). We treat that carefully in Appendix E.

Lemma C.2. Suppose Œ± ‚àà (0, 1) and Œ±
e=

Œ±
k(pF Cs (F )+pG Cs (G)) ,

there exist M = poly(CŒ±e (F), CŒ±e (G), Œ±
e‚àí1 )

satisfying that for every m ‚â• M , with high probability over A, W(0) , V(0) , one can construct
W> ‚àà Rm√ó(d+1) and V> ‚àà Rm√ó(k+1) with
œÑw def e
œÑv def e
kW> kF ‚â§
= O(kpF Cs (F)) and kV> kF ‚â§
= O(e
Œ±kpG Cs (G))
10
10
satisfying
20

i
h
e4 ;
(a) E(x,y)‚àºD kADW(0) W> (x, 1) ‚àí F(x)k22 ‚â§ Œ±
(b) for all x and W, ADV(0) ,W V> (out1 (x), 1) ‚àí Œ±G (out1 (x))

2

‚â§Œ±
e2 ¬∑ k(out1 (x), 1)k2 .

Proof.


P
>
(a) For each r ‚àà [k], we have ADW(0) W> (x, 1) k =
i‚àà[m] ar,i 1hw(0) ,(x,1)i‚â•0 hwi , (x, 1)i. By
i

applying Lemma E.1 (which is a simple modification on top of the existential result from [4]),
>
we can construct matrix W> satisfying kAD
e2 ¬∑ k(x, 1)k2 for each
(0) W (x, 1) ‚àí F(x)k2 ‚â§ Œ±
‚àö W
x ‚àà Rd with probability at least 1 ‚àí e‚àí‚Ñ¶( m) . This translates to an expected guarantee with
respect to (x, y) ‚àº D.
(b) For each r ‚àà [k], we have
X


ar,i 1hv(0) ,(out
ADV(0) ,W V> (out1 (x), 1) k =
i‚àà[m]

1 (x),1)i‚â•0

i

hvi> , (out1 (x), 1)i .

Now, applying Lemma E.1 again,
we can construct matrix V> satisfying for each z ‚àà Rk with
‚àö
probability at least 1 ‚àí e‚àí‚Ñ¶( m) :
X X

ar,i 1hv(0) ,(z,1)i‚â•0 hvi> , (z, 1)i ‚àí Œ±G(z) ‚â§
i

r‚àà[k] i‚àà[m]

Œ±
e2
¬∑ k(z, 1)k2 .
2

By applying a careful Œµ-net
argument and using m ‚â• poly(k),19 this translates to, with prob‚àö
‚àí‚Ñ¶(
m)
ability at least 1 ‚àí e
, for all vectors z ‚àà Rk .
X X

ar,i 1hv(0) ,(z,1)i‚â•0 hvi> , (z, 1)i ‚àí Œ±G(z) ‚â§ Œ±
e2 ¬∑ k(z, 1)k2

r‚àà[k] i‚àà[m]

(C.2)

i

Finally, choosing z = out1 (x) finishes the proof.


Next, we can combine coupling and existential lemmas:
Lemma C.3. Under the assumptions of Lemma C.1 and Lemma C.2, we have
h
i
(a) E(x,y)‚àºD kADW W> (x, 1) ‚àí F(x)k22 ‚â§ O(e
Œ±4 )

(b) ‚àÄx ‚àà Rd , kADV,W V> (out1 (x), 1) ‚àí Œ±
eG (out1 (x))k2 ‚â§ Œ±
e2 + O(œÑv (œÑv /œÉv )1/3 ) ¬∑ k(out1 (x), 1)k2
h
i
2
>
(c) E(x,y)‚àºD kADW (W ‚àí W)(x, 1) ‚àí (F(x) ‚àí out1 (x))k2 ‚â§ O(e
Œ±4 )
Proof.
‚àö

e ‚àö s ) ¬∑ kyk2 with high probability (see
(a) For every s-sparse vectors y, it satisfies kAyk2 ‚â§ O(
m
Proposition B.2). We also have kW> (x, 1)k2 ‚â§ O(kW> kF ) ‚â§ O(œÑw ). Therefore, kA(DW(0) ‚àí
‚àö
‚àö
DW )W> (x, 1)k ‚â§ O( sœÑw / m) where s is the maximum sparsity of DW(0) ‚àí DW , which
satisfies s = O((œÑw /œÉw )2/3 m2/3 ) by Lemma C.1a. This, combining with Lemma C.2a gives
h
i
2
E
ADW W> (x, 1) ‚àí F(x) 2 ‚â§ 2e
Œ±2 + O(œÑw (œÑw /œÉw )1/3 /m1/6 )2 ‚â§ O(e
Œ±4 ) .
(x,y)‚àºD

19

This is a bit non-trivial to derive, because one has to argue that if z changes a little bit (i.e., by 1/poly(m)),
then according to Lemma C.1d, the number of sign changes in {1hv(0) ,(z,1)i‚â•0 }i‚àà[m] is o(m), and thus the interested
i

quantity changes by at most 1/poly(m).

21

‚àö

e ‚àö s )¬∑kyk2 with high probability. We
(b) Again, for every s-sparse vectors y, it satisfies kAyk2 ‚â§ O(
m
also have kV> (out1 (x), 1)k2 ‚â§ O(kV> kF ) ¬∑ k(out1 (x), 1)k2 ‚â§ O(œÑv ) ¬∑ k(out1 (x), 1)k2 . Therefore,
‚àö
‚àö
kA(DV(0) ,W ‚àí DV,W )V> (out1 (x), 1)k ‚â§ O( sœÑv / m) ¬∑ k(out1 (x), 1)k2
where s is the maximum sparsity of DV(0) ,W ‚àí DV,W , which satisfies s = O((œÑv /œÉv )2/3 m) by
Lemma C.1d. This, combining with Lemma C.2b gives


  2
E
ADW W> (x, 1) ‚àí F(x) 2 ‚â§ Œ±
e + O(œÑv (œÑv /œÉv )1/3 ) ¬∑ k(out1 (x), 1)k2 .
(x,y)‚àºD

(c) This combines Lemma C.1b and Lemma C.3a, together with our sufficiently large choice of m.


C.3

Optimization

In this subsection we give some structural results that shall be later used in the optimization step.
The first fact gives an explicit formula of the gradient.
Fact C.4. When Obj(W, V; (x, y)) = 12 ky ‚àí out(W, V; x)k22 , we can write its gradient as follows.
h‚àáW,V Obj(W, V; (x, y)), (‚àíW0 , ‚àíV0 )i = hy ‚àí out(x), f (W0 ; x) + g(V0 ; x)i
where

f (W0 ; x) = ADV,W (V(0) + V) ADW W0 (x, 1) , 0 + ADW W0 (x, 1)
g(V0 ; x) = ADV,W V0 (out1 (x), 1)
The next claim gives simple upper bound on the norm of the gradient.
Claim C.5. For all (x, y) in the support of D, with high probability over A, W(0) , V(0) , we have
that for all W, V satisfying kWkF ‚â§ œÑw and kVkF ‚â§ œÑv , it holds that
k‚àáW Obj (W, V; (x, y))kF ‚â§ ky ‚àí out(x)k2 ¬∑ O(œÉv + 1)
k‚àáV Obj (W, V; (x, y))kF ‚â§ ky ‚àí out(x)k2 ¬∑ O(œÑw + 1) .
Proof. For the gradient in W, we derive using the gradient formula Fact C.4 that


k‚àáW Obj (W, V; (x, y))kF = (x, 1)(y ‚àí out(x))> ADV,W (V(0) + V)(ADW , 0) + ADW
F


>
(0)
= k(x, 1)k2 ¬∑ (y ‚àí out(x)) ADV,W (V + V)(ADW , 0) + ADW
‚â§ 2 ky ‚àí out(x)k2 ¬∑ ADV,W (V

(0)

+ V)(ADW , 0) + ADW

2

2

‚â§ ky ‚àí out(x)k2 ¬∑ O(œÉv + 1) .
Above, the last inequality uses kAk2 ‚â§ O(1) and kV(0) k2 ‚â§ O(œÉv ) with high probability (using
random matrix theory, see Proposition B.2), as well as œÑv ‚â§ œÉv . Similarly, using the gradient
formula Fact C.4, we derive that
k‚àáV Obj (W, V; (x, y))kF = (out1 (x), 1)(y ‚àí out(x))> ADV,W

F

>

= k(out1 (x), 1)k2 ¬∑ (y ‚àí out(x)) ADV,W

2

‚â§ ky ‚àí out(x)k2 ¬∑ O(œÑw + 1) ¬∑ O(1)
where the last inequality uses Lemma C.1c and kAk2 ‚â§ O(1).
22



The next claim gives a careful approximation to f (W> ‚àíW; x)+g(V> ‚àíV; x), which according
to Fact C.4 is related to the correlation between the gradient direction and (W ‚àí W> , V ‚àí V> ).
Claim C.6. In the same setting as Lemma C.1 and Lemma C.2, suppose we set parameters according to Table 1. Then, we can write
f (W> ‚àí W; x) + g(V> ‚àí V; x) = H(x) ‚àí out(x) + Err
with

E

(x,y)‚àºD

kErrk22 ‚â§ O(œÑv + Œ±LG )2 ¬∑

E

(x,y)‚àºD

kH(x) ‚àí out(x)k22

+O Œ±
e2 + œÑv2 (1 + BF ) + Œ±œÑv LG (BF + 1)

2

.

and for every (x, y) ‚àº D, with high probability kErrk2 ‚â§ O(œÑw ).
Proof of Claim C.6.
f (W> ‚àí W; x) + g(V> ‚àí V; x)

= ADV,W (V(0) + V) ADW (W> ‚àí W)(x, 1), 0 + ADW (W> ‚àí W)(x, 1) + ADV,W (V> ‚àí V)(out1 (x), 1)


= ADV,W (V(0) + V) ADW (W> ‚àí W)(x, 1), 0 + ADW W> (x, 1) + ADV,W V> (out1 (x), 1)
|
{z
} |
{z
}
‚ô£

‚ô†

‚àí (ADW W(x, 1) + ADV,W V(out1 (x), 1))
|
{z
}
‚ô¶

We treat the three terms separately.
‚Ä¢ For the ‚ô£ term, under expectation over (x, y) ‚àº D,
k‚ô£k22 ‚â§ kADV,W V(0) k2 + kAk22 kVk22



ADW (W> ‚àí W)x

‚â§ O(1) ¬∑ O(œÑv )2 ¬∑ kF(x) ‚àí out1 (x)k22 + O(e
Œ±2 )

2
2

where the last inequality uses Lemma C.1f and Lemma C.3c, together with œÑv ‚â§

1
polylog(m) œÉv .

‚Ä¢ For the ‚ô† term, under expectation over (x, y) ‚àº D,
k‚ô† ‚àí (F(x) + Œ±G(F(x))k22 ‚â§ O(e
Œ±2 + œÑv (œÑv /œÉv )1/3 )2 ¬∑ (kout1 (x)k2 + 1)2
+ O(Œ±LG )2 kF(x) ‚àí out1 (x)k22
‚â§ O(œÑv2 )2 ¬∑ (kout1 (x)k2 + 1)2 + O(Œ±LG )2 kF(x) ‚àí out1 (x)k22
where the first inequality uses Lemma C.3a and Lemma C.3b, as well as the Lipscthiz continuity of G(x) (which satisfies kG(x) ‚àí G(y)k ‚â§ LG kx ‚àí yk); and the second inequality uses
1
2
e.
œÉv ‚â§ œÑv and the definition of Œ±
‚Ä¢ For the ‚ô¶ term, under expectation over (x, y) ‚àº D,
k‚ô¶ ‚àí out(x)k22 ‚â§ O (kout1 (x)k2 + 1)œÑv2
where the inequality uses Lemma C.1b, Lemma C.1e and

1
œÉv

2

‚â§ œÑv2 .

In sum, we have
def

Err = f (W> ‚àí W; x) + g(V> ‚àí V; x) ‚àí (F(x) + Œ±G(F(x)) ‚àí out(x)
satisfies
E

(x,y)‚àºD

kErrk22 ‚â§

h
E

(x,y)‚àºD

O(œÑv + Œ±LG )2 ¬∑ kF(x) ‚àí out1 (x)k22 + O Œ±
e2 + (kout1 (x)k2 + 1)œÑv2

23

2 i

.

Combining this with Claim C.7, and using kout1 (x)k2 ‚â§ kout1 (x) ‚àí F(x)k2 + BF , we have
2
E kErrk22 ‚â§ O(œÑv + Œ±LG )2 ¬∑ E kH(x) ‚àí out(x)k22 + O Œ±
e2 + (1 + BF )œÑv2
(x,y)‚àºD

(x,y)‚àºD

+ O(œÑv + Œ±LG )2 ¬∑ (œÑv (BF + 1) + Œ±BF ‚ó¶G )2 .
‚àö
Using BF ‚ó¶G ‚â§ kpF Cs (F)BF ‚â§ œÑŒ±v BF (see Fact A.3), we finish the bound on E(x,y)‚àºD kErrk22 .
As for the absolute value bound, one can naively derive that with high probability kf (W> ‚àí
W; x)k2 ‚â§ O(œÑw ), kg(V> ‚àí V; x)k2 ‚â§ O(œÑw œÑv ), kH(x)k2 ‚â§ B‚àö
F + Œ±BF ‚ó¶G , and kout(œÑw )k2 ‚â§
O(œÑw ) (by Lemma
C.1c
and
C.1g).
Combining
them
with
B
‚â§
kpF Cs (F) ‚â§ œÑw and Œ±BF ‚ó¶G ‚â§
F


1

Œ± BF LG + Cs (G) ‚â§ kpG Cs (G) BF LG + Cs (G) ‚â§ œÑw finishes the proof.
Finally, we state a simple claim that bounds the norm of kout1 (x) ‚àí F(x)k2 given the norm of
kout(x) ‚àí H(x)k2 .
Claim C.7. In the same setting as Lemma C.1, if we additionally have œÑv ‚â§
fixed x, with high probability over

1
polylog(m) ,

for every

A, W(0) , V(0) ,

e v (BF + 1) + Œ±BF ‚ó¶G ) .
kout1 (x) ‚àí F(x)k2 ‚â§ 2kout(x) ‚àí H(x)k2 + O(œÑ
Proof. We can rewrite


out1 (x) ‚àí F(x) = (out(x) ‚àí H(x)) ‚àí ADV,W (V(0) + V)(out1 (x), 1) + Œ±G(F(x)) .
e v (kout1 (x)k2 + 1)), and using
Using Lemma C.1g we have kADV,W (V(0) + V)(out1 (x), 1)k ‚â§ O(œÑ
the boundedness we have kG(F(x))k2 ‚â§ BF ‚ó¶G . We also have kout1 (x)k2 ‚â§ kout1 (x)‚àíF(x)k2 +BF .
Together, we have
e v (kout1 (x) ‚àí F(x)k2 + BF + 1)) + Œ±BF ‚ó¶G .
kout1 (x) ‚àí F(x)k2 ‚â§ kout(x) ‚àí H(x)k2 + O(œÑ
Using œÑv ‚â§

C.4

1
polylog(m)



we finish the proof.

Proof of Theorem 1


1
e
Theorem 1. Under Concept 1 or Concept 2, for every Œ± ‚àà 0, Œò(
kpG Cs (G) ) and Œ¥ ‚â• OPT +

e Œ±4 (kpG Cs (G))4 (1 + BF )2 . There exist M = poly(CŒ± (F), CŒ± (G), pF , Œ±‚àí1 ) satisfying that for
Œò
every m ‚â• M , with high probability over A, W(0) , V(0) , for a wide range of random initialization
parameters œÉw , œÉv (see Table 1), choosing




(kpF Cs (F))2
Œ±pG Cs (G) 2
e
e
e
Œ∑w = Œò (min{1, Œ¥}) Œ∑v = Œ∑w ¬∑ Œò
T =Œò
min{1, Œ¥ 2 }
pF Cs (F)
With high probability, the SGD algorithm satisfies
T ‚àí1
1 X
E kH(x) ‚àí out(Wt , Vt ; x)k22 ‚â§ O(Œ¥) .
T
(x,y)‚àºD
t=0

Proof of Theorem 1. We first assume that throughout the SGD algorithm, it satisfies
kWt kF ‚â§ œÑw

and kVt kF ‚â§ œÑv .

We shall prove in the end that (C.3) holds throughout the SGD algorithm.

24

(C.3)

On one hand, using Claim C.6, at any point Wt , Vt , we have
h‚àáW,V Obj(Wt , Vt ; (xt , yt )), (Wt ‚àí W> , Vt ‚àí V> ))i
= hyt ‚àí out(Wt , Vt ; xt ), H(xt ) ‚àí out(Wt , Vt ; xt ) + Errt i
1
‚â• kH(xt ) ‚àí out(Wt , Vt ; xt )k22 ‚àí 2kErrt k22 ‚àí 2kH(xt ) ‚àí yt k22
2
where Errt comes from Claim C.6. On the other hand, using Wt+1 = Wt ‚àíŒ∑w ‚àáW Obj(Wt , Vt ; (xt , yt ))
and Vt+1 = Vt ‚àí Œ∑v ‚àáV Obj(Wt , Vt ; (xt , yt )), we have
h‚àáW,V Obj(Wt , Vt ; (xt , yt )), (W ‚àí W> , V ‚àí V> ))i
Œ∑w
Œ∑v
=
k‚àáW Obj(Wt , Vt ; (xt , yt ))k2F + k‚àáV Obj(Wt , Vt ; (xt , yt ))k2F
{z 2
}
|2
‚ô•

1
1
1
1
kWt ‚àí W> k2F ‚àí
kWt+1 ‚àí W> k2F +
kVt ‚àí V> k2F ‚àí
kVt+1 ‚àí V> k2F
+
2Œ∑w
2Œ∑w
2Œ∑v
2Œ∑v
Recall from Claim C.5,
‚ô• ‚â§ O(Œ∑w + Œ∑v œÑw2 ) ¬∑ kyt ‚àí out(Wt , Vt ; xt )k22 ‚â§ O(Œ∑w + Œ∑v œÑw2 ) ¬∑ kH(xt ) ‚àí out(Wt , Vt ; xt )k22 + kH(xt ) ‚àí yt k22
Therefore, as long as O(Œ∑w + Œ∑v œÑw2 ) ‚â§ 0.1, it satisfies
1
1
1
kH(xt ) ‚àí out(Wt , Vt ; xt )k22 ‚â§ 2kErrt k22 + 4kH(xt ) ‚àí yt k22 +
kWt ‚àí W> k2F ‚àí
kWt+1 ‚àí W> k2F
4
2Œ∑w
2Œ∑w
1
1
+
kVt ‚àí V> k2F ‚àí
kVt+1 ‚àí V> k2F
2Œ∑v
2Œ∑v
After telescoping for t = 0, 1, . . . , T0 ‚àí 1,
T0 ‚àí1
kWT0 ‚àí V> k2F
kWT0 ‚àí W> k2F
1 X
+
+
kH(xt ) ‚àí out(Wt , Vt ; xt )k22
2Œ∑w T0
2Œ∑v T0
2T0
t=0

‚â§

kW> k2F
kV> k2F
O(1)
+
+
2Œ∑w T0
2Œ∑v T0
T0

TX
0 ‚àí1

kErrt k22 + kH(xt ) ‚àí yt k22 .

(C.4)

t=0

Choosing T0 = T , taking expectation with respect to {(xt , yt )}t=0,1,...,T ‚àí1 on both sides, and using
Claim C.6 (by noticing O(œÑv + Œ±LG ) ‚â§ 0.1) and the definition of OPT, we have
T ‚àí1
kW> k2F
kV> k2F
1 X
+
+ O(OPT + Œ¥0 )
E kH(x) ‚àí out(Wt , Vt ; x)k22 ‚â§
4T
2Œ∑w T
2Œ∑v T
(x,y)‚àºD
t=0

where
2
Œ¥0 = Œò Œ±
e2 + œÑv2 (1 + BF ) + Œ±œÑv LG (BF + 1)

e Œ±
=Œò
e4 + Œ±4 (kpG Cs (G))4 (1 + BF )2 + Œ±4 (kpG Cs (G))2 L2G (BF + 1)2

e Œ±4 (kpG Cs (G))4 (1 + BF )2
=Œò
1
Above, the last inequality uses kpG C1s (G) ‚â§ O( 1+L
) (see Fact A.3) and the choice of Œ±
e from
G
Lemma C.2.
Using kW> kF ‚â§ œÑw /10, kV> kF ‚â§ œÑv /10, we have as long as Œ¥ ‚â• OPT + Œ¥0 ,
T ‚àí1
1 X
œÑ 2 /Œ∑w + œÑv2 /Œ∑v
E kH(x) ‚àí out(Wt , Vt ; x)k22 ‚â§ O(Œ¥) as long as T ‚â• ‚Ñ¶( w
).
T
Œ¥
(x,y)‚àºD
t=0

Finally, we need to check that (C.3) holds. To do so, we use kErrt k2 ‚â§ O(œÑ ) from Claim C.6 and
25



apply martingale concentration on (C.4) and derive that, with high probability


kWT0 ‚àí V> k2F
kW> k2F
kV> k2F
kWT0 ‚àí W> k2F
œÑw
e
.
+
‚â§
+
+ O(Œ¥) + O ‚àö
2Œ∑w T0
2Œ∑v T0
2Œ∑w T0
2Œ∑v T0
T0
This implies
kWT0 k2F
kWT0 k2F
kW> k2F
kV> k2F
e
+
‚â§
+
+ O(Œ¥) + O
4Œ∑w T0
4Œ∑v T0
Œ∑w T0
Œ∑v T0



œÑ
‚àöw
T0
2


.
2

Using kW> kF ‚â§ œÑw /10 and kV> kF ‚â§ œÑv /10, and using the relationship Œ∑œÑww = œÑŒ∑vv , we have
 ‚àö 
kWT0 k2F
kWT0 k2F
4kW> k2F
4kV> k2F
Œ∑w T0
e
.
+
‚â§
+
+ 0.1 + O
œÑw2
œÑv2
œÑw2
œÑv2
œÑw
Therefore, choosing

e
T =Œò
kW

k2

œÑw2
min{1, Œ¥ 2 }

kW


e (min{1, Œ¥}) ‚â§ 0.1
Œ∑w = Œò

k2

we can ensure that œÑT20 F + œÑT20 F ‚â§ 1 with high probability for all T0 = 0, 1, . . . , T ‚àí 1 (so (C.3)
w
v
holds).
Finally, we note that it satisfies poly(CŒ±e (F), CŒ±e (G), Œ±
e‚àí1 ) ‚â§ poly(CŒ± (F), CŒ± (G), pF , Œ±‚àí1 ) with
Œ±
the choice Œ±
e = k(pF Cs (F )+pG Cs (G)) .


D

Theorem 2 and Theorem 3 Proof Details

Our proof relies on the following two structural lemmas. The first one is a simple corollary of the
Parseval‚Äôs equality from Boolean analysis.
P
Q
Lemma D.1. For every k ‚àà {2, 3, ¬∑ ¬∑ ¬∑ , d}, for every function f (x) = S 0 ‚äÜ[d] ŒªS 0 j‚ààS 0 xj , suppose
there exists S ‚äÜ [d] of size k and i ‚àà S such that
i
h
Q
1 2
Œ±
(D.1)
Ex‚àºU ({‚àí1,1}d ) |f (x) ‚àí (xi + Œ± j‚ààS xj )|2 ‚â§ 16
P
1 2
Then we must have ŒªS ‚â• 43 Œ± and S 0 ‚äÜ[d],|S 0 |=k,S 0 6=S Œª2S 0 ‚â§ 16
Œ± .
Proof of Lemma D.1. The lemma follows from the following equality that can be easily verified:
h
i
Y
X
E
|f (x) ‚àí (xi + Œ±
xj )|2 = (Œª{i} ‚àí 1)2 + (ŒªS ‚àí Œ±)2 +
Œª2S 0 .

x‚àºU ({‚àí1,1}d )

S 0 ‚äÜ[d],S 0 6=S,S 0 6={i}

j‚ààS

The next one can be proved by carefully bounding the matrix rank (see Section D.3).
Lemma D.2. For every Œ± > 0, for every matrix M ‚àà RN √óR where R ‚â• 2N , then there can not
be vectors a1 , ¬∑ ¬∑ ¬∑ , aR ‚àà RN such that for every r ‚àà [R]:
P
1 2
2
hMr , ar i ‚â• 34 Œ± and
r0 ‚àà[R],r0 6=r hMr0 , ar i ‚â§ 16 Œ± .

D.1

Proof of Theorem 2

‚àö
Throughout the proof of Theorem 2, for notational simplicity, we re-scale inputs x by d so that
x ‚àà {¬±1}d , and also re-scale W‚àó in the target function (7.1) to W‚àó = (ei1 , ei2 , ¬∑ ¬∑ ¬∑ eik ).
For notation simplicity, below we restate Theorem 2 with respect to one single output k = 1
and d1 = d. The full statement for multiple outputs and more general distributions is a simple
corollary (see Remark D.3).
26


d1
1
Theorem 2 (simplified). For every integers k, d, N satisfying 2 ‚â§ k ‚â§ d and N ‚â§ 1000
k ,
for every(Mercer) kernel K(x, y) : Rd√ód ‚Üí R, for every x(1) , ¬∑ ¬∑ ¬∑ , x(N ) ‚àà Rd , there exist at least
0.99 √ó kd many S ‚äÜ [d] of size k such that, for every i ‚àà S, for every w ‚àà RN and the associated
P
kernel function K(x) = n‚àà[N ] K(x, x(n) )wn ,


 2
Q
1 2
> 16
Ex‚àºU ({‚àí1,1}d ) K(x) ‚àí xi + Œ± j‚ààS xj
Œ± .
Proof of Theorem 2. By property of (mercer) kernel, there exists feature mapping Œ¶(x) = (œÜ` (x))`‚ààN
where each œÜ` : Rd ‚Üí R such that:
P
K(x, y) = `‚ààN œÜ` (x)œÜ` (y) .
Since we only care x ‚àà {‚àí1, 1}d , we can write each œÜ` (x) in its (Boolean) Fourier basis:
P
Q
‚àÄx ‚àà {‚àí1, 1}d : œÜ` (x) = S‚äÜ[d] ŒªS,` j‚ààS xj .
d

Given arbitrary x(1) , . . . , x(N ) ‚àà Rd , we can define matrix M ‚àà RN √ó(k) as follows:
X
def
‚àÄn ‚àà [N ], ‚àÄS ‚äÜ [d] with |S| = k :
Mn,S =
ŒªS,` œÜ` (x(n) ) .
`‚ààN

For any w ‚àà

RN ,

we can write
X
X X
K(x) =
K(x, x(n) )wn =
œÜ` (x)œÜ` (x(n) )wn
n‚àà[N ] `‚ààN

n‚àà[N ]

=

X  X X
S 0 ‚äÜ[d]

ŒªS 0 ,` œÜ` (x(n) )wn

Y

xj =

j‚ààS 0

n‚àà[N ] `‚ààN

X

Y

hMS 0 , wi ¬∑

xj

(D.2)

j‚ààS 0

S 0 ‚äÜ[d]

Hence, byPLemma D.1, if for some S ‚äÜ [d] of size k, there exists i ‚àà S and exists wS ‚àà RN with
KS (x) = n‚àà[N ] K(x, x(n) )[wS ]n satisfying
h
i
Y
1
E
|KS (x) ‚àí (xi + Œ±
xj )|2 ‚â§ Œ±2 ,
16
x‚àºU ({‚àí1,1}d )
j‚ààS

then it must satisfy
hMS , wS i ‚â• 34 Œ±

and

2
|S|‚äÜ[d],|S 0 |=k,S 0 6=S hMS 0 , wS i

P

‚â§

1 2
16 Œ±

.


d

However, according to Lemma D.2, as long as k ‚â• 1000N , we know that the above condition
cannot hold for at least 0.99 fraction of the S ‚äÜ [d] of size k. This completes the proof.

Remark D.3. In the full statement of Theorem 2, there are multiple outputs K1 (x), . . . , Kk (x). It
suffices to focus on an arbitrary (say the first) coordinate and then apply the above lower bound.

def
In the full statement of Theorem 2, we have x ‚àº D = U {‚àí1, 1}d1 √ó D2 for k ‚â§ d1 ‚â§ d. In
such a case, one can write each x = (x/ , x. ) for x/ ‚àà Rd1 and x. ‚àà Rd‚àíd1 . Now, equation (D.2)
becomes
X
Y
E [K(x)] = E [K(x/ , x. )] =
hMS , wi ¬∑
xj
x. ‚àºD2

x. ‚àºD2

S 0 ‚äÜ[d1 ]

j‚ààS 0

and the final statement can be derived using the following simple property, for every S ‚äÜ [d1 ]
i
i
h
h
Y
Y
E
|K(x/ , x. ) ‚àí (xi + Œ±
xj )|2 ‚â•
E
| E [K(x/ , x. )] ‚àí (xi + Œ±
xj )|2 .
(x/ ,x. )‚àºD

j‚ààS

x/ ‚àºU ({‚àí1,1}d1 )

27

x. ‚àºD2

j‚ààS

D.2

Proof of Theorem 3

‚àö
For notation simplicity, we re-scale inputs x by d so that x ‚àà {¬±1}d , and also re-scale W‚àó in the
target function (7.1) to W‚àó = (ei1 , ei2 , ¬∑ ¬∑ ¬∑ eik ).
Again for notation simplicity, below we restate Theorem 3 with respect to one single output
k = 1 and d1 = d. The full statement for multiple outputs and more general distributions is
analogous (in the same spirit as Remark D.3).

d
1
Theorem 3 (simplified). For every integers k, d, D satisfying 2 ‚â§ k ‚â§ d and D ‚â§ 1000
k , for

every Œ± ‚àà (0, 1), for every feature mapping œÜ : Rd ‚Üí RD , there exist at least 0.99 √ó kd many
S ‚äÜ [d] of size k such that, for every i ‚àà S, for every w ‚àà RD and the associated linear function
F(x) = w> œÜ(x),


 2
Q
1 2
> 16
Ex‚àºU ({‚àí1,1}d ) F(x) ‚àí xi + Œ± j‚ààS xj
Œ± .
Proof of Theorem 3. Let us write œÜ(x) = (œÜ1 (x), ¬∑ ¬∑ ¬∑ , œÜD (x)) where each œÜi : Rd ‚Üí R. Since we
only focus on x ‚àà {‚àí1, 1}d we can write
P
Q
œÜi (x) = S‚äÜ[d] ŒªS,i j‚ààS xj
d

for some set of coefficients ŒªS,i ‚àà R. Now, define matrix M ‚àà RD√ó2 as follows:
‚àÄi ‚àà [D],

‚àÄS ‚äÜ [d] :

Mi,S = ŒªS,i .

We have for every w ‚àà RD (that can possibly depend on S),
P
Q
w> œÜ(x) = S‚äÜ[d] hMS , wi j‚ààS xj
This is exactly (D.2) in the proof of Theorem 2, so the rest of the proof follows analogously by
applying Lemma D.2.


D.3

Proof of Lemma D.2

Proof of Lemma D.2. Suppose by way towards contradiction that there exist vectors a1 , ¬∑ ¬∑ ¬∑ , aR ‚àà
RN such that for every r ‚àà [R]:
P
1 2
2
hMr , ar i ‚â• 34 Œ± and
r0 ‚àà[R],r0 6=r hMr0 , ar i ‚â§ 16 Œ±
Let us define br =

1
hMr ,ar i ar

so they become

hMr , br i = 1

and

P

r0 ‚àà[R],r0 6=r hMr0 , br i

2

‚â§

1
9

Now, defining matrix B = {br }r‚àà[R] ‚àà RN √óR , we can rewrite
B> M = I + E ‚àà RR√óR

(D.3)

2
where E is matrix with zero diagonals. Since for every r ‚àà [R], it satisfies
r0 ‚àà[R] Er,r0 =
P
1
1
2
2
r0 ‚àà[R],r0 6=r hMr0 , br i ‚â§ 9 , we conclude that kEkF ‚â§ 9 R.
1
Next, since E cannot have more than 9 R singular values that are ‚â• 1. By the min-max
theorem for singular values (a.k.a. Courant-Fischer theorem), there exists a subspace U of RR with
dimension 98 R such that maxx‚ààU,kxk2 =1 kExk2 < 1. As a result, for every non-zero x ‚àà U , we have
k(I + E)xk2 ‚â• kxk2 ‚àí kExk2 > 0. This implies
8
rank(I + E) ‚â• R .
9
To the contrary, we have rank(B> M) ‚â§ N ‚â§ 12 R. This gives a contradiction.


P

28

D.4

Proof of Corollary 7.1

Proof of Corollary 7.1. To apply Theorem 1, we need to carefully verify Concept 1 by appropriately
re-scaling. Without loss of generality suppose (i1 , . . . , ik ) = (1, . . . , k). For every i ‚àà [k], let us
define
‚àö
d
def
def
zi = Fi (x) = ‚àö xi
k
‚àö
which satisfies Cs (F) = O( d), pF = 1, and kF(x)k2 = 1. Next, let us define
X
X (‚àí1)si zi k
kk
def
‚àö
(‚àí1)s1 +¬∑¬∑¬∑+sk
Gr (z) = ‚àö
kk!2k s‚àà{0,1}k
k
i‚àà[k]
and one can verify that Gr (z) =
satisfies Cs (G) = 2O(k) and pG =

k/2 Q
k‚àö
i‚àà[k] zi and therefore Gr (F(x))
k
k
2 . In sum, we have constructed

F(x) + Œ±G(F(x)) =

‚àö1
k

‚àö

dxi + Œ±

Q

j‚àà[k] (

‚àö

=


dxj )

‚àö1
k

Q

‚àö
(
dxi ). It also
i‚àà[k]

i‚àà[k]

‚àö



and we can thus apply Theorem 1 (after rescaling the label by 1/ k).

E

Existential Tool

In this section we include a simple variant of the existential lemma from [4]. We include the proofs
only for completeness‚Äô sake.
Consider random function G((x, 1); W> ) = (G1 ((x, 1); W> ), . . . , Gk ((x, 1); W> )) in which
>

def

Gr ((x, 1); W ) =

m
X

ar,i ¬∑ hwi> , (x, 1)i ¬∑ 1hw(0) ,(x,1)i‚â•0
i

i=1

(0)

where W> ‚àà Rm√ó(d+1) is a given matrix, W(0) ‚àà Rm√ó(d+1) is a random matrix where each wi
I
i.i.d. from N (0, m
), and each ar,i is i.i.d. from N (0, 1).
We have the following main lemma of this section:

is

Lemma E.1. Given any F : Rd ‚Üí Rk with general complexity (p, Cs (F), CŒµ (F)), for every Œµ ‚àà
(0, pkC1s (F ) ), there exists M = poly(CŒµ (F), 1/Œµ) such that if m ‚â• M , then with high probability there
> ) ‚àà Rm√ód (that does not depend on x) with
is a construction W> = (w1> , . . . , wm

kW> k2,‚àû ‚â§

kpCŒµ (F )
m

and

(F )
e kpC
‚àös
kW> kF ‚â§ O(
)
m
‚àö

satisfying, for every x ‚àà Rd , with probability at least 1 ‚àí e‚àí‚Ñ¶( m)
Pk
>
r=1 |Fr (x) ‚àí Gr ((x, 1); W )| ‚â§ Œµ ¬∑ k(x, 1)k2 ,

E.1

Restate Lemma E.1

We first note that, by replacing (x, 1) with x, we can restate Lemma E.1 as follows. Consider a
target function Œ¶ : Rd ‚Üí Rk where
 ‚àó

p
X
hw1,r,i , xi
def
‚àó
‚àó
ar,i ¬∑ œÜr,i
¬∑ hw2,r,i
, xi
Œ¶r (x) =
kxk2
i=1

29

and œÜr,i : R ‚Üí R has only zero-order and odd-order terms in its Taylor expansion at zero, and
‚àó k = kw ‚àó k = 1, C (Œ¶) = max {C (œÜ )} and C (Œ¶) = max {C (œÜ )}. Let
|a‚àór,i | ‚â§ 1, kw1,i
2
Œµ
r,i
Œµ r,i
s
r,i
s r,i
2,i 2
def

Gr (x; W> ) =

m
X

ar,i ¬∑ hwi> , xi ¬∑ 1hw(0) ,xi‚â•0 .
i

i=1

be the similarly defined random function. We have the following:
Lemma E.1‚Äô. For every Œµ ‚àà (0, pkCs1(Œ¶,1) ), there exists M = poly(CŒµ (Œ¶, 1), 1/Œµ) such that if m ‚â• M ,
> ) ‚àà Rm√ód (that does not depend
then with high probability there is a construction W> = (w1> , . . . , wm
on x) with
kpCŒµ (Œ¶, 1)
s (Œ¶, 1)
e kpC‚àö
)
kW> k2,‚àû ‚â§
and kW> kF ‚â§ O(
m
m
‚àö

satisfying, for every x ‚àà Rd , with probability at least 1 ‚àí e‚àí‚Ñ¶(
k
X

m)

Œ¶r (x) ‚àí Gr (x; W> ) ‚â§ Œµ ¬∑ kxk2 ,

r=1

We stress that Lemma E.1‚Äô is a modified version of Lemma G.1 from [4, ver.4]. The only
difference is that in their original Lemma G.1, the indicator function 1hw(0) ,xi‚â•0 has an additional
i

random bias term (that is, becomes 1hw(0) ,xi+b(0) ‚â•0 ). In our Lemma E.1‚Äô, we do not allow such bias
i
i
and thus we can only fit functions Œ¶ whose Taylor expansions have only zero-order and odd-order
terms (as opposed to arbitrary smooth functions in the original Lemma G.1).
The proof of Lemma E.1‚Äô is based on the following ‚Äúindicator to function‚Äù lemma, which is a
simple modification from Lemma 5.2 of [4, ver.4]. It says that given unit vector w‚àó ‚àà Rd , we can
approximate function œÜ(hw‚àó , xi) (over x) by designing a random function 1hw,xi‚â•0 h(hw, w‚àó i) where
w is a random Gaussian and h(¬∑) is a function at our choice. Again, the only difference between
our Lemma E.2 and Lemma 5.2 of [4, ver.4] is that we do not have the random bias term.
Lemma E.2 (indicator to function). For every smooth function œÜ that only
 has zero-order and
1
odd-order terms in its Taylor expansion at point zero, every Œµ ‚àà 0, Cs (œÜ,1)
, there exists a function h : R ‚Üí [‚àíCŒµ (œÜ, 1), CŒµ (œÜ, 1)] that is also CŒµ (œÜ, 1)-Lipschitz continuous with the following two
(equivalent) properties:
(a) For every x1 ‚àà [‚àí1, 1]:
h
E 1Œ±

1 x1 +Œ≤1

‚àö

i
h(Œ±
)
‚àí œÜ(x1 ) ‚â§ Œµ
1
1‚àíx2 ‚â•0
1

where Œ±1 , Œ≤1 ‚àº N (0, 1) are independent random variables.
(b) For every w‚àó , x ‚àà Rd with kw‚àó k2 = kxk2 = 1:


E 1hw,xi‚â•0 h(hw, w‚àó i) ‚àí œÜ(hw‚àó , xi) ‚â§ Œµ
where w ‚àº N (0, I) is an d-dimensional Gaussian.


Furthermore, h satisfies EŒ±1 ‚àºN (0,1) h(Œ±1 )2 ‚â§ Cs (œÜ, 1)2 .
In the remainder of this section, for sake of completeness, we first prove Lemma E.2 in Section E.2,
and then prove Lemma E.1‚Äô and Section E.3.

30

E.2

Proof of Lemma E.2: Indicator to Function

Recall from [4] by renaming variablesp
it suffices to prove Lemma E.2a. For notation simplicity, let
us denote w0 = (Œ±1 , Œ≤1 ) and x = (x1 , 1 ‚àí x21 ) where Œ±1 , Œ≤1 are two independent random standard
Gaussians.
Throughout the
p proof, we also take an alternative view of the randomness. We write hw0 , xi = Œ±
and Œ±1 = Œ±x1 + 1 ‚àí x21 Œ≤ for two independent Œ±, Œ≤ ‚àº N (0, 1).20
We first make a technical claim involving in fitting monomials in x1 . It is a simplified version
of Claim B.1 of [4, ver.4].
Claim E.3. Let hi (x) be the degree-i Hermite polynomial (see Definition A.4 of [4, ver.4]). For
such that
every odd integer i ‚â• 1 there exists constant p0i with |p0i | ‚â• (i‚àí1)!!
4
xi1 =

1
p0i

[hi (Œ±1 ) ¬∑ 1[hx, w0 i ‚â• 0]]

E

w0 ‚àºN (0,I)‚àºN (0,1)

(The proof of Claim E.3 is identical to that of the original Claim B.1 of [4, ver.4] by forcing the
bias term b0 = 0.)
We next use Claim E.3 to fit arbitrary functions œÜ(x1 ). By Taylor expansion, we have
œÜ(x1 ) = c0 +

‚àû
X

ci xi1 = c0 +

‚àû
X

c0i ¬∑

i=1

i=1, odd i

E

Œ±,Œ≤,b0 ‚àºN (0,1)



hi (Œ±1 ) ¬∑ 1[hx, w0 i + b0 ‚â• 0]

where
c0i =

def

ci
,
p0i

|c0i | ‚â§

4 |ci |
(i ‚àí 1)!!

(E.1)

Next, recall the following claim on absolute values of the Hermite polynomials (see Claim B.2 of
[4, ver.4]).
q
def
Claim E.4. Setting Bi = 100i1/2 + 10 log 1Œµ , we have


P‚àû 0
|c
|
¬∑
E
|h
(z)|
¬∑
1[|z|
‚â•
B
]
‚â§ /8
(a)
i
i
z‚àºN
(0,1)
i
i=1


P‚àû 0
(b)
i=1 |ci | ¬∑ Ez‚àºN (0,1) |hi (Bi )| ¬∑ 1[|z| ‚â• Bi ] ‚â§ /8

 1
P‚àû 0
(c)
i=1 |ci | ¬∑ Ez‚àºN (0,1) |hi (z)| ¬∑ 1[|z| ‚â§ Bi ] ‚â§ 2 CŒµ (œÜ, 1)
 d
 1
P‚àû 0
(d)
i=1 |ci | ¬∑ Ez‚àºN (0,1) dz hi (z) ¬∑ 1[|z| ‚â§ Bi ] ‚â§ 2 CŒµ (œÜ, 1)
def
Now, let us define b
hi (Œ±1 ) = hi (Œ±1 ) ¬∑ 1[|Œ±1 | ‚â§ Bi ] + hi (sign(Œ±1 )Bi ) ¬∑ 1[|Œ±1 | > Bi ] as the truncated
version of the Hermite polynomial hi (¬∑). Using Claim E.4, we have
‚àû
h
i
X
0
b
œÜ(x1 ) = c0 + R (x1 ) +
c0i ¬∑
E
hi (Œ±1 ) ¬∑ 1[hx, w0 i ‚â• 0]

i=1

Œ±,Œ≤‚àºN (0,1)

where |R0 (x1 )| < /4 uses Claim E.4a and Claim E.4b. In other words, if we define
def

h(Œ±1 ) = 2c0 +

‚àû
X

c0i ¬∑ b
hi (Œ±1 )

i=1

This is possible for the following reason. Let x‚ä• = ( 1 ‚àí x21 , ‚àíx1 ) be unit vector orthogonal to x. We can write
w0 = Œ±x + Œ≤x‚ä• where Œ±, Œ≤ ‚àº N (0, 1) are two independent Gaussians.
20

p

31

then we have


E

Œ±,Œ≤‚àºN (0,1)


1[hx, w0 i ‚â• 0] ¬∑ h(Œ±1 ) ‚àí œÜ(x1 ) = R0 (x1 ) ‚â§ Œµ/4 .

As for the range of h, we use Claim E.4b and Claim E.4c to derive that
Œµ 1
|h(Œ±1 )| ‚â§ 2c0 + + CŒµ (œÜ, 1) ‚â§ CŒµ (œÜ, 1) .
8 2
As for the Lipschitz continuity of h on its first coordinate Œ±1 , we observe that for each i > 0,
d
b
hi (z) for |z| < Bi .
hi (z) has zero sub-gradient for all |z| ‚â• Bi . Therefore, it suffices to bound dz
Replacing the use of Claim E.4c by Claim E.4d immediately gives us the same bound on the
Lipschitz continuity of h with respect to Œ±1 .

As for the expected square EŒ±1 ‚àºN (0,1) h(Œ±1 )2 , we can write
h(Œ±1 ) = 2c0 +

‚àû
X

c0i

¬¨
¬∑b
hi (Œ±1 ) = 2c0 +

i=1

‚àû
X

c0i ¬∑ hi (Œ±1 ) ¬±

i=1

Œµ
4

Above, ¬¨ uses Claim E.4a and Claim E.4b.
Using the othogonality condition of Hermite polyno‚àö
mials (that is, Ex‚àºN (0,1) [hi (x)hj (x)] = 2œÄj!Œ¥i,j ), we immediately have
2

E

Œ±1 ‚àºN (0,1)

2

[h(Œ±1 ) ] ‚â§ O(Œµ +

c20 )

‚àû
X
+ O(1) ¬∑
(c0i )2 (i!)
i=1

‚àû
X
(i!) ¬∑ |ci |2
2
2
‚â§ O(Œµ + c0 ) + O(1) ¬∑
((i ‚àí 1)!!)2
i=1

‚â§ O(Œµ2 + c20 ) + O(1) ¬∑
Above, ¬¨ uses inequality

i!
((i‚àí1)!!)2

‚àö
‚â§ 2 i for all i ‚â• 1.

‚àû
X

i0.5 ¬∑ |ci |2 ‚â§ Cs (œÜ, 1)2 .

i=1



This finishes the proof of Lemma E.2a.

E.3

Proof of Lemma E.1‚Äô

Without loss of generality we assume kxk2 = 1 in this proof. (Both Œ¶ and G are positive homogeneous in x.)
‚àó
‚àó
Fit a single function a‚àór,i œÜr,i (hw1,r,i
, xi)hw2,r,i
, xi. We first fix some r ‚àà [k] and i ‚àà [p]
>
d
(r,i)
and construct weights wj ‚àà R . Let h (¬∑) be the function h(¬∑) constructed from œÜ = œÜr,i using
Lemma E.2. We have |h(r,i) | ‚â§ CŒµ (Œ¶, 1). Define
‚àö

def
(0)
‚àó
‚àó
wj> = ar,j a‚àór,i h(r,i)
mhwj , w1,i
i w2,i
(E.2)

where
that

‚àö

(0)

‚àó i has the same distribution with Œ± in Lemma E.2. By Lemma E.2, we have
mhwj , w1,i
1


E

(0)

wj ,ar,j




‚àö

(0)
‚àó
‚àó
ar,j 1hw(0) ,xi‚â•0 hwj> , xi = E a‚àór,i 1hw(0) ,xi‚â•0 h(r,i)
mhwj , w1,i
i hw2,i
, xi
(0)

j

j

wj

‚àó
‚àó
= a‚àór,i œÜr,i (hw1,i
, xi)hw2,i
, xi ¬± Œµ .

Fit a combination

P

i‚àà[p]

‚àó
‚àó
a‚àór,i œÜr,i (hw1,r,i
, xi)hw2,r,i
, xi. We can re-define (the norm grows by

32

a maximum factor of p)
wj> = ar,j

X

a‚àór,i h(r,i)

‚àö


(0)
‚àó
‚àó
mhwj , w1,i
i w2,i

i‚àà[p]

and the same above argument gives

 X
‚àó
‚àó
>
a‚àór,i œÜr,i (hw1,i
, xi)hw2,i
, xi ¬± Œµp.
E
ar,j 1hw(0) ,xi‚â•0 hwj , xi =
(0)

j

wj ,ar,j

i‚àà[p]

Fit multiple outputs. If there are k outputs let us re-define (the norm grows by a maximum
factor of k)
‚àö

X
X
(0)
‚àó
‚àó
mhwj , w1,i
i w2,i
.
(E.3)
wj> =
ar,j
a‚àór,i h(r,i)
r‚àà[k]

i‚àà[p]

and consider the quantity
def

Œûr,j = ar,j hwj> , xi1hw(0) ,xi‚â•0 .
j

r0

By randomness of a we know that for
[Œûr,j ]

E

(0)

6= r, E[ar,j ar0 ,j ] = 0. Thus, for every r ‚àà [k], it satisfies

wj ,a1,j ,...,ak,j

Ô£Æ
=

E

X

(0)
wj ,a1,j ,...,ak,j

Ô£∞

X

ar,j ar0 ,j

r0 ‚àà[k]

= E Ô£∞
(0)
wj

=

i‚àà[p]

X

j

i‚àà[p]

Ô£Æ
X

1hw(0) ,xi‚â•0 a‚àór0 ,i h(r,i)

‚àö

1hw(0) ,xi‚â•0 a‚àór,i h(r,i)
j

‚àö

Ô£π
(0)



‚àó
‚àó
, xiÔ£ª
mhwj , w1,i
i hw2,i

Ô£π


(0)
‚àó
‚àó
mhwj , w1,i
i hw2,i
, xiÔ£ª

‚àó
‚àó
a‚àór,i œÜr,i (hw1,i
, xi))hw2,i
, xi ¬± pŒµ = Œ¶‚àór (x) ¬± pŒµ .

i‚àà[p]

Now, re-scaling each wj> by a factor of
>

Gr (x; W ) =

m
X

1
m

Œûr,j

and re-scaling Œµ by

1
2pk ,

we can write

h
i
Œµ
E Gr (x; W> ) = Œ¶‚àór (x) ¬±
.
2k

and

j=1

Now, we use |h(r,i) | ‚â§ CŒµ (Œ¶, 1) and apply the concentration from Lemma B.3, which implies for our
2
4 2
parameter choice of m, with probability at least 1 ‚àí e‚àí‚Ñ¶(mŒµ /(k p CŒµ (Œ¶,1)))
Œµ
|Gr (x; W> ) ‚àí Œ¶‚àór (x)| ‚â§
.
k
Norm on W> . According to its definition in (E.3), we have for each j ‚àà [m], with high probability

e kpCŒµ (Œ¶,1) (here the additional 1 is because we have re-scaled w> by 1 ). This means
kwj> k2 ‚â§ O
j
m
m
m
kpCŒµ (Œ¶,1) 
>
e
kW k2,‚àû ‚â§ O
. As for the Frobenius norm,
m

kW> k2F =

X

kwj> k22 ‚â§

j‚àà[m]

Now, for each i ‚àà [p], we know that

X
j‚àà[m]

(r,i)
j‚àà[m] h

P

2

e k p) ¬∑
O(
m2
‚àö

X

h(r,i)

‚àö

(0)

‚àó
mhwj , w1,i
i

2

(E.4)

i‚àà[p]

2
(0)
‚àó i
mhwj , w1,i
is a summation of i.i.d. random

variables, each with expectation at most Cs (Œ¶, 1)2 by Lemma E.2. Applying Hoeffding‚Äôs concen33

‚àö

tration, we have with probability at least 1 ‚àí e‚àí‚Ñ¶( m)
‚àö
X
‚àö (0) 2
(0)
‚àó
h(r,i)
‚â§ m ¬∑ Cs (Œ¶, 1)2 + m3/4 ¬∑ CŒµ (Œ¶, 1)2 ‚â§ 2mCs (Œ¶, 1)2
mhwj , w1,i
i, mbj
j‚àà[m]

e k2 p2 Cs (Œ¶,1)2 ). This finishes the proof of Lemma E.1‚Äô.
Putting this back to (E.4) we have kW> k2F ‚â§ O(
m

References
[1] Zeyuan Allen-Zhu and Yuanzhi Li. Can SGD learn recurrent neural networks with provable generalization? In NeurIPS, 2019. Full version available at http://arxiv.org/abs/1902.01028.
[2] Zeyuan Allen-Zhu and Yuanzhi Li. Backward Feature Correction: How Deep Learning Performs Deep
Learning. arXiv preprint, January 2020. Full version available at http://arxiv.org/abs/2001.04413.
[3] Zeyuan Allen-Zhu and Yuanzhi Li. Feature purification: How adversarial training performs robust deep
learning. arXiv preprint arXiv:2005.10190, 2020.
[4] Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and Generalization in Overparameterized
Neural Networks, Going Beyond Two Layers. In NeurIPS, 2019. Full version available at http:
//arxiv.org/abs/1811.04918.
[5] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural
networks. In NeurIPS, 2019. Full version available at http://arxiv.org/abs/1810.12065.
[6] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In ICML, 2019. Full version available at http://arxiv.org/abs/1811.03962.
[7] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On exact
computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019.
[8] Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. CoRR, abs/1901.08584,
2019. URL http://arxiv.org/abs/1901.08584.
[9] Ainesh Bakshi, Rajesh Jayaram, and David P Woodruff. Learning two layer rectified neural networks
in polynomial time. arXiv preprint arXiv:1811.01885, 2018.
[10] Digvijay Boob and Guanghui Lan. Theoretical properties of the global optimizer of two layer neural
network. arXiv preprint arXiv:1710.11241, 2017.
[11] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. arXiv preprint arXiv:1702.07966, 2017.
[12] Amit Daniely. Sgd learns the conjugate kernel class of the network. In Advances in Neural Information
Processing Systems, pages 2422‚Äì2430, 2017.
[13] Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks: The
power of initialization and a dual view on expressivity. In Advances in Neural Information Processing
Systems (NIPS), pages 2253‚Äì2261, 2016.
[14] Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018.
[15] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.
[16] Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. arXiv preprint arXiv:1711.00501, 2017.
[17] Rong Ge, Rohith Kuditipudi, Zhize Li, and Xiang Wang. Learning two-layer neural networks with
symmetric inputs. In International Conference on Learning Representations, 2019.
[18] Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural
networks. In Proceedings of the Conference on Learning Theory, 2018.
[19] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent

34

neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international conference
on, pages 6645‚Äì6649. IEEE, 2013.
[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770‚Äì778, 2016.
[21] Arthur Jacot, Franck Gabriel, and CleÃÅment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, pages 8571‚Äì8580,
2018.
[22] Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information
Processing Systems, pages 586‚Äì594, 2016.
[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional
neural networks. In Advances in neural information processing systems, pages 1097‚Äì1105, 2012.
[24] Yuanzhi Li and Zehao Dou. When can wasserstein gans minimize wasserstein distance? arXiv preprint
arXiv:2003.04033, 2020.
[25] Yuanzhi Li and Yingyu Liang. Provable alternating gradient descent for non-negative matrix factorization with strong correlations. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pages 2062‚Äì2070. JMLR. org, 2017.
[26] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, 2018.
[27] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In
Advances in Neural Information Processing Systems, pages 597‚Äì607, 2017.
[28] Yuanzhi Li, Yingyu Liang, and Andrej Risteski. Recovery guarantee of non-negative matrix factorization
via alternating updates. In Advances in neural information processing systems, pages 4987‚Äì4995, 2016.
[29] Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix
sensing and neural networks with quadratic activations. In COLT, 2018.
[30] Tengyu Ma. CS229T/STAT231: Statistical Learning Theory (Fall 2017). https://web.stanford.edu/
class/cs229t/scribe_notes/10_17_final.pdf, October 2017. accessed May 2019.
[31] Martin J. Wainwright. Basic tail and concentration bounds. https://www.stat.berkeley.edu/
~mjwain/stat210b/Chap2_TailBounds_Jan22_2015.pdf, 2015. Online; accessed Oct 2018.
[32] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In Conference on Learning Theory, pages 1376‚Äì1401, 2015.
[33] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do CIFAR-10 Classifiers
Generalize to CIFAR-10? arXiv preprint arXiv:1806.00451, 2018.
[34] Mark Rudelson and Roman Vershynin. Non-asymptotic theory of random matrices: extreme singular
values. In Proceedings of the International Congress of Mathematicians 2010 (ICM 2010) (In 4 Volumes) Vol. I: Plenary Lectures and Ceremonies Vols. II‚ÄìIV: Invited Lectures, pages 1576‚Äì1602. World
Scientific, 2010.
[35] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the
game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
[36] Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926, 2017.
[37] Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for
multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
[38] Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.
[39] Santosh Vempala and John Wilmes. Polynomial convergence of gradient descent for training one-hiddenlayer neural networks. arXiv preprint arXiv:1805.02677, 2018.
[40] Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. On the margin theory of feedforward neural
networks. arXiv preprint arXiv:1810.05369, 2018.

35

[41] Bo Xie, Yingyu Liang, and Le Song. Diversity leads to generalization in neural networks. arXiv preprint
Arxiv:1611.03131, 2016.
[42] Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760, 2019.
[43] Yuchen Zhang, Jason D Lee, and Michael I Jordan. l1-regularized neural networks are improperly
learnable in polynomial time. In International Conference on Machine Learning, pages 993‚Äì1001, 2016.
[44] Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for
one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.
[45] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes overparameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.

36

