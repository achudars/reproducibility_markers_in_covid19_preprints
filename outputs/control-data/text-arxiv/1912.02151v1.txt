High Dimensional Latent Panel Quantile Regression

arXiv:1912.02151v1 [econ.EM] 4 Dec 2019

with an Application to Asset Pricing ∗
Alexandre Belloni1 , Mingli Chen2 , Oscar Hernan Madrid Padilla3 , and Zixuan (Kevin) Wang4
1

Fuqua Business School, Duke University
Department of Economics, University of Warwick
3
Department of Statistics, University of California, Los Angeles
4
Harvard University
2

December 5, 2019

Abstract

We propose a generalization of the linear panel quantile regression model to accommodate
both sparse and dense parts: sparse means while the number of covariates available is large,
potentially only a much smaller number of them have a nonzero impact on each conditional
quantile of the response variable; while the dense part is represented by a low-rank matrix that
can be approximated by latent factors and their loadings. Such a structure poses problems for
traditional sparse estimators, such as the `1 -penalised Quantile Regression, and for traditional
latent factor estimator, such as PCA. We propose a new estimation procedure, based on the
ADMM algorithm, that consists of combining the quantile loss function with `1 and nuclear
norm regularization. We show, under general conditions, that our estimator can consistently
estimate both the nonzero coefficients of the covariates and the latent low-rank matrix.
Our proposed model has a “Characteristics + Latent Factors” Asset Pricing Model interpretation: we apply our model and estimator with a large-dimensional panel of financial data and
find that (i) characteristics have sparser predictive power once latent factors were controlled (ii)
the factors and coefficients at upper and lower quantiles are different from the median.
Keywords: High-dimensional quantile regression; factor model; nuclear norm regularization; panel data; asset pricing; characteristic-based model
∗

We are grateful to conversations with Victor Chernozhukov, Iván Fernández-Val, Bryan Graham, Hiroaki Kaido,
Anna Mikusheva, Whitney Newey, and Vasilis Syrgkanis.

1

1

Introduction

A central question in asset pricing is to explain the returns of stocks. According to the Arbitrage Pricing
Theory, when the asset returns are generated by a linear factor model, there exists a stochastic discount factor
linear in the factors that prices the returns (Ross (1976), Cochrane (2009)). However, from an empirical
perspective, testing the theory is difficult because the factors are not directly observed.
To overcome this challenge, one approach uses characteristic-sorted portfolio returns to mimic the unknown factors. This approach for understanding expected returns can be captured by the following panel
data model (Cochrane (2011)):
0
E(Yi,t |Xi,t−1 ) = Xi,t−1
θ.
(1)
The drawback of this approach is that it requires a list of pre-specified characteristics which are chosen
based on empirical experience and thus somewhat arbitrarily (Fama and French (1993)). In addition, the literature has documented a zoo of new characteristics, and the proliferation of characteristics in this “variable
zoo” leads to the following questions: “which characteristics really provide independent information about
average returns, and which are subsumed by others?” (Cochrane (2011)). Another approach uses statistical factor analysis, e.g. Principal Component Analysis (PCA), to extract latent factors from asset returns
(Chamberlain and Rothschild (1983), Connor and Korajczyk (1988)):
E(Yi,t ) = λ0i gt .

(2)

However, one main critique with this approach is that the latent factors estimated via PCA are purely statistical factors, thus lacks economic insight Campbell (2017).
We extend both modeling approaches and propose a “Characteristics + Latent Factors” framework. By
incorporating the “Characteristics” documented, we improve the economic interpretability and explanatory
power of the model. The characteristics can have a sparse structure, meaning although a large set of variables
is available, only a much smaller subset of them might have predictive power. We also incorporate “Latent
Factors”, one benefit of having this part is that it might help alleviate the “omitted variable bias” problem
(Giglio and Xiu (2018)). As in the literature typically these latent factors are estimated via PCA, which
means all possible latent explanatory variables (those are not included in the model) might be important for
prediction although their individual contribution might be small, we term this as the dense part. 1
Hence, our framework allows for “Sparse + Dense” modeling with the time series and cross-section of
asset returns. In addition, we focus on understanding the quantiles (hence the entire distribution) of returns
rather than just the mean, in line with recent interest in quantile factor models (e.g. Ando and Bai (2019),
Chen et al. (2018), Ma et al. (2019), Feng (2019), and Sagner (2019)). Specifically, we study the following
high dimensional latent panel quantile regression model with Y ∈ Rn×T and X ∈ Rn×T ×p satisfying
0
FY−1
(τ ) = Xi,t
θ(τ ) + λi (τ )0 gt (τ ),
i,t |Xi,t ;θ(τ ),λi (τ ),gt (τ )
1

i = 1 . . . , n, t = 1, . . . , T,

(3)

More about sparse modeling and dense modeling can be found in Giannone et al. (2017). See also Chernozhukov et al. (2017).

2

where i denotes subjects (or assets in our asset pricing setting), t denotes time, θ(τ ) ∈ Rp is the vector of
coefficients, λi (τ ), gt (τ ) ∈ Rrτ with rτ  min{n, T } (denote Πi,t (τ ) = λi (τ )0 gt (τ ), then Π(τ ) ∈ Rn×T
is a low-rank matrix with unknown rank rτ ), τ ∈ [0, 1] is the quantile index, and FYi,t |Xi,t ;θ(τ ),λi (τ ),gt (τ ) (or
FYi,t |Xi,t ;θ(τ ),Πi,t (τ ) ) is the cumulative distribution function of Yi,t conditioning on Xi,t , θ(τ ) and λi (τ ), gt (τ )
(or Πi,t (τ )). Our framework also allows for the possibility of lagged dependent data. Thus, we model the
quantile function at level τ as a linear combination of the predictors plus a low-rank matrix (or a factor
structure). Here, we allow for the number of covariates p, and the time horizon T , to grow to infinity as n
grows. Throughout the paper we mainly focus on the case where p is large, possibly much larger than nT ,
but for the true model θ(τ ) is sparse and has only sτ  p non-zero components.
Our framework is flexible enough that allows us to jointly answer the following three questions in
asset pricing: (i) Which characteristics are important to explain the time series and cross-section of stock
returns, after controlling for the factors? (ii) How much would the latent factors explain stock returns
after controlling for firm characteristics? (iii) Does the relationship of stock returns and firm characteristics
change across quantiles? The first question is related to the recent literature on variable selection in asset
pricing using machine learning (Kozak et al. (2019); Feng et al. (2019); Han et al. (2018)). The second
question is related to an classical literature starting from 1980s on statistical factor models of stock returns
(Chamberlain and Rothschild (1983); Connor and Korajczyk (1988) and recently Lettau and Pelger (2018)).
The third question extends the literature in late 1990s on stock return and firm characteristics (Daniel and
Titman (1997, 1998)) and further asks whether the relationship is heterogenous across quantiles.
There are several key features of considering prediction problem at the panel quantile model in this
setting. First, stock returns are known to be asymmetric and exhibits heavy tail, thus modeling different
quantiles of return provides extra information in addition to models of first and second moments. Second,
quantile regression provides a richer characterization of the data, allowing heterogeneous relationship between stock returns and firm characteristics across the entire return distribution. Third, the latent factors
might also be different at different quantiles of stock returns. Finally, quantile regression is more robust
to the presence of outliers relative to other widely used mean-based approaches. Using a robust method is
crucial when estimating low-rank structures (see e.g. She and Chen (2017)). As our framework is based on
modeling the quantiles of the response variable, we do not put assumptions directly on the moments of the
dependent variable.
Our main goal is to consistently estimate both the sparse part and the low-rank matrix. Recovery of a
low-rank matrix, when there are additional high dimensional covariates, in a nonlinear model can be very
challenging. The rank constraint will result in the optimization problem NP-hard. In addition, estimation in
high dimensional regression is known to be a challenging task, which in our frameworks becomes even more
difficult due to the additional latent structure. We address the former challenge via nuclear norm regularization which is similar to Candès and Recht (2009) in the matrix completion setting. Without covariates, the
estimation can be done via solving a convex problem, and similarly there are strong statistical guarantees of

3

recovery of the underlying low-rank structure. We address the latter challenge by imposing `1 regularization
on the vector of coefficients of the control variables, similarly to Belloni and Chernozhukov (2011) which
mainly focused on the cross-sectional data setting. Note that with regards to sparsity, we must be cautious,
specially when considering predictive models (She and Chen (2017)). Furthermore, we explore the performance of our procedure under settings where the vector of coefficients can be dense (due to the low-rank
matrix).
We also propose a novel Alternating Direction Method of Multipliers (ADMM) algorithm (Boyd et al.
(2011)) that allows us to estimate, at different quantile levels, both the vector of coefficients and the low-rank
matrix. Our proposed algorithm can easily be adjusted to other nonlinear models with a low-rank matrix
(with or without covariates).
We view our work as complementary to the low dimensional quantile regression with interactive fixed
effects framework as of the recent work of Feng (2019), and the mean estimation setting in Moon and Weidner (2018). However, unlike Moon and Weidner (2018) and Feng (2019), we allow the number of covariates
p to be large, perhaps p  nT . This comes with different significant challenges. On the computational
side, it requires us to develop novel estimation algorithms, which turns out can also be used for the contexts
in Moon and Weidner (2018) and Feng (2019). On the theoretical side, allowing p  nT requires a systematically different analysis as compared to Feng (2019), as it is known that ordinary quantile regression
is inconsistent in high dimensional settings (p  nT ), see Belloni and Chernozhukov (2011).

Related Literature. Our work contributes to the recent growing literature on panel quantile model. Abrevaya and Dahl (2008), Graham et al. (2018), Arellano and Bonhomme (2017), considered the fixed T asymptotic case. Kato et al. (2012) formally derived the asymptotic properties of the fixed effect quantile regression
estimator under large T asymptotics, and Galvao and Kato (2016) further proposed fixed effects smoothed
quantile regression estimator. Galvao (2011) works on dynamic panel. Koenker (2004) proposed a penalized estimation method where the individual effects are treated as pure location shift parameters common to
all quantiles, for other related literature see Lamarche (2010), Galvao and Montes-Rojas (2010). We refer
to Chapter 19 of Koenker et al. (2017) for a review.
Our work also contributes to the literature on nuclear norm penalisation, which has been widely studied
in the machine learning and statistical learning literature, Fazel (2002), Recht et al. (2010); Koltchinskii et al.
(2011); Rohde and Tsybakov (2011), Negahban and Wainwright (2011), Brahma et al. (2017). Recently, in
the econometrics literature Athey et al. (2018) proposes a framework of matrix completion for estimating
causal effects, Bai and Ng (2017) for estimating approximate factor model, Chernozhukov et al. (2018)
considered the heterogeneous coefficients version of the linear panel data interactive fixed model where
the main coefficients has a latent low-rank structure, Bai and Feng (2019) for robust principal component
analysis, and Bai and Ng (2019) for imputing counterfactual outcome.

4

Finally, our results contribute to a growing literature on high dimensional quantile regression. Wang et al.
(2012) considered quantile regression with concave penalties for ultra-high dimensional data; Zheng et al.
(2015) proposed an adaptively weighted `1 -penalty for globally concerned quantile regression. Screening
procedures based on moment conditions motivated by the quantile models have been proposed and analyzed
in He et al. (2013) and Wu and Yin (2015) in the high-dimensional regression setting. We refer to Koenker
et al. (2017) for a review.
To sum-up, our paper makes the following contributions. First, we propose a new class models that
consist of both high dimensional regressors and latent factor structures. We provide a scalable estimation
procedure, and show that the resulting estimator is consistent under suitable regularity conditions. Second,
the high dimensional and non-smooth objective function require innovative strategies to derive all the abovementioned results. This leads to the use in our proofs of some novel techniques from high dimensional
statistics/econometrics, spectral theory, empirical process, etc. Third, the proposed estimators inherit from
quantile regression certain robustness properties to the presence of outliers and heavy-tailed distributions in
the idiosyncratic component of a factor model. Finally, we apply our proposed model and estimator to a
large-dimensional panel of financial data in the US stock market and find that different return quantiles have
different selected firm characteristics and that the number of latent factors can be also be different.

Outline. The rest of the paper is organized as follows. Section 2 introduces the high dimensional latent
quantile regression model, and provides an overview of the main theoretical results. Section 3 presents the
estimator and our proposed ADMM algorithm. Section 4 discusses the statistical properties of the proposed
estimator. Section 5 provides simulation results. Section 6 consists of the empirical results of our model
applied to a real data set. The proofs of the main results are in the Appendix.

Notation. For m ∈ N, we write [m] = {1, . . . , m}. For a vector v ∈ Rp we define its `0 norm as
P
kvk0 = pj=1 1{vj 6= 0}, where 1{·} takes value 1 if the statement inside {} is true, and zero otherwise;
P
P
its `1 norm as kvk1 = pj=1 |vj |. We denote kvk1,n,T = pj σ̂j |vj | the `1 -norm weighted by σ̂j ’s (details
qP
p
n×T
2
can be found in eq (20)). The Euclidean norm is denoted by k · k, thus kvk =
j=1 vj . If A ∈ R
qP P
n
T
2
is a matrix, its Frobenius norm is denoted by kAkF =
i=1
t=1 Ai,t , its spectral norm by kAk2 =
√
supx : kxk=1 x0 A0 Ax, its infinity norm by kAk∞ = max{|Ai,j | : i ∈ [n], j ∈ [T ]} , its rank by rank(A),
√
and its nuclear norm by kAk∗ = trace( A0 A) where A0 is the transpose of A. The jth column A is denoted
by A·,j . Furthermore, the multiplication of a tensor X ∈ RI1 ×...×Im with a vector a ∈ RIm is denoted by
Pm
Z := Xa ∈ RI1 ×...×Im−1 , and, explicitly, Zi1 ,...im−1 = Ij=1
Xi1 ,...,im−1 ,j aj . We also use the notation
a ∨ b = max{a, b}, a ∧ b = min{a, b}, (a)− = max{−a, 0}. For a sequence of random variables {zj }∞
j=1
∞ and
we denote by σ(z1 , z2 , . . .) the sigma algebra generated by {zj }∞
.
Finally,
for
sequences
{a
}
n
n=1
j=1
{bn }∞
we
write
a

b
if
there
exists
positive
constants
c
and
c
such
that
c
b
≤
a
≤
c
n
n
1
2
1
n
n
2 bn for
n=1
sufficiently large n.
5

2

The Estimator and Overview of Rate Results

2.1

Basic Setting

The setting of interest corresponds to a high dimension latent panel quantile regression model, where Y ∈
Rn×T , and X ∈ Rn×T ×p satisfying
0
FY−1
(τ ) = Xi,t
θ(τ ) + Πi,t (τ ),
i,t |Xi,t ;θ(τ ),Πi,t (τ )

i = 1 . . . , n, t = 1, . . . , T,

(4)

where i denotes subjects, t denotes time, θ(τ ) ∈ Rp is the vector of coefficients, Π(τ ) ∈ Rn×T is a low-rank
matrix with unknown rank rτ  min{n, T }, τ ∈ [0, 1] is the quantile index, and FYi,t |Xi,t ;θ(τ ),Πi,t (τ ) is the
cumulative distribution function of Yi,t conditioning on Xi,t , θ(τ ) and Πi,t (τ ). Thus, we model the quantile
function at level τ as a linear combination of the predictors plus a low-rank matrix. Here, we allow for
the number of covariates p, and the time horizon T , to grow to infinity as n grows. Throughout the paper
the quantile index τ ∈ (0, 1) is fixed. We mainly focus on the case where p is large, possibly much larger
than nT , but for the true model θ(τ ) is sparse and has only sτ  p non-zero components. Mathematically,
sτ := kθ(τ )k0 .
When Πi,t (τ ) = λi (τ )0 gt (τ ), with λi (τ ), gt (τ ) ∈ Rrτ , this immediately leads to the following setting
0
FY−1
(τ ) = Xi,t
θ(τ ) + λi (τ )0 gt (τ ).
i,t |Xi,t ;θ(τ ),Πi,t (τ )

(5)

where we model the quantile function at level τ as a linear combination of the covariates (as predictors)
plus a latent factor structure. This is directly related to the panel data models with interactive fixed effects
literature in econometrics, e.g. linear panel data model (Bai (2009)), nonlinear panel data models (Chen
(2014); Chen et al. (2014)).
Note, for eq (5), additional identification restrictions are needed for estimating λi (τ ) and gt (τ ) (see Bai
and Ng (2013)). In addition, in nonlinear panel data models, this create additional difficulties in estimation,
as the latent factors and their loadings part are nonconvex. 2

2.2

The Penalized Estimator, and its Convex Relaxation

In this subsection, we describe the high dimensional latent quantile estimator. With the sparsity and low-rank
constraints in mind, a natural formulation for the estimation of (θ(τ ), Π(τ )) is
minimize
θ̃∈Rp , Π̃∈Rn×T

subject to

T
n
1 XX
0
ρτ (Yi,t − Xi,t
θ̃ − Π̃i,t )
nT
t=1 i=1

rank(Π̃) ≤ rτ ,
kθ̃k0 = sτ ,

2

(6)

Different identification conditions might result in different estimation procedures for λ and f , see Bai and Li (2012) and Chen
(2014).

6

where ρτ (t) = (τ − 1{t ≤ 0})t is the quantile loss function as in Koenker (2005), sτ is a parameter that
directly controls the sparsity of θ̃, and rτ controls the rank of the estimated latent matrix.
While the formulation in (6) seems appealing, as it enforces variable selection and low-rank matrix
estimation simultaneously, (6) is a non-convex problem due to the constraints posed by the k · k0 and rank(·)
functions. We propose a convex relaxation of (6). Inspired by the seminal works of Tibshirani (1996) and
Candès and Recht (2009), we formulate the problem
min
θ̃∈Rp , Π̃∈Rn×T

subject to

T
n
1 XX
0
ρτ (Yi,t − Xi,t
θ̃ − Π̃i,t )
nT
t=1 i=1

kΠ̃k∗ ≤ ν2 ,
p
X
wj |θ̃j | ≤ ν1 ,

(7)

j=1

where ν1 > 0 and ν2 > 0 are tuning parameters, and w1 , . . . , wp are user specified weights (more on this in
Section 4 ).
In principle, one can use any convex solver software to solve (7), since this is a convex optimization
problem. However, for large scale problems a more careful implementation might be needed. Section 3.1
presents a scheme for solving (7) that is based on the ADMM algorithm ( (Boyd et al., 2011)).

2.3

Summary of results

We now summarize our main results. For the model defined in (4):
• Under (4), sτ  min{n, T }, an assumption that implicitly requires rτ  min{n, T }, and other
regularity conditions defined in Section 4, we show that our estimator (θ̂(τ ), Π̂(τ )) defined in Section
3 is consistent for (θ(τ ), Π(τ )). Specifically, for the independent data case (across i and t), under
suitable regularity conditions that can be found in Section 4, we have



p
p
√
√
1
1
sτ max{ log p, log n, rτ } √ + √
.
(8)
kθ̂(τ ) − θ(τ )k = OP
n
T
and




1
1
1
2
kΠ̂(τ ) − Π(τ )kF = OP sτ max{log p, log n, rτ }
+
,
nT
n T

(9)

Importantly, the rates in (8) and (9), ignoring logarithmic factors, match those in previous works.
However, our setting allows for modeling at different quantile levels. We also complement our results
by allowing for the possibility of lagged dependent data. Specifically, under a β-mixing assumption,
Theorem 1 provides a statistical guarantee for estimating (θ(τ ), Π(τ )). This result can be thought as
a generalization of the statements in (8) and (9).
7

• An important aspect of our analysis is that we contrast the performance of our estimator in settings
where the possibility of a dense θ(τ ) provided that the features are highly correlated. We show that
there exist choices of the tuning parameters for our estimator that lead to consistent estimation.
• For estimation, we provide an efficient algorithm (details can be found in Section 3.1), which is based
on the ADMM algorithm (Boyd et al. (2011)).
• Section 6 provides thorough examples on financial data that illustrate the flexibility and interpretability
of our approach.
Although our theoretical analysis builds on the work by Belloni and Chernozhukov (2011), there are
multiple challenges that we must face in order to prove the consistency of our estimator. First, the construction of the restricted set now involves the nuclear norm penalty. This requires us to define a new restricted
set that captures the contributions of the low-rank matrix. Second, when bounding the empirical processes
that naturally arise in our proof, we have to simultaneously deal with the sparse and dense components.
Furthermore, throughout our proofs, we have to carefully handle the weak dependence assumption that can
be found in Section 4.

3
3.1

High Dimensional Latent Panel Quantile Regression
Estimation with High Dimensional Covariates

In this subsection, we describe the main steps of our proposed ADMM algorithm, details can be found
in Section A. We start by introducing slack variables to the original problem (7). As a result, a problem
equivalent to (7) is
min
θ̃,Π̃,V

Zθ ,ZΠ ,W

p
n T
X
1 XX
ρτ (Vi,t ) + ν1
wj |Zθj | + ν2 kΠ̃k∗
nT
i=1 t=1

j=1

(10)

subject to V = W, W = Y − X θ̃ − ZΠ ,
ZΠ − Π̃ = 0, Zθ − θ̃ = 0.
To solve (10), we propose a scaled version of the ADMM algorithm which relies on the following

8

Augmented Lagrangian
L(θ̃, Π̃, V Zθ , ZΠ , W, UV , UW , UΠ , Uθ ) =

p
n T
X
1 XX
ρτ (Vi,t ) + ν1
wj |Zθj | + ν2 kΠ̃k∗
nT
i=1 t=1

j=1

η
η
+ kV − W + UV k2F + kW − Y + Xθ + ZΠ + UW k2F
2
2
η
η
+ kZΠ − Π̃ + UΠ k2F + kZθ − θ̃ + Uθ k2F ,
2
2
(11)
where η > 0 is a penalty parameter.
Notice that in (11), we have followed the usual construction of ADMM via introducing the scaled dual
variables corresponding to the constraints in (10) – those are UV , UW , UΠ , and Uθ . Next, recall that
ADMM proceeds by iteratively minimizing the Augmented Lagrangian in blocks with respected to the
original variables, in our case (V, θ̃, Π̃) and (W, Zθ , ZΠ ), and then updating the scaled dual variables (see
Equations 3.5–3.7 in Boyd et al. (2011)). The explicit updates can be found in the Appendix. Here, we
highlight the updates for Zθ , Π̃, and V . For updating Zθ at iteration k + 1, we solve the problem


p

1
X
ν
(k)
(k+1)
1
kZθ − θ̃(k+1) + Uθ k2F +
wj |(Zθ )j | .
Zθ
← arg min

η
Zθ ∈Rp  2
j=1

This can be solved in closed form exploiting the well known thresholding operator, see the details in Section
B.2. As for updating Π̃, we solve

Π̃(k+1) ← arg min
Π̃∈Rn×T



1 (k)
ν2
(k)
kΠ̃k∗ + kZΠ − Π̃ + UΠ k2F
η
2


,

(12)

via the singular value shrinkage operator, see Theorem 2.1 in Cai et al. (2010).
Furthermore, we update V , at iteration k + 1, via
)
(
n X
T
X
1
η
(k)
ρτ (Vi,t ) + kV − W (k) + UV k2F ,
V (k+1) ← arg min
nT
2
V ∈Rn×T

(13)

i=1 t=1

which can be found in closed formula by Lemma 5.1 from Ali et al. (2016).
Remark 1. After estimating Π(τ ), we can estimate λi (τ ) and gt (τ ) via the singular value decomposition
of Π̂(τ ) and following equation
Π̂(τ )i,t = λ̂i (τ )0 ĝt (τ ),
(14)
where λ̂i (τ ) and ĝt (τ ) are of dimension r̂τ . This immediately leads to factors and loadings estimated that
can be used to obtain insights about the structure of the data. A formal identification statement is given in
Corollary 2.

9

3.2

Estimation without Covariates

Note, when there are no covariates, our proposed ADMM can be simplified. In this case, we face the
following problem


n X
T
 1 X

min
ρτ (Yi,t − Π̃i,t ) + ν2 kΠ̃k∗ .
(15)

Π̃∈Rn×T  nT
i=1 j=1

This can be thought as a convex relaxation of the estimator studied in Chen et al. (2018). Problem (15) is
also related to the setting of robust estimation of a latent low-rank matrix, e.g. Elsener and van de Geer
(2018). However, our approach can also be used to estimate different quantile levels. As for solving (15),
we can proceed by doing the iterative updates
(
)
n T
1 XX
η
(k)
(k) 2
(k+1)
Π̃
← arg min
ρτ (Yi,t − Π̃i,t ) + kΠ̃ − ZΠ + UΠ kF ,
(16)
nT
2
Π̃
i=1 t=1

(k+1)

ZΠ

← arg min
ZΠ

nη
2

o
(k)
kΠ̃(k+1) − ZΠ + UΠ k2F + ν2 kZΠ k∗ ,

(17)

and
(k+1)

UΠ

(k+1)

← Π(k+1) − ZΠ

(k)

+ UΠ ,

(18)

where η > 0 is the penalty parameter (Boyd et al. (2011)). The minimization in (16) is similar to (13),
whereas (17) can be done similarly as in (12).
Although our proposed estimation procedure can be applied to settings (i) with low dimensional covariates, or (ii) without covariates, in what follows, we focus on the high dimensional covariates setting.

4

Theory

The purpose of this section is to provide statistical guarantees for the estimator developed in the previous
section. We focus on estimating the quantile function, allowing for the high dimensional scenario where
p and T can grow as n grows. Our analysis combines tools from high dimensional quantile regression
theory (e.g. Belloni and Chernozhukov (2011)), spectral theory (e.g. Vu (2007) and Chatterjee (2015)), and
empirical process theory (e.g. Yu (1994) and van der Vaart and Wellner (1996)).

4.1

Main Result

We show that our proposed estimator is consistent in a broad range of models, and in some cases attains
minimax rates, as in Candès and Plan (2011).

10

Before arriving at our main theorem, we start by stating some modeling assumptions. For a fixed τ > 0,
we assume that (4) holds. We also let Tτ be the support of θ(τ ), thus
Tτ = {j ∈ [p] : θj (τ ) 6= 0} ,
and we write sτ = |Tτ |, and rτ = rank(Π(τ )).
Throughout, we treat Π(τ ) as parameters. As for the data generation process, our next condition requires
that the observations are independent across i, and weakly dependent across time.
Assumption 1. The following holds:
(i) Conditional on Π, {(Yi,t , Xi,t }t=1,...,T is independent across i. Also, for each i ∈ [n], the sequence
{(Yi,t , Xi,t )}t=1,...,T is stationary and β-mixing with mixing coefficients satisfying supi γi (k) =
O(k −µ ) for some µ > 2. Moreover, there exists µ0 ∈ (0, µ), such that
k−µ
j
0
npT T 1/(1+µ )
→ 0.
(19)
Here, γi (k) =

1
2

sup

PL

l≥1

j=1

PL0

j 0 =1 |P(Aj

∩ Bj 0 ) − P(Aj )P(Bj 0 )| with {Aj }L
j=1 is a partition of
0

σ({Xi,1 , Yi,1 }, . . . , {Xi,l , Yi,l }), and {Bj 0 }L
j 0 =1 partition of σ({Xi,l+k , Yi,l+k }, {Xi,l+k+1 , Yi,l+k+1 } . . .).
(ii) There exists f > 0 satisfying
inf

1≤i≤n, 1≤t≤T, x∈X ,

fYi,t |Xi,t ;θ(τ ),Πi,t (τ ) (x0 θ(τ ) + Πi,t (τ )|x; θ(τ ), Πi,t (τ )) > f ,

where fYi,t |Xi,t ;θ,Πi,t is the probability density function associated with Yi,t when conditioning on
Xi,t , and with parameters θ(τ ) and Πi,t (τ ). Furthermore, fYi,t |Xi,t ;θ(τ ),Πi,t (τ ) (y|x; θ(τ ), Πi,t (τ )) and
∂
¯
¯0
∂y fYi,t |Xi,t ;θ(τ ),Πi,t (τ ) (y|x; θ(τ ), Πi,t (τ )) are both bounded by f and f , respectively, uniformly in y
and x in the support of Xi,t .
Note that Assumption 1 is a generalization of the sampling and smoothness assumption of Belloni and
Chernozhukov (2011). Furthermore, we highlight that similar to Belloni and Chernozhukov (2011), our
framework is rich enough that avoids imposing Gaussian or homoscedastic modeling constraints. However,
unlike Belloni and Chernozhukov (2011), we consider panel data with weak correlation across time. In
particular, we refer readers to Yu (1994) for thorough discussions on β-mixing.
It is worth mentioning that the parameter µ in Assumption 1 controls the strength of the time dependence
in the data. In the case that {(Yi,t , Xi,t )}i∈[n],t∈[T ] are independent our theoretical results will hold without
imposing (19).
Next, we require that along each dimension the second moment of the covariates is one. We also assume
that the second moments can be reasonably well estimated by their empirical counterparts.
11

2 ) = 1 for all i ∈ [n], t ∈ [T ], j ∈ [p]. Then
Assumption 2. We assume E(Xi,t,j

σ̂j2 =

n T
1 XX 2
Xi,t,j , ∀j ∈ [p],
nT

(20)

i=1 t=1

and we require that

P

max

1≤j≤p

|σ̂j2

1
− 1| ≤
4


≥ 1−γ

→ 1,

as

n → ∞.

Assumption 2 appeared as Condition D.3 in Belloni and Chernozhukov (2011). It is met by general
models on the covariates, see for instance Design 2 in Belloni and Chernozhukov (2011).
Using the empirical second order moments {σ̂j2 }pj=1 , we analyze the performance of the estimator
n
o
(21)
(θ̂(τ ), Π̂(τ )) = arg min Q̂τ (θ̃, Π̃) + ν1 kθ̃k1,n,T + ν2 kΠ̃k∗ ,
(θ̃,Π̃)

where ν2 > 0 is a tuning parameter, kθ̃k1,n,T :=

Pp

j=1 σ̂j |θ̃j |,

and

n T
1 XX
0
Q̂τ (θ̃, Π̃) =
ρτ (Yi,t − Xi,t
θ̃ − Π̃i,t ),
nT
i=1 t=1

with ρτ as defined in Section 2.2.
As it can been seen in Lemma 7 from Appendix B.1, (θ̂(τ ) − θ(τ ), Π̂(τ ) − Π(τ )) belongs to a restricted
set, which in our framework is defined as
(

)
√
r
k∆k
k∆k
F
∗
Aτ = (δ, ∆) ∈ Rp × Rn×T : kδTτc k1 + √ √
≤ C0 kδTτ k1 + √ √ τ
,
nT

log(max{n,pcT })

nT

log(max{n,pcT })

(22)
for an appropriate positive constant C0 .
Similar in spirit to other high dimensional settings such as those in Candès and Tao (2007), Bickel et al.
(2009), Belloni and Chernozhukov (2011) and Dalalyan et al. (2017), we impose an identifiability condition
involving the restricted set which is expressed next and will be used in order to attain our main results.
Before that, we introduce some notation.
For m ≥ 0, we denote by T τ (δ, m) ⊂ {1, . . . , p}\Tτ the support ot the m largest components, excluding
entries in Tτ , of the vector (|δ1 |, . . . , |δp |)T . We also use the convention T τ (δ, 0) = ∅.
Assumption 3. For (δ, ∆) ∈ Aτ , let
v
u

n X
T
2 
u f X
1/2
t
0
Jτ (δ, ∆) :=
.
E Xi,t δ + ∆i,t
nT
i=1 t=1

12

Then there exists m ≥ 0 such that
1/2

0 < κm :=

Jτ (δ, ∆)

inf
(δ,∆)∈Aτ ,δ6=0

kδTτ ∪T τ (δ,m) k +

k∆kF
√
nT

,

(23)

0

where cT = dT 1/(1+µ ) e for µ0 as defined in Assumption 2. Moreover, we assume that the following holds
  P P
3/2
n
T
1
0 δ + ∆ )2
3/2
E
(X
i,t
i,t
i=1
t=1
nT
3f
 P P
 ,
inf
0 < q :=
(24)
0
n
T
1
8 f (δ,∆)∈Aτ ,δ6=0
E
|X 0 δ + ∆ |3
nT

i=1

t=1

i,t

i,t

0

with f and f as in Assumption 1.
Few comments are in order. First, if ∆ = 0 then (23) and (24) become the restricted identifiability and
nonlinearity conditions as of Belloni and Chernozhukov (2011). Second, the denominator of (23) contains
√
the term k∆kF /( nT ). To see why this is reasonable, consider the case where E(Xi,t ) = 0, and Xi,t are
i.i.d.. Then
f
Jτ (δ, ∆) = f E((δ 0 Xi,t )2 ) +
k∆k2F .
nT
√
Hence, k∆kF /( nT ) appears also in the numerator of (23) and its presence in the denominator of (23) is
not restrictive.
We now state our result for estimating θ(τ ) and Π(τ ).
Theorem 1. Suppose that Assumptions 1-3 hold and that
p
√
√
φn cT (1 + sτ ) log p( n + dT )
,
(25)
q ≥C
√
ndT κ0 f 1/2
p
for a large enough constant C, and {φn } is a sequence with φn /( f log(cT + 1)) → ∞. Then
p p
!

φn 1 + smτ
cT (1 + sτ ) max{log(pcT ∨ n), rτ }
1
1
√ +√
kθ̂(τ ) − θ(τ )k = OP
, (26)
κm
n
dT
κ0 f 1/2
and
1
kΠ̂(τ ) − Π(τ )k2F = OP
nT

φ2n cT (1 + sτ ) max{log(pcT ∨ n), rτ }
κ40 f

for choices of the tuning parameters satisfying
s
p
cT log(max{n, pcT }) √
ν1 
( n + dT ),
ndT
and
ν2 

p 
cT √
n + dT ,
nT

0

where cT = dT 1/(1+µ ) e, dT = bT /(2cT )c.
13



1
1
+
n dT

!
,

(27)

Theorem 1 gives an upper bound on the performance of (θ̂(τ ), Π̂(τ )) for estimating the vector of coefficients θ(τ ) and the latent matrix Π(τ ). For simplicity, consider the case of i.i.d data. Then the convergence
√ √
√
rate of our estimation of θ(τ ), under the Euclidean norm, is in the order of sτ rτ / min{ n, T }, if
we ignore all the other factors. Hence, we can consistently estimate θ(τ ) provided that max{sτ , rτ } <<
min{n, T }. This is similar to the low-rank condition in Negahban and Wainwright (2011). In the low di√ √
√
mensional case sτ = O(1), the rate rτ / min{ n, T } matches that of Theorem 1 in Moon and Weidner
(2018). However, we mainly focus on a loss function that is robust to outliers, and our assumptions also
allow for weak dependence across time. Furthermore, the same applies to our rate on the mean squared error
for estimating Π(τ ), which also matches that in Theorem 1 of Moon and Weidner (2018).
Interestingly, it is expected that the rate in Theorem 1 is optimal. To elaborate on this point, consider the
simple case where n = T , θ = 0, τ = 0.5, and ei,t := Yi,t − Πi,t (τ ) are mean zero i.i.d. sub-Gaussian(σ 2 ).
The latter implies that


z2
P(|e1,1 | > z) ≤ C1 exp − 2 ,
2σ
for a positive constant C1 , and for all z > 0. Then by Theorem 2.3 in Candès and Plan (2011), we have the
following lower bound for estimating Π(τ ):
!
kΠ̂(τ ) − Π(τ )k2F
rτ σ 2
E
≥
.
(28)
inf
sup
nT
n
Π̂ Π(τ ) : rank(Π(τ ))≤rτ
Notably, the lower bound in (28) matches the rate implied by Theorem 1, ignoring other factors depending
on sτ , κ0 , κm , p and φn . However, we highlight that the upper bound (27) in Theorem 1 holds without the
perhaps restrictive condition that the errors are sub-Gaussian.
We conclude this section with a result regarding the estimation of the factors and loadings of the latent
matrix Π(τ ). This is expressed in Corollary 2 below and is immediate consequence of Theorem 1 and
Theorem 3 in Yu et al. (2014).
Corollary 2. Suppose that the all the conditions of Theorem 1 hold. Let σ1 (τ ) ≥ σ2 (τ ) ≥ . . . ≥ σrτ (τ ) >
0 be the singular values of Π(τ ), and σ̂1 (τ ) ≥ . . . ≥ σ̂min{n,T } (τ ) the singular values of Π̂(τ ). Let
˜
g(τ ), ĝ(τ ) ∈ RT ×rτ and λ(τ ), λ̂(τ ), λ̃(τ ), λ̂(τ ) ∈ Rn×rτ be matrices with orthonormal columns satisfying
Π(τ ) =

rτ
X

σj λ̃·,j (τ )g·,j (τ )0 =

j=1

rτ
X

λ·,j (τ )g·,j (τ )0 ,

j=1

˜
and Π̂(τ )ĝ·,j (τ ) = σ̂j (τ )λ̂·,j (τ ) = λ̂·,j (τ ) for j = 1, . . . , rτ . Then


√
(σ1 (τ ) + rτ Err)Err
v1 := min kĝ(τ )O − g(τ )kF = OP
,
O∈Orτ
(σrτ −1 (τ ))2 − (σrτ (τ ))2

14

(29)

and
kλ̂(τ ) − λ(τ )k2F
nT

v2 =:

= OP

rτ φ2n cT (1+sτ ) max{log(pcT ∨n),rτ }
κ40 f
√
σ12 (σ1 (τ )+ rτ Err)2 Err2
nT ((σrτ −1 (τ ))2 −(σrτ (τ ))2 )2



1
n

+

1
dT



+
(30)

!
.

Here, Orτ is the group of rτ × rτ orthonormal matrices, and
p
p 
φn cT (1 + sτ ) max{log(pcT ∨ n), rτ } √
Err :=
n
+
dT .
κ20 f 1/2
A particularly interesting instance of Corollary 2 is when
(σ1 (τ ))2 , (σrτ −1 (τ ))2 − (σrτ (τ ))2  nT,
a natural setting if the entries of Π(τ ) are O(1). Then the upper bound (29) becomes
p

!
φn cT (1 + sτ ) max{log(pcT ∨ n), rτ }
1
1
√ +√
v1 = OP
,
n
dT
κ20 f 1/2
whereas (30) is now
v2 = OP

rτ φ2n cT (1 + sτ ) max{log(pcT ∨ n), rτ }
κ40 f



1
1
+
n dT

!
.

The conclusion of Corollary 2 allows us to provide an upper on the estimation of factors (g(τ )) and
loadings (λ(τ )) of the latent matrix Π(τ ). In particular this justifies the heuristic discussed in (14).

4.2

Correlated predictors

We conclude our theory section by studying the case where the vector of coefficients θ(τ ) can be dense, in
the sense that the number of non-zero entries can be large, perhaps even larger than nT . To make estimation
feasible, we impose the condition that Xθ(τ ) can be perturbed into a low-rank matrix, a scenario that can
happen when the covariates are highly correlated to each other.
We view our setting below as an extension of the linear model in Chernozhukov et al. (2018) to the
quantile framework and reduced rank regression. The specific condition is stated next.
Assumption 4. With probability approaching one, it holds that rank(Xθ(τ ) + ξ) = O(rτ ), and
!
√
√ √
cT φn rτ ( n + dT )
kξk∗
√
√
= OP
,
nT
nT f
with cT as defined in Theorem 1. Furthermore, kXθ(τ ) + Π(τ )k = OP (1).
15

Notice that in Assumption 4, ξ is an approximation error. In the case ξ = 0, the condition implies that
rank(Xθ(τ )) = O(rτ ) with probability close to one.
Next, exploiting Assumption 4, we show that (21) provides consistent estimation of the quantile function, namely, of Xθ(τ ) + Π(τ ).
Theorem 3. Suppose that Assumptions 1–2 and 4 hold. Let (θ̂(τ ), Π̂(τ )) be the solution to (21) with the
additional constraint that kΠ̃k∞ ≤ C, for a large enough positive constant C.
Then


!
0 2 2
1
)
φ
c
r
(f
1
1
n T τ
,
kΠ̂(τ ) − Π(τ )k2F = OP
+
nT
n dT
f4
p
where {φn } is a sequence with φn /( f log(1 + cT )) → ∞, and for choices

ν1 

n T
1 XX
kXi,t k∞ ,
nT
i=1 t=1

and
ν2 

p 
cT √
n + dT .
nT

Interestingly, unlike Theorem 1, Theorem 3 does not show that we can estimate θ(τ ) and Π(τ ) separately. Instead, we show that Π̂(τ ), the estimated matrix of latent factors, captures the overall contribution of
both θ(τ ) and Π(τ ). This is expected since Assumption 4 states that, with high probability, Xθ(τ ) has rank
of the same order as of Π(τ ). Notably, Π̂(τ ) is able to estimate Xθ(τ ) + Π(τ ) via requiring that the value of
√ √
ν1 increases significantly with respect to the choice in Theorem 1, while keeping ν2  cT ( n+ dT )/(nT ).
As for the convergence rate in Theorem 3 for estimating Π(τ ), this is of the order rτ cT (n−1 + d−1
T ), if
0
−1
−1
we ignore f , f , and φn . When the data are independent, the rate becomes of the order rτ (n + T ). In
such framework, our result matches the minimax rate of estimation in Candès and Plan (2010) for estimating
an n × T matrix of rank rτ , provided that n  T , see our discussion in Section 4.1.
Finally, notice that we have added an additional tuning parameter C that controls the magnitude of the
possible estimate Π̃. This is done for technical reasons. We expect that the same upper bound holds without
this additional constraint.

5

Simulation

In this section, we evaluate the performance of our proposed approach (`1 -NN-QR) with extensive numerical
simulations focusing on the median case, namely the case when τ = 0.5. As benchmarks, we consider the
`1 -penalized quantile regression studied in Belloni and Chernozhukov (2009), and similarly we refer to
16

this procedure as `1 -QR. We also compare with the mean case, which we denote it as `1 -NN-LS as it
combines the `2 -loss function with `1 and nuclear norm regularization. We consider different generative
scenarios. For each scenario we randomly generate 100 different data sets and compute the estimates of
the methods for a grid of values of ν1 and ν2 . Specifically, these tuning parameters are taken to satisfy
ν1 ∈ {10−4 , 10−4.5 , . . . , 10−8 } and ν2 ∈ {10−3 , 10−4 , . . . , 10−9 }. Given any choice of tuning parameters,
we evaluate the performance of each competing method, averaging over the 100 data sets, and report values
that correspond to the best performance. These are referred as optimal tuning parameters and can be thought
of as oracle choices.
We also propose a modified Bayesian Information Criterion (BIC) to select the best pair of tuning parameters. Given a pair (ν1 , ν2 ), our method produces a score (θ̂(τ ), Π̂(τ )). Specifically, denote ŝτ = |{j :
θ̂j (τ ) 6= 0}| and r̂τ = rank(Π̂(τ )),
BIC(ν1 , ν2 ) =

n X
T
X

0
ρτ (Yi,t − Xi,t
θ̂(ν1 , ν2 ) − Π̂(ν1 , ν2 ))+

(31)
log(nT )
(c1 · ŝτ (ν1 , ν2 ) + (1 + n + T ) · r̂τ (ν1 , ν2 )) ,
2
where c1 > 0 is a constant. The intuition here is that the first term in the right hand side of (31) corresponds
to the fit to the data. The second term includes the factor log(nT )/2 to emulate the usual penalization in
BIC. The number of parameters in the model with choices ν1 and ν2 is estimated by ŝτ for the vector of
coefficients, and (1 + n + T ) · r̂τ for the latent matrix. The latter is reasonable since Π̂(τ ) is potentially a low
rank matrix and we simply count the number of parameters in its singular value decomposition. As for the
extra quantity c1 , we have included this term to balance the dominating contribution of the (1 + n + T ) · r̂τ .
We find that in practice c1 = log2 (nT ) gives reasonable performance in both simulated and real data. This
is the choice that we use in our experiments. Then for each of data set under each design, we calculate the
minimum value of BIC(ν1 , ν2 ), over the different choices of ν1 and ν2 , and report the average over the 100
Monte Carlo simulations. We refer to this as BIC-`1 -NN-QR.
i=1 t=1

As performance measure we use a scaled version (see Tables 1-2) of the squared distance between
the true vector of coefficients θ and the corresponding estimate. We also consider a different metric, the
“Quantile error” (Koenker and Machado (1999)):
n T
1 X X −1
(FYi,t |Xi,t ;θ(τ ),Π(τ ) (0.5) − F̂Y−1
(0.5))2 ,
i,t |Xi,t ;θ(τ ),Π(τ )
nT

(32)

i=1 t=1

which measures the average squared error between the quantile functions at the samples and their respective estimates. Since our simulations consider models with symmetric mean zero error, the above metric
corresponds to the mean squared error for estimating the conditional expectation.
Next, we provide a detailed description of each of the generative models that we consider in our experiments. In each model design the dimensions of the problem are given by n ∈ {100, 500}, p ∈ {5, 30} and
T ∈ {100, 500}. The covariates {Xi,t } are i.i.d N (0, Ip ).
17

Design 1. (Location shift model) The data is generated from the model
0
Yi,t = Xi,t
θ + Πi,t + i,t ,

(33)

√
i.i.d.
where 3i,t ∼ t(3), i = 1, . . . , n and t = 1, . . . , T , with t(3) the Student’s t-distribution with 3 degrees
√
of freedom. The scaling factor 3 simply ensures that the errors have variance 1. In (33), we take the vector
θ ∈ Rp to satisfy

1 if j ∈ {1, . . . , min{10, p}}
θj =
0 otherwise.
We also construct Π ∈ Rn×T to be rank one, defined as Πi,t = 5i (cos(4πt/T ))/n.
Design 2. (Location-scale shift model) We consider the model
0
0
Yi,t = Xi,t
θ + Πi,t + (Xi,t
θ)i,t ,

(34)

i.i.d.

where i,t ∼ N (0, 1), i = 1, . . . , n and t = 1, . . . , T . The parameters in θ and Π in (34) are taken to be
the same as in (33). The only difference now is that we have the extra parameter θ ∈ Rp , which we define
as θj = j/(2p) for j ∈ {1, . . . , p}.
Design 3. (Location shift model with random factors) This is the same as Design 1 with the difference
that we now generate Π as
5
X
Πi,t =
ck uk vkT ,
(35)
k=1

where
ck ∼ U [0, 1/4], uk =

ṽk
ũk
, ũk ∼ N (0, In ), vk =
, ṽk ∼ N (0, In ), k = 1, . . . , 5.
kũk k
kṽk k

(36)

Design 4. (Location-scale shift model with random factors) This is a combination of Designs 2 and
3. Specifically, we generate data as in (34) but with Π satisfying (35) and (36).
The results in Tables 1-2 show a clear advantage of our proposed method against the benchmarks across
the four designs we consider. This is true for estimating the vector of coefficients, and under the measure of
quantile error. Importantly, our approach is not only the best under the optimal choice of tuning parameters
but it remains competitive with the BIC type of criteria defined with the score (31). In particular, under
Designs 1 and 2, the data driven version of our estimator, BIC-`1 -NN-QR, performs very closely to the
ideally tuned one `1 -NN-QR. In the more challenging settings of Designs 3 and 4, we noticed that BIC-`1 NN-QR performs reasonably well compared to `1 -NN-QR.

18

Table 1: For Designs 1-2 described in the main text, under different values of (n, p, T ), we compare the
performance of different methods. The metrics use are the scaled `2 distance for estimating θ(τ ), and the
Quantile error defined in (32). For each method we report the average, over 100 Monte Carlo simulations,
of the two performance measures.

Method
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR

n
300
300
300
300
300
300
300
300
300
300
300
300
300
300
300
300
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

p
30
30
30
30
30
30
30
30
5
5
5
5
5
5
5
5
30
30
30
30
30
30
30
30
5
5
5
5
5
5
5
5

T
300
300
300
300
100
100
100
100
300
300
300
300
100
100
100
100
300
300
300
300
100
100
100
100
300
300
300
300
100
100
100
100

Design 1

Design 2

kθ̂(τ )−θ(τ )k2
10−4

kθ̂(τ )−θ(τ )k2
10−4

0.86
0.86
2.58
8.18
2.99
2.99
8.06
41.0
0.22
0.22
0.37
2.6
0.50
0.53
1.12
7.87
2.97
2.97
9.77
40.0
2.3
2.3
8.4
229.0
0.64
0.65
1.25
8.79
1.82
1.85
4.06
32.0

19

Quantile error
0.03
0.03
0.05
4.19
0.04
0.04
0.12
4.19
0.003
0.003
0.01
4.19
0.008
0.009
0.02
4.19
0.04
0.04
0.12
4.23
0.04
0.06
0.79
4.23
0.008
0.008
0.02
4.23
0.009
0.01
0.03
4.23

0.69
0.89
1.03
6.81
2.39
2.39
3.11
26.0
0.39
0.48
0.69
3.27
0.80
0.97
1.46
8.56
2.26
2.81
3.39
24.0
10.0
11.0
13.0
177.0
0.89
1.32
1.67
8.61
3.30
3.74
4.45
27.0

Quantile error
0.03
0.03
0.04
4.18
0.04
0.04
0.08
4.19
0.03
0.03
0.03
4.19
0.03
0.03
0.04
4.19
0.04
0.04
0.06
4.23
0.03
0.04
0.16
4.23
0.03
0.03
0.05
4.23
0.03
0.04
0.14
4.23

Table 2: For Designs 3-4 described in the main text, under different values of (n, p, T ), we compare the
performance of different methods. The metrics use are the scaled `2 distance for estimating θ(τ ), and the
Quantile error defined in (32). For each method we report the average, over 100 Monte Carlo simulations,
of the two performance measures.

Method
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR

n
300
300
300
300
300
300
300
300
300
300
300
300
300
300
300
300
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

p
30
30
30
30
30
30
30
30
5
5
5
5
5
5
5
5
30
30
30
30
30
30
30
30
5
5
5
5
5
5
5
5

T
300
300
300
300
100
100
100
100
300
300
300
300
100
100
100
100
300
300
300
300
100
100
100
100
300
300
300
300
100
100
100
100

Design 3

Design 4

kθ̂(τ )−θ(τ )k2
10−4

kθ̂(τ )−θ(τ )k2
10−4

2.17
2.17
3.33
6.59
10.0
11.0
11.0
27.0
1.13
1.56
1.58
2.47
3.04
4.37
4.43
7.65
11.0
12.0
12.0
26.0
6.12
6.64
8.63
16.5
2.99
4.19
5.27
8.59
12.3
13.1
15.0
24.7

20

Quantile error
0.17
0.29
0.19
1.09
0.18
0.26
0.25
11.10
0.17
0.33
0.19
1.10
0.19
0.27
0.27
1.11
0.18
0.27
0.34
1.10
0.17
0.22
1.08
1.12
0.18
0.26
0.33
1.10
0.16
0.20
1.09
1.10

1.58
2.81
3.54
12.0
4.61
4.61
9.21
47.2
0.27
0.74
0.49
5.52
0.69
1.11
1.12
13.4
7.06
7.29
11.2
51.0
32.1
35.4
82.0
267.6
0.86
1.43
1.45
21.0
2.15
2.43
3.61
45.7

Quantile error
0.11
0.26
0.12
1.01
0.13
0.16
0.17
1.10
0.03
0.13
0.05
1.09
0.05
0.15
0.05
1.10
0.15
0.24
0.18
1.12
0.10
0.15
0.84
1.13
0.05
0.14
0.05
1.09
0.04
0.07
0.07
1.09

6

Empirical Performance of the “Characteristics + Latent Factor” Model
in Asset Pricing

Data Description
We use data from CRSP and Compustat to constrtuct 24 firm level characteristics that are documented to
explain the cross section and time series of stock returns in the finance and accounting literature. The
characteristics we choose include well-known drivers of stock returns such as beta, size, book-to-market,
momentum, volatility, liquidity, investment and profitability. Table 7 in the Appendix lists details of the
characteristics used and the methods to construct the data. We follow the procedures of Green et al. (2017)
to construct the characteristics of interest. The characteristics used in our model are standardized to have
zero mean and unit variance. Figure 1 plots the histogram of monthly stock returns and 9 standardized
firm characteristics. Each of them have different distribution patterns, suggesting the potential nonlinear
relationship between returns and firm characteristics, which can be potentially captured by our quantile
model.
Our empirical design is closely related to the characteristics model proposed by Daniel and Titman
(1997, 1998). To avoid any data snooping issue cause by grouping, we conduct the empirical analysis
at individual stock level. Specifically, we use the sample period from January 2000 to December 2018,
and estimate our model using monthly returns (228 months) from 1306 firms that have non-missing values
during this period.

Figure 1: Histograms of monthly stock returns (left) and firm characteristics (right).

21

A “Characteristic + Latent Factor” Asset Pricing Model
We apply our model to fit the cross section and time series of stock returns ((Lettau and Pelger, 2018)).
There are n assets (stocks), and the return of the each asset can potentially be explained by p observed asset
characteristics (sparse part) and r latent factors (dense part). The asset characteristics are the covariates in
our model. Our model imposes a sparse structure on the p characteristics so that only the characteristics
having the strongest explanatory powers are selected by the model. The part that’s unexplained by the firm
characteristics are captured by latent factors.
Suppose we have n stock returns (R1 ,...,Rn ), and p observed firm characteristics (X1 ,...,Xp ) over T
periods. The return quantile at level τ of portfolio i in time t is assumed to be the following:
FR−1
(τ ) = Xi,t−1,1 θ1 (τ ) + ... + Xi,t−1,k θk (τ ) + ... + Xi,t−1,p θp (τ )+
i,t |Xi,t−1 ;θ(τ ),λi (τ ),gt (τ )

λi (τ )

gt (τ )

(1 × rτ ) (rτ × 1)
where Xi,t−1,k is the k-th characteristic (for example, the book-to-market ratio) of asset i in time t − 1.
The coefficient θk captures the extent to which assets with higher/lower characteristic Xi,t,k delivers higher
average return. The term gt contains the rτ latent factors in period t which captures systematic risks in the
market, and λi contains portfolio i’s loading on these factors (i.e. exposure to risk).
There is a discussion in academic research on “factor versus characteristics” in late 1990s and early
2000s. The factor/risk based view argues that an asset has higher expected returns because of its exposure
to risk factors (e.g. Fama-French 3 factors) which represent some unobserved systematic risk. An asset’s
exposure to risk factors are measured by factor loadings. The characteristics view claims that stocks have
higher expected returns simply because they have certain characteristics (e.g. higher book-to-market ratios,
smaller market capitalization), which might be independent of systematic risk (Daniel and Titman (1997,
1998)). The formulation of our model accommodates both the factor view and the characteristics view.
The sparse part is similar to Daniel and Titman (1997, 1998), in which stock returns are explained by
firm characteristics. The dense part assumes a low-dimensional latent factor structure where the common
variations in stock returns are driven by several “risk factors”.

Empirical Results
We first get the estimates θ̂(τ ) and Π̂(τ ) at three different quantiles, τ = {0.1, 0.5, 0.9} using our proposed
ADMM algorithm. We then decompose Π̂(τ ) into the products of its r̂τ principal components ĝ(τ ) and
their loadings λ̂(τ ) via eq(14). The (i, k)-th element of λ̂(τ ), denoted as λ̂i,k (τ ), can be interpreted as the
exposure of asset i to the k-th latent factor (or in finance terminology, “quantity of risk”). And the (t, k)-th
elements of ĝ(τ ), denoted as ĝt,k (τ ), can be interpreted as the compensation of the risk exposure to the
k-th latent factor in time period t (or in finance terminology, “price of risk”). The model are estimated with

22

different tuning parameters ν1 and ν2 , and we use our proposed BIC to select the optimal tuning parameters.
The details of the information criteria can be found in equation (31).
The tuning parameter ν1 governs the sparsity of the coefficient vector θ. The larger ν1 is, the larger
the shrinkage effect on θ. Figure 2 illustrate the effect of this shrinkage. With ν2 fixed, as the value of ν1
increases, more coefficients in the estimated θ vector shrink to zero. From a statistical point of view, the
“effective characteristics” that can explain stock returns are those with non-zero coefficient θ at relatively
large values of ν1 .

Figure 2: Estimated Coefficients as a Function of ν1
The figure plots the estimated coefficient θ when the tuning parameter ν1 changes, for τ = {0.1, 0.5, 0.9}.
The parameter ν2 is fixed at log10 (ν2 ) = −4.
Table 3 reports the relationship between tuning parameter ν2 and rank of estimated Π at different quantiles. It shows that the tuning parameter ν2 governs the rank of matrix Π, and that as ν2 increases, we
penalize more on the rank of matrix Π through its nuclear norm.
The left panel of Table 4 reports the estimated coefficients in the sparse part when we fix the tuning
parameters at log10 (ν1 ) = −3.5 and log10 (ν2 ) = −4. The signs of some characteristics are the same across
the quantiles, e.g. size (mve), book-to-market (bm), momentum (mom1m, mom12m), accurals (acc), book
equity growth (egr), leverage (lev), and standardized unexpected earnings (sue). However, some characteristics have heterogenous effects on future returns at different quantiles. For example, at the 10% quantile,
23

Table 3: The estimated rank of Π.
log10 (ν2 )

τ = 0.1

τ = 0.5

τ = 0.9

-6.0
-5.5
-5.0
-4.5
-4.0
-3.5
-3.0

228
228
228
164
1
1
0

228
228
228
228
7
1
0

228
228
228
168
2
1
0

Note: Estimated under different values of turning parameter ν2 , when ν1 = 10−5 is fixed. The results are
reported for quantiles 10%, 50% and 90%.

high beta stocks have high future returns, which is consistent with results found via the CAPM; while at
50% and 90% quantile, high beta stocks have low future returns, which conforms the “low beta anomaly”
phenomenon. Volatility (measured by both range and idiosyncratic volatility) is positively correlated with
future returns at 90% quantile, but negatively correlated with future returns at 10% and 50% percentile. The
result suggests that quantile models can capture a wider picture of the heterogenous relationship between
asset returns and firm characteristics at different parts of the distribution (Koenker (2000)).
Table 5 reports the selected optimal tuning parameters ν1 and ν2 for different quantiles. The tuning
parameters are selected via BIC based on (31) as discussed in Section 5. For every ν1 and ν2 , we get the
e 1 , ν2 ) and Π(ν
e 1 , ν2 ) and the number of factors r = rank(Π(ν
e 1 , ν2 )). The θ vector is sparse
estimates θ(ν
with non-zero coefficients on selected characteristics. The 10% quantile of returns has only 1 latent factor,
and 3 selected characteristics. The median of returns has 7 latent factors and 2 selected characteristics. The
90% quantile of returns has 2 latent factors and 7 selected characteristics. Range is the only characteristic
selected across all 3 quantiles. Idiosyncratic volatility is selected at 10% and 90% quantiles, with opposite
signs. 1-month momentum is selected at 50% and 90% percentiles, with negative sign suggesting reversal
in returns.
Overall, the empirical evidence suggests that both firm characteristics and latent risk factors have valuable information in explaining stock returns. In addition, we find that the selected characteristics and number
of latent factors differ across the quantiles.

24

Table 4: Sparse Part Coefficients at Different Quantiles.

Fixed ν1 and ν2
τ = 0.1
τ = 0.5
τ = 0.9
acc
range
beta
bm
chinv
dy
egr
ep
gma
idiovol
ill
invest
lev
lgr
mom12m
mom1m
mve
operprof
roaq
roeq
std dolvol
std turn
sue
turn

-0.089
-2.574
0.174
0.371
0
-0.086
-0.106
0.199
0
-1.229
-0.334
-0.097
0.183
-0.106
-0.166
-0.150
-0.038
0
0.221
0.073
0
0.310
0.105
-0.796

-0.074
-0.481
-0.116
0.175
0
0
-0.053
0.057
0.091
-0.071
-0.218
0
0.063
-0.037
-0.077
-0.384
-0.093
0.025
0.242
0.041
0
0
0.061
-0.083

-0.041
2.526
-0.406
0.263
-0.152
0.119
-0.091
-0.479
0.201
1.438
0
0.146
0.129
0
-0.117
-0.571
-0.811
0.088
-0.147
0
-0.039
-0.247
0.045
0.386

Optimal ν1 and ν2 (BIC)
τ = 0.1
τ = 0.5
τ = 0.9
0
-2.372
0
0
0
0
0
0
0
-1.055
0
0
0
0
0
0
0
0
0
0
0
0
0
-0.330

0
-0.356
0
0
0
0
0
0
0
0
0
0
0
0
0
-0.286
0
0
0
0
0
0
0
0

0
2.429
-0.115
0.168
0
0
0
-0.391
0
1.286
0
0
0
0
0
-0.477
-0.667
0
0
0
0
0
0
0

Note: The left panel reports the estimated coefficient vector θ in the sparse part for quantiles
10%, 50% and 90%, when the tuning parameters are fixed at log10 (ν1 ) = −3.5, log10 (ν2 ) =
−4. The right panel reports the estimated coefficient vector θ under the when the turning
parameters are optimal, as selected by BIC (indicated in Table 5).

25

Table 5: Selected Optimal Tuning Parameters and
Number of Factors

optimal r
optimal ν1
optimal ν2

τ = 0.1
1
−2.5
10
10−4

τ = 0.5
7
−2.5
10
10−4

τ = 0.9
2
−2.75
10
10−4

Note: This table reports the selected optimal tuning
parameter ν1 and ν2 that minimize the objective function in equation (31) for different quantiles.

Interpretation of Latent Factors
Table 6 below reports the variance in the matrix Π explained by each Principal Component (PC) or latent
factor. At upper and lower quantiles, the first PC dominates. At the median there are more latent factors
accounting for the variations in Π, with second PC explaining 13.8% and third PC explaining 6.8%.
Table 6: Percentage of Π explained by PC

PC1
PC2
PC3
PC4
PC5
PC6
PC7
Total

τ = 0.1
100.00%

100.00%

τ = 0.5
73.82%
13.71%
6.78%
4.12%
1.11%
0.45%
0.01%
100.00%

τ = 0.9
99.68%
0.32%

100.00%

Note: Variance of matrix Π explained by each principal component for different quantiles.

We also found the first PC captures the market returns in all three quantiles: Figure 3 plots the first
principal component against the monthly returns of S&P500 index, showing that they have strong positive
correlations.

26

Figure 3: The S&P 500 Index Return and the First PC at Different Quantiles.
This figure plots the first PC of matrix Π against S&P500 index monthly return for quantiles 10% (left),
50% (middle), and 90% (right).

References
Jason Abrevaya and Christian M Dahl. The effects of birth inputs on birthweight: evidence from quantile
estimation on panel data. Journal of Business & Economic Statistics, 26(4):379–397, 2008.
Alnur Ali, Zico Kolter, and Ryan Tibshirani. The multiple quantile graphical model. In Advances in Neural
Information Processing Systems, pages 3747–3755, 2016.
Tomohiro Ando and Jushan Bai. Quantile co-movement in financial markets: A panel quantile model with
unobserved heterogeneity. Journal of the American Statistical Association, pages 1–31, 2019.
Manuel Arellano and Stéphane Bonhomme. Quantile selection models with an application to understanding
changes in wage inequality. Econometrica, 85(1):1–28, 2017.
Susan Athey, Mohsen Bayati, Nikolay Doudchenko, Guido Imbens, and Khashayar Khosravi. Matrix completion methods for causal panel data models. 2018.
Jushan Bai. Panel data models with interactive fixed effects. Econometrica, 77(4):1229–1279, 2009.
Jushan Bai and Junlong Feng. Robust principal components analysis with non-sparse errors. arXiv preprint
arXiv:1902.08735, 2019.
Jushan Bai and Kunpeng Li. Statistical analysis of factor models of high dimension. The Annals of Statistics,
40(1):436–465, 2012.
Jushan Bai and Serena Ng. Principal components estimation and identification of static factors. Journal of
Econometrics, 176(1):18–29, 2013.
Jushan Bai and Serena Ng. Principal components and regularized estimation of factor models. arXiv preprint
arXiv:1708.08137, 2017.
Jushan Bai and Serena Ng. Matrix completion, counterfactuals, and factor analysis of missing data. arXiv
preprint arXiv:1910.06677, 2019.

27

Alexandre Belloni and Victor Chernozhukov. On the computational complexity of mcmc-based estimators
in large samples. The Annals of Statistics, 37(4):2011–2055, 2009.
Alexandre Belloni and Victor Chernozhukov. `1 -penalized quantile regression in high-dimensional sparse
models. The Annals of Statistics, 39(1):82–130, 2011.
Peter Bickel, Ya’acov Ritov, and Alexandre Tsybakov. Simultaneous analysis of lasso and dantzig selector.
The Annals of Statistics, 37(4):1705–1732, 2009.
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization
and statistical learning via the alternating direction method of multipliers. Foundations and Trends R in
Machine learning, 3(1):1–122, 2011.
Pratik Prabhanjan Brahma, Yiyuan She, Shijie Li, Jiade Li, and Dapeng Wu. Reinforced robust principal
component pursuit. IEEE transactions on neural networks and learning systems, 29(5):1525–1538, 2017.
Jian-Feng Cai, Emmanuel Candès, and Zuowei Shen. A singular value thresholding algorithm for matrix
completion. SIAM Journal on Optimization, 20(4):1956–1982, 2010.
John Campbell. Financial decisions and markets: a course in asset pricing. Princeton University Press,
2017.
Emmanuel Candès and Yaniv Plan. Matrix completion with noise. Proceedings of the IEEE, 98(6):925–936,
2010.
Emmanuel Candès and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery from a minimal
number of noisy random measurements. IEEE Transactions on Information Theory, 57(4):2342–2359,
2011.
Emmanuel Candès and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of
Computational mathematics, 9(6):717, 2009.
Emmanuel Candès and Terence Tao. The dantzig selector: Statistical estimation when p is much larger than
n. The annals of Statistics, 35(6):2313–2351, 2007.
Gary Chamberlain and Michael Rothschild. Arbitrage, factor structure, and mean-variance analysis on large
asset markets. Econometrica (pre-1986), 51(5):1281, 1983.
Sourav Chatterjee. Matrix estimation by universal singular value thresholding. The Annals of Statistics, 43
(1):177–214, 2015.
Liang Chen, Juan Dolado, and Jesús Gonzalo. Quantile factor models. 2018.
Mingli Chen. Estimation of nonlinear panel models with multiple unobserved effects. Warwick Economics
Research Paper Series No. 1120, 2014.
Mingli Chen, Iván Fernández-Val, and Martin Weidner. Nonlinear panel models with interactive effects.
arXiv preprint arXiv:1412.5647, 2014.
Victor Chernozhukov, Christian Hansen, and Yuan Liao. A lava attack on the recovery of sums of dense and
sparse signals. The Annals of Statistics, 45(1):39–76, 2017.

28

Victor Chernozhukov, Christian Hansen, Yuan Liao, and Yinchu Zhu. Inference for heterogeneous effects
using low-rank estimations. arXiv preprint arXiv:1812.08089, 2018.
John H Cochrane. Asset pricing: Revised edition. Princeton university press, 2009.
John H Cochrane. Presidential address: Discount rates. The Journal of finance, 66(4):1047–1108, 2011.
Gregory Connor and Robert A Korajczyk. Risk and return in an equilibrium apt: Application of a new test
methodology. Journal of financial economics, 21(2):255–289, 1988.
Arnak Dalalyan, Mohamed Hebiri, and Johannes Lederer. On the prediction performance of the lasso.
Bernoulli, 23(1):552–581, 2017.
Kent Daniel and Sheridan Titman. Evidence on the characteristics of cross sectional variation in stock
returns. the Journal of Finance, 52(1):1–33, 1997.
Kent Daniel and Sheridan Titman. Characteristics or covariances. Journal of Portfolio Management, 24(4):
24–33, 1998.
Andreas Elsener and Sara van de Geer. Robust low-rank matrix estimation. The Annals of Statistics, 46
(6B):3481–3509, 2018.
Eugene F Fama and Kenneth R French. Common risk factors in the returns on stocks and bonds. Journal of
financial economics, 33(1):3–56, 1993.
Maryam Fazel. Matrix rank minimization with applications. 2002.
Guanhao Feng, Stefano Giglio, and Dacheng Xiu. Taming the factor zoo: A test of new factors. Technical
report, National Bureau of Economic Research, 2019.
Junlong Feng.
Regularized quantile regression with interactive fixed effects.
arXiv:1911.00166, 2019.

arXiv preprint

Antonio Galvao. Quantile regression for dynamic panel data with fixed effects. Journal of Econometrics,
164(1):142–157, 2011.
Antonio Galvao and Kengo Kato. Smoothed quantile regression for panel data. Journal of econometrics,
193(1):92–112, 2016.
Antonio Galvao and Gabriel V Montes-Rojas. Penalized quantile regression for dynamic panel data. Journal
of Statistical Planning and Inference, 140(11):3476–3497, 2010.
Domenico Giannone, Michele Lenza, and Giorgio Primiceri. Economic predictions with big data: The
illusion of sparsity. 2017.
Stefano Giglio and Dacheng Xiu. Asset pricing with omitted factors. Chicago Booth Research Paper,
(16-21), 2018.
Bryan S Graham, Jinyong Hahn, Alexandre Poirier, and James L Powell. A quantile correlated random
coefficients panel data model. Journal of Econometrics, 206(2):305–335, 2018.
Jeremiah Green, John Hand, and Frank Zhang. The characteristics that provide independent information
about average us monthly stock returns. The Review of Financial Studies, 30(12):4389–4436, 2017.

29

Yufeng Han, Ai He, David Rapach, and Guofu Zhou. What firm characteristics drive us stock returns?
Available at SSRN 3185335, 2018.
Xuming He, Lan Wang, and Hyokyoung Grace Hong. Quantile-adaptive model-free variable screening for
high-dimensional heterogeneous data. The Annals of Statistics, 41(1):342–369, 2013.
Kengo Kato, Antonio F Galvao Jr, and Gabriel V Montes-Rojas. Asymptotics for panel quantile regression
models with individual effects. Journal of Econometrics, 170(1):76–91, 2012.
Roger Koenker. Galton, edgeworth, frisch, and prospects for quantile regression in econometrics. Journal
of Econometrics, 95(2):347–374, 2000.
Roger Koenker. Quantile regression for longitudinal data. Journal of Multivariate Analysis, 91(1):74–89,
2004.
Roger Koenker. Quantile regression. Cambridge University Press, New York, 2005.
Roger Koenker and Jose AF Machado. Goodness of fit and related inference processes for quantile regression. Journal of the american statistical association, 94(448):1296–1310, 1999.
Roger Koenker, Victor Chernozhukov, Xuming He, and Limin Peng. Handbook of quantile regression. CRC
press, 2017.
Vladimir Koltchinskii, Karim Lounici, and Alexandre Tsybakov. Nuclear-norm penalization and optimal
rates for noisy low-rank matrix completion. The Annals of Statistics, 39(5):2302–2329, 2011.
Serhiy Kozak, Stefan Nagel, and Shrihari Santosh. Shrinking the cross-section. Journal of Financial
Economics, 2019.
Carlos Lamarche. Robust penalized quantile regression estimation for panel data. Journal of Econometrics,
157(2):396–408, 2010.
Martin Lettau and Markus Pelger. Estimating latent asset-pricing factors. Technical report, National Bureau
of Economic Research, 2018.
Shujie Ma, Oliver Linton, and Jiti Gao. Estimation and inference in semiparametric quantile factor models.
2019.
Hyungsik Roger Moon and Martin Weidner. Nuclear norm regularized estimation of panel regression models. arXiv preprint arXiv:1810.10987, 2018.
Sahand Negahban and Martin Wainwright. Estimation of (near) low-rank matrices with noise and highdimensional scaling. The Annals of Statistics, 39(2):1069–1097, 2011.
Benjamin Recht, Maryam Fazel, and Pablo Parrilo. Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization. SIAM review, 52(3):471–501, 2010.
Angelika Rohde and Alexandre Tsybakov. Estimation of high-dimensional low-rank matrices. The Annals
of Statistics, 39(2):887–930, 2011.
Stephen Ross. The arbitrage theory of capital asset pricing. Journal of Economic Theory, 13(3):341–360,
1976.

30

Andrés G Sagner. Three essays on quantile factor analysis. PhD thesis, Boston University, 2019.
Yiyuan She and Kun Chen. Robust reduced-rank regression. Biometrika, 104(3):633–647, 2017.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society:
Series B (Methodological), 58(1):267–288, 1996.
Aad W. van der Vaart and Jon A Wellner. Weak convergence and empirical processes: with applications to
statistics. Springer, 1996.
Van Vu. Spectral norm of random matrices. Combinatorica, 27(6):721–736, 2007.
Lan Wang, Yichao Wu, and Runze Li. Quantile regression for analyzing heterogeneity in ultra-high dimension. Journal of the American Statistical Association, 107(497):214–222, 2012.
Yuanshan Wu and Guosheng Yin. Conditional quantile screening in ultrahigh-dimensional heterogeneous
data. Biometrika, 102(1):65–76, 2015.
Bin Yu. Rates of convergence for empirical processes of stationary mixing sequences. The Annals of
Probability, pages 94–116, 1994.
Yi Yu, Tengyao Wang, and Richard J Samworth. A useful variant of the davis–kahan theorem for statisticians. Biometrika, 102(2):315–323, 2014.
Qi Zheng, Limin Peng, and Xuming He. Globally adaptive quantile regression with ultra-high dimensional
data. Annals of statistics, 43(5):2225, 2015.

31

A

Implementation Details of the Proposed ADMM Algorithm

Denoting by P+ (·) and P− (·) the element-wise positive and negative part operators, the ADMM proceeds
doing the iterative updates

V

(k+1)

θ̃(k+1)





τ
τ
(k)
(k)
(k)
(k)
0
0
← P+ W − UV −
11 + P− W − UV −
11
(37)
nT η
nT η
( n T
)
2 η
η X X  (k)
(k)
(k)
(k)
(k)
0
← arg min
Wi,t − Yi,t + Xi,t
θ + (ZΠ )i,t + (UW )i,t + kZθ − θ + Uθ k2
2
2
θ
i=1 t=1

(38)
Π̃(k+1) ← arg min



Π̃

ν2
1 (k)
(k)
kZΠ − Π̃ + UΠ k2F + kΠ̃k∗
2
η


(39)

(40)

p

ν1 X
(k+1)
(k)
Zθ
← arg min
kθ̃(k+1) − Uθ − Zθ k2 +
wj |(Zθ )j |
(41)
2

η
Zθ
j=1
(
η
η
(k)
(k)
(k+1)
kW − Y + X θ̃(k+1) + ZΠ + UW k2F + kV (k+1) − W + UV k2F
(ZΠ
, W (k+1) ) ← arg min
2
2
ZΠ ,W

1

(42)
η
(k)
+ kZΠ − Π̃(k+1) + UΠ k2F
2

(k+1)

UV

(k+1)

(k+1)

(43)

(k)

UW

(k+1)

− Π̃(k+1) + UΠ , Uθ

← V (k+1) − W (k+1) + UV ,
UΠ

)

← ZΠ

(k+1)

← W (k+1) − Y + X θ̃(k+1) + ZΠ
(k)

(k+1)

(k+1)

← Zθ

(k)

− θ̃(k+1) + Uθ ,

where η > 0 is the penalty, see Boyd et al. (2011).
The update for θ̃ is
"
θ̃

(k+1)

←

n X
T
X

#−1 "
0
Xi,t Xi,t

−

+ Ip

i=1 t=1

n X
T
X

#
Xi,t Ai,t +

(k)
Zθ

i=1 t=1

where
(k)

(k)

A := W (k) + ZΠ + UW − Y.
The update for Π̃ is
Π̃

(k+1)



ν2
← P diag max 0, vj −
η
32

!



Q0 ,
1≤j≤l

+

(k)

+ UW ,

(k)
Uθ

,

where
(k)

(k)

Z Π + UΠ

= P diag({vj }1≤j≤l )Q0 .

Furthermore, for Zθ ,
(k+1)

Zθ,j

(k+1)

← sign(θ̃j



ν1 wj
(k)
(k+1)
(k)
− Uθ,j ) |θ̃j
− Uθ,j | −
.
η

Finally, defining
(k)

(k)

(k)

Ã = −Y + X θ̃(k+1) + UW , B̃ = −V (k+1) − UV , C̃ = −Π̃(k+1) + UΠ ,
the remaining updates are
(k+1)

ZΠ

←

−Ã − 2C̃ + B̃
,
3

and
(k+1)

W (k+1) ← −Ã − C̃ − 2ZΠ

B

.

Proofs of the Main Results in the Paper

B.1

Auxiliary lemmas for proof of Theorem 1

Throughout, we use the notation
Qτ (θ̃, Π̃) = E(Q̂τ (θ̃, Π̃)).
Moreover, as in Yu (1994), we define the sequence {(Ỹi,t , X̃i,t )}i∈[n],t∈[T ] such that
ei,t )}i∈[n],t∈[T ] is independent of {(Yi,t , Xi,t )}i∈[n],t∈[T ] ;
• {(Yei,t , X
ei,t )}i∈[n] are independent;
• for a fixed t the random vectors {(Yei,t , X
• for a fixed i:
ei,t )}t∈H ) = L({(Yi,t , Xi,t )}t∈H ) = L({(Yi,t , Xi,t )}t∈H ) ∀l ∈ [dT ],
L({(Yei,t , X
1
l
l
ei,t )}t∈H , . . . , {(Yei,t , X
ei,t )}t∈H are independent.
and the blocks {(Yei,t , X
1
dT
Here, we define Λ := {H1 , H10 , . . . , HdT , Hd0 T , R} with
Hj
Hj0

= {t : 1 + 2(j − 1)cT ≤ t ≤ (2j − 1)cT } ,
= {t : 1 + (2j − 1)cT ≤ t ≤ 2jcT } , j = 1, . . . , dT ,
and R = {t : 2cT dT + 1 ≤ t ≤ T }.
33

(44)

We also use the symbol L(·) to denote the distribution of a sequence of random variables.
0 θ(τ ) + Π (τ )}, and ã
e
e0
Next, define the scores ai,t = τ − 1{Yi,t ≤ Xi,t
i,t
i,t = τ − 1{Yi,t ≤ Xi,t θ(τ ) +
Πi,t (τ )}.

Lemma 4. Under Assumptions 1–3, we have


s
 µ
n X
T
X
Xi,t,j ai,t
1
16
1
cT log(max{n, pcT }) 

P
max
≥ 9
≤
+ 8npT
.
j=1,...,p nT
σ̂j
ndT
n
cT
i=1 t=1

Proof. Notice that
!
n T
1 X X Xi,t,j ai,t
P max
≥ η|X
j=1,...,p nT
σ̂j


 i=1 t=1

2(l−1)+cT
dT
n X
X
X
Xi,t,j ait 
1
η
1
≤ 2p max P 
≥ |X  +
j=1,...,p
ndT
cT
σ̂j
9
i=1 l=1
t=2(l−1)+1
!
!
n
1 X 1 X Xi,t,j
η
p max P
≥ |X
j=1,...,p
ndT
c
σ̂j
9
i=1 T
t∈R
!
d
−1
n
T
1 X X Xi, (2lcT +m), j ãi,t
η
≤ 4p max P
max
≥ |X +
m=1,...,cT ndT
j=1,...,p
σ̂j
9
i=1 l=0
!
 µ
n
1 X Xi, (2dT cT +m) j ãi,t
1
η
max
2p max P
≥ |X + 8npT
j=1,...,p
σ̂j
9
cT
m=1,...,|R| ndT
i=1
!
d
−1
n
T
Xi, (2lcT +m), j ãi,t
1 X X
η
≤ 4pcT
max P
≥ |X +
ndT
σ̂j
9
j∈[p],m∈[cT ]
i=1 l=0
!
 µ
n
X
Xi, (2dT cT +m), j ãi,t
1
1
η
2pcT
max
P
≥ |X + 8npT
ndT
σ̂j
9
cT
j∈[p],m∈[|R|]

(45)

i=1

where the first inequality follows from union bound, and the second by Lemmas 4.1 and 4.2 from Yu (1994).
Therefore, since
n dT −1
1 X X
Xi,2 (2lcT +m) ,j ≤ 3cT σ̂j2 ,
ndT
i=1 l=0

and with a similar argument for the second term in the last inequality of (45), we obtain the result by
Hoeffding’s inequality and integrating over X.
Lemma 5. Supposes that Assumptions 1–3 hold, and let
(
G =

∆∈

Rn×T

34

: k∆k∗ ≤ 1

)
.

(46)

Then there exists positive constants c1 and c2 such that
n T
1 XX
sup
∆i,t ai,t
∆∈G nT

≤

i=1 t=1

p 
100cT √
n + dT ,
nT

with probability at least

1 − 2nT

1
cT

µ
− 2c1 exp(−c2 max{n, dT } + log cT ),

for some positive constants c1 and c2 .
Proof. Notice that by Lemma 4.3 from Yu (1994),




1
nT

dT X
n X
X

dT X
n X
X

n X
X





1
1
∆i,t ai,t  ≥ η 
∆i,t ai,t +
nT
nT
∆∈G
0
i=1 l=1 t∈Hl
i=1 t∈R
i=1 l=1
t∈Hl


d
d
n
n
T X
T X
X
X
X
X
1
η
η
1
∆i,t ãi,t ≥  + P  sup
∆i,t0 ãi,t ≥  +
≤ P  sup
3
3
∆∈G nT i=1 l=1
∆∈G nT i=1 l=1 t∈H
t∈Hl0
l
!


n
1 XX
η
1 µ
P sup
+ 2nT
∆i,t ãi,t ≥
3
cT
∆∈G nT i=1 t∈R
!
d
−1
n
T
1 X X
η
≤ 2cT max P sup
∆i, (2cT l+m) ãi, (2cT l+m) ≥
+
9
m∈[cT ]
∆∈G ndT i=1 l=0
!
 µ
n
1 X
1
η
+ 2nT
2cT max P sup
∆i, (2cT dT +m) ãi, (2cT dT +m) ≥
9
cT
m∈[|R|]
∆∈G ndT i=1
(47)
P  sup 

∆i,t ai,t +

We now proceed to bound each of the terms in the upper bound of (47). For the first term, notice that for
a fixed m
n dT −1
1 X X
sup
∆i, (2cT l+m) ãi, (2cT l+m)
∆∈G ndT
i=1 l=0

1 
ãi, (2cT l+m)
∆∈G ndT
p 
3 √
≤
n + dT ,
ndT
≤ sup

i∈[n],l∈[dT ] 2

k∆k∗

(48)

where the first inequality holds by the duality between the nuclear norm and spectral norm, and the second
inequality happens with probability at least 1 − c1 exp (−c2 max{n, dT }) by Theorem 3.4 from Chatterjee
(2015).
On the other hand,
n
1 X
sup
∆i, (2cT dT +m) ãi, (2cT dT +m)
∆∈G ndT
i=1

35

√

nk{∆i, (2cT dT +m) }i∈[n] k
≤ sup
ndT
∆∈G
√
nk∆k∗
≤
,
ndT

(49)

with probability at least 1 − c1 exp (−c2 max{n, dT }), also by Theorem 3.4 from Chatterjee (2015).
√
√
√
The claim follows by combining (47), (48), and (49), taking η = 30( n + dT )/ ndT , and the fact
that cT /T ≤ 1/3.

Lemma 6. For every Π̃, Π̌ ∈ Rn×T , we have that
q
kΠ̌ − Π̃k∗ + kΠ̌k∗ − kΠ̃k∗ ≤ 6 rank(Π̌)kΠ̌ − Π̃kF
Proof. This follows directly from Lemma 2.3 in Elsener and van de Geer (2018).
Lemma 7. Assume that 1–3 hold. Then, with probability approaching one,
3
5
kθk1 ≤ kθk1,n,T ≤ kθk1 ,
4
4

(50)

for all θ ∈ Rp .
Moreover, for c0 ∈ (0, 1) letting
9
ν1 =
1 − c0

s

p
cT log(max{n, pcT }) √
( n + dT ),
ndT

and
ν2 =

p 
200cT √
n + dT ,
nT

we have that
(θ̂(τ ) − θ(τ ), Π̂(τ ) − Π(τ )) ∈ Aτ ,
with probability approaching one, where
(
Aτ =

(δ, ∆) : kδTτc k1 +

k∆k∗
√ √
nT log(max{n,pcT })


≤ C0 kδTτ k1 +

√
√

and C0 is a positive constant that depends on τ and c0 .
Proof. By Lemma 4, Lemma 5, Lemma 6, and Assumption 2, we have that

36

nT

√

rτ k∆kF

log(max{n,pcT })

)
,



0 ≤ Q̂(θ(τ ), Π(τ )) − Q̂(θ̂(τ ), Π̂(τ )) + ν1 kθ(τ )k1,n − kθ̂(τ )k1,n,T + ν2 (kΠ(τ )k∗ − kΠ̂(τ )k∗ )
n T
1 X X Xi,t,j ai,t
≤ max
1≤j≤p nT
σ̂j

" p
X

i=1 t=1

#



σ̂k |θk (τ ) − θ̂k (τ )| + ν1 kθ(τ )k1,n,T − kθ̂(τ )k1,n,T +

k=1

n X
T
X

1
ai,t (Πi,t (τ ) − Π̂i,t (τ )) + ν2 (kΠ(τ )k∗ − kΠ̂(τ )k∗ )
nT
i=1 t=1
s
" p
#


cT log(max{n, pcT }) X
σ̂k |θk (τ ) − θ̂k (τ )| + ν1 kθ(τ )k1,n,T − kθ̂(τ )k1,n,T
≤ 9
ndT
k=1
!
n T
1 XX
˜ i,t + ν2 (kΠ(τ )k∗ − kΠ̂(τ )k∗ )
+ kΠ(τ ) − Π̂(τ )k∗
sup
ai,t ∆
nT
˜
k∆k∗ ≤1
i=1 t=1
s
#
" p


cT log(max{n, pcT }) X
σ̂k |θk (τ ) − θ̂k (τ )| + ν1 kθ(τ )k1,n,T − kθ̂(τ )k1,n,T
≤ 9
ndT
k=1

+


p 
p 
200cT √
200cT √
n + dT kΠ(τ ) − Π̂(τ )k∗ +
n + dT
kΠ(τ )k∗ − kΠ̂(τ )k∗ ,
nT
nT

p 
100cT √
n + dT kΠ(τ ) − Π̂(τ )k∗
nT
s
" p
#


cT log(max{n, pcT }) X
σ̂k |θk (τ ) − θ̂k (τ )| + ν1 kθ(τ )k1,n,T − kθ̂(τ )k1,n,T
≤ 9
ndT
−

k=1

√

+

p 
1200cT rτ √
n + dT kΠ(τ ) − Π̂(τ )kF
nT

−

p 
100cT √
n + dT kΠ(τ ) − Π̂(τ )k∗
nT

with probability at least
16
− 8npT
1−γ−
n



1
cT

µ


− 2nT

1
cT

µ
− 2c1 exp(−c2 max{n, dT } + log cT ).

Therefore, with probability approaching one, for positive constants C1 and C2 , we have


p 

X
0 ≤ 
(1 − c0 )σ̂j |θ̂j (τ ) − θj (τ )| + σ̂j |θj (τ )| − σ̂j |θ̂j (τ )| 
j=1


+

√
3C1 rτ kΠ(τ )−Π̂(τ )kF
√ √
nT log(max{n,pcT })

−
37

C2 kΠ(τ )−Π̂(τ )k∗
√ √
nT log(max{n,pcT })


,

and the claim follows.

Lemma 8. Under Assumption 3, for all (δ, ∆) ∈ Aτ , we have that


2


 Jτ1/2 (δ, ∆)

1/2
Qτ (θ(τ ) + δ, Π(τ ) + ∆) − Qτ (θ(τ ), Π(τ )) ≥ min
, qJτ (δ, ∆) .


4


Proof. Let
(
vAτ



˜ − Qτ (θ(τ ), Π(τ )) ≥
= sup v : Qτ (θ(τ ) + δ̃, Π(τ ) + ∆)
v
)
1/2
˜ ≤v .
Jτ (δ̃, ∆)

1/2

Jτ

˜
(δ̃,∆)
4

2

˜ ∈ Aτ ,
, ∀(δ̃, ∆)

Then by the convexity of Qτ (·) and the definition of vAu , we have that
˜ − Qτ (θ(τ ), Π(τ ))
Qτ (θ(τ ) + δ̃, Π(τ(
) + ∆)


≥
≥

(δ,∆)

2

4

2
1/2
Jτ (δ,∆)
4


≥

1/2

Jτ

1/2

Jτ

(δ,∆)

1/2

∧

Jτ

vAτ


∧

(δ,∆)

·

inf
1/2

˜
(δ̃,∆)∈A
τ , Jτ
2

1/2

Jτ

)

(δ,∆) vAτ
vAτ
4

˜
(δ̃,∆)≥v
Aτ

˜ − Qτ (θ(τ ), Π(τ ))
Qτ (θ(τ ) + δ̃, Π(τ ) + ∆)



2

4

1/2

∧ qJτ (δ, ∆),

where in last inequality we have used the fact that vAτ ≥ 4q. To see why this is true, notice that there exists
zXit ,z ∈ [0, z] such that
Qτ (θ(τ ) + δ, Π(τ ) + ∆) − Qτ (θ(τ ), Π(τ ))
!
Z X 0 δ+∆it 
n T

i,t
1 XX
0
0
=
FYi,t |Xi,t ,Πi,t (Xi,t θ(τ ) + Πi,t + z) − FYi,t |Xi,t ,Πi,t (Xi,t θ(τ ) + Πi,t (τ )) dz
E
nT
0
i=1 t=1
Z X 0 δ+∆i,t 
n T

i,t
1 XX
0
=
E
zfYi,t |Xi,t ,Πi,t (τ ) (Xi,t
θ(τ ) + Πi,t (τ )) +
nT
0
i=1 t=1
!
z2 0
0
2 fYi,t |Xi,t ,Πi,t (Xi,t θ(τ )

+ Πi,t (τ ) + zXi,t ,z )dz

n X
T
n T


2 
f X
1 f¯0 X X  0
3
0
E Xi,t δ + ∆i,t
−
E Xi,t δ + ∆i,t
.
≥
nT
6 nT
i=1 t=1

i=1 t=1

(51)
1/2

Hence, if (δ, ∆) ∈ Aτ with Jτ (δ, ∆) ≤ 4q then
  P P
3/2
v
u
n
T
1


0 δ + ∆ )2
n
T
3/2


E
(X
u f XX
i,t
2
i,t
i=1
t=1
nT
3f
t
0 δ+∆
 P P

≤
E Xi,t
·
inf
i,t
0
n
T
nT
2 f¯ (δ,∆)∈Aτ ,δ6=0
E 1
|X 0 δ + ∆ |3
i=1 t=1

nT

38

i=1

t=1

i,t

i,t

combined with (51) implies


2
1/2
Jτ (δ, ∆)

Qτ (θ(τ ) + δ, Π(τ ) + ∆) − Qτ (θ(τ ), Π(τ )) ≥

4

.

Lemma 9. Under Assumption 3, for all (δ, ∆) ∈ Aτ , we have
(
)
√
√
rτ
2(C0 + 1) sτ + 1
kδk1,n,T ≤
max p
, 1 Jτ1/2 (δ, ∆),
κ0
log(n ∨ pcT )
and
√
rτ

(

√

p
k∆k∗ ≤ (C0 + 1) sτ + 1 nT log(max{pcT , n})κ−1
0 max

)

p
, 1 Jτ1/2 (δ, ∆),
log(n ∨ pcT )

with C0 as in Lemma 7.
1/2

Proof. By Cauchy-Schwartz’s inequality, and the definition of Aτ and Jτ (δ, ∆) we have

kδk1,n,T ≤ 45 kδTτ k1 + kδTτc k1

√
rτ k∆kF
5C0
5
≤ 4 kδTτ k1 + 4 kδTτ k1 + √
T)

nT log(n∨pc
√
√
rτ k∆kF
≤ 2(C0 + 1) sτ kδTτ k2 + 2C0 √
nT log(n∨pcT )


√
√
rτ
≤ 2(C0 + 1) sτ + 1 kδTτ k2 + √
k∆kF
nT log(n∨pcT )


√
1/2
√
rτ
≤ 2(C0 + 1) sτ + 1 max √
, 1 Jτ κ(δ,∆)
.
0
log(n∨pcT )

On the other hand, by the triangle inequality, the construction of the set Aτ , and Cauchy-Schwartz’s inequality


√
p
r k∆kF
k∆k∗ ≤ C0 nT log(n ∨ pcT ) kδTτ k1 + √ τ
nT log(n∨pcT )


√
p
√
rτ k∆kF
≤
sτ + 1(C0 + 1) nT log(n ∨ pcT ) kδTτ k2 + √
nT log(n∨pcT )


√
1/2
p
√
rτ
√
, 1 Jτ κ(δ,∆)
≤
sτ + 1(C0 + 1) nT log(n ∨ pcT ) max
.
0
log(n∨pcT )

Lemma 10. Let
(η) =

Q̂τ (θ(τ ) + δ, Π(τ ) + ∆) − Q̂τ (θ, Π)−

sup
1/2

(δ,∆)∈Aτ : Jτ

(δ,∆)≤η

Qτ (θ(τ ) + δ, Π(τ ) + ∆) + Qτ (θ(τ ), Π(τ )) ,

39

p
f log(cT + 1)) → ∞. Then for all η > 0
p
√
√
C̃0 ηcT φn (1 + sτ ) max{log(pcT ∨ n), rτ }( n + dT )
√
(η) ≤
,
nT κ0 f 1/2

and {φn } a sequence with φn /(

for some constant C̃0 > 0, with probability at least 1 − αn . Here, the sequence {αn } is independent of η,
and αn → 0.
Proof. Let Ω1 be the event maxj≤p |σ̂j − 1| ≤ 1/4. Then, by Assumption , P (Ω1 ) ≥ 1 − γ . Next let
κ > 0, and f = (δ, ∆) ∈ Aτ and write
Jτ1/2 (δ, ∆) ≤ η}.

F = {(δ, ∆) ∈ Aτ :

Then notice that by Lemmas 4.1 and 4.2 from Yu (1994),


dT X
n X


X
√
Z
(f
)
κ
1
i,t
≤ 2 P  sup √
≥ +
P (η) nT ≥ κ
√
cT
3
nd
f ∈F
T i=1 l=1 t∈H
l
!
 µ
n X
X
Zi,t (f )
1
1
κ
P sup √
+ 2nT
≥
,
√
cT
3
cT
ndT i=1 t∈R
f ∈F
 µ
,
=: A1 + A2 + 2nT c1T

(52)

where
e 0 (θ(τ ) + δ) − (Πi,t (τ ) + ∆i,t )) − ρτ (Yei,t − X
e 0 θ(τ ) − Πi,t (τ )))
Zi,t (f ) = ρτ (Yei,t − X
i,t
i,t

e 0 (θ(τ ) + δ) − (Πi,t (τ ) + ∆i,t )) − ρτ (Yei,t − X
e 0 θ(τ ) − Πi,t (τ ))) .
−E ρτ (Yei,t − X
i,t

i,t

Next we proceed to bound each term in (52). To that end, notice that


2 

dT X
dT
n X
n X
X
X
X
Z
(f
)
1
i,t
 ≤
Var 
E  √
Zi,t (f ) 
√
cT
cT
i=1 l=1 t∈Hl
i=1 l=1
t∈Hl

dT X
n X
2 
X
0
≤
E
X̃i,t δ + ∆i,t
i=1 l=1 t∈Hl

≤

2
nT  1/2
Jτ (δ, ∆) .
f

Let {εi,l }i∈[n], l∈[dT ] be i.i.d Rademacher variables independent of the data.
Therefore, by Lemma 2.3.7 in van der Vaart and Wellner (1996)

P  sup √
f ∈F

1
ndT


dT X
n X
X
Zi,t (f )
≥ κ ≤
√
cT
i=1 l=1 t∈Hl

≤

P

sup
f ∈F

1−

√1
ndT

Pn PdT

12
nT κ2

sup Var(

f ∈F
κ
P(A0 (η) ≥ 12
|Ω1 ) + P(Ωc1 )
1−

72cT η 2
f κ2

l=1 εi,l

i=1

P

t∈Hl

Pn PdT P
i=1

l=1

Zi,t (f )
√
cT

t∈Hl



!
≥

κ
4

Zi,t (f )
√
cT )

,
(53)

40

where
A0 (η) :=
sup √
f ∈F

1
ndT

dT
n X
X
i=1 l=1




e 0 (θ(τ ) + δ) − (Πi,t (τ ) + ∆i,t )) − ρτ (Yei,t − X
e 0 θ(τ ) − Πi,t (τ ))
X ρτ (Yei,t − X
i,t
i,t
 .
εi,l 
√
cT
t∈Hl

Next, note that


0
0
0
e
e
e
e
e
ρτ (Yi,t − Xi,t (θ(τ ) + δ) − (Πi,t (τ ) + ∆i,t )) − ρτ (Yi,t − Xi,t θ(τ ) − Πi,t (τ )) = τ Xi,t δ + ∆i,t
+vi,t (δ, ∆) + wi,t (δ, ∆),
where
|vi,t (δ, ∆)| =

e 0 (θ(τ ) + δ) − (Πi,t (τ ) + ∆i,t ))− − (Yi,t − X
e 0 (θ(τ ) + δ) − Πi,t (τ ))−
(Yei,t − X
i,t
i,t

≤ |∆i,t |.
(54)
and
e 0 θ(τ ) − Πi,t (τ ))−
e 0 (θ(τ ) + δ) − Πi,t (τ ))− − (Yei,t − X
(Yei,t − X
i,t
i,t
0
e
≤ |Xi,t δ|.

|wi,t (δ, ∆)| =

Moreover, notice that by Lemma 9,
{(δ, ∆) ∈ Aτ : Jτ1/2 (δ, ∆) ≤ η} ⊂ {(δ, ∆) ∈ Aτ : kδk1,n,T ≤ ηυ},
where

(
)
√
√
rτ
2(C0 + 1) 1 + sτ
max p
,1 .
υ :=
κ0
log(n ∨ pcT )

Also by Lemma 9, for (δ, ∆) ∈ Aτ
√
k∆k∗ ≤

p
1/2
1 + sτ (C0 + 1)Jτ (δ, ∆) nT max{log(pcT ∨ n), rτ }
,
κ0

and so,
1/2

{(δ,
≤ η} ⊂
n ∆) ∈ Aτ : Jτ (δ, ∆)
o
p
√
(δ, ∆) ∈ Aτ : k∆k∗ ≤ 1 + sτ (C0 + 1)η nT max{log(pcT ∨ n), rτ }/κ0 .

41

(55)

Hence, defining
B10 (η) =

√

cT

√

sup
δ : kδk1,n,T ≤ηυ

1
ndT

dT
n X
X
i=1 l=1


e0 δ
X X
i,t 
,
εi,l 
cT


t∈Hl



dT
n X
X
X ∆i,t
1

√
B20 (η) = cT
sup
εi,l 
√
√
cT
ndT i=1 l=1
∆ : k∆k∗ ≤ 1+sτ (C0 +1)η nT max{log(pcT ∨n),rτ }/κ0
t∈Hl

dT
n X
X
X vi,t (δ, ∆)
√
1
 ,
√
B30 (η) = cT
sup
εi,l 
√
√
c
nd
T
T
∆ : k∆k∗ ≤ 1+sτ (C0 +1)η nT max{log(pcT ∨n),rτ }/κ0
i=1 l=1
t∈Hl


d
n
T
X wi,t (δ, ∆)
√
1 XX
 .
√
sup
B40 (η) = cT
εi,l 
cT
ndT
δ : kδk1,n,T ≤ηυ
√

i=1 l=1

t∈Hl

By union bound we obtain that
P(A0 (η) ≥ κ|Ω1 ) ≤

4
X

P(Bj0 (η) ≥ κ|Ω1 ),

(56)

j=1

so we proceed to bound each term in the right hand side of the inequality above.
First, notice that
B10 (η)

≤ 2cT max

sup

m∈[cT ] δ : kδk1,n,T ≤ηυ

n dT −1
1 X X
e0
√
εi,l X
i, (2lcT +m) δ ,
nT i=1 l=0

and hence by a union bound and the same argument on the proof of Lemma 5 in Belloni and Chernozhukov
(2011), we have that
κ2
√
P(B10 (η) ≥ κ|Ω1 ) ≤ 2pcT exp − 2
4cT (16 2ηυ)2

!
.

(57)

Next we proceed to bound B30 (η), by noticing that
B30 (η) ≤

max
m∈[cT ]

sup
√

√
∆ : k∆k∗ ≤ 1+sτ (C0 +1)η

nT max{log(pcT ∨n),rτ }/κ0

√
n dT −1
cT X X
√
εi,l vi, (2lcT +m) (δ, ∆) .
ndT i=1 l=0

Towards that end we proceed to bound the moment generating function of B30 (η) and the use that to obtain

42

an upper bound on B30 (η). Now fix m ∈ [dT ] and notice that




√
n dX
T −1
X
cT
√
E exp λ
εi,l vi, (2lcT +m) (δ, ∆) 
sup
√
√
nd
T
∆ : k∆k∗ ≤ 1+sτ (C0 +1)η nT max{log(pcT ∨n),rτ }/κ0
i=1 l=0



√
dT
n X
X
c
T
√
≤ E exp λ
sup
εi,l ∆i, (2lcT +m) 
√
√
nd
T i=1 l=1
∆ : k∆k∗ ≤ 1+sτ (C0 +1)η nT max{log(pcT ∨n),rτ }/κ0
 √

λ cT k∆k∗ E(k{εil }k2 )
√
exp
≤ E
sup
√
√
ndT
∆ : k∆k∗ ≤ 1+sτ (C0 +1)η nT max{log(pcT ∨n),rτ }/κ0
 !!
 √
cT k∆k∗ (k{εi,l }k2 − E(k{εi,l }k2 ))
√
exp λ
ndT
p
√ !
√
√
1 + sτ (C0 + 1)c4 ηcT 3 max{log(pcT ∨ n), rτ } n + dT
·
≤ exp λ
κ0


(sτ + 1)(C0 + 1)2 c4 λ2 c2T η 2 max{log(pcT ∨ n), rτ }
exp
,
κ20
(58)
for a positive constant c4 > 0, and where the first inequality holds by Ledoux-Talagrand’s contraction
inequality, the second by the the duality of the spectral and nuclear norms and the triangle inequality, the
third by Theorem 1.2 in Vu (2007) and by basic properties of sub-Gaussian random variables.
Therefore, by Markov’s inequality and (58),
P B30 (η) ≥κ|Ω1




√
n dX
T −1
X
c
T
√
≤ cT max P 
sup
εi,l vi, (2lcT +m) (δ, ∆) ≥ κ
√
√
m∈[cT ]
nd
T i=1 l=0
∆ : k∆k∗ ≤ 1+sτ (C0 +1)η nT max{log(pcT ∨n),rτ }/κ0
"
 √

√
√ √
1+sτ (C0 +1)c4 ηcT 3 max{log(pcT ∨n),rτ }( n+ dT )
≤ inf exp (−λκ) exp λ
·
κ0
λ>0

#
(1 + sτ )(C0 + 1)2 c4 λ2 c2T η 2 max{log(pcT ∨ n), rτ }
exp
+ log cT
κ20


κκ0
√
≤ c5 exp − √
+ log cT ,
√ √
1+sτ (C0 +1)ηcT 3 max{log(pcT ∨n),rτ }( n+ dT )
(59)
for a positive constant c5 > 0.
Furthermore, we observe that
B20 (η)

≤ max
m∈[dT ]

sup

√
√
∆ : k∆k∗ ≤ 1+sτ (C0 +1)η nT max{log(pcT ∨n),rτ }/κ0

43

√
n dT −1
cT X X
√
εi,l ∆i, (2lcT +m) .
ndT i=1 l=0

Hence, with the same argument for bounding B30 (η), we have
P B20 (η) ≥ κ|Ω1 ≤ c5 exp − √

!

κκ0



1 + sτ (C0 + 1)ηcT

p
√  + log cT
√
3 max{log(pcT ∨ n), rτ } n + dT
(60)

Finally, we proceed to bound B40 (η). To that end, notice that
B40 (η)

≤ max

sup

m∈[dT ] δ : kδk1,n,T ≤ηυ

√
n dT −1
cT X X
√
εi,l wi, (2lcT +m) (δ, ∆) ,
ndT i=1 l=0

and by (55) and Ledoux-Talagrand’s inequality, as in (57), we obtain

P(B40 (η)

κ2
√
≥ κ|Ω1 ) ≤ 2pcT exp − 2
4cT (16 2ηυ)2

!
.

(61)

Therefore, letting
κ =

ηcT φn (1 + C0 )2

p
√
√
(1 + sτ ) max{log(pcT ∨ n), rτ }( n + dT )
κ0 f 1/2

,

and repeating the argument above for bounding A2 in (52), combining (52), (53), (56), (57), (59), (60) and
(61), we obtain that
P((η) ≥

√κ )
nT

≤ 5




√
φ2 max{log(pcT ∨n),rτ } √
2 +2c exp −C
γ+4 exp max{log(pcT ∨n),rτ }−C1 n
(
n+
d
)
5
2
T
f
1−

+ nT



1
cT

µ

3κ2
√0
√
cT φn (sτ +1) (1+C0 )2 ( n+ dT )2 max{log(pcT ∨n),rτ }

,

for some positive constants C1 and C2 .

B.2

Proof of Theorem 1

Proof. Recall from Lemma 7, our choices of ν1 and ν2 are
s
p 
cT log(max{n, pcT }) √
n + dT ,
ν1 = C00
ndT
and
ν2 =

p 
200cT √
n + dT ,
nT

for C00 = 9/(1 − c0 ), and c0 as in Lemma 7.

44

2φn
f 1/2



.

Let
p
√
√
8φn (C00 (1 + C0 ) + C̃0 + 200(1 + C0 )) cT (1 + sτ ) max{log(pcT ∨ n), rτ }( n + dT )
η =
, (62)
√
ndT κ0 f 1/2
for C0 as in Lemma 7, and C̃0 as in Lemma 10.
Throughout we assume that the following events happen:
• Ω1 := the event that (θ̂(τ ) − θ(τ ), Π̂(τ ) − Π(τ )) ∈ Aτ .
• Ω2 := the event for which the upper bound on (η) in Lemma 10 holds.
Suppose that
|Jτ1/2 (θ̂(τ ) − θ(τ ), Π̂(τ ) − Π(τ ))| > η.

(63)

Then, by the convexity of Aτ , and of the objective Q̂ with its constraint, we obtain that
0 >

Q̂τ (θ(τ ) + δ, Π(τ ) + ∆) − Q̂(θ(τ ), Π(τ )) + ν1 [kθ(τ ) + δk1,n,T − kθ(τ )k1,n,T ]

min
1/2

(δ,∆)∈Aτ : |Jτ

(δ,∆)|=η

+ν2 [kΠ(τ ) + ∆k∗ − kΠ(τ )k∗ ]
Moreover, by Lemma 9 and the triangle inequality,
kθ(τ )k1,n,T − kθ(τ ) + δk1,n,T

≤ kδTτ k1,n,T
1/2
√
≤ 2(1 + C0 ) 1 + sτ Jτ κ(δ,∆)
max
0



√

√

rτ

log(n∨pcT )


,1 ,

and
kΠ(τ )k∗ − kΠ(τ ) + ∆k∗ ≤ k∆k∗

1/2
p
√
≤ (1 + C0 ) 1 + sτ nT max{log(pcT ∨ n), rτ } Jτ κ(δ,∆)
.
0

Therefore,
0 >

min
1/2

(δ,∆)∈Aτ : |Jτ

=

Q̂(θ(τ ) + δ, ∆ + Π(τ ))−
(δ,∆)|=η



√
1/2
√
rτ
Jτ (δ,∆)
max √
,1 ,
Q̂(θ(τ ), Π(τ )) − 2ν1 (1 + C0 ) 1 + sτ
κ0
log(n∨pcT )
1/2
p
√
Jτ (δ,∆)
−ν
" 2 (1 + C0 ) 1 + sτ nT max{log(pcT ∨ n), rτ } κ0
Q̂τ (θ(τ ) + δ, ∆ + Π(τ )) − Q̂(θ(τ ), Π(τ ))

min
1/2

(δ,∆)∈Aτ : |Jτ

(δ,∆)|=η

−Q(θ(τ ) + δ, ∆ + Π(τ )) + Q(θ(τ ), Π(τ ))
+Q(θ(τ ) + δ, ∆ + Π(τ )) − Q(θ(τ ), Π(τ )) −
q
1/2
√
√
T ∨n),rτ }
2C00 (1 + C0 ) cT (1+sτ ) max{log(pc
( n + dT ) Jτ κ(δ,∆)
−
ndT
0
#
1/2
p
√ √
Jτ (δ,∆)
200cT √
n + dT
1 + sτ (1 + C0 ) nT max{log(pcT ∨ n), rτ } κ0
nT
45

≥

min
1/2

(δ,∆)∈Aτ : |Jτ

≥

η2
∧ (ηq) − [2C00 (1 + C0 ) + 200(C0 + 1)]
4
√
√ √

C̃0 ηφn

≥

Q(θ(τ ) + δ, ∆ + Π(τ )) − Q(θ(τ ), Π(τ ))
q
1/2
√
√
Jτ (δ,∆)
T ∨n),rτ }
−[2C00 (1 + C0 ) + 200(C0 + 1)] cT (1+sτ ) max{log(pc
(
n
+
d
)
T
nd
κ0
T
p
√
√
C̃0 ηφn cT (1 + sτ ) max{log(pcT ∨ n), rτ }( n + dT )
−
√
ndT κ0 f 1/2

(δ,∆)|=η

η2
−
4

cT sτ max{log(pcT ∨n),rτ }( n+ dT )
√
ndT κ0 f 1/2
0
2ηφn (C0 (1 + C0 ) + C̃0 + 200(C0

s

p
cT (1 + sτ ) max{log(pcT ∨ n), rτ } √
η
( n + dT ) −
ndT
κ0

p
√
√
+ 1)) cT (1 + sτ ) max{log(pcT ∨ n), rτ }( n + dT )
√
ndT κ0 f 1/2

= 0,
(64)
where the the second inequality follows from Lemma 10, the third from Lemma 8, the fourth from our
choice of η and (25), and the equality also from our choice of η. Hence, (64) leads to a contradiction which
shows that (63) cannot happen in the first place. As a result, by Assumption 3,
kΠ̂(τ ) − Π(τ )kF
1 1/2
η
√
≤
|Jτ (θ̂(τ ) − θ(τ ), Π̂(τ ) − Π(τ ))| ≤
,
κ0
κ0
nT
which holds with probability approaching one.
To conclude the proof, let δ̂ = θ̂ − θ and notice that
kδ̂(Tτ ∪T τ (δ̂,m))c k2

≤
≤
≤
≤

which implies
kδ̂k ≤
≤

X kδ̂T c k21
τ
k2

k≥m+1
kδ̂Tτc k21

m"
#
rτ kΠ(τ ) − Π̂(τ )k2F
4C0
2
kδ̂Tτ k1 +
m
nT log(pcT ∨ n)
"
#
2
r
kΠ(τ
)
−
Π̂(τ
)k
4C0
τ
F
sτ kδ̂Tτ ∪T τ (δ̂,m) k2 +
,
m
nT log(pcT ∨ n)



√
rτ kΠ(τ )−Π̂(τ )kF
√
1 + 2C0 m
kδ̂Tτ ∪T τ (δ̂,m) k +
nT log(cT p∨n)
1/2
p sτ 
Jτ (δ̂,Π̂(τ ) − Π(τ ))
1 + 2C0 m ,
κm
p sτ 

and the result follows.

46

B.3

Proof of Theorem 3

Lemma 11. Suppose that Assumptions 1–2 and 4 hold. Let
n T
2 XX
ν1 =
kXi,t k∞ .
nT
i=1 t=1

and
ν2 =

p 
200cT √
n + dT .
nT

We have that
(Π̂(τ ) − Π(τ ) − Xθ(τ )) ∈ A0τ ,
with probability approaching one, where
(
A0τ =

)

√

∆ ∈ Rn×T : k∆k∗ ≤ c0 rτ (k∆kF + kξk∗ ) , k∆k∞ ≤ c1 ,

and c0 and c1 are positive constants that depend on τ . Furthermore, θ̂(τ ) = 0.
Proof. First, we observe that C in the statement of Theorem 3 can be take as C = kX 0 θ(τ ) + Π(τ )k∞ . And
so,
Xθ(τ ) + Π(τ ) − Π̂(τ )
≤ 2C =: c1 .
∞

Next, notice that for any Π̌ ∈ Rn×T and θ̌ ∈ Rp ,
Q̂τ (0, Π̌) − Q̂τ (θ̌, Π̌) − ν1 kθ̌k1,n,T

≤

n T
1 XX 0
|Xi,t θ̌| − ν1 kθ̌k1
nT

≤

n T
1 XX 0
|Xi,t θ̌| − ν1 kθ̌k1
nT

≤

n T
1 XX
0
k∞ kθ̌k1 − ν1 kθ̌k1
kXi,t
nT

i=1 t=1

i=1 t=1

i=1 t=1

< 0,
where the first inequality follows since ρτ is a contraction map. Therefore, θ̂(τ ) = 0. Furthermore, by

47

Lemma 5, we have


0 ≤ Q̂τ (0, Xθ(τ ) + Π(τ )) − Q̂τ (0, Π̂(τ )) + ν2 kXθ(τ ) + Π(τ )k∗ − kΠ̂(τ )k∗
n T


1 XX
ai,t (Xθ(τ ) + Π(τ )) − Π̂(τ )
≤
nT
i=1
 t=1

+ ν2 kXθ(τ ) + Π(τ )k∗ − kΠ̂(τ )k∗
!
n T
1 XX
˜ i,t
sup
≤ kXθ(τ ) + Π(τ ) − Π̂(τ )k∗
ai,t ∆
nT
˜
k∆k∗ ≤1 
i=1 t=1

+ ν2 kXθ(τ ) + Π(τ )k∗ − kΠ̂(τ )k∗

p 
200cT √
≤
n + dT
kXθ(τ ) + Π(τ ) − Π̂(τ )k∗ + kXθ(τ ) + Π(τ )k∗ − kΠ̂(τ )k∗
nT
p 
100cT √
−
n + dT kXθ(τ ) + Π(τ ) − Π̂(τ )k∗
nT

p 
200cT √
n + dT
≤
kXθ(τ ) + Π(τ ) + ξ − Π̂(τ )k∗ + kXθ(τ ) + Π(τ ) + ξk∗ − kΠ̂(τ )k∗
nT
p 
p 
100cT √
400cT √
n + dT kξk∗ −
n + dT kXθ(τ ) + Π(τ ) − Π̂(τ )k∗
+
nT
√
p √
c1 cT nT
≤
n + dT
rτ kXθ(τ ) + Π(τ ) + ξ − Π̂(τ )kF
nT
p 
p 
400cT √
100cT √
+
n + dT kξk∗ −
n + dT kXθ(τ ) + Π(τ ) − Π̂(τ )k∗
nT
nT
for some positive constant c1 ,

Lemma 12. Let
0 (η) =

Q̂τ (0, Xθ(τ ) + Π(τ ) + ∆) − Q̂τ (0, Xθ(τ ) + Π(τ ))−

sup
1/2

(δ,∆)∈Aτ : Jτ

(δ,∆)≤η

Qτ (0, Xθ(τ ) + Π(τ ) + ∆) + Qτ (0, Xθ(τ ) + Π(τ )) ,
and {φn } a sequence with φn /(

p
f log(cT + 1)) → ∞. Then for all η > 0
√
√ √
C̃0 ηcT φn rτ ( n + dT )
√
 (η) ≤
,
nT f
0

for some constant C̃0 > 0, with probability at least 1 − αn . Here, the sequence {αn } is independent of η,
and αn → 0.
Proof. This follows similarly to the proof of Lemma 10.

48

Lemma 13. Let
A00τ =
with



∆ ∈ A0τ : q(∆) ≥ 2η0 , ∆ 6= 0 ,

√
√ √
C̃1 cT φn rτ ( n + dT )
√
η0 =
,
nT f

for an appropriate constant C̃1 > 0, and
3/2
3f
q(∆) =
2 f¯0



1
nT

3/2
2
(∆
)
i,t
t=1
i=1
.
Pn PT
3
i=1
t=1 |∆i,t |

Pn PT

1
nT

Under Assumptions 1-2 and 4, for any ∆ ∈ A00τ we have that
(
Qτ (0, Xθ(τ ) + Π(τ ) + ∆) − Qτ (0, Xθ(τ ) + Π(τ )) ≥ min

f k∆k2 2ηf 1/2 k∆k
√
,
4nT
nT

)
.

Proof. This follows as the proof of Lemma 8.
The proof of Theorem 3 proceeds by exploiting Lemmas 11 and 13. By Lemma 11, we have that
ˆ := Π̂(τ ) − Xθ(τ ) − Π(τ ) ∈ A0 ,
∆
τ
ˆ ∈
with high probability. Therefore, we assume that (65) holds. Hence, if ∆
/ A00τ , then

P P
T
n
ˆ i,t |3
0
0 ˆ
|
∆
4
η
t=1
i=1
1
f
4f k∆k
∞η
ˆ F <

P P
√ k∆k
≤
.
T
n
ˆ2
nT
f 3/2
3f 3/2
∆
3
i=1

t=1

(65)

(66)

i,t

ˆ ∈ A00τ , then we proceed as in the proof of Theorem 1 by exploiting Lemma 12, and treating
If ∆
Xθ(τ ) + Π(τ ) as the latent factors matrix, the design matrix as the matrix zero, A00τ as Aτ , and
κ0 = f 1/2 ,
in Assumption 3. This leads to
1
ˆ F ≤ η,
√ k∆k
nT
and the claim in Theorem 3 follows combining (66) and (67).

49

(67)

B.4

Proof of Corollary 2

First notice that by Theorem 1 and Theorem 3 in Yu et al. (2014),




√
(σ1 (τ ) + rτ Err)Err
˜
.
v := max min kĝ(τ )O − g(τ )kF , min kλ̂(τ )O − λ̃(τ )kF = OP
O∈Orτ
O∈Orτ
(σrτ −1 (τ ))2 − (σrτ (τ ))2
(68)
Furthermore,
kλ̂(τ ) − λ(τ )k2F
nT

=
≤
≤

rτ
1 X
kλ·,j (τ ) − λ̂·,j (τ )k2
nT
j=1
r
rτ
X
2 X
2 τ
˜
2
(σj − σ̂j ) +
σj2 kλ̂j (τ ) − λ̃j (τ )k2
nT
nT
j=1
j=1
r
rτ
τ
2 X
2 X
2
σ
˜
1
(σj − σ̂j )2 +
kλ̂j (τ ) − λ̃j (τ )k2
nT
nT
j=1

j=1

2 rτ
2 σ12 ˜
≤
(σ1 − σ̂1 )2 +
kλ̂(τ ) − λ̃(τ )k2F
nT
nT
2 σ12 ˜
2 rτ
kΠ(τ ) − Π̂(τ )k2F +
kλ̂(τ ) − λ̃(τ )k2F
≤
nT
nT

!
1
rτ φ2n cT (1 + sτ ) max{log(pcT ∨ n), rτ } 1
+
+
= OP
n dT
κ40 f
 2

√
σ1 (σ1 (τ ) + rτ Err)2 Err2
OP
,
nT ((σrτ −1 (τ ))2 − (σrτ (τ ))2 )2
where the third inequality follows from Weyl’s inequality, and the last one from (68).

50

Table 7: Firm Characteristics Construction.
Characteristics

Name

Construction

acc

Working capital accruals

agr
beta

Asset growth
Beta

bm

Book-to-market

chinv

Change in inventory

chmom

Change in 6-month momentum

dolvol

Dollar trading volume

dy

Dividend to price

egr

Earnings announcement return

ep

Earnings to price

gma

Gross profitability

idiovol

Idiosyncratic return volatility

ill

Illiquidity (Amihud)

invest

Capital expenditures and inventory

lev

Leverage

lgr
mom1m

Growth in long-term debt
1-month momentum

Annual income before extraordinary items (ib)
minus operating cash flows (oancf) divided by
average total assets (at)
Annual percent change in total assets (at)
Estimated market beta from weekly returns and
equal weighted market returns for 3 years
Book value of equity (ceq) divided by end of
fiscal year-end market capitalization
Change in inventory (inv) scaled by average total assets (at)
Cumulative returns from months t-6 to t-1 minus months t-12 to t-7
Natural log of trading volume times price per
share from month t-2
Total dividends (dvt) divided by market capitalization at fiscal year-end
Annual percent change in book value of equity
(ceq)
Annual income before extraordinary items (ib)
divided by end of fiscal year market cap
Revenues (revt) minus cost of goods sold (cogs)
divided by lagged total assets (at)
Standard deviation of residuals of weekly returns on weekly equal weighted market returns
for 3 years prior to month end
Average of daily (absolute return / dollar volume).
Annual change in gross property, plant, and
equipment (ppegt) + annual change in inventories (invt) all scaled by lagged total assets (at)
Annual change in gross property, plant, and
equipment (ppegt) + annual change in inventories (invt) all scaled by lagged total assets (at)
Annual percent change in total liabilities (lt)
1-month cumulative return

51

Continuation of Table 7
mom6m

6-month momentum

5-month cumulative returns ending one month
before month end
mve
Size
Natural log of market capitalization at end of
month t-1
operprof
Operating profitability
Revenue minus cost of goods sold - SG&A expense - interest expense divided by lagged common shareholders’ equity
range
Range of stock price
Monthly average of daily price range: (highlow)/((high+low)/2) (alternative measure of
volatility)
retvol
Return volatility
Standard deviation of daily returns from month
t-1
roaq
Return on assets
Income before extraordinary items (ibq) divided by one quarter lagged total assets (atq)
roeq
Return on equity
Earnings before extraordinary items divided by
lagged common shareholders’ equity
sue
Unexpected quarterly earnings
Unexpected quarterly earnings divided by
fiscal-quarter-end market cap. Unexpected
earnings is I/B/E/S actual earnings minus median forecasted earnings if available, else it
is the seasonally differenced quarterly earnings before extraordinary items from Compustat quarterly file
turn
Share turnover
Average monthly trading volume for most recent 3 months scaled by number of shares outstanding in current month
Note: Estimated under different values of turning parameter ν2 , when ν1 = 10−5 is fixed. The results are
reported for quantiles 10%, 50% and 90%.

52

