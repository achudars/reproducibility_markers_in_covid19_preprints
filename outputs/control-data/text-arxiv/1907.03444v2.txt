1

Capacity and Algorithms for a Cognitive
Network with Primary-Secondary User
Cooperation
arXiv:1907.03444v2 [cs.IT] 28 Aug 2019

Athanasios Papadopoulos, Student Member, IEEE, Nestor D. Chatzidiamantis, Member, IEEE,
and Leonidas Georgiadis, Senior Member, IEEE

Abstract
In this work, we examine cognitive radio networks, where secondary users may act as relays for messages sent by
the primary user, hence offering performance improvement of primary transmissions, while at the same time obtaining
more transmission opportunities for their own data. In particular, assuming the broadcast packet erasure model with
feedback, we investigate the capacity of the fundamental cooperative cognitive radio network which consists of one
primary and one secondary transmitter-receiver pairs. The primary transmitter is the owner of the channel and as such,
we intend to keep its operations simple and to avoid increasing its storage requirements. Specifically, the primary
transmitter does not receive data sent by the secondary transmitter and does not perform any coding operations. The
only requirement on the primary transmitter is to listen to public feedback and take appropriate scheduling actions. On
the other hand, the secondary transmitter can overhear primary transmissions and is allowed to perform any coding
operations. We develop an outer bound to the capacity of the fundamental cooperative cognitive radio network under
consideration. Then, we propose a coding-scheduling algorithm suitable for this type of networks, which involves only
XOR network coding operations. The complexity of the scheduling decisions of the proposed algorithm depends on
the channel statistical parameters and three cases, depending on the relations between channel erasure probabilities,
are distinguished. For the first two cases the rate region of the proposed algorithm coincides with the developed
capacity outer bound, hence the algorithm is capacity achieving. For the third case, the rate region of the proposed
algorithm is not identical to the outer bound; however, numerical results show that it is fairly close to the derived
outer bound for a wide range of the statistical parameters of the system.
Index Terms
Cognitive radio networks, primary user, secondary user, cooperation, capacity, coding algorithms, scheduling
algorithms, network coding.
Athanasios Papadopoulos, Nestor D. Chatzidiamantis and Leonidas Georgiadis are with Department of Electrical and Computer Engineering,
Aristotle University of Thessaloniki, Thessaloniki, Greece, Emails: {athanapg,nestoras,leonid}@auth.gr

August 29, 2019

DRAFT

2

I. I NTRODUCTION
Cognitive networks attracted a lot of attention in recent years due to their potential for improving spectral efficiency
[1]. In this type of networks, unlicensed users, also known as secondary users, are allowed to communicate with each
other utilizing the licensed spectrum, thus taking advantage of the underutilized shared spectrum, while maintaining
limited or no interference to the licensed users, also known as primary users.
Initial designs of cognitive radio networks assumed that there is no interaction between primary and secondary
users (see [2] and the references therein). However, it was soon realized that by allowing secondary users to
cooperate with primary users, several benefits for both types of users arise. These benefits stem from the fact that
by allowing secondary users to relay primary transmissions, the channel between the secondary transmitter and
primary receiver can be exploited, thus, increasing the primary user’s effective transmission rate, as well as offering
more transmission opportunities to secondary user. This type of cognitive radio networks are referred to in the
literature as cooperative cognitive radio networks.
Due to their advantages, cooperative cognitive radio networks have gained a lot of attention in recent years.
Physical layer cooperation between primary and secondary users was examined in [3], while non-orthogonal multiple
access techniques based on successive interference cancellation were proposed in [4]. Queuing theoretic analysis
and transmission protocol design for cooperative cognitive radio networks were presented in [5], [6], [7], [8].
Specifically, a cooperative transmission protocol for cognitive radio networks where the secondary transmitter acts
as a relay for primary user’s transmissions was initially presented in [5] and the benefits of such cooperation for
both types of users were investigated. In [6], cooperative cognitive radio networks with multiple secondary users
were investigated and advanced relaying techniques which involved physical layer coding between primary and
secondary transmissions were suggested. Cooperative transmission policies which take into account the available
power resources at the secondary transmitter in order for the latter to decide whether to cooperate or not, have been
presented in [7], [8].
Network coding has been applied in cooperative cognitive radio networks as a means to increase capacity for
both type of users (see [9] and the references therein). However, in most of these works, the network coding
operations that were performed by secondary users (acting as relays) involved only primary user’s packets. Relatively
recently, network coding schemes which involved both primary and secondary users packets have been suggested
as an effective means of cooperation in cooperative cognitive radio networks [10], [11], [12], [13]. Specifically,
in [10] and [11] a first attempt was made to design transmission algorithms where secondary users employ
network coding between their data and the overheard primary transmissions; however, the presented algorithms
leave room for improvement by exploiting more opportunities for transmitting network coded packets. More
efficient similar network coding based transmission algorithms for cooperative cognitive radio networks were
presented in our previous works, [12], [13], whose performance was investigated based on queuing theory. While
the presented algorithms offered an enhancement of the primary-secondary user throughput region compared to
previously proposed cooperation schemes, this approach did not address the problem of optimality or near optimality
of the proposed algorithms in terms of achievable throughput region. This issue is addressed in the current work

August 29, 2019

DRAFT

3

using an information theoretic approach.
The current work focuses on investigating the capacity region of the fundamental cooperative cognitive radio
network when the channel is modeled as broadcast erasure with feedback, that models well the network at the
MAC layer, and aims on designing efficient coding-scheduling algorithms - transmission algorithms for short. In
the past, the capacity of several wireless communications systems setups has been investigated under the assumption
of erasure channel model [14], [15], [16], [17], [18], [19]. Specifically, the capacity of broadcast erasure networks
was investigated in [14] and [15], while the capacity with side information available to the receivers has been
characterized in [16]. Moreover, the capacity region for the fully-connected 3-node packet erasure network is
investigated in [17], while a simple and a more complicated butterfly erasure network is analyzed in [18] and [19],
respectively. A related channel model is investigated in [20], where a single source broadcast erasure channel with
two receivers and a relay is examined; the source has two independent messages, one for each destination and the
messages may be delivered to the receivers either directly or through the relay, using Linear Network Coding.
The major difference between the previous setups and the setup considered in this work stems from the requirements imposed by the fact that the primary transmitter, as owner of the channel, has certain privileges. Specifically,
motivated by our previous works, [12], [13], we require that the primary transmitter does not receive any data sent
by the secondary transmitter and, in order to avoid increasing its complexity and memory requirements, does not
perform any coding operations; in contrast the secondary transmitter may perform arbitrary coding operations. The
only requirement on the primary transmitter is to listen to public feedback and take appropriate scheduling actions.
Based on the above, the contribution of the paper is summarized as follows:
1) We consider a basic cognitive radio network setup which is composed by one primary and one secondary
transmitter-receiver pairs. All the underlying channels are considered to be broadcast packet erasure channels
with public feedback. The primary transmitter does not receive any data transmitted by the secondary
transmitter and does not perform coding operations; it only listens to the feedback and takes scheduling
actions. On the other hand, the secondary transmitter can overhear primary transmissions and is allowed to
perform network coding operations based on its own packets as well as the overheard packets during primary
transmissions. The objective of the presented analysis is to maximize the secondary user’s transmission rate
without reducing primary user’s channel capacity.
2) We develop an outer bound to the capacity region of the fundamental cooperative cognitive radio network
under consideration.
3) We propose a transmission algorithm suitable for the cooperative cognitive radio system under consideration.
The proposed algorithm involves only XOR network coding operations, while the complexity of scheduling
decisions depends on channel statistical parameters. Specifically we consider three cases depending on relations
between channel erasure probabilities. For the first two cases the rate region of the proposed algorithm
coincides with the developed capacity outer bound, hence the algorithm is capacity achieving. For the third
case, involving more complex scheduling decisions, the rate region of the proposed algorithm is not identical
to the outer bound, but in general it is fairly close to it.

August 29, 2019

DRAFT

4

The remainder of the paper is organized as follows. In Section II we provide the notation that is used in the analysis
that follows along with the system model studied in this work. In Section III we present the main results of this
paper which include the derived outer bound, the description of the cases where this outer bound is in fact the
capacity region of the system and an inner bound for the case where the system capacity is not known. Section IV
describes the proposed transmission algorithm and investigates its performance in terms of achievable rate region.
Section V provides concluding remarks and suggestions for future research. Proofs of the main results are provided
in the Appendix A.
II. N OTATION , S YSTEM MODEL AND C HANNEL C ODES
A. Notation
We use the following notation.
•

Sets are denoted by calligraphic letters e.g., F.

•

Random variables are denoted by capital letters and their values by small letters.

•

Vectors are denoted by bold letters.

•

For a sequence Y (t), t = 1, · · · , we denote Y t = (Y (1), · · · , Y (t)) . Also, X ∈ Y t means that X = Y (s)
for some s ∈ {1, · · · , t}.

•

If Y = (Y1 , · · · , Yn ) and S ⊆ {1, 2, · · · , n} , we denote by Y S = (Yi : i ∈ S) the vector of coordinates of
Y with index in the set S. If S = ∅ by convection we set Y S = c, a constant.

•

For i ∈ {1, 2} we denote by ic the element in the set {1, 2} − {i}.

•

For random variables X, Y, the notation X ⊥ Y means that the random variables are independent.

•

A sentence between brackets next to a formula provides explanation of the relations involved in the formula,
e.g., f (x) = y "since ... ".

B. System Model
We consider the four-node cognitive radio system model depicted in Fig. 1. The system consists of two (transmitter,
receiver) pairs (1,3), (2,4). Pair (1,3) - odd numbers- represents the primary channel. Node 1 is the primary transmitter
who is the licensed owner of the channel . Node 2 is the secondary transmitter; this node does not have any licensed
spectrum and seeks transmission opportunities on the primary channel in order to deliver data to secondary receiver,
node 4.
Let
N1 = {2, 3, 4} , N2 = {3, 4} .
Erasure events
Erasure events are characterized by a sequence of tuples of 0-1 random variables,
Z (t) = (Z 1 (t) , Z 2 (t)) , ([Z1j (t), j ∈ N1 ] , [Z2j (t), j ∈ N2 ]) , t = 1, · · · ,
with the following interpretation. A symbol transmitted by node i at time t is received correctly by node j if
Zij (t) = 1, and erased at node j if Zij (t) = 0.

August 29, 2019

DRAFT

5

Fig. 1. System Model

We assume that the tuples Z (t) = (Z 1 (t) , Z 2 (t)) , t = 1, · · · , are independent, however, for given t, the
random variables Zij (t) can be arbitrarily dependent. We denote by iS , i ∈ {1, 2} , S ⊆ Ni , the probability that
a message transmitted by node i is erased at all nodes in the set S, i.e., Z1j (t) = 0 for all j ∈ S. Let Zi =

z = (zl )l∈Ni : zl ∈ {0, 1} , be the set of possible erasure events when node i transmits, and for S ⊆ Ni , S =
6 ∅,
ZSi = {z ∈ Zi : ∃l ∈ S, zl = 1} ,
be the set of all vectors in Zi for which at least one component with index in set S is equal with 1.
Feedback and Scheduling
We assume that after a transmission by node 1 (2) a 1-0 feedback is sent by each node in N1 (N2 ) to the
rest of the nodes, indicating correct reception-1 or erasure-0. Hence, if node i transmits at time t, at the end of
transmission all nodes know Z i (t) .
At each time t = 0, · · · , only one of the nodes in {1, 2} is scheduled to transmit. This scheduling depends only
on node feedback. Specifically, denoting by σ(t) ∈ {1, 2} the index of the node scheduled for transmission at time
t, we set σ(1) = 1 (or σ(1) = 2) and

σ(t) = σ Z t−1 , t = 2, · · · .

(1)

t
where with a slight abuse of notation we denote Z t , Z σ(s) (s) s=1 .
Transmission and reception symbol alphabets
The transmitted symbols, called “packets”, belong to a finite field F.
We denote by Xi (t) ∈ F the symbol transmitted by node i at time t - if node i is not scheduled for transmission
at time t, i.e., σ (t) 6= i we set Xi (t) = η (null).
We denote by Yij (t) the symbol received by node j ∈ Ni if node i transmits at time t, where erasure is indicated
by the symbol ε - if node i is not scheduled for transmission at time t, we set Yij (t) = η.
The following facts follow directly from the definitions.
Fact 1. Let i ∈ {1, 2} , S1 ⊆ N1 , S2 ⊆ N2 , Si =
6 ∅. For any t,

1) Z (t) is independent of σ(t) = σ Z t−1 , t ≥ 2.

August 29, 2019

DRAFT

6

2) Let σ (t) = i and Z iSi (t) =z ∈
/ ZSi i . Then, Y iSi (t) = ε, Y ic Sic (t) = η.
3) Let σ (t) = i and Z iSi (t) =z ∈ ZSi i . Then, Xi (t) = f (Y iSi ) (specifically, Xi (t) = Yil (t) , where l is any
coordinate of z with zl = 1) and Y ic Sic (t) = η. Moreover, the function f (·) is one-to-one.
C. Channel Codes and Channel Capacity
A channel code Cn of rate vector R = (R1 , R2 ) , Ri ≥ 0, consists of the following:
•

n symbol transmissions.

•


Messages, (W 1,n , W 2,n ). Message W 1,n consists of ki,n = ki,n = dnRi e packets, i.e. W i,n = Wi,1 , · · · , Wi,ki,n ,
Wi,l ∈ F, that need to be delivered to node 3 if i = 1, and node 4 if i = 2. Messages are independent of
feedback variables (Z 1 (t) , Z 2 (t)) , t = 1, · · · . We assume that each packet is a uniformly selected element
from the finite field F and that packets are independent.

•

Encoders that specify the symbol to be transmitted by one of the nodes 1, 2, as follows.
– If σ(t) = 2, then


t−1
X2 (t) = f2,n W 2,n , Y t−1
,
Z
,
1{2}
where f2 is an arbitrary function. Thus the secondary transmitter can perform any coding operation that
depends on its own packets, the packets received by primary node 1 and the channel feedback.
– If σ(t) = 1, then node 1 transmits one of the packets in W 1,n , where the index Jn (t) of the packet
to be transmitted depends only on channel feedback, i.e., Jn (1) is selected arbitrarily, and for t ≥ 2,

Jn (t) = Jn Z t−1 . Hence,
X1 (t) = W1,Jn (t) .
Thus the primary node 1 does not perform coding operations, and only schedules packets according to
received feedback. For convenience in the description below, whenever σ(t) = 2, we define Jn (t) = η.

•

Decoders gj,n (Y n1{j} , Y n2{j} , Z n ), for receivers j ∈ {3, 4} . Within n channel uses, receiver j estimates the
message transmitted by its intended transmitter (j − 2),
Ŵ j−2,n = gj,n (Y n1{j} , Y n2{j} , Z n ).

(2)

Thus the channel code Cn is fully specified by the tuple (n, dnR1 e , dnR2 e , σ, Jn , f2,n , g3,n , g4,n ). The probability
of erroneous decoding of code Cn is λn = Pr(

∪

i∈{1,2}

{Ŵi,n 6= Wi,n }). A vector rate R is called achievable under

the sequence of codes Cn if for this rate vector, limn→∞ λn = 0. In this case, we also say that the sequence of
code Cn achieves rate R. A rate vector R is achievable under a class of codes C if there is a sequence of codes in
C that achieves R. The closure of the set of rate vectors R that are achievable under C constitutes the rate region
of C . The capacity region of the channel, C, is the closure of the set of all achievable rates under the class of all
codes.

August 29, 2019

DRAFT

7

III. M AIN R ESULTS
In this section we present the main results of the paper. Since the Primary transmitter is the owner of the channel
and our intention is not to degrade its performance, we concentrate on cases where cooperation has the potential
of increasing the Primary rate, i.e., 13 ≥ 23 . The next theorem provides an outer bound to system capacity.
Theorem 2. Let 13 ≥ 23 . If (R1 , R2 ) is achievable, then (R1 , R2 ) ∈ R where R is the region defined by,
R2
R1
+
≤ 1 − G − S − U,
1 − 123
1 − 24




13 − 123
R2
1 − 13
1
R1 +
≤1−G−S−U +
(G + S) ,
+
(1 − 23 ) (1 − 123 ) 1 − 123
1 − 234
1 − 23


134 − 1234
1
1 − 134
1 − 14
1
R
+
R
≤
1
−
G
−
S
−
U
+
G
+
(S + U )
+
1
2
(1 − 1234 ) (1 − 234 ) 1 − 123
1 − 24
1 − 234
1 − 24

(3)
(4)
(5)

G ≥ 0, S ≥ 0, U ≥ 0, Ri ≥ 0, i ∈ {1, 2} .
Proof: The proof can be found in Appendix A.
The next corollary provides a more concise description of the outer bound in Theorem 2.
Corollary 3. If 13 ≥ 23 , the region R can be described as follows depending on system erasure probabilities:
1) If

max

1 − 134 1 − 14
,
1 − 234 1 − 24


≤ 1,

then R = R1 where R1 is defined by the following inequalities


13 − 123
R2
1
R1 +
≤ 1,
+
(1 − 23 ) (1 − 123 ) 1 − 123
1 − 234


1
134 − 1234
1
R1 +
R2 ≤ 1,
+
1
1
2
(1 − 234 ) (1 − 34 ) 1 − 23
1 − 24

(6)

(7)
(8)

Ri ≥ 0, i ∈ {1, 2} .
2) If

max

1 − 134 1 − 14
,
1 − 234 1 − 24


=

1 − 134
> 1,
1 − 234

(9)

then R = R2 where R2 is defined by the following inequalities,
134 − 1234
R1 ,
(1 − 1234 ) (1 − 134 )


1 − 13
≤1−G+
G,
1 − 23

G≤


13 − 123
1
R2
+
R1 +
(1 − 23 ) (1 − 123 ) 1 − 123
1 − 234


134 − 1234
1
1
1 − 134
+
R1 +
R2 ≤ 1 − G +
G,
1
2
1
2
(1 − 234 ) (1 − 34 ) 1 − 23
1 − 4
1 − 234


(10)
(11)
(12)

Q ≥ 0, Ri ≥ 0, i ∈ {1, 2} .
3) If

max

August 29, 2019

1 − 134 1 − 14
,
1 − 234 1 − 24


=

1 − 14
> 1,
1 − 24

(13)

DRAFT

8

then R = R3 where R3 is defined by the following inequalities
134 − 1234
R1 ,
(1 − 1234 ) (1 − 134 )


1 − 13
≤1−S+
S,
1 − 23

S≤

1
R2
13 − 123
+
R1 +
(1 − 23 ) (1 − 123 ) 1 − 123
1 − 234


1
134 − 1234
1
1 − 14
+
R
+
R
≤
1
−
S
+
S,
1
2
(1 − 1234 ) (1 − 234 ) 1 − 123
1 − 24
1 − 24




S ≥ 0, Ri ≥ 0, i ∈ {1, 2} .

(14)
(15)
(16)
(17)

Proof: The proof can be found in Appendix A-D.
The next theorem expresses either the system capacity or an inner bound to system capacity region, depending
on system erasure probabilities.
Theorem 4. Let 13 ≥ 23 .
1) If

max

1 − 134 1 − 14
,
1 − 234 1 − 24


≤ 1,

the system capacity region is R1 .
2) If

max

1 − 134 1 − 14
,
1 − 234 1 − 24

=

1 − 134
> 1,
1 − 234

=

1 − 14
> 1,
1 − 24



the system capacity region is R2 .
3) If

max

1 − 134 1 − 14
,
1 − 234 1 − 24



then an inner bound to system capacity region is the region described by the equations below.




13 − 123
1
R2
1 − 13
+
R1 +
≤1−G−S−U +
(G + S) ,
(1 − 23 ) (1 − 123 ) 1 − 123
1 − 234
1 − 23


134 − 1234
1
1 − 134
1 − 14
1
R1 +
R2 ≤ 1 − G − S − U +
G+
(S + U ) ,
+
1
1
2
2
2
(1 − 234 ) (1 − 34 ) 1 − 23
1 − 4
1 − 34
1 − 24




23 − 234 134 − 1234
2
2
2
R1 ,
3 − 34 G + 1 − 34 S ≤
(1 − 134 ) (1 − 1234 )
!




1 − 24 1 − 134
2
2
2
2
3 − 34 U ≤
− 3 − 34 S,
(1 − 14 )



1 − 14 1 − 234
134 − 1234
G+
(U + S) ≤
R1 ,
(1 − 24 ) (1 − 134 )
(1 − 134 ) (1 − 1234 )


23 − 234 24 − 234
S≤
R2 ,
(1 − 134 ) (1 − 24 ) (1 − 234 )
G ≥ 0, S ≥ 0, U ≥ 0, Ri ≥ 0, i ∈ {1, 2} .
Proof: The proof follows from the performance analysis of the transmission algorithm that is proposed in
Section IV.

August 29, 2019

DRAFT

9

Fig. 2. Histogram of Deviation Proportion D

To examine the proximity of the inner and outer bound in part 3 of Theorem 4, we conducted the following
numerical investigation. Assuming that erasure events are independent, all statistical parameters of the system are
determined by the erasure probabilities, 1j , j ∈ {2, 3, 4} , 2j , j ∈ {3, 4} . We varied these probabilities from 0.1
to 0.9 in step 0.1 and kept the values satisfying condition (13). For these values we varied the rate R1 from 0.1B
to 0.9B in steps of 0.05, where

B=

13 − 123
1
+
2
1
(1 − 3 ) (1 − 23 ) 1 − 123

−1
,

is the upper bound on R1 determined by (14)-(17). Next, for a given rate R1 , based on the inequalities determining
the inner bound in part 3 of Theorem 4 we calculated the maximum rate R2 as well as the rate R̂2 obtained using
the inequalities of the outer bound, and registered the relative deviation,
D=

R̂2 − R2
R̂2

.

In Figure 2 we present the histogram of this relative deviation. We see that deviation smaller that 0.05 is achieved
for 75% of the cases. We note that most of larger deviations occur for large values of 14 , 24 . For example, if we
restrict these values to be below 0.6, deviation of at most 0.05 occurs for 99.9% of the cases, while the rest of the
cases have deviation between 0.05 and 0.087. It is worth noting that the regions R1 and R2 described above are
the same as the throughput regions of the algorithms presented in [13].
IV. T RANSMISSION A LGORITHMS
In this section we present transmission algorithms that achieve the rates described by Theorem 4. For the reader’s
convenience, we initially present an algorithm, Algorithm 1, whose description is simple to follow and which
achieves capacity under the condition of part 1 of Theorem 4. Next, we describe the general transmission algorithm,
Algorithm 2, which achieves the rates described in every part of Theorem 4.

August 29, 2019

DRAFT

10

In the description of the transmissions algorithms that follow, for a given number of packets ki = dnRi e, n ∈ dne,
instead of stopping after n transmissions, packets are transmitted until all receivers receive correctly all the packets
destined to them - in general this requires a random number of transmissions. By stopping the algorithm after n
transmissions, and declaring an error if at least one receiver does not receive all packets destined to it, we obtain
an algorithm that performs only n transmission steps (please see the proof of Proposition 6).
Furthermore, to simplify the description of the algorithms, we use the following notation. Queue Qi , i = 1, 2,
contains dnRi e packets, initially located at node i, that form message W i,n . These packets must be delivered to
node i + 2; we refer to them as “packets destined to node i + 2” or “packets with origin node i”. A generic symbol
Xj,kl̄ denotes a queue that is located at node j, contains packets that were initially in queue Qj (destined to node
j + 2), have been received by node k and has not been received by node l. Queue Xj,i k̄l is located at node j,
contains packets that were initially in queue Qi (i.e. these packets have been received by node j through earlier
transmissions of node i), have not been received by node k and have been received by node l. A similar notation,
1
with small letters instead of capital, is used for packets. For example, x12,3̄4 is a packet from queue X2,
3̄4 , hence

the packet was originally in Q1 , is located at node 2, has been received by node 4 and has not been received by
1
1
1
1
node 3. Note that by definition all packets in X2,
3̄4 are also in X4,23̄ ; X2,3̄4 is located at node 2, while X4,23̄ is
2
located at node 4. Similarly, X2,34̄ and X3,
4̄ contain the same packets.

At some steps of the algorithms described below, XOR combinations of packets from different queues may be
sent. If the sent packet is of the form q = q1 ⊕ q2 , we say that packets q1 , q2 “constitute” packet q. We note that
movements and insertion of packets in queues can be done distributively by the nodes at which the queues are
located, by following the channel feedback.
A. Description of Algorithm 1
The full description of Algorithm 1 is given in detail in subsection IV-A1 and is summarized as follows. In Step
1, transmitter 1 sends packets from Q1 until they are received by at least one of the nodes 2, 3; during this process,
1
packets that are received by node 4 are “marked” by node 1 and placed in buffer B4,
2̄3̄ . At the end of this step, Q1

is empty, and queues Q12,3̄4̄ , Q12,3̄4 and Q14,23̄ may be nonempty. In Step 2, transmitter 2 sends packets from Q12,3̄4̄
until they are received by at least one of the nodes 3 or 4, placing packets that are erased at node 3 and received
by node 4 in Q12,3̄4 and Q14,23̄ . In Step 3, transmitter 2 sends packets from Q2 until they are received by at least
one of the nodes 3, 4, placing packets that are erased at node 4 and received by node 3 in Q2,34̄ and Q23,4̄ . At
the end of this step, only queues Q12,3̄4 and Q2,34̄ (and the corresponding queues Q14,23̄ , Q23,4̄ ) may be nonempty.
Finally, in Step 4, transmitter 2 sends XOR combinations of packets from Q12,3̄4 and Q2,34̄ , i.e., packets of the form
1
q = q2,
3̄4 ⊕ q2,34̄ . Since each of the nodes 3, 4 has already received one of the packets that constitute the XOR

combination of q, upon reception of q the node can extract the packet that is destined to it. For example, if node
1
1
1
4 receives q, then since q2,
¯ , node 4 can extract q2,34̄ = q ⊕ q2,3̄4 ; packet q2,34̄ can therefore be removed
3̄4 ∈ Q4,23

from Q2,34̄ and Q23,4̄ . This process continues until one of the queues Q12,3̄4 , Q2,34̄ empties; the packets remaining
in the nonempty queue (if any) are transmitted by node 2 until received by the corresponding destination.

August 29, 2019

DRAFT

11

1) Detailed description:
Algorithm 1
1) If Q1 is nonempty, transmitter 1 sends packet q from the head of line of Q1 until it is received by at least
one of the nodes 2,3.
a) If q is received by node 3, it is removed from Q1 .
1
b) If q is received by node 4 and erased at nodes 2, 3, it is “marked” by node 1 and placed in B4,
2̄3̄ (this

queue is a buffer containing at most one packet). The packet is re-transmitted by transmitter 1.
c) If q is received by nodes 2 and 4 and erased at node 3, it is removed from Q1 and placed in Q12,3̄4 and
1
Q14,23̄ . If q is also “marked” by node 1, hence it is in B4,
2̄3̄ (i.e., the p has been received earlier by node

4), q is removed from this buffer.
d) If q is received by node 2, erased at nodes 3, 4 and and q is “marked” (i.e. it has been received earlier
1
1
1
by node 4), it is removed from Q1 and B4,
2̄3̄ , and placed in Q2,3̄4 and Q4,23̄ .

e) If q is received by node 2, erased at nodes 3, 4 and q is not “marked” (i.e., the packet has not been
received earlier by node 4), q is removed from Q1 and placed in Q12,3̄4̄ .
2) If Q12,3̄4̄ is nonempty, transmitter 2 sends packet q from from the head of line of Q12,3̄4̄ until it is received by
at least one of the nodes 3, 4.
a) If q is received by node 3, it is removed removed from Q12,3̄4̄ .
b) If q is erased at node 3 and received by node 4, it is removed from Q12,3̄4̄ and placed in queue Q12,3̄4 .
Also, q is placed in Q14,23̄ .
3) If Q2 is nonempty, transmitter 2 sends packet q from the head of line of Q2 until it is received by at least
one of the nodes 3, 4.
a) If q is received by node 4, it is removed from Q2 .
b) If q is erased at node 4 and received by node 3, it is removed from Q2 and placed in queue Q2,34̄ . Also,
p is placed in Q23,4̄ .
1
1
4) Transmitter 2 sends packet q = q2,
3̄4 ⊕ q2,34̄ where q2,3̄4 , q2,34̄ are the packets at the head of line of queues

Q12,3̄4 , Q2,34̄ respectively. If q is erased at both nodes 3, 4, it is re-transmitted. Else,
1
1
1
a) If q is received by node 3, packet q2,
3̄4 is removed from Q2,3̄4 and Q4,23̄ .

b) If q is received by node 4, packet q2,34̄ is removed from Q2,34̄ and Q23,4̄ .
This process continues until at least one of the queues Q12,3̄4 , Q2,34̄ empties; Then, the remaining packets (if
any) of the queue that is nonempty, are sent by Transmitter 2 until they are received by their destination.
B. Description of Algorithm 2
The full description of Algorithm 2 is given in detail in subsection IV-B1. Below we provide the rationale for
the steps taken by Algorithm 2, in addition to those taken by Algorithm 1. In algorithm 2, we introduce three
parameters, g, s, u, corresponding to, and motivated by, the operational interpretation of the parameters G, S, U
appearing in the capacity outer bound in Theorem 2. This interpretation can be seen from the proof of the theorem.

August 29, 2019

DRAFT

12

In Step 2 of Algorithm 1, packets from Q12,3̄4̄ are always re-transmitted by node 2 until they are received by at
least one of the nodes 3, 4. While it can be shown that this option is optimal (i.e., the algorithm is capacity achieving)
if relation (6) holds, it may be sub-optimal in other cases. Specifically if (6) does not hold, two possibilities for
improving the performance of the Algorithm 1 arise.
1) If 124 < 234 , it may be beneficial for node 1 to re-transmit a portion g of packets that are received by node
2 and not received by nodes 3 and 4 (i.e the packets in Q12,3̄4̄ ). This can be done by selecting each packet in
Q12,3̄4̄ to be re-transmitted by node 1 with probability g. We place the selected packets in queue G1,23̄4̄ (Step
1e of Algorithm 2). Packets in this queue are re-transmitted by node 1 until they are received by either of
the nodes 3, 4 (Step 2 of Algorithm 2).
2) If 14 < 24 , it may be beneficial for node 1 (instead of node 2) to transmit packets that, if received by node
4, permit this node to reconstruct packets destined to it. However, for this to be possible, since node 1 never
receives packets transmitted by node 2, node 4 must be able to discover a packet destined to it (i.e., a packet
that was originally in Q2 ) by receiving a packet transmitted by node 1 (that was originally in Q1 ). This can
1
be accomplished as follows. Suppose that node 2, instead of transmitting packet q2,
3̄4̄ , transmits
1
q = q2,
3̄4̄ ⊕ q2,34̄ .

(18)

Consider the following cases.
1
a) q is received by node 3 and erased at node 4: Then node 3 recovers q2,
3̄4̄ hence this packet is removed

from Q12,3̄4̄ .
b) q is received by node 4 and erased at node 3: Then node 4 cannot recover packet q2,34̄ since it has not
1
received q2,
3̄4̄ . However, now the following flexibility regarding future transmissions is obtained: Node
1
1 has the ability to re-transmit q2,
3̄4̄ . Upon such a re-transmission, we observe the following cases.
1
i) q2,
3̄4̄ is received by both nodes 3 and 4: Then both nodes recover the constituent packet of q that is
1
1
destined to them (node 3 packet q2,
3̄4̄ and node 4 packet q2,34̄ = q ⊕ q2,3̄4̄ ).
1
ii) q2,
3̄4̄ is received by node 4 and erased at node 3: Then node 4 recovers the constituent packet of q
1
1
1
destined to it; in addition, it also knows q2,
3̄4̄ , hence this packet can be placed in Q2,3̄4 and Q4,23̄ .
1
1
1
iii) q2,
3̄4̄ is received by node 3 and erased at node 4: Then q2,3̄4̄ is removed from Q2,3̄4̄ . Even though
1
received by node 3, packet q2,
3̄4̄ is still useful, since node 4 can recover q2,34̄ if node 1 re-transmits
1
q2,
3̄4̄ .
1
c) q is received by both nodes 3 and 4: Again, even though received by node 3, packet q2,
3̄4̄ is still useful,
1
since node 4 can recover q2,34̄ if node 1 re-transmits q2,
3̄4̄ .

Motivated by this reasoning, in Step 1e of Algorithm 2, we select a portion s of the packets in Q12,3̄4̄ to be
1
transmitted coded by node 2 in the form (18). These packets are placed in queues S1,23̄4̄ and S2,
3̄4̄ .

In Step 5 of Algorithm 2, node 2 transmits packets of the form q = s12,3̄4̄ ⊕ q2,34̄ . At this step, based on
channel feedback, queues are formed that contain coded packets; to emphasize this fact and with a slight
abuse of notation, these queues are denoted by the capital bold letter A. A queue at node 1 that contains the
constituent packets of queue A with origin node 1, will be denoted by A. For example, A2,3̄4 is a queue

August 29, 2019

DRAFT

13

containing coded packets of the form q = s12,3̄4̄ ⊕ q2,34̄ , transmitted by node 2, erased at node 3 and received
by node 4, A24,3̄ is located at node 4 and contains the same packets as A2,3̄4 , and A1,23̄4 is located at node
1 and contains all of the constituent packets of the coded packets in A2,3̄4 that have origin node 1. The
placement of packets in these queues for each possible feedback is based on the corresponding cases 2a, 2b,
2c described in the previous paragraph.
In Step 6 of Algorithm 2, node 1 transmits packets from queue A1,23̄4 . The placement of packets in these
queues for each possible feedback is based on the corresponding cases 2(b)i, 2(b)ii, 2(b)iii described in the
penultimate paragraph.
At the end of Step 6 of Algorithm 2, queues A1,234 , A2,34 , A24,3 may be nonempty. Note that all packets
with destination node 3 that are constituents of packets in A2,34 , A24,3 , have already been received by node 3,
hence node 4 has to recover only the packets with destination node 4 that are constituents of packets in these
queues. Two options for recovering these packets are the following: a) by transmitting the uncoded packets
by node 2 or b) by having node 1 transmit packets from A1,234 and doing the appropriate decoding at node
4. The latter option may seem preferable since the channel from 1 to 4 is better than the channel from 2 to
4, i.e., 14 < 24 . However, there is a third option: node 2 may have the opportunity to transmit these packets
network coded while attempting to deliver packets from Q12,3̄4 to node 3, hence in effect at no transmission
cost (Step 8 of Algorithm 2). To address this trade-off, in Step 7 of Algorithm 2 we select a portion u of the
packets from A1,234 to be transmitted by node 1; the rest are transmitted by node 2 in Step 8 of Algorithm
2.
1) Detailed description:
Algorithm 2
1) If Q1 is nonempty, transmitter 1 sends packet q1 from the head of line of Q1 until it is received by at least
one of the nodes 2,3. Steps 1a, 1b, 1c, 1d are the same as steps 1a, 1b,1c, 1d of Algorithm 1 respectively.
1
e) If q1 is received by node 2, erased at nodes 3, 4 and q1 is not in B4,
2̄3̄ (i.e., the packet has not been

received earlier by node 4), q1 is removed from Q1 . With probability g the packet is placed in queues
1
G1,23̄4̄ and G12,3̄4̄ , with probability s, where g + s ≤ 1, it is placed in queues S1,23̄4̄ and S2,
3̄4̄ , and with

probability 1 − s − g the packet is placed in Q12,3̄4̄ .
1
1
1
Possible nonempty queues at this point: G1,23̄4̄ , G12,3̄4̄ , Q12,3̄4̄ , Q2 , S1,23̄4̄ , S2,
¯.
3̄4̄ , Q2,3̄4 , ,Q4,23

2) If G1,23̄4̄ is nonempty, transmitter 1 sends packet g1,23̄4̄ from the head of line of G1,23̄4̄ until it is received
by at least one of the nodes 3,4.
1
a) If g1,23̄4̄ is received by node 3, it is removed from G1,23
¯ 4̄ and G2,3̄4̄ .
1
b) If g1,23̄4̄ is erased at by node 3 and received by node 4, it is removed from G1,23
¯ 4̄ and G2,3̄4̄ , and placed

in Q12,3̄4 and Q14,23̄ .
1
1
1
Possible nonempty queues at this point: Q12,3̄4̄ , Q2 ,S1,23̄4̄ , S2,
3̄4̄ , Q2,3̄4 , ,Q4,23̄ .
1
1
3) If Q12,3̄4̄ is nonempty, transmitter 2 sends packet q2,
3̄4̄ from from the head of line of Q2,3̄4̄ until it is received

by at least one of the nodes 3, 4. The same actions as in Step 2 of Algorithm 1 are taken.

August 29, 2019

DRAFT

14

1
1
1
Possible nonempty queues at this point: Q2 , S1,23̄4̄ , S2,
3̄4̄ , Q2,3̄4 , ,Q4,23̄ .

4) If Q2 is nonempty, transmitter 2 sends packet q2 from the head of line of Q2 until it is received by at least
one of the nodes 3, 4. The same actions as in Step 3 of Algorithm 1 are taken.
1
1
1
2
Possible nonempty queues at this point: S1,23̄4̄ , S2,
3̄4̄ , Q2,3̄4 , ,Q4,23̄ , Q2,34̄ , Q3,4̄ .
1
1
5) If S2,
3̄4̄ (hence also S1,23̄4̄ ) and Q2,34̄ are nonempty, transmitter 2 sends packet q = s2,3̄4̄ ⊕ q2,34̄ where
1
s12,3̄4 , q2,34̄ are the packets at the head of line of queues S2,
3̄4̄ , Q2,34̄ respectively, until q is received by at

least one of the nodes 3, 4.
1
a) If q is received by node 3 and erased at node 4, packet s12,3̄4̄ is removed from queues S1,23̄4̄ and S2,
3̄4̄ .

The reason for this action is that node 3 can recover s12,3̄4̄ as s12,3̄4̄ = q ⊕ q2,34̄ .
2
1
b) If q is received by node 4 and erased at node 3, q is removed from S2,
3̄4̄ and placed in A2,3̄4 and A4,3̄

Moreover, its constituent packet s12,3̄4̄ is removed from S1,23̄4̄ and placed in A1,23̄4 .
2
1
c) If q is received by both nodes 3, 4, q is removed from S2,
3̄4̄ and placed in queue A2,34 and A4,3 .

Moreover, its constituent packet s12,3̄4̄ is removed from S1,23̄4̄ and placed in A1,234 .
Possible nonempty queues at this point: A1,23̄4 , A2,3̄4 , A24,3̄ , A1,234 , A2,34 , A24,3 , Q12,3̄4 ,Q14,23
¯ , Q2,34̄ ,
Q23,4̄ .
6) If A1,23̄4 is nonempty, transmitter 1 transmits packet a1,23̄4 from the head of line of A1,23̄4 until the packet
is received by at least one of the nodes 3, 4.
a) If a1,23̄4 is received by both nodes 3, 4, a1,23̄4 is removed from A1,23̄4̄ . Moreover the packet whose
constituent is h1,23̄4 in A2,3̄4 and A24,3̄ is also removed.
b) If a1,23̄4 is received by node 4 and erased at node 3, a1,23̄4 is removed from A1,23̄4 and added to Q12,3̄4
and Q14,23̄ . Moreover, the packet whose constituent is a1,23̄4 in A2,3̄4 and A24,3̄ is removed.
c) If a1,23̄4 is received by node 3 and erased at node 4, a1,23̄4 is moved to A1,234 . Moreover, the packet
whose constituent is a1,23̄4 in A2,3̄4 is moved to A2,34 .
Possible nonempty queues at this point: A1,234 , A2,34 , A24,3 , Q12,3̄4 , ,Q14,3̄ , Q2,34̄ , Q23,4̄ .
7) If A1,234 is nonempty, with probability 1 − u each packet a1,234 ∈ A1,234 is removed from A1,234 , and the
packet q whose constituent is a1,234 in A2,34 and A24,3 , is removed. Moreover, the other constituent of packet
q is moved to Q2,34̄ . Node 1 re-transmits any remaining packet a1,234 in A1,234 until it is received by node
4, at which point the packet is removed from A1,234 ; moreover, the packet q in A2,34 , A24,3 whose constituent
is a1,234 is also removed (note that reception of a1,234 by node 4 enables the recovery of the constituent of
q with destination node 4).
2
Possible nonempty queues at this point: Q12,3̄4 ,Q14,23
¯ , Q2,34̄ , Q3,4̄ .
1
1
8) Transmitter 2 sends packet q = q2,
3̄4 ⊕ q2,34̄ where q2,3̄4 , q2,34̄ are the packets at the head of line of queues

Q12,3̄4 , Q2,34̄ respectively. The same actions as in Step 4 of Algorithm 1 are taken.
C. Performance Analysis of Algorithms
The performance analysis of Algorithm 1 is done in a similar way as in [15], [16]. Let dnRe = (dnR1 e , dnR2 e)
be the vector consisting of the number of packets destined to each of the receivers. Let T (dnRe) be the (random)

August 29, 2019

DRAFT

15

time it takes for all packets to be delivered to their destinations when Algorithm 1 is employed.
Proposition 5. It holds,

T (dnRe)
lim
n→∞
n


1
R2
13 − 123
+
+
,
max R1
1
2
1
(1 − 3 ) (1 − 23 ) 1 − 23
1 − 234


R2
1
134 − 1234
+
R
+
1
1 − 24
1 − 123
(1 − 1234 ) (1 − 234 )


=



= T̂ (R) .

(19)

Proof: We provide an outline of the arguments; detailed description can be found in [15]. Let Ti be the time
it takes for Step i of Algorithm 1 to complete. According to the description of the algorithm, and based on the
Strong Law of Large Numbers, we derive the following limiting quantities at the end of each step.
1) The following limit holds for time T1 ,
lim

n→∞

T1
R1
=
.
n
1 − 123

(20)

Furthermore at the end of this step, it also hold for the number packets that are placed in queues Q12,3̄4̄ and
Q12,3̄4 - for simplicity we denote the number of packets in a queue X with the same letter.
lim

n→∞

lim

n→∞

Q12,3̄4̄

=

n
Q12,3̄4

=

n

134 − 1234
,
1 − 1234
 1

 − 123
134 − 1234
R1 3
−
.
1 − 123
1 − 1234
R1

(21)
(22)

2) The following limit holds for the time needed in order for transmitter 2 to send the Q12,3̄4̄ packets to either
node 3 or 4.
1
T2
1 Q2,3̄4̄
134 − 1234
= lim
=
R
.
1
n→∞ n
n→∞ n 1 − 2
(1 − 1234 ) (1 − 234 )
34

lim

(23)

Moreover, if M if the number of packets from Q12,3̄4̄ that are received by node 3 and erased at node 4, it
holds,



Q12,3̄4̄
134 − 1234 23 − 234
M
23 − 234
lim
=
lim
= R1
.
n→∞ n
1 − 234 n→∞ n
(1 − 1234 ) (1 − 234 )

(24)

At the end of this step, only queue Q12,3̄4 exists and the new number of the packets in this queue, denoted as
Q̂12,3̄4 , is: Q̂12,3̄4 = Q12,3̄4 + M . According to (22) and (24) we have,
lim

n→∞

Q̂12,3̄4
n


= R1

13 − 123
1 − 123


+ R1

134 − 1234
1 − 1234




23 − 234
−
1
.
1 − 234

(25)

3) The following limit holds for the time needed in order for transmitter 2 to send dnR2 e packets to either node
3 or 4 is given by:
lim

n→∞

T3
R2
=
.
n
1 − 234

(26)

Furthermore, at the end of this step, it holds for the number of packets in queue Q2,34̄ ,
Q2,34̄
2 − 234
= R2 4
.
n→∞
n
1 − 234
lim

August 29, 2019

(27)

DRAFT

16

4) Let T4,3 , T4,4 , be the time needed for node 2 to deliver packets in queue Q̂12,3̄4 , Q2,34̄ , to destinations 3 and
4 respectively, if packets in these queues were sent uncoded. It then holds,
Q̂12,3̄4
T4,3
,
=
n→∞ n
1 − 23
Q2,34̄
T4,4
lim
.
=
n→∞ n
1 − 24
lim

Since at this step packets are sent coded whenever both queues Q̂12,3̄4 , Q2,34̄ , are nonempty, we have, T4 =
max {T4,3 , T4,4 }, hence according to (25) and (27),
 2 2



3 −34
134 −1234
13 −123
24 −234 

+
R
−
1
R
R
1
1
2
1
1
2
2 1−
T4
1−23
1−234
1−34
34
lim
,
.
= max
n→∞ n

1 − 23
1 − 24 

(28)

Since the total time for the algorithm to complete is given by
T (dnRe) = T1 + T2 + T3 + T4 ,
taking into account (20), (23), (26), and (28), (19) arises.
The next proposition provides a sufficient condition for achievability and follows easily from Proposition 5.
Proposition 6. If the rate vector R = (R1 , R2 ) , Ri ≥ 0, i = 1, 2,satisfies,
 



13 − 123
R2
R2
1
134 − 1234
1
max R1
+
,
+
R
+
+
<1
1
(1 − 23 ) (1 − 123 ) 1 − 123
1 − 234 1 − 24
1 − 123
(1 − 1234 ) (1 − 234 )
then R is achievable.
Proof: The proof is identical to the proof in [15, Appendix C-B]. We present it here for completeness. Consider
the following code:
1) Use Algorithm 1 to transmit dnRe packets.
2) If T (dnRe) ≤ n then transmit n − T (dnRe) arbitrary packets and stop. In this case, both receivers receive
correctly their packets.
3) Else declare error.
The probability of error of this code is computed as follows.
lim pe (n)

n→∞

=
=
=
=

lim Pr (T (dnRe) > n)


T (dnRe)
>1
lim Pr
n→∞
n


T (dnRe)
lim Pr
− T̂ (R) > 1 − T̂ (R)
n→∞
n
n→∞

0 by (19).

The performance analysis of Algorithm 2, although more complicated, is similar. From this analysis it follows
that the rate region of Algorithm 2 is the set of pairs R = (R1 , R2 ) that satisfy the following relations.


August 29, 2019

13 − 123
1
+
2
1
(1 − 3 ) (1 − 23 ) 1 − 123



R2
R1 +
≤1−G−S−U +
1 − 234



1 − 13
1 − 23


(G + S) ,

DRAFT

17



134 − 1234
1
+
1
2
(1 − 234 ) (1 − 34 ) 1 − 123

U = us

1
1 − 134
1 − 14
R
≤
1
−
G
−
S
−
U
+
G
+
(S + U ) ,
2
1 − 24
1 − 234
1 − 24

R1 134 − 1234
G=g
,
(1 − 134 ) (1 − 1234 )


R1 23 − 234 134 − 1234
,
S=s
(1 − 134 ) (1 − 1234 ) (1 − 234 )


R1 +


 !


R1 23 − 234 134 − 1234
R1 134 − 1234 1 − 24
,
−
(1 − 14 ) (1 − 234 ) (1 − 1234 ) (1 − 134 ) (1 − 1234 ) (1 − 234 )

g + s ≤ 1, u ≤ 1, g ≥ 0, s ≥ 0, u ≥ 0, S ≥ 0, Ri ≥ 0, i ∈ {1, 2} .
It can be seen that the region of pairs R = (R1 , R2 ) satisfying these relations is the same as the region defining
the inner bound in part 3 of Theorem 4. Note that when s = u = g = 0 (u = s = 0), the region is identical to the
outer bound in part 1 (2) of Corollary 3, hence the algorithm is capacity achieving in theses cases.
V. C ONCLUSION & F URTHER W ORK
In this paper we developed an outer bound for the capacity of a fundamental cooperative cognitive network.
We distinguished three cases based on the statistical parameters of the channel. Through the design of appropriate
algorithm, we showed that in the first two cases the outer bound is indeed tight. For the third case, the rate region
of the developed algorithm is close to the outer bound for a wide range of channel statistics.
Directions for future work include the investigation of benefits of cooperation in the cases of multiple secondary
user and/or primary user pairs.
A PPENDIX A
P ROOF OF T HEOREM 12
In the following, for convenience in notation, we write nRi instead of dnRi e, and we omit the index n whenever
there is no possibility for confusion, e.g. we write W i and ki instead of W i,n and ki,n respectively. Also we use
base |F| for logarithms concerning information measures. Hence, since packets are uniformly selected elements of
F, H (Wi,k ) = 1, 1 ≤ k ≤ ki and since packets are independent, H (W i ) = ki .
A. Preliminary Results
In this subsection we present preliminary results that are used in the development of the outer bound to system
capacity in Section III. The next lemma relates achievable rates to mutual information measures.
Lemma 7. Let (R1 , R2 ) be achievable.
•

•

If {2, 3} ⊆ S1 then
0 ≤ nR1 − I(W 1 ; Y n1S1 , Z n |W 2 ) ≤ o (n) .

(29)


0 ≤ nR2 − I W 2 ; Y n2S2 , Z n |W 1 ≤ o(n).

(30)

If 4 ∈ S2 then

August 29, 2019

DRAFT

18

•

•

If 3 ∈ S1 ∩ S2 then
0 ≤ nR1 − I(W 1 ; Y n1S1 , Y n2S2 , Z n ) ≤ o (n) ,

(31)

0 ≤ nR1 − I(W 1 ; Y n1S1 , Y n2S2 , Z n |W 2 ) ≤ o (n) .

(32)

0 ≤ nR2 − I(W 2 ; Y n1S1 , Y n2S2 , Z n ) ≤ o(n).

(33)

If 4 ∈ S1 ∩ S2 then

Proof: We prove (29). The rest of relations follow by similar arguments.

nR1 = H (W 1 ) "indeq. uniformly distributed packets”
= H (W 1 |W 2 ) "W 1 ⊥ W 2 "
= I(W 1 ; Y n1S1 , Z n |W 2 ) + H(W 1 |Y n1S1 , Z n , W 2 )




= I(W 1 ; Y n1S1 , Z n |W 2 ) + H W 1 |Y n1S1 , Z n , W 2 , Y n2{3} "Y n2{3} = f2 W 2 , Y n1{2} , Z n "


= I(W 1 ; Y n1S1 , Z n |W 2 ) + H W 1 |Y n1S1 , Z n , W 2 , Y n2{3} , Ŵ 1 "Ŵ 1 = g3 (Y n1{3} , Y n2{3} , Z n )"


≤ I(W 1 ; Y n1S1 , Z n |W 2 ) + H W 1 |Ŵ 1
≤ I(W 1 ; Y n1S1 , Z n |W 2 ) + o (n) "Fano inequality"
Also,
nR1 = H (W 1 |W 2 )

≥ I W 1 ; Y n1S1 , Z n |W 2 .

The next lemma expresses mutual information measures appearing in Lemma 7 in terms of more elementary
ones, which will be useful in the development of the outer bound in Section A-B.
Lemma 8. Let Si ⊆ Ni , i ∈ {1, 2} . For random variables A, B, it holds,
I (A; Y 1S1 (t), Y 2S2 (t) | B, Z(t), σ(t)) =

X

I (A; X1 (t) | B, Z 1 (t) = z, σ(t) = 1) Pr (Z 1 (t) = z) Pr (σ(t) = 1)

1
z∈ZS

1

+

X

I (A; X2 (t) | B, Z 2 (t) = z, σ(t) = 2) Pr (Z 2 (t) = z) Pr (σ(t) = 2) . (34)

2
z∈ZS

2


t−1
t−1
then
For random variables Q and P , if Z(t) is independent of Q, X1 (t), X2 (t), P , Y t−1
1S1 , Y 2S2 , Z
n

X

t−1
t−1
I Q; Y n1S1 , Y n2S2 , Z n | P = 1 − 1S1
I Q; X1 (t) | P , Y t−1
, σ(t) = 1 Pr (σ(t) = 1)
1S1 , Y 2S2 , Z
t=1

+ 1 − 2S2

n
X


t−1
t−1
I Q; X2 (t) | P , Y t−1
, σ(t) = 2 Pr (σ(t) = 2) .
1S1 , Y 2S2 , Z

(35)

t=1

August 29, 2019

DRAFT

19

Proof: To show (34), using the definition of conditional mutual information we have,
I (A; Y 1S1 (t), Y 2S2 (t) | B, Z(t), σ(t))
X
I (A; Y 1S1 (t), Y 2S2 (t) | B, Z 1 (t) = z, σ(t) = 1) Pr (Z 1 (t) = z, σ(t) = 1)
=
z∈Z1

+

X

I (A; Y 1S1 (t), Y 2S2 (t) | B, Z(t) = z, σ(t) = 2) Pr (Z 2 (t) = z, σ(t) = 2)

z∈Z2

=

X

I (A; Y 1S1 (t) | B, Z 1 (t) = z, σ(t) = 1) Pr (Z 1 (t) = z) Pr (σ(t) = 1)

z∈Z1

+

X

I (A; Y 2S2 (t) | B, Z 2 (t) = z, σ(t) = 2) Pr (Z 2 (t) = z) Pr (σ(t) = 2)

z∈Z2

"Z (t) ⊥ σ (t) and if σ(t) = i then Y ic Sic (t)=η."
X
=
I (A; Y 1S1 (t) | B, Z 1 (t) = z, σ(t) = 1) Pr (Z 1 (t) = z) Pr (σ(t) = 1)
1
z∈ZS

1

+

X

I (A; Y 2S2 (t) | B, Z 2 (t) = z, σ(t) = 2) Pr (Z 2 (t) = z) Pr (σ(t) = 2)

2
z∈ZS
2

"if z ∈
/ ZSi 1 then Y iSi (t) = ε."
X
=
I (A; X1 (t) | B, Z 1 (t) = z, σ(t) = 1) Pr (Z 1 (t) = z) Pr (σ(t) = 1)
1
z∈ZS

1

+

X

I (A; X2 (t) | B, Z 2 (t) = z, σ(t) = 2) Pr (Z 2 (t) = z) Pr (σ(t) = 2)

(36)

2
z∈ZS

2

"Fact 1, item 3."
To show equality (35) we apply first the chain rule to obtain,
n

 X
t−1
t−1
I Q; Y 1S1 (t), Y 2S2 (t), Z(t) | P , Y t−1
.
I Q; Y n1S1 , Y n2S2 , Z n | P =
1S1 , Y 2S2 , Z
t=1

August 29, 2019

DRAFT

20

Next, we write,

t−1
t−1
I Q; Y 1S1 (t), Y 2S2 (t), Z(t) | P , Y t−1
1S1 , Y 2S2 , Z


t−1
t−1
t−1
t−1
= I Q; Z(t) | P , Y t−1
+ I Q; Y 1S1 (t), Y 2S2 (t) | P , Y t−1
, Z(t)
1S1 , Y 2S2 , Z
1S1 , Y 2S2 , Z


t−1
t−1
t−1
t−1
= I Q; Y 1S1 (t), Y 2S2 (t) | P , Y t−1
, Z(t) "Z(t) ⊥ Q, P , Y 1S
, Y 2S
, Z t−1 "
1S1 , Y 2S2 , Z
1
2

t−1
t−1
= I Q; Y 1S1 (t), Y 2S2 (t) | P , Y t−1
, Z(t), σ(t) "σ(t) = σ(Z t−1 )"
1S1 , Y 2S2 , Z
X

t−1
t−1
=
I Q; X1 (t) | P , Y t−1
, Z(t) = z, σ(t) = 1 Pr (Z 1 (t) = z) Pr (σ(t) = 1)
1S1 , Y 2S2 , Z
1
z∈ZS

1

+

X


t−1
t−1
I Q; X2 (t) | P , Y t−1
, Z(t) = z, σ(t) = 2 Pr (Z 2 (t) = z) Pr (σ(t) = 2) "by 34),"
1S1 , Y 2S2 , Z

2
z∈ZS

2

=

X


t−1
t−1
I Q; X1 (t) | P , Y t−1
, σ(t) = 1 Pr (Z 1 (t) = z) Pr (σ(t) = 1)
1S1 , Y 2S2 , Z

1
z∈ZS

1

+

X


t−1
t−1
I Q; X2 (t) | P , Y t−1
, σ(t) = 2 Pr (Z 2 (t) = z) Pr (σ(t) = 2)
1S1 , Y 2S2 , Z

2
z∈ZS

2


t−1
t−1
"Z(t) ⊥ Q, X1 (t), X2 (t), P , Y t−1
"
1S1 , Y 2S2 , Z


t−1
t−1
= 1 − 1S1 I Q; X1 (t) | P , Y t−1
, σ(t) = 1 Pr (σ(t) = 1)
1S1 , Y 2S2 , Z


t−1
t−1
+ 1 − 2S2 I Q; X2 (t) | P , Y t−1
, σ(t) = 2 Pr (σ(t) = 2) .
1S1 , Y 2S2 , Z

(37)

Equality (35) follows from (36) and (37).
1) Node Scheduling Times: In this subsection we define certain node scheduling times and relate them to
information theoretic quantities. These relations are needed for the development of the outer bound.
•

Ti , i ∈ {1, 2} : the number of times t, 1 ≤ t ≤ n, that node i is scheduled to transmit, i.e.,
Ti =

n
X

I{σ(t)=1} ,

t=1

where IA denotes the indicator function of event A. Clearly, T1 + T2 = n, hence, denoting τ̃i = E [Ti ] we
have,
τ̃1 + τ̃2 = n,
where
τ˜i =

n
X

Pr (σ(t) = i) .

(38)

(39)

t=1
•

At time t let H1,i (t) be the index of packet transmitted by node 1 and received by node i. If node 1 does not
transmit at time t, or if node 1 transmits but the packet is erased at node i, we set the packet index to null,
η. Similarly, we define H1,ī (t) the index of packet transmitted by node 1 and not received by node i. If node
1 does not transmit at time t, or if node 1 transmits but the packet is received by node i, we set the packet
index to null, η. We extend this definition to packet indices determined by functions φ(i, j, k) of node indices
involving logical AND (∧) , OR (∨) and NOT (x̄) operations. For example, if φ(i, j) = i ∧ j̄, then H1,i∧j̄ (t)
is the index of packets in W 1 , transmitted by node 1, received by node i and not received by node j at time

August 29, 2019

DRAFT

21

t (and null in the rest of the cases). Similarly, H1,i∨j (t) is the index of packet in W 1 , transmitted by node 1
and received either by node i or by node j (or both) at time t. This is the index of the packet in the vector
Y 1,{i,j} (t). We now define the following scheduling times.
– T1,φ(i,j,k) : number of times t, 1 ≤ t ≤ n, that node 1 transmits one of the packets in W 1 with index in
H t−1
1,φ(i,j,k) , i.e.,
T1,φ(i,j,k) =

n


X

I σ (t) = 1, J Z t−1 ∈ H t−1
1,φ(i,j,k) .

(40)

t=1



From the definitions, setting τ̃1,φ(i,j,k) = E T1,φ(i,j,k) it follows that
τ̃1,φ(i,j,k) + τ̃1,φ(i,j,k) = τ̃1 ,
τ̃1,φ(i,j,k) =

n
X

(41)




t−1
.
Pr σ (t) = 1, J Z t−1 ∈ H 1,φ(i,j,k)

(42)

t=1

We now discuss some properties that follow from the fact that node 1 performs only packet scheduling operations
t−1
based on channel feedback. Note that for a given feedback z t−1 , the packet indices in hi,φ(·)
are completely

determined. For the conditional probabilities below, the conditioning event is assumed to be nonempty.


Lemma 9. a)For any S ⊆ {2, 3, 4}, if Z t−1 = z t−1 , σ z t−1 = 1 and J z t−1 ∈ ht−1
, it holds for any
1,∧i∈S ī
m ∈ F, y t−1
1S ,



1
t−1
t−1
,
(43)
Pr X1 (t) = m|Y t−1
= z t−1 , σ z t−1 = 1 =
1S = y 1S , Z
|F|


1
t−1
t−1
Pr X1 (t) = m|Y t−1
= z t−1 , W 2 = w2 , σ z t−1 = 1 =
.
(44)
1S = y 1S , Z
|F|


b) For any S1 ⊆ N1 , S2 ⊆ N2 , if Z t−1 = z t−1 , σ z t−1 = 1 and J z t−1 ∈ ht−1
, it holds for any m ∈ F,
1,2̄∧i∈S ī
1

t−1
y t−1
1S1 , y 2S2 ,



1
t−1
t−1
t−1
t−1
= z t−1 , W 2 = w2 , σ z t−1 = 1 =
Pr X1 (t) = m|Y t−1
,
(45)
1S1 = y 1S1 , Y 2S2 = y 2S2 Z
|F|


1
t−1
t−1
t−1
t−1
Pr X1 (t) = m|Y t−1
= z t−1 , σ z t−1 = 1 =
.
(46)
1S1 = y 1S1 , Y 2S2 = y 2S2 Z
|F|

Proof: To show (43) notice that since J z t−1 ∈ ht−1
, the indices of all packets in Y t−1
1,S are different
1,∧i∈S ī

t−1
from J z
, hence, since the elements of W 1 are independent, X1 (t) = WJ(zt−1 ) is independent of Y t−1
1S .

t−1
Moreover, since W 1 is independent of Z t−1 , given Z t−1 = z t−1 , X1 (t) , Y 1S
are independent of Z t−1 .
Hence,


t−1
t−1
Pr X1 (t) = m|Y t−1
= z t−1 , σ z t−1 = 1
1S = y 1S , Z

t−1
= Pr WJ(zt−1 ) = m|Y t−1
1S = y 1S

= Pr WJ(zt−1 ) = m
=

1
"packet are selected uniformly from F".
|F|

Equality (44) follows from the fact that W 2 is independent of the rest of the variables.

August 29, 2019

DRAFT

22



t−1
t−1
, Z t−1 we have
To show (45) assume first that 2 ∈ S1 . Then, since Y 2S
= f2 W 2 , Y 1{2}
2


t−1
t−1
t−1
t−1
Pr X1 (t) = m|Y t−1
= z t−1 , W 2 = w2 , σ z t−1 = 1
1S1 = y 1S1 , Y 2S2 = y 2S2 Z


t−1
t−1
= Pr X1 (t) = m|Y t−1
= z t−1 , W 2 = w2 , σ z t−1 = 1
1S1 = y 1S1 , Z
=

1
"by (44)".
|F|

(47)

If 2 ∈
/ S1 then we write,


t−1
t−1
t−1
t−1
Pr X1 (t) = m|Y t−1
= z t−1 , W 2 = w2 , σ z t−1 = 1
1S1 = y 1S1 , Y 2S2 = y 2S2 Z
=

X 

n
o

t−1
t−1
t−1
t−1
Pr X1 (t) = m|Y t−1
= z t−1 , W 2 = w2 , σ z t−1 = 1
1S1 ∪{2} = y 1S1 ∪{2} , Y 2S2 = y 2S2 Z

y t−1
1{2}

o
n

t−1
t−1
t−1
t−1
t−1
t−1
t−1
t−1
=
y
|Y
=
y
,
Y
=
y
Z
=
z
,
W
=
w
,
σ
z
=
1
× Pr Y t−1
2
2
1S1
2S2
1S1
2S2
1{2}
1{2}
=

1
"by (47)".
|F|

Equality (46) follows by similar arguments.
From Lemma 9 and the definitions above we conclude the following.
Lemma 10. The following hold for all S1 ⊆ N1 , S2 ⊆ N2 .


t−1
1) If J z t−1 ∈ ht−1
= 1, then
1,∨i∈S , σ z
1


t−1
t−1
H X1 (t) |Y 1S
= y t−1
= z t−1 , σ(t) = 1 = 0.
1S1 , Z
1

2) If J z t−1 ∈ ht−1
1,∧i∈S

then

ī

1


t−1
t−1
H X1 (t) |Y t−1
= z t−1 , W 2 = w2 , σ(t) = 1 = 1.
1S1 = y 1S1 , Z

3) If J z t−1 ∈ ht−1
1,2̄∧i∈S

1

(48)

ī

(49)

then


t−1
t−1
t−1
t−1
H X1 (t) |Y t−1
= z t−1 , W 2 = w2 , σ(t) = 1 = 1.
1S1 = y 1S1 , Y 2S2 = y 2S2 , Z

4) For all G1 ⊆ N1 , S1 ⊆ N1 , G2 ⊆ N2 , S1 ⊆ N1 , if J z t−1 ∈ ht−1
1,2̄∧i∈S

1 ∪G1

ī

(50)

, then,


t−1
t−1
t−1
t−1
t−1
t−1
I W 2 , Y t−1
= z t−1 , σ (t) = 1 = 0.
1G1 , Y 2G2 ; X1 (t) |Y 1S1 = y 1S1 , Y 2S2 = y 2S2 , Z

(51)


Proof: Equality (48) follows from the fact that if J z t−1 ∈ ht−1
1,∨i∈S then X1 (t) = WJ(z t−1 ) is one of the
packets in y t−1
1S1 .
Equalities (49) and (50) follow from (44) and (45) respectively.
For (51) we write,

t−1
t−1
t−1
t−1
t−1
t−1
0 ≤ I W 2 , Y t−1
= z t−1 , σ (t) = 1
1G1 , Y G2 ; X1 (t) |Y 1S1 = y 1S1 , Y 2S2 = y 2S2 , Z

August 29, 2019

DRAFT

23


t−1
t−1
t−1
t−1
= H X1 (t) |Y t−1
= z t−1 , σ (t) = 1
1S1 = y 1S1 , Y 2S2 = y 2S2 , Z

t−1
t−1
t−1
t−1
t−1
t−1
− H X1 (t) |Y t−1
= z t−1 , W 2 , σ (t) = 1
1S1 = y 1S1 , Y 1G1 , Y 2S2 = y 2S2 , Y 2G2 , Z

t−1
t−1
t−1
t−1
= H X1 (t) |Y t−1
= z t−1 , σ (t) = 1
1S1 = y 1S1 , Y 2S2 = y 2S2 , Z
X

t−1
t−1
t−1
t−1
−
H X1 (t) |Y t−1
= z t−1 , W 2 = w2 , σ (t) = 1
1S1 ∪G1 = y 1S1 ∪G1 , Y 2S2 ∪G2 = y 2S2 ∪G2 , Z
t−1
y t−1
1G ,y 2G ,w 2
1



× Pr Y

2

t−1
1G1

t−1
t−1
t−1
t−1
t−1
t−1
t−1
= y t−1
= z t−1 , σ (t) = 1
1G1 , Y 2G2 = y 2G2 , W 2 = w 2 |Y 1S1 = y 1S1 , Y 2S2 = y 2S2 , Z



≤ 1 − 1 "by (50)"
= 0.

We can now connect information theoretic measures to scheduling times. This is done in the next lemma.
Lemma 11. The following hold:
1) For any S1 ⊆ N1 ,
n
X


t−1
H X1 (t) |Y t−1
, W 2 , σ(t) = 1 Pr (σ (t) = 1) = τ̃1,∧i∈S1 ī .
1S1 , Z

(52)

t=1

2) For any S1 ⊆ N1 , S2 ⊆ N2 ,
n
X


t−1
t−1
H X1 (t) |Y t−1
, σ(t) = 1 Pr (σ (t) = 1) ≤ τ̃1,∧i∈S1 ī .
1S1 , Y 2S2 , Z

(53)

t=1

3) For any S1 ⊆ N1 , S2 ⊆ N2 , if 2 ∈ S1 ,
n
X


t−1
t−1
H X1 (t) |Y t−1
, σ(t) = 1 Pr (σ (t) = 1) = τ̃1,∧i∈S1 ī .
1S1 , Y 2S2 , Z

(54)

t=1

4) If 2 ∈
/ S1 ,
n


X
t−1
t−1
t−1
I Y t−1
,
W
;
X
(t)
|Y
,
Y
,
Z
,
σ
(t)
=
1
Pr (σ (t) = 1) ≤ τ̃1,2∧i∈S1 ī .
2
1
1S
2S
1{2}
1
2

(55)

t=1

Proof: To show (52), using the definition of conditional entropy we write,
t−1
H X1 (t) |Y t−1
, W 2 , σ(t) = 1
1S1 , Z

August 29, 2019



DRAFT

24

X

=

t−1
t−1
H X1 (t) |Y t−1
= z t−1 , W 2 = w2 , σ(t) = 1
1S1 = y 1S1 , Z



t−1
y 1S
,z t−1 ,w2
1


t−1
t−1
× Pr Y t−1
= z t−1 , W 2 = w2 |σ(t) = 1
1S1 = y 1S1 , Z
X

t−1
t−1
=
H X1 (t) |Y t−1
= z t−1 , W 2 = w2 , σ(t) = 1
1S1 = y 1S1 , Z
t−1
y t−1
, w2
1S , z

t−1
J z t−1 ∈ h1,∨i∈S

1

× Pr Y
+

t−1
1S1

t−1
y t−1
1S1 , Z

=
X


= z t−1 , W 2 = w2 |σ(t) = 1

t−1
t−1
H X1 (t) |Y 1S
= y t−1
= z t−1 , W 2 = w2 , σ(t) = 1
1S1 , Z
1

t−1
y t−1
, w2
1S , z

t−1
t−1
J z
∈ h1,∧i∈S

1

ī


t−1
t−1
× Pr Y t−1
= z t−1 W 2 = w2 , |σ(t) = 1
1S1 = y 1S1 , Z
X

t−1
t−1
=
H X1 (t) |Y t−1
= z t−1 , W 2 = w2 , σ(t) = 1
1S1 = y 1S1 , Z
t−1
y t−1
, w2
1S , z

t−1
t−1
J z
∈ h1,∧i∈S

1

ī


t−1
t−1
× Pr Y t−1
= z t−1 W 2 = w2 , |σ(t) = 1 "by (48)"
1S1 = y 1S1 , Z
X

t−1
t−1
=
Pr Y 1S
= y t−1
= z t−1 W 2 = w2 , |σ(t) = 1 "by(49)”
1S1 , Z
1
t−1
y t−1
, w2
1S1 , z

t−1
J z t−1 ∈ h1,∧i∈S ī
1



t−1
t−1
= Pr J Z
∈ H1,∧i∈S ī |σ (t) = 1 .
1

Hence,

n
X



t−1
H X1 (t) |Y t−1
,
Z
,
σ(t)
=
1
Pr (σ (t) = 1)
1,S

t=1

=

n
X




t−1
Pr J Z t−1 ∈ H1,∧
|σ
(t)
=
1
Pr (σ (t) = 1)
i∈S ī

t=1

= τ̃1,∧i∈S ī "by (42)".
Inequality (53) follows by a similar argument, using the fact that H (X1 (t)) ≤ 1.
Equality (54) follows by a similar argument, using (50).
Inequality (55) follows similarly by observing also that


t−1
t−1
t−1
I Y t−1
, σ (t) = 1 = 0,
1{2} , W 2 ; X1 (t) |Y 1S1 , Y 2S2 , Z

if either J z t−1 ∈ ht−1
1,2̄∧i∈S

1

August 29, 2019

ī


(according to (51)), or J z t−1 ∈ ht−1
1,∨i∈S

1

i

(according to (48)).

DRAFT

25

B. Capacity Outer Bound
We can now proceed with the development of an outer bound to system capacity. The next lemma relates relates
achievable rates to node scheduling times.
Lemma 12. Let (R1 , R2 ) be achievable. Then,

0 ≤ nR1 − 1 − 1S τ̃1,∧i∈S ī ≤ o (n) , {2, 3} ⊆ S,
nR1
nR2
+
≤ n − τ̃1,2∧3̄ − τ̃1,3 + o(n),
1 − 123
1 − 24




nR2
1 − 13
1
13 − 123
nR
+
≤
n
−
τ̃
−
τ̃
+
τ̃1,2∧3̄ + o (n) ,
+
1
1,3
1,2∧3̄
(1 − 23 ) (1 − 123 ) 1 − 123
1 − 234
1 − 23


134 − 1234
1
+
(1 − 1234 ) (1 − 234 ) 1 − 123


nR1 +

(56)

(57)
(58)

1 − 134
1 − 14
1
nR
≤
n
−
τ̃
−
τ̃
+
ũ
+
ṽ
+ o (n) ,
2
1,3
1,2∧
3̄
1,2∧
3̄∧
4̄
1 − 24
1 − 234
1 − 24 1,2∧4̄
(59)

where
ũ1,2∧3̄∧4̄ ≤ τ̃1,2∧3̄∧4̄ ,

(60)

ũ1,2∧3̄∧4̄ + ṽ1,2∧4̄ ≤ τ̃1,2∧3̄∧4̄ + τ̃1,2∧3∧4̄ .

(61)

Proof: To show (56) we write according to (35),
I (W 1 ; Y

n
n
1S , Z |W 2 )

= 1−

1S

n
X


t−1
I W 1 ; X1 (t) |Y t−1
, W 2 , σ (t) = 1 Pr (σ (t) = 1)
1S , Z

t=1
n
X
1

= 1 − S


t−1
H X1 (t) |Y t−1
, W 2 , σ (t) = 1 Pr (σ (t) = 1)
1S , Z

t=1


"X(t)=WJ (Z t−1 ) =f W 1 , Z t−1 "

= 1 − 1S τ̃1,∧i∈S ī "by (52)".
Relation (56) follows now from (29).
To show (57), notice first that,
τ̃2 = n − τ̃1 "by(38)"
= n − τ̃1,2̄∧3̄ − τ̃1,2∧3̄ − τ̃1,3 "by definition"
≤n−

nR1
− τ̃1,2∧3̄ − τ̃1,3 + o(n) "by (56)".
1 − 123

(62)

Next according to (30),


t−1
nR2 ≤ I W 2 ; Y 2{4}
, Z n |W 1 + o(n),

August 29, 2019

(63)

DRAFT

26

and according to (35),
n




X
t−1
I W 2 ; Y n2{4} , Z n |W 1 = 1 − 24
I W 2 ; X2 (t) | W 1 , Y t−1
,
Z
,
σ(t)
=
2
Pr (σ(t) = 2)
2{4}
t=1
n
X
2

≤ 1 − 4

Pr (σ(t) = 2) "since H (X2 (t) ≤ 1)"

t=1


= 1 − 24 τ̃2 "by 39"



nR1
2
≤ 1 − 4 n −
− τ̃1,2∧3̄ − τ̃1,3 "by (62)".
1 − 123
The last inequality and (63) imply inequality (57).
Next we show inequality (58). According to (31),
n
t−1
nR1 ≤ I(W 1 ; Y t−1
1{3} , Y 2{3} , Z ) + o(n),

(64)

and according to (35),
n




X
t−1
t−1
I W 1 ; Y n1{3},2{3} , Z n = 1 − 13
I W 1 ; X1 (t) |Y t−1
,
Y
,
Z
,
σ
(t)
=
1
Pr (σ (t) = 1)
1{3}
2{3}
t=1
n


X
t−1
t−1
I W 1 ; X2 (t) |Y t−1
, σ (t) = 2 Pr (σ (t) = 2)
+ 1 − 23
1{3} , Y 2{3} , Z

= 1 − 13



t=1
n
X




t−1
t−1
H X1 (t) |Y 1{3}
, Y t−1
, σ (t) = 1 Pr (σ (t) = 1) "X1 (t) = f W 1 , Z t−1 "
2{3} , Z

t=1
n


X
t−1
t−1
I W 1 ; X2 (t) |Y t−1
,
Y
,
Z
,
σ
(t)
=
2
Pr (σ (t) = 2)
+ 1 − 23
1{3}
2{3}
t=1

≤ 1−

13

+ 1 − 23



τ̃1,3̄ "by (53)"

n


X
t−1
t−1
,
Z
,
σ
(t)
=
2
Pr (σ (t) = 2) .
,
Y
I W 1 ; X2 (t) |Y t−1
2{3}
1{3}
t=1

Combining the last inequality with (64) and using the fact that by definition,
τ̃1,3̄ = τ̃1,2̄∧3̄ + τ̃1,2∧3̄ ≤

nR1
+ τ̃1,2∧3̄ "by (56)",
1 − 123

we get by rearranging terms,


1
1
13 − 123
−
nR
=
nR1
1
(1 − 23 ) (1 − 123 )
(1 − 23 ) (1 − 123 )


1 − 13
≤
τ̃1,2∧3̄
(65)
1 − 23
n


X
t−1
t−1
+
I W 1 ; X2 (t) |Y t−1
,
Y
,
Z
,
σ
(t)
=
2
Pr (σ (t) = 2) + o (n) . (66)
1{3}
2{3}
t=1

Next, according to (30),


nR2 ≤ I W 2 ; Y n2{34} , Z n |W 1 + o(n),

August 29, 2019

(67)

DRAFT

27

and according to (35),
n




X
t−1
I W 2 ; Y n2{34} , Z n |W 1 = 1 − 234
I W 2 ; X2 (t) |Y t−1
,
Z
,
W
,
σ
(t)
=
2
Pr (σ(t) = 2)
1
2{34}
t=1
n



X
t−1
t−1
t−1
I W 2 ; X2 (t) |Y t−1
,
Y
,
Z
,
W
,
σ
(t)
=
2
”Y t−1
,W1 ”
= 1 − 234
1
1{34}
2{34}
1{34} = f Z
t=1
n


X
t−1
t−1
t−1
t−1
, W 1 , σ (t) = 2 "chain rule".
≤ 1 − 234
I W 2 , Y t−1
1{4} , Y 2{4} ; X2 (t) |Y 1{3} , Y 2{3} , Z
t=1

Combining with (67) we get
n


X
nR2
t−1
t−1
, W 1 , σ (t) = 2 + o (n) .
≤
I W 2 , Y n1{4},2{4} ; X2 (t) |Y t−1
1{3} , Y 2{3} , Z
2
1 − 34
t=1

(68)

Adding (66) and (68) we have,
13 − 123
nR2
nR1 +
≤
2
1
(1 − 3 ) (1 − 23 )
1 − 234


1 − 13
τ̃1,2∧3̄
1 − 23
n


X
t−1
t−1
+
I W 1 ; X2 (t) |Y t−1
, σ (t) = 2 Pr (σ (t) = 2)
1{3} , Y 2{3} , Z


t=1
n


X
t−1
t−1
+
I W 2 , Y n1{4},2{4} ; X2 (t) |Y 1{3}
, Y t−1
,
Z
,
W
,
σ
(t)
=
2
+ o (n)
1
2{3}
t=1


1 − 13
τ̃1,2∧3̄
1 − 23
n


X
t−1
t−1
+
I W 1 , W 2 , Y n1{4},2{4} ; X2 (t) |Y t−1
, σ (t) = 2 Pr (σ (t) = 2) + o (n)
1{3} , Y 2{3} , Z


=

t=1


≤

≤

1 − 13
1 − 23


τ̃1,2∧3̄ + τ̃2 + o (n) "by (39) and since H (X2 (t)) ≤ 1"


1

1 − 3
1 − 23

τ̃1,2∧3̄ + n −

nR1
− τ̃1,2∧3̄ − τ̃1,3 + o (n) "by (62)".
1 − 123

By rearranging terms we get (58).
It remains to show (59), (60) and (61). According to (32),
t−1
n
nR1 ≤ I(W 1 ; Y t−1
1{34} , Y 2{34} , Z |W 2 ) + o(n),

(69)

and according to (35),


I W 1 ; Y n1{34} , Y n2{34} , Z n |W 2
n


X
t−1
t−1
I W 1 ; X1 (t) |Y t−1
, W 2, σ (t) = 1 Pr (σ (t) = 1)
1{34} , Y 2{34} , Z

=

1 − 134

+

n


X
t−1
t−1
1 − 234
I W 1 ; X2 (t) |Y t−1
,
Y
,
Z
,
W
,
σ
(t)
=
2
Pr (σ (t) = 2) .
2
1{34}
2{34}

t=1

(70)

t=1

Now,


t−1
t−1
I W 1 ; X1 (t) |Y 1{34}
, Y t−1
,
Z
,
W
,
σ
(t)
=
1
2
2{34}

August 29, 2019

DRAFT

28




t−1
t−1
= H X1 (t) |Y t−1
, W 2 , σ (t) = 1 "X1 (t) = f W 1 , Z t−1 "
1{34} , Y 2{34} , Z


t−1
t−1
t−1
= I Y t−1
, W 2 , σ (t) = 1
1{2} ; X1 (t) |Y 1{34} , Y 2{34} , Z


t−1
t−1
, W 2 , σ (t) = 1
+ H X1 (t) |Y t−1
1{234} , Y 2{34} , Z


t−1
t−1
t−1
,
Z
,
W
,
σ
(t)
=
1
,
Y
;
X
(t)
|Y
= I Y t−1
2
1
2{34}
1{34}
1{2}




t−1
t−1
t−1
t−1
”
,
Z
=
f
W
,
Y
,
Z
,
W
,
σ
(t)
=
1
”Y
+ H X1 (t) |Y t−1
2
2
2
1{2}
2{34}
1{234}


t−1
t−1
t−1
, W 2 , σ (t) = 1
= I Y t−1
1{2} ; X1 (t) |Y 1{34} , Y 2{34} , Z


t−1
+ H X1 (t) |Y t−1
, σ (t) = 1 "W 2 independent of rest".
1{234} , Z

(71)

According to (52),
n
X



t−1
H X1 (t) |Y t−1
,
Z
,
σ
(t)
=
1
Pr (σ (t) = 1) = τ̃1,2̄∧3̄∧4̄
1{234}

t=1

≤

nR1
"by (56)"
1 − 1234

(72)

Replacing (71) and (72) in (70) we have


1 − 134
I W 1 ; Y n1{34} , Y n2{34} , Z n |W 2 ≤
nR1
1 − 1234
n


X
t−1
t−1
t−1
1
, W 2 , σ (t) = 1 Pr (σ (t) = 1)
+ 1 − 34
I Y t−1
1{2} ; X1 (t) |Y 1{34} , Y 2{34} , Z
+ 1 − 234



t=1
n
X



t−1
t−1
I W 1 ; X2 (t) |Y t−1
,
Y
,
Z
,
W
,
σ
(t)
=
2
Pr (σ (t) = 2) .
2
1{34}
2{34}

t=1

Combining the last inequality with (69) and rearranging terms we have,


n

1
1 − 134
1 − 134 X  t−1
t−1
t−1
I Y 1{2} ; X1 (t) |Y t−1
, W 2 , σ (t) = 1 Pr (σ (t) = 1)
−
nR1 ≤
1{34} , Y 2{34} , Z
2
1
2
2
1 − 34
(1 − 234 ) (1 − 34 )
1 − 34 t=1
+

n


X
t−1
t−1
I W 1 ; X2 (t) |Y 1{34}
, Y 2{34}
, Z t−1 , W 2 , σ (t) = 2 Pr (σ (t) = 2) + o (n) .
t=1

(73)
Next we write similarly,


nR2 ≤ I W 2 ; Y n1{4} , Y n2{4} , Z n + o (n) ,


I W 2 ; Y n1{4} , Y n2{4} , Z n
n


X
t−1
t−1
I W 2 ; X1 (t) |Y 1{4}
, Y t−1
,
Z
,
σ
(t)
=
1
Pr (σ (t) = 1)
2{4}

=

1 − 14

+

n


X
t−1
t−1
1 − 24
I W 2 ; X2 (t) |Y t−1
, σ (t) = 2 Pr (σ (t) = 2)
1{4} , Y 2{4} , Z

≤

n


X
t−1
t−1
1 − 14
I W 2 ; X1 (t) |Y t−1
,
Y
,
Z
,
σ
(t)
=
1
Pr (σ (t) = 1)
1{4}
2{4}

t=1

t=1

t=1

+ 1 − 24

n


X
t−1
t−1
t−1
t−1
I W 2 , Y t−1
,
Y
;
X
(t)
|Y
,
Y
,
Z
,
σ
(t)
=
2
Pr (σ (t) = 2) .
2
1{3}
2{3}
1{4}
2{4}

(74)

t=1

August 29, 2019

DRAFT

29

Combining the last two relations we conclude,
n

1 − 14 X 
nR2
t−1
t−1
t−1
≤
I
W
;
X
(t)
|Y
,
Y
,
Z
,
σ
(t)
=
1
Pr (σ (t) = 1)
2
1
1{4}
2{4}
1 − 24
1 − 24 t=1
+

n


X
t−1
t−1
t−1
t−1
I W 2 , Y t−1
,
Y
;
X
(t)
|Y
,
Y
,
Z
,
σ
(t)
=
2
Pr (σ (t) = 2) + o (n) .
2
1{3}
2{3}
1{4}
2{4}

(75)

t=1

Adding (73), (75), observing that by the chain rule,




t−1
t−1
t−1
t−1
t−1
t−1
t−1
I W 1 ; X2 (t) |Y t−1
,
Y
,
Z
,
W
,
σ
(t)
=
2
+
I
W
,
Y
,
Y
;
X
(t)
|Y
,
Y
,
Z
,
σ
(t)
=
2
2
2
2
1{34}
2{34}
1{3}
2{3}
1{4}
2{4}


t−1
t−1
t−1
t−1
= I W 1 , W 2 , Y 1{3}
, Y t−1
, σ (t) = 2
2{3} ; X2 (t) |Y 1{4} , Y 2{4} , Z
≤ 1,
and using (62) we obtain after rearranging terms,


134 − 1234
1
1
nR2 ≤ n − τ̃1,2∧3̄ − τ̃1,3
+
nR1 +
1
1
2
(1 − 234 ) (1 − 34 ) 1 − 23
1 − 24
+

n

1 − 134 X  t−1
t−1
t−1
t−1
I
Y
;
X
(t)
|Y
,
Y
,
Z
,
W
,
σ
(t)
=
1
Pr (σ (t) = 1)
1
2
1{2}
1{34}
2{34}
1 − 234 t=1

+

n

1 − 14 X 
t−1
t−1
t−1
I
W
;
X
(t)
|Y
,
Y
,
Z
,
σ
(t)
=
1
Pr (σ (t) = 1) .
2
1
1{4}
2{4}
1 − 24 t=1

Next, we write,





t−1
t−1
t−1
t−1
t−1
I Y t−1
, W 2 , σ (t) = 1 + I W 2 ; X1 (t) |Y t−1
, σ (t) = 1
1{2} ; X1 (t) |Y 1{34} , Y 2{34} , Z
1{4} , Y 2{4} , Z


t−1
t−1
t−1
= I Y t−1
, W 2 , σ (t) = 1
1{2} ; X1 (t) |Y 1{34} , Y 2{34} , Z


t−1
t−1
t−1
t−1
, σ (t) = 1
+ I Y t−1
1{3} , Y 2{3} , W 2 ; X1 (t) |Y 1{4} , Y 2{4} , Z


t−1
t−1
t−1
t−1
− I Y t−1
,
Y
;
X
(t)
|Y
,
Y
,
Z
,
W
,
σ
(t)
=
1
1
2
1{3}
2{3}
1{4}
2{4}


t−1
t−1
t−1
t−1
t−1
= I Y t−1
,
Y
,
Y
,
W
;
X
(t)
|Y
,
Y
,
Z
,
σ
(t)
=
1
2
1
1{2}
1{3}
2{3}
1{4}
2{4}


t−1
t−1
t−1
t−1
− I Y t−1
,
Y
;
X
(t)
|Y
,
Y
,
Z
,
W
,
σ
(t)
=
1
1
2
1{3}
2{3}
1{4}
2{4}


t−1
t−1
t−1
= I Y t−1
,
W
;
X
(t)
|Y
,
Y
,
Z
,
σ
(t)
=
1
2
1
1{2}
1{4}
2{4}


t−1
t−1
t−1
t−1
+ I Y t−1
, W 2 , σ (t) = 1
1{3} , Y 2{3} ; X1 (t) |Y 1{24} , Y 2{4} , Z


t−1
t−1
t−1
t−1
− I Y t−1
, W 2 , σ (t) = 1 .
1{3} , Y 2{3} ; X1 (t) |Y 1{4} , Y 2{4} , Z

(76)

We claim that,


t−1
t−1
t−1
t−1
I Y t−1
,
Y
;
X
(t)
|Y
,
Y
,
Z
,
W
,
σ
(t)
=
1
1
2
1{3}
2{3}
1{24}
2{4}


t−1
t−1
t−1
t−1
≤ I Y t−1
,
Y
;
X
(t)
|Y
,
Y
,
Z
,
W
,
σ
(t)
=
1
. (77)
1
2
1{3}
2{3}
1{4}
2{4}

August 29, 2019

DRAFT

30

To see this write,


t−1
t−1
t−1
t−1
I Y t−1
,
Y
;
X
(t)
|Y
,
Y
,
Z
,
W
,
σ
(t)
=
1
1
2
1{3}
2{3}
1{24}
2{4}


t−1
t−1
t−1
t−1
t−1
t−1
t−1
I Y t−1
,
Y
;
X
(t)
|Y
=
y
,
Y
=
y
Z
=
z
,
W
=
w,
σ
(t)
=
1
1
2
1{3}
2{3}
1{24}
2{4}
1{24}
2{4}

X

=

y t−1
y t−1 z t−1 ,w
1{24} 2{4}



t−1
t−1
t−1
t−1
=
y
Z
=
z
,
W
=
w|σ
(t)
=
1
,
× Pr Y = y t−1
,
Y
2
2{4}
2{4}
1{24}

(78)

and


t−1
t−1
t−1
t−1
I Y t−1
, W 2 , σ (t) = 1
1{3} , Y 2{3} ; X1 (t) |Y 1{4} , Y 2{4} , Z


t−1
t−1
t−1
t−1
t−1
t−1
I Y t−1
= z t−1 , W 2 = w, σ (t) = 1
1{3} , Y 2{3} ; X1 (t) |Y 1{4} = y 1{4} , Y 2{4} = y 2{4} Z

X

=

y t−1
y t−1 z t−1 ,w
1{24} 2{4}



t−1
t−1
t−1
t−1
= z t−1 , W 2 = w|σ (t) = 1 .
× Pr Y t−1
1{24} = y 1{24} , Y 2{4} = y 2{4} Z

(79)

We now consider the following (exhaustive) cases regarding the summation terms in (78) and (79).

1) J z t−1 ∈ ht−1
1,4 : In this case by (48), both mutual information terms in (78) and (79) are zero.

t−1
2) J z
∈ ht−1
: In this case by (48), the mutual information term in (78) is zero.
1,24̄

3) J z t−1 ∈ ht−1
: In this case by (51), both mutual information terms in (78) and (79) are zero.
1,2̄3̄4̄

t−1
t−1
4) J z
∈ h1,2̄34̄ : In this case, the mutual information term in (79) is one. This is due to the fact that by
(50),


t−1
t−1
t−1
t−1
t−1
=
y
Z
=
z
,
W
=
w,
σ
(t)
=
1
=1
=
y
,
Y
H X1 (t) |Y t−1
2
2{4}
1{4}
2{4}
1{4}
while by (48),


t−1
t−1
t−1
t−1
= z t−1 , W 2 = w, σ (t) = 1 = 0.
H X1 (t) |Y t−1
1{34} = y 1{34} , Y 2{34} = y 2{34} Z
Set now,
ũ1,2∧3̄∧4̄ =

n


X
t−1
t−1
t−1
I Y t−1
;
X
(t)
|Y
,
Y
,
Z
,
W
,
σ
(t)
=
1
Pr (σ (t) = 1)
1
2
1{2}
1{34}
2{34}
t=1

≤ τ̃1,2∧3̄∧4̄ "by (55)",
ṽ1,2∧4̄ =

n


X
t−1
t−1
I W 2 ; X1 (t) |Y t−1
, σ (t) = 1 Pr (σ (t) = 1) .
1{4} , Y 2{4} , Z
t=1

Using (76), (77) we conclude,
ũ1,2∧3̄∧4̄ + ṽ1,2∧4̄ ≤

n


X
t−1
t−1
t−1
I Y t−1
,
W
;
X
(t)
|Y
,
Y
,
Z
,
σ
(t)
=
1
2
1
1{2}
1{4}
2{4}
t=1

≤ τ̃1,2∧4̄ "by (55)"
= τ̃1,2∧3̄∧4̄ + τ̃1,2∧3∧4̄ .

We can now provide the proof of Theorem 2.

August 29, 2019

DRAFT

31

C. Proof of Theorem 2
Proof: (of Theorem 2) Let δ > 0 and assume n large enough so that o (n) /n ≤ δ for all inequalities in Lemma
12 and all n ≥ n(δ). Let R (δ) be the region of (R1 , R2 ) defined by,
R2
R1
+
≤ 1 − Q − S − U + δ,
(80)
1 − 123
1 − 24




13 − 123
R2
1 − 13
1
R
+
≤
1
−
Q
−
S
−
U
+
(Q + S) + δ,
(81)
+
1
(1 − 23 ) (1 − 123 ) 1 − 123
1 − 234
1 − 23


134 − 1234
1
1
1 − 134
1 − 14
+
R
+
R
≤
1
−
Q
−
S
−
U
+
Q
+
(S + U ) + δ, (82)
1
2
(1 − 1234 ) (1 − 234 ) 1 − 123
1 − 24
1 − 234
1 − 24
Q ≥ 0, S ≥ 0, U ≥ 0, Ri ≥ 0, i ∈ {1, 2} .
Taking into account that τ̃1,2∧3̄∧4̄ ≤ τ̃1,2∧3̄ τ̃1,2∧3∧4̄ ≤ τ̃1,3 , we conclude from (57) that,
τ̃1,2∧3̄∧4̄
τ̃1,2∧3∧4̄
R1
nR2
+
≤1−
−
+ δ.
1
2
1 − 23
1 − 4
n
n


Taking also into account that 1 − 13 / 1 − 23 ≤ 1, (58) implies that,




τ̃1,3
R2
1 − 13 τ̃1,2∧3̄
1
13 − 123
−
+δ
R1 +
≤1− 1−
+
(1 − 23 ) (1 − 123 ) 1 − 123
1 − 234
1 − 23
n
n


τ̃1,2∧3̄∧4̄
τ̃1,2∧3∧4̄
1 − 13 τ̃1,2∧3̄∧4̄
≤1−
−
+
+ δ.
n
n
1 − 23
n
and similarly (59) implies that,


τ̃1,2∧3∧4̄
τ̃1,2∧3̄∧4̄
134 − 1234
1 − 134 ũ1,2∧3̄∧4̄
1 − 14 ṽ1,2∧4̄
1
1
−
+
+
+ δ,
R1 +
R2 ≤ 1 −
+
1
1
2
2
2
(1 − 234 ) (1 − 34 ) 1 − 23
1 − 4
n
n
1 − 34
n
1 − 24 n
where,

ũ1,2∧3̄∧4̄
n

ũ1,2∧3̄∧4̄
τ̃1,2∧3̄∧4̄
≤
,
n
n
ṽ1,2∧4̄
τ̃1,2∧3̄∧4̄
τ̃1,2∧3∧4̄
+
≤
+
.
n
n
n

(83)
(84)

Consider now the change of variables,
ũ1,2∧3̄∧4̄
≥ 0,
n
ũ1,2∧3̄∧4̄
τ̃1,2∧3̄∧4̄
Sn =
−
≥ 0, "by (83)"
n
n
τ̃1,2∧3∧4̄
Un =
≥ 0.
n

Qn =

With this change of variables and since by (84)
ṽ1,2∧4̄
≤ Sn + Un ,
n
we see that (R1 , R2 ) satisfies the inequalities,
R1
nR2
+
≤ 1 − Qn − Sn + δ,
1 − 123
1 − 24




13 − 123
1
R2
1 − 13
+
R
+
≤
1
−
Q
−
S
+
(Qn + Sn ) + δ,
1
n
n
(1 − 23 ) (1 − 123 ) 1 − 123
1 − 234
1 − 23

August 29, 2019

DRAFT

32



134 − 1234
1
+
1
2
(1 − 234 ) (1 − 34 ) 1 − 123


R1 +

1
1 − 134
1 − 14
R
≤
1
−
Q
−
S
+
Q
+
(Sn + Un ) + δ,
2
n
n
n
1 − 24
1 − 234
1 − 24

Qn ≥ 0, Sn ≥ 0, Un ≥ 0, Ri ≥ 0, i ∈ {1, 2} .
From the last set of inequalities we conclude that for all δ > 0, (R1 , R2 ) ∈ R (δ) , which implies that (R1 , R2 ) ∈
R (0) = R.

D. Proof of Corollary 3
1) Let (R1 , R2 ) ∈ R. Then, conditions 13 ≥ 23 and (6) imply that the right hand sides of (4) and (5) are both
at most 1, hence (7) and (8) are satisfied, i.e., (R1 , R2 ) ∈ R1 . Assume next that (R1 , R2 ) ∈ R1 . Selecting
Q = S = U = 0, we conclude from (4), (5) that (7) and (8) are satisfied; also, (3) is implied by (8), i.e.,
(R1 , R2 ) ∈ R.
2) Since
1 − 134
1 − 14
≥
,
1 − 234
1 − 24
it is easy to see that the region R is equal to the region Ra defined by the following inequalities.
R2
R1
+
≤ 1 − Q,
1 − 123
1 − 24




13 − 123
1 − 13
R2
1
R1 +
≤1−Q+
Q,
+
(1 − 23 ) (1 − 123 ) 1 − 123
1 − 234
1 − 23


134 − 1234
1
1 − 134
1
R
+
R
≤
1
−
Q
+
Q,
+
1
2
(1 − 1234 ) (1 − 234 ) 1 − 123
1 − 24
1 − 234

(85)
(86)
(87)

Q ≥ 0, Ri ≥ 0, i ∈ {1, 2} .
To show that Ra is equal to the region R2 , we only need to show that replacing (85) with (10) does not
affect the region, since the rest of the inequalities are the same. Assume first that (R1 , R2 ) ∈ R2 . Inequalities
(10) and (12) imply that


134 − 1234
134 − 1234
1
1 − 134
1
R1 +
R2 ≤ 1 − Q +
+
R1
1
1
2
2
2
(1 − 234 ) (1 − 34 ) 1 − 23
1 − 4
1 − 34 (1 − 1234 ) (1 − 134 )
=1−Q+

134 − 1234
R1 ,
(1 − 1234 ) (1 − 234 )

hence (85) holds. We conclude that R2 ⊆ R1 . Next let (R1 , R2 ) ∈ Ra . If for the selected pair (R1 , R2 ) and
Q inequality (10) is satisfied, then R1 ⊆ R2 . Let us assume now that inequality (10) is not satisfied, i.e.,
134 − 1234
R1 < Q.
(1 − 1234 ) (1 − 134 )

(88)

We claim that for the same pair (R1 , R2 ) we can also select Q0 > 0 so that inequalities (10)-(12) are satisfied,
which will imply that (R1 , R2 ) ∈ R2 . To see this let Q0 be the infimum of all Q ≥ 0 satisfying (88) and
(85)-(87). From the definition it easily follows that Q0 satisfies (85)-(87) and
134 − 1234
R1 ≤ Q0 .
(1 − 1234 ) (1 − 134 )

August 29, 2019

(89)

DRAFT

33

We claim that Q0 satisfies (89) with equality. Indeed assume that
134 − 1234
R1 < Q0 .
(90)
(1 − 1234 ) (1 − 134 )


Multiplying both terms of (90) by 1 − 134 / 1 − 234 and adding the terms of the resulting inequality with
those of (85) we get,


1
134 − 1234
+
(1 − 1234 ) (1 − 234 ) 1 − 123


R1 +

1
1 − 134
R2 < 1 − Q̂ +
Q̂.
2
1 − 4
1 − 234

Hence we can reduce Q0 without violating (87). Since (85) ,(86) are actually strengthened by this reduction,
we conclude that we can find Q > 0 smaller that Q0 satisfying (88) and also (85)-(87), which contradicts
the definition of Q0 .
3) The arguments for item 3 are similar to those of item 2.
ACKNOWLEDGMENT
The work of Athanasios Papadopoulos was funded by ELIDEK since August 2017 and by the Onassis Foundation
(October 2015-August 2017).
R EFERENCES
[1] S. Haykin, “Cognitive radio: Brain-empowered wireless communications,” IEEE J. Sel. Areas Commun., vol. 23, no. 2, pp. 201–220, Feb.
2005.
[2] Q. Zhao and B. Sadler, “A survey of dynamic spectrum access,” IEEE Signal Processing Magazine, vol. 24, no. 3, pp. 79–89, May 2007.
[3] A. Goldsmith, S. A. Jafar, I. Maric, and S. Srinivasa, “Breaking spectrum gridlock with cognitive radios: An information theoretic
perspective,” Proc. IEEE, vol. 97, no. 5, pp. 894–914, Jan 2009.
[4] L. Lv, Q. Ni, Z. Ding, and J. Chen, “Application of non-orthogonal multiple access in cooperative spectrum-sharing networks over
nakagami-m fading channels,” IEEE Transactions on Vehicular Technology, vol. 66, no. 6, pp. 5506–5511, 2017.
[5] O. Simeone, Y. Bar-Ness, and U. Spagnolini, “Stable throughput of cognitive radios with and without relaying capability,” IEEE Trans.
Commun., vol. 55, pp. 2351–2360, Jan 2007.
[6] I. Krikidis, J. Laneman, J. Thompson, and S. Mclaughlin, “Protocol design and throughput analysis for multi-user cognitive cooperative
systems,” IEEE Trans. Wireless Commun., vol. 8, pp. 4740–4751, Jan 2009.
[7] R. Urgaonkar and M. Neely, “Opportunistic cooperation in cognitive femtocell networks,” IEEE J. Sel. Areas Commun., vol. 30, no. 3,
pp. 607 –616, April 2012.
[8] N. D. Chatzidiamantis, E. Matskani, L. Georgiadis, I. Koutsopoulos, and L. Tassiulas, “Optimal primary-secondary user cooperation policies
in cognitive radio networks,” IEEE Trans. Wireless Commun., vol. 14, no. 6, pp. 3443–3455, June 2015.
[9] A. Naeem, M. H. Rehmani, Y. Saleem, I. Rashid, and N. Crespi, “Network coding in cognitive radio networks: A comprehensive survey,”
IEEE Commun. Surveys Tuts., vol. 19, no. 3, pp. 1945–1973, Jan 2017.
[10] N. Li, M. Xiao, and L. K. Rasmussen, “Cooperation-based network coding in cognitive radio networks,” in IEEE 80th Veh. Techn. Conf.
(VTC Fall 2014), 2014, pp. 1–5.
[11] N. Li, L. K. Rasmussen, and M. Xiao, “Performance analysis of cognitive user cooperation using binary network coding,” IEEE Transactions
on Vehicular Technology, 2018.
[12] A. Papadopoulos, N. D. Chatzidiamantis, and L. Georgiadis, “Network coding techniques in cooperative cognitive networks,” in Global
Infrastructure and Netw. Symp. (GIIS 2018), Oct. 2018.
[13] ——, “Network coding techniques in cooperative cognitive networks,” arXiv preprint arXiv:1808.00263, pp. 1–13, 2018. [Online].
Available: https://arxiv.org/abs/1808.00263
[14] C.-C. Wang and J. Han, “The capacity region of two-receiver multiple-input broadcast packet erasure channels with channel output
feedback,” IEEE Transactions on Information Theory, vol. 60, no. 9, pp. 5597–5626, 2014.

August 29, 2019

DRAFT

34

[15] M. Gatzianas, L. Georgiadis, and L. Tassiulas, “Multiuser broadcast erasure channel with feedback-capacity and algorithms,” IEEE
Transactions on Information Theory, vol. 59, no. 9, pp. 5779–5804, 2013.
[16] A. Papadopoulos and L. Georgiadis, “Broadcast erasure channel with feeback and message side information, and related index coding
result,” IEEE Trans. Inf. Theory, vol. 63, no. 5, pp. 3161–3180, 2017.
[17] J. Han and C.-C. Wang, “General capacity region for the fully-connected 3-node packet erasure network,” IEEE Trans. Inf. Theory, vol. 62,
no. 10, pp. 5503–5523, Oct. 2016.
[18] W.-C. Kuo and C.-C. Wang, “Two-flow capacity region of the cope principle for wireless butterfly networks with broadcast erasure
channels.” IEEE Trans. Information Theory, vol. 59, no. 11, pp. 7553–7575, 2013.
[19] C.-C. Wang and N. B. Shroff, “Beyond the butterfly-a graph-theoretic characterization of the feasibility of network coding with two simple
unicast sessions,” in IEEE Int. Symp. Inf. Theory, (ISIT 2007).

IEEE, 2007, pp. 121–125.

[20] J. Han and C.-C. Wang, “Linear network coding capacity region of the smart repeater with broadcast erasure channels,” in IEEE Int. Symp.
Inf. Theory (ISIT 2016), Jul. 2016.

August 29, 2019

DRAFT

