arXiv:1912.08283v1 [cs.CV] 17 Dec 2019

Progressive VAE Training on Highly Sparse and Imbalanced Data
Dmitry Utyamishev
University of Illinois at Chicago
1200 W Harrison St, Chicago, IL 60607

Inna Partin-Vaisband
University of Illinois at Chicago
1200 W Harrison St, Chicago, IL 60607

dutyam2@uic.edu

vaisband@uic.edu

Abstract
In this paper, we present a novel approach for training a Variational Autoencoder (VAE) on a highly imbalanced data set. The proposed training of a high-resolution
VAE model begins with the training of a low-resolution
core model, which can be successfully trained on imbalanced data set. In subsequent training steps, new convolutional, upsampling, deconvolutional, and downsampling
layers are iteratively attached to the model. In each iteration, the additional layers are trained based on the intermediate pretrained model – a result of previous training iterations. Thus, the resolution of the model is progressively
increased up to the required resolution level. In this paper, the progressive VAE training is exploited for learning
a latent representation with imbalanced, highly sparse data
sets and, consequently, generating routes in a constrained
2D space. Routing problems (e.g., vehicle routing problem,
travelling salesman problem, and arc routing) are of special
significance in many modern applications (e.g., route planning, network maintenance, developing high-performance
nanoelectronic systems, and others) and typically associated with sparse imbalanced data. In this paper, the critical problem of routing billions of components in nanoelectronic devices is considered. The proposed approach
exhibits a significant training speedup as compared with
state-of-the-art existing VAE training methods, while generating expected image outputs from unseen input data. Furthermore, the final progressive VAE models exhibit much
more precise output representation, than the Generative Adversarial Network (GAN) models trained with comparable
training time. The proposed method is expected to be applicable to a wide range of applications, including but not
limited image impainting, sentence interpolation, and semisupervised learning.

Figure 1: Illustration of local connectivity within convolutional layers a CNN attribute that enables effective detection of local features in the input image.

tivity of convolutional layers, CNNs are commonly used for
detecting complex local patterns within high-resolution 2D
maps, as illustrated in Figure 1. In particular, image manipulation problems can be efficiently solved with CNNs [10].
Training a CNN model is, however, a non-trivial problem.
A typical CNN comprises multiple convolutional and deconvolutional layers separated with upsampling or downsampling layers. One of the most fundamental and common configurations of CNN is VAE a CNN topology
which enables the encoding of a high-dimensional 2D input
into a low-dimensional inner representation. Consequently,
the high-dimensional output image is reconstructed solely
based on the low-resolution inner representation. The structure of a stacked autoencoder is illustrated in Figure 2.
In its simplest form, the multidimensional VAE input and
output images are identical. In this case, VAE acts as an image compression tool. Alternatively, more complex VAE
configurations make processing of the input image possible
by mapping one image onto another. For advanced imageto-image transformations, a reasonably deep neural network
is required, yielding image processing with complex nonlinear logic. Increasing the number of VAE layers, however,
poses new challenges on the training process. For example, the vanishing gradient problem is a primary concern in

1. Introduction
A Convolutional Neural Network (CNN) [12] is a machine learning (ML) architecture. Owing to local connec1

Figure 2: Stacked VAE.
deep VAE networks. This problem is a direct consequence
of backpropagation (i.e., gradient descent) algorithm, which
minimizes the error function, J(Θ) : Rn → R, by iteratively updating the network model weights, Θ ∈ Rn , in
opposite direction to the gradient of the error function with
respect to the network weights, Θ := Θ − η∆J. Here η is
the learning rate of the algorithm. Intuitively, the gradients
become smaller with the increasing model accuracy. These
small gradients tend to further decrease through continuous
matrix multiplications in those inner network layers, significantly impeding the model training [9].
Furthermore, owing to the high dimensionality of the
data and, accordingly, the large number of learning parameters, those early layers of a neural network (as shown
by the green shaded substructure in Figure 2) are trained
significantly slower than the later layers (as shown by the
blue shaded substructure in Figure 2), intensifying the deep
training problem. In particular, the surface of the error
function becomes flatter with increasing number of weights
[18] and vanishing gradients, further decreasing the training
speed [9]. Finally, backpropagation with a flat error function often saturates in a local minimum, as it is illustrated in
the Figure 3. Training speed and convergence with complex
deep VAE architectures is, therefore, a primary concern and
the main focus of this paper.

Figure 3: Illustration of model training, saturating at a local
minimum.

complex transformations in neural networks) and no signal
degradation through multiple neural network layers. Alternatively, the zero derivative of ReLU function for a negative
argument increases the gradient vanishing probability for
the negative inputs. Saturation of backpropagation at zero
local minimum is, therefore, still a primary concern with
ReLU activation function.
Momentum-based training is yet another approach to
speed up deep training and mitigate the saturation in a local minimum with sparse input data. The effectiveness of
the traditional gradient descent algorithms degrades with
increasing sparsity of the training data [18]. Ideally, suboptimal local minima should be avoided. In practice, the
error function is, however, often a complex non-convex surface with saddle points surrounded by flat regions (i.e., constant error function). Escaping these regions is a significant
challenge for the gradient descent algorithms. Momentumbased functions, such as Adadelta [24] and RMSprop [18]
have been demonstrated to accelerate gradient descent convergence and mitigate oscillations of the algorithm caused
by ravines slopes around local minima. With these techniques, momentum is accumulated for those parameters
with similar gradient direction. As a result, oscillation is
reduced, and network model converges faster.
Sophisticated methods with adaptive learning rates have
also been considered. With these methods, deep learning
training converges faster with default network parameters,
eliminating the need for manually adjusting the learning
rates. As a result, deep training performance with sparse
input data is significantly increased with these approaches.
Yet another approach commonly used with computer vision problems is transfer learning [21]. To mitigate the com-

2. Related work
Several methods have been proposed to mitigate the issue of training speed and convergence. For example, reasonable training speedup and convergence has recently been
demonstrated with ReLU-based activation functions in deep
neural networks. The traditional sigmoid activation function saturates quickly at both, very positive and negative
argument values, yielding vanishing gradient values within
these argument ranges. Alternatively, with the ReLU activation function, f (x) = max(0, x), first derivative is unity
when function argument is positive. Thus, ReLU [1] activation function exhibits non-linear behavior (as required for
2

plexity, transfer learning approaches heavily rely on pretrained reference models. A common practice is to lock
majority of the pretrained model layers associated with fundamental low-level features, attach new layers associated
with an application/object-specific specialized feature, and
retrain, repurposing the learning features of the reference
model. To effectively leverage transfer learning with sparse
data sets, the number of locked layers should be increased
with the increasing sparsity of data. At the limit, all the pretrained model layers are locked and only the additional new
layers are trained.
The method proposed in this paper borrows from the
transfer learning approach, as described in Section 3. The
proposed method is experimentally verified and compared
with existing state-of-the-art methods, yielding a significant
increase in training speedup and performance, as described
in Section 4.

Figure 4: Illustration of progressive VAE training. The desired, high-resolution VAE shares layers with the intermediate autoencoders and the most inner VAE (as shown with
grey shade).
well as the principles of repurposing a pretrained model, as
demonstrated with transfer learning methods.
To train a stacked deep VAE, an inner low-dimensional
substructure of the autoencoder is first identified (referred
here as core VAE). The objective in the first iteration is to
determine an inner substructure, which (due to lower dimensionality of the inner VAE layers) enables successful
model training on sparse data. Once the core autoencoder is
identified, the outer, high dimensional layers are stripped,
and the core model is successfully trained on a low dimensional data set. In consequent iterations, the stripped
layers are progressively added back, and the model is retrained on training sets with increasingly higher resolutions.
Finally, a complete VAE is trained on the original, highresolution data set. This approach is illustrated in Figure 4
with a eight-step training process, yielding the lowest resolution core network, VAE0 (as shown with grey shade),
intermediate networks, VAE1 . . . VAE6 with progressively
higher resolutions, and the final high-resolution network
VAE, comprising layers shared among all the networks,
VAE0 . . . VAE6 .

3. Method
3.1. ML objective
A general image impainting problem is formulated in
this section as a supervised ML task. Let X and Y be the set
of, respectively, learning objectives and the corresponding
impainting solutions. For a single ML objective, x ∈ X , the
corresponding output image, yx ∈ Y, is an n × n bitmap of
pixels indexed, ti,j , 1 ≤ i, j ≤ n. Each pixel within an output image, yx is associated with a binary score, yxi,j = 0 or
yxi,j = 1 if the pixel ti,j is, respectively, excluded from or
included within the output image.
The primary goal is to train a ML system f (X , Θ) that
provides the conditional probability of each pixel, ti,j , to be
either included within (i.e., fi,j ≥ 0.5) or excluded from
(i.e., fi,j < 0.5) the preferred ML solution,
fi,j (X , Θ) = P (ti,j = 1|X ),

(1)

where Θ is the trained model of ML weights. The training data set {(Xi , Yi )}N
i=1 comprises N synthetic ML objectives in the bitmap representation and N corresponding
reference image outputs (i.e., the true labels). Note that the
model Θ is trained based on the joint probability distribution of the input features, Xi , and output observations, Yi ,
yielding a generative network.

3.2.1

VAE loss function

Mean square error (MSE) loss function is typically used
with autoencoders for evaluating sum of squared distances
between the predicted values and true labels [17]. This loss
function is, however, less effective with highly imbalance
data, where an empty output exhibits a small MSE, thus
yielding a legitimate solution. In a typical routing problem,
an expected routing output with a single, often quite short
path, exhibits high similarity with an empty (i.e., no path)
solution, yielding low routability with MSE metric. The
problem escalates with increasing input resolution, further
increasing the VAE training complexity and degrading the
accuracy of the trained models.

3.2. Concept overview
In many image-to-image transformations, training sets
are available with various resolution levels. While models
trained on high-resolution data often suffer from gradient
vanishing and other limitations (see Section 1) those lowerresolution models typically exhibit high performance and
convergence due to lower number of layers and model parameters [6, 9, 18]. The proposed solution leverages iterative training with gradually increasing dimensionality as
3

To account for specifics of routing problems with an unbalanced data set, a custom loss function is proposed. Note
that this function is relevant with other ML problems, such
as image impainting, super resolution, and style transfer
[13, 25, 14, 4, 22]. The proposed loss function is designed
to penalize the model if the number of tiles, nt , included by
the model within a routing path is different from the number of tiles in a reference routing path, nt,ref . The penalties
for nt exceeding and falling short of nt,ref differ. A path
with redundant tiles is not optimal in terms of its length. It
also reduces the routing capacity of the 2D space beyond the
expected. Yet, such a path is considered to be legal if it connects all the input/output pins. Alternatively, if nt < nt,ref
and the reference path is optimal, some components in the
model solution are disconnected and the path is, therefore,
incorrect. In particular, the nt < nt,ref penalization pertain
to the all-zeros local minimum. The proposed loss function accounts for |nt − nt,ref | 6= 0 with penalty rate of
ksub-opt , and for incomplete routes with additional penalty
rate of kerr , yielding the following loss function for a predicted routing output, ŷ, and a reference routing solution,
y ref ,

floss = MSE ŷ, y ref · (1 + ksub-opt · step · distance) , (2)

VAE v0 = train the core autoencoder;
list models = C ONSTRUCT(v0 );
T RAIN(models);

1

list Construct(VAE v0 )
while !(the required model resolution) do
Detach the first convolutional and down sampling
layers and the last upsampling and
deconvolutional layers. Consider the remaining
structure throughout the algorithm.

2

Attach a pair of new convolutional and
corresponding downsampling layers (in this
order) to the current most outer input layer.

3

Attach a pair of new upsampling and
deconvolutional layers (in this order) to the
current most outer output later.

4

Attach a new three-channel input and one-channel
output convolutional layers to layers described in
steps 2 and 3.

5

Append the outermost layers described in step 4 to
the models list.
end

where
distance =

X

H(ŷi,j ) −

i,j

X

ref
H(yi,j
)

void Train(list models)
for each model in models do
if training data for model is available then
Train model with the training data;
end
end

(3)

i,j

step = kerr · sign(distance − 1) + 1.

(4)

Here H() is the Heaviside [23] step function. In this paper,
the proposed loss function is used with ksub-opt = 10−3 and
kerr = 102 .
3.2.2

Algorithm 1: Progressive construction of VAE substructures.

Formal training approach

The progressive training process starts with identifying the
core autoencoder VAE0 . This core VAE is trained with a
training set of resolution 2n × 2n. After training, the layers
of VAE0 are locked and VAE1 is trained with training set of
resolution 22 n × 22 n. Note, that the inner layers of VAE1
are already trained and locked and only the outer layers are
trained. After all intermediate VAEi layers are trained with
the individual training sets of resolution 21+i n × 21+i n, the
final VAE is trained with the desired resolution. The pseudocode of the training algorithm is described in Algorithm
1.
Preliminary route-free training is another technique developed in this work for mitigating the probability of convergence to the all-zeros local minimum. For this purpose,
a route-free training set is generated. The true label for each
data point in this set comprises the tiles indices of the input
pins (and no additional routed tiles). Intuitively, this routefree training promotes inclusion of the pin tiles within the

routing path. As a result, the solution space is shifted towards the non-flat regions of the loss function hyperplane,
reducing the probability of the convergence to all-zeros solution.
Model training with stochastic gradient descent (SGD)
and RMSProp is shown in Figure 5 with the MSE function
and the customized floss function (see (2)(3)(4)), exhibiting
the convergence trends, as described in this section.

4. Experimental data
4.1. Experimental results
The proposed approach is demonstrated on a practical
routing problem [11]. Routing is a major phase of electronic circuit design process. During this phase, the electronic components that have previously been placed within
restricted space, are connected with physical wires with re4

(a)

(b)

(c)

Figure 5: Model training with various training methods and loss functions. (a) SGD optimization with MSE loss function,
saturating at a local minimum. (b) RMSProp optimization (with learning rate of 10−4 ) with MSE, saturating at a local
minimum. (c) Training of the outermost layers of the desired high-resolution model with progressive VAE, custom loss
function, floss , (see (2)(3)(4)), and preliminary training, converging to correct routing solution.
propose to solve this critical problem by formulating it as a
ML problem. Consider the following definitions.
A general routing problem is mapped here onto the classical image impainting problem, as defined in Section 3. Let
X and Y be the set of, respectively, single-path routing objectives (as defined by all the starting and ending points of
a single path) and the corresponding single-net routing solutions. A routing objective is to find the preferred routing
path of tiles, Yx ∈ Y, connecting a certain number of placed
pins within a given vertical and horizontal per-tile capacity,
as defined by x ∈ X . For any routing objective, x ∈ X , the
corresponding single-net routed output image, yx ∈ Y, is an
n × n bitmap of tiles indexed with their physical locations,
ti,j , 1 ≤ i, j ≤ n. Each tile within an output image, yx , is
associated with a binary score, yxi,j = 0 or yxi,j = 1 if the
routing tile ti,j is, respectively, excluded from or included
within a routing path.
Note that a primary objective is maximizing the routability (i.e., the number of routed paths), while minimizing the
overall wirelength (i.e., the total number of tiles included
within all the routed paths) of the routing solution. For
each routed wire, the number of tiles included within a path,
O(n), is significantly lower than the total number of tiles,
n2 , yielding a highly sparse and imbalanced data. These
definitions are illustrated in Figure 7. An example of correct and incorrect routing solution is shown in Figure 8.
Synthetically-generated exhaustively-routed training set
was produced with an auxiliary state-of-the-art router [16]
and a regular VAE model was trained with RMSprop [18]
training method. With this approach, training process has

spect to their intended functionality. For example, to implement a Boolean function N OR = (a + b) (inversion of a
OR b), signal pins a and b are connected with logic nets to
the individual inputs of gate OR, the output of the gate OR
is connected with a net to the input of gate N OT , and the
output of the gate N OT is connected to the output pin of
the system, as illustrated in Figure 6.
During the routing process, all the logic nets are implemented as physical routes within the technology constraints
(e.g., wires can only be routed in vertical and horizontal directions, but not horizontally). The wire route within a constrained area with limited net capacity exhibits a complex
path even with this simple two-pin function. Alternatively,
modern microprocessors comprise billions of Boolean gates
and complex technology and routing constraints [15]. Routing in these systems is a NP-hard problem and a critical
challenge for next generation high-performance nanoelectronic systems [5].
In this paper, an input image is represented by array of
pixels. Each tile exhibits several characteristics, such as
color channels and other special constraints. The input of a
image impainting problem is a set of per-pixel nts and a set
of all the logic nets, as defined by the physical input and output pin positions. The output of the routing problem is a set
of tiles that should be included within the routing paths of
the individual input nets. Note, that optimal routing of a net
with more than two pins (e.g., a net that connects the output
of an inverter to inputs of two other inverters) is a NP-hard
problem. Traditionally routing problems are solved with approximation methods, yielding suboptimal solution [3]. We
5

later nets. In this paper, the routability of the individual
models is measure as per cent of routability of a state-ofthe-art deterministic router, FastRoute 4.1.

4.2. Implementation and performance comparison
(a)

ISPD98 64 × 64 resolution benchmark ibm02.modified
[2] is used for evaluation. With three features per tile (vertical and horizontal available net capacity and binary pin metric), a total of 64 × 64 × 3 features are considered, yielding
a 64 × 64 × 3 dimensional input. Similarly, a 64 × 64 × 1 dimensional output space is required to describe the inclusion
or exclusion of each tile from the individual routing solutions. Architecture and detailed description of each layer of
the proposed VAE is illustrated in the Figure 10. A simple VAE, GAN, and progressive VAE network are implemented and evaluated with this benchmark. All networks
described in this section are prototyped in Python 3.7 using
Keras neural-network library [8] with Tensorflow backend.

4.3. Variational autoencoder
Stacked VAE is designed to solve the critical problem of
routing in nanoelectronic devices by utilizing ML imaging
methods and parallelization provided by GPU platforms.
Input and output spaces are defined as described in Section
3. This model is trained on the training set of 12,000 nets
defined within the 64 × 64 routing space. Input nets are
generated with random pin and obstacles locations. Here
synthetic obstacles are used to analogize the already placed
wires, fully or partially occupying the available net capacitance at certain tiles. The reference outputs (i.e., truth labels) are obtained with an auxiliary state-of-the-art router
FastRoute 4.1. The proposed VAE is trained on this synthetic model. Fastest convergence rate is observed with the
RMSprop method with cyclical learning rate. The trained
VAE is however unable to predict correct routing solutions
neither on new, previously unseen nets, nor on those nets
selected from the training set. In all these cases, a trivial
output of empty wiring paths is produced by the trained autoencoder. Example of a typical routing output with this
method is shown in Figure 9a. Benchmark routability with
simple VAE is, therefore, zero per cent of the routability
with the reference router on the same benchmark [2]. Note
those trivial cases where all pins are placed in the same tile
and no routing is required are excluded from the routability
calculations.

(b)

Figure 6: Schematic of a basic Boolean function N OR =
(a + b), (a) logic representation, where the connections between the gates are the logic nets of the system, and (b)
physical implementation in an electronic device. The physical wires connecting functional circuits are shown in red,
exhibiting a non-trivial path for this simple two-pin component.

not successfully converged, but saturated at a local minimum. The output of this model is, therefore, of limited use
to attain routing of high performance computing devices.
Due to sparsity of the data, the most common outcome of
the training is the saturation of the model in a local minimum that corresponds to an empty output with all output
tiles in the same class, yi,j = 0, ∀i, j. Routing problem is
selected in this paper as a demonstration vehicle due to its
high impact on the microprocessor industry and our ability
to perform perceptual evaluation of the results in addition
to synthetic ML performance metrics. To verify the correctness of the models, individual routing solutions are traversed with BFS algorithm, evaluating the connectivity of
all the pins and nets. Number of successfully routed nets
is used as a main metric due to its qualitative (i.e., indicates the system routing ability) and quantitative characteristics. If model tends to include less amount of tiles, that
is needed, pins may be not connected, and number of the
routed net will be lower, than reference. Similarly, if model
tends to include more tiles in the routing path, capacity of
design saturates faster, that leads to unavailability to route

4.4. Generative Adversarial Network
GAN systems are commonly used to generate previously
unseen images. With these methods, training is approached
as a minimax game with a generating and discriminating
mechanisms. The objective of the generator is to produce
output similar to the reference . Alternatively, the objective
of the discriminator is to distinguish between generated out6

Figure 7: Illustration of a single net routing objective x ∈ X and the corresponding single-net routed output yx ∈ Y.

(a)

(c)

(b)

Figure 8: A typical output of ML based router, (a) a blurry path is generated with a conventional neural network, (b) the
output of (a) is refined with the thresholding function, yielding a disconnected and, thus, illegal path, and (c) a legal routing
path.
puts and reference outputs. The structure of the proposed
GAN is shown in Figure 10. The network is trained on the
same input data as the simple autoencoder. The training
process has not converged after 10,000 iterations, yielding
a total runtime of ten hours on Nvidia GTX1080 GPU. Oscillations between two local minima as observed [7], exhibiting a typical mode collapse behavior of a GAN system [19]. As a result only few modes of a multimodal data
are generated, producing perceptually variable routing outputs, as illustrated in Figures 9b and 9c. All the results,
however, exhibit low performance based on the routability
metric and perceptual evaluation. A total of 2.7% of all
non-trivial nets are routed with this network, showing better result, than VAE router (2.7% non-trivial net routed), but
still not enough for using it in real applications.

resolution. For example, synthesizing exhaustively-routed,
64 × 64-resolution training set is significantly more time
consuming than synthesizing 4 training sets with resolutions of 64 × 64, 32 × 32, 16 × 16, and 8 × 8. The lowresolution core router comprises same layers, as layers 7-15
in Figure 10 and an additional 8 × 8 × 1 convolution output
layer. This router is trained on an 8 × 8 data set. With every
progressive VAE iteration, an additional convolutional and
pool layer is attached to the input of the pretpre-rained VAE
network and an additional upsamping and deconvolutional
layer is appended at the end of the network. Finally, an output i × i × 1 layer is added to convert the dimensionality
of the last convolutional layer, i × i, to the routing output
dimensionality of one.

5. Conclusion
4.5. Progressive VAE

This research introduces a new approach for iteratively
training VAE on highly sparse imbalanced data with progressively increasing training data resolution. The proposed method has been evaluated on routing benchmarks
[2], successfully generating routes between placed pins

The proposed progressive VAE network is designed to
utilize all available high, intermediate, and low-resolution
training sets. Note that generation of reference net routes
for model training exhibits high time complexity with data
7

(a)

(b)

(c)

(d)

Figure 9: Routing output of various neural networks trained on the same synthetic data set and evaluated on a typical unseen
three-pin net, (a) simple VAE, (b,c) GAN in two stable states, and (d) progressive VAE.

(a)

(b)
*25% dropout [20] (a fraction rate of input units set to zero at each update during the training to prevent overfitting)

Figure 10: Architecture of the proposed generative network, (a) block level schematics comprising convolutional, dense, and
deconvolutional layers, as well as an example of input features and impainting output, and (b) neural network parameters for
each layer of the 64 × 64 router.
in a constrained 2D space with limited routing capacity.
The proposed method exhibits faster convergence and 96%

routability, as compared with 0% and 2.7% routability with
simple VAE and GAN networks.

8

References

[16] Min Pan, Yue Xu, Yanheng Zhang, and Chris Chu. FastRoute: An efficient and high-quality global router. VLSI
Design, 2012, 2012.
[17] C Radhakrishna Rao. Some comments on the minimum
mean square error as a criterion of estimation. Technical
report, PITTSBURGH UNIV PA INST FOR STATISTICS
AND APPLICATIONS, 1980.
[18] Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016.
[19] Akash Srivastava, Lazar Valkov, Chris Russell, Michael U
Gutmann, and Charles Sutton. VEEGAN: Reducing mode
collapse in GANs using implicit variational learning. In
Advances in Neural Information Processing Systems, pages
3308–3318, 2017.
[20] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way
to prevent neural networks from overfitting. The journal of
machine learning research, 15(1):1929–1958, 2014.
[21] Lisa Torrey and Jude Shavlik. Transfer learning. In Handbook of research on machine learning applications and
trends: algorithms, methods, and techniques, pages 242–
264. IGI Global, 2010.
[22] Ruxin Wang and Dacheng Tao. Non-local auto-encoder
with collaborative stabilization for image restoration. IEEE
Transactions on Image Processing, 25(5):2117–2129, 2016.
[23] Eric W Weisstein. Heaviside step function. 2002.
[24] Matthew D Zeiler. ADADELTA: an adaptive learning rate
method. arXiv preprint arXiv:1212.5701, 2012.
[25] Hang Zhao, Orazio Gallo, Iuri Frosio, and Jan Kautz. Loss
functions for image restoration with neural networks. IEEE
Transactions on Computational Imaging, 3(1):47–57, 2016.

[1] Forest Agostinelli, Matthew Hoffman, Peter Sadowski, and
Pierre Baldi. Learning activation functions to improve deep
neural networks. arXiv preprint arXiv:1412.6830, 2014.
[2] Charles J Alpert. The ISPD98 circuit benchmark suite. In
Proceedings of the 1998 international symposium on Physical design, pages 80–85. ACM, 1998.
[3] Ke-Ren Dai, Wen-Hao Liu, and Yih-Lang Li. NCTUGR: Efficient simulated evolution-based rerouting and
congestion-relaxed layer assignment on 3-d global routing.
IEEE Transactions on very large scale integration (VLSI)
systems, 20(3):459–472, 2011.
[4] Aditya Deshpande, Jiajun Lu, Mao-Chuang Yeh, Min
Jin Chong, and David Forsyth. Learning diverse image colorization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6837–6845,
2017.
[5] Ding-Zhu Du, JM Smith, and J Hyam Rubinstein. Advances
in Steiner trees, volume 6. Springer Science & Business Media, 2013.
[6] Aude Genevay, Gabriel Peyré, and Marco Cuturi. GAN and
VAE from an optimal transport point of view. arXiv preprint
arXiv:1706.01807, 2017.
[7] Ian Goodfellow. NIPS 2016 tutorial: Generative adversarial
networks. arXiv preprint arXiv:1701.00160, 2016.
[8] Antonio Gulli and Sujit Pal. Deep learning with Keras. Packt
Publishing Ltd, 2017.
[9] Sepp Hochreiter. The vanishing gradient problem during
learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and KnowledgeBased Systems, 6(02):107–116, 1998.
[10] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1125–1134,
2017.
[11] Andrew B Kahng, Jens Lienig, Igor L Markov, and Jin Hu.
VLSI physical design: from graph partitioning to timing closure. Springer Science & Business Media, 2011.
[12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012.
[13] Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew Aitken,
Alykhan Tejani, Johannes Totz, and Zehan Wang. Photorealistic single image super-resolution using a generative adversarial network. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 4681–4690,
2017.
[14] Michael Mathieu, Camille Couprie, and Yann LeCun. Deep
multi-scale video prediction beyond mean square error.
arXiv preprint arXiv:1511.05440, 2015.
[15] Michael D Moffitt, Jarrod A Roy, and Igor L Markov. The
coming of age of (academic) global routing. In Proceedings of the 2008 international symposium on Physical design, pages 148–155. ACM, 2008.

9

