On the Convergence of Projected-Gradient Methods with
Low-Rank Projections for Smooth Convex Minimization
over Trace-Norm Balls and Related Problems

arXiv:1902.01644v2 [math.OC] 28 Nov 2020

Dan Garber
Technion - Israel Institute of Technology
dangar@technion.ac.il

Abstract
Smooth convex minimization over the unit trace-norm ball is an important
optimization problem in machine learning, signal processing, statistics and
other fields, that underlies many tasks in which one wishes to recover a lowrank matrix given certain measurements. While first-order methods for convex
optimization enjoy optimal convergence rates, they require in worst-case to
compute a full-rank singular value decomposition on each iteration in order to
compute the Euclidean projection onto the trace-norm ball. These full-rank
SVD computations however prohibit the application of such methods to largescale problems. A simple and natural heuristic to reduce the computational
cost of such methods is to approximate the Euclidean projection using only a
low-rank singular value decomposition. This raises the question if, and under
what conditions, this simple heuristic can indeed result in provable convergence
to optimal solutions.
In this paper we show that any optimal solution is a center of a Euclidean
ball inside-which the projected-gradient mapping admits rank that is at most
the multiplicity of the largest singular value of the gradient vector at this
optimal point. Moreover, the radius of the ball scales with the spectral gap
of this gradient vector. We show how this readily implies the local convergence (i.e., from a ”warm-start” initialization) of standard first-order methods
such as the projected-gradient method and accelerated gradient methods, using only low-rank SVD computations. We also quantify the effect of ”overparameterization”, i.e., using SVD computations with higher rank, on the radius of this ball, showing it can increase dramatically with moderately larger
rank. We extend our results also to the setting of smooth convex minimization
with trace-norm regularization and smooth convex optimization over boundedtrace positive semidefinite matrices. Our theoretical investigation is supported
by concrete empirical evidence that demonstrates the correct convergence of
first-order methods with low-rank projections for the matrix completion task
on real-world datasets.

1

Introduction

The main subject of investigation in this paper is the following optimization problem:
min f (X),

kXk∗ ≤1

1

(1)

where f : Rm×n → R is convex and β-smooth (i.e., gradient-Lipschitz), and k·k∗
denotes the trace-norm, i.e., sum of singular values (aka the nuclear norm).
Problem (1) has received much attention in recent years and has many applications in machine learning, signal processing, statistics and engineering, such as
the celebrated matrix completion problem [11, 30, 22], affine rank minimization
problems [31, 23], robust PCA [10], and more.
Many standard first-order methods such as the projected-gradient descent [28],
Nesterov’s accelerated gradient method [28] and FISTA [5], when applied to Problem (1), require on each iteration to compute the projected-gradient mapping w.r.t.
the trace-norm ball, given by
Πk·k∗ ≤1 [X − η∇f (X)],

(2)

for some current iterate X ∈ Rm×n and step-size η > 0, where Πk·k∗ ≤1 [·] denotes
the Euclidean projection onto the unit trace-norm ball.
It is well known that computing the projection step in (2) amounts to computing
the singular value decomposition of the matrix Y = X−η∇f (X) and projecting the
vector of singular values onto the unit simplex (keeping the left and right singular
vectors without change). Unfortunately, in worst-case, a full-rank SVD computation
is required which amounts to O(mn2 ) runtime per iteration, assuming m ≤ n. This
naturally prohibits the use of such methods for large scale problems in which both
m, n are large.
Since (2) requires in general expensive full-rank SVD computations, a very natural and simple heuristic to reduce the computational complexity is to replace the
expensive projection operation Πk·k∗ ≤1 [Y] with an approximate ”projection”, which
b r - the best rank-r approximation of Y. That is,
(accurately) projects the matrix Y
we consider replacing Πk·k∗ ≤1 [Y] with the operation
⊤
br
b
Π
k·k∗ ≤1 [Y] := Πk·k∗ ≤1 [Yr ] = Πk·k∗ ≤1 [Ur Σr Vr ],

where Ur Σr Vr⊤ corresponds to the rank-r truncated SVD of Y (i.e., we consider
only the top r components of the SVD).
Using state-of-the-art Krylov subspace methods, such as the Power Iterations
algorithm or the Lanczos algorithm (see for instance the classical text [18] and also
br
the recent works [26, 3]), Π
k·k∗ ≤1 [·] could be computed with runtime proportional
to only O(rmn) - a very significant speedup when r << min{m, n}. Moreover,
in many problems the gradient matrix is sparse (e.g., in the well studied matrix
completion problem), in which case further significant accelerations apply. The
br
drawback of course is that when using the approximated procedure Π
k·k∗ ≤1 [Y] , the
highly desired convergence guarantees of first-order methods need no longer hold.
A motivation for the plausible effectiveness of this heuristic is that Problem (1)
is often used as a convex relaxation for non-convex rank-constrained optimization
problems, which are often assumed to admit a low-rank global minimizer (as is
in all examples provided above). Given this low-rank structure of the optimal
solution, one may wonder if indeed storing and manipulating high-rank matrices
when optimizing (1) is mandatory, or alternatively, that at some stage during the
run of the algorithm the iterates all become low-rank.
2

It is thus natural to ask: under which conditions is it possible to replace the
br
projection Πk·k∗ ≤1 [·] with the approximation Π
k·k∗ ≤1 [·], while keeping the original
convergence guarantees of first-order methods?
Or, put differently, we ask for which X, r (and a suitable η) does
br
Πk·k∗ ≤1 [X − η∇f (X)] = Π
k·k∗ ≤1 [X − η∇f (X)]

(3)

hold?
Our main result in this paper is the formulation and proof of the following
proposition, presented at this point only informally.
Proposition 1. For any optimal solution X∗ to Problem (1), if the truncated-SVD
rank parameter r satisfies r ≥ r0 = #σ1 (∇f (X∗ )) - the multiplicity of the largest
singular value in the gradient vector ∇f (X∗ ) then, there exists a Euclidean ball
centered at X∗ , inside-which (3) holds. Moreover, the radius of the ball scales with
the spectral gap σ1 (∇f (X∗ )) − σr0 +1 (∇f (X∗ )).
As we show, Proposition 1 readily implies that standard gradient methods such
as the Projected Gradient Method and Nesterov’s Accelerated Gradient Method,
when initialized in the proximity of an optimal solution, converge with their original
convergence guarantees, i.e., producing the exact same sequences of iterates,
when the exact Euclidean projection is replaced with the truncated-SVD-based
br
projection Π
k·k∗ ≤1 [·].
Some complexity implications of our results to first-order methods for Problem (1) are summarized in Table 1, together with comparison to other first-order
methods.
The connection between r - the rank parameter in the approximated projecb k·k ≤1 [·] and the parameter #σ1 (∇f (X∗ )) may seem unintuitive at first. In
tion Π
∗
particular, one might expect that r should be comparable directly with rank(X∗ ).
However, as we show they are indeed tightly related. In particular, the radius of the
ball around an optimal solution X∗ in which (3) holds is strongly related to spectral
gaps in the gradient vector ∇f (X∗ ). This further implies ”over-parameterization”
results in which we show how the radius of the ball inside which (3) applies, increases with the rank parameter r, showing it can increase quite dramatically with
only a moderate increase in r. We also bring two complementary results showing
that rank(X∗ ) < #σ1 (∇(X∗ )) implies that the optimization problem (1) is ill-posed
in a sense, and that in general, a result in the spirit of Proposition 1 may not hold
when r < #σ1 (∇f (X∗ )).

1.1

Organization of this paper

The rest of this paper is organized as follows. In the remaining of this section we
discuss related work. In Section 2 we present our main result: we formalize and
prove Proposition 1 in the context of Problem (1). In this section we also present
several complementing results that further strengthen our claims. In Section 3 we
demonstrate how the results of Section 2 readily imply the local convergence of
standard projection-based first-order methods for Problem (1), using only low-rank
SVD to compute the Euclidean projection. In Sections 4 and 5 we formalize and
prove versions of Proposition 1 for smooth convex optimization with trace-norm
3

Algorithm

Conv.
↓

Rate

SVD size

β-smooth and convex f

Max iterates rank

↓

Projected Gradient

global

d

global

β/ǫ
p
β/ǫ

d

Accelerated Gradient

d

d

Frank-Wolfe [22]

global

β/ǫ

1

Proj. Grad. (this paper)

local

Acc. Grad. (this paper)

local
↓

global

Accelerated Gradient

global

BlockFW [2]

#σ1 (∇f (X∗ ))

∗

#σ1 (∇f (X∗ ))

#σ1 (∇f (X ))
#σ1 (∇f (X ))

β-smooth and α-strongly convex f ↓

Projected Gradident
ROR-FW [14]

β/ǫ
p
β/ǫ

min{t, d}
∗

global
global

Proj. Grad. (this paper)

local

Acc. Grad. (this paper)

local

(β/α) log 1/ǫ
p
β/α log 1/ǫ

d

d

d

d

β √
σmin (X∗ ) αǫ

1

(β/α) log 1/ǫ
(β/α) log 1/ǫ
p
β/α log 1/ǫ

min{t, d}
∗

rank(X )
∗

min{rank(X∗ )(β/α), d}

#σ1 (∇f (X ))

#σ1 (∇f (X∗ ))

#σ1 (∇f (X∗ ))

#σ1 (∇f (X∗ ))

Table 1: Comparison of first-order methods for solving Problem (1). The 2nd
column (from the left) states the type of convergence (either from arbitrary initialization or from a ”warm-start”), the 3rd column states the number iterations
to reach ǫ accuracy, the 4th column states and upper bound on the rank of SVD
required on each iteration, and the last column states an upper-bound on the rank
of iterates produced by the method.

regularization, and smooth convex optimization over the set of unit-trace positive
semidefinite matrices, respectively. Finally, in Section 6 we present supporting
empirical evidence.

1.2

Related work

The subject of efficient algorithms for low-rank matrix optimization problems has
enjoyed significant interest in recent years. Below we survey some notable results
both for the convex problem (1), as-well as other related convex models, and also
for related non-convex optimization problems.
Convex methods: Besides projection-based methods, other highly popular methods for Problem (1) are conditional gradient methods (aka Frank-Wolfe algorithm)
[13, 21, 22, 20]. These algorithms require only a rank-one SVD computation on
each iteration, hence each iteration is very efficient, however their convergence rates,
which are typically of the form O(1/t) for smooth problems (even when the objective is also strongly convex) are in general inferior to projection-based methods such
as Nesterov’s accelerated method [28] and FISTA [5]. Recently, several works have
developed variants of the basic method with faster rates, though these hold only under the additional assumption that the objective is also strongly convex [14, 2, 16].
Additionally, these new variants require to store in memory potentially high-rank
matrices, which may limit their applicability to large problems. In [34] the authors
present a novel conditional gradient method which enjoys a low-memory footprint
4

for certain instances of (1) such as the well known matrix completion problem,
however there is no improvement in convergence rate beyond that of the standard
method.
Besides first-order conditional gradient-type methods, in [25] the authors present
a second-order trust-region algorithm for the trace norm-regularized variant of (1).
Nonconvex methods: Problem (1) is often considered as a convex relaxation to
the non-convex problem of minimizing f under an explicit rank constraint. Two
popular approaches to solving this non-convex problem are i) apply projected gradient descent to the rank-constrained formulation, in which case the projection is
onto the set of low-rank matrices, and ii) incorporating the rank-constraint in the
objective by considering the factorized objective g(U, V) := f (UV⊤ ), where U, V
are m × r and n × r respectively, where r is an upper-bound on the rank, but otherwise unconstrained. Obtaining global convergence guarantees for these non-convex
optimization problems is a research direction of significant interest in recent years,
however efficient algorithms are obtained usually only under specific statistical assumptions on the data, which we do not make in this current work, see for instance
[23, 24, 12, 7, 17] and references therein.
In the works [6, 29] the authors consider first-order methods for factorized formulations of problems related to (1), which are not based on statistical assumptions.
In these works the authors establish the convergence of specific algorithms from a
good initialization point to the global low-rank optimum with convergence rates
similar to that of the standard projected gradient descent method.

2

Optimization over the Unit Trace-Norm Ball

We begin with introducing some notation. For a positive integer n we let [n] denote
the set {1, 2, . . . , n}. We let A • B denote the standard inner product for matrices,
i.e., A • B = Tr(A⊤ B). For a real matrix A, we let σi (A) denote its ith largest
singular value (including multiplicities), and we let #σi (A) denote the multiplicity
of the ith largest singular value. Similarly, for a real symmetric matrix A, we let
λi (A) denote its ith largest (signed) eigenvalue, and we let #λi (A) denote the
multiplicity of the ithe largest eigenvalue. We denote by X ∗ the set of optimal
solutions to Problem (1), and by f ∗ the corresponding optimal value.
For any X ∈ Rm×n , step-size η and radius τ we denote the projected gradient
mapping w.r.t. the trace-norm ball of radius τ :
Gη,τ (X) := Πk·k∗ ≤τ [X − η∇f (X)] .
When τ = 1, i.e., we consider the unit trace-norm ball, we will omit the τ subscript
and simply write Gη .
Given an optimal solution X∗ ∈ X ∗ , a step-size η > 0, and an integer r in
{rank(X∗ ), . . . min{m, n}}, we let δ(X∗ , η, r) denote the radius of the largest Euclidean ball centered at X∗ , such that for all X in the ball it holds that rank (Gη (X)) ≤
r. Or equivalently, δ(X∗ , η, r) is the solution to the optimization problem
sup δ ≥ 0 s.t.

∀X ∈ B(X∗ , δ) : rank (Gη (X)) ≤ r,
5

where B(X, R) denotes the Euclidean ball of radius R centered at X.
Towards formalizing and proving Proposition 1, deriving lower-bounds on the
radius δ(X∗ , η, r) will be our main interest in this section.
Since our objective is to study the properties of the projected-gradient mapping
over the trace-norm ball, we begin with the following well-known lemma which
connects between the SVD of the point to project and the resulting projection.
Lemma 1 (projection onto the trace-norm ball). Fix a parameter τ > 0. Let
Pmin{m,n]
X ∈ Rm×n and consider its singular-value decomposition X = i=1
σi ui vi⊤ .
If kXk∗ ≥ τ , then the Euclidean projection of X onto the trace-norm ball of radius
τ is given by
min{m,n}

Πk·k∗ ≤τ [X] =

X
i=1

max{0, σi − σ}ui vi⊤ ,

(4)

Pmin{m,n}
where σ ≥ 0 is the unique solution to the equation i=1
max{0,
Pr σi − σ} = τ .
Moreover, there exists r ∈ {1,
 . . . , min{m, n} − 1} such that i=1 σi ≥ τ + rσr+1
if and only if rank Πk·k∗ ≤1 [X] ≤ r.

Proof. The first part of the lemma is a well-known fact, see for instance
P[4]. The
second part of the lemma comes from the simple observation, that if ri=1 σi ≥
τ + rσr+1 for some r, then σ, as defined in the lemma, must satisfy σ ≥ σr+1 , in
which case Eq. (4) sets all bottom (min{m, n} − r) components of the SVD of X
to zero, and hence the projection is of rank at most r. It is not hard to show using
the same reasoning that the reversed direction also holds.
The following lemma which connects between the singular value decomposition
of an optimal solution and its corresponding gradient vector, will play an important
technical role in our analysis. The proof of the lemma follows essentially from simple
optimality conditions.
Lemma 2. Let X∗ ∈ P
X ∗ be any optimal solution and write its singular value
∗
decomposition as X = ri=1 σi ui vi⊤ . Then, the gradient vector ∇f (X∗ ) admits a
singular-value decomposition such that the set of pairs of vectors {(−ui , vi )}ri=1 is a
set of top singular-vector pairs of ∇f (X∗ ) which corresponds to the largest singular
value σ1 (∇f (X∗ )).
Proof. First, note that if ∇f (X∗ ) = 0 then the claim holds trivially. Thus, henceforth we consider the case ∇f (X∗ ) 6= 0.
∗
It suffices to show that for all i ∈ {1, . . . , r} it holds that −u⊤
i ∇f (X )vi =
σ1 (∇f (X∗ )).
∗
Assume by contradiction that for some i ∈ {1, . . . , r} it holds that −u⊤
i ∇f (X )vi <
∗
σ1 (∇f (X )). Let u, v denote a singular vector pair corresponding to the top
singular value σ1 (∇f (X∗ )). Observe that for all α ∈ (0, σi ], the point Xα :=
X∗ + α(−uv⊤ − ui vi⊤ ) is a feasible solution to Problem (1), i.e., kXα k∗ ≤ 1. Moreover, it holds that
(Xα − X∗ ) • ∇f (X∗ ) = α(−uv⊤ − ui vi⊤ ) • ∇f (X∗ )

< α(−σ1 (∇f (X∗ )) + σ1 (∇f (X∗ ))) = 0,

which clearly contradicts the optimality of X∗ .
6

Corollary 1. For any X∗ ∈ X ∗ it holds that rank(X∗ ) ≤ #σ1 (∇f (X∗ )). Moreover,
if ∇f is non-zero over X ∗ , it holds that
max{rank(X) | X ∈ X ∗ } ≤ min{#σ1 (∇f (Y)) | Y ∈ X ∗ }.

(5)

Proof. Lemma 2 directly implies that for all X∗ ∈ X ∗ it holds that rank(X∗ ) ≤
#σ1 (∇f (X∗ )).
For the second part of the lemma, suppose there exist X∗1 , X∗2 ∈ X ∗ such that
rank(X∗2 ) > #σ1 (∇f (X∗1 )).
Since ∇f (X∗1 ) 6= 0, it follows from simple optimality conditions that kX∗1 k∗ = 1,
which together with Lemma 2 implies that X∗1 •∇f (X∗1 ) = −σ1 (∇f (X∗ )). Moreover,
since rank(X∗2 ) > #σ1 (∇f (X∗1 )) and kX∗2 k∗ ≤ 1, it follows that X∗2 • ∇f (X∗1 ) >
−σ1 (X∗1 ).
Thus, using again the convexity of f we have that
f (X∗1 ) − f (X∗2 ) ≤ (X∗1 − X∗2 ) • ∇f (X∗1 ) < −σ1 (∇f (X∗1 )) + σ1 (∇f (X∗1 )) = 0
and hence we arrive at a contradiction.
One may wonder if the reversed inequality to (5) holds (i.e., the inequality holds
with equality). The following simple example shows that in general the inequality
can be strict. Consider the following example.
min {f (X) :=

kXk∗ ≤1

1
kX − Ak2F },
2

A = diag(1 + σ, σ, . . . , σ) ∈ Rm×n ,

for some σ > 0.
Clearly, using Lemma 1, the problem admits a unique optimal rank-one solution
solution X∗ = E1,1 , where E1,1 denotes the m×n diagonal matrix with only the first
entry along the main diagonal is non-zero and equal to 1. However, one can easily
observe that ∇f (X∗ ) = diag(−σ, . . . , −σ), meaning #σ1 (∇f (X∗ )) = min{m, n}.
While the above example demonstrates that in general it is possible that #σ1 (∇f (X∗ )) >>
rank(X∗ ) and that, as a result, Proposition 1 may not imply significant computational benefits for Problem (1), the following lemma shows that such cases always
imply that the optimization problem (1) is ill-posed in the following sense: increasing the radius of the trace-norm ball by an arbitrary small amount will cause the
projected gradient mapping to map such original low-rank solution to a higher rank
matrix, implying certain instability of low-rank optimal solutions.
Lemma 3 (gap necessary for stability of rank of optimal solutions). Suppose there
exists X∗ ∈ X ∗ of rank r ∗ such that ∇f (X∗ ) 6= 0, and suppose that #σ1 (∇f ∗ ) > r ∗ .
Then, for any step-size η > 0 and for any ǫ small enough, it holds that the projectedgradient mapping at X∗ w.r.t. the trace-norm ball of radius 1 + ǫ satisfies
rank (Gη,1+ǫ (X∗ )) ≥ #σ1 (∇f (X∗ )).
Proof. Fix some X∗ ∈ X ∗ and denote Y ∗ := X∗ − η∇f (X∗ ). Using Lemma 2 we
have that the singular values of Y ∗ are given by
∀i ∈ [r ∗ ] :
∀j > r ∗ :

σi = σi (X∗ ) + ηµ1 ;
σj

= ηµj ,
7

where {µi }i∈[min{m,n}] are the singular values of ∇f (X∗ ). Since ∇f (X∗ ) 6= 0,
which implies that kX∗ k∗ = 1, it holds that kY ∗ k∗ > 1. Let ǫ > 0 be such that
kY ∗ k∗ ≥ 1 + ǫ. Then, by Lemma 1, we have that the projected-gradient mapping
w.r.t. the trace-norm ball of radius (1 + ǫ) satisfies:
min{m,n}

X

Gη,1+ǫ (X∗ ) = Πk·k∗ ≤1+ǫ [Y ∗ ] =
where σ satisfies:
we have that

Pmin{m,n}
i=1

max{0, σi − σ} = 1 + ǫ. Observe that for σ = ηµ1 ,

min{m,n}

X
i=1

i=1

max{0, σi − σ}ui vi⊤ ,

∗

max{0, σi − σ} =

r
X

σi (X∗ ) = 1 < 1 + ǫ.

i=1

Thus, it must hold that σ < ηµ1 . However, then it follows that for all i ∈
[#σ1 (∇f (X∗ ))], σi − σ > 0 and thus, rank Πk·k∗ ≤1+ǫ [Y ∗ ] ≥ #σ1 (∇f ∗ ).

The following lemma demonstrates why setting the rank of the truncated-SVD
projection to be at least #σ1 (∇f (X∗ )) is necessary. The lemma shows that in
general, a result similar in spirit to Proposition 1 may not hold with SVD rank
parameter r satisfying r < #σ1 (∇f (X∗ )).
Lemma 4. Fix a positive integer n and r ∈ {2, . . . , n − 1}. Then, for any a ∈ (0, 1)
small enough and for any σ > 0, there exists a convex and 1-smooth function
f : Rn×n → R such that
1. f admits a rank-r minimizer over the unit trace-norm ball X∗ for which
it holds that #σ1 (∇f (X∗ )) = r + 1 and the spectral gap is σ1 (∇f (X∗ )) −
σr+2 (∇f (X∗ )) = σ,
2. √
there exists a matrix Xa such that kXa k∗ ≤ 1, rank(Xa ) = r, kXa − X∗ kF ≤
2a, and for any η ∈ (0, 1] it holds that rank(Gη (Xa )) = r + 1.
Proof. Consider the following function f : Rn×n → R.
2

r
1
1
1X
(X • Ei,i − (λi + σ))2 +
X • Er+1,r+1 − σ ,
f (X) :=
2
2
2
i=1

where Ei,i denotes the indicator for the ith diagonal entry. Note that f is indeed
1-smooth.
We set values λi = 1−a
r−1 , i = 1, . . . , r − 1, λr = a. It is not hard to verify that
the rank-r matrix
r−1

X∗ =

1−aX
Ei,i + aEr,r
r−1
i=1

is a minimizer of f over the unit trace-norm ball. In particular, it holds that
∗

∇f (X ) = −σ
8

r+1
X
i=1

Ei,i .

Hence, we have #σ1 (∇f (X∗ )) = r + 1 > rank(X∗ ).
Consider now the matrix Xa given by
r−1

1−aX
Ei,i + aEr+1,r+1 .
Xa =
r−1
i=1

Note that Xa is rank-r as well. Clearly, it holds that kX∗ − Xa kF =
Also,
∇f (Xa ) = −σ

r−1
X
i=1

√

2a.

a
Ei,i − (a + σ)Er,r + ( − σ)Er+1,r+1 .
2

Thus, for any step-size η ∈ (0, 1] we have

X
r−1
1−a
Ya := Xa − η∇f (Xa ) =
Ei,i + η(a + σ)Er,r
+ ησ
r−1
i=1
a
+(a − η + ησ)Er+1,r+1 .
2
Note that Ya has r + 1 positive singular values, which we denote (in nonincreasing order) by γ1 , . . . , γr+1 . In particular, for any a ≤ 1/r it holds that
γi = (1 − a)/(rP− 1) + ησ for i = 1, . . . , r − 1.
Note that r+1
i=1 γi > 1. Thus, by Lemma 1, the singular values of Gη (Xa ) are
given by max{γi − γ, 0}, i = 1, . . . , r + 1, where γ > 0 satisfies
r+1
X
i=1

max{0, γi − γ} = 1.

For rank(Gη (Xa )) ≤ r to hold, it must hold that γ ≥ γr+1 . We consider now
two cases.
In the first case we have η(a + σ) ≥ a(1 − η/2) + ησ, i.e., γr = η(a + σ),
γr+1 = a(1 − η/2) + ησ. Then, for rank(Gη (Xa )) ≤ r to hold, it must hold that
1=

r+1
X
i=1

max{0, γi − γ} ≤

r+1
X
i=1

max{0, γi − a(1 − η/2) − ησ}

< (r − 1)

1−a
+ ηa = 1,
r−1

and hence we arrive at a contradiction.
In the second case we have η(a + σ) < a(1 − η/2) + ησ, i.e., γr = a(1 − η/2) + ησ,
γr+1 = η(a + σ). As in the first case, in order for rank(Gη (Xa )) ≤ r to hold, it must
hold that
1=

r+1
X
i=1

max{0, γi − γ} ≤

r+1
X
i=1

max{0, γi − η(a + σ)}

< (r − 1)

1−a
3a
+a−η
< 1,
r−1
2

and thus, in this case also we arrive at a contradiction.
We thus conclude that rank(Gη (Xa )) = r + 1 > r.
9

We now present and prove our main technical theorem which lower bounds
δ(X∗ , η, r) - the radius of the ball around an optimal solution X∗ inside which the
projected gradient mapping Gη (·) has rank at most r, hence proving Proposition 1.
Theorem 1. Let f : Rm×n → R be β-smooth and convex. Assume ∇f is non-zero
over the unit trace-norm ball and fix some X∗ ∈ X ∗ . Let r denote the multiplicity
of σ1 (∇f (X∗ )), and let µ1 , µ2 , . . . , µmin{m,n} denote the singular values of ∇f (X∗ )
(including multiplicities). Then, for any η > 0 it holds that
δ(X∗ , η, r) ≥

η(µ1 − µr+1 )
√
.
(1 + 1/ r)(1 + ηβ)

(6)

More generally, for any η > 0 and r ′ ∈ {r, . . . , min{m, n} − 1}, it holds that
δ(X∗ , η, r ′ ) ≥

η(µ1 − µr′ +1 )
√
.
(1 + 1/ r)(1 + ηβ)

(7)

Moreover, for any η > 0 and r ′ ∈ {r, . . . , min{m, n} − r}, it holds that
√
rη(µ1 − µr′ +1 )
∗
′
δ(X , η, r + r − 1) ≥
.
2(1 + ηβ)

(8)

Proof. Throughout the proof we assume without loss of generality that m ≤ n. Fix
a step-size η > 0.
Denote Y ∗ := X∗ −η∇f (X∗ ) and let σ1 , . . . , σm denote the singular values of Y ∗ .
Let us also denote by µ1 , . . . µm the singular values of ∇f (X∗ ), and r ∗ := rank(X∗ ).
From Lemma 2 we can deduce that
∀i ∈ [r ∗ ] :
∀j > r ∗ :

σi = σi (X∗ ) + ηµ1 ;
σj

= ηµj .

(9)

For any integer r ′ ∈ {r, . . . , m} let us define
′

ξ(r ) =

r
X
i=1

σi − r · σr′ +1 − 1.

Since ∇f (X∗ ) 6= 0, it follows that
we have that

ξ(r ′ ) :=

r
X
i=1

Pr ∗

∗
i=1 σi (X )

=

Pr

∗
i=1 σi (X )

= kX∗ k∗ = 1,

σi − r · σr′ +1 − 1 = 1 + ηrµ1 − rηµr′ +1 − 1 = ηr(µ1 − µr′ +1 ),
(a)

(10)

where (a) follows from (9).
Now, given some X ∈ Rm×n , denote Y := X − η∇f (X) and let γ1 , . . . γm denote
the singular values of Y. It holds that

10

r
X
i=1

γi

≥

r
X

≥

r
X

(a)

=
≥
≥

(b)

i=1

i=1

r
X

i=1
r
X
i=1

r
X
i=1

σi −

r
X
i=1

σi (Y − Y ∗ ) ≥

r
X
i=1

v
u r
u X
σi − tr
σ 2 (Y − Y ∗ )
i

i=1

v
u m
r
X
u X
√
2
∗
t
σi − r
σi − rkY − Y ∗ kF
σ (Y − Y ) =
i

i=1

i=1

σi −
σi −
σi −

√

√
√

rkX − η∇f (X) − X∗ + η∇f (X∗ )kF

r (kX − X∗ kF + ηk∇f (X) − ∇f (X∗ )kF )
r(1 + ηβ)kX − X∗ kF ,

(11)

where (a) follows from Ky Fan’s inequality for the singular values, and (b) follows
from the β-smoothness of f (·).
Also, similarly, using Weyl’s inequality, it holds that
γr′ +1 ≤ σr′ +1 + σ1 (Y − Y ∗ ) ≤ σr′ +1 + (1 + ηβ)kX − X∗ kF .

(12)

Combining Eq. (10), (11), (12), we have that
′

r
X
i=1

γi − r ′ γr′ +1 ≥
≥

r
X

i=1
r
X
i=1

γi − rγr′ +1
σi −

√
r(1 + ηβ)kX − X∗ kF − r (σr′ +1 + (1 + ηβ)kX − X∗ kF )

r
X
√
(σi − σr′ +1 ) − (r + r)(1 + ηβ)kX − X∗ kF
=
i=1

= 1 + ξ(r ′ ) − (r +

√

r)(1 + ηβ)kX − X∗ kF .

(13)

Thus, it follows that if X satisfies:
kX − X∗ kF ≤

ξ(r ′ )
η(µ1 − µr′ +1 )
√
√
,
=
(r + r)(1 + ηβ)
(1 + 1/ r)(1 + ηβ)

P′
we have that ri=1 γi −r ′ γr′ +1 ≥ 1, which implies via Lemma 1 that rank (Gη (X)) ≤
r. This proves (6), (7).
Alternatively, for any r ′′ ≥ r ′ + r − 1, using the more general version of Weyl’s
inequality, we can replace Eq. (12) with
q
γr′′ +1 ≤ σr′ +1 + σr′′ −r′ +1 (Y − Y ∗ ) = σr′ +1 + σr2′′ −r′ +1 (Y − Y ∗ )
r
1
≤ σr′ +1 +
kY − Y ∗ k2F
′′
r − r′ + 1
1
≤ σr′ +1 + √ ′′
(1 + ηβ)kX − X∗ kF .
(14)
′
r −r +1
11

Thus, similarly to Eq. (13), but replacing Eq. (12) with Eq. (13), we obtain
′′

r
X
i=1

γi − r ′′ γr′′ +1 ≥

r
X
i=1

γi − rγr′′ +1 ≥



r
X
i=1

σi −

√

r(1 + ηβ)kX − X∗ kF


1
∗
−r σr′ +1 + √ ′′
(1 + ηβ)kX − X kF
r − r′ + 1


r
X
√
r
(1 + ηβ)kX − X∗ kF
(σi − σr′ +1 ) −
r + √ ′′
=
′+1
−
r
r
i=1


√
r
= 1 + ξ(r ′ ) −
r + √ ′′
(1 + ηβ)kX − X∗ kF .
′
r −r +1
In particular, for r ′′ = r ′ + r − 1, we obtain
′′

r
X
i=1

√
γi − r ′′ γr′′ +1 ≥ 1 + ξ(r ′ ) − 2 r(1 + ηβ)kX − X∗ kF .

Thus, it follows that if X satisfies:
ξ(r ′ )
rη(µ1 − µr′ +1 )
kX − X kF ≤ √
= √
=
2 r(1 + ηβ)
2 r(1 + ηβ)
∗

√

rη(µ1 − µr′ +1 )
,
2(1 + ηβ)

we have that rank (Gη (X)) ≤ r ′′ , which proves (8).
Note that the last two parts of Theorem 1, i.e., Eq. (7), (8), provide strong
”over-parameterization” results, showing that, depending on the singular values
of ∇f (X∗ ), the radius δ(X∗ , η, r) can increase dramatically by taking the rank
parameter r to be only moderately larger than #σ1 (∇f (X∗ )).
The following theorem complements Theorem 1, stating that the estimate (6)
on δ(X∗ , η, r) is tight up to a small universal constant.
Theorem 2. Fix β > 0, r ∈ {2, . . . , min{m, n} − 1}, a real scalar σ > 0, and
η ∈ (0, 1/β] such that ησ < 1. For any ǫ > 0 small enough, there exists a convex and
β-smooth function f : Rm×n → R which admits a rank-r minimizer over the unit
trace-norm ball X∗ for which #σ1 (∇f (X∗ )) = r, σ1 (∇f (X∗ ))− σr+1 (∇f (X∗ )) = σ,
and
√
δ(X∗ , η, r) < 2ησ + ǫ.
Proof. We use a construction similar to the one used in the proof of Lemma 4.
Consider the following function f : Rn×n → R.
f (X) :=

r
βX
(X • Eii − (λi + σ/β))2 ,
2
i=1

where Ei,i denotes the indicator for the ith diagonal entry.

12

We set values λi = 1−a
r−1 , i = 1, . . . , r − 1, and λr = a, for some a ∈ (0, 1) to be
determined later. It is not hard to verify that
r−1

1−aX
X =
Ei,i + aEr,r
r−1
∗

i=1

is a minimizer of f over the unit trace-norm ball. In particular, it holds that
∗

∇f (X ) = −σ

r
X

Ei,i .

i=1

Hence we have σ1 (∇f (X∗ )) − σr+1 (∇f (X∗ )) = σ.
Consider now the point Xa given by
r−1

Xa =

1−aX
Ei,i + aE(r+1),(r+1) .
r−1
i=1

Note that Xa is rank-r as well. Clearly, it holds that kX∗ − Xa kF =
Also,
∇f (Xa ) = −σ

r−1
X
i=1

√

2a.

Ei,i − (βa + σ)Er,r .

Thus, for any step-size η ∈ (0, 1/β] we have
Xa − η∇f (Xa ) =



X
r−1
1−a
Ei,i + η(βa + σ)Er,r + aE(r+1),(r+1) .
+ ησ
r−1
i=1

We now show that if rank (Gη (Xa )) ≤ r, then it must hold that a ≤ ησ. To see
this, assume by contradiction that a > ησ. We consider two cases. First, if a ≤
ηβa + ησ, then denoting the r + 1 non-zero singular values of Ya := Xa − η∇f (Xa )
by σ1 , . . . , σr+1 , we have that σr+1 = a. Thus, according to Lemma 1, in order for
rank (Gη (Xa )) ≤ r to hold, it must hold that
1 ≤

r
X
i=1

σi − rσr+1 = (1 − a) + (r − 1)ησ + ηβa + ησ − ra

= 1 − (r + 1)a + rησ + ηβa ≤ 1 − r(a − ησ) < 1,
(a)

(b)

where (a) follows from our assumption that η ∈ (0, 1/β] and (b) follows from the
assumption a > ησ. Hence, we arrive at a contradiction.
In the second case, we assume a > ηβa + ησ. Now, the smallest non-zero
singular value of Ya is σr+1 = ηβa + ησ. As before, using Lemma 1, now, in order
for rank (Gη (Xa )) ≤ r to hold, it must hold that
1 ≤

r
X
i=1

σi − rσr+1 = (1 − a) + (r − 1)ησ + a − r(ηβa + ησ)

= 1 − ησ − rηβa < 1.
13

Hence, in this case we also arrive at a contradiction.
Thus, we conclude that
∀η ∈ (0, 1/β] :

rank (Gη (Xa )) ≤ r =⇒ ησ ≥ a.

√
Thus, for any
ǫ > 0 small enough, setting a = ησ + ǫ/ 2 we have that
√
kXa − X∗ kF = 2ησ + ǫ, and that rank(Gη (Xa )) > r.
Since inside the ball of radius δ(X∗ , r, η) around X∗ all iterates of a projected
gradient-based method are of rank at most r, one may wonder if Theorem 2 still
holds if we restrict our attention to the intersection of this ball with the set of all
matrices with rank at most r. The answer is yes, since as we see from the proof,
the constructed ”bad” matrix Xa is of rank r as well.

3

Local convergence results with low-rank projections

In this section we discuss concrete algorithmic implications of Theorem 1 to the
local convergence of first-order methods for solving Problem (1). We demonstrate
the immediate applicability of our results to the local convergence of the projected
gradient descent method and Nesterov’s accelerated gradient method [27]. At the
end of this section we also partially discuss implications for the stochastic gradient
descent method [8].
Throughout this section we assume the following assumption holds true.
Assumption 1. For all X∗ ∈ X ∗ it holds that ∇f (X∗ ) 6= 0 (which in turn implies
that for all X∗ ∈ X ∗ , kX∗ k∗ = 1).
Theorem 3. [local convergence of Gradient Descent] Consider the sequence {Xt }t≥0
produced by the Projected Gradient Method (PGD):
X0 ∈ {X ∈ Rm×n | kXk∗ ≤ 1},

∀t ≥ 0 : Xt+1 ← G1/β (Xt ).

(15)

For any X∗ ∈ X ∗ and any r ≥ #σ1 (∇f (X∗ )), it holds that if PGD is initialized
with a feasible X0 which satisfies kX − X∗ kF ≤ δ(X∗ , r, 1/β), then, replacing the
projection Πk·k∗ ≤1 [·] in the mapping G1/β with the rank-r projection Π̂rk·k∗ ≤1 [·] does
not change the sequence {Xt }t≥0 .
In particular, the standard convergence guarantees on the error f (Xt ) − f ∗
(O(βkX0 − X∗ k2F /t) for convex f and O (exp(−Θ(α/β)t)) for α-strongly convex
f [27]) hold.
Proof. The proof is by simple induction. By definition of δ(X∗ , r, 1/β), using the
short notation δ = δ(X∗ , r, 1/β), we have that if Xt ∈ B(X∗ , δ) then rank(G1/β (Xt )) ≤
r, which means the accurate projection can be replaced with the rank-r truncated
projection Π̂rk·k∗ ≤1 [·]. Thus, it remains to show that if X0 ∈ B(X∗ , δ) then for all
t ≥ 1, Xt ∈ B(X∗ , δ) holds as well. However, the latter is a well known fact. Indeed
for the projected gradient mapping, for any feasible X and for any optimal solution X∗ it holds that kG1/β (X) − X∗ kF ≤ kX − X∗ kF , see for instance [8] (proof of
Theorem 3.7). Thus, the result holds.
14

Theorem 4. [local convergence of Accelerated Gradient Method for strongly convex
f ] Suppose f is α-strongly convex with α < β. Consider Nesterov’s Accelerated
Gradient Method for smooth and strongly convex minimization [28], given by the
update rule:
∀t ≥ 0 :

Xt+1 ← G1/β (Yt )
√
√
β− α
Yt+1 ← Xt+1 + √
√ (Xt+1 − Xt ),
β+ α

(16)

where Y0 = X0 ∈ {X | kXk∗ ≤ 1}. Let X∗ denote the unique optimal
solution
and let r ∈ {#σ1 (∇f (X∗ )), . . . , min{m, n}}. Then, if kX0 − X∗ kF ≤
√
α
√
δ(X∗ , r, 1/β), we have that replacing the projection Πk·k∗ ≤1 [·] in Eq. (16) with
3 α+β

the rank-r projection Π̂rk·k∗ ≤1 [·] does not change the produced sequences {Xt }t≥0 , {Yt }t≥0 .
In particular, the following standard convergence rate guarantee [28] holds
∀t ≥ 1 :

f (Xt ) − f (X∗ ) ≤


t
p
α+β
kX0 − X∗ k2F 1 − α/β .
2

Proof. As in the proof of Theorem 3, it suffices to show that for all t ≥ 0, kYt − X∗ kF ≤
δ(X∗ , r, 1/β). By the update rule in (16) we have that for all t ≥ 0
√
√
β− α
kYt+1 − Xt+1 kF ≤ √
√ kXt+1 − Xt kF ≤ kXt+1 − Xt kF .
β+ α
Thus, we have that
kYt+1 − X∗ kF

≤ kYt+1 − Xt+1 kF + kXt+1 − X∗ kF

≤ kXt+1 − Xt kF + kXt+1 − X∗ kF

≤ kXt − X∗ kF + 2kXt+1 − X∗ kF .
Thus, it suffices to show that for all t ≥ 0, kXt − X∗ kF ≤ δ(X∗ , r, 1/β)/3. The
proof is by simple induction. Clearly the claim holds for t = 0. Suppose now
that the claim holds for all iterations up to some t ≥ 0. Then, it follows that the
sequence {Xi }i∈[t+1] produced by replacing the accurate projection with the rank-r
truncated projection Π̂rk·k∗ ≤1 [·] is identical to one produced when using the accurate
projection. Thus, by the convergence rate of the accelerated gradient method stated
in the theorem, we have that

t+1
p
α+β
kX0 − X∗ k2F 1 − α/β
f (Xt+1 ) − f (X∗ ) ≤
2
α+β
kX0 − X∗ k2F .
≤
2
Now, using the strong-convexity of f we have that
kXt+1 − X∗ k2F ≤

2
α+β
(f (Xt+1 ) − f (X∗ )) ≤
kX0 − X∗ k2F .
α
α
√

α
δ(X∗ , r, 1/β), it indeed holds that kXt+1 − X∗ kF ≤
Thus, if kX0 − X∗ kF ≤ 3√α+β
δ(X∗ , 1/β, r)/3 as needed.

15

We remark that in the context of low-rank matrix optimization, especially in
statisticaly-motivated settings (e.g., [1, 6]), it is often assumed that the objective f
is not strongly-convex but only (α, k)-restricted strongly convex, which means in a
nutshell that the standard strong convexity inequality
f (X) − f (Y) ≤ (X − Y) • ∇f (X) −

α
kX − Yk2F
2

holds for any two matrices X, Y of rank at most k. It is not difficult to verify that
for k ≥ r, Theorems 3,4 imply local convergence with linear rate under the weaker
assumption of restricted strong convexity of f . A full account of this argument is
however beyond the scope of this paper.
We now turn to discuss the local convergence of the accelerated gradient method
without strong-convexity (or restricted strong-convexity). Unfortunately, unlike the
case for the standard projected-gradient method or the strongly-convex case, the
iterates of the method may in-principle step-outside of the ball around an optimal
solution X∗ in which the projected-gradient mapping is low-rank. For this reason,
in the non-strongly convex case we require a stronger initialization condition which
prohibits such divergence 1 . Also, for this result we assume the optimal solution
is unique, i.e., X ∗ = {X∗ }. We note this assumption is quite mild since naturally
the addition of a regularizing term of the form λkXk2F to the objective with λ > 0
being arbitrarily small, will result in such consequence.
Theorem 5. [local convergence of Accelerated Gradient Method without strong convexity] Assume there is a unique minimizer of f over the unit trace-norm ball, i.e.,
X ∗ = {X∗ }. Fix some integer r such that min{m, n} ≥ r ≥ #σ1 (∇f (X∗ )). Consider the function:
R(X) :=

sup
kZk∗ ≤1:

f (Z)≤f ∗ +c0 βkX−X∗ k2F

kZ − X∗ kF ,

where c0 is a universal constant to be specified in the sequel. Consider Nesterov’s
Accelerated Gradient Method [28], given by the update rule:
∀t ≥ 0 :

Xt+1 ← G1/β (Yt )

(17)

Yt+1 ← Xt+1 + bt (Xt+1 − Xt ),

where Y0 = X0 ∈ {X | kXk∗ ≤ 1}, and bt =

equation a2t+1 = (1 −
)} ≤ δ(X∗ , r, 1/β) , we

at (1−at )
,
a2t +at+1
2
at+1 )at ,

where at+1 ∈ (0, 1) is a

and a0 = 1/2. Then, if
solution to the quadratic
max{kX0 − X∗ kF , 3R(X0
have that replacing the projection
Πk·k∗ ≤1 [·] in(17) with the rank-r projection Π̂rk·k∗ ≤1 [·] does not change the produced
sequences {Xt }t≥0 , {Yt }t≥0 .
In particular, the following standard convergence rate guarantee holds
∀t ≥ 1 :

f (Xt ) − f (X∗ ) ≤ c0 βkX0 − X∗ k2F /t2 ,

where c0 is the suitable universal constant.
1

In particular, we remark that the introduction of the function R(·) in the theorem is very
similar in nature to use of the function R0 (·), which bounds the size of the initial level set, in the
work of Nesterov on randomized coordinate descent methods [27].

16

Proof. We prove by induction that for all t ≥ 0, kYt − X∗ kF ≤ δ = δ(X∗ , 1/β, r).
Clearly the induction holds for the base case t = 0, by our choice of Y0 . Suppose
now the induction holds for all iterations up to (and including) iteration t. Then, it
follows that the sequence {Xi }i∈[t+1] produced by replacing the accurate projection
with the rank-r truncated projection Π̂rk·k∗ ≤1 [·] is identical to one produced when
using the accurate projection. Thus, by the convergence rate of the accelerated
gradient method stated in the theorem, we have that
f (Xt+1 ) − f (X∗ ) ≤ c0 βkX0 − X∗ k2F /(t + 1)2 ,
which implies by the definition of the function R(·) that
kXt+1 − X∗ kF ≤ R(X0 ) ≤ δ/3.
A simple calculation shows that bt ∈ [0, 1] and thus we have that
kYt+1 − X∗ kF

= kXt+1 − X∗ + bt (Xt+1 − Xt )kF

≤ kXt+1 − X∗ kF + bt kXt+1 − Xt kF

≤ (1 + bt )kXt+1 − X∗ kF + bt kXt − X∗ kF

≤ 2kXt+1 − X∗ kF + kXt − X∗ kF ≤ δ,
hence the result follows.

We remark that while our focus with respect to accelerated gradient methods
was specifically on Nesterov’s method [28], similar results can be obtained in a
similar manner to other accelerated methods such as FISTA [5] (which we use for
our experiments).

3.1

Initialization, Certificates for Convergence, and Applications
to Global Optimization

All convergence results described above are local and hold only from a “warmstart” initialization which naturally depends on the lower bound on the radius
δ(X∗ , η, r), which in turn crucially depends on the spectral gap in ∇f (X∗ ), a quantity which is clearly not available in general, or easily estimated. Also, all methods
require an upper bound estimate r on the multiplicity of the largest singular value #σ1 (∇f (X∗ )). While the focus of this current work is mainly theoretical - attempting to provide theory in support of the empirical success of these methods (which is
also reported in Section 6), we now discuss several aspects of more practical flavor
concerning the use of the above results.
First, we note that if the objective is strongly convex with parameter α > 0, then
clearly a standard bound on the approximation error with respect to the function
value could be translated to a bound on the distance to X∗ . This could be used to
analyze the complexity of a scheme that applies basically any globally-convergent
optimization method at first, and then switches to the methods described here, once
close enough to X∗ .
More importantly, we would like to highlight that from a practical point of
view, it does not matter whether the method is initialized as described in the
17

above results, and knowledge of the spectral gap parameter is also not relevant in a
sense. In practice, it is mainly important that the truncated-SVD-based projection
is indeed the correct Euclidean projection, which facilitates the correct convergence
of the methods discussed. This however could be easily verified on each iteration,
as already mentioned in Lemma 1, and as we now reemphasize. The moderate price
for obtaining such a certificate for the projection, is to compute a rank-(r + 1) SVD
instead of only a rank-r SVD on each iteration.
Observation 1 (follows directly from Lemma 1). Let X ∈ Rm×n and consider
the point Y = X − η∇f (X), for some step-size η ≥ 0. Then, using the top (r + 1)
br
components of the SVD of Y, it is possible to both compute Π
k·k∗ ≤1 [Y] and to certify
b k·k ≤1 [Y] = Πk·k ≤1 [Y] holds or not.
whether Π
∗

∗

Thus, Observation 1 gives us a way to verify that the low-rank-projection-based
method indeed converges correctly, without requirement of unavailable parameters
such as the spectral gap in ∇f (X∗ ).
We can take the above observation a step further. Observation 1 allows us for
instance to combine a low-rank projected gradient method, such as in Theorem
3, with an efficient globally-convergent method, such as the conditional gradient
method (aka Frank-Wolfe method, see for instance [22]) with line-search, to obtain
an improved hybrid globally-convergent method. This could again be done without tuning of parameters such as the spectral gap. Once the conditional gradient
method gets close enough to an optimal solution, it switches to the low-rank projected gradient method. Concretely, consider updating the feasible iterate on each
step t, Xt according to the following scheme.
Given the feasible iterate Xt at step t of the algorithm:
◦ Compute the top r + 1 SVD components of Yt = Xt − β −1 ∇f (Xt ).
br
◦ If Π
k·k∗ ≤1 [Yt ] = Πk·k∗ ≤1 [Yt ] (using Observation 1) then
br
Xt+1 ← Π
k·k∗ ≤1 [Yt ]. (proj. grad. update)

◦ Otherwise, let (u, v) ∈ Rm × Rn be the top singular vectors of ∇f (Xt ).
Xt+1 ← (1 − ηt )Xt + ηt (−uv⊤ ), where


ηt ← arg minη∈[0,1] f (1 − η)Xt + η(−uv⊤ ) . (cond. grad. update)

It can be seen that in worst case, the above scheme requires one rank-(r+1) SVD
and an additional one rank-one SVD. Moreover, since both the projected gradient
method and the conditional gradient method with line-search are descent methods,
it is not difficult to show that the above scheme converges globally (i.e., without
any warm-start requirement) with rate O(β/t). Moreover, as can be seen, once the
above hybrid method gets into the ball of radius δ(X∗ , 1/β, r) around X∗ , only lowrank projected gradient updates are used, and hence from that point onwards, the
method only maintains iterates of rank at most r. This improves the storage issue
18

with the standard conditional gradient method, that typically maintains iterates
with rank that grows linearly with the iteration counter t, and thus after many
iterations requires to store high-rank matrices in memory which is inefficient for
large-scale problems. For a formal and full account of such an argument we refer
the reader to the sequel work [15] (see Section 2.3).
We note that while all methods considered (including the above hybrid approach) require to set the SVD rank parameter r, this task should not be very
difficult in practice since r is only required to satisfy r ≥ #σ1 (∇f (X∗ )), and so,
any upper-bound will work (in particular, as indicated in Theorem 1, increasing r
will also increase the radius δ(X∗ , η, r)). Moreover, again with the use of Observation 1, it could be easily verified during the run if the parameter r is set correctly.
The issue of setting a rank parameter r is inherent also in non-convex methods for
low-rank optimization which consider an explicit factorization of the matrix variable
(see the related work section).
Finally, we note that another practical issue when using low-rank SVD computations is the fact that in practice these computations are never accurate and are
only approximated using fast Krylov subspace methods such as power iterations or
the Lanczos algorithm (see for instance [26]). However, often in practice, setting
the accuracy of these methods when applied with an iterative gradient method is
not difficult. Accounting for such approximation errors in the convergence analysis
of first-order methods, while somewhat technical, is often straightforward and has
been discussed in many recent works (see for instance [21, 14, 2, 32]). Since these
considerations are orthogonal to the main arguments introduced in this work, we
omit such technical details, and assume all SVD computations are accurate.

3.2

Stochastic Gradient Descent with mini-batches

While our focus in this current paper is on deterministic first-order methods, we
briefly outline application of our results to the recently highly-popular Stochastic
Gradient Descent (SGD) method (see for instance [8]).
Assumption 2. Suppose that f : Rm×n → R is convex, β-smooth and ∇f is
nonzero on the unit trace-norm ball. Assume further that f is given by a stochastic
sampling oracle which, when queried with a matrix X, returns a random matrix Ĝ
such that
1. E[Ĝ|X] = ∇f (X),
2. kĜkF ≤ G for some scalar G ≥ 0.
Lemma 5. Let f : Rm×n :→ R be a function that satisfies Assumption 2. Fix some
X∗ ∈ X ∗ , a step-size η > 0 and an integer r ′ ∈ {r = #σ1 (∇f (X∗ )), . . . min{m, n} −
η(µ1 −µr ′ +1 )
, where µi , i = 1, . . . , min{m, n}
1}. Let X ∈ Rm×n be such that kX − X∗ kF ≤ 4(1+ηβ)
are the singular values of ∇f (X∗ ). Let Ĝ1 , . . . , Ĝk be stochastic gradients of f , produced by k queries with the matrix X to the stochastic oracle of f , and consider the
projected stochastic gradient mapping:
"
#
k
X
η
Gbη,k (X) := Πk·k∗ ≤1 X −
Ĝi .
k
i=1

19

Then, if
k≥

128G2
log ((m + n) min{m, n}) ,
3(µ1 − µr′ +1 )2

it holds that

h

i
E rank Gbη,k (X) ≤ r ′ + 1.

Proof. Clearly,
h

i




E rank Gbη,k (X)
≤ r ′ · Pr rank Gbη,k (X) ≤ r ′





+ min{m, n} · 1 − Pr rank Gbη,k (X) ≤ r ′ .

Let us denote Y ∗ = X∗ − η∇f (X∗ ), Y = P
X − η∇f (X), and Ŷ = X −
1
ˆ
ˆ
η ∇f (X), where we use the notation ∇f (X) = k ki=1 Ĝi . Let us further denote
by γ1 , . . . , γmin{m,n} the singular values of Y and by γ̂1 , . . . , γ̂min{m,n} the singular
values of Ŷ.
Going through the stepsof the proof of Theorem 1 we can see that a sufficient
condition for rank Gbη,k (X) ≤ r ′ to hold is that
r
X
i=1

γ̂i − rγ̂r′ +1 ≥ 1.

(18)

Using Weyl’s inequality for the singular values we have that
r
X
i=1

γ̂i − rγ̂r′ +1 ≥

r
X
i=1

γi − rγr′ +1 − 2rσ1 (Y − Ŷ)

≥ 1 + ηr(µ1 − µr′ +1 ) − (r +
−2rkY − Ŷk,

√

r)(1 + ηβ)kX − X∗ kF

where the last inequality follows from the lower bound on
in the proof of Theorem 1.
It holds that

Pr

i=1 γi −rγr ′ +1

developed

ˆ (X) − X + η∇f (X)k = ηk∇f
ˆ (X) − ∇f (X)k.
kŶ − Yk = kX − η ∇f
Thus, for (18) to hold, it suffices that
√
ˆ (X) − ∇f (X)k ≤ ηr(µ1 − µr′ +1 ).
(r + r)(1 + ηβ)kX − X∗ kF + 2ηrk∇f
Under our assumption on kX − X∗ kF , a sufficient condition for (18) to hold is
that
ˆ (X) − ∇f (X)k ≤ 1 (µ1 − µr′ +1 ).
k∇f
4
Using the Matrix-Bernstein inequality [33], we have that




3(µ1 − µr′ +1 )2 k
1
ˆ
′
.
Pr ∇f (X) − ∇f (X) > (µ1 − µr +1 ) ≤ (m + n) · exp −
4
128G2
20

Thus, for
k≥
we indeed obtain

128G2
log ((m + n) min{m, n}) ,
3(µ1 − µr′ +1 )2
h

i
E rank Gbη,k (X) ≤ r ′ + 1.

Using the same concentration arguments one can easily obtain a version of
Lemma 5 that holds with high probability, however these are beyond the scope of
this current paper.

4

Trace-Norm Regularization

In this section we focus our attention to a different optimization problem closely
related to Problem (1), which considers the trace-norm as a regularizer instead of
a feasible constraint.
min f (X) + kXk∗ .

X∈Rm×n

(19)

In this setting, first-order methods for composite optimization which handle the
trace-norm in a close-form manner, e.g., FISTA [5], rely on computing the softthresholding operator:
min{m,n}

Tη (X − η∇f (X)) =

X
i=1

max{σi − η, 0}ui vi⊤ ,

(20)

Pmin{m,n}
σi ui vi⊤ is the SVD of Y := X − η∇f (X), see for instance [9].
where i=1
In this section we overload notation and define the corresponding proximalgradient mapping:
Gη (X) := Tη (X − η∇f (X)).
Similarly, for any optimal solution X∗ ∈ X ∗ , step-size η and r ∈ {1, . . . , min{m, n}},
we also overload the notation δ(X∗ , η, r) and define it as
sup δ ≥ 0 s.t.

∀X ∈ B(X∗ , δ) : rank (Gη (X)) ≤ r,

where, as before, B(X, R) denotes the Euclidean ball of radius R centered at X.
The following theorem is analogues to Theorem 1.
Theorem 6. Let f : Rm×n → R be β-smooth and convex. Assume ∇f is non-zero
over X ∗ and fix some X∗ ∈ X ∗ . Let r denote the multiplicity of σ1 (∇f (X∗ )), and
let µ1 , µ2 , . . . , µmin{m,n} denote the singular values of ∇f (X∗ ) (including multiplicities). Then, for any η > 0 it holds that
δ(X∗ , η, r) ≥

η(µ1 − µr+1 )
.
1 + ηβ
21

(21)

More generally, for any r ′ ∈ {r, . . . , min{m, n} − 1} and any η > 0, it holds that
δ(X∗ , η, r ′ ) ≥

η(µ1 − µr′ +1 )
.
1 + ηβ

(22)

Moreover, for any r ′ ∈ {r, . . . , min{m, n} − r} and any η > 0, it holds that
√
rη(µ1 − µr′ +1 )
δ(X∗ , η, r ′ + r − 1) ≥
.
1 + ηβ

(23)

Proof. Since X∗ is an optimal solution it follows that
Gη (X∗ ) = Tη (X∗ − η∇f (X∗ )) = X∗ .
We first note that Lemma 2 also applies for the regularized problem, Problem
(19) (extending its proof to thePregularized case is straightforward). Thus, we can
∗
write the SVD of X∗ as X∗ = ri=1 σi ui vi , where each pair (−ui , vi ) is a singular
vectors pair corresponding to the top singular value of ∇f (X∗ ) - σ1 (∇f (X∗ ). Thus,
we have that
 ∗

min{m,n}
r∗
r
X
X
X
σi ui vi⊤ = Tη 
σi ui vi⊤ − η
µi (−ui vi⊤ )
X∗ =
i=1



= Tη 

i=1

min{m,n}

X
i=1

Thus, it follows that



(σi + ηµi )ui vi⊤ 

i=1

min{m,n}

X

=

i=1

∀i ∈ {1, . . . , r ∗ } : σi = σi + ηµi − η

max{σi + ηµi − η, 0}ui vi⊤ .

∀i ∈ {1, . . . , r ∗ } : µi = 1

=⇒
=⇒

∀i ∈ {1, . . . , r} : µi = 1.

In particular, it follows that for all i > r, µi < 1.
Now, given some point X, let us denote Y := X − η∇f (X), and let us denote
the singular values of Y by γ1 , . . . , γmin{m,n} .
Using the notation Y ∗ := X∗ − η∇f (X∗ ), it holds via Weyl’s inequality that for
all r ′ ≥ r
γr′ +1 ≤ σr′ +1 (Y ∗ ) + σ1 (Y − Y ∗ ) ≤ ηµr′ +1 + (1 + ηβ)kX − X∗ kF ,

where the bound on σ1 (Y − Y ∗ ) follows as in the proof of Theorem 1.
Clearly, by definition of Tη (·) it follows that if γr′ +1 < η then rank (Tη (Y)) ≤ r ′ .
η(1−µ

)

η(µ1 −µ

)

r +1
r +1
=
, then Tη (X − η∇f (X))
Thus, it follows that if kX − X∗ kF ≤ 1+ηβ
1+ηβ
′
has rank at most r . This proves (21), (22).
Alternatively, for any r ′′ ≥ r ′ + r − 1, using Weyl’s inequality, we have that
q
γr′′ +1 ≤ σr′ +1 (Y ∗ ) + σr′′ −r′ +1 (Y − Y ∗ ) = ηµr′ +1 + σr2′′ −r′ +1 (Y − Y ∗ )
r
1
1
kY − Y ∗ kF
≤ ηµr′ +1 +
kY − Y ∗ k2F = ηµr′ +1 + √ ′′
′′
′
r −r +1
r − r′ + 1
1 + ηβ
kX − X∗ kF
≤ ηµr′ +1 + √ ′′
′
r −r +1

√
η r ′′ −r ′ +1(µ −µ

)

′

′

1
r +1
Thus, if kX − X∗ kF ≤
it follows that Tη (X − η∇f (X)) has
1+ηβ
′′
rank at most r . This proves (23) when taking r ′′ = r ′ + r − 1.
′

22

5

Optimization with Positive Semidefinite Matrices

In this section we consider the related problem of optimization over positive semidefinite matrices with unit trace. Towards this end we define the spectrahedron:
Sn := {X ∈ Sn n | X  0, Tr(X) = 1}, where Sn denotes the space of n × n
real symmetric matrices, and consider the following optimization problem:
min f (X),

X∈Sn

where, as before, f : Rn×n → R is assumed to be convex and β-smooth. For ease of
presentation, throughout this section we assume that the gradient vector is always
a symmetric matrix, i.e., ∇f (X) ∈ Sn for all X ∈ Sn .
In this section we again overload notation and consider the projected-gradient
mapping w.r.t. Sn :
Gη (X) := ΠSn [X − η∇f (X)].
Similarly, for any optimal solution X∗ ∈ X ∗ , step-size η and integer r ∈
{1, . . . , n}, we also overload the notation δ(X∗ , η, r) and define it as
sup δ ≥ 0 s.t.

∀X ∈ B(X∗ , δ) : rank (Gη (X)) ≤ r.

We begin by recalling the structure of the Euclidean projection onto Sn .
n
Lemma 6 (projection
Pnonto the⊤spectrahedron). Let X ∈ S and consider its eigendecomposition X = i=1 λi vi vi , where λ1 ≥ λ2 · · · ≥ λn . The Euclidean projection
of X onto Sn is given by

ΠSn [X] =

n
X
i=1

max{0, λi − λ}vi vi⊤ ,

P
where λ ∈ R is the unique solution to the equation ni=1
λi − λ} = 1.
Pmax{0,
r
Moreover, there exists r ∈ {1, . . . , n − 1} such that i=1 λi ≥ 1 + rλr+1 if and
only if rank (ΠS [X]) ≤ r.
Proof. The first part of the Lemma is a well-known fact, see for instanceP[4].
r
To see why the second part holds, we first observe that in case
i=1 λi ≥
1 + rλr+1 , it must hold that λ ≥ λr+1 . This is true since if λ < λr+1 we have that
Tr (ΠSn [X]) =
=

n
X

i=1
r
X
i=1

max{0, λi − λ} ≥

r
X
i=1

max{0, λi − λ}

(λi − λ) > (1 + rλr+1 ) − rλr+1 = 1.

Thus, it follows that λ ≥ λr+1 . However, then it clearly follows that for all i ≥ r +1,
λi − λ ≤ 0 which implies that rank (ΠSn [X]) ≤ r. The reversed direction follows
from the same reasoning.
The following lemma is analogues to Lemma 2 and its proof (which we omit)
follows the same reasoning.
23

Lemma P
7. Let X∗ ∈ X ∗ be any optimal solution and write its eigendecomposition
as X∗ = ri=1 λi vi vi⊤ . Then, the gradient vector ∇f (X∗ ) admits an eigendecomposition such that the set of vectors {vi }ri=1 is a set of top eigen-vectors of (−∇f (X∗ ))
which corresponds to the eigenvalue λ1 (−∇f (X∗ )) = −λn (∇f (X∗ )).
The following theorem is analogues to Theorem 1.
Theorem 7. Let f : Sn → R be β-smooth and convex. Assume ∇f is non-zero
over Sn and fix some X∗ ∈ X ∗ . Let r denote the multiplicity of λn (∇f (X∗ )), and
let µ1 , . . . , µn denote the eigenvalues of ∇f (X∗ ) in non-increasing order. Then, for
any η > 0 it holds that
δ(X∗ , η, r) ≥

η(µn−r − µn )
√
.
(1 + 1/ r)(1 + ηβ)

(24)

More generally, for any r ′ ∈ {r, . . . , n − 1} and any η > 0, it holds that
δ(r ′ , η) ≥

η(µn−r′ − µn )
√
.
(1 + 1/ r)(1 + ηβ)

(25)

Moreover, for any r ′ ∈ {r, . . . , n − r} and any η > 0, it holds that
√
rη(µn−r′ − µn )
∗
′
.
δ(X , η, r + r − 1) ≥
2(1 + ηβ)

(26)

Proof. Fix a step-size η > 0. Denote Y ∗ := X∗ −η∇f (X∗ ) and let λ1 , . . . , λn denote
the eigenvalues of Y ∗ in non-increasing order.
Let us denote by r ∗ := rank(X∗ ). It follows from Lemma 7 that
∀i ∈ [r ∗ ] :

λi = λi (X∗ ) − ηµn ;

∀j > r ∗ : λj = −ηµn−j+1 .
P
P∗
Since ri=1 λi (X∗ ) = ri=1 λi (X∗ ) = Tr(X∗ ) = 1, we have that
′

ξ(r ) :=

r
X
i=1

λi − r · λr′ +1 − 1 = 1 − ηrµn + rηµn−r′ − 1 = ηr(µn−r′ − µn ).

(27)

Now, given a matrix X, denote Y := X − η∇f (X) and let γ1 , . . . γn denote the
eigenvalues of Y in non-increasing order. It holds that
v
u r
r
r
r
r
X
X
X
X
u X
∗
λi − tr
λi (Y − Y) ≥
λi −
γi ≥
λ2i (Y − Y ∗ )
i=1

(a)

≥
=

≥

(b)

i=1

v
u n
r
r
X
X
u X
√
2
∗
t
λi − rkY − Y ∗ kF
λ (Y − Y ) =
λi − r
i

r
X

r
X
i=1

r
X
i=1

i=1

i=1

i=1

i=1

≥

i=1

i=1

i=1

λi −

√
rkX − η∇f (X) − X∗ + η∇f (X∗ )kF

λi −

√
r (kX − X∗ kF + ηk∇f (X) − ∇f (X∗ )kF )

λi −

√
r(1 + ηβ)kX − X∗ kF ,
24

(28)

where (a) follows from Ky Fan’s inequality for the eigenvalues, and (b) follows from
the β-smoothness of f .
Also, similarly, it holds that
γr′ +1 ≤ λr′ +1 + λ1 (Y − Y ∗ ) ≤ λr′ +1 + (1 + ηβ)kX − X∗ kF .

(29)

Combining Eq. (27), (28), (29), we have that
′

r
X
i=1

γi − r ′ γr′ +1 ≥
≥
=

r
X

i=1
r
X

i=1
r
X
i=1

γi − rγr′ +1
λi −

√

r(1 + ηβ)kX − X∗ kF − r (λr′ +1 + (1 + ηβ)kX − X∗ kF )

(λi − λr′ +1 ) − (r +

= 1 + ξ(r ′ ) − (r +

√

√

r)(1 + ηβ)kX − X∗ kF

r)(1 + ηβ)kX − X∗ kF .

(30)

Thus, it follows that if X satisfies:
ξ(r ′ )
η(µn−r′ − µn )
√
√
=
,
(r + r)(1 + ηβ)
(1 + 1/ r)(1 + ηβ)

kX − X∗ kF ≤

P′
we have that ri=1 γi −r ′ γr′ +1 ≥ 1, which implies via Lemma 6, that ΠSn [X − η∇f (X)]
has rank at most r ′ . This proves (24), (25).
Alternatively, for any r ′′ ≥ r ′ + r − 1, using the more general version of Weyl’s
inequality, we can replace Eq. (29) with
q
γr′′ +1 ≤ λr′ +1 + λr′′ −r′ +1 (Y − Y ∗ ) ≤ λr′ +1 + λ2r′′ −r′ +1 (Y − Y ∗ )
r
1
≤ λr′ +1 +
kY − Y ∗ k2F
′′
r − r′ + 1
1
(1 + ηβ)kF .
(31)
≤ λr′ +1 + √ ′′
r − r′ + 1
Thus, similarly to Eq. (30), but replacing Eq. (29) with Eq. (31), we obtain

′′

r
X
i=1

′′

γi − r γr′′ +1 ≥

r
X
i=1

γi − rγr′′ +1 ≥

r
X
i=1

λi −

√

r(1 + ηβ)kX − X∗ kF


1
(1 + ηβ)kF
−r λr′ +1 + √ ′′
r − r′ + 1


r
X
√
r
(1 + ηβ)kX − X∗ kF
(λi − λr′ +1 ) −
r + √ ′′
=
′+1
−
r
r
i=1


√
r
′
= 1 + ξ(r ) −
r + √ ′′
(1 + ηβ)kX − X∗ kF .
r − r′ + 1


In particular, for r ′′ = r ′ + r − 1, we obtain
′′

r
X
i=1

√
γi − r ′′ γr′′ +1 ≥ 1 + ξ(r ′ ) − 2 r(1 + ηβ)kX − X∗ kF .
25

Thus, it follows that if X satisfies:
ξ(r ′ )
kX − X kF ≤ √
=
2 r(1 + ηβ)
∗

√

rη(µn−r − µn )
,
2(1 + ηβ)

we have that ΠSn [X − η∇f (X)] has rank at most r ′ + r − 1, which proves (26).

6

Motivating Empirical Evidence

Our goal in this final section is to provide empirical evidence motivating our theoretical investigation. In particular, focusing on the well-studied low-rank matrix
completion problem [11, 30, 22], our results demonstrate that i) the optimal solution
in real-world datasets is indeed low-rank, and ii) standard first-order methods, when
initialized in a very simple and efficient way, indeed converge correctly using only
SVD computations with rank that either matches the rank of the optimal solution
or exceeds it by a very small constant (1 or 2 in our experiments). To be clear, by
the phrase converge correctly, we mean that the methods produce exactly the same
iterates as they would have produced when using a full-rank SVD computation on
each iteration. This fact is verified on each iteration by checking that the condition
stated at the end of Lemma 1 indeed holds.
To be more concrete, we consider the task of low-rank matrix completion in the
following convex optimization formulation:
min {f (X) :=

kXk∗ ≤τ

X

(i,j)∈S

(X • Ei,j − ri,j )2 },

where S ⊂ [m] × [n] is the set of observed entries and ri,j denotes the observed
value.
We apply all algorithms with the following simple initialization scheme. We set
br
the first iterate X0 to X0 := Π
k·k∗ ≤τ [R], where
Ri,j =



ri,j P
1
|S|

if (i, j) ∈ S;
r
/ S.
(i,j)∈S i,j if (i, j) ∈

In words: we construct a matrix which contains the observed matrix entries and
every unobserved entry is set to the mean value of observed entries. We then
compute the initialization by projecting the rank-r truncated SVD of this matrix
onto the trace-norm ball.
We use two highly popular datasets for the matrix completion problem, namely
MovieLens100k (943 × 1682 matrix, 100,000 observed entries) and MovieLens1M
(6040 × 3952 matrix, 1,000,209 observed entries) [19]. For each dataset we experiment with different trace bounds (parameter τ ) which naturally influences the
optimal value and the rank of the optimal solution. For every dataset and trace
1
f (X∗ )), the rank of the optibound τ we find the optimal mean-square-error ( |S|
mal solution, the multiplicity of the largest singular value in the gradient vector
- #σ1 (∇f (X∗ )), and the spectral gap between this largest singular value and the
second largest (not counting multiplicities). These values are found by running any
26

of the methods until a point with negligible dual-gap is reached, that is, we find a
point Xǫ such that
max (Xǫ − V) • ∇f (Xǫ ) ≤ ǫ.

kVk∗ ≤τ

Since f is convex, this implies in particular that f (Xǫ ) − f (X∗ ) ≤ ǫ. For ML100k
we use ǫ = 0.01 and for ML1M we use ǫ = 0.5.
For each of the algorithms tested - the standard projected-gradient method
(PGD) [28] and FISTA [5], we manually find the minimum rank parameter r for
which the algorithm converges correctly from the ”warm-start” initialization to the
optimal solution using only a rank-r SVD computation on each iteration. These
parameters are recorded in the columns titled ”PGD rank” and ”FISTA rank” in
Table 2.
As can be seen in Table 2, the results clearly show that in all considered cases
it indeed holds that rank(X∗ ) = #σ1 (∇f (X∗ )), and that rank(X∗ ) is significantly
smaller than min{m, n}. We also see that PGD converges correctly to the optimal
solution with rank that does not exceed that of the optimal solution, while in some
cases FISTA requires rank a bit larger than that of the optimal solution (by at most
2).
dataset

ML100k

ML1M

trace

rank(X∗ )

#σ1 (∇f ∗ )

FISTA rank

PGD rank

MSE

spectral gap

2500

3

3

3

3

1.3589

5.5844

3000

10

10

10

10

0.9871

0.3234

3500

41

41

42

41

0.7573

0.0456

4000

70

70

71

70

0.5846

0.0227

5000

117

117

118

117

0.3314

0.0148

10000

3

3

3

3

1.2184

3.2861

12000

12

12

12

12

0.9043

1.2056

14000

74

74

75

74

0.7236

0.0698

16000

155

155

157

155

0.5918

0.0119

Table 2: Convergence results for low-rank matrix completion with low-rank projections.

27

References
[1] Alekh Agarwal, Sahand Negahban, and Martin J Wainwright. Fast global convergence rates of gradient methods for high-dimensional statistical recovery. In
J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta,
editors, Advances in Neural Information Processing Systems 23, pages 37–45.
Curran Associates, Inc., 2010.
[2] Zeyuan Allen-Zhu, Elad Hazan, Wei Hu, and Yuanzhi Li. Linear convergence
of a frank-wolfe type algorithm over trace-norm balls. In Advances in Neural
Information Processing Systems, pages 6192–6201, 2017.
[3] Zeyuan Allen-Zhu and Yuanzhi Li. Lazysvd: Even faster svd decomposition
yet without agonizing pain. In Advances in Neural Information Processing
Systems, pages 974–982, 2016.
[4] Amir Beck. First-order methods in optimization, volume 25. SIAM, 2017.
[5] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183–
202, 2009.
[6] Srinadh Bhojanapalli, Anastasios Kyrillidis, and Sujay Sanghavi. Dropping
convexity for faster semi-definite optimization. In Conference on Learning
Theory, pages 530–582, 2016.
[7] Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality
of local search for low rank matrix recovery. In Advances in Neural Information
Processing Systems, pages 3873–3881, 2016.
[8] Sébastien Bubeck et al. Convex optimization: Algorithms and complexity.
Foundations and Trends® in Machine Learning, 8(3-4):231–357, 2015.
[9] Jian-Feng Cai, Emmanuel J Candès, and Zuowei Shen. A singular value
thresholding algorithm for matrix completion. SIAM Journal on Optimization, 20(4):1956–1982, 2010.
[10] Emmanuel J Candès, Xiaodong Li, Yi Ma, and John Wright. Robust principal
component analysis? Journal of the ACM (JACM), 58(3):11, 2011.
[11] Emmanuel J Candès and Benjamin Recht. Exact matrix completion via convex
optimization. Foundations of Computational mathematics, 9(6):717–772, 2009.
[12] Yudong Chen and Martin J Wainwright. Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees. arXiv
preprint arXiv:1509.03025, 2015.
[13] M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval
Research Logistics Quarterly, 3:149–154, 1956.
[14] Dan Garber. Faster projection-free convex optimization over the spectrahedron. In Advances in Neural Information Processing Systems, pages 874–882,
2016.
28

[15] Dan Garber. Linear convergence of frank-wolfe for rank-one matrix recovery
without strong convexity. arXiv preprint arXiv:1912.01467, 2019.
[16] Dan Garber, Shoham Sabach, and Atara Kaplan. Fast generalized conditional
gradient method with applications to matrix recovery problems. arXiv preprint
arXiv:1802.05581, 2018.
[17] Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious
local minimum. In Advances in Neural Information Processing Systems, pages
2973–2981, 2016.
[18] Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU
Press, 2012.
[19] F Maxwell Harper and Joseph A Konstan. The movielens datasets: History
and context. Acm transactions on interactive intelligent systems (tiis), 5(4):19,
2016.
[20] Elad Hazan. Sparse approximate solutions to semidefinite programs. In 8th
Latin American Theoretical Informatics Symposium, LATIN, 2008.
[21] Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In Proceedings of the 30th International Conference on Machine Learning,
ICML, 2013.
[22] Martin Jaggi and Marek Sulovský. A simple algorithm for nuclear norm regularized problems. In Proceedings of the 27th International Conference on
Machine Learning, ICML, 2010.
[23] Prateek Jain, Raghu Meka, and Inderjit S. Dhillon. Guaranteed rank minimization via singular value projection. In J. D. Lafferty, C. K. I. Williams,
J. Shawe-Taylor, R. S. Zemel, and A. Culotta, editors, Advances in Neural
Information Processing Systems 23, pages 937–945. 2010.
[24] Prateek Jain, Ambuj Tewari, and Purushottam Kar. On iterative hard thresholding methods for high-dimensional m-estimation. In Advances in Neural
Information Processing Systems, pages 685–693, 2014.
[25] Bamdev Mishra, Gilles Meyer, Francis R. Bach, and Rodolphe Sepulchre. Lowrank optimization with trace norm penalty. SIAM Journal on Optimization,
23(4):2124–2149, 2013.
[26] Cameron Musco and Christopher Musco. Randomized block krylov methods
for stronger and faster approximate singular value decomposition. In Advances
in Neural Information Processing Systems, pages 1396–1404, 2015.
[27] Yu Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341–362, 2012.
[28] Yurii Nesterov. Introductory lectures on convex optimization: A basic course,
volume 87. Springer Science & Business Media, 2013.

29

[29] Dohyung Park, Anastasios Kyrillidis, Constantine Caramanis, and Sujay Sanghavi. Finding low-rank solutions via nonconvex matrix factorization, efficiently
and provably. SIAM Journal on Imaging Sciences, 11(4):2165–2204, 2018.
[30] Benjamin Recht. A simpler approach to matrix completion. The Journal of
Machine Learning Research, 12:3413–3430, 2011.
[31] Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimumrank solutions of linear matrix equations via nuclear norm minimization. SIAM
review, 52(3):471–501, 2010.
[32] Mohammadreza Soltani and Chinmay Hegde. Fast low-rank matrix estimation
without the condition number. arXiv preprint arXiv:1712.03281, 2017.
[33] Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389–434, Aug 2012.
[34] A. Yurtsever, M. Udell, J. A. Tropp, and V. Cevher. Sketchy decisions: Convex
low-rank matrix optimization with optimal storage. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2017.

30

