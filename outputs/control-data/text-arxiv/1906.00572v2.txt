arXiv:1906.00572v2 [cs.LG] 23 Dec 2019

Using a Logarithmic Mapping to Enable Lower
Discount Factors in Reinforcement Learning

Harm van Seijen
Microsoft Research Montréal
harm.vanseijen@microsoft.com

Mehdi Fatemi
Microsoft Research Montréal
mehdi.fatemi@microsoft.com

Arash Tavakoli
Imperial College London
a.tavakoli@imperial.ac.uk

Abstract
In an effort to better understand the different ways in which the discount factor
affects the optimization process in reinforcement learning, we designed a set of
experiments to study each effect in isolation. Our analysis reveals that the common
perception that poor performance of low discount factors is caused by (too) small
action-gaps requires revision. We propose an alternative hypothesis that identifies
the size-difference of the action-gap across the state-space as the primary cause.
We then introduce a new method that enables more homogeneous action-gaps by
mapping value estimates to a logarithmic space. We prove convergence for this
method under standard assumptions and demonstrate empirically that it indeed
enables lower discount factors for approximate reinforcement-learning methods.
This in turn allows tackling a class of reinforcement-learning problems that are
challenging to solve with traditional methods.

1

Introduction

In reinforcement learning (RL), the objective that one wants to optimize for is often best described
as an undiscounted sum of rewards (e.g., maximizing the total score in a game) and a discount
factor is merely introduced so as to avoid some of the optimization challenges that can occur when
directly optimizing on an undiscounted objective (Bertsekas and Tsitsiklis, 1996). In this scenario, the
discount factor plays the role of a hyper-parameter that can be tuned to obtain a better performance
on the true objective. Furthermore, for practical reasons, a policy can only be evaluated for a finite
amount of time, making the effective performance metric a finite-horizon, undiscounted objective.1
To gain a better understanding of the interaction between the discount factor and a finite-horizon,
undiscounted objective, we designed a number of experiments to study this relation. One surprising
finding is that for some problems a low discount factor can result in better asymptotic performance,
when a finite-horizon, undiscounted objective is indirectly optimized through the proxy of an infinitehorizon, discounted sum. This motivates us to look deeper into the effect of the discount factor on the
optimization process.
We analyze why in practice the performance of low discount factors tends to fall flat when combined
with function approximation, especially in tasks with long horizons. Specifically, we refute a number
of common hypotheses and present a new one instead, identifying the primary culprit to be the size1
As an example, in the seminal work of Mnih et al. (2015), the (undiscounted) score of Atari games is
reported with a time-limit of 5 minutes per game.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

difference of the action gap (i.e., the difference between the values of the best and the second-best
actions of a state) across the state-space.
Our main contribution is a new method that yields more homogeneous action-gap sizes for sparsereward problems. This is achieved by mapping the update target to a logarithmic space and performing
updates in that space instead. We prove convergence of this method under standard conditions.
Finally, we demonstrate empirically that our method achieves much better performance for low
discount factors than previously possible, providing supporting evidence for our new hypothesis.
Combining this with our analytical result that there exist tasks where low discount factors outperform
higher ones asymptotically suggests that our method can unlock a performance on certain problems
that is not achievable by contemporary RL methods.

2

Problem Setting

Consider a Markov decision process (MDP, (Puterman, 1994)) M = hS, A, P, R, S0 i, where S
denotes the set of states, A the set of actions, R the reward function R : S × A × S → R, P the
transition probability function P : S × A × S → [0, 1], and S0 the starting state distribution. At
each time step t, the agent observes state st ∈ S and takes action at ∈ A. The agent observes
the next state st+1 , drawn from the transition probability distribution P (st , at , ·), and a reward
rt = R(st , at , st+1 ). A terminal state is one that, once entered, terminates the interaction with the
environment; mathematically, it can be interpreted as an absorbing state that transitions only to itself
with a corresponding reward of 0. The behavior of an agent is defined by a policy π, which, at time
step t, takes as input the history of states, actions, and rewards, s0 , a0 , r0 , s1 , a1 , ....rt−1 , st , and
outputs a distribution over actions, in accordance to which action at is selected. If action at only
depends on the current state st , we will call the policy a stationary one; if the policy depends on
more than the current state st , we will call the policy non-stationary.
We define a task to be the combination of an MDP M and a performance metric F . The metric F is
a function that takes as input a policy π and outputs a score that represents the performance of π on
M . By contrast, we define the learning metric Fl to be the metric that the agent optimizes. Within
the context of this paper, unless otherwise stated, the performance metric F considers the expected,
finite-horizon, undiscounted sum of rewards over the start-state distribution; the learning metric Fl
considers the expected, infinite-horizon, discounted sum of rewards:
"h−1
#
"∞
#
X
X
i
F (π, M ) = E
ri π, M
;
Fl (π, M ) = E
γ ri π, M ,
(1)
i=0

i=0

where the horizon h and the discount factor γ are hyper-parameters of F and Fl , respectively.
The optimal policy of a task, π ∗ , is the policy that maximizes the metric F on the MDP M . Note
that in general π ∗ will be a non-stationary policy. In particular, the optimal policy depends besides
the current state on the time step. We denote the policy that is optimal w.r.t. the learning metric
Fl by πl∗ . Because Fl is not a finite-horizon objective, there exists a stationary, optimal policy for
it, considerably simplifying the learning problem.2 Due to the difference between the learning and
performance metrics, the policy that is optimal w.r.t. the learning metric does not need to be optimal
w.r.t. the performance metric. We call the difference in performance between πl∗ and π ∗ , as measured
by F , the metric gap:
∆F = F (π ∗ , M ) − F (πl∗ , M )
The relation between γ and the metric gap will be analyzed in Section 3.1.
We consider model-free, value-based methods. These are methods that aim to find a good policy by
iteratively improving an estimate of the optimal action-value function Q∗ , which, generally, predicts
the expected discounted sum of rewards under the optimal policy πl∗ conditioned on state-action pairs.
The canonical example is Q-learning (Watkins and Dayan, 1992), which updates its estimates as
follows:


0
Qt+1 (st , at ) := (1 − α)Qt (st , at ) + α rt + γ max
Q
(s
,
a
)
,
(2)
t
t+1
0
a

2

This is the main reason why optimizing on an infinite-horizon objective, rather than a finite-horizon one, is
an attractive choice.

2

where α ∈ [0, 1] is the step-size. The action-value function is commonly estimated using a function
approximator with weight vector θ: Q(s, a; θ). Deep Q-Networks (DQN) (Mnih et al., 2015) use
a deep neural network as function approximator and iteratively improve an estimate of Q∗ by
minimizing a sequence of loss functions:

with

Li (θi ) = Es,a,r,s0 [(yiDQN − Q(s, a; θi ))2 ] ,

(3)

yiDQN

(4)

0

0

= r + γ max
Q(s , a ; θi−1 ),
0
a

The weight vector from the previous iteration, θi−1 , is encoded using a separate target network.

Analysis of Discount Factor Effects
Effect on Metric Gap

B
C

+1

-5
+1

+5
+5

4

6.0

A

3
2
1
0
0.0

5.8

0.3

B
performance

-1

A

performance

3.1

performance

3

5.6
5.4
5.2
5.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

C

0.2

0.1

0.0
0.0

0.2

0.4

0.6

0.8

1.0

Figure 1: Illustrations of three different tasks (blue diamond: starting position; green circle: positive
object; red circle: negative object; gray arrows: wind direction; numbers indicate rewards). The
graphs show the performance—as measured by F —on these tasks for π ∗ (black, dotted line) and πl∗
(red, solid line) as function of the discount factor of the learning metric. The difference between the
two represents the metric gap.
The question that is central to this section is the following: given a finite-horizon, undiscounted
performance metric, what can be said about the relation between the discount factor of the learning
metric and the metric gap?
To study this problem, we designed a variety of different tasks and measured the dependence between
the metric gap and the discount factor. In Figure 1, we illustrate three of those tasks, as well as the
metric gap on those tasks as function of the discount factor. In each task, an agent, starting from
a particular position, has to collect rewards by collecting the positive objects while avoiding the
negative objects. The transition dynamics of tasks A and B is deterministic; whereas, in task C
wind blows in the direction of the arrows, making the agent move towards left with a 40% chance,
regardless of its performed action. For all three tasks, the horizon of the performance metric is 12.
On task A, where a small negative reward has to be traded off for a large positive reward that is
received later, high discount factors result in a smaller metric gap. By contrast, on task B, low
discount factors result in a smaller metric gap. The reason is that for high discount factors the optimal
learning policy takes the longer route by first trying to collect the large object, before going to the
small object. However, with a performance metric horizon of 12, there is not enough time to take the
long route and get both rewards. The low discount factor takes a shorter route by first going to the
smaller object and is able to collect all objects in time. On task C, a trade-off has to be made between
the risk of falling into the negative object (due to domain stochasticity) versus taking a longer detour
that minimizes this risk. On this task, the optimal policy π ∗ is non-stationary (the optimal action
depends on the time step). However, because the learning objective Fl is not finite-horizon, it has
a stationary optimal policy πl∗ . Hence, the metric gap cannot be reduced to 0 for any value of the
discount factor. The best discount factor is something that is not too high nor too low.
While the policy πl∗ is derived from an infinite-horizon metric, this does not preclude it from being
learned with finite-length training episodes. As an example, consider using Q-learning to learn πl∗ for
any of the tasks from Figure 1. With a uniformly random behavior policy and training episodes of
length 12 (the same as the horizon of the performance metric), there is a non-zero probability for
each state-action pair that it will be visited within an episode. Hence, with the right step-size decay
schedule, convergence in the limit can be guaranteed (Jaakkola et al., 1994). A key detail to enable
3

Figure 2: Chain task consisting of 50 states and two terminal ones. Each (non-terminal) state has
two actions: aL which results in transitioning to the left with probability 1 − p and to the right with
probability p, and vice versa for the other action, aR . All rewards are 0, except for transitioning to
the far-left or far-right terminal states that result in rL and rR , respectively.
this is that the state that is reached at the final time step is not treated as a terminal state (which has
value 0 by default), but normal bootstrapping occurs (Pardo et al., 2018).
A finite-horizon performance metric is not essential to observe strong dependence of the metric gap
on γ. For example, if on task B the performance metric would measure the number of steps it takes
to collect all objects, a similar graph is obtained. In general, the examples in this section demonstrate
that the best discount factor is task-dependent and can be anywhere in the range between 0 and 1.
3.2

Optimization Effects

To keep the experiment as simple as possible, we remove exploration effects by performing update sweeps over the entire stateaction space (using a step-size of 0.001) and measure performance
at the end of each update sweep. Figure 3 shows the performance
during early learning (average performance over the first 10, 000
sweeps) as well as the final performance (average between sweeps
100, 000 and 110, 000).

average performance

To study the optimization effects under function approximation,
we use linear function approximation with features constructed
by tile-coding (Sutton, 1996), using tile-widths of 1, 2, 3, and 5.
A tile-width of w corresponds to a binary feature that is non-zero
for w neighbouring states and zero for the remaining ones. The
number and offset of the tilings are such that any value function
can be represented. Hence, error-free reconstruction of the optimal
action-value function is possible in principle, for any discount
factor. Note that for a width of 1, the representation reduces to a
tabular one.

average performance

The performance of πl∗ gives the theoretical limit of what the agent can achieve given its learning
metric. However, the discount factor also affects the optimization process; for some discount factors,
finding πl∗ could be more challenging than for others. In this section, using the task shown in Figure 2,
we evaluate the correlation between the discount factor and how hard it could be to find πl∗ . It is easy
to see that the policy that always takes the left action aL maximizes both discounted and undiscounted
sum of rewards for any discount factor or horizon value, respectively. We define the learning metric
Fl as before (1), but use a different performance metric F . Specifically, we define F to be 1 if the
policy takes aL in every state, and 0 otherwise. The metric gap for this setting of F and Fl is 0, with
the optimal performance (for π ∗ and πl∗ ) being 1.
1.0
0.8
0.6
0.4

w: 1
w: 2
0.2
w: 3
w: 5
0.0
0.2 0.4

0.6

0.8

1.0

1.0
0.8
0.6
0.4

w: 1
w: 2
w: 3
w: 5
0.0
0.2 0.4
0.2

0.6 0.8 1.0
These experiments demonstrate a common empirical observation:
when using function approximation, low discount factors do not
work well in sparse-reward domains. More specifically, the main Figure 3: Early performance
observations are: 1) there is a sharp drop in final performance (top) and final performance (botfor discount factors below some threshold; 2) this threshold value tom) on the chain task.
depends on the tile-width, with larger ones resulting in worse (i.e.,
higher) threshold values; and 3) the tabular representation performs well for all discount factors.

It is commonly believed that the action gap has a strong influence on the optimization process
(Bellemare et al., 2016; Farahmand, 2011). The action gap of a state s is defined as the difference
in Q∗ between the best and the second best actions at that state. To examine this common belief,
we start by evaluating two straightforward hypotheses involving the action gap: 1) lower discount
factors cause poor performance because they result in smaller action gaps; 2) lower discount factors
cause poor performance because they result in smaller relative action gaps (i.e, the action gap of a
4

state divided by the maximum action-value of that state). Since both hypotheses are supported by the
results from Figure 3, we performed more experiments to test them. To test the first hypothesis, we
performed the same experiment as above, but with rewards that are a factor 100 larger. This in turn
increases the action gaps by a factor 100 as well. Hence, to validate the first hypothesis, this change
should improve (i.e., lower) the threshold value where the performance falls flat. To test the second
hypothesis, we pushed all action-values up by 100 through additional rewards, reducing the relative
action-gap. Hence, to validate the second hypothesis, performance should degrade for this variation.
However, neither of the modifications caused significant changes to the early or final performance,
invalidating these hypotheses. The corresponding graphs can be found in Appendix B.
Because our two naïve action-gap hypotheses have failed, we
propose an alternative hypothesis: lower discount factors cause
20
poor performance because they result in a larger difference in the
action-gap sizes across the state-space. To illustrate the statement
10
about the difference in action-gap sizes, we define a metric, which
we call the action-gap deviation κ, that aims to capture the notion
0 0.2 0.4 0.6 0.8 1.0
of action-gap variations. Specifically, let X be a random variable
+
and let S ⊆ S be the subset of states that have a non-zero action
gap. X draws uniformly at random a state s ∈ S + and outputs Figure 4: Action-gap deviation
log10 (AG(s)), where AG(s) is the action gap of state s. We now as function of discount factor.
define κ to be the standard deviation of the variable X. Figure 4
plots κ as function of the discount factor for the task in Figure 2.
To test this new hypothesis, we have to develop a method that reduces the action-gap deviation κ for
low discount factors, without changing the optimal policy. We do so in the next section.

4

Logarithmic Q-learning

In this section, we introduce our new method, logarithmic Q-learning, which reduces the action-gap
deviation κ for sparse-reward domains. We present the method in three steps, in each step adding
a layer of complexity in order to extend the generality of the method. In Appendix A, we prove
convergence of the method in its most general form. As the first step, we now consider domains with
deterministic dynamics and rewards that are either positive or zero.
4.1

Deterministic Domains with Positive Rewards

Our method is based on the same general approach as used by Pohlen et al. (2018): mapping the
update target to a different space and performing updates in that space instead. We indicate the
mapping function by f , and its inverse by f −1 . Values in the mapping space are updated as follows:



e t+1 (st , at ) := (1 − α)Q
e t (st , at ) + αf rt + γ max f −1 Q
e t (st+1 , a0 )
Q
.
(5)
0
a

e in this equation is not an estimate of an expected return; it is an estimate of an expected
Note that Q
return mapped to a different space. To obtain a regular Q-value the inverse mapping has to be applied
e Because the updates occur in the mapping space, κ is now measured w.r.t. Q.
e That is, the action
to Q.
e abest ) − Q(s,
e a2nd best ).
gap of state s is now defined in the mapping space as Q(s,
To reduce κ, we propose to use a logarithmic mapping function.
Specifically, we propose the following mapping function:

3

(6)

2

with inverse function: f −1 (x) = e(x−d)/c − γ k , where c, d, and
k are mapping hyper-parameters.

1

f (x) := c ln(x + γ k ) + d ,

reg
log, k = 40
log, k = 50
log, k = 200

0 0.25 0.50 0.75 1.00
To understand the effect of (6) on κ, we plot κ, based on action gaps
in the logarithmic space, on a variation of the chain task (Figure 2)
that uses rR = 0 and p = 0. We also plot κ based on actions in the
regular space. Figure 5 shows that with an appropriate value of k, Figure 5: Action-gap deviation
the action-gap deviation can almost be reduced to 0 for low values as function of discount factor.
of γ. Setting k too high increases the deviation a little, while setting it too low increases it a lot for
5

low discount factors. In short, k controls the smallest Q-value that can still be accurately represented
(i.e., for which the action gap in the log-space is still significant). Roughly, the smallest value that
can still be accurately represented is about γ k . In other words, the cut-off point lies approximately
at a state from which it takes k time steps to experience a +1 reward. Setting k too high causes
actions that have 0 value in the regular space to have a large negative value in the log-space. This
can increase the action gap substantially for the corresponding states, thus, resulting in an overall
increase of the action-gap deviation.
The parameters c and d scale and shift values in the logarithmic space and do not have any effect on
the action-gap deviation. The parameter d controls the initialization of the Q-values. Setting d as
follows:
d = −c ln(qinit + γ k ) ,
(7)
−1
ensures that f (0) = qinit for any value of c, k, and γ. This can be useful in practice, e.g., when
e as it enables standard initialization methods (which produce
using neural networks to represent Q,
e values correspond with qinit in
output values around 0) while still ensuring that the initialized Q
the regular space. The parameter c scales values in the log-space. For most tabular and linear
methods, scaling values does not affect the optimization process. Nevertheless, in deep RL methods
more advanced optimization techniques are commonly used and, thus, such scaling can impact the
optimization process significantly. In all our experiments, except the deep RL experiments, we fixed
d according to the equation above with qinit = 0 and used c = 1.
In stochastic environments, the approach described in this section causes issues, because averaging
over stochastic samples in the log-space produces an underestimate compared to averaging in the
regular space and then mapping the result to the log-space. Specifically, if X is a random variable,
E [ln(X)] ≤ ln (E[X]) (i.e., Jensen’s inequality). Fortunately, within our specific context, there is a
way around this limitation that we discuss in the next section.
4.2

Stochastic Domains with Positive Rewards

The step-size α generally conflates two forms of averaging: averaging of stochastic update targets
due to environment stochasticity, and, in the case of function approximation, averaging over different
states. To amend our method for stochastic environments, ideally, we would separate these forms
of averaging and perform the averaging over stochastic update targets in the regular space and the
averaging over different states in the log-space. While such a separation is hard to achieve, the
approach presented below, which is inspired by the above observation, achieves many of the same
e to f (Q∗ ), even when the environment is stochastic.
benefits. In particular, it enables convergence of Q
Let βlog be the step-size for averaging in the log-space, and βreg be the step-size for averaging in
the regular space. We amend the approach from the previous section by computing an alternative
update target that is based on performing an averaging operation in the regular space. Specifically,
the update target Ut is transformed into an alternative update target Ût as follows:


e t (st , at )) + βreg Ut − f −1 (Q
e t (st , at ) ,
Ût := f −1 (Q
(8)
e t (st+1 , a0 )). The modified update target Ût is used for the update in
with Ut := rt + γ maxa0 f −1 (Q
the log-space:


e t+1 (st , at ) := Q
e t (st , at ) + βlog f (Ût ) − Q
e t (st , at ) .
Q
(9)

RMS error

Note that if βreg = 1, then Ût = Ut , and update (9) reduces to update (5) from the previous section,
with α = βlog .
0.20
The conditions for convergence are discussed in the next section,
log = 0.001, reg = 1
log = 0.01, reg = 0.1
but one of the conditions is that βreg should go to 0 in the limit.
0.15
log = 1, reg = 0.001
From a more practical point of view, when using fixed values
0.10
for βreg and βlog , βreg should be set sufficiently small to keep
0.05
underestimation of values due to the averaging in the log-space
0.000 250 500 750 1000
under control. To illustrate this, we plot the RMS error on a
# update sweeps
positive-reward variant of the chain task (rR = 0, rL = +1, p =
Figure 6: RMS error.
0.25). The RMS error plotted is based on the difference between
−1 e
∗
f (Q(s, a)) and Q (s, a) over all state-action pairs. We used a tile-width of 1, corresponding with
6

a tabular representation, and used k = 200. Note that for βreg = 1, which reduces the method to the
one from the previous section, the error never comes close to zero.
4.3

Stochastic Domains with Positive and/or Negative Rewards

We now consider the general case where the rewards can be both positive or negative (or zero). It
might seem that we can generalize to negative rewards simply by replacing x in the mapping function
(6) by x + D, where D is a sufficiently large constant that prevents x + D from becoming negative.
The problem with this approach, as we will demonstrate empirically, is that it does not decrease κ for
low discount factors. Hence, in this section we present an alternative approach, based on decomposing
the Q-value function into two functions.
Consider a decomposition of the reward rt into two components, rt+ and rt− , as follows:


r if rt ≥ 0
|rt | if rt < 0
rt+ := t
; rt− :=
.
0 otherwise
0
otherwise

(10)

Note that rt+ and rt− are always non-negative and that rt = rt+ − rt− at all times. By decomposing
the observed reward in this manner, these two reward components can be used to train two separate
e + , which represents the value function in the mapping space corresponding
Q-value functions: Q
+
−
e
to rt , and Q , which plays the same role for rt− . To train these value functions, we construct the
following update targets:




−
−
−1 e −
e+
Ut+ := rt+ + γf −1 Q
(s
,
ã
)
;
U
:=
r
+
γf
Q
(s
,
ã
)
(11)
t+1 t+1
t+1 t+1
t
t
t
t


0
−1 e −
e+
with ãt+1 := arg maxa0 f −1 (Q
(Qt (st+1 , a0 )) . These update targets are
t (st+1 , a )) − f
e + and Q
e− ,
modified into Ût+ and Ût− , respectively, based on (8), which are then used to update Q
respectively, based on (9). Action-selection at time t is based on Qt , which we define as follows:




−1 e −
e+
Qt (s, a) := f −1 Q
(s,
a)
−
f
Q
(s,
a)
(12)
t
t
In Appendix A, we prove convergence of logarithmic Q-learning under similar conditions as regular
Q-learning. In particular, the product βlog,t · βreg,t has to satisfy the same conditions as αt does for
regular Q-learning. There is one additional condition on βreg,t , which states that it should go to zero
in the limit.
We now compute κ for the full version of the chain task. Because
reg
log bias
e + and Q
e − , we have to generalize the
20
there are two functions, Q
log plus-only
log min-only
definition of κ to this situation. We consider three generalizations:
log both
e + (‘log plus-only’); 2) κ is
10
1) κ is based on the action-gaps of Q
e − (‘log min-only’); and 3) κ is based
based on the action-gaps of Q
0 0.2 0.4 0.6 0.8 1.0
e − and Q
e + (‘log both’). Furthermore,
on the action-gaps of both Q
we plot a version that resolves the issue of negative rewards naïvely,
by adding a value D = 1 to the input of the log-function (‘log
bias’). We plot κ for these variants in Figure 7, using k = 200, Figure 7: Action-gap deviation
together with κ for regular Q-learning (‘reg’). Interestingly, only as function of discount factor.
for the ‘log plus-only’ variant κ is small for all discount factors.
Further analysis showed that the reason for this is that under the
optimal policy, the chance that the agent moves from a state close to the positive terminal state to the
negative terminal state is very small, which means that k = 200 is too small to make the action-gaps
e − homogeneous. However, as we will see in the next section, the performance with k = 200 is
for Q
e − is not
good for all discount factors, demonstrating that not having homogeneous action-gaps for Q
a huge issue. We argue that this could be because of the behavior related to the nature of positive
and negative rewards: it might be worthwhile to travel a long distance to get a positive reward, but
avoiding a negative reward is typically a short-horizon challenge.

7

We test our method by returning to the full version of the chain task
and the same performance metric F as used in Section 3.2, which
measures whether or not the greedy policy is optimal. We used
k = 200, βreg = 0.1, and βlog = 0.01 (the value of βreg · βlog is
equal to the value of α used in Section 3.2). Figure 8 plots the result
for early learning as well as the final performance. Comparing
these graphs with the graphs from Figure 3 shows that logarithmic
Q-learning has successfully resolved the optimization issues of
regular Q-learning related to the use of low discount factors in
conjunction with function approximation.
Combined with the observation from Section 3.1 that the best discount factor is task-dependent, and the convergence proof, which
guarantees that logarithmic Q-learning converges to the same policy as regular Q-learning, these results demonstrate that logarithmic Q-learning is able to solve tasks that are challenging to solve
with Q-learning. Specifically, if a finite-horizon performance metric is used and the task is such that the metric gap is substantially
smaller for lower discount factors, but performance falls flat for
these discount factors due to function approximation.

average performance

Experiments

average performance

5

1.0
0.8
0.6
0.4

w: 1
w: 2
w: 3
w: 5
0.0
0.2 0.4
0.2

0.6

0.8

1.0

0.6

0.8

1.0

1.0
0.8
0.6
0.4

w: 1
w: 2
w: 3
w: 5
0.0
0.2 0.4
0.2

Finally, we test our approach in a more complex setting by compar- Figure 8: Early performance
ing the performance of DQN (Mnih et al., 2015) with a variant of it (top) and final performance (botthat implements our method, which we will refer to as LogDQN.3 tom) on the chain task.
To enable easy baseline comparisons, we used the Dopamine
framework for our experiments (Castro et al., 2018). This framework not only contains open-source
code of several important deep RL methods, but also contains the results obtained with these methods
for a set of 60 games from the Arcade Learning Environment (Bellemare et al., 2013; Machado et al.,
2018). This means that direct comparison to some important baselines is possible.
Our LogDQN implementation consists of a modification of the Dopamine’s DQN code. Specifically,
e + and Q
e − , the final output layer is
in order to adapt DQN’s model to provide estimates of both Q
+
e
e − . All the other
doubled in size, and half of it is used to estimate Q while the other half estimates Q
+
−
+
e
e
e
e − are updated
layers are shared between Q and Q and remain unchanged. Because both Q and Q
using the same samples, the replay memory does not require modification, so the memory footprint
e + and Q
e − are updated simultaneously using a single pass
does not change. Furthermore, because Q
through the model, the computational cost of LogDQN and DQN is similar. Further implementation
details are provided in Appendix C.
The published Dopamine baselines are obtained on a stochastic version of Atari using sticky actions
(Machado et al., 2018) where with 25% probability the environment executes the action from
the previous time step instead of the agent’s new action. Hence, we conducted all our LogDQN
experiments on this stochastic version of Atari as well.
While Dopamine provides baselines for 60 games in total, we only consider the subset of 55 games
for which human scores have been published, because only for these games a ‘human-normalized
score’ can be computed, which is defined as:
ScoreAgent − ScoreRandom
.
ScoreHuman − ScoreRandom
We use Table 2 from Wang et al. (2016) to retrieve the human and random scores.

(13)

We optimized hyper-parameters using a subset of 6 games. In particular, we performed a scan over
the discount factor γ between γ = 0.84 and γ = 0.99. For DQN, γ = 0.99 was optimal; for
LogDQN, the best value in this range was γ = 0.96. We tried lower γ values as well, such as γ = 0.1
and γ = 0.5, but this did not improve the overall performance over these 6 games. For the other
hyper-parameters of LogDQN we used k = 100, c = 0.5, βlog = 0.0025, and βreg = 0.1. The
product of βlog and βreg is 0.00025, which is the same value as the (default) step-size α of DQN. We
3

The code for the experiments can be found at: https://github.com/microsoft/logrl

8

DoubleDunk
Skiing
StarGunner
Kangaroo
Krull
Assault
IceHockey
Jamesbond
Hero
BeamRider
Amidar
Centipede
Gopher
MsPacman
Riverraid
TimePilot
Alien
Solaris
Venture
KungFuMaster
Asteroids
Bowling
MontezumaRevenge
Pitfall
FishingDerby
PrivateEye
Gravitar
Robotank
Berzerk
Seaquest
Boxing
YarsRevenge
DemonAttack
Pong
Phoenix
CrazyClimber
BankHeist
Atlantis
Qbert
UpNDown
BattleZone
RoadRunner
Enduro
ChopperCommand
Freeway
NameThisGame
Tennis
SpaceInvaders
WizardOfWor
Zaxxon
Tutankham
Asterix
Frostbite
Breakout
VideoPinball

300%
200%
100%
0%
-100%

Figure 10: Relative performance of LogDQN w.r.t. DQN (positive percentage means LogDQN
outperforms DQN). Orange bars indicate a performance difference larger than 50%; dark-blue bars
indicate a performance difference between 10% and 50%; light-blue bars indicate a performance
difference smaller than 10%.
used different values of d for the positive and negative heads: we set d based on (7) with qinit = 1 for
the positive head, and qinit = 0 for the negative head. Results from the hyper-parameter optimization,
as well as further implementation details are provided in Appendix C.
Figure 10 shows the performance of LogDQN compared to DQN
per game, using the same comparison equation as used by Wang
et al. (2016):

Mean

ScoreLogDQN − ScoreDQN
.
max(ScoreDQN , ScoreHuman ) − ScoreRandom
where ScoreLogDQN/DQN is computed by averaging over the last
10% of each learning curve (i.e., last 20 epochs).
Figure 9 shows the mean and median of the human-normalized
score of LogDQN, as well as DQN. We also plot the performance
of the other baselines that Dopamine provides: C51 (Bellemare
et al., 2017), Implicit Quantile Networks (Dabney et al., 2018),
and Rainbow (Hessel et al., 2018). These baselines are just for
reference; we have not attempted to combine our technique with
the techniques that these other baselines make use of.

6

Median

Discussion and Future Work

Our results provide strong evidence for our hypothesis that large
differences in action-gap sizes are detrimental to the performance
of approximate RL. A possible explanation could be that optimizing on the L2 -norm (3) might drive towards an average squarederror that is similar across the state-space. However, the errorlandscape required to bring the approximation error below the
action-gap across the state-space has a very different shape if the
action-gap is orders of magnitude different in size across the statespace. This mismatch between the required error-landscape and
that produced by the L2 -norm might lead to an ineffective use
of the function approximator. Further experiments are needed to
confirm this.

Figure 9: Human-normalized
mean (left) and median (right)
scores on 55 Atari games for
LogDQN and various other algorithms.

The strong performance we observed for γ = 0.96 in the deep RL setting is unlikely solely due to a
difference in metric gap. We suspect that there are also other effects at play that make LogDQN as
effective as it is. On the other hand, at (much) lower discount factors, the performance was not as
good as it was for the high discount factors. We believe a possible reason could be that since such low
values are very different than the original DQN settings, some of the other DQN hyper-parameters
might no longer be ideal in the low discount factor region. An interesting future direction would be
to re-evaluate some of the other hyper-parameters in the low discount factor region.
9

Acknowledgments
We like to thank Kimia Nadjahi for her contributions to a convergence proof of an early version
of logarithmic Q-learning. This early version ultimately was replaced by a significantly improved
version that required a different convergence proof.

References
Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, 1996.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529–533, 2015.
Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, 1994.
Christopher J. C. H. Watkins and Peter Dayan. Q-learning. Machine Learning, 8(3):279–292, 1992.
Tommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. On the convergence of stochastic iterative
dynamic programming algorithms. In Advances in Neural Information Processing Systems, pages
703–710, 1994.
Fabio Pardo, Arash Tavakoli, Vitaly Levdik, and Petar Kormushev. Time limits in reinforcement
learning. In Proceedings of the 35th International Conference on Machine Learning, volume 80,
pages 4045–4054, 2018.
Richard S. Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse
coding. In Advances in Neural Information Processing Systems, pages 1038–1044, 1996.
Marc G. Bellemare, Georg Ostrovski, Arthur Guez, Philip Thomas, and Rémi Munos. Increasing the
action gap: New operators for reinforcement learning. In Proceedings of the 30th AAAI Conference
on Artificial Intelligence, pages 1476–1483, 2016.
Amir-massoud Farahmand. Action-gap phenomenon in reinforcement learning. In Advances in
Neural Information Processing Systems, pages 172–180, 2011.
Tobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi Azar, Dan Horgan, David Budden,
Gabriel Barth-Maron, Hado van Hasselt, John Quan, Mel Večerík, Matteo Hessel, Rémi Munos,
and Olivier Pietquin. Observe and look further: Achieving consistent performance on Atari. arXiv
preprint arXiv:1805.11593, 2018.
Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Bellemare. Dopamine: A research framework for deep reinforcement learning. arXiv preprint
arXiv:1812.06110, 2018.
Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:
253–279, 2013.
Marlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and
Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open
problems for general agents. Journal of Artificial Intelligence Research, 61:523–562, 2018.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas.
Dueling network architectures for deep reinforcement learning. In Proceedings of the 33rd
International Conference on Machine Learning, volume 48, pages 1995–2003, 2016.
Marc G. Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning, volume 70,
pages 449–458, 2017.
10

Will Dabney, Georg Ostrovski, David Silver, and Rémi Munos. Implicit quantile networks for
distributional reinforcement learning. In Proceedings of the 35th International Conference on
Machine Learning, volume 80, pages 1096–1105, 2018.
Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence,
pages 3215–3222, 2018.
Satinder Singh, Tommi Jaakkola, Michael L. Littman, and Csaba Szepesvári. Convergence results for
single-step on-policy reinforcement-learning algorithms. Machine Learning, 38(3):287–308, 2000.
Peter J. Huber. Robust estimation of a location parameter. In Breakthroughs in Statistics, pages
492–518. Springer, 1992.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the 13th International Conference on Artificial Intelligence and
Statistics, pages 249–256, 2010.

11

A
A.1

Proof of Convergence for Logarithmic Q-learning
Definitions and Theorem

Our logarithmic Q-learning method is defined by the following equations:
f (x) := c ln(x + γ k ) + d ;

rt+


:=

rt
0

if rt ≥ 0
otherwise

;

f −1 (x) := e(x−d)/c − γ k

rt−


:=

|rt | if rt < 0
0
otherwise

(14)

(15)





−1 e −
e+
Qt (s, a) := f −1 Q
(s,
a)
−
f
Q
(s,
a)
t
t

(16)



0
ãt+1 := arg max
Q
(s
,
a
)
t
t+1
0

(17)



e+
Ut+ := rt+ + γf −1 Q
t (st+1 , ãt+1 )

(18)






+
−1 e +
e+
Ût+ := f −1 Q
(s
,
a
)
+
β
U
−
f
Q
(s
,
a
)
t
t
reg,t
t
t
t
t
t

(19)

  

+
e + (st , at ) := Q
e+
e+
Q
−Q
t (st , at ) + βlog,t f Ût
t (st , at )
t+1

(20)



e−
Ut− := rt− + γf −1 Q
t (st+1 , ãt+1 )

(21)






−
−1 e −
e−
Ût− := f −1 Q
(s
,
a
)
+
β
U
−
f
Q
(s
,
a
)
t
t
reg,t
t
t
t
t
t

(22)

  

−
−
e − (st , at ) := Q
e−
e
Q
(s
,
a
)
+
β
f
Û
−
Q
(s
,
a
)
t t
log,t
t t
t
t
t
t+1

(23)

a

For these equations, the following theorem holds:
Theorem 1 Under the definitions above, Qt converges to Q∗ w.p. 1 if the following conditions hold:
1. 0 ≤ βlog,t · βreg,t ≤ 1
P∞
2.
t=0 βlog,t · βreg,t = ∞
P∞
2
3.
t=0 (βlog,t · βreg,t ) < ∞
4. limt→∞ βreg,t = 0
A.2

Proof - part 1

−1 e +
We define Q+
(Qt (s, a)) and prove in part 2 that from (20), (19), and (18) it follows
t (s, a) := f
that:

+
+
+
+
Q+
,
(24)
t+1 (st , at ) = Qt (st , at ) + βreg,t · βlog,t Ut − Qt (st , at ) + ct
+
with c+
t converging to zero w.p. 1 under condition 4 of the theorem, and Ut defined as:

Ut+ := rt+ + γ Q+
t (st+1 , ãt+1 ) .
−1 e −
Similarly, using definition Q−
(Qt (s, a)) and (23), (22), and (21) it follows that:
t (s, a) := f

−
−
−
Qt+1 (st , at ) = Qt (st , at ) + βreg,t · βlog,t Ut− − Q−
,
(25)
t (st , at ) + ct

12

−
with c−
t converging to zero w.p. 1 under condition 4 of the theorem, and Ut defined as:

Ut− := rt+ + γ Q−
t (st+1 , ãt+1 ) .
−
It follows directly from the definitions of Q+
t and Qt and (16) that:
−
Qt (s, a) = Q+
t (s, a) − Qt (s, a) .

(26)

Subtracting (25) from (24) and substituting this equivalence yields:

−
Qt+1 (st , at ) = Qt (st , at ) + βreg,t · βlog,t rt+ − rt− + γQt (st+1 , ãt+1 ) − Qt (st , at ) + c+
.
t − ct
From (15) it follows that rt = rt+ − rt− . Furthermore, the following holds:


0
Qt (st+1 , ãt+1 ) = Qt st+1 , arg max
Q
(s
,
a
)
t
t+1
a0

= max
Qt st+1 , a0
0
a

−
Using these equivalences and defining αt := βreg,t · βlog,t and ct := c+
t − ct , it follows that:


0
Q
(s
,
a
(27)
Qt+1 (st , at ) = Qt (st , at ) + αt rt + γ max
)
−
Q
(s
,
a
)
+
c
t t+1
t t t
t ,
0
a

with ct converging to zero w.p. 1 under condition 4 of the theorem. This is a noisy Q-learning
algorithm with the noise term decaying to zero. As we show in part 2, ct is fully specified (in the
+
positive case, and likewise in the negative case) by Q+
t , Ut , and βreg,t , which implies that ct is
measurable given information at time t, as required by Lemma 1 in Singh et al. (2000). Invoking
that Lemma,P
it can therefore beP
shown that the iterative process defined by (27) converges to Q∗t if
∞
∞
0 ≤ α ≤ 1, t=0 α = ∞, and t=0 αt2 < ∞, as is guaranteed by the first three conditions of the
theorem. The steps are similar to the proof of Theorem 1 of the same reference, which we do not
repeat here.
A.3

Proof - part 2

In this section, we prove that (24) holds under the definitions from Section A.1, Q+
t (s, a) :=
e+
f −1 (Q
(s,
a)),
and
condition
4
of
the
theorem.
The
proof
of
(25)
follows
the
same
steps,
but with
t
the ‘-’ variants of the different variables instead. For readability, we use β1 for βlog,t and β2 for
βreg,t .

+
e+
The definition of Q+
t implies Qt (s, a) = f Qt (s, a) . Using these equivalences, we can rewrite
(20), (19), and (18) in terms of Qt :


+
+
+
f (Q+
(28)
t+1 (st , at )) = f (Qt (st , at )) + β1 f (Ût ) − f (Qt (st , at )) ,
with


+
+
+
Ût+ = Q+
t (st , at ) + β2 rt + γ Qt (st+1 , ãt+1 ) − Qt (st , at ) .

By applying f

(29)

−1

to both sides of (28), we get:

  

+
+
+
−1
Q+
(s
,
a
)
=
f
f
(Q
(s
,
a
))
+
β
f
Û
−
f
(Q
(s
,
a
))
,
t
t
t
t
1
t
t
t
t
t
t+1

(30)

which can be rewritten as:


+
+
+
+
Q+
t+1 (st , at ) = Qt (st , at ) + β1 Ût − Qt (st , at ) + et ,

13

(31)

where e+
t is the error due to averaging in the log-space instead of in the regular space:



+
+
+
−1
e+
f
(Q
:=
f
(s
,
a
))
+
β
f
(
Û
)
−
f
(Q
(s
,
a
))
t
t
1
t
t
t
t
t
t


+
+
− Q+
(32)
t (st , at ) − β1 Ût − Qt (st , at )
The key to proving (24), and by extension the theorem, is proving that e+
t goes sufficiently fast to 0.
We prove this by defining a bound on |e+
t | and showing that this bound goes to 0. Figure 11 illustrates
the bound. The variables in the figure refer to the following quantities:
a →

Q+
t (st , at )

b →
v →
w̃ →

Ût+
(1 − β1 ) a + β1 b
(1 − β1 )f (a) + β1 f (b)

w

→

f −1 (w̃)

The error e+
t corresponds with:


−1
e+
(1 − β1 )f (a) + β1 f (b) − (1 − β1 )a + β1 b = f −1 (w̃) − v = w − v
t =f
Note here that since f is a strictly concave function, the definition of w̃ and v directly imply w̃ < f (v).
Because f −1 is monotonically increasing, it follow that w < v, which yields |e+
t | = v − w.

Figure 11: Bounding the error, for the case a < b (left) and for a > b (right).
In both graphs of Figure 11, besides the mapping function f (x), three more functions are plotted:
g0 (x), g1 (x), and g2 (x). These three functions are all linear functions passing through the point
(a, f (a)). The function g0 (x) has derivative f 0 (a), while g2 (x) has derivative f 0 (b). The function
g1 (x) passes through point (b, f (b)) as well, giving it derivative (f (a) − f (b))/(a − b).
As illustrated by the figure, g1 (v) = w̃ and g1−1 (w̃) = v. Furthermore, for x between a and b the
following holds (in both cases):
g0 (x) ≥ f (x) ≥ g1 (x) ≥ g2 (x)
And, equivalently:
g0−1 (x) ≤ f −1 (x) ≤ g1−1 (x) ≤ g2−1 (x) .
−
+
We bound |e+
t | = v − w, by using a lowerbound w for w and an upperbound v for v. Specifically,
−1
−1
−
+
we define w := g0 (w̃) and v := g2 (w̃), and can now bound the error as follows: |e+
t | <=
v + − w− . Next, we compute an expression for the bound in terms of a, b, and f .

First, note that for the derivatives of g0 and g2 the following holds:
g00 (x) = f 0 (a) =

f (a) − w̃
a − w−

;
14

g20 (x) = f 0 (b) =

f (a) − w̃
.
a − v+

From this it follows that:
w− =

w̃ − f (a)
+a ;
f 0 (a)

v+ =

w̃ − f (a)
+ a.
f 0 (b)

Using this, we rewrite our bound as:
w̃ − f (a) w̃ − f (a)
−
f 0 (b)
f 0 (a)


1
1
−
· (w̃ − f (a))
=
f 0 (b) f 0 (a)

 

1
1
=
−
·
(1
−
β
)f
(a)
+
β
f
(b)
−
f
(a)
1
1
f 0 (b) f 0 (a)



1
1
= β1
−
f (b) − f (a)
f 0 (b) f 0 (a)

v + − w−

=

Recall that f (x) := c ln(x + γ k ) + d. The derivative of f (x) is
c
f 0 (x) =
x + γk
Substituting f (x) and f 0 (x) in the expression for the bound gives:



a + γk
b + γk
+
−
−
c ln(b + γ k ) + d − (c ln(a + γ k ) + d)
v −w
= β1
c
c
= β1 (b − a)(ln(b + γ k ) − ln(a + γ k ))
= β1 (a − b)(ln(a + γ k ) − ln(b + γ k ))


a + γk
= β1 (a − b) ln
b + γk


a−b
= β1 (a − b) ln
+1
b + γk
Using the definitions of a and b, the results for the bound for e+
t :
|e+
t |

−

+

≤v −w ≤

β1 (Q+
t (st , at )

−

Ût+ ) ln

+
Q+
t (st , at ) − Ût

Ût+ + γ k

!
+1

(33)

Definition (19) can be written as:
+
+
Ût+ := Q+
t (st , at ) + βreg,t Ut − Qt (st , at )



(34)

yielding:
+
Q+
t (st , at ) − Ût



+
+
+
= Q+
(s
,
a
)
−
Q
(s
,
a
)
+
β
U
−
Q
(s
,
a
)
t t
t t
t t
reg,t
t
t
t
t

+
+
= β2 Qt (st , at ) − Ut

Substituting this in (33) gives:
|e+
t |

≤

β1 β2 Q+
t (st , at )

Ut+

−

Q+
t (st , at )

Ut+



ln

+
β2 Q+
t (st , at ) − Ut

Ût+ + γ k

!


+1

Let us define c+
t as:
c+
t

:=

−



ln

+
β2 Q+
t (st , at ) − Ut

Ût+ + γ k

!


+1

+
+
Hence, |e+
t | ≤ β1 β2 ct . Substituting maximum bound of |et | and (34) in (31), we get:

+
+
+
+
Q+
t+1 (st , at ) = Qt (st , at ) + β1 β2 Ut − Qt (st , at ) + ct

with

c+
t

going to 0, if β2 goes to 0, which concludes part 2 of the proof.
15

(35)

B

Hypothesis Testing

The following hypotheses are tested: 1) lower discount factors cause poor performance because they
result in smaller action gaps; 2) lower discount factors cause poor performance because they result in
smaller relative action gaps (i.e, the action gap of a state divided by the maximum action-value of
that state).

0.50

w: 1
w: 2
0.25
w: 3
w: 5
0.00
0.2 0.4
1.00

0.6

0.8

1.0

regular ; long

0.75
0.50

w: 1
w: 2
0.25
w: 3
w: 5
0.00
0.2 0.4

0.6

0.8

1.0

x 100 ; short
average performance

0.75

1.00
0.75
0.50

w: 1
w: 2
0.25
w: 3
w: 5
0.00
0.2 0.4
1.00

0.6

0.8

1.0

x 100 ; long
average performance

regular ; short
average performance

1.00

average performance

average performance

average performance

To test the first hypothesis, we performed the same experiment as in Section 3.2, but with rewards
that are a factor 100 larger. This in turn increases the action gaps by a factor 100 as well. Hence,
to validate the first hypothesis, this modification should improve (i.e., lower) the threshold value
where the performance falls flat. To test the second hypothesis, we pushed all action-values up by
100 through additional rewards, reducing the relative action-gap. Specifically, the extra reward upon
transitioning to a non-terminal state 100 · (1 − γ), while the extra reward upon transition to a terminal
state is 100. This effectively pushes all action-values up by exactly 100. To validate the second
hypothesis, performance should degrade for this variation. We plotted the performance of these task
variations, together with the performance on the regular task, in Figure 12. Both variations show
roughly the same performance as the performance on the regular tasks, invalidating both hypotheses.

0.75
0.50

w: 1
w: 2
0.25
w: 3
w: 5
0.00
0.2 0.4

0.6

0.8

1.0

1.00

+ 100 ; short

0.75
0.50

w: 1
w: 2
w: 3
w: 5
0.00
0.2 0.4
0.25

1.00

0.6

0.8

1.0

0.8

1.0

+ 100 ; long

0.75
0.50

w: 1
w: 2
w: 3
w: 5
0.00
0.2 0.4
0.25

0.6

Figure 12: Performance on 3 variations of the chain task. Left: performance on the regular version;
middle: performance on the variant with values 100 times larger; right: performance on the variant
with values pushed up by 100. All versions result in roughly the same performance curves.

C

Additional Details for Logarithmic DQN

In this section, we describe additional details specific to our deep RL (Atari) experiments.
C.1

Implementation

In order to support reproducibility and enable reliable and accessible baseline comparisons, we base
our implementation upon the Google’s Dopamine framework (Castro et al., 2018). Dopamine provides
reliable, open-source code for several important deep RL algorithms (including DQN) and enables
standardized benchmarking, yielding ‘apples to apples’ comparison under best known evaluation
practices in RL. Therefore, we evaluate LogDQN without modifications of agent or environment
parameters (w.r.t. those outlined by Castro et al. (2018)), except for LogDQN’s hyper-parameters
(i.e., γ, k, c, βlog , βreg , and d; for which the chosen values are stated in the paper).
We now highlight any settings in our LogDQN implementation that differs from our formulation of
the logarithmic Q-learning update rules, as follows:
• The most commonly-used loss function for DQN (and the default setting in Dopamine) is
based on the Huber loss function (Huber, 1992), which slightly differs from the squared-error
16

loss specified as the general setting. While our results are for the standard Huber loss setting,
in our primary experiments we did not observe any significant difference between the two.
• To optimize the loss function, we use the standard RMSProp optimizer4 (as the default
setting in Dopamine’s DQN). This choice differs slightly from our logarithmic Q-learning
formulation which illustrates the case for the fundamental gradient descent method.
• To initialize the LogDQN network, we generally use the standard Xavier initialization
(Glorot and Bengio, 2010) scheme (also a Dopamine’s default setting), with the mere
exception of initializing the output-layer weights of our Q− function to zero (instead of
small, noisy values around zero).
• We replaced the additive γ k in our original formulation of the mapping function, its inverse,
and d hyper-parameter with a minimum-clipping at γ k (i.e., enforcing the aforementioned
value to be the minimum possible value in the corresponding computations). This gives a
hard bound on the values that can be represented, instead of a soft bound, and increases
the independence between the k and γ parameters, which is useful when optimizing hyperparameters.
C.2

Hyper-parameter tuning

The hyper-parameters of LogDQN used for the experiments are the result from an earlier hyperparameter optimization performed using an older version of LogDQN that did not have a strategy to
deal with stochastic environments (as described in Section 4.2). Due to time-constraints, we were
unable to perform a new hyper-parameter optimization for the full version of LogDQN.
This earlier hyper-parameter optimization was performed across these 6 games: A LIEN, Z AXXON,
B REAKOUT, D OUBLE D UNK, S PACE I NVADERS, and F ISHING D ERBY. For the discount factor, we
tried γ ∈ {0.84, 0.92, 0.96, 0.98, 0.99} and for c we tried c ∈ {0.1, 0.5, 1.0, 2.0, 5.0}. Furthermore,
k was fixed at 100. For DQN, we tried the same γ values. Figure 13 shows the mean and median
human-normalized score across these 6 games.

Human-Normalized Score

In Figure 13, for LogDQN, the performance at the best c-value is plotted for each γ. The best values
for LogDQN are γ = 0.96 and c = 0.5; for DQN, the best value is γ = 0.99 (according to the more
robust median metric).
Mean

Median

Figure 13: Mean and median performance across 6 games for (an incomplete version of) LogDQN
and DQN.

4

See http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf

17

Figure 14: Learning curves for all 55 games.

18

