arXiv:1909.08156v1 [cs.LG] 18 Sep 2019

Dynamics of Deep Neural Networks and Neural Tangent Hierarchy

Jiaoyang Huang

Horng-Tzer Yau

IAS
E-mail: jiaoyang@ias.edu

Harvard University
E-mail: htyau@math.harvard.edu

Abstract
The evolution of a deep neural network trained by the gradient descent can be described by its neural
tangent kernel (NTK) as introduced in [20], where it was proven that in the infinite width limit the
NTK converges to an explicit limiting kernel and it stays constant during training. The NTK was also
implicit in some other recent papers [6, 13, 14]. In the overparametrization regime, a fully-trained deep
neural network is indeed equivalent to the kernel regression predictor using the limiting NTK. And the
gradient descent achieves zero training loss for a deep overparameterized neural network. However, it
was observed in [5] that there is a performance gap between the kernel regression using the limiting NTK
and the deep neural networks. This performance gap is likely to originate from the change of the NTK
along training due to the finite width effect. The change of the NTK along the training is central to
describe the generalization features of deep neural networks.
In the current paper, we study the dynamic of the NTK for finite width deep fully-connected neural
networks. We derive an infinite hierarchy of ordinary differential equations, the neural tangent hierarchy
(NTH) which captures the gradient descent dynamic of the deep neural network. Moreover, under
certain conditions on the neural network width and the data set dimension, we prove that the truncated
hierarchy of NTH approximates the dynamic of the NTK up to arbitrary precision. This description
makes it possible to directly study the change of the NTK for deep neural networks, and sheds light on
the observation that deep neural networks outperform kernel regressions using the corresponding limiting
NTK.

1

Introduction

Deep neural networks have become popular due to their unprecedented success in a variety of machine
learning tasks. Image recognition [25, 26, 42], speech recognition [19, 34], playing Go [35, 36] and natural
language understanding [10, 12, 44] are just a few of the recent achievements. However, one aspect of deep
The work of H.-T. Y. is partially supported by NSF Grants DMS-1606305 and DMS-1855509, and a Simons Investigator
award.

1

neural networks that is not well understood is training. Training a deep neural network is usually done
via a gradient decent based algorithm. Analyzing such training dynamics is challenging. Firstly, as highly
nonlinear structures, deep neural networks usually involve a large number of parameters. Secondly, as highly
non-convex optimization problems, there is no guarantee that a gradient based algorithm will be able to find
the optimal parameters efficiently during the training of neural networks. One question then arises: given
such complexities, is it possible to obtain a succinct description of the training dynamics?
In this paper, we focus on the empirical risk minimization problem with the quadratic loss function
min L(θ) =
θ

n
1 X
(f (xα , θ) − yα )2 ,
2n α=1

where {xα }nα=1 are the training inputs, {yα }nα=1 are the labels, and the dependence is modeled by a deep
fully-connected feedforward neural network with H hidden layers. The network has d input nodes, and the
input vector is given by x ∈ Rd . For 1 6 ℓ 6 H, the ℓ-th hidden layer has m neurons. Let x(ℓ) be the
output of the ℓ-th layer with x(0) = x. Then the feedforward neural network is given by the set of recursive
equations:
1
(1.1)
x(ℓ) = √ σ(W (ℓ) x(ℓ−1) ), ℓ = 1, 2, · · · , H,
m
where W (ℓ) ∈ Rm×d if ℓ = 1 and W (ℓ) ∈ Rm×m if 2 6 ℓ 6 H are the weight matrices, and σ is the activation
unit, which is applied coordinate-wise to its input. The output of the neural network is
f (x, θ) = a⊤ x(H) ∈ R,

(1.2)

where a ∈ Rm is the weight matrix for the output layer. We denote the vector containing all trainable parameters by θ = (vec(W√(1) ), vec(W (2) ) . . . , vec(W (H) ), a). We remark that this parametrization is nonstandard
because of those 1/ m factors. However, it has already been adopted in several recent works [13, 14, 20, 27].
We note that the predictions
and training dynamics of (1.1) are identical to those of standard networks, up
√
to a scaling factor 1/ m in the learning rate for each parameter.
We initialize the neural network with random Gaussian weights following the Xavier initialization scheme
(ℓ)
2
), ai ∼ N (0, σa2 ). In this way,
[18]. More precisely, we set the initial parameter vector θ0 as Wij ∼ N (0, σw
for the randomly initialized neural network, we have that the L2 norms of the output of each layer are of
order one, i.e. kx(ℓ) k22 = O(1) for 0 6 ℓ 6 H, and f (x, θ0 ) = O(1) with high probability. In this paper, we
train all layers of the neural network with continuous time gradient descent (gradient flow): for any time
t>0
(ℓ)

∂t Wt
(1)

= −∂W (ℓ) L(θt ),

(2)

(H)

where θt = (vec(Wt ), vec(Wt ) . . . , vec(Wt

ℓ = 1, 2, · · · , H,

∂t at = −∂a L(θt ),

(1.3)

), at ).

For simplicity of notations, we write σ(W (ℓ) x(ℓ−1) ) as σℓ (x), or simply σℓ if the context is clear. We
(1)
write its derivative diag(σ ′ (W (ℓ) x(ℓ−1) )) as σℓ′ (x) = σℓ (x), and r-th derivative diag(σ (r) (W (ℓ) x(ℓ−1) )) as
(r)
(r)
(r)
σℓ (x), or σℓ for r > 1. In this notation, σℓ (x) are diagonal matrices. With those notations, explicitly,
2

the continuous time gradient descent dynamic (1.3) is
(ℓ)

∂t Wt
1
=−
n

n
X

= −∂W (ℓ) L(θt )

β=1

(ℓ+1)

at
(W
)⊤
′
· · · σH
(xβ ) √
σℓ′ (xβ ) t√
m
m

!

(ℓ−1) ⊤

⊗ (xβ

) (f (xβ , θt ) − yβ ),

(1.4)

for ℓ = 1, 2, · · · , H, and
∂t at = −∂a L(θt ) = −

1.1

n
1 X (H)
xβ (f (xβ , θt ) − yβ ).
n

(1.5)

β=1

Neural Tangent Kernel

A recent paper [20] introduced the Neural Tangent Kernel (NTK) and proved the limiting NTK captures the
behavior of fully-connected deep neural networks in the infinite width limit trained by gradient descent:
∂t f (x, θt ) = ∂θ f (x, θt )∂t θt = −∂θ f (x, θt )∂θ L(θt )
n
n
X
1
1 X (2)
= − ∂θ f (x, θt )
∂θ f (xβ , θt )(f (xβ , θt ) − yβ ) = −
Kt (x, xβ )(f (xβ , θt ) − yβ ),
n
n
β=1

(1.6)

β=1

(2)

where the NTK Kt (·, ·) is given by
(2)

Kt (xα , xβ ) = h∂θ f (xα , θt ), ∂θ f (xβ , θt )i =

H+1
X

(ℓ)

Gt (xα , xβ )

(1.7)

ℓ=1

and for 1 6 ℓ 6 H,
(ℓ)

Gt (xα , xβ ) = h∂W (ℓ) f (xα , θt ), ∂W (ℓ) f (xβ , θt )i
+
*
(ℓ+1) ⊤
(ℓ+1) ⊤
at
(Wt
)
at
(Wt
)
(ℓ−1)
′
′
′
′
√
√
hx(ℓ−1)
, xβ
i
· · · σH (xα ) √ , σℓ (xβ )
· · · σH (xβ ) √
= σℓ (xα )
α
m
m
m
m
and
(H+1)

Gt

(H)

(H)
= h∂a f (xα , θt ), ∂a f (xβ , θt )i = hxα
, xβ i.

(2)

The NTK Kt (·, ·) varies along training. However, in the infinite width limit, the training dynamic is very
(2)
(2)
simple: The NTK does not change along training, Kt (·, ·) = K∞ (·, ·). The network function f (x, θt )
follows a linear differential equation [20]:
∂t f (x, θt ) = −

n
1 X (2)
K∞ (x, xβ )(f (xβ , θt ) − yβ ),
n
β=1

3

(1.8)

which becomes analytically tractable. In other words, the training dynamic is equivalent to the kernel
(2)
regression using the limiting NTK K∞ (·, ·). While the linearization (1.8) is only exact in the infinite width
limit, for a sufficiently wide deep neural network, (1.8) still provides a good approximation of the learning
dynamic for the corresponding deep neural network [13, 14, 27]. As a consequence, it was proven in [13, 14]
that, for a fully-connected wide neural network with m & n4 under certain assumptions on the data set, the
gradient descent converges to zero training loss at a linear rate. Although highly overparametrized neural
networks is equivalent to the kernel regression, it is possible to show that the class of finite width neural
networks is more expressive than the limiting NTK. It has been constructed in [1,17,46] that there are simple
functions that can be efficiently learnt by finite width neural networks, but not the kernel regression using
the limiting NTK.

1.2

Contribution

There is a performance gap between the kernel regression (1.8) using the limiting NTK and the deep neural
networks. It was observed in [5] that the convolutional neural networks outperform their corresponding limiting NTK by 5% - 6%. This performance gap is likely to originate from the change of the NTK along training
due to the finite width effect. The change of the NTK along training has its benefits on generalization.
In the current paper, we study the dynamic of the NTK for finite width deep fully-connected neural
networks. Here we summarize our main contributions:
• We show the gradient descent dynamic is captured by an infinite hierarchy of ordinary differential
equations, the neural tangent hierarchy (NTH). Different from the limiting NTK (1.7), which depends
only on the neural network architecture, the NTH is data dependent and capable of learning datadependent features.
• We derive a priori estimates of the higher order kernels involved in the NTH. Using these a priori
estimates as input, we confirm a numerical observation in [27] that the NTK varies at a rate of order
O(1/m). As a corollary, this implies that for a fully-connected wide neural network with m & n3 , the
gradient descent converges to zero training loss at a linear rate, which improves the results in [13].
• The NTH is just an infinite sequence of relationship. Without truncation, it cannot be used to determine the dynamic of the NTK. Using the a priori estimates of the higher order kernels as input,
we construct a truncated hierarchy of ordinary differential equations, the truncated NTH. We show
that this system of truncated equations approximates the dynamic of the NTK to certain time up to
arbitrary precision. This description makes it possible to directly study the change of the NTK for
deep neural networks.

1.3

Notations

In the paper, we fix a large constant p > 0, which appears in Assumptions (2.1) and (2.2). We use c, C to
represent universal constants, which might be different from line to line. In the paper, we write a = O(b)
or a . b if there exists some large universal constant C such that |a| 6 Cb. We write a & b if there exists
some small universal constant c > 0 such that a > cb. We write a ≍ b if there exist universal constants c, C
such that cb 6 |a| 6 Cb. We reserve n for the number of input samples and m for the width of the neural
network. For practical neural networks, we always have that m . poly(n) and n . poly(m). We denote
4

the set of input samples as X = {x1 , x2 , · · · , xn }. For simplicity of notations, we write the output of the
neural network as fβ (t) = f (xβ , θt ). We denote vector L2 norm as k · k2 , vector or function L∞ norm as
k · k∞ , matrix spectral norm as k · k2→2 , and matrix Frobenius norm as k · kF . We say that an event holds
c
with high probability, if it holds with probability at least 1 − e−m for some c > 0. Then the intersection of
poly(n, m) many high probability events is still a high probability event, provided m is large enough. In the
paper, we treat cr , Cr in Assumption 2.1 and 2.2, and the depth H as constants. We will not keep track of
them.

1.4

Related Work

In this section, we survey an incomplete list of previous works on optimization aspect of deep neural networks.
Because of the highly non-convexity nature of deep neural networks, the gradient based algorithms can
potentially get stuck near a critical point, i.e., saddle point or local minimum. So one important question in
deep neural networks is: what does the loss landscape look like. One promising candidate for loss landscapes
is the class of functions that satisfy: (i) all local minima are global minima and (ii) there exists a negative
curvature for every saddle point. A line of recent results show that, in many optimization problems of
interest [7, 15, 16, 33, 40, 41], loss landscapes are in such class. For this function class, (perturbed) gradient
descent [15, 21, 28] can find a global minimum. However, even for a three-layer linear network, there exists
a saddle point that does not have a negative curvature [22]. So it is unclear whether this geometry-based
approach can be used to obtain the global convergence guarantee of first-order methods. Another approach
is to show that practical deep neural networks allow some additional structure or assumption to make nonconvex optimizations tractable. Under certain simplification assumptions, it has been proven recently that
there are novel loss landscape structures in deep neural networks, which may play a role in making the
optimization tractable [9, 11, 22, 24, 30].
Recently, it was proved in a series of papers that, if the size of a neural network is significantly larger
than the size of the dataset, the (stochastic) gradient descent algorithm can find optimal parameters [3, 13,
14, 29, 39, 47]. In the overparametrization regime, a fully-trained deep neural network is indeed equivalent
to the kernel regression predictor using the limiting NTK (1.8). As a consequence, the gradient descent
achieves zero training loss for a deep overparameterized neural network. Under further assumptions, it can
be shown that the trained networks generalize [2, 6]. Unfortunately, there is a significant gap between the
overparametrized neural networks, which are provably trainable, and neural networks in common practice.
Typically, deep neural networks used in practical applications are trainable, and yet, much smaller than
what the previous theories require to ensure trainability. In [23], it is proven that gradient descent can find
a global minimum for certain deep neural networks of sizes commonly encountered in practice.
Training dynamics of neural networks in the mean field setting have been studied in [4, 8, 31, 32, 37,
38]. Their mean field analysis describes distributional dynamics of neural network parameters via certain
nonlinear partial differential equations, in the asymptotic regime of large network sizes and large number
of stochastic gradient descent training iterations. However, their analysis is restricted
√ to neural networks in
the mean-field framework with a normalization factor 1/m, different from ours 1/ m, which is commonly
used in modern networks [18].
5

2

Main results

Assumption 2.1. The activation function σ is smooth, and for any 1 6 r 6 2p + 1, there exists a constant
Cr > 0 such that the r-th derivative of σ satisfies kσ (r) (x)k∞ 6 Cr .
Assumption 2.1 is satisfied by using common activation units such as sigmoid and hyperbolic tangents.
Moreover, the softplus activation, which is defined as σa (x) = ln(1 + exp(ax))/a, satisfies Assumption 2.1
with any hyperparameter a ∈ R>0 . The softplus activation can approximate the ReLU activation for any
desired accuracy as
σa (x) → relu(x) as a → ∞,
where relu represents the ReLU activation.
Assumption 2.2. There exists a small constant c > 0 such that the training inputs satisfy c < kxα k2 6 c−1 .
For any 1 6 r 6 2p + 1, there exists a constant cr > 0 such that for any distinct indices 1 6 α1 , α2 , · · · , αr 6
n, the smallest singular value of the data matrix [xα1 , xα2 , · · · , xαr ] is at least cr .
For more general input data, we can always normalize them such that c < kxα k2 6 c−1 . Under
(ℓ)
this normalization, for the randomly initialized deep neural network, it holds that kxα k2 = O(1) for all
1 6 ℓ 6 H, where the implicit constants depend on ℓ. The second part of Assumption 2.2 requires that for
any small number of input data: xα1 , xα2 , · · · , xαr , they are linearly independent.
(r)

Theorem 2.3. Under Assumptions 2.1 and 2.2, there exists an infinite family of operators Kt : X r 7→ R for
r > 2, the continuous time gradient descent dynamic is given by an infinite hierarchy of ordinary differential
equations, i.e., the NTH,
∂t (fα (t) − yα ) = −

n
1 X (2)
Kt (xα , xβ )(fβ (t) − yβ ),
n

(2.1)

β=1

and for any r > 2,
(r)

∂t Kt (xα1 , xα2 , · · · , xαr ) = −

n
1 X (r+1)
Kt
(xα1 , xα2 , · · · , xαr , xβ )(fβ (t) − yβ ).
n

(2.2)

β=1

There exists a deterministic family (independent of m) of operators K(r) : X r 7→ R for 2 6 r 6 p + 1 and
K(r) = 0 if r is odd, such that with high probability with respect to the random initialization, there exist some
constants C, C′ > 0 such that
(r)

K0 −
p

K(r)
mr/2−1

.
∞

(ln m)C
,
m(r−1)/2

(2.3)

′

and for 0 6 t 6 m 2(p+1) /(ln m)C ,
(r)

kKt k∞ .
6

(ln m)C
.
mr/2−1

(2.4)

It√was proven in [13,27] that the change of the NTK for a wide deep neural network is upper bounded by
O(1/ m). However, the numerical experiments in [27] indicate the change of the NTK is closer to O(1/m).
As a corollary of Theorem 2.3, we confirm the numerical observation that the NTK varies at a rate of order
O(1/m).
(2)

Corollary 2.4. Under Assumptions 2.1 and 2.2, the NTK Kt (·, ·) varies at a rate of order O(1/m): with
high probability with respect to the random initialization, there exist some constants C, C′ > 0 such that for
p
′
0 6 t 6 m 2(p+1) /(ln m)C , it holds
(2)

k∂t Kt k∞ .

(1 + t)(ln m)C
.
m

As another corollary of Theorem 2.3, for a fully-connected wide neural network with m & n3 , the gradient
descent converges to zero training loss at a linear rate.
Corollary 2.5. Under Assumptions 2.1 and 2.2, we further assume that there exists λ > 0 (which might
depend on n)
i
h
(2)
> λ,
(2.5)
λmin K0 (xα , xβ )
16α,β6n

and the width m of the neural network satisfies
m > C′

 n 3
λ

(ln m)C ln(n/ε)2 ,

(2.6)

for some large constants C, C′ > 0. Then with high probability with respect to the random initialization, the
training error decays exponentially,
n
X

λt

(fβ (t) − yβ )2 . ne− 2n ,

β=1

which reaches ε at time t ≍ (n/λ) ln(n/ε).

It is proven in [13] that if there exists λ(H) > 0,
h
i
(H)
λmin G0 (xα , xβ )

16α,β6n

> λ(H) ,

then for m > C(n/λ(H) )4 the gradient descent finds a global minimum. Corollary 2.5 improves this result
in two ways: (i) We improve the quartic dependence of n to a cubic dependence. (ii) We recall that
PH+1 (ℓ)
(2)
(ℓ)
(2)
Kt = ℓ=1 G0 , and those kernels G0 are all non-negative definite. The smallest eigenvalue of K0 is
(H)
(2)
typically much bigger than that of G0 , i.e., λ ≫ λ(H) . Moreover, since Kt is a sum of H + 1 non-negative
definite operators, we expect that λ gets larger, if the depth H is larger.
The NTH, i.e., (2.1) and (2.2), is just an infinite sequence of relationship. It cannot be used to determine
the dynamic of NTK. However, thanks to the a priori estimates of the higher order kernels (2.4), it holds that
(p+1)
(p)
with high probability kKt
k∞ . (ln m)C /mp/2 . The derivative ∂t Kt is an expression involves the higher
(p+1)
order kernel Kt
, which is small provided that p is large enough. Therefore, we can approximate the
7

(p)

original NTH (2.2) by simply setting ∂t Kt = 0. In this way, we obtain the following truncated hierarchy
of ordinary differential equations of p levels, which we call the truncated NTH,
1
∂t f˜α (t) = −
n

n
X

β=1

(2)
K̃t (xα , xβ )(f˜β (t) − yβ ),

(r)

∂t K̃t (xα1 , xα2 , · · · , xαr ) = −
(p)
∂t K̃t (xα1 , xα2 , · · ·

n
1 X (r+1)
K̃t
(xα1 , xα2 , · · · , xαr , xβ )(f˜β (t) − yβ ),
n
β=1

2 6 r 6 p − 1,

(2.7)

, xαp ) = 0.

where
f˜β (0) = fβ (0),

β = 1, 2, · · · , n,

(r)

K̃0

(r)

= K0 ,

r = 2, 3, · · · , p.

In the following theorem, we show this system of truncated equations (2.7) approximates the dynamic
of the NTK up to arbitrary precision, provided that p is large enough.
Theorem 2.6. Under Assumptions 2.1 and 2.2, we take an even p and further assume that
h
i
(2)
> λ.
λmin K0 (xα , xβ )
16α,β6n

Then there exist constants c, C, C′ > 0 such that for t
p
p
′
t 6 min{c λm/n/(ln m)C , m 2(p+1) /(ln m)C },

(2.8)

the dynamic (2.1) can be approximated by the truncated dynamic (2.7),



n
X

1/2

(fβ (t) − f˜β (t))2 

β=1

and
(2)

(2)

|Kt (xα , xβ ) − K̃t (xα , xβ )| .

√
n no
(1 + t)tp−1 n
.
,
min
t,
λ
mp/2

(1 + t)tp−1
mp/2


n n o
(1 + t)t(ln m)C
min t,
.
1+
m
λ

(2.9)

(2.10)

We remark that the error terms, i.e., the righthand sides of (2.9) and (2.10) can be arbitrarily small,
provided that p is large enough. In other words, if we the p large enough, the truncated NTH (2.7) can
approximate the original dynamic (2.1), (2.2) up to any precision provided that the time constraint (2.8) is
satisfied. Now if we take t ≍ (n/λ) ln(n/ε), so that Corollary 2.5 guarantees the convergence of the dynamics.
Consider two special cases: (i) If we take p = 2, then the error in (2.9) is O(n7/2 ln(n/ε)3 /λ3 m) when
t ≍ (n/λ) ln(n/ε), which is negligible provided that the width m is much bigger than n7/2 . We conclude that
if m is much bigger than n7/2 , the truncated NTH gives a complete description of the original dynamic of the
NTK up to the equilibrium. The condition that m is much bigger than n7/2 is better than the previous best
available one which requires m & n4 . (ii) If we take p = 3, then the error in (2.9) is O(n9/2 ln(n/ε)4 /λ4 m3/2 )
when t ≍ (n/λ) ln(n/ε), which is negligible provided that the width m is much bigger than n3 . We conclude
8

that if m is much bigger than n3 , the truncated NTH gives a complete description of the original dynamic
of the NTK up to the equilibrium. Finally, we note that the estimates in Theorem 2.6 clearly improved for
smaller t.
The previous convergence theory of overparametrized neural networks works only for very wide neural
networks, i.e., m & n3 . For any width (not necessary that m & n3 ), Theorem 2.6 guarantees that the
truncated NTH approximates the training dynamics of deep neural networks. The effect of the width
appears in the approximation time and the error terms, (2.9) and (2.10), i.e., the wider the neural networks
are, the truncated dynamic (2.7) approximates the training dynamic for longer time and the approximation
error is smaller. We recall from (1.7) that the NTK is the sum of H + 1 non-negative definite operators,
PH+1 (ℓ)
(2)
Kt = ℓ=1 Gt . We expect that λ gets bigger, if the depth H is larger. Therefore, large width and depth
makes the truncated dynamic (2.7) a better approximation.
Thanks to Theorem 2.6, the truncated NTH (2.7) provides a good approximation for the evolution of
the NTK. The truncated dynamic can be used to predict the output of new data points. Recall that the
training data are {(xβ , yβ )}16β6n ⊂ Rd × R. The goal is to predict the output of a new data point x. To do
this, we can first use the truncated dynamic to solve for the approximated outputs {f˜β (t)}16β6n . Then the
prediction on the new test point x ∈ Rd can be estimated by sequentially solving the higher order kernels
(p)
(p−1)
(2)
K̃t (x, X p−1 ), K̃t
(x, X p−2 ), · · · , K̃t (x, X ) and f˜x (t),
∂t f˜x (t) = −

n
1 X (2)
K̃t (x, xβ )(f˜β (t) − yβ ),
n
β=1

(r)

∂t K̃t (x, xα1 , xα2 , · · · , xαr−1 ) = −
(p)

n
1 X (r+1)
K̃t
(x, xα1 , xα2 , · · · , xαr −1 , xβ )(f˜β (t) − yβ ),
n
β=1

2 6 r 6 p − 1,

∂t K̃t (x, xα1 , xα2 , · · · , xαp−1 ) = 0.

3

(2.11)

Technique overview

We recall the NTK from (1.7),
(2)

(H)

(H)
Kt (xα , xβ ) = hxα
, xβ i+
+
*
H
(ℓ+1) ⊤
(ℓ+1) ⊤
X
at
(Wt
)
at
(Wt
)
(ℓ−1)
′
′
′
′
√
√
hx(ℓ−1)
, xβ
i.
· · · σH (xα ) √ , σℓ (xβ )
· · · σH (xβ ) √
+
σℓ (xα )
α
m
m
m
m
ℓ=1

(2)

The kernel Kt (·, ·) is a sum of terms, which are product of inner products of vectors involving the quantities
(2)
(ℓ)
(ℓ)
at , Wt , xα and σℓ′ (xα ). To compute the derivatives of Kt (·, ·), we need the following ordinary differential
(ℓ)
(ℓ)
equations derived by using (1.4), (1.5) and the chain rule, which characterize the dynamics of at , Wt , xα
9

(r)

and σℓ (xα ) along the gradient flow.

∂t at = −

n
1 X (H)
xβ (fβ (t) − yβ ),
n
β=1

n
(ℓ+1) ⊤
1X
at
(W
)
′
∂t √ = −
· · · σH
(xβ ) √
diag σℓ′ (xβ ) t√
m
n
m
m
(ℓ)
Wt

β=1

∂t

(ℓ)
(Wt )⊤

√
m

∂t x(ℓ)
α =

n
1 X 1 (ℓ−1)
√ xβ
=−
⊗
n
m

(ℓ+1)

k=1

1
√ ⊗ (x(ℓ−1)
)⊤ (fβ (t) − yβ ),
β
m
!

Wt
a⊤ ′
√t σH
(xβ ) · · · √
σ ′ (xβ ) (fβ (t) − yβ ),
m
m ℓ

β=1

ℓ
X

!

n
(k+1)
(k+1) ⊤
(ℓ)
1X
Wt
(W
)
at
W
′
−
σk′ (xα )σk′ (xβ ) t√
· · · σH
(xβ ) √
diag σℓ′ (xα ) √t · · · √
n
m
m
m
m
β=1

!

1
(k−1)
(k−1)
× √ hxα
, xβ
i(fβ (t) − yβ ),
m

(r)

(r+1)

∂t σℓ (xα ) = σℓ

(ℓ)

(xα ) diag(∂t (Wt x(ℓ−1)
))
α

n
(ℓ+1) ⊤
1 X (r+1)
at
(W
)
′
=−
· · · σH
(xβ ) √
σℓ
(xα ) diag σℓ′ (xβ ) t√
n
m
m
β=1

+

ℓ−1
X

k=1

×

n
1 X (r+1)
σℓ
(xα ) diag
−
n
β=1

(k−1)
(k−1)
hxα
, xβ
i(fβ (t)

!

(ℓ−1)

hx(ℓ−1)
, xβ
α

(k+1)

(ℓ)

i(fβ (t) − yβ )
(k+1)

Wt
(W
)⊤
at
Wt
′
′
√ σℓ−1
(xα ) · · · √
σk′ (xα )σk′ (xβ ) t√
· · · σH
(xβ ) √
m
m
m
m

!

− yβ ).
(k)

We remark that the k = ℓ term on the right hand side of the expression in ∂t xα is

∂t x(k)
α

n
(ℓ+1) ⊤
1X
at
(W
)
′
=−
· · · σH
(xβ ) √
diag σℓ′ (xα )σℓ′ (xβ ) t√
n
m
m
β=1

!

1
(k−1)
(k−1)
√ hxα
, xβ
i(fβ (t) − yβ ).
m

(k)

All other cases with k < ℓ can be read clearly from the expression of ∂t xα given above.
(2)

Using the chain rule and the above expressions, the derivative of Kt (·, ·) is given by
(2)

∂t Kt (xα1 , xα2 ) = −

n
1 X (3)
Kt (xα1 , xα2 , xβ )(f (xβ , θt ) − yβ ),
n
β=1

(2)

(3)

where Kt (xα1 , xα2 , xβ ) is the sum of all the possible terms from Kt (xα1 , xα2 ) by performing one of the
10

following replacement:
(H)

at → xβ ,
(ℓ)

(ℓ+1)

Wt
(W
)⊤
at
′
√ → diag σℓ′ (xβ ) t√
· · · σH
(xβ ) √
m
m
m
(ℓ)

1 (ℓ−1)
(Wt )⊤
√
→ √ xβ
⊗
m
m
ℓ
X

!

(ℓ+1)

1
√ ⊗ (x(ℓ−1)
)⊤ ,
β
m
!

Wt
a⊤ ′
√t σH
(xβ ) · · · √
σ ′ (xβ ) ,
m
m ℓ
(ℓ)

(k+1)

(k+1)

W
Wt
(W
)⊤
at
′
x(ℓ)
diag σℓ′ (xα ) √t · · · √
σk′ (xα )σk′ (xβ ) t√
· · · σH
(xβ ) √
α →
m
m
m
m
k=1
!
(ℓ+1) ⊤
at
(W
)
(ℓ−1)
(r)
(r+1)
′
hx(ℓ−1)
, xβ
i
· · · σH
(xβ ) √
σℓ (xα ) → σℓ
(xα ) diag σℓ′ (xβ ) t√
α
m
m
+

ℓ−1
X

(r+1)
σℓ
(xα ) diag

k=1

(k+1)

(ℓ)

!

1
(k−1)
(k−1)
√ hxα
, xβ
i,
m

(k+1)

Wt
(W
)⊤
at
Wt
′
′
√ σℓ−1
(xα ) · · · √
σk′ (xα )σk′ (xβ ) t√
· · · σH
(xβ ) √
m
m
m
m

!

(k−1)

(k−1)
hxα
, xβ

i,

(3.1)
(r)

with α = α1 , α2 , where 1 = (1, 1, · · · , 1)⊤ ∈ Rm . By the same reasoning, the derivative of Kt
by
(r)

∂t Kt (xα1 , xα2 , · · · , xαr ) = −

is given

n
1 X (r+1)
Kt
(xα1 , xα2 , · · · , xαr xβ )(f (xβ , θt ) − yβ ),
n
β=1

(r)

(r+1)

where Kt
(xα1 , xα2 , · · · , xαr , xβ ) is the sum of all the possible terms from Kt (xα1 , xα2 , · · · , xαr ) by
performing any of the replacements in (3.1) with α = α1 , α2 , · · · , αr .
(3)

The followings are some examples of terms in Kt (xα1 , xα2 , xα3 )
*
+


(H) ⊤
(H) ⊤
(W
)
a
a
(W
)
a
t
t
t
(2)
t
t
′
′
′
′
√ , σH−1
σH−1
(xα1 ) √
(xα3 ) √
σH (xα1 ) diag σH
(xα2 ) √
σH
(xα2 ) √
m
m
m
m
m


at
at
1
′
′
σH
(xα1 ) √ , σH
(xα3 ) √
i; √
, x(H−1)
iihx(H−1)
, x(H−2)
× hx(H−2)
α3
α1
α2
α1
m
m
m
*
+
(H) ⊤
(Wt ) ′
at
′
′
, σH−1
(xα2 ) √
× σH−1
(xα1 )x(H−1)
i.
, x(H−2)
hx(H−2)
σH (xα2 ) √
α3
α2
α1
m
m
(r)

In general, from the construction, the summands appearing in Kt (xα1 , xα2 , · · · , xαr ) are product of inner
products of vectors obtained in the following way: starting from one of the vectors
at
√ ,
m

1
√ ,
m

(1)

(2)

(H)

{xβ , xβ , · · · , xβ }β∈{α1 ,α2 ,··· ,αr } ,

(i) multiply one of the matrices
(
)
(2)
(2)
(H)
(H)
Wt
(Wt )⊤
Wt
(Wt )⊤
′
√ , √
, {σ1′ (xβ ), σ2′ (xβ ), · · · , σH
(xβ )}β∈{α1 ,α2 ,··· ,αr } ;
,··· , √ , √
m
m
m
m
11

(3.2)

(3.3)

(ii) multiply one of the matrices
diag(· · · ),

σ (s) (xβ ) diag(· · · ) · · · diag(· · · ),
|
{z
}

s>2

(3.4)

s−1 terms

where diag(· · · ) is the diagonalization of a vector obtained by recursively using 1) and 2).
(r)

To describe the vectors appearing in Kt (xα1 , xα2 , · · · , xαr ) in a formal way, we need to introduce some
more notations. We denote D0 the set of expressions in the following form
D0 := {es es−1 · · · e1 e0 : 0 6 s 6 4H − 3},

(3.5)

where ej is chosen from the following sets:
o
n
√ (H)
√ (1) √ (2)
e0 ∈ at , { mxβ , mxβ , · · · , mxβ }16β6n

and for 1 6 j 6 s,
((
ej ∈

(2)

(2)

(H)

(H)

Wt
(W )⊤
W
(Wt )⊤
√ , √t
, · · · , √t , √
m
m
m
m

)

, {σ1′ (xβ ), σ2′ (xβ ), · · ·

′
, σH
(xβ )}16β6n

)

.

(2)

We remark that from expression (1.7), each summand in Kt (xα1 , xα2 ) is of the form
hv1 (t), v2 (t)i
,
m

hv1 (t), v2 (t)i hv3 (t), v4 (t)i
,
m
m
(2)

where v1 (t), v2 (t), v3 (t), v4 (t) ∈ D0 . But the set D0 contains more terms than those appearing in Kt (·, ·).
Given that we have constructed D0 , D1 , · · · , Dr , we denote Dr+1 the set of expressions in the following
form
Dr+1 := {es es−1 · · · e1 e0 : 0 6 s 6 4H − 3},

(3.6)

where ej is chosen from the following sets (notice that we have included 1 in the following set, which does
not appear in the definition of D0 ):
o
n
√ (H)
√ (1) √ (2)
e0 ∈ at , 1, { mxβ , mxβ , · · · , mxβ }16β6n ,
and for 1 6 j 6 s, ej belongs to one of the sets
((
)
)
(2)
(2)
(H)
(H)
Wt
(Wt )⊤
Wt
(Wt )⊤
′
√ , √
, {σ1′ (xβ ), σ2′ (xβ ), · · · , σH
(xβ )}16β6n ,
,··· , √ , √
m
m
m
m
{diag(d), d ∈ D0 ∪ D1 ∪ · · · ∪ Dr } ,
n
(u+1)
σℓ
(xβ ) diag(d1 ) diag(d2 ) · · · diag(du ) : 1 6 ℓ 6 H,

o
1 6 β 6 n, 1 6 u 6 r, d1 , d2 , · · · , du ∈ D0 ∪ D1 ∪ · · · ∪ Dr .
12

Moreover, the total number of diag operations in the expression es es−1 · · · e1 e0 ∈ Dr+1 is exactly r + 1. We
remark that if d ∈ Ds , then it contains s diag operations. On the other hand, by definition, we view diag(d)
as an element with s + 1 diag operations because the diag in diag(d) counted as one diag operation.
(2)

(3)

The kernel Kt (xα1 , xα2 , xα3 ) is obtained from Kt (xα1 , xα2 ) by the replacements (3.1) and taking
(3)
α = α1 , α2 and β = α3 . The summands in Kt (xα1 , xα2 , xα3 ) are of the forms
1 hv1 (t), v2 (t)i
√
,
m
m

1 hv1 (t), v2 (t)i hv3 (t), v4 (t)i
√
,
m
m
m

1 hv1 (t), v2 (t)i hv3 (t), v4 (t)i hv5 (t), v6 (t)i
√
,
m
m
m
m

(3.7)

where v1 (t), v2 (t), · · · , v6 (t) ∈ D0 ∪D1 . The first two terms in (3.7) are obtained from using the replacements
√
(ℓ) √
(ℓ)
for at , and the last two terms in (3.7) are obtained from using the replacements for Wt / m, (Wt )⊤ / m,
(r)
(r)
(ℓ)
xα and σℓ (xα ). More generally, we will show that each summand in Kt (xα1 , xα2 , · · · , xαr ) is of the
form
1
mr/2−1

s
Y
hv2j−1 (t), v2j (t)i
,
m
j=1

1 6 s 6 r,

v1 (t), v2 (t), · · · , v2s (t) ∈ D0 ∪ D1 ∪ · · · ∪ Dr−2 .

(3.8)

(r)

The initial value K0 (xα1 , xα2 , · · · , xαr ) can be estimated by successively conditioning based on the
depth of the neural network. A convenient scheme is given by the tensor program [45], which was developed
to characterize the scaling limit of neural network computations. In Appendix A, we show at time t = 0, those
vectors vj (0) in (3.8) are combinations of projections of independent Gaussian vectors. As a consequence,
we have Q
that hv2j−1 (0), v2j (0)i/m concentrates around certain constant with high probability. So does the
s
product j=1 hv2j−1 (0), v2j (0)i/m. This gives the claim (2.3).
In Appendix B, we consider the quantity:

ξ(t) = max{kvj (t)k∞ : vj (t) ∈ D0 ∪ D1 ∪ · · · ∪ Dp−1 }.
Again using the tensor program, we show that with high probability kvj (0)k∞ . (ln m)C . This gives the
estimate of ξ(t) at t = 0. Next we show that the (p + 1)-th derivative of ξ(t) can be controlled by itself. This
gives a self-consistent differential equation of ξ(t):
(p+1)

∂t

ξ(t) .

ξ(t)2p
.
mp/2

(3.9)
p

′

Combining with the initial estimate of ξ(t), it follows that for time 0 6 t 6 m 2(p+1) /(ln m)C , it holds that
ξ(t) . (ln m)C . Especially kvj (t)k∞ . (ln m)C . Then the claim (2.4) in Theorem 2.3 follows.
Thanks to the a priori estimate (2.4), we show that along the continuous time gradient descent, the
(r)
higher order kernels Kt vary slowly. We prove Corollary 2.4 and 2.5, and Theorem 2.6 in Appendix C by
a Grönwall type argument.

4

Discussion and future directions

In this paper, we study the continuous time gradient descent (gradient flow) of deep fully-connected neural
networks. We show that the training dynamic is given by a data dependent infinite hierarchy of ordinary
13

differential equations, i.e., the NTH. We also show that this dynamic of the NTK can be approximated by a
finite truncated dynamic up to any precision. This description makes it possible to directly study the change
of the NTK for deep neural networks. Here we list some future directions.
Firstly, we mainly study deep fully-connected neural networks here, we believe the same statements can
be proven for convolutional and residual neural networks.
Secondly, in this paper, for simplicity, we focus on the continuous time gradient descent. Our approach
developed here can be generalized to analyze discrete time gradient descent. We elaborate the main idea
here. The discrete time gradient descent is given by
θt+1 = θt − η∇θ L(θt ) = θt −

n
ηX
∇θ fβ (t)(fβ (t) − yβ ),
n
β=1

where η is the learning rate. We write the NTK as K(2) (xα , xβ ; θt ) to make the dependence on θt explicit.
To estimate the NTK K(2) (xα1 , xα2 ; θt+1 ) at time t + 1, we use the taylor expansion,
K(2) (xα1 , xα2 ; θt+1 ) ≈ K(2) (xα1 , xα2 ; θt ) +

p−1
X
r=3

(−η)r
nr

X

(4.1)

16β1 ,β2 ,··· ,βr−2 6n

K(r) (xα1 , xα2 , xβ1 , · · · , xβr−2 ; θt )(fβ1 (t) − yβ1 ) · · · (fβr−2 (t) − yβr−2 ),
where the higher order kernels K(r) are given by
(r−2)

K(r) (xα1 , xα2 , xβ1 , · · · , xβr−2 ; θt ) = ∇θ

K(2) (xα1 , xα2 ; θt )(∇θ fβ1 (t), ∇θ fβ2 (t), · · · , ∇θ fβr−2 (t)).

A similar argument as for (2.4) can be used to derive the a priori estimates of these kernels K(r) . We expect
to have that kK(r) k∞ . (ln m)C /mr/2−1 with high probability with respect to the random initialization.
Therefore the righthand side of (4.1) gives an approximation of the NTK K(2) (xα1 , xα2 ; θt+1 ) at time t + 1
up to arbitrary precision, provided that p is large enough. This gives a description of the NTK dynamics
under discrete time gradient descent.

References
[1] Z. Allen-Zhu and Y. Li. What can resnet learn efficiently, going beyond kernels?
arXiv:1905.10337, 2019.

arXiv preprint

[2] Z. Allen-Zhu, Y. Li, and Y. Liang. Learning and generalization in overparameterized neural networks,
going beyond two layers. arXiv preprint arXiv:1811.04918, 2018.
[3] Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over-parameterization. In
ICML, arXiv:1811.03962, 2018.
[4] D. Araújo, R. I. Oliveira, and D. Yukimura. A mean-field limit for certain deep neural networks. arXiv
preprint arXiv:1906.00193, 2019.
[5] S. Arora, S. S. Du, W. Hu, Z. Li, R. Salakhutdinov, and R. Wang. On exact computation with an
infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019.
14

[6] S. Arora, S. S. Du, W. Hu, Z. Li, and R. Wang. Fine-grained analysis of optimization and generalization
for overparameterized two-layer neural networks. arXiv preprint arXiv:1901.08584, 2019.
[7] S. Bhojanapalli, B. Neyshabur, and N. Srebro. Global optimality of local search for low rank matrix
recovery. In Advances in Neural Information Processing Systems, pages 3873–3881, 2016.
[8] L. Chizat and F. Bach. On the global convergence of gradient descent for over-parameterized models
using optimal transport. In Advances in neural information processing systems, pages 3036–3046, 2018.
[9] A. Choromanska, M. Henaff, M. Mathieu, G. Ben Arous, and Y. LeCun. The loss surfaces of multilayer networks. In Proceedings of the Eighteenth International Conference on Artificial Intelligence and
Statistics, pages 192–204, 2015.
[10] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language
processing (almost) from scratch. Journal of machine learning research, 12(Aug):2493–2537, 2011.
[11] Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in Neural
Information Processing Systems, pages 2933–2941, 2014.
[12] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers
for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[13] S. S. Du, J. D. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima of deep neural
networks. ICML, arXiv:1811.03804, 2018.
[14] S. S. Du, X. Zhai, B. Poczos, and A. Singh. Gradient descent provably optimizes over-parameterized
neural networks. In ICLR, arXiv:1810.02054, 2018.
[15] R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle points—online stochastic gradient for
tensor decomposition. In Proceedings of The 28th Conference on Learning Theory, pages 797–842, 2015.
[16] R. Ge, J. D. Lee, and T. Ma. Matrix completion has no spurious local minimum. In Advances in Neural
Information Processing Systems, pages 2973–2981, 2016.
[17] B. Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. Linearized two-layers neural networks in high
dimension. arXiv preprint arXiv:1904.12191, 2019.
[18] X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks.
In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages
249–256, 2010.
[19] G. Hinton, L. Deng, D. Yu, G. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
B. Kingsbury, et al. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal
processing magazine, 29, 2012.
[20] A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural
networks. In Advances in neural information processing systems, pages 8571–8580, 2018.
[21] C. Jin, R. Ge, P. Netrapalli, S. M. Kakade, and M. I. Jordan. How to escape saddle points efficiently.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1724–1732.
JMLR. org, 2017.
15

[22] K. Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing
Systems, pages 586–594, 2016.
[23] K. Kawaguchi and J. Huang. Gradient descent finds global minima for generalizable deep neural networks of practical sizes. arXiv preprint arXiv:1908.02419, 2019.
[24] K. Kawaguchi and L. P. Kaelbling. Elimination of all bad local minima in deep learning. arXiv preprint
arXiv:1901.00279, 2019.
[25] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural
networks. In Advances in neural information processing systems, pages 1097–1105, 2012.
[26] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
[27] J. Lee, L. Xiao, S. S. Schoenholz, Y. Bahri, J. Sohl-Dickstein, and J. Pennington. Wide neural networks
of any depth evolve as linear models under gradient descent. arXiv preprint arXiv:1902.06720, 2019.
[28] J. D. Lee, M. Simchowitz, M. I. Jordan, and B. Recht. Gradient descent only converges to minimizers.
In Conference on learning theory, pages 1246–1257, 2016.
[29] Y. Li and Y. Liang. Learning overparameterized neural networks via stochastic gradient descent on
structured data. In Advances in Neural Information Processing Systems, pages 8157–8166, 2018.
[30] S. Liang, R. Sun, J. D. Lee, and R. Srikant. Adding one neuron can eliminate all bad local minima. In
Advances in Neural Information Processing Systems, 2018.
[31] S. Mei, T. Misiakiewicz, and A. Montanari. Mean-field theory of two-layers neural networks: dimensionfree bounds and kernel limit. arXiv preprint arXiv:1902.06015, 2019.
[32] P.-M. Nguyen. Mean field limit of the learning dynamics of multilayer neural networks. arXiv preprint
arXiv:1902.02880, 2019.
[33] D. Park, A. Kyrillidis, C. Caramanis, and S. Sanghavi. Non-square matrix sensing without spurious
local minima via the burer-monteiro approach. arXiv preprint arXiv:1609.03240, 2016.
[34] T. N. Sainath, A.-r. Mohamed, B. Kingsbury, and B. Ramabhadran. Deep convolutional neural networks
for lvcsr. In 2013 IEEE international conference on acoustics, speech and signal processing, pages 8614–
8618. IEEE, 2013.
[35] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser,
I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural
networks and tree search. Nature, 529(7587):484–489, 2016.
[36] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,
M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354,
2017.
[37] J. Sirignano and K. Spiliopoulos.
arXiv:1903.04440, 2019.

Mean field analysis of deep neural networks.

arXiv preprint

[38] M. Song, A. Montanari, and P. Nguyen. A mean field view of the landscape of two-layers neural
networks. Proceedings of the National Academy of Sciences, 115:E7665–E7671, 2018.
16

[39] Z. Song and X. Yang. Quadratic suffices for over-parametrization via matrix chernoff bound. arXiv
preprint arXiv:1906.03593, 2019.
[40] J. Sun, Q. Qu, and J. Wright. Complete dictionary recovery over the sphere i: Overview and the
geometric picture. IEEE Transactions on Information Theory, 63(2):853–884, 2016.
[41] J. Sun, Q. Qu, and J. Wright. A geometric analysis of phase retrieval. Foundations of Computational
Mathematics, 18(5):1131–1198, 2018.
[42] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 1–9, 2015.
[43] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Compressed sensing,
pages 210–268. Cambridge Univ. Press, Cambridge, 2012.
[44] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao,
K. Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and
machine translation. arXiv preprint arXiv:1609.08144, 2016.
[45] G. Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. CoRR, abs/1902.04760, 2019.
[46] G. Yehudai and O. Shamir. On the power and limitations of random features for understanding neural
networks. arXiv preprint arXiv:1904.00687, 2019.
[47] D. Zou, Y. Cao, D. Zhou, and Q. Gu. Stochastic gradient descent optimizes over-parameterized deep
relu networks. arXiv preprint arXiv:1811.08888, 2018.

A

Initial Estimates
(r+1)

We have derived the dynamic (2.2) of the NTK in Section 3. The kernel Kt
(xα1 , xα2 , · · · , xαr , xβ ) is the
(r)
sum of all the possible terms from Kt (xα1 , xα2 , · · · , xαr ) by performing any of the replacements in (3.1)
with α = α1 , α2 , · · · , αr . We recall the sets Dr from Section 3, which are constructed recursively. Each
vector in Dr contains exact r diag operations. We have the following proposition on the structures of vectors
in Dr .
Proposition A.1. Given any expression v(t) ∈ Dr with some r > 0, new expressions obtained from v(t) by
performing one of the replacements in (3.1) are sum of terms of the following forms:
• v1 (t) with v1 (t) ∈ Dr ∪ Dr+1 ;
•
•
•

√
(k−1)
(k−1) √
, mxβ
i
v1 (t) h mxα
√
m
m

with 1 6 k 6 H and v1 (t) ∈ Dr+1 ;

√
(ℓ)
v1 (t) h mxβ ,v2 (t)i
√
with 1 6 ℓ 6 H and v1 (t) ∈ Dr−s+1 and v2 (t) ∈ Ds for some s > 1;
m
m
D
E
(ℓ+1) ⊤ √
′
′
) / m···σH
(xβ )at ,v2 (t)
v1 (t) σℓ (xβ )(Wt
√
with 1 6 ℓ 6 H, v1 (t) ∈ Dr−s and v2 (t) ∈ Ds
m
m

17

for some s > 1.

We remark that the time t in Proposition A.1 is only a parameter and this proposition does not involve
dynamics.
√
Proof of Proposition A.1. By performing the replacement for at , the new expression is given by v1 (t)/ m,
with v1 (t) ∈ Dr .
(ℓ)

By performing the replacement for xα , we get a sum of ℓ terms. Each of them is of the form
√ √ (k−1) √ (k−1)
, mxβ
i/m with v1 (t) containing one more diag operations. It is easy to check
v1 (t)/ mh mxα
that v1 (t) ∈ Dr+1 .
(ℓ) √
By performing the replacement for Wt / m, the new expression is given by
√ (ℓ)
v1 (t) h mxβ , v2 (t)i
√
,
m
m
with v1 (t) ∈ Dr−s+1 and v2 (t) ∈ Ds for some s > 1.
√
(ℓ)
By performing the replacement for (Wt )⊤ / m, the new expression is given by
*
+
(ℓ+1) ⊤
(Wt
)
v1 (t) 1
′
′
√
√
σℓ (xβ )
· · · σH (xβ )at , v2 (t) ,
m m
m
with v1 (t) ∈ Dr−s and v2 (t) ∈ Ds for some s > 1.

√
(u)
Finally, by performing the replacement for σℓ (xα ), we get a sum of ℓ terms of the form v1 (t)/ m, with
v1 (t) ∈ Dr+1 .
(r)

As a consequence of Proposition A.1, each summand in Kt (xα1 , xα2 , · · · , xαr ) is of the form
1
mr/2−1

s
Y
hv2j−1 (t), v2j (t)i
,
m
j=1

1 6 s 6 r,

v1 (t), v2 (t), · · · , v2s (t) ∈ D0 ∪ D1 ∪ · · · ∪ Dr−2 .

(A.1)

(r)

In the rest of this section we prove claim (2.3) in Theorem 2.3. To evaluate K0 (xα1 , xα2 , · · · , xαr ),
we use the tensor program in [45], which was developed to characterize the scaling limit of neural network
computations. We show at time t = 0, those vectors vj (0) in (3.8) are combinations of projections of
independent Gaussian vectors. As a consequence, we have Q
that hv2j−1 (0), v2j (0)i/m concentrates around
s
certain constant with high probability. So does the product j=1 hv2j−1 (0), v2j (0)i/m. This gives the claim
(2.3).
In the next section, we consider the quantity:
ξ(t) = max{kvj (t)k∞ : vj (t) ∈ D0 ∪ D1 ∪ · · · ∪ Dp−1 }.
Again using the tensor program, we show that with high probability kvj (0)k∞ . (ln m)C . This gives the
estimate of ξ(t) at t = 0. Next we show that the (p + 1)-th derivative of ξ(t) can be controlled by itself. This
gives a self-consistent differential equation of ξ(t). Combining with the initial estimate of ξ(t), it follows
18

p

′

that for time 0 6 t 6 m 2(p+1) /(ln m)C , it holds that ξ(t) . (ln m)C . Especially kvj (t)k∞ . (ln m)C . Then
the claim (2.4) in Theorem 2.3 follows.
Proposition A.2. Under Assumptions 2.1 and 2.2, there exists a deterministic family of operators K(r) :
X r 7→ R for 2 6 r 6 p + 1 (independent of m) and K(r) = 0 if r is odd, such that with high probability with
respect to the random initialization, it holds that
(r)

K0 −

K(r)
mr/2−1

.
∞

(ln m)C
.
m(r−1)/2

(A.2)

(r)

As we have shown in (A.1), the kernel Kt (xα1 , xα2 , · · · , xαr ) is a sum of terms in the form
1
mr/2−1

s
Y
hv2j−1 (t), v2j (t)i
,
m
j=1

1 6 s 6 r,

v1 (t), v2 (t), · · · , v2s (t) ∈ D0 ∪ D1 ∪ · · · ∪ Dr−2 .

(A.3)

(r)

To evaluate K0 (xα1 , xα2 , · · · , xαr ), we recall the following conditioning Lemma from [45]. With this lemma,
(r)
we can keep track of vectors appearing in the expression of K0 (xα1 , xα2 , · · · , xαr ), and their decomposition
into combinations of projections of independent Gaussian vectors.
Lemma A.3. Let W ∈ Rm×m be a matrix with random Gaussian entries Wij ∼ N (0, cw ). Consider fixed
matrices Q ∈ Rm×q , Y ∈ Rm×q , P ∈ Rm×p , X ∈ Rm×p . Then the distribution of W conditioned on Y = W Q
and X = W ⊤ P is
d

⊥
W |Y =W Q,X=W ⊤ P = E + Π⊥
P W̃ ΠQ ,

where W̃ is an independent copy of W ,
⊥
+
+ ⊤ ⊤
E = Y Q+ + (P + )⊤ X ⊤ Π⊥
Q = ΠP Y Q + (P ) X ,
+
⊥
Q+ , P + are Moore-Penrose pseudoinverse of Q, P respectively, and ΠQ = Im − Π⊥
Q = QQ , ΠP = Im − ΠP =
+
P P are the orthogonal projection on the space spanned by the columns of Q, P repsectively.

Proof of Proposition A.2. Without loss of generality, we simply take xα1 = x1 , xα2 = x2 , · · · , xαr = xr . We
(r)
decompose the expression of K0 (x1 , x2 , · · · , xr ) into sub-expressions. We denote
e0 = a0 ,

(ℓ) (ℓ−1)

er(ℓ−1)+i = W0 xi

,

1 6 i 6 r,

1 6 ℓ 6 H.

In the rest of the proof, we view ei as formal expressions, and we denote their values as val(ei ). For the
computation, to evaluate f (x1 , θ0 ), f (x2 , θ0 ), · · · , f (xr , θ0 ), we need to sequentially evaluate the expressions
e1 , e2 , · · · , erH . We will express the values of these expressions as combinations of Gaussian vectors in the
following way. By repeatedly using Lemma A.3, we have
(1)

d

(1)

d

val(e1 ) = W0 x1 = a1,1 g1 ,
val(e2 ) = W0 x2 = a2,2 g2 + a2,1 g1 ,
······

(1)

d

val(er ) = W0 xr = ar,r gr + ar,r−1 gr−1 + · · · + ar,1 g1 .
19

(A.4)

where g1 , g2 , · · · , gr are independent standard Gaussian vectors in Rm ; the coefficients ai,j can be computed
by performing the Gram-Schmidt algorithm over the input vectors x1 , x2 , · · · , xr , which depend only on the
inner products hxi , xj i and we call them A-variables. In general A-variables are random variables, however
in (A.4), they are deterministic. Thanks to the Assumption 2.2, the smallest singular value of the matrix
[x1 , x2 , · · · , xr ] is at least cr > 0, the leading coefficients |a1,1 |, |a2,2 |, · · · , |ar,r | ≍ 1. As a consequence, each
of the evaluations of ei for 1 6 i 6 r contains a new standard Gaussian vector.
For the output of the second layer, again using Lemma A.3, we have
(2)

W
d
val(er+1 ) = √0 σ(val(e1 )) = ar+1,r+1gr+1 ,
m
(2)

W
d
val(er+2 ) = √0 σ(val(e2 )) = ar+2,r+2gr+2 + ar+2,r+1 gr+1 ,
m
······
(2)

W
d
val(e2r ) = √0 σ(val(er )) = a2r,2r g2r + a2r,2r−1 g2r−1 + · · · + a2r,r+1 gr+1 .
m
where gr+1 , gr+2 , · · · , g2r are independent standard Gaussian vectors in Rm , which are also independent of
(1)
(1)
(1)
g1 , g2 , · · · , gr ; the coefficients ai,j are computed by performing the Gram-Schmidt algorithm over x1 , x2 , · · · , xr .
(1)
(1)
In this case, the coefficients ai,j are random, which depend on the inner products hxi , xj i. However, the
(1)

(1)

inner products hxi , xj i
(1)

(1)

hxi , xj i =

1
hσ(ai,i gi + ai,i−1 gi−1 + · · · + ai,1 g1 ), σ(aj,j gj + aj,j−1 gj−1 + · · · + aj,1 g1 )i,
m

are average of m independent identically distributed quantities, each is a function of Gaussian variables.
(1)
(1)
Therefore, hxi , xj i has a scaling limit as the width of the neural network m → ∞, and strongly concentrates around this limit. In other words, with high probability we have


(ln m)C
√
lim ai,j = ãi,j , ai,j = ãi,j + O
.
(A.5)
m→∞
m
We will see soon, in fact, by the same reasoning, all the A-variables appearing in this section satisfy the
(1)
(1)
(1)
relation (A.5). Moreover, in the limit m → ∞, The Gram matrix of x1 , x2 , · · · , xr is full rank. Otherwise
there exist constants λ1 , λ2 , · · · , λr such that
λ1 σ(ã1,1 G1 ) + λ1 σ(ã2,2 G2 + ã2,1 G1 ) + · · · + λr σ(ãr,r Gr + ãr,r−1 Gr−1 + · · · + ar,1 G1 ) = 0,

(A.6)

for independent Gaussian variables G1 , G2 , · · · , Gr ∼ N (0, 1). This is impossible, unless the expression
(A.6) is literally zero, i.e. λ1 , λ2 , · · · , λr = 0. Therefore, in the limit m → ∞, The Gram matrix of
(1)
(1)
(1)
x1 , x2 , · · · , xr is full rank. We conclude that |ãr+1,r+1 |, |ãr+2,r+2 |, · · · , |ã2r,2r | ≍ 1. Combining with
(A.5), with high probability, it holds |ar+1,r+1|, |ar+2,r+2 |, · · · , |a2r,2r | ≍ 1. Again, each of the evaluations of
ei for r + 1 6 i 6 2r contains a new standard Gaussian vector.
By repeating the above argument, we get that for any 1 6 i 6 rH,
d

val(ej ) = aj,j gj + aj,j−1 gj−1 · · · + aj,1 g1 ,
20

where g1 , g2 , · · · , grH are independent standard Gaussian vectors, the A-variables aj,j , aj,j−1 , · · · , aj,1 concentrate around their limits, i.e. with high probability (A.5) holds, and |aj,j | ≍ 1.
Formally as expressions, we have for 2 6 ℓ 6 H,




σ(er(ℓ−1) )
(ℓ) σ(er(ℓ−2)+1 ) σ(er(ℓ−2)+2 )
√
√
√
er(ℓ−1)+1 , er(ℓ−1)+2 , · · · , erℓ = W0
.
,
,··· ,
m
m
m

To use Lemma A.3 in the future, we denote for 2 6 ℓ 6 H,


(ℓ)
Y0 = val er(ℓ−1)+1 , er(ℓ−1)+2 , · · · , erℓ ,


σ(er(ℓ−1) )
σ(er(ℓ−2)+1 ) σ(er(ℓ−2)+2 )
(ℓ)
√
√
√
.
,
,··· ,
Q0 = val
m
m
m
(ℓ)

Then Y0

(ℓ)

(ℓ)

= W0 Q0 , for 2 6 ℓ 6 H.
(r)

(r)

To estimate K0 (x1 , x2 , · · · , xr ), we need to decompose the expression of K0 (x1 , x2 , · · · , xr ) into subex(r)
pressions erH+1 , erH+2 , erH+3 , · · · in the following way. Since each summand in K0 (x1 , x2 , · · · , xr ) is of
the form (3.8). For each of these vectors vj (0), uj (0), we evaluate it from right to left. Each time, when
(2)
(2)
(3)
(3)
(H)
(H)
we need to multiply one of these matrices W0 , (W0 )⊤ , W0 , (W0 )⊤ , · · · , W0 , (W0 )⊤ , we add a new
subexpression corresponding to the whole expression if it has not appeared before. For example we have the
(2)
following expression in K0 (·, ·):
(ℓ+1)

σℓ′ (x)

(W0
)⊤
′
√
· · · σH
(x)a0 .
m

(A.7)

We decompose it into subexpressions in the following way
(H)

erH+1 =

(W0 )⊤ ′
√
σH (x)a0 ,
m

erH+2 =

(W0
)⊤ ′
(W0 )⊤ ′
√
σH−1 (x) √
σH (x)a0 ,
m
m

(H−1)

······
e(r+1)H−ℓ =

(H)

(ℓ+1) ⊤

(W0
√

)

m

′
· · · σH
(x)a0 = (A.7).

√
(ℓ) √
(ℓ)
Then for each ej with j > rH + 1, either ej = (W0 / m)fj or ej = ((W0 )⊤ / m)fj for some 2 6 ℓ 6 H,
and fj is an expression in the following form
Mul(e0 , e1 , · · · , ej−1 ) = {entrywise products of e0 , {σ (s) (ei )}06s6r−1,16i6rH , {ei }rH+16i6j−1 }.
For 2 6 ℓ 6 H, we denote the sets
(ℓ) √
Sτ(ℓ) = {1 6 j 6 rH + τ : ej ends with multiplying W0 / m},
√
(ℓ)
Tτ(ℓ) = {1 6 j 6 rH + τ : ej ends with multiplying (W0 )⊤ / m}.

21

Formally as expressions, we have for 2 6 ℓ 6 H,

τ

[ej ]j∈T (ℓ)
τ




fj
√
,
m j∈Sτ(ℓ)


fj
(ℓ) ⊤
.
= (W0 ) √
m j∈Tτ(ℓ)
(ℓ)

[ej ]j∈S (ℓ) = W0

To use Lemma A.3 in the future, we denote for 2 6 ℓ 6 H,
Yτ(ℓ)

= val [ej ]j∈S (ℓ) ,
τ

Xτ(ℓ) = val [ej ]j∈T (ℓ) ,
τ


fj
,
= val √
m j∈Sτ(ℓ)


fj
Pτ(ℓ) = val √
.
m j∈Tτ(ℓ)


Q(ℓ)
τ

(A.8)

Then for 2 6 ℓ 6 H,
(ℓ)

Yτ(ℓ) = W0 Q(ℓ)
τ ,

(ℓ)

Xτ(ℓ) = (W0 )⊤ Pτ(ℓ) .

In the following we prove by induction that
Claim A.4. For τ > 1, the following holds.
(ℓ)

(ℓ)

(i) The limits as m → ∞ of the Gram matrices of columns Qτ , and columns of Pτ , as defined in (A.8),
are non-degenerate;
(ii) Let Mul(a0 , g1 , g2 , · · · , grH+τ −1 ) be the set of entrywise products of a0 , {σ (s) (ai,i gi + ai,i−1 gi−1 + · · · +
ai,1 g1 )}06s6r−1,16i6rH and {gi }rH+16i6rH+τ −1 and LinMul(a0 , g1 , g2 , · · · , grH+τ −1 ) be the set of linear combinations of Mul(a0 , g1 , g2 , · · · , grH+τ −1 ) with A-variables as coefficients. The evaluation of
erH+τ has the following form
val(erH+τ ) = arH+τ,rH+τ grH+τ + LinMul(a0 , g1 , g2 , · · · , grH+τ −1 ),
where g̃rH+τ ∼ N (0, Im ) is the standard Gaussian vector,
grH+τ = Π⊥
(ℓ) g̃rH+τ ,
Q
τ

(ℓ) √
if the expression erH+τ ends with multiplying W0 / m and

grH+τ = Π⊥
(ℓ) g̃rH+τ ,
P
τ

√
(ℓ)
if the expression erH+τ ends with multiplying (W0 )⊤ / m. Moreover, with high probability we have
lim arH+τ,rH+τ = ãrH+τ,rH+τ 6= 0,

m→∞

arH+τ,rH+τ = ãrH+τ,rH+τ + O
22



(ln m)C
√
m



.

Proof of Claim A.4. We assume that the statements of Claim A.4 hold up to τ and prove it for τ + 1
(ℓ) √
Without loss of generality, we assume that erH+τ +1 ends with multiplying W0 / m, then erH+τ +1 =
√
(ℓ)
(ℓ)
(ℓ)
(ℓ)
(W0 / m)frH+τ +1 , and frH+τ +1 ∈ Mul(e0 , e1 , · · · , erH+τ ). Moreover, Sτ +1 = Sτ ∪ {rH + τ + 1}, Tτ +1 =
(ℓ)
Tτ , and for 2 6 ℓ 6 H,




fj
(ℓ)
(ℓ)
(ℓ) val(fτ )
(ℓ)
,
= Qτ , √
Yτ +1 = val [ej ]j∈S (ℓ) = [Yτ , val(eτ +1 )], Qτ +1 = val √
τ +1
m j∈S (ℓ)
m
τ +1


(A.9)
fj
(ℓ)
(ℓ)
(ℓ)
(ℓ)
√
Xτ +1 = val [ej ]j∈T (ℓ) = Xτ , Pτ +1 = val
= Pτ .
τ +1
m j∈T (ℓ)
τ +1

(ℓ)

By our induction assumption, we have that the limits as m → ∞ of the Gram matrix of columns of Pτ +1
is non-degenerate. To prove (i) in Claim A.4, we only need to show that the limits as m → ∞ of the
(ℓ)
Gram matrix of columns of Qτ +1 is non-degenerate. We prove it by contradiction. We recall from (A.9)
√
(ℓ)
(ℓ)
Qτ +1 = val[fj / m]j∈S (ℓ) . If the limit of the Gram matrix of columns of Qτ +1 is degenerate, informally,
τ +1

there exists constants λj such that
lim

m→∞

X

λj val(fj ) + λrH+τ +1 val(frH+τ +1 ) = 0.

(A.10)

(ℓ)
j∈Sτ

We recall that fj is an expression of entrywise products of e0 , {σ (s) (ei )}06s6r−1,16i6rH and {ei }rH+16i6j−1 ,
and by our induction hypothesis ei = ai,i gi +LinMul(a0 , g1 , g2 , · · · , gi−1 ), with |ai,i | ≍ 1 with high probability.
Moreover, as m → ∞, the vectors a0 , g1 , g2 , · · · , grH+τ converge to independent standard Gaussian vectors.
(A.10) implies that as formal expressions
X
λj fj + λrH+τ +1 frH+τ +1 = 0.
(ℓ)

j∈Sτ

However, this indicates that frH+τ +1 = λfj with some j 6 rH + τ and contradicts with our construction
that erH+τ +1 has not appeared before. This finishes the proof of (i) in Claim A.4.
For the proof of (ii) in Claim A.4, thanks to Lemma A.3, we have

 val(f
d
rH+τ +1 )
+
(ℓ) + ⊤
(ℓ) ⊤ ⊥
⊥
√
val(erH+τ +1 ) = Yτ(ℓ) (Q(ℓ)
+ Π⊥
.
(ℓ) + W̃ Π (ℓ)
τ ) + ((Pτ ) ) (Xτ ) ΠQ(ℓ)
(Pτ )
Qτ
τ
m

(A.11)

Since frH+τ +1 ∈ Mul(e0 , e1 , · · · , erH+τ ) is an expression of entrywise products of e0 , {σ (s) (ei )}06s6r−1,16i6rH
and {ei }rH+16i6rH+τ ,, and by our induction assumption for 1 6 i 6 rH + τ ,
val(ei ) = ai,i gi + LinMul(a0 , g1 , g2 , · · · , gi−1 ),
we conclude that
val(frH+τ +1 ) ∈ LinMul(a0 , g1 , g2 , · · · , grH+τ ).
(ℓ)

(ℓ)

By our induction assumption, the columns of Qτ and Pτ as m → ∞ are of full rank. The first two terms
(ℓ)
(ℓ)
in (A.11) are linear combinations of columns of Yτ and columns of Pτ with A-variables as coefficients:

 val(f
rH+τ +1 )
+
(ℓ) + ⊤
(ℓ) ⊤ ⊥
√
Yτ(ℓ) (Q(ℓ)
∈ LinMul(a, g1 , g2 , · · · , grH+τ ).
(A.12)
τ ) + ((Pτ ) ) (Xτ ) ΠQ(ℓ)
τ
m
23

For the last term in (A.11), we can rewrite it as
⊥
Π⊥
W̃ Π⊥
g̃
,
(ℓ)
(ℓ) val(frH+τ +1 ) = arH+τ +1,rH+τ +1 Π
(ℓ)
(P )+
Q
(P )+ rH+τ +1
τ

τ

τ

(A.13)

where g̃rH+τ +1 is an independent Gaussian vector and
1
arH+τ +1,rH+τ +1 = √ kΠ⊥
(ℓ) val(frH+τ +1 )k2 .
m Qτ
By the same argument as before, the A-variable arH+τ +1,rH+τ +1 strongly concentrates around this limit.
With high probability we have


(ln m)C
√
lim arH+τ +1,rH+τ +1 = ãrH+τ +1,rH+τ +1 , arH+τ +1,rH+τ +1 = ãrH+τ +1,rH+τ +1 + O
.
m→∞
m
Moreover,
A.4 implies that as m → ∞, the limit of the Gram matrix of
√
√ as we just proven, (i) in Claim
{val(fj )/ m}j∈S (ℓ) ∪ {val(frH+τ +1 )/ m} is non-degenerate. We conclude that ãrH+τ +1rH+τ +1 6= 0, then
τ
with high probability |arH+τ +1,rH+τ +1 | ≍ 1. This finishes the proof of Claim A.4.
(r)

From the discussion above, the evaluation of any subexpression ei in K0 (x1 , x2 , · · · , xr ) is of the form
ei = ai,i gi + LinMul(a0 , g1 , a2 , · · · , gi−1 ).

(A.14)

Especially, the vectors vj (t) at time t = 0 in (A.1) are also of the form (A.14). Their inner products
concentrate around their limits as m → ∞,


hv2j−1 (0), v2j (0)i
(ln m)C
hv2j−1 (0), v2j (0)i
√
.
(A.15)
= lim
+O
m→∞
m
m
m
There exists a deterministic operator K(r) : X r 7→ R for 2 6 r 6 p + 1, it holds that with high probability
(r)

mr/2−1 K0 (x1 , x2 , · · · , xr ) − K(r) (x1 , x2 , · · · , xr )

∞

. (ln m)C .

By an union bound over all r-tuple of data points (xα1 , xα2 , · · · , xαr ), we conclude that with high probability
(r)

K0 −
(r)

If 2 ∤ r, then the degree of a0 in K0
finishes the proof of Proposition A.2.

K(r)
mr/2−1

.
∞

(ln m)C
.
m(r−1)/2
(r)

is odd, we have E[K0 ] = 0. It is necessary that K(r) = 0. This

Corollary A.5. Under Assumptions 2.1 and 2.2, for any expression v(t) ∈ Dr with 0 6 r 6 2p, the following
holds with high probability
kv(0)k∞ . (ln m)C .
24

Proof. By the same argument as in the proof of Proposition A.2, we can evaluate v(0) as combinations of
standard Gaussian vectors
v(0) ∈ LinMul(a0 , g1 , g2 , · · · ),
where the set LinMul is as defined in Claim A.4: LinMul(a0 , g1 , g2 , · · · ) is the set of linear combinations
of Mul(a0 , g1 , g2 , · · · ) with A-variables as coefficients; Mul(a0 , g1 , g2 , · · · ) is the set of entrywise products
of a0 , {σ (s) (ai,i gi + ai,i−1 gi−1 + · · · + ai,1 g1 )}06s6r+1,16i6rH and {gi }i>rH+1 . Since those vectors gi are
projections of independent Gaussian vectors, with high probability kgi k∞ . (ln m)C . So is any vector in
LinMul(a0 , g1 , g2 , · · · ).

B

A Priori Estimates

In this section, we prove the claim (2.4) in Theorem 2.3.
Proposition B.1 (A priori L2 bounds). Under Assumptions 2.1 and 2.2, for any time t > 0, we have
n
X

β=1

2

|fβ (t) − yβ | 6

n
X

β=1

|fβ (0) − yβ |2 = O(n),

and with high probability with respect to the random initialization, for t .

(B.1)

√
m

1
√ max{kWt(1) k2→2 , kWt(2) k2→2 , k(Wt(2) )⊤ k2→2 · · · , kWt(H) k2→2 , k(Wt(H) )⊤ k2→2 , kat k2 } . 1.
m

(B.2)

(2)

Proof of Proposition B.1. From the defining relation (1.7) of Kt (·, ·), it is non-negative definite. Using
(1.6), we get
∂t

n
X

β=1

kfβ (t) − yβ k2 = −

n
X

α,β=1

(2)

Kt (xα , xβ )(fα (t) − yα )(fβ (t) − yβ ) 6 0,

and (B.1) follows
n
X

β=1

kfβ (t) − yβ k2 6

n
X

β=1

kfβ (0) − yβ k2 = O(n),

To prove (B.2), we define
1
(1)
(2)
(2)
(H)
(H)
ξ(t) = √ max{kWt k2→2 , kWt k2→2 , k(Wt )⊤ k2→2 · · · , kWt k2→2 , k(Wt )⊤ k2→2 , kat k2 }.
m
(1)

(2)

(3)

(H)

We notice that for t = 0, W0 is an m×d random gaussian matrix, W0 , W0 , · · · , W0 are m×m random
gaussian matrices, and a0 is a gaussian vector of length m. From random matrix theory [43], we have that,
with high probability,
ξ(0) . 1.
25

(B.3)

In the following we derive an upper bound of ∂t ξ(t), which combining with (B.3) gives us the desired
bound (B.2). For any ℓ > 1, we have
v
um
uX
1
1
(ℓ)
(ℓ) (ℓ−1)
kx k2 = √ kσ(W x
)k2 6 √ t (|σ(0)| + c1 (W (ℓ) x(ℓ−1) )i )2
m
m i=1
c1
6 |σ(0)| + √ kW (ℓ) x(ℓ−1) k2 6 |σ(0)| + c1 ξ(t)kx(ℓ−1) k2 ,
m

where we used Assumption 2.1 that σ is c1 -Lipschitz, and Assumption 2.2 kxk2 . 1. Inductively, we have
the following estimate
ℓ−1
kx(ℓ) k2 6 cℓ1 ξ(t)ℓ kxk2 + |σ(0)|(1 + c1 ξ(t) + · · · + cℓ−1
) . cℓ1 ξ(t)ℓ .
1 ξ(t)

(B.4)

Using (1.4), (1.5) and (B.4), we have the following bounds:
n
(ℓ+1) ⊤
1X ′
at
(W
)
(ℓ−1)
′
kxβ
k2 |fβ (t) − yβ |
· · · σH
(xβ ) √
σℓ (xβ ) t√
n
m
m
β=1
2
v
u X
n
n
u
1X H
H
H
Ht 1
H
.
c1 ξ(t) |fβ (t) − yβ | . c1 ξ(t)
|fβ (t) − yβ |2 . cH
1 ξ(t) ,
n
n
(ℓ)

∂t kWt k2→2 6

β=1

and

∂t ka(t)k2 6

(B.5)

β=1

n
n
1 X (H)
1X H
kxβ k2 |fβ (t) − yβ | .
c1 ξ(t)H (1 + kxβ k2 )|fβ (t) − yβ |
n
n
β=1

β=1

v
u X
n
u
Ht 1
H
. cH
ξ(t)
|fβ (t) − yβ |2 . cH
1
1 ξ(t) ,
n

(B.6)

β=1

where we used the AM-GM inequality and (B.1). And similarly,
(ℓ)

H
∂t k(Wt )⊤ k2→2 . cH
1 ξ(t) .

(B.7)

The estimates (B.5), (B.6) and (B.7) together implies the upper bounds for ∂t ξ(t): there exists some
large constant C > 0
CcH
∂t ξ(t) 6 √ 1 ξ(t)H .
m
If H = 1, we have
H

ξ(t) 6 eCc1 t/

√
m

ξ(0),

and for H > 2, we get
√ −1/(H−1)
.
ξ(t) 6 ξ(0)H−1 − CcH
1 t/ m
√
In both cases, we have that ξ(t) . 1 provided that t . m, where the implicit constants depend on the
depth H. This finishes the proof of Proposition B.1.
26

(r)

As we have shown in Section 3 (3.8), the kernel Kt (xα1 , xα2 , · · · , xαr ) is a sum of terms in the
form
1
mr/2−1

s
Y
hv2j−1 (t), v2j (t)i
,
m
j=1

1 6 s 6 r,

vj (t) ∈ D0 ∪ D1 ∪ · · · ∪ Dr−2 .

(B.8)

In the following we derive an upper bound of k diag(ft )k2→2 = kft k∞ , where diag(· · · ) is the diagonalization
of a vector, for any ft ∈ D0 ∪ D1 ∪ · · · ∪ Dr .
Proposition B.2. We assume Assumptions 2.1 and 2.2. Fix time t > 0 and r > 0. Suppose that for all
expressions ft ∈ D0 ∪ D1 ∪ · · · ∪ Dr , the following holds
k diag(ft )k2→2 6 M,
for some constant M ≥ 1. Then for any ft ∈ Ds with 0 6 s 6 2r + 2, the following holds
√
k diag(ft )k2→2 6 kft k2 . M s m.

(B.9)

(B.10)

Proof of Proposition B.2. We notice that k diag(ft )k2→2 equals the L∞ norm of the vector ft . The L∞ norm
of ft is bounded by its L2 norm,
k diag(ft )k2→2 = kft k∞ 6 kft k2 ,
which gives
the first inequality of (B.10). Notice that the last inequality above is not optimal and it costs a
√
factor m generically.
For ft ∈ Ds with some s 6 2r + 2. We can write it as ft = ek ek−1 · · · e1 e0 where 0 6 k 6 4H − 3,
n
o
√ (1) √ (2)
√ (H)
e0 ∈ at , 1, { mxβ , mxβ , · · · , mxβ }16β6n ,

and for 1 6 j 6 k, ej belongs to one of the sets
((
)
)
(2)
(2)
(H)
(H)
Wt
(Wt )⊤
Wt
(Wt )⊤
′
√ , √
, {σ1′ (xβ ), σ2′ (xβ ), · · · , σH
(xβ )}16β6n ,
,··· , √ , √
m
m
m
m
{diag(d), d ∈ D0 ∪ D1 ∪ · · · ∪ Ds−1 } ,
n
(u+1)
σℓ
(xβ ) diag(d1 ) diag(d2 ) · · · diag(du ) : 1 6 ℓ 6 H,

o
1 6 β 6 n, 1 6 u 6 s − 1, d1 , d2 , · · · , du ∈ D0 ∪ D1 ∪ · · · ∪ Ds−1 .

(B.11)
(B.12)
(B.13)

Moreover, the total number of diag operations in the expression ft = ek ek−1 · · · e1 e0 is exactly s. We remark
(ℓ)
that xβ depends on time t.
In the following we prove by induction on s that
√
kft k2 = kek ek−1 · · · e1 e0 k2 . M s m,
which gives the claim (B.10).
27

(B.14)

For s = 0, ft does not contain diag operations, and all the ej belong to (B.11). In this case, thanks
to Assumption
2.1 and Proposition B.1, with high probability with respect to the random initialization,
√
ke0 k2 . m and kej k2→2 . 1 for all 1 6 j 6 k. Therefore, we have that
√
kft k2 = kek ek−1 · · · e1 e0 k2 6 kek k2→2 kek−1 k2→2 · · · ke1 k2→2 ke0 k2 . m.
For 1 6 s 6 r, by our assumption (B.9)
√
√
√
√
kft k2 = kek ek−1 · · · e1 e0 k2 6 mkek ek−1 · · · e1 e0 k∞ = mk diag(ft )k2→2 6 M m 6 M s m.
Thus the claim (B.14) holds for any s 6 r.
In the following we assume that (B.14) holds for 1, 2, · · · , s − 1 and prove it for s. For each 1 6 j 6 k,
we denote the number of diag operations in ej by sj . Then the total number of diag operations in ft is
s1 + s2 + · · · + sk = s 6 2r + 2. As an easy consequence, sj 6 s 6 2r + 2 for any 1 6 j 6 k. For each term
ej , there are several cases:
(i) ej belongs to (B.11), then it does not contain any diag operation, and we have sj = 0. In this case, we
have proven that kej k2→2 . 1.
(ii) ej = diag(d) belongs to (B.12) and sj 6 r+1. Since the expression d contains sj −1 6 r diag operations,
d also belongs to Dsj −1 ⊂ D0 ∪ D1 ∪ · · · ∪ Dr . By the assumption (B.9)
kej k2→2 = k diag(d)k2→2 6 M.
(iii) ej = diag(d) belongs to (B.12) and sj > r + 1. In this case we still have that sj − 1 6 s − 1. And by
our induction assumption (B.14) and d ∈ Dsj −1 , it holds
√
kdk2 . M sj −1 m.
(B.15)
(u+1)

(iv) ej = σℓ
(xβ ) diag(d1 ) diag(d2 ) · · · diag(du ) belongs to (B.13), and each of those subexpressions
diag(d1 ), diag(d2 ), · · · , diag(du ) contains at most r + 1 diag operations. In this case we have u 6 sj
and d1 , d2 , · · · , du ∈ D0 ∪ D1 ∪ · · · ∪ Dr . By the assumption (B.9) and Assumption 2.1
(u+1)

kej k2→2 = kσℓ

(xβ )k2→2 k diag(d1 )k2→2 k diag(d2 )k2→2 · · · k diag(du )k2→2

6 cu+1 M u . M sj

(u+1)

(v) ej = σℓ
(xβ ) diag(d1 ) diag(d2 ) · · · diag(du ) belongs to (B.13), and some of those subexpressions
diag(d1 ), diag(d2 ), · · · , diag(du ) contain more than r + 1 diag operations. In this case sj > r + 1. Since
the total number of diag operations in ej is sj 6 2r + 2, exact one of diag(d1 ), diag(d2 ), · · · , diag(du )
contains more than r + 1 diag operations. Say it is diag(dv ). For any 1 6 i 6= v 6 u, diag(di ) contains
at most r + 1 diag operations and di ∈ D0 ∪ D1 ∪ · · · ∪ Dr . By the assumption (B.9)
kdi k∞ = k diag(di )k2→2 6 M.

(B.16)

For dv , it contains at most sj − u 6 s − 1 diag operations. Thus by our induction assumption (B.14),
it holds
√
(B.17)
kdv k2 . M sj −u m.
28

By our assumption, the total number of diag operations in ft = ek ek−1 · · · e0 is s1 + s2 + · · · + sk = s 6
2r + 2. At most one of those sj is bigger than r + 1. Especially at most one of those ej belongs to cases (iii)
or (v).
√
If none of those ej belongs to cases (iii) or (v), then with the bound ke0 k2 . m, we have
kek ek−1 · · · e1 e0 k2 6 kek k2→2 kek−1 k2→2 · · · ke1 k2→2 ke0 k2 .

k
Y

√
√
M si m = M s m.

(B.18)

i=1

If for some 1 6 j 6 k, ej belongs to the case (iii), we write
kek ek−1 · · · e1 e0 k2 6 kek k2→2 kek−1 k2→2 · · · kej+1 k2→2 kej ej−1 · · · e1 e0 k2

6 kek k2→2 kek−1 k2→2 · · · kej+1 k2→2 kdk2 kej−1 ej−2 · · · e1 e0 k∞ .

(B.19)

The expression ej−1 ej−2 · · · e1 e0 contains at most s1 + s2 + · · · + sj−1 6 r diag operators, and j − 1 6
k − 1 < 4H − 3. Therefore, the expression ej−1 ej−2 · · · e1 e0 is in the set Ds1 +s2 +···+sj−1 , which is contained
in D0 ∪ D1 ∪ · · · ∪ Dr . Thus by our assumption (B.9),
kej−1 ej−2 · · · e1 e0 k∞ = k diag(ej−1 ej−2 · · · e1 e0 )k2→2 6 M.

(B.20)

We estimate kdk2 using (B.15), and estimate kek k2→2 kek−1 k2→2 · · · kej+1 k2→2 by (i), (ii) and (iv). By
plugging (B.15), (B.20) into (B.19), we get
kek k2→2 kek−1 k2→2 · · · kej+1 k2→2 kdk2 kej−1 ej−2 · · · e1 e0 k∞
.

k
Y

√
√
M si (M sj −1 m)M . M s m.

i=j+1

If for some 1 6 j 6 k, ej belongs to the case (iii), we write
kek ek−1 · · · e1 e0 k2 6 kek k2→2 kek−1 k2→2 · · · kej+1 k2→2 kej ej−1 · · · e1 e0 k2
(u+1)

= kek k2→2 kek−1 k2→2 · · · kej+1 k2→2 kσℓ

(xβ ) diag(d1 ) diag(d2 ) · · · diag(du )ej−1 · · · e1 e0 k2 .

(B.21)

For the last term in (B.21), we have
(u+1)

kσℓ

6
6

(xβ ) diag(d1 ) diag(d2 ) · · · diag(du )ej−1 · · · e1 e0 k2

(u+1)
kσℓ
(xβ ) diag(d1 ) · · · diag(dv−1 )k2→2 kdv k2 k diag(dv+1 ) · · · diag(du )ej−1 · · · e1 e0 k∞
(u+1)
kσℓ
(xβ )k2→2 kd1 k∞ · · · kdv−1 k∞ kdv k2 kdv+1 k∞ · · · kdu k∞ kej−1 · · · e1 e0 k∞ .

(B.22)

Plugging (B.16), (B.17) and (B.20) into (B.22), we get
(u+1)

kσℓ

.M

u−1

(xβ ) diag(d1 ) diag(d2 ) · · · diag(du )ej−1 · · · e1 e0 k2
√
√
kdv k2 kej−1 · · · e1 e0 k2 . M u−1 M sj −u mM = M sj m.
29

(B.23)

We estimate kek k2→2 kek−1 k2→2 · · · kej+1 k2→2 by (i), (ii) and (iv). By plugging (B.23) into (B.21), we get
kek k2→2 kek−1 k2→2 · · · kej+1 k2→2 kej ej−1 ej−2 · · · e1 e0 k∞
.

k
Y

√
√
M si (M sj m) . M s m.

i=j+1

This finishes the proof of √
(B.14), and hence Proposition B.2. Notice that the inequalities (B.15) and (B.17)
which contain the factor m were used only once in this proof.
Proposition B.3. We assume Assumptions 2.1 and 2.2. With high probability, uniformly for any vector
p
′
ft ∈ D0 ∪ D1 ∪ · · · ∪ Dp−1 , and time 0 6 t 6 m 2(p+1) /(ln m)C the following holds
kft k∞ . (ln m)C .
Proof of Proposition B.3. Thanks to Corollary A.5, with high probability, uniformly for all ft ∈ D0 ∪ D1 ∪
· · · ∪ Dp−1 , we have that
kf0 k∞ . (ln m)C .
We denote
ξ(t) = max{kft k∞ : ft ∈ D0 ∪ D1 ∪ · · · ∪ Dp−1 }.

(B.24)

In the following we derive a self-consistent differential equation of ξ(t). Proposition B.3 follows from analyzing
it.
For any ft (xα1 , xα2 , · · · , xαs ) ∈ D0 ∪ D1 ∪ · · · ∪ Dp−1 , by taking derivative we have
n
1 X 1 (1)
√ ft (xα1 , xα2 , · · · , xαs , xβ )(fβ (t) − yβ ).
∂t ft (xα1 , xα2 , · · · , xαs ) = −
n
m

(B.25)

β=1

(1)

where ft (xα1 , xα2 , · · · , xαs , xβ ) is obtained from ft (xα1 , xα2 , · · · , xαs ) by the replacements (3.1). We define,




u
Y
hv2j−1 (t), v2j (t)i 
LinProd(D0 ∪ D1 ∪ · · · ∪ Dp+r ) = linear combinations of v0 (t)
,


m
j=1

where 0 6 u 6 r + 1, vj (t) ∈ Dsj , s0 , s1 , · · · , s2u > 0 and s0 + s1 + · · · + s2u 6 p + r. Thanks to Proposition
(1)
A.1, ft ∈ LinProd(D0 ∪ D1 ∪ · · · ∪ Dp ). More generally, for any integer 1 6 r 6 p + 1,
(r)

∂t ft (xα1 , xα2 , · · · , xαs+r ) = −

n
1 X 1 (r+1)
√ ft
(xα1 , xα2 , · · · , xαs+r , xβ )(fβ (t) − yβ ),
n
m
β=1

30

(B.26)

(r+1)

where ft

∈ LinProd(D0 ∪ D1 ∪ · · · ∪ Dp+r ). Using the bound (B.1), we have
n
1 X 1 (r+1)
√ ft
(xα1 , xα2 , · · · , xαs+r , xβ )(fβ (t) − yβ )
n
m
β=1

n

∞

1
1X
(r+1)
6√
max kft
(xα1 , xα2 , · · · , xαs+r , xβ )k∞
|fβ (t) − yβ |
m 16β6n
n
β=1
v
u X
u1 n
1
(r+1)
6√
max kft
(xα1 , xα2 , · · · , xαs+r , xβ )k∞ t
(fβ (t) − yβ )2
m 16β6n
n
β=1

1
(r+1)
max kf
(xα1 , xα2 , · · · , xαs+r , xβ )k∞ .
.√
m 16β6n t
Therefore, (B.25) and (B.26) together give

1
(1)
k∂t ft (xα1 , xα2 , · · · , xαs )k∞ . √
max kf (xα1 , xα2 , · · · , xαs , xβ )k∞ ,
m 16β6n t
1
(r)
(r+1)
k∂t ft (xα1 , xα2 , · · · , xαs+r )k∞ . √
max kf
(xα1 , xα2 , · · · , xαs+r , xβ )k∞ ,
m 16β6n t

(B.27)
(B.28)

for any 1 6 r 6 p + 1. By taking higher derivatives on both sides of (B.27), and using (B.28) to bound the
righthand side, we have that
(p+1)

∂t
kft (xα1 , xα2 , · · · , xαs )k∞
1 (p)
(1)
. √ ∂t
max kft (xα1 , xα2 , · · · , xαs , xβ1 )k∞ . · · · · · ·
16β1 6n
m
1
(p+1)
max
kft
(xα1 , xα2 , · · · , xαs , xβ1 , xβ2 , · · · , xβp+1 )k∞ .
. (p+1)/2
16β1 ,β2 ,··· ,βp+1 6n
m
(p+1)

From the discussion above, ft

(B.29)

is a linear combination of terms in the form
v0 (t)

u
Y
hv2j−1 (t), v2j (t)i
,
m
j=1

(B.30)

where 0 6 u 6 p + 1, vj (t) ∈ Dsj , s0 , s1 , · · · , s2u > 0 and s0 + s1 + · · · + s2u 6 2p. We can use Proposition
B.2 for r = p − 1,
√
kv0 (t)k∞ . ξ(t)s0 m,
(B.31)
and for 1 6 j 6 2u
√
kvj (t)k2 . ξ(t)sj m.

(B.32)
(p+1)

The estimates (B.31) and (B.32) together give an upper bound for the L∞ norm of ft
(p+1)

kft

(xα1 , xα2 , · · · , xαs , xβ1 , xβ2 , · · · , xβp+1 )k∞ .

31

√

m

u
Y

j=0

,

√
ξ(t)sj . ξ(t)2p m.

(B.33)

We obtain a self-consistent differential equation of ξ(t) by taking maximum on both sides of (B.29) over
1 6 α1 , α2 , · · · , αs 6 n, and using (B.33)
(p+1)

∂t

ξ(t) .

ξ(t)2p
.
mp/2

(B.34)

To obtain an upper bound of ξ(t) using (B.34), we still need an upper bound for the initial data, i.e.
(r)
ξ(0) and {∂t ξ(0)}16r6p . Fortunately Corollary A.5 provides such estimates. In fact, Corollary A.5 implies
that with high probability ξ(0) . (ln m)C . For the derivatives of ξ(t) at t = 0, we use (B.29)
(r)

|∂t ξ(0)| .
1
. r/2
m

max

(r)

16α1 ,α2 ,··· ,αs 6n

max

∂t kft (xα1 , xα2 , · · · , xαs )k∞

max

16α1 ,α2 ,··· ,αs 6n 16β1 ,β2 ,··· ,βp+1 6n

(r)
kf0 (xα1 , xα2 , · · ·

t=0

, xαs , xβ1 , xβ2 , · · · , xβr )k∞ .

(r)

Again f0 is a linear combination of terms in the form (B.30) with vj (t) ∈ Dsj for some 0 6 u 6 r,
s0 , s1 , · · · , s2u > 0 and s0 +s1 +· · ·+s2u 6 p+r−1. Using Corollary A.5, for 0 6 j 6 2u, kvj (0)k∞ . (ln m)C .
We conclude that
(r)

|∂t ξ(0)| .

(ln m)(2r+1)C
,
mr/2

for any 1 6 r 6 p.
The ordinary differential equation (B.34) has an exact solution in the following form:
p

A0 m 2(2p−1)
˜ =
ξ(t)
p+1 ,

 2p−1
p
2p−1
C
2(p+1)
p+1
/(ln m)
A1 m
−t

˜ is an exact solution of (B.34),
where A0 , A1 are constants depending on p, which are chosen such that ξ(t)
C
˜
and ξ(0) = ξ(0). It is easy to check that ξ̃(0) ≍ (ln m) , and for 1 6 r 6 p + 1,
pr
(2p−1)r
(ln m)(2r+1)C
(r) ˜
(r)
∂t ξ(0)
≍ (ln m)(1+ p+1 )C m− 2(p+1) ≫
≍ ∂t ξ(0),
mr/2

˜ provides an upper bound for ξ(t). We conclude that for
provided that m is large enough. Therefore, ξ(t)
p

2p−1

t . m 2(p+1) /(ln m) p+1 C ,
it holds that
p

A0 m 2(2p−1)
C
˜ .
ξ(t) . ξ(t)
p+1 . (ln m) .

 2p−1
p
2p−1
C
A1 m 2(p+1) /(ln m) p+1

This finishes the proof of Proposition B.3.

32

Remark B.4. By the same argument as for (B.34), for any 0 6 r 6 p, we have
(r+1)

∂t

ξ(t) .

ξ(t)p+r
,
mr/2

r

′

which gives us that ξ(t) . (ln m)C for t 6 m 2(r+1) /(ln m)C . Therefore, for bigger r, we have the a prior
estimate ξ(t) . (ln m)C for longer time.
Proof of (2.4) in Theorem 2.3. From the discussion in Section 3 (3.8), we have that each summand in
(r)
Kt (xα1 , xα2 , · · · , xαr ) is of the form
1
mr/2−1

s
Y
hv2j−1 (t), v2j (t)i
,
m
j=1

1 6 s 6 r,

vj ∈ D0 ∪ D1 ∪ · · · ∪ Dr−2 .

(B.35)

If r 6 p+1, Proposition B.3 provides an upper bound on the L∞ norm of those vectors vj (t). So we can bound
p
′
these inner products hv2j−1 (t), v2j (t)i using Proposition B.3. If r 6 p + 1, then for 0 6 t 6 m 2(p+1) /(ln m)C ,
it holds that
kvj (t)k∞ . (ln m)C .
As a consequence, with high probability with respect to the random initialization,
1
mr/2−1

s
s
Y
Y
hv2j−1 (t), v2j (t)i
(ln m)2C m
1
(ln m)2sC
(ln m)2rC
. r/2−1
=
6
.
r/2−1
m
m
m
m
mr/2−1
j=1
j=1

(r)

Since Kt (xα1 , xα2 , · · · , xαr ) is a linear combination of terms in the form (B.35), the claim (2.4) follows.

C

Proof of Corollary 2.4 and 2.5, and Theorem 2.6
(3)

Proof of Corollary 2.4. We first derive an upper bound of the kernel Kt (·, ·, ·), using its derivative
(3)

∂t Kt (xα1 , xα2 , xα3 ) = −
p

n
1 X (4)
Kt (xα1 , xα2 , xα3 , xβ )(fβ (t) − yβ ).
n

(C.1)

β=1

′

Thanks to (2.4), for 0 6 t 6 m 2(p+1) /(ln m)C , it holds that
(4)

kKt k∞ .

(ln m)C
.
m

(C.2)

(C.2) combining with (B.1) implies an upper bound of the righthand side of (C.1),
(3)

(4)

∂t Kt (xα1 , xα2 , xα3 ) 6 max |Kt (xα1 , xα2 , xα3 , xβ )|
16β6n

33

n
(ln m)C
1X
.
|fβ (t) − yβ | .
n
m
β=1

(C.3)

(3)

(2.3) gives an upper bound of K0 (xα1 , xα2 , xα3 ) . (ln m)C /m, and (C.3) gives an upper bound of the
(3)
derivative of Kt (xα1 , xα2 , xα3 ). They together implies that with high probability
(3)

|Kt (xα1 , xα2 , xα3 )| .

(1 + t)(ln m)C
,
m

(C.4)

for any 1 6 α1 , α2 , α3 6 n. We recall that
(2)

∂t Kt (xα1 , xα2 ) = −

n
1 X (3)
Kt (xα1 , xα2 , xβ )(fβ (t) − yβ ).
n

(C.5)

β=1

Similarly as in (C.3), we can use (C.4) to upper bound the righthand side of (C.5),
(2)

(3)

∂t Kt (xα1 , xα2 ) 6 max |Kt (xα1 , xα2 , xβ )|
16β6n

n
1X
(1 + t)(ln m)C
.
|fβ (t) − yβ | .
n
m
β=1

This finishes the proof of Corollary (2.4).
Proof of Corollary 2.5. Corollary 2.4 gives the change rate for each entry of the NTK up to time t 6
p
′
m 2(p+1) /(ln m)C ,
(2)

k∂t Kt k∞ .

(1 + t)(ln m)C
.
m

(C.6)

By integrating both sides of (C.6) from 0 to t, we get an L∞ bound of the change of the NTK,
(2)

kKt

(2)

− K0 k∞ .

t(1 + t)(ln m)C
.
m

(C.7)

The L∞ bound in (C.7) can be used to derive a norm bound of the change of the NTK,
(2)

kKt

(2)

(2)

− K0 k2→2 6 kKt

(2)

(2)

− K0 kF 6 nkKt

(2)

− K0 k∞ .

t(1 + t)(ln m)C n
.
m

The change
of the smallest eigenvalue of the NTK is upper bounded by the change of its norm. If
p
(2)
(2)
the change of the norm kKt −K0 k2→2 6 λ/2.
0 6 t 6 c λm/n/(ln m)C/2 , with some c > 0 small enough,
p
Combining with (2.5), we conclude that for 0 6 t 6 c λm/n/(ln m)C/2
h
i
h
i
(2)
(2)
(2)
(2)
− kKt − K0 k2→2 > λ/2.
(C.8)
> λmin K0 (xα , xβ )
λmin Kt (xα , xβ )
16α,β6n

16α,β6n

From the defining relation (1.6) of the NTK and using (C.8), we have
∂t

n
X

β=1

kfβ (t) − yβ k2 = −

n
1 X
(2)
Kt (xα , xβ )(fα (t) − yα )(fβ (t) − yβ )
n
α,β=1
n
X

λ
6−
2n

β=1

2

kfβ (t) − yβ k ,

34

(C.9)

for 0 6 t 6 c

p
λm/n/(ln m)C/2 . Especially, (C.9) implies an exponential decay of the training error,
n
X

β=1

n
X

λt

kfβ (t) − yβ k2 6 e− 2n

β=1

λt

kfβ (0) − yβ k2 . ne− 2n ,

(C.10)

p
for 0 6 t 6 c λm/n/(ln m)C/2 . It takes time t ≍ (2n/λ) ln(n/ε), for the training error in (C.10) to reach ε.
Therefore if
p
(C.11)
c λm/n/(ln m)C/2 & (2n/λ) ln(n/ε),

the dynamic (2.1) finds a global minimum, the training error reaches ε at time t ≍ (n/λ) ln(n/ε). For (C.11)
to hold, the neural network needs to be wide
m > C′

 n 3
λ

(ln m)C ln(n/ε)2 ,

with some large constant C′ . This finishes the proof of Corollary 2.5.
Proof of Theorem 2.6. We have proven in (B.1) that
n
X

(fβ (t) − yβ )2 = O(n).

(C.12)

β=1

We recall the a priori estimate (2.4) that with high probability with respect to the random initialization, for
p
′
0 6 t 6 m 2(p+1) /(ln m)C , it holds that
(r)

kKt k∞ .

(ln m)C
.
mr/2−1

(C.13)

We have better estimates if r is odd. In fact, thanks to the equations for the dynamic of the NTK (2.2),
(r)

(r+1)

∂t Kt (xα1 , xα2 , · · · , xαr ) 6 max |Kt
16β6n

(ln m)C
. (r−1)/2
m

s

(xα1 , xα2 , · · · , xαr , xβ )|

1X
|fβ (t) − yβ |
n
β

(ln m)C
1X
|fβ (t) − yβ |2 . (r−1)/2 .
n
m
β

(C.14)

Moreover, thanks to (2.3), if r is odd, K(r) = 0 and
(r)

kK0 k∞ .

(ln m)C
.
m(r−1)/2

(C.15)
p

The estimates (C.14) and (C.15) together imply that if r is odd, for t 6 m 2(p+1) /(ln m)C
(r)

kKt k∞ .

(1 + t)(ln m)C
,
m(r−1)/2

which is slightly better than the estimate (C.13).
35

′

(C.16)

We denote the vector
∆f (t) = (f1 (t) − f˜1 (t), f2 (t) − f˜2 (t), · · · , fn (t) − f˜n (t))⊤ ,
At t = 0, we√have ∆f (t) = 0. We denote time T the first time that k∆f (t)k2 >
k∆f (t)k2 > n}. Then for t 6 T , we have that
n
X

n
X

(fβ (t) − yβ )2 = O(n),

β=1

√
n, i.e. T = inf t>0 {t :

(f˜β (t) − yβ )2 = O(n).

β=1

Next we study the difference of the original dynamic and p
the truncated dynamic for t 6 T . We show
p
√
′
that k∆f (t)k2 is much smaller than n, when t 6 min{c λm/n/(ln m)C/2 , m 2(p+1) /(ln m)C , T }. As a
p
p
′
consequence T > min{c λm/n/(ln m)C/2 , m 2(p+1) /(ln m)C }.
Thanks to (2.4) and (B.1), we have that

(p)

(p)

∂t (Kt (xα1 , xα2 , · · · , xαp ) − K̃t (xα1 , xα2 , · · · , xαp ))
(p+1)

6 max |Kt
16β6n

p

(xα1 , xα2 , · · · , xαp , xβ )|

n
1X
1+t
|fβ (t) − y(t)| . p/2 .
n
m
β=1

′

Thus for t 6 m 2(p+1) /(ln m)C we have
(p)

kKt

(p)

− K̃t k∞ .

(1 + t)t
.
mp/2

By taking difference of (2.2) and (2.7), we have


(r)
(r)
∂t Kt (xα1 , xα2 , · · · , xαr ) − K̃t (xα1 , xα2 , · · · , xαr )
=−

n

1 X  (r+1)
(r+1)
(xα1 , xα2 , · · · , xαr , xβ )(f˜β (t) − yβ )
Kt
(xα1 , xα2 , · · · , xαr , xβ )(fβ (t) − yβ ) − K̃t
n

1
=−
n

β=1
n 
X
β=1

(r+1)

+Kt

(r+1)
Kt
(xα1 , xα2 , · · ·

, xαr , xβ ) −

(r+1)
K̃t
(xα1 , xα2 , · · ·


(xα1 , xα2 , · · · , xαr , xβ )(fβ (t) − f˜β (t)) .


, xαr , xβ ) (f˜β (t) − yβ )

(C.17)

We estimate the first term on the righthand side of (C.17) as
n

1 X  (r+1)
(r+1)
(xα1 , xα2 , · · · , xαr , xβ ) (f˜β (t) − yβ )
Kt
(xα1 , xα2 , · · · , xαr , xβ ) − K̃t
n
β=1

n
1X ˜
(r+1)
(r+1)
(r+1)
(r+1)
|fβ (t) − yβ | . kKt
− K̃t
k∞ ,
6 kKt
− K̃t
k∞
n
β=1

36

(C.18)

p

′

provided that t 6 min{m 2(p+1) /(ln m)C , T }. For the second term on the righthand side of (C.17), for
p
′
t 6 m 2(p+1) /(ln m)C , it holds
n
1 X (r+1)
Kt
(xα1 , xα2 , · · · , xαr , xβ )(fβ (t) − f˜β (t))
n
β=1
v
u X
n
X
u1 n
1
(r+1)
(r+1)
6 kKt
k∞
|fβ (t) − f˜β (t)| 6 kKt
k∞ t
|fβ (t) − f˜β (t)|2
n
n
β=1

(r+1)

= kKt

k∞

(C.19)

β=1

k∆f (t)k2
(ln m)C
√
. (r−1)/2
n
m



1+t
√
m

12|r

k∆f (t)k2
√
,
n

where we used (C.13) and (C.16). The estimates (C.18) and (C.19) together imply
(r)

(r)

∂t (Kt (xα1 , xα2 , · · · , xαr ) − K̃t (xα1 , xα2 , · · · , xαr ))
1

(ln m)C
1 + t 2|r k∆f (t)k2
(r+1)
(r+1)
√
√
.
. kKt
− K̃t
k∞ + (r−1)/2
m
n
m
We integrate both sides, and get
(r)
kKt

−

(r)
K̃t k∞

.

Z

0

p

t

kKs(r+1)

−

K̃s(r+1) k∞ ds

(ln m)C
+ (r−1)/2
m



1+t
√
m

12|r R t
0

k∆f (s)k2 ds
√
,
n

′

for t 6 min{m 2(p+1) /(ln m)C , T }. We notice that in our setting t is much smaller than
and (C.13) for r = p − 1 and recursively with r = (p − 2), · · · , 2, we have
(r)
kKt

−

(r)
K̃t k∞

(1 + t)tp+1−r
(ln m)C
√
.
+
mp/2
m(r−1)/2 n



1+t
√
m

12|r Z

0

√
(p)
m. Using K̃t = 0

t

k∆f (s)k2 ds.

And especially,
(2)
kKt

−

(2)
K̃t k∞

(1 + t)(ln m)C
(1 + t)tp−1
√
+
.
m n
mp/2

Z

0

t

k∆f (s)k2 ds.

(C.20)

By taking difference of (2.1) and (2.7) we have
∂t ∆fα (t) =

n
n
1 X (2)
1 X (2)
(2)
(K̃t (xα , xβ ) − Kt (xα , xβ ))(f˜β (t) − yβ ) −
Kt (xα , xβ )∆fβ (t).
n
n
β=1

(C.21)

β=1

We multiply the vector ∆f (t) on both sides of (C.21)
n

∂t k∆f (t)k22 =

X (2)
1
1
(2)
(2)
h∆f (t),
(K̃t (xα , xβ ) − Kt (xα , xβ ))(f˜β (t) − yβ )i − h∆f (t), Kt ∆f (t)i.
n
n
β=1

37

(C.22)

Pn

(2)
β=1 (K̃t (xα , xβ )

(2)

− Kt (xα , xβ ))(f˜β (t) − yβ ) in the above
Pn
(2)
(2)
expression is understood as a vector with the α-component being β=1 (K̃t (xα , xβ )− Kt (xα , xβ ))(f˜β (t)−
yβ ). For the first term on the righthand side of (C.22), we estimate it using (C.20),

Here we have abused the notation so that

n

X (2)
1
(2)
h∆f (t),
(K̃t (xα , xβ ) − Kt (xα , xβ ))(f˜β (t) − yβ )i
n
β=1

6

n
X

α=1

.
.





n
1X ˜
−
|fβ (t) − yβ |
n
β=1
X
Z
n
(1 + t)(ln m)C t
√
k∆f (s)k2 ds
+
|∆fα (t)|
m n
0
α=1

Z
√
(1 + t)(ln m)C t
√
nk∆f (t)k2 .
k∆f (s)k2 ds
+
m n
0

(2)
|∆fα (t)|kK̃t
p−1

(1 + t)t
mp/2

(1 + t)tp−1
mp/2

(2)
Kt k∞

(C.23)

(2)

For the second term on the righthand side of (C.22), we use
t (xα , xβ )]16α,β6n is positive
p the fact that [K
C/2
definite. In fact, in (C.8), we have proven that for 0 6 t 6 c λm/n/(ln m) ,
h
i
(2)
> λ/2.
λmin Kt (xα , xβ )
16α,β6n

Therefore,

λ
1
(2)
(C.24)
− h∆f (t), Kt ∆f (t)i 6 − k∆f (t)k22 .
n
2n
By plugging (C.23) and (C.24) into (C.22), and divide both sides by 2k∆f (t)k2 , we get


Z
√
(1 + t)tp−1
λ
(1 + t)(ln m)C t
√
∂t k∆f (t)k2 . n
k∆f (t)k,
(C.25)
k∆f
(s)k
ds
−
+
2
p/2
m n
2n
m
0
p
p
′
for t 6 min{c λm/n/(ln m)C/2 , m 2(p+1) /(ln m)C , T }. To analyze (C.25), we introduce a new quantity,
∆(t) = max k∆f (t)k2 .
06s6t

Then (C.25) implies




Z
√
λ
(1 + t)(ln m)C t
(1 + t)tp−1
√
∆(t)
∆(s)ds
−
+
∂t ∆(t) . max 0, n
m n
2n
mp/2
0
 


√
p−1
C
λ
(1 + t)t
n (1 + t)t(ln m)
. max 0,
∆(t) −
∆(t)
+
m
2n
mp/2




√
λ
(1 + t)tp−1 n
(1 + t)t(ln m)C
∆(t) ,
−
. max 0,
+
m
2n
mp/2

(C.26)

where we used that ∆(t) is monotonic increasing. We can further simplify the righthand side of (C.26), for
p
p
′
t 6 min{c λm/n/(ln m)C/2 , m 2(p+1) /(ln m)C , T }, where c > 0 is small enough,


√
(1 + t)tp−1 n
λ
∆(t)
.
(C.27)
∂t ∆(t) . max 0,
−
4n
mp/2
38

We recall that ∆(0) = 0. We can solve (C.27),
√
√
Z s
(1 + s)sp−1 n λs/4n
(1 + t)tp−1 n
∆(t) . e−λt/4n
e
ds
.
min{t, n/λ}.
mp/2
mp/2
0
p
p
′
It follows that for t 6 min{c λm/n/(ln m)C/2 , m 2(p+1) /(ln m)C , T }
√
(1 + t)tp−1 n
min{t, n/λ},
k∆f (t)k2 .
mp/2

(C.28)

and
(2)

kKt

(2)

− K̃t k∞ .

(1 + t)tp−1
mp/2



1+

n n o
(1 + t)t(ln m)C
.
min t,
m
λ

(C.29)

p
p
′
We notice that for t 6 min{c λm/n/(ln m)C/2 , m 2(p+1) /(ln m)C , T }, and p > 3, the righthand side of (C.28)
p
p
√
′
is much smaller than n. From the definition of T , it is necessary that T > min{c λm/n/(ln m)C/2 , m 2(p+1) /(ln m)C }.
p
p
′
Thus we can conclude that (C.28) and (C.29) hold for any t 6 min{c λm/n/(ln m)C , m 2(p+1) /(ln m)C }.
This finishes the proof of Theorem 2.6

39

