Convergence rates for optimised adaptive importance
samplers
Ömer Deniz Akyildiz⋆,† , Joaquín Míguez‡

arXiv:1903.12044v4 [stat.CO] 7 May 2020

⋆

The Alan Turing Institute, London, UK.
†
University of Warwick, Coventry, UK.
‡
Universidad Carlos III de Madrid, Leganes, Spain.
omer.akyildiz@warwick.ac.uk, joaquin.miguez@uc3m.es

May 8, 2020

Abstract
Adaptive importance samplers are adaptive Monte Carlo algorithms to estimate expectations with respect to some target distribution which adapt themselves to obtain better
estimators over a sequence
of iterations. Although it is straightforward to show that they
√
have the same O(1/ N ) convergence rate as standard importance samplers, where N is
the number of Monte Carlo samples, the behaviour of adaptive importance samplers over
the number of iterations has been left relatively unexplored. In this work, we investigate an
adaptation strategy based on convex optimisation which leads to a class of adaptive importance samplers termed optimised adaptive importance samplers (OAIS). These samplers rely
on the iterative minimisation of the χ2 -divergence between an exponential-family proposal
and the target. The analysed algorithms are closely related to the class of adaptive importance samplers which minimise the variance of the weight function. We first prove nonasymptotic error bounds for the mean squared errors (MSEs) of these algorithms, which
explicitly depend on the number of iterations and the number of samples together. The
non-asymptotic bounds derived in this paper imply that when the target belongs to the exponential
√ family, the L2 errors of the optimised samplers converge to the optimal rate of
O(1/ N ) and the rate of convergence in the number of iterations are explicitly provided.
When the target does not belong to the exponential family,
the rate of convergence is the
√
same but the asymptotic L2 error increases by a factor ρ⋆ > 1, where ρ⋆ −1 is the minimum
χ2 -divergence between the target and an exponential-family proposal.

1 Introduction
The class of adaptive importance sampling (AIS) methods is a key Monte Carlo methodology
for estimating integrals that cannot be obtained in closed form (Robert and Casella, 2004).
This problem arises in many settings, such as Bayesian signal processing and machine learning
(Bugallo et al., 2015, 2017) or optimal control, (Kappen and Ruiz, 2016) where the quantities
of interest are usually defined as intractable expectations. Adaptive importance samplers are
versions of classical importance samplers (IS) which iteratively improve the proposals to generate samples better suited to the estimation problem at hand. Its variants include, for example,
population Monte Carlo methods (Cappé et al., 2004) and adaptive mixture importance sampling (Cappé et al., 2008). Since there has been a surge of papers on the topic of AIS recently,
1

a comprehensive review is beyond the scope of this article; see e.g. Bugallo et al. (2017) for a
recent review.
Due to the popularity of the adaptive importance samplers, their theoretical performance
has also received attention in the
√ past few years. The same as conventional IS methods, AIS
schemes enjoy the classical O(1/ N) convergence rate of the L2 error, where N is the number
of Monte Carlo samples used in the approximations, see e.g. Robert and Casella (2004) and
Agapiou et al. (2017). However, since an adaptation is performed over the iterations and the
goal of this adaptation is to improve the proposal quality, an insightful convergence result would
provide a bound which explicitly depends on the number of iterations, t, (which sometimes
we refer to as time) and the number of samples, N . Although there are convergence results
of adaptive methods (see Douc et al. (2007) for a convergence theory for population Monte
Carlo based on minimizing Kullback-Leibler divergence), none of the available results yields an
explicit bound of the error in terms of the number of iterations and the number of particles at
the same time.
One difficulty of proving such a result for adaptive mixture samplers is that the adaptive mixtures form an interacting particle system and it is unclear what kind of adaptation they perform
or whether the adapted proposals actually get closer to the target for some metric. An alternative to adaptation using mixtures is the idea of minimizing a cost function in order to adapt the
proposal. This idea has been popular in the literature, in particular, minimizing the variance
of the weight function has received significant attention, see, e.g., Arouna (2004a,b), Kawai
(2008), Lapeyre and Lelong (2011), Ryu and Boyd (2014), Kawai (2017, 2018). Relevant to
us, in particular, is the work of Ryu and Boyd (2014), who have have proposed an algorithm
called Convex Adaptive Monte Carlo (Convex AdaMC). This scheme is based on minimizing the
variance of the IS estimator, which is a quantity related to the χ2 divergence between the target
and the proposal. Ryu and Boyd (2014) have shown that the variance of the IS estimator is a
convex function of the parameters of the proposal when the latter is chosen within the exponential family. Based on this observation, Ryu and Boyd (2014) have formulated Convex AdaMC,
which draws one sample at each iteration and construct the IS estimator, which requires access
to the normalised target. They proved a central limit theorem (CLT) for the resulting sampler.
The idea has been further extended for self-normalised importance samplers by Ryu (2016),
who considered minimising the α-divergence between the target and an exponential family.
Similarly, Ryu (2016) proved a CLT for the resulting sampler. Similar ideas were also considered by Kawai (2017, 2018), who also aimed at minimizing the variance expression. Similarly,
Kawai (2018) showed that the variance of the weight function is convex when the proposal
family is suitably chosen and provided general conditions for such proposals. Kawai (2018) has
also developed an adaptation technique based on the stochastic approximation, which is similar to the scheme we analyse in this paper. There have been other results also considering χ2
divergence and relating it to the necessary sample size of the IS methods, see, e.g., Sanz-Alonso
(2018). Following the approach of Chatterjee et al. (2018), Sanz-Alonso (2018) considers and
ties the necessary sample size to χ2 -divergence, in particular, shows that the necessary sample
size grows with χ2 -divergence, hence implying that minimizing it can lead to more efficient
importance sampling procedures.
In this work, we develop and analyse a family of adaptive importance samplers, coined optimised adaptive importance samplers (OAIS), which relies on a particular adaptation strategy
based on convex optimisation. We adapt the proposal with respect to a quantity (essentially the
χ2 -divergence between the target and the proposal) that also happens to be the constant in the
error bounds of the IS (see, e.g., (Agapiou et al., 2017)). Assuming that proposal distributions
belong to the exponential family, we recast the adaptation of the proposal as a convex optimisation problem and then develop a procedure which essentially optimises the L2 error bound of
2

the algorithm. By using results from convex optimisation, we obtain error rates depending on
the number of iterations, denoted as t, and the number of Monte Carlo samples, denoted as N ,
together. In this way, we explicitly display the trade-off between these two essential quantities.
To the best of our knowledge, none of the papers on the topic provides convergence rates depending explicitly on the number of iterations and the number of particles together, as we do
herein.
The paper is organised as follows. In Sec. 2, we introduce the problem definition, the IS
and the AIS algorithms. In Sec. 3, we introduce the OAIS algorithms. In Sec. 4, we provide the
theoretical results regarding optimised AIS and show its convergence using results from convex
optimisation. Finally, we make some concluding remarks in Sec. 5.

Notation
For L ∈ N, we use the shorthand [L] = {1, . . . , L}. We denote the state space as X and assume
X ⊆ Rdx , dx ≥ 1. The space of bounded real-valued functions and the set of probability
measures on space X are denoted as B(X) and P(X), respectively. Given ϕ ∈ RB(X) and π ∈
P(X), the expectation of ϕ with respect to (w.r.t.) π is written as (ϕ, π) = ϕ(x)π(dx) or
Eπ [ϕ(X)]. The variance of ϕ w.r.t. π is defined as varπ (ϕ) = (ϕ2 , π) − (ϕ, π)2 . If ϕ ∈ B(X), then
kϕk∞ = supx∈X |ϕ(x)| < ∞. The unnormalised density associated to π is denoted with Π(x).
We denote the proposal as qθ ∈ P(X), with an explicit dependence on the parameter θ ∈ Θ. The
parameter space is assumed to be a subset of dθ -dimensional Euclidean space, i.e., Θ ⊆ Rdθ .
Whenever necessary we denote both the probability measures, π and qθ , and their densities
with the same notation. To be specific, we assume that both π(dx) and qθ (dx) are absolutely
continuous with respect to the Lebesgue measure and we denote their associated densities
as π(x) and qθ (x). The use of either the measure or the density will be clear from both the
argument (sets or points, respectively) and the context.

2 Background
In this section, we review importance and adaptive importance samplers.

2.1 Importance sampling
Consider a target density π ∈ P(X) and a bounded function ϕ ∈ B(X). Often, the main interest
is to compute an integral of the form
Z
ϕ(x)π(x)dx.
(1)
(ϕ, π) =
X

While perfect Monte Carlo can be used to estimate this expectation when it is possible to sample
exactly from π(x), this is in general not tractable. Hereafter, we consider the cases when the
target can be evaluated exactly and up to a normalising constant, respectively.
Importance sampling (IS) uses a proposal distribution which is easy to sample and evaluate.
The method consists in weighting these samples, in order to correct the discrepancy between
the target and the proposal, and finally constructing an estimator of the integral. To be precise,
let qθ ∈ P(X) be the proposal which is parameterized by the vector θ ∈ Θ. The unnormalised
target density is denoted as Π : X → R+ . Therefore, we have
π(x) =

3

Π(x)
,
Zπ

where Zπ :=

R

X Π(x)dx

< ∞. Next, we define functions wθ , Wθ : X × Θ → R+ as
wθ (x) =

π(x)
qθ (x)

and Wθ (x) =

Π(x)
,
qθ (x)

respectively. For a chosen proposal qθ , the IS proceeds as follows. First, a set of independent and
identically distributed (iid) samples {x(i) }N
i=1 is generated from qθ . When π(x) can be evaluated,
one constructs the empirical approximation of the probability measure π, denoted πθN , as
πθN (dx)

N
1 X
wθ (x(i) )δx(i) (dx),
=
N
i=1

where δx′ (dx) denotes the Dirac delta measure that places unit probability mass at x = x′ . For
this case, the IS estimate of the integral in (1) can be given as
(ϕ, πθN ) =

N
1 X
wθ (x(i) )ϕ(x(i) ).
N

(2)

i=1

However, in most practical cases, the target density π(x) can only be evaluated up to an unknown normalizing proportionality constant (i.e., we can evaluate Π(x) but not Zπ ). In this
case, we construct the empirical measure πθN as
πθN (dx) =

N
X

(i)

wθ δx(i) (dx),

i=1

where
Wθ (x(i) )
(i)
wθ = PN
.
(j)
j=1 Wθ (x )

Finally this construction leads to the so called self-normalizing importance sampling (SNIS)
estimator
(ϕ, πθN ) =

N
X

(i)

wθ ϕ(x(i) ).

(3)

i=1

Although the IS estimator (2) is unbiased, the SNIS estimator (3) is in general biased. However,
the bias and the MSE vanish with a rate O(1/N ), therefore providing guarantees of convergence
as N → ∞. Crucially for us, the MSE of both estimators. For clarity, below we present an MSE
bound for the (more general) SNIS estimator (3) which is adapted from Agapiou et al. (2017).
Theorem 1. Assume that (Wθ2 , qθ ) < ∞. Then for any ϕ ∈ B(X), we have
E

h

2 i cϕ ρ(θ)
,
≤
(ϕ, π) − (ϕ, πθN )
N

where cϕ = 4kϕk2∞ and the function ρ : Θ → [ρ⋆ , ∞) is defined as

 2
π (X)
,
ρ(θ) = Eqθ 2
qθ (X)
where ρ⋆ := inf θ∈Θ ρ(θ) ≥ 1.
4

(4)

(5)

Proof. See Appendix A.1 for a self-contained proof. 
Remark 1. For the IS estimator (2), this bound can be improved so that cϕ = kϕk2∞ . However,
this improvement does not effect our results in this paper, hence we present a single bound of
the form in (4) for the estimators (2) and (3) for conciseness. 
Remark 2. As pointed out by Agapiou et al. (2017), the function ρ is essentially the χ2 divergence between π and qθ , i.e.,
ρ(θ) := χ2 (π||qθ ) + 1.
Note that ρ(θ) can also be expressed in terms of the variance of the weight function wθ , which
coincides with the χ2 -divergence, i.e.,
ρ(θ) = varqθ (wθ (X)) + 1.
Therefore, minimizing ρ(θ) is equivalent to minimizing χ2 -divergence and the variance of the
weight function wθ , i.e., varqθ (wθ (X)). 
Remark 3. Remark 2 implies that, when both π and qθ belong to the same parametric family
(i.e., there exists θ ∈ Θ such that π = qθ ), one readily obtains
ρ⋆ := inf ρ(θ) = 1. 
θ∈Θ

Remark 4. For the IS estimator (2), the bound in Theorem 1 can be modified so that it holds
for unbounded test functions ϕ as well; see, e.g. Ryu and Boyd (2014). Therefore, a similar
quantity to ρ(θ), which includes ϕ whilst still retaining convexity, can be optimised for this case.
Unfortunately, obtaining such a bound is not straightforward for the SNIS estimator (3) as
shown by Agapiou et al. (2017). In order to significantly simplify the presentation, we restrict
ourselves to the class of bounded test functions, i.e., we assume kϕk∞ < ∞. 
Finally, we present a bias result from Agapiou et al. (2017).
Theorem 2. Assume that (Wθ2 , qθ ) < ∞. Then for any ϕ ∈ B(X), we have


c̄ϕ ρ(θ)
E (ϕ, πθN ) − (ϕ, π) ≤
,
N

where c̄ϕ = 12kϕk2∞ and the function ρ : Θ → [ρ⋆ , ∞) is the same as in Theorem 1.
Proof. See Theorem 2.1 in Agapiou et al. (2017). 

2.2 Parametric adaptive importance samplers
Standard importance sampling may be inefficient in practice when the proposal is poorly calibrated with respect to the target. In particular, as implied by the error bound provided in
Theorem 1, the error made by the IS estimator can be high if the χ2 -divergence between the
target and the proposal is large. Therefore, it is more common to employ an iterative version
of importance sampling, also called as adaptive importance sampling (AIS). The AIS algorithms
are importance sampling methods which aim at iteratively improving the proposal distributions.
More specifically, the AIS methods specify a sequence of proposals (qt )t≥1 and perform importance sampling at each iteration. The aim is to improve the proposal so that the samples are
5

Algorithm 1 Parametric AIS
1: Choose a parametric proposal qθ with initial parameter θ = θ0 .
2: for t ≥ 1 do
3:
Adapt the proposal,
θt = Tt (θt−1 ),
4:

Sample,
(i)

xt ∼ q θ t ,
5:

for i = 1, . . . , N,

Compute weights,
(i)

6:

Wθt (xt )
(i)
wθt = P
,
(i)
N
i=1 Wθt (xt )

(i)

W θt =

where

(i)

Π(xt )
.
qθt (x(i) )

Report the point-mass probability measure
πθNt (dx) =

N
X

(i)

wθt δx(i) (dx),
t

i=1

and the estimator
(ϕ, πθNt )

=

N
X

(i)

(i)

wθt ϕ(xt ).

i=1

7:

end for

better matched with the target, which results in less variance and more accuracy in the estimators. There are several variants, the most popular one being population Monte Carlo methods
(Cappé et al., 2004) which uses previous samples in the proposal.
In this section, we review one particular AIS, which we refer to as parametric AIS. In this
variant, the proposal distribution is a parametric distribution, denoted qθ . Over time, this parameter θ is updated (or optimised) with respect to a predefined criterion resulting in a sequence
(θt )t≥1 . This yields a sequence of proposal distributions denoted as (qθt )t≥1 .
One iteration of the algorithm goes as follows. Assume at time t − 1 we are given a proposal
distribution qθt−1 . At time t, we first update the parameter of this proposal,
θt = Tt (θt−1 ),
where {Tt : Θ → Θ, t ≥ 1}, is a sequence of (deterministic or stochastic) maps, e.g., gradient
mappings, constructed so that they minimise a certain cost function. Then, in the same way we
have done in conventional IS, we sample
(i)

xt ∼ qθt (dx),

for i = 1, . . . , N,

compute weights
(i)

Wθt (xt )
(i)
,
wθt = P
(i)
N
)
(x
W
θ
t
t
i=1
6

and finally construct the empirical measure
πθNt (dx) =

N
X

(i)

wθt δx(i) (dx).
t

i=1

The estimator of the integral (1) is then computed as in Eq. (3).
The full procedure of the parametric AIS method is summarized in Algorithm 1. Since this
is a valid IS scheme, this algorithm enjoys the same guarantee provided in Theorem 1. In
particular, we have the following theorem.
Theorem 3. Assume that, given a sequence of proposals (qθt )t≥1 ∈ P(X), we have (Wθ2t , qθt ) < ∞
for every t. Then for any ϕ ∈ B(X), we have
i c ρ(θ )
h
2
ϕ
t
,
≤
E (ϕ, π) − (ϕ, πθNt )
N
where cϕ = 4kϕk2∞ and the function ρ(θt ) : Θ → [ρ⋆ , ∞) is defined as in Eq. (5).

Proof. The proof is identical to the proof of Theorem 1. We have just re-stated the result to
introduce the iteration index t. 
However, this theorem does not give an insight of what happens as the number of iterations
increases, i.e., when t → ∞, with the bound. Ideally, the adaptation of the AIS should improve
this bound with time. In other words, in the ideal case, the error should decrease as t grows.
Fortunately, Theorem 3 suggests that the maps Tt : Θ → Θ can be chosen so that the function
ρ is minimised over time. More specifically, the sequence (θt )t≥1 can be chosen so that it leads
to a decreasing sequence (at least in expectation) (ρ(θt ))t≥1 . In the following sections, we will
summarize the deterministic and stochastic strategies to achieve this aim.
Remark 5. We define the unnormalised version of ρ(θ) and denote it as R(θ). It is characterised
as follows
Z
R(θ)
Π(x)dx < ∞.
where Zπ =
ρ(θ) =
Zπ2
X
Hence, R(θ) can also be expressed as
R(θ) = Eqθ




Π2 (X)
.
qθ2 (X)

(6)



2.3 AIS with exponential family proposals
Following Ryu and Boyd (2014), we note that when qθ is chosen as an exponential family density, the function ρ(θ) is convex. In particular, we define
qθ (x) = exp(θ ⊤ T (x) − A(θ))h(x),

(7)

where A : Rdθ → R ∪ {∞} is the log of the normalization constant, i.e.,
Z
A(θ) = log exp(θ ⊤ T (x))h(x)dx,
while T : Rdx → Rdθ and h : Rdx → R+ . Then we have the following lemma adapted from
Ryu and Boyd (2014).
7

Lemma 1. Let qθ be chosen as in (7). Then ρ : Θ → [ρ⋆ , ∞) is convex, i.e., for any θ1 , θ2 ∈ Θ and
λ ∈ [0, 1], the following inequality holds
ρ(λθ1 + (1 − λ)θ2 ) ≤ λρ(θ1 ) + (1 − λ)ρ(θ2 ).
Proof. See Appendix A.2 for a self-contained proof. 
Lemma 1 shows that ρ is a convex function, therefore, optimising it could give us provably
convergent algorithms (as t increases). Next lemma, borrowed from Ryu and Boyd (2014),
shows that ρ is differentiable and its gradient can indeed be computed as an expectation.
Lemma 2. The gradient ∇ρ(θ) can be written as


π 2 (X)
∇ρ(θ) = Eqθ (∇A(θ) − T (X)) 2
.
qθ (X)

(8)

Proof. The proof is straightforward since qθ is from an exponential family and A(θ) is differentiable. 
Remark 6. Note that Eqs. (6) and (8) together imply that


Π2 (X)
.
∇R(θ) = Eqθ (∇A(θ) − T (X)) 2
qθ (X)

(9)

We also note (see Remark 5) that
∇R(θ) = Zπ2 ∇ρ(θ).

(10)


In the following sections, we assume that ρ(θ) is a convex function. Thus Lemma 1 constitutes an important motivation for our approach. We leave general proposals which lead to
nonconvex ρ(θ) for future work.

3 Algorithms
In this section, we describe adaptation strategies based on minimizing ρ(θ). In particular, we
design maps Tt : Θ → Θ, for t ≥ 1, for scenarios where
(i) the gradient of ρ(θ) can be exactly computed,
(ii) an unbiased estimate of the gradient of ρ(θ) can be obtained, and
(iii) an unbiased estimate of the gradient of R(θ) can be obtained.
Scenario (i) is unrealistic in practice but gives us a guideline in order to further develop the idea.
In particular, the error bounds for the more complicated cases follow the same structure as this
case. Therefore, the results obtained in case (i) provide a good qualitative understanding of the
results introduced later. Scenario (ii) can be realized in cases where it is possible to evaluate
π(x), in which case the IS leads to unbiased estimators. Scenario (iii) is what a practitioner
would most often encounter: the target can only be evaluated up to the normalizing constant,
i.e., Π(x) can be evaluated but π(x) cannot.
We finally remark that, for the cases where we assume a stochastic gradient can be obtained
for ρ and R (namely, the case (ii) and the case (iii) respectively), we consider two possible
8

algorithms to perform adaptation. The first method is a vanilla SGD algorithm (Bottou et al.,
2016) and the second method is a SGD scheme with iterate averaging (Schmidt et al., 2017).
While vanilla SGD is easier to implement and algorithmically related to population-based Monte
Carlo methods, iterate averaged SGD results in a better theoretical bound and it has some
desirable variance reduction properties.

3.1 Exact gradient OAIS
We first introduce the OAIS scheme where we assume that the exact gradients of ρ(θ) are
available. Since ρ is defined as an expectation (an integral), this assumption is unrealistic.
However, the results we can prove for this procedure shed light onto the results that will be
proved for practical scenarios in the following sections.
In particular, in this scheme, given θt−1 , we specify Tt as
θt = Tt (θt−1 ) = ProjΘ (θt−1 − γ∇ρ(θt−1 )),

(11)

where γ > 0 is the step-size parameter of the map and ProjΘ denotes projection onto the
compact parameter space Θ. This is a classical gradient descent scheme on ρ(θ). In Section 4.1,
we provide non-asymptotic results for this scheme. However, as we have noted, this idea does
not lead to a practical scheme and cannot be used in most cases in practice as the gradients of
ρ in exact form are rarely available.
Remark 7. We use a projection operator in Eq. (11) because we assume throughout the analysis
in Section 4 that the parameter space Θ is compact. 

3.2 Stochastic gradient OAIS
Although it has a nice and simple form, exact-gradient OAIS is often intractable as, in most
practical cases, the gradient can only be estimated. In this section, we first look at the case
where π(x) can be evaluated, which means that an unbiased estimate of ∇ρ(θ) can be obtained. Then we consider the general case, where one can only evaluate Π(x) and can obtain
an unbiased estimate of ∇R(θ).
In the following subsections, we consider an algorithm where the gradient is estimated using
samples which can also be used to construct importance sampling estimators. The procedure
is outlined in Algorithm 3 for the case in which only Π(x) can be evaluated and ∇R(θ) is
estimated.
3.2.1 Normalised case
If we assume that the density π(x) can be evaluated exactly, then the algorithm can be
described as follows. Given (θk )1≤k≤t−1 , at iteration t we compute the next parameter iterate
as
θt = ProjΘ (θt−1 − γt gt ),
where gt is an unbiased estimator of ∇ρ(θt−1 ). We note that, due to the analytical form of ∇ρ
n
oN
(i)
(i)
(see Eq. (8)), the samples and weights generated at iteration t − 1, i.e., xt−1 , wθt−1 (xt−1 )
i=1

can be reused to estimate the gradient. This makes an algorithmic connection to the population
Monte Carlo methods where previous samples and weights are used to adapt the proposal
(Cappé et al., 2004).
9

Algorithm 2 Stochastic gradient OAIS
1: Choose a parametric proposal qθ with initial parameter θ = θ0 .
2: for t ≥ 1 do
3:
Update the proposal parameter,
θt = ProjΘ (θt−1 − γt g̃t )
(i)

where g̃t is computed by approximating the expectation in Eq. (9) using the samples xt−1
(i)

(i)

(i)

and weights wθt−1 = Π(xt−1 )qθt−1 (xt−1 )−1 , i = 1, ..., N .
4:
Sample,
(i)

xt ∼ q θ t ,

for i = 1, . . . , N,

Compute weights,

5:

(i)

Report,

6:

Wθt (xt )
(i)
.
wθt = P
(i)
N
)
(x
W
θ
t
t
i=1
πθNt (dx) =

N
X

(i)

wθt δx(i) (dx),
t

i=1

and
(ϕ, πθNt )

=

N
X

(i)

(i)

wθt ϕ(xt ).

i=1

7:

end for

Given the updated parameter θt , the algorithm first samples from the updated proposal
(i)
xt ∼ qθt , i = 1, . . . , N , and then proceeds to construct the IS estimator as in (2). Namely,
(ϕ, πθNt ) =

N
1 X
(i)
(i)
wθt (xt )ϕ(xt ).
N

(12)

i=1

3.2.2

Self-normalised case

For the general case, where we can only evaluate Π(x), the algorithm proceeds similarly.
Given (θk )1≤k≤t−1 , the method proceeds by first updating the parameter
θt = ProjΘ (θt−1 − γt g̃t ),
where g̃t is an unbiased estimator of ∇R(θt−1 ). Given the updated parameter, we first sample
(i)
xt ∼ qθt , i = 1, ..., N , and then construct the SNIS estimate as in (3), i.e.,
(ϕ, πθNt ) =

N
X
i=1

10

(i)

(i)

wθt ϕ(xt ).

where
Wθ (x(i) )
(i)
,
wθt = PN t
(j)
j=1 Wθt (x )

3.3 Stochastic gradient OAIS with averaged iterates
Next, we describe a variant of the stochastic gradient OAIS that uses averages of the iterates
generated by the SGD scheme (Schmidt et al., 2017) in order to compute the proposal densities,
generate samples and compute weights. In Section 4 we show that the convergence rate for this
method is better than the rate that can be guaranteed for Algorithm 2.
3.3.1

Normalised case

We assume first that the density π(x) can be evaluated. At the beginning of the t-th iteration,
the algorithm has generated the sequence (θk )1≤k≤t−1 . First, in order to perform the adaptive
importance sampling steps, we set
t−1

θ̄t =

1X
θk
t

(13)

k=0

(i)

and sample x̄t ∼ qθ̄t for i = 1, . . . , N . Following the standard parametric AIS procedure
(Algorithm 1), we obtain the estimate of (ϕ, π) as,
(ϕ, πθ̄Nt ) =

N
1 X
(i)
(i)
wθ̄t (x̄t )ϕ(x̄t ).
N
i=1

Next, we update the parameter vector using the projected stochastic gradient step
θt = Tt (θt−1 ) = ProjΘ (θt−1 − γt gt ),

(14)

where gt is an unbiased estimate of ∇ρ(θt−1 ), i.e., E[gt ] = ∇ρ(θt−1 ) and ProjΘ denotes projec(i)
tion onto the set Θ. Note that in order to estimate this gradient using (8), we sample xt ∼ qθt−1
for i = 1, . . . , N , and estimate the expectation in (8). It is worth noting that the samples
(i) N
(i)
{xt }M
i=1 are different from the samples {x̄t }i=1 used to estimate (ϕ, π).
3.3.2

Self-normalised case

In general, π(x) cannot be evaluated exactly, hence a stochastic unbiased estimate of ∇ρ(θ)
cannot be obtained. When the target can only be evaluated up to a normalisation constant, i.e.,
only Π(x) can be computed, we can use the SNIS procedure as explained in Section 2. Therefore, we introduce here the most general version of the stochastic method, coined stochastic
gradient OAIS, which uses the averaged iterates in (13) to construct the proposal functions. The
scheme is outlined in Algorithm 3.
To run this algorithm, given the parameter vector θ̄t in (13), we first generate a set of
(i)
samples {x̄t }N
i=1 from the proposal qθ̄t . Then the integral estimate given by the SNIS can be
written as,
(ϕ, πθ̄Nt ) =

N
X
i=1

11

(i)

(i)

wθ̄ ϕ(x̄t ),
t

Algorithm 3 Stochastic gradient OAIS with averaged iterates
1: Choose a parametric proposal qθ with initial parameter θ = θ0 .
2: for t ≥ 1 do
3:
Compute the average parameter vector
t−1

θ̄t =

1X
θk
t
k=0

Sample,

4:

(i)

x̄t ∼ qθ̄t ,

for i = 1, . . . , N,

Compute weights,

5:

(i)

W (x̄ )
(i)
wθ̄ = P θ̄t t (i) .
N
t
i=1 Wθ̄t (x̄t )

Report the point-mass probability measure

6:

πθ̄Nt (dx) =

N
X

(i)

wθ̄ δx̄(i) (dx),
t

t

i=1

and the estimator
(ϕ, πθ̄Nt )

=

N
X

(i)

(i)

wθ̄ ϕ(x̄t ).
t

i=1

7:

Update the parameter vector,
θt = ProjΘ (θt−1 − γt g̃t )

where g̃t is an estimate of ∇R(θt−1 ) computed by approximating the expectation in Eq. (9)
(i)
using a set of iid samples xt ∼ qθt−1 , i = 1, ..., N .
8: end for
where
W (x̄(i) )
(i)
wθ̄ = PN θ̄t
.
t
(j)
j=1 Wθ̄t (x̄ )

Finally, for the adaptation step, we obtain the unbiased estimate of the gradient ∇R(θ) and
adapt the parameter as
θt = ProjΘ (θt−1 − γt g̃t )

(15)

where g̃t is an unbiased estimate of ∇R(θt−1 ), i.e., E[g̃t ] = ∇R(θt−1 ). Note that, as in the
normalised case, this gradient is estimated by approximating the expectation in (9) using iid
(i)
(i)
samples xt ∼ qθt−1 , i = 1, . . . , N . These samples are different, again, from the set {x̄t }N
i=1
employed to estimate (ϕ, π).
12

(i)

Remark 8. In Algorithm 3 the samples {x̄t }N
i=1 drawn from the proposal distribution qθ̄t−1 (dx)
are not used to compute the gradient estimator g̃t which, in turn, is needed to generate the next
iterate θt . Therefore, if we can afford to generate T iterates, θ0 , . . . , θT −1 , with T known before
hand, and we are only interested in the estimator (ϕ, πθ̄N ) obtained at the last iteration (once
T
the proposal function has been optimized) then it is be possible to skip steps 3–6 in Algorithm
3 up to time T − 1. Only at time t = T , we would compute the average parameter vector θ̄T ,
(i)
sample x̄T from the proposal qθ̄T (dx) and generate the point-mass probability measure πθ̄N and
T
the estimator (ϕ, πθ̄N ) .
T

4 Analysis
Theorem 1 yields an intuitive result about the performance of IS methods in terms of the divergence between the target π and the proposal qθ . We now apply ideas from convex optimisation
theory in order to minimize ρ(θ) and obtain finite-time, finite-sample convergence rates for the
AIS procedures outlined in Section 3.

4.1 Convergence rate with exact gradients
Let us first assume that we can compute the gradient of ρ(θ) exactly. In particular, we consider
the update rule in Eq. (11). For the sake of the analysis, we impose some regularity assumptions
on the ρ(θ).
Assumption 1. Let ρ(θ) be a convex function with Lipschitz derivatives in the compact space Θ. To
be specific, ρ is convex and differentiable, and there exists a constant L < ∞ such that
k∇ρ(θ) − ∇ρ(θ ′ )k2 ≤ Lkθ − θ ′ k2
for any θ, θ ′ ∈ Θ.
Remark 9. Assumption 1 holds when the density qθ (x) belongs to an exponential family (see
Section 2.3) and Θ is compact (Ryu and Boyd, 2014), even if it may not hold in general for
θ ∈ Rdθ . 
Lemma 3. If Assumption 1 holds and we set a step-size γ ≤ 1/L, then the inequality
ρ(θt ) − ρ⋆ ≤

kθ0 − θ ⋆ k2
,
2γt

(16)

is satisfied for the sequence {θt }t≥0 generated by the recursion (11) where θ ⋆ is a minimum of ρ.
Proof. See, e.g., Nesterov (2013). 
This rate in (16) is one of the most fundamental results in convex optimisation. Lemma 3
enables us to prove the following result for the MSE of the AIS estimator adapted using exact
gradient descent in Eq. (12).
Theorem 4. Let Assumption 1 hold and construct the sequence (θt )t≥1 using recursion (11), where
(qθt )t≥1 is the sequence of proposal distributions. Then, the inequality
E

h

2 i cϕ kθ0 − θ ⋆ k22 cϕ ρ⋆
≤
(ϕ, π) − (ϕ, πθNt )
+
2γtN
N

(17)

is satisfied, where cϕ = 4kϕk2∞ , 0 < γ ≤ 1/L and L is the Lipschitz constant of the gradient ∇ρ(θ)
in Assumption 1.
13

Proof. See Appendix A.3. 
Remark 10. Theorem 4 sheds light onto several facts. We first note that ρ⋆ in the error bound
(17) can be interpreted as an indicator of the quality of the parametric proposal. We recall
that ρ⋆ = 1 when both π and qθ belong to the same exponential family. For this special case,
Theorem 4 implies that
lim (ϕ, π) −

t→∞

(ϕ, πθNt ) 2

≤O



1
√
N



.

In other words, when the target and the proposal are both from the exponential family, this
adaptation strategy is leading to an asymptotically optimal Monte Carlo estimator (optimal
meaning that we attain the same rate as a Monte Carlo estimator with N iid samples from
π). On the other hand, when π and qθ do not belong to the same family, we obtain
lim (ϕ, π) −

t→∞

(ϕ, πθNt ) 2

≤O

r

ρ⋆
N

!

,

i.e., the L2 rate is again asymptotically optimal, but the constant in the error bound is worse
√
(bigger) by a factor ρ⋆ > 1. 
This bound shows that as t → ∞, what we are left with is essentially the minimum attainable
IS error for a given parametric family {qθ }θ∈Θ . Intuitively, when the proposal qθ is from a
different parametric family than π, the gradient OAIS optimises the error bound in order to
obtain the best possible proposal. In particular, the MSE has two components: First an O(1/tN )
component which can be made to vanish over time by improving the proposal and a second
O(1/N ) component which is related to ρ⋆ . The quantity ρ⋆ is related to the minimum χ2 divergence between the target and proposal. This means that the discrepancy between the
target and optimal proposal (according to the χ2 -divergence) can only be tackled by increasing
N . This intuition is the same for the schemes we analyse in the next sections, although the rate
with respect to the number of iterations necessarily worsens because of the uncertainty in the
gradient estimators.
Remark 11. When γ = 1/L, Theorem 4 implies that if t = O(L/ρ⋆ ) and N = O(ρ⋆ /ε), for some
ε > 0, then we have
h
2 i
E (ϕ, π) − (ϕ, πθNt )
≤ O(ε).

We remark that once we choose the number of samples N = O(ρ⋆ /ε), the number of iterations
t for adaptation is independent of N and ε. 

Remark 12. One can use different maps Tt for optimisation. For example, one can use Nesterov’s accelerated gradient descent (which has more parameters than just a step size), in which
case, one could prove (by a similar argument) the inequality (Nesterov, 2013)
E

h

(ϕ, π) −

2
(ϕ, πθNt )

i

≤O



ρ⋆
1
+
t2 N
N



.

This is an improved convergence rate, going from O(1/t) to O(1/t2 ) in the first term of the
bound. 
14

4.2 Convergence rate with averaged SGD iterates
While, for the purpose of analysis, it is convenient to assume that the minimization of ρ(θ)
can be done deterministically, this is rarely the case in practice. The ‘best’ realistic case is that
we can obtain an unbiased estimator of the gradient. Hence, we address this scenario, under
the assumption that the actual gradient functions ∇ρ and ∇R are bounded in Θ (i.e., ρ(θ) is
Lipschitz in Θ).
Assumption 2. The gradient functions ∇ρ(θ) and ∇R(θ) are bounded in Θ. To be specific, there
exist finite constants Gρ and GR such that
sup k∇ρ(θ)k2 < Gρ < ∞

and

θ∈Θ

sup k∇R(θ)k2 < GR < ∞.
θ∈Θ

We note that this is a mild assumption in the case of interest in this paper, where Θ ⊂ Rdθ is
assumed to be compact.
4.2.1

Normalised target

First, we assume that we can evaluate π(x), which means that at iteration t, we can obtain an
unbiased estimate of ∇ρ(θt−1 ), denoted gt . We use the optimisation algorithms called stochastic
gradient methods, which use stochastic and unbiased estimates of the gradients to optimise a
given cost function (Robbins and Monro, 1951). Particularly, we focus on optimised samplers
using stochastic gradient descent (SGD) as an adaptation strategy.
We start proving that the stochastic gradient estimates (gt )t≥0 have a finite mean-squared
error (MSE) w.r.t. the true gradients. To prove this result, we need an additional regularity
condition.
Assumption 3. The normalised target and proposal densities satisfy the inequality
#
"
2
π 2 (X) ∂ log qθ
=: Dπj < ∞.
(X)
sup Eqθ
2 (X)
∂θ
q
j
θ∈Θ
θ
for j = 1, . . . , dθ . We denote Dπ := maxj∈{1,...,dθ } Dπj < ∞.
Remark 13. Let us rewrite Dπj in Assumption 3 in terms of the weight function, namely
"
#
2
∂
log
q
θ
(X)
Dπj = sup Eqθ wθ2 (X)
.
∂θj
θ∈Θ
When qθ (x) belongs to the exponential family, we readily obtain
"
2 #

∂A(θ)
j
4
,
− Ti (X)
Dπ = sup Eqθ wθ (X)
∂θi
θ∈Θ
where Ti (X) is the i-th sufficient statistic for qθ (x). Let us construct a bounding function for the
weights of the form
K(θ) := sup wθ (x).
x∈X

15

If we choose the compact space Θ in such a way that K(θ) is bounded, then we readily have
"
2 #
∂A(θ)
j
4
− Ti (X)
Dπ ≤ sup K (θ)Eqθ
∂θi
θ∈Θ
≤ kKk4∞ Var(Ti (X)),
m

where we have used the fact that ∂ ∂θA(θ)
= Eqθ [Tim (X)]. Therefore, if the weights remain
i
bounded in Θ, a sufficient condition for Assumption 3 to hold is that the sufficient statistics of
the proposal distribution all have finite variances, i.e., maxi∈{1,...,dθ } Ti (X) < ∞.
There are alternative conditions that, when satisfied, lead to Assumption 3 holding true. As
an example, in Appendix A.4 we provide an alternative sufficient condition in terms of the
function ρ2 (θ) := E[wθ4 (X)].
Now we show that when gt is an iid Monte Carlo estimator of ∇ρ, we have the following
finite-sample bound for the MSE.
Lemma 4. If Assumption 3 holds, the following inequality holds,
dθ cρ Dπ
,
N
where dθ is the parameter dimension and cρ , Dπ < ∞ are constant w.r.t. N .
E[kgt − ∇ρ(θt−1 )k22 ] ≤

Proof. See Appendix A.5. 

In order to obtain convergence rates for the estimator (ϕ, πθ̄Nt ) we first recall a classical result
for the SGD (see, e.g., Bubeck et al. (2015)).
Lemma 5. Let Assumptions 2 and 3 hold, apply recursion (14) and let (g√
t )t≥0 be the stochastic
gradient estimates in Lemma 4. If we choose the step-size sequence γk = α/ k, 1 ≤ k ≤ t, for any
α > 0, then
E[ρ(θ̄t ) − ρ⋆ ] ≤
where θ̄t =

1
t

αG2ρ
Ekθ0 − θ ⋆ k22 αdθ cρ Dπ
√
+ √
+ √ ,
2α t
tN
t

(18)

Pt−1

k=0 θk .

Proof. See Appendix A.6 for a self-contained proof. 
We can now state the first core result of the paper, which is the convergence rate for the AIS
algorithm using a SGD adaptation of the parameter vectors θt .
Theorem 5. Let Assumptions 2 and 3 hold,
P let the sequence (θt )t≥1 be computed using (14)
and construct the averaged iterates θ̄t = 1t t−1
k=0 θk . Then, the sequence of proposal distributions
(qθ̄t )t≥1 satisfies the inequality

2 
c2
c3
c4
c1
N
+√ 2+√
+
(19)
≤√
E (ϕ, π) − (ϕ, πθ̄t )
N
tN
tN
tN

for t ≥ 1 and any ϕ ∈ B(X), where

cϕ Ekθ0 − θ ⋆ k22
,
2α
c2 = cϕ cρ αdθ Dπ ,

c1 =

c3 = cϕ αG2ρ ,
c4 = cϕ ρ⋆ ,
and cϕ = 4kϕk2∞ are finite constants independent of t and N .
16

Proof. See Appendix A.7. 
Remark 14. Note that the expectation on the left hand side of (19) is taken w.r.t. the distribution of the measure-valued random variable πθ̄Nt . 
similarly
to Theorem 4. One can see that the overall rate
Theorem 5 can be interpreted
√

of the MSE bound is O 1/ tN + 1/N . This means that, as t → ∞, we are only left with a
rate that is optimal for the AIS for a given parametric proposal family. In particular, again, ρ⋆
is related to the minimal χ2 -divergence between the target π and the parametric proposal qθ .
⋆
When the proposal and the target are from the same family, we
√ are back to the case ρ = 1,
thus the adaptation leads to the optimal Monte Carlo rate O(1/ N ) for the L2 error within this
setting as well.
4.2.2

Self-normalised estimators

We have noted that it is possible to obtain an unbiased estimate of ∇ρ(θ) when the normalised
target π(x) can be evaluated. However, if we can only evaluate the unnormalised density
Π(x) instead of π(x) and use the self-normalized IS estimator, the estimate of ∇ρ(θ) is no
longer unbiased. We refer to Sec. 5 of Tadić and Doucet (2017) for stochastic optimisation
with biased gradients for adaptive Monte Carlo, where the discussion revolves around minimizing the Kullback-Leibler divergence rather than the χ2 -divergence. The results presented
in Tadić and Doucet (2017), however, are asymptotic, while herein we are interested in finitetime bounds. Due to the structure of the AIS scheme, it is possible to avoid working with biased
gradient estimators. In particular, we can implement the proposed AIS schemes using unbiased
estimators of ∇R(θ) instead of biased estimators of ∇ρ(θ). Since optimizing the unnormalised
function R(θ) leads to the same minima as optimizing the normalised function ρ(θ), we can
simply use ∇R(θ) for the adaptation in the self-normalised case.
Similar to the argument in Section 4.2.1, we first start the assumption below, which is the
obvious counterpart of Assumption 3.
Assumption 4. The unnormalized target Π(x) and the proposal densities qθ (x) satisfy the inequalities
#
"
2
Π2 (X) ∂ log qθ
j
=: DΠ
<∞
(X)
sup Eqθ
qθ2 (X) ∂θj
θ∈Θ
for j = 1, . . . , dθ . We denote DΠ :=

1
dθ

j
j=1 DΠ

Pdθ

< ∞.

Remark 13 holds directly for Assumption 4 as long as Zπ < ∞. Next, we prove an MSE
bound for the stochastic gradients (g̃t )t≥0 employed in recursion (15), i.e., the unbiased stochastic estimates of ∇R(θ).
Lemma 6. If Assumptions 2 and 4 hold, the inequality
E[kg̃t − ∇R(θt−1 )k22 ] ≤

dθ cR DΠ
,
N

is satisfied, where cR , DΠ < ∞ are constants w.r.t. of N .
Proof. The proof is identical to the proof of Lemma 4. 
We can now obtain explicit rates for the convergence of the unnormalized function R(θ̄t ),
evaluated at the averaged iterates θ̄t .
17

Lemma 7. If Assumptions
√ 2 and 4 hold and the sequence (θt )t≥1 is computed via recursion (15),
with step-sizes γk = β/ k for 1 ≤ k ≤ t and β > 0, we obtain the inequality
E[R(θ̄t ) − R⋆ ] ≤

Ekθ0 − θ ⋆ k22 βdθ cR DΠ βG2R
√
+ √
+ √
2β t
tN
t

(20)

where cR , DΠ < ∞ are constants w.r.t. t and N . Relationship 20 implies that
E[ρ(θ̄t ) − ρ⋆ ] ≤

βG2R
Ekθ0 − θ ⋆ k22 βdθ cR DΠ
√
√
√ .
+
+
2βZπ2 t
Zπ2 tN
Zπ2 t

(21)

Proof. The proof of the rate in (20) is identical to the proof of Lemma 5. The rate in (21)
follows by observing that ρ(θ) = R(θ)/Zπ2 for every θ ∈ Θ. 
Finally, using Lemma 7, we can state our main result: an explicit error rate for the MSE of
Algorithm 3 as a function of the number of iterations t and the number of samples N .
Theorem 6. Let Assumptions√2 and 4 hold and let the sequence (θt )t≥1 be computed via recursion
(15), with step-sizes γk = β/ k for 1 ≤ k ≤ t and β > 0. We have the following inequality for the
sequence of proposal distributions (qθ̄t )t≥1 ,

2 
C2
C3
C4
C1
N
+√ 2+√
+
,
(22)
E (ϕ, π) − (ϕ, πθ̄t )
≤√
N
tN
tN
tN
where
cϕ Ekθ0 − θ ⋆ k22
,
2βZπ2
cϕ βcR dθ DΠ
,
C2 =
Zπ2
cϕ βG2R
C3 =
,
Zπ2
C4 = cϕ ρ⋆ ,
C1 =

and cϕ = 4kϕk2∞ are finite constants independent of t and N .
Proof. The proof follows from Lemma 7 and mimicking the exact same steps as in the proof of
Theorem 5. 
Remark 15. Theorem 6, as in Remark 10, provides relevant insights regarding the performance
of the stochastic gradient OAIS algorithm. In particular, for a general target π, we obtain
r !
ρ⋆
.
lim (ϕ, π) − (ϕ, πθ̄Nt ) = O
t→∞
N
2
This result shows that the L2 error is asymptotically optimal. As in previous√cases, if the target
π is in the exponential family, then the asymptotic convergence rate is O(1/ N ) as t → ∞. 
Remark 16. Theorem 6 also yields a practical heuristic to tune the step-size and the number of
particles together. Assume that 0 < β < 1 and let N = 1/β (which we assume to be an integer
without loss of generality). In this case, the rate (22) simplifies into

2  c Ekθ − θ ⋆ k2 c β 3 c d D
cϕ β 2 G2R
ϕ
R θ Π
ϕ
0
2
N
√
√
√ + cϕ ρ⋆ β
+
+
E (ϕ, π) − (ϕ, πθ̄t )
≤
2Zπ2 t
Zπ2 t
Zπ2 t
18

Now, if we let t = O(1/β 2 ) we readily obtain

2 
N
≤ O(β).
E (ϕ, π) − (ϕ, πθ̄t )
Therefore, one can control the error using the step-size of the optimisation scheme provided
that other parameters of the algorithm are chosen accordingly. The same argument also holds
for Theorem 5. 
Remark 17. It is not straightforward to compare the rates in inequality (22) (for the unnormalized target Π(x)) and inequality (19) (for the normalized target π(x)). Even if (22) may “look
better” by a constant factor compared to the rate in (19), this is usually not the case. Indeed,
the variance of the errors in the unnormalised gradient estimators is typically higher and this
reflects on the variance of the moment estimators. Another way to look at this issue is to realise
that, very often, Zπ << 1, which makes the bound in (22) much greater than the bound in
(19).
Finally, we can adapt Theorem 2 to our case, providing a convergence rate of the bias of the
importance sampler given by Algorithm 3.
Theorem 7. Under the setting of Theorem 6, we have
h
i
3C2
3C3
3C4
3C1
+√ 2+√
+
E (ϕ, πθ̄Nt ) − (ϕ, π) ≤ √
,
N
tN
tN
tN

(23)

where C1 , C2 , C3 , C4 are finite constants given in Theorem 6 and independent of t and N .
Proof. The proof follows from Theorem 2 and mimicking the same proof technique used to
prove Theorem 6. 

4.3 Convergence rate with vanilla SGD
The arguments of Section 4.2 can be carried over to the analysis of Algorithm 2, where the
proposal functions qθt (x) are constructed
√ using the iterates θt rather than the averages θ̄t . Unfortunately, achieving the optimal O(1/ t) rate for the vanilla SGD is difficult in general. The
best available rate without significant restrictions on the step-size is given by Shamir and Zhang
(2013). In particular, we can adapt Shamir and Zhang (2013, Theorem 2) to our setting in order to state the following lemma.
Lemma √
8. Apply recursion (15) for the computation of the iterates (θt )t≥1 , choose the step-sizes
γk = β/ k for 1 ≤ k ≤ t, where β > 0, and let Assumptions 2 and 4 hold. Then, we have the
inequality

 2
βdθ cR DΠ βG2R
D
√ + √
+ √
(2 + log t),
(24)
E[R(θt ) − R⋆ ] ≤
β t
tN
t
where D := supθ,θ′ ∈Θ kθ − θ ′ k < ∞. This in turn implies that
E[ρ(θt ) − ρ⋆ ] ≤



D2
βdθ cR DΠ βG2R
√ + √
+ √
β t
tN
t



(2 + log t)
.
Zπ2

(25)

Proof. It is straightforward to prove this result using Shamir and Zhang (2013, Theorem 2) and
the proof of Lemma 5. 
19


. This is known to be suboptimal and it can be
√
improved to the information-theoretical optimal O(1/ t) rate by choosing a specific step-size
scheduling, see, e.g., Jain et al. (2019). However, in this case, the scheduling of (γt )t≥1 depends
√
directly on the total number of iterates to be generated, in such a way that the error O(1/ t) is
guaranteed only for the last iterate, at the final time t.
We can extend Lemma 8 to obtain the following result.
The obtained rate is, in general, O



log
√t
t

Theorem
√8. Apply recursion (15) for the computation of the iterates (θt )t≥1 , choose the step-sizes
γk = β/ k for 1 ≤ k ≤ t, where β > 0, and let Assumptions 2 and 4 hold. If we construct the
sequence of proposal distributions (qθt )t≥1 be the sequence of proposal distributions we obtain the
following MSE bounds


h
 i
C1
C2
C3
C4
N 2
+√ 2+√
≤ √
E (ϕ, π) − (ϕ, πθt )
(2 + log t) +
,
(26)
N
tN
tN
tN
where

cϕ D 2
,
2βZπ2
cϕ βcR dθ DΠ
C2 =
,
Zπ2
cϕ βG2R
,
C3 =
Zπ2
C4 = cϕ ρ⋆ ,
C1 =

and cϕ = 4kϕk2∞ are finite constants independent of t and N .
Proof. The proof follows from Lemma 8 with the exact same steps as in the proof of Theorem 5.

Finally, it is also straightforward to adapt the bias result in Theorem 7 to this case, which
results in the similar bound. We skip it for space reasons and also because it has the same form
as in Theorem 7 with an extra log t factor.

5 Conclusions
We have presented and analysed optimised parametric adaptive importance samplers and provided non-asymptotic convergence bounds for the MSE of these samplers. Our results display
the precise interplay between the number of iterations t and the number of samples N . In particular, we have shown that the optimised
samplers converge to an optimal proposal as t → ∞,
p
leading to an asymptotic rate of O( ρ⋆ /N ). This intuitively shows that the number of samples
N should be set in proportion to the minimum χ2 -divergence between the target and the exponential family proposal, as we have shown that the adaptation (in the sense of minimising
χ2 -divergence or,
p equivalently, the variance of the weight function) cannot improve the error
rate beyond O( ρ⋆ /N ). The error rates in this regime may be dominated by how close the
target is to the exponential family.
Note that the algorithms we have analysed require constant computational load at each
iteration and the computational load does not increase with t as we do not re-use the samples
in past iterations. Such schemes, however, can also be considered and analysed in the same
20

manner. More specifically, in the present setup the computational cost of each iteration depends
on the cost of evaluating Π(x).
Our work opens up several other paths for research. One direction is to analyse the methods
with more advanced optimisation algorithms. Another challenging direction is to consider more
general proposals than the natural exponential family, which may lead to non-convex optimisation problems of adaptation. Analysing and providing guarantees for this general case would
provide foundational insights for general adaptive importance sampling procedures. Also, as
shown by Ryu (2016), similar theorems can also be proved for α-divergences.
Another related piece of work arises from variational inference (Wainwright and Jordan,
2008). In particular, Dieng et al. (2017) have recently considered performing variational inference by minimising the χ2 -divergence, which is close to the setting in this paper. In particular,
the variational approximation of the target distribution in the variational setting coincides with
the proposal distribution we consider within the importance sampling context in this paper. This
also implies that our results may be used to obtain finite-time guarantees for the expectations
estimated using the variational approximations of target distributions.
Finally, the adaptation procedure can be modified to handle the non-convex case as well. In
particular, the SGD step can be converted into a stochastic gradient Langevin dynamics (SGLD)
step. The SGLD method can be used as a global optimiser when ρ and R are non-convex
and a global convergence rate can be obtained using the standard SGLD results, see, e.g.,
Raginsky et al. (2017), Zhang et al. (2019). Global convergence results for other adaptation
schemes such as stochastic gradient Hamiltonian Monte Carlo (SGHMC) can also be achieved
using results from nonconvex optimisation literature, see, e.g., Akyildiz and Sabanis (2020).

Acknowledgements
Ö. D. A. is funded by the Lloyds Register Foundation programme on Data Centric Engineering
through the London Air Quality project. This work was supported by The Alan Turing Institute
for Data Science and AI under EPSRC grant EP/N510129/1. J. M. acknowledges the support
of the Spanish Agencia Estatal de Investigación (awards TEC2015-69868-C2-1-R ADVENTURE
and RTI2018-099655-B-I00 CLARA) and the Office of Naval Research (award no. N00014-191-2226).

21

A Appendix
A.1 Proof of Theorem 1
We first note the following inequalities,
|(ϕ, π) − (ϕ, πθN )| =

(ϕWθ , qθ ) (ϕWθ , qθN )
−
(Wθ , qθ )
(Wθ , qθN )

(ϕWθ , qθ ) − (ϕWθ , qθN )
|(Wθ , qθ )|
1
1
−
+ |(ϕWθ , qθN )|
(Wθ , qθ ) (Wθ , qθN )
≤

(ϕWθ , qθ ) − (ϕWθ , qθN )
|(Wθ , qθ )|
(Wθ , qθN ) − (Wθ , qθ )
+ kϕk∞ |(Wθ , qθN )|
(Wθ , qθ )(Wθ , qθN )
=

(ϕWθ , qθ ) − (ϕWθ , qθN )
(Wθ , qθ )
kϕk∞ |(Wθ , qθN ) − (Wθ , qθ )|
+
.
(Wθ , qθ )
=

We take squares of both sides and apply the inequality (a + b)2 ≤ 2(a2 + b2 ) to further bound
the rhs,
2

|(ϕ, π) −

(ϕ, πθN )|2

(ϕWθ , qθ ) − (ϕWθ , qθN )
≤2
(Wθ , qθ )2
kϕk2∞ |(Wθ , qθN ) − (Wθ , qθ )|2
+2
(Wθ , qθ )2

We now take the expectation of both sides,

E

h

2 i 2E
≤
(ϕ, π) − (ϕ, πθN )

h

2 i
(ϕWθ , qθ ) − (ϕWθ , qθN )

+
h
2 i
2kϕk2∞ E (Wθ , qθN ) − (Wθ , qθ )
(Wθ , qθ )2

(Wθ , qθ )2

.

Note that, both terms in the right hand side are perfect Monte Carlo estimates of the integrals.
Bounding the MSE of these integrals yields
E

h

2 i
2 (ϕ2 Wθ2 , qθ ) − (ϕWθ , qθ )2 2kϕk2∞ (Wθ2 , qθ ) − (Wθ , qθ )2
≤
(ϕ, π) − (ϕ, πθN )
+
,
N
(Wθ , qθ )2
N
(Wθ , qθ )2
2kϕk2∞ (Wθ2 , qθ ) − (Wθ , qθ )2
2kϕk2∞ (Wθ2 , qθ )
+
.
≤
N (Wθ , qθ )2
N
(Wθ , qθ )2

Therefore, we can straightforwardly write,
E

h

2 i
4kϕk2∞ (Wθ2 , qθ )
.
≤
(ϕ, π) − (ϕ, πθN )
(Wθ , qθ )2
N
22

Now it remains to show the relation of the bound to χ2 divergence. Note that,
R Π2 (x)
q (x)dx
2
(Wθ , qθ )
qθ2 (x) θ
=

2
R Π(x)
(Wθ , qθ )2
q
(x)dx
θ
qθ (x)
R
2 (x)
qθ (x)dx
Z 2 πq2 (x)
θ
=
R
2
Z2
πdx

 2
π (X)
:= ρ(θ).
= Eqθ 2
qθ (X)
Note that ρ is not exactly χ2 divergence, which is defined as ρ − 1. Plugging everything into our
bound, we have the result,
h
2 i 4kϕk2∞ ρ(θ)
≤
E (ϕ, π) − (ϕ, πθN )
.
N


A.2 Proof of Lemma 1
We adapt this proof from Ryu and Boyd (2014) by following the same steps. We first show that
A(θ) is convex by first showing that exp(A(θ)) is convex. Choose 0 < η < 1 and using Hölder’s
inequality,
Z
exp(A(ηθ1 + (1 − η)θ2 )) = exp((ηθ1 + (1 − η)θ2 )⊤ T (x))h(x)dx
Z 
η 
1−η
exp(θ1⊤ T (x))h(x)
exp(θ2⊤ T (x))h(x)
dx
=
1−η
η Z
Z
⊤
⊤
.
exp(θ2 T (x))h(x)dx
exp(θ1 T (x))h(x)dx
≤
Taking log of both sides yields
A(ηθ1 + (1 − η)θ2 ) ≤ ηA(θ1 ) + (1 − η)A(θ2 ),
which shows the convexity of A(θ). Note that A(θ) − θ ⊤ T (x) is convex in θ since it is a sum
of a convex and a linear function of θ. Since exp is an increasing convex function and the
composition of convex functions is convex, M (θ, x) := exp(A(θ) − θ ⊤ T (x)) is convex in θ.
Finally we prove that ρ(θ) is convex. First let us write it as
Z 2
Z 2
π (x)
π (x)
q
(x)dx
=
M (θ, x)dx.
ρ(θ) =
θ
qθ (x)2
h(x)
Then we have the following sequence of inequalities
Z 2
π (x)
ρ(ηθ1 + (1 − η)θ2 ) =
M (ηθ1 + (1 − η)θ2 , x)dx
h(x)
Z 2
π (x)
≤
(ηM (θ1 , x) + (1 − η)M (θ2 , x))dx
h(x)
Z 2
Z 2
π (x)
π (x)
=η
M (θ1 , x)dx + (1 − η)
M (θ2 , x)dx
h(x)
h(x)
= ηρ(θ1 ) + (1 − η)ρ(θ2 ),
which concludes the claim. 
23

A.3 Proof of Theorem 4
First note that, using Theorem 3, we have
h
2 i cϕ ρ(θt )
,
≤
E (ϕ, π) − (ϕ, πθNt )
N
cϕ (ρ(θt ) − ρ⋆ ) cϕ ρ⋆
=
+
,
N
N
cϕ kθ0 − θ ⋆ k2 cϕ ρ⋆
+
,
≤
2γtN
N
where the last inequality follows from Lemma 3. 

A.4 A sufficient condition for Assumption 3 to hold
Recall that we have defined ρ2 (θ) = E[wθ4 (X)] = E

h

π 4 (X)
qθ4 (X)

i

and qθ (x) = exp




θ ⊤ T (x) − A(θ) h(x)

whenever qθ (x) belongs to the exponential family. We have the following result.

Proposition 1. Let the ρ2 be Lipschitz with Lipschitz derivatives, let qθ (x) belong to the exponential
family and let Θ be compact. If the sufficient statistics T (X) of the distribution qθ all have finite
variances, i.e.,
max Var(Ti (X)) < ∞,
i=1,...,dθ

then Assumption 3 holds.
Proof. Using the fact that
∂qθ (x)
∂ log qθ (x)
= qθ (x)
∂θi
∂θi
one can readily calculate the second order derivatives of ρ2 (θ). In particular,
"

 #
∂A(θ) 2
∂ 2 ρ2 (θ)
4
= 9E wθ (X) Ti (X) −
∂θi
∂θi2
 ∂ 2 A(θ)

+3E wθ4 (X)
< ∞,
∂θi2

(27)
2

A(θ)
=
where the inequality holds because ρ2 (θ) has Lipschitz derivatives in Θ. However, ∂ ∂θ
2
i

 4
Var(Ti (X)) and, by assumption, maxi Var(Ti (X)) < ∞. Moreover, E wθ (X) = ρ2 (θ) < ∞
because ρ2 (θ) is Lipschitz and the parameter space Θ is compact. Therefore, it follows that
"

 #
∂A(θ) 2
4
E wθ (X) Ti (X) −
<∞
∂θi

and Assumption 3 holds. 

A.5 Proof of Lemma 4
We first note that the exact gradient can be written as
 2

Z 2
π (X)
π (x)
∇θ E q θ 2
dx
= ∇θ
qθ (x)
qθ (X)
Z 2
π (x)
∇θ log qθ (x)qθ (x)dx.
=−
qθ2 (x)
24

Now, note that,




∇θ log qθ (x) = 



(i)

∂ log qθ
∂θ1
∂ log qθ
∂θ2

..
.

∂ log qθ
∂θdθ





.



Given the samples xt ∼ qθt−1 for i = 1, . . . , N to estimate the gradient, we can write the
mean-squared error Ekgt − ∇ρ(θt−1 )k22 as

#
Z 2
N
2
2 (x(i) )
X
π
(x)
π
1
(i)
t
∇θ log qθt−1 (xt ) −
∇θ log qθt−1 (x)qθt−1 (x)dx
E
(i)
2
N
qθ2 (x)
2
i=1 qθt−1 (xt )

2 
Z
dθ
N
2 (x) ∂ log q
2 (x(i) ) ∂ log q
X
X
1
π
π
θt−1
θt−1
(i)
t
=
E 
(xt ) −
(x)qθt−1 (x)dx  .
2
(i)
2
N
∂θ
∂θ
q
(x)
j
j
q
(x )
θt−1
j=1

i=1

θt−1

t

Now the expectation is a standard Monte Carlo error for the test function,
ϕj (x) =

π 2 (x) ∂ log qθ
(x).
qθ2 (x) ∂θj

Assumption 3 together with Lemma A.1 in Crisan and Míguez (2014) yields
E[kgt − ∇ρ(θ)k22 ] ≤

dθ cρ Dπ
,
N

where cρ < ∞ and Dπ = maxj∈{1,...,dθ } Dπj < ∞ are constants independent of N . 

A.6 Proof of Lemma 5
Since projections reduce distances, we have,
kθk − θ ⋆ k22 ≤ kθk−1 − γk gk − θ ⋆ k22

= kθk−1 − θ ⋆ k22 − 2γk gk⊤ (θk−1 − θ ⋆ ) + γk2 kgk k22 .

Let Fk−1 = σ(θ0 , . . . , θk−1 , g1 , . . . , gk−1 ) be the σ-algebra generated by random variables θ0 , . . . , θk−1 ,
g1 , . . . , gk−1 and take the conditional expectations with respect to Fk−1




E kθk − θ ⋆ k22 |Fk−1 ≤ kθk−1 − θ ⋆ k22 − 2γk ∇ρ(θk−1 )⊤ (θk−1 − θ ⋆ ) + γk2 E kgk k22 |Fk−1 .

Next, using the convexity of ρ yields




E kθk − θ ⋆ k22 |Fk−1 ≤ kθk−1 − θ ⋆ k22 − 2γk [ρ(θk−1 ) − ρ(θ ⋆ )] + γk2 E kgk k22 |Fk−1 .

Finally, we take unconditional expectations of both sides,

Ekθk − θ ⋆ k22 ≤ Ekθk−1 − θ ⋆ k22 − 2γk E[(ρ(θk−1 ) − ρ(θ ⋆ )] + γk2 Ekgk k22 .
With rearranging, using Ekgk −∇ρ(θk−1 )k22 = Ekgk k2 −k∇ρ(θk−1 )k2 and invoking Assumption 2,
we arrive at
E[ρ(θk−1 ) − ρ(θ ⋆ )] ≤

Ekθk−1 − θ ⋆ k22 − Ekθk − θ ⋆ k22 γk (σρ2 + G2ρ )
+
.
2γk
2
25

where σρ2 = dθ Dπ /N as given in Lemma 4. Now summing both sides from k = 1 to t and
dividing both sides by t,
E[ρ(θ̄t ) − ρ(θ ⋆ )] ≤
since

1
γk

≤

1
γt

t
t
Ekθ0 − θ ⋆ k22 X γk (σρ2 + G2ρ )
1X
E[ρ(θk−1 ) − ρ(θ ⋆ )] ≤
+
,
t
2γt t
2t
k=1

k=1

√
for all k ≤ t. Substituting γk = α/ k and noting that
Z t
t
X
√
1
1
√ ≤
√ dτ = 2 t,
τ
k
0
k=1

we arrive at
E[ρ(θ̄t ) − ρ(θ ⋆ )] ≤
where θ̄t =

1
t

Pt−1

k=0 θk .

Ekθ0 − θ ⋆ k22 α(σρ2 + G2ρ )
√
√
+
,
2α t
t



A.7 Proof of Theorem 5
Let Ft−1 = σ(θ0 , . . . , θt−1 , g1 , . . . , gt−1 ) be the σ-algebra generated by the random variables
θ0 , . . . , θt−1 , g1 , . . . , gt−1 . Then


2
cϕ ρ(θ̄t )
N
Ft−1 ≤
E (ϕ, π) − (ϕ, πθ̄t )
N
cϕ (ρ(θ̄t ) − ρ⋆ ) cϕ ρ⋆
+
,
=
N
N
P
where θ̄t = 1t t−1
k=0 θk is an Ft−1 -measurable random variable. Now if we take unconditional
expectations of both sides,

2  c E (ρ(θ̄ ) − ρ⋆ ) c ρ⋆
ϕ
t
ϕ
N
≤
E (ϕ, π) − (ϕ, πθ̄t )
+
.
N
N


The result follows from applying Lemma 5 for E (ρ(θ̄t ) − ρ⋆ ) . 

References
S Agapiou, Omiros Papaspiliopoulos, D Sanz-Alonso, and AM Stuart. Importance sampling:
Intrinsic dimension and computational cost. Statistical Science, 32(3):405–431, 2017.
Ömer Deniz Akyildiz and Sotirios Sabanis. Nonasymptotic analysis of Stochastic Gradient
Hamiltonian Monte Carlo under local conditions for nonconvex optimization. arXiv preprint
arXiv:2002.05465, 2020.
Bouhari Arouna. Adaptative monte carlo method, a variance reduction technique. Monte Carlo
Methods and Applications, 10(1):1–24, 2004a.
Bouhari Arouna. Robbins-Monro algorithms and variance reduction in finance. Journal of
Computational Finance, 7(2):35–62, 2004b.
26

Léon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. arXiv:1606.04838, 2016.
Sébastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and
Trends R in Machine Learning, 8(3-4):231–357, 2015.
Mónica F Bugallo, Luca Martino, and Jukka Corander. Adaptive importance sampling in signal
processing. Digital Signal Processing, 47:36–49, 2015.
Monica F Bugallo, Victor Elvira, Luca Martino, David Luengo, Joaquin Miguez, and Petar M
Djuric. Adaptive Importance Sampling: The past, the present, and the future. IEEE Signal
Processing Magazine, 34(4):60–79, 2017.
Olivier Cappé, Arnaud Guillin, Jean-Michel Marin, and Christian P Robert. Population Monte
Carlo. Journal of Computational and Graphical Statistics, 13(4):907–929, 2004.
Olivier Cappé, Randal Douc, Arnaud Guillin, Jean-Michel Marin, and Christian P Robert. Adaptive importance sampling in general mixture classes. Statistics and Computing, 18(4):447–
459, 2008.
Sourav Chatterjee, Persi Diaconis, et al. The sample size required in importance sampling. The
Annals of Applied Probability, 28(2):1099–1135, 2018.
Dan Crisan and Joaquín Míguez. Particle-kernel estimation of the filter density in state-space
models. Bernoulli, 20(4):1879–1929, 2014.
Adji Bousso Dieng, Dustin Tran, Rajesh Ranganath, John Paisley, and David Blei. Variational
inference via χ-upper bound minimization. In Advances in Neural Information Processing
Systems, pages 2732–2741, 2017.
Randal Douc, Arnaud Guillin, J-M Marin, and Christian P Robert. Convergence of adaptive
mixtures of importance sampling schemes. The Annals of Statistics, 35(1):420–448, 2007.
Prateek Jain, Dheeraj Nagaraj, and Praneeth Netrapalli. Making the Last Iterate of SGD Information Theoretically Optimal. In Conference on Learning Theory, pages 1752–1755, 2019.
Hilbert Johan Kappen and Hans Christian Ruiz. Adaptive importance sampling for control and
inference. Journal of Statistical Physics, 162(5):1244–1266, 2016.
Reiichiro Kawai. Adaptive monte carlo variance reduction for lévy processes with two-time-scale
stochastic approximation. Methodology and Computing in Applied Probability, 10(2):199–223,
2008.
Reiichiro Kawai. Acceleration on adaptive importance sampling with sample average approximation. SIAM Journal on Scientific Computing, 39(4):A1586–A1615, 2017.
Reiichiro Kawai. Optimizing adaptive importance sampling by stochastic approximation. SIAM
Journal on Scientific Computing, 40(4):A2774–A2800, 2018.
Bernard Lapeyre and Jérôme Lelong. A framework for adaptive monte carlo procedures. Monte
Carlo Methods and Applications, 17(1):77–98, 2011.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
27

Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic
gradient Langevin dynamics: a nonasymptotic analysis. In Conference on Learning Theory,
pages 1674–1703, 2017.
H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics,
22:400–407, 1951.
Christian P Robert and George Casella. Monte Carlo statistical methods. John Wiley & Sons,
2004.
Ernest K Ryu. Convex optimization for Monte Carlo: Stochastic optimization for importance sampling. PhD thesis, Stanford University, 2016.
Ernest K Ryu and Stephen P Boyd. Adaptive importance sampling via stochastic convex programming. arXiv:1412.4845, 2014.
Daniel Sanz-Alonso. Importance sampling and necessary sample size: an information theory
approach. SIAM/ASA Journal on Uncertainty Quantification, 6(2):867–879, 2018.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient. Mathematical Programming, 162(1-2):83–112, 2017.
Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In International Conference on Machine
Learning, pages 71–79, 2013.
Vladislav B Tadić and Arnaud Doucet. Asymptotic bias of stochastic gradient search. The Annals
of Applied Probability, 27(6):3255–3304, 2017.
Martin J Wainwright and Michael I Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends R in Machine Learning, 1(1–2):1–305, 2008.
Ying Zhang, Ömer Deniz Akyildiz, Theo Damoulas, and Sotirios Sabanis. Nonasymptotic estimates for Stochastic Gradient Langevin Dynamics under local conditions in nonconvex optimization. arXiv preprint arXiv:1910.02008, 2019.

28

