Revisiting the Asymptotic Optimality of RRT*

arXiv:1909.09688v2 [cs.RO] 22 Apr 2020

Kiril Solovey1 , Lucas Janson2 , Edward Schmerling3 , Emilio Frazzoli4 , and Marco Pavone1

Abstract— RRT∗ is one of the most widely used samplingbased algorithms for asymptotically-optimal motion planning.
RRT∗ laid the foundations for optimality in motion planning
as a whole, and inspired the development of numerous new
algorithms in the field, many of which build upon RRT∗ itself.
In this paper, we first identify a logical gap in the optimality
proof of RRT∗ , which was developed by Karaman and Frazzoli
(2011). Then, we present an alternative and mathematicallyrigorous proof for asymptotic optimality. Our proof suggests
that the connection radius used by RRT∗ should be increased


n 1/d
n 1/(d+1)
from γ log
to γ 0 log
in order to account
n
n
for the additional dimension of time that dictates the samples’
ordering. Here γ, γ 0 are constants, and n, d are the number
of samples and the dimension of the problem, respectively.

I. I NTRODUCTION
For many robot motion-planning applications, feasibility
is not enough—we further desire path plans that are of high
quality, reflecting a need for robots that can achieve their
goals with efficiency, alacrity, and economy of motion. To
this end we seek planning algorithms that can be trusted,
whatever obstacle environment a robot faces, to produce
optimal or near-optimal plans with minimal scenario-specific
tuning. The advent of the asymptotically-optimal rapidlyexploring random tree (RRT∗ ) algorithm [1] has ushered in a
decade of theoretical and practical successes in the development of optimal sampling-based motion-planning algorithms.
Although proposed in its initial form for the case of
minimum-length path planning for robots without dynamic
constraints, RRT∗ has been extended to handle kinodynamic
planning problems [2] including robotic systems governed
by non-holonomic constraints [3], more expressive costs
accounting for robot energy expenditure [4], [5], and even to
plan paths that minimize violation of safety rules [6] or that
otherwise balance performance considerations with safety
constraints [7]. Heuristic modifications to the core algorithm
have also been demonstrated that improve practical RRT∗
implementations [8], [9].
Each of these extensions leverages the simple yet powerful
iterative local graph-rewiring technique introduced by RRT∗
to enable convergence to the optimal solution (as computation budget increases), provided an appropriate choice for the
scaling of the rewiring radius as a function of sample count.
Moreover, each of these extensions draws upon the original
analysis presented in [1] for the fundamental asymptotic
1 Department of Aeronautics and Astronautics, Stanford University, CA,
USA.
2 Department of Statistics, Harvard University, MA, USA.
3 Waymo Research, CA, USA.
4 Institute for Dynamic Systems and Control, ETH Zurich, Switzerland.

scaling of this algorithm parameter; this analysis is therefore
core to each of their optimality guarantees.
Contribution. The primary contribution of this paper is an
in-depth study of the theoretical analysis underpinning the
asymptotic-optimality criterion for the RRT∗ algorithm. In
revisiting this analysis, we identify a logical gap in the original proof and provide an amended proof suggesting a larger
radius scaling exponent to ensure asymptotic optimality. The
impact of this paper is potentially far-reaching in the large
number of works that currently appeal to RRT∗ optimality
to make their theoretical guarantees.
The paper is organized as follows. Section II provides
preliminaries and a description of RRT∗ . In Section III we
review the original optimality proof of RRT∗ and identify a
logical gap within it. In Section IV we provide the main
contribution of this paper, which is an alternative proof
that circumvents this logical gap. We conclude the paper in
Section V.
II. P RELIMINARIES
We provide several basic definitions that will be used
throughout the paper. Given two points x, y ∈ Rd , denote
by kx − yk the standard Euclidean distance. Denote by
Br (x) the d-dimensional ballS of radius r > 0 centered at
x ∈ Rd . Define Br (Γ) := x∈Γ Br (x) for any Γ ⊆ Rd .
Similarly,
given a curve σ : [0, 1] → Rd , define Br (σ) =
S
d
B
τ ∈[0,1] r (σ(τ )). For a subset D ⊂ R , |D| denotes its
Lebesgue measure. All logarithms used herein are to base e.
A. Motion planning
Denote by C the robot’s configuration space, and by F ⊆ C
the free space, i.e., the set of all collision free configurations.
We assume that C is a subset of the Euclidean space. For
simplicity, let C = [0, 1]d ⊂ Rd for some fixed d ≥ 2. Given
start and target configurations s, t ∈ F, the motion-planning
problem consists of finding a continuous path (curve) σ :
[0, 1] → F such that σ(0) = s and σ(1) = t. That is,
the robot starts its motion along σ at s, and ends at t, while
avoiding collisions. An instance of the problem is defined by
(F, s, t). We consider the standard path length as a measure
of quality:
Definition 1. Given a path σ, its length (cost), which
corresponds to its Hausdorff measure, is represented by
n
X
c(σ) =
sup
kσ(τi ) − σ(τi−1 )k.
n∈N+ ,0=τ1 ≤...≤τn =1 i=2

We proceed to describe the notion of robustness, which is
essential when discussing theoretical properties of samplingbased planners. Given a subset Γ ⊂ C and two configurations

x, y ∈ Γ, denote by ΣΓx,y the set of all continuous paths,
whose image is in Γ, that start in x and end in y, i.e., if
σ ∈ ΣΓx,y then σ : [0, 1] → Γ and σ(0) = x, σ(1) = y.
We mention that the following definition is slightly different
than the one used in [1], [10].
Definition 2. Let (F, s, t) be a motion-planning problem.
A path σ ∈ ΣF
s,t is robust if there exists δ > 0 such that
Bδ (σ) ⊂ F. We also say that (F, s, t) is robustly feasible if
there exists such a robust path.
Definition 3. The robust optimum is defined as

c∗ = inf c(σ) σ ∈ ΣF
s,t is robust .
B. Algorithms
While our main focus in this paper is the RRT∗ algorithm,
we also rely on the properties of the RRT algorithm, which is
described first. The following description of the (geometric)
RRT algorithm is based on [11] and [1].
Algorithm 1 RRT(xinit := s, xgoal := t, n, η)
1: V = {xinit }
2: for j = 1 to n do
3:
xrand ← SAMPLE - FREE( )
4:
xnear ← NEAREST(xrand , V )
5:
xnew ← STEER(xnear , xrand , η)
6:
if COLLISION - FREE(xnear , xnew ) then
7:
V = V ∪ {xnew }; E = E ∪ {(xnear , xnew )}
8: return G = (V, E)

The input for RRT (Algorithm 1) is an initial and goal
configurations xinit , xgoal , number of iterations n, and a
steering parameter η > 0. RRT constructs a tree G = (V, E)
by performing n iterations. In each iteration, a new sample
xrand is returned from F uniformly at random by calling
SAMPLE - FREE. Then, the vertex xnear ∈ V that is nearest
(according to k · k) to xrand is found using NEAREST. A new
configuration xnew ∈ X is then returned by STEER, such that
xnew is on the line segment between xnear and xrand , and the
distance kxnear − xnew k is at most η. Finally, COLLISION FREE(xnear , xnew ) checks whether the straight-line path from
xnear to xnew is collision free. If so, xnew is added as a vertex
to G and is connected by an edge from xnear .
We proceed to describe RRT∗ [1] in Algorithm 2. Every RRT∗ iteration begins with an RRT-style extension.
The difference lies in the subsequent lines. First, RRT∗
attempts to connect the tree to xnew from all its neighbors in V within a min{r(|V |), η} vicinity (lines 7-15).
Notice that the expression r(|V |) determines the radius
based on the current number of vertices in V . (The operation NEAR(xnew , V, min{r(|V |), η}) returns the subset
V ∩ Bmin{r(|V |),η} (xnew ), i.e., the vertices that are within
a distance of min{r(|V |), η} from xnew .) However, it only
adds a single edge to xnew from the neighbor xmin ∈ Xnear
such that COST(xnew ) is minimized (line 16). In the next
step, RRT∗ attempts to perform rewires (lines 17-21): with
the addition of xnew , it may be beneficial to reroute the
existing path of xnear to use xnew . RRT∗ checks whether

changing the parent of xnear to be xnew reduces COST(xnear ).
(PARENT(xnear ) returns the immediate predecessor of xnear in
G. COST(x) for x ∈ V returns the cost of the path leading
from xinit to x in G.)
Algorithm 2 RRT∗ (xinit := s, xgoal := t, n, r, η)
1: V = {xinit }
2: for j = 1 to n do
3:
xrand ← SAMPLE - FREE( )
4:
xnear ← NEAREST(xrand , V )
5:
xnew ← STEER(xnear , xrand , η)
6:
if COLLISION - FREE(xnear , xnew ) then
7:
Xnear = NEAR(xnew , V, min{r(|V |), η})
8:
V = V ∪ {xnew }
9:
xmin = xnear
10:
cmin = COST(xnear ) + kxnew − xnear k
11:
for xnear ∈ Xnear do
12:
if COLLISION - FREE(xnear , xnew ) then
13:
if COST(xnear ) + kxnew − xnear k < cmin then
14:
xmin = xnear
15:
cmin = COST(xnear ) + kxnew − xnear k

E = E ∪ {(xmin , xnew )}
for xnear ∈ Xnear do
if COLLISION - FREE(xnew , xnear ) then
if COST(xnew ) + kxnear − xnew k < COST(xnear ) then
xparent = PARENT(xnear )
E = E ∪ {(xnew , xnear )} \ {(xparent , xnear )}
22: return G = (V, E)
16:
17:
18:
19:
20:
21:

Remark 1. As mentioned above, RRT∗ performs extensions
of the tree in a manner similar to RRT. That is, STEER
generates xnew , which lies on the straight line connecting
xnear , xrand , such that kxnew − xnear k ≤ η. Note that initially
xnew 6= xrand , but once
S the space is sufficiently covered by
G, i.e., when F ⊂ v∈V Bη (v), then in all the following
iterations it will hold that xnew = xrand . This property will
be important in the analysis of RRT∗ , as it indicates that
xnew is uniformly sampled from F. This notion will be
formalized below. For now, it is useful to note that given
the same sequence of samples, RRT and RRT∗ will generate
two (possibly distinct) graphs that have a common vertex set.
III. O RIGINAL OPTIMALITY PROOF
In this section we review the original proof [1] for
asymptotic optimality of RRT∗ , and point out a logical gap.
Specifically, Theorem 38 in [1] states that if the connection
radius used by RRT∗ is of the form

1/d
log n
KF
KF
r (n) = γ
,
(1)
n
where n ∈ N+ , and for some constant γ KF > 0, the cost
of the solution obtained by RRT∗ converges to the robust
optimum c∗ as n → ∞, almost surely.
A. Review of previous proof
We provide a sketch of the original proof and identify a
logical gap. We mention that our definitions of robustness
(Definition 2) and robust optimum (Definition 3) are simplified versions of the ones used originally in [1], where the

G
Xj i

0
Xji+1

0
Xji+2

s

σn
σε

t

Bn,i Bn,i+1 B
n,i+2
Fig. 1. Illustration of the components in the original proof [1]. (a) The robustly-optimal path σε is drawn as a black curve. (b) Discs represent the balls
Bn,1 , . . . , Bn,Mn , whose centers are denoted as red bullets along σε . The path σn connecting samples between adjacent balls in an increasing order is
illustrated as a blue curve. (c) A problematic scenario (Section III-B), where the RRT∗ tree G yields a suboptimal solution, is depicted in green.

latter are slightly less convenient to work with (especially in
correction of the proof which we give in Section IV). We
thus adapt the original proof details presented in this section
to our setting. We emphasize that the logical gap is unrelated
to those definitions, and our argument presented below can
be easily remapped to the original formulation.
Recall that the sample set of RRT∗ consists of n timelabeled configurations. Denote by {X1 , . . . , Xn } the sample
set, where indices denote the order in which the samples
are drawn. Fix ε > 0 and let σε be a robust solution
path such that c(σε ) ≤ (1 + ε)c∗ . The proof constructs a
sequence of Mn ≤ n identical balls Bn,1 , . . . , Bn,Mn that
are centered on some equally-spaced points along σε . The
size and spacing of balls
is set so that (a) σε is completely
SM
n
covered by them, (b) i=1
Bn,i ⊆ F, and (c) for every
0
1 ≤ i ≤ Mn , x ∈ Bn,i , x ∈ Bn,i+1 it holds that kx − x0 k ≤
rKF (n) ≤ rKF (|V |). Furthermore, it is shown in [1] that given
xi ∈ Bn,i for every 1 ≤ i ≤ Mn , the length of the path σ
connecting each xi to the point in the next ball with a straight
line converges (as n → ∞) to the length of σε (see Figure 1).
The proof establishes that if for every 1 ≤ i < Mn there
exist Xji , Xji+1 such that (i) Xji ∈ Bn,i , Xji+1 ∈ Bn,i+1
and (ii) ji < ji+1 , then RRT∗ is asymptotically optimal (see
Section G.3 in [1]). The rationale behind these conditions
is as follows. Condition (i) makes sure that the optimal
path is approximated by samples drawn by RRT∗ , i.e., for
every point along σε there is a sample point in its vicinity.
Condition (ii) ensures that RRT∗ will have the opportunity to
add a directed edge from Xji to Xji+1 : as Xji+1 is sampled
after Xji then RRT∗ would consider drawing a directed
edge from the latter to the former, considering the fact that
Xji ∈ NEAR(Xji+1 , V, min{r(n), η}) (this is formalized in
Claim 1 below). Observe that r(n) is used as a conservative
lower-bound for r(|V |) throughout [1], as we do too.
Consequently, the proof deduces that if these conditions
are met RRT∗ is guaranteed to find a solution with cost at
most c∗ (1 + ε) with probability that converges to 1 as n →
∞. In particular, denote by Xj1 , . . . , XjMn the sequence of
samples satisfying the conditions above, and let σn be a path
that is induced by those Mn samples in the prescribed order.
Then the claim is that the solution returned by RRT∗ is of
length c(σn ), if not shorter.

B. A logical gap
We identify an issue with the proof technique described
above, and in particular with the conditions (i) and (ii).
We assert that the line of reasoning mentioned above
overlooks the fact that the existence of pairwise sequential
samples does not directly imply the existence of a whole
chain of samples with a proper ordering such that a path
in G traces through all the balls in sequence. That is, the
fact that for every 1 ≤ i < Mn (i) there exist Xji , Xji+1
such that Xji ∈ Bn,i , Xji+1 ∈ Bn,i+1 and (ii) ji < ji+1 ,
does not necessarily mean that (iii) there exists a sequence
j1 ≤ j2 ≤ . . . ≤ jMn such that Xji ∈ Bn,i for every
1 ≤ i < Mn ; (iii) is a sufficient (but not necessary) condition
for recovering a path that is at least as good as σn .
Consider for instance the case where Xji ∈ Bn,i , Xji+1 ∈
0
0
∈ Bn,i+2 and ji <
∈ Bn,i+1 , Xji+2
Bn,i+1 , Xji+1
0
0
0
ji+1 , ji+1
< ji+2
, but ji+2
< ji , where there are two points
0
Xji+1 , Xji+1
that fall into the same ball Bn,i+1 (see Figure 1
Si−1
(c)). Define X1ji −1 = {X1 , . . . , Xji −1 }, B1i−1 = k=1 Bn,k ,
and let X B = X1ji −1 ∩ B1i−1 ∩ Br(ji )KF (Xji ). Namely, X B
contains all the sampled points that were drawn before Xji ,
which lie in previous balls along σε , and whose distance
from Xji is at most r(ji )KF .
Now assume that X B = ∅. We can choose the current
0
structure of G and the locations of Xji , Xji+1
such that
the only directed edge that is added in iteration ji is
0
0
(Xji+1
, Xji ), i.e., from Xji+1
to Xji (rather than the other
way around). Note that in iteration ji+1 the addition of
sample Xji+1 would not resolve this problematic wiring
since the latter sample will be connected by a directed edge
0
either from Xji or Xji+1
. Moreover, we can repeat this
argument for preceding balls to yield a long chain of samples
that are connected in the opposite direction.
In this discussion it is important to keep in mind that
RRT∗ performs rewiring (i.e., changing the predecessor of
a given vertex) only locally (lines 17-21 of Algorithm 2).
That is, in order to force a rewiring of a given vertex Xj
RRT∗ must sample a vertex xnew in the vicinity of Xj , and
this rewiring would not cause a chain of rewires for Xj s
predecessors or successors in G. Consequently, in order to
0
reverse the direction of the aforementioned chain from Xji+1
,
RRT∗ would need to sample new vertices along the chain

in the correct order. For a more detailed example see the
appendix.
As we show in our proof in the next section, condition
(iii) is in fact sufficient to guarantee asymptotic optimality,
and we prove that it indeed holds with high probability when
we slightly increase the connection radius from Equation (1),
and modify the constant γ KF .
IV. A LTERNATIVE PROOF
In order to account for the additional dimension of time,
1
 d+1

,
we set the connection radius to be r(n) = γ logn n
where γ is a constant that will be determined below. We
state our main theorem and provide an overview of the proof.
The full proof is presented later on. Note that our result
suggests that the exponent should be decreased from 1/d
to 1/(d + 1), which yields a larger radius overall. Denote
by σn the path connecting s to t returned by RRT∗ after n
iterations. Recall that c(σn ) denotes its length (in case that
no solution is found, the length of σn is assumed to be ∞).
Our main theorem, which appears below, states that if γ is
set correctly, then the cost of the solution returned by RRT∗
is upper-bounded asymptotically by (1 + ε)c∗ , where c∗ is
the robust optimum, and ε is a tuning parameter. Additional
tuning parameters that appear in the theorem are as follows:
η is the steering size of RRT∗ (Algorithm 2, line 5), while
µ and θ are constants whose purpose will become clear in
the proof of the theorem.
Theorem 1. Suppose that (F, s, t) is robustly feasible and
fix η > 0, ε ∈ (0, 1),1 θ ∈ (0, 1/4), and µ ∈ (0, 1). Define
the radius of RRT∗ to be
 1

log n d+1
,
(2)
r(n) = γ
n
such that

γ ≥ (2 + θ)

|F|
(1 + ε/4)c∗
·
(d + 1)θ(1 − µ) ζd

1
 d+1

,

(3)

where ζd is the volume of a unit d-dimensional hypersphere.
Then
lim Pr[c(σn ) ≤ (1 + ε)c∗ ] = 1.
n→∞

Our proof of Theorem 1 proceeds similarly to the proof
of the asymptotic optimality of FMT∗ [10] (which is in turn
based on [1]), but with additional complications due to the
time dimension and the coupling with the RRT algorithm.
We proceed to describe the main ingredients of the proof.
Fix the parameters ε ∈ (0, 1), θ ∈ (0, 1/4), µ ∈ (0, 1), η >
0. Due to the fact that (F, s, t) is robustly feasible, there
exists a robust path σε ∈ ΣF
s,t and δ > 0 such that c(σε ) ≤
(1 + ε/4)c∗ and Bδ (σε ) ⊂ F. We will show that the RRT∗
graph G contains a path that is in the vicinity of σε , which
implies that the solution returned by RRT∗ is of cost at most
(1 + ε)c∗ (which is slightly larger than (1 + ε/4)c∗ due to
the fact that this is still an approximation of the path σε ).
1 For simplicity, we upper-bound ε with 1 although the proof can be
adapted to accommodate larger stretch factors.

The first part of the proof deals with the technicality
involved with the samples produced by the algorithm. Denote
by V = {X1 . . . , Xn } the sequence of vertices produced
by RRT∗ , where Xj is equal to xnew generated in iteration
j. Due to the fact that RRT∗ (and RRT) perform steering
(line 5), samples are not distributed in a uniform manner,
as xrand is not necessarily identical to xnew (see Remark 1).
However, we do show that most of the vertices in V that
are in the vicinity of σε are distributed uniformly at random,
with probability approaching 1 (see Lemma 1). This event
is denoted by E1 (see Definition 4).
Next, we proceed in a manner similar to other proofs of
asymptotic optimality (see, [1], [10], [13]), by defining a
sequence of points x1 , . . . , xMn along the path σε and specifying a sequence of balls Bn,1 , . . . , Bn,Mn that are centered
on those points respectively, and whose radius
is 
proportional
 m
l
−1

,
to r(n). More formally, define Mn = c(σε ) · r(n)
2+θ
and let x1 , . . . , xMn be a sequence of points along σε such
that kxi − xi−1 k ≤ θr(n)
2+θ , x1 = s, xMn = t. For every
1 ≤ i ≤ Mn define Bn,i := B r(n) (xi ).
2+θ
As suggested in Section III, we need to reason both about
the existence of samples inside those balls, and the order
of those samples. We assign to every ball Bn,i a specific
time window Ti , corresponding to allowed timestamps of
samples, and partition the sample set V = {X1 , . . . , Xn }
into the subsets V0 , V1 , . . . , VMn , where Xj ∈ Vi if j ∈ Ti . In
particular, T0 consists of the first n0 indices, where n0 = µn,
and every Ti , where i > 1 consists of (n − n0 )/Mn indices,
and µ ∈ (0, 1) is a constant:
n
j
k
j
ko
0
0
n−n0
Ti6=0 = n0 + (i − 1) · n−n
+
1,
.
.
.
,
n
+
i
·
.
Mn
Mn
We show that the event E2 (Definition 5) indicating that
every Bn,i contains a vertex from Vi occurs with probability
approaching 1 as well (Lemma 2). The motivation for this
event is the following claim, which indicates that edges
between points in consecutive balls are added if deemed
beneficial.
Claim 1. There exists n ∈ N+ large enough such that
the following holds with respect to Gji+1 = (Vji+1 , Eji+1 ):
Suppose that there exist Xji ∈ Vi ∩ Bn,i , Xji+1 ∈ Vi+1 ∩
Bn,i+1 and denote by Gji+1 the RRT∗ graph at the end of
iteration ji+1 . Then in Gji +1 it follows that COST(Xji+1 ) ≤
COST (Xji ) + kXji − Xji+1 k.
Proof. Recall that Bn,i = B r(n) (xi ) and kxi − xi+1 k ≤
2+θ

θr(n)
2+θ .

For any x ∈ Bn,i , x0 ∈ Bn,i+1 it follows that
kx − x0 k ≤

r(n)
2+θ

+

θr(n)
2+θ

+

r(n)
2+θ

= r(n).

This implies that Xji ∈ Xnear = NEAR(Xji+1 , Vji , r(n)),
which will cause the execution of the test
COLLISION - FREE (Xji , Xji+1 ) (line 12 of RRT∗ ). The
latter will be evaluated to be true since Bδ (σε ) ⊆ F and
r(n)  δ (for n large enough). Thus, in line 13 the edge
(Xji , Xji+1 ) will be added to the graph, unless there is a
lower-cost alternative for connection.

Thus, E2 guarantees that the RRT∗ tree G contains a path
connecting s to t that follows σε closely. In order to
ensure that c(σn0 ) ≤ (1 + ε)c∗ we need one more step, since
σn0 could stay close to σε but zig-zag around it, resulting in
a high-cost solution.
Define the constants α ∈ (0, θε/16), β ∈ (0, θε/16).
β
Additionally, define for every 1 ≤ i ≤ Mn the ball Bn,i
:=
3
B βr(n) (xi ). The event E (Definition 6) indicates that a

the latter is the vertex set of RRT∗ after n0 iterations, and
1
assume that E0 n holds . Fix an iteration n0 < j < n and
some 1 ≤ k ≤ `. Due to the fact that η > 0 is fixed, by the
proof of Lemma 1 in [14] it follows that if xjrand ∈ Bκ (zj )
then xjnear ∈ B5κ (zj ), and consequently

β
fraction of at most α of the smaller balls Bn,i
does not
3
contain samples from Vi . We show that E occurs with
probability approaching 1 (Lemma 3). We then proceed
to show that if E2 , E3 occur simultaneously then RRT∗ is
guaranteed to return a solution with cost at most (1 + ε)c∗
(Lemma 4).

This implies that xjnew = xjrand . Additionally, observe that
due to the fact that the straight-line path from xjnear to xjrand
is contained in Bκ (zj ), where κ < δ/5, it is also collision
free. Thus, at the end of iteration j, xrand will be added to
the RRT∗ graph as a vertex.

σn0

kxjrand − xjnear k = kxjrand − zj + zj − xjnear k
≤ kxjrand − zj k + kzj − xjnear k ≤ κ + 5κ ≤ η.

2+θ

A. Proof of Theorem 1
We start with a formal definition of E1 :
xjrand , xjnew

Definition 4. For every 1 ≤ j ≤ n denote by
the
random and new samples of RRT∗ in iteration j (line 3 and
line 5 in Algorithm 2, respectively). Define n0 := µn and
E1n := {∀1 ≤ i ≤ Mn , n0 ≤ j ≤ n :
if xjrand ∈ Bn,i then xjrand = xjnew }.

We will prove that the following event E2 holds with
probability approaching 1 by conditioning on E1 .
Definition 5. E2n represents the event that every Bn,i contains at least one vertex from Vi . That is,
E2n := {∀1 ≤ i ≤ Mn , Vi ∩ Bn,i 6= ∅}.
Lemma 2. limn→∞ Pr[E2n ] = 1.
Proof. Observe that

That is, E1n is the event that all xjrand ∈ Bn,i for j between
n0 and n satisfy xjrand = xjnew .

Pr[E2n ] = Pr[E2n |E1n ] · Pr[E1n ] + Pr[E2n |E1n ] · Pr[E1n ]

Remark 2. We wish to stress that the following lemma,
which lower bounds the probability of E1n , is a key ingredient
in our proof. As we shall see below, this would allow us
to treat some of the vertices added by RRT∗ as uniformly
sampled, which is not true for all samples, as some are
perturbed by the STEER operation. We mention that this
issue was not addressed in the original proof in [1], where
the RRT∗ nodes were assumed (incorrectly) to be uniformly
distributed. Furthermore, setting the steering step η = ∞
does not resolve this issue.

We shall lower-bound the expression Pr[E2n |E1n ]. By definition of E1n , for every n0 < j ≤ n, and i such that j ∈ Ti , if
xjrand ∈ Bn,i , then xjnew = xjrand is a valid vertex of the RRT∗
graph. Thus, by conditioning on E1n we can treat V \ V0 as
uniform random samples from F. This will come in handy
in bounding the probability of E2 :

Lemma 1. There exist two constants a, b > 0 such that
Pr[E1n ] ≥ 1 − a · e−bn .
Proof. A similar proof appears in [13, Claim 6], albeit for a
different type of sampling scheme and in the context of an
RRG analysis. The main challenge here is to show that while
it is not true that all the new samples xnew are distributed
uniformly randomly (due to lines 4,5 in Algorithm 2), most
of them are. Define κ := min{η, δ}/10 and set z1 , . . . , z`
to be a sequence of points placed along σε , such that ` =
c(σε )/κ, and kzk − S
zk+1 k ≤ κ. S
Observe that for n large
Mn
`
enough it holds that i=1
Bn,i ⊂ k=1 Bκ (zk ).
Denote by VnRRT
the vertex set of RRT after n0 iterations.
0
Theorem 1 in [14] states that there exist constants a, c > 0
such that the probability that for every 1 ≤ k ≤ ` it holds
0
that VnRRT
∩ Bκ (zk ) 6= ∅ is at least a · e−cn = a · e−bn , where
0
b := cµ. Notice that this theorem requires η to be fixed (i.e.,
independent of n) and strictly positive.
1
Denote the latter event to be E0 n . Next, we show that
∗
01
1
E n implies En . First, observe that VnRRT
= VnRRT
, where
0
0

≥ Pr[E2n |E1n ] · Pr[E1n ].

Pr[E2n |E1n ] = Pr [∃1 ≤ i ≤ Mn , Vi ∩ Bn,i = ∅]
|Ti |
Mn 
Mn
X
X
|Bn,i |
1−
Pr[Vi ∩ Bn,i = ∅] =
≤
|F|
i=1
i=1

d !(n−n0 )/Mn
≤ Mn

1−

ζd

r(n)
2+θ

|F |



r(n)d
n − n0 ζd
≤ Mn exp −
·
·
Mn
|F| (2 + θ)d


r(n)d
nr(n)θ(1 − µ) ζd
≤ Mn exp −
·
·
c(σε )(2 + θ) |F| (2 + θ)d


θζd (1 − µ)
d+1
= Mn exp −
·
n
·
r(n)
c(σε )(2 + θ)d+1 |F|


log n
=: Mn exp −ξ · n · γ d+1
n


−1 
n
o
= c(σε ) · r(n)
exp −ξγ d+1 log n
2+θ



−1
n
o
< c(σε ) · r(n)
+
1
exp −ξγ d+1 log n
2+θ
=

(4)

(5)

(6)

d+1
c(σε )(2+θ)
(log n)−1/(d+1) n1/(d+1)−ξγ
θγ

n
o
+ exp −ξγ d+1 log n ,

(7)

where (4) is due to the union bound and the fact that Vi
is uniformly sampled at random from F, (5) is due to the

inequality 1 − x ≤ e−x for x ∈ (0, 1) which applies here
d (1−µ)
for n large enough, and (6) defines ξ := c(σεθζ
. If
)(2+θ)d+1 |F |
−1
d+1
(d + 1) − ξγ
≤ 0 then the final expression tends to 0.
Indeed,
θζd (1 − µ)
1
−
· γ d+1 ≤ 0 ⇐⇒
d+1
c(σε )(2 + θ)d+1 |F|

 1
 ∗
 1
d+1
d+1
c(σε )|F |
c (1+ε/4)|F |
(2 + θ) (d+1)θζ
≤
(2
+
θ)
≤ γ.
(1−µ)
(d+1)θζ
(1−µ)
d
d

It remains to show that limn→∞ Pr[E2n |E1n ] · Pr[E1n ] = 1:
Pr[E2n |E1n ] · Pr[E1n ] = (1 − Pr[E2n |E1n ])(1 − Pr[E1n ])
= 1 + Pr[E2n |E1n ] · Pr[E1n ] − Pr[E2n |E1n ] − Pr[E1n ]
> 1 − Pr[E2n |E1n ] − Pr[E1n ],
where the final expression converges to 1, according to
Equation 7 and Lemma 1.
Next we consider the existence of samples in a collection
of smaller balls.
β
Definition 6. Let Knβ := |{i ∈ {1, . . . , Mn } : Bn,i
∩ Vi = ∅}|.
3
β
En := {Kn ≤ αMn } is the event that at most αMn of the
β
smaller balls Bn,i
do not contain any samples from Vi .

Lemma 3. limn→∞ Pr[E3n ] = 1.
Proof. Similarly to Lemma 2, it is sufficient to show that
limn→∞ Pr[E3n |E1n ] = 0. We shall upper bound the probability that Knβ > αMn assuming that E1n holds. To this
end, we compute the expectation of Knβ and apply Markov’s
inequality.
For every 1 ≤ i ≤ Mn , denote by Ii the indicatorP
variable
Mn
β
for the event that Bn,i
∩Vi = ∅. Observe that Knβ = i=1
Ii .
For n large enough we have that

E[Ii ] = Pr[Ii = 1] =
≤

1−

β d ζd



1−

r(n)
2+θ

β
|Bn,i |

|Ti |

|F |

Lemma 4. For n large enough, if the events E2n , E3n occur,
then c(σn ) ≤ (1 + ε)c∗ .
Proof. As E2n ∧ E3n we may define the sequence of vertices
Xj1 , . . . , XjMn ∈ V , such that Xj1 = s, XjMn = t, and for
β
β
every 1 < i < Mn , Xji ∈ Vi ∩ Bn,i
if Vi ∩ Bn,i
6= ∅, and
Xji ∈ Vi ∩ Bn,i otherwise.
Denote by σn0 the path induced by concatenating those
points, and notice that it is collision free by definition of
Bn,i and σε . Next, we claim that the cost of the path σn
∗
0
obtained by RRT
PMn is upper-bounded by the cost of σn , which
is equal to
i=2 kXji − Xji−1 k. Consider iteration ji of
i
= Xji ,
RRT∗ , for 1 < i ≤ Mn and observe that (i) xjnew
ji
(ii) Xji−1 ∈ Xnear . By Claim 1, it follows that COST(Xji ) ≤
Pi
0
k=2 kXj(k) − Xj(k−1) k, as desired. Thus, c(σn ) ≤ c(σn ).
0
We proceed to bound c(σn ). Observe that for any 1 < i ≤
Mn it holds that kXji − Xji−1 k is at most
 θr(n)

 2+θ +
θr(n)
+
2+θ

 θr(n) +
2+θ

βr(n)
+ βr(n)
,
2+θ
2+θ
βr(n)
r(n)
+
,
2+θ
2+θ
r(n)
+ r(n)
,
2+θ
2+θ

β
β
AND Xji ∈ Bn,i
if Xji−1 ∈ Bn,i−1
β
β
if Xji−1 ∈ Bn,i−1 XOR Xji ∈ Bn,i .
otherwise

Thus,
c(σn0 ) ≤

Mn
X

kXji − Xji−1 k

i=2

≤ (Mn − 1) θr(n)
+ d(1 − α)(Mn − 1)e 2βr(n)
2+θ
2+θ
θ + 2β + 2α
2r(n)
+ bα(Mn − 1)c 2+θ ≤ (Mn − 1)r(n)
2+θ
 ∗ θ + 2β + 2α
c(σε )(2 + θ)
θ + 2β + 2α
ε
≤
r(n)
≤ 1+ 4 c ·
θr(n)
2+θ
θ
2θε
2θε


θ
+
+
2
16
16
= 1 + 4ε c∗
< 1 + 4ε c∗
θ


 ∗
2
ε
= 1 + 2ε + ε16 c∗ < 1 + 2ε + 16
c < (1 + ε)c∗ .

It remains to show that E2 ∧ E3 occurs with probability
approaching 1:
lim Pr[E2 ∧ E3 ] = 1 − lim Pr[E2 ∨ E3 ]
n→∞


≥ 1 − lim Pr[E2 ] + Pr[E3 ] = 1.

d !n(1−µ)/Mn

n→∞

|F |

n→∞

n
o
d
d+1
θζd (1−µ)
≤ exp − c(σβ )(2+θ)
d+1 |F | · n · r(n)
ε
n
o
d
d+1
θζd (1−µ)
≤ exp − c(σβ )(2+θ)
· log n
d+1 |F | · γ
ε
n
o
d
βd
= exp − d+1 log n = n−β /(d+1) .

PMn
d
Thus, E[Knβ ] = i=1
E[Ii ] ≤ Mn n−β /(d+1) . By Markov’s
inequality, it follows that
Pr[Knβ > αMn ] ≤

β
E[Kn
]
αMn

d

≤

Mn n−β /(d+1)
αMn

=

d
n−β /(d+1)
.
α

(8)

As α is fixed, the last expression tends to 0 as n tends to
∞. While the upper bound obtained in (8) is sufficient for
our purpose, we mention that a tighter bound can be derived
by using a slightly more complex Poissonization argument
similar to that used in [10].
Next, we show that if E2 , E3 occur simultaneously, then
the cost of c(σn ) is bounded by (1 + ε)c∗ .

V. C ONCLUSION
In this paper we revisited the original asymptoticoptimality proof of RRT∗ in [1], and discussed an apparent
logical gap within it. We then introduced an alternative proof
that amends this logical gap. Our new proof suggests that
the connection radius of RRT∗ should be slightly larger than
the original bound on the radius that was developed in [1].
We leave the question of whether our bound is tight, i.e.,
whether the exponent of 1/(d + 1) in Equation (2) can be
lowered to 1/d, to future research. The practical successes
of the algorithm and its extensions, using the exponent 1/d,
provide some evidence that this might be the case.
ACKNOWLEDGMENTS
We thank Sertac Karaman for insightful discussions on his
work [1]. We also thank Michal Kleinbort for feedback on
the manuscript. This work was supported in part by NSF,
Award Number: 1931815.

A PPENDIX
We provide a detailed counter example (Figures 2-12)
illustrating our argument that the fact that for every 1 ≤ i <
Mn (i) there exist Xji , Xji+1 such that Xji ∈ Bn,i , Xji+1 ∈
Bn,i+1 and (ii) ji < ji+1 , does not necessarily mean that
(iii) there exists a sequence j1 ≤ j2 ≤ . . . ≤ jMn such that
Xji ∈ Bn,i for every 1 ≤ i < Mn (see Section III-B).
R EFERENCES
[1] S. Karaman and E. Frazzoli, “Sampling-based algorithms for optimal
motion planning,” International Journal of Robotics Research, vol. 30,
no. 7, pp. 846–894, 2011. 1, 2, 3, 4, 5, 6
[2] ——, “Optimal kinodynamic motion planning using incremental
sampling-based methods,” in IEEE Conference on Decision and Control, 2010, pp. 7681–7687. 1
[3] ——, “Sampling-based optimal motion planning for non-holonomic
dynamical systems,” in IEEE International Conference on Robotics
and Automation, 2013, pp. 5041–5047. 1
[4] G. Goretkin, A. Perez, R. Platt, and G. Konidaris, “Optimal samplingbased planning for linear-quadratic kinodynamic systems,” in IEEE
International Conference on Robotics and Automation, 2013, pp.
2429–2436. 1
[5] D. J. Webb and J. van den Berg, “Kinodynamic RRT*: Optimal
motion planning for systems with linear differential constraints,” in
IEEE International Conference on Robotics and Automation, 2013,
pp. 5054–5061. 1
[6] L. I. Reyes Castro, P. Chaudhari, J. Tumova, S. Karaman, E. Frazzoli,
and D. Rus, “Incremental sampling-based algorithm for minimumviolation motion planning,” in IEEE Conference on Decision and
Control, 2013, pp. 3217–3224. 1
[7] W. Liu and M. H. Ang, Jr., “Incremental sampling-based algorithm for
risk-aware planning under motion uncertainty,” in IEEE International
Conference on Robotics and Automation, 2014, pp. 2051–2058. 1
[8] B. Akgun and M. Stilman, “Sampling heuristics for optimal motion
planning in high dimensions,” in IEEE/RSJ International Conference
on Intelligent Robots and Systems, 2011, pp. 2640–2645. 1
[9] J. D. Gammell, S. S. Srinivasa, and T. D. Barfoot, “Informed RRT*:
Optimal sampling-based path planning focused via direct sampling
of an admissible ellipsoidal heuristic,” in IEEE/RSJ International
Conference on Intelligent Robots and Systems, 2014, pp. 2997–3004.
1
[10] L. Janson, E. Schmerling, A. A. Clark, and M. Pavone, “Fast marching
tree: A fast marching sampling-based method for optimal motion
planning in many dimensions,” International Journal of Robotics
Research, vol. 34, no. 7, pp. 883–921, 2015. 2, 4, 6
[11] J. J. Kuffner and S. M. LaValle, “RRT-Connect: An efficient approach
to single-query path planning,” in IEEE International Conference on
Robotics and Automation, 2000, pp. 995–1001. 2
[12] K. Solovey, L. Janson, E. Schmerling, E. Frazzoli, and M. Pavone,
“Revisiting the asymptotic optimality of RRT,” CoRR, vol.
abs/1909.09688, 2019.
[13] K. Solovey and M. Kleinbort, “The critical radius in sampling-based
motion planning,” International Journal of Robotics Reseasrch, 2019.
4, 5
[14] M. Kleinbort, K. Solovey, Z. Littlefield, K. E. Bekris, and D. Halperin,
“Probabilistic completeness of RRT for geometric and kinodynamic
planning with forward propagation,” IEEE Robotics and Automation
Letters, 2019. 5

s

t

Fig. 2. The input scenario for the counter example. The goal is to find a path from configuration s on the left to t on the right, while avoiding the two
gray obstacles. The path σε is drawn as a black curve.

X1

s

X2

t= X3

Fig. 3. The first three samples X1 , X2 , X3 are drawn by the algorithm, where X3 = t. The edge (s, X1 ) is added first through line 5 of Algorithm 2.
The edges (X1 , X2 ), (X2 , X3 ) are added in a similar fashion. We assume that no rewiring occurs in those steps due to the smaller magnitude of r1
in comparison to kX1 − X3 k. We also mention that the length of the new path to t just discovered can be made arbitrarily long with respect to σε by
moving X1 , X2 further away from s and t respectively, and setting the steering parameter η to be large enough to support such long connections.

X1

X2

b8

b1

b9

b10

b11

s

t= X3

b2

b12

b7

b3

b4

b5

b6

Fig. 4. We construct a sequence of Mn balls Bn,1 , . . . , Bn,Mn , which we denote for simplicity by b1 , . . . , bMn , and we fix n = 23. For simplicity,
we set Mn = 12 in the illustration, to avoid unnecessarily complicating the visualization. Below we also assume that the connection radius r23 used by
RRT∗ is equal to the diameter of any ball bi , although a similar out come will follow when r23 is much larger (as long as r23 < kX2 − X3 k).

X1

X2

b8

b1

b9

b10

b11

s

t= X3

b2

b7

b12

X4

b3

b4

b5

b6

Fig. 5. Next, we generate the sample X4 ∈ b11 , which introduces the edge (X3 , X4 ) and does not result in rewiring. As we mentioned earlier, we
assume that rn < kX2 − X3 k, which implies that the edge (X2 , X4 ) will not be considered.

X1

X2

b8

b1

b9

b10

b11

s

t= X3

b2

X4

b7

b12

X5

b3

b4

b5

b6

Fig. 6. Next, we generate the sample X5 ∈ b12 , which introduces the edge (X4 , X5 ), since the cost-to-come via X3 is smaller than through a
connection from X4 . Clearly, the edge (X5 , X4 ) cannot improve the cost-to-come of X4 , and it is therefore not added in the rewiring stage. Observe that
X4 ∈ b11 , X5 ∈ b12 , and X4 was sampled before X5 , which implies that conditions (i), (ii) are satisfied locally for b11 , b12 .

X1

X2

b8

b1

b9

b10

b11

s

t= X3

b2

b7

X6

X4

b12

X5

b3

b4
Fig. 7.

b5

b6

Similarly to X4 , the sample X6 is produced in b10 , which yields the edge (X4 , X6 ), and introduces no rewiring.

X1

X2

b8

b1

b9

b10

b11

s

t= X3

b2

b7

X6

b12

X4 X7 X
5

b3

b4
Fig. 8.

b5

b6

Similarly to X5 , the sample X7 is produced in b11 , and the edge (X4 , X6 ) is left intact.

X1

X2

b8

b1

b9

b10

s

b11

t= X3

b12

X9
b2

X8

b7

X6

X4 X7 X
5

b3

b4

b5

b6

Fig. 9. In a similar manner, we introduce incrementally the samples X8 ∈ b9 , X9 ∈ b10 . Notice that a path from t in the opposite direction of the balls
towards s (currently till X8 ) is beginning to form.

X1

X2

b8

b9

b10

b11

X11

s

b1

t= X3
X13

b2
X22

X10
X15 X8 b
7
X12

b3
X20

X18

X19
X21
X16
b4
b5

b12

X9
X6

X4 X7 X
5

X17

X14
b6

Fig. 10. This sample scheme can be repeated until the sample X22 ∈ b2 is produced, which is within r23 distance from s. This introduces the edge
(s, X22 ), which minimizes the cost-to-come to X22 .

X1

X2

b8

b9

b10

b11

X11

s

b1

t= X3
X13

b2
X22

X10
X15 X8 b
7
X12

b3
X20

X18

X19
X21
X16
b4
b5

b12

X9

X14

X6

X4 X7 X
5

X17

b6

Fig. 11. The introduction of X22 forces a rewire of X20 and X18 within the same iteration: the edges (X22 , X20 ), (X22 , X18 ) are added, whereas
(X18 , X20 ) and (X16 , X18 ) are removed. It is important to note that by definition of RRT∗ , this rewire does not promote further rewiring to the
predecessors of X18 , X20 in G.

X1

X2

b8

b9

b10

b11

X11

s

b1

t= X3
X13

b2
X22

X23

X10
X15 X8 b
7
X12

b3
X20

X18

X19
X21
X16
b4
b5

b12

X9

X14

X6

X4 X7 X
5

X17

b6

Fig. 12. Finally, the sample X23 ∈ b3 is drawn, and the edge (s, X23 ) is added. This will promote additional rewires to X16 , X19 , X18 , X21 ,in the
vicinity of X23 , although as earlier those rewires will not propagate to other vertices of G. It is clear that at this point for every 1 ≤ i < Mn it holds
that there exist (i) Xji ∈ bi , Xji+1 ∈ bi+1 and (ii) ji < ji+1 . Unfortunately, the graph G does not contain a path starting at s and going sequentially
through the balls b1 , . . . , b12 until t is reached. To conclude, even though conditions (i), (ii) hold, RRT∗ will return the path consisting of the vertices
s, X1 , X2 , t, which is substantially longer than σε .

