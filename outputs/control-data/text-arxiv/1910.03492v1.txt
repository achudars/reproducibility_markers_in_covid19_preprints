Neural Language Priors
Joseph Enguehard
Dan Busbridge
Vitalii Zhelezniak
Nils Hammerla
firstname.lastname@babylonhealth.com
Babylon Health, 60 Sloane Ave, Chelsea, London SW3 3DD

arXiv:1910.03492v1 [cs.CL] 4 Oct 2019

Abstract
The choice of sentence encoder architecture
reflects assumptions about how a sentences
meaning is composed from its constituent
words. We examine the contribution of these
architectures by holding them randomly initialised and fixed, effectively treating them as
as hand-crafted language priors, and evaluating the resulting sentence encoders on downstream language tasks. We find that even when
encoders are presented with additional information that can be used to solve tasks, the corresponding priors do not leverage this information, except in an isolated case. We also
find that apparently uninformative priors are
just as good as seemingly informative priors
on almost all tasks, indicating that learning is a
necessary component to leverage information
provided by architecture choice.

1

Introduction

Sentence representations are fixed-length vectors
that encode sentence properties and allow models to learn across many Natural Language Processing (NLP) tasks. These representations enable
learning procedures to focus on the training signal
from specific “downstream” NLP tasks (Conneau
and Kiela, 2018), circumventing the often limited
amount of labelled data. Naturally, sentence representations that can effectively encode semantic
and syntactic properties into a representations are
highly sought after, and are a cornerstone of modern NLP systems.
In practice, sentence representations are formed
by applying an encoding function (or encoder)
provided by a Neural Network (NN) architecture,
to the word vectors of the corresponding sentence.
Encoders have been successfully trained to predict the context of sentence (Kiros et al., 2015; Ba
et al., 2016), or to leverage supervised multi-task
objectives (Conneau et al., 2017; Dehghani et al.,
2018).

The choice of encoder architecture asserts an
inductive bias (Battaglia et al., 2018), and reflects assumptions about the data-generating process. Different encoders naturally prioritise one
solution over another (Mitchell, 1991), independent of the observed data, trading sample complexity for flexibility (Geman et al., 2008). Given
that NNs, which are able to generalise well, can
also overfit when presented with random labels
(Zhang et al., 2016), we expect that architecture
plays a dominant role in generalisation capability
(Lempitsky et al., 2018).
The inductive biases of encoder architectures
reflect assumptions about how a sentence’s meaning is composed from its constituent words. A
plethora of architectures have been investigated,
each designed with a specific set of inductive biases in mind. Bag of Embeddings (BOE) architectures disregard word order (Harris, 1954;
Salton et al., 1975; Manning et al., 2008), Recurrent Neural Network (RNN) architectures can
leverage word positional information (Kiros et al.,
2015; Ba et al., 2016), Convolutional Neural Network (CNN) architectures compose information
at the n-gram level (Collobert et al., 2011; Vieira
and Moura, 2017; Gan et al., 2016), self-attention
models leverage explicit positional information
with long range context (Vaswani et al., 2017;
Ahmed et al., 2017; Shaw et al., 2018; Dehghani
et al., 2018; Radford et al., 2019; Devlin et al.,
2018; Cer et al., 2018), and graph-based models can exploit linguistic structures extracted by
traditional NLP methods (Tai et al., 2015; Li
et al., 2018; Zhang et al., 2019; Teng and Zhang,
2016; Kim et al., 2018a; Ahmed et al., 2019;
Bastings et al., 2017; Marcheggiani and Titov,
2017; Marcheggiani et al., 2018; Marcheggiani
and Perez-Beltrachini, 2018). This list is far from
exhaustive.
Given the critical role of encoder architectures

in NLP, we set out to examine their contribution
to downstream task performance independent of
biases induced by learning processes. We find
that even architectures expected to have extremely
strong language priors yield almost no gains when
compared to architectures that are equipped with
apparently uninformative priors, consistent with
the results found in Wieting and Kiela (2019).
This suggests that for NLP tasks, relying on the
prior is insufficient, and the learning process is
necessary, in contrast to what was found in the
vision field (Lempitsky et al., 2018). In short, although there are known strong inductive biases for
language, there is no best language prior, and in
practice there is surprisingly little correspondence
between the two.
To show this, given a set of pre-trained word
embeddings, we evaluate the classification accuracy of a variety architectures on a set of NLP
tasks, only updating the parameters specific to the
task, holding the parameters of the architecture
fixed at their random initialisation.

2

Method

2.1

Priors from Random Sentence Encoders

The line of investigation we take follows Wieting and Kiela (2019) closely. We treat randomly
initialized NNs as handcrafted priors for how the
meaning of a sentence is composed from its constituent words. Concretely, let each word w have
a pre-trained and fixed D-dimensional word representation ew ∈ RD . Consider a sentence S consisting of TS words S = w1 , . . . , wTS . Using an
encoding function fenc , the meaning of the sentence is distilled into a sentence representation hS :
hS (θ) = fenc (e1 , . . . , eTS ; θ),

(1)

where θ are the parameters of the encoding function. For NN architectures that output a matrix
0
HS ∈ RTS ×D , where D0 is an output dimensionality and TS is a temporal dimensionality1 , we
pool along the temporal dimension using a pooling function fpool . For our main results we use
0
max pooling hS = fpool (HS ) = max(HS ) ∈ RD
throughout, as it has been successful in InferSent
(Conneau et al., 2017).
The θ are typically learned using e.g. Maximum Likelihood Estimation (MLE) on sentence
1

In practice TS may not directly correspond to the length
of the input sentence due to e.g. finite kernel sizes in convolution operations.

context, resulting in hS (θ) representing a sample
from the encoder’s posterior over functions applied to S given a corpus. Instead of learning θ,
we simply sample θ from its own prior. hS (θ)
then represents a sample from the encoder’s prior
over functions applied to S.
For each encoding function, we take multiple
samples of θ. For each sample, the resulting encoder function is used to produce sentence embeddings for a set of downstream tasks. These downstream tasks are the supervised transfer tasks of
the SentEval (Conneau and Kiela, 2018) framework, where the transfer model is a simple logistic regression model or a MLP2 . Combining the
results from multiple samples then gives a performance estimate of each encoder’s prior.
2.2

BOREPs, Random LSTMs and ESNs

We take the architectures investigated in (Wieting
and Kiela, 2019) as a starting point: Bag of Random Embedding Projections (BOREP), Random
Long Short Term Memory (LSTM) Neworks and
Echo State Networks (ESNs). BOREP is simply a random projection of word embeddings to
a higher dimension, RandLSTM is a randomly
initialised bi-directional LSTM (Hochreiter and
Schmidhuber, 1997), and ESN is a hypertuned
randomly initialised bi-directional ESN (Jaeger,
2001). For more details please see (Wieting and
Kiela, 2019).
2.3

Random CNNs

Although CNNs are more famously used in the
image domain (Simonyan and Zisserman, 2014;
He et al., 2015), they have also enjoyed much success as sentence encoders (Collobert et al., 2011;
Vieira and Moura, 2017; Gan et al., 2016). A temporal one-dimensional convolution is performed
0
by applying a D0 -channel filter W ∈ RD×k×D
to a window of k words and a bias added. This
weight W is initialised uniformly at random from
[− √1d , √1d ], where d is the word embedding dimension. The representation hS is then obtained
by pooling
0

hS = fpool [CNN(e1 , . . . , eTS )] ∈ RD .

(2)

Note that using a window size k = 1 corresponds
to BOREP.
2
For emphasis: the parameters of these logistic regression
model and MLP are updated by the task.

Model
BOE

Dim MR

†
†

BOREP
BOREP (ours)
RandLSTM†
RandLSTM (ours)
ESN†
ESN (ours)
CNN Window = 3
CNN Window = 4
Self-Attention
TreeLSTM

CR

MPQA SUBJ

SST2

TREC

SICK-E MRPC

300

77.3(.2) 78.6(.3) 87.6(.1) 91.3(.1) 80.0(.5)

81.5(.8)

78.7(.1) 72.9(.3)

4096
4096
4096
4096
4096
4096
4096
4096
4096
4096

77.4(.4)
75.3(.2)
77.2(.3)
76.9(.2)
78.1(.3)
70.4(.1)
74.9(.3)
74.3(.3)
68.0(.3)
75.6(.2)

88.8(.3)
88.5(1.3)
86.5(1.1)
89.2(.4)
87.9(1.0)
88.9(1.2)
88.7(1.2)
85.2(1.1)
84.9(1.3)
90.3(.7)

82.7(.7)
82.1(.2)
81.8(.5)
81.7(.5)
83.1(.4)
78.4(.3)
79.1(.2)
78.0(.2)
73.7(.7)
80.7(.9)

79.5(.2)
78.2(.5)
78.7(.5)
80.9(.3)
80.0(.6)
76.9(.8)
76.9(.7)
74.8(.8)
77.1(.5)
78.5(.3)

88.3(.2)
88.5(.2)
87.9(.1)
88.7(.1)
88.5(.2)
86.3(.1)
85.4(.2)
84.2(.3)
82.0(.5)
87.7(.1)

91.9(.2)
90.3(.4)
91.9(.2)
91.7(.1)
92.6(.1)
88.7(.4)
88.6(.1)
86.8(.3)
90.1(.3)
91.4(.0)

81.8(.4)
79.3(1.1)
81.5(.3)
81.3(.5)
83.0(.5)
76.4(.5)
75.6(.5)
75.5(.5)
78.8(1.2)
79.9(.5)

73.9(.4)
71.8(.7)
74.1(.5)
71.8(.6)
73.4(.4)
67.4(.7)
69.4(.5)
69.2(.3)
67.1(1.1)
71.1(.5)

Table 1: Performance (accuracy) for fpool = max on all eight tasks. The results indicated by † are taken from (Wieting and
Kiela, 2019). Mean (standard deviation) for each model is reported across five seeds. Our ESN was evaluated using a spectral
radius of 1.0, a maximum kernel deviation from 0.0 of 0.1, and a sparsity 0.5, whereas the result from (Wieting and Kiela,
2019) is the best performing model from a hyperparameter search.

2.4

Random Self-Attention

Attention mechanisms have been employed on
many NLP tasks with tremendous success
(Vaswani et al., 2017; Ahmed et al., 2017; Shaw
et al., 2018; Dehghani et al., 2018; Radford et al.,
2019; Devlin et al., 2018; Cer et al., 2018). Selfattention in particular has enabled the incorporation of incredibly long ranged contexts, as well
as hierarchical contextualisations of word embeddings within a highly parallel setting.
In our random setting, the word embeddings
e1 , . . . , eTS are first projected up to a D0 dimensional space. We then optionally add sinusoidal
positional encodings (Vaswani et al., 2017). We
then apply two layers of random self-attention
with residual connections, each followed by layer
normalisation. A single head of a self-attention
layer produces new embeddings for each query
representations q ∈ Rdk out of the value repre0
sentations vi ∈ RD , controlled by the key representations ki ∈ Rdk
q0 =

TS
X


p 
exp qT ki / dk vi /constant. (3)

i=1

The dk -dimensional key and query representations
are given by independent random projections acting upon the self-attention layer input. We use
eight heads of attention in each layer. The pooling function is applied to this output to produce
the sentence representation hS .
We keep the default initialisation of the FairSeq
implementation, which is Xavier uniform (Glorot and Bengio, 2010) for the weights of the selfattention layer.

2.5

Random TreeLSTMs

The final architecture we consider is the
TreeLSTM. This architecture is particularly
interesting as it can potentially incorporate
syntactic information into the sentence representations (Tai et al., 2015; Li et al., 2018; Zhang
et al., 2019; Teng and Zhang, 2016; Kim et al.,
2018a; Ahmed et al., 2019).
We specifically consider the Binary Constituency TreeLSTM (Tai et al., 2015). This differs from a regular LSTM by having a two forget
gates - one for each child node given by the structure of the parsed sentence.
Word representations are first presented to a
random bi-directional LSTM of combined dimensionality D0 to provide contextualised representations E0S ∈ RTS ×D
E0S = BiLSTM(e1 , . . . , eTS ).

(4)

The contextualised representations are then presented to a random TreeLSTM, whose outputs are
pooled to produce the sentence representation


hS = fpool TreeLSTM(E0S ) .
(5)
Both weights of the bi-directional LSTM and the
TreeLSTM are initialised uniformly at random
from [− √1d , √1d ]. We used the Stanford parser
(Manning et al., 2014) to parse each sentence.
Punctuation and special characters were removed,
and numbers were only kept if they formed an independent word and were not part of a mixed word
of letters and numbers. Then, in the length of a
word was reduced to zero, the word was replaced
with a placeholder * character. After parsing, the
prepossessing described in (Kim et al., 2018a) was
used to compute the parse tree for the TreeLSTM.

CR
MR

MPQA
SUBJ

SST2
TREC

MRPC
SICKE

90

90

90

90

90

90

85

85

85

85

85

85

80

80

80

80

80

80

75

75

75

75

75

75

70

70

70

70

70

70

65

65

65

65

65

65

60

60

60

60

60

60

1024

BOREP

1024

CNN Window = 3

1024

1024

CNN Window = 4

1024

Self-Attention Self-Attention + PosEnc

1024

TreeLSTM

Figure 1: Performance (accuracy) for fpool = max on all eight tasks across five seeds. We observe: 1) Almost every encoder
architecture performs at best, similarly to the relatively uninformative BOREP, and at worst, much worse. 2) Taking BOREP
as CNN with a window size of 1, we note that increasing CNN window size impairs performance. This indicates that any
gains to be made from employing n-grams over word representations as a basis for distilling meaning needs to be learned.
3) The performance of the Self-Attention Network with and without positional encoding is fairly similar. This indicates that
although the encoder architecture has positional information available, the transfer model cannot learn to use it. It would be
interesting to look at the BShift task to probe this directly (Conneau et al., 2018). 4) Random Self-Attention networks perform
poorly even though they form a cornerstone of modern state of the art NLP systems. Considering Equation (3), we see that
the random contextualisation can be any linear combination of the input, with none selected by an inductive bias. There is no
reason to expect this random combination to outperform BOREP. 5) The TreeLSTM performs noticeably better than other
encoder architectures on TREC, a question-type task which relies heavily on sentence syntax to solve (Li and Roth, 2002). It
appears that in this instance, the encoder may be using the syntactic information available, however, its performance on all other
tasks is comparable to BOREP.

2.6

Evaluation

The SentEval tasks we evaluate on are sentiment
analysis (MR, SST), question-type (TREC), product review (CR), subjectivity (SUBJ), opinion polarity (MPQA), paraphrasing (MRPC), and entailment (SICK-E). We use the default SentEval settings defined in (Conneau and Kiela, 2018). We
evaluate for five samples (seeds) per architecture
per task.
We follow the FairSeq implementation (Ott
et al., 2019) to build our CNN and self-attention
networks. We also follow the implementation of
(Kim et al., 2018b) without the structure-aware tag
representations to build our TreeLSTMs.

3

Results

Our investigation is concerned with the priors of
encoder architectures, rather than the posteriors
they may learn from data; we only compare untrained encoders acting upon word embeddings.
Table 1 contains the performance of architectures discussed in Section 2 at dimensionality
4096 on the selected SentEval tasks, together with
the results from Wieting and Kiela (2019). Fig-

ure 1 contains the performance for these architectures across a range of dimensionalities.
As a sanity check, we evaluated BOREP and
CNN with a window size of 1 and found the performance indistinguishable.
In general, we find that even if encoders have inductive biases that present additional information
that can be used to solve a task, the corresponding
priors do not leverage this information, except in
an isolated case. This strongly indicates that learning is an essential component of building encoder
architectures if any gains are to be made beyond
apparently uninformative priors.

4

Conclusion

We have evaluated randomly initialised architectures to measure the contribution of priors in distilling sentence meaning. We find that apparently
uninformative priors are just as good as seemingly
informative priors on almost all tasks, indicating
that learning is a necessary component to leverage
information provided by architecture choice.

5

Acknowledgements

We thank Jeremie Vallee for assistance with the
experimental setup, and the wider machine learning group at Babylon for useful comments and
support throughout this project.

References
Karim Ahmed, Nitish Shirish Keskar, and Richard
Socher. 2017. Weighted Transformer Network for
Machine Translation. pages 1–10.
Mahtab Ahmed, Muhammad Rifayat Samee, and
Robert E. Mercer. 2019. Improving Tree-LSTM
with Tree Attention. In 2019 IEEE 13th International Conference on Semantic Computing (ICSC),
pages 247–254. IEEE.
Jimmy Lei Ba, Ryan Kiros, and Geoffrey E. Hinton.
2016. Layer Normalization.
Joost Bastings, Ivan Titov, Wilker Aziz, Diego
Marcheggiani, and Khalil Sima’an. 2017. Graph
Convolutional Encoders for Syntax-aware Neural
Machine Translation. pages 1957–1967.
Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst,
Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer,
George Dahl, Ashish Vaswani, Kelsey Allen,
Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt
Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. 2018. Relational inductive biases, deep learning, and graph networks. Under Review, pages 1–
37.
Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,
Nicole Limtiaco, Rhomni St. John, Noah Constant,
Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,
Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil.
2018. Universal Sentence Encoder.
Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural Language Processing (almost) from
Scratch.
Alexis Conneau and Douwe Kiela. 2018. SentEval: An
Evaluation Toolkit for Universal Sentence Representations.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic
Barrault, and Antoine Bordes. 2017. Supervised
Learning of Universal Sentence Representations
from Natural Language Inference Data.
Alexis Conneau, German Kruszewski, Guillaume
Lample, Loı̈c Barrault, and Marco Baroni. 2018.
What you can cram into a single vector: Probing
sentence embeddings for linguistic properties.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals,
Jakob Uszkoreit, and Łukasz Kaiser. 2018. Universal Transformers. pages 1–23.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: Pre-training of
Deep Bidirectional Transformers for Language Understanding.
Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li,
Xiaodong He, and Lawrence Carin. 2016. Learning
Generic Sentence Representations Using Convolutional Neural Networks. Emnlp, pages 2380–2390.
Stuart Geman, Elie Bienenstock, and René Doursat.
2008. Neural Networks and the Bias/Variance
Dilemma. Neural Computation, 4(1):1–58.
Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. Proceedings of the Thirteenth International Conference on Artificial Intelligence and
Statistics (AISTATS-10), 9:249–256.
Zellig S. Harris. 1954.
Distributional Structure.
WORD, 10(2-3):146–162.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Deep Residual Learning for Image
Recognition.
Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long Short-Term Memory. Neural Computation,
9(8):1735–1780.
Herbert Jaeger. 2001. The echo state approach to
analysing and training recurrent neural networks with an Erratum note. Technical Report 148.
Sungwon Kim, Sang-gil Lee, Jongyoon Song, and Sungroh Yoon. 2018a. FloWaveNet : A Generative
Flow for Raw Audio.
Taeuk Kim, Jihun Choi, Daniel Edmiston, Sanghwan
Bae, and Sang-goo Lee. 2018b. Dynamic Compositionality in Recursive Neural Networks with
Structure-aware Tag Representations.
Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,
Richard S. Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. 2015. Skip-Thought Vectors.
Victor Lempitsky, Andrea Vedaldi, and Dmitry
Ulyanov. 2018. Deep Image Prior. In 2018
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9446–9454. IEEE.
Xin Li and Dan Roth. 2002. Learning question classifiers. In Proceedings of the 19th international conference on Computational linguistics -, volume 1,
pages 1–7, Morristown, NJ, USA. Association for
Computational Linguistics.
Yu Li, Peter Richtarik, Lizhong Ding, and Xin Gao.
2018. On the Decision Boundary of Deep Neural
Networks.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The Stanford CoreNLP Natural Language
Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational
Linguistics: System Demonstrations, pages 55–60,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schutze. 2008. Introduction to Information
Retrieval.
Diego Marcheggiani, Joost Bastings, and Ivan Titov.
2018. Exploiting Semantics in Neural Machine
Translation with Graph Convolutional Networks.
Diego Marcheggiani and Laura Perez-Beltrachini.
2018. Deep Graph Convolutional Encoders for
Structured Data to Text Generation. pages 1–9.
Diego Marcheggiani and Ivan Titov. 2017. Encoding
Sentences with Graph Convolutional Networks for
Semantic Role Labeling. pages 1506–1515.
Tom Mitchell. 1991. The need for biases in learning generalisations. Readings in Machine Learning,
(May).
Myle Ott, Sergey Edunov, Alexei Baevski, Angela
Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. 2019. fairseq: A Fast, Extensible
Toolkit for Sequence Modeling.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
Models are Unsupervised Multitask Learners. Open
AI.
G. Salton, A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Communications of the ACM, 18(11):613–620.
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
2018. Self-Attention with Relative Position Representations.
Karen Simonyan and Andrew Zisserman. 2014. Very
Deep Convolutional Networks for Large-Scale Image Recognition. pages 1–14.
Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks.
Zhiyang Teng and Yue Zhang. 2016. Bidirectional
Tree-Structured LSTM with Head Lexicalization.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention Is All
You Need.

Joao Paulo Albuquerque Vieira and Raimundo Santos
Moura. 2017. An analysis of convolutional neural
networks for sentence classification. In 2017 XLIII
Latin American Computer Conference (CLEI), volume 2017-Janua, pages 1–5. IEEE.
John Wieting and Douwe Kiela. 2019. No Training Required: Exploring Random Encoders for Sentence
Classification. pages 1–16.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. 2016. Understanding deep learning requires rethinking generalization.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, and
Yoram Singer. 2019. Identity Crisis: Memorization
and Generalization under Extreme Overparameterization. pages 1–28.

