Robust Attribution Regularization

arXiv:1905.09957v3 [cs.LG] 26 Oct 2019

Jiefeng Chen ∗ 1 Xi Wu ∗ 2 Vaibhav Rastogi †2
1
University of Wisconsin-Madison

2

Yingyu Liang 1
Somesh Jha 1,3
3
Google
XaiPient

Abstract
An emerging problem in trustworthy machine learning is to train models that produce robust interpretations for their predictions. We take a step towards solving
this problem through the lens of axiomatic attribution of neural networks. Our
theory is grounded in the recent work, Integrated Gradients (IG) [STY17], in
axiomatically attributing a neural network’s output change to its input change.
We propose training objectives in classic robust optimization models to achieve
robust IG attributions. Our objectives give principled generalizations of previous
objectives designed for robust predictions, and they naturally degenerate to classic
soft-margin training for one-layer neural networks. We also generalize previous
theory and prove that the objectives for different robust optimization models are
closely related. Experiments demonstrate the effectiveness of our method, and
also point to intriguing problems which hint at the need for better optimization
techniques or better neural network architectures for robust attribution training.

1

Introduction

Trustworthy machine learning has received considerable attention in recent years. An emerging
problem to tackle in this domain is to train models that produce reliable interpretations for their
predictions. For example, a pathology prediction model may predict certain images as containing
malignant tumor. Then one would hope that under visually indistinguishable perturbations of an
image, similar sections of the image, instead of entirely different ones, can account for the prediction. However, as Ghorbani, Abid, and Zou [GAZ17] convincingly demonstrated, for existing
models, one can generate minimal perturbations that substantially change model interpretations,
while keeping their predictions intact. Unfortunately, while the robust prediction problem of machine learning models is well known and has been extensively studied in recent years (for example,
[MMS+ 17a, SND18, WK18], and also the tutorial by Madry and Kolter [KM18]), there has only
been limited progress on the problem of robust interpretations.
In this paper we take a step towards solving this problem by viewing it through the lens of axiomatic attribution of neural networks, and propose Robust Attribution Regularization. Our theory
is grounded in the recent work, Integrated Gradients (IG) [STY17], in axiomatically attributing a
neural network’s output change to its input change. Specifically, given a model f , two input vecx, x 0 ) defines a path integration (parameterized by a curve
tors x , x 0 , and an input coordinate i, IGfi (x
from x to x 0 ) that assigns a number to the i-th input as its “contribution” to the change of the model’s
x) to f (x
x0 ). IG enjoys several natural theoretical properties (such as the Axiom of
output from f (x
3
Completeness ) that other related methods violate.
∗
†

Equal contribution.
Work done while at UW-Madison.

Due to lack of space and for completeness, we put some definitions (such as coupling) to Section B.1. Code for this paper is publicly available at the following repository: https://github.com/jfc43/
robust-attribution-regularization
3
x0 ) − f (x
x).
Axiom of Completeness says that summing up attributions of all components should give f (x
33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

NATURAL

IG-NORM

Top-1000 Intersection: 0.1%
Kendall’s Correlation: 0.2607

IG-SUM-NORM

Top-1000 Intersection: 58.8%
Kendall’s Correlation: 0.6736

Top-1000 Intersection: 60.1%
Kendall’s Correlation: 0.6951

Figure 1: Attribution robustness comparing different models. Top-1000 Intersection and
Kendall’s Correlation are rank correlations between original and perturbed saliency maps. NATURAL is the naturally trained model, IG-NORM and IG-SUM-NORM are models trained using
our robust attribution method. We use attribution attacks described in [GAZ17] to perturb the attributions while keeping predictions intact. For all images, the models give correct prediction –
Windflower. However, the saliency maps (also called feature importance maps), computed via IG,
show that attributions of the naturally trained model are very fragile, either visually or quantitatively
as measured by correlation analyses, while models trained using our method are much more robust
in their attributions.
We briefly overview our approach. Given a loss function ` and a data generating distribution P , our
Robust Attribution Regularization objective contains two parts: (1) Achieving a small loss over the
distribution P , and (2) The IG attributions of the loss ` over P are “close” to the IG attributions
over Q, if distributions P and Q are close to each other. We can naturally encode these two goals
in two classic robust optimization models: (1) In the uncertainty set model [BTEGN09] where we
treat sample points as “nominal” points, and assume that true sample points are from certain vicinity
around them, which gives:
minimize
θ

E

x,y)∼P
(x

x, y; θ)]
[ρ(x

x, y; θ) = `(x
x, y; θ) + λ
where ρ(x

max

x,ε)
x 0 ∈N (x

`

x, x 0 ; r))
s(IGhy (x

`

where IGhy (·) is the attribution w.r.t. neurons in an intermediate layer h , and s(·) is a size function
(e.g., k · k2 ) measuring the size of IG, and (2) In the distributional robustness model [SND18,
MEK15], where closeness between P and Q is measured using metrics such as Wasserstein distance,
which gives:
n
o
minimize E[`(P ; θ)] + λ
sup
E 0 [dIG (Z, Z 0 )] s.t. E 0 [c(Z, Z 0 )] ≤ ρ ,
Q
θ

P

Q;M ∈

(P,Q)

Z,Z

Z,Z

Q
In this formulation, (P, Q) is the set of couplings of P and Q, and M = (Z, Z 0 ) is one coupling.
c(·, ·) is a metric, such as k · k2 , to measure the cost of an adversary perturbing z to z 0 . ρ is an
upper bound on the expected perturbation cost, thus constraining P and Q to be “close” with each
together. dIG is a metric to measure the change of attributions from Z to Q
Z 0 , where we want a large
dIG -change under a small c-change. The supremum is taken over Q and (P, Q).
We provide theoretical characterizations of our objectives. First, we show that they give principled
generalizations of previous objectives designed for robust predictions. Specifically, under weak instantiations of size function s(·), and how we estimate IG computationally, we can leverage axioms
satisfied by IG to recover the robust prediction objective of [MMS+ 17a], the input gradient regularization objective of [RD18], and also the distributional robust prediction objective of [SND18].
These results provide theoretical evidence that robust prediction training can provide some control
over robust interpretations. Second, for one-layer neural networks, we prove that instantiating s(·) as
1-norm coincides with the instantiation of s(·) as sum, and further coincides with classic soft-margin
2

training, which implies that for generalized linear classifiers, soft-margin training will robustify both
predictions and interpretations. Finally, we generalize previous theory on distributional robust prediction [SND18] to our objectives, and show that they are closely related.
Through detailed experiments we study the effect of our method in robustifying attributions. On
MNIST, Fashion-MNIST, GTSRB and Flower datasets, we report encouraging improvement in attribution robustness. Compared with naturally trained models, we show significantly improved attribution robustness, as well as prediction robustness. Compared with Madry et al.’s model [MMS+ 17a]
trained for robust predictions, we demonstrate comparable prediction robustness (sometimes even
better), while consistently improving attribution robustness. We observe that even when our training
stops, the attribution regularization term remains much more significant compared to the natural loss
term. We discuss this problem and point out that current optimization techniques may not have effectively optimized our objectives. These results hint at the need for better optimization techniques
or new neural network architectures that are more amenable to robust attribution training.
The rest of the paper is organized as follows: Section 2 briefly reviews necessary background. Section 3 presents our framework for robustifying attributions, and proves theoretical characterizations.
Section 4 presents instantiations of our method and their optimization, and we report experimental
results in Section 5. Finally, Section 6 concludes with a discussion on future directions.

2

Preliminaries

Axiomatic attribution and Integrated Gradients Let f : Rd 7→ R be a real-valued function, and
x) to f (x
x0 ), a basic
x and x 0 be two input vectors. Given that function values changes from f (x
question is: “How to attribute the function value change to the input variables?” A recent work by
Sundararajan, Taly and Yan [STY17] provides an axiomatic answer to this question. Formally, let
r : [0, 1] 7→ Rd be a curve such that r(0) = x , and r(1) = x 0 , Integrated Gradients (IG) for input
variable i is defined as the following integral:
Z 1
∂f (r(t)) 0
x, x 0 ; r) =
IGfi (x
ri (t)dt,
(1)
∂ xi
0
which formalizes the contribution of the i-th variable as the integration of the i-th partial as we move
x, x 0 ; r) be the vector where the i-th component is IGfi , then IGf satisfies
along curve r. Let IGf (x
some natural axioms. For example, the Axiom of Completeness says that summing all coordinates
P
x, x 0 ; r)) = di=1 IGfi (x
x, x 0 ; r) = f (x
x0 ) − f (x
x). We
gives the change of function value: sum(IGf (x
refer readers to the paper [STY17] for other axioms IG satisfies.
Integrated Gradients for an intermediate layer. We can generalize the theory of IG to an intermediate layer of neurons. The key insight is to leverage the fact that Integrated Gradients is a curve
x)
integration. Therefore, given some hidden layer h = [h1 , . . . , hl ], computed by a function h(x
induced by previous layers, one can then naturally view the previous layers as inducing a curve h ◦ r
x) to h(x
x0 ), as we move from x to x 0 along curve r. Viewed this way, we can
which moves from h(x
thus naturally compute IG for h in a way that leverages all layers of the network4 ,
Lemma 1. Under curve r : [0, 1] 7→ Rd such that r(0) = x and r(1) = x0 for moving x to x0 , and
the function induced by layers before h , the attribution for hi for a differentiable f is

d Z 1
X
∂f (h(r(t))) ∂hi (r(t)) 0
x, x 0 ) =
IGfhi (x
rj (t)dt .
(2)
∂hi
∂ xj
0
j=1
The corresponding summation approximation is:
(m−1
)
d
X
X ∂f (h(r(k/m))) ∂hi (r(k/m))
1
f
x, x 0 ) =
rj0 (k/m)
IGhi (x
m j=1
∂hi
∂ xj

(3)

k=0

3

Robust Attribution Regularization

In this section we propose objectives for achieving robust attribution, and study their connections
with existing robust training objectives. At a high level, given a loss function ` and a data generating
4

Proofs are deferred to B.2.

3

distribution P , our objectives contain two parts: (1) Achieving a small loss over the data generating
distribution P , and (2) The IG attributions of the loss ` over P are “close” to the IG attributions
over distribution Q, if P and Q are close to each other. We can naturally encode these two goals in
existing robust optimization models. Below we do so for two popular models: the uncertainty set
model and the distributional robustness model.
3.1

Uncertainty Set Model

x, y) ∼ P for a data generating distribution P , we
In the uncertainty set model, for any sample (x
think of it as a “nominal” point and assume that the real sample comes from a neighborhood around
x . In this case, given any intermediate layer h , we propose the following objective function:
minimize
θ

E

x,y)∼P
(x

x, y; θ)]
[ρ(x

x, y; θ) = `(x
x, y; θ) + λ
where ρ(x

max
0

x,ε)
x ∈N (x

`

x, x 0 ; r))
s(IGhy (x

(4)

x; θ) =
where λ ≥ 0 is a regularization parameter, `y is the loss function with label y fixed: `y (x
x, y; θ), r : [0, 1] 7→ Rd is a curve parameterization from x to x 0 , and IG`y is the integrated
`(x
gradients of `y , and therefore gives attribution of changes of `y as we go from x to x 0 . s(·) is a size
function that measures the “size” of the attribution.5
We now study some particular instantiations of the objective (4). Specifically, we recover existing
robust training objectives under weak instantiations (such as choosing s(·) as summation function,
which is not metric, or use crude approximation of IG), and also derive new instantiations that are
natural extensions to existing ones.
Proposition 1 (Madry et al.’s robust prediction objective). If we set λ = 1 , and let s(·) be
the sum function (sum all components of a vector), then for any curve r and any intermediate
x, y; θ) =
layer h , (4) is exactly the objective proposed by Madry et al. [MMS+ 17a] where ρ(x
x0 , y; θ).
maxx 0 ∈N (xx,ε) `(x
We note that: (1) sum is a weak size function which does not give a metric. (2) As a result, while
this robust prediction objective falls within our framework, and regularizes robust attributions, it
allows a small regularization term where attributions actually change significantly but they cancel
each other in summation. Therefore, the control over robust attributions can be weak.
Proposition 2 (Input gradient regularization). For any λ0 > 0 and q ≥ 1, if we set λ = λ0 /εq ,
s(·) = k · kq1 , and use only the first term of summation approximation (3) to approximate IG, then
(4) becomes exactly the input gradient regularization of Drucker and LeCun [DL92], where we have
x, y; θ) = `(x
x, y; θ) + λk∇x `(x
x, y; θ)kqq .
ρ(x
In the above we have considered instantiations of a weak size function (summation function), which
recovers Madry et al.’s objective, and of a weak approximation of IG (picking the first term), which
recovers input gradient regularization. In the next example, we pick a nontrivial size function, the
1-norm k · k1 , use the precise IG, but then we use a trivial intermediate layer, the output loss `y .
Proposition 3 (Regularizing by attribution of the loss output). Let us set λ = 1, s(·) =
x, y; θ) = `y (x
x) +
k · k1 , and h = `y (the output layer of loss function!), then we have ρ(x
x0 ) − `y (x
x)|}.
maxx 0 ∈N (xx,ε) {|`y (x
We note that this loss function is a “surrogate” loss function for Madry et al.’s loss function bex) + maxx 0 ∈N (xx,ε) {|`y (x
x0 ) − `y (x
x)|} ≥ `y (x
x) + maxx 0 ∈N (xx,ε) {(`y (x
x0 ) − `y (x
x))} =
cause `y (x
0
x ). Therefore, even at such a trivial instantiation, robust attribution regularization
maxx 0 ∈N (xx,ε) `y (x
provides interesting guarantees.
3.2

Distributional Robustness Model

A different but popular model for robust optimization is the distributional robustness model. In this
case we consider a family of distributions P, each of which is supposed to be a “slight variation” of
a base distribution P . The goal of robust optimization is then that certain objective functions obtain
stable values over this entire family. Here we apply the same underlying idea to the distributional
5

We stress that this regularization term depends on model parameters θ through loss function `y .

4

robustness model: One should get a small loss value over the base distribution P , and for any
distribution Q ∈ P, the IG-based attributions change only a little if we move from P to Q. This is
formalized as:
E[`(P ; θ)] + λ sup {WdIG (P, Q)} ,

minimize

P

θ

Q∈P

where the WdIG (P, Q) is the Wasserstein distance between P and Q under a distance metric dIG .6
We use IG to highlight that this metric is related to integrated gradients.
We propose again dIG (zz , z 0 ) = s(IGh` (zz , z 0 )). We are particularly interested in the case where P is
a Wasserstein ball around the base distribution P , using “perturbation” cost metric c(·). This gives
regularization term λ EWc (P,Q)≤ρ sup{WdIG (P, Q)}. An unsatisfying aspect of this objective, as
one can observe now, is that WdIG and Wc can take two different couplings, while intuitively we
want to use only one coupling to transport P to Q. For example, this objective allows us to pick
a coupling M1 under which we achieve WdIG (recall that Wasserstein distance is an infimum over
couplings), and a different coupling M2 under which we achieve Wc , but under M1 = (Z, Z 0 ),
Ez,z0 ∼M1 [c(z, z 0 )] > ρ, violating the constraint. This motivates the following modification:
n
o
0
0
minimize E[`(P ; θ)] + λ
sup
E
[d
(Z,
Z
)]
s.t.
E
[c(Z,
Z
)]
≤
ρ
,
IG
(5)
Q
P
Z,Z 0
Z,Z 0
θ
Q;M ∈

(P,Q)

Q
In this formulation, (P, Q) is the set of couplings of P and Q, and M = (Z, Z 0 ) is one coupling.
c(·, ·) is a metric, such as k · k2 , to measure the cost of an adversary perturbing z to z 0 . ρ is an
upper bound on the expected perturbation cost, thus constraining P and Q to be “close” with each
together. dIG is a metric to measure the change of attributions from Z to Q
Z 0 , where we want a large
dIG -change under a small c-change. The supremum is taken over Q and (P, Q).
Proposition 4 (Wasserstein prediction robustness). Let s(·) be the summation function and λ = 1,
then for any curve γ and any layer h , (5) reduces to supQ:Wc (P,Q)≤ρ {EQ [`(Q; θ)]}, which is the
objective proposed by Sinha, Namhoong, and Duchi [SND18] for robust predictions.
Lagrange relaxation. For any γ ≥ 0, the Lagrange relaxation of (5) is


n

o
0
0
minimize
E[`(P ; θ)] + λ
sup
E
d
(Z,
Z
)
−
γc(Z,
Z
)
IG
Q
0
θ

P

Q;M ∈

M =(Z,Z )

(P,Q)

(6)

where the supremum is taken over Q (unconstrained) and all couplings of P and Q, and we want
to find a coupling under which IG attributions change a lot, while the perturbation cost from P to
Q with respect to c is small. Recall that g : Rd × Rd → R is a normal integrand if for each α, the
mapping z → {z 0 |g(z, z 0 ) ≤ α} is closed-valued and measurable [RW09].
Our next two theorems generalize the duality theory in [SND18] to a much larger, but natural, class
of objectives.
Theorem 1. Suppose c(z, z) = 0 and dIG (z, z) = 0 for any z, and suppose γc(z, z 0 ) − dIG (z, z 0 ) is
a normal integrand. Then, supQ;M ∈Q(P,Q) {EM =(Z,Z 0 ) [dγIG (Z, Z 0 )]} = Ez∼P [supz0 {dγIG (z, z 0 )}].
Consequently, we have (6) to be equal to the following:
h
i
minimize
E `(z; θ) + λ sup{dIG (z, z 0 ) − γc(z, z 0 )}
(7)
0
θ

z∼P

z

The assumption dIG (z, z) = 0 is true for what we propose, and c(z, z) = 0 is true for any typical
cost such as `p distances. The normal integrand assumption is also very weak, e.g., it is satisfied
when dIG is continuous and c is closed convex.
Note that (7) and (4) are very similar, and so we use (4) for the rest the paper. Finally, given
Theorem 1, we are also able to connect (5) and (7) with the following duality result:
Theorem 2. Suppose c(z, z) = 0 and dIG (z, z) = 0 for any z, and suppose γc(z, z 0 ) − dIG (z, z 0 )
is a normal integrand. For any ρ > 0, there exists γ ≥ 0 such that the optimal solutions of (7) are
optimal for (5).
6
For supervised learning problem where P is of the form Z = (X, Y ), we use the same treatment as
in [SND18] so that cost function is defined as c(z, z 0 ) = cx (x, x0 ) + ∞ · 1{y 6= y 0 }. All our theory carries
over to such c which has range R+ ∪{∞}.

5

3.3

One Layer Neural Networks

We now consider the special case of one-layer neural networks, where the loss function takes the
x, y; w ) = g(−yhw
w , x i), w is the model parameters, x is a feature vector, y is a label, and
form of `(x
g is nonnegative. We take s(·) to be k · k1 , which corresponds to a strong instantiation that does not
allow attributions to cancel each other. Interestingly, we prove that for natural choices of g, this is
however exactly Madry et al.’s objective [MMS+ 17a], which corresponds to s(·) = sum(·). That
is, the strong (s(·) = k · k1 ) and weak instantiations (s(·) = sum(·)) coincide for one-layer neural
networks. This thus says that for generalized linear classifiers, “robust interpretation” coincides with
“robust predictions,” and further with classic soft-margin training.
Theorem 3. Suppose that g is differentiable, non-decreasing, and convex. Then for λ = 1, s(·) =
k · k1 , and `∞ neighborhood, (4) reduces to Madry et al.’s objective:
m
X
i=1

=

m
X

max

k x 0i − x i k∞ ≤ε

w , x 0i i) (Madry et al.’s objective)
g(−yi hw

w , x i i + εk w k1 ) (soft-margin).
g(−yi hw

i=1

Natural losses, such as Negative Log-Likelihood and softplus hinge loss, satisfy the conditions of
this theorem.

4

Instantiations and Optimizations

In this section we discuss instantiations of (4) and how to optimize them. We start by presenting two
objectives instantiated from our method: (1) IG-NORM, and (2) IG-SUM-NORM. Then we discuss
how to use gradient descent to optimize these objectives.
IG-NORM. As our first instantiation, we pick s(·) = k · k1 , h to be the input layer, and r to be the
straightline connecting x and x 0 . This gives:


`y
0
x
x
x
minimize
E
`(x , y; θ) + λ 0 max k IG (x , )k1
θ

x,ε)
x ∈N (x

x,y)∼P
(x

IG-SUM-NORM. In the second instantiation we combine the sum size function and norm size
function, and define s(·) = sum(·) + βk · k1 . Where β ≥ 0 is a regularization parameter. Now with
the same h and r as above, and put λ = 1, then our method simplifies to:

n
o
x, x 0 )k1
x0 , y; θ) + βk IG`y (x
minimize
E
max
`(x
0
θ

x,y)∼P
(x

x,ε)
x ∈N (x

x0 ).
which can be viewed as appending an extra robust IG term to `(x
Gradient descent optimization. We propose the following gradient descent framework to optimize
the objectives. The framework is parameterized by an adversary A which is supposed to solve
the inner max by finding a point x ? which changes attribution significantly. Specifically, given a
x, y) at time step t during SGD training, we have the following two steps (this can be easily
point (x
generalized to mini-batches):
x, y) to find x ? that produces a large inner max term (that is
Attack step. We run A on (x
`y
?
x, x )k1 for IG-NORM, and `(x
x? ) + βk IG`y (x
x, x ? )k1 for IG-SUM-NORM.
k IG (x
Gradient step. Fixing x ? , we can then compute the gradient of the corresponding objective with
respect to θ, and then update the model.
Important objective parameters. In both attack and gradient steps, we need to differentiate IG
(in attack step, θ is fixed and we differentiate w.r.t. x , while in gradient step, this is reversed), and
this induces a set of parameters of the objectives to tune for optimization, which is summarized
in Table 1. Differentiating summation approximation of IG amounts to compute second partial
derivatives. We rely on the auto-differentiation capability of TensorFlow [ABC+ 16] to compute
second derivatives.
6

Adversary to find x? . Note that our goal is simply to maximize the inner term in a neighborhood, thus in this paper
we choose Projected Gradient Descent for this purpose.
m in the attack step
To differentiate IG in the attack step, we use summation
approximation of IG, and this is the number of segments
for apprioximation.
m in the gradient step Same as above, but in the gradient step. We have this m
separately due to efficiency consideration.
λ
Regularization parameter for IG-NORM.
β
Regularization parameter for IG-SUM-NORM.
Table 1: Optimization parameters.
Adversary A

5

Experiments

We now perform experiments using our method. We ask the following questions: (1) Comparing
models trained by our method and naturally trained models at test time, do we maintain the accuracy
on unperturbed test inputs? (2) At test time, if we use attribution attacks mentioned in [GAZ17] to
perturb attributions while keeping predictions intact, how does the attribution robustness of our models compare with that of the naturally trained models? (3) Finally, how do we compare attribution
robustness of our models with weak instantiations for robust predictions?
To answer these questions, We perform experiments on four classic datasets: MNIST [LCB98],
Fashion-MNIST [XRV17], GTSRB [SSSI12], and Flower [NZ06]. In summary, our findings are
the following: (1) Our method results in very small drop in test accuracy compared with naturally
trained models. (2) On the other hand, our method gives signficantly better attribution robustness, as
measured by correlation analyses. (3) Finally, our models yield comparable prediction robustness
(sometimes even better), while consistently improving attribution robustness. In the rest of the
section we give more details.
Evaluation setup. In this work we use IG to compute attributions (i.e. feature importance map),
which, as demonstrated by [GAZ17], is more robust compared to other related methods (note that,
IG also enjoys other theoretical properties). To attack attribution while retaining model predictions,
we use Iterative Feature Importance Attacks (IFIA) proposed by [GAZ17]. Due to lack of space,
we defer details of parameters and other settings to the appendix. We use two metrics to measure
attribution robustness (i.e. how similar the attributions are between original and perturbed images):
Kendall’s tau rank order correlation. Attribution methods rank all of the features in order of their
importance, we thus use the rank correlation [Ken38] to compare similarity between interpretations.
Top-k intersection. We compute the size of intersection of the k most important features before and
after perturbation.
Compared with [GAZ17], we use Kendall’s tau correlation, instead of Spearman’s rank correlation.
The reason is that we found that on the GTSRB and Flower datasets, Spearman’s correlation is not
consistent with visual inspection, and often produces too high correlations. In comparison, Kendall’s
tau correlation consistently produces lower correlations and aligns better with visual inspection.
Finally, when computing attribution robustness, we only consider the test samples that are correctly
classified by the model.
Comparing with natural models. Figures (a), (b), (c), and (d) in Figure 2 show that, compared
with naturally trained models, robust attribution training gives significant improvements in attribution robustness (measured by either median or confidence intervals). The exact numbers are recorded
in Table 2: Compared with naturally trained models (rows where “Approach” is NATURAL), robust
attribution training has significantly better adversarial accuracy and attribution robustness, while
having a small drop in natural accuracy (denoted by Nat Acc.).
Ineffective optimization. We observe that even when our training stops, the attribution regularization term remains much more significant compared to the natural loss term. For example for
x) typically stays at 1, but k IG(x
x, x 0 )k1 stays at
IG-NORM, when training stops on MNIST, `(x
10 ∼ 20. This indicates that optimization has not been very effective in minimizing the regularization term. There are two possible reasons to this: (1) Because we use summation approximation of
IG, it forces us to compute second derivatives, which may not be numerically stable for deep net7

(a) MNIST

(b) Fashion-MNIST

(c) GTSRB

(d) Flower

Figure 2: Experiment results on MNIST, Fashion-MNIST, GTSRB and Flower.
works. (2) The network architecture may be inherently unsuitable for robust attributions, rendering
the optimization hard to converge.
Comparing with robust prediction models. Finally we compare with Madry et al.’s models, which
are trained for robust prediction. We use Adv Acc. to denote adversarial accuracy (prediction
accuracy on perturbed inputs). Again, TopK Inter. denotes the average topK intersection (K =
100 for MNIST, Fashion-MNIST and GTSRB datasets, K = 1000 for Flower), and Rank Corr.
denotes the average Kendall’s rank order correlation. Table 2 gives the details of the results. As we
can see, our models give comparable adversarial accuracy, and are sometimes even better (on the
Flower dataset). On the other hand, we are consistently better in terms of attribution robustness.
Dataset
MNIST

Fashion-MNIST

GTSRB

Flower

Approach
NATURAL
Madry et al.
IG-NORM
IG-SUM-NORM
NATURAL
Madry et al.
IG-NORM
IG-SUM-NORM
NATURAL
Madry et al.
IG-NORM
IG-SUM-NORM
NATURAL
Madry et al.
IG-NORM
IG-SUM-NORM

Nat Acc.
99.17%
98.40%
98.74%
98.34%
90.86%
85.73%
85.13%
85.44%
98.57%
97.59%
97.02%
95.68%
86.76%
83.82%
85.29%
82.35%

Adv Acc.
0.00%
92.47%
81.43%
88.17%
0.01%
73.01%
65.95%
70.26%
21.05%
83.24%
75.24%
77.12%
0.00%
41.91%
24.26%
47.06%

TopK Inter.
46.61%
62.56%
71.36%
72.45%
39.01%
46.12%
59.22%
72.08%
54.16%
68.85%
74.81%
74.04%
8.12%
55.87%
64.68%
66.33%

Rank Corr.
0.1758
0.2422
0.2841
0.3111
0.4610
0.6251
0.6171
0.6747
0.6790
0.7520
0.7555
0.7684
0.4978
0.7784
0.7591
0.7974

Table 2: Experiment results including prediction accuracy, prediction robustness and attribution
robustness.
8

6

Discussion and Conclusion

This paper builds a theory to robustify model interpretations through the lens of axiomatic attributions of neural networks. We show that our theory gives principled generalizations of previous
formulations for robust predictions, and we characterize our objectives for one-layer neural networks. We believe that our work opens many intriguing avenues for future research, and we discuss
a few topics below.
Why we want robust attributions? Model attributions are facts about model behaviors. While
robust attribution does not necessarily mean that the attribution is correct, a model with brittle attribution can never be trusted. To this end, it seems interesting to examine attribution methods other
than Integrated Gradients.
Robust attribution leads to more human-aligned attribution. Note that our proposed training
scheme requires both prediction correctness and robust attributions, and therefore it encourages to
learn invariant features from data that are also highly predictive. In our experiments, we found
an intriguing phenomenon that our regularized models produce attributions that are much more
aligned with human perceptions (for example, see Figure 1). Our results are aligned with the recent
work [TSE+ 19, EIS+ 19].
Robust attribution may help tackle spurious correlations. In view of our discussion so far, we
think it is plausible that robust attribution regularization can help remove spurious correlations because intuitively spurious correlations should not be able to be reliably attributed to. Future research
on this potential connection seems warranted.
Difficulty of optimization. While our experimental results are encouraging, we observe that when
training stops, the attribution regularization term remains significant (typically around tens to hundreds), which indicates ineffective optimization for the objectives. To this end, a main problem is
network depth, where as depth increases, we get very unstable trajectories of gradient descent, which
seems to be related to the use of second order information during robust attribution optimization (due
to summation approximation, we have first order terms in the training objectives). Therefore, it is
natural to further study better optimization tchniques or better architectures for robust attribution
training.

7

Acknowledgments

This work is partially supported by Air Force Grant FA9550-18-1-0166, the National Science Foundation (NSF) Grants CCF-FMitF-1836978, SaTC-Frontiers-1804648 and CCF-1652140 and ARO
grant number W911NF-17-1-0405.

9

References
[ABC+ 16]

Martı́n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean,
Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow:
A system for large-scale machine learning. In 12th USENIX Symposium on Operating
Systems Design and Implementation (OSDI), pages 265–283, 2016.

[BTEGN09] A. Ben-Tal, L. El Ghaoui, and A.S. Nemirovski. Robust Optimization. Princeton
Series in Applied Mathematics. Princeton University Press, October 2009.
[DL92]

Harris Drucker and Yann LeCun. Improving generalization performance using double
backpropagation. IEEE Trans. Neural Networks, 3(6):991–997, 1992.

[EIS+ 19]

Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran,
and Aleksander Madry. Adversarial robustness as a prior for learned representations.
arXiv 1906.00945, 2019.

[GAZ17]

Amirata Ghorbani, Abubakar Abid, and James Y. Zou. Interpretation of neural networks is fragile. CoRR, abs/1710.10547, 2017.

[HZRS16]

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
for image recognition. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 770–778, 2016.

[Ken38]

Maurice G Kendall. A new measure of rank correlation. Biometrika, 30(1/2):81–93,
1938.

[KM18]

Zico Kolter and Aleksander Madry. Adversarial robustness – theory and practice,
2018. https://adversarial-ml-tutorial.org/.

[LCB98]

Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. The mnist database of
handwritten digits, 1998. http://yann.lecun.com/exdb/mnist/.

[Lue97]

David G. Luenberger. Optimization by Vector Space Methods. John Wiley & Sons,
Inc., New York, NY, USA, 1st edition, 1997.

[MEK15]

Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust
optimization using the wasserstein metric: performance guarantees and tractable reformulations. arXiv preprint arXiv:1505.05116, 2015.

[MMS+ 17a] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. Towards deep learning models resistant to adversarial attacks. CoRR,
abs/1706.06083, 2017.
[MMS+ 17b] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv
preprint arXiv:1706.06083, 2017.
[NZ06]

M-E Nilsback and Andrew Zisserman. A visual vocabulary for flower classification.
In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), volume 2, pages 1447–1454. IEEE, 2006.

[RD18]

Andrew Slavin Ross and Finale Doshi-Velez. Improving the adversarial robustness
and interpretability of deep neural networks by regularizing their input gradients. In
Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI18), pages 1660–1669, 2018.

[RW09]

R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis, volume 317. Springer
Science & Business Media, 2009.

[SND18]

Aman Sinha, Hongseok Namkoong, and John C. Duchi. Certifying some distributional robustness with principled adversarial training. In 6th International Conference
on Learning Representations, ICLR 2018, 2018.

[SSSI12]

Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition. Neural
networks, 32:323–332, 2012.

[STY17]

Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep
networks. In Proceedings of the 34th International Conference on Machine Learning,
pages 3319–3328, 2017.
10

[SVZ13]

[TSE+ 19]

[WK18]

[XRV17]

Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional
networks: Visualising image classification models and saliency maps. arXiv preprint
arXiv:1312.6034, 2013.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. In International Conference
on Learning Representations, 2019.
Eric Wong and J. Zico Kolter. Provable defenses against adversarial examples via the
convex outer adversarial polytope. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, pages 5283–5292, 2018.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset
for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747,
2017.

11

A

Code

Code for this paper is publicly available at the following repository:
https://github.com/jfc43/robust-attribution-regularization

B
B.1

Proofs
Additional definitions

Let P, Q be two distributions, a coupling M = (Z, Z 0 ) is a joint distribution, where, if we marginalize M to the first component, Z, it is identically distributed Q
as P , and if we marginalize M to the
second component, Z 0 , it is identically distributed as Q. Let (P, Q) be the set of all couplings of
P and Q, and let c(·, ·) be a “cost” function that maps (z, z 0 ) to a real value. Wasserstein distance
between P and Q w.r.t. c is defined as


0
Wc (P, Q) =
inf
E
[c(z, z )] .
Q
0
M∈

(P,Q)

(z,z )∼M

Intuitively, this is to find the “best transportation plan” (a coupling M ) to minimize the expected
transportation cost (transporting z to z 0 where the cost is c(z, z 0 )).
B.2

Integrated Gradients for an Intermediate Layer

In this section we show how to compute Integrated Gradients for an intermediate layer of a neural
network. Let h : Rd 7→ Rk be a function that computes a hidden layer of a neural network, where
we map a d-dimensional input vector to a k-dimensional output vector. Given two points x and x 0
for computing attribution, again we consider a parameterization (which is a mapping r : R 7→ Rd )
such that r(0) = x , and r(1) = x 0 .
The key insight is to leverage the fact that Integrated Gradients is a curve integration. Therefore,
given some hidden layer, one can then naturally view the previous layers as inducing a curve h ◦ r
x) to h(x
x0 ), as we move from x to x 0 along curve r. Viewed this way, we
which moves from h(x
can thus naturally compute IG for h in a way that leverages all layers of the network. Specifically,
consider another curve γ(t) : R 7→ Rk , defined as γ(t) = h(r(t)), to compute a curve integral. By
x) = g(h(x
x))
definition we have f (x
x0 ) − f (x
x) = g(h(x
x0 )) − g(h(x
x))
f (x
= g(γ(1)) − g(γ(0))
Z 1X
k
∂f (γ(t)) 0
=
γi (t)dt
∂hi
0 i=1
k Z 1
X
∂f (γ(t)) 0
=
γi (t)dt
∂hi
i=1 0
Therefore we can define the attribution of hi naturally as
Z 1
∂f (γ(t)) 0
x, x 0 ) =
IGfhi (x
γi (t)dt
∂hi
0
Let’s unpack this a little more:
Z 1
Z 1
d
∂f (h(r(t))) X ∂hi (r(t)) 0
∂f (γ(t)) 0
γi (t)dt =
rj (t)dt
∂hi
∂hi
∂ xj
0
0
j=1
Z

1

=
0

=

d

∂f (h(r(t))) X ∂hi (r(t)) 0
rj (t)dt
∂hi
∂ xj
j=1

d Z
X
j=1

1

0

This thus gives the lemma
12


∂f (h(r(t))) ∂hi (r(t)) 0
rj (t)dt
∂hi
∂ xj

Lemma 2. Under curve r : R 7→ Rd where r(0) = x and r(1) = x 0 , the attribution for hi for a
differentiable function f is

d Z 1
X
∂f (h(r(t))) ∂hi (r(t)) 0
x, x 0 , r) =
rj (t)dt
IGfhi (x
(8)
∂hi
∂ xj
0
j=1
Note that (6) nicely recovers attributions for input layer, in which case h is the identity function.
Summation approximation. Similarly, we can approximate the above Riemann integral using a
summation. Suppose we slice [0, 1] into m equal segments, then (2) can be approximated as:
(m−1
)
d
X
X ∂f (h(r(k/m))) ∂hi (r(k/m))
1
f
x, x0 ) =
rj0 (k/m)
IGhi (x
(9)
m j=1
∂hi
∂ xj
k=0

B.3

Proof of Proposition 1

If we put λ = 1 and let s(·) be the sum function (sum all components of a vector), then for any
curve r and any intermediate layer h , (4) becomes:
x, y; θ) = `(x
x, y; θ) +
ρ(x
x, y; θ) +
= `(x
=

max

x,ε)
x 0 ∈N (x

x, x0 ; r))}
max {sum(IG`y (x

x,ε)
x 0 ∈N (x

x0 , y; θ) − `(x
x, y; θ)}
max {`(x

x,ε)
x 0 ∈N (x

x0 , y; θ)
`(x

where the second equality is due to the Axiom of Completeness of IG.
B.4

Proof of Proposition 2

Input gradient regularization is an old idea proposed by Drucker and LeCun [DL92], and is recently
used by Ross and Doshi-Velez [RD18] in adversarial training setting. Basically, for q ≥ 1, they
x, y; θ) = `(x
x, y; θ) + λk∇x `(x
x, y; θ)kqq , where they want small gradient at x. To recover
propose ρ(x
this objective from robust attribution regularization, let us pick s(·) as the k · kq1 function (1-norm
x0 − x ). With the naı̈ve summation
to the q-th power), and consider the simplest curve r(t) = x +t(x
x + k−1
x0
∂`(x
`y
`y
x0i − x i ) Pm
(x
0
m (x − x ),y;θ)
x, x ; r) ≈
,
approximation of the integral IGi we have IGi (x
k=1
m
∂ xi
where larger m is, more accurate we approximate the integral. Now, if we put m = 1, which is the
x,y;θ)
x, x 0 ; θ) = (x
x0 − x )
x0i − x i ) ∂`(x
coarsest approximation, this becomes (x
, and we have IG`y (x
∂ xi
x, y; θ). Therefore (4) becomes:
∇x `(x
x, y; θ) =`(x
x, y; θ) + λ
ρ(x
x, y; θ) + λ
≈`(x

x, x 0 ; θ)kq1 }
max {k IG`y (x

x,ε)
x 0 ∈N (x

x0 − x )
max {k(x

x,ε)
x 0 ∈N (x

x, y; θ)kq1 }
∇x `(x

Put the neighborhood as k x 0 − x kp ≤ ε where p ∈ [1, ∞] and p1 + 1q = 1. By Hölder’s inequality,
x0 − x ) ∇x `(x
x, y; θ)kq1 ≤ k x 0 − x kqp k∇`(x
x, y; θ)kqq ≤ εq k∇`(x
x, y; θ)kqq which means that
k(x
q
x0 − x) ∇x `(x
x, y; θ)k1 } = εq k∇`(x
x, y; θ)kqq . Thus by putting λ = λ0 /εq , we
maxk x 0 − x kp ≤ε {k(x
recover gradient regularization with regularization parameter λ0 .
B.5

Proof of Proposition 3

Let us put s(·) = k · k1 , and h = `y (the output layer of loss function!), then we have
x, y; θ) =`y (x
x) +
ρ(x
x) +
=`y (x

`

x, x0 ; r)k1 }
max {k IG`yy (x

x,ε)
x 0 ∈N (x

x0 ) − `y (x
x)|}
max {|`y (x

x,ε)
x 0 ∈N (x

`

x, x 0 ; r) = `y (x
x0 ) − `y (x
x).
where the second equality is because IG`yy (x
13

B.6

Proof of Proposition 4

Specifically, again, let s(·) be the summation function and λ = 1, then we have EZ,Z 0 [dIG (Z, Z 0 )] =
EZ,Z 0 [sum(IGh` (Z, Z 0 ))] = EZ,Z 0 [`(Z 0 ; θ) − `(Z; θ)]. Because P and Z are identically distributed,
thus the objective reduces to
n
E 0 [`(Z; θ) + `(Z 0 ; θ) − `(Z; θ)]
sup
Q
Q;M ∈

Z,Z

(P,Q)

o
s.t. E 0 [c(Z, Z 0 )] ≤ ρ
Z,Z


=

sup
Q

Q;M ∈

(P,Q)

Z


=

sup
Q:Wc (P,Q)≤ρ

E0 [`(Z 0 ; θ)] s.t. E 0 [c(Z, Z 0 )] ≤ ρ



Z,Z


E[`(Q; θ)] ,
Q

which is exactly Wasserstein prediction robustness objective.
B.7

Proof of Theorem 1

The proof largely follows that for Theorem 5 in [SND18],
and we provide it here for completeness.
Q
Since we have a joint supremum over Q and M ∈ (P, Q) we have that


Z
 γ

0
[dIG (z, z 0 ) − γc(z, z 0 )]dM (z, z 0 )
sup
E 0 dIG (Z, Z ) =
sup
Q
Q
Q;M ∈

(P,Q)

M =(Z,Z )

Q;M ∈

(P,Q)

Z

sup{dIG (z, z 0 ) − γc(z, z 0 )}dP (z)
z0


γ
0
= E sup{dIG (z, z )} .

≤

z∼P

z0

We would like to show equality in the above.
Let Q denote the space of regular conditional probabilities from Z to Z 0 . Then
Z
Z
0
0
0
sup
[dIG (z, z ) − γc(z, z )]dM (z, z ) ≥ sup [dIG (z, z 0 ) − γc(z, z 0 )]dQ(z 0 |z)dP (z).
Q
Q;M ∈

Q∈Q

(P,Q)

Let Z 0 denote all measurable mappings z → z 0 (z) from Z to Z 0 . Using the measurability result in
Theorem 14.60 in [RW09], we have
Z
Z
sup [dIG (z, z 0 (z)) − γc(z, z 0 (z))]dP (z) = sup[dIG (z, z 0 ) − γc(z, z 0 )]dP (z)
z0

z 0 ∈Z 0

since γc − dIG is a normal integrand.
Let z 0 (z) be any measurable function that is -close to attaining the supremum above, and define the
conditional distribution Q(z 0 |z) to be supported on z 0 (z). Then
Z
Z
0
0
0
sup
[d
(z,
z
)
−
γc(z,
z
)]dM
(z,
z
)
≥
[dIG (z, z 0 ) − γc(z, z 0 )]dQ(z 0 |z)dP (z)
IG
Q
Q;M ∈

(P,Q)

Z

[dIG (z, z 0 (z)) − γc(z, z 0 (z))]dP (z)

Z

sup[dIG (z, z 0 ) − γc(z, z 0 )]dP (z) − 
z0
Z
sup
[dIG (z, z 0 ) − γc(z, z 0 )]dM (z, z 0 ) − .
Q

=
≥
≥

Q;M ∈

Since  ≥ 0 is arbitrary, this completes the proof.
14

(P,Q)

B.8

Proof of Theorem 2: Connections Between the Distributional Robustness Objectives

Let θ∗ denote an optimal solution of (5) and let θ0 be any non-optimal solution. Let γ(θ∗ ) denote
the corresponding γ by Lemma 3, and γ(θ0 ) denote that for θ0 .
Since γ(θ0 ) achieves the infimum, we have


E `(z; θ0 ) + λ sup{dIG (z, z 0 ) − γ(θ∗ )c(z, z 0 )}
z∼P
z0


≥ E `(z; θ0 ) + λ sup{dIG (z, z 0 ) − γ(θ0 )c(z, z 0 )}
z∼P
z0


> E `(z; θ∗ ) + λ sup{dIG (z, z 0 ) − γ(θ∗ )c(z, z 0 )} .
z∼P

(10)
(11)
(12)

z0

So θ0 is not optimal for (7). This then completes the proof.
Lemma 3. Suppose c(z, z) = 0 and dIG (z, z) = 0 for any z, and suppose γc(z, z 0 ) − dIG (z, z 0 ) is
a normal integrand. For any ρ > 0, there exists γ ≥ 0 such that


0
0
sup
E
[d
(Z,
Z
)]
s.t.
E
[c(Z,
Z
)]
≤
ρ
(13)
IG
Q
0
0
Q;M ∈

(Z,Z )∼M

(P,Q)

(Z,Z )∼M



0
0
= inf E sup{dIG (z, z ) − ζc(z, z ) + ζρ} .
ζ≥0 z∼P

(14)

z0

Furthermore, there exists γ ≥ 0 achieving the infimum.
This lemma generalizes Theorem 5 in [SND18] to a larger, but natural, class of objectives.
Proof. For Q and M ∈ Π(P, Q), let
ΛIG (Q, M ) :=
Λc (Q, M ) :=

E

[dIG (Z, Z 0 )]

(15)

E

[c(Z, Z 0 )]

(16)

(Z,Z 0 )∼M
(Z,Z 0 )∼M

First, the pair (Q, M ) forms a convex set, and ΛIG (Q, M ) and Λc (Q, M ) are linear functionals over
the convex set. Set Q = P and set M to the identity coupling (such that (Z, Z 0 ) ∼ M always has
Z = Z 0 ). Then Λc (Q, M ) = 0 < ρ and thus the Slater’s condition holds. Applying standard infinite
dimensional duality results (Theorem 8.6.1 in [Lue97]) leads to
sup
Q;M ∈

=

ΛIG (Q, M )

(17)

Q
(P,Q);Λc (Q,M )≤ρ

inf {ΛIG (Q, M ) − ζΛc (Q, M ) + ζρ}

sup
Q

Q;M ∈

(P,Q) ζ≥0

= inf

sup

ζ≥0 Q;M ∈Q(P,Q)

{ΛIG (Q, M ) − ζΛc (Q, M ) + ζρ} .

(18)
(19)

Furthermore, there exists γ ≥ 0 achieving the infimum in the last line.
Now, it suffices to show that
sup
Q

Q;M ∈

{ΛIG (Q, M ) − γΛc (Q, M ) + γρ}



0
0
= E sup{dIG (z, z ) − γc(z, z ) + γρ} .
z∼P

(20)

(P,Q)

(21)

z0

This is exactly what Theorem 1 shows.
B.9

Proof of Theorem 3

w , x i) + λ maxx0 ∈N (xx,ε) k IGx`y (x
x, x 0 ; w )k1 . Due to
Let us fix any one point x , and consider g(−yi hw
the special form of g, we know that:

x0 − x )i
w (x
`
x, x 0 ; w ) = i 0
w , x 0 i) − g(−yhw
w , x i)
· g(−yhw
IGi y (x
w, x − xi
hw
15

Let ∆ = x 0 − x (which satisfies that k∆k∞ ≤ ε), therefore its absolute value (note that we are
taking 1-norm):
w , x i − yhw
w , ∆i) − g(−yhw
w , x i) )
g(−yhw
· | w i ∆i |
w , ∆i|
|hw
w , x i and δ = −yhw
w , ∆i, this is further simplified as
Let z = −yhw
0

|g(z+δ)−g(z)|
|δi |.
|δ|

Because g is

g(z+δ)−g(z)
,
δ

non-decreasing, so g ≥ 0, and so this is indeed
which is the slope of the secant from
(z, g(z)) to (z + δ, g(z + δ)). Because g is convex so the secant slopes are non-decreasing, so we
w i )ε, and so δ = k w k1 ε, and so that k IG k1 becomes
can simply pick ∆i = −y sgn(w
P
P
| w i ∆i |
| w i |ε
|g(z + εk w k1 ) − g(z)| · i
= |g(z + εk w k1 ) − g(z)| · i
|δ|
k w k1 ε
= |g(z + εk w k1 ) − g(z)|
= g(z + εk w k1 ) − g(z)
where
Pm the last equality follows because g is nondecreasing. Therefore the objective simplifies to
w , xi i+εk w k1 ), which is exactly Madry et al.’s objective under `∞ perturbations.
i=1 g(−yi hw
Let us consider two examples:
w , x i) recovers the Negative LogLogistic Regression. Let g(z) = ln(1 + exp(z)). Then g(−yhw
Likelihood loss for logistic regression. Clearly g is nondecreasing and g 0 is also nondecreasing. As a
result, adversarial training for logistic regression is exactly “robustifying” attributions/explanations.
Softplus hinge loss. Alternatively, we can let g(z) = ln(1 + exp(1 + z)), and therefore
w , x i) = ln(1 + exp(1 − yhw
w , x i)) is the softplus version of the hinge loss function. Clearly
g(−yhw
this g also satisfy our requirements, and therefore adversarial training for softplus hinge loss function
is also exactly about “robustifying” attributions/explanations.

C

More Details of Experiments

C.1

Experiment Settings

We perform experiments on four datasets: MNIST, Fashion-MNIST, GTSRB and Flower. Robust
attribution regularization training requires extensive computing power. We conducted experiments
in parallel over multiple NVIDIA Tesla V100 and NVDIA GeForce RTX 2080Ti GPUs both on
premises and on cloud. Detailed experiment settings for each dataset are described below.
C.1.1

MNIST

Data. The MNIST dataset [LCB98] is a large dataset of handwritten digits. Each digit has 5,500
training images and 1,000 test images. Each image is a 28 × 28 grayscale. We normalize the range
of pixel values to [0, 1].
Model. We use a network consisting of two convolutional layers with 32 and 64 filters respectively,
each followed by 2 × 2 max-pooling, and a fully connected layer of size 1024. Note that we use the
same MNIST model as [MMS+ 17b].
Training hyper-parameters. The hyper-parameters to train different models are listed below:
NATURAL. We set learning rate as 10−4 , batch size as 50, training steps as 25,000, and use Adam
Optimizer.
Madry et al.. We set learning rate as 10−4 , batch size as 50, training steps as 100,000, and use Adam
Optimizer. We use PGD attack as adversary with random start, number of steps of 40, step size of
0.01, and adversarial budget  of 0.3.
IG-NORM. We set λ = 1, m = 50 for gradient step, learning rate as 10−4 , batch size as 50, training
steps as 100,000, and use Adam Optimizer. We use PGD attack as adversary with random start,
number of steps of 40, step size of 0.01, m = 10 for attack step, and adversarial budget  = 0.3.
IG-SUM-NORM. We set β as 0.1, m in the gradient step as 50, learning rate as 10−4 , batch size
as 50, training steps as 100,000, and use Adam Optimizer. We use PGD attack as adversary with
16

random start, number of steps of 40, step size of 0.01, m = 10 in the attack step, and adversarial
budget  = 0.3.
Evaluation Attacks. For attacking inputs to change model predictions, we use PGD attack with
random start, number of steps of 100, adversarial budget  of 0.3 and step size of 0.01. For attacking
inputs to change interpretations, we use Iterative Feature Importance Attacks (IFIA) proposed by
[GAZ17]. We use their top-k attack with k = 200, adversarial budget  = 0.3, step size α =
0.01 and number of iterations P = 100. We set the feature importance function as Integrated
Gradients(IG) and dissimilarity function D as Kendall’s rank order correlation. We find that IFIA
is not stable if we use GPU parallel computing (non-deterministic is a behavior of GPU), so we run
IFIA three times on each test example and use the best result with the lowest Kendall’s rank order
correlation.
C.1.2

Fashion-MNIST

Data. The Fashion-MNIST dataset [XRV17] contains images depicting wearables such as shirts
and boots instead of digits, which is more complex than MNIST dataset. The image format, the
number of classes, as well as the number of examples are all identical to MNIST.
Model. We use a network consisting of two convolutional layers with 32 and 64 filters respectively,
each followed by 2 × 2 max-pooling, and a fully connected layer of size 1024.
Training hyper-parameters. The hyper-parameters to train different models are listed below:
NATURAL. We set learning rate as 10−4 , batch size as 50, training steps as 25,000, and use Adam
Optimizer.
Madry et al.. We set learning rate as 10−4 , batch size as 50, training steps as 100,000, and use Adam
Optimizer. We use PGD attack as adversary with random start, number of steps of 20, step size of
0.01, and adversarial budget  of 0.1.
IG-NORM. We set λ = 1, m = 50 for gradient step, learning rate as 10−4 , batch size as 50, training
steps as 100,000, and use Adam Optimizer. We use PGD attack as adversary with random start,
number of steps of 20, step size of 0.01, m = 10 for attack step, and adversarial budget  = 0.1.
IG-SUM-NORM. We set β as 0.1, m in the gradient step as 50, learning rate as 10−4 , batch size
as 50, training steps as 100,000, and use Adam Optimizer. We use PGD attack as adversary with
random start, number of steps of 20, step size of 0.01, m = 10 in the attack step, and adversarial
budget  = 0.1.
Evaluation Attacks. For attacking inputs to change model predictions, we use PGD attack with
random start, number of steps of 100, adversarial budget  of 0.1 and step size of 0.01. For attacking
inputs to change interpretations, we use Iterative Feature Importance Attacks (IFIA) proposed by
[GAZ17]. We use their top-k attack with k = 100, adversarial budget  = 0.1, step size α =
0.01 and number of iterations P = 100. We set the feature importance function as Integrated
Gradients(IG) and dissimilarity function D as Kendall’s rank order correlation. We find that IFIA
is not stable if we use GPU parallel computing (non-deterministic is a behavior of GPU), so we run
IFIA three times on each test example and use the best result with the lowest Kendall’s rank order
correlation.
C.1.3

GTSRB

Data. The German Traffic Sign Recognition Benchmark (GTSRB) [SSSI12] is a dataset of color
images depicting 43 different traffic signs. The images are not of a fixed dimensions and have rich
background and varying light conditions as would be expected of photographed images of traffic
signs. There are about 34,799 training images, 4,410 validation images and 12,630 test images.
We resize each image to 32 × 32. The pixel values are in range of [0, 255]. The dataset has a
large imbalance in the number of sample occurrences across classes. We use data augmentation
techniques to enlarge the training data and make the number of samples in each class balanced.
We construct a class preserving data augmentation pipeline consisting of rotation, translation, and
projection transforms and apply this pipeline to images in the training set until each class contained
10,000 training examples. We use this new augmented training data set containing 430,000 samples
in total to train models. We also preprocess images via image brightness normalization.
17

Model . We use the Resnet model [HZRS16]. We perform per image standardization before feeding
images to the neural network. The network has 5 residual units with (16, 16, 32, 64) filters each.
The model is adapted from CIFAR-10 model of [MMS+ 17b]. Refer to our codes for details.
Training hyper-parameters. The hyper-parameters to train different models are listed below:
NATURAL. We use Momentum Optimizer with weight decay. We set momentum rate as 0.9, weight
decay rate as 0.0002, batch size as 64, and training steps as 70,000. We use learning rate schedule:
the first 500 steps, we use learning rate of 10−3 ; after 500 steps and before 60,000 steps, we use
learning rate of 10−2 ; after 60,000 steps, we use learning rate of 10−3 .
Madry et al.. We use Momentum Optimizer with weight decay. We set momentum rate as 0.9,
weight decay rate as 0.0002, batch size as 64, and training steps as 70,000. We use learning rate
schedule: the first 500 steps, we use learning rate of 10−3 ; after 500 steps and before 60,000 steps,
we use learning rate of 10−2 ; after 60,000 steps, we use learning rate of 10−3 . We use PGD attack
as adversary with random start, number of steps of 7, step size of 2, and adversarial budget  of 8.
IG-NORM. We set λ as 1, m in the gradient step as 50. We use Momentum Optimizer with weight
decay. We set momentum rate as 0.9, weight decay rate as 0.0002, batch size as 64, and training
steps as 70,000. We use learning rate schedule: the first 500 steps, we use learning rate of 10−6 ;
after 500 steps and before 60,000 steps, we use learning rate of 10−4 ; after 60,000 steps, we use
learning rate of 10−5 . We use PGD attack as adversary with random start, number of steps of 7, step
size of 2, m in the attack step of 5, and adversarial budget  of 8.
IG-SUM-NORM. We set β as 1, m in the gradient step as 50. We use Momentum Optimizer with
weight decay. We set momentum rate as 0.9, weight decay rate as 0.0002, batch size as 64, and
training steps as 70,000. We use learning rate schedule: the first 500 steps, we use learning rate of
10−5 ; after 500 steps and before 60,000 steps, we use learning rate of 10−4 ; after 60,000 steps, we
use learning rate of 10−5 . We use PGD attack as adversary with random start, number of steps of 7,
step size of 2, m in the attack step of 5, and adversarial budget  of 8.
Evaluation Attacks. For attacking inputs to change model predictions, we use PGD attack with
number of steps of 40, adversarial budget  of 8 and step size of 2. For attacking inputs to change
interpretations, we use Iterative Feature Importance Attacks (IFIA) proposed by [GAZ17]. We use
their top-k attack with k = 100, adversarial budget  = 8, step size α = 1 and number of iterations
P = 50. We set the feature importance function as Integrated Gradients(IG) and dissimilarity
function D as Kendall’s rank order correlation. We find that IFIA is not stable if we use GPU
parallel computing (non-deterministic is a behavior of GPU), so we run IFIA three times on each
test example and use the best result with the lowest Kendall’s rank order correlation.
C.1.4

Flower

Data. Flower dataset [NZ06] is a dataset of 17 category flowers with 80 images for each class (1,360
image in total). The flowers chosen are some common flowers in the UK. The images have large
scale, pose and light variations and there are also classes with large variations of images within the
class and close similarity to other classes. We randomly split the dataset into training and test sets.
The training set has totally 1,224 images with 72 images per class. The test set has totally 136 images
with 8 images per class. We resize each image to 128×128. The pixel values are in range of [0, 255].
We use data augmentation techniques to enlarge the training data. We construct a class preserving
data augmentation pipeline consisting of rotation, translation, and projection transforms and apply
this pipeline to images in the training set until each class contained 1,000 training examples. We use
this new augmented training data set containing 17,000 samples in total to train models.
Model. We use the Resnet model [HZRS16]. We perform per image standardization before feeding
images to the neural network. The network has 5 residual units with (16, 16, 32, 64) filters each.
The model is adapted from CIFAR-10 model of [MMS+ 17b]. Refer to our codes for details.
Training hyper-parameters. The hyper-parameters to train different models are listed below:
NATURAL. We use Momentum Optimizer with weight decay. We set momentum rate as 0.9, weight
decay rate as 0.0002, batch size as 16, and training steps as 70,000. We use learning rate schedule:
the first 500 steps, we use learning rate of 10−3 ; after 500 steps and before 60,000 steps, we use
learning rate of 10−2 ; after 60,000 steps, we use learning rate of 10−3 .
18

Madry et al.. We use Momentum Optimizer with weight decay. We set momentum rate as 0.9,
weight decay rate as 0.0002, batch size as 16, and training steps as 70,000. We use learning rate
schedule: the first 500 steps, we use learning rate of 10−3 ; after 500 steps and before 60,000 steps,
we use learning rate of 10−2 ; after 60,000 steps, we use learning rate of 10−3 . We use PGD attack
as adversary with random start, number of steps of 7, step size of 2, and adversarial budget  of 8.
IG-NORM. We set λ as 0.1, m in the gradient step as 50. We use Momentum Optimizer with weight
decay. We set momentum rate as 0.9, weight decay rate as 0.0002, batch size as 16, and training
steps as 70,000. We use learning rate schedule: the first 500 steps, we use learning rate of 10−4 ;
after 500 steps and before 60,000 steps, we use learning rate of 10−3 ; after 60,000 steps, we use
learning rate of 10−4 . We use PGD attack as adversary with random start, number of steps of 7, step
size of 2, m in the attack step of 5, and adversarial budget  of 8.
IG-SUM-NORM. We set β as 0.1, m in the gradient step as 50. We use Momentum Optimizer with
weight decay. We set momentum rate as 0.9, weight decay rate as 0.0002, batch size as 16, and
training steps as 70,000. We use learning rate schedule: the first 500 steps, we use learning rate of
10−4 ; after 500 steps and before 60,000 steps, we use learning rate of 10−3 ; after 60,000 steps, we
use learning rate of 10−4 . We use PGD attack as adversary with random start, number of steps of 7,
step size of 2, m in the attack step of 5, and adversarial budget  of 8.
Evaluation Attacks. For attacking inputs to change model predictions, we use PGD attack with
number of steps of 40, adversarial budget  of 8 and step size of 2. For attacking inputs to change
interpretations, we use Iterative Feature Importance Attacks (IFIA) proposed by [GAZ17]. We use
their top-k attack with k = 1000, adversarial budget  = 8, step size α = 1 and number of iterations
P = 100. We set the feature importance function as Integrated Gradients(IG) and dissimilarity
function D as Kendall’s rank order correlation. We find that IFIA is not stable if we use GPU
parallel computing (non-deterministic is a behavior of GPU), so we run IFIA three times on each
test example and use the best result with the lowest Kendall’s rank order correlation.
C.2

Why a different m in the Attack Step?

From our experiments, we find that the most time consuming part during training is using adversary
A to find x∗ . It is because we need to run several PGD steps to find x∗ . To speed it up, we set a
smaller m (no more than 10) in the attack step.
C.3

Choosing Hyper-parameters

Our IG-NORM (or IG-SUM-NORM) objective contains hyper-parameters m in the attack step, m
in the gradient step and λ (or β). From our experiments, we find that if λ (or β) is too large, the
training cannot converge. And if λ (or β) is too small, we cannot get good attribution robustness. To
select best λ (or β), we try three values: 1, 0.1, and 0.01, and use the one with the best attribution
robustness. For m in the attack step, due to the limitation of computing power, we usually set a
small value, typically 5 or 10. We study how m in the gradient step affects results on MNIST using
IG-NORM objective. We try m ∈ {10, 20, 30, · · · , 100}, and set λ = 1 and m in the attack step as
10. Other training settings are the same. The results are summarized in Table 3.
m
NA
AA
IN
CO
10 98.54% 78.05% 67.14% 0.2574
20 98.72% 80.29% 70.78% 0.2699
30 98.70% 80.44% 71.06% 0.2640
40 98.79% 73.41% 64.76% 0.2733
50 98.74% 81.43% 71.36% 0.2841
60 98.78% 89.25% 63.55% 0.2230
70 98.80% 74.78% 67.37% 0.2556
80 98.75% 80.26% 69.90% 0.2633
90 98.61% 78.54% 70.88% 0.2787
100 98.59% 89.36% 59.70% 0.2210
Table 3: Experiment results for different m in gradient step on MNIST.
19

From the results, we can see when m = 50, we can get the best attribution robustness. For objective
IG-SUM-NORM and other datasets, we do similar search for m in the gradient step. We find that
usually, m = 50 can give good attribution robustness.
C.4

Dimensionality and effectiveness of attribution attack

Similar to [GAZ17], we observe that IFIA is not so successful when number of dimensions is
relatively small. For example, on GTSRB dataset the number of dimensions is relatively small
(32 × 32 × 3), and if one uses small adversarial budget (8/255 ≈ 0.031), the attacks become not
very effective. On the other hand, even though MNIST dimension is small (28 × 28 × 1) , the attack remains effective for large budget (0.3). On Flower dataset the number of dimension is large
(128 × 128 × 3), and the attack is very effective on this dataset.
C.5

Use Simple Gradient to Compute Feature Importance Maps

We also experiment with Simple Gradient (SG) [SVZ13] instead of Integrated Gradients (IG) to
compute feature importance map. The experiment settings are the same as previous ones except that
we use SG to compute feature importance map in order to compute rank correlation and top intersection, and also in the Iterative Feature Importance Attacks (IFIA) (evaluation attacks). The results
are summarized in Table 4. Our method produces significantly better attribution robustness than
both natural training and adversarial training, except being slightly worse than adversarial training
on Fashion-MNIST. We note that Fashion-MNIST is also the only data set in our experiments where
IG results are significantly different from that of SG (where under IG, IG-SUM-NORM is significantly better). Note that IG is a princpled sommothed verison of SG and so this result highlights
differences between these two attribution methods on a particular data set. More investigation into
this phenomenon seems warranted.
Dataset

Approach
NA
AA
IN
CO
NATURAL
99.17% 0.00%
16.64% 0.0107
MNIST
Madry et al.
98.40% 92.47% 47.95% 0.2524
IG-SUM-NORM 98.34% 88.17% 61.67% 0.2918
NATURAL
90.86% 0.01%
21.55% 0.0734
Fashion-MNIST
Madry et al.
85.73% 73.01% 58.37% 0.3947
IG-SUM-NORM 85.44% 70.26% 54.91% 0.3674
NATURAL
98.57% 21.05% 51.31% 0.6000
GTSRB
Madry et al.
97.59% 83.24% 70.27% 0.6965
IG-SUM-NORM 95.68% 77.12% 75.03% 0.7151
NATURAL
86.76% 0.00%
6.72%
0.2996
Flower
Madry et al.
83.82% 41.91% 54.10% 0.7282
IG-SUM-NORM 82.35% 47.06% 65.59% 0.7503
Table 4: Experiment results for using Simple Gradient to compute feature importance maps.
C.6

Additional Visualization Results

Here we provide more visualization results for MNIST in Figure 3, for Fashion-MNIST in Figure 4,
for GTSRB in Figure 5, and for Flower in Figure 6.

20

NATURAL

IG-NORM

Top-100 Intersection: 37.0%
Kendall’s Correlation: 0.0567

Top-100 Intersection: 64.0%
Kendall’s Correlation: 0.1823

IG-SUM-NORM

Top-100 Intersection: 67.0%
Kendall’s Correlation: 0.2180

(a) For all images, the models give correct prediction – 6.

Top-100 Intersection: 43.0%
Kendall’s Correlation: 0.0563

Top-100 Intersection: 74.0%
Kendall’s Correlation: 0.1718

Top-100 Intersection: 84.0%
Kendall’s Correlation: 0.2501

(b) For all images, the models give correct prediction – 3.

Top-100 Intersection: 41.0%
Kendall’s Correlation: 0.1065

Top-100 Intersection: 83.0%
Kendall’s Correlation: 0.2837

Top-100 Intersection: 84.0%
Kendall’s Correlation: 0.3151

(c) For all images, the models give correct prediction – 2.

Figure 3: Top-100 and Kendall’s Correlation are rank correlations between original and perturbed
saliency maps. NATURAL is the naturally trained model, IG-NORM and IG-SUM-NORM are
models trained using our robust attribution method. We use attribution attacks described in [GAZ17]
to perturb the attributions while keeping predictions intact. For all images, the models give correct
predictions. However, the saliency maps (also called feature importance maps), computed via IG,
show that attributions of the naturally trained model are very fragile, either visually or quantitatively
as measured by correlation analysis, while models trained using our method are much more robust
in their attributions.

21

NATURAL

Top-100 Intersection: 50.0%
Kendall’s Correlation: 0.4595

IG-NORM

Top-100 Intersection: 63.0%
Kendall’s Correlation: 0.6099

IG-SUM-NORM

Top-100 Intersection: 87.0%
Kendall’s Correlation: 0.6607

(a) For all images, the models give correct prediction – Ankle boot.

Top-100 Intersection: 47.0%
Kendall’s Correlation: 0.1293

Top-100 Intersection: 54.0%
Kendall’s Correlation: 0.2508

Top-100 Intersection: 65.0%
Kendall’s Correlation: 0.3136

(b) For all images, the models give correct prediction – Sandal.

Top-100 Intersection: 39.0%
Kendall’s Correlation: 0.4129

Top-100 Intersection: 61.0%
Kendall’s Correlation: 0.5983

Top-100 Intersection: 71.0%
Kendall’s Correlation: 0.6699

(c) For all images, the models give correct prediction – Trouser.

Figure 4: Top-100 and Kendall’s Correlation are rank correlations between original and perturbed
saliency maps. NATURAL is the naturally trained model, IG-NORM and IG-SUM-NORM are
models trained using our robust attribution method. We use attribution attacks described in [GAZ17]
to perturb the attributions while keeping predictions intact. For all images, the models give correct
predictions. However, the saliency maps (also called feature importance maps), computed via IG,
show that attributions of the naturally trained model are very fragile, either visually or quantitatively
as measured by correlation analysis, while models trained using our method are much more robust
in their attributions.

22

NATURAL

Top-100 Intersection: 45.0%
Kendall’s Correlation: 0.5822

IG-NORM

Top-100 Intersection: 78.0%
Kendall’s Correlation: 0.7471

IG-SUM-NORM

Top-100 Intersection: 80.0%
Kendall’s Correlation: 0.7886

(a) For all images, the models give correct prediction – Dangerous Curve to The Left.

Top-100 Intersection: 56.0%
Kendall’s Correlation: 0.6679

Top-100 Intersection: 85.0%
Kendall’s Correlation: 0.7963

Top-100 Intersection: 83.0%
Kendall’s Correlation: 0.8338

(b) For all images, the models give correct prediction – General Caution.

Top-100 Intersection: 43.0%
Kendall’s Correlation: 0.6160

Top-100 Intersection: 67.0%
Kendall’s Correlation: 0.7595

Top-100 Intersection: 81.0%
Kendall’s Correlation: 0.8128

(c) For all images, the models give correct prediction – No Entry.

Figure 5: Top-100 and Kendall’s Correlation are rank correlations between original and perturbed
saliency maps. NATURAL is the naturally trained model, IG-NORM and IG-SUM-NORM are
models trained using our robust attribution method. We use attribution attacks described in [GAZ17]
to perturb the attributions while keeping predictions intact. For all images, the models give correct
predictions. However, the saliency maps (also called feature importance maps), computed via IG,
show that attributions of the naturally trained model are very fragile, either visually or quantitatively
as measured by correlation analyses, while models trained using our method are much more robust
in their attributions.

23

NATURAL

Top-1000 Intersection: 1.0%
Kendall’s Correlation: 0.4601

IG-NORM

Top-1000 Intersection: 65.4%
Kendall’s Correlation: 0.7248

IG-SUM-NORM

Top-1000 Intersection: 63.9%
Kendall’s Correlation: 0.8036

(a) For all images, the models give correct prediction – Bluebell.

Top-1000 Intersection: 6.2%
Kendall’s Correlation: 0.3863

Top-1000 Intersection: 58.20%
Kendall’s Correlation: 0.6694

Top-1000 Intersection: 65.9%
Kendall’s Correlation: 0.7970

(b) For all images, the models give correct prediction – Cowslip.

Top-1000 Intersection: 6.8%
Kendall’s Correlation: 0.4653

Top-1000 Intersection: 58.0%
Kendall’s Correlation: 0.7165

Top-1000 Intersection: 63.4%
Kendall’s Correlation: 0.8201

(c) For all images, the models give correct prediction – Tigerlily.

Figure 6: Top-1000 and Kendall’s Correlation are rank correlations between original and perturbed
saliency maps. NATURAL is the naturally trained model, IG-NORM and IG-SUM-NORM are
models trained using our robust attribution method. We use attribution attacks described in [GAZ17]
to perturb the attributions while keeping predictions intact. For all images, the models give correct
predictions. However, the saliency maps (also called feature importance maps), computed via IG,
show that attributions of the naturally trained model are very fragile, either visually or quantitatively
as measured by correlation analyses, while models trained using our method are much more robust
in their attributions.

24

