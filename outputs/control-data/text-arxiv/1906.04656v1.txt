Deep learning control of artificial avatars in group coordination tasks

arXiv:1906.04656v1 [cs.MA] 11 Jun 2019

Maria Lombardi1 , Davide Liuzza2 and Mario di Bernardo3
Abstract‚ÄîIn many joint-action scenarios, humans and robots
have to coordinate their movements to accomplish a given
shared task. Lifting an object together, sawing a wood log,
transferring objects from a point to another are all examples
where motor coordination between humans and machines is a
crucial requirement. While the dyadic coordination between a
human and a robot has been studied in previous investigations,
the multi-agent scenario in which a robot has to be integrated
into a human group still remains a less explored field of research.
In this paper we discuss how to synthesise an artificial agent
able to coordinate its motion in human ensembles. Driven by a
control architecture based on deep reinforcement learning, such
an artificial agent will be able to autonomously move itself in
order to synchronise its motion with that of the group while
exhibiting human-like kinematic features. As a paradigmatic
coordination task we take a group version of the so-called mirrorgame which is highlighted as a good benchmark in the human
movement literature.

I. INTRODUCTION
Interest in using robots and artificial avatars in joint tasks
with humans to reach a common goal is growing rapidly.
Indeed, it is possible to find numerous applications that span
from industrial tasks to entertainment, from navigation to
orientation and so on, in which artificial agents are required
to interact cooperatively with people. Examples of humanrobot interaction include the problem of jointly handling an
object [1], sawing a log of wood [2], managing a common
work-piece in a production system [3], or performing a ‚Äúpick
and place‚Äù coordination task [4]. While dyadic coordination
between humans and robots is the subject of much ongoing
research, the problem of having robots or avatars interacting
with a human team remains a seldom investigated field. This is
probably due to the complex mechanisms underlying interpersonal coordination, the different ways in which coordination
can emerge in human groups, and the potentially large amount
of data to be collected and processed in real-time.
From a control viewpoint, the emergence of multi-agent
synchronisation while performing a joint task is a phenomenon
characterised by non-linear dynamics in which an individual
has to predict what others are going to do and adjust his/her
movements in order to complement the movements of the
others in order to achieve precise and accurate temporal
correspondence [5].
In this context, an open question is to investigate whether it
is possible to influence the emergent group behaviour via the
introduction of artificial agents able be accepted in a natural
way by the group and help it achieving a collective control
goal. To reach such a goal, the artificial agent has to be able
to integrate its motion with that of the others exhibiting at the
same time typical human-like kinematic features. In this way,
the artificial agent can merge with the rest of the group and

enhance, rather disrupt, social attachment between its members
and group cohesiveness.
The problem is crucial in social robotics, where new advancements in human-robot interaction can promote novel
diagnostic and rehabilitation strategies for patient suffering
from social and motor disorders [6].
In this work we define a ‚Äúhuman-like‚Äù motion by using
the concept of ‚ÄúIndividual Motor Signature‚Äù (IMS), proposed
in [7] as a valid biomarker able to capture the peculiarity of
the human motion. Specifically, the IMS has been defined in
terms of the probability density function (PDF) of the velocity
profiles characterising a specific joint task.
The aim of this paper is to present a control architecture
based on deep learning to drive an artificial agent able in
performing a joint task in a multi-agent scenario while exhibiting a desired IMS. As a scenario of interest, we take a
multiplayer version of the mirror game proposed in [8] as
a paradigmatic task of interpersonal motor coordination. In
our version of the mirror game, first proposed in [7], a group
of players is asked to oscillate a finger sideways performing
some interesting motion and synchronising theirs with that of
the others (see [9] for further details).
The approach we follow is an extension to groups of the
strategy we presented in [10], [11], [12] in the case of dyadic
interactions. Specifically, in [12] we designed an autonomous
cyber player able to play dyadic leader-follower sessions of the
mirror game with different human players. Such a cyber player
was driven by a Q-learning algorithm aiming at exhibiting the
kinematic features of a target human player in order to emulate
hers/his way of moving when engaged in a dyadic interaction.
Extending the Q-learning approach to multi-agent systems is
cumbersome as the approach is unscalable with the growth of
the system state space due to the addition of other players. To
overcome this limitation, we use ‚Äúdeep reinforcement learning‚Äù [13], [14], [15], combining the reinforcement learning
strategy with the powerful generalization capabilities of neural
networks. To design the control architecture of the cyber
player, we collect motor measurements signals of four different
players involved in a joint oscillatory task and then train
the CP to mimic the way of moving of one of them. The
validation is done replacing the target player with the cyber
player and comparing the group performance in order to prove
the effectiveness of the proposed control approach.
II. PRELIMINARIES
A group of people interacting with each other can be described as a complex network system in which each individual
is represented as a node (or agent) with its own dynamics
while the visual coupling with the other members of the
group as edges in a graph describing the network of their

interactions. The structure of the interconnections established
among the groups‚Äô members is formalised by the adjacency
matrix A := aij , in which the element aij = 1 only if the node
j is linked with the node i, or in other terms if the individual
j is visually coupled with the individual i.
Four different topologies are considered in this work as
shown in Fig. 1. As described in [9] these different topologies
can be implemented experimentally by changing the way in
which participants sit with respect to each other and by asking
them to wear appropriate goggles restricting their field-ofview.

all possible combinations between the set X and the set U in
order to evaluate them in terms of action-value functions in a
tabular form. As this is unfeasible in our group scenario, we
use the deep learning control approach shown in Fig. 2 where:
‚Ä¢

‚Ä¢

‚Ä¢

Fig. 1: Network topologies. (a) Complete graph. (b) Ring
graph. (c) Path graph. (d) Star graph.
Let Œ∏k (t) be the phase of the kth agent estimated by
taking the Hilbert transform of its position signal, say xk (t).
The cluster phase of N agents is defined as q(t) :=
1 PN
ejŒ∏k (t) , which represents the average phase of the
N k=1
group at time t. The term œÜk (t) := Œ∏k (t) ‚àí q(t) is the relative
phase between the kth agent and the group phase at time t.
The level of coordination reached by a human group performing an oscillatory task can be investigated by evaluating
the group synchronisation index œÅg (t) introduced in [9], [16]
and defined as follows:
œÅg (t) :=

N
1 X j(œÜk (t)‚àíœÜÃÑk )
e
N

‚àà [0, 1] ,

‚Ä¢

Àô where [x, xÃá] are
the state space is x := [x, xÃá, xÃÑ, xÃÑ]
Àô are mean
position and velocity of the CP, while [xÃÑ, xÃÑ]
position and mean velocity of the players connected to
the target players;
the action space is made up of 9 different values of
acceleration in [‚àí4, 4], empirically chosen looking at the
typical human accelerations while performing the same
joint tasks;
2
the reward function is selected as œÅ := ‚àí (x ‚àí xt ) ‚àí
2
2
0.1 (xÃá ‚àí xÃát ) ‚àí Œ∑u where [xt , xÃát ] are position and velocity of the target player, while the constant parameter
Œ∑ tunes the control effort. Maximising a reward function
so designed means to minimise the squared error both in
position and in velocity between the CP and the target
player;
the policy œÄ according to which the CP chooses the action
to take in a specific state is an -greedy policy [13].
Following that policy, the CP takes the best known action
with (1 ‚àí ) probability (exploitation), whereas with 
probability it takes a random action (exploration). The
value  follows a monotonic decreasing function, since
as time increases the exploration phase is replaced by
the exploitation phase.

In particular, we exploit the Deep Q-network (DQN) strategy where an artificial neural network (ANN) is used to
approximate the optimal action-value function Q‚àó defined as:

(1)

k=1

where œÜÃÑk is œÜk (t) averaged over time. Closer the synchronisation index is to 1, higher is the level of synchronisation in
the group.
III. DEEP REINFORCEMENT LEARNING
APPROACH
A. Brief overview
Reinforcement learning is a machine learning technique in
which an agent tries to learn how to behave in an unknown
environment taking, in any situation, the best action that it can
perform. The problem is formalized by considering a set X
of all possible states in which the environment can be (statespace), a set U of all possible actions that the agent can take
(action-space) and an auxiliary function, named action-value
function (or Q-function), that quantifies the expected return
(reward) starting from a specific state and taking a specific
action. Through the action-value function, the goal of the
learning agent is to iteratively refine its policy in order to
maximise the expected reward. Solving a problem with the
classical Q-learning approach [17] means to iteratively explore

"
Q‚àó (x, u)

=

max E
œÄ

‚àû
X

#
Œ≥ m rk+m+1 |xk = x, u , (2)

m=0

which maximises the expected value of the sum of the rewards
r discounted by a positive factor Œ≥ < 1, obtained taking the
action u in the state x following the policy œÄ at any time
instant k.
Training an ANN in order to approximate a desired function
(Q-function) means to find the vector of network weights Œ∏
of the connections between the neurons, iteratively evaluated
by back-propagation algorithms in order to minimise a loss
function. The loss function is used to measure the error
between the actual and the predicted output of the neural
network (e.g. mean squared error) (see [18] for further details).
Contrary to what is done in traditional supervised learning
with ANN where the predicted output is well defined, in
the Deep Q-network approach the loss function is iteratively
changed because the predicted output itself depends on the
network parameters Œ∏k at every instant k. Namely, the loss

Process

1

2

xk := [xk , xÃák , xÃÑk , xÃÑÀôk ]

uk

Policy

uk = œÄ (qk , xk )
4

3

qk

xk+1
Reward

z ‚àí1

rk+1 = œÅ (xk , uk , xk+1 )

"T
! u1
u2
u9
, qk+1
, . . . , qk+1
qk+1 := qk+1

rk+1
NN training

Qk+1 (Œ∏k+1 ) = g (Qk (Œ∏k ) , xk , uk , xk+1 , rk+1 )
Œ∏k1,2

Hidden layer 1

Œ∏k2,3

Hidden layer 2

u1
qk+1

Œ∏k3,4

u2
qk+1

u3
qk+1
u4
qk+1

xk
x 64

xÃák

x 32

u5
qk+1

xÃÑk

u6
qk+1

xÃÑÀô k

u7
qk+1

u8
qk+1

u9
qk+1

Fig. 2: Block scheme of the deep Q network algorithm. At each iteration, the RL controller chooses the control u according
to the current neural network and the system‚Äôs state. The process evolves in a new state generating the reward r. The reward,
previous and current state are then used as a new sample to train the neural network.

function is chosen as:
"
Lk (Œ∏k ) = E

rk + Œ≥ max Q (xk+1 , uk+1 , Œ∏k‚àí1 ) ‚àí
uk+1

!2 #
Q (xk , uk , Œ∏k )

, (3)

which represents the mean squared error between the current
estimated Q function and the approximate optimal actionvalue function.
It has been proved that an ANN with a single hidden
layer containing a large enough number of sigmoid units
can approximate any continuous function, while a second
layer is added to improve accuracy [19]. Relying on that, the
neural network we considered to approximate the action-value
function Q in (2) is designed as a feedforward network with
(Fig. 2):
‚Ä¢ an input layer with 4 different nodes, one for each state
Àô
variable [x, xÃá, xÃÑ, xÃÑ];
‚Ä¢ two hidden layers, empirically found, made up of 64
and 32 nodes each implementing a sigmoidal activation
function;
‚Ä¢ an output layer with 9 different nodes, one for each
action available in the set U . In the DQN, the network

output returns the estimated action-value q u for each
possible action u ‚àà U in a single shot reducing in this
way the time needed for the training. Then, the action
corresponding to the maximum q-value (neuron‚Äôs output)
is chosen as the next control input.
Reinforcement learning is known to be unstable or even
to diverge when a nonlinear function approximator, such
as an ANN, is used to estimate the Q-function [13], [14].
According to the existing literature, this instability is caused
by: (i) the presence of correlation in the sequence of observed
states and (ii) the presence of correlation between the current
estimated Q and the target network, resulting in the loss of the
Markov property. The correlation in the observation sequence
is removed by introducing the experience replay mechanism,
where the observed states used to train the ANN are not
taken sequentially but are sampled randomly from a circular
buffer. Also, the correlation between the current estimate of the
function Q and the target optimal one Q‚àó is reduced updating
the latter at a slower rate.
B. CP Implementation
According to the reinforcement learning strategy with the
Deep Q-network described above, the CP refines its policy
according to the system states and the reward received so as

to take the best action it can to mimic the target player(s).
To implement the DQN as a first step, a feedforward neural
network needs to be initialized with random values. The
experience replay mechanism is implemented instantiating an
empty circular buffer in order to store the system‚Äôs state at
each iteration.
Then at each iteration we have (Fig. 2):
1) the CP observes the process‚Äôs state xk at time instant
k and performs an action uk according to the policy œÄ,
that is an -greedy policy;
2) the process evolves to a new state xk+1 and the CP
receives the reward rk+1 that measures how good taking
the action u in the state xk has been;
3) the new sample hxk , uk , xk+1 , rk+1 i is added to the
circular buffer and a random batch taken from it is
used to train the NN. The training is done through
the gradient descend back-propagation algorithm with
momentum [13] so as to tune the network‚Äôs weights Œ∏
in order to minimize the loss function (3). We denote
the network‚Äôs weights between the layer n and n + 1 at
instant k as Œ∏kn,n+1 .
The steps above are repeated until convergence is achieved
according to the ‚Äútermination criterion‚Äù:
kRM ST P,xÃÑ ‚àí RM SCP,xÃÑ k ‚â§ 

(4)

where RM Si,xÃÑ is the root mean square error between the
position of the player i ‚àà [T P, CP ] and the mean position
of the group, while  is a non-negative parameter.
IV. TRAINING AND VALIDATION
A. Training
Ideally, data used to train the CP are extracted from real
human players playing the mirror game. In our case, due to
the lack of a large enough dataset, the data needed to feed
the CP during the training are generated synthetically making
artificial agents modelling human players perform sessions of
the mirror game against each other. We refer to these other
artificial agents as Virtual Players (VP) to distinguish them
from the CP since they are driven by a completely different
architecture which is not based on AI and was presented in
[10] and improved in [11]. The use of virtual players for
training AI based CPs was first proposed in [12] for dyadic
interaction and is applied here for the first time to the multiplayer version of the game.
Specifically, the motion of the virtual player used to generate
synthetic data is that of a controlled nonlinear HKB oscillator
[20] of the form:

xÃà + Œ±x2 + Œ≤ xÃá2 ‚àí Œ≥ xÃá + œâ 2 x = u,
(5)
where x, xÃá and xÃà are position, velocity and acceleration of the
VP end effector, respectively, Œ±, Œ≤, Œ≥ are positive empirically
tuned damping parameters while œâ is the oscillation frequency.

The control input u is chosen following an optimal control
strategy aiming at minimising the following cost function [21]:
min J (tk ) =
u

Œ∏œÉ
2
Œ∏v
2

Z

Œ∏p
2
(x (tk+1 ) ‚àí rp (tk+1 )) +
2Z

tk+1

tk+1

2

(xÃá (œÑ ) ‚àí rÃáœÉ (œÑ )) dœÑ +
tk
2

(xÃá (œÑ ) ‚àí rÃáp (œÑ )) dœÑ +
tk

Œ∑
2

Z

tk+1

2

u (œÑ ) dœÑ, (6)
tk

where rp , rÃáp is the position and the velocity time series of
the partner player, rÃáœÉ is the reference signal modelling the
desired human motor signature, Œ∑ tunes the control effort, tk
and tk+1 represent the current and the next optimization time
instant. Œ∏p , Œ∏s , Œ∏v are positive control parameters satisfying
the constraint Œ∏p + Œ∏s + Œ∏v = 1. By tuning appropriately
these parameters, it is possible to change the VP configuration
making it act as a leader, follower or joint improviser (more
details are in [10], [21]). In the case of a multi-player scenario,
rp and rÃáp are taken as the mean value of the position and the
velocity of the target player‚Äôs neighbours, that is:
rp :=

M
1 X
xj ;
M j=1

rÃáp :=

M
1 X
xÃáj ,
M j=1

(7)

where M is the number of neighbours and xj and xÃáj are the
position and the velocity of the jth neighbour.
The reference signal rÃáœÉ captures in some way the desired
human kinematic features that the VP has to exhibit during
the game. In [11] we developed a methodology based on the
theory of stochastic processes and observational learning to
generate human-like trajectories in real time. In particular,
a Markov Chain (MC) was derived to capture the peculiar
internal description model of the motion of a human player
simply observing him/her playing sessions of the mirror game
in isolation.
To train the CP to coordinate its movements in the group
like the virtual player target does, a group of 4 different virtual
players interconnected in a complete graph were used (Fig. 3).
In particular we selected four Markov models (one for each
player) of different human players which were parametrized in
[11]. Without loss of generality, VP 4 was taken as the target
player the Deep Learning driven CP has to mimic.
The parameters proposed for the control architecture of the
VPs were tuned experimentally as follows: Œ± = 1, Œ≤ = 2, Œ≥ =
‚àí1, œâ = 1 for the inner dynamics, Œ∏p = 0.8, Œ∏œÉ = 0.15, Œ∏v =
0.05, Œ∑ = 10‚àí4 and T = 0.03s for the control law. The
experience replay in the CP algorithm was implemented with a
buffer of 200, 000 elements, batches of 32 sampled states were
used to train the feedforward neural network at each iteration.
A target network updated every 150 time steps was considered
in the Q-function, with a discount factor Œ≥ = 0.95.
The training stage was carried out on a Desktop computer
having an Intel Core i7-6700 CPU, 16 GB of RAM and 64bit Windows operative system. It took 1, 500 trials of 500
observations each to converge (around 8.5 hours) according
to the criterion (4). In Fig. 4 the training curve is reported

graph, star graph (described in Sec. II) with node 3 as center.
For the sake of brevity, in Fig. 5 only the validation for the
complete and the path graph are reported. The performance of
the CP has been evaluated by comparing its behaviour with
that of the target virtual player it was trained to mimic. The
CP (in red) successfully tracks the mean position of the group
(dashed line in black) being able to mimic the target player it
has been trained to imitate (in blue) [panel (a)]. The relative
position error (RPE) defined as
Ô£±

Àô
Ô£¥
Ô£≤ xÃÑ(t) ‚àí xV P/CP (t) sgn (xÃÑ(t))

Àô
(8)
RP E =
if sgn (xÃÑ(t)) = sgn xV P/CP (t) 6= 0
Ô£¥
Ô£≥ xÃÑ(t) ‚àí x
,
otherwise
(t)
V P/CP

Fig. 3: Architecture used to train the CP to mimic the target
player (ball in green). The group is simulated by 4 VPs playing
the mirror game in joint improvisation and driven by the
cognitive architecture based on optimal control and Markov
chains. The CP, driven by a deep reinforcement learning
approach, receives in real time the data from the group and
learns how to interact in order to replace the player target.

Fig. 4: Training curve showing the convergence of the algorithm. The root mean square error in position (y-axis) is
reported for each trial (x-axis) both for the VP (in blue) and
the CP (in red). xÃÑ represents the mean position of the target
player‚Äôs neighbours.

showing for each trial the RMS between the VP and the
group (in blue), and between the CP and the group (in red).
Convergence is reached in about 1, 000 trials on.
B. Validation
To show that the CP is effectively able to emulate the VP
target when engaged in a group, training was carried out by
considering a group described by a complete graph, we then
validated the performance of the CP when interacting over
different topologies. Specifically we used the ring graph, path

has been also evaluated between the VP target and the mean
position of the neighbours and compared with the relative
position error between the CP and the same mean position
[panel (c)]. Both the errors are very small and with comparable
mean values.
Similar considerations can be done for behaviour of the CP
when the group topology is a path graph as shown in panels
(b) and (d).
The key features of the motion of the CP and the VP it
has been trained upon are captured by the following metrics:
1) relative phase error defined as ‚àÜŒ¶ = Œ¶xÃÑ ‚àí Œ¶CP/V P , 2)
the RMS error between the position of the CP (or VP) and
that of the group mean position, and 3) the time lag which
describes the amount of time shift that achieves the maximum
cross-covariance between the two position time series. This
can be interpreted as the average reaction time of the player
in the mirror game [9].
The metrics described above were evaluated performing 20
trials and reporting both the mean value and the standard
deviation for both the complete graph (Tab. Ia) and the path
graph (Tab. Ib). It is possible to notice that all indexes show
a remarkable degree of similarity between the motion of the
CP and that of the target VP.
TABLE I: Metrics are reported for 20 trials of the multiplayer
mirror game both in complete graph (a) and in path graph (b).
The relative phase, the RMS and the time lag are evaluated
between the mean of the neighbours with the CP (first column)
and with the VP target (second column).
Metric
Relative phase
RMS
Time lag

CP
‚àí0.1680 ¬± 0.0513
0.0443 ¬± 0.0054
‚àí0.1140 ¬± 0.0157

VP target
‚àí0.0498 ¬± 0.0948
0.0556 ¬± 0.0037
‚àí0.0165 ¬± 0.0357

(a) Complete Graph
Metric
Relative phase
RMS
Time lag

CP
‚àí0.2296 ¬± 0.0788
0.0555 ¬± 0.0047
‚àí0.1035 ¬± 0.0153

VP target
‚àí0.1589 ¬± 0.0982
0.0597 ¬± 0.0035
‚àí0.0675 ¬± 0.0236

(b) Path Graph

For further evidence, in Fig. 6 the group level synchronization is reported for each tested topology. Despite the

(a)

(b)

Complete Graph

0.5

Mean Group

0.5

VP target

Mean Group

VP target

0

0

-0.5

-0.5
0

10

20

30

40

0.5

50

Mean Group

0

60

10

20

30

40

0.5

CP

50

Mean Group

60

CP

0

0

-0.5

-0.5
0

10

20

30

40

50

0

60

(c)
0.4

0.2

0.2

0

10

20

30

40

10

20

30

40

50

60

(d)

0.4

0

Path Graph

10

20

30

40

50

60

0

0

50

60

Fig. 5: Position time series of group sessions. (a)-(b) The position trajectories of the VP target (in blue) and of the CP (in
red) are reported together with the mean position of the neighbours (in dashed back). (c)-(d) Relative position error evaluated
between the mean position and the position of the VP target (in blue) and the CP respectively. Two different topologies are
depicted: (a)-(c) the complete graph and (b)-(d) the path graph.
different topologies, the presence of the CP does not alter the
group dynamics when the CP is substituted to the VP it was
trained upon. We notice that the level of coordination varies
with the topology, confirming what found in [9]. Specifically,
in [9] as confirmed by Fig. 6, the complete and the star
graph were found to be associated with the highest level of
synchronization.
V. CONCLUSIONS
In this work we addressed the problem of synthesizing
an autonomous artificial agent able to coordinate its movements and perform a joint motor task in a group setting.
In particular, a multiplayer version of the mirror game was
used as a paradigmatic task where different individuals have
to synchronize their oscillatory motion. To achieve our goal
and overcome the limitations of previous approaches, we
introduced a deep reinforcement learning control algorithm in
which a feedforward neural network is used to approximate
the nonlinear action-value function. The DQN allowed us to
overcome the limitations of the Q-learning approach presented
in [12] which is impractical when the state space becomes too
large, as in the case of multiplayer coordination tasks. The
effectiveness of the cyber player trained upon a target group
member was shown by comparing its performance when playing in groups with different interconnection topologies. The

numerical validations show the effectiveness of our approach.
Ongoing work is being carried out to validate the behaviour of
the CP when interacting with a real group of people through
the experimental platform Chronos we presented in [22].
R EFERENCES
[1] A. Edsinger and C. C. Kemp, ‚ÄúHuman-Robot Interaction for Cooperative
Manipulation : Handing Objects to One Another,‚Äù in IEEE International
Symposium on Robot and Human Interactive Communication, 2007, p.
e9825497.
[2] L. Peternel, T. PetricÃå, E. Oztop, and J. BabicÃå, ‚ÄúTeaching robots to
cooperate with humans in dynamic manipulation tasks based on multimodal human-in-the-loop approach,‚Äù Autonomous Robots, vol. 36, no.
1-2, pp. 123‚Äì136, 2014.
[3] M. Faber, J. BuÃàtzler, and C. M. Schlick, ‚ÄúHuman-robot Cooperation in
Future Production Systems: Analysis of Requirements for Designing an
Ergonomic Work System,‚Äù Procedia Manufacturing, vol. 3, pp. 510‚Äì517,
2015.
[4] M. Lamb, T. Lorenz, S. J. Harrison, R. W. Kallen, A. Minai, and
M. J. Richardson, ‚ÄúPAPAc: A Pick and Place Agent Based on Human
Behavioral Dynamics,‚Äù Proceedings of the 5th International Conference
on Human Agent Interaction - HAI ‚Äô17, pp. 131‚Äì141, 2017.
[5] C. Vesper, S. Butterfill, G. Knoblich, and N. Sebanz, ‚ÄúA minimal
architecture for joint action,‚Äù Neural Networks, vol. 23, no. 8-9, pp.
998‚Äì1003, 2010.
[6] I. Amado, L. BreÃÅnugat-HerneÃÅ, E. Orriols, C. Desombre, M. Dos Santos,
Z. Prost, M. O. Krebs, and P. Piolino, ‚ÄúA serious game to improve cognitive functions in schizophrenia: A pilot study,‚Äù Frontiers in Psychiatry,
vol. 7, pp. 1‚Äì11, 2016.

Group Synchronization
1
0.9

CP
VP target

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

CG

PG

RG

SG

Fig. 6: Histogram reporting the group synchronization level
reached by the group both with the VP target (blue bars) and
with the CP (red bars). Different topologies were implemented
during the validation: complete graph (CG), path graph (PG),
ring graph (RG) and star graph (SG) with player 3 as center
node.

[7] P. S≈ÇowinÃÅski, C. Zhai, F. Alderisio, R. Salesse, M. Gueugnon, L. Marin,
B. G. Bardy, M. di Bernardo, and K. Tsaneva-Atanasova, ‚ÄúDynamic
similarity promotes interpersonal coordination in joint action,‚Äù Journal
of The Royal Society Interface, vol. 13, no. 116, p. 20151093, 2016.
[8] L. Noy, E. Dekel, and U. Alon, ‚ÄúThe mirror game as a paradigm for
studying the dynamics of two people improvising motion together,‚Äù
Proceedings of the National Academy of Sciences, vol. 108, no. 52,
pp. 20 947‚Äì20 952, 2011.
[9] F. Alderisio, G. Fiore, R. N. Salesse, B. G. Bardy, and M. D. Bernardo,
‚ÄúInteraction patterns and individual dynamics shape the way we move
in synchrony,‚Äù Scientific Reports, vol. 7, no. 1, p. 6846, 2017.
[10] C. Zhai, F. Alderisio, P. Slowinski, K. Tsaneva-Atanasova, and M. Di
Bernardo, ‚ÄúDesign and Validation of a Virtual Player for Studying
Interpersonal Coordination in the Mirror Game,‚Äù IEEE Transactions on

Cybernetics, vol. 48, no. 3, pp. 1018‚Äì1029, 2018.
[11] M. Lombardi, D. Liuzza, and M. di Bernardo, ‚ÄúGeneration and classification of individual behaviours for virtual players control in motor
coordination tasks,‚Äù in European Control Conference (ECC), Limassol
(Cyprus), 2018, pp. 2374 ‚Äì 2379.
[12] ‚Äî‚Äî, ‚ÄúUsing learning to control artificial avatars in human motor
coordination tasks,‚Äù pp. 1‚Äì12, 2018.
[13] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction,
2nd ed. MIT press, 2018.
[14] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, and A. K. Fidjeland, ‚ÄúHumanlevel control through deep reinforcement learning,‚Äù Nature Letter, vol.
518, pp. 529‚Äì533, 2015.
[15] Y. Li, ‚ÄúDeep Reinforcement Learning,‚Äù pp. 1‚Äì150, 2018.
[16] M. J. Richardson, R. L. Garcia, T. D. Frank, M. Gergor, and K. L. Marsh,
‚ÄúMeasuring group synchrony: A cluster-phase method for analyzing
multivariate movement time-series,‚Äù Frontiers in Physiology, vol. 3 OCT,
no. October, pp. 1‚Äì10, 2012.
[17] C. J. C. H. Watkins and P. Dayan, ‚ÄúQ-learning,‚Äù Machine Learning,
vol. 8, no. 3-4, pp. 279‚Äì292, 1992.
[18] S. Russel and P. Norvig, Artificial Intelligence: A Modern Approach,
3rd ed. Prentice hall, 2003.
[19] G. Cybenko, ‚ÄúApproximation by superpositions of a sigmoidal function,‚Äù
Mathematics of Control, Signals and Systems, vol. 2, pp. 303‚Äì314, 1989.
[20] H. Haken, J. A. S. Kelso, and H. Bunz, ‚ÄúA Theoretical Model of
Phase Transitions in Human Hand Movements,‚Äù Biological Cybernetics,
vol. 51, pp. 347‚Äì356, 1985.
[21] C. Zhai, F. Alderisio, P. Slowinski, K. Tsaneva-Atanasova, and
M. di Bernardo, ‚ÄúDesign of a Virtual Player for Joint Improvisation with
Humans in the Mirror Game,‚Äù PLoS ONE, vol. 11, no. 4, p. e0154361,
2016.
[22] F. Alderisio, M. Lombardi, G. Fiore, and M. di Bernardo, ‚ÄúA novel
computer-based set-up to study movement coordination in human ensembles,‚Äù Frontiers in Psychology, vol. 8, p. 967, 2017.

