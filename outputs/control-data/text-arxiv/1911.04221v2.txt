CONVERGENCE TO MINIMA FOR THE CONTINUOUS VERSION
OF BACKTRACKING GRADIENT DESCENT

arXiv:1911.04221v2 [math.OC] 13 Nov 2019

TUYEN TRUNG TRUONG
Abstract. Lee et al. and Panageas and Piliouras showed that if f : Rk → R is both
a C 2 and CL1,1 function, and if δ < 1/L, then there is a set E of Lebesgue measure
0 so that if x0 ∈ Rk \E, then the Standard Gradient Descent (Standard GD) process
xn+1 = xn − δ∇f (xn ), if converges, cannot converge to a generalised saddle point.
(Remark: for the convergence of {xn }, more assumptions are needed, see the appendix
of this paper.) In this paper, we prove a stronger result when replacing the Standard
GD by a continuous version of Backtracking GD. More precisely, we have:
Theorem. Let f : Rk → R be a C 1 function, so that ∇f is locally Lipschitz
continuous. Assume moreover that f is C 2 near its generalised saddle points. Fix real
numbers δ0 > 0 and 0 < α < 1. Then there is a smooth function h : Rk → (0, δ0 ] so that
the map H : Rk → Rk defined by H(x) = x − h(x)∇f (x) has the following property:
(i) For all x ∈ Rk , we have f (H(x))) − f (x) ≤ −αh(x)||∇f (x)||2 .
(ii) For every x0 ∈ Rk , the sequence xn+1 = H(xn ) either satisfies limn→∞ ||xn+1 −
xn || = 0 or limn→∞ ||xn || = ∞. Each cluster point of {xn } is a critical point of f . If
moreover f has at most countably many critical points, then {xn } either converges to a
critical point of f or limn→∞ ||xn || = ∞.
(iii) There is a set E1 ⊂ Rk of Lebesgue measure 0 so that for all x0 ∈ Rk \E1 , the
sequence xn+1 = H(xn ), if converges, cannot converge to a generalised saddle point.
(iv) There is a set E2 ⊂ Rk of Lebesgue measure 0 so that for all x0 ∈ Rk \E2 , any
cluster point of the sequence xn+1 = H(xn ) is not a saddle point, and more generally
cannot be an isolated generalised saddle point.
When the local Lipschitz constants for ∇f are bounded from above by a continuous
function, we use the same idea to prove the same result for a new discrete version
of Backtracking GD. The condition we need is still more general than that required in
those results mentioned above by Lee et al. and Panageas and Piliouras. Similar results
hold for Backtracking versions of Momentum and NAG, first defined in our joint work
with T. H. Nguyen.
Since the literature on convergence for Gradient Descent methods can be very confusing, in the Appendix of this paper we will also provide a brief overview of previous
major convergence results for Gradient Descent methods for the readers’ convenience.

Date: November 14, 2019.
1

2

TUYEN TRUNG TRUONG

1. Introduction
This paper is about convergence to minima for Gradient Descent (GD) methods, with
a view towards applications in Deep Neural Networks. In this section we explain briefly
GD methods and state the main results of the paper, together with some remarks. A
review the current state-of-the-art of theoretical results on convergence for GD methods,
together with arguments for why one should care about convergence of these methods
and allow more general cost functions in applications in Deep Neural Networks, is given
in the Appendix of the paper.
1.1. Gradient descent methods. Being able to minimise a C 1 function f : Rk → R
is an important problem in both theory and applications. In practical applications, one
does not hope to find closed form solutions to this minimisation problem, but instead
switch to iterative methods.
In this paper, we concentrate on Gradient Descent (GD) methods, which are used in
many fields such as Deep Learning. The general version of this method, invented by
Cauchy in 1847, is as follows. Let ∇f (x) be the gradient of f at a point x, and ||∇f (x)||
its Euclidean norm in Rk . We choose randomly a point x0 ∈ Rk and define a sequence
xn+1 = xn − δ(xn )∇f (xn ),
where δ(xn ) > 0 (learning rate), is appropriately chosen. We hope that the sequence
{xn } will converge to a (global) minimum point of f .
The simplest and most known version of GD is Standard GD, where we choose δ(xn ) =
δ0 for all n, here δ0 is a given positive number. Because of its simplicity, it has been used
frequently in Deep Neural Networks and other applications. Another basic version of
GD is (discrete) Backtracking GD, which works as follows. We fix real numbers δ0 > 0
and 0 < α, β < 1. We choose δ(xn ) to be the largest number δ among the sequence
{β m δ0 : m = 0, 1, 2, . . .} satisfying the Amijo’s condition:
f (xn − δ∇f (xn )) − f (xn ) ≤ −αδ||∇f (xn )||2 .
There are also the inexact version of GD (see e.g. [4, 27]). The main results of this
paper hold also for the inexact version of Backtracking GD, but to keep the paper concise
in order to convey better the main ideas behind, we concentrate on this exact version
only. More complicated variants of the above two basic GD methods include: Momentum,
NAG, Adam, for Standard GD (see an overview in [25]); and Two-way Backtracking GD,
Backtracking Momentum, Backtracking NAG for Backtracking GD (first defined in [27]).
There is also a stochastic version, denoted by SGD, which is usually used to justify the

3

use of Standard GD in Deep Neural Networks. An overview of the asymptotic behaviour
of GD methods is included in Subsection 4.2.
For later use, here we recall that a function f is in class CL1,1 , for a positive number
L < ∞, if ∇f (x) is globally Lipschitz continuous with Lipschitz constant L. We also
recall that a point x∞ is a cluster point of a sequence {xn } if there is a subsequence {xnj }
which converges to x∞ .
Remark. In Subsection 1.3, we will define a new discrete version of Backtracking
GD, for a class of cost functions including all C 2 functions and all CL1,1 functions.
1.2. Abundance of saddle points. Besides minima, other common critical points for a
function are maxima and saddle points. In fact, for a C 2 cost function, a non-degenerate
critical point can only be one of these three types. While maxima are rarely a problem
for descent methods, saddle points can theoretically be problematic, as we will present
later in this subsection. Before then, we recall definitions of saddle points and generalised
saddle points for the sake of unambiguous presentation. Let f : Rk → R be a C 1 function.
Let x0 be a critical point of f near it f is C 2 .
Saddle point. We say that x0 is a saddle point if the Hessian ∇2 f (x0 ) is non-singular
and has both positive and negative eigenvalues.
Generalised saddle point. We say that x0 is a generalised saddle point if the
Hessian ∇2 f (x0 ) has at least one negative eigenvalue. Hence, this is the case for a nondegenerate maximum point.
In practical applications, we would like the sequence {xn } to converge to a minimum
point. It has been shown in [11] via experiments that for cost functions appearing in DNN
the ratio between minima and other types of critical points becomes exponentially small
when the dimension k increases, which illustrates a theoretical result for generic functions
[7]. Which leads to the question: Would in most cases GD converge to a minimum? This
question will be addressed in our main theorems, and readers can consult Subsection 4.2
for a detailed discussion about previous work. Before the work in [17], we are not aware
of any work in the literature which systematically and rigorously treats the avoidance of
saddle points under general settings, both for GD and other iterative methods such as
Newton’s.
1.3. The main results. We are now ready to state the main results of this paper.
Theorem 1.1. Let f : Rk → R be a C 1 function, so that ∇f is locally Lipschitz continuous. Assume moreover that f is C 2 near its generalised saddle points. Fix real numbers
δ0 > 0 and 0 < α < 1. Then there is a smooth function h : Rk → (0, δ0 ] so that the map
H : Rk → Rk defined by H(x) = x − h(x)∇f (x) has the following property:

4

TUYEN TRUNG TRUONG

(i) For all x ∈ Rk , we have f (H(x))) − f (x) ≤ −αh(x)||∇f (x)||2.
(ii) For every x0 ∈ Rk , the sequence xn+1 = H(xn ) either satisfies limn→∞ ||xn+1 −
xn || = 0 or limn→∞ ||xn || = ∞. Each cluster point of {xn } is a critical point of f . If
moreover, f has at most countably many critical points, then {xn } either converges to a
critical point of f or limn→∞ ||xn || = ∞.
(iii) There is a set E1 ⊂ Rk of Lebesgue measure 0 so that for all x0 ∈ Rk \E1 , the
sequence xn+1 = H(xn ), if converges, cannot converge to a generalised saddle point.
(iv) There is a set E2 ⊂ Rk of Lebesgue measure 0 so that for all x0 ∈ Rk \E2, any
cluster point of the sequence xn+1 = H(xn ) is not a saddle point, and more generally
cannot be an isolated generalised saddle point.
Using the idea in the proof of Theorem 1.1, we can prove the same conclusion for
a new discrete version of Backtracking GD, under assumptions more general than
those needed in [17, 21] for Standard GD. We first describe this new discrete version of
Backtracking GD.
Definition 1.2. (Backtracking GD-New.) Let f : Rk → R be a C 1 function. Assume
that there are continuous functions r, L : Rk → (0, ∞) so that for each x ∈ Rk , the map
∇f is Lipschitz continuous on B(x, r(x)) with Lipschitz constant L(x).
The Backtracking GD-New procedure is defined as follows. Fix δ0 > 0 and 0 < α, β < 1.
b to be the largest number δ among {β n δ0 : n = 0, 1, 2, . . .}
For each x ∈ Rk , we define δ(x)
which satisfies the two conditions

δ < α/L(x),
δ||∇f (x)|| < r(x).
For any x0 ∈ Rk , we then define the sequence {xn } as follows
b n )∇f (xn ).
xn+1 = xn − δ(x
Examples. (i) If f is in CL1,1 , then f satisfies the condition in Definition 1.2 by defining
L(x) = L for all x. (ii) If f is in C 2 , then we can choose any continuous function L(x) so
that L(x) ≥ maxz∈B(x,r(x)) ||∇2 f ||, where r : Rk → (0, ∞) is any continuous function.
We have the following result for the new discrete version of Backtracking GD in Definition 1.2.
Theorem 1.3. Let f : Rk → R be a C 1 function which satisfies the condition in Definition
1.2. Assume moreover that ∇f is C 2 near its generalised saddle points. Choose 0 < δ0

5

b n )∇f (xn )
and 0 < α, β < 1. For any x0 ∈ Rk , we construct the sequence xn+1 = xn − δ(x
as in Definition 1.2. Then:

b n )∇f (xn )) − f (xn ) ≤ −(1 − α)δ(x
b n )||∇f (xn )||2 .
(i) For all n we have f (xn − δ(x

(ii) For every x0 ∈ Rk , the sequence {xn } either satisfies limn→∞ ||xn+1 − xn || = 0 or

limn→∞ ||xn || = ∞. Each cluster point of {xn } is a critical point of f . If moreover, f
has at most countably many critical points, then {xn } either converges to a critical point
of f or limn→∞ ||xn || = ∞.
(iii) For random choices of δ0 , α and β, there is a set E1 ⊂ Rk of Lebesgue measure
0 so that for all x0 ∈ Rk \E1 , any cluster point of the sequence {xn } cannot be a saddle
point, and more generally cannot be an isolated generalised saddle point.
(iv) Assume that there is L > 0 so that if x is a non-isolated generalised saddle point
of f , then L(x) ≤ L. Then, for random choices of δ0 , α and β with δ0 < α/L or
β n0 +1 δ0 < α/L < β n0 δ0 for some n0 , there is a set E2 ⊂ Rk of Lebesgue measure 0 so
that for all x0 ∈ Rk \E2 , if the sequence {xn } converges, then the limit point cannot be a
generalised saddle point.
Remarks. Since the learning rates in both theorems are bounded by local Lipschitz
constants of ∇f , if the sequence {xn } converges, then the convergence rate will satisfy the
usual estimates in [2]. One non-trivial situation when the condition in (iv) of Theorem
1.3 is when we know by some reason that the set of all non-isolated critical points of f is
bounded.
The main idea for the proof of Theorem 1.1 is as follows. From the assumption that ∇f
is locally Lipschitz, we can choose locally in small open sets U small enough learning rates
δ so that Armijo’s condition is satisfied, x 7→ x−δ∇f (x) is injective in that neighbourhood
U and moreover is a local diffeomorphism near generalised saddle points. Then we use
a partition of unity to carefully glue together these local learning rates into a smooth
positive, bounded function h : Rk → (0, δ0 ]. Then (i) holds by construction. In (ii),
for showing that any cluster point of {xn } is a critical point of f , we use the arguments
in [4]. For showing the remaining assertions in (ii), we follow the proofs in [27] of the
corresponding assertions for Backtracking GD. From the proof, we see that for parts (i)
and (ii) only, we do not need the assumption that ∇f is C 2 near its generalised saddle
points. For proof of (iii), we use arguments in [17, 21]. For (iv), we use additionally the
fact that a saddle point is an isolated critical point of f , and the result in [27] that the
set of cluster points of {xn }, considered in the real projective space Pk , is connected.
Note that parts (iii) and (iv) of Theorems 1.1 and 1.3 have the same conclusion as
the main results in [17, 21], while here we do not require that ∇f is globally Lipschitz
continuous as in those papers. While the learning rates in Theorem 1.1 (the function

6

TUYEN TRUNG TRUONG

h(x)) here are not very explicitly determined, provided we can have an explicit estimate
for the local Lipschitz constants of ∇f (x) then we can make h(x) explicit. See examples
in the next section for details, where we show that a similar construction can also be done
under careful analysis for some maps whose gradient ∇f need not be locally Lipschitz.
Note that in part (iv) of Theorem 1.1 and part (iii) of Theorem 1.3, when the cluster set
of {xn } contains an isolated saddle point, a priori we not have that {xn } must converge,
but this fact turns out to follow from results in [27]. For a more detailed overview of
previous results, the readers are invited to consult Subsection 4.2. We note that (iii)
and (iv) of Theorems 1.1 and 1.3 answer in affirmative some variants of, and hence give
support to, the following conjecture in [27] (here stated a bit stronger than the original
version, in view of the results we prove in this paper):
Conjecture 1.4. (Conjecture 5.1 in [27]) Let f : Rk → R be a C 1 function and C 2 near
its generalised saddle points. Then the set of initial points x0 ∈ Rk for which the cluster
points of the sequence {xn } - constructed by the Backtracking GD method - contains a
generalised saddle point has Lebesgue measure zero.
1.4. Plan of the paper. In Section 2 we prove Theorems 1.1 and 1.3 and provide some
examples. In Section 3 we give a summary and discuss future research directions. In the
Appendix we discuss why convergence in GD is important in practice, why we should
allow cost functions f as general as possible, and give an overview of previous work on
convergence for GD methods to help reduce readers’ confusion with the state-of-the-art
in this subject.
2. Proofs of the main results and Some examples
In this section, first we prove Theorems 1.1 and 1.3. After that, we give some examples
applying the theorems. For the proof, we will make use of the following two simple results.
The first of which was used since [2] in GD methods, and the second is a simple estimate
on the Lebesgue measure of the preimage of a map whose distortion is bounded away
from 0.
Lemma 2.1. Let U ⊂ Rk be an open and convex set, and let f : U → R be a C 1 -function,
whose gradient ∇f is Lipschitz continuous on U with Lipschitz constant L. Let x0 ∈ U
and δ > 0 so that x0 − δ∇f (x0 ) ∈ U. Then
f (x0 − δ∇f (x0 )) − f (x0 ) ≤ −δ(1 − δ/L)||∇f (x0 )||2 .

7

Proof. By the Fundamental Theorem of Calculus, we have
Z 1
f (x0 − δ∇f (x0 )) − f (x0 ) = −δ
∇f (x0 − sδ∇f (x0 )).∇f (x0 )ds.
0

Plugging into the RHS the following estimate
∇f (x0 − sδ∇f (x0 )).∇f (x0 ) = ||∇f (x0 )||2 + (∇f (x0 − sδ∇f (x0 ) − ∇f (x0 )).∇f (x0 ))
≥ ||∇f (x0 )||2 − sδL||∇f (x0 )||2 ,
we obtain the result.



Lemma 2.2. Let U ⊂ Rk be an open and convex subset, and H : U → Rk a continuous
function. Assume that there is λ > 0 so that ||H(x) − H(y)|| ≥ λ||x − y|| for all x, y ∈ U.
Let E ⊂ Rk be a measurable set of Lebesgue measure 0. Then H −1 (E) is also of Lebesgue
measure 0.
Proof. Since E has Lebesgue measure 0, for each ǫ > 0 there is a sequence of balls
P
S
P k
{B(xi , ri )}i=1,2,... so that E ⊂ ∞
i=1 B(xi , ri ) and
i (V ol(B(xi , ri ))) ∼
i ri < ǫ.
S
−1
Since ||H(x)−H(y)|| ≥ λ||x−y|| for all x, y ∈ U, we get that H (E) ⊂ i B(H −1 (xi ), ri /λ)
and

X

V ol(B(H −1 (xi ), ri /δ)) ∼

i

X
(ri /δ)k < ǫ/λk .
i

Therefore, the Lebesgue measure of H −1 (E) is < ǫ/λk for all ǫ > 0, and hence must be
0.



2.1. Proof of Theorem 1.1. Since ∇f is locally Lipschitz continuous, for each x ∈ Rk ,
there are positive numbers r(x), L(x) > 0 so that ∇f (x) has Lipschitz constant L(x)
in the ball B(x, r(x)). That is, for all y, z ∈ B(x, r(x)) we have ||∇f (y) − ∇f (z)|| ≤
L(x)||y − z||. Also, we can choose L(x) large enough so that for all 0 < δ ≤ 1/(L(x))
then f (z − δ∇f (z)) − f (z) ≤ −αδ||∇f (z)||2 for all z ∈ B(x, r(x)).
There is a partition of unity {ϕj }j=1,2,... of Rk with compact supports so that for every
j ∈ N, there is a point zj ∈ Rk for which the support supp(ϕj ) of ϕj is contained in
B(zj , r(zj )). Moreover, {supp(ϕj )}j is locally finite, that is every x ∈ Rk has an open
neighbourhood U which intersects only a finite number of those supp(ϕj )’s. (This is
related to Lindelöff theorem, mentioned in the appendix of this paper. We recall here
the main idea: Any open cover of Rk has a subcover which is locally finite. We can even
arrange that each point x ∈ Rk is contained in at most k + 1 open sets in the subcover.
Then we construct a partition of unity with compact supports contained in open sets in
that subcover.). For each j = 1, 2, . . ., we let
Mj =

max

y1 ,y2 ∈B(zj ,r(zj ))

||∇ϕj (y1 )|| × ||∇f (y2)||.

8

TUYEN TRUNG TRUONG

We now define the function h : Rk → R by the following formula:
∞
X

h(x) :=

1

10j (Mj
j=1

+ 1)

ϕj (x) min{

1
, δ0 , 1}.
2L(zj )

Since ϕj ’s are non-negative, smooth and their supports are locally finite, it follows
P
that the function h(x) is well-defined, smooth and non-negative. Since j ϕj (x) = 1, it
follows that 0 < h(x) ≤ δ0 for all x ∈ Rk .

Fix a point x ∈ Rk . Then there is at least one j so that ϕj (x) > 0, and hence
h(x) ≥

1
1
ϕ (x) min{ 2L(z
, δ0 , 1}
10j (Mj +1) j
j)

> 0. Then there is a finite index set J and an open

neighbourhood V of x so that if supp(ϕj )∩V 6= ∅ then j ∈ J. Since supp(ϕj )’s are all comT
1
, δ0 }.
pact, we can shrink V so that x ∈ i∈J B(zj , r(zj )) and h(x) ≤ maxj∈J min{ 2L(z
j)
Since h is a smooth function, we can find an open neighbourhood U of x so that
T
2
U ⊂ V ∩ i∈J B(zj , r(zj )) and for all y ∈ U we have h(y) ≤ maxj∈J min{ 3L(z
, δ0 }.
j)
It then follows by the choice of L(zj )’s that for all y ∈ U we have
f (y − h(y)∇f (y)) − f (y) ≤ −αh(y)||∇f (y)||2.
We will now show also that H(y) = y −h(y)∇f (y) is injective on this same set U. In fact,
assume that there are distinct y1 , y2 ∈ U so that y1 − h(y1 )∇f (y1 ) = y2 − h(y2 )∇f (y2 ).
We rewrite this as:
(y1 − y2 ) − h(y1 )(∇f (y1) − ∇f (y2)) − (h(y1 ) − h(y2 ))∇f (y2) = 0.
Since U ⊂ V and by the choice of V , if ϕj (y) 6= 0 for some y ∈ V then j ∈ J. Hence, we
obtain a contradiction because
2
||h(y1 )(∇f (y1) − ∇f (y2 ))|| ≤ ||y1 − y2 ||,
3
and using that |ϕj (y1 ) − ϕj (y2 )| ≤ ||y1 − y2 || × maxB(zj ,r(zj )) ||∇ϕj ||
||(h(y1 ) − h(y2 ))∇f (y2 )||
X
1
1
(ϕj (y1 ) − ϕj (y2 )) min{
, δ0 , 1}| × ||∇f (y2)||
= |
j
10
(M
2L(z
j + 1)
j)
j∈J
≤

X

≤

1
||y1 − y2 ||.
9

1

10j (Mj
j∈J

+ 1)

Mj min{

1
, δ0 , 1}||y1 − y2 ||
2L(zj )

Near generalised saddle points of f , the map x 7→ H(x) is moreover C 1 by the assumption that f is C 2 near those points. Hence, x 7→ H(x) is a local diffeomorphism near
generalised saddle points of f .
Proof of (i): Already given above.

9

Proof of (ii): Since h(x) is smooth and positive, it follows that for every compact
subset K ⊂ Rk where inf x∈K ||∇f || > 0, we have inf x∈K h(x) > 0. Hence, we can use the
same arguments as in [4] to show that any cluster point of the sequence {xn+1 = H(xn )}
is a critical point of f .
Since h(x) ≤ δ0 for all x ∈ Rk , we can use the same proof of part 1 of Theorem 2.1 in
[27] to conclude that either limn→∞ ||xn+1 − xn || = 0 or limn→∞ ||xn || = ∞.
Then we can use the same proof of part 2 of Theorem 2.1 in [27], by employing the
real projective space Pk and result in [3] for cluster points of sequences {xn } in compact
metric space (X, d) satisfying limn→∞ d(xn+1 , xn ) = 0, to show that if f has at most
countably many critical points, then either {xn } converges to a critical point of f or
limn→∞ ||xn || = ∞.
Proof of (iii): By using Stable-Center Manifold theorem for local diffeomorphisms
and using that the map x 7→ H(x) is a local diffeomorphism near generalised saddle
points, we can argue as in [17, 21] that there is an open neighbourhood U of generalised
saddle points of f , and a subset F1 ⊂ U of Lebesgue measure 0, so that if all {xn } ⊂ U\F1
and {xn } converges, then the limit point cannot be a saddle point.
Since we showed in the construction that the map x 7→ H(x) is locally injective, it
S
follows that E1 = n∈N H −n (F1 ) also has Lebesgue measure 0. Then it follows that if
x0 ∈ Rk \E1 , then {xn } cannot converge to a generalised saddle point.

Proof of (iv): We use the ideas in [27]. Note that a saddle point of f is a nondegenerate critical point, and hence is an isolated generalised saddle point. Assume that
the set of cluster points A of {xn } contains an isolated generalised saddle point y0 . Then
the property limn→∞ ||xn || = ∞ does not hold, and hence by part (ii) we must have
limn→∞ ||xn+1 − xn || = 0. Then, by the result in [3] for the real projective space Pk , it
follows that the closure A ⊂ Pk is connected. By part (ii) again, the set A is contained
in the set of all critical points of f , and hence y0 is also an isolated point of A, and hence
of A. Thus A = {y0}, and hence limn→∞ xn = y0 . Then we can use part (iii) to conclude.
b n ) ≤ δ0 for all n.
2.2. Proof of Theorem 1.3. Note that by construction we have δ(x
Since L(x), r(x) and ∇f (x) are continuous in x, it follows that for all compact subset

K ⊂ Rk , we have inf x∈K δ(x) > 0. Therefore, (i) and (ii) of Theorem 1.3 follows from
b
[27], note that by construction x − δ(x)∇f
(x) ∈ B(x, r(x)/2) for all x ∈ Rk and hence
Armijo’s condition is satisfied.

Now we prove (iii). Let S be the set of isolated generalised saddle points of f . Then S
is countable. Therefore, for a random choice of α, β, δ0, we have the following: for every
x ∈ S then α/L(x) does not belong to {β n δ0 : n = 0, 1, 2, . . .}, and also r(x)/||∇f (x)||
does not belong to {β n δ0 : n = 0, 1, 2, . . .}.

10

TUYEN TRUNG TRUONG

Hence, for each x ∈ S, either α/L(x) > δ0 , or there is a number n(x0 ) so that
β n(x0 )+1 δ0 <

α
< β n(x0 ) δ0 .
L(x)

In both cases, since z 7→ L(z) is continuous, there is an open neighbourhood U(x) of x so
that for all z ∈ U(x), then α/L(z) has the same behaviour. Shrinking U(x) if necessary,
we can assure that ||∇f (z)|| is small in U(x), and hence r(z)/||∇f (z)|| > δ0 .Therefore,
b = δ(x)
b
by Definition 1.2, we have that δ(z)
= β n(x0 )+1 δ0 for all z ∈ U(x). In particular,

b
the map z 7→ H(z) = z − δ(z)∇f
(z) is a local diffeomorphism in U(x).

By [27], if the cluster set of {xn } contains an isolated generalised saddle point, then

{xn } converges to that generalised saddle point. Then, we can apply Stable-Central
theorem to obtain that there is an open neighbourhood U of S and a set F ⊂ U of
Lebesgue measure 0 such that if x0 ∈ Rk is so that the cluster points of {xn } contains an
isolated generalised saddle point, then there is n(x0 ) for which H n(x0 ) (x0 ) ∈ F .
We note that since z 7→ L(z), z 7→ r(z) and z 7→ ||∇f (z)|| are continuous, for every
b = δ(x)
b or δ(z)
b =
x ∈ Rk , there is a neighbourhood U(x) so that for all z ∈ U(x) then δ(z)

b
b = δ(x)}
b
δ(x)/β.
Then we see that z 7→ H(z) is injective on both sets {z ∈ U(x) : δ(z)
b = δ(x)/β}.
b
and {z ∈ U(x) : δ(z)
Then argue as in the proof of (iii) and (iv) in Theorem
1.1, we get that the set

E=

[

H −n (F )

n∈N

has Lebesgue measure 0, and for all x0 ∈ Rk \E, the sequence {xn } constructed from
Definition 1.2 cannot have any cluster point which is an isolated generalised saddle point.
The proof of (iv) is similar. Here, the assumption that α/L > δ0 or β n0 +1 δ0 < α/L <
β n0 and that L ≥ L(x) for all non-isolated generalised saddle points imply that for z
near a non-isolated saddle point we have δ(z) = δ(x) (note that since x is a critical point
we have ∇f = 0, and hence for z near x we always have δ0 ||∇f (z)|| < r(z)), and hence
the map z 7→ H(z) is a local diffeomorphism near those generalised saddle points as well.
2.3. Some examples. Looking at the proofs of Theorems 1.1 and 1.3, we see that if
we can estimate L(x) and r(x) more exactly, then we can construct more explicitly the
function h(x). Below we illustrate this with a specific function.
Example 1: f : R2 → R given by f (x, y) = x3 sin(1/x)+y 3 sin(1/y). It can be checked
that f is C 1 on R2 , f is C 2 on R2 \({x = 0} ∪ {y = 0}), but ∇f is not locally Lipschitz at
any point on {x = 0} ∪ {y = 0}. Moreover, f is neither convex nor real analytic. Since
the function p(t) = t 7→ t3 sin(1/t) satisfies limt→±∞ p(t) = +∞, the function f (x, y) has
compact sublevels.
First, we apply Theorem 1.1.

11

In this example, when both x, y 6= 0, we can compute
∇f (x, y) = (3x2 sin(1/x) − x cos(1/x), 3y 2 sin(1/y) − y cos(1/y)).
From this, by using properties of analytic functions, we can see that f has countably
many critical points, and countably many of them are saddle points.
Computing the Hessian, and using the rough estimates | cos(z)|, | sin(z)| ≤ 1, we obtain
a rough estimate for the Hessian at points (x, y) ∈
/ {x = 0} ∪ {y = 0}):
||∇2 f (x, y)|| ≤ 6(|x| + |y|) + 8 + (

1
1
+ ).
|x| |y|

Note that sup(x,y)∈{x=0}∪{y=0})
||∇2 f (x, y)|| = ∞, and hence the gradient ∇f is not glob/
ally Lipschitz continuous.
Hence, we can explicitly construct, as in the proof of Theorem 1.1, a smooth function
h : R2 \({x = 0} ∪ {y = 0}) → (0, δ0 ] which is locally injective and a local diffeomorphism
near generalised saddle points in the domain R2 \({x = 0}∪{y = 0}). Moreover, Armijo’s
condition is satisfied.
Near the points in {x = 0} ∪ {y = 0}, since ∇f is not locally Lipschitz, we cannot do
the same construction as in the proof of Theorem 1.1. However, we observe that if x = 0
then ∂f /∂x is also 0. Hence, for points in {x = 0}\{(0, 0)}, we construct h(0, y) for the
restriction of the function f to {x = 0}. Similarly, for points in {y = 0}\{(0, 0)} we
construct h(x, 0) for the restriction of the function f to {y = 0}. The only point left
where we need to construct h is (0, 0), but since this point is a critical point of f , we can
just choose any value for h(0, 0). Alternatively, on points in {x = 0} ∪ {y = 0}, we just
define h(x, y) using the usual Backtracking GD procedure.
Note that the function h : R2 → R we construct here is not even continuous.
However, we can use the same ideas as in [27] to prove the conclusions (i) and (ii) of
Theorem 1.1 for the map H(x, y) : (x, y) 7→ (x, y) − h(x, y)∇f (x, y). Moreover, we
have that on R2 \({x = 0} ∪ {y = 0}), the map H(x, y) is locally injective and is a local
diffeomorphism near generalised saddle points of f . Note that by definition, generalised
saddle points of f can only be contained in R2 \({x = 0} ∪ {y = 0}). Similarly, the
restriction H(0, y) is locally injective on {x = 0}\{(0, 0)}, and the restriction H(x, 0)
is locally injective on {y = 0}\{(0, 0)}. Moreover, the preimage of any point in {x =
0} ∪ {y = 0} is at most countable. From this, we obtain that if E ⊂ R2 is of Lebesgue
S
measure 0, then n∈N H −n (E) also has Lebesgue measure 0. In this case, except the point
(0, 0), other critical points of f are isolated. Hence, we can state the following stronger

property than stated in (iii) and (iv) of Theorem 1.1:

12

TUYEN TRUNG TRUONG

Proposition: There exists a set E ⊂ R2 of Lebesgue measure 0, so that for all (x0 , y0 ) ∈
R2 \E, the sequence (xn+1 , yn+1) = H(xn , yn ) satisfies the following: (xn , yn ) converges to
a critical point of f which is not a generalised saddle point.
We can also obtain a similar Proposition by applying Theorem 1.3. We just need to
make sure to choose r(x, y) in the remark after Definition 1.2 to be smaller than the
distance from (x, y) to {x = 0} ∪ {y = 0}.
Experiments show that the behaviour of the usual discrete Backtracking GD for this
function is also similar to that in the above Proposition, and conforms to Conjecture 1.4.
Example 2: We can explore similar examples such as f (x, y) = axp sinq (1/x) +
by p sinq (1/y).

3. Conclusions
In Example 2.17 in [27], it was shown that even for a smooth cost function f , the map
x 7→ x − δ(x)∇f (x), where δ(x) is constructed from (discrete) Backtracking GD, is not
always continuous. Hence, we cannot apply the usual continuous Dynamical Systems
theory to study the asymptotic behaviour of Backtracking GD as in the case of the
Standard GD algorithm where learning rate is fixed. In this paper, we have shown
that if ∇f is locally Lipschitz- but need not be globally Lipschitz - then a continuous
Dynamical System can be associated to the continuous version of Backtracking GD. We
can prove the same conclusions as in the main results in [27]. In the case f is C 2 near
its generalised saddle points, we can prove the same conclusions as in the main results in
[17, 21] but under less restrictive assumptions. In fact, as far as we know, the assumptions
in Theorem 1.1 are also the least restrictive and most practical among all current known
results for convergence to minima for all iterative methods, for general non-convex cost
functions. The same results can be proven for the Inexact version of Backtracking GD,
and also for Backtracking versions of Momentum and NAG as defined in [27]. If moreover
f satisfies the condition in Definition 1.2, then we can prove similar results for a new
discrete version of Backtracking GD. We remind that the condition in Definition 1.2 is
satisfied for cost functions which are either CL1,1 or C 2 . We have illustrated in Examples
1 and 2 in Subsection 2.3 how the construction can be done in practice, for very singular
functions.
Future work. There are some promising directions worth pursue in the future.
First direction: Since the construction of h(x) in Theorem 1.1 is rather implicit, it
is interesting to see how to implement it in practice, in particular for complicated cost
b
functions used in Deep Neural Networks. While the construction of δ(x)
in Theorem 1.3

is easy provided L(x) and r(x) are known, determining L(x), r(x) effectively in practice

13

(with complicated cost functions such as those used in DNN) can be challenging. The
ideas presented in Subsection 2.3 may be helpful.
Second direction: Extending the results in this paper and [27] to more general functions
and optimization on infinite dimensional vector spaces or other manifolds. It is also of
great interest to extend the results to constraint optimization problems.
Third direction: Using ideas from this paper, combined with Random Dynamical Systems, to solve Conjecture 1.4.
Fourth direction: Using (discrete or continuous) Backtracking GD to help resolve challenges such as adversarial attacks [12, 13, 22], since learning rates in Backtracking GD
are chosen very adaptively with respect to the data, and since so far the best convergence
results for iterative optimisation methods are obtained for Backtracking GD under least
restrictive conditions.
4. Appendix
In this section we first explain why convergence of the iteration process in GD and
allowing more general cost functions are important from a practical view point. Then
we present a brief overview of major convergence results in previous literature for GD
methods and their use in Deep Neural Networks, to help dispel some widespread misunderstandings about this topic, which are apparent from lecture notes, books, videos
and posts on professional websites, in both Optimization and Deep Learning communities. Given that Deep Learning is still not as reliable, reproducible and safe as desired
(existence of adversarial images - both synthetic and physical and in medical imaging
[12, 13, 22], as well as fatal accidents in self driving cars - as recent as September 2019),
there is still a large demand for rigorously theoretical guarantees for the practices employed in Deep Learning - even though good experimental results are abundant and
appear more and more.
4.1. Why is convergence of the iteration in GD important and Why should we
allow general cost functions? Here we provide some reasons for why we should allow
the cost function f as most general as possible and the convergence of the sequence {xn }
constructed in the previous paragraph is important, based on its use in Deep Learning.
The main idea behind the use of Deep Neural Networks (DNNs) in Deep Learning is an
approximation result for continuous functions, which we will briefly present here. Assume
for example that we want to have an AI to classify very well hand written digits. Then
we assume that there is a correct assignment, for each image z, a classification y(z) ∈
{0, 1, . . . , 9}. Since it is impractical and impossible to employ humans to look at each
and every possible image x out there and assign the correct label y(x) ∈ {0, 1, 2, . . . , 9},

14

TUYEN TRUNG TRUONG

we will only classify by hand a small subset I (training set) of images, and then employ
an appropriate DNN. Mathematically, a DNN is a finite composition of functions h of the
form (σ ◦ L1 , . . . , σ ◦ Lj ), where L1 , . . . , Lj are affine functions from a finite dimensional
vector space (whose dimensions can vary, and whose coefficients γ are not given in advance
but will be chosen later via an optimisation problem so that to best approximate the given
data in the training set I with respect to a choice of metric) to R, and σ : R → R is
an appropriate nonlinear function. The justification of such a use of DNN is that any
continuous function can be approximated by such DNN, see e.g. [23] for details. In
analogy to biology, each function h appearing in a DNN corresponds to a layer of that
DNN, and the dimension of the corresponding finite dimension vector space is called the
number of neurons of the layer. Modern DNN can have hundreds of layers and millions
of parameters.
While currently some choices of the activation function σ are favourited (such as the
sigmoid function), we now argue that it is better to allow σ as general as possible. In fact,
because of limitations in resources and time, we cannot work with a DNN of arbitrary
large number of neurons or layers. Hence, since the approximation theorem mentioned
above is only valid when we allow the number of neurons or layers go to infinity, and
since no finite model can work well for all questions (No-free-lunch-theorems, see [29]), it
is wise that we work with as general DNNs as possible.
As explained above, when we already chose a DNN for the question at hand, we are
led to an optimisation problem of finding parameters γmin achieving minimum for a cost
function f (γ). Then such a γmin is used to provide predictions for new data outside of
the training set I. In practice, we cannot find a closed form for such γmin , and hence
must make use of one of iterative methods, such as GD. In that case, we choose a random
value γ0 and then compute iteratively γn+1 = γn − δ(γn )∇f (γn ). Even if we already chose
on such numerical method, we cannot run the iteration infinitely many times, and so we
need to stop after a finite time, say n0 , and then use γn0 for new predictions. Hence, we
see here that convergence results for such iterative processes is important if we want the
results to be stable and reproducible. This is because if another (or even oneself) will
run the iteration again, even with the same choice of the initial point γ0 , there can be
errors in computing which leads to another stop time n1 , and hence will use γn1 . The
practice of using mini-batches in Deep Learning makes this scence even more realistic.
If no convergence result is guaranteed for the sequence {γn }, there is reason to doubt
that the predictions one gets when using γn0 and γn1 are similar. This could lead to
non-reproducibility and instability.
While it is favourited in many previous and current works to require quite strong
assumptions (see Subsection 4.2) on the cost function f , such as being CL1,1 , having

15

compact sublevels, having just a finite number of critical points, being convex or real
analytic, here we argue that even just from a practical view point (without taken into
account the theoretical viewpoint), it is better to allow more general cost functions in
the treatment. First, if one agrees with the previous paragraphs that the activation
functions should be as general as possible, one sees that one needs to deal with general cost
functions. Second, when one computes with functions in practice, errors are unavoidable,
and hence one in principle must work with perturbations of the ideal cost functions, and
perturbations in general do not preserve the mentioned properties. There is also another
source where perturbations come from. To avoid convergence to bad minima, it is a
common practice in the Deep Learning community to perturb the cost function by a
term of the form λ||γ||p , where λ, p > 0 are given numbers. If p 6= 2, then the resulting
is not CL1,1 . Even if p = 2, the assumption about finiteness of critical points may not be
preserved in general. Third, even when the cost function one starts with is in CL1,1 , it may
be difficult to have a good estimate of L to make sure that one actually chooses correctly
a learning rate δ which consents with theoretical assumptions. Hence, even in this good
case, it is better if one can use a method which requires less checking or guaranteed to
work under very general assumptions. Also, when new data comes, it would be tedious
and difficult for one to change learning rates by hand. This has led to the current practice
of manual fine-tuning of learning rates in Deep Learning, which - as we explained in the
above paragraph - can lead to instability and unreproducibility, besides costing time and
computer resources to redo with many different learning rates.
All in all, we advocate for that it is better to use methods guaranteed for more general
assumptions preserved under small perturbations.

4.2. Overview on previous major results on convergence and avoidance of saddle points. Since GD has been developed in a long time and there are too many documents devoted to it, it may be an impression to many casual readers, or even experts, that
the convergence of these methods have been established under very general assumptions
and since a long time ago. This impression can be amplified when one reads or watches
books, lecture notes, papers or videos, where there are many handwaving statements such
as ”Gradient Descent always converges to minimum”, without the bother to give precise
references and conditions under which proofs for such statements have been rigorously
given. It may come as a surprise for one when finding that many such statements are
groundless, and may contribute to wrong applications of the results or false/unfair judgements/attributions about results. To help dispel such misunderstandings, we collect here
some previous major convergence results for GD so far with proper references for proofs.

16

TUYEN TRUNG TRUONG

In the influential paper [2], where Armijo’s condition was introduced into GD, Armijo
proved the convergence of Standard GD and Backtracking GD for functions in CL1,1 , under
the further assumptions including that the function has only one critical point. The most
general result which can be proven with his method is (see e.g. Proposition 12.6.1 in [16])
the convergence of Standard GD under the assumption that f is in CL1,1 , the learning
rate δ is < 1/(2L), f has compact sublevels (that is, all the sets {x : f (x) ≤ b} for
b ∈ R are compact), and the set of critical points of f is bounded and isolated (which
in effect implies that f has only a finite number of critical points). An analog of this
result for gradient descent flow (solutions to x′ (t) = −∇f (x(t))) is also known (see e.g.
Appendix C.12 in [15]). Besides gradient flows, there are currently also work employing
more techniques in PDE to solve optimization questions, however as far as we know
assumptions needed are still similar to those mentioned in this paragraph. Both the
assumptions that f is in CL1,1 and δ is small enough are necessary for the conclusion of
Proposition 12.6.1 in [16], even for very simple functions, as shown by Examples 2.14 (for
functions of the form f (x) = |x|1+γ , where 0 < γ < 1 is a rational number) and 2.15
(for functions which are smooth versions of f (x) = |x|) in [27]. In contrast, for these
examples, Backtracking GD always converges to the global minimum 0.
Concerning the issue of saddle points, ([17, 21]) proved the very strong result that for
f in CL1,1 and δ < 1/L, there exists a set E ⊂ Rk of Lebesgue measure 0 so that for x0 ∈
Rk \E, if the sequence {xn } is constructed from Standard GD converges, then the limit is
not a generalised saddle point. The main idea is that then the map x 7→ x − δ∇f (x) is a
diffeomorphism, and hence we can use the Stable-Center manifold theorem in dynamical
systems (cited as Theorem 4.4 in [17]). For to deal with the case where the set of critical
points of the function is uncountable, the new idea in [21] is to use Lindelöff lemma that
any open cover of an open subset of Rm has a countable subcover. Note that here the
convergence of {xn } is important, otherwise one may not be able to use the Stable-Center
manifolds. However, for convergence of {xn }, one has to use results about convergence
of Standard GD (such as Proposition 12.6.1 in [16]), and needs to assume more, as seen
from Example 2.14 in [27]. Therefore, Proposition 4.9 in [17] is not valid as stated.
There are other variants of GD which are regarded as state-of-the-art algorithms in
DNN such as Momentum and NAG, Adam, Adagrad, Adadelta, and RMSProp (see an
overview in [25]). Some of these variants (such as Adagrad and Adadelta) allow choosing
learning rates δn to decrease to 0 (inspired by Stochastic GD, see next paragraph) in some
complicated manners which depend on the values of gradients at the previous points
x0 , . . . , xn−1 . However, as far as we know, convergence for such methods are not yet
available beyond the usual setting such as in Proposition 12.6.1 in [16]. Indeed, as seen in

17

the next paragraph, for the Stochastic GD, such assumptions are only enough to prove the
convergence of {∇f (xn )} to 0, and not of {xn } itself, even if one assumes that f ∈ CL1,1 .
Stochastic GD is the default method used to justify the use of GD in DNN, which goes
back to Robbins and Monro, see [5]. The most common version of it is to assume that we
have a fixed cost function F (as in the deterministic case), but we replace the gradient
∇κ F (κn ) by a random vector vn (here the random variables are points in the dataset,
see also Inexact GD), and then show the convergence in probability of the sequence of
values F (κn ) and of gradients ∇κ F (κn ) to 0 (in application the random vector vn will
be ∇κ FIn (κn )). However, the assumptions for these convergence results (for F (κn ) and
∇κ F (κn )) to be valid still require those in the usual setting as in Proposition 12.6.1 in
[16], in particular requiring that f ∈ CL1,1 and the learning rate is small compared to 1/L,
and in addition requiring that f is strongly convex. In the case where there is noise,
the following additional conditions on the learning rates are needed [24]:
(1)

X
n≥1

δn = ∞,

X

δn2 < ∞,

n≥1

under which convergence of ∇κ F (κn ) to 0 is shown. However, in Standard GD, which
is a popular version in DNN, all the learning rates are the same and hence condition
P
2
n≥1 δn < ∞ is violated. Moreover, showing that the gradients ∇κ F (κn ) converge to

0 is far from proving the convergence of κn itself. In the original paper of Robbins and

Monro, one can also see that it is required that the equation one wants to solve M(x) = 0
has a unique solution.
There is also a method proposed by Wolfe, which is close to Backtracking GD. The
original definition in [28]) is very complicated, and consists of choosing at each step one
of 5 choices listed in Definition on page 228 in that paper. The modern version of Wolfe’s
conditions consists of two assumptions, one is Armijo’s condition in Backtracking GD, and
the other is so-called curvature condition (which is only a half of condition v) in Wolfe’s
paper). It is shown that if f is a C 1 function which is bounded from below, then a
positive δn can be chosen to satisfy these two Wolfe’s conditions. Even though requiring
more conditions than Backtracking GD, the best result so far using Wolfe’s conditions
requires that f is in CL1,1 to show the convergence of ∇f (zn ) to 0 (G. Zoutendijk’s result,
see [20]). Hence, as evident from the next two paragraphs and results proven in this paper,
one can conclude that as current, Backtracking GD, even though simpler than Wolfe’s
conditions, is theoretically proven better. Recently, Wolfe’s conditions are implemented
in DNN [18].
For (discrete) Backtracking GD, it is known in literature before [27] that any cluster
point of the sequence {xn } is a critical point of f (see [4]), for all C 1 functions f . It

18

TUYEN TRUNG TRUONG

should be noted that the terminology ”limit points” in [4] actually means cluster points,
and hence does not mean that the sequence converges - as the word ”limit” may suggest.
Moreover, if f is real analytic, then [1] proves convergence of the sequence {xn } without
additional assumptions. However, the real analyticity assumption is quite restrictive, and
not preserved under small perturbations.
In [27], we proved convergence of Backtracking GD under the assumption that f is C 1
and has at most countably many critical points. We obtained as a corollary - as far as we
know not mentioned in previous literature - the convergence of the method using Wolfe’s
conditions for cost functions in CL1,1 and which has at most countably many critical points.
The assumption of having at most countably many critical points is satisfied by all Morse
functions, i.e. functions whose all critical points are non-degenerate. Note that Morse
functions are open and dense in the set of all functions, and hence are preserved under
small perturbations. Hence, when using Backtracking GD, basically one does not need
to check any condition. There are two main points which separate the proofs in [27] and
those in previous literature. The first is we need to find a new way to prove that either
limn→∞ ||xn+1 − xn || = 0 or limn→∞ ||xn || = ∞, given that no bootstrap techniques are
available as in the case of CL1,1 cost functions. The second is that we introduced into the
study of optimisation on Rk the real projective space Pk . For a general C 1 function f , we
also proved a type of avoiding of saddle points for Backtracking GD, whose conclusion
is weaker than that in [17, 21] and in Theorems 1.1 and Theorem 1.3. In [27] we also
provided a heuristic argument showing that in the long term Backtracking GD will become
a finite union of Standard GD processes. We also proposed new optimization algorithms
to automatically find learning rates, working very well across many different network
architectures and mini-batch sizes. These new algorithms can be integrated into existing
DNN architectures to improve stability and reproducibility of those models. The source
code of our algorithms can be found in [30].
To summarise, at the moment, Backtracking GD is theoretically proven better than
other GD methods, concerning guarantee of convergence to a critical point and convergence to mimima.

References
[1] P.-A. Absil, R. Mahony and B. Andrews, Convergence of the iterates of descent methods for analytic
cost functions, SIAM J. Optim. 16 (2005), vol 16, no 2, 531–547.
[2] L. Armijo, Minimization of functions having Lipschitz continuous first partial derivatives, Pacific J.
Math. 16 (1966), no. 1, 1–3.
[3] M. D. Asic and D. D. Adamovic, Limit points of sequences in metric spaces, The American mathematical monthly, vol 77, so 6 (June–July 1970), 613–616.

19

[4] D. P. Bertsekas, Nonlinear programming, 2nd edition, Athena Scientific, Belmont, Massachusetts,
1999.
[5] L. Bottou, F. E. Curtis and J. Nocedal, Optimization methods for large-scale machine learning,
SIAM Rev. 60(2), 223–311.
[6] S. Boyd and L. Vandenberghe, Convex optimization, 7th printing with corrections, Cambridge University Press, 2009.
[7] A. J. Bray and and D. S. Dean, Statistics of critical points of gaussian fields on large-dimensional
spaces, Physics Review Letter, 98, 150201.
[8] A. Cauchy, Method général pour la résolution des systemes d’équations simulanées, Comptes Rendus
25 (1847), no. 2, 536.
[9] J. B. Crockett and H. Chernoff, Gradient methods of maximization, Pacific J. Math. 5 (1955), 33–50.
[10] H. B. Curry, The method of steepest descent for non-linear minimization problems, Quarterly of
applied mathematics, 2 (October 1944), no 3, 258–261.
[11] Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli and Y. Bengjo, Identifying and attacking the saddle point problem in high-dimensional non-convex optimization, NIPS’ 14 Proceedings
of the 27th International conference on neural information processing systems, Volume 2, pages
2933–2941.
[12] K. Eyholt, I. Evtimov, E. Fernades, B. Li, A. Rahmati, C. Xiao, A. Prakash, T. Kohno and D. Song,
Robust physical-world attacks on Deep Learning visual classification, arXiv: 1707.08945. Appears at
CVPR 2018.
[13] S. G. Finlayson, J. D. Bowers, J. Ito, J. L. Zittrain, A. L. Beam and I. S. Kohane, Adversarial
attacks on medical machine learning, Science (22 March 2019), Vol 363, Issue 6433, pp. 1287–1289.
[14] A. A. Goldstein, Cauchy’s method of minimization, Numerische Mathematik 4 (1962), 146–150.
[15] U. Helmke and J. B. Moore, Optimization and dynamical systems, 2nd edition, 1996, online book.
[16] K. Lange, Optimization, 2nd edition, Springer texts in statistics, New York 2013.
[17] J. D. Lee, M. Simchowitz, M. I. Jordan and B. Recht, Gradient descent only converges to minimizers,
JMRL: Workshop and conference proceedings, vol 49 (2016), 1–12.
[18] M. Mahrsereci and P. Hennig, Probabilistic line searches for stochastic optimisation, JMLR, vol 18
(2017), 1–59.
[19] Y. Nesterov, Introductory lectures on convex optimization : a basic course, 2004, Kluwer Academic
Publishers. ISBN 978-1402075537.
[20] J. Nocedal and S. J. Wright, Numerical optimization, Springer series in operations research, 1999.
[21] I. Panageas and G. Piliouras, Gradient descent only converges to minimizers: Non-isolated critical
points and invariant regions, 8th Innovations in theoretical computer science conference (ITCS
2017), Editor: C. H. Papadimitrou, article no 2, pp. 2:1–2:12, Leibniz international proceedings in
informatics (LIPICS), Dagstuhl Publishing. Germany.
[22] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik and A. Swami, Practical black-box
attacks against machine learning, ACM Asia conference on computer and communications security,
UAE 2017.
[23] A. Pinkus, Approximation theory of the MLP model in neural networks, Acta Numerica (1999), pp.
143–195.
[24] H. Robbins and S. Monro, A stochastic approximation method, Annals of Mathematical Statistics,
vol 22, pp. 400–407, 1951.

20

TUYEN TRUNG TRUONG

[25] S. Ruder, An overview of gradient descent optimisation algorithms, arXiv: 1609.04747.
[26] L. N.Smith, No more pesky learning rate guessing games, CoRR, 2015 , arXiv:1506.01186
[27] T. T. Truong and T. H. Nguyen, Backtracking gradient descent method for general C 1 functions
with applications to Deep Learning, arXiv: 1808.05160v2.
[28] P. Wolfe, Convergence conditions for ascent methods, SIAM Review 11 (April 1969), no 2, 226–235.
[29] D. H. Wolpert and W. G. Macready, No free lunch theorems for optimisation, IEEE Transactions
on evolutionary computation, Vol. 1, No 1, April 1997, 67–82.
[30] https://github.com/hank-nguyen/MBT-optimizer
Matematikk Institut, Universitetet i Oslo, Blindern, 0851 Oslo, Norway
E-mail address: tuyentt@math.uio.no

