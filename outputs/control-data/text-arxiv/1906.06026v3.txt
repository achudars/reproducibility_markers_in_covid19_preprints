1

Adversarial Robustness Assessment:
Why both L0 and L∞ Attacks Are Necessary

arXiv:1906.06026v3 [cs.LG] 16 Jul 2020

Shashank Kotyan and Danilo Vasconcellos Vargas

Abstract—There exists a vast number of adversarial attacks
and defences for machine learning algorithms of various types
which makes assessing the robustness of algorithms a daunting
task. To make matters worse, there is an intrinsic bias in these
adversarial algorithms. Here, we organise the problems faced:
a) Model Dependence, b) Insufficient Evaluation, c) False
Adversarial Samples, and d) Perturbation Dependent Results).
Based on this, we propose a model agnostic dual quality
assessment method, together with the concept of robustness
levels to tackle them. We validate the dual quality assessment
on state-of-the-art neural networks (WideResNet, ResNet,
AllConv, DenseNet, NIN, LeNet and CapsNet) as well as
adversarial defences for image classification problem. We
further show that current networks and defences are
vulnerable at all levels of robustness. The proposed robustness
assessment reveals that depending on the metric used (i.e., L0
or L∞ ), the robustness may vary significantly. Hence, the
duality should be taken into account for a correct evaluation.
Moreover, a mathematical derivation, as well as a
counter-example, suggest that L1 and L2 metrics alone are not
sufficient to avoid spurious adversarial samples. Interestingly,
the threshold attack of the proposed assessment is a novel L∞
black-box adversarial method which requires even less
perturbation than the One-Pixel Attack (only 12% of One-Pixel
Attack’s amount of perturbation) to achieve similar results.
Index Terms—Deep Learning, Neural Networks, Adversarial
Attacks, Few-Pixel Attack, Threshold Attack

I. I NTRODUCTION

N

EURAL networks have empowered us to obtain high
accuracy in several applications like speech recognition
and face recognition. Most of these applications are only
feasible by the aid of neural networks. Despite these
accomplishments, neural networks have been shown to
misclassify if small perturbations are added to original
samples, called adversarial samples. Further, these
adversarial samples exhibit that conventional neural network
architectures are not capable of understanding concepts or
high-level abstractions as we earlier speculated.
Security and safety risks created by these adversarial
samples is also prohibiting the use of neural networks in
many critical applications such as autonomous vehicles.
Therefore, it is of utmost significance to formulate not only
accurate but robust neural networks. However, to do so, a
quality assessment is required, which would let robustness to
be evaluated efficiently without in-depth knowledge of
adversarial machine learning.
S. Kotyan and D.V. Vargas are with the Laboratory of Intelligent Systems,
Department of Informatics, Kyushu University, Japan. http://lis.inf.kyushu-u.
ac.jp/. E-mail:vargas@inf.kyushu-u.ac.jp

Regarding the development of a quality assessment for
robustness, the field of adversarial machine learning has
provided some tools which could be useful for the
development. However, the sheer amount of scenarios,
adversarial attacking methods, defences and metrics (L0 , L1 ,
L2 and L∞ ) make the current state-of-the-art difficult to
perceive. Moreover, most of the contemporary adversarial
attacks are white-box ones which can not be used to assess
hybrids, non-standard neural networks and other classifiers in
general. Giving the vast amount of possibilities and many
definitions with their exceptions and trade-offs, it turns out
that a simple robustness quality assessment is a daunting
task.
Moreover, adversarial samples point out to reasoning
shortcomings in machine learning. Improvements in
robustness should also result in learning systems that can
better reason over data as well as achieve a new level of
abstraction. Therefore, a quality assessment procedure would
also be helpful in this regard, checking for failures in both
reasoning and high-level abstractions.
Therefore, to create a quality assessment procedure, we
formalise some of the problems which must be tackled:
P1 Model Dependence: A
model
agnostic
quality
assessment is crucial to enable neural networks to be
compared with other approaches which may be completely
different (logic hybrids and evolutionary hybrids).
P2 Insufficient Evaluation: There are several types of
adversarial samples as well as potential attack variations
and scenarios each with their own bias. The attacks also
differ substantially depending on metrics optimized, namely
L0 , L1 , L2 and L∞ . However, not all of them are vital for
the evaluation of robustness. A quality assessment should
have few but sufficient tests to provide an in-depth analysis
without compromising its utility.
P3 False Adversarial Samples: Adversarial attacks are
known sometimes to produce misleading adversarial
samples (samples that can not be recognised even by a
human observer) seldomly. Such deceptive adversarial
samples can only be detected through inspection, which
causes the evaluation to be error-prone. Both the need for
inspection, together with the feasibility of fraudulent
adversarial samples, should not be present.
P4 Perturbation Dependent Results: Varying amount of
perturbation leads to varying adversarial accuracy.
Moreover, networks differ in their sensitivity to attacks
given a varied amount of perturbation. Consequently, this
might result in double standards or hide important

2

Few-Pixel (L0 ) Attack

Threshold (L∞ ) Attack

Fig. 1: Adversarial samples found with Few-pixel (L0 ) black-box attack and threshold (L∞ ) black-box attack.

information.
In this article, we propose a quality assessment to tackle the
problems mentioned above with the following features:
Non-gradient based Black-box Attack (Address P1):
Black-box attacks are desirable for a model agnostic
evaluation which does not depend on specific features of
the learning process such as gradients. Therefore, here, the
proposed quality assessment is based on black-box attacks,
one of which is a novel L∞ black-box attack. In fact, to
the knowledge of the authors, this is the first L∞ black-box

Attack that does not make any assumptions over the target
machine learning system. Figure 1 show some adversarial
samples crafted with the L0 and L∞ black-box Attacks
used in the quality assessment.
Dual Evaluation (Address P2 and P3): We propose to use
solely attacks based on L0 and L∞ to avoid creating
adversarial samples which are not correctly classified by
human beings after modification. These metrics impose a
constraint over the spatial distribution of noise which
guarantees the quality of the adversarial sample. In Section

3

IV, this is explained mathematically as well as illustrated
with a counter-example.
Robustness Levels (Address P4): In this article, we define
robustness levels in terms of the constraint’s threshold th.
We then compare multiple robustness levels of results with
their respective values at the same robustness level.
Robustness levels constrain the comparison of equal
perturbation, avoiding the comparison of results with
different degrees of perturbation (Problem P4). In fact,
robustness levels add a concept which may aid in the
classification of algorithms. For example, an algorithm
which is robust to One-Pixel Attack belongs to the
1-pixel-safe category.
II. R ELATED W ORKS
Recently, it was exhibited that neural networks contain
many vulnerabilities. The first article on the topic dates back
to 2013 when it was revealed that neural networks behave
oddly for almost the same images [1]. Afterwards, a series
of vulnerabilities were found and exploited by the use of
adversarial attacks. In [2], the authors demonstrated that
neural networks show high confidence when presented with
textures and random noise. Adversarial perturbations which
can be added to most of the samples to fool a neural
network was shown to be possible [3]. Patches can also
make them misclassify, and the addition of them in an image
turn it into a different class [4]. Moreover, an extreme attack
was shown to be effective in which it is possible to make
neural networks misclassify with a single-pixel change [5].
Many of these attacks can be easily made into real-world
threats by printing out adversarial samples, as shown in [6].
Moreover, carefully crafted glasses can also be made into
attacks [7]. Alternatively, even general 3D adversarial objects
were shown possible [8].
Regarding understanding the phenomenon, it is argued in
[9] that neural networks’ linearity is one of the main reasons.
Another recent investigation proposes the conflicting saliency
added by adversarial samples as the reason for
misclassification [10].
Many defensive systems and detection systems have also
been proposed to mitigate some of the problems. However,
there are still no current solutions or promising ones which
can negate the adversarial attacks consistently. Regarding
defensive systems, defensive distillation in which a smaller
neural network squeezes the content learned by the original
one was proposed as a defence [11]. However, it was shown
not to be robust enough in [12]. Adversarial training was
also proposed, in which adversarial samples are used to
augment the training dataset [9], [13], [14]. Augmentation of
the dataset is done in such a way that the neural network
should be able to classify the adversarial samples, increasing
its robustness. Although adversarial training can increase the
robustness slightly, the resulting neural network is still
vulnerable to attacks [15]. There are many recent variations
of defenses [16], [17], [18], [19], [20], [21], [22], [23] which
are carefully analysed and many of their shortcomings are
explained in [24], [25].

Regarding detection systems, a study from [26]
demonstrated that indeed some adversarial samples have
different statistical properties which could be exploited for
detection. In [21], the authors proposed to compare the
prediction of a classifier with the prediction of the same
input but ”squeezed”. This technique allowed classifiers to
detect adversarial samples with small perturbations. Many
detection systems fail when adversarial samples deviate from
test conditions [27], [28]. Thus, the clear benefits of
detection systems remain inconclusive.
III. A DVERSARIAL M ACHINE L EARNING A S
O PTIMISATION P ROBLEM
Adversarial machine learning can be perceived as a
constrained optimisation problem. Before defining it, let us
formalise adversarial samples first. Let f (x) ∈ [[0, 1]] be the
output of a machine learning algorithm in binary
classification setting. Extrapolating the algorithm in
multi-label classification setting, the output can be defined as
f (x) ∈ [[1..N ]]. Here, x ∈ Rk is the input of the algorithm
for the input of size k and N is the number of classes in
which x can be classified. An adversarial sample x0 for an
original sample x can be thus, defined as follows:
x 0 = x + x

such that

f (x0 ) 6= f (x)

in which x ∈ Rk is a small perturbation added to the input.
Therefore, adversarial machine learning can be defined as an
optimization problem1 :
minimize g(x + x )c
x

subject to kx k ≤ th

where th is a pre-defined threshold value and g()c is the softlabel or the confidence for the correct class c such that f (x) =
argmax g(x).
The constraint in the optimisation problem has the
objective of disallowing perturbations which could make x
unrecognisable or change its correct class. Therefore, the
constraint is itself a mathematical definition of what
constitutes an imperceptible perturbation. Many different
norms are used in the literature (e.g., L0 , L1 , L2 and L∞ ).
Intuitively, the norms allow for different types of attacks.
For simplicity, we are narrowing the scope of this article to
the image classification problem alone. However, the proposed
attacks and the quality assessment can be also be extended to
other problems as well.
IV. G UARANTEEING THE Q UALITY OF A DVERSARIAL
S AMPLES
Constraining the perturbation is decisive in adversarial
samples to avoid producing samples that can not be
recognised by human beings or samples that have, by the
amount of perturbation, changed its correct class. However,
restraining the total amount of perturbation is not enough as
a small amount of perturbation concentrated in a few pixels
might be able to create false adversarial samples. Therefore,
1 Here the definition will only concern untargeted attacks in classification
setting but a similar optimization problem can be defined for targeted attacks

4

Fig. 2: Example of a false adversarial sample (right) and its respective
original sample (left). The false adversarial sample is built with few
total perturbations (i.e., low L1 and L2 ) but with unrecognisable
final image (false adversarial sample). This is a result of the nonconstrained spatial distribution of perturbations which is prevented if
low L0 or L∞ is used. This hypothetical attack has a L2 of merely
356, well below the maximum L2 for the One-Pixel (L0 ≤ 1) Attack
(765).

a spatial constraint over the perturbation of pixels P would
be a desirable feature.
This can be achieved mathematically as follows: Given an
image x and its perturbed counterpart x0 , it is possible to
calculate L1 norm between the original by the Manhattan
distance of both matrices: kx − x0 k1 . Constraining L1 to be
less than a certain number does not guarantee any spatial
distribution constraint. Let us define a set based on all
non-zero
pixel
perturbations
as
follows:
Nz = {Pi : kPi − Pi0 k1 > 0} where Pi and Pi0 are pixels
from respectively the original image x and the perturbed
image x0 and i is an image index. Both Nz and its
cardinality |Nz | has information about the spatial distribution
of perturbations and constraining any of these values would
result in a spatially limited perturbation.
Provided that th is low enough, a modification preserving
the white noise of that intensity would bound |Nz | < th.
Moreover, |Nz | is precise L0 , demonstrating that L0 is based
on the set Nz , which stores spatial information about the
differences. At the same time, L1 uses the Manhattan norm,
which does not have this information. Similarly, the L∞
norm can be rewritten as the following optimisation
constraint: ∀Pi ∈ x, kPi − Pi0 k∞ ≤ th Notice that this
constraint is also defined over the spatial distribution of
perturbations.
Figure 2 gives empirical evidence of misleading
adversarial sample of an image that is constrained by
L2 ≤ 765. Notice that this value is precisely the maximum
change of one pixel, i.e., the maximum possible perturbation
of the One-Pixel attack (L0 ≤ 1) which when no limits are
imposed over its spatial distribution may create false
adversarial samples.
The reasoning behind the L0 and L∞ are as follows, without
altering much the original sample, attacks can perturb a few
pixels strongly (L0 ), all pixels slightly (L∞ ) or a mix of both
(L1 and L2 ). The hurdle is that L1 and L2 which mix both
strategies vary strongly with the size of images, if not used
with caution may cause unrecognisable adversarial samples
(Problem P3). Also, it is difficult to compare between methods
using L1 and L2 norm because the amount of perturbations

will often differ (Problem P4).
Threshold Attack (L∞ black-box Attack): The threshold
attack optimizes the constrained optimization problem with
the constraint kx k∞ ≤ th, i.e., it uses the L∞ norm. The
algorithm search in Rk space as the search space is the
same as the input space. This is because the variables can
be any variation of the input as long as the threshold is
respected. In image classification problem k = m × n × c
where m × n is the size, and c is the number of channels of
the image.
Few-Pixel Attack (L0 black-box Attack): The
few-pixel
attack is a variation of our previous proposed attack, the
One-Pixel Attack [5]. It optimizes the constrained
optimization problem by using the constraint kx k0 ≤ th,
i.e., it uses the L0 norm. The search variable is a
combination of pixel values (depending on channels c in
the image) and position (2 values X, Y) for all of the pixels
(th pixels). Therefore, the search space is smaller than the
threshold attack defined below with dimensions of
R(2+c)×th .
Robustness Levels: Here we propose robustness levels, as
machine learning algorithms might perform differently to
varying amount of perturbations. Robustness levels evaluate
classifiers in a couple of th thresholds. Explicitly, we define
four levels of robustness 1, 3, 5, 10 for both of our L0 Norm
Attack and L∞ Norm Attack. We then name them
respectively pixel and threshold robustness levels.
Algorithms that pass a level of robustness (0% adversarial
accuracy) are called level-threshold-safe or level-pixel-safe.
For example, an algorithm that passes the level-one in
threshold (L∞ ) attack is called 1-threshold-safe.
V. E XPERIMENTAL R ESULTS AND D ISCUSSIONS
In this section, we aim to validate the dual quality
assessment2 empirically as well as analyse the current
state-of-the-art neural networks in terms of robustness.
Preliminary
Tests
(Section
V-B): Tests on two
state-of-the-art neural networks are presented (ResNet [29]
and CapsNet [30]). These tests are done to choose the
black-box optimisation algorithm to be used for the further
sections. The performance of both Differential Evolution
(DE) [31] and Covariance Matrix Adaptation Evolution
Strategy (CMA-ES) [32] are evaluated.
Evaluating Learning and Defense Systems (Section V-C):
Tests are extended to the seven different state-of-the-art
neural networks - WideResNet [33], DenseNet [34], ResNet
[29], Network in Network (NIN) [35], All Convolutional
Network (AllConv) [36], CapsNet [30], and LeNet [37]. We
also evaluate three adversarial defences applied to the
standard ResNet architecture - Adversarial training (AT)
[14], Total Variance Minimization (TVM) [19], and Feature
Squeezing (FS) [21]. We have chosen defences based on
entirely different principles to be tested. In this way, the
results achieved here can be extended to other similar types
of defences in the literature.
2 Code

is available at http://bit.ly/DualQualityAssessment

5

Attack

Parameters

FGM
BIM
PGD
DeepFool
NewtonFool

norm = L∞ ,  = 8, step = 2
norm = L∞ ,  = 8, step = 2, iterations = 10
norm = L∞ ,  = 8, step = 2, iterations = 20
iterations = 100,  = 0.000001
iterations = 100, eta = 0.01

L0 Attack

Common
DE
CMA-ES

Parameter Size = 5,
NP = 400, Number of Generations = 100, CR = 1
Function Evaluations = 40000, σ = 31.75

L∞ Attack

Common
DE
CMA-ES

Parameter Size = 3072,
NP = 3072, Number of Generations = 100, CR = 1
Function Evaluations = 39200, σ = th/4

TABLE I: Description of various parameters of different adversarial
attacks.

Evaluating Other Adversarial Attacks (Section V-D): The
evaluated learning systems are tested against other existing
white-box and black-box adversarial attacks such as - Fast
Gradient Method (FGM) [9], Basic Iterative Method (BIM)
[6], Projected Gradient Descent Method (PGD) [14],
DeepFool [38], and NewtonFool [39]. This analysis further
helps to demonstrate the necessity of duality in quality
assessment.
Extremely Fast Quality Assessment (Section V-F): In this
section, we apply and evaluate the principle of
transferability of adversarial samples. We verify the
possibility of a speedy version of the proposed quality
assessment. We implement this by using already crafted
adversarial samples to fool neural networks, instead of a
full-fledged optimisation. This would enable attacks to have
a O(1) time complexity, being significantly faster.
Quality Assessment’s Attack Distribution (Section V-G):
Here, we assess the dual-attack distribution (Few-Pixel
Attack and Threshold Attack). The analysis of the
distribution demonstrates the necessity of such duality. The
distribution of successful attacks are shown, and previous
attacks are analysed in this perspective.
Effect of threshold (Section V-H): We analyse the
complete behaviour of the adversarial accuracy of our
black-box attacks without restricting the threshold’s th
value. Using this analysis, we prove the results using a
fixed th in robustness levels is a reasonable approximation
for our proposed quality assessment.
A. Experimental Settings
We use CIFAR-10 dataset [40] to evaluate our dual quality
assessment. Table I gives the parameter description of
various adversarial attacks used. All the pre-existing
adversarial attacks used in the article have been evaluated
using Adversarial Robustness 360 Toolbox (ART v1.2.0)
[41].
For our L0 and L∞ Attacks, we use the canonical
versions of the DE and CMA-ES algorithms to have a clear
standard. DE uses a repair method in which values that go
beyond range are set to random points within the valid
range. While in CMA-ES, to satisfy the constraints, a simple
repair method is employed in which pixels that surpass the
minimum/maximum
are
brought
back
to
the
minimum/maximum value. Moreover, a clipping function is

Model

Attack
Optimiser

th = 1

Adversarial Accuracy
th = 3
th = 5
th = 10

Few-Pixel (L0 ) Attack
ResNet
CapsNet

DE
CMA-ES
DE
CMA-ES

24%
12%
21%
20%

70%
52%
37%
39%

75%
73%
49%
40%

79%
85%
57%
41%

53%
76%
15%
72%

82%
83%
23%
97%

Threshold (L∞ ) Attack
ResNet
CapsNet

DE
CMA-ES
DE
CMA-ES

5%
33%
11%
13%

23%
71%
13%
34%

TABLE II: Adversarial accuracy results for Few-Pixel (L0 ) and
Threshold (L∞ ) Attacks with DE and CMA-ES

used to keep values inside the feasible region. The constraint
is always satisfied because the number of parameters is itself
modelled after the constraint. In other words, when searching
for one pixel perturbation, the number of variables are fixed
to pixel values (three values) plus position values (two
values). Therefore it will always modify only one pixel,
respecting the constraint. Since the optimisation is done in
real values, to force the values to be within range, a simple
clipping function is used for pixel values. For position
values, a modulo operation is executed.
B. Preliminary Tests: Choosing the Optimization Algorithm
Table II shows the adversarial accuracy results performed
over 100 random samples. Here adversarial accuracy
corresponds to the accuracy of the adversarial attack to
create adversarial samples to fool neural networks. Both
black-box attacks can craft adversarial samples in all levels
of robustness. This fact demonstrates that without knowing
anything about the learning system and in a constrained
setting, black-box attacks are still able to reach more than
80% adversarial accuracy in state-of-the-art neural networks.
Concerning the comparison of CMA-ES and DE, the
outcomes favour the choice of CMA-ES for the quality
assessment. Both CMA-ES and DE perform likewise for the
Few-Fixel Attack, with both DE and CMA-ES having the
same number of wins. However, for the Threshold Attack,
the performance varies significantly. CMA-ES this time
always wins (eight wins) against DE (no win). This
domination of CMA-ES is expected since the Threshold
Attack has a high dimensional search space which is more
suitable for CMA-ES. This happens in part because DE’s
operators may allow some variables to converge prematurely.
CMA-ES, on the other hand, is always generating slightly
different solutions while evolving a distribution.
In these preliminary tests, CapsNet was shown overall
superior to ResNet. Few-pixel (L0 ) Attack reach 85%
adversarial accuracy for ResNet when ten pixels are
modified. CapsNet, on the other hand, is more robust to
Few-Pixel Attacks, allowing them to reach only 52% and
41% adversarial accuracy when ten pixels are modified for
DE and CMA-ES respectively. CapsNet is less robust than
ResNet to the Threshold Attack with th = 10 in which

6

Model and
Standard Accuracy

th = 1

Adversarial Accuracy
th = 3
th = 5 th = 10

Few-Pixel (L0 ) Attack
WideResNet
DenseNet
ResNet
NIN
AllConv
CapsNet
LeNet

95.12%
94.54%
92.67%
90.87%
88.46%
79.03%
73.57%

11%
9%
12%
18%
11%
21%
58%

55%
43%
52%
62%
31%
37%
86%

75%
66%
73%
81%
57%
49%
94%

94%
78%
85%
90%
77%
57%
99%

AT
TVM
FS

87.11%
47.55%
92.37%

22%
16%
17%

52%
12%
49%

66%
20%
69%

86%
24%
78%

Threshold (L∞ ) Attack
WideResNet
DenseNet
ResNet
NIN
AllConv
CapsNet
LeNet

95.12%
94.54%
92.67%
90.87%
88.46%
79.03%
73.57%

15%
23%
33%
11%
9%
13%
44%

97%
68%
71%
86%
70%
34%
96%

98%
72%
76%
88%
73%
72%
100%

100%
74%
83%
92%
75%
97%
100%

AT
TVM
FS

87.11%
47.55%
92.37%

3%
4%
26%

12%
4%
63%

25%
6%
66%

57%
14%
74%

TABLE III: Adversarial accuracy results for L0 and L∞ Attacks over
100 random samples

almost all images were vulnerable (97%). At the same time,
CapsNet is reasonably robust to 1-threshold-safe (only 13%
adversarial accuracy). ResNet is almost equally not robust
throughout, with low robustness even when th = 3, losing to
CapsNet in robustness in all other values of th of the
threshold attack. These preliminary tests also show that
different networks have different robustness. This is not only
regarding the type of attacks (L0 and L∞ ) but also with the
degree of attack (e.g., 1-threshold and 10-threshold attacks
have very different results on CapsNet).
C. Evaluating Learning and Defense Systems
Table III extends the CMA-ES attacks on various neural
networks: WideResNet [33], DenseNet [34], ResNet [29],
Network in Network (NIN) [35], All Convolutional Network
(AllConv) [36], CapsNet [30], and LeNet [37]. We also
evaluate with three contemporary defences: Adversarial
training (AT) [14], Total Variance Minimization (TVM) [19],
and Feature Squeezing (FS) [21].
Results in bold (Only for learning systems and not
defensive systems) are the lowest adversarial accuracy and
other results which are within a distance of five from the
lowest one. For CapsNet only 88 samples could be attacked
with maximum th = 127 for L0 Attack. Twelve samples
could not be overwhelmed when the th < 128. Here, taking
into account an existing variance of results, we consider
results within five of the lowest to be equally good. If we
consider the number of bold results for each of the neural
networks, a qualitative measure of robustness CapsNet and
AllConv can be considered the most robust with five bold
results. The third place in robustness achieves only three

bold results and consequently is far away from the prime
performers.
Regarding the adversarial training, it is easier to attack
with the Few-Pixel Attack than with Threshold Attack. This
result should derive from the fact that the adversarial
samples used in adversarial training contained images from
Projected Gradient Descent (PGD) Attack, which is L∞ type
of attack. Therefore, it suggests that given an attack bias
that differs from the invariance bias used to train the
networks, the attack can easily succeed. Regarding TVM, the
attacks were less successful. We trained a ResNet on TVM
modified images and, albeit many trials with different
hyper-parameters, we were able to craft a classifier with at
best 47.55% accuracy. This is a steep drop from the 92.37%
accuracy of the original ResNet and happens because TVM
was initially conceived for Imagenet and did not scale well
to CIFAR-10. However, as the original accuracy of the
model trained with TVM is also not high; therefore, even
with a small attack percentage of 24%, the resulting model
accuracy is 35%. Attacks on Feature Squeezing had
relatively high adversarial accuracy both L0 and L∞ attacks.
Moreover, both types of attacks had similar accuracy,
revealing a lack of bias in the defence system.
Notice that none of the neural networks was able to
reduce low th attacks to zero. This illustrates that although
robustness may differ between current neural networks, none
of them can effectively overcome even the lowest level of
perturbation feasible. Moreover, since a th = 5 is enough to
achieve around 70% accuracy in many settings, this suggests
that achieving 100% adversarial accuracy may depend more
on a few samples which are harder to attack, such as
samples far away from the decision boundary. Consequently,
the focus on 100% adversarial accuracy rather than the
amount of threshold might give preference to methods which
set a couple of input projections far away from others
without improving the accuracy overall. An example can be
examined by making some input projections far away
enough to make them harder to attack.
The difference in the behaviour of L0 and L∞ Norm Attacks
shows that the robustness is achieved with some trade-offs.
This further justifies the importance of using both metrics to
evaluate neural networks.
D. Evaluating Other Adversarial Attacks
We evaluated our assessed neural networks further against
well-known adversarial attacks such as Fast Gradient Method
(FGM) [9], Basic Iterative Method (BIM) [6], Projected
Gradient Descent Method (PGD) [14], DeepFool [38], and
NewtonFool [39]. Please, note that for FGM, BIM, PGD
attacks  = 8(Default Value) ≈ th = 10 of L∞ Attack on
our robustness scales. While DeepFool and NewtonFool do
not explicitly control the robustness scale. Table IV
compares the existing white-box attacks and black-box
attacks with our proposed attacks. Notice that, although all
the existing attacks are capable of fooling neural networks.
We notice some peculiar results, like DeepFool Attack, was
less successful against the LeNet, which was most vulnerable

7

Adversarial Attacks
FGM
BIM
PGD
DeepFool
NewtonFool

WideResNet

DenseNet

ResNet

NIN

AllConv

CapsNet

LeNet

69% (159.88)
89% (208.44)
89% (208.49)
60% (613.14)
82% (63.13)

50% (120.03)
52% (160.34)
52% (160.38)
60% (478.03)
50% (53.89)

52% (124.70)
55% (164.64)
55% (164.64)
58% (458.57)
54% (51.56)

72% (140.46)
74% (216.97)
74% (216.96)
59% (492.90)
66% (54.78)

67% (155.95)
69% (273.90)
69% (274.15)
51% (487.46)
61% (61.05)

70% (208.89)
82% (361.63)
84% (370.90)
87% (258.08)
90% (1680.83)

84% (152.37)
89% (345.27)
89% (357.34)
31% (132.32)
84% (49.61)

29%
43%
52%
63%

(177.86)
(247.97)
(300.19)
(359.55)

61%
89%
96%
98%

(191.69)
(248.21)
(265.18)
(271.90)

13% (39.09)
34% (70.79)
72% (130.80)
97% (184.93)

47%
96%
99%
100%

(39.28)
(62.86)
(66.42)
(66.65)

Few-Pixel (L0 ) Attack

th = 1
th = 3
th = 5
th = 10

20%
54%
75%
91%

(181.43)
(276.47)
(326.14)
(366.60)

20%
50%
68%
81%

(179.48)
(270.50)
(315.53)
(354.42)

29%
63%
79%
90%

(191.73)
(275.57)
(314.27)
(342.56)

28%
62%
81%
93%

(185.09)
(274.91)
(318.71)
(354.61)

24%
49%
67%
81%

(172.01)
(262.66)
(318.99)
(365.10)

Threshold (L∞ ) Attack

th = 1
th = 3
th = 5
th = 10

30%
92%
95%
98%

(39.24)
(65.07)
(67.84)
(70.70)

38%
69%
72%
78%

(39.24)
(53.89)
(56.81)
(67.63)

43%
74%
77%
83%

(39.27)
(52.82)
(55.38)
(64.50)

23%
81%
85%
90%

(39.23)
(72.29)
(77.09)
(84.20)

23%
72%
76%
79%

(39.21)
(68.11)
(72.45)
(77.76)

TABLE IV: Adversarial accuracy of the proposed L0 and L∞ black-box Attacks used in the dual quality assessment and their comparison
with other methods from the literature. The value in the brackets represents the Mean L2 score of the adversarial sample with the original
sample. The results were drawn by attacking a different set of samples from previous tests. Therefore the accuracy results may differ slightly
from previous tables.

Fig. 3: Adversarial accuracy from Table III across classes. The two diagrams at left and right are respectively L0 and L∞ attacks. The top
diagrams used th = 10 while the bottom ones used th = 1.

to our proposed attacks (Table III). Moreover, ResNet and
DenseNet had much better robustness for the existing attacks
compared to our attacks.
The objective of this article is not to propose better or
more effective attacking methods but rather to propose an
assessment methodology, and its related duality conjecture
(the necessity of evaluating both L0 and L∞ Attacks).
However, the proposed Threshold L∞ Attack in the
assessment methodology is more accurate than other attacks
while requiring less amount of perturbation. The Threshold
Attack requires less perturbation than the One-Pixel attack
(only circa 12% of the amount of perturbation of the

One-Pixel Attack th = 1) which was already considered one
of the most extreme attacks needing less perturbation to fool
neural networks. This sets up an even lower threshold to the
perturbation, which is inevitable to fool neural networks.
Notice that, the behaviour of the existing attacks is similar
to our Threshold L∞ Attack (Table IV). This suggests that
the current evaluations of the neural networks focus on
increasing the robustness based on L∞ Norm. However, our
study shows that behaviour of L0 Norm differs from the L∞
Norm (Table IV), and the robustness for the L∞ Norm may
not be sufficient to study the robustness and vulnerabilities
of the neural networks as a whole.

8

E. Dependency Of Proposed Adversarial Attacks On Classes

Model

We further separated the adversarial accuracy (Table III)
into classes (Figure 3). This is to evaluate the dependence
of proposed adversarial attacks on specific classes, Figure 3
shows an already known feature that some classes are more
natural to attack than others. For example, the columns for
bird and cat classes are visually darker than frog and truck
classes for all diagrams. This happens because classes with
similar features and therefore, closer decision boundaries are
more natural to attack.
Interestingly, the Figure 3 reveals that neural networks tend
to be harder to attack in only a few classes. This may suggest
that these networks encode some classes far away from others
(e.g., projection of the features of these classes into a different
vector). Consequently, the reason for their relative robustness
may lie on a simple construction of the decision boundary
with a few distinct and sharply separated classes.

WideResNet
DenseNet
ResNet
NIN
AllConv
CapsNet
LeNet

F. Extremely Fast Quality Assessment: Transferability of
Adversarial Samples
If adversarial samples from one model can be used to attack
different models and defences, it would be possible to create
an ultra-fast quality assessment. Figure 4 shows that indeed,
it is possible to qualitatively assess a neural network based on
the transferability of adversarial samples.
Beyond being a faster method, the transferability of
samples has the benefit of ignoring any masking of gradients
which makes hard to search but not to transfer. This shows
that the vulnerability in neural networks is still there but
hidden. Interestingly, the transferability is mostly
independent on the type of attack (L0 or L∞ ), with most of
the previously discussed differences disappearing. There are
some differences like L0 attacks are less accurate than most
of the L∞ ones. This suggests that positions of pixel and
their variance are relatively more model-specific than small
changes in the whole image.
Generally speaking, transferability is a quick assessment
method which, when used with many different types of
adversarial samples, gives an approximation of the model’s
robustness. This approximation is not better or worse but
different. It differs from usual attacks because (a) it is not
affected by how difficult it is to search adversarial samples,
taking into account only their existence, and (b) it measures
the accuracy to commonly found adversarial samples rather
than all searchable ones.
Therefore, in the case of low th values, transferability can
be used as a qualitative measure of robustness. However, its
values are not equivalent to or close to real adversarial
accuracy. Thus, it serves only as a lower bound.
G. Adversarial Sample Distribution of Quality Assessment
To understand the importance of the duality for the
proposed quality assessment. We analyse the distribution of
our proposed attacks across samples. In some cases, the
distribution of samples for L0 and L∞ can be easily verified
by the difference in adversarial accuracy. For example,

L0 Attack

L∞ Attack

425.0
989.5
674.0
528.0
1123.5
2493.0
137.5

141.5
696.0
575.5
364.0
849.0
404.5
104.0

TABLE V: Area under the curve (AUC) for both Few-Pixel (L0 ) and
Threshold (L∞ ) black-box Attacks

CapsNet is more susceptible to L∞ than L0 types of attacks
while for adversarial training [14] the opposite is true (Table
III). Naturally, adversarial training depends strongly on the
adversarial samples used in training, Therefore, different
robustness could be acquired depending on the type of
adversarial samples used.
Moreover, the distribution shows here that even when
adversarial accuracy seems close, the distribution of L0 and
L∞ Attacks may differ. For example, the adversarial
accuracy on ResNet for both L0 and L∞ with th = 10 differ
by mere 2%. However, the distribution of adversarial
samples shows that around 17% of the samples can only be
attacked by either one of the attack types (Figure 5). Thus,
the evaluation of both L0 and L∞ are essential to verify the
robustness of a given neural network or adversarial defence.
Moreover, this is true even when a similar adversarial
accuracy is observed.
H. Analysing effect of threshold th on learning systems
To evaluate how networks behave with the increase in
threshold, we plot here the adversarial accuracy with the
increase of th (Figure 6). These plots reveal an even more
evident difference of behaviour for the same method when
attacked with either L0 or L∞ norm of attacks. It shows that
the curve inclination itself is different. Therefore, L0 and
L∞ Attacks scale differently.
From Figure 6, two classes of curves can be seen. CapsNet
behaves on a class of its own while the other networks behave
similarly. CapsNet, which has an entirely different architecture
with dynamic routing, shows that a very different robustness
behaviour is achieved. LeNet is justifiably lower because of
its lower accuracy and complexity.
To assess the quality of the algorithms in relation to their
curves, the Area Under the Curve (AUC) is calculated by the
trapezoidal
rule.
defined
as:

AUC = ∆na th21 + th2 + th3 + . . . + thn−1 + th2n where
na is the number of images attacked and th1 , th2 , . . . thn are
different values of th threshold for a maximum of n = 127.
Table V shows a quantitative evaluation of Figure 6 by
calculating the Area Under the Curve (AUC).
There is no network which is robust in both attacks. CapsNet
is the most robust neural network for L0 attacks while AllConv
wins while being followed closely by other neural networks for
L∞ . Although requiring a lot more resources to be drawn, the
curves here result in the same conclusion achieved by Table

9

Fig. 4: Accuracy of adversarial samples when transferring from the a given source model (row) to a target model (column) for both L∞
black-box Attacks (left) and L0 black-box Attacks (right). The source of the adversarial samples is on the y-axis with the target model on
the x-axis. The adversarial samples were acquired from 100 original images attacked with th varying mostly from one to ten. The maximum
value of th is set to 127.

this article analyses the robustness of neural networks and
defences by elucidating the problems as well as proposing
solutions to them. Hopefully, the proposed dual quality
assessment and analysis on current neural networks’
robustness will aid the development of more robust neural
networks and hybrids alike.
Fig. 5: Distribution of adversarial samples found on DenseNet (left)
and ResNet (right) using th = 10 with both few-pixel (L0 ) and
threshold (L∞ ) Attacks.

III. Therefore, the previous results are a good approximation
of the behaviour promptly.
VI. C ONCLUSIONS
In this article, we propose a model agnostic dual quality
assessment for adversarial machine learning, especially for
neural networks. By investigating the various state-of-the-art
neural networks as well as arguably the contemporary
adversarial defences, it was possible to: (a) show that
robustness to L0 and L∞ Norm Attacks differ significantly,
which is why the duality should be taken into consideration.
(b) verify that current methods and defences, in general, are
vulnerable even for L0 and L∞ black-box Attacks of low
threshold th, and (c) validate the dual quality assessment
with robustness level as a good and efficient approximation
to the full accuracy per threshold curve. Interestingly, the
evaluation of the proposed method (Threshold Attack) was
shown to require surprisingly less amount of perturbation.
This novel L∞ black-box Attack based on CMA-ES required
only circa 12% of the amount of perturbation used by the
One-Pixel Attack while achieving similar accuracy. Thus,

ACKNOWLEDGMENTS
This work was supported by JST, ACT-I Grant Number
JP-50243 and JSPS KAKENHI Grant Number JP20241216.
Additionally, we would like to thank Prof. Junichi Murata
for the kind support without which it would not be possible
to conduct this research.
R EFERENCES
[1] C. e. a. Szegedy, “Intriguing properties of neural networks,” in In ICLR.
Citeseer, 2014.
[2] A. Nguyen, J. Yosinski, and J. Clune, “Deep neural networks are
easily fooled: High confidence predictions for unrecognizable images,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2015, pp. 427–436.
[3] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard,
“Universal adversarial perturbations,” in 2017 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR). Ieee, 2017, pp.
86–94.
[4] T. B. Brown, D. Mané, A. Roy, M. Abadi, and J. Gilmer, “Adversarial
patch,” arXiv preprint arXiv:1712.09665, 2017.
[5] J. Su, D. V. Vargas, and K. Sakurai, “One pixel attack for fooling deep
neural networks,” IEEE Transactions on Evolutionary Computation,
vol. 23, no. 5, pp. 828–841, 2019.
[6] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in the
physical world,” arXiv preprint arXiv:1607.02533, 2016.
[7] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, “Accessorize to
a crime: Real and stealthy attacks on state-of-the-art face recognition,”
in Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security. Acm, 2016, pp. 1528–1540.
[8] A. Athalye and I. Sutskever, “Synthesizing robust adversarial examples,”
in Icml, 2018.

10

Fig. 6: Adversarial accuracy per th for L0 and L∞ Attack.

[9] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.
[10] D. V. Vargas and J. Su, “Understanding the one-pixel attack: Propagation
maps and locality analysis,” arXiv preprint arXiv:1902.02947, 2019.
[11] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation
as a defense to adversarial perturbations against deep neural networks,”
in 2016 IEEE Symposium on Security and Privacy (SP). Ieee, 2016,
pp. 582–597.
[12] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
networks,” in 2017 IEEE Symposium on Security and Privacy (SP).
Ieee, 2017, pp. 39–57.
[13] R. Huang, B. Xu, D. Schuurmans, and C. Szepesvári, “Learning with a
strong adversary,” arXiv preprint arXiv:1511.03034, 2015.
[14] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
deep learning models resistant to adversarial attacks,” in Iclr, 2018.
[15] F. Tramèr, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and
P. McDaniel, “Ensemble adversarial training: Attacks and defenses,”
arXiv preprint arXiv:1705.07204, 2017.
[16] G. K. Dziugaite, Z. Ghahramani, and D. M. Roy, “A study of the
effect of jpg compression on adversarial images,” arXiv preprint
arXiv:1608.00853, 2016.
[17] T. Hazan, G. Papandreou, and D. Tarlow, Perturbations, Optimization,
and Statistics. MIT Press, 2016.
[18] N. Das, M. Shanbhogue, S.-T. Chen, F. Hohman, L. Chen, M. E.
Kounavis, and D. H. Chau, “Keeping the bad guys out: Protecting
and vaccinating deep learning with jpeg compression,” arXiv preprint
arXiv:1705.02900, 2017.
[19] C. Guo, M. Rana, M. Cisse, and L. van der Maaten, “Countering
adversarial images using input transformations,” in Iclr, 2018.
[20] Y. Song, T. Kim, S. Nowozin, S. Ermon, and N. Kushman, “Pixeldefend:
Leveraging generative models to understand and defend against
adversarial examples,” in Iclr, 2018.
[21] W. Xu, D. Evans, and Y. Qi, “Feature squeezing: Detecting adversarial
examples in deep neural networks,” arXiv preprint arXiv:1704.01155,
2017.
[22] X. Ma, B. Li, Y. Wang, S. M. Erfani, S. Wijewickrema,
G. Schoenebeck, D. Song, M. E. Houle, and J. Bailey, “Characterizing
adversarial subspaces using local intrinsic dimensionality,” arXiv
preprint arXiv:1801.02613, 2018.
[23] J. Buckman, A. Roy, C. Raffel, and I. Goodfellow, “Thermometer
encoding: One hot way to resist adversarial examples,” Iclr, 2018.
[24] A. Athalye, N. Carlini, and D. Wagner, “Obfuscated gradients give a
false sense of security: Circumventing defenses to adversarial examples,”
in Icml, 2018.
[25] J. Uesato, B. O’Donoghue, P. Kohli, and A. Oord, “Adversarial risk

[26]

[27]

[28]

[29]

[30]

[31]

[32]

[33]
[34]

[35]
[36]

[37]

[38]

[39]

and the dangers of evaluating against weak attacks,” in International
Conference on Machine Learning, 2018, pp. 5032–5041.
K. Grosse, P. Manoharan, N. Papernot, M. Backes, and P. McDaniel,
“On the (statistical) detection of adversarial examples,” arXiv preprint
arXiv:1702.06280, 2017.
N. Carlini and D. Wagner, “Adversarial examples are not easily detected:
Bypassing ten detection methods,” in Proceedings of the 10th ACM
Workshop on Artificial Intelligence and Security. Acm, 2017, pp. 3–14.
——, “Magnet and” efficient defenses against adversarial attacks” are
not robust to adversarial examples,” arXiv preprint arXiv:1711.08478,
2017.
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.
S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between
capsules,” in Advances in neural information processing systems, 2017,
pp. 3856–3866.
R. Storn and K. Price, “Differential evolution–a simple and efficient
heuristic for global optimization over continuous spaces,” Journal of
global optimization, vol. 11, no. 4, pp. 341–359, 1997.
N. Hansen, S. D. Müller, and P. Koumoutsakos, “Reducing the time
complexity of the derandomized evolution strategy with covariance
matrix adaptation (cma-es),” Evolutionary computation, vol. 11, no. 1,
pp. 1–18, 2003.
S. Zagoruyko and N. Komodakis, “Wide residual networks,” arXiv
preprint arXiv:1605.07146, 2016.
F. Iandola, M. Moskewicz, S. Karayev, R. Girshick, T. Darrell,
and K. Keutzer, “Densenet: Implementing efficient convnet descriptor
pyramids,” arXiv preprint arXiv:1404.1869, 2014.
M. Lin, Q. Chen, and S. Yan, “Network in network,” arXiv preprint
arXiv:1312.4400, 2013.
J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller,
“Striving for simplicity: The all convolutional net,” arXiv preprint
arXiv:1412.6806, 2014.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, 1998.
S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: a simple
and accurate method to fool deep neural networks,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2016,
pp. 2574–2582.
U. Jang, X. Wu, and S. Jha, “Objective metrics and gradient
descent algorithms for adversarial examples in machine learning,”
in Proceedings of the 33rd Annual Computer Security Applications
Conference. Acm, 2017, pp. 262–277.

11

[40] A. Krizhevsky, G. Hinton et al., “Learning multiple layers of features
from tiny images,” Tech. Rep., 2009.
[41] M.-I. Nicolae, M. Sinn, M. N. Tran, B. Buesser, A. Rawat, M. Wistuba,
V. Zantedeschi, N. Baracaldo, B. Chen, H. Ludwig, I. Molloy, and

B. Edwards, “Adversarial robustness toolbox v1.1.0,” CoRR, vol.
1807.01069, 2018. [Online]. Available: https://arxiv.org/pdf/1807.01069

