Mean Field Limit of the Learning Dynamics of
Multilayer Neural Networks

arXiv:1902.02880v1 [cs.LG] 7 Feb 2019

Phan-Minh Nguyen∗
February 11, 2019

Abstract
Can multilayer neural networks – typically constructed as highly complex structures with
many nonlinearly activated neurons across layers – behave in a non-trivial way that yet simplifies
away a major part of their complexities? In this work, we uncover a phenomenon in which the
behavior of these complex networks – under suitable scalings and stochastic gradient descent
dynamics – becomes independent of the number of neurons as this number grows sufficiently
large. We develop a formalism in which this many-neurons limiting behavior is captured by a set
of equations, thereby exposing a previously unknown operating regime of these networks. While
the current pursuit is mathematically non-rigorous, it is complemented with several experiments
that validate the existence of this behavior.

1

Introduction

The breakthrough empirical success of deep learning [LBH15] has spurred strong interests in theoretical understanding of multilayer neural networks. Recent progresses have been made from
various perspectives within and beyond traditional learning-theoretic frameworks and tools – a
very incomplete list of references includes [ABGM14, CHM+ 15, MP16, SS16, Mal16, SGGSD17,
ZSJ+ 17, SZT17, SJL18, NH18, GSd+ 18, HNP+ 18, WLLM18]. Analyzing these networks is challenging due to their inherent complexities, first as highly nonlinear structures, usually involving a
large number of neurons at each layer, and second as highly non-convex optimization problems,
typically solved by gradient-based learning rules without strong guarantees. One question then
arises: given such complex nature, is it possible to obtain a succinct description of their behavior?
In this work, we show that under suitable scalings and stochastic gradient descent (SGD)
learning dynamics, the behavior of a multilayer neural network tends to a non-trivial limit as its
number of neurons approaches infinity. We refer to this limit as the mean field (MF) limit. In
this limit, the complexity of the network becomes independent of the number of neurons, and the
network admits a simplified description that depends only on other intrinsic characteristics, such
as the number of layers and the data distribution. Interestingly this implies that two networks,
which differ by the number of neurons and hence the degree of over-parameterization, can perform
almost equally, so long as both have sufficiently many neurons. A similar phenomenon has been
recently discovered and studied in two-layers neural networks [MMN18, CB18b, RVE18, SS18a].
∗

Department of Electrical Engineering, Stanford University.

1

Training loss

Test loss

2.0

2.0

1.5

1.5

1.0
0.5
0.0

100
200
400
800
1600
101

1.0
0.5
103

105

100
200
400
800
1600
101

Training error
0.8

0.6

0.6

0.2
0.0

100
200
400
800
1600
101

0.4
0.2

103

105

Test error

0.8

0.4

103

0.0

105

100
200
400
800
1600
101

103

105

Figure 1: The performance of five 4-layers fully-connected networks on MNIST classification, plotted against training iterations. The number of neurons at each hidden layer is 100, 200, 400, 800
or 1600, for each network. Details are available in Section 4.
To give a glimpse into the MF limit, in Fig. 1, we plot the evolution of the performance
of several 4-layers networks, each with a distinct number of neurons per layer, on the MNIST
classification task, under the chosen scalings. Observe how well the curves with large numbers of
neurons coincide, while the networks still achieve non-trivial performance.
In the following, we shall give a motivating example via the two-layers network case, before
presenting the contributions of our work, discussing related works and outlining the rest of the
paper. Before we proceed, we introduce some mathematical conventions.

1.1

Notations, definitions and conventions

We use boldface letters to denote vectors if lowercase (e.g. x, θ) and matrices if uppercase (e.g.
W ). For n ∈ N>0 , we use [n] to denote the set {1, 2, ..., n}. For a scalar mapping f : R 7→ R and a
vector u ∈ Rn , we use f (u) to denote (f (u1 ) , ..., f (un ))⊤ entry-wise. For two vectors u, v ∈ Rn ,
hu, vi denotes the usual Euclidean inner product. For a set E ⊆ R and x ∈ R, we use E + x to
denote {u + x : u ∈ E}.
We reserve the notation P (Ω) for the set of probability measures on the set Ω. Strictly speaking,

2

one should associate Ω with a sigma-algebra to define P (Ω). We ignore this important technical
fact in this paper. We will also make use of the concept of stochastic kernels: ν is a stochastic
kernel1 with a source set Ω (associated with a sigma-algebra F) and a target set S (associated with
a sigma-algebra S) if ν (·|·) is a mapping S × Ω 7→ [0, 1] such that ν (·|ω) ∈ P (S) for each ω ∈ Ω
and ν (E|·) is F-measurable for each E ∈ S. We reserve the notation K (Ω, S) for the set of all
such kernels. For a stochastic kernel ν ∈ K (Ω, S), we define
CE {ν} (·) =

Z

xν (dx|·) ,

S

i.e. the conditional expectation operator. We assume that CE {ν} exists almost everywhere for all
stochastic kernelsν to be considered
in the paper.

P
We use Emp {xi }i∈[n] to denote the empirical distribution (1/n) · ni=1 δxi . For a random
variable X, the distributional law of X is denoted by Law (X). For a measure µ, we use supp (µ)
to denote its support. A statement of the form A (x) = B (x) for µ-a.e. x for a probability measure
µ means that
Z
Z
A (x) φ (x) µ (dx) =

B (x) φ (x) µ (dx)

for all smooth and bounded φ.
For a functional f : F 7→ R on a suitable vector space F, we use Df to denote its differential:
for each g ∈ F, Df {g} is a linear functional from F to R such that
1
(f (g + ǫϕ) − f (g)) = Df {g} (ϕ)
ǫ→0 ǫ
lim

for ϕ ∈ F. We shall ignore the fact that F is not arbitrary for this concept to apply.
A subscript in the differential operator (∂, ∇ or D) indicates the partial differentiation w.r.t.
the respective argument. For example, for f (u, v, g) : R × Rm × F 7→ R where F is a set of
functions, we use ∂1 f , ∇2 f and D3 f (or ∂u f , ∇v f and Dg f respectively) to denote its partial
derivative w.r.t. u, v and g respectively.
We use kuk2 to denote the Euclidean norm of a vector u, kW kF the Frobenius norm of a matrix
W and kf k∞ the max norm of a function f .

1.2

A motivating example: two-layers neural networks

We give a brief and informal overview of relevant results from [MMN18]. Consider the following
two-layers neural network:
n
1X
σ (x; θ i ) ,
(1)
ŷn (x; W) =
n i=1
where x ∈ Rd is the input, W = {θ i }i∈[n] is the collection of weights θ i ∈ RD , and σ : Rd ×RD 7→ R
is the (nonlinear) activation. Here each term σ (x; θ i ) is a neuron. With θ i = (βi , wi , bi ) ∈ R×Rd ×R
and σ (x; θ i ) = βi ϕ (hw i , xi + bi ) for a scalar nonlinearity ϕ, this network reduces to the usual twolayers fully-connected neural network. An illustration
is given in Fig. 2.

k
Suppose that at each time k ∈ N, the data x , y k ∈ Rd × R is drawn independently from
a probabilistic source P. We train the network with the loss L (y, ŷn (x; W)) for a loss function
1

Not to be confused with a kernel function that is randomly generated.

3

θj

βj

(w j , bj )

ŷn

ŷn

x

x
(a)

(b)

Figure 2: (a): A graphical representation of a two-layers network, as in Eq. (1). (b): An equivalent
representation for σ (x; θ i ) = βi ϕ (hwi , xi + bi ).
n

L : R × R 7→ R. In particular, starting from an initialization W 0 = θ 0i
α > 0, we perform (discrete-time) SGD:




θ k+1
= θ ki − αn∇θi L y k , ŷn xk ; W k
i





o

i∈[n]



= θ ki − α∂2 L y k , ŷn xk ; W k



, for a learning rate





∇θ σ xk ; θ ki .

(2)

We take a note on the scalings by n in Eq. (1) and (2). The MF limit behavior can then be
observed in the following two senses: statics and dynamics.
Statics.
Observe that the sum in (1) exhibits symmetry in the role of the neurons. In the limit n → ∞, one
can replace this sum with an integral:
ŷ (x; ρ) =


Z

σ (x; θ) ρ (dθ) ,

(3)







for ρ ∈ P RD . In particular, on one hand, for a given W, the identification ρ = Emp {θ i }i∈[n]
results in ŷ (x; ρ) = ŷn (x; W). On the other hand, for a given ρ, taking θ i ∼ ρ i.i.d., one gets
ŷn (x; W) ≈ ŷ (x; ρ). An intriguing observation is that, given L convex in the second argument,
EP {L (y, ŷ (x; ρ))} is convex in ρ [BRV+ 06]. As another interesting fact, [MMN18] proves that
under certain regularity conditions and with L being the squared loss,
inf EP {L (y, ŷn (x; W))} − inf EP {L (y, ŷ (x; ρ))} = O
W

ρ



1
n

 

n→∞

−−−→ 0.

(4)



In short, ρ is a surrogate measure for Emp {θ i }i∈[n] .
Dynamics.
Using the same idea of replacing the sum with an integral, one can usethe aboveinfinite-n representation to describe the SGD dynamics. In particular, let ρ̂kn = Emp

n

θ ki

o

i∈[n]

the empirical

distribution of neuronal weights at time k on the SGD dynamics. Suppose that given some ρ0 , at
4

initialization, ρ̂0n ⇒ ρ0 as n → ∞. For α ↓ 0 as n → ∞, under suitable conditions, [MMN18] shows
⌊t/α⌋
that almost surely ρ̂n
converges weakly to a deterministic limit ρt . This limit is defined via the
following differential equation with random initialization:
n




o
d t
θ = −EP ∂2 L y, ŷ x; ρt ∇θ σ x; θ t ,
dt

(5)

where θ t ∼ ρt and θ 0 ∼ ρ0 . More specifically, given ρ0 , we generate θ 0 ∼ ρ0 . Then we let θ t
evolve according to Eq. (5) from the initialization θ 0 with ρt = Law θ t at any time t. Note that
while [MMN18] defines ρt via a partial differential equation, what we present here is an equivalent
definition that is more convenient for our discussion.
The behavior of the network throughout the SGD dynamics thus tends to a non-trivial limit,
given in an explicit formula, as the number of neurons tends to infinity. In fact, [MMN18] proves
a more quantitative statement that holds so long as n ≫ d the data dimension and t ≤ T not too
large.
Heuristic derivation.
A heuristic to derive Eq. (5) from Eq. (2) is by firstly, identifying t = kα and recognizing that
α ↓ 0 leads to time continuum and in-expectation property w.r.t. the data, for i ∈ [n]:

n



o
d t
θ i ≈ −EP ∂2 L y, ŷn x; W t ∇θ σ x; θ ti ,
dt

n

W t = θ ti

o

i∈[n]

.

Secondly, we again replace a sum with an integral, wherever possible.
Here ŷn x; W t ≈ ŷ x; ρ̃t

t
t
from the statics, with ρ̃ being the surrogate measure for Emp W at each time t ≥ 0, which yields


n




o
d t
θ i ≈ −EP ∂2 L y, ŷ x; ρ̃t ∇θ σ x; θ ti .
dt



(6)

Thirdly, we observe symmetry among the neurons in the above. If this symmetry is attained at
t = 0 by
proper initialization, it should be maintained at all subsequent t, and hence one has

Law θ ti ≈ ρ̃t in the limit n → ∞, in which case we drop the subscript i. If at initialization
ρ̃0 = ρ0 , then by comparing the resultant dynamics with Eq. (5), one identifies ρ̃t ≈ ρt .

1.3

Contributions

In this work, we aim to develop a formalism which describes and derives the MF limit for multilayer
neural networks under suitable scalings.
From the derivation for two-layers networks, we observe that symmetry among the neurons
plays a key role in the MF limit. Intuitively one may expect the same for multilayer networks in
that there is symmetry among neurons of the same layer – see Fig. 3 of a three-layers network and
Fig. 4 of a generic multilayer one, for visualization. Yet when one attempts to extend the argument
from the two-layers case, several difficulties and questions arise:
• In the two-layers case, the network output ŷn (x; W) is a sum of signals from individual
neurons which do not share weights. However in a multilayer network, neurons at layer ℓ
receive signals that are constrained to come from the same set of neurons of layer ℓ − 1 or
ℓ + 1.
5

• In the two-layers case, each neuron is represented by its respective weight θ ki at each time
k. This representation is natural: ŷn (x; W) assumes the simple form of a sum, and once
the approximation ŷn (x; W) ≈ ŷ (x; ρ) is made, each neuron is updated separately under the
SGD dynamics, in light of Eq. (6). However, as said above, the multilayer case presents
a certain structural constraint. Moreover due to the layering structure, the update of each
neuron is influenced by other neurons of the adjacent layers. In what way can we give a
quantitative representation for each neuron that respects the complexity in the structure,
and at the same time, exploits the neuronal symmetry to make simplifications?
• Observe that the scalings are chosen so that quantities of interests remain O (1), roughly
speaking. In the particular case of two-layers networks, the scaling 1/n in ŷn (x; W) and the
factor
n in the gradient update in Eq. (2) ensure that ŷn (x; W) and the differential change

k+1
θ i − θ ki /α are O (1). Under what scalings does the MF limit behavior occur for the
multilayer case?
To make a first step, we postulate that neuronal symmetry gives rise to two crucial properties, which
we call marginal uniformity and self-averaging. While they are used already in the two-layers case,
the complexity in a multilayer network requires more extensive and explicit use of these properties,
especially self-averaging. This enables a heuristic derivation of the MF limit, revealing the answers
to the aforementioned questions:
• We propose that each certain neuron is represented not directly by its corresponding weights,
but by a stochastic kernel which outputs at random the corresponding weights, conditional on
the neurons of the previous layer. We show how this representation, which changes from layer
to layer, can adapt to the constraint of multilayer hierarchies and formalize the properties
arising from neuronal symmetry.
• Despite the somewhat complex representation, interestingly to describe the MF limit of multilayer fully-connected networks, one requires only one simple statistic of the stochastic kernel:
its conditional expectation. A key insight is the following: since a neuron at layer ℓ receives an
aggregate of signals from a set of neurons of layer ℓ + 1 or ℓ − 1, this aggregate simplifies itself
when one views these neurons as one whole ensemble, thanks to the self-averaging property.
• We also find that the appropriate scalings are not uniform across different layers.
We must caution the readers that our current development is non-rigorous. As such, an outstanding
challenge remains: under what regularity conditions, as well as what precise mathematical sense,
can the formalism hold? On the other hand, the formalism is meant to be informative: firstly,
it is predictive of the MF limit behavior to be observed in real simulations when the number of
neurons is sufficiently large; secondly, it explores a regime, associated with specific scalings, that
is under-studied by current experimental and theoretical pursuits; thirdly, it signifies the potential
for a theoretical framework to analyze and design multilayer neural networks – a quest that has
recently witnessed progresses in the two-layers case.

1.4

Outline

In Section 2, we present the formalism in the particular case of a three-layers network. In particular,
Section 2.1 and Section 2.2 run in parallel, each describing the forward pass, the backward pass and
6

the (learning or evolution) dynamics. The former section is on the neural network with scalings,
and the latter is on its MF limit. We give a heuristic derivation of their connection in Section 2.3
and several remarks in Section 2.4. The case of general multilayer networks is presented in Section
3, with its heuristic derivation deferred to Appendix A. Since the treatments of these two cases
are similar in spirit, the readers are urged to read Section 2, where the key ideas are explained in
greater details. In Section 4, we present several experiments and a theoretical result to validate the
existence of the MF limit. We particularly do not aim for achieving competitive empirical results
in our experiments. It remains open to find good practices to train a network in this regime, a task
that deserves another investigation.
While our main focus is fully-connected multilayer networks, the generality of the principles
allows us to draw similar conclusions on certain other settings. See Appendix C where we discuss
the case of multilayer convolutional neural networks.
In the following, we discuss related works.

1.5

Related works

As mentioned, several recent works have studied the MF limit in the two-layers network case. The
works [MMN18, CB18b, SS18a, RVE18] establish the MF limit, and in particular, [MMN18] proves
that this holds as soon as the number of neurons exceeds the data dimension. [MMN18, CB18b]
utilize this limit to prove that (noisy) SGD can converge to (near) global optimum under different
assumptions. For a specific class of activations and data distribution, [JMM19] proves that this
convergence is exponentially fast using the displacement convexity property of the MF limit. Taking
the same viewpoint, [WLLM18] proves a convergence result for a specifically chosen many-neurons
limit. [RVE18, SS18b] study the fluctuations around the MF limit. Our analysis of the multilayer
case requires substantial extension and new ideas, uncovering certain properties that are not obvious
from the two-layers analysis (see also Section 2.4).
We take a note on the work [HJ15], which shares a few similarities with our work in the forward
pass description (for instance, in Eq. (1) of [HJ15] as compared to Eq. (16) in our work). [HJ15]
differs in that it takes a kernel method perspective and develops a Gaussian process formulation,
which makes strong assumptions on the distribution of the weights. Its formulation does not
extend beyond three layers. Meanwhile our work focuses on the MF limit, points out explicitly
the appropriate scalings, proposes new crucial ideas to address the backward pass and the learning
dynamics, and is not limited to any specific number of layers.
There is a vast literature on settings that assume a large number of neurons – typically
specific to the over-parameterized regime. We shall mention here a recent subset. The highly
non-convex nature of the optimization landscape enjoys attention from a major body of works
[SS16, SC16, FB17, NH17, MBM18, NH18, VBB18, Coo18, DL18, SJL18, YSJ19]. This is yet far
from a complete picture without a study of the trajectory of the learning dynamics, which has
witnessed recent progresses. Several works [LL18, DZPS19, DLL+ 18, AZLL18, AZLS18, ZCZG18]
concurrently show that gradient-based learning dynamics can find the global optimum in multilayer networks, provided an extremely large number of neurons. The work [JGH18] develops a
complementary viewpoint on the dynamics, the so-called neural tangent kernel, also in the limit
of infinitely many neurons. A common feature of these works is that throughout the considered
training period, certain properties of the network remain close to the randomized initialization, and
the network behaves like kernel regression. Further discussions in this regard can be found in the
recent note [CB18a]. Complementing these mathematical approaches, [GSd+ 18, SGd+ 18, GJS+ 19]
7

utilize the physics of jamming to make a quantitative prediction of the boundary between the
over-parameterized and under-parameterized regions, as well as the generalization behavior of overparameterized networks, under a specific choice of the loss function. In another development, several
works [PLR+ 16, SGGSD17, PSG17, YS17, CPS18, XBSD+ 18, HR18, Han18, LN19, YPR+ 19] obtain good initialization strategies by studying networks with infinitely many neurons and random
weights (which hence disregard the learning dynamics). These works form a basis for a Gaussian
process perspective [LSdP+ 18, dGMHR+ 18, GARA19, NXB+ 19]. All these directions are not directly comparable with ours. Furthermore we note that the settings in these works assume different
scalings from ours and thus do not exhibit the same MF limit behavior that is to be presented here.

2

Mean field limit in three-layers fully-connected networks

In this section, we develop a formalism in which the MF limit is derived for a three-layers neural
network under suitable scalings. The focus on this specific case is made for simplicity of the
presentation and illustration of the key ideas. While certain elements have already been seen in
the two-layers case, there are important and substantial differences that shall be highlighted.

2.1

Setting: A three-layers network

Forward pass.
We consider the following three-layers neural network with fully-connected layers and no biases:
ŷn (x; W) =

1
hβ, σ (h2 )i ,
n2

h2 =

1
W 2 σ (h1 ) ,
n1

h1 = W 1 x,

(7)

in which x ∈ Rd is the input to the network, ŷn (x; W) ∈ R is the output, W = {W 1 , W 2 , β} is
the collection of weights, W 1 ∈ Rn1 ×d , W 2 ∈ Rn2 ×n1 , β ∈ Rn2 , and σ : R 7→ R is a nonlinear
activation. h1 and h2 are commonly called the pre-activations. Here n1 = n1 (n) and n2 = n2 (n),
both of which shall be taken to ∞ as n → ∞. An illustration is given in Fig. 3.(a).
Backward pass.
The backward pass computes several derivative quantities to be used for learning. Let us define:
˜ β ŷn (x; W) = n2 ∇β ŷn (x; W) = σ (h2 ) ,
∇
˜ h ŷn (x; W) = n2 ∇h ŷn (x; W) = β ⊙ σ ′ (h2 ) ,
∇
2

(8)
(9)

2

⊤

˜ W ŷn (x; W) = n1 n2 ∇W ŷn (x; W) = ∇
˜ h ŷn (x; W) σ (h1 ) ,
∇
2
2
2


1
⊤˜
˜ h ŷn (x; W) = n1 ∇h ŷn (x; W) =
∇
W
∇
ŷ
(x;
W)
⊙ σ ′ (h1 ) ,
n
h
2
1
1
2
n2
˜ W ŷn (x; W) = n1 ∇W ŷn (x; W) = ∇
˜ h ŷn (x; W) x⊤ .
∇
1
1
1

(10)
(11)
(12)

Learning dynamics.




Similar to Section 1.2, we assume that at each time k ∈ N, the data xk , y k ∈ Rd × R is drawn
independently from a probabilistic source P. We train the network with the loss L (y, ŷn (x; W))
8

wi

νj (or fj )

βj

ŷn

ŷn

x

x
(a)

(b)

Figure 3: (a): A graphical representation of a three-layers network. (b): An equivalent representation, as proposed in Section 2.3. Neuron j of the second layer is represented by νj , and
fj = CE {νj }. Notice that neuron j of the second layer receives the forward pass information averaged over all neurons of the first layer. Likewise, neuron i of the first layer receives the backward
pass information averaged over all neurons of the second layer. However neuron j of the third layer
does not average its received forward pass information over all neurons of the second layer, due to
its connectivity. Likewise, neuron j of the second layer does not average its received backward pass
information over all neurons of the third layer.
n

for a loss function L : R × R 7→ R, using SGD with an initialization W 0 = W 01 , W 02 , β 0
learning rate α > 0:




β k+1 = β k − α∂2 L y k , ŷn xk ; W k






W k+1
= W k2 − α∂2 L y k , ŷn xk ; W k
2




W k+1
= W k1 − α∂2 L y k , ŷn xk ; W k
1
n





˜ β ŷn xk ; W k ,
∇







o

and a
(13)



˜ W ŷn xk ; W k ,
∇
2

(14)

˜ W ŷn xk ; W k .
∇
1

(15)

o





This yields the learning dynamics of W k = W k1 , W k2 , β k . Notice the scaling by n1 , n2 and n1 n2
at various places in Eq. (7) and Eq. (8)-(12).

2.2

Mean field limit

In the following, we describe a time-evolving system which resembles the three-layers network but
does not involve the numbers of neurons n1 and n2 . We then state a prediction that connects
this formal system with the three-layers network. This, in particular, specifies the MF limit of the
three-layers network.
Forward pass.
Let us define
ŷ (x; ρ1 , ρ2 ) =




Z

βσ (H2 (f ; x, ρ1 )) ρ2 (df, dβ) ,
n

o

where ρ1 ∈ P Rd , ρ2 ∈ P (F × R) for F = f : Rd 7→ R , and
H1 (w; x) = hw, xi ,

H2 (f ; x, ρ1 ) =
9

Z

f (w) σ (H1 (w; x)) ρ1 (dw) .

(16)

This describes a system defined via ρ1 and ρ2 . More specifically, ρ1 and ρ2 are the state of the
system, and the system takes x ∈ Rd as input and outputs ŷ (x; ρ1 , ρ2 ) ∈ R. One should compare
ŷ (x; ρ1 , ρ2 ), H1 (w; x) and H2 (f ; x, ρ1 ) with respectively ŷn (x; W), h1 and h2 of the three-layers
network.
Backward pass.
Let us define the following quantities:
∆β (f ; x, ρ1 ) = σ (H2 (f ; x, ρ1 )) ,

(17)

′

∆H2 (β, f ; x, ρ1 ) = βσ (H2 (f ; x, ρ1 )) ,

(18)

∆w2 (β, f, w; x, ρ1 ) = ∆H2 (β, f ; x, ρ1 ) σ (H1 (w; x)) ,
∆H1 (w; x, ρ1 , ρ2 ) = σ ′ (H1 (w; x))

Z

(19)

f (w) ∆H2 (β, f ; x, ρ1 ) ρ2 (df, dβ) ,

(20)

∆w1 (w; x, ρ1 , ρ2 ) = ∆H1 (w; x, ρ1 , ρ2 ) x.

(21)

One should compare Eq. (17)-(21) with Eq. (8)-(12) respectively.
Evolution dynamics.
Now we describe
a continuous-time evolution dynamics of the system,
  defined at each time t via

ρt1 ∈ P Rd and ρt2 ∈ P (F × R). Specifically, given ρ01 ∈ P Rd and ρ02 ∈ P (F × R), we

generate w0 ∼ ρ01 and f 0 , β 0 ∼ ρ02 . Taking them as the initialization, we then let wt , f t and β t
evolve according to


d t
w = Gw w t ; ρt1 , ρt2 ,
dt E

D


t
t
t
∂t f (w) + ∇f (w) , Gw w; ρ1 , ρt2 = Gf β t , f t , w; ρt1 , ρt2

d t
β = Gβ f t ; ρt1 , ρt2 ,
dt




∀w ∈ Rd ,

(23)



with ρt1 = Law wt and ρt2 = Law f t , β t , where we define


(22)

(24)

Gw (w; ρ1 , ρ2 ) = −EP {∂2 L (y, ŷ (x; ρ1 , ρ2 )) ∆w1 (w; x, ρ1 , ρ2 )} ,

Gf (β, f, w; ρ1 , ρ2 ) = −EP {∂2 L (y, ŷ (x; ρ1 , ρ2 )) ∆w2 (β, f, w; x, ρ1 )} ,
Gβ (f ; ρ1 , ρ2 ) = −EP {∂2 L (y, ŷ (x; ρ1 , ρ2 )) ∆β (f ; x, ρ1 )} .

The evolution is thus described by a system of partial differential equations with a random initialization.
The prediction.
We state our prediction on the connection
between this system and the three-layers neural network.

d
0
0
Given two measures ρ1 ∈ P R and ρ2 ∈ P (F × R), we generate W 01 , W 02 and β 0 as follows.
n

We draw the rows w01,i
from

ρ02

o

i∈[n1 ]

n

of W 01 i.i.d. from ρ01 . We also draw n2 i.i.d. samples fj0 , βj0

independently. We then form

W 02

by making
10

fj0



w01,i



o

j∈[n2 ]

its (j, i)-th entry. Finally we form

⊤

β 0 = β10 , ..., βn0 2 . We then run the system initialized at ρ01 and ρ02 to obtain ρt1 and ρt2 for any
t. We also train the neural network initialized at W 01 , W 02 and β 0 to obtain W k for any k. Our
formalism states that for any t ≥ 0, with n → ∞ (and hence n1 , n2 → ∞) and α ↓ 0, for sufficiently
regular (e.g. smooth and bounded) φ : R × R 7→ R,
n 



o

→ EP φ y, ŷ x; ρt1 , ρt2

n 



o

→ EPtest φ y, ŷ x; ρt1 , ρt2

EP φ y, ŷn x; W ⌊t/α⌋

n 



o

in probability over the randomness of initialization and data generation throughout SGD learning.
In fact, it is our expectation that a more general behavior could be observed. For example, we
expect that for any t ≥ 0, with n → ∞ and α ↓ 0,
EPtest φ y, ŷn x; W ⌊t/α⌋

n 

in probability, where Ptest is an out-of-sample distribution.

2.3



o

From three-layers network to the mean field limit: a heuristic derivation

To heuristically derive a connection between the three-layers network and its corresponding formal
system, we first state our postulates.
The postulates.
We observe that there is symmetry in the role among the neurons of the same layer. This symmetry,
once attained by proper initialization, is expected to hold at all subsequent time. This, in particular,
suggests the following two properties:
(a) Marginal uniformity: If a law that governs neuron i of layer ℓ depends on other neurons
of layer ℓ only through global statistics of layer ℓ, then this law applies to all neurons of layer
ℓ.
(b) Self-averaging: One can replace a sum of sufficiently many terms, which display symmetry
in their roles and each of which corresponds to one neuron from the same layer, with an
appropriate integral. More explicitly, if we associate neuron i among the n neurons of the
same layer with g (xi , Ai ), where Ai is a random quantity sampled independently from a
measure µi of neuron i, then for sufficiently large n,
n
1X
g (xi , Ai ) ≈
n i=1

Z

g (x, a) µ (da) ρ (dx, dµ)

(25)

for an appropriate probability measure ρ which plays a surrogate role for the ensemble of
neurons of this layer.
These properties suggest that one can obtain a non-trivial description, independent of the number
of neurons, of the network as n grows large.
To quantify the above properties, it is necessary to represent each neuron with a quantity. We
propose the following representation, which accords with the graphical model in Fig. 3.(b):
• At the first layer, neuron i is represented by the weight vector w 1,i ∈ Rd (the i-th row of
W 1 ).
11





• At the second layer, neuron j is represented by a stochastic kernel νj ∈ K Rd , R . Neuron
j generates the weight w2,ji (the (j, i)-th entry of W 2 ) according to νj (·|w1,i ).
• At the third layer, neuron j is represented by the weight βj ∈ R.
Representation by the weights {w1,i }i∈[n1 ] and {βj }j∈[n2 ] is natural. The crucial role of stochastic
kernels {νj }j∈[n2 ] will be clearer later (cf. Section 2.4), even though they do not appear in the
description of the formal system in Section 2.2.
We are now ready to give a heuristic derivation of the connection between the three-layers
network and the formal system.
Forward pass.
Let us derive Eq. (16) from Eq. (7). Similar to the two-layers case, we have for large n1 , at each
neuron j of the second layer for j ∈ [n2 ]:
h2,j =

1
hw2,j , σ (W 1 x)i ≈
n1

Z

w2 σ (hw, xi) νj (dw2 |w) ρ1 (dw) .


(26)


Here in the approximation, we have replaced the empirical measure Emp {w1,i , w2,ji }i∈[n1 ] with
νj ρ1 , by the self-averaging property. We also note that unlike the two-layers case, h2,j ’s for different
neuron j’s involve the same W 1 of the first layer. This is reflected in the use of ρ1 independent of
j. Now by setting
Z
fj (w) = CE {νj } (w) =

w2 νj (dw2 |w) ,

(27)

we obtain

h2,j ≈ H2 (fj ; x, ρ1 ) .
This results in
ŷn (x; W) =

(28)

n2
n2
1 X
1 X
βj σ (h2,j ) ≈
βj σ (H2 (fj ; x, ρ1 )) .
n2 j=1
n2 j=1

Applying self-averaging again, we finally obtain, for large n2 ,
ŷn (x; W) ≈

Z

βσ (H2 (f ; x, ρ1 )) ρ2 (df, dβ) = ŷ (x; ρ1 , ρ2 ) .

(29)

From this derivation, we see that:




• ρ1 is a surrogate measure for Emp {w1,i }i∈[n1 ] for the first layer’s neurons;




• ρ2 is a surrogate measure for Emp {fj , βj }j∈[n2 ] for the second and third layers’ neurons;
• as per Eq. (27), the only information about νj that is used to compute the forward pass is
fj .

12

Backward pass.
We derive the respective connection between Eq. (17)-(21) and Eq. (8)-(12) . From Eq. (28), we
have immediately:




˜ β ŷn (x; W)
∇

˜ h ŷn (x; W)
∇
2



= σ (h2,j ) ≈ ∆β (fj ; x, ρ1 ) ,

j



(30)

= βj σ ′ (h2,j ) ≈ ∆H2 (βj , fj ; x, ρ1 ) ,

j

which gives


˜ W ŷn (x; W)
∇
2



ji



˜ h ŷn (x; W)
= ∇
2



j

σ (h1,i )

≈ ∆H2 (βj , fj ; x, ρ1 ) σ (hw1,i , xi) = ∆w2 (βj , fj , w 1,i ; x, ρ1 ) .

(31)

˜ h ŷn (x; W):
Let us consider ∇
1


˜ h ŷn (x; W)
∇
1



i









n2


1 X
˜ h ŷn (x; W)  σ ′ (h1,i )
=
w2,ji ∇
2
j
n2 j=1

n2
1 X
≈
w2,ji ∆H2 (βj , fj ; x, ρ1 ) σ ′ (hw1,i , xi) .
n2 j=1

Recall that fj = CE {νj }. By our proposed representation, given a fixed w1,i we have w2,ji ∼
νj (·|w 1,i ). Hence we can again apply self-averaging in the following way:
Z
n2
1 X
w2,ji ∆H2 (βj , fj ; x, ρ1 ) ≈ w2 ∆H2 (β, CE {ν} ; x, ρ1 ) ν (dw2 |w1,i ) µ (dν, dβ)
n2 j=1

=

Z

(32)

CE {ν} (w 1,i ) ∆H2 (β, CE {ν} ; x, ρ1 ) µ (dν, dβ) ,




for a probability measure µ surrogate for Emp {νj , βj }j∈[n2 ] . We make further simplification
by observing that the integrand
dependson ν only through CE {ν} and recalling that ρ2 is the

surrogate measure for Emp {fj , βj }j∈[n2 ] :


˜ h ŷn (x; W)
∇
1



i

≈

Z



f (w1,i ) ∆H2 (β, f ; x, ρ1 ) ρ2 (df, dβ) σ ′ (hw1,i , xi) = ∆H1 (w 1,i ; x, ρ1 , ρ2 ) .

˜ W ŷn (x; W):
Finally we consider the i-th row of ∇
1


˜ W ŷn (x; W)
∇
1



i



˜ h ŷn (x; W)
= ∇
1



i

x ≈ ∆H1 (w1,i ; x, ρ1 , ρ2 ) x = ∆w1 (w1,i ; x, ρ1 , ρ2 ) .

(33)

We observe that like the forward pass, the only information about νj that is used to compute the
backward pass is fj .

13

Learning dynamics.
We derive the evolution dynamics (22)-(24) of the formal system from the SGD dynamics (13)-(15)
of the neural network. First, by identifying t = kα and taking α ↓ 0, one obtains time continuum
and in-expectation property w.r.t. P from the SGD dynamics:


n

o

d t
˜ β ŷn x; W t ,
β = −EP ∂2 L y, ŷn x; W t ∇
dt


o
n


d
˜ W ŷn x; W t ,
W t2 = −EP ∂2 L y, ŷn x; W t ∇
2
dt


o
n


d
˜ W ŷn x; W t ,
W t1 = −EP ∂2 L y, ŷn x; W t ∇
1
dt
n

in which W t = W t1 , W t2 , β t . We represent the neurons, at time t, by wt1,i
n

βjt

o



j∈[n2 ]

o

i∈[n1 ]

, in which wt1,i is the i-th row of W t1 , βjt is the j-th entry of β t , νjt ∈ K
n o





n o

, νjt

j∈[n2 ]


and

Rd , R and the

t
(j, i)-th entry of W t2 being w2,ji
∼ νjt · wt1,i . We also define fjt = CE νjt .
t
t
From
 ρ̃1 and ρ̃2 be surrogate measures for respectively
 Eq. (29), (30), (31) and (33), letting

Emp

n

wt1,i

o

and Emp

i∈[n1 ]

and i ∈ [n1 ],

n

fjt , βjt

o

j∈[n2 ]

, it is easy to see that at any time t ≥ 0, for j ∈ [n2 ]



d t
βj ≈ Gβ fjt ; ρ̃t1 , ρ̃t2 ,
dt

(34)

d t
w
≈ Gf βjt , fjt , wt1,i ; ρ̃t1 , ρ̃t2 ,
dt 2,ji


d t
w1,i ≈ Gw wt1,i ; ρ̃t1 , ρ̃t2 .
dt






(35)
(36)



t
∼ νjt · wt1,i . A key observation is that the right-hand side of Eq. (35) does not
Recall that w2,ji
t . As such, for ∆t → 0 and any event E ⊆ R,
depend on w2,ji













νjt+∆t E + Gf βjt , fjt , w t1,i ; ρ̃t1 , ρ̃t2 ∆t wt+∆t
≈ νjt E wt1,i .
1,i
We then get:





1
d  t  t 
w2 νjt+∆t dw2 wt+∆t
fj w1,i = lim
− w2 νjt dw2 wt1,i
1,i
∆t→0 ∆t
dt
Z 

  
 Z


1
w2 + Gf βjt , fjt , wt1,i ; ρ̃t1 , ρ̃t2 ∆t νjt dw2 wt1,i − w2 νjt dw2 wt1,i
≈ lim
∆t→0 ∆t
Z



Z





= Gf βjt , fjt , w t1,i ; ρ̃t1 , ρ̃t2 .

On the other hand,


 d
d  t  t   t  t 
fj w1,i = ∂t fj w1,i + ∇fjt wt1,i , wt1,i
dt
dt


≈ ∂t fjt






D









wt1,i + ∇fjt wt1,i , Gw wt1,i ; ρ̃t1 , ρ̃t2
14

E

,

by Eq. (36). Hence,


∂t fjt





D







wt1,i + ∇fjt wt1,i , Gw wt1,i ; ρ̃t1 , ρ̃t2

E





≈ Gf βjt , fjt , wt1,i ; ρ̃t1 , ρ̃t2 .




The marginal uniformity property applied to Eq. (36) posits that Law wt1,i is independent of i,




and the self-averaging property then suggests Law wt1,i ≈ ρ̃t1 in the limit n → ∞. In this case we
also have



D
E


∂t fjt (w) + ∇fjt (w) , Gw w; ρ̃t1 , ρ̃t2 ≈ Gf βjt , fjt , w; ρ̃t1 , ρ̃t2
(37)

/ supp ρ̃t1 are used in the computation
for ρ̃t1 -a.e. w. Observe that at any time t, values of fjt at w ∈
of neither the forward pass nor the backward pass. As such, we can extend the dynamic (37) to all
w ∈ Rd without affecting the prediction stated in Section
 2.2. Applying again marginal uniformity
and self-averaging to Eq. (34) and (37), we have Law fjt , βjt ≈ ρ̃t2 independent of j in the limit
n → ∞. If ρ̃01 = ρ01 and ρ̃02 = ρ02 then one identifies ρ̃t1 ≈ ρt1 and ρ̃t2 ≈ ρt2 . This completes the
derivation.
Finally we note that the initialization in the prediction statement in Section 2.2 is sufficient
to ensure firstly that symmetry among the neurons is attained at initialization and hence at all
subsequent time, and secondly ρ̃01 = ρ01 and ρ̃02 = ρ02 .


2.4

Discussions

Having established the MF limit and its derivation, we now make several discussions. These discussions extend in a similar spirit to the case of general multilayer networks.
Comparison with the two-layers case. We remark on two differences, which are not apparent
from the last sections, in the formulations between the two-layers case and the three-layers (or
multilayer) case:
• In the two-layers case, the population loss EP {L (y, ŷ (x; ρ))} is convex in ρ, if L is convex in
the second argument. In the three-layers case, the many-neurons description ŷ (x; ρ1 , ρ2 ) as
per Eq. (16) is no longer linear in (ρ1 , ρ2 ) and hence EP {L (y, ŷ (x; ρ1 , ρ2 ))} is generally nonconvex in (ρ1 , ρ2 ) (although it is convex in ρ2 ). This highlights the complexity of multilayer
structures.
• In the two-layers case, in both the forward and backward passes, self-averaging is used only
at the output ŷn (x; W) ≈ ŷ (x; ρ). In the three-layers case, self-averaging occurs not only at
the output but also at certain neurons, which is evident from Eq. (26) and (32).
Occurrences of self-averaging can be spotted visually from the connectivity among layers.
Compare Fig. 3.(b) (or Fig. 4) against Fig. 2.(b) for visualization. Except for the connection
between the last layer and the second last one, all other pairs of adjacent layers are densely
connected. Also notice that the last layer’s connectivity is similar to that of a two-layers
network. For a neuron at layer ℓ, self-averaging occurs in the forward pass information it
receives from layer ℓ − 1, if layer ℓ is not the last layer, provided that this piece of information
assumes the form (25). Likewise, self-averaging occurs in the backward pass information
it receives from layer ℓ + 1, if layer ℓ + 1 is not the last layer, provided that this piece of
information assumes the form (25).
15

Stochastic kernel representation. We remark on the use of stochastic kernels {νj }j∈[n2 ] . On
one hand, given any W 1 , any W 2 can be realized by means of the random generation w2,ji ∼
νj (·|w 1,i ) for suitable {νj }j∈[n2 ] . On the other hand, this random generation enables the application
of self-averaging to make the approximation (32), which is a crucial step in the backward pass
analysis.
The scalings. We comment on the rationale behind the scalings by the numbers of neurons n1
and n2 in the three-layers network (7) and its gradient update quantities (8)-(12). The principle is,
roughly speaking, to maintain each entry in the pre-activations h1 and h2 , the output ŷn (x; W),
as well as their iteration-to-iteration changes, to be O (1). It is done by a simple practice: we
normalize any quantity which is a sum over neurons of the same layer by its number of neurons in
both the forward and backward passes. This effectively enables self-averaging in light of Eq. (25).
A consequence of this principle is that while the gradient update of W 1 has a factor of n1
similar to the two-layers case (recalling Eq. (12) and Eq. (2)), that of W 2 is n1 n2 as per Eq.
(10). In general, for multilayer networks, the gradient update of a weight matrix that is not of the
first layer is scaled by its total number of entries. Furthermore, each entry of W 2 (or any weight
matrices, not of the first layer, in the multilayer case) remains O (1), distinct from W 1 whose rows
must adapt to x and whose entries are therefore not necessarily O (1).
On the learning rule. While we have used a fixed learning rate for simplicity of presentation,
the formalism can easily incorporate a non-uniform varying learning rate schedule. Specifically if
we modify the SGD updates (13)-(15) as follows:




β k+1 = β k − αξ0 (kα) ∂2 L y k , ŷn xk ; W k






W k+1
= W k2 − αξ2 (kα) ∂2 L y k , ŷn xk ; W k
2




W k+1
= W k1 − αξ1 (kα) ∂2 L y k , ŷn xk ; W k
1





˜ β ŷn xk ; W k ,
∇









˜ W ŷn xk ; W k ,
∇
2




˜ W ŷn xk ; W k ,
∇
1

for sufficiently regular functions ξ0 , ξ1 and ξ2 , then the evolution dynamics of the formal system
(22)-(24) should be adjusted to:


d t
w = ξ1 (t) Gw wt ; ρt1 , ρt2 ,
dt E

D


t
t
t
∂t f (w) + ∇f (w) , Gw w; ρ1 , ρt2 = ξ2 (t) Gf β t , f t , w; ρt1 , ρt2


d t
β = ξ0 (t) Gβ f t ; ρt1 , ρt2 .
dt

∀w ∈ Rd ,

The same prediction holds as we take α ↓ 0.
Non-fully-connected structures. While we have focused entirely on fully-connected networks,
we expect the same principle is applicable to other types of structure that maintain the same key
features. In Appendix C, we give a brief argument, as well as an experiment, to justify that this is
indeed the case for one example of interest: multilayer convolutional neural networks (CNNs).

16

On the local operation. One key structure that is exploited here is the summation of the form
(25), which does not explicitly requires a specific form of local interaction between a weight entry
and the pre-activation of a neuron. Here we are interested in more general local interactions. For
example, recalling the three-layers network (7), we consider the following form of the pre-activation
h2,j of neuron j of the second layer:
h2,j =

n1
1 X
σ ∗ (w2,ji , h1,i ) ,
n1 i=1

σ ∗ : R × R 7→ R.

The local operation σ ∗ reduces to the considered three-layers case if we set σ ∗ (w, h) = wσ (h).
Since the summation structure is retained, we expect that the choice of σ ∗ does not play a very
critical role: for a general σ ∗ , under the introduced scalings, the MF limit behavior can still be
observed. This is demonstrated for the case of CNNs in Appendix C.

3

Mean field limit in multilayer fully-connected networks

The development in this section is parallel to Section 2.2. We describe the multilayer neural network,
as well as its corresponding formal system and the prediction. This, in particular, specifies the MF
limit of the network. We defer to Appendix A to give a heuristic derivation.

3.1

Setting: Multilayer fully-connected networks

Forward pass.
We describe a neural network with L hidden layers, for a given L ≥ 1, a collection of integers
{d, n1 , n2 , ..., nL } and an integer q ≥ 1:
ŷn (x; W) =
hℓ =

nL
1 X
(σL (ΘL , hL ))i ,
nL i=1

1

W ℓ σℓ−1 (Θℓ−1 , hℓ−1 ) ,
nℓ−1
h1 = W 1 x,

(38)
ℓ = 2, ..., L,

in which x ∈ Rd is the input to the network, ŷn (x; W) ∈ R is the output, W = {W 1 , ..., W L , Θ1 , ..., ΘL }
is the collection of weights, W 1 ∈ Rn1 ×d , W ℓ ∈ Rnℓ ×nℓ−1 , Θℓ ∈ (Rq )nℓ , and σℓ : Rq × R 7→ R is a
nonlinear activation. (We treat Θℓ as a vector of length nℓ with each entry being an element in Rq .)
Here nℓ = nℓ (n) → ∞ as n → ∞. It is a common practice to use q = 3, σL (θ, h) = θ1 σ (h + θ2 )+θ3
and σℓ (θ, h) = σ (h + θ1 ) for ℓ < L and some scalar nonlinearity σ, in which case we obtain the
usual (L + 1)-layers fully-connected network with biases. An illustration is given in Fig. 4.
Backward pass.
Let us define the following derivative quantities:
˜ Θ ŷn (x; W) = nL ∇Θ ŷn (x; W) = ∇1 σ L (ΘL , hL ) ,
∇
L
L
˜
∇h ŷn (x; W) = nL ∇h ŷn (x; W) = ∂2 σ L (ΘL , hL ) ,
L

L

17

(θ 2 , f2 )

(θ 1 , w)

(θ L−1 , fL−1 ) (θ L , fL )

(θ 3 , f3 )

...

ŷn

x
Figure 4: A graphical representation of a multilayer neural network, with L + 1 fully-connected
layers. Here neuron j at layer ℓ > 1 is represented by (θ ℓ,j , fℓ,j ) to be consistent with the information
presented in Section 3, while we note the actual representation is (θ ℓ,j , νℓ,j ) for some stochastic
kernel νℓ,j and fℓ,j = CE {νℓ,j }, as per the derivation in Appendix A.
˜ Θ ŷn (x; W) = nℓ ∇Θ ŷn (x; W) =
∇
ℓ
ℓ



1



˜
W⊤
ℓ+1 ∇hℓ+1 ŷn (x; W) ⊙ ∇1 σℓ (Θℓ , hℓ ) ,

nℓ+1

1
⊤ ˜
˜
W
∇h ŷn (x; W) ⊙ ∂2 σℓ (Θℓ , hℓ ) ,
ℓ = L − 1, ..., 1,
∇hℓ ŷn (x; W) = nℓ ∇hℓ ŷn (x; W) =
nℓ+1 ℓ+1 ℓ+1


˜ h ŷn (x; W) σℓ−1 (Θℓ−1 , hℓ−1 )⊤ ,
˜ W ŷn (x; W) = nℓ nℓ−1 ∇W ŷn (x; W) = ∇
ℓ = L, ..., 2,
∇


ℓ

ℓ

ℓ





˜ h ŷn (x; W) x⊤ .
˜ W ŷn (x; W) = n1 ∇W ŷn (x; W) = ∇
∇
1
1
1
Notice the scalings by nℓ and nℓ nℓ−1 .
Learning dynamics.




We assume that at each time k ∈ N, the data xk , y k ∈ Rd × R is drawn independently from
a probabilistic source P. We train the network with then loss L (y, ŷn (x; W)) foro a loss function
L : R × R 7→ R, using SGD with an initialization W 0 = W 01 , ..., W 0L , Θ01 , ..., Θ0L and a learning
rate α > 0:




= W kℓ − α∂2 L y k , ŷn xk ; W k
W k+1
ℓ




= Θkℓ − α∂2 L y k , ŷn xk ; W k
Θk+1
ℓ





n





˜ W ŷn xk ; W k ,
∇
ℓ




˜ Θ ŷn xk ; W k ,
∇
ℓ

ℓ = 1, ..., L,
ℓ = 1, ..., L.

o

This yields the dynamics of W k = W k1 , ..., W kL , Θk1 , ..., ΘkL .

3.2

Mean field limit

Similar to Section 2.2, we describe a time-evolving system which does not involve the numbers of
neurons n1 , ..., nL . This leads to a MF limit which characterizes the behavior of the multilayer
network (38) during learning in the limit n → ∞ via this formal system. Before all, let F1 = Rd
and Fℓ = {f |f : Rq × Fℓ−1 7→ R} a vector space for ℓ = 2, ..., L.

18

Forward pass.
The forward pass of the formal system is defined by the following:
ŷ (x; ρ) =

Z





L−1
σL θ, HL f ; x, {ρi }i=1

where we define inductively:



ρL (dθ, df ) ,

(39)

H1 (w; x) = hw, xi ,
H2 (f ; x, ρ1 ) =




ℓ−1
Hℓ f ; x, {ρi }i=1
=

Z

Z

f (θ, w) σ1 (θ, H1 (w; x)) ρ1 (dθ, dw) ,




ℓ−2
f (θ, g) σℓ−1 θ, Hℓ−1 g; x, {ρi }i=1



ρℓ−1 (dθ, dg) ,

ℓ = 3, ..., L,

q
for which ρ = {ρi }L
i=1 , ρℓ ∈ P (R × Fℓ ) for ℓ = 1, ..., L − 1. Specifically, ρ is the state of the
system, and the system takes x ∈ Rd as input and outputs ŷ (x; ρ) ∈ R. One should compare Eq.
(39) with the multilayer network (38).

Backward pass.
We define the quantities ∆θ,ℓ and ∆H,ℓ for ℓ = 1, ..., L as follows. First we define the quantities for
ℓ = L:








L−1
L−1
∆θ,L θ, f ; x, {ρi }i=1
= ∇1 σL θ, HL f ; x, {ρi }i=1









L−1
L−1
∆H,L θ, f ; x, {ρi }i=1
= ∂2 σL θ, HL f ; x, {ρi }i=1

Then we define the rest inductively:
∆θ,L−1 (θ, f ; x, ρ) =
∆H,L−1 (θ, f ; x, ρ) =
∆θ,ℓ (θ, f ; x, ρ) =

Z
Z
Z





L−1
g (θ, f ) ∆H,L θ ′ , g; x, {ρi }i=1
ρL dθ′ , dg





L−1
g (θ, f ) ∆H,L θ ′ , g; x, {ρi }i=1
ρL dθ′ , dg

g (θ, f ) ∆H,ℓ+1 θ ′ , g; x, ρ ρℓ+1 dθ ′ , dg


∆H,ℓ (θ, f ; x, ρ) =

Z

g (θ, f ) ∆H,ℓ+1 θ ′ , g; x, ρ ρℓ+1 dθ ′ , dg

∆θ,1 (θ, w; x, ρ) =

Z

g (θ, w) ∆H,2 θ ′ , g; x, ρ ρ2 dθ′ , dg



ℓ = L − 2, ..., 2,

∆H,1 (θ, f ; x, ρ) =

Z



g (θ, w) ∆H,2 θ ′ , g; x, ρ ρ2 dθ′ , dg


From these quantities, we define ∆W,ℓ for ℓ = 1, ..., L:





























,

.





L−2
∇1 σL−1 θ, HL−1 f ; x, {ρi }i=1





L−2
∂2 σL−1 θ, HL−1 f ; x, {ρi }i=1





ℓ−1
∇1 σℓ θ, Hℓ f ; x, {ρi }i=1





ℓ−1
∂2 σℓ θ, Hℓ f ; x, {ρi }i=1





∆W,2 θ, f, θ ′ , w; x, ρ = ∆H,2 (θ, f ; x, ρ) σ1 θ ′ , H1 (w; x) ,


∆W,1 (θ, w; x, ρ) = ∆H,1 (θ, w; x, ρ) x.
19

,

∂2 σ1 (θ, H1 (w; x)) .

ℓ−2
∆W,ℓ θ, f, θ ′ , g; x, ρ = ∆H,ℓ (θ, f ; x, ρ) σℓ−1 θ ′ , Hℓ−1 g; x, {ρi }i=1





,

∇1 σ1 (θ, H1 (w; x)) ,

L−1
L−1
L−2
∆W,L θ, f, θ ′ , g; x, {ρi }i=1
= ∆H,L θ, f ; x, {ρi }i=1
σL−1 θ ′ , HL−1 g; x, {ρi }i=1









,



,

ℓ = L − 1, ..., 3,





,

,

As a note, except for ∆W,1 whose range is Rd and ∆θ,ℓ whose range is Rq for ℓ = 1, ..., L, all other
derivative quantities map to R.
Evolution dynamics.
We describe a continuous-time evolution dynamics of the system, defined at each time t via ρt =
L
ρtℓ ℓ=1 where ρtℓ ∈ P (Rq × Fℓ ) for ℓ = 1, ..., L. First we define



Gθ,1 (θ, w; ρ) = −EP {∂2 L (y, ŷ (x; ρ)) ∆θ,1 (θ, w; x, ρ)} ,
Gθ,ℓ (θ, f ; ρ) = −EP {∂2 L (y, ŷ (x; ρ)) ∆θ,ℓ (θ, f ; x, ρ)} ,
n

Gθ,L (θ, f ; ρ) = −EP ∂2 L (y, ŷ (x; ρ)) ∆θ,L



L−1
θ, f ; x, {ρi }i=1

GW,1 (θ, w; ρ) = −EP {∂2 L (y, ŷ (x; ρ)) ∆W,1 (θ, w; x, ρ)} ,

Gf,2 θ, f, θ ′ , w; ρ = −EP ∂2 L (y, ŷ (x; ρ)) ∆W,2 θ, f, θ ′ , w; x, ρ
′







n



′



Gf,ℓ θ, f, θ , g; ρ = −EP ∂2 L (y, ŷ (x; ρ)) ∆W,ℓ θ, f, θ , g; x, ρ
′



Gf,L θ, f, θ , g; ρ = −EP ∂2 L (y, ŷ (x; ρ)) ∆W,L θ, f, θ

′





ℓ = 2, ..., L − 1,

o

,

,
,

L−1
, g; x, {ρi }i=1

ℓ = 3, ..., L − 1,

o

.

In addition, for each ℓ = 2, ..., L − 1, we define Gℓ : Rq × Fℓ × Rq × Fℓ−1 7→ R such that, inductively,
G2 θ, f, θ ′ , w; ρ = Gf,2 θ, f, θ ′ , w; ρ − ∇1 f θ ′ , w , Gθ,1 θ ′ , w; ρ
′




′



′







′

Gℓ θ, f, θ , g; ρ = Gf,ℓ θ, f, θ , g; ρ − ∇1 f θ , g , Gθ,ℓ−1 θ , g; ρ
ℓ = 3, ..., L.





− ∇2 f θ ′ , w , GW,1 θ ′ , w; ρ


′

− D2 f θ , g



′

Gf,ℓ−1 θ , g, ·, ·; ρ

The evolution dynamics is then defined by the following differential equations:


d t
w = GW,1 θ t1 , wt ; ρt ,
dt


d t
θ 1 = Gθ,1 θ t1 , w t ; ρt ,
dt


t
∂t f2 (θ, w) = G2 θ t2 , f2t , θ, w; ρt

∀ (θ, w) ∈ Rq × F1 ,



d t
ℓ = 2, ..., L,
θ ℓ = Gθ,ℓ θ tℓ , fℓt ; ρt ,
dt


∂t fℓt (θ, g) = Gf,ℓ θ tℓ , fℓt , θ, g; ρt
∀ (θ, g) ∈ Rq × Fℓ−1 , ℓ = 3, ..., L,

for θ t1 , wt

∼ ρt1 , θ tℓ , fℓt

L

∼ ρtℓ for ℓ = 2, ..., L, and ρt = ρtℓ ℓ=1
. More
specifically, given



L
0
0
0
0
0
q
0
0
0
ρ = ρℓ ℓ=1 where ρℓ ∈ P (R × Fℓ ), we generate θ 1 , w ∼ ρ1 and θ ℓ , fℓ ∼ ρ0ℓ for ℓ = 2, ..., L.






L

L

Taking them as the initialization, we let wt , θ tℓ ℓ=1 and
fℓt ℓ=2 evolve according
to the afore

t
t
t
mentioned differential equations, with ρ1 = Law θ 1 , w and ρtℓ = Law θ tℓ , fℓt for ℓ = 2, ..., L.




The prediction.

We state our prediction on the connection between the formal system andnthe multilayer neural neto

L
work. First, given ρ0 = ρ0ℓ : ρ0ℓ ∈ P (Rq × Fℓ ) ℓ=1 , we generate W 0 = W 01 , ..., W 0L , Θ01 , ..., Θ0L
n

for the neural network as follows. We draw θ 01,i , w01,i
20

o

i∈[n1 ]



i.i.d. from ρ01 , where θ 01,i is the i-th

,


,

element of Θ01 and w01,i is the i-th row of W 01 . We also independently draw nℓ i.i.d. samples
n

0
θ 0ℓ,j , fℓ,j

o

from ρ0ℓ , for ℓ = 2, ..., L. We then form Θ0ℓ by using θ 0ℓ,i as its i-th element. We

j∈[nℓ ]
W 0ℓ by









0
0
0
also form
letting its (j, i)-th entry equal fℓ,j
θ 0ℓ−1,i , fℓ−1,i
θ 01,i , w 01,i if
if ℓ ≥ 3 and f2,j
ℓ = 2.
Given the above initialization, we run the formal system initialized at ρ0 to obtain ρt for any t.
We also train the neural network initialized at W 0 to obtain W k for any k. Our formalism states
that for any t ≥ 0, with n → ∞ (and hence n1 , ..., nL → ∞) and α ↓ 0, for sufficiently regular (e.g.
smooth and bounded) φ : R × R 7→ R,

n 



o

n 



o

→ EP φ y, ŷ x; ρt

n 



o

→ EPtest φ y, ŷ x; ρt

EP φ y, ŷn x; W ⌊t/α⌋

in probability over the randomness of initialization and data generation throughout SGD learning.
Similar to the three-layers case in Section 2.2, we expect to observe a more general behavior,
for example, that for any t ≥ 0, with n → ∞ and α ↓ 0,
EPtest φ y, ŷn x; W ⌊t/α⌋

in probability, where Ptest is an out-of-sample distribution.

4

n 



o

Validation of the formalism

In this section, we perform validation tests on the MF limit behavior as predicted by the formalism.

4.1

Statics: a theoretical justification

As a first test, we ask whether the forward pass description of the formalism is meaningful, in
particular, whether one can obtain a result similar to Eq. (4) of the two-layers case. We shall argue
that it is indeed the case, in particular,
lim inf EP {L (y, ŷn (x; W))} = inf EP {L (y, ŷ (x; ρ))}
ρ

n→∞ W

under suitable conditions. Here we recall the multilayer network (38) and its formal system (39)
from Section 3, and also that nℓ = nℓ (n) → ∞ as n → ∞. This establishes an asymptotic
equivalence between the global optimum of the network and that of the formal system. We defer
this task to Appendix B.

4.2

Dynamics: an experimental justification

We present a second test, which aims to validate the predictions as stated in Section 2.2 and
Section 3.2, via experiments. In particular, we would like to verify whether the evolution curve of
a multilayer network approaches some non-trivial limiting curve as its number of neurons grows
large. As a reminder, our experimental settings are not tuned to attain competitive performances
since it is not our goal.

21

4.2.1

Experimental setting

We shall mainly consider the following three supervised learning tasks:
• Isotropic Gaussians classification: this is an artificial 2-classes dataset,
 considered in[MMN18].
The data is generated as follow: y ∼ Unif ({−1, +1}), and x|y ∼ N 0, (1 + y∆)2 I d , for some
∆ ∈ (0, 1). Here we choose d = 32 and ∆ = 0.4. Note that no linear classifiers can attain nontrivial performance on this problem. We use the squared loss L (y1 , y2 ) = (y1 − y2 )2 . We measure the population loss EP {L (y, ŷn (x; W))} and the classification error P (y ŷn (x; W) < 0),
estimated by Monte-Carlo averaging over 104 samples.
• MNIST classification: this is the popular MNIST 10-classes dataset. We normalize all grayscale pixels in the image to the range [−1, +1]. We use the cross-entropy loss L, and use the
whole training set of size 60 × 103 . Here d = 28 × 28 = 784. We measure the loss and the
classification error, averaged over 104 samples drawn from the training set or the test set.
• CIFAR-10 classification: this is the popular CIFAR-10 dataset with 10 classes. We normalize
each RGB value in the image to the range [−1, +1]. We use the cross-entropy loss L, and use
the whole training set of size 50 × 103 . Here d = 3 × 32 × 32 = 3072. We measure the loss and
the classification error, averaged over 104 samples drawn from the training set or the test set.
To further the validation, the following task is also considered:
• CIFAR-10 classification with VGG16 features: the setting is almost the same as the above
CIFAR-10 task, except that instead of the raw CIFAR-10 images, we use the features which
are computed by the convolutional layers of the VGG16 network [SZ15] pre-trained on the
ImageNet dataset [RDS+ 15]. We first upscale the images to the size 128 × 128 × 3, then feed
them into the VGG16 network to extract the features of dimension d = 4 × 4 × 512 = 8192.
Note that the VGG16 network is not under our scalings; only the networks that we train on
the VGG16 features employ the scalings.
We use the usual structure (with scalings) for an (L + 1)-layers network:
1
1
1
βσ bL +
W L σ ... W 2 σ (b1 + W 1 x)
ŷn (x; W) =
nL
nL−1
n1






+ bL+1 ,

where the weight matrices are W 1 ∈ Rn1 ×d and W ℓ ∈ Rnℓ ×nℓ−1 for ℓ = 2, ..., L, the output weight
is β ∈ Rnout ×nL , the biases are bℓ ∈ Rnℓ for ℓ = 1, ..., L and bL+1 ∈ Rnout , and σ is the nonlinearity.
We observe that this fits into the general description (38) of multilayer networks, except for that
nout can be larger than 1. Here the isotropic Gaussians classification task has nout = 1, whereas
the other two tasks have nout = 10. While strictly speaking this is not covered by our theory, it
can be extended easily to nout > 1, provided that nout remains a finite constant as n → ∞. We
shall also restrict our experiments to uniform widths n1 = ... = nL = n. The scalings for gradient
updates can also be easily deduced from Section 3.1.
In the experiments, unless otherwise stated, we use mini-batch SGD to perform training. Note
that while we develop our theory for SGD learning dynamics in Sections 2 and 3, an inspection
of their derivations reveals that the batch size does not play a role in the asymptotic n → ∞ and
hence does not affect the MF limit. In finite-size simulations, the use of batch size larger than 1
has the advantage of smoother evolution. We shall use a batch size of 100 in our experiments.
22

We now make a remark on the initialization practice. As the formalism suggests, and also as
discussed in Section 2.4, the entries of the weight matrices W ℓ at layer ℓ > 1 and the output weight
β should be of the order O (1). Therefore, we do the following initialization:


W 01



ij

τ2
∼ N 0, 1
d

!

,



W 0ℓ



ij



∼ N µ2 , τ22


b0ℓ,i ∼ N µ3 , τ32





for ℓ = 2..., L,



β0



ij





∼ N µ2 , τ22 ,

for ℓ = 1..., L + 1.

This initialization is different from the usual practice, due to the introduced scalings. A quick
calculation shows that this initialization may appear degenerate in the following sense: for each
x, in the limit n → ∞, the output ŷn x; W 0 at initialization becomes independent of τ2 , and if
τ3 = 0, it converges to a non-random quantity, unlike the usual practice. This is not an issue, since
it corresponds to a non-trivial initialization ρ0 of the formal system. In general, we use µ2 6= 0.
4.2.2

Experimental results

In Fig. 5, 6 and 8, we present the results for several fully-connected networks of different n and L
on the three classification tasks, with σ being the rectifier linear unit (ReLU) σ (u) = max {0, u}.
We observe that in each plot, the curves become increasingly matching on the whole training period
as n increases, even when the networks overfit in the CIFAR-10 task. We also see that the networks
display highly nonlinear dynamics and attain non-trivial performances by the end of training. In
particular, it is evident from the isotropic Gaussians task that the trained networks must exploit
the nonlinearity of the ReLU – even though they are initialized to be completely in the linear region
of the ReLU of layer ℓ > 1. This is because otherwise the resultant classifiers would be linear and
cannot attain close to zero classification error. For the other two tasks, at each layer, we count
the number of non-positive pre-activation entries and average it over the test set. Our simulations
similarly indicate that by the end of training, this number amounts to a significant fraction, and
therefore, the networks also exploit the nonlinearity of the ReLU.
The performance on the real datasets is realistic and not trivialized by the introduced scalings.
To illustrate the point, we note that the work [PSG17] reports of a 200-layers vanilla fully-connected
network, which attains a test error rate of more than 45% on the CIFAR-10 dataset. This network
is initialized with i.i.d. Gaussian weights of zero mean and carefully selected variance, is trained
without regularization, but is not under our scalings. We contrast this with the 4-layers network
with n = 800 in Fig. 8, which achieves a similar test error rate of about 43%.
In Fig. 5, 7 and 9, we compare the evolutions of two initializations which differ only by the
choice of τ2 , but share the same τ3 = 0. Recall that due to the way we initialize the networks,
the value of τ2 does not affect the initial values of the network output or the pre-activations in the
limit n → ∞. Despite this fact, we observe in Fig. 5, 7 and 9 that the two initializations yield
two different trajectories. This is consistent with our formalism: each τ2 gives rise to a different
initialization ρ0 of the formal system and hence a different evolution trajectory.
In Fig. 10, we plot the evolution for a different choice of σ: the tanh activation. In Fig. 11, we
plot the evolution of 4-layers networks for the specific task of CIFAR-10 classification with VGG16
features. We still observe that the larger n, the better the curves match. The performance in Fig.
11 is also reasonable and shows an expectedly marked improvement over networks trained on raw
CIFAR-10 images; for instance, by the end of training, the network with n = 800 achieves a test
error rate of about 14%.
23

Population loss

Classification error

50
100
200
400
800

0.8
0.6
0.4
0.2
101

0.2

105

103

0.0

Population loss
0.75
0.50
0.25
101

103

101

103

105

Classification error

50
100
200
400
800

1.00

50
100
200
400
800

0.4

50
100
200
400
800

0.4
0.2

105

0.0

101

103

105

Figure 5: The performance of five 5-layers fully-connected networks on isotropic Gaussians classification, plotted against training iteration. Here for each network, n = 50, 100, 200, 400, √
800
respectively, σ is the ReLU, and the learning rate α = 0.001. Top row: we initialize with τ1 = 2,
µ2 = 1, τ2 = 0.1, µ3 = 0 and τ3 = 0. Bottom row: aside from the same initialization (solid lines),
we perform another initialization that differs by τ2 = 3 (dotted lines).

Acknowledgement
The work was partially supported by NSF through grants CCF-1714305 and IIS-1741162, and
the William R. Hewlett Stanford Graduate Fellowship. The author would like to thank Andrea
Montanari and Huy Tuan Pham for many helpful and inspiring discussions and their encouragement.

24

Training loss
2

101

100
200
400
800
1600

1.5
1.0
0.5
103

105

0.0

101

Training loss
2

0

101

1.5
1.0
0.5
105

0

101

101

2.0

105

1.0
0.5
105

101

0.4
0.2
103

105

101

0.4
0.2
105

0.0

101

101

0.8
0.4
0.2
103

105

0.25
105

0.00

101

0.0

101

0.8

103

105

100
200
400
800
1600

0.6
0.4
0.2
103

105

Test error

100
200
400
800
1600

0.50

103

100
200
400
800
1600

0.6

Training error
0.75

103

0.0

Test error

100
200
400
800
1600

0.6

103

100
200
400
800
1600

0.6

Training error

100
200
400
800
1600

1.5

103

103

Test loss

100
200
400
800
1600

1

0.2

100
200
400
800
1600

Training loss
2

0.4
0.0

0.8

0.8

2.0

103

Test error

100
200
400
800
1600

0.6

Test loss

100
200
400
800
1600

1

Training error
0.8

2.0

100
200
400
800
1600

1
0

Test loss

105

0.0

101

103

105

Figure 6: The performance of five fully-connected networks on MNIST classification, plotted against
training iteration, in each plot. Here for each network, n = 100, 200, 400,
√ 800, 1600 respectively, σ
is the ReLU, and the learning rate α = 0.01. We initialize with τ1 = 2, µ2 = 1, τ2 = 0.1, µ3 = 0
and τ3 = 0. From the top row: 3-layers networks, 4-layers networks and 5-layers networks.
Training loss
2

2.0

100
200
400
800
1600

1
0

Test loss

101

1.0
0.5
105

0.0

0.75

100
200
400
800
1600

1.5

103

Training error

101

0.25
103

105

0.8

100
200
400
800
1600

0.50

0.00

Test error

101

100
200
400
800
1600

0.6
0.4
0.2
103

105

0.0

101

103

105

Figure 7: The performance of 4-layers fully-connected networks on MNIST classification, plotted
against training iteration, in each plot. For each network, n = 100, 200,√
400, 800, 1600 respectively,
σ is the ReLU, and the learning rate α = 0.01. We initialize with τ1 = 2, µ2 = 1, µ3 = 0, τ3 = 0,
with τ2 = 0.1 for the solid lines and τ2 = 2 for the dotted lines.
25

Training loss
2

0

101

6
4
105

101

0

4

103

105

101

5
200
400
800
1600
2400
101

3

105

0.4
0.2
103

101

103

105

101

105

0.0

0.8

101

103

105

0.4

101

Training error

0.4
0.2
105

0.0

101

0.8

103

105

200
400
800
1600
2400

0.6

103

105

Test error

200
400
800
1600
2400

0.6

103

200
400
800
1600
2400

0.6

0.8

103

0.4

Test error

200
400
800
1600
2400

0.6

2
103

0.6

0.8

200
400
800
1600
2400

4

101

200
400
800
1600
2400

Training error

Test loss

2

0

0.0

105

2

Training loss

1

0.2

200
400
800
1600
2400

6

200
400
800
1600
2400
101

0.4

103

0.8

200
400
800
1600
2400

0.6

Test loss

2

Test error

0.8

2
103

Training loss

1

Training error

200
400
800
1600
2400

8

200
400
800
1600
2400

1

Test loss

10

105

101

103

105

Figure 8: The performance of five fully-connected networks on CIFAR-10 classification, plotted
against training iteration, in each plot. For each network, n = 200, 400,√
800, 1600, 2400 respectively,
σ is the ReLU, and the learning rate α = 0.07. We initialize with τ1 = 2, µ2 = 1, τ2 = 0.1, µ3 = 0
and τ3 = 0. From the top row: 3-layers networks, 4-layers networks and 5-layers networks.
Training loss
2

200
400
800
1600
2400

1
0

101

Test loss

8

Training error

200
400
800
1600
2400

6
4

0.8

105

101

0.4
0.2
103

105

0.0

0.8

200
400
800
1600
2400

0.6

2
103

Test error

101

200
400
800
1600
2400

0.6

103

105

0.4

101

103

105

Figure 9: The performance of 4-layers fully-connected networks on CIFAR-10 classification, plotted
against training iteration, in each plot. For each network, n = 200, 400, 800,
√ 1600, 2400 respectively,
σ is the ReLU, and the learning rate α = 0.07. We initialize with τ1 = 2, µ2 = 1, µ3 = 0, τ3 = 0,
with τ2 = 0.1 for the solid lines and τ2 = 2 for the dotted lines.
26

1.00

Population loss

0.75
0.25
101

Training loss
2
1
0

101

2.0
1.0
0.5
105

101

Training loss
2.0
1.0
0.5
101

105

0.0

0.75

0.25
103

2.0

105

0.00

105

101

105

Test error
0.8

101

0.4
0.2
103

105

0.2
103

105

101

101

103

200
400
800
1600
2400

0.7
0.6
103

105

0.5

105

Test error

0.8

200
400
800
1600
2400

0.4

0.0

0.9

0.8
0.6

100
400
600
800
1600

0.6

Training error

1.5
103

103

100
400
600
800
1600

0.50

200
400
800
1600
2400

2.5

101

Training error

Test loss

200
400
800
1600
2400

1.5

103

100
400
600
800
1600

1.5

103

0.2

Test loss

100
400
600
800
1600

50
100
200
400
800

0.4

50
100
200
400
800

0.50
0.00

Classification error

101

103

105

Figure 10: The performance of several fully-connected networks,
√ plotted against training iteration,
in each plot. Here the activation σ = tanh. We choose τ1 = 2, µ2 = 1, µ3 = 0 and τ3 = 0 in all
plots. First row: isotropic Gaussians task, 5 layers, n = 50, 100, 200, 400, 800, τ2 = 3, and annealed
learning rate αk = 0.003k−0.1 for k ≥ 1 the SGD iteration. Second row: MNIST task, 4 layers,
n = 100, 400, 600, 800, 1600, τ2 = 2, and αk = 0.01k−0.1 . Third row: CIFAR-10 task, 4 layers,
n = 200, 400, 800, 1600, 2400, τ2 = 2, and αk = 0.2k−0.1 .
Training loss
2

0

101

200
400
800
1600
2400

1.5
1.0
103

105

0.5

Training error

Test error

0.8

2.0

200
400
800
1600
2400

1

Test loss

2.5

101

0.4
0.2
103

105

0.0

0.8

200
400
800
1600
2400

0.6

101

200
400
800
1600
2400

0.6
0.4
0.2
103

105

101

103

105

Figure 11: The performance of five 4-layers fully-connected networks on CIFAR-10 classification with VGG16 features, plotted against training
√ iteration, in each plot. Here n =
200, 400, 800, 1600, 2400, σ is the ReLU, α = 0.02, τ1 = 2, µ2 = 1, τ2 = 0.1, µ3 = 0, τ3 = 0.
27

A

A heuristic derivation for the mean field limit of multilayer
networks

We first recall the setting, as well as the formalism, in Section 3. We also recall the two properties,
as discussed in Section 2.3: marginal uniformity and self-averaging. These properties still hold in
the multilayer case, since symmetry among the neurons of the same layer holds. To make use of
these properties, we represent each neuron with the following (see also Fig. 4):
• At the first layer, neuron i is represented by (θ 1,i , w 1,i ) ∈ Rq × F1 , where w1,i is the i-th row
of W 1 and θ 1,i is the i-th element of Θ1 .
• At layer ℓ ≥ 2, neuron j is represented by (θ ℓ,j , νℓ,j ) where νℓ,j ∈ K (Rq × Fℓ , R) is a
stochastic kernel and θ ℓ,j is the j-th element of Θℓ . Neuron j generates the weight wℓ,ji
(the (j, i)-th entry of W ℓ ) according to ν2,j (·|θ 1,i , w1,i ) if ℓ = 2 and νℓ,j (·|θ ℓ−1,i , CE {νℓ−1,i })
otherwise.
We are now ready to give a heuristic derivation of the connection between the three-layers network
and the formal system.
Forward pass.
Let us derive Eq. (39) from Eq. (38). We have for large n1 , at each neuron j of the second layer
for j ∈ [n2 ]:
h2,j

1
hw2,j , σ1 (Θ1 , W 1 x)i ≈
=
n1

Z

wσ1 (θ, H1 (w; x)) ν2,j (dw|θ, w) ρ1 (dθ, dw) ,





which replaces the empirical measure Emp {θ 1,i , w 1,i , w2,ji }i∈[n1 ] with ν2,j ρ1 , by the self-averaging
property. Here we note that h2,j ’s for different neuron j’s involve the same Θ1 and W 1 of the first
layer. This is reflected by the use of ρ1 independent of j. By setting
f2,j (θ, w) = CE {ν2,j } (θ, w) =

Z

wν2,j (dw|θ, w) ,

we obtain
h2,j ≈ H2 (f2,j ; x, ρ1 ) .
Consequently,
h3,j

n2
1 X
1
w3,ji σ2 (θ 2,i , H2 (f2,i ; x, ρ1 )) .
hw3,j , σ2 (Θ2 , h2 )i ≈
=
n2
n2 i=1

The same argument then gives us:
h3,j ≈

Z

wσ2 (θ, H2 (f ; x, ρ1 )) ν3,j (dw|θ, f ) ρ2 (dθ, df ) = H3 (f3,j ; x, ρ1 , ρ2 ) ,

in which we set f3,j = CE {ν3,j }. Similarly, performing a chain of the same argument, we thus get,
for ℓ = 2, ..., L:


ℓ−1
hℓ,j ≈ Hℓ fℓ,j ; x, {ρi }i=1
,
fℓ,j = CE {νℓ,j } .
(40)
28

This yields
ŷn (x; W) =

Z
nL



1 X
L−1
σL (θ L,i , hL,i ) ≈ σL θ, HL f ; x, {ρi }i=1
ρL (dθ, df ) = ŷ (x; ρ) .
nL i=1




Note that ρ1 is a surrogate measure of Emp {θ 1,i , w 1,i }i∈[n1 ] , and for ℓ = 2, ..., L, ρℓ is that of




Emp {θ ℓ,i , CE {νℓ,i }}i∈[nℓ ] .
Backward pass.
We have from Eq. (40):


˜ Θ ŷn (x; W)
∇
L



˜ h ŷn (x; W)
∇
L









j

L−1
≈ ∇1 σL θ L,j , HL fL,j ; x, {ρi }i=1

j

L−1
≈ ∂2 σL θ L,j , HL fL,j ; x, {ρi }i=1













L−1
= ∆θ,L θ L,j , fL,j ; x, {ρi }i=1
,



(41)



L−1
= ∆H,L θ L,j , fL,j ; x, {ρi }i=1
.

˜ Θ ŷn (x; W):
Consider ∇
L−1


˜ Θ ŷn (x; W)
∇
L−1



j

=
≈

nL


1 X
˜ h ŷn (x; W)
∇1 σL−1 (θ L−1,j , hL−1,j )
wL,kj ∇
L
k
nL k=1

!

nL


1 X
L−1
wL,kj ∆H,L θ L,k , fL,k ; x, {ρi }i=1
nL k=1

!





L−2
× ∇1 σL−1 θ L−1,j , HL−1 fL−1,j ; x, {ρi }i=1



.

Recall that wL,kj ∼ νL,k (·|θ L−1,j , fL−1,j ), fL,k = CE {νL,k }, and that neuron k of layer L is
represented by (θ L,k , νL,k ). By the self-averaging property:


˜ Θ ŷn (x; W)
∇
L−1



j

≈

Z

f (θ L−1,j , fL−1,j ) ∆H,L



L−1
θ, f ; x, {ρi }i=1







ρL (dθ, df )

L−2
× ∇1 σL−1 θ L−1,j , HL−1 fL−1,j ; x, {ρi }i=1

= ∆θ,L−1 (θ L−1,j , fL−1,j ; x, ρ) ,





and similarly,


˜ h ŷn (x; W)
∇
L−1



j

≈ ∆H,L−1 (θ L−1,j , fL−1,j ; x, ρ) .

Performing the same argument and recalling that fℓ,j = CE {νℓ,j }, we obtain that


˜ Θ ŷn (x; W)
∇
ℓ





˜ h ŷn (x; W)
∇
ℓ

˜ Θ ŷn (x; W)
∇
1



˜ h ŷn (x; W)
∇
1



j



j



j



j

≈ ∆θ,ℓ (θ ℓ,j , fℓ,j ; x, ρ) ,

ℓ = 2, ..., L − 1,

≈ ∆H,ℓ (θ ℓ,j , fℓ,j ; x, ρ) ,

ℓ = 2, ..., L − 1,

≈ ∆θ,1 (θ 1,j , w1,j ; x, ρ) ,
≈ ∆H,1 (θ 1,j , w1,j ; x, ρ) .
29

(42)

(43)

Finally, it is then easy to see that


˜ W ŷn (x; W)
∇
L





˜ W ŷn (x; W)
∇
ℓ

˜ W ŷn (x; W)
∇
2







˜ W ŷn (x; W)
where ∇
1



i



ij

(44)

ij

≈ ∆W,ℓ (θ ℓ,i , fℓ,i , θ ℓ−1,j , fℓ−1,j ; x, ρ) ,

(45)

ij

≈ ∆W,2 (θ 2,i , f2,i , θ 1,j , w 1,j ; x, ρ) ,

(46)



≈ ∆W,1 (θ 1,i , w 1,i ; x, ρ) ,

(47)





˜ W ŷn (x; W)
∇
1



L−1
≈ ∆W,L θ L,i , fL,i , θ L−1,i , fL−1,i ; x, {ρi }i=1
,

i

ℓ = 3, ..., L − 1,

˜ W ŷn (x; W).
is the i-th row of ∇
1

Learning dynamics.
We now connect the evolution dynamics of the formal system with the SGD dynamics of the neural
network. Similar to the three-layers case in Section 2.3, we first take t = kα and α ↓ 0 to obtain
time continuum and in-expectation property w.r.t. P from the SGD dynamics:

o


n

d
˜ W ŷn x; W t ,
W tℓ = −EP ∂2 L y, ŷn x; W t ∇
ℓ
dt


n

o

d t
˜ Θ ŷn x; W t ,
Θℓ = −EP ∂2 L y, ŷn x; W t ∇
ℓ
dt

where W t = W t1 , ..., W tL , Θt1 , ..., ΘtL .
the first layer and neuron j of layer ℓ ≥
meanings accord n
withothe representation
t = CE ν t
define fℓ,j
for j ∈ [nℓ ] and
ℓ,j


Emp

n

θ t1,i , wt1,i
L

o

i∈[n1 ]



and

ρ̃tℓ

Given this, at any time t, we
neuron
i of
 represent


t
t
t
t
2 with respectively θ 1,i , w1,i and θ ℓ,j , νℓ,j , whose
described at the beginning of Appendix A. We also
ℓ = 2, ..., L. We let ρ̃t1 be the surrogate measure of

be that of Emp

ρ̃t = ρ̃tℓ ℓ=1 .
It is easy to see from Eq. (41)-(47) that


d t
w
dt 1,j
d t
w
dt 2,ij
d t
w
dt ℓ,ij
d t
θ
dt 1,j
d t
θ
dt ℓ,j

ℓ = 1, ..., L,





n

θ tℓ,i , CE

n

t
νℓ,i

oo

i∈[nℓ ]



for ℓ = 2, ..., L. Let

≈ GW,1 θ t1,j , wt1,j ; ρ̃t ,


(48)


t
≈ Gf,2 θ t2,i , f2,i
, θ t1,j , wt1,j ; ρ̃t ,



(49)


t
t
≈ Gf,ℓ θ tℓ,i , fℓ,i
, θ tℓ−1,j , fℓ−1,j
; ρ̃t ,





≈ Gθ,1 θ t1,j , wt1,j ; ρ̃t ,




t
≈ Gθ,ℓ θ tℓ,j , fℓ,j
; ρ̃t ,

ℓ = 3, ..., L,

(50)
(51)

ℓ = 2, ..., L.

(52)




Applying the marginal uniformity property to Eq. (48) and Eq. (51), we have Law θ t1,j , w t1,j is in



dependent of j, and thanks to the self-averaging property in addition, we obtain Law θ t1,j , wt1,j ≈
ρ̃t1 .

30

t , and since wt
Observe that in Eq. (49), the right-hand side does not depend on w2,ij
2,ij ∼





t
· θ t1,j , w t1,j , we get for ∆t → 0 and any event E ⊆ R,
ν2,i













t+∆t
t+∆t
t
t
ν2,i
E + Gf,2 θ t2,i , f2,i
, θ t1,j , w t1,j ; ρ̃t ∆t θ t+∆t
≈ ν2,i
E θ t1,j , w t1,j .
1,j , w 1,j

This gives us


d  t  t
1
f2,i θ1,j , wt1,j = lim
∆t→0 ∆t
dt

1
∆t→0 ∆t

≈ lim

Z

t+∆t
wν2,i

Z 



dw

t+∆t
θ t+∆t
1,j , w 1,j



−

Z

t
wν2,i







dw



θ t1,j , w t1,j





t
t
w + Gf,2 θ t2,i , f2,i
, θ t1,j , w t1,j ; ρ̃t ∆t ν2,i
dw θ t1,j , w t1,j

−

Z

t
wν2,i





dw

θ t1,j , w t1,j






!

t
= Gf,2 θ t2,i , f2,i
, θ t1,j , wt1,j ; ρ̃t .

On the other hand, from Eq. (48) and (51),

E
 

 D



d  t  t
t
t
f2,i θ 1,j , w t1,j = ∂t f2,i
θ t1,j , w t1,j + ∇1 f2,i
θ t1,j , w t1,j , Gθ,1 θ t1,j , w t1,j ; ρ̃t
dt
D



E
t
+ ∇2 f2,i
θ t1,j , wt1,j , GW,1 θ t1,j , wt1,j ; ρ̃t .

We thus get:





t
t
, θ, w; ρ̃t ,
∂t f2,i
(θ, w) ≈ G2 θ t2,i , f2,i





(53)

t at
for ρ̃t1 -a.e. (θ, w), recalling Law θ t1,j , wt1,j ≈ ρ̃t1 for any j ∈ [n1 ]. Observe that values of f2,i

(θ, w) ∈
/ supp ρ̃t1 are used in the computation of neither the forward pass nor the backward pass
at time t. As such, one can extend the dynamic (53) to all (θ, w) ∈ Rq × F1 without affecting the
prediction stated in Section 3.2. Applying
the marginal uniformity and self-averaging properties

t
≈ ρ̃t2 for any i ∈ [n2 ]. One can then perform a similar
again, we then obtain Law θ t2,i , f2,i
argument on Eq. (50) inductively for ℓ = 3, ..., L and get:




d  t  t
t
t
t
fℓ,i θ ℓ−1,j , fℓ−1,j
≈ Gf,ℓ θ tℓ,i , fℓ,i
, θ tℓ−1,j , fℓ−1,j
; ρ̃t ,
dt
E
 

 D



d  t  t
t
t
t
t
t
t
fℓ,i θ ℓ−1,j , fℓ−1,j
= ∂t fℓ,i
θ tℓ−1,j , fℓ−1,j
+ ∇1 fℓ,i
θ tℓ−1,j , fℓ−1,j
, Gθ,ℓ θ tℓ−1,j , fℓ−1,j
; ρ̃t
dt
n
o

t
t
t
+ D2 fℓ,i
θtℓ−1,j , fℓ−1,j
∂t fℓ−1,j
,

which yields



t
t
∂t fℓ,i
(θ, f ) ≈ Gℓ θ tℓ,i , fℓ,i
, θ, f ; ρt



∀ (θ, f ) ∈ Rq × Fℓ−1 ,





t
Law θ tℓ,i , fℓ,i
≈ ρ̃tℓ ,

for all i ∈ [nℓ ]. Hence, if ρ̃0ℓ = ρ0ℓ for all ℓ ∈ [L], then ρ̃tℓ ≈ ρtℓ for all ℓ ∈ [L] at any time t. This
completes the derivation.
Finally we note that the initialization in the prediction statement in Section 3.2 is sufficient
to ensure firstly that symmetry among the neurons is attained at initialization and hence at all
subsequent time, and secondly ρ̃0ℓ = ρ0ℓ for all ℓ ∈ [L].
31

B

Statics: equivalence of the optima for multilayer networks

We recall the multilayer network (38) and its formal system (39). Also recall that nℓ = nℓ (n) → ∞
as n → ∞. In the following, we argue that
lim inf EP {L (y, ŷn (x; W))} = inf EP {L (y, ŷ (x; ρ))} ,
ρ

n→∞ W

under certain assumptions to be stated below. This result is similar to, though not as quantitative
as, Eq. (4) of the two-layers case. To show the above, it is decomposed into the two inequalities:
inf EP {L (y, ŷ (x; ρ))} ≤ inf EP {L (y, ŷn (x; W))} ,
ρ
W
lim sup inf EP {L (y, ŷn (x; W))} ≤ inf EP {L (y, ŷ (x; ρ))} .
ρ
n→∞ W

(54)
(55)

Then the thesis follows immediately.
Below we shall let C denote any (immaterial) constant, and similarly, c = c (ǫ) denote any
“constant” that depends on ǫ – a parameter that is to be defined below. That is, C and c (ǫ) are
constants that may change from line to line. We state our assumptions:
• σL satisfies that for some constant C > 0:
|σL (θ 1 , h1 ) − σL (θ 2 , h2 )| ≤ C (1 + kθ 1 k2 + kθ 2 k2 ) (kθ 1 − θ 2 k2 + |h1 − h2 |) ,
|σL (θ, h)| ≤ C (1 + kθk2 ) ,

(56)
(57)

for any θ, θ 1 , θ 2 ∈ Rq and h, h1 , h2 ∈ R.
• For each ℓ = 1, ..., L − 1, σℓ satisfies that for some constant C > 0:
|σℓ (θ 1 , h1 ) − σℓ (θ 2 , h2 )| ≤ C (kθ 1 − θ 2 k2 + |h1 − h2 |) ,
kσℓ k∞ ≤ C,

(58)
(59)

for any θ 1 , θ 2 ∈ Rq and h1 , h2 ∈ R.
• L satisfies that for some constant C > 0:
|L (y1 , y2 ) − L (y3 , y4 )| ≤ C (1 + |y1 | + |y2 | + |y3 | + |y4 |) (|y1 − y3 | + |y2 − y4 |) ,

(60)

for any y1 , y2 , y3 , y4 ∈ R.
• There exists a constant C such that the data satisfies:
P (|y| > C) = 0.

(61)

• For all ǫ > 0 sufficiently small, there exists ρ = {ρℓ }L
ℓ=1 such that
EP {L (y, ŷ (x; ρ))} ≤ inf′ EP L y, ŷ x; ρ′
ρ


as well as that
ρL ({kθk2 > c}) = 0,



+ ǫ,

ρℓ ({f ∈ Fℓ : kf k∞ > c}) = 0,

for some c = c (ǫ).
32

ℓ = 2, ..., L,

(62)

(63)

B.1

Derivation of Eq. (54)

For ǫ > 0, we take W = {W 1 , ..., W L , Θ1 , ..., ΘL } that yields
EP {L (y, ŷn (x; W))} ≤ inf′ EP L y, ŷn x; W ′
W





+ ǫ.

Let Θℓ = (θ ℓ,i )i∈[nℓ ] , W 1 = (w1,i )i∈[n1 ] and W ℓ = (wℓ,ij )i∈[nℓ ],j∈[nℓ−1 ] . We construct ρ = {ρℓ }L
ℓ=1
as follows. Take
n1
1 X
δθ ,w .
ρ1 =
n1 i=1 1,i 1,i
We choose (any) f2,i ∈ F2 , for each i ∈ [n2 ], such that for any j ∈ [n1 ],
f2,i (θ 1,j , w1,j ) =

X
1
w2,ik ,
|S1,j | k∈S
1,j

We then take
ρ2 =

S1,j = {k ∈ [n1 ] : (θ 1,k , w 1,k ) = (θ 1,j , w1,j )} .
n2
1 X
δθ ,f .
n2 i=1 2,i 2,i

We continue this process inductively, i.e. for ℓ ≥ 3, we choose fℓ,i ∈ Fℓ , for each i ∈ [nℓ ], such that
for any j ∈ [nℓ−1 ],
fℓ,i (θ ℓ−1,j , fℓ−1,j ) =

1

X

|Sℓ−1,j | k∈S

wℓ,ik ,

ℓ−1,j

and we take

Sℓ−1,j = {k ∈ [nℓ−1 ] : (θ ℓ−1,k , fℓ−1,k ) = (θ ℓ−1,j , fℓ−1,j )} ,

nℓ
1 X
ρℓ =
δθ ,f .
nℓ i=1 ℓ,i ℓ,i

Then it is easy to check that
EP {L (y, ŷn (x; W))} = EP {L (y, ŷ (x; ρ))} .
This yields Eq. (54), since ǫ is arbitrary.

B.2

Derivation of Eq. (55)

For ǫ > 0 sufficiently small, we take ρ = {ρℓ }L
ℓ=1 that satisfies Assumptions (62) and (63). Let us
generate W = {W 1 , ..., W L , Θ1 , ..., ΘL } at random as follows. First we generate {(θ 1,i , w1,i )}i∈[n1 ] ∼
ρ1 i.i.d. and form W 1 = (w1,i )i∈[n1 ] and Θ1 = (θ 1,i )i∈[n1 ] . Then inductively for ℓ = 2, ..., L, we generate {(θ ℓ,i , fℓ,i )}i∈[nℓ ] ∼ ρℓ i.i.d., all independently of each other and of {(θ 1,i , w 1,i )}i∈[n1 ] , and form
W 2 = (f2,i (θ 1,j , w 1,j ))i∈[n2 ],j∈[n1 ] , W ℓ = (fℓ,i (θ ℓ−1,j , fℓ−1,j ))i∈[nℓ ],j∈[nℓ−1 ] and Θℓ = (θ ℓ,i )i∈[nℓ ] . We
shall argue that
EW {EP {L (y, ŷn (x; W))}} ≤ EP {L (y, ŷ (x; ρ))} + c (ǫ)

33

L
X

1
√ .
nℓ
ℓ=1

(64)

This implies
inf EP {L (y, ŷn (x; W))} ≤ inf EP {L (y, ŷ (x; ρ))} + c (ǫ)
ρ
W

L
X

1
√ + ǫ,
nℓ
ℓ=1

which immediately gives Eq. (55) by taking n → ∞ and then ǫ → 0. To that end, for each
(ℓ)
ℓ = 2, ..., L, let us define ŷn : Rnℓ 7→ R such that
ŷn(ℓ) (hℓ ) = ŷn (x; W) .

Here recall that
nL
1 X
ŷn (x; W) =
σL (θ L,i , hL,i ) ,
nL i=1

1
W ℓ σℓ−1 (Θℓ−1 , hℓ−1 ) ,
nℓ−1
h1 = W 1 x.
hℓ =

ℓ = 2, ..., L,

(ℓ)

Note that in the above definition, ŷn depends on Θℓ , ..., ΘL , W ℓ+1 , ..., W L and x, which are not
displayed to lighten the notation. In the following, the dependency on W or ρ is also not displayed.
The derivation of Eq. (64) contains several steps.
Step 1.

We argue that for each ℓ ≥ 2 and any u, v ∈ Rnℓ ,

c (ǫ)
ŷn(ℓ) (u) − ŷn(ℓ) (v) ≤ √ ku − vk2 .
nℓ

(65)

First, we have:
ŷn(L) (u) − ŷn(L) (v)

nL
nL
1 X
1 X
=
σL (θ L,i , ui ) −
σL (θ L,i , vi )
nL i=1
nL i=1
(a)

≤

nL

C X
1 + kθ L,i k2 |ui − vi |
nL i=1

C
nL + kΘL k2F ku − vk2
nL
(b) c (ǫ)
≤ √ ku − vk2 ,
nL

≤

q

where (a) is from Assumption (56), and (b) is from Assumption (63). We then show the thesis by
induction. Indeed, assuming the claim for ℓ + 1, we have:




1
1
(ℓ+1)
(ℓ)
(ℓ)
(ℓ+1)
ŷn (u) − ŷn (v) = ŷn
W ℓ+1 σℓ (Θℓ , u) − ŷn
W ℓ+1 σℓ (Θℓ , v)
nℓ
nℓ
(a)
c (ǫ)
≤ √
kW ℓ+1 σℓ (Θℓ , u) − W ℓ+1 σℓ (Θℓ , v)k2
nℓ nℓ+1
(b)

≤

c (ǫ)
kW ℓ+1 kF ku − vk2
√
nℓ nℓ+1

(c)

c (ǫ)
≤ √ ku − vk2 ,
nℓ
34

where (a) is by the induction hypothesis, (b) is by Assumption (58), and (c) is from Assumption
(63). This shows the thesis.
We argue that

Step 2.

E {|ŷn (x) − ŷ (x)|} ≤ c (ǫ)
Notice that




ŷn(L) (HL (fL,i ; x))i∈[nL ] =
ŷn(ℓ)
ŷn(2)





(Hℓ (fℓ,i ; x))i∈[nℓ ] =



L
X

1
√ .
nℓ
ℓ=1

nL
1 X
σL (θL,i , HL (fL,i ; x)) ,
nL i=1

ŷn(ℓ+1)



1
W 2 σ1 (Θ1 , h1 ) = ŷn (x) .
n1


(66)



1
W ℓ+1 σℓ Θℓ , (Hℓ (fℓ,i ; x))i∈[nℓ ] ,
nℓ


ℓ = 2, ..., L − 1,

We thus have the following decomposition:
E {|ŷn (x) − ŷ (x)|}
≤E

(

+

nL
1 X
σL (θ L,i , HL (fL,i ; x))
ŷ (x) −
nL i=1
L−1
X

E

ℓ=2

+E
≡ AL +





ŷn(2)

L−1
X

ŷn(ℓ+1)




)

(Hℓ+1 (fℓ+1,i ; x))i∈[nℓ+1 ]


(H2 (f2,i ; x))i∈[n2 ] −

ŷn(2)





− ŷn(ℓ+1)





1
W ℓ+1 σℓ Θℓ , (Hℓ (fℓ,i ; x))i∈[nℓ ]
nℓ

1
W 2 σ1 (Θ1 , h1 )
n1



Aℓ + A1 .

ℓ=2

From Eq. (65), we have for ℓ = 2, ..., L − 1:
A2ℓ

≤E

(

ŷn(ℓ+1)
nℓ+1

≤



c (ǫ) X

nℓ+1

i=1





(Hℓ+1 (fℓ+1,i ; x))i∈[nℓ+1 ] −

ŷn(ℓ+1)





1
W ℓ+1 σℓ Θℓ , (Hℓ (fℓ,i ; x))i∈[nℓ ]
nℓ

 2)


2 


nℓ


X
1


E
fℓ+1,i (θ ℓ,j , fℓ,j ) σℓ (θ ℓ,j , Hℓ (fℓ,j ; x))
Hℓ+1 (fℓ+1,i ; x) −


nℓ j=1



nℓ+1
c (ǫ) X
E {Varℓ {fℓ+1,i (θ, f ) σℓ (θ, Hℓ (f ; x))}}
=
nℓ nℓ+1 i=1

(a)

nℓ+1 n
n
oo
c (ǫ) X
2
≤
E Eℓ fℓ+1,i
(θ, f ) σℓ2 (θ, Hℓ (f ; x))
nℓ nℓ+1 i=1
(b)

≤

c (ǫ)
nℓ

where Varℓ and Eℓ indicate the variance and the mean w.r.t. (θ, f ) ∼ ρℓ . Here the factor 1/nℓ in
step (a) is due to {(θ ℓ,i , fℓ,i )}i∈[nℓ ] ∼ ρℓ i.i.d., and step (b) is due to Assumptions (63) and (59).
Similarly we have A21 ≤ c (ǫ) /n1 and A2L ≤ c (ǫ) /nL . The thesis then follows.
35

Step 3.

From Assumptions (60) and (61), we have:

|EW {EP {L (y, ŷn (x))}} − EP {L (y, ŷ (x))}| ≤ CE {(1 + |ŷn (x)| + |ŷ (x)|) |ŷn (x) − ŷ (x)|} .
Notice that from Assumptions (57) and (63),
nL
nL

1 X
C X
|ŷn (x)| ≤
|σL (θ L,i , hL,i )| ≤
1 + kθ L,i k2 ≤ c (ǫ) .
nL i=1
nL i=1

Similarly, |ŷ (x)| ≤ c (ǫ). Combining with Eq. (66), we arrive at Eq. (64).

C

Mean field limit in multilayer convolutional neural networks

We consider convolutional neural networks (CNNs), which are most interesting when they have
many layers. In a CNN, the number of neurons nℓ at layer ℓ is the number of filters at that layer.
A simple convolutional analog of the three-layers fully-connected network (7) can be described by
the following:
ŷn (x; W) =

1
hβ, σ2 (h2 )i ,
n2

h2 =

1
W 2 ⊛ σ1 (h1 ) ,
n1

h1 = W 1 ⊛ x.

Here ⊛ is the operator defined by


W ⊛u=

m1
X

j=1



wij ∗ uj 

,
i∈[m2 ]

u ∈ (Rp )m1 ,

W = (wij )i∈[m2 ],j∈[m1 ] ∈ (Rs )m2 ×m1 ,

for a convolutional operator ∗, in which w ∗ u ∈ Rr(s,p) for any u ∈ Rp and w ∈ Rs , and r (s, p) is an
integer to be determined by the exact operation of ∗. It can be made r (s, p) = p with appropriate
paddings and no striding. In our context,
• x ∈ (Rp )d a p-pixels d-channels (1-dimensional) input image (e.g. d = 3 for an RGB image),
• W 1 ∈ (Rs1 )n1 ×d where each entry is an element in Rs1 and the receptive field size is s1 ,


 n1

• h1 ∈ Rr(s1 ,p) , σ1 : Rr(s1 ,p) 7→ Rp1 some nonlinear mapping for p1 = p1 (r (s1 , p)), and
σ1 (h1 ) ∈ (Rp1 )n1 element-wise,
• W 2 ∈ (Rs2 )n2 ×n1 where each entry is an element in Rs2 and the receptive field size is s2 ,


n2

• h2 ∈ Rr(s2 ,p1 ) , σ2 : Rr(s2 ,p1 ) 7→ Rp2 some nonlinear mapping for p2 = p2 (r (s2 , p1 )), and
σ2 (h2 ) ∈ (Rp2 )n2 element-wise,
• β ∈ Rn2 p2 and h·, ·i computes the usual Euclidean inner product after vectorizing its arguments.
The nonlinear mapping can be, as in the usual practice, a composition of a pooling operation and a
scalar nonlinear activation. Now observe the similarity between this network and its fully-connected
counterpart, especially the summation structure shared by both, as evident from the definition of
36

Layer

Structure

1
2
3
4
5
6
7
8

CONV-(3, 3, n), stride (2, 2) — ReLU
CONV-(3, 3, n), stride (2, 2) — ReLU
CONV-(3, 3, n), stride (1, 1) — ReLU
CONV-(3, 3, n), stride (1, 1) — ReLU
CONV-(3, 3, n), stride (1, 1) — ReLU — POOL-(3, 3), stride (2, 2)
CONV-(3, 3, n), stride (1, 1) — ReLU
CONV-(3, 3, n), stride (1, 1) — ReLU — POOL-(3, 3), stride (2, 2)
FC-(10, 4n)

Output
spatial dimension
16 × 16
8×8
8×8
8×8
4×4
4×4
2×2
–

Table 1: Structure of the CNNs. Here CONV-(s, s, n) is a convolutional layer with a receptive field
size of s and n filters. POOL-(s, s) is the max pooling operation over a spatial region of size s × s.
Stride (s, s) is the stride of size s in each dimension, applied to the accompanied operation. FC(10, 4n) is a fully-connected layer with dimensions 10 × 4n. ReLU indicates an entry-wise rectifier
linear unit nonlinearity. All layers have trainable biases. We apply appropriate paddings to obtain
the corresponding output spatial dimension.
the ⊛ operator. Recall that this summation structure is key to the self-averaging property in light
of Eq. (25). The only difference is that local operations (such as the ∗ operator) are no longer
scalar-valued, but rather vector-valued (or matrix-valued). As discussed in Section 2.4, we expect
that this difference is not very critical and the MF limit behavior still occurs, provided that the
number of filters nℓ → ∞ while all other dimensions are kept constant. The scalings can be deduced
from Section 3.1.
In the following, we present an experimental validation of the existence of the MF limit in
multilayer CNNs on the CIFAR-10 classification task. We construct 8-layers networks according to
Table 1. Note that we apply a stride of size (2, 2) in the first two layers, which reduces the spatial
dimensions by a considerable amount and hence limits memory consumption at the cost of the
networks’ performance. We use the same number of filters at each layer n, where n is to be varied
among the networks. We normalize each RGB value in the image to the range [−1, +1]. We use the
cross-entropy loss L, and use the whole training set of size 50 × 103 . To train the networks, we use
mini-batch SGD with an annealed learning rate αk = 0.08k−0.1 , where k ≥ 1 is the SGD iteration,
and a batch size of 100. We initialize the networks in a similar fashion to those in Section 4.2, i.e.
the first layer weight entries are initialized with N (0, 2/ (9d)), the other layers’ weight entries are
initialized with N (1, 0.1), and all biases are initialized to zero.
The result is shown in Fig. 12. We observe the good match among the networks – the larger n,
the better match. The performance is also realistic: the test error rate for the network with n = 400
is about 27%. This is similar to the performance of a 259-layers CNN reported in [XBSD+ 18], which
attains a test error rate of 30%. It has a similar vanilla structure, is initialized with i.i.d. Gaussian
weights of zero mean and carefully selected variance, is trained without regularization, but is not
under our scalings. Hence the introduced scalings do not trivialize the performance of the networks.

37

Training loss
2
100
200
400
800
101

2.0
1.5
1.0
103

Training error
0.8

2.5

1
0

Test loss
0.6

100
200
400
800
101

0.4
0.2
103

0.0

Test error
0.8

100
200
400
800
101

0.6
0.4
103

100
200
400
800
101

103

Figure 12: The performance of 8-layers CNNs on CIFAR-10 classification, plotted against training
iteration. For each network, n = 100, 200, 400, 800 respectively.

References
[ABGM14]

Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma, Provable bounds for
learning some deep representations, International Conference on Machine Learning,
2014, pp. 584–592.

[AZLL18]

Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang, Learning and generalization
in overparameterized neural networks, going beyond two layers, arXiv preprint
arXiv:1811.04918 (2018).

[AZLS18]

Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song, A convergence theory for deep learning via over-parameterization, arXiv preprint arXiv:1811.03962 (2018).

[BRV+ 06]

Yoshua Bengio, Nicolas L Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte, Convex neural networks, Advances in neural information processing systems,
2006, pp. 123–130.

[CB18a]

Lenaic Chizat and Francis Bach, A note on lazy training in supervised differentiable
programming, arXiv preprint arXiv:1812.07956 (2018).

[CB18b]

Lénaïc Chizat and Francis Bach, On the global convergence of gradient descent for
over-parameterized models using optimal transport, Advances in Neural Information
Processing Systems, 2018, pp. 3040–3050.

[CHM+ 15]

Anna Choromanska, Mikael Henaff, Michael Mathieu, Gérard Ben Arous, and Yann
LeCun, The loss surfaces of multilayer networks, Artificial Intelligence and Statistics,
2015, pp. 192–204.

[Coo18]

Yaim Cooper, The loss landscape of overparameterized neural networks, arXiv
preprint arXiv:1804.10200 (2018).

[CPS18]

Minmin Chen, Jeffrey Pennington, and Samuel Schoenholz, Dynamical isometry and
a mean field theory of RNNs: Gating enables signal propagation in recurrent neural
networks, Proceedings of the 35th International Conference on Machine Learning,
vol. 80, 2018, pp. 873–882.

38

[dGMHR+ 18] Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and
Zoubin Ghahramani, Gaussian process behaviour in wide deep neural networks, International Conference on Learning Representations, 2018.
[DL18]

Simon Du and Jason Lee, On the power of over-parametrization in neural networks
with quadratic activation, Proceedings of the 35th International Conference on Machine Learning, vol. 80, 2018, pp. 1329–1338.

[DLL+ 18]

Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai, Gradient
descent finds global minima of deep neural networks, arXiv preprint arXiv:1811.03804
(2018).

[DZPS19]

Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh, Gradient descent
provably optimizes over-parameterized neural networks, International Conference on
Learning Representations, 2019.

[FB17]

C Daniel Freeman and Joan Bruna, Topology and geometry of half-rectified network
optimization, International Conference on Learning Representations (2017).

[GARA19]

Adrià Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison, Deep
convolutional networks as shallow gaussian processes, International Conference on
Learning Representations, 2019.

[GJS+ 19]

Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun,
Stéphane d’Ascoli, Giulio Biroli, Clément Hongler, and Matthieu Wyart, Scaling
description of generalization with number of parameters in deep learning, arXiv
preprint arXiv:1901.01608 (2019).

[GSd+ 18]

Mario Geiger, Stefano Spigler, Stéphane d’Ascoli, Levent Sagun, Marco Baity-Jesi,
Giulio Biroli, and Matthieu Wyart, The jamming transition as a paradigm to understand the loss landscape of deep neural networks, arXiv preprint arXiv:1809.09349
(2018).

[Han18]

Boris Hanin, Which neural net architectures give rise to exploding and vanishing
gradients?, Advances in Neural Information Processing Systems 31, 2018, pp. 580–
589.

[HJ15]

Tamir Hazan and Tommi Jaakkola, Steps toward deep kernel methods from infinite
neural networks, arXiv preprint arXiv:1508.05133 (2015).

[HNP+ 18]

Nhat Ho, Tan Nguyen, Ankit Patel, Anima Anandkumar, Michael I Jordan, and
Richard G Baraniuk, Neural rendering model: Joint generation and prediction for
semi-supervised learning, arXiv preprint arXiv:1811.02657 (2018).

[HR18]

Boris Hanin and David Rolnick, How to start training: The effect of initialization and
architecture, Advances in Neural Information Processing Systems 31, 2018, pp. 569–
579.

[JGH18]

Arthur Jacot, Franck Gabriel, and Clement Hongler, Neural tangent kernel: Convergence and generalization in neural networks, Advances in Neural Information
Processing Systems 31, 2018, pp. 8580–8589.
39

[JMM19]

Adel Javanmard, Marco Mondelli, and Andrea Montanari, Analysis of a two-layer
neural network via displacement convexity, arXiv preprint arXiv:1901.01375 (2019).

[LBH15]

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, Deep learning, nature 521
(2015), no. 7553, 436.

[LL18]

Yuanzhi Li and Yingyu Liang, Learning overparameterized neural networks via
stochastic gradient descent on structured data, Advances in Neural Information Processing Systems, 2018, pp. 8168–8177.

[LN19]

Ping Li and Phan-Minh Nguyen, On random deep weight-tied autoencoders: Exact
asymptotic analysis, phase transitions, and implications to training, International
Conference on Learning Representations, 2019.

[LSdP+ 18]

Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and Yasaman Bahri, Deep neural networks as gaussian processes, International
Conference on Learning Representations, 2018.

[Mal16]

Stéphane Mallat, Understanding deep convolutional networks, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences
374 (2016), no. 2065, 20150203.

[MBM18]

Song Mei, Yu Bai, and Andrea Montanari, The landscape of empirical risk for nonconvex losses, The Annals of Statistics 46 (2018), no. 6A, 2747–2774.

[MMN18]

Song Mei, Andrea Montanari, and Phan-Minh Nguyen, A mean field view of the
landscape of two-layers neural networks, Proceedings of the National Academy of
Sciences, vol. 115, 2018, pp. 7665–7671.

[MP16]

Hrushikesh N Mhaskar and Tomaso Poggio, Deep vs. shallow networks: An approximation theory perspective, Analysis and Applications 14 (2016), no. 06, 829–848.

[NH17]

Quynh Nguyen and Matthias Hein, The loss surface of deep and wide neural networks, Proceedings of the 34th International Conference on Machine Learning,
vol. 70, 2017, pp. 2603–2612.

[NH18]

Quynh Nguyen and Matthias Hein, Optimization landscape and expressivity of deep
cnns, International Conference on Machine Learning, 2018, pp. 3727–3736.

[NXB+ 19]

Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel A.
Abolafia, Jeffrey Pennington, and Jascha Sohl-dickstein, Bayesian deep convolutional
networks with many channels are gaussian processes, International Conference on
Learning Representations, 2019.

[PLR+ 16]

Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli, Exponential expressivity in deep neural networks through transient chaos, Advances in neural information processing systems, 2016, pp. 3360–3368.

[PSG17]

Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli, Resurrecting the sigmoid
in deep learning through dynamical isometry: theory and practice, Advances in neural
information processing systems, 2017, pp. 4785–4795.
40

[RDS+ 15]

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander
Berg, and Fei-Fei Li, Imagenet large scale visual recognition challenge, International
Journal of Computer Vision 115 (2015), no. 3, 211–252.

[RVE18]

Grant M Rotskoff and Eric Vanden-Eijnden, Neural networks as interacting particle
systems: Asymptotic convexity of the loss landscape and universal scaling of the
approximation error, arXiv preprint arXiv:1805.00915 (2018).

[SC16]

Daniel Soudry and Yair Carmon, No bad local minima: Data independent training
error guarantees for multilayer neural networks, arXiv preprint arXiv:1605.08361
(2016).

[SGd+ 18]

Stefano Spigler, Mario Geiger, Stéphane d’Ascoli, Levent Sagun, Giulio Biroli, and
Matthieu Wyart, A jamming transition from under-to over-parametrization affects
loss landscape and generalization, arXiv preprint arXiv:1810.09665 (2018).

[SGGSD17]

Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein,
Deep information propagation, International Conference on Learning Representations (2017).

[SJL18]

Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee, Theoretical insights into
the optimization landscape of over-parameterized shallow neural networks, IEEE
Transactions on Information Theory (2018).

[SS16]

Itay Safran and Ohad Shamir, On the quality of the initial basin in overspecified
neural networks, International Conference on Machine Learning, 2016, pp. 774–782.

[SS18a]

Justin Sirignano and Konstantinos Spiliopoulos, Mean field analysis of neural networks, arXiv preprint arXiv:1805.01053 (2018).

[SS18b]

, Mean field analysis of neural networks: A central limit theorem, arXiv
preprint arXiv:1808.09372 (2018).

[SZ15]

Karen Simonyan and Andrew Zisserman, Very deep convolutional networks for
large-scale image recognition, International Conference on Learning Representations
(2015).

[SZT17]

Ravid Shwartz-Ziv and Naftali Tishby, Opening the black box of deep neural networks
via information, arXiv preprint arXiv:1703.00810 (2017).

[VBB18]

Luca Venturi, Afonso Bandeira, and Joan Bruna, Spurious valleys in two-layer neural network optimization landscapes, arXiv preprint arXiv:1802.06384 (2018).

[WLLM18]

Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma, On the margin theory of
feedforward neural networks, arXiv preprint arXiv:1810.05369 (2018).

[XBSD+ 18]

Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington, Dynamical isometry and a mean field theory of CNNs: How to
train 10,000-layer vanilla convolutional neural networks, Proceedings of the 35th
International Conference on Machine Learning, vol. 80, 2018, pp. 5393–5402.
41

[YPR+ 19]

Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S.
Schoenholz, A mean field theory of batch normalization, International Conference on
Learning Representations, 2019.

[YS17]

Ge Yang and Samuel Schoenholz, Mean field residual networks: On the edge of chaos,
Advances in neural information processing systems, 2017, pp. 7103–7114.

[YSJ19]

Chulhee Yun, Suvrit Sra, and Ali Jadbabaie, Small nonlinearities in activation functions create bad local minima in neural networks, International Conference on Learning Representations, 2019.

[ZCZG18]

Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu, Stochastic gradient descent
optimizes over-parameterized deep relu networks, arXiv preprint arXiv:1811.08888
(2018).

[ZSJ+ 17]

Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, and Inderjit S. Dhillon,
Recovery guarantees for one-hidden-layer neural networks, Proceedings of the 34th
International Conference on Machine Learning, vol. 70, 2017, pp. 4140–4149.

42

