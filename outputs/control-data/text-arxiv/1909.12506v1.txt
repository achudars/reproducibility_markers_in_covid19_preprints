Distributionally Robust Tuning of Anomaly Detectors
in Cyber-Physical Systems with Stealthy Attacks**

arXiv:1909.12506v1 [eess.SY] 27 Sep 2019

Venkatraman Renganathan*, Navid Hashemi*, Justin Ruths, and Tyler H. Summers
Abstract— Designing resilient control strategies for mitigating
stealthy attacks is a crucial task in emerging cyber-physical
systems. In the design of anomaly detectors, it is common
to assume Gaussian noise models to maintain tractability;
however, this assumption can lead the actual false alarm rate
to be significantly higher than expected. We propose a distributionally robust anomaly detector for noise distributions in
moment-based ambiguity sets. We design a detection threshold
that guarantees that the actual false alarm rate is upper
bounded by the desired one by using generalized Chebyshev
inequalities. Furthermore, we highlight an important tradeoff between the worst-case false alarm rate and the potential
impact of a stealthy attacker by efficiently computing an outer
ellipsoidal bound for the attack-reachable states corresponding
to the distributionally robust detector threshold. We illustrate
this trade-off with a numerical example and compare the
proposed approach with a traditional chi-squared detector.

I. INTRODUCTION
Many emerging complex dynamical networks, from critical infrastructures to industrial cyber-physical systems (CPS)
and various biological networks, are increasingly able to be
instrumented with new sensing and actuation capabilities.
These networks comprise growing webs of interconnected
feedback loops and must operate efficiently and resiliently
in dynamic and uncertain environments. As these systems
become large, devising automated methods for detecting
anomalies (such as component failures or malicious attacks)
are critical for smooth and efficient operation. Such critically
important cyber-physical networks have become an attractive
target to attackers. These systems are large and complex
enough – and often not monitored well enough – for attackers
to manipulate the system without being detected and cause
significant damage [1]–[10].
To simplify analysis and design, often such complex
cyber-networks are modeled as a discrete-time linear time
invariant system. In such systems, noises are often modelled
as Gaussian in the name of tractability. However, this can
lead to a significant miscalculation of probabilities and risk
if the underlying processes behave differently, for example
due to various nonlinearities or malicious attacks. In the
context of attacks, it is possible for an attacker to modify the sensor outputs and effectively generate aggressive
and strategic noise profiles to sabotage the operation of
* These authors contributed equally.
**This work was partially supported by the Army Research Office and
was accomplished under Grant Number: W911NF-17-1-0058 and the Air
Force Office of Scientific Research under award number FA2386-19-1-4073.
The authors are with the Department of Mechanical Engineering, The
University of Texas at Dallas, 800 W. Campbell Rd, Richardson, TX,
USA. Email: (vrengana, nxh150030, tyler.summers,

jruths)@utdallas.edu

the system. In stochastic optimization, these limitations are
being recognized and addressed in the emerging area of
distributionally robust optimization (DRO) [11].
Here, we propose to use distributionally robust optimization (DRO) methods to improve modeling and reduce false
alarm rates in cyber-physical networks. DRO enables modelers to explicitly incorporate inherent ambiguity in probability
distributions into optimization problems. This more realistic
account of uncertainty mitigates the so-called optimizers
curse, where overly strong assumptions about uncertainty
distributions can lead to poor out-of-sample performance.
Moreover, several important DRO problems can be tractably
solved. The central idea is to represent uncertainty through an
ambiguity set as a family of possibly infinitely many probability distributions consistent with the available raw data
or prior structural information, and to model the decisionmaking process as a game against “nature”. In this game, the
modeler first selects a decision with the goal to optimize his
objective or maximize the probability of constraint satisfaction, in response to which nature selects a distribution from
within the ambiguity set with the goal to inflict maximum
harm to the modeler. This game mimics the adversarial
nature of attacks and makes DRO an obvious choice to
analyze worst case behavior not just from noise/nature, but
from corruption by an attacker.
A model-based approach to attack detection uses a detector
that raises alarms when there is a large enough discrepancy
between the actual measurements and an estimate of the system, a statistic termed the residual. The detector’s sensitivity
can be increased by decreasing the threshold of detection, but
there is an inherent tradeoff between sensitivity and the rate
at which false alarms are generated. Keeping false alarms
to a manageable level requires decreasing sensitivity and the
tuning of the detector threshold is typically informed by the
distribution of the residual. When this distribution is only
known to an ambiguity set, traditional tools no longer suffice
to select the threshold and so we turn to a distributionally
robust approach. In the context of attacks, the tuning of the
detector has a direct implication on the effect an attacker can
have while still remaining stealthy.
Contributions: Our main contributions are: 1) design of
a detection threshold that guarantees that the actual false
alarm rate is upper bounded by the desired one by using
generalized Chebyshev inequalities (Proposition 1); 2) formulation of ellipsoidal outer bounds on the reachable sets
of the system corresponding to attacks despite the exact
distribution of the noise being unknown using distributional
robustness (Proposition 2); 3) demonstrating via a numerical

example an important trade-off between the worst-case false
alarm rate and the potential impact of a stealthy attacker.
Specifically, we use generalized Chebyshev inequalities [12],
[13], to find the detector threshold values so that the anomaly
detector we design achieves a desired worst-case false alarm
rate. Finally, using the optimum detector threshold values,
we derive the outer bounding ellipsoid that contains the
reachable set induced by a stealthy attack by solving a convex
optimization problem. While anomaly and attack detection
is widely stuied in CPS literature, our distributionally robust
approach marks the novel contribution of this paper.
The rest of the paper is organized as follows. Section II
formulates the problem statement and explains the distributionally robust approach and further using the generalized
Chebyshev bounds to design anomaly detectors. Section III
describes the convex optimization problem formulation to
find an ellipsoidal bound on the reachable sets obtained using
distributionally robust tuned detector. Section IV discusses
about the numerical results using an empirical system and
highlights the trade-offs observed between the attacker’s
capability and being distributionally robust against any noise
distribution. Finally, Section V concludes and summarizes
future research directions.

To leverage a fault-detection approach, we require an estimator of some type to produce a prediction of the system
behavior. In this work, we use the steady state Kalman filter

II. P ROBLEM F ORMULATION AND D ISTRIBUTIONALLY
ROBUST A NOMALY D ETECTOR T UNING

Σr = E[rt rt> ] = CP C > + Σv .

We model an uncertain cyber-physical system using a
stochastic discrete-time linear time invariant (LTI) system
xt+1 = Axt + But + wt ,

t∈N

yt = Cxt + vt ,

(1)

where xt ∈ Rn is the system state at time t, ut ∈ Rm
is the input at time t, A is the dynamics matrix, B is the
input matrix. The process noise wt is modeled using a zeromean random vector independent and identically distributed
across time with covariance matrix Σw . The output yt ∈ Rp
aggregates a linear combination, given by the observation
matrix C ∈ Rp×n , of the states and the sensor noise, vk
modeled using a zero-mean random vector independent and
identically distributed across time with covariance matrix Σv .
The distributions Pw of wt and Pv of vt are unknown (and
not necessarily Gaussian) and will be assumed to belong
to the ambiguity sets P w , P v of distributions respectively.
With the second moments of the process noise and sensor
noise denoted by Σw = E[wt wt> ] and Σv = E[vt vt> ] being
known, we can then define the moment based ambiguity sets
as follows,
P v = {Pv | Evt = 0, E[vt vt> ] = Σv },
P

w

= {Pw | Ewt = 0,

E[wt wt> ]

= Σw }.

(2)
(3)

We assume that the pair (A, C) is detectable and (A, B)
is stabilizable. In this work, we consider the scenario that
the actual measurement yt can be corrupted by an additive
attack, δt ∈ Rp . Due to this additive attack, the output of the
system fed to the controller becomes
ȳt = yt + δt = Cxt + vt + δt .

(4)

x̂t+1 = Ax̂t + But + L(ȳt − C x̂t ),

(5)

where x̂t ∈ Rn is the estimated state. The observer gain L
is designed to minimize the steady state covariance matrix
P in the absence of attacks, where the estimation error is
et = xt − x̂t ,

and

P := lim Pt := E[et e>
t ].

(6)

t→∞

Recall that P is the solution of an algebraic Ricatti equation.
Since (A, C) is assumed to be detectable, the existence of
such a steady state covariance matrix P is guaranteed. Now,
we define a residual sequence, rt as the difference between
what we actually receive ȳt and expect to receive C x̂t as,
rt = ȳt − C x̂t = Cet + vt + δt ,

(7)

and the estimation error evolves according to
et+1 = (A − LC)et − Lvt − Lδt .

(8)

When there is no attack, that is, δt = 0, the residual sequence
rt falls according to a zero mean distribution with covariance
(9)

Note that rt is a zero-mean random vector independent and
identically distributed across time with covariance matrix Σr
and the distribution Pr of rt is unknown (and not necessarily
Gaussian) but belongs to an ambiguity set P r whose second
moment, Σr , can be calculated from (9).
P r = {Pr | Ert = 0, E[rt rt> ] = Σr }.

(10)

A. Distributionally Robust Optimization Approach
Distributionally robust optimization approaches can be
categorized based on the form of the ambiguity set. There are
several different parameterizations, including those based on
moments, support, directional derivatives [14], and Wasserstein balls [15]. For example, a moment-based ambiguity set
includes all distributions with a fixed moments up to some
order (e.g., fixed first and second moments), and Wassersteinbased ambiguity sets include a ball of distributions within a
given Wasserstein distance from some base distribution (such
as an empirical distribution on a training data set). We will
focus here on the moment-based ambiguity set as explained
in [11], though other parameterizations are interesting and
relevant for bounding reachable sets using ellipsoids will
be pursued in future work. Similar in structure to a chisquared detector though tuned using a distributionally robust
approach, we define a quadratic distance measure zt to be
sensitive to changes in the variance of the distribution as well
as the expected value,
zt = rt> Σ−1
r rt .

(11)

Notice that zt is also a random variable expressed as the sum
of the squares of rt . In the case that the residual is Gaussian,
zt would be a chi-squared random variable (hence the name
of the chi-squared detector).

B. Anomaly Detector Thresholds and False Alarm Rates
For a given threshold α ∈ R>0 and the distance measure
zt = rt> Σ−1
r rt ,
(
zt ≤ α, no alarm
(12)
zt > α, alarm: t∗ = t.
∗

alarm time(s) t are produced. Due to the infinite support of
the sensor noise vt , the distance measure zt will also have
infinite support. Thus even in the absence of attacks, the
detector is expected to generate false alarms because some
values drawn from the distance measure distribution will
exceed the threshold α. Usually the detectors are designed
for a desired false alarm rate, A, through an appropriate
choice of threshold α. This, however, requires knowing the
distribution governing the detector random variable, zt . If the
distribution of the quadratic distance measure zt is known,
then it is possible to extract the optimum threshold values α
from the knowledge of the distribution. Suppose for instance,
if the traditional chi-squared detector is used as in [8], with
threshold α ∈ R>0 , rt ∼ N (0, Σr ). Then corresponding
to the desired false alarm rate A = A∗ , we can obtain the
optimum threshold as

p
,
(13)
α = α∗ := 2P−1 1 − A∗ ,
2
where P−1 (·, ·) denotes the inverse regularized lower incomplete gamma function.
C. Tuning the Threshold via Generalized Chebyshev Bounds
When the complete distribution is not available, tuning
methods like the one above may design thresholds that
generate actual false alarm rates significantly higher than
what is desired. With the distributionally robust approach,
we aim to achieve a desired false alarm rate A, and the
challenge is to select α such that
sup Pr (rt> Σ−1
r rt ≤ α) = 1 − A.

Pr ∈P r

(14)

Proposition 1: Given a desired false alarm rate A and
the true distribution of the system residual rt belonging to
an ambiguity set P r defined as in (10), the distributionally
robust detector threshold α satisfying (14) is given by
p
α= ,
(15)
A
where p denotes the number of outputs.
Proof: A sharp lower bound on the probability of a set
defined by quadratic inequalities, given the first two moments
of the distribution can be efficiently computed using a
semidefinite program described in [12], which generalizes
Chebyshev’s inequality to vector random variables. However,
for the particular form of the residual set in (14) there is an
analytical solution [13]. Using this result, we can obtain the
Chebyshev bound α satisfying (14) as follows,
p
,
sup Pr (rt> Σ−1
(16)
r rt ≥ α) ≤
r
α
Pr ∈P

where p is the number of outputs. Comparing (16) with the
(14), we can see that
p
p
= A =⇒ α = .
α
A
III. ATTACK -R EACHABLE S ET B OUNDS
The threshold of the anomaly detectors limit what the
attacker is able to accomplish if he/she seeks to remain
undetected. These attack models require strong attacker
knowledge and access, namely we assume that the attacker
has perfect knowledge of the system dynamics, the Kalman
filter, control inputs, and measurements. In addition, the
attacker has read and write access to all the sensors at each
time step. In this section, we describe a stealthy attack by
an attacker and define reachable set to quantify the system
impact due to the attack and process noise.
A. Zero Alarm Attacks
Zero-alarm attacks generate attack sequences that maintain
the distance measure at or below the threshold zt ≤ α, so
that no alarms are raised during the attack. To satisfy this
condition we define the attack as
1

δt = −Cet − vt + Σr2 δ̄t

(17)

1
2

where Σr is the symmetric square root of Σr and δ̄t ∈ Rp is
any vector that the attacker has the freedom to design such
that δ̄t> δ̄t ≤ α. With such an attack sequence, the distance
measure becomes
>
zt = rt> Σ−1
r rt = δ̄t δ̄t ≤ α.

(18)

B. Computing Ellipsoidal Bound for the Reachable Set
In order to compare the effects of an attack, we require
a metric to quantify the impact of it. A popular choice to
quantify system impact due to a disturbance is the set of
states reachable by the action of the disturbance. We intend
to find a best bound for α that will result in the required
user prescribed false alarm rate A, keeping in mind the
noise defining zt may fall under any distribution from its
corresponding moment-based ambiguity set. The generalized Chebyshev bound α addressing the above problem is
obtained by (15). When the distribution of the residual is
not known exactly, the lack of information inherently leads
to more conservatism, making the value of α larger, and
therefore, the magnitude of zero-alarm attacks larger.
When there is an attack as in (17), the evolution of the
system dynamics can be written in a new and reduced form
where the measurement noise vt gets cancelled and thus
resulting in the state and estimation error dynamics of the
system as
of wt and δ̄t . Defining
 a function
>
> the joint state
as ξt = xt et with input ζt = wt δ̄t , we can write
the joint evolution as
ξt+1 = Âξt + B̂ζt ,
(19)




I
0
A + BK −BK
where Â =
and B̂ =
1/2 .
0
A
I −LΣr
Since some distributions in the ambiguity set of P w may

have unbounded support, it is necessary for us to truncate
them at some confidence level (since unbounded noise
would, theoretically, lead to unbounded reachable sets, albiet
for infinitesimal probabilities). We follow the distributionally
robust approach using (15) and (16) to obtain noise threshold
w̄ satisfying,
sup Pw (wt> Σ−1
w wt ≤ w̄) = 1 − A.

(20)

Pw ∈P w

The reachable set of interest, driven by the ellipsoidally
bounded inputs wt and δ̄t , is


ξt+1 = Âξt + B̂ζt ,




>
Rx = xt = [In , 0n×n ]ξt ξ1 = 0, δt δt ≤ α,
.




> −1
wt Σw wt ≤ w̄, ∀t ∈ N
(21)
We use Linear Matrix Inequalities, for some positive definite
matrix Qx , to derive outer ellipsoidal bounds of the form
Rx ⊆ E(Qx ) = {xt | x>
t Pxt ≤ 1},

(22)

where the ellipsoid E has minimum volume and shape matrix
Qx . We define Pξ as the inverse of the shape matrix of the
ellipsoidal bound for the ξ reachable set,


Qx Qxe
−1
Pξ = Qξ =
.
(23)
Q>
Qe
xe
The following proposition will introduce the optimization
problem to find the minimum volume ellipsoidal bound for
the reachable set.
Proposition 2: Given the system matrices A, B, C, gain
matrices K, L, a positive semi-definite matrix F, attack
input threshold α, system noise threshold w̄, and constant
a ∈ [0, 1) the following convex optimization generates the
smallest reachable set ellipsoidal bound E(Qx ),
minimize

tr(Qx )

subject to

a1 + a2 ≥ a,

a1 ,a2 ,Qx ,Qxe ,Qe

a1 , a2 ∈ [0, 1)

(24)

Qξ ≥ 0, F ≥ 0.
Proof: In order to prove the proposition, we leverage
the results from [16] and Lemma 1 in [7] (restated below).
Lemma 3: Let Vt be a positive definite function, V1 =
>
0, ζit
Wi ζit ≤ 1, i = 1 . . . N , where N is the number of
available inputs and Wi is the inverse of shape matrix for
the ellipsoidal bound over input ζit and is positive definite.
−a
, if there exists a
Then, it can be shown that Vt ≤ N1−a
constant
a
∈
(0,
1)
and
a
∈
(0,
1),
∀i
=
1,
. . . , N such that
i
PN
a
≥
a
and
i
i=1
Vt+1 − aVt −

N
X
>
(1 − ai )ζit
Wi ζit ≤ 0.

(25)

i=1

The proof of the lemma is available in [16]. To derive the
reachable set bound of ξt , let us define the positive definite
function required in (25) as follows,
Vt = ξt> P̃ξ ξt ≤

2−a
1−a
, P̃ξ > 0, Pξ =
P̃ξ .
1−a
2−a

(26)

Substituting (26) in (25), we get
1 − a2 >
1 − a1 > −1
δ̄t δ̄t −
wt Σw wt ≤ 0, (27)
α
w̄
and further solving it using Schur complement results in the
following linear matrix inequality


aPξ Â> Pξ
0
H = Pξ Â
(28)
Pξ
Pξ B̂  ≥ 0,
0
B̂ > Pξ
Ŵ
 1−a1 −1

Σw
0
1−a
w̄
. To
where, Ŵ = 2−a Wa and Wa =
(1−a2 )
0
Ip
α
replace Pξ with Qξ , we use diagonal transformation matrix
Θ such that, F = Θ> HΘ where, Θ = diag(Qξ , Qξ , I). Now,
(28) equivalently gets transformed into


aQξ Qξ A> 0
Qξ
B  ≥ 0.
F = AQξ
(29)
>
0
B
Ŵ
Vt+1 − aVt −

Thus F ≥ 0 with Qξ ≥ 0 will ensure that the convex
optimization problem given by (24) will result in an ellipsoid
bounding the reachable set. There are many choices for the
objective function to tighten the outer ellipsoid bound, but
the trace criteria tends to find compact ellipsoids without a
large principal axis.
IV. N UMERICAL S IMULATION
In this section, we demonstrate the performance of the
distributionally robust fault detector when there is no attack
on the system. Using the same detector we present our
analysis of the effects of stealthy attacks on the system by
studying the ellipsoids that bound the reachable states. We
consider the following system for this study with the detector
tuned to a false alarm rate A = 0.05 (5%):






0.84 0.23
0.07 −0.32
1 0
A=
,B =
,C =
−0.47 0.12
0.23 0.58
2 1




1.404 −1.402
0.0276
0.0448
K=
,L =
,
1.842 1.008
−0.01998 −0.0290


0.045 −0.011
Σv = 2Ip , Σw =
.
−0.011
0.02
A. Advantages of Distributional Robustness
The purpose of this first simulation is to demonstrate the
effectiveness of distributionally robust approach while comparing it with the traditional chi-squared detector approach
which assumes a normal distribution for the noises. We
assume that there is no attack on the system so that the
estimation error evolves according to (8) but with δt = 0.
We run an extensive Monte-Carlo simulation to derive an
empirical probability of the distance measure, zt , lying above
a threshold. To demonstrate the effectiveness of our proposed
approach, we investigate two detectors: 1) a detector tuned
assuming the noises are Gaussian distributed with threshold αχ2 using (13); and 2) a detector tuned making no
assumption about the distribution of the noises with threshold
αDR using (15). For demonstration, we test these detectors
with two scenarios: 1) a scenario where both noises are

Fig. 1. The histogram plot shows the probability of the quadratic distance
measure random variable zt in two noise scenarios where in the first case
the system is driven by Gaussian noises and in the second case the system
is driven by fat-tailed noises. Two detectors are tuned using (13) and (15)
respectively. While the chi-squared detector generated 5% false alarms for
the Gaussian noise scenario, it generated 29.81% when the noises were
fat-tailed. The distributionally robust detector generated 0% false alarms
for the Gaussian noise scenario and 0.83% false alarms when the noises
were fat-tailed. The inner plot shows the zoomed in part for zt ∈ [25, 45].

Gaussian and 2) a scenario where both noises are distributed
according to Student’s t distribution (having heavy tails) with
the degree of freedom ν = 5. In both cases the noises belong
to the ambiguity sets P v and P w characterized by zeromean and respective covariances Σv and Σw listed above.
Finally, we estimate the false alarm rate of each detector
under each noise scenario, by evaluating the probability of zt
falling above each detector threshold. The result of a MonteCarlo simulation with 100,000 trials (for each noise scenario)
is shown as an histogram plot of the distance measure of
zt in Fig. 1. In the first noise scenario, when the noises
are Gaussian, the distributionally robust detector provides
a conservative threshold bound with 0% false alarm rate.
The chi-squared detector resulted in the user prescribed false
alarm rate of 5%.
In the second noise scenario, we see an increased false
alarm rate with the traditional chi-squared detector. Due to
the wrong assumption for system noise and sensor noise, the
chi-squared detector generated 29.81% false alarms resulting
in significantly miscalculated risk. The distributionally robust
detector generated 0.83% false alarms. Since our goal in
tuning the detector is to create a monitor that generates false
alarms no more than A = 5% of the time, we see that the DR
approach achieves this aim, while the traditional chi-squared
approach does not.
Remark 1: While the low false alarm rate of the distributionally robust detector looks appealing, it allows a malicious
attacker to execute stealthy attacks with larger impact which
will be demonstrated in the following simulations.
Remark 2: The performance of the distributionally robust
detector can be improved significantly if higher order moments or other structural information about the distributions
(e.g., symmetry or unimodality) of the residual sequence are
also utilized. This will result in sharper probability estimates
as mentioned in [17] and hence in tighter threshold values.

Fig. 2. Two empirical reachable states when the system is driven by
zero-alarm attacks and system noise: one larger (green) corresponding to a
distributionally robust tuned detector and another smaller (magenta) corresponding to a chi-squared tuned detector. Their respective outer bounding
ellipsoids, which can be efficiently calculated with Proposition 2, in blue
and red.

Fig. 3. The variation of worst case false alarm rate as a function of the
user prescribed desired false alarm rate is shown here. The markers in both
the lines along the same vertical axis correspondingly represent the same
threshold. It is evident that a superlinear behavior (blue curve) is observed
depicting that worst case false alarm rate is bigger than the desired false
alarm rate for a given threshold.

B. Cost of Being Distributionally Robust
We will now demonstrate the trade-off between being
distributionally robust against any noise distribution and the
increased attacker capabilities caused by the conservative
robust tuning. We quantify this trade-off through reachable
set analysis. We continue to consider the two detectors:
one tuned assuming the noises are Gaussian and one tuned
without any distributional assumptions through the robust
tuning presented in this paper. We study the reachable set
that an attacker is able to accomplish with each detector,
while remaining stealthy, i.e., the attack input satisfies the
zero-alarm stealthiness criteria in (18), and, therefore, do not
raise alarms. We quantify the reachable set through the outer
ellipsoidal bounds found by the optimization in Proposition
2.
Fig. 2 shows the outcome of the two zero-alarm attacks,
one made stealthy to the chi-squared detector and one made
stealthy to the distributionally robust detector. The chisquared ellipsoid is obtained as a function of the detector
threshold computed from (13). Similarly, the distributionally
robust ellipsoid is obtained as a function of the distribu-

the traditional chi-squared detector approach, however the
advantages come at the price of increased attacker capabilities. Our future work will seek to ameliorate this downside
as it is possible to obtain sharper probability estimates for
the threshold values if we include the higher order moments
as explained in [17].
R EFERENCES

Fig. 4. The trade-off shown here indicates that as the distributionally robust
detector is tuned for larger false alarm rates, the trace of the bounding
ellipsoid obtained using the αDR tuning decreases. This trend is inversely
proportional, indicating tuning the distributionally robust detector for a very
small false alarm rate is very expensive in terms of the size of the reachable
set.

tionally robust detector threshold computed from (15). As
αDR ≥ αχ2 , we anticipate the attacker to be able to make
a larger impact under the distributionally robust detector.
In general, computing the exact (empirical) reachable set
is computationally intensive, however, we plot each in this
example for reference.
When there is no attack, the traditional chi-squared detector can generate high rates of false alarms in worst
case noise settings while the distributionally robust detector
is guaranteed to remain below the designed false alarm
rate. Fig. 3 compares for each detector (chi-squared and
distributionally robust) the false alarm rate they are tuned
for (horizontal axis) with the worst possible false alarm
rate they may generate under arbitrary noise distributions
(vertical axis). It is evident that the worst case false alarm
rate is much larger than the desired false alarm rate when a
chi-squared detector is used. In contrast the distributionally
robust detector’s worst case false alarm rate is exactly what
is designed.
However, this performance comes at the cost of increased
attacker capabilities when the system is under attack. Fig.
4 depicts the trade-off observed between the desired false
alarm rate, A, and the the trace of the ellipsoid that bounds
the reachable set obtained using αDR tuning. It is evident
that as A increases, the trace of the distributionally robust
bounding ellipsoid decreases. The same trend pertains even
when the process noise covariance is varied, where the trace
of the corresponding bounding ellipsoid is larger for the
states driven by the process noise having higher covariance.
This inversely proportional trend suggests it is increasingly
more costly to tune the distributionally robust detector for
smaller false alarm rates.
V. C ONCLUSION & F UTURE O UTLOOK
We have proposed a distributionally robust approach to
tuning anomaly detectors by using the generalized Chebyshev moments to find a threshold that guarantees the false
alarm rate will not exceed a desired value. We have demonstrated our ideas with a numerical example that emphasizes
the effectiveness of the distributionally robust approach over

[1] Y. Mo and B. Sinopoli, “Integrity attacks on cyber-physical systems,”
in Proceedings of the 1st international conference on High Confidence
Networked Systems. ACM, 2012, pp. 47–54.
[2] S. Nateghi, Y. Shtessel, J. Barbot, and C. Edwards, “Cyber attack
reconstruction of nonlinear systems via higher-order sliding-mode
observer and sparse recovery algorithm,” in 2018 IEEE Conference
on Decision and Control (CDC), Dec 2018, pp. 5963–5968.
[3] A. Cardenas, S. Amin, B. Sinopoli, A. Giani, A. Perrig, and S. Sastry,
“Challenges for securing cyber physical systems,” in Workshop on
Future Directions in Cyber-physical Systems Security, 2009.
[4] F. Pasqualetti, F. Dorfler, and F. Bullo, “Attack detection and identification in cyber-physical systems,” IEEE Transactions on Automatic
Control, vol. 58, pp. 2715–2729, 2013.
[5] A. Cárdenas, S. Amin, Z. Lin, Y. Huang, C. Huang, and S. Sastry, “Attacks against process control systems: Risk assessment, detection, and
response,” in Proceedings of the 6th ACM Symposium on Information,
Computer and Communications Security, 2011, pp. 355–366.
[6] Y. Mo and B. Sinopoli, “On the performance degradation of cyberphysical systems under stealthy integrity attacks,” IEEE Transactions
on Automatic Control, vol. 61, pp. 2618–2624, 2016.
[7] C. Murguia, N. van de Wouw, and J. Ruths, “Reachable sets of hidden
cps sensor attacks: Analysis and synthesis tools,” in proceedings of the
IFAC World Congress, 2016.
[8] N. Hashemi, C. Murguia, and J. Ruths, “A comparison of stealthy
sensor attacks on control systems,” in 2018 Annual American Control
Conference (ACC), June 2018, pp. 973–979.
[9] Y. Mo and B. Sinopoli, “Secure control against replay attacks,” in
Communication, Control, and Computing, 2009. Allerton 2009. 47th
Annual Allerton Conference on, 2009, pp. 911–918.
[10] Z. Guo, D. Shi, K. Johansson, and L. Shi, “Worst-case stealthy
innovation-based linear attack on remote state estimation,” Automatica,
vol. 89, pp. 117–124, 03 2018.
[11] W. Wiesemann, D. Kuhn, and M. Sim, “Distributionally robust convex
optimization,” Operations Research, vol. 62, no. 6, pp. 1358–1376,
2014.
[12] L. Vandenberghe, S. Boyd, and K. Comanor, “Generalized chebyshev
bounds via semidefinite programming,” SIAM review, vol. 49, no. 1,
pp. 52–64, 2007.
[13] X. Chen, “A New Generalization of Chebyshev Inequality for Random
Vectors,” arXiv e-prints, Jul. 2007.
[14] J. Goh and M. Sim, “Distributionally robust optimization and its
tractable approximations,” Operations research, vol. 58, no. 4-part-1,
pp. 902–917, 2010.
[15] P. M. Esfahani and D. Kuhn, “Data-driven distributionally robust
optimization using the wasserstein metric: Performance guarantees and
tractable reformulations,” Mathematical Programming, vol. 171, no.
1-2, pp. 115–166, 2018.
[16] C. Murguia, I. Shames, J. Ruths, and D. Nešić, “Security metrics of
networked control systems under sensor attacks (extended preprint),”
ArXiv, vol. abs/1809.01808, 2018.
[17] D. Bertsimas and I. Popescu, “Optimal inequalities in probability theory: A convex optimization approach,” SIAM Journal on Optimization,
vol. 15, no. 3, pp. 780–804, 2005.

