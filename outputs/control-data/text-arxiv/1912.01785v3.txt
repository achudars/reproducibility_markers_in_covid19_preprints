MEAN FIELD INTERACTION ON RANDOM GRAPHS WITH
DYNAMICALLY CHANGING MULTI-COLOR EDGES

arXiv:1912.01785v3 [math.PR] 20 Sep 2020

ERHAN BAYRAKTAR AND RUOYU WU
Abstract. We consider weakly interacting jump processes on time-varying random graphs
with dynamically changing multi-color edges. The system consists of a large number of nodes
in which the node dynamics depends on the joint empirical distribution of all the other nodes
and the edges connected to it, while the edge dynamics depends only on the corresponding
nodes it connects. Asymptotic results, including law of large numbers, propagation of chaos,
and central limit theorems, are established. In contrast to the classic McKean-Vlasov limit,
the limiting system exhibits a path-dependent feature in that the evolution of a given particle
depends on its own conditional distribution given its past trajectory. We also analyze the
asymptotic behavior of the system when the edge dynamics is accelerated. A law of large
number and a propagation of chaos result is established, and the limiting system is given as
independent McKean-Vlasov processes. Error between the two limiting systems, with and
without acceleration in edge dynamics, is also analyzed.

Contents
1. Introduction
1.1. Organization
1.2. Notation
2. Systems with node-dependent edge dynamics
2.1. Systems with independent edge dynamics
3. Systems with accelerated edge dynamics
3.1. Approximation error
3.2. Riccati equation for limiting marginal probabilities
4. Fluctuations and central limit theorems
4.1. Canonical processes
4.2. Some integral operators
4.3. Central limit theorems
4.4. An example with explicit variance
5. Proofs of laws of large numbers and propagation of chaos results
5.1. Proofs of Theorems 2.1 and 2.2
5.2. Proofs of Theorems 3.1 and 3.2
6. Proofs of central limit theorems
6.1. Asymptotics of symmetric statistics
6.2. Girsanov change of measure
6.3. Asymptotics of J n
6.4. Completing the proof of Theorem 4.1

2
4
4
5
7
8
9
10
11
11
11
12
13
14
15
21
29
29
30
31
40

Date: September 22, 2020.
2010 Mathematics Subject Classification. 60F05 60K35 60J75 05C80 60G09 60J27 60K37.
Key words and phrases. Dynamical random graphs; Mean field interaction; Propagation of chaos; Central
limit theorems; Endogenous common noise; Exchangeability; Interacting particle systems.
1

2

BAYRAKTAR AND WU

6.5. Completing the proof of Theorem 4.2
Appendix A. Proof of Theorem 2.3
References

41
41
42

1. Introduction
In this work we study some asymptotic results for large particle systems with mean field
interactions on time varying random graphs. The model is described in terms of two collections
of countable-state pure jump processes, one that gives the evolution of (the states of) nodes in
the system, and the other that drives the evolution of (the colors of) edges which govern the
interaction between nodes in the system. We consider mean field interaction between nodes,
in that the node dynamics depends on the joint empirical distribution of all the other nodes
and the edges connected to it. The edge dynamics on the other hand depends only on the
nodes it connects. More precisely,
Z
y1[0,Γ(y,Xin (s−),νin (s−))] (z) Ni (ds dy dz),
Xin (t) = Xi (0) +
[0,t]×Z×R+
Z
n
y1[0,β(n)Γ(y,ξ
ξij (t) = ξij (0) +
(1.1)
n (s−),X n (s−),X n (s−))] (z) Nij (ds dy dz),
e
[0,t]×Z×R+

νin (t) =

1
n

n
X

δ(Xjn (t),ξijn (t)) ,

ij

i

j

i, j = 1, . . . , n,

j=1

where {Xi (0)} are independent and identically distributed (i.i.d.) Z-valued random variables
with some probability distribution µ(0), {ξij (0)} are i.i.d. Z-valued random variables with
some probability distribution θ(0), {Ni } and {Nij } are i.i.d. Poisson random measures (whose
precise definition will be introduced in Section 1.2) with intensity ds × ρ(dy) × dz for some
e are functions governing the jump rates with Γ(y, x, ν) =
finite measure ρ on Z, Γ and Γ
R
e ν(de
e for some measurable function γ. Here X n denotes the state of node
e, ξ)
x dξ)
i
Z2 γ(y, x, x
n
i, ξij describes the color of the edge between nodes i and j, and β(n) ≥ 0 is a sequence of real
numbers representing the scale of jump rates of edges.
A typical example where such a system arises is in the study of gossip algorithms [26],
n denotes whether there is an edge between nodes i, j in a graph with n nodes, and
where ξij
n
Xi denotes whether certain information has spread to node i. In neuroscience, the system
in (1.1) may be used to describe a collection of interacting neurons, where Xin is the state
n
of each neuron, and connections between neurons are denoted by dynamically changing ξij
(see e.g. [3, 30] for a diffusion setup with static graphs). In simpler terms, one may also view
the system as n children playing at M places with K types friendship between each pair of
children, where M and K could be infinity. The node Xin (t) ∈ {1, . . . , M } denotes the place
n (t) ∈ {1, . . . , K} denotes the type of friendship
at which the i-th child is, while the edge ξij
in which the i-th child views the j-th child at time t. The jump rate of Xin depends on the
empirical distribution of all children’s positions and their friendship from the viewpoint of the
i-th child, that is, νin .
n ≡ 1,
When there is only one possible color, i.e. the graph is simply a complete graph with ξij
the model reduces to the classic mean-field system, the study of which dates back to works
of Boltzmann, Vlasov, McKean and others (see [17, 29] and references therein). The original
motivation for the study of mean-field systems came from statistical physics but in recent

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

3

years similar models have arisen in many different application areas, ranging from economics
and chemical and biological systems to communication networks and social sciences (see e.g.
[6] for an extensive list of references). The asymptotic picture is well resolved and many
different results have been established, including laws of large numbers (LLN), propagation
of chaos (POC) properties, and central limit theorems (CLT), see e.g. [4] and the references
therein.
When there are two possible colors (denoted by 0 and 1 for example) and edges are independently drawn and fixed at time 0, i.e. the edges could be present or absent and form an
Erdős-Rényi random graph, the model has recently drawn much attention. It has been shown
that the same LLN, CLT, and large deviation principles (LDP), as in the mean-field case,
hold under certain conditions. In particular for interacting diffusions, quenched and annealed
LLN are studied in [13], CLT is established in [4], and LDP is obtained in [11, 24]. For certain
pure jump processes arising from the study of large-scale queuing networks, LLN is studied
in [8]. Moreover, mean field games on Erdős-Rényi random graphs are analyzed in [12], and
graphon mean field games on static graphs with possibly uncountable players have recently
been studied (see e.g. [9, 10, 25]).
The goal of the current work is to study asymptotic behaviors of the system (1.1) as n → ∞
and β(n) → β ∈ [0, ∞]. Our first main result is LLN, POC, and CLT (Theorems 2.2, 4.1
and 4.2) for node states and their empirical measures when β < ∞. The proof of LLN
and POC relies on certain coupling arguments, exchangeability properties of the nodes and
edges, and a key conditional independence structure for the limiting system (Theorem 2.1).
Intuitively speaking, due to the state-dependent evolution of edges, one would not expect in
the limiting system that edges are independent, although nodes are i.i.d. Indeed, Theorem
2.1 states that conditioning on the path of a node, edges connected with this node are i.i.d.
The proof of such a statement follows from a careful time-discretization argument along with
applying de Finetti’s theorem. Moreover, we generalize the special case β(n) → 0, in which
n
the limit represents a random but a static graph, to a case in which the edge processes ξij
are i.i.d. adapted and could be non-Markovian. LLN and POC for such systems are obtained
in Theorem 2.3. The CLT result characterizes the fluctuation of functionals of the empirical
measures of nodes and edges. The proof of CLT relies on a change of measure technique
using Girsanov’s theorem, and this approach goes back to [27, 28]. This technique reduces the
problem to working with the limiting system where nodes are i.i.d. and edges are conditionally
independent, while the price to pay is that one must carefully analyze the asymptotic behavior
of the Radon–Nikodym derivative. The presence of conditionally independent edges requires
more challenging work than what has been done in the single-color case (e.g. in [27, 28]) and
the two-color case (in [4]). In particular, the node plays the role of common noises in the
analysis (see Lemma 6.6) and as a result the CLT limit is not a Gaussian random variable
but rather a Gaussian mixture.
Our second main result is the study of the averaging principle of the system (1.1) when
β(n) → ∞. Systems of stochastic processes with fast components or regime-switching features
have a long history of applications and the averaging principle has been well studied, when
there is one fast component (see e.g. [7, 23, 32]). However, a collection of fast state-dependent
switching edges are present in the system considered here, and more careful analysis is needed.
In particular in the limiting system, the jump rate of the slow component Xi corresponding
to node i depends on its own probability distribution and the conditional invariant measure
of the fast component given slow components (see (3.1) for the precise form). LLN and
POC for (1.1) when β(n) → ∞ are obtained in Theorem 3.1. Compared to the limiting
system in the regime β(n) → β < ∞, this one does not suffer from the path-dependent

4

BAYRAKTAR AND WU

conditional independence subtlety and serves as a nice approximation of the former, with the
approximation error analyzed in Theorem 3.2.
1.1. Organization. The paper is organized as follows. In Section 2 we analyze the system
(1.1) when β(n) → β ∈ [0, ∞). A basic condition (Condition 2.1) is stated, under which the
limiting system (2.1) has a unique solution and a certain (conditional) independence property
(Theorem 2.1). A law of large numbers and a propagation of chaos property are obtained in
Theorem 2.2. In Section 2.1, we also present a LLN and POC (Theorem 2.3) for a system
n . In Section 3, the system (1.1) with
with i.i.d. and possibly non-Markovian edge processes ξij
accelerated edge dynamics, namely when β(n) → ∞, is studied. LLN and POC are obtained
in Theorem 3.1, and the approximation error, as β → ∞, between the corresponding limiting
system and (2.1) are characterized in Theorem 3.2. The convenience of this limiting system is
illustrated in Section 3.2, by characterizing the evolution of marginal distributions as Riccati
equations. In Section 4 we present a CLT (Theorem 4.1) for the fluctuation of functionals of
the empirical measures of nodes and edges connecting to a given node. As noted above, the
limit is not a Gaussian random variable but rather a Gaussian mixture. We also provide a
CLT (Theorem 4.2) for the fluctuation of functionals of the empirical measures of nodes. The
limit is given by a simpler form and this point is illustrated through an example in Section
4.4 where the variance of the limit Gaussian random variable has an explicit form. Proofs of
all LLN and POC are given in Section 5. Finally Section 6 contains proofs of Theorems 4.1
and 4.2.
1.2. Notation. Given a Polish space S, denote by B(S) the Borel σ-field. Let P(S) be
the space of probability measures on S endowed with the topology of weak convergence. A
convenient metric for this topology is the bounded-Lipschitz metric dBL , defined by
dBL (ν1 , ν2 ) = sup |hf, ν1 − ν2 i| ,
f ∈B1

ν1 , ν2 ∈ P(S),

where B1 is the collection of all Lipschitz functions f that are bounded by 1 and
R such that
the corresponding Lipschitz constant is bounded by 1 as well; and hf, νi := f dν for a
signed measure ν on S and ν-integrable f : S → R. Given a collection of random probability
measures ν n , ν on S, we write ν n → ν in P(S) in probability, if d(ν n , ν) → 0 in probability
as n → ∞, for any metric d on P(S) that metrizes the weak convergence topology. We say
a collection {Xn } of S-valued random variables is tight if the distributions of Xn are tight
d

in P(S). We use the symbol ‘⇒’ to denote convergence in distribution and ‘=’ to denote
the equality in distribution. The probability law of a random variable X will be denoted by
L(X). For a measurable function f : S → R, let kf k∞ := supx∈S |f (x)|. Fix T ∈ (0, ∞).
Denote by C([0, T ] : S) (resp. D([0, T ] : S)) the space of continuous functions (resp. right
continuous functions with left limits) from [0, T ] to S, endowed with the uniform topology
(resp. Skorokhod topology). We will use the notations X(t) and Xt interchangeably for
stochastic processes. For x ∈ D([0, T ] : S), let kxk∗,t := sup0≤s≤t kx(s)k, x[t] := (x(s) : 0 ≤
s ≤ t), and x[t−] := (x(s) : 0 ≤ s < t). For a Hilbert space H, denote the norm and inner
product in H by k · kH and h·, ·iH , respectively. For a σ-finite measure ν on a Polish space S,
denote by L2 (S, ν) the Hilbert space of ν-square integrable functions from S to R. We denote
by S∞ the countable product space of copies of S, equipped with the usual product topology.
Let [k] := {1, . . . , k} for each k ∈ N. We will use κ, κ0 , κ1 , . . . for constants in the proofs,
whose value may change over lines. Let Xt = [0, t]×Z×R+ for each t ∈ [0, T ], and let Mt be the
space of σ-finite measures on (Xt , B(Xt )) with the topology of vague convergence. A Poisson
random measure (PRM) N on XT with intensity measure ν ∈ MT is an MT -valued random

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

5

variable such that for each A ∈ B(XT ) with ν(A) < ∞, N (A) is Poisson distributed with
mean ν(A) and for disjoint A1 , . . . , Ak ∈ B(XT ), N (A1 ), . . . , N (Ak ) are mutually independent
random variables (cf. [16]).
Let (Ω, F, P, {Ft }) be a filtered probability space on which we are given i.i.d. PRM {Ni , Nij :
i, j ∈ N} on XT with intensity measure ds × ρ(dy) × dz for some finite measure ρ on Z.
Expectations under P (resp. some other probability measure Q) will be denoted by E (resp.
EQ ).

2. Systems with node-dependent edge dynamics
In this section we study the system (1.1) when β(n) → β ∈ [0, ∞). Recall that Γ(y, x, ν) =
e ν(de
e for y, x ∈ Z and ν ∈ P(Z2 ), where γ is some measurable function
e, ξ)
x dξ)
Z2 γ(y, x, x
e
from Z4 to R. We make the following assumptions on γ and Γ.
R

e ≤ γy and
e, ξ)
Condition 2.1. (i) There exists {γy ∈ [0, ∞) : y ∈ Z} such that 0 ≤ γ(y, x, x
R
e
e
e ξ, x, x
0 ≤ Γ(y,
e) ≤ γy for all y, x, x
e, ξ ∈ Z and Cγ := Z |y|γy ρ(dy) < ∞.
(ii) {Xi (0)} are i.i.d. with some common probability distribution µ(0) ∈ P(Z) and E|Xi (0)| <
∞. {ξij (0)} are i.i.d. with some common probability distribution θ(0) ∈ P(Z) and
E|ξij (0)| < ∞.
Remark 2.1. (a) Condition 2.1(i) holds clearly if the system is finite state, such as
e = 0 whenever |y + x| > r, for some r ∈ N.
γ(y, x, x
e, ξ)
(b) Noting that every bounded function on Zd is automatically Lipschitz, we see that
e is γy -Lipschitz (with respect to all variables) by Condition 2.1(i).
γ(y, x, x
e, ξ)

The next two theorems show that, under Condition 2.1, the limiting system is given by the
pathwise unique solution to the following equations:
Xi (t) = Xi (0) +
ξij (t) = ξij (0) +

Z

Z Xt
Xt

y1[0,Γ(y,Xi (s−),νi (s−))] (z) Ni (ds dy dz),
y1[0,β Γ(y,ξ
(z) Nij (ds dy dz),
e
ij (s−),Xi (s−),Xj (s−))]

(2.1)

n

1X
δ(Xj (t),ξij (t)) ,
n→∞ n

νi (t) = lim

j=1

where the limit in νi (t) is understood as almost surely in P(Z2 ) for each i ∈ N and t ∈ [0, T ].
Note that the PRMs are the same as those in (1.1), for ease of deriving Theorem 2.2 below.
The proofs of Theorems 2.1 and 2.2 are given in Section 5.1.
Theorem 2.1. Suppose Condition 2.1 holds. Then
(a) The system (2.1) has a unique pathwise solution.
(b) Xi are i.i.d., {(Xj [t], ξij [t]) : j ∈ N, j 6= i} are i.i.d. conditioning on Xi [t], and νi (t) =
L((Xj (t), ξij (t)) | Xi [t]) = Φt (Xi [t]) for each j 6= i, where Φt : D([0, t] : Z) → P(Z2 ) is
some measurable map independent of the choice of i.

6

BAYRAKTAR AND WU

Remark 2.2. Using Theorem 2.1, the system (2.1) could be rewritten in the following equivalent and perhaps more familiar form:
Z
y1[0,Γ(y,Xi (s−),νi (s−))] (z) Ni (ds dy dz),
Xi (t) = Xi (0) +
Xt
Z
y1[0,β Γ(y,ξ
(z) Nij (ds dy dz),
ξij (t) = ξij (0) +
e
ij (s−),Xi (s−),Xj (s−))]
Xt

νi (t) = L((Xk (t), ξik (t)) | Xi [t]) = Φt (Xi [t]),

For i ∈ [n] and t ∈ [0, T ], let
n

νin :=

1X
δ(Xjn (·),ξijn (·)) ,
n
j=1

∀k 6= i.

n

µn :=

1X
δXin (·) ,
n

n

µn (t) :=

i=1

1X
δXin (t) .
n

(2.2)

i=1

Theorem 2.2. Suppose Condition 2.1 holds. Then
(a) There exists some κ = κ(T, β) < ∞ such that

κ
n
max EkXin − Xi k∗,T + max Ekξij
− ξij k∗,T ≤ √ + κ|β(n) − β|.
n
i∈[n]
i,j∈[n]

(2.3)

(b) (POC) For any k ∈ N, as n → ∞,
L(X1n , . . . , Xkn ) → µ⊗k ,

L(X1n (t), . . . , Xkn (t)) → [µ(t)]⊗k for each t ∈ [0, T ],

(2.4)

where µ := L(Xi ) ∈ P(D([0, T ] : Z)) and µ(t) := L(Xi (t)) ∈ P(Z) for i ∈ N.
(c) (LLN) As n → ∞,
n

1X
δ(Xj (·),ξij (·)) in P(D([0, T ] : Z2 )) in probability,
n→∞ n

νin → νi := lim
νin (t)

(2.5)

j=1

2

→ νi (t) in P(Z ) in probability, for each t ∈ [0, T ],

(2.6)

µn → µ in P(D([0, T ] : Z)) in probability,

(2.7)

for each i ∈ [n], and
n

µ (t) → µ(t) in P(Z) in probability, for each t ∈ [0, T ].

(2.8)

Remark 2.3. (a) We abuse the notation to use νin , νi , µn , µ to denote the empirical measures
on the path space, and use νin (t), νi (t), µn (t), µ(t) to denote the processes of the marginal
empirical measures. We always precisely state the space to avoid the ambiguity in statements
such as (2.5) and (2.7).
(b) Although (2.7) only states the convergence on the path space, by applying standard
arguments (cf. [22, Theorem 4.7 and Lemma 4.8]), one can make use of the fact that µ
is deterministic and obtain suitable controls of jump sizes of Xi , to argue that the process
{µn (t) : t ∈ [0, T ]} converges in probability to {µ(t) : t ∈ [0, T ]} in the space D([0, T ] : P(Z))
endowed with the uniform topology.
n ) in (1.1) (resp. L(ξ ) 6= L(ξ ) in (2.1))
Remark 2.4. (a) We note that L(ξiin ) 6= L(ξij
ii
ij
n
for j 6= i. However, the contribution of ξii to νin (resp. ξii to νi ) is negligible. Therefore
one does not
have to worry about the special evolution of ξiin . One may also simply define
1 Pn
n
νi (t) = n j6=i δ(Xjn (t),ξijn (t)) and the LLN, POC, and CLT results in this work will still be
valid.
(b) Although in this paper we consider the case of directed graphs, that is, we do not assume
n = ξ n for j 6= i, we note that the results naturally extend to the undirected graph scenario,
ξij
ji

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

7

e x, x
ex
e ξ,
e ξ,
with an additional symmetry assumption Γ(y,
e) = Γ(y,
e, x) and minor changes to the
proofs.

2.1. Systems with independent edge dynamics. In this section we consider a system that
generalizes the β(n) → 0 limit of (1.1), in that we allow for non-Markovian edge processes
(such as processes with delays and renewal processes). Recall the node process Xin and the
local empirical measure process νin
Z
n
y1[0,Γ(y,Xin (s−),νin (s−))] (z) Ni (ds dy dz),
Xi (t) = Xi (0) +
Xt

νin (t) =

1
n

n
X

δ(Xjn (t),ξijn (t)) ,

(2.9)

i = 1, . . . , n.

j=1

n (t) = ξ (t) are adapted, i.i.d. with L(ξ ) = θ ∈ P(D([0, T ] : Z)), and independent
Suppose ξij
ij
ij
of {Xi (0), Ni }.
We make the following assumption on γ.

e ≤ γy for all
Condition 2.2. There exists {γy ∈ [0, ∞) : y ∈ Z} such that 0 ≤ γ(y, x, x
e, ξ)
R
y, x, x
e, ξe ∈ Z and Cγ := Z |y|γy ρ(dy) < ∞.

The next theorem shows that, under Condition 2.2, the limiting system is given by the
unique solution to the following equations:
Z
y1[0,Γ(y,Xi (s−),ν(s−))] (z) Ni (ds dy dz),
Xi (t) = Xi (0) +
(2.10)
Xt
ν(t) := µ(t) ⊗ θ(t) := L(Xi (t)) ⊗ L(ξij (t)), i, j ∈ N.
The proof of Theorem 2.3 is a standard application of a coupling argument. For completeness
it is given in Appendix A.

Theorem 2.3. Suppose Condition 2.2 hold. Then
(a) The system (2.10) has a unique pathwise solution.
(b) There exists some κ = κ(T ) < ∞ such that

κ
max EkXin − Xi k∗,T ≤ √ .
n
i∈[n]

(c) (POC) For any k ∈ N, as n → ∞,
L(X1n , . . . , Xkn ) → µ⊗k ,

L(X1n (t), . . . , Xkn (t)) → [µ(t)]⊗k for each t ∈ [0, T ],

where µ := L(Xi ) ∈ P(D([0, T ] : Z)) for i ∈ N.
(d) (LLN) As n → ∞, for each i ∈ [n],
n
1X
νin :=
δ(Xjn (·),ξijn (·)) → ν := µ ⊗ θ in P(D([0, T ] : Z2 )) in probability,
n

(2.11)

(2.12)

(2.13)

j=1

νin (t) → ν(t) in P(Z2 ) in probability, for each t ∈ [0, T ].

Moreover, as n → ∞,
n
1X
n
δXin (·) → µ in P(D([0, T ] : Z)) in probability,
µ :=
n
µn (t) :=

1
n

i=1
n
X
i=1

δXin (t) → µ(t) in P(Z) in probability, for each t ∈ [0, T ].

(2.14)

(2.15)
(2.16)

8

BAYRAKTAR AND WU

Remark 2.5. Theorem 2.3 is not a simple consequence of Theorems 2.1 and 2.2 for the case
n
β(n) → 0, although it seems to be. In particular, note that the system (2.9) only assumes ξij
to be adapted, which may be a non-Markovian process.
3. Systems with accelerated edge dynamics
In this section we study the system (1.1) with accelerated edge dynamics compared to the
node dynamics, that is when β(n) → ∞. In addition to Condition 2.1, we make the following
e
assumption on Γ and Γ.
(i) For ξe0 , x, x
e ∈ Z, denote by Ye(x,ex,ξe0 ) the continuous-time Markov chain
e x, x
e ξ,
with transition rate matrix Γ(y,
e) (representing the rate of jumping from ξe to ξe+ y)
e of
starting at ξe0 . Suppose that there exists a unique invariant distribution Q(x, x
e, dξ)
e
Y(x,ex,ξe0 ) and
Z
e
e ≤κ
e
e, ξ)Q(x,
x
e, dξ)
Eγ(y, x, x
e, Y(x,ex,ξe0 ) (t)) − γ(y, x, x
e(t)γy (1 + |x| + |e
x|)

Condition 3.1.

Z

R∞
for some κ
e(t) such that 0 κ
eR(t) dt < ∞.
2
(ii) E[|Xi (0)| ] < ∞ and Cγ,2 := Z |y|2 γy ρ(dy) < ∞.

Remark 3.1. (a) Condition 3.1(i) holds if Ye(x,ex,ξe0 ) is uniformly exponentially ergodic in
(x, x
e, ξe0 ) in the following sense: There exists α > 0 and C > 0 such that
X
e − Q(x, x
e ≤ Ce−αt (1 + |x| + |e
|P(Ye(x,ex,ξe0 ) (t) = ξ)
e, {ξ})|
x|),
e
ξ∈Z

e is given, one may refer to sufficient
for all ξe0 , x, x
e ∈ Z. Once the transition rate matrix Γ
criteria (see e.g. [31]) that guarantees such an assumption. In particular, it is satisfied if
the system is finite
R ∞state (cf. [31] and [2, Theorem 6.5]).
(b) The assumption 0 κ
e(t) dt < ∞ in Condition 3.1(i) is needed to obtain the rate of convergence in Theorem 3.1. If one is only interested in the convergence of Xin , µn , νin , then
Rt
κ(s) ds = 0.
it would be sufficient (see Remark 5.1) to assume that limt→∞ 1t 0 e
The next theorem shows that, under Conditions 2.1 and 3.1, the limiting system is given
by the unique solution to the following equations:
Z
y1[0,Γ(y,Xi (s−),νi (s−))] (z) Ni (ds dy dz),
Xi (t) = Xi (0) +
Xt

e = µt (de
e
νi (t)(de
x dξ)
x) Q(Xi (t), x
e, dξ),
µ = L(Xi ),

µt = L(Xi (t)),

(3.1)
i ∈ N.

We note that here the system does not suffer from the path-dependent subtlety in (2.1), and
hence serves as a simpler approximation of (1.1) than (2.1), when β(n) is large.
The proof of Theorem 3.1 is given in Section 5.2.
Theorem 3.1. Suppose Conditions 2.1 and 3.1 hold. Then
(a) There is a unique pathwise solution {Xi } to the system (3.1).
(b) There exists some κ = κ(T ) < ∞ such that
κ
κ
EkXin − Xi k∗,T ≤ √ + p
,
n
β(n)

(3.2)

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

9

(c) (POC) For any k ∈ N, as n → ∞,
L(X1n , . . . , Xkn ) → µ⊗k ,

(d) (LLN) As n → ∞,

L(X1n (t), . . . , Xkn (t)) → [µ(t)]⊗k for each t ∈ [0, T ].

µn → µ in P(D[0, T ] : Z) and µn (t) → µ(t) in P(Z), in probability,

for each t ∈ [0, T ], where µn and µn (t) are introduced in (2.2).
(e) Suppose in addition that
X
e − Q(x, x
e ≤ C(t)(1 + |x| + |e
|P(Ye(x,ex,ξe0 ) (t) = ξ)
e, {ξ})|
x|)

(3.3)
(3.4)

(3.5)

e
ξ∈Z

e are as in
for some positive C(t) such that limt→∞ C(t) = 0, and Ye(x,ex,ξe0 ) and Q(x, x
e, {ξ})
Condition 3.1. Then
νin (t) → νi (t) in P(Z2 ) in probability, for each i ∈ [n] and t ∈ (0, T ]

(3.6)

e dt → ηi (dt de
e := νi (t)(de
e dt
e := ν n (t)(de
x dξ)
x dξ)
x dξ)
x dξ)
ηin (dt de
i

(3.7)

and hence

in P([0, T ] × Z2 ), in probability, for each i ∈ [n].

3.1. Approximation error. In this section we study the approximation error in terms of
β → ∞ between two limiting systems (2.1) and (3.1) obtained as β(n) → β ∈ [0, ∞) and
β(n) → ∞. To distinguish the two systems, we rewrite (2.1) as
Z
β
y1[0,Γ(y,X β (s−),ν β (s−))] (z) Ni (ds dy dz),
Xi (t) = Xi (0) +
i
i
Xt
Z
β
y1[0,β Γ(y,ξ
(z) Nij (ds dy dz),
ξij
(t) = ξij (0) +
β
e
(s−),X β (s−),X β (s−))]
νiβ (t) = lim

n→∞

1
n

Xt
n
X

ij

i

j

δ(X β (t),ξ β (t)) ,

j=1

j

ij

and recall (3.1). The following theorem characterizes the approximation error of these two
limiting systems as β → ∞. The proof is given in Section 5.2.
Theorem 3.2. Suppose Conditions 2.1 and 3.1 hold. Then
(a) There exists some κ = κ(T ) < ∞ such that
κ
EkXiβ − Xi k∗,T ≤ √
β
and
κ
dBL (µβ , µ) ≤ √ ,
β

(3.8)
(3.9)

where µβ := L(Xiβ ).
(b) Suppose in addition that (3.5) holds, then
νiβ (t) → νi (t) in P(Z2 ), in probability, for each i ∈ N and t ∈ (0, T ]

(3.10)

e := ν β (t)(de
e dt → ηi (dt de
e := νi (t)(de
e dt
ηiβ (dt de
x dξ)
x dξ)
x dξ)
x dξ)
i

(3.11)

and hence

in P([0, T ] ×

Z2 ),

in probability, for each i = 1, . . . , n.

10

BAYRAKTAR AND WU

R∞
Remark 3.2. Similar to Remark 3.1(b), the assumption 0 κ
e(t) dt < ∞ in Condition 3.1(i)
is needed to obtain the rate of convergence in Theorem 3.2. If one is only interested in
the convergence of Xiβ , µβ , νiβ , then it would be sufficient (see Remark 5.2) to assume that
Rt
e(s) ds = 0.
limt→∞ 1t 0 κ

3.2. Riccati equation for limiting marginal probabilities. In this section we will get a
Riccati equation for the evolution of µ = L(Xi ).
To simplify the notation for the evolution of µ, we will rewrite the limiting system as
follows:
Z
(y − Xi (s−))1[0,Γ̄(y,Xi (s−),νi (s−))] (z) Ni (ds dy dz),
Xi (t) = Xi (0) +
Xt

where

e = µt (de
e
νi (t)(de
x dξ)
x) Q(Xi (t), x
e, dξ),

Γ̄(y, x, ν) =

Z

Z2

e ν(de
e :=
γ̄(y, x, x
e, ξ)
x dξ)

Z

Z2

i ∈ N,

e ν(de
e = Γ(y − x, x, ν)
γ(y − x, x, x
e, ξ)
x dξ)

e is still the unique stationis the rate of particles jumping from x to y, and Q(x, x
e, dξ)
e x, x
e ξ,
ary/invariant distribution of Markov chain with transition rate matrix Γ(y,
e) (repree
e
senting the rate of jumping from ξ to ξ + y). Let
pt (k) = µt ({k}) = P(Xi (t) = k).

Then
X
d
pt (k) = −pt (k)
dt
y6=k

+

X

pt (y)

y6=k

= −pt (k)
+

X
y6=k

Z

Z2

Z

Z2

e µt (de
e
ρ(y − k)γ̄(y, k, x
e, ξ)
x) Q(k, x
e, dξ)

e µt (de
e
ρ(k − y)γ̄(k, y, x
e, ξ)
x) Q(y, x
e, dξ)

XXZ

y6=k x
e∈Z Z

pt (y)

XZ

x
e∈Z Z

e t (e
e
ρ(y − k)γ̄(y, k, x
e, ξ)p
x) Q(k, x
e, dξ)

e t (e
e
ρ(k − y)γ̄(k, y, x
e, ξ)p
x) Q(y, x
e, dξ).

This is an infinite dimensional Riccati equation.

e =
Remark 3.3. If, for simplicity, the system is finite dimensional with ρ(y) = 1, γ̄(y, x, x
e, ξ)
e does not depend on x
e = Q0 (dξ)
e does not depend on x or x
γ̄0 (y, x, ξ)
e, and Q(x, x
e, dξ)
e, then the
above Riccati equation reduces to the following finite dimensional linear ordinary differential
equations:
Z
XZ
X
d
e Q0 (dξ)
e +
e Q0 (dξ).
e
pt (k) = −pt (k)
γ̄0 (y, k, ξ)
pt (y) γ̄0 (k, y, ξ)
dt
Z
Z
y6=k

y6=k

Note that the special choice of γ̄ and Q does not mean that the particle system is trivial.
e still allows interaction as there is the dependence on relationship ξ.
e Also
Indeed, γ̄0 (y, x, ξ)
e does not depend on x or x
e x, x
e ξ,
note that although Q0 (dξ)
e, one can still have general Γ(y,
e)
n
e
associated to ξij such that the stationary distribution is given by Q0 (dξ).

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

11

4. Fluctuations and central limit theorems
Finally we will study the fluctuations of empirical measures about the law of large numbers
limit when β(n) → β ∈ [0, ∞). For simplicity, we assume β(n) = β ∈ [0, ∞). In addition to
Condition 2.1, we make the following assumption on Γ.
e ≤
Condition 4.1. There exists ε ∈ (0, 1] such that ε ≤ γ(y, x, x
e, ξ)

1
ε

for all y, x, x
e, ξe ∈ Z.

The fluctuations will be characterized by CLT in Theorems 4.1 and 4.2 in Section 4.3.
Variances in these CLT will be expressed in terms of norms of certain integral operators
defined in Section 4.2 using canonical spaces and processes introduced in Section 4.1.
4.1. Canonical processes. We first introduce the following canonical spaces and stochastic
processes. Let Ωv = MT × D([0, T ] : Z)2 , Ωe = D([0, T ] : Z), and Ω0 = D([0, T ] : Z). Define
n(n−1)
for n ∈ N the probability measure P n on Ωn := Ω0 × Ωnv × Ωe
by
P n := L (X1 , (N1 , X1 , ξ11 ), (N2 , X2 , ξ12 ), . . . , (Nn , Xn , ξ1n ), {ξij : i = 2, . . . , n, j ∈ [n]}) .

For ω = (ω0 , ω1 , ω2 , . . . , ωn , ω̄) ∈ Ωn with ωi = (ωik : k = 1, 2, 3) for i = 1, . . . , n and
ω̄ = (ω̄ij : i = 2, . . . , n, j ∈ [n]), let
V0 (ω) := ω0 ,

and, abusing notation, write
V0 := X1 ,

Vi (ω) := ωi = (ωi1 , ωi2 , ωi3 ), i ∈ [n],

Vi := (Ni , Xi , ξ1i ), i ∈ [n],

ξij (ω) := ω̄ij , i = 2, . . . , n, j ∈ [n].

Note that such an abuse of notation just says that the distribution of the canonical processes
(Ni , Xi , ξij : i, j = 1, . . . , n) under P n is the same as that of processes (Ni , Xi , ξij : i, j =
1, . . . , n) in (2.1). Also define the canonical processes V∗ := (N∗ , X∗ , ξ∗ ) on Ωv by
V∗ (ω) := (N∗ (ω), X∗ (ω), ξ∗ (ω)) := (ω1 , ω2 , ω3 ),

ω = (ω1 , ω2 , ω3 ) ∈ Ωv .

e∗ on Ωv by
Define the compensated PRM N
e∗ (ω)(ds dy dz) := N∗ (ω)(ds dy dz) − ds × ρ(dy) × dz.
N

4.2. Some integral operators. We will need some functions for stating our central limit
theorem. Let
α(x, ·) := L((N2 , X2 , ξ12 ) ∈ ·|V0 = x)
and
θ(t, x[t], x
e[t]) := L(ξ12 (t) | X1 [t] = x[t], X2 [t] = x
e[t])
be the corresponding regular conditional probabilities, for x, x
e ∈ D([0, T ] : Z). Let
P0 := L(X1 ),

Ξ := L(N2 , X2 , ξ12 ).

Note that P0 is just µ but we write it in this way to emphasize its role as a common factor. Recall νi (t) = L((Xj (t), ξij (t)) | Xi [t]) and we rewrite it as ν(t, Xi [t]) to emphasize its
dependence on Xi [t]. Define the function h : Ωv × Ωv → R (Ξ × Ξ a.s.) by
Z
1[0,Γ(y,X∗ (ω1 )(s−),ν(s−,X∗ (ω1 )[s−]))] (z)
h(ω1 , ω2 ) :=
XT

where

hγ̄s−,y (X∗ (ω1 )[s−], X∗ (ω2 )(s−), ·), θ(s−, X∗ (ω1 )[s−], X∗ (ω2 )[s−])i e
·
N∗ (ω1 )(ds dy dz),
Γ(y, X∗ (ω1 )(s−), ν(s−, X∗ (ω1 )[s−]))
γ̄t,y (x1 , x2 , ξ) := γ(y, x1 (t), x2 , ξ) − hγ(y, x1 (t), ·, ·), ν(t, x1 [t])i,

12

BAYRAKTAR AND WU

for t ∈ [0, T ], x1 ∈ D([0, t] : Z) and y, x2 , ξ ∈ Z. Fix ω0 ∈ Ω0 and consider the Hilbert space
Hω0 := L2 (Ωv , α(ω0 , ·)). Define the integral operator Aω0 on Hω0 by
Z
g(ω1 )h(ω1 , ω2 ) α(ω0 , dω1 ), g ∈ Hω0 , ω2 ∈ Ωv .
Aω0 g(ω2 ) =
Ωv

Denote by I the identity operator.

4.3. Central limit theorems. We denote by A the collection of all measurable maps
ϕ : D([0, T ] : Z)2 → R such that ϕ(X∗ , ξ∗ ) ∈ L2 (Ωv , α(ω0 , ·)) for P0 a.e. ω0 ∈ Ω0 . For
ϕ ∈ A and ω0 ∈ Ω0 , let
Z
ϕ(X∗ (ω1 ), ξ∗ (ω1 )) α(ω0 , dω1 ), Φω0 (ω) := ϕ(X∗ (ω), ξ∗ (ω))−mϕ (ω0 ), ω ∈ Ωv .
mϕ (ω0 ) :=
Ωv

For ω0 ∈ Ω0 and ϕ ∈ A, define

σωϕ0 := k(I − Aω0 )−1 Φω0 kHω0 ,

and denote by πωϕ0 the normal distribution with mean 0 and standard deviation σωϕ0 . Here the
operator I − Aω0 is invertible by Lemma 6.7. Define π ϕ ∈ P(R) by
Z
ϕ
πωϕ0 P0 (dω0 ).
π :=
Ω0

Finally let
η n (ϕ) :=

√

√

nhϕ, ν1n − ν1 i =


n
X
1
n
ϕ(Xjn , ξij
) − mϕ (X1n ) .
n
n


j=1

The following is the CLT for the empirical measure ν1n of neighboring nodes and edges of
node 1. The proof is given in Section 6.
Theorem 4.1. Suppose that Conditions 2.1 and 4.1 hold. Then, for all ϕ ∈ A, L(η n (ϕ)) →
π ϕ weakly as n → ∞.

We note that the limit of the fluctuation of ν1n is a Gaussian mixture. This is due to the
conditional independence of {ξij : j 6= i} given Xi , so that Xi serves as a common noise which
would not be averaged out. However, the limit could be written as a single Gaussian random
variable if one is interested in the fluctuation of µn . To be precise, let Ax := L2 (D([0, T ] : Z), µ)
and H = L2 (Ωv , Ξ). Define the integral operator A on H by
Z
g(ω1 )h(ω1 , ω2 ) Ξ(dω1 ), g ∈ H, ω2 ∈ Ωv .
Ag(ω2 ) =
Ωv

For ϕ ∈ Ax , let

Φ(ω) := ϕ(X∗ (ω)) −
and

Z

Ωv

ϕ(X∗ (ω1 )) Ξ(dω1 ),


ω ∈ Ωv ,


n
X
√
√
1
ηxn (ϕ) := nhϕ, µn − µi = n 
ϕ(Xjn ) − Eϕ(X1 ) .
n

The following is the CLT for empirical measure
given in Section 6.

j=1

µn

of all nodes in the system. The proof is

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

13

Theorem 4.2. Suppose that Conditions 2.1 and 4.1 hold. Then {ηxn (ϕ) : ϕ ∈ Ax } converges
as n → ∞ to a mean 0 Gaussian field {ηx (ϕ) : ϕ ∈ Ax } in the sense of convergence of finite
dimensional distributions, where for ϕ, ψ ∈ Ax ,
E[ηx (ϕ)ηx (ψ)] = h(I − A)−1 Φ, (I − A)−1 ΨiH ,

where Ψ := ψ(X∗ ) − Eψ(X1 ) ∈ H.

4.4. An example with explicit variance. In this section we provide an example where
one can get explicit variance from Theorem 4.2 for some functionals ϕ ∈ Ax .
Suppose Conditions 2.1 and 4.1 hold, and
e = c0 (y)b0 (x) + c1 (y)b1 (e
e + c3 (y),
γ(y, x, x
e, ξ)
x) + c2 (y)b2 (ξ)

e x, x
e ξ,
for some bounded functions b0 , b1 , b2 , c0 , c1 , c2 , c3 : Z → R. Assume that ρ({·}), Γ(·,
e) are
even functions from Z to R+ , b1 , b2 are odd functions, and c0 , b0 , c3 are even functions. Also
suppose β = 1, Xi (0) = 0 and ξij (0) = 0.
Consider the system
Z
y1[0,c0(y)b0 (Xi (s−))+c3 (y)] (z) Ni (ds dy dz),
Xi (t) =
Xt
Z
y1[0,β Γ(y,ξ
(z) Nij (ds dy dz),
(4.1)
ξij (t) =
e
ij (s−),Xi (s−),Xj (s−))]
Xt

n

1X
δ(Xj (t),ξij (t)) = L((Xk (t), ξik (t)) | Xi [t]),
νi (t) = lim
n→∞ n
j=1

k 6= i.

e x, x
e ξ,
From the even function property of ρ({·}), c0 , b0 , c3 and Γ(·,
e) we see that L(Xi (t)) =
L(−Xi (t)) and L(ξij (t)|Xi [t], Xj [t]) = L(−ξij (t)|Xi [t], Xj [t]). It then follows from the odd
function property of b1 , b2 that
Γ(y, Xi (s), νi (s)) = c0 (y)b0 (Xi (s)) + c3 (y)

(4.2)

and
hγ̄s,y (X∗ (ω1 )[s], X∗ (ω2 )(s), ·), θ(s, X∗ (ω1 )[s], X∗ (ω2 )[s])i = c1 (y)b1 (X∗ (ω2 )(s))

(4.3)

for ω1 , ω2 ∈ Ωv . The equality (4.2) and Theorem 2.1(a) imply that (4.1) is indeed the limiting
system (2.1).
Now consider ϕ ∈ Ax defined by
Z T
Z
ϕ(x) = xT −
b1 (xs ) ds yc1 (y) ρ(dy), x ∈ D([0, T ] : Z).
0

Z

For this example we can explicitly describe the asymptotic distribution of

Z
Z T
n 
√
1 X
n
n
n
n
b1 (Xj (s)) ds yc1 (y) ρ(dy) .
Xj (T ) −
ηx (ϕ) = nhϕ, µ − µi = √
n
Z
0
j=1

Following the notation above Theorem 4.2, we have
Z
Z T
b1 (X∗,s (ω)) ds yc1 (y) ρ(dy),
Φ(ω) = X∗,T (ω) −
0

and from (4.3) we have
Z
1[0,c0 (y)b0 (X∗,s (ω1 ))+c3 (y)] (z)
h(ω1 , ω2 ) :=
XT

Z

ω ∈ Ωv ,

c1 (y)b1 (X∗,s (ω2 ))
e∗ (ω1 )(ds dy dz)
N
c0 (y)b0 (X∗,s (ω1 )) + c3 (y)

14

BAYRAKTAR AND WU

for ω1 , ω2 ∈ Ωv . The special form of ϕ allows us to determine (I − A)−1 Φ. Indeed, let
Z
e∗ (ω)(ds dy dz)
y1[0,c0(y)b0 (X∗,s (ω))+c3 (y)] (z) N
Ψ(ω) =
XT
Z
y1[0,c0(y)b0 (X∗,s (ω))+c3 (y)] (z) N∗ (ω)(ds dy dz) = X∗,T (ω), Ξ-a.s. ω ∈ Ωv ,
=
XT

where the second line uses the even function property of c0 , c3 and ρ({·}). Note that
Z
yc1 (y)b1 (X∗,s (ω)) ds ρ(dy),
AΨ(ω) =
[0,T ]×Z

(I − A)Ψ(ω) = X∗,T (ω) −

Z

T

b1 (X∗,s (ω)) ds

0

Z

yc1 (y) ρ(dy) = Φ(ω),
Z

Ξ-a.s. ω ∈ Ωv .

Therefore (I −A)−1 Φ = Ψ and from Theorem 4.2 we have that ηxn (ϕ) converges in distribution
to a mean zero Gaussian random variable with variance
i
h
σϕ2 = kΨk2H = E (Xi (T ))2 .

Remark 4.1. (a) We note that the variance is not simply the variance of Xi (T ) −
RT
R
b
(X
(s))
ds
1
i
0
Z yc1 (y) ρ(dy), since the propagation of chaos property in Theorem 2.2(b)
only guarantees asymptotic independence for finite collection of Xin .
(b) It is straightforward to extend above calculation to functionals that depend on states at
finitely many time instants. Indeed, taking


Z tk
Z
m
X
ak xtk −
b1 (xs ) ds yc1 (y) ρ(dy) , x ∈ D([0, T ] : Z),
ϕ(x) =
0

k=1

Z

for some 0 ≤ t1 < · · · < tm ≤ T , a1 , . . . , am ∈ R, m ∈ N, one has


Z tk
Z
m
X
ak X∗,tk (ω) −
b1 (X∗,s (ω)) ds yc1 (y) ρ(dy)
Φ(ω) =
k=1

0

Z

Pm

and (I − A)−1 Φ = Ψ, where Ψ(ω)√= k=1 ak X∗,tk (ω) for Ξ−a.s. ω ∈ Ωv . It then follows
from Theorem 4.2 that ηxn (ϕ) = nhϕ, µn − µi converges in distribution to a mean zero
Gaussian random variable with variance

!2 
m
X
ak Xi (tk )  .
σϕ2 = kΨk2H = E 
k=1

5. Proofs of laws of large numbers and propagation of chaos results
We first state an elementary result on (conditionally) i.i.d. random variables. The proof is
omitted.
Lemma 5.1. Let {Yi : i = 1, . . . , n} be a collection of S-valued random variables defined on
some probability space (Ω, F, P), where S is some Polish space. Suppose {Yi : i = 1, . . . , n} are
conditionally i.i.d. given some σ-field G ⊂ F. Then for each k ∈ N, there exists ak ∈ (0, ∞)
such that
k
n
ak
1X
sup E
(f (Yi ) − E[f (Yi )|G]) ≤ k/2 .
n
n
kf k∞ ≤1
i=1

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

15

5.1. Proofs of Theorems 2.1 and 2.2.
Proof of Theorem 2.1. (a) We first prove pathwise uniqueness. Suppose {(Xi , ξij , νi ) : i, j ∈
ei , ξeij , νei ) : i, j ∈ N} are two solutions of (2.1) with Xi (0) = X(0)
e
N} and {(X
and ξij (0) = ξeij (0)
for i, j ∈ N. By adding and subtracting terms, for t ∈ [0, T ],
Z
e
|y| 1[0,Γ(y,Xi (s−),νi (s−))] (z) − 1[0,Γ(y,Xei (s−),eνi (s−))] (z) Ni (ds dy dz)
EkXi − Xi k∗,t ≤ E
ZXt
ei (s), νei (s)) ds ρ(dy)
|y| Γ(y, Xi (s), νi (s)) − Γ(y, X
≤E
[0,t]×Z
Z
|y| |hγ(y, Xi (s), ·), νi (s) − νei (s)i| ds ρ(dy)
≤E
[0,t]×Z
Z
ei (s), ·), νei (s)i ds ρ(dy)
|y| hγ(y, Xi (s), ·) − γ(y, X
+E
[0,t]×Z
Z


ei (s) ds ρ(dy)
|y|γy EdBL (νi (s), νei (s)) + E Xi (s) − X
≤
[0,t]×Z

≤ Cγ

Z

0

t

E

"

#

ei k∗,s
sup dBL (νi (u), νei (u)) + EkXi − X

u∈[0,s]

!

ds,

where the fifth line uses the Lipschitz property of γ guaranteed by Condition 2.1(i) and
Remark 2.1(b). Similarly,
Ekξij − ξeij k∗,t
Z
|y| 1[0,β Γ(y,ξ
(z) − 1[0,β Γ(y,
≤E
e
e ξeij (s−),X
ei (s−),X
ej (s−))] (z) Nij (ds dy dz)
ij (s−),Xi (s−),Xj (s−))]
Xt
Z
e ξij (s), Xi (s), Xj (s)) − Γ(y,
e ξeij (s), X
ei (s), X
ej (s)) ds ρ(dy)
|y|β Γ(y,
≤E
[0,t]×Z
Z


ei (s) + E Xj (s) − X
ej (s) ds ρ(dy)
|y|βγy E ξij (s) − ξeij (s) + E Xi (s) − X
≤
[0,t]×Z

Z t

ei k∗,s + EkXj − X
ej k∗,s ds,
Ekξij − ξeij k∗,s + EkXi − X
≤ βCγ
0

e guaranteed by Condition 2.1(i) and
where the fourth line uses the Lipschitz property of Γ
Remark 2.1(b). Also by Fatou’s lemma,
#
"
#
"
E

sup dBL (νi (s), νei (s)) = E

s∈[0,t]

sup sup |hf, νi (s) − νei (s)i|

s∈[0,t] f ∈B1


1 X
ej k∗,t + Ekξij − ξeij k∗,t
EkXj − X
n→∞ n
j=1


ej k∗,t + Ekξij − ξeij k∗,t .
≤ sup EkXj − X
n

≤ lim inf
i,j∈N

Combining these three estimates gives

ej k∗,t + Ekξij − ξeij k∗,t ≤ 2(1 + β)Cγ
EkXj − X

Z

t



ej k∗,s + Ekξij − ξeij k∗,s ds.
sup EkXj − X

0 i,j∈N

16

BAYRAKTAR AND WU

The pathwise uniqueness then follows from Gronwall’s lemma.
Next we will adapt the argument in [20] to prove existence of the system (2.1). For m ∈ N,
let
Z
m
e
1[0,γy ] (z)Ni (ds dy dz),
(5.1)
Ni ([0, t] × A) :=
]×A
[0, ⌊mt⌋
m

e m ([0, t] × A) :=
N
ij

Z

[0,

⌊mt⌋
]×A
m

1[0,βγy ] (z)Nij (ds dy dz),

(5.2)

for t ∈ [0, T ], A ⊂ B(Z × R+ ) and i, j ∈ N. Consider an approximation system that is driven
e m, N
e m : i, j ∈ N} as follows:
by {N
i
ij
Z
m
e m (ds dy dz),
e
y1[0,Γ(y,Xe m (s−),eν m (s−))] (z) N
(5.3)
Xi (t) = Xi (0) +
i
i
i
Xt
Z
m
em
y1[0,β Γ(y,
(5.4)
ξeij
(t) = ξij (0) +
e ξem (s−),X
e m (s−),X
e m (s−))] (z) Nij (ds dy dz),
νeim (t) = lim

n→∞

1
n

1
n→∞ n

= lim

Xt
n
X
j=1
n
X
j=1

ij

i

δ(Xe m (t),ξem (t)) = lim
j

n→∞

ij

δ(Xe m ( k ),ξem ( k ))
j

m

ij

m

for

j

1
n

n
X
j=1

δ(Xe m ( ⌊mt⌋ ),ξem ( ⌊mt⌋ ))
j

m

k
k+1
≤t<
,
m
m

ij

m

k ∈ N0 .

Note that the system is piece-wise constant and determined recursively over intervals of length
1
m . We claim that the following holds for each k ∈ N0 : There exists a unique solution
e m [ k ], ξem [ k ], νem [ k ]) : i, j ∈ N}, which is exchangeable, namely for each K ∈ N and
{(X
ij m
i m
i m
permutation π on {1, . . . , K},
eim [
{(X

k em k
k
k
k
d
m
e m [ k ], ξem
], ξij [ ], νeim [ ]) : i, j ∈ N} = {(X
], νeπ(i)
[ ]) : i, j ∈ N}.
π(i)
π(i)π(j) [
m
m
m
m
m
m

m : j ∈ N) for each fixed i, as the
Note that we are not claiming the exchangeability of (ξeij
m . Now we prove this claim by induction.
evolution of ξeiim is different from that of other ξeij
Since (Xi (0)) and (ξij (0)) are all i.i.d., the claim holds for k = 0. Now assume the claim
k k+1
holds for some k ∈ N0 . Since the solution is linear on [ m
, m ), we have the existence and
k+1 em k+1
m
e
uniqueness of {(Xi [ m ], ξij [ m ]) : i, j ∈ N} by Condition 2.1, and the exchangeability of
e m [ k+1 ], ξem [ k+1 ], νem [ k ]) : i, j ∈ N}. This further implies the existence and uniqueness of
{(X
i
ij m
i m
m
)
by
de
Finetti’s
theorem (cf. [18, Theorem 4.1], see also [1, Theorem 3.1]), and the
νeim ( k+1
m
m
em k+1 em [ k+1 ]) : i, j ∈ N}. Therefore the claim holds for
e
exchangeability of {(Xi [ k+1
i
m ], ξij [ m ], ν
m
k + 1 and hence holds for each k ∈ N0 by induction.
Using Condition 2.1 and the evolution in (5.3) and (5.4), we have

eim k∗,T ≤ E|Xi (0)| + T Cγ < ∞,
EkX

m
Ekξeij
k∗,T ≤ E|ξij (0)| + T βCγ < ∞,

e m (t), ξem (t)) : i, j ∈ N} in Z∞ for each t ∈ [0, T ]. Moreover,
which implies the tightness of {(X
ij
i
for the fluctuations, one can easily verify that




1
1
m
m
m
m
e
e
e
e
E|Xi (τ + δ) − Xi (τ )| ≤ Cγ δ +
, E|ξij (τ + δ) − ξij (τ )| ≤ βCγ δ +
, (5.5)
m
m

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

17

for each δ ∈ (0, 1) and Ft -stopping times τ with τ ∈ [0, T − δ] a.s. Therefore the sequence
e m , ξem ) : i, j ∈ N} is tight in D([0, T ] : Z∞ ) by applying Aldous’ tightness criterion
of {(X
i
ij
[19, Theorem 2.7] with m → ∞ and then δ → 0.
e m , ξem ) : i, j ∈ N} ⇒ {(X
ei , ξeij ) : i, j ∈
Taking a subsequence if necessary, we assume that {(X
i
ij
e Fe, P),
e as m → ∞. Since {(X
e m , ξem ) : j ∈ N, j 6= i}
N}, defined on some probability space (Ω,
j
ij
ej , ξeij ) : j ∈ N, j 6= i}, and we can define
is exchangeable, so is {(X
n

1X
δ(Xej (t),ξeij (t)) .
νei (t) := lim
n→∞ n

(5.6)

j=1

It then follows from [20, Lemma 2.1] (see also [18, Lemma 4.2]), and appealing to the Skorokhod representation theorem, that
e m (tk ), ξem (tk ), νem (tk )) : i, j ∈ N, k ∈ [K]}
{(X
i
ij
i

ei (tk ), ξeij (tk ), νei (tk )) : i, j ∈ N, k ∈ [K]}
⇒ {(X

(5.7)

in Z∞ × P(Z4 )∞ as m → ∞, for each K ∈ N and t1 , . . . , tK ∈ TC ⊂ [0, T ], where [0, T ] \ TC
is at most countable. Let
n
X
e m := lim 1
δ(Xe m (·),ξem (·)) .
Ξ
i
n→∞ n
j
ij
j=1

Denoting by ρ̄ the Prohorov metric on P(Z2 ), we have
h
i
e m {x ∈ D([0, T ] : Z2 ) : |x(τ + δ) − x(τ )| ≥ ε} + ε
Eρ̄(e
νim (τ + δ), νeim (τ )) ≤ E Ξ
i

e m (τ + δ), ξem (τ + δ)) − (X
e m (τ ), ξem (τ ))| ≥ ε) + ε
= P(|(X
j
ij
j
ij
≤

(1 + β)Cγ (δ +
ε

1
m)

+ε

(5.8)

for each ε, δ ∈ (0, 1) and Ft -stopping times τ with τ ∈ [0, T − δ] a.s., where the last line uses
e m , ξem , νem ) : i, j ∈ N} is tight in D([0, T ] : Z∞ × (P(Z2 ))∞ ) by applying
(5.5). Therefore {(X
i
ij
i
Aldous’ tightness criterion [19, Theorem 2.7] with m → ∞, δ → 0 and then ε → 0. By (5.7),
the finite dimensional distributions converge for time instants in a dense subset set of [0, T ].
So
m m
eim , ξeij
ei , ξeij , νei ) : i, j ∈ N}
{(X
, νei ) : i, j ∈ N} ⇒ {(X
(5.9)

in D([0, T ] : Z∞ × (P(Z2 ))∞ ).
ei , ξeij , νei ) : i, j ∈ N} satisfies (2.1). We will need the weak conNext, we will verify that {(X
vergence of stochastic integrals with respect to Poisson random measures later, and the following notations from [21]. Let H := L2 (Z × R+ , ρ(dy) dz). Given a polish space S, a collection
of S-valued stochastic processes S m , S, and Poisson random measures {Yim (ds dy dz) : i ∈ N}
and {Yi (ds dy dz) : i ∈ N} viewed as H# -semimartingales (see [21, Section 3.3] for the precise
definition), we say that (S m , {Yim : i ∈ N}) ⇒ (S, {Yi : i ∈ N}) in D([0, T ] : S) ⊗ H# if


Z
m
m
ϕk (y, z) Yi (ds dy dz) : i ∈ N, k ∈ [K]
S ,
X·

 Z
ϕk (y, z) Yi (ds dy dz) : i ∈ N, k ∈ [K]
⇒ S,
X·

18

BAYRAKTAR AND WU

in D([0, T ] : S × R∞ ) for each K ∈ N and ϕ1 , . . . , ϕK ∈ H. Now take a dense sequence
e m and N
e m defined in (5.1) and (5.2). Clearly
{ϕk } ⊂ H and recall N
i
ij



Z
Z
m
m
m
m
m
e
e
e
e
ϕk (y, z)Nij (ds dy dz) : i, j ∈ N
ϕk (y, z)Ni (ds dy dz),
Xi (t), ξij (t), νei (t),
Xt

Xt

is tight in Z∞ × (P(Z2 ))∞ × R∞ , for each t ∈ [0, T ]. Using Condition 2.1 and the Cauchy–
Schwarz inequality we have
Z
Z
m
e m (ds dy dz)
e
E
ϕk (y, z) N
ϕk (y, z) Ni (ds dy dz) −
i
[0,τ +δ]×Z×R+

≤

≤
≤
E

Z







δ+

1
m

1
δ+
m
δ+

1
m





E
E

≤

δ+

1
m



[0,τ ]×Z×R+

Z×R+

"Z

kϕk kH

[0,τ +δ]×Z×R+



Z

|ϕk (y, z)|1[0,γy ] (z) ρ(dy) dz

Z×R+

p

|ϕk (y, z)| ρ(dy) dz

kϕk kH

1#
2

Z×R+

1[0,γy ] (z) ρ(dy) dz

Cγ ,

eijm (ds dy dz) −
ϕk (y, z) N
p

 1 Z
2

2

Z

[0,τ ]×Z×R+

βCγ ,

eijm (ds dy dz)
ϕk (y, z) N

for each δ ∈ (0, 1) and Ft -stopping times τ with τ ∈ [0, T − δ] a.s. Combining this with (5.5)
and (5.8) implies that



Z
Z
m
m
m em m
e
e
e
ϕk (y, z)Nij (ds dy dz) : i, j ∈ N
ϕk (y, z)Ni (ds dy dz),
Xi , ξij , νei ,
X·

X·

is tight in D([0, T ] : Z∞ × (P(Z2 ))∞ × R∞ ), once again by Aldous’ tightness criterion [19,
e m , ξem , νem , N
e m, N
e m) :
Theorem 2.7] (by taking m → ∞, δ → 0 and then ε → 0). Therefore {(X
i
ij
i
i
ij
i, j ∈ N} is tight in D([0, T ] : Z∞ × (P(Z2 ))∞ ) ⊗ H# . From this, (5.9), (5.1) and (5.2) we have
that, taking a subsequence if necessary,
m m em em
eim , ξeij
ei , ξeij , νei , N
ei , N
eij ) : i, j ∈ N}
{(X
, νei , Ni , Nij ) : i, j ∈ N} ⇒ {(X

(5.10)

e Fe, P),
e in D([0, T ] : Z∞ × (P(Z2 ))∞ ) ⊗ H# , where
defined again on the probability space (Ω,
Z
Z
d
d
eij ([0, t] × ·) =
ei ([0, t] × ·) =
1[0,βγy ] (z)Nij (ds dy dz)
1[0,γy ] (z)Ni (ds dy dz), N
N
[0,t]×·

[0,t]×·

are mutually independent.
Noting that the maps

Z × P(Z2 ) ∋ (x, ν) 7→ y1[0,Γ(y,x,ν)] (z) ∈ H,

Z3 ∋ (ξ, x, x′ ) 7→ y1[0,β Γ(y,ξ,x,x
′ )] (z) ∈ H
e

are continuous, from (5.10) and the continuous mapping theorem we have
y1[0,Γ(y,Xe m (·),eν m (·))] (z) ⇒ y1[0,Γ(y,Xei (·),eνi (·))] (z) in D([0, T ] : H),
i

i

y1[0,β Γ(y,
e ξem (·),X
e m (·),X
e m (·))] (z) ⇒ y1[0,β Γ(y,
e ξeij (·),X
ei (·),X
ej (·))] (z) in D([0, T ] : H),
ij

i

j

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

19

jointly with the convergence in (5.10). Also note that for any stochastic process Z ∈ D([0, T ] :
H) with sup0≤t≤T kZ(t)kH ≤ 1, we have
E

Z

X·

≤ 2E
≤ 2E

ei (ds dy dz)
Z(s, y, z) N

Z

XT

+E
∗,T

Z

X·

eij (ds dy dz)
Z(s, y, z) N

∗,T

|Z(s, y, z)|1[0,(β+1)γy ] (z) ds ρ(dy) dz

Z

2

XT

|Z(s, y, z)| ds ρ(dy) dz

√ q
≤ 2 T T (β + 1)Cγ

 1 Z
2

1

2

XT

1[0,(β+1)γy ] (z) ds ρ(dy) dz

by the Cauchy–Schwarz inequality and Condition 2.1. It then follows from the convergence of
ei , ξeij , νei ) : i, j ∈ N}
stochastic integrals (cf. [21, Theorem 4.2]), (5.3), (5.4), and (5.6) that {(X
is a solution to the limiting system (2.1). From the pathwise uniqueness established earlier it
now follows that there exists a pathwise solution of (2.1).
e m , ξem , νem ) given in (5.3) and (5.4). We claim that the
(b) Recall the discretized system (X
i
ij
i
following holds for each k ∈ N0 :
e m [ k ] : i ∈ N} are i.i.d.;
(i) {X
i m
e m ) : i, j ∈ N};
e m [ k ] : i ∈ N} are independent of {(ξij (0), N
(ii) {X
ij
i m
e m ) : j ∈ N, j 6= i} are i.i.d. conditioning on X
e m [ k ], and the condie m [ k ], ξem [ k ], N
(iii) {(X
ij m
ij
i m
j m
e m [ k ], ξem [ k ], N
e m ) : j ∈ N, j 6= i} | X
e m [ k ]) = Φk,m(X
e m [ k ]) for some
tional law L({(X
j m
ij m
ij
i m
i m
k
k
k
] : Z) → P((D([0, m
] : Z) × D([0, m
] : Z) × MT )∞ )
measurable map Φk,m : D([0, m
independent of the choice of i.

Again, we will prove this by induction. Since (Xi (0)) and (ξij (0)) are all i.i.d., the claim holds
for k = 0. Now assume the claim holds for some k ∈ N0 . From (iii) we have
νeim (

k
e m ( k ), ξem ( k )) | X
e m [ k ]) = Φ
e k,m(X
e m [ k ])
) = L((X
j
ij
i
i
m
m
m
m
m

e k,m : D([0, k ] : Z) → P(Z2 ) independent of the choice of i.
for some measurable map Φ
m
e m we see that {X
e m [ k+1 ] : i ∈ N} are i.i.d., and
From (i), (ii) and the evolution of X
i
i
m
m
e ) : i, j ∈ N}. It then follows from the evolution of ξem that
independent of {(ξij (0), N
ij
ij
k+1 em k+1 e m
k+1
m
m
e
e
{(Xj [ m ], ξij [ m ], Nij ) : j ∈ N, j 6= i} are i.i.d. conditioning on Xi [ m ], and the cone m [ k+1 ], ξem [ k+1 ], N
e m ) : j ∈ N, j 6= i} | X
e m [ k+1 ]) = Φk+1,m (X
e m [ k+1 ]) for
ditional law L({(X
j
ij m
ij
i
i
m
m
m
k+1
k+1
k+1
some measurable map Φk+1,m : D([0, m ] : Z) → P((D([0, m ] : Z) × D([0, m ] : Z) × MT )∞ )
independent of the choice of i. Therefore the claim holds for k + 1 and hence holds for each
k ∈ N0 by induction.
Now from the convergence (5.10) we have that {Xi : i ∈ N} are i.i.d., and independent of
{(ξij (0), Nij ) : i, j ∈ N}. From the evolution of ξij we see that ξij = Ψ(ξij (0), Xi , Xj , Nij ) for
some measurable map Ψ. Therefore {(Xj [t], ξij [t]) : j ∈ N, j 6= i} are i.i.d. conditioning on
Xi [t], and νi (t) = L((ξij (t), Xj (t)) | Xi [t]) = Φt (Xi [t]) for some measurable map Φt : D([0, t] :
Z) → P(Z2 ) independent of the choice of i. This gives (b) and completes the proof.


20

BAYRAKTAR AND WU

Proof of Theorem 2.2. (a) For each fixed i ∈ [n] and t ∈ [0, T ], we have

EkXin − Xi k∗,t
Z
|y| 1[0,Γ(y,Xin (s−),νin (s−))] (z) − 1[0,Γ(y,Xi (s−),νi (s−))] (z) Ni (ds dy dz)
≤E
Xt
Z
|y| |Γ(y, Xin (s), νin (s)) − Γ(y, Xi (s), νi (s))| ds ρ(dy).
=E
[0,t]×Z

Since γ is Lipschitz by Condition 2.1(i), and {(Xj (s), ξij (s)) : j ∈ N, j 6= i} are conditionally
independent given Xi [s] with the conditional law L((Xj (s), ξij (s))|Xi [s]) = νi (s), for j 6= i,
by Theorem 2.1(b), we have
E|Γ(y, Xin (s), νin (s)) − Γ(y, Xi (s), νi (s))|
≤E

n

n

j=1

j=1

1X
1X
n
γ(y, Xin (s), Xjn (s), ξij
(s)) −
γ(y, Xi (s), Xj (s), ξij (s))
n
n

Z
n
1X
e νi (s)(de
e
+E
γ(y, Xi (s), x
e, ξ)
x dξ)
γ(y, Xi (s), Xj (s), ξij (s)) −
n
2
Z
j=1


n
n
X
X
1
κ1
1
n
E|Xjn (s) − Xj (s)| +
E|ξij
(s) − ξij (s)| + √ 
≤ γy E|Xin (s) − Xi (s)| +
n
n
n
j=1
j=1


κ1
n
− ξij k∗,s + √
≤ γy 2 max EkXin − Xi k∗,s + max Ekξij
,
n
i∈[n]
i,j∈[n]
κ1
e is Lipschitz
in the second inequality follows from Lemma 5.1. Also since Γ
where the term √
n
by Condition 2.1(i), we have
n
Ekξij
− ξij k∗,t
Z
|y| 1[0,β(n)Γ(y,ξ
(z) − 1[0,β Γ(y,ξ
(z) Nij (ds dy dz)
≤E
n
n
n
e
e
ij (s−),Xi (s−),Xj (s−))]
ij (s−),Xi (s−),Xj (s−))]
ZXt
e ξ n (s), X n (s), X n (s)) − β Γ(y,
e ξij (s), Xi (s), Xj (s)) ds ρ(dy)
|y| β(n)Γ(y,
=E
ij
i
j
[0,t]×Z
Z
|y| [γy |β(n) − β|
≤E
[0,t]×Z


n
(s) − ξij (s)| + |Xin (s) − Xi (s)| + |Xjn (s) − Xj (s)| ds ρ(dy)
+γy β |ξij
Z t
Z t
n
≤ Cγ |β(n) − β|t + Cγ β
− ξij k∗,s ds + 2Cγ β
max Ekξij
max EkXin − Xi k∗,s ds.
0 i,j∈[n]

0 i∈[n]

Combining above three displays gives

n
max EkXin − Xi k∗,t + max Ekξij
− ξij k∗,t
i∈[n]
i,j∈[n]

Z t
κ1 Cγ t
n
n
≤ 2Cγ (β + 1)
max EkXi − Xi k∗,s + max Ekξij − ξij k∗,s ds + √ + Cγ |β(n) − β|t.
n
i∈[n]
i,j∈[n]
0

From Gronwall’s inequality we have (2.3).
(b) Using (a), Theorem 2.1(b) and a standard argument (see [29, Chapter 1]) one has (2.4).

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

21

(c) Fix t ∈ [0, T ] and i ∈ N. First note that νi is well defined, thanks to the conditional
i.i.d. property in Theorem 2.1(b) of {(Xj , ξij ) : j ∈ N, j 6= i} given Xi . Let
n

ν̄in (t)

1X
δ(Xi (t),ξij (t)) ,
:=
n

n

1X
:=
δ(Xi (·),ξij (·)) .
n

ν̄in

j=1

j=1

From (2.3) we have
EdBL (νin (t), ν̄in (t)) ≤
EdBL (νin , ν̄in ) ≤

n

1X
n
E |Xjn (t) − Xj (t)| + |ξij
(t) − ξij (t)| → 0,
n

1
n

j=1
n
X
j=1


n
E kXjn − Xj k∗,T + kξij
− ξij k∗,T → 0.

Using the definition of νi (t) in (2.1) and νi in (2.5), we have
ν̄in (t) → νi (t),

ν̄in → νi a.s.

Combining these gives (2.5) and (2.6). The last two statements (2.7) and (2.8) follow immediately from (2.5) and (2.6), respectively.

5.2. Proofs of Theorems 3.1 and 3.2.
Proof of Theorem 3.1. (a) Since the limiting system is McKean–Vlasov, the proof of existence
and uniqueness is standard (cf. [29, Chapter 1], see also [15, Theorem 2.1]) and hence omitted.
(b) It would be helpful to freeze the slow components X and analyze the averaging effect
of the fast component ξ first. Consider the following auxiliary process with ∆ = ∆(n) → 0
(whose precise value will be stated later):


t
∆
Xi (t) = Xi ⌊ ⌋∆ ,
∆

 Z
t
n,∆
n
ξij (t) = ξij ⌊ ⌋∆ +
y1[0,β(n)Γ(y,ξ
(z) Nij (ds dy dz).
n,∆
∆
∆
e
ij (s−),Xi (s−),Xj (s−))]
∆
[⌊ t ⌋∆,t]×Z×R+
∆

Note that for each t ∈ [0, T ],
E

sup
t
⌋∆,t]
s∈[⌊ ∆

|Xi∆ (s) −

Xi (s)| ≤ E
≤∆

Z

t
[⌊ ∆
⌋∆,t]×Z×R+

Z

R+

|y|1[0,Γ(y,Xi (s−),νi (s−))] (z) Ni (ds dy dz)

|y|γy ρ(dy) = Cγ ∆

(5.11)

by Condition 2.1(i). From this we have
t
⌋∆, t])
∆
|Xin (s) − Xi (s)|

n,∆
n
(t) 6= ξij
(t)) ≤ P((Xi∆ (s), Xj∆ (s)) 6= (Xin (s), Xjn (s)) for some s ∈ [⌊
P(ξij

≤ 2E

sup
t
s∈[⌊ ∆
⌋∆,t]

|Xi∆ (s) − Xi (s)| + 2E

≤ 2Cγ ∆ + 2EkXin − Xi k∗,t .

sup
t
s∈[⌊ ∆
⌋∆,t]

(5.12)

Also note that from Condition 2.1(i) and Condition 3.1(ii) we have
EkXi∆ k2∗,T ≤ EkXi k2∗,T ≤ κ1 (E|Xi (0)|2 + Cγ2 + Cγ,2 ) < ∞.

(5.13)

22

BAYRAKTAR AND WU

Let
n,∆
(s)) −
(s) := γ(y, Xi∆ (s), Xj∆ (s), ξij
Qn,∆,y
i,j

Z

Z

e Q(X ∆ (s), X ∆ (s), dξ).
e
γ(y, Xi∆ (s), Xj∆ (s), ξ)
i
j



 n s
 n

s
⌊ ∆ ⌋∆ , ξik
⌋∆ , ξij
(s) and
⌊ ∆s ⌋∆ , clearly Qn,∆,y
Given Xi ⌊ ∆s ⌋∆ , Xj ⌊ ∆s ⌋∆ , Xk ⌊ ∆
i,j
Qn,∆,y
(s) are conditionally independent. Using this, (5.13), Condition 3.1(i), and an api,k
plication of time change s 7→ s (so that the evolution of ξ n,∆ matches that of Ye introduced
ij

β(n)

in Condition 3.1) we have

h
i
n,∆,y
E Qn,∆,y
(s)Q
(s)
i,j
i,k
 s
 s
 s


 s


i
 s
n h
n
n
⌊
⌊
⌊
⌋∆
,
X
⌋∆
,
X
⌊
⌋∆
,
ξ
⌋∆
,
ξ
⌋∆
⌊
= E E Qn,∆,y
(s)
X
j
i
k
ij
ik
i,j
 s∆ 
h
 s∆ 
 s∆ io
 s∆ 
 s∆ 
n,∆,y
n
n
·E Qi,k (s) Xi ⌊ ⌋∆ , Xj ⌊ ⌋∆ , Xk ⌊ ⌋∆ , ξij ⌊ ⌋∆ , ξik ⌊ ⌋∆
∆
∆
∆
∆
∆





s
∆
∆
2
∆
∆
2
≤κ
e β(n) s − ⌊ ⌋∆ γy E 1 + kXi k∗,T + kXj k∗,T 1 + kXi k∗,T + kXk k∗,T

∆ s

2 2
≤ κ2 γy κ
e β(n) s − ⌊ ⌋∆
∆

for each i, j, k ∈ [n] with j 6= k. Therefore

2 

n
n
n
n
2 
i
X
1 X
1 X X h n,∆,y
1
n,∆,y
n,∆,y
n,∆,y




E Qi,j (s)
Qi,j (s)
E Qi,j (s)Qi,k (s) + 2
= 2
E
n
n
n


j=1 k6=j

j=1



≤ κ2 γy2 κ
e2 β(n) s − ⌊

and hence
Z

n

k∆

E
(k−1)∆

j=1



1 X n,∆,y
Qi,j (s) ds ≤
n
j=1

Z

k∆
(k−1)∆

√



s
⌋∆
∆



+

4γy2
n

,



 2γ
√
s
y
κ2 γy κ
e β(n) s − ⌊ ⌋∆ + √
∆
n

Z
κ2 γy β(n)∆
2∆γy
κ
e(s) ds + √
=
β(n) 0
n
2∆γy
κ3 γy
+ √
≤
β(n)
n



for each k ∈ N.
Now we show (3.2). Note that
EkXin − Xi k∗,t
Z
|y| 1[0,Γ(y,Xin (s−),νin (s−))] (z) − 1[0,Γ(y,Xi (s−),νi (s−))] (z) Ni (ds dy dz)
≤E
Xt
Z
|y| |Γ(y, Xin (s), νin (s)) − Γ(y, Xi (s), νi (s))| ds ρ(dy).
=E
[0,t]×Z

ds

(5.14)

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

23

Fixing y ∈ N and s ∈ [0, T ], we have

E|Γ(y, Xin (s), νin (s)) − Γ(y, Xi (s), νi (s))|
Z
n
1X
n
n
n
e µs (de
e
γ(y, Xi (s), x
e, ξ)
x) Q(Xi (s), x
e, dξ)
γ(y, Xi (s), Xj (s), ξij (s)) −
=E
n
Z2
j=1

≤

1
n

n
X
j=1

n,∆
n
E γ(y, Xin (s), Xjn (s), ξij
(s)) − γ(y, Xi∆ (s), Xj∆ (s), ξij
(s))


Z
n 
1X
n,∆
∆
∆
∆
∆
∆
∆
e
e
γ(y, Xi (s), Xj (s), ξij (s)) − γ(y, Xi (s), Xj (s), ξ) Q(Xi (s), Xj (s), dξ)
+E
n
Z
j=1

Z
n
1X
e Q(X ∆ (s), X ∆ (s), dξ)
e
+
E
γ(y, Xi∆ (s), Xj∆ (s), ξ)
i
j
n
Z
j=1
Z
e Q(Xi (s), Xj (s), dξ)
e
− γ(y, Xi (s), Xj (s), ξ)
Z

n Z
1X
e Q(Xi (s), Xj (s), dξ)
e
γ(y, Xi (s), Xj (s), ξ)
+E
n
j=1 Z
Z
e µs (de
e .
γ(y, Xi (s), x
e, ξ)
x) Q(Xi (s), x
e, dξ)
−

(5.15)

Z2

Next we analyze each of the four terms on the RHS of (5.15). For the first term, it follows
from Condition 2.1(i), the exchangeability of {(Xjn , Xj∆ , Xj ) : j ∈ N}, (5.11) and (5.12) that
n,∆
n
E γ(y, Xin (s), Xjn (s), ξij
(s)) − γ(y, Xi∆ (s), Xj∆ (s), ξij
(s))

≤ γy E|Xin (s) − Xi (s)| + E|Xi∆ (s) − Xi (s)| + E|Xjn (s) − Xj (s)| + E|Xj∆ (s) − Xj (s)|

n,∆
n
+P(ξij
(s) 6= ξij
(s))
≤ 4γy EkXin − Xi k∗,s + 4γy Cγ ∆.

For the second term, from (5.14) we have
Z

t

E

0

≤


Z
n 
1X
n,∆
e Q(X ∆ (s), X ∆ (s), dξ)
e ds
(s)) − γ(y, Xi∆ (s), Xj∆ (s), ξ)
γ(y, Xi∆ (s), Xj∆ (s), ξij
i
j
n
Z
j=1

t
⌊∆
⌋

XZ
k=1

≤ κ3

n

1 X n,∆,y
Qi,j (s) ds + ∆γy
E
n
(k−1)∆
k∆

j=1

γy t
2γy t
+ √ + ∆γy .
β(n)∆
n

For the third term, we have
Z
Z
∆
∆
∆
∆
e Q(Xi (s), Xj (s), dξ)
e
e
e
E
γ(y, Xi (s), Xj (s), ξ) Q(Xi (s), Xj (s), dξ) − γ(y, Xi (s), Xj (s), ξ)
Z
Z

≤ γy P(Xi∆ (s) 6= Xi (s)) + P(Xj∆ (s) 6= Xj (s)) ≤ 2γy Cγ ∆,

24

BAYRAKTAR AND WU

where the last inequality follows from (5.11). For the fourth term, since Xj are i.i.d. with law
µ, from Lemma 5.1 we have
n

E

1X
n
j=1

Z

κ4 γy
≤ √ .
n

Z

e Q(Xi (s), Xj (s), dξ)
e −
γ(y, Xi (s), Xj (s), ξ)

Z

Z2

e µs (de
e
γ(y, Xi (s), x
e, ξ)
x) Q(Xi (s), x
e, dξ)

Combining all of the above estimates and using Condition 2.1(i) gives
Z t
(κ4 + 2)Cγ t
Cγ t
√
+ Cγ ∆ + 6Cγ2 t∆ +
.
EkXin − Xi k∗,s ds + κ3
EkXin − Xi k∗,t ≤ 4Cγ
β(n)∆
n
0
Using Gronwall’s inequality and taking ∆ = ∆(n) = √ 1

β(n)

, we have (3.2).

(c) Using (b), the independence of {Xi } and a standard argument (see [29, Chapter 1]) one
has (3.3).
(d) Fix t ∈ [0, T ]. Let
n

n

1X
δXi (·) ,
µ̄ :=
n

1X
µ̄ (t) :=
δXi (t) .
n
n

n

i=1

i=1

From (3.2) we have
n

EdBL (µn (t), µ̄n (t)) ≤
EdBL (µn , µ̄n ) ≤

1X
E|Xin (t) − Xi (t)| → 0,
n
1
n

i=1
n
X
i=1

EkXin − Xi k∗,T → 0.

Also note that
µ̄n (t) → µ(t),

µ̄n → µ,

in probability by independence of Xi . Combining these gives (3.4).
(e) Fix t ∈ (0, T ]. Take ∆ = ∆(n) = O( √ 1 ) such that (k + 21 )∆ ≤ t < (k + 1)∆ for some
β(n)

k = k(n) ∈ N. Define

n

νin,∆,1 (t)

1X
δ(X ∆ (t),ξ n,∆ (t)) ,
=
j
ij
n

νin,∆,2 (t) =
νin,∆,3 (t) =
νin,∆,4 (t) =

1
n
1
n
1
n

j=1
n
X
j=1
n
X
j=1
n
X
j=1

n,∆
n
δX ∆ (t) ⊗ L(ξij
(t) | ξij
(k∆), Xi∆ (t), Xj∆ (t)),
j

δX ∆ (t) ⊗ Q(Xi∆ (t), Xj∆ (t), ·),
j

δXj (t) ⊗ Q(Xi (t), Xj (t), ·).

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

25

From (3.2), (5.11) and (5.12) we have
n

EdBL (νin (t), νin,∆,1 (t))

1X
n,∆
n
(t)) − f (Xj∆ (t), ξij
(t))
sup f (Xjn (t), ξij
≤
n
f ∈B1
≤

1
n

j=1
n 
X
j=1


n,∆
n
(t)) → 0,
E|Xjn (t) − Xj∆ (t)| + 2P(ξij
(t) 6= ξij

n
1X
n,∆,3
n,∆,4
EdBL (νi
(t), νi
(t)) ≤
P(Xj∆ (t) 6= Xj (t)) → 0.
n
j=1

n,∆
Using the conditional independence of {ξij
(t) : j ∈ [n]} given {Xj∆ (t) : j ∈ [n]}, we have


2 
n,∆,1
n,∆,2
(t)i − hf, νi
(t)i
E hf, νi


n
2 
1 X
n,∆
n,∆
∆
n
∆
∆
= 2
E f (Xj (t), ξij (t)) − hf, δX ∆ (t) ⊗ L(ξij (t) | ξij (k∆), Xi (t), Xj (t))i
j
n
j=1

kf k2∞
≤
→0
n

for each bounded and continuous function f . Similarly, using the independence of {Xi : i ∈
[n]} and the weak law of large numbers we have
νin,∆,4 (t) → νi (t)
n,∆
in probability. Lastly, from (3.5) and the definition of ξij
we have

E hf, νin,∆,2 (t)i − hf, νin,∆,3 (t)i


n
2kf k∞ X X
n,∆
n
e 
E
P(ξij
(t) = ξe| ξij
(k∆), Xi∆ (t), Xj∆ (t)) − Q(Xi∆ (t), Xj∆ (t), {ξ})
≤
n
j=1

≤

e
ξ∈Z

n

2kf k∞ X 
E C(β(n)(t − k∆))(1 + |Xi∆ (t)| + |Xj∆ (t)|) → 0
n
j=1

≤ κ5 C(β(n)(t − k∆)) → 0

for each bounded and continuous function f . Combining above estimates gives (3.6). Finally,
(3.7) is a direct consequence of (3.6).

Remark 5.1. If we are only interested in the convergence of Xin , µn , νin , then it would be
Rt
R∞
e(s) ds = 0, instead of 0 κ
e(t) dt < ∞. We only have
sufficient to assume that limt→∞ 1t 0 κ
R β(n)∆
to replace κ3 by κ3 0
κ
e(s) ds in (5.14) and in what follows.

Proof of Theorem 3.2. The proof is quite similar to that of Theorem 3.1, except that νin there
is replaced by νiβ here. So we will omit certain common arguments.

26

BAYRAKTAR AND WU

(a) First consider the following auxiliary process with ∆ = ∆(β) → 0 (whose precise value
will be stated later):

t
= Xi ⌊ ⌋∆ ,
∆
 Z

t
β,∆
β
y1[0,β Γ(y,ξ
(z) Nij (ds dy dz).
ξij (t) = ξij ⌊ ⌋∆ +
β,∆
∆
∆
e
ij (s−),Xi (s−),Xj (s−))]
∆
[⌊ t ⌋∆,t]×Z×R+


Xi∆ (t)

∆

Using Condition 2.1(i), one can check that
|Xi∆ (s) − Xi (s)| ≤ Cγ ∆,

(5.16)

β,∆
β
P(ξij
(t) 6= ξij
(t)) ≤ 2Cγ ∆ + 2EkXiβ − Xi k∗,t .

(5.17)

sup

E

t
s∈[⌊ ∆
⌋∆,t]

s
β

(so that the

2∆γy
κ1 γy
+ √
β
n

(5.18)

From this, (5.13), Condition 3.1(i) and an application of time change s 7→
β,∆
evolution of ξij
matches that of Ye ) we have
Z

E

(k−1)∆

−

1 X
β,∆
(s))
γ(y, Xi∆ (s), Xj∆ (s), ξij
n
n

k∆

Z

Z

j=1



e Q(X ∆ (s), X ∆ (s), dξ)
e
γ(y, Xi∆ (s), Xj∆ (s), ξ)
i
j

ds ≤

for each k ∈ N.
Now we show (3.8). Since Xi are i.i.d. with law µ, we can write
n

1X
e
δXj (t) (de
x) ⊗ Q(Xi (t), Xj (t), dξ).
n→∞ n

e = lim
νi (t)(de
x dξ)

j=1

Hence
EkXiβ

− Xi k∗,t ≤ E
≤

Z

Z

[0,t]×Z

|y| Γ(y, Xiβ (s), νiβ (s)) − Γ(y, Xi (s), νi (s)) ds ρ(dy)

1 X
β
γ(y, Xiβ (s), Xjβ (s), ξij
(s))
n→∞
n
[0,t]×Z
j=1

Z
e
e
− γ(y, Xi (s), Xj (s), ξ) Q(Xi (s), Xj (s), dξ) ds ρ(dy),
n

|y| lim E

Z

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

27

where the last line uses the dominated convergence theorem. For each y ∈ Z, j ∈ N and
s ∈ [0, T ],

Z
n 
1X
β
β
β
e
e
γ(y, Xi (s), Xj (s), ξij (s)) − γ(y, Xi (s), Xj (s), ξ) Q(Xi (s), Xj (s), dξ)
E
n
Z
j=1
n

1X
β
β,∆
≤
E γ(y, Xiβ (s), Xjβ (s), ξij
(s)) − γ(y, Xi∆ (s), Xj∆ (s), ξij
(s))
n
j=1


Z
n 
1X
β,∆
∆
∆
∆
∆
∆
∆
e
e
γ(y, Xi (s), Xj (s), ξij (s)) − γ(y, Xi (s), Xj (s), ξ) Q(Xi (s), Xj (s), dξ)
+E
n
Z
j=1

Z
n
1X
e Q(X ∆ (s), X ∆ (s), dξ)
e
+
E
γ(y, Xi∆ (s), Xj∆ (s), ξ)
i
j
n
Z
j=1
Z
e Q(Xi (s), Xj (s), dξ)
e .
− γ(y, Xi (s), Xj (s), ξ)
Z

Similar to the analysis of first three terms on the RHS of (5.15), it follows from Condition
2.1(i), (5.16), (5.17) and (5.18) that
β
β,∆
E γ(y, Xiβ (s), Xjβ (s), ξij
(s)) − γ(y, Xi∆ (s), Xj∆ (s), ξij
(s))

≤ 4γy EkXiβ − Xi k∗,s + 4γy Cγ ∆,

Z t
Z
n 
1X
β,∆
∆
∆
∆
∆
∆
∆
e
e
E
γ(y, Xi (s), Xj (s), ξij (s)) − γ(y, Xi (s), Xj (s), ξ) Q(Xi (s), Xj (s), dξ) ds
n
0
Z
j=1

E

Z

Z

≤ κ1

γy t 2γy t
+ √ + ∆γy ,
β∆
n

e Q(X ∆ (s), X ∆ (s), dξ)
e −
γ(y, Xi∆ (s), Xj∆ (s), ξ)
i
j

≤ 2γy Cγ ∆.

Z

Z

e Q(Xi (s), Xj (s), dξ)
e
γ(y, Xi (s), Xj (s), ξ)

Combining all of the above estimates and using Condition 2.1(i) gives
EkXiβ

− Xi k∗,t ≤ 4Cγ

Z

t
0

EkXiβ − Xi k∗,s ds + κ1

Using Gronwall’s inequality and taking ∆ = ∆(β) =
Using the definition of dBL and (3.8), we have

√1
β

Cγ t
+ Cγ ∆ + 6Cγ2 t∆.
β∆

we have (3.8).

κ
dBL (µβ , µ) = sup Ef (Xiβ ) − Ef (Xi ) ≤ EkXiβ − Xi k∗,T ≤ √ .
β
f ∈B1
This gives (3.9).

28

BAYRAKTAR AND WU

(b) Fix t ∈ (0, T ]. Take ∆ = ∆(β) = O( √1β ) such that (k + 21 )∆ ≤ t < (k + 1)∆ for some
k = k(β) ∈ N. Define
n

1X
δ(X ∆ (t),ξ β,∆ (t)) ,
n→∞ n
j
ij

νiβ,∆,1 (t) = lim

νiβ,∆,2 (t) = lim

n→∞

1
n

1
n→∞ n

νiβ,∆,3 (t) = lim

νiβ,∆,4 (t) = lim

n→∞

1
n

j=1
n
X
j=1
n
X
j=1
n
X
j=1

β,∆
β
(t) | ξij
(k∆), Xi∆ (t), Xj∆ (t)),
δX ∆ (t) ⊗ L(ξij
j

δX ∆ (t) ⊗ Q(Xi∆ (t), Xj∆ (t), ·),
j

δXj (t) ⊗ Q(Xi (t), Xj (t), ·).

From (3.8), (5.16) and (5.17) we have
EdBL (νiβ (t), νiβ,∆,1 (t)) ≤ lim inf
n→∞

n

1 X
β
β,∆
E|Xjβ (t) − Xj∆ (t)| + 2P(ξij
(t) 6= ξij
(t)) → 0,
n
j=1

n
1X
EdBL (νiβ,∆,3 (t), νiβ,∆,4 (t)) ≤ lim inf
P(Xj∆ (t) 6= Xj (t)) → 0,
n→∞ n
j=1

β,∆
(t) : j ∈ N} given {Xj∆ (t) : j ∈ N},
as β → ∞. Using the conditional independence of {ξij
we have

2 
E hf, νiβ,∆,1 (t)i − hf, νiβ,∆,2 (t)i


n
2 
1 X
β
β,∆
β,∆
∆
∆
∆
≤ lim inf 2
E f (Xj (t), ξij (t)) − hf, δX ∆ (t) ⊗ L(ξij (t) | ξij (k∆), Xi (t), Xj (t))i
j
n→∞ n
j=1

=0

for each bounded and continuous function f . This means
νiβ,∆,1 (t) = νiβ,∆,2 (t) a.s.
Similarly, using the independence of {Xi : i ∈ N} and the weak law of large numbers we have
νiβ,∆,4 (t) = νi (t) a.s.

β,∆
Moreover, from (3.5) and the definition of ξij
we have

E hf, νiβ,∆,2 (t)i − hf, νiβ,∆,3 (t)i


n
X
X
2kf k∞
β,∆
β
e 
≤ lim inf
E
P(ξij
(t) = ξe| ξij
(k∆), Xi∆ (t), Xj∆ (t)) − Q(Xi∆ (t), Xj∆ (t), {ξ})
n→∞
n
j=1

≤ lim inf
n→∞

e
ξ∈Z

n

2kf k∞ X 
E C((t − k∆)β)(1 + |Xi∆ (t)| + |Xj∆ (t)|) → 0
n
j=1

≤ κ2 C((t − k∆)β) → 0

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

29

as β → ∞, for each bounded and continuous function f . Combining above estimates gives
(3.10). Finally, (3.11) is a direct consequence of (3.10).

Remark 5.2. If we are only interested in the convergence of Xiβ , µβ , νiβ , then it would be
Rt
R∞
e(s) ds = 0, instead of 0 κ
e(t) dt < ∞. We only have
sufficient to assume that limt→∞ 1t 0 κ
R β∆
to replace κ1 by κ1 0 e
κ(s) ds in (5.18) and in what follows.
6. Proofs of central limit theorems

In this section we prove Theorems 4.1 and 4.2.
6.1. Asymptotics of symmetric statistics. The proof of CLT crucially relies on certain
classical results from [14] on limit laws of degenerate symmetric statistics. In this section we
briefly review these results.
Let S be a Polish space and let {Yn }∞
n=1 be a sequence of i.i.d. S-valued random variables
having common probability law θ̄. For k ∈ N, let L2 (θ̄ ⊗k ) be the space of all real-valued
square integrable functions on (Sk , B(S)⊗k , θ̄ ⊗k ). Denote by L2c (θ̄ ⊗k ) the subspace of centered
functions, namely φ ∈ L2 (θ̄ ⊗k ) such that for all 1 ≤ j ≤ k,
Z
φ(x1 , . . . , xj−1 , x, xj+1 , . . . , xk ) θ̄(dx) = 0, θ̄ ⊗k−1 a.e. (x1 , . . . , xj−1 , xj+1 , . . . , xk ).
S

Denote by L2sym (θ̄ ⊗k ) the subspace of symmetric functions, namely φ ∈ L2 (θ̄ ⊗k ) such that for
every permutation π on {1, . . . , k},
φ(x1 , . . . , xk ) = φ(xπ(1) , . . . , xπ(k) ),

θ̄ ⊗k a.e. (x1 , . . . , xk ).

Also, denote by L2c,sym (θ̄ ⊗k ) the subspace of centered symmetric functions in L2 (θ̄ ⊗k ), namely
T
L2c,sym(θ̄ ⊗k ) := L2c (θ̄ ⊗k ) L2sym (θ̄ ⊗k ). Given φk ∈ L2sym (θ̄ ⊗k ) define the symmetric statistic
Ukn (φk ) as
X

φk (Yi1 , . . . , Yik ) for n ≥ k

Ukn (φk ) := 1≤i1 <i2 <···<ik ≤n

0
for n < k.
In order to describe the asymptotic distributions of such statistics consider a Gaussian field
{I1 (h) : h ∈ L2 (θ̄)} such that
h, g ∈ L2 (θ̄).

E (I1 (h)) = 0, E (I1 (h)I1 (g)) = hh, giL2 (θ̄) ,

For h ∈ L2 (θ̄), define φhk ∈ L2sym (θ̄ ⊗k ) as

φhk (x1 , . . . , xk ) := h(x1 ) . . . h(xk )

and set φh0 := 1.
The multiple Wiener integral (MWI) of φhk , denoted as Ik (φhk ), is defined through the
following formula. For k ≥ 1,
Ik (φhk )

:=

⌊k/2⌋

X
j=0

(−1)j Ck,j ||h||2j
(I (h))k−2j , where Ck,j :=
L2 (θ̄) 1

k!
, j = 0, . . . , ⌊k/2⌋.
(k − 2j)!2j j!

The following representation gives an equivalent way to characterize the MWI of φhk :


∞ k
X
t2
t
h
2
Ik (φk ) = exp tI1 (h) − ||h||L2 (θ̄) , t ∈ R,
k!
2
k=0

30

BAYRAKTAR AND WU

where we set I0 (φh0 ) := 1. We extend the definition of Ik to the linear span of {φhk , h ∈ L2 (θ̄)}
by linearity. It can be checked that for all f in this linear span,
E(Ik (f ))2 = k! ||f ||2L2 (θ̄⊗k ) .

(6.1)

Using this identity and standard denseness arguments, the definition of Ik (f ) can be extended
to all f ∈ L2sym (θ̄ ⊗k ) and the identity (6.1) holds for all f ∈ L2sym (θ̄ ⊗k ). The following result
is taken from [14].
⊗k ) for each
2
Lemma 6.1 (Dynkin-Mandelbaum [14]). Let {φk }∞
k=1 be such that φk ∈ Lc,sym (θ̄
k ≥ 1. Then the following convergence holds as n → ∞:


 k

1
−2 n
n Uk (φk )
Ik (φk )
⇒
k!
k≥1
k≥1

as a sequence of R∞ -valued random variables.

6.2. Girsanov change of measure. Recall the probability space (Ω, F, P) under which
systems (1.1) and (2.1) are defined. Recall the canonical spaces and processes introduced in
Section 4.1. Let
n
1X
δ(Xj (t),ξij (t)) .
ν̄in (t) =
n
j=1

Define

n

J (t) :=

Z
n
X
i=1

where

Xt

n,i
(y, z)Ni (ds dy dz)
rs−

rsn,i (y, z) := 1[0,Γ(y,Xi (s),νi (s))] (z) log

−

Z

[0,t]×Z

!

en,i
s (y) ds ρ(dy)

,

Γ(y, Xi (s), ν̄in (s))
,
Γ(y, Xi (s), νi (s))

n
n
en,i
s (y) := Γ(y, Xi (s), ν̄i (s)) − Γ(y, Xi (s), νi (s)) = Γ(y, Xi (s), ν̄i (s) − νi (s)).

Let F̄tn := σ{Ni (A), Xi [t], ξij [t] : i, j ∈ [n], A ∈ B(Xt )}. Note that {exp(J n (t))} is an F̄tn martingale under P n . Define a new probability measure Qn on Ωn by
dQn
:= exp (J n (T )) .
dP n
By Girsanov’s Theorem, {Xi , ξij : i, j ∈ [n]} has the same probability distribution under
n : i, j ∈ [n]} under P. Let
Qn as {Xin , ξij


n
X
√
1
ϕ(Xj , ξ1j ) − mϕ (X1 ) , ϕ ∈ A,
η̄ n (ϕ) := n 
n
j=1


n
X
√
1
η̄xn (ϕ) := n 
ϕ(Xj ) − Eϕ(X1 ) , ϕ ∈ Ax .
n
j=1

Thus in order to prove Theorems 4.1 and 4.2 it suffices to show that


Z
√

1 ϕ 2
n
exp − (σω0 ) P0 (dω0 ), ϕ ∈ A,
lim EQn exp −1η̄ (ϕ)) =
n→∞
2
Ω0


√

1
−1
2
n
lim EQn exp −1η̄x (ϕ)) = exp − k(I − A) ΦkH , ϕ ∈ Ax ,
n→∞
2

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

which is equivalent to showing
Z
√

n
n
lim EP n exp −1η̄ (ϕ) + J (T ) =


1 ϕ 2
exp − (σω0 ) P0 (dω0 ), ϕ ∈ A,
n→∞
2
Ω0


√

1
−1
2
n
n
lim EP n exp −1η̄x (ϕ) + J (T ) = exp − k(I − A) ΦkH , ϕ ∈ Ax .
n→∞
2
n
For this we will need to study the asymptotics of J as n → ∞.


31

(6.2)
(6.3)

6.3. Asymptotics of J n . Now we analyze the asymptotics of J n . Recall the constant ε
from Condition 4.1. From Taylor’s expansion, there exists a κ0 ∈ (0, ∞) such that for all
α, β ∈ [ε, 1/ε],
α
α
1 α
α
log = ( − 1) − ( − 1)2 + ϑ(α, β)( − 1)3 ,
β
β
2 β
β
n,i
n
where |ϑ(α, β)| ≤ κ0 . Letting ϑs (y) := ϑ(Γ(y, Xi (s), ν̄i (s)), Γ(y, Xi (s), νi (s))), we have


2

Γ(y, Xi (s), ν̄in (s))
Γ(y, Xi (s), ν̄in (s))
1 Γ(y, Xi (s), ν̄in (s))
log
=
−1 −
−1
Γ(y, Xi (s), νi (s))
Γ(y, Xi (s), νi (s))
2 Γ(y, Xi (s), νi (s))

3
Γ(y, Xi (s), ν̄in (s))
n,i
+ ϑs (y)
−1 .
Γ(y, Xi (s), νi (s))
ei be the compensated PRM of Ni , we have
Letting N
n Z
n Z
X
X
n,i
en,i
(y, z)Ni (ds dy dz) −
rs−
J n (T ) =
s (y) ds ρ(dy)
=

i=1 XT
n Z
X

i=1

[0,T ]×Z




Γ(y, Xi (s−), ν̄in (s−))
ei (ds dy dz)
−1 N
Γ(y, Xi (s−), νi (s−))
X
T
i=1
2

n Z
Γ(y, Xi (s−), ν̄in (s−))
1X
− 1 Ni (ds dy dz)
−
1[0,Γ(y,Xi (s−),νi (s−))] (z)
2
Γ(y, Xi (s−), νi (s−))
i=1 XT

3
n Z
X
Γ(y, Xi (s−), ν̄in (s−))
n,i
1[0,Γ(y,Xi (s−),νi (s−))] (z)ϑs− (y)
+
− 1 Ni (ds dy dz)
Γ(y, Xi (s−), νi (s−))
XT
1[0,Γ(y,Xi (s−),νi (s−))] (z)

i=1

1
=: J n,1 − J n,2 + J n,3 .
2
First we analyze J n,3 .

(6.4)

Lemma 6.2. EP n |J n,3 | → 0 as n → ∞.

Proof. Since |ϑn,i
s (y)| ≤ κ0 and Γ is bounded from above and away from 0 by Condition 4.1,
we have
n Z
X
κ0
n,3
|Γ(y, Xi (s), ν̄in (s) − νi (s))|3 ds ρ(dy).
EP n |J | ≤ 2 EP n
ε
[0,T ]×Z
i=1

Note that

max sup sup EP n |Γ(y, Xi (s), ν̄in (s) − νi (s))|3 ≤
i∈[n] y∈Z s∈[0,T ]

κ1
n3/2

by conditional i.i.d. property of (Xj , ξij ) given Xi and Lemma 5.1. Therefore
κ2
EP n |J n,3 | ≤ 1/2 → 0
n

32

BAYRAKTAR AND WU

as n → ∞.



Before analyzing J n,2 , let
γsij,y := γ(y, Xi (s), Xj (s), ξij (s)) − hγ(y, Xi (s), ·, ·), νi (s)i.

(6.5)

h
i
EP n γsij,y Xi = 0, for j 6= i.

(6.6)

Note that

We first show the following estimate.

Lemma 6.3. For each s ∈ [0, T ] and bounded measurable function f ,
EP n

1
n2
≤

X

1≤i<j<k≤n

i

h
γsij,y γsik,y f (Xi (s), νi (s)) − EP n γsij,y γsik,y f (Xi (s), νi (s)) Xj , Xk

2

4kf k2∞
.
ε4 n

Proof. We claim that for i < j < k and ei < e
j<e
k,
h
i
h
EP n γsij,y γsik,y f (Xi (s), νi (s)) − EP n γsij,y γsik,y f (Xi (s), νi (s)) Xj , Xk
ii
h

ee
ee
ee
ee
=0
· γsij,y γsik,y f (Xei (s), νei (s)) − EP n γsij,y γsik,y f (Xei (s), νei (s)) Xej , Xek

(6.7)

except when i = ei, j = e
j and k = e
k.
e
To see this, first consider k < k. Using the independence of the collection {Xi , ξij (0), Nij :
i, j ∈ N} and (6.6), we have
i
i
h
h
ee
ee
EP n EP n γsij,y γsik,y f (Xei (s), νei (s)) Xej , Xek Xi , Xj , Xk , ξij (0), Nij , ξik (0), Nik , Xei , Xej
i
h
ee
ee
= EP n γsij,y γsik,y f (Xei (s), νei (s)) Xej
i
i
h
h
ee
ee
= EP n EP n γsij,y γsik,y f (Xei (s), νei (s)) Xei , Xej , ξeiej Xej = 0

and

h
i
ee
ee
EP n γsij,y γsik,y f (Xei (s), νei (s)) Xi , Xj , Xk , ξij (0), Nij , ξik (0), Nik , Xei , Xej = 0.

Combining these two and conditioning on Xi , Xj , Xk , ξij (0), Nij , ξik (0), Nik , Xei , Xej in the LHS
of (6.7), we have verified (6.7) for k < e
k. Therefore (6.7) holds whenever k 6= e
k.
e but j < e
Next consider k = k
j. Note that we still have
h
i
i
h
ee
ee
EP n EP n γsij,y γsik,y f (Xei (s), νei (s)) Xej , Xek Xi , Xj , ξij (0), Nij , ξiek (0), Niek , Xei , Xek
i
h
ee
ee
= EP n γsij,y γsik,y f (Xei (s), νei (s)) Xek
i
i
h
h
ee
ee
= EP n EP n γsij,y γsik,y f (Xei (s), νei (s)) Xei , Xek , ξeiek Xek = 0

and

i
h
ee
ee
EP n γsij,y γsik,y f (Xei (s), νei (s)) Xi , Xj , ξij (0), Nij , ξiek (0), Niek , Xei , Xek = 0.

So again we have (6.7) for k = e
k and j < e
j. Therefore (6.7) holds whenever k 6= e
k or j 6= e
j.

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

33

Consider k = e
k, j = e
j but i < ei. Note that
i
i
h
h
ee
ee
EP n EP n γsij,y γsik,y f (Xei (s), νei (s)) Xej , Xek Xi , ξiej , ξiek , Xej , Xek
i
h
ee
ee
= EP n γsij,y γsik,y f (Xei (s), νei (s)) Xej , Xek
i
h
ee
ee
= EP n γsij,y γsik,y f (Xei (s), νei (s)) Xi , ξiej , ξiek , Xej , Xek .

k, j = e
j
Conditioning on Xi , ξiej , ξiek , Xej , Xek in the LHS of (6.7), we have verified (6.7) for k = e
e
e
e
e
but i < i. Therefore (6.7) holds whenever k 6= k, j 6= j, or i 6= i. So we have verified the
claim.
Using the claim and Condition 4.1 we have
1
n2

EP n
=

1
n4

X

1≤i<j<k≤n

X

1≤i<j<k≤n



i
h
γsij,y γsik,y f (Xi (s), νi (s)) − EP n γsij,y γsik,y f (Xi (s), νi (s)) Xj , Xk

2


h
i2
EP n γsij,y γsik,y f (Xi (s), νi (s)) − EP n γsij,y γsik,y f (Xi (s), νi (s)) Xj , Xk

4kf k2∞
.
ε4 n
This completes the proof.
≤



Now we analyze J n,2 . Let
Ŝ n,2 := {(j, k) ∈ [n]2 : j 6= 1, k 6= 1, j 6= k}.

Recall γsij,y introduced in (6.5). Note that
Z 
2
ij,y 2
e − hγ(y, Xi (s), ·, ·), νi (s)i νi (s)(de
e
γ(y, Xi (s), x
e, ξ)
x dξ)
h(γs ) , νi (s)i =
2
Z
i
h
= EP n (γsij,y )2 Xi , j 6= i.

Lemma 6.4.
J

n,2

#
h(γs12,y )2 , ν1 (s)i
ds ρ(dy)
EP n
=
Γ(y, X1 (s), ν1 (s))
[0,T ]×Z
"
#
Z
γs1j,y γs1k,y
1 X
EP n
Xj , Xk ds ρ(dy) + Rn,2 ,
+
n
Γ(y, X1 (s), ν1 (s))
[0,T ]×Z
"

Z

(j,k)∈Ŝ n,2

where Rn,2 → 0 in probability as n → ∞.
Proof. We can write
n Z
X
Γ2 (y, Xi (s−), ν̄in (s−) − νi (s−))
n,2
1[0,Γ(y,Xi (s−),νi (s−))] (z)
J =
Ni (ds dy dz)
Γ2 (y, Xi (s−), νi (s−))
i=1 XT
Z
n
ij,y ik,y
γs−
γs−
1 X
1[0,Γ(y,Xi (s−),νi (s−))] (z) 2
Ni (ds dy dz).
= 2
n
Γ (y, Xi (s−), νi (s−))
XT
i,j,k=1

(6.8)

34

BAYRAKTAR AND WU

Let
Z
n
ij,y ik,y
γs−
γs−
1 X
n,2
e
ds ρ(dy).
J
:= 2
n
[0,T ]×Z Γ(y, Xi (s−), νi (s−))
i,j,k=1

Note that
max
i∈[n]

Z

XT



EP n 

n
X

j,k=1

2
γsij,y γsik,y
 ds ρ(dy) dz ≤ κ1 n2
1[0,Γ(y,Xi (s),νi (s))] (z) 2
Γ (y, Xi (s), νi (s))

by conditional i.i.d. property of (Xj , ξij ) given Xi and Lemma 5.1. Therefore
EP n |J n,2 − Jen,2 |2

Z
n
ij,y ik,y
γs−
γs−
1 X
ei (ds dy dz)
= EP n 2
1[0,Γ(y,Xi (s−),νi (s−))] (z) 2
N
n
Γ (y, Xi (s−), νi (s−))
X
T
i,j,k=1

2
n
n Z
ij,y ik,y
X
X
γs γs
1
 ds ρ(dy) dz
EP n 
1[0,Γ(y,Xi (s),νi (s))] (z) 2
= 4
n
Γ
(y,
Xi (s), νi (s))
XT
i=1

2

j,k=1

κ1
→0
≤
n
as n → ∞.
We rewrite Jen,2 as

5
1 X
Jen,2 = 2
n m=1

X

(i,j,k)∈S n,m

Z

[0,T ]×Z

5
X
γsij,y γsik,y
T n,m ,
ds ρ(dy) =:
Γ(y, Xi (s), νi (s))
m=1

where S n,1 , S n,2 , S n,3 , S n,4 , S n,5 are collections of (i, j, k) ∈ [n]3 which equal {i = j = k},
{i = j 6= k}, {i = k 6= j}, {i 6= j = k}, {i, j, k distinct}, respectively. Using Conditions 2.1
and 4.1, and conditional i.i.d. property of (Xj , ξij ) given Xi , we can show
κ2
κ2
→ 0, EP n |T n,2 + T n,3 |2 ≤
→0
EP n |T n,1 | ≤
n
n
as n → ∞.
For the fourth term T n,4 , we have
Z
1 X
(γsij,y )2
n,4
T
= 2
ds ρ(dy)
n
Γ(y, Xi (s), νi (s))
1≤i6=j≤n [0,T ]×Z
Z
(γsij,y )2 − h(γsij,y )2 , νi (s)i
1 X
ds ρ(dy)
= 2
n
Γ(y, Xi (s), νi (s))
[0,T
]×Z
1≤i6=j≤n
Z
h(γsij,y )2 , νi (s)i
1 X
ds ρ(dy).
+ 2
n
[0,T ]×Z Γ(y, Xi (s), νi (s))
1≤i6=j≤n

Here the first term
1
n2

X

Z

1≤i6=j≤n [0,T ]×Z

(γsij,y )2 − h(γsij,y )2 , νi (s)i
ds ρ(dy) → 0
Γ(y, Xi (s), νi (s))

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

35

in probability, by conditional i.i.d. property of (Xj , ξij ) given Xi , and the second term
#
"
Z
Z
h(γsij,y )2 , νi (s)i
h(γs12,y )2 , ν1 (s)i
1 X
ds ρ(dy)
EP n
ds ρ(dy) →
n2
Γ(y, X1 (s), ν1 (s))
[0,T ]×Z
[0,T ]×Z Γ(y, Xi (s), νi (s))
1≤i6=j≤n

in probability, by i.i.d. property of (Xi , νi ), as n → ∞.
Finally for the last term T n,5 , we have
X Z
1
γsij,y γsik,y
T n,5 = 2
ds ρ(dy)
n
[0,T ]×Z Γ(y, Xi (s), νi (s))
n,5
(i,j,k)∈S
"
#!
X Z
γsij,y γsik,y
γsij,y γsik,y
1
− EP n
Xj , Xk
ds ρ(dy)
= 2
n
Γ(y, Xi (s), νi (s))
Γ(y, Xi (s), νi (s))
[0,T
]×Z
n,5
(i,j,k)∈S
"
#
X Z
1
γsij,y γsik,y
+ 2
EP n
Xj , Xk ds ρ(dy).
(6.9)
n
Γ(y, Xi (s), νi (s))
n,5 [0,T ]×Z
(i,j,k)∈S

From Lemma 6.3 we have

X Z
1

EP n
n2
n,5 [0,T ]×Z
(i,j,k)∈S

2
"
#!
γsij,y γsik,y
γsij,y γsik,y
− EP n
Xj , Xk
ds ρ(dy)
Γ(y, Xi (s), νi (s))
Γ(y, Xi (s), νi (s))

κ3
≤
.
n
So the first term in (6.9) goes to 0 in probability. The second term in (6.9) could be written
as
"
#
Z
X
γs1j,y γs1k,y
n−2
EP n
Xj , Xk ds ρ(dy)
n2
Γ(y, X1 (s), ν1 (s))
[0,T ]×Z
(j,k)∈[n]2 :j6=k
"
#
Z
1 X
γs1j,y γs1k,y
EP n
=
Xj , Xk ds ρ(dy) + Rn,5 ,
n
Γ(y, X1 (s), ν1 (s))
[0,T ]×Z
(j,k)∈Ŝ n,2

κ4
.
and one can easily check that EP n |Rn,5 | ≤ √
n
Combining above estimates completes the proof.



Next we analyze J n,1 . Let θij (s) := L(ξij (s) | Xi [s], Xj [s]) and note that

hγsij,y , θij (s)i = hγ(y, Xi (s), Xj (s), ·), θij (s)i − hγ(y, Xi (s), ·, ·), νi (s)i
h
i
= EP n γsij,y Xi (s), Xj (s) .

Lemma 6.5.
J n,1 =

1
n

X

(i,j)∈Ŝ n,2

1
+
n

Z

XT

X

(i,j)∈Ŝ n,2

1[0,Γ(y,Xi (s−),νi (s−))] (z)
Z

XT

ij,y
ij,y
, θij i e
− hγs−
γs−
Ni (ds dy dz)
Γ(y, Xi (s−), νi (s−))

(6.10)

ij,y
, θij i
hγs−
ei (ds dy dz) + Rn,1 ,
1[0,Γ(y,Xi (s−),νi (s−))] (z)
N
Γ(y, Xi (s−), νi (s−))

where EP n |Rn,1 | → 0 as n → ∞.

36

BAYRAKTAR AND WU

Proof. We can write
n Z
X
Γ(y, Xi (s−), ν̄in (s−) − νi (s−)) e
Ni (ds dy dz)
1[0,Γ(y,Xi (s−),νi (s−))] (z)
J n,1 =
Γ(y, Xi (s−), νi (s−))
X
T
i=1
n Z
ij,y
γs−
1 X
ei (ds dy dz).
1[0,Γ(y,Xi (s−),νi (s−))] (z)
=
N
n
Γ(y, Xi (s−), νi (s−))
XT
i,j=1

Therefore (6.10) holds with
n Z
1j,y
γs−
1X
n,1
e1 (ds dy dz)
1[0,Γ(y,X1 (s−),ν1 (s−))] (z)
N
R =
n
Γ(y,
X
(s−),
ν
(s−))
1
1
X
T
j=1
Z
n
i1,y
i2,y
γs−
+ γs−
1X
ei (ds dy dz).
+
1[0,Γ(y,Xi (s−),νi (s−))] (z)
N
n
Γ(y, Xi (s−), νi (s−))
XT
i=2

Here for the first term,

2 
n Z
1j,y
X
γs−
1
e1 (ds dy dz) 
1[0,Γ(y,X1 (s−),ν1 (s−))] (z)
N
EP n 
n
Γ(y,
X
(s−),
ν1 (s−))
1
X
T
j=1
2
 P
1j,y
n
1
Z
γ
j=1 s
n
κ1
ds ρ(dy) ≤
,
= EP n
n
[0,T ]×Z Γ(y, X1 (s), ν1 (s))

since
EP n

"

#
γs1j,y γs1k,y
= 0,
Γ(y, X1 (s), ν1 (s))

whenever j 6= k.

ei clearly we have
For the second term, using the independence of N
n Z
i1,y
i2,y
γs−
+ γs−
1X
κ2
ei (ds dy dz) ≤ √
EP n
.
1[0,Γ(y,Xi (s−),νi (s−))] (z)
N
n
Γ(y,
X
(s−),
ν
(s−))
n
i
i
XT
i=2

Therefore EP n |Rn,1 | ≤

κ3
√
n

→ 0 as n → ∞, and this completes the proof.

The most difficult part is to analyze the following term in J n,1 :
1 X
U n :=
un (i, j),
n



(6.11)

2≤i<j≤n

where
un (i, j) :=

Z

XT

+

1[0,Γ(y,Xi (s−),νi (s−))] (z)
Z

XT

ij,y
ij,y
, θij i e
− hγs−
γs−
Ni (ds dy dz)
Γ(y, Xi (s−), νi (s−))

ji,y
ji,y
, θji i e
− hγs−
γs−
Nj (ds dy dz).
1[0,Γ(y,Xj (s−),νj (s−))] (z)
Γ(y, Xj (s−), νj (s−))

Recall Ωv , Ω0 , α(ω0 , ·), P0 introduced in Sections 4.1 and 4.2. We now use the results
from Section 6.1 with S = Ωv and θ̄ = α(ω0 , ·), ω0 ∈ Ω0 to get the asymptotics of
(U n , J n,1 , J n,2 , J n,3 ). For each ω0 ∈ Ω0 , k ≥ 1 and g ∈ L2sym (α(ω0 , ·)⊗k ), the MWI Ikω0 (g)

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

37

is defined as in Section 6.1. More precisely, let Ak be the collection of all measurable
g : : Ω0 × Ωkv → R such that
Z
|g(ω0 , ω1 , . . . , ωk )|2 α(ω0 , dω1 ) · · · α(ω0 , dωk ) < ∞, P0 a.e. ω0
Ωkv

and for every permutation π on [k], g(ω0 , ω1 , . . . , ωk ) = g(ω0 , ωπ(1) , . . . , ωπ(k) ), P0 ⊗ α⊗k a.s.
Q
where P0 ⊗ α⊗k (dω0 , dω1 , . . . , dωk ) := P0 (dω0 ) ki=1 α(ω0 , dωi ). Then there is a measurable
space (Ω∗ , F ∗ ) and a regular conditional probability distribution λ∗ : Ω0 × F ∗ → [0, 1] such
that on the probability space (Ω0 × Ω∗ , B(Ω0 ) ⊗ F ∗ , P0 ⊗ λ∗ ), where
Z
∗
λ∗ (ω0 , B) P0 (dω0 ), A × B ∈ B(Ω0 ) ⊗ F ∗ ,
P0 ⊗ λ (A × B) :=
A

there is a collection of real-valued random variables {Ik (g) : g ∈ Ak , k ≥ 1} satisfying
(a) For all g ∈ A1 the conditional
of I1 (g) given G ∗ := B(Ω0 ) ⊗ {∅, Ω∗ } is Normal
R distribution
2
with mean 0 and variance Ωv g (ω0 , ω1 ) α(ω0 , dω1 );
(b) Ik is (a.s.) linear map on Ak ;
(c) For g ∈ Ak of the form
Z
k
Y
ge2 (ω0 , ω1 ) α(ω0 , dω1 ) < ∞, P0 a.e. ω0 ,
ge(ω0 , ωi ), s.t.
g(ω0 , ω1 , . . . , ωk ) =
Ωv

i=1

we have

∗

Ik (g)(ω0 , ω ) =

⌊k/2⌋

X

j

(−1) Ck,j

j=1

and

Z

P0 ⊗ λ∗ a.e. (ω0 , ω ∗ )
Z

Ω∗

∗

2

∗

∗

j
g )(ω0 , ω ∗ ))k−2j ,
ge (ω0 , ω1 ) α(ω0 , dω1 ) (I1 (e
2

Ωv

(Ik (g)(ω0 , ω )) λ (ω0 , dω ) = k!

Z

Ωv

k
ge (ω0 , ω1 ) α(ω0 , dω1 ) , P0 a.e. ω0 ,
2

where Ck,j are as in (6.1).
We write Ik (g)(ω0 , ·) as Ikω0 (g).
Recall U n from (6.11) and the canonical processes Vi from Section 4.1. The following lemma
is a key ingredient to get the asymptotics of (U n , J n,1 , J n,2 , J n,3 ).
k
Lemma 6.6. Let {φk }∞
k=1 be such that φk ∈ A for each k ≥ 1. Let
X


φk (Vi1 , . . . , Vik ) for n ≥ k
Ûkn (φk ) := 2≤i1 <i2 <···<ik ≤n

0
for n < k.

Then the following convergence holds as n → ∞:
 !


 
 k
1
X
I 1 (φk )
U n , n− 2 Ûkn (φk )
⇒ Z,
k! k
k≥1
k≥1

as a sequence of R∞ -valued random variables, where Z is Gaussian with mean 0 variance
Z
ij,y
ij,y
, θij i)2
− hγs−
(γs−
σ 2 := EP n
ds ρ(dy)
(6.12)
[0,T ]×Z Γ(y, Xi (s−), νi (s−))

38

BAYRAKTAR AND WU

and is independent of {IkX1 (·)}k≥1 .
Proof. Fix m ∈ N, φk ∈ Ak and t, sk ∈ R for k = 1, . . . , m. Denote by EP n ,V the conditional expectation under P n given (Xi , Ni )ni=1 . Since ξij is conditionally independent given
(Xi , Ni )ni=1 , we have
1 X
EP n,V [un (i, j)]2 .
σn2 := EP n ,V [U n ]2 = 2
n
2≤i<j≤n

Note that
EP n [σn2 ]

1
= 2
n

X

n

2≤i<j≤n

2

EP n [u (i, j)] → EP n

Z

[0,T ]×Z

ij,y
ij,y
(γs−
− hγs−
, θij i)2
ds ρ(dy) = σ 2
Γ(y, Xi (s−), νi (s−))

and (since the cross product term below is zero when (i, j, ei, e
j) are distinct)
2

X

1
κ1
EP n [σn2 − EP n [σn2 ]]2 = 4 EP n 
EP n ,V [un (i, j)]2 − EP n [un (i, j)]2  ≤
→ 0.
n
n
2≤i<j≤n

So σn2 → σ 2 in probability as n → ∞. Suppose without loss of generality that σ 2 > 0, since
otherwise we have that Z = 0, U n → 0 in probability as n → ∞ and the desired convergence
holds trivially by Lemma 6.1. Also note that
E

X

Pn

E

P n ,V

2≤i6=j≤n

un (i, j)
n

4

≤

κ2
→0
n2

as n → ∞. Hence the Lyapunov’s condition for CLT (see [5, Theorem 27.3]) holds with δ = 2:
lim

n→∞

X

1
σn2+δ

EP n ,V

2≤i6=j≤n

un (i, j)
n

2+δ

= 0,

where the convergence is in probability. It then follows from standard proofs of CLT and a
subsequence argument that for each t ∈ R,
i
h √
2 2
n
EP n ,V e −1tU − e−t σn /2 → 0

in probability as n → ∞. This together with the convergence of σn2 → σ 2 implies that
h √
i
2 2
n
EP n ,V e −1tU → e−t σ /2
(6.13)

in probability as n → ∞. Now let (t, s1 , . . . sm ) 7→ ϕn (t, s1 , . . . , sm ) be the characteristic
function of
m
1
n
(φm )),
(U n , n− 2 Û1n (φ1 ), . . . , n− 2 Ûm
and

1 2 2
σ

ϕ(t, s1 , . . . , sm ) := e− 2 t

ψ(s1 , . . . , sm ), (t, s1 , . . . , sm ) ∈ Rm+1

1 X1
be that of (Z, I1X1 (φ1 ), . . . , m!
Im (φm )). From Lemma 6.1 it follows that for all (s1 , . . . , sm ) ∈
m
R
√

EP n [e

−1

Pm

k=1 sk n

−k
2

Ûkn (φk )

] → ψ(s1 , . . . , sm ) as n → ∞.

(6.14)

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

39

Thus as n → ∞,

ϕn (t, s1 , . . . , sm ) − ϕ(t, s1 , . . . , sm )

 √
√
Pm
−k
1 2 2
n + −1
2 Û n (φk )
−
−1tU
s
n
t
σ
k
k=1
k
−e 2
ψ(s1 , . . . , sm )
= EP n e


 √ Pm
h √
i
k
−1tU n
−1 k=1 sk n− 2 Ûkn (φk )
− 21 t2 σ2
n
n
e
EP ,V e
= EP
−e


√
Pm
−k
1 2 2
2 Û n (φk )
−1
s
n
k
k=1
k
] − ψ(s1 , . . . , sm ) e− 2 t σ
+ EP n [e
→ 0,

where the convergence follows from (6.13) and (6.14). This completes the proof.



Now we analyze the asymptotics of (U n , J n,1 , J n,2 , J n,3 ). Recall Aω0 , h, Ξ introduced in
Section 4.2. For ω0 ∈ Ω0 , denote by A∗ω0 the adjoint operator of Aω0 , that is
Z
∗
g(ω2 )h(ω1 , ω2 ) α(ω0 , dω2 ), g ∈ Hω0 , ω1 ∈ Ωv .
Aω0 g(ω1 ) =
Ωv

R
Lemma 6.7. For P0 a.e. ω0 , (a) Trace(Aω0 A∗ω0 ) = Ω2 h2 (ω1 , ω2 ) α(ω0 , dω1 ) α(ω0 , dω2 ) =
v
R
R
2 (ω , ω ) Ξ(dω ) Ξ(dω ) =
h
λ(t,
y)
dt
ρ(dy),
where
2
1
2
1
2
Ωv
[0,T ]×Z


hγ̄t,y (X1 [t], X2 (t), ·), θ(t, X1 [t], X2 [t])i2
, (t, y) ∈ [0, T ] × Z.
λ(t, y) := EP n
Γ(y, X1 (t), ν1 (t))

(b) Trace(Anω0 ) = 0 for all n ≥ 2. (c) I − Aω0 is invertible.

Proof. The first equality in part (a) follows from the definition of Aω0 , the second uses the
observation that h(ω1 , ω2 ) does not depend on ξ∗ (ω1 ) or ξ∗ (ω2 ), and the third follows from
the definition of λ and h. Part (b) follows on noting that
Z
n
h(ω1 , ω2 )h(ω2 , ω3 ) · · · h(ωn , ω1 ) α(ω0 , dω1 ) α(ω0 , dω2 ) · · · α(ω0 , dωn ) = 0;
Trace(Aω0 ) =
Ωn
v

see also [27, Lemma 2.7]. Part (c) is immediate from [27, Lemma 1.3].
Let
m(y, x[s], x
e[s]) := EP n

"



#
γs12,y γs13,y
X2 [s] = x[s], X3 [s] = x
e[s]
Γ(y, X1 (s), ν1 (s))

and define functions ℓ, F : Ωv × Ωv → R (Ξ × Ξ a.s.) is by
Z
m(y, X∗ (ω1 )[s], X∗ (ω2 )[s]) ds ρ(dy),
ℓ(ω1 , ω2 ) :=

(6.15)

[0,T ]×Z

Note that

F (ω1 , ω2 ) := h(ω1 , ω2 ) + h(ω2 , ω1 ) − ℓ(ω1 , ω2 ).
ℓ(ω1 , ω2 ) =

Z

h(ω3 , ω1 )h(ω3 , ω2 ) Ξ(dω3 ),

P0 a.s. ω0 .

(6.16)
(6.17)

Ωv

Recall the independent normal random variable Z from Lemma 6.6. Let
1
1
1
J := I2X1 (F ) − Trace(AX1 A∗X1 ) + Z − σ 2 .
2
2
2
The following lemma is the key step.

(6.18)

40

BAYRAKTAR AND WU

Lemma 6.8. As n → ∞,

√

−1η̄ n (ϕ) + J n (T ) ⇒

√
−1I1X1 (ϕ) + J.

Proof. Recall Ûkn in Lemma 6.6, Ŝ n,2 in (6.8), U n in (6.11), and ℓ in (6.15). From Lemmas
6.4 and 6.5 we have
J n,1 = U n + Û2n (hsym ) + Rn,1 ,
"
#
Z
12,y 2
h(γ
)
,
ν
(s)i
s
1
EP n
J n,2 =
ds ρ(dy) + Û2n (ℓ) + Rn,2 ,
Γ(y, X1 (s), ν1 (s))
[0,T ]×Z
where hsym (ω1 , ω2 ) := 12 [h(ω1 , ω2 ) + h(ω2 , ω1 )] for ω1 , ω2 ∈ Ωv . It then follows from Lemmas
6.2, 6.4, 6.5 and 6.6 that

η̄ n (ϕ), J n,1 , J n,2 , J n,3
#
!
"
Z
h(γs12,y )2 , ν1 (s)i
X1
X1
X1 sym
ds ρ(dy) + I2 (ℓ), 0 .
EP n
⇒ I1 (ϕ), Z + I2 (h
),
Γ(y, X1 (s), ν1 (s))
[0,T ]×Z
Also note that
#
"
h(γs12,y )2 , ν1 (s)i
EP n
Γ(y, X1 (s), ν1 (s))
#
"
EP n [h(γs12,y )2 , ν1 (s)i | X1 ]
= EP n
Γ(y, X1 (s), ν1 (s))
"
#
EP n [(γsij,y − hγsij,y , θij i)2 | X1 ] + EP n [hγ̄t,y (X1 [t], X2 (t), ·), θ(t, X1 [t], X2 [t])i2 | X1 ]
= EP n
Γ(y, X1 (t), ν1 (t))
= σ 2 + Trace(AX1 A∗X1 )
a.s., by (6.12) and Lemma 6.7. The result follows on combining above two displays with (6.4),
(6.16), and (6.18).

6.4. Completing the proof of Theorem 4.1. Recall G ∗ = B(Ω0 ) ⊗ {∅, Ω∗ }. It follows from
(6.17), Lemma 6.7 and [27, Lemma 1.2] that P0 a.s.






1
1 X1
∗
∗
I (F ) G = exp
Trace(AX1 AX1 ) .
EP0 ⊗λ∗ exp
2 2
2
Therefore


1
1 X1
∗
I (F ) − Trace(AX1 AX1 )
= 1.
EP0 ⊗λ∗ exp
2 2
2
It then follows from Lemma 6.6 that




EP0 ⊗λ∗ [exp (J)] = 1.
Also, recall that
EP n [exp (J n (T ))] = 1.
It then follows from Lemma 6.8 √
(with ϕ ≡ 0) and Scheffe’s lemma that {exp (J n (T ))} is
uniformly integrable. Since | exp −1η̄ n (ϕ) | = 1,
√

{exp −1η̄ n (ϕ) + J n (T ) }

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

41

is also uniformly integrable. Hence from Lemma 6.8 we have
√


−1η̄ n (ϕ) + J n (T )

√
−1I1X1 (ϕ) + J
= EP0 ⊗λ∗ exp




√
1
1 2
1 X1
X1
∗
= EP0 ⊗λ∗ exp
−1I1 (ϕ) + I2 (F ) − Trace(AX1 AX1 ) EP0 ⊗λ∗ exp Z − σ
2
2
2





√
1
1
= EP0 ⊗λ∗ EP0 ⊗λ∗ exp
−1I1X1 (ϕ) + I2X1 (F ) − Trace(AX1 A∗X1 ) G ∗
2
2


Z
1
(6.19)
exp − (σωϕ0 )2 P0 (dω0 ),
=
2
Ω0
lim EP n exp

n→∞

where the last line is a consequence of Lemma 6.7 and [27, Lemma 1.3]. Thus we have proved
(6.2) which completes the proof of Theorem 4.1.
6.5. Completing the proof of Theorem 4.2. Clearly the function ϕ ∈ Ax can be viewed
as an element (abusing notation) ϕ of A defined by ϕ(x, ξ) := ϕ(x), x, ξ ∈ D([0, T ] : Z)2 , and
Φω0 = Φ for P0 a.s. ω0 . Note that h(ω1 , ω2 ) depends on ω2 only through X∗ (ω2 ). It then
follows from the definition of Aω0 and A that (I − A)−1 Φ(ω) = (I − Aω0 )−1 Φω0 (ω) for P0
a.s. ω0 and the dependence on ω is actually only through X∗ (ω). It then follows from the
definition of σωϕ0 that
σωϕ0 = k(I − A)−1 ΦkH ,

P0 a.s. ω0 .

Therefore from (6.19) we have
lim EP n exp

n→∞



1
exp − k(I − A)−1 Φk2H P0 (dω0 )
2
Ω0


1
−1
2
= exp − k(I − A) ΦkH .
2

√

−1η̄xn (ϕ) + J n (T ) =

Z

This gives (6.3) and completes the proof of Theorem 4.2.

Appendix A. Proof of Theorem 2.3
Proof of Theorem 2.3. (a) Since the limiting system is McKean–Vlasov, the proof of existence
and uniqueness is standard (cf. [29, Chapter 1], see also [15, Theorem 2.1]) and hence omitted.
(b) Now we show (2.11). For each fixed i ∈ [n] and t ∈ [0, T ], we have
EkXin − Xi k∗,t
Z
|y| 1[0,Γ(y,Xin (s−),νin (s−))] (z) − 1[0,Γ(y,Xi (s−),ν(s−))] (z) Ni (ds dy dz)
≤E
Xt
Z
|y| |Γ(y, Xin (s), νin (s)) − Γ(y, Xi (s), ν(s))| ds ρ(dy).
=E
[0,t]×Z

42

BAYRAKTAR AND WU

Fixing y ∈ Z and s ∈ [0, T ], we have

E |Γ(y, Xin (s), νin (s)) − Γ(y, Xi (s), ν(s))|
Z
n
1X
n
n
e ν(s)(de
e
γ(y, Xi (s), x
e, ξ)
x dξ)
γ(y, Xi (s), Xj (s), ξij (s)) −
=E
n
Z2
j=1
n

n

j=1

j=1

1X
1X
γ(y, Xin (s), Xjn (s), ξij (s)) −
γ(y, Xi (s), Xj (s), ξij (s))
≤E
n
n
n

+E

1X
γ(y, Xi (s), Xj (s), ξij (s)) −
n
j=1

Z

Z2

e ν(s)(de
e .
γ(y, Xi (s), x
e, ξ)
x dξ)

From Condition 2.2, Remark 2.1(b) and the exchangeability of {(Xjn , Xj ) : j ∈ [n]} we have
n

n

j=1

j=1

1X
1X
γ(y, Xin (s), Xjn (s), ξij (s)) −
γ(y, Xi (s), Xj (s), ξij (s))
E
n
n
≤

1
n

n
X
j=1


γy E|Xin (s) − Xi (s)| + E|Xjn (s) − Xj (s)| = 2γy E|Xin (s) − Xi (s)|.

Since {(Xj , ξij ) : j ∈ [n]} are independent with common joint law ν, using Lemma 5.1 we
have
Z
n
1X
1 γy
e ν(s)(de
e ≤ κ√
γ(y, Xi (s), x
e, ξ)
x dξ)
γ(y, Xi (s), Xj (s), ξij (s)) −
.
E
n
n
Z2
j=1

Combining above four displays gives


Z
κ1 γy
n
n
√
ds ρ(dy)
|y| 2γy E|Xi (s) − Xi (s)| +
EkXi − Xi k∗,t ≤
n
[0,t]×Z
Z t
κ1 Cγ
EkXin − Xi k∗,s ds + √ .
≤ 2Cγ
n
0

It then follows from Gronwall’s inequality that

κ2
EkXin − Xi k∗,T ≤ √
n
for some κ2 < ∞. This gives (2.11).
(c) Using (b), the independence of {Xi } and a standard argument (see [29, Chapter 1]) one
has (2.12).
(d) Using (b) and a standard argument (see [29, Chapter 1] and [4, Appendix A]) one has
(2.13) and (2.14). The last two statements (2.15) and (2.16) follow immediately from (2.13)
and (2.14), respectively.

References
[1] D. J. Aldous, Exchangeability and related topics, École d’été de probabilités de saint-flour xiii — 1983,
1985, pp. 1–198.
[2] W. J. Anderson, Strong and exponential ergodicity, Continuous-time markov chains: An applicationsoriented approach, 1991, pp. 204–232.

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

43

[3] J. Baladron, D. Fasoli, O. Faugeras, and J. Touboul, Mean-field description and propagation of chaos in
networks of Hodgkin–Huxley and FitzHugh–Nagumo neurons, The Journal of Mathematical Neuroscience
2 (2012), no. 1, 10.
[4] S. Bhamidi, A. Budhiraja, and R. Wu, Weakly interacting particle systems on inhomogeneous random
graphs, Stochastic Processes and their Applications 129 (2019), no. 6, 2174–2206.
[5] P. Billingsley, Probability and Measure, Wiley series in probability and mathematical statistics: Probability
and mathematical statistics, John Wiley & Sons, New York, 1995.
[6] A. Budhiraja, P. Dupuis, M. Fischer, and K. Ramanan, Limits of relative entropies associated with weakly
interacting particle systems, Electronic Journal of Probability 20 (2015), no. 80, 1–22.
[7] A. Budhiraja, P. Dupuis, and A. Ganguly, Large deviations for small noise diffusions in a fast markovian
environment, Electron. J. Probab. 23 (2018), 33 pp.
[8] A. Budhiraja, D. Mukherjee, and R. Wu, Supermarket model on graphs, Ann. Appl. Probab. 29 (201906),
no. 3, 1740–1777.
[9] P. E Caines and M. Huang, Graphon mean field games and the GMFG equations, 2018 IEEE Conference
on Decision and Control (CDC), 2018, pp. 4129–4134.
[10] R. Carmona, D. Cooney, C. Graves, and M. Lauriere, Stochastic graphon games: I. The static case, arXiv
preprint arXiv:1911.10664 (2019).
[11] F. Coppini, H. Dietert, and G. Giacomin, A law of large numbers and large deviations for interacting diffusions on Erdős–Rényi graphs, Stochastics and Dynamics 0 (2019), no. 0, 2050010, available at
https://doi.org/10.1142/S0219493720500100.
[12] F. Delarue, Mean field games: A toy model on an Erdös-Renyi graph., ESAIM: Proceedings and Surveys
60 (2017), 1–26.
[13] S. Delattre, G. Giacomin, and E. Luçon, A note on dynamical models on random graphs and Fokker–Planck
equations, Journal of Statistical Physics 165 (2016), no. 4, 785–798.
[14] E. B. Dynkin and A. Mandelbaum, Symmetric statistics, Poisson point processes, and multiple Wiener
integrals, The Annals of Statistics 11 (1983), no. 3, 739–745.
[15] C. Graham, Mckean-Vlasov Ito-Skorohod equations, and nonlinear diffusions with discrete jump sets, Stochastic Processes and their Applications 40 (1992), no. 1, 69 –82.
[16] N. Ikeda and S. Watanabe, Stochastic Differential Equations and Diffusion Processes, North-Holland
Mathematical Library, vol. 24, Elsevier, 1981.
[17] V. N. Kolokoltsov, Nonlinear Markov Processes and Kinetic Equations, Cambridge Tracts in Mathematics,
vol. 182, Cambridge University Press, 2010.
[18] P. M. Kotelenez and T. G. Kurtz, Macroscopic limits for stochastic partial differential equations of
McKean–Vlasov type, Probability Theory and Related Fields 146 (2008Dec), no. 1, 189–222.
[19] T. G. Kurtz, Approximation of Population Processes, CBMS-NSF Regional Conference Series in Applied
Mathematics, vol. 36, SIAM, 1981.
[20] T. G. Kurtz and J. Xiong, Particle representations for a class of nonlinear SPDEs, Stochastic Processes
and their Applications 83 (1999), no. 1, 103–126.
[21] T. G. Kurtz and P. E. Protter, Weak convergence of stochastic integrals and differential equations II:
Infinite dimensional case, Probabilistic Models for Nonlinear Partial Differential Equations, 1996, pp. 197–
285.
[22] S. Méléard, Asymptotic behaviour of some interacting particle systems; Mckean-Vlasov and Boltzmann
models, Probabilistic Models for Nonlinear Partial Differential Equations, 1996, pp. 42–95.
[23] S. L Nguyen, G. Yin, and T. A Hoang, On laws of large numbers for systems with mean-field interactions
and Markovian switching, Stochastic Processes and their Applications (2019).
[24] R. I. Oliveira and G. H. Reis, Interacting diffusions on random graphs with diverging average degrees:
Hydrodynamics and large deviations, Journal of Statistical Physics (2019Jul).
[25] F. Parise and A. E Ozdaglar, Graphon games: A statistical framework for network games and interventions,
Available at SSRN: https://ssrn.com/abstract=3437293 (2019).
[26] D. Shah, Gossip algorithms, Foundations and Trends in Networking 3 (2009), no. 1, 1–125.
[27] T. Shiga and H. Tanaka, Central limit theorem for a system of Markovian particles with mean field interactions, Probability Theory and Related Fields 69 (1985), no. 3, 439–459.
[28] A-S. Sznitman, Nonlinear reflecting diffusion process, and the propagation of chaos and fluctuations associated, Journal of Functional Analysis 56 (1984), no. 3, 311–336.
[29] A-S. Sznitman, Topics in propagation of chaos, Ecole d’Eté de Probabilités de Saint-Flour XIX—1989,
1991, pp. 165–251.

44

BAYRAKTAR AND WU

[30] J. Touboul, Propagation of chaos in neural fields, The Annals of Applied Probability 24 (2014), no. 3,
1298–1328.
[31] R. L. Tweedie, Criteria for ergodicity, exponential ergodicity and strong ergodicity of markov processes,
Journal of Applied Probability 18 (1981), no. 1, 122–130.
[32] G G. Yin and C. Zhu, Hybrid Switching Diffusions: Properties and Applications, Stochastic Modelling and
Applied Probability, vol. 63, Springer-Verlag New York, 2010.
Department of Mathematics, University of Michigan, 530 Church Street, Ann Arbor, MI
48109
Department of Mathematics, Iowa State University, 411 Morrill Road, Ames, IA 50011
E-mail address: erhan@umich.edu, ruoyu@iastate.edu

