Query by Semantic Sketch
Luca Rossetto1,2 , Ralph Gasser1 , and Heiko Schuldt1

arXiv:1909.12526v1 [cs.MM] 27 Sep 2019

1

2

Department of Mathematics and Computer Science
University of Basel, Basel, Switzerland
{firstname.lastname}@unibas.ch
Department of Informatics, University of Zurich, Zurich, Switzerland
rossetto@ifi.uzh.ch

Abstract. Sketch-based query formulation is very common in image
and video retrieval as these techniques often complement textual retrieval methods that are based on either manual or machine generated
annotations. In this paper, we present a retrieval approach that allows
to query visual media collections by sketching concept maps, thereby
merging sketch-based retrieval with the search for semantic labels. Users
can draw a spatial distribution of different concept labels, such as “sky”,
“sea” or “person” and then use these sketches to find images or video
scenes that exhibit a similar distribution of these concepts. Hence, this
approach does not only take the semantic concepts themselves into account, but also their semantic relations as well as their spatial context.
The efficient vector representation enables efficient retrieval even in large
multimedia collections. We have integrated the semantic sketch query
mode into our retrieval engine vitrivr and demonstrated its effectiveness.

Keywords: Content-based Retrieval, Semantic Retrieval, Sketch-based Retrieval,
Query-by-Sketch, Concept Map

1

Introduction

Effective and efficient retrieval of information from large media collections remains a challenge that is still subject to research from many different domains in
computer science. As these multimedia collections grow larger, finding a particular item of interest becomes more challenging. This can mainly be attributed to
the fact that the traditional and still very common approach of textually annotating individual media objects and retrieving them later based on these labels
or descriptions does not scale well – especially nowadays, as media collections
grow at an ever increasing pace. Furthermore, textual descriptions are often subject to bias or error, and would even have to anticipate future interpretations
of an object. In addition, in some cases textual descriptions may not be well
suited to, for example, describe the temporal changes inherent to a video or the
complex scenery that makes up an image.
Content-based retrieval approaches try to remedy this situation by deriving
features directly from the media objects’ content and to retrieve similar objects w.r.t. these features. This removes the necessity for a manual, laborious

2

Luca Rossetto, Ralph Gasser, and Heiko Schuldt

and tedious annotation process. Common query modes for the visual domain,
i.e., videos and images, include Query-by-Example (QbE) or Query-by-Sketch
(QbS). These query modes can be summarized as finding an object based on
the similarity of that object to either a user-provided example image (QbE) or
a user-generated sketch (QbS). For instance, a user could try to use some similar image they found on Google to retrieve the specific image or scene they
are looking for. Or they could try to come up with a sketch that resembles the
content of the desired object, either as an outline (line sketch), or in terms of
colour distribution. Also combinations of QbE and QbS would be possible, for
instance by using a query image and superimposing a sketch to add information
not present on the query image.
Commonly, the aforementioned query modes take visual similarity based on
either color, edge or point-of-interest-based features into account. However, reproducing the visual content of an image by means of a hand-drawn sketch is very
difficult in practice and retrieval quality often suffers because of the user’s inability to do so. Inspired by and improving upon the work in [8], we present a video
and image retrieval method that exploits concept-maps [32] for sketch-based retrieval. Our approach facilitates retrieval from huge image or video collections,
such as the V3C [27] used for TRECVID. It allows to search for simple concept
labels, such as “person”, “sky” or “horse”, while at the same time constraining
the occurrence of these labels to specific areas in the image or key-frame. Leveraging recent progress in the area of machine learning and image understanding,
we use a deep neural network to produce these concept maps from which we
subsequently derive our features for retrieval. Queries can then be expressed by
the user, simply by drawing the spatial distribution of the desired concepts as
they can recall it from memory. The semantic sketch query mode has been implemented into the retrieval engine vitrivr [26,10] and successfully field-tested
on different occasions.
The contribution of this paper is two-fold: First, we introduce a compact vector representation that can be used for efficient sketch-based retrieval of semantic
concepts using kNN-lookup, conserving both the spatial as well as the semantic
relationship of the sketched objects. Second, we compare our new vector representation to similar methods and we present performance metrics obtained on
the V3C1 video collection [27] using vitrivr, our multimedia information retrieval
stack.
The remainder of this paper is structured as follows: Section 2 surveys related
work. Section 3 describes how we derive the features from an input image and
Section 4 details implementation aspects. Section 5 provides an overview of the
method’s performance and Section 6 concludes and offers some outlook to future
work.

2

Related Work

The approach of sketch-based retrieval of visual content such as images or videos
has been in use for many years as a way to reduce the media discontinuity that

Query by Semantic Sketch

3

occurs when text (or another non-visual modality) is used for query expression.
Earlier approaches primarily rely purely on low-level features, such as properties
extracted from edge [12,28] or color [3,17,23] information. Either independently
or in conjunction with these sketches, semantic information can be expressed
by keywords or descriptions that are derived from manual or machine generated
annotations [14,31]. There are some approaches that try to bridge the divide
between the visual sketches and the textual annotations, by generating the semantic labels based on sketched input [6,30]. However, these methods differ
mostly in the user-facing query formulation stage and do not offer any different
or even richer query information. In addition, none of these approaches are able
to incorporate the spatial relation of the semantic concepts into a query.
With the advent of deep learning-based image and video analysis and its
influence on multimedia information retrieval, querying for semantic concepts,
while at the same time considering the spatial relation of these concepts within
the visual content, became feasible without a textual representation. One way
of achieving this goal is by mapping both the query and the target images into
a common space that encodes both the visual as well as the semantic and spatial information. Such transformations exist both for image sketches [11] and for
annotated region ‘sketches’, usually consisting of placements of concepts, potentially accompanied by a bounding box delineating the spatial extension of the
concept [19,32]. Similar approaches have also been used to retrieve results that
are both visually and semantically similar to a given sketch [29].
Another technique has been proposed in [24]. The authors use neural networks that generate bounding boxes for the detected instances of semantic concepts. A query, which must also be composed of semantically labelled boxes, can
then be answered without the need for a dedicated learned space [2]. Semantic
queries based on labelled rectangular boxes have also already been proposed before the advent of deep learning [15], some even taking additional information
such as the approximate color of the relevant object into account [16].
The approach presented in [8] goes beyond labelled, rectangular areas by
employing a fully convolutional neural network architecture to obtain pixel-wise
semantic labels for an input image. The output of such a network can then be linearized and transformed into a binary vector using a one-shot encoding scheme
for every pixel. In order to handle the resulting vectors with their dimensionalities ranging up to the order of 105 , the authors use product quantization
(PQ) [13]. The resulting representation enables true sketch-based retrieval with
a high spatial resolution.
Our method builds upon the ideas in [8] and improves upon their method by
introducing a more compact vector representation that does not rely on a specific
indexing scheme for efficient retrieval. We achieve this, while simultaneously
preserving the semantic relations between the different concepts supported in a
query.

4

Luca Rossetto, Ralph Gasser, and Heiko Schuldt

Fig. 1. Example input image used for the DeepLab pixel-wise annotation. Image ccby-nc ‘alpopescu’, taken from video 04507 of the V3C [27] video dataset.

3

Methods

In order to transform an input image or video frame into a representation suitable
for retrieval via a semantic sketch query, we first apply the DeepLab pixel-wise
concept annotation described in [4], which assigns a concept label to every pixel
in the image. This process is exemplified in Figures 1 and 2, which depict an
input image and the resulting map corresponding to the assignment of labels.
Since the resolution of this pixel-wise label assignment is higher than necessary for a sketch-based application, the map is subsequently down-sampled to
an n × n grid. For each grid cell, all the labels that fall within that cell are used
to characterize it. The resulting label of the cell is then simply the label which
occurs most often within the cell. This sampling method is chosen for its simplicity, since it does not require any additional information about the semantic
relation of the concept labels. In case such relations were available – e.g., as
hierarchical structure between the labels, such as [22] – other methods that preserve more of the semantic information could be employed. Figure 3 illustrates
the resulting grid representation for a n × n grid with n = 32. This procedure
is equivalent to a non-aspect ratio preserving down-scaling of the output map
and it hence preserves the spatial relation of the classes as much as the reduced
spatial resolution allows.
The next step involves linearizing this 2D map into a 1D representation,
which is required in order to be useful for nearest neighbor retrieval. Rather
than just assigning arbitrary scalar values to the individual concepts, we use
Word2vec [21] to obtain a representation of the concepts, which allows us to
reason about their respective semantic similarity. Since the vectors produced
for every concept are too long to be used for the map representation directly,
we reduce their dimensions to a substantially lower number d (typically 2 or 3)

Query by Semantic Sketch

5

Fig. 2. Pixel-wise assignment of labels generated by DeepLab based on the input image
depicted in Figure 1.

by applying a t-SNE embedding [18]. A possible result of such an embedding
for d = 2 is illustrated in Figure 4. The resulting coordinates for every label
can finally be concatenated by traversing the aggregated two-dimensional label
map in a fixed order. The resulting vector has a dimensionality of n2 d (in our
case 322 · 2 = 2048). This is substantially lower than the one produced by [8],
which produces binary vectors of 245’760 dimensions. Nevertheless, it still retains
sufficient information so as to be useful for retrieval.
The query vector is constructed analogously from a pixel-wise semantic annotation that can be sketched by a user. This sketch is transformed to the same
grid representation and linearized in the same order, so that the grid cells are
aligned between query and feature vectors, thus preserving the spatial relation.
Using this encoding scheme has the advantage that it considers both spatial
as well as semantic similarity when comparing two images. For example, a query
for a dog lying in the grass will return a lower distance for images showing a cat
in the grass than for images of grass without an animal in it, which would not be
the case if arbitrary scalar substitutes for the labels had been used. The proposed
method also supports non-rectangular areas for one semantic class, which enables
more precise representation and querying in comparison to bounding-box based
annotations such as those produced by [24].

4

Implementation

This section describes some details of our implementation, both in terms of
feature extraction and retrieval.

6

Luca Rossetto, Ralph Gasser, and Heiko Schuldt

Fig. 3. Aggregation of the pixel-wise labels illustrated in Figure 2 into a 32 × 32 grid.

4.1

Extraction

We use the TensorFlow [1] framework to run inference on the DeepLab network.
Several instances of the same network can be run, each trained on one of the
three different datasets such as Cityscapes [5], PascalVOC [7] and ADE20K [33],
pre-trained versions of which have been provided by the authors of [4]. We refer
to their repository on GitHub3 for more information.
Each of these networks is applied to all visual inputs (either individual images or key frames generated from videos) as part of the extraction pipeline. The
outputs of these independent neural networks are then combined during the spatial aggregation step. Since the aggregation simply uses the most common label
per cell, it can easily handle labels originating from an arbitrary number of
sources without any further requirements towards the sources themselves. This
combination procedure enables us to extend the range of detected semantic concepts without the need for retraining the network on a combined dataset. Since
this increases extraction time, network instances can be disabled individually
depending on their applicability to the available content. Adding additional semantic classes does therefore not necessitate the retraining of any existing neural
network classifiers, the embedding however needs to be updated in such cases.
The Word2vec model used for the representation of the semantic concepts
is available pre-trained on the Google News Corpus 4 . Since the concepts are
known beforehand and remain constant, both the word vectors and their low3
4

https://github.com/tensorflow/models/tree/master/research/deeplab
https://code.google.com/archive/p/word2vec/

Query by Semantic Sketch

7

Fig. 4. t-SNE embedding of Word2vec representations of detectable semantic concepts

dimensional embedding can be pre-computed and do therefore not affect extraction time.
4.2

Retrieval

For retrieval, the user is required to sketch the spatial distribution of the different concepts on a 2D canvas. They can pick the concepts from a palette, and
each concept is represented by a random, yet distinct color. Therefore, the user
manually reproduces the aforementioned pixel-wise 2D assignment of concepts
to pixels. This is illustrated in Figure 5.
The resulting 2D map plus an assignment of colors to concepts is subsequently
sent to the retrieval engine and undergoes the processing described in Section 3,
hence yielding a n2 d vector representation of the original sketch. This query
vector is then used to perform a kNN-lookup using the Manhattan (L1) metric.
The top k results are subsequently returned to the UI and displayed to the user.
The results for the query in Figure 5 are depicted in Figure 6.

5

Evaluation

To demonstrate the feasibility of our approach, we evaluate the retrieval time
on the V3C1 dataset [27]. For our test collection of 1046235 vectors, a kNNlookup leveraging naive, linear scanning of the table took 974 ms on average
(Intel Core i7@2.6GHz, 32GB RAM; all the data was stored on a Samsung T5
Portable SSD and read from disk). In order to support even larger collections
or to further reduce retrieval time, the kNN-lookup can be supported by both
precise and approximate index structures. In contrast to [8], no particular index

8

Luca Rossetto, Ralph Gasser, and Heiko Schuldt

Fig. 5. Example of a query image as created by a user via the semantic sketch user
interface. In this example, the user is employing the concepts “grass” and “person”.

is required by our method. The interactive nature of this method has also been
demonstrated publicly on multiple occasions [25,9].
Since our proposed method has several free parameters that can be adjusted
to suit a particular use case, it is not possible to make any definite statements
about the storage requirements. Table 1 therefore shows several possible configurations and compares their storage footprint with those of [8], which we use
as a baseline. It can be seen that our proposed method can reduce the storage
requirements substantially.
Table 1. Storage requirement comparison of the method described in [8] with our
proposed method using different parameters for spatial aggregation, number of dimensions for the semantic embedding as well as the data type for storing numbers (bits
per dimension).
Method
Spatial Aggregation Semantic Dimensions Bits/Dimension Storage use
[8] (baseline)
N/A
N/A
1
100%
our method
32
2
32
26.7%
our method
16
3
32
10%
our method
8
2
8
4.2%

6

Conclusion and Future Work

In this paper, we have presented a practical approach for retrieval of visual
content based on semantic sketches. Our method produces vector representations

Query by Semantic Sketch

9

Fig. 6. Query results returned by the semantic sketch query depicted in Figure 5. In
this example, we searched for the concepts “grass” (yellow) and “person” (cyan). The
results were produced using the V3C video collection [27].

that are substantially more compact than those produced by previous techniques
and does therefore not necessitate the use of any dedicated index structures for
effective retrieval. The length of the produced vectors is dependent on several
parameters which can be tuned to adjust the trade-off between retrieval time
and spatial as well as semantic accuracy to the relevant use case. How much the
query time can be further reduced by the use of such index structures remains
subject to future evaluations. The proposed method is also able to take the
semantic similarity between the supported classes into account and enables users
to easily add support for new semantic classes without the need for retraining of
existing neural network classifiers. Future work will investigate how more stable
embedding alternatives to t-SNE, such as umap [20] can be used to simplify the
process of adding further semantic classes.
Our next step will be to further demonstrate the efficiency and effectiveness
of our approach, which will be the scope of a larger study. The exploration of
different combination strategies for the results produced by several network instances as well as the aforementioned use of other embedding and dimensionality
reduction methods also remains subject to future work.

References
1. Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.: Tensorflow: a system for large-scale machine
learning. In: OSDI. vol. 16, pp. 265–283 (2016)
2. Amato, G., Bolettieri, P., Carrara, F., Debole, F., Falchi, F., Gennaro, C.,
Vadicamo, L., Vairo, C.: Visione at vbs2019. In: International Conference on Multimedia Modeling. pp. 591–596. Springer (2019)

10

Luca Rossetto, Ralph Gasser, and Heiko Schuldt

3. Bui, T., Collomosse, J.: Scalable sketch-based image retrieval using color gradient features. In: The IEEE International Conference on Computer Vision (ICCV)
Workshops (December 2015)
4. Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H.: Encoder-decoder with
atrous separable convolution for semantic image segmentation (2018)
5. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,
Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene
understanding. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR). pp. 3213–3223. IEEE, Las Vegas, NV, USA (2016)
6. Eitz, M., Hays, J., Alexa, M.: How do humans sketch objects? ACM Trans. Graph.
31(4), 44–1 (2012)
7. Everingham, M., Eslami, S.A., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal visual object classes challenge: A retrospective. International
Journal of computer vision (IJCV) 111(1), 98–136 (2015)
8. Furuta, R., Inoue, N., Yamasaki, T.: Efficient and interactive spatial-semantic image retrieval. In: Proceedings of the International Conference on Multimedia Modeling (MMM). pp. 190–202. Springer, Bangkok, Thailand (2018)
9. Gasser, R., Rossetto, L., Schuldt, H.: Multimodal multimedia retrieval with vitrivr.
In: Proceedings of the 2019 on International Conference on Multimedia Retrieval.
pp. 391–394. ACM (2019)
10. Gasser, R., Rossetto, L., Schuldt, H.: Towards an all-purpose content-based multimedia information retrieval system. arXiv preprint arXiv:1902.03878 (2019)
11. Gordo, A., Almazán, J., Revaud, J., Larlus, D.: Deep image retrieval: Learning
global representations for image search. In: European Conference on Computer
Vision. pp. 241–257. Springer (2016)
12. Hu, R., Barnard, M., Collomosse, J.: Gradient field descriptor for sketch based retrieval and localization. In: Image Processing (ICIP), 2010 17th IEEE International
Conference on. pp. 1025–1028. IEEE (2010)
13. Jegou, H., Douze, M., Schmid, C.: Product quantization for nearest neighbor
search. IEEE transactions on pattern analysis and machine intelligence 33(1), 117–
128 (2011)
14. Johnson, J., Karpathy, A., Fei-Fei, L.: Densecap: Fully convolutional localization
networks for dense captioning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2016)
15. Lim, J.H.: Building visual vocabulary for image indexation and query formulation.
Pattern Analysis & Applications 4(2-3), 125–139 (2001)
16. Liu, C., Wang, D., Liu, X., Wang, C., Zhang, L., Zhang, B.: Robust semantic
sketch based specific image retrieval. In: 2010 IEEE International Conference on
Multimedia and Expo. pp. 30–35. IEEE (2010)
17. Lokoč, J., Kuboň, D., Blažek, A.: A comparative study for known item visual
search using position color feature signatures. In: Amsaleg, L., Gumundsson, G..,
Gurrin, C., Jónsson, B.., Satoh, S. (eds.) MultiMedia Modeling. pp. 3–14. Springer
International Publishing, Cham (2017)
18. Maaten, L.v.d., Hinton, G.: Visualizing data using t-sne. Journal of Machine Learning Research (JMLR) 9(Nov), 2579–2605 (2008)
19. Mai, L., Jin, H., Lin, Z., Fang, C., Brandt, J., Liu, F.: Spatial-semantic image
search by visual feature synthesis. In: Computer Vision and Pattern Recognition
(CVPR), 2017 IEEE Conference on. pp. 1121–1130. IEEE (2017)
20. McInnes, L., Healy, J., Melville, J.: Umap: Uniform manifold approximation and
projection for dimension reduction. arXiv preprint arXiv:1802.03426 (2018)

Query by Semantic Sketch

11

21. Mikolov, T., Chen, K., Corrado, G., Dean, J.: Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 pp. 1–12 (2013)
22. Miller, G.A.: Wordnet: a lexical database for english. Communications of the ACM
38(11), 39–41 (1995)
23. Niblack, C.W., Barber, R., Equitz, W., Flickner, M.D., Glasman, E.H., Petkovic,
D., Yanker, P., Faloutsos, C., Taubin, G.: Qbic project: querying images by content,
using color, texture, and shape. In: Storage and retrieval for image and video
databases. vol. 1908, pp. 173–188. International Society for Optics and Photonics
(1993)
24. Redmon, J., Farhadi, A.: Yolov3: An incremental improvement. arXiv preprint
arXiv:1804.02767 (2018)
25. Rossetto, L., Gasser, R., Heller, S., Amiri Parian, M., Schuldt, H.: Retrieval of
structured and unstructured data with vitrivr. In: Proceedings of the ACM Workshop on Lifelog Search Challenge. pp. 27–31. ACM (2019)
26. Rossetto, L., Giangreco, I., Tanase, C., Schuldt, H.: vitrivr: A flexible retrieval
stack supporting multiple query modes for searching in multimedia collections. In:
Proceedings of the 24th ACM international conference on Multimedia. pp. 1183–
1186. ACM (2016)
27. Rossetto, L., Schuldt, H., Awad, G., Butt, A.A.: V3c–a research video collection. In:
International Conference on Multimedia Modeling. pp. 349–360. Springer (2019)
28. Saavedra, J.M., Bustos, B.: An improved histogram of edge local orientations for
sketch-based image retrieval. In: Joint Pattern Recognition Symposium. pp. 432–
441. Springer (2010)
29. Sangkloy, P., Burnell, N., Ham, C., Hays, J.: The sketchy database: learning to
retrieve badly drawn bunnies. ACM Transactions on Graphics (TOG) 35(4), 119
(2016)
30. Tanase, C., Giangreco, I., Rossetto, L., Schuldt, H., Seddati, O., Dupont, S., Altiok,
O.C., Sezgin, M.: Semantic sketch-based video retrieval with autocompletion. In:
Companion Publication of the 21st International Conference on Intelligent User
Interfaces. pp. 97–101. ACM (2016)
31. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: Lessons learned
from the 2015 mscoco image captioning challenge. IEEE transactions on pattern
analysis and machine intelligence 39(4), 652–663 (2017)
32. Xu, H., Wang, J., Hua, X.S., Li, S.: Image search by concept map. In: Proceedings
of the 33rd international ACM SIGIR conference on Research and development in
information retrieval. pp. 275–282. ACM (2010)
33. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing
through ade20k dataset. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR). vol. 1, p. 4. IEEE, Honolulu, HI, USA
(2017)

