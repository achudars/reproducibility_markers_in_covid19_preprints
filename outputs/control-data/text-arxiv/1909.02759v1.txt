A new operation mode for depth-focused high-sensitivity ToF range
finding
Sebastian Werner1 , Henrik Schäfer2 and Matthias Hullin1
1

University of Bonn, Department of Computer Graphics

arXiv:1909.02759v1 [eess.IV] 6 Sep 2019

2

Sony Europe B.V., European Technology Center

Abstract

The sensor then receives a delayed pulse after a certain travel
time. The time delay between emission and acquisition is directly proportional to the distance travelled and subsequently
the depth of the scene. Recent pulse-based systems rely on
the determination of the pulse-shape, altered by scene traversal [8, 23] and implementation of a fast image shutter in front
of the sensor chip. Use cases are as diverse as acquiring images [12] or object motion [16] in an “around the corner” setting, measuring 3D shape [21] or separation of light transport
components [22]. The simplicity of the underlying concept
comes at the cost of elevated hardware requirements, enabling
the measurement of time delays in the order of picoseconds
in low SNR scenarios. Due to these limitations, such systems
often consist of a single-pixel sensor only and require timeconsuming pixel-wise scanning of the scene. Despite those
shortcomings, the strength of P-ToF systems lies in their high
depth resolution.

We introduce pulsed correlation time-of-flight (PC-ToF) sensing, a new operation mode for correlation time-of-flight range
sensors that combines a sub-nanosecond laser pulse source
with a rectangular demodulation at the sensor side. In contrast to previous work, our proposed measurement scheme attempts not to optimize depth accuracy over the full measurement: With PC-ToF we trade the global sensitivity of a standard C-ToF setup for measurements with strongly localized
high sensitivity – we greatly enhance the depth resolution for
the acquisition of scene features around a desired depth of interest. Using real-world experiments, we show that our technique is capable of achieving depth resolutions down to 2 mm
using a modulation frequency as low as 10 MHz and an optical
power as low as 1 mW. This makes PC-ToF especially viable
for low-power applications.

1

Correlation time-of-flight range finding. To alleviate the
need for fast and costly hardware, amplitude-modulated
continuous-wave (AMCW) ToF systems have been developed
that consist of temporally modulated light sources and sensors [20, 15]. At the core of these correlation time-of-flight
(C-ToF) setups lie the modulation (at the light source) and demodulation (at the sensor) functions, that are used to code and
decode the illumination signal. Current C-ToF setups utilize
sinusoidal or square coding functions. Upon scene traversal,
the amplitude modulated illumination undergoes a phase shift
with respect to the original signal emitted by the light source.
This phase shift is proportional to the traveled distance and is
acquired using a correlation measurement between the emitted
and received signal. As modulation and demodulation function are periodic, these measurements implicitly are limited to
the so-called unambiguity range, which depends on the frequency of the modulation signal. Our approach relies on a
homodyne setup, where the frequency of the modulation and
demodulation signals are equal.

Introduction

Time-of-flight (ToF) range finding setups support a vast
amount of applications, ranging from robotics closely tied with
exploration and automated manufacturing to motion capture
and 3D mapping, as well as biometrics [14]. They are all connected by the common need for truthful representations of the
three-dimensional environment. As a consequence, all applications share the desire for both – high spatial resolution as
well as precise depth estimation. Thanks to advances in sensor
technology, the former rises with every generation of sensors,
whereas the depth resoltuion depends on deisgn choices, such
as the time and power budget of any such sensor and hence
fundamentally limited by noise. Especially for low signal-tonoise ratio (SNR) measurements, accurate detection of distances becomes a challenge that received a lot of attention
from the scientific community. There exists a number of range
finding approaches based on ToF measurements, which can be
divided into two classes that differ in both hardware requirements and reconstruction techniques.

Depth resolution enhancements for C-ToF systems. In
comparison to P-ToF systems, correlation-based ToF systems
exhibit considerably lower depth resolution. This is due to
the fact that C-ToF systems rely on single- or few-frequency
signals such as sinusoidal or triangular [2] modulation and de-

Direct pulsed time-of-flight range finding. Direct pulsebased ToF systems (P-ToF) [4, 13] were the first ToF systems
to be employed for range finding purposes. These setups emit a
single short (pico-/nanosecond) laser light pulse into the scene.
1

modulation signals, which in turn renders the depth estimation
more prone to errors from measurement noise [1]. In recent
years a great amount of research has been done to mitigate effects from higher harmonics of such modulation-demodulation
signal pairs [18, 19]. Dual-frequency setups try to enhance
depth resolution without the loss of unambiguous measurement range by combining high- and low-frequency measurements [10, 9]. More recently, Gupta et al. [6] presented a
framework for general C-ToF range finding, which allows for
the simulation and computation of the depth resolution performance for arbitrary modulation-demodulation signal pairs.
In addition, they also used their framework to develop an optimized Hamiltonian coding function, which achieves depth resolutions below 1 cm over the full ambiguity range. Closely tied
to the work presented in this paper, Payne et al. [17] discuss
the optimal choice of the duty cycle for the chosen illumination
signal for the special case of sinusoidal and square illumination modulation. They point out that a reduction of duty cycle
results in an increased peak power and thus better SNR for the
illumination signal, whereat the linear relation used for phase
estimation is violated by a change of frequency content of the
signal. All these approaches share the desire for improvements
of the depth sensitivity over the full unambiguity range, which
are inherently deemed to result in a tradeoff due to their respective relations to the modulation frequency. This is due to the
fact, that the limited bandwidth of the illumination signal(s)
directly relates to the depth variations the procedure can truthfully distinguish. With pulsed correlation time-of-flight sensing (PC-ToF), we propose a dual-measurement scheme that
explicitly makes use of this information content. This novel
hybrid approach combines the high depth resolution and noise
resilience of P-ToF with the low-cost hardware of C-ToF setups at the cost of ambiguity: We replace the (continuous)
modulation function with pulsed illumination but maintain a
continuous demodulation signal. This way, we make use of
higher harmonics in the modulation signals, previously treated
as artefacts. The two steps of PC-ToF are:

Figure 1: Upper panel: Schematic visualization of our correlation ToF setup. A signal generator drives both – modulation
of the illumination and the sensor gain. In pulsed operation
mode, the laser light source emits a pulse chain i(t). Otherwise, we utilize a sinusoidal illumination modulation. The
light is guided onto a mirror on a linear stage, which allows to
control the distance traveled, required for our validation procedure. For uniform illumination of the scene, the light is focused onto a diffuser.Per pixel p, the sensor then retrieves a
shifted version of the illumination signal, which is correlated
with the sensor modulation s(t). Lower panel: Pictures of our
lab setup, as indicated in the upper panel.

their notation: Correlation time-of-flight setups (see Fig. 1)
consist of an amplitude-modulated light source and a gainmodulated sensor. We start by defining the modulation functions of the light source i(t) and sensor gain s(t) respectively.
Like [6, 3, 7, 11], we assume the absence of any indirect or
multi-bounce light, which allows us to describe the scene response as a single scattering event at the precise depth Γ. This
1. Obtain a rough depth estimate with standard C-ToF range results in a shift of the modulation function i(t). In general, the
finding methods and select a depth of interest (DOI) for irradiance E(p, t) that arrives at pixel p can then be described
close inspection.
as

E(p, t) = Ea (p) + Ec (p) i(t − 2Γ/c)
(1)
2. precisely measure the depth around a user-specified DOI
with depth resolutions down to 2 mm, utilizing our hybrid
where c is the speed of light, Ea (p) denotes the ambient light
approach.
component and Ec (p) is the mean pixel irradiance due to the
The power consumption of C-ToF systems is dominated by op- modulated light, encoding the optical properties of the scene.
tical output power and sensor modulation. Its light efficiency The shifted normalized illumination modulation is described
and the use of slow modulation frequencies make PC-ToF es- by i(t − 2Γ/c). The sensor then records the pixel intensity
pecially suitable for low power applications, for example in
Z τ
mobile hardware.
I(p) =
E(p, t)s(t)dt
0

Z

2

Correlation time-of-flight image formation

τ

i (t − 2Γ(p)/c) s(t) dt,

= Ia (p) + Ec (p)

(2)

0

Rτ
where τ is the exposure time and Ia (p) = 0 Ea (p)s(t)dt
We will briefly revisit the image formation model for corre- is the incident ambient light. Eq. 2 can be understood as a
lation ToF as described in [5, 6] and, for simplicity, adapt cross-correlation function. From this, we define the normal2

ized correlation function
Z
C(Γ) =

then described as the convolution ~ of a Dirac comb with a
Gaussian G with standard deviation σM ,
X
i(ϕ) =
[δ(ϕ − n 2π − ϕΓ ) ~ G(ϕ, σM )] ,
(8)

τ

i (t − 2Γ/c) s(t) dt.

(3)

0

n

This way, we are able to express the full image formation pro- where the pulse width is assumed to equal the FWHM. We
cess of correlation time-of-flight imaging via the image forma- model the demodulation signal s as a square signal onto which
tion equation
we apply a Gaussian smoothing kernel to account for nonvanishing rise times:
I(p) = Ec (p)C(Γ) + Ia (p).
(4)



X
ϕ − n 2π + θi
~ G(ϕ, σD ) . (9)
si (ϕ) =
rect
This equation reveals three unknowns Ec (p), Γ, Ia (p) which
π
n
have to be determined pixel-wise. We require K ≥ 3 measurements or samples of the correlation function Ci (Γ) for To compute the correlation function (Eq. 3), we utilize the fact
i ∈ {0, . . . , K}. These measurements are commonly realized that the convolution of
p two Gaussians yields another Gaussian
2 + σ 2 . Assuming the pulse width
σD
function
with
σ
=
by inserting an additional phase shift θi into the demodulation
M
being smaller than the period of the modulation (FWHM <<
function, such that
ωT ), we obtain


s(t) → si (t + θi /ω) ; θi ∈ [0, 2π)
(5)
Z
ϕ + θi − ϕΓ
1 2π
rect
~ G(ϕ, σ) dϕ
Ci (ϕΓ ) =
ω 0
π
and data acquisition is performed for K equally spaced phases.
√
Gupta et al. [6] continue to develop a depth precision measure
√
√
1 π
√ {erf( aϕ2 ) − erf( aϕ1 )
=
χ̄c , which encodes the average depth accuracy depending on
ω2 a
the average optical properties encoded
π
1
qP in Ec,mean as well as
ϕ2,1 = ± + ϕΓ − θi ; a =
.
(10)
K
2
the noise standard deviation Ω =
2
2σ 2
i=1 σi (assumed to be
constant) for K-tap correlation ToF measurements as
Depth sensitivity. The depth sensitivity (Eq. 6) is driven by
Z sX
Ec
the gradient of the normalized correlation function
2
(∂Ci (Γ)/∂Γ) dΓ,
(6)
χ̄ =
ΩΓrange Γ
∂Ci (ϕΓ ) 1 n
π
i
=
exp(−a(ϕΓ − θi + )2 )
∂ϕΓ
ω
2
where Γrange is the unambiguous depth range.
π o
(11)
− exp(−a(ϕΓ − θi − )2 ) ,
2
3 A new operation mode for time-of- which essentially are two Gaussians located at
π
(12)
ϕΓ = θi ± ,
flight range finding
2
Our foremost aim is to increase the depth sensitivity not on a one with negative, the other with positive amplitude (see
global scale (over the full ambiguity range) but locally. This Fig. 2, middle). These Gaussians indicate that the maxiallows us to select a certain depth of interest (DOI), around mum (absolute) gradient of the correlation function C(Γ) is
which we can retrieve the depth information of the scene with achieved at this local maximum and minimum respectively,
high accuracy. From Eq. 6 we directly see that depth sensi- which depend on the value of ϕΓ and hence the distance totivity depends on the gradient of the (normalized) correlation wards an observed object. This means that our PC-ToF apsignal. Ideally, ∂C/∂Γ → ∞ which would result in a vanish- proach exhibits strong sensitivity in a narrow range around a
ing rise time TRise = t(max(C)) − t(min(C)) → 0. We base specific phase, the phase of maximum sensitivity ϕ0 .
our considerations on the idealized case of a combination of
pulse trains (Dirac comb) for our modulation signal and using The depth of interest (DOI). From Fig. 2 (middle, right) it
a rectangular demodulation signal on the sensor side, both with becomes clear that only measurements with a specific depth
frequency ν. To unify considerations and clarify, that we are (Γ0 (ϕ0 )) can be made at maximum sensitivity and produces
limited to exactly one period of the modulation and demodula- meaningful results. Relation Eq. 12 reveals, that a sensible
tion signals before ambiguities arise, we switch the integration choice of θi allows to shift the correlation function such, that
the extrema are located at the desired phase ϕΓ = ϕ0 and
variable to phase ϕ via
hence depth Γ0 . This depth we call the depth of interest (DOI).
2ωΓ
ϕ = ωt; ω = 2πν; ϕΓ =
.
(7) In a K-tap measurement system it is not directly clear which
c
of the phase shifts θi should be chosen such that ∂C/∂ϕΓ exhibits an extremum. For simplicity we will choose θ0 which
We describe our (real) modulation and demodulation signals introduces a global phase shift θG as the θi are equally spaced:
as a chain of Gaussian pulses and a smoothed rectangular sigπ
2ωΓ0
π
− ; θi → θi + θG
(13)
ϕ0 = ϕΓ ∓ − θ0 =
nal chain respectively (cf. Fig. 2). The modulation signal is
2
c
2
3

Figure 2: Left: Signals of the pulsed ToF approach. Note that the modulation signal i(ϕ) is rescaled for visualization. The
demodulation signals are plotted for the denoted shifts applied, corresponding to a standard 4-tap ToF measurement. Middle:
Correlation function C0 (ϕΓ ) and its first derivative ∂C0 /∂ϕΓ in dependence on the phase (and hence, depth). Given a chosen
phase shift θi , only certain depths lie within the sensitive range, here indicated as the region between two dashed red vertical
lines surrounding an extremum. Only depths corresponding to phases within this range lead to reliable depth measurements.
The phase of maximum sensitivity is reached when ∂C0 /∂Γ reaches an extremum, denoted by the dashed blue line. We denote
the corresponding depth as the depth of interest Γ0 , onto which we are able to focus by shifting θi . Right: Close-up of C0 (ϕΓ )
and ∂C0 /∂ϕΓ around the first extremum ϕ0 . To restrict the sensitive range to non-negligible values, we define it as the beam
width of the Gaussian.

Figure 3: Demodulation signals and reflected pulse and corresponding correlation function measured in a single pixel. The
demodulation signals are color coded to represent their respective phase shift for a 4-tap sampling procedure. Left: The
reflected pulse coincides with the lower and upper plateaus of s(ϕ) due to a bad choice of Γ0 and hence θG . The 4 measured
samples are obtained at maximum and minimum value respectively (see right panel, solid line). This way, ambiguity arises and
no certain phase can be reconstructed, as nearby depths (or reflected pulses) will yield the same result. Middle: We choose a
rough estimate for the DOI Γ0 and adjust the phase shift ϕ0 by applying θG such that the reflected pulse lies within the sensitive
range. This results in a coincidence of reflected pulse and rising signal edge of s0 (ϕ). The 4 measured samples now refer to a
unique phase (see right panel, dashed line) and a small change in measurement value will result in a large change of the phase
estimate.

4

Sensitive range. The phase of maximum sensitivity is restricted to one particular value and corresponding depth,
whereat measurements within a surrounding phase interval
also benefit from increased depth sensitivity. We call this interval the sensitive range ∆Γ and corresponding phase ∆ϕΓ .
Mathematically it can be described as the interval with nonzero
first derivative, i.e., ∂C0 /∂ϕΓ 6= 0. We estimate this interval
as the rise time of the signal edge surrounding ϕ0 . In general
we would focus on the rising signal edge (see Fig. 2, right)
and ask for the roots of the first derivative. As an example
consider a sinusoidal correlation function, where the phases
corresponding to the sensitive range are exactly π apart, resulting in full sensitivity over the ambiguity range. On the other
hand, the derivative of our correlation function consists of two
Gaussians with nonzero value everywhere. We therefore define the sensitive range as the interval bounded by the points
where the Gaussian reaches 1/e2 of its peak, often also called
the beam width.
∆ϕΓ = 4σ; ∆Γ =

∆ϕΓ c
2ω

Table 1: Parameters of the hardware used for our measurement
setup (cf. Fig. 1)
Laser light source
Omicron QuixX 852-150
Wavelength
852 nm
Pulse width (FWHM) <500 ps
Average power
<1mW
Sensitive range ∆Γ
/ 0.75 m

Lens
Fujinon HF35SA-1
Focal length
35mm
Aperture
f/2.0
#Acquisitions 25

this affects the final resolution of the range imaging system.
The modulation source allows setting the phase with 14 bits
precision, leading to phase steps as small as ∆ϕ = 2π/214 .
The parameters for our measurements, as well as the specific
hardware used, can be found in Tab. 1.

3.2

Setup and measurement procedure

Fig. 1 shows a schematic illustration as well as pictures of our
setup: The pulsed laser illumination is guided onto a mirror
mounted on a linear stage before being reflected back onto a
diffuser for uniform illumination of the scene. The linear stage
allows to control the distance travelled which directly translates to a proportional phase shift. This is equivalent to adding
a phase shift in hardware and is used for validation only, but
could in principle also be utilized for calibration. The sensor
observing the scene then retrieves a delayed version of the illumination signal, which is correlated with the demodulation
signal on a per-pixel level.

(14)

For illustration, consider a PC-ToF measurement which has
a true depth of Γ. To achieve maximum sensitivity, the ideal
solution would be to set the DOI to the exact depth and acquire
the necessary phase shift θG . Physically this is the case when
the reflected illumination pulse coincides with the rising edge
of the demodulation signal. In contrast, the reflected pulse
coincides with one of the plateaus of the demodulation signals
(Fig. 3 left) if the true depth Γ and Γ0 differ too strongly. The
measured Ci do not change upon small changes of the depth –
the measured values are outside the sensitive range ∆Γ.
However, a priori the exact depth value is unknown. To still
achieve a high resolution depth measurement we will choose
the DOI Γ0 such that it is close to the (unknown exact) depth
Γ. Having an estimate for Γ0 that lies within the sensitive
range results in a coincidence of reflected pulse and signal
edge (Fig. 3 right) – the measurement operates at increased,
albeit not necessarily maximum sensitivity and yields an accurate result for Γ.

3.1

Sensor
PMD 19k-S3
Resolution
160x120
Frequency
10 MHz
Shutter time 1 ms

Depth reconstruction. The CamBoard nano,
as
most available C-ToF systems, employs a four-tap
measurement procedure that acquires four samples
{C0 (p), C1 (p), C2 (p), C3 (p)} of the correlation function per pixel p measured using demodulation functions
shifted by θi ∈ {0, 21 π, π, 32 π}. Instead of disclosing these
four values, the CamBoard nano returns the differences of
samples separated by ∆ϕ = π. For C-ToF systems utilizing
sinusoidal modulation and demodulation signals it can be
shown [20] that the unknown phase ϕΓ (p) corresponding to
the range Γ(p) can be computed as
2ωΓ
= atan (Ψ)
c
C0 (p) − C2 (p)
Ψ=
C1 (p) − C3 (p)

Hardware

ϕΓ =

Our approach describes an additional mode of operation for
existing correlation ToF range finding setups, which requires
certain hardware characteristics to be available.
First, we require a correlation ToF sensor. These devices are
either externally modulated by a high-frequency signal or employ their own signal generator for this purpose. We utilize
a PMD CamBoard nano (based on their 19k-S3 sensor) with
external DDS modulation source at 10 MHz [7], which also
triggers the laser source. Second, we require the light source
to emit pulses with the given modulation frequency and narrow pulsewidth. To this end we utilize an Omicron QuixX
laser with pulse width FWHM ≤ 500 ps.
Third, for calibration we require a phase shift to be applied to
either the modulation or demodulation signal. This phase shift
needs to be adjustable with as high an accuracy as possible, as

(15)

and we denote the argument Ψ as the raw fraction. This expression has two major benefits: First, the result of the differences is independent on ambient light Ia (p) and second, the
fraction of the two differences cancels out the scene dependent
factor Ec (p). Still, Eq. 15 is only valid for sinusoidal signals
and results in strong systematic errors [18] for non-harmonic
correlation functions such as ours. Instead, we will rely only
on measurements of the raw fraction in dependence of a chosen depth of interest Γ0 and phase shift θG .
Calibration. In contrast to the simple expression for sinusoidal correlation ToF (see Eq. 15), we cannot easily invert
5

Figure 4: Left: Measured raw fraction Ψ for 512 equally spaced phase shifts ϕ0 ∈ [0, 2π). We estimate the values for the
upper and lower plateaus of the correlation function (red horizontal lines). From the plateau values we compute a zero-crossing
equivalent value, which is exactly half the difference between the plateaus. We further estimate the sensitive range, here denoted
by the red rectangle. Middle: We perform an additional measuremetn of Ψ for phases ϕ0 within the sensitive range enclosing
the rising signal edge, which is performed at the highest possible accuracy in terms of ϕ0 (14 bit). The data acquired exhibits
noise, which leads to ambiguities when used for a lookup table. Instead, we fit a continuous spline representation to circumvent
that issue. This way, we obtain a ϕ0 -Ψ mapping that allows to estimate the offset a measurement exhibits from a reference
phase. Right: As we can only chose a single depth of interest via ϕ0 per measurement, we obtain the phase of maximum
sensitivity for each single pixel and subtract it from the median over all pixels. This way, we obtain a calibration mask that
employs a per-pixel phase correction with respect to ϕ0 .
our correlation function for our pulsed approach (cf. Eq. 10).
Instead we perform a calibration step in which we measure
the raw fraction with a homogeneous calibration target (white
diffuse plate) at a fixed distance to sensor and light source.
Figure 4 (left and middle) visualizes the calibration process
for a single pixel. First, we perform measurements with 512
equally spaced phase shifts θG . This reveals the upper and
lower plateaus of the (ideally) rectangular correlation function.
In theory, these plateaus have equal absolute value and hence
the phase of maximum sensitivity is at Ψ = 0. However, for
real measurements we have to compute a zero equivalent value
which relates to the actual phase of maximum sensitivity exactly between the plateaus. This value refers to the depth Γ at
which the calibration target is placed. We estimate the limits
of the sensitive region ∆Γ. Second, we measure Ψ within the
sensitive region by stepping over the corresponding θG with as
high precision as possible (14 bit). This yields lookup values
that could be used directly for estimating phases from measured values. However, noisy measurements introduce ambiguities into a numerical inversion, which requires a smoothing
step to obtain a monotonously rising function for unambiguous phase estimation. To this end, we fit a univariate spline
representation to the measurements on a per-pixel level. The
resulting lookup table can now be used to estimate the phase
offset from ϕ0 , which in turn can be controlled to have a desired value. Third, we note that not necessarily all pixels of
a C-ToF sensor exhibit the exact same behaviour, which in
our case leads to a different spline representation and phase
of maximum sensitivity per pixel. These values are distributed
around the reference phase, which corresponds to our fixed
distance. As we can only chose a single depth of interest Γ0
and corresponding phase ϕ0 , we obtain the zero equivalent for
each single pixel and subtract it from the median over all pix-

Figure 5: Example validation procedure for camera pixel
(60, 80). Left: The acquired calibration data and its fitted
spline representation. The extracted phase of maximum sensitivity ϕ0 is given as a dashed red line. To validate our calibration, we offset the mirror on the linear rail (cf. Fig. 1) by values
within [−2.5,2.5] cm from the reference depth used for calibration. We invert the phase value from the measurement value
using the spline representation. The measurements and phase
estimates are color coded from yellow to blue, representing
the order of measurements. Right: All 51 measurements, averaged over all camera pixels, plotted against the ground truth
depth. The RMS error is approximately 0.6 mm.

els. This way we obtain a calibration mask that employs a
per-pixel phase correction with respect to the DOI.
Validation. To validate our calibration, we need to assess
how closely we can reconstruct changes of depth within a
scene with the available calibration. As the calibration allows
to measure an offset from ϕ0 , we perform measurements with
a planar calibration target again but now we change the distance the light has to travel by offsetting the mirror on the linear rail (cf. Fig. 1). We perform a total of 51 measurements
with offsets in the range of [−2.5,2.5] cm at 1 mm accuracy
6

which this systematic error could result: First, we compute our
ground truth via simulation, which relies on the correct pose
estimation for the 3D models. This is a difficult task on its
own, especially given the low resolution images. For example,
Fig. 9 reveals that indeed our pose estimation is not perfectly
accurate, but a tilt is clearly visible. Second, our simulation
assumes only a simple pinhole camera and the conjunction of
light source and camera at the same position. However, in reality it is not possible to place both at the exact same location, an
offset comes into play which will result in deviations with spatial dependency. In addition our approach, as well as all available correlation ToF systems, inherently suffers from the socalled multipath interference problem (MPI): Whenever multiple light paths from different scene points end up in one sensor pixel, the resulting depth estimate for this pixel is shifted
to higher values, best visible near corners (Fig. 8 bottom left).
Still, our approach reveals depth differences as fine as 2 mm
(cf. Fig. 8 middle), utilizing measurements with only 1 mW
average power and 10 MHz modulation frequency by choosing
a depth of interest based on a rough depth estimate. The approach is inherently dependent on the pulse shape of the modulation and rise time of the demodulation signal and hence the
shape of those signals. The modulation frequency is only a
secondary factor.
In future work, we would like to test the limits of the approach
for more contemporary ToF sensors, which operate at frequencies of 100 MHz or more. With higher frequencies, we expect
shorter rise times of the sensor modulation and at the same
time we can accumulate more laser pulses in the same exposure time, increasing the SNR. This should also allow us to
test our approach on scenes with larger depth ranges - focusing on different targets that ideally are allowed to be meters
apart and reconstructing the depths within a few centimeters
around the respective DOI with high accuracy. Future work
should also include a way to circumvent the calibration procedure described in Sec. 3.2: Using approximations and by
allowing more realistic modulation and demodulation signals
(e.g., the saddlepoint found in the measurement of Ψ on the rising signal edge - Fig. 5), we would like to find a closed-form
solution to invert the measurement formalism. A calibration
could then find the parameters of this analytic solution instead
of generating a lookup table.
In the end, our approach is most suited for low power scenarios, such as mobile devices or for static measurement scenarios, where a detailed depth estimation for objects at a distinct
range is required. With PC-ToF we trade the global sensitivity
of a standard C-ToF setup (covering the unambiguity range)
for highly increased sensitivity around a depth of interest: The
broader the sensitive range, the less the maximum sensitivity
and vice versa. In turn this allows for a task-specific tailoring of the pulse width and rise time, the two parameters that
drive the depth resolution and sensitive range achievable with
PC-ToF.

Figure 6: Pictures of the 3D models we printed and measured
using our pulsed ToF approach. Left: Standard target box
consisting of 2 ramps and stairs with a step height of 5 mm.
This scene is used to validate the working principle of our approach. Right: Target box with detailed stairs. We generated
2 variants of this scene with the different measures separated
as 1.5 mm / 3 mm. The most detailed stairs have a step height
of 1 mm and are considered the limit test case for our method.
with respect to the reference depth used for calibration. The
results (cf. Fig. 5) indicate a good match between our depths
obtained from a phase estimate using the spline representation
for inversion and the ground truth depth values. Note that the
validation is performed within a close range around the phase
of maximum sensitivity, well within the sensitive range.
Performing a PC-ToF measurement. After validation, our
measurement procedure (cf. Fig. 7) is straightforward: We first
acquire a rough depth estimate using a low power C-ToF measurement with sinusoidal modulation and demodulation with
our system. With this, we obtain a rough estimate of the depth
the object of interest is located at and adjust θG such that the
DOI is matched and interesting scene features lie within the
sensitive range. Another measurement, now in pulsed operation delivers a much better resolved depth estimate. All measurements are obtained using the parameters given in Tab. 1,
whereat the only difference between the operation modes is
the usage of the different modulation signals.

4

Results and conclusion

To assess the capabilities of our approach, we designed and
3D-printed three different targets, depicted in Fig. 6. The targets dimensions are chosen such that we cover a range of relative depth differences, starting from discrete steps of 5 mm
down to 1 mm. Given the results from the validation measurement (cf. Sec. 3.2), we regard signal changes that originate
from such small depth differences as the limit our approach
can resolve; in fact, we cannot assume to exactly match DOI
and real depth (see Sec. 2), but as long as the depth remains
in the sensitive range, the measurement is performed at high
albeit not maximum sensitivity.
Fig. 8 shows the results of our measurements for three distinct
setups, each with a different target. All of our results show
an increasing discrepancy between ground truth and measurement towards the right edge of the depth maps, best visible
in the depth slices in Fig. 8. There are two main factors from
7

Figure 7: (Left): We perform a single ToF measurement with sinusoidal modulation and demodulation signals at 1 mW – The
low SNR disallows to obtain a high resolution depth map but instead returns a rough depth estimate (about 50 cm) for features
of the scene we are interested in, the depth of interest (DOI). (Middle): Using Eq. 13 we compute the phase shift required
to let the reflected pulse and the signal edge coincide, focussing our measurement onto the DOI Γ0 . The shaded rectangles
here illustrate the sensitive range surrounding the DOI, both in phase space and for 3D depths. (Right): We perform a single
measurement at 1 mW after applying the shift and switching the mode of operation to our pulsed acqusition. The obtained
depth reconstruction exhibits well improved depth resolution for all scene features that lie within the sensitive range.

Figure 9: Left: Ground truth depth map obtained from our
simulation for the first target. Note that our pose estimation
does not deliver a perfect match, comparison with Fig. 8 (lower
left) reveals that the orientation does not perfectly match, leading to systematic errors. Right: Comparison of the C-ToF
measurement with sinusoidal modulation of the scene at 1 mW
(cf.Fig. 7) with the ground truth depth for the indicated slices.
Figure 8: Depth maps (left) and comparison of depth slices The measured depth overall follows the trend of the ground
(right) with our simulated ground truth data. All slices taken truth but is too noisy to reveal any details.
across stairs are averaged over 5 pixels in vertical direction.
From top to bottom, the step heights of the stairs are 1 mm
(green), 1.5 mm (violet); 2 mm (blue), 3 mm (violet); 5 mm
(green). The remaining three slices (middle, green; bottom,
blue and violet) are taken across a more complex inhomogeneous geometry and two slopes with same maximum height
but different length respectively.

8

Acknowledgements This work was supported by the Computational Imaging Group from the Stuttgart Technology Center of Sony Europe B.V. and by the ERC Starting Grant
ECHO.
[16]

References
[17]

[1] B. Buttgen and P. Seitz. Robust optical time-of-flight range
imaging based on smart pixel structures. IEEE Transactions
on Circuits and Systems I: Regular Papers, 55(6):1512–1525,
July 2008.
[2] R. Ferriere, J. Cussey, and J. M. Dudley. Time-of-flight range
detection using low-frequency intensity modulation of a CW
laser diode: application to fiber length measurement. Optical
Engineering, 47:47 – 47 – 6, 2008.
[3] D. Freedman, Y. Smolin, E. Krupka, I. Leichter, and
M. Schmidt. SRA: Fast removal of general multipath for ToF
sensors. In D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars,
editors, Computer Vision – ECCV 2014, pages 234–249, Cham,
2014. Springer International Publishing.
[4] B. S. Goldstein and G. F. Dalrymple. Gallium arsenide injection
laser radar. Proceedings of the IEEE, 55(2):181–188, Feb 1967.
[5] M. Gupta, S. K. Nayar, M. B. Hullin, and J. Martin. Phasor
imaging: A generalization of correlation-based time-of-flight
imaging. ACM Trans. Graph., 34(5):156:1–156:18, Nov. 2015.
[6] M. Gupta, A. Velten, S. K. Nayar, and E. Breitbach. What
are optimal coding functions for time-of-flight imaging? ACM
Trans. Graph., 37(2):13:1–13:18, Feb. 2018.
[7] F. Heide, M. B. Hullin, J. Gregson, and W. Heidrich. Lowbudget transient imaging using photonic mixer devices. ACM
Trans. Graph., 32(4):45:1–45:10, July 2013.
[8] G. J. Iddan and G. Yahav. 3D imaging in the studio (and elsewhere...). In Three-Dimensional Image Capture and Applications IV, volume 4298, 2001.
[9] A. P. P. Jongenelen, D. G. Bailey, A. D. Payne, A. A. Dorrington, and D. A. Carnegie. Analysis of errors in ToF range imaging with dual-frequency modulation. IEEE Transactions on Instrumentation and Measurement, 60(5):1861–1868, May 2011.
[10] A. P. P. Jongenelen, D. A. Carnegie, A. D. Payne, and A. A.
Dorrington. Maximizing precision over extended unambiguous
range for ToF range imaging systems. In 2010 IEEE Instrumentation Measurement Technology Conference Proceedings, pages
1575–1580, May 2010.
[11] A. Kadambi, R. Whyte, A. Bhandari, L. Streeter, C. Barsi,
A. Dorrington, and R. Raskar. Coded time of flight cameras:
sparse deconvolution to address multipath interference and recover time profiles. ACM Transactions on Graphics (TOG),
32(6):167, 2013.
[12] A. Kirmani, T. Hutchison, J. Davis, and R. Raskar. Looking
around the corner using transient imaging. In 2009 IEEE 12th
International Conference on Computer Vision, pages 159–166,
Sept 2009.
[13] W. Koechner. Optical ranging system employing a high power
injection laser diode. IEEE Transactions on Aerospace Electronic Systems, 4:81–91, Jan. 1968.
[14] A. Kolb, E. Barth, R. Koch, and R. Larsen. Time-of-flight sensors in computer graphics. In M. Pauly and G. Greiner, editors,
Eurographics 2009 - State of the Art Reports. The Eurographics
Association, 2009.
[15] R. Lange, P. Seitz, A. Biber, and S. C. Lauxtermann. Demodulation pixels in ccd and cmos technologies for time-of-flight

[18]

[19]

[20]

[21]

[22]

[23]

9

ranging. In Sensors and camera systems for scientific, industrial, and digital photography applications. International Society for Optics and Photonics, 2000.
R. Pandharkar, A. Velten, A. Bardagjy, E. Lawson, M. Bawendi,
and R. Raskar. Estimating motion and size of moving non-lineof-sight objects in cluttered environments. In CVPR 2011, pages
265–272, June 2011.
A. D. Payne, A. A. Dorrington, and M. J. Cree. Illumination
waveform optimization for time-of-flight range imaging cameras. In Videometrics, Range Imaging, and Applications XI. International Society for Optics and Photonics, 2011.
A. D. Payne, A. A. Dorrington, M. J. Cree, and D. A. Carnegie.
Improved linearity using harmonic error rejection in a full-field
range imaging system. In Three-Dimensional Image Capture
and Applications 2008. International Society for Optics and
Photonics, 2008.
A. D. Payne, A. A. Dorrington, M. J. Cree, and D. A. Carnegie.
Improved measurement linearity and precision for amcw timeof-flight range imaging cameras. Applied optics, 49 23:4392–
403, 2010.
R. Schwarte, Z. Xu, H.-G. Heinol, J. Olk, R. Klein,
B. Buxbaum, H. Fischer, and J. Schulte. New electro-optical
mixing and correlating sensor: facilities and applications of the
photonic mixer device (pmd). In Sensors, Sensor Systems, and
Sensor Data Processing. International Society for Optics and
Photonics, 1997.
A. Velten, T. Willwacher, O. Gupta, A. Veeraraghavan, M. G.
Bawendi, and R. Raskar. Recovering three-dimensional shape
around a corner using ultrafast time-of-flight imaging. Nature
communications, 3:745, 2012.
D. Wu, M. O’Toole, A. Velten, A. Agrawal, and R. Raskar. Decomposing global light transport using time of flight imaging. In
2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 366–373, June 2012.
G. Yahav, G. J. Iddan, and D. Mandelboum. 3D imaging camera for gaming application. In 2007 Digest of Technical Papers
International Conference on Consumer Electronics, pages 1–2,
Jan 2007.

