Don’t Jump Through Hoops and Remove Those
Loops: SVRG and Katyusha are Better Without the
Outer Loop

arXiv:1901.08689v2 [cs.LG] 5 Jun 2019

Dmitry Kovalev
KAUST, KSA
dmitry.kovalev@kaust.edu.sa

Samuel Horváth
KAUST, KSA
samuel.horvath@kaust.edu.sa

Peter Richtárik
KAUST, KSA and MIPT, Russia
peter.richtarik@kaust.edu.sa

Abstract
The stochastic variance-reduced gradient method (SVRG) and its accelerated variant
(Katyusha) have attracted enormous attention in the machine learning community
in the last few years due to their superior theoretical properties and empirical
behaviour on training supervised machine learning models via the empirical risk
minimization paradigm. A key structural element in both of these methods is
the inclusion of an outer loop at the beginning of which a full pass over the
training data is made in order to compute the exact gradient, which is then used
in an inner loop to construct a variance-reduced estimator of the gradient using
new stochastic gradient information. In this work we design loopless variants of
both of these methods. In particular, we remove the outer loop and replace its
function by a coin flip performed in each iteration designed to trigger, with a small
probability, the computation of the gradient. We prove that the new methods enjoy
the same superior theoretical convergence properties as the original methods. For
loopless SVRG, the same rate is obtained for a large interval of coin flip probabilities,
including the probability 1/n, where n is the number of functions. This is the first
result where a variant of SVRG is shown to converge with the same rate without the
need for the algorithm to know the condition number, which is often unknown or
hard to estimate correctly. We demonstrate through numerical experiments that the
loopless methods can have superior and more robust practical behavior.

1

Introduction

Empirical risk minimization (aka finite-sum) problems form the dominant paradigm for training
supervised machine learning models such as ridge regression, support vector machines, logistic
regression, and neural networks. In its most general form, a finite sum problem has the form
n
P
def
minx∈Rd f (x) = n1
fi (x) ,
(1)
i=1

where n refers to the number of training data points (e.g., videos, images, molecules), x is the vector
representation of a model using d features, and fi (x) is the loss of model x on data point i.
Variance-reduced methods. One of the most remarkable algorithmic breakthroughs in recent years
was the development of variance-reduced stochastic gradient algorithms for solving (1). These
methods are significantly faster than SGD [18, 17, 27] in theory and practice on convex and strongly
Preprint. Under review.

convex problems, and faster in theory on several classes on nonconvex problems (unfortunately, these
methods are no yet successful in training production-grade neural networks).
Two of the most notable and popular methods belonging to the family of variance-reduced methods
are SVRG [12] and its accelerated variant known as Katyusha [1]. The latter method accelerates
the former via the employment of a novel “negative momentum” idea. Both of these methods
have a double loop design. At the beginning of the outer loop, a full pass over the training data is
made to compute the gradient of f at a reference point wk , which is chosen as the freshest iterate
(SVRG) or a weighted average of recent iterates (for Katyusha). This gradient is then used in the
inner loop to adjust the stochastic gradient ∇fi (xk ), where i is sampled uniformly at random from
def
[n] = {1, 2, . . . , n}, and xk is the current iterate, so as to reduce its variance. In particular, both
SVRG and Katyusha perform the adjustment g k = ∇fi (xk ) − ∇fi (wk ) + ∇f (wk ). Note that, like
∇fi (xk ), the new search direction g k is an unbiased estimator of ∇f (xk ). Indeed,
 
E g k = ∇f (xk ) − ∇f (wk ) + ∇f (wk ) = ∇f (xk ).
(2)
where the expectation is taken over random choice of i ∈ [n]. However, it turns out that as the
methods progress, the variance of g k , unlike that of ∇fi (xk ), progressively decreases to zero. The
total effect of this is significantly faster convergence.
Converegnce of SVRG and Katyusha for L–smooth and µ–strongly convex functions. For instance, consider the regime where fi is L–smooth for each i, and f is µ–strongly convex:
Assumption 1.1 (L–smoothness). Functions fi : Rd → R are L–smooth for some L > 0:
f (y) ≤ f (x) + h∇f (x), y − xi +

L
2

2

ky − xk ,

∀x, y ∈ Rd .

(3)

Assumption 1.2 (µ–strong convexity). Function f : Rd → R is µ–strongly convex for µ > 0:
f (y) ≥ f (x) + h∇f (x), y − xi +

µ
2

2

ky − xk ,

∀x, y ∈ Rd .

(4)

In this regime, the iteration complexity of SVRG is O ((n + L/µ) log 1/) , which is a vast improvement
nL
1
on the linear rate of gradient descent (GD), which
P is O ( /µ log /), and on the sublinear rate of
2
SGD, which is O(L/µ + σ /µ2 ), where σ 2 = 1/n i k∇fi (x∗ )k2 and x∗ is the (necessarily
unique)
p
minimizer of f . On the other hand, Katyusha enjoys the accelerated rate O((n + nL/µ) log 1/),
which is superior to that of SVRG in the ill-conditioned regime where L/µ ≥ n. This rate has been
shown to be optimal in a certain precise sense [19].
In the past several years, an enormous effort of the machine learning and optimization communities
was exerted into designing new efficient variance-reduced methods algorithms to tackle problem (1).
These developments have brought about a renaissance in the field. The historically first provably
variance-reduced method, the stochastic average gradient (SAG) method of [23, 24], was awarded
the Lagrange prize in continuous optimization in 2018. The SAG method was later modified to
an unbiased variant called SAGA [6], achieving the same theoretical rates. Alternative variancereduced method include MISO [16], FINITO [7], SDCA [25], dfSDCA [4], AdaSDCA [3], QUARTZ [22],
SBFGS [9], SDNA [21], SARAH [20] and S2GD [14], mS2GD [13], RBCN [8], JacSketch [10] and
SAGD [2]. Accelerated variance-reduced method were developed in [26], [5], [28] and [29].

2

Contributions

As explained in the introduction, a trade-mark structural feature of SVRG and its accelerated variant,
Katyusha, is the presence of the outer loop in which a full pass over the data is made. However, the
presence of this outer loop is the source of several issues. First, the methods are harder to analyze.
Second, one needs to decide at which point the inner loop is terminated and the outer loop entered.
For SVRG, the theoretically optimal inner loop size depends on both L and µ. However, µ is not
always known. Moreover, even when an estimate is available, as is the case in regularized problems
with an explicit strongly convex regularizer, the estimate can often be very loose. Because of these
issues, one often chooses the inner loop size in a suboptimal way, such as by setting it to n or O(n).
Two loopless methods. In this paper we address the above issues by developing loopless variants
of both SVRG and Katyusha; we refer to them as L-SVRG and L-Katyusha, respectively. In these
2

methods, we dispose of the outer loop and replace its role by a biased coin-flip, to be performed in
every step of the methods, used to trigger the computation of the gradient ∇f (wk ) via a pass over
the data. In particular, in each step, with (a small) probability p > 0 we perform a full pass over data
and update the reference gradient ∇f (wk ). With probability 1 − p we keep the previous reference
gradient. This procedure can alternatively be interpreted as having an outer loop of a random length.
However, the resulting methods are easier to write down, comprehend and analyze.
Fast rates are preserved. We show that L-SVRG and L-Katyusha enjoy the same fast theoretical
rates as their loopy forefathers. Our proofs are different and the complexity results more insightful.
For L-SVRG with fixed stepsize η = 1/6L and probability p = 1/n, we show (see Theorem 3.5) that
for the Lyapunov function
def

Φ k = xk − x∗

2

+

4η 2
pn

n
P

∇fi (wk ) − ∇fi (x∗ )

2

.

(5)

i=1

 
we get E Φk ≤ Φ0 as long as k = O ((n + L/µ) log 1/) . In
the classical
SVRG result
 contrast,

k
∗
shows convergence of the expected functional suboptimality E f (x ) − f (x ) to zero at the same
rate. Note that the classical result follows from our theorem by utilizing the inequality f (xk ) −
f (x∗ ) ≤ L/2kxk − x∗ k2 , which is a simple consequence of L–smoothness. However, our result
provides a deeper insight into the behavior of the method. In particular, it follows that the gradients
∇fi (wk ) at the reference points wk converge to the gradients at the optimum. This is a key intuition
behind the workings of SVRG, one not revealed by the classical analysis. Hereby we close the gap in
the theoretical understanding of the the SVRG convergence mechanism. Moreover, our theory predicts
that as long as p is chosen in the (possibly very large) interval
min {c/n, cµ/L} ≤ p ≤ max {c/n, cµ/L} ,

(6)

where c = Θ(1), L-SVRG will enjoy the optimal complexity O ((n +
In the illconditioned regime L/µ  n, for instance, we roughly have p ∈ [µ/L, 1/n]. This is in contrast
with the (loopy/standard) SVRG method the outer loop of which needs to be of the size ≈ L/µ. To
the best of our knowledge, SVRG does not enjoy this rate for an outer loop of size n (or any value
independent of µ, which is often not known in practice), even though this is the setting most often
used in practice. Several authors have tried to establish such a result, but without success. We thus
answer an open problem since 2013, the inception of SVRG.
L/µ) log 1/).

For L-Katyusha with stepsize η =

θ2
(1+θ2 )θ1

we show convergence of the Lyapunov function

Ψk = Z k + Y k + W k ,

(7)

∗ 2

1)
z k − x , Y k = θ11 (f (y k ) − f (x∗ )), and W k = θ2 (1+θ
(f (wk ) − f (x∗ )),
where Z k = L(1+ησ)
2η
pθ1
k k
k
and where x , y andpw are iterates produced by the method, with the parameters defined by
σ = µ/L, θ1 = min{ 2σn/3, 1/2}, θ2 p
= 1/2, p = 1/n. Our main result (Theorem 4.6) states that
 k
0
E Ψ ≤ Ψ as long as k = O((n + nL/µ) log 1/).

Simplified analysis. Advantage of the loopless approach is that a single iteration analysis is sufficient
to establish convergence. In contrast, one needs to perform elaborate aggregation across the inner
loop to prove the convergence of the original loopy methods.
Superior empirical behaviour. We show through extensive numerical testing on both synthetic and
real data that our loopless methods are superior to their loopy variants. We show through experiments
that L-SVRG is very robust to the choice of p from the optimal interval (6) predicted by our theory.
Moreover, even the worst case for L-SVRG outperforms the best case for SVRG. This shows how
further randomization can significantly speed up and stabilize the algorithm.


k
k
Notation.
Throughout
the
whole
paper
we
use
conditional
expectation
E
X
|
x
,
w
for L-SVRG


k k
k
and E X | y , z , w for L-Katyusha, but for simplicity we will denote these expectations as
E [X ]. If E [X ] refers to unconditional expectation, it is directly mentioned.

3

Loopless SVRG (L-SVRG)

In this section we describe in detail the Loopless SVRG method (L-SVRG), and its convergence.
3

The algorithm. The L-SVRG method, formalized as Algorithm 1, is inspired by the original SVRG
[12] method. We remove the outer loop present in SVRG and instead use a probabilistic update of the
full gradient.1 This update can be also seen in a way that outer loop size is generated by geometric
distribution similar to [14, 15].
Algorithm 1 Loopless SVRG (L-SVRG)
Parameters: stepsize η > 0, probability p ∈ (0, 1]
Initialization: x0 = w0 ∈ Rd
for k = 0, 1, 2, . . . do
g k = ∇fi (xk ) − ∇fi (wk ) + ∇f (wk )
(i ∈ {1, . . . , n} is sampled uniformly at random)
xk+1 = xk − ηg k
xk with probability p
wk+1 =
wk with probability 1 − p
end for
Note that the reference point wk (at which a full gradient is computed) is updated in each iteration
with probability p to the current iterate xk , and is left unchanged with probability 1 − p. Alternatively,
the probability p can be seen as a parameter that controls the expected time before next full pass over
data. To be more precise, the expected time before next full pass over data is 1/p. Intuitively, we wish
to keep p small so that full passes over data are computed rarely enough. As we shall see next, the
simple choice p = 1/n leads to complexity identical to that of original SVRG.
Convergence theory. A key role in the analysis is played by the gradient learning quantity
n
2 P
2
def
Dk = 4η
∇fi (wk ) − ∇fi (x∗ )
pn

(8)

i=1

def

2

and the Lyapunov function Φk = xk − x∗ + Dk . The analysis involves four lemmas, followed by
the main theorem. We wish to mention the lemmas as they highlight the way in which the argument
works. All lemmas combined, together with the main theorem, can be proved on a single page, which
underlines the simplicity of our approach.
Our first lemma upper bounds the expected squared distance of xk+1 from x∗ in terms of the same
distance but for xk , function suboptimality, and variance of g k .
Lemma 3.1. We have
i
h
i
h
2
2
2
. (9)
≤ (1 − ηµ) xk − x∗ − 2η(f (xk ) − f (x∗ )) + η 2 E g k
E xk+1 − x∗
In our next lemma, we further bound the variance of g k in terms of function suboptimality and Dk .
Lemma 3.2. We have
h
i
2
E gk
≤ 4L(f (xk ) − f (x∗ )) + 2ηp2 Dk .
(10)


Finally, we bound E Dk+1 in terms of Dk and function suboptimality.
Lemma 3.3. We have


E Dk+1 ≤ (1 − p)Dk + 8Lη 2 (f (xk ) − f (x∗ )).

(11)

Putting the above three lemmas together naturally leads to the following result involving Lyapunov
function (5).
Lemma 3.4. Let the step size η ≤ 1/6L. Then for all k ≥ 0 the following inequality holds:


2
E Φk+1 ≤ (1 − ηµ) xk − x∗ + (1 − p/2) Dk .
(12)
In order to obtain a recursion involving the Lyapunov function on the right-hand side of (12)
1

This idea was independently explored in [11]; we have learned about this work after a first draft of our paper
was finished.

4

 
Theorem 3.5. Let η = 1/6L, p = 1/n. Then E Φk ≤ εΦ0 as long as k ≥ O ((n + L/µ) log 1/ε) .
 
Proof. As the corollary of Lemma 3.4 we have E Φk ≤ max {1 − ηµ, 1 − p/2} Φk−1 . Setting η = 1/6L, p = 1/n and unrolling conditional probability one obtains E Φk ≤
k
max {1 − µ/6L, 1 − 1/2n} Φ0 , which concludes the proof.
Note that the step size does not depend on the strong convexity parameter µ and yet the resulting
complexity adapts to it.
Discussion. Examining (12), we can see that contraction of the Lyapunov function is max{1−ηµ, 1−
p/2}. Due to the limitation of η ≤ 1/6L, the first term is at least 1 − η/6µ, thus the complexity cannot
better than O (L/µ log 1/ε). In terms of total complexity (number of stochastic gradient calls), L-SVRG
calls the stochastic gradient oracle in expectation O(1 + pn) times times in each iteration. Combining
these two complexities together, one gets the total complexity O ((1/p + n + L/µ + Lpn/µ) log 1/ε) .
Note that any choice of p ∈ [min {c/n, cµ/L} , max {c/n, cµ/L}] , where c = Θ(1), leads to the optimal
total complexity O ((n + L/µ) log 1/ε). This fills the gap in SVRG theory, where the outer loop length
(in our case 1/p in expectation) needs to be proportional to L/µ. Moreover, analysis for L-SVRG is
much simpler and provides more insights.

4

Loopless Katyusha (L-Katyusha)

In this section we describe in detail the Loopless Katyusha method (L-Katyusha), and its convergence properties.
The algorithm. The L-Katyusha method, formalized as Algorithm 2, is inspired by the original
Katyusha [1] method. We use the same technique as for Algorithm 1, where we remove the outer
loop present in Katyusha and instead use a probabilistic update of the full gradient.
Algorithm 2 Loopless Katyusha (L-Katyusha)
Parameters: θ1 , θ2 , probability p ∈ (0, 1]
θ2
and set σ = µ/L
Initialization: Choose y 0 = w0 = z 0 ∈ Rd , stepsize η = (1+θ
2 )θ1
for k = 0, 1, 2, . . . do
xk = θ1 z k + θ2 wk + (1 − θ1 − θ2 )y k
k
g k = ∇fi (xk ) − ∇fi (wk ) + ∇f (w
(i ∈ {1, . . . , n} is sampled uniformly at random)
 )
1
z k+1 = 1+ησ
ησxk + z k − Lη g k
y k+1 = xk + θ1 (z k+1 − z k )
 k
y
with probability p
k+1
w
=
wk with probability 1 − p
end for
The exact analogy applies to the reference point wk (at which a full gradient is computed) as
for L-SVRG. Instead of updating this point in a deterministic way every m iteration, we use the
probabilistic update with parameter p, when we update wk+1 to the current iterate y k with this
probability and is left unchanged with probability 1 − p. As we shall see next, the same choice
p = 1/n as for L-SVRG leads to complexity identical to that of original Katyusha.
Convergence theory. In comparison to L-SVRG, we don’t use gradient mapping as the key component of our analysis. Instead, we prove convergence of functional values in y k , wk and point-wise
convergence of z k . This is summarized in the following Lyapunov function:
Ψk = Z k + Y k + W k ,
2

(13)

1)
where Z k = L(1+ησ)
z k − x∗ , Y k = θ11 (f (y k ) − f (x∗ )), W k = θ2 (1+θ
(f (wk ) − f (x∗ )).
2η
pθ1
Note that even if xk is not in this function, its point-wise convergence is directly implied by the
convergence of Ψk due to the definition of xk in Algorithm 2 and L-smoothness of f .

5

w8a, = 10

100
10 4
10 8
10 12
10 16
10 20
10 24

w8a, = 10

1

100
10 2
10 4
10 6
10 8
10 10
10 12
10 14
8 10 12 14 16 18 20
0

2

L-SVRG
SVRG

0

2

4

6

a9a, = 10

100
10 4

2

L-SVRG
SVRG

10 8
10 12
10 16
10 20
10 24
0

10

20

30

40

2

4

6

8 10 12 14 16 18 20
a9a, = 10

100
10 2
10 4
10 6
10 8
10 10
10 12
10 14
10 16
10 18
50
0

3

20

30

40

10

2

10

4

10

6

10

8

10

3

L-SVRG
SVRG

w8a, = 10

100

10

4

L-SVRG
SVRG

2

10

0 10 20 30 40 50 60 70 80 90 100
a9a, = 10

100

L-SVRG
SVRG

10

w8a, = 10

100

L-SVRG
SVRG

10

2

10

4

10

6

50

4

L-SVRG
SVRG

0 10 20 30 40 50 60 70 80 90 100

0

50

a9a, = 10

100

10

2

10

4

0

100

150

200

5

L-SVRG
SVRG

50 100 150 200 250 300 350 400

Figure 1: Comparison of SVRG and L-SVRG for different datasets and regularizer weights µ.
The analysis involves five lemmas, followed by the convergence summarized in the main theorem.
The lemmas highlight important steps of our analysis. The simplicity of our approach is still preserved:
all lemmas and the main theorem can be proved on not more than two pages.
Our first lemma upper bounds the variance of the gradient estimator g k , which eventually goes to
zero as our algorithm progresses.
Lemma 4.1. We have
h
i

2
E g k − ∇f (xk )
≤ 2L f (wk ) − f (xk ) − ∇f (xk ), wk − xk .
(14)
Next two lemmas are more technical, but essential for proving the convergence.
Lemma 4.2. We have
g k , x∗ − z k+1 +

µ
2

xk − x∗

2

≥

L
2η

g k − ∇f (xk )

2

z k − z k+1

2

+ Z k+1 −

k
1
1+ησ Z .

(15)

Lemma 4.3. We have
1
θ1


f (y k+1 ) − f (xk ) −

θ2
2Lθ1

≤

L
2η

z k+1 − z k

2

+ g k , z k+1 − z k . (16)

Finally, we use the update of Algorithm 2 to decompose W k+1 in terms of W k and Y k , which is one
of the main components that allow for simpler analysis than the one of original Katyusha.
Lemma 4.4. We have


(17)
E W k+1 = (1 − p)W k + θ2 (1 + θ1 )Y k .
Putting all lemmas together, we obtain the following contraction of the Lyapunov function (7).
Lemma 4.5. Let θ1 , θ2 > 0, θ1 + θ2 ≤ 1, σ =

µ
L

and η =

θ2
(1+θ2 )θ1 ,

then we have




pθ1
1
E Z k+1 + Y k+1 + W k+1 ≤ 1+ησ
Z k + (1 − θ1 (1 − θ2 ))Y k + 1 − 1+θ
Wk.
1

(18)

In order to obtain a recursion involving the Lyapunov function on the right-hand side of (18)
p
 
Theorem 4.6. Let θ1 = min{ 2σn/3, 1/2}, θ2 = 1/2, p = 1/n. Then E Ψk ≤ εΨ0 after the
p
following number of iterations: k = O((n + nL/µ) log 1/ε).


Proof. From Lemma 4.5 we get E Ψk+1 ≤ max {1/(1+ησ), 1 − θ1 (1 − θ2 ), 1 − pθ1/(1+θ1 )} Ψk .
p
1
2σn
1
1
Setting
 k+1p= /n, θ1 = min{
 k  /3, /2}, θ2 = /2, and unrolling conditional probability one obtains
E Ψ
≤ (1 − θ)E Ψ , where θ = min {σ/6θ1 , θ1/2n} . Choosing σ = µ/L concludes the
proof.
6

100
10 2
10 4
10 6
10 8
10 10
10 12
10 14
10 16
10 18
10 20
0
100
10 2
10 4
10 6
10 8
10 10
10 12
10 14
0

a9a, = 10

100
10 1
10 2
10 3
10 4
10 5
10 6
10 7
10 8
10 9
10 10
50 100 150 200 250 300 350 400
0
L-Katyusha
Katyusha

w8a, = 10

4

100

150

10

1

10

2

10

3

10

4

10

5

200 10 0
6

a9a, = 10

6

100
10 1
10 2
10 3
10 4
10 5
10 6
7
50 100 150 200 250 300 350 400 10 0

7

L-Katyusha
Katyusha

w8a, = 10

100

L-Katyusha
Katyusha

50

a9a, = 10

5

5

L-Katyusha
Katyusha

50

100

150

200

L-Katyusha
Katyusha

100 200 300 400 500 600 700 800
w8a, = 10

100
10

1

10

2

10

3

0

6

L-Katyusha
Katyusha

100

200

300

400

500

Figure 2: Comparison of Katyusha & L-Katyusha for different datasets and regularizer weights µ.

100
10
10

4
8

10

12

10

16

10

20

0

cod-rna, = 102

S_1
S_2
S_3
S_4
S_5
L_1
L_2
L_3
L_4
L_5
10

mushrooms, = 10

10
10
10
20

30

40

50

10

phishing, = 10

3

100
S_1
S_2
10 4
S_3
2
S_4
10 8
S_5
4
10 12
L_1
L_2
10 16
L_3
6
L_4
10 20
L_5
8
0 10 20 30 40 50 60 70 80 90 100
0 10

100

4

S_1
S_2
S_3
S_4
S_5
L_1
L_2
L_3
L_4
L_5
20 30 40 50 60 70 80 90 100

Figure 3: Comparison of SVRG (S) and L-SVRG (L) for several choices of expected outer loop length
(L-SVRG)
or deterministic
outer loop length (SVRG). Numbers 1–5 correspondent to loop-lengths
√
√
√
4
4
n, κn3 , κn, κ3 n, κ, respectively, where κ = L/µ.
L
Discussion. One can
p show by analyzing (18) that for ill-conditioned problems (n < /µ), the iteration
complexity is O( L/µp log 1/ε). Algorithm 2 calls stochastic gradient
oracle
O(1
+ pn) times per
p
L
1
iteration in expectation. Thus, the total complexity is O((1 + pn) /µp log /ε). One can see that
p = Θ (1/n) leads to optimal rate.

5

Numerical Experiments

In this section, we perform experiments with logistic regression for binary classification with L2
2
µ
regularizer, where our loss function has the form fi (x) = log(1 + exp(−bi a>
i x)) + 2 kxk , where
d
ai ∈ R , bi ∈ {−1, +1}, i ∈ [n]. Hence, f is smooth and µ-strongly convex. We use four LIBSVM
library2 : a9a, w8a, mushrooms, phishing, cod-rna.
We compare our methods L-SVRG and L-Katyusha with their original version. It is well-known that
whenever practical, SAGA is a bit faster than SVRG. While a comparison to SAGA seems natural as it
also does not have a double loop structure, we position our loopless methods for applications where
the high memory requirements of SAGA prevent it to be applied. Thus, we do not compare to SAGA.
2

2

Plots are constructed in such a way that the y-axis displays xk − x? for L-SVRG and y k − x?
for L-Katyusha, where x? were obtained by running gradient descent for a large number of epochs.
The x-axis displays the number of epochs (full gradient evaluations). That is, n computations of
∇fi (x) equals one epoch.
Superior practical behaviour of the loopless approach. Here we show that L-SVRG and
L-Katyusha perform better in experiments than their loopy variants. In terms of theoretical iteration complexity, both the loopy and the loopless methods are the same. However, as we can see
from Figure 1, the improvement of the loopless approach can be significant. One can see that for
these datasets, L-SVRG is always better than SVRG, and can be faster by several orders of magnitude!
2

The LIBSVM dataset collection is available at https://www.csie.ntu.edu.tw/~cjlin/
libsvmtools/datasets/

7

100
10 2
10 4
10 6
10 8
10 10
10 12
10 14
0

w8a, = 10

2

100
10 2
10 4
10 6
10 8
10 10
10 12
10 14
L-SVRG
10 16
SVRG
10 18
L-Katyusha
10 20
Katyusha
10 22
2 4 6 8 10 12 14 16 18 20
0 10

100
10 2
10 4
10 6
10 8
10 10
10 12
10 14
10 16
10 18
0
102
100
10 2
10 4
10 6
10 8
10 10
10 12
10 14
10 16
10 18
0 10

a9a, = 10

L-SVRG
SVRG
L-Katyusha
Katyusha
10
20

3

30

mushrooms, = 10

40

100
10 2
10 4
10 6
10 8
10 10
10 12
10 14
10 16
10 18
50
0 10

102
100
10 2
10 4
10 6
10 8
L-SVRG
SVRG
10 10
L-Katyusha
12
10
Katyusha
14
20 30 40 50 60 70 80 90 100 10 0

100
10 2
10 4
10 6
10 8
10 10
10 12
10 14
0 10

phishing, = 10

100
10 2
10 4
10 6
10 8
L-SVRG
10 10
SVRG
10 12
L-Katyusha
Katyusha
10 14
20 30 40 50 60 70 80 90 100
0
a9a, = 10

L-SVRG
SVRG
L-Katyusha
Katyusha
50

150

5

102
100
10 2
10 4
10 6
10 8
L-SVRG
SVRG
10 10
L-Katyusha
10 12
Katyusha
10 14
20 30 40 50 60 70 80 90 100
0
100
10 1
10 2
10 3
10 4
10 5
L-SVRG
10 6
SVRG
10 7
L-Katyusha
10 8
Katyusha
9
20 30 40 50 60 70 80 90 100 10 0

phishing, = 10

L-SVRG
SVRG
L-Katyusha
Katyusha
100
200

300

100

10

2

10

3

10

4

10

5

L-SVRG
SVRG
L-Katyusha
Katyusha
50

6

400

102
101
100
10 1
10 2
10 3
10 4
10 5
10 6
10 7
500
0
100

150

200

10

1

10

2

10

3

0

L-SVRG
SVRG
L-Katyusha
Katyusha
50
100

150

6

L-SVRG
SVRG
L-Katyusha
Katyusha
500 1000 1500 2000 2500 3000
7

600

cod-rna, = 10

200

6

mushrooms, = 10

5

L-SVRG
SVRG
L-Katyusha
Katyusha
200
400

150

L-SVRG
SVRG
L-Katyusha
Katyusha
50 100 150 200 250 300 350 400

phishing, = 10

6

5

100
a9a, = 10

5

104
102
100
10 2
10 4
10 6
L-SVRG
10 8
SVRG
10
10
L-Katyusha
12
10
Katyusha
10 14
100 200 300 400 500 600 700 800
0

cod-rna, = 100

L-SVRG
SVRG
L-Katyusha
Katyusha
50

1

200 10 0

150

mushrooms, = 10

102
100
10 2
10 4
10 6
10 8
10 10
300
0

250

w8a, = 10

100
10

100
10 1
10 2
10 3
10 4
10 5
10 6
L-SVRG
10 7
SVRG
10 8
L-Katyusha
10 9
Katyusha
10 10
50 100 150 200 250 300 350 400
0

4

200

4

100

a9a, = 10

4

100
10 2
10 4
10 6
10 8
10 10
10 12
L-SVRG
10 14
SVRG
10 16
L-Katyusha
10 18
Katyusha
10 20
20 30 40 50 60 70 80 90 100
0

L-SVRG
SVRG
L-Katyusha
Katyusha
50
100

cod-rna, = 101

w8a, = 10

3

mushrooms, = 10

3

101
100
10 1
10 2
10 3
10 4
10 5
10 6
10 7
10 8
0 10

w8a, = 10

800

1000

1

200

250

300

Figure 4: All methods together for different datasets and different regularizer weights.

Looking at Figure 2, we see that the performance of L-Katyusha is at least as good as that of
Katyusha, and can be significantly faster in some cases. All parameters of the methods were chosen
as suggested by theory. For L-SVRG and L-Katyusha they are chosen based on Theorems 3.5 and
4.6, respectively. For SVRG and Katyusha we also choose the parameters based on the theory, as
described in the original papers. The initial point x0 is chosen to be the origin.
Different choices of probability/ outer loop size. We now compare several choices of the probability
p of updating the full gradient for SVRG and several outer loop sizes m for SVRG. Since our analysis
guarantees the optimal rate for any choice of p between 1/n and µ/L for well condition problems,
we decided to perform experiments for p within this range. More precisely, we choose 5 values
of
√
4
3,
p, uniformly
distributed
in
logarithmic
scale
across
this
interval,
and
thus
our
choices
are
n,
κn
√
√
4
3
κn, κ n, and κ, where κ = L/µ, denoted in the figures by 1, 2, 3, 4, 5, respectively. Since the
expected “outer loop” length (length for which reference point stays the same) is 1/p, for SVRG we
choose m = 1/p. Looking at Figure 3, one can see that L-SVRG is very robust to the choice of p
from the “optimal interval” predicted by our theory. Moreover, even the worst case for L-SVRG
outperforms the best case for SVRG.
All methods together. Finally, we provide all algorithms together in one plot for different datasets
with different regularizer weight, thus with different condition numbers, displayed in Figure 4. As for
the previous experiments, loopless methods are not worse and sometimes significantly better.
8

References
[1] Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. In
Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages
1200–1205. ACM, 2017.
[2] Adel Bibi, Alibek Sailanbayev, Bernard Ghanem, Robert Mansel Gower, and Peter Richtárik.
Improving SAGA via a probabilistic interpolation with gradient descent. arXiv: 1806.05633,
2018.
[3] Dominik Csiba, Zheng Qu, and Peter Richtárik. Stochastic dual coordinate ascent with adaptive
probabilities. In Proceedings of the 32nd International Conference on Machine Learning, pages
674–683, 2015.
[4] Dominik Csiba and Peter Richtárik. Primal method for ERM with flexible mini-batching
schemes and non-convex losses. arXiv:1506.02227, 2015.
[5] Aaron Defazio. A simple practical accelerated method for finite sums. In Advances in Neural
Information Processing Systems, pages 676–684, 2016.
[6] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in Neural
Information Processing Systems, pages 1646–1654, 2014.
[7] Aaron Defazio, Tiberio Caetano, and Justin Domke. Finito: A faster, permutable incremental
gradient method for Big Data problems. The 31st International Conference on Machine
Learning, 2014.
[8] Nikita Doikov and Peter Richtárik. Randomized block cubic Newton method. In Proceedings
of the 35th International Conference on Machine Learning, 2018.
[9] Robert Mansel Gower, Donald Goldfarb, and Peter Richtárik. Stochastic block BFGS: squeezing
more curvature out of data. In 33rd International Conference on Machine Learning, pages
1869–1878, 2016.
[10] Robert Mansel Gower, Peter Richtárik, and Francis Bach. Stochastic quasi-gradient methods:
variance reduction via Jacobian sketching. arXiv:1805.02632, 2018.
[11] Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien, and Brian McWilliams. Variance reduced stochastic gradient descent with neighbors. In Advances in Neural Information Processing
Systems, pages 2305–2313, 2015.
[12] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems, pages 315–323, 2013.
[13] Jakub Konečný, Jie Lu, Peter Richtárik, and Martin Takáč. Mini-batch semi-stochastic gradient
descent in the proximal setting. IEEE Journal of Selected Topics in Signal Processing, 10(2):242–
255, 2016.
[14] Jakub Konečný and Peter Richtárik. S2GD: Semi-stochastic gradient descent methods. Frontiers
in Applied Mathematics and Statistics, pages 1–14, 2017.
[15] Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization
via CSSG methods. In Advances in Neural Information Processing Systems, pages 2348–2358,
2017.
[16] Julien Mairal. Incremental majorization-minimization optimization with application to largescale machine learning. SIAM Journal on Optimization, 25(2):829–855, 2015.
[17] A Nemirovski, A Juditsky, G Lan, and A Shapiro. Robust stochastic approximation approach to
stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.
[18] Arkadi Nemirovsky and David B. Yudin. Problem complexity and method efficiency in optimization. Wiley, New York, 1983.
[19] Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87.
Springer Science & Business Media, 2013.
[20] Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takáč. SARAH: A novel method for
machine learning problems using stochastic recursive gradient. In International Conference on
Machine Learning, pages 2613–2621, 2017.
9

[21] Zheng Qu, Peter Richtárik, Martin Takáč, and Olivier Fercoq. SDNA: Stochastic dual Newton
ascent for empirical risk minimization. In The 33rd International Conference on Machine
Learning, pages 1823–1832, 2016.
[22] Zheng Qu, Peter Richtárik, and Tong Zhang. Quartz: Randomized dual coordinate ascent with
arbitrary sampling. In Advances in Neural Information Processing Systems 28, pages 865–873,
2015.
[23] Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an
exponential convergence rate for finite training sets. In Advances in Neural Information
Processing Systems, pages 2663–2671, 2012.
[24] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient. Mathematical Programming, 162(1-2):83–112, 2017.
[25] Shai Shalev-Shwartz. SDCA without duality, regularization, and individual convexity. In
International Conference on Machine Learning, pages 747–754, 2016.
[26] Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent
for regularized loss minimization. In International Conference on Machine Learning, pages
64–72, 2014.
[27] Martin Takáč, Avleen Bijral, Peter Richtárik, and Nathan Srebro. Mini-batch primal and dual
methods for SVMs. In 30th International Conference on Machine Learning, pages 537–552,
2013.
[28] Kaiwen Zhou. Direct acceleration of SAGA using sampled negative momentum. arXiv preprint
arXiv:1806.11048, 2018.
[29] Kaiwen Zhou, Fanhua Shang, and James Cheng. A simple stochastic variance reduced algorithm
with fast convergence rates. arXiv preprint arXiv:1806.11027, 2018.

10

Supplementary Material:
SVRG and Katyusha are Better Without the Outer Loop
A

Auxiliary Lemmas

Lemma A.1. For random vector x ∈ Rd and any y ∈ Rd , the variance of y can be decomposed as
h
i
h
i
h
i
2
2
2
E kx − E [x]k = E kx − yk − E kE [x] − yk .
(19)
The next lemma is a consequence of Jensen’s inequality applied to x 7→ kxk2 .
Lemma A.2. For any vectors a1 , a2 , . . . , ak ∈ Rd , the following inequality holds:
k
X

2

ai

≤k

i=1

B

k
X

2

kai k .

(20)

i=1

Proofs for Algorithm 1 (L-SVRG)
def

In all proofs below, we will for simplicity write f ∗ = f (x∗ ).
B.1

Proof of Lemma 3.1

Definition of xk+1 and unbiasness of g k guarantee that
h
i

2
2
E xk+1 − x∗
=
E xk − x∗ − ηg k
h
i


2
2
Alg. 1
=
xk − x∗ + E 2η g k , x∗ − xk + η 2 E g k
h
i
2
2
(2)
=
xk − x∗ + 2η ∇f (xk ), x∗ − xk + η 2 E g k

h

(4)
µ k
2
x − x∗ + η 2 E g k
≤
xk − x∗ + 2η f ∗ − f (xk ) −
2
h
i

2
2
=
xk − x∗ (1 − ηµ) + 2η f ∗ − f (xk ) + η 2 E g k
.
B.2

2

i

Proof of Lemma 3.2

Using definition of g k
h
i Alg. 1
h
i
2
2
E gk
=
E ∇fi (xk ) − ∇fi (x∗ ) + ∇fi (x∗ ) − ∇fi (wk ) + ∇f (wk )
h
i
h
(20)


2
≤
2E ∇fi (xk ) − ∇fi (x∗ )
+ 2E ∇fi (x∗ ) − ∇fi (wk ) − E ∇fi (x∗ ) − ∇fi (wk )
h
i
(3)+(19)
2
≤
4L(f (xk ) − f ∗ ) + 2E ∇fi (wk ) − ∇fi (x∗ )
(8)

=

B.3

4L(f (xk ) − f ∗ ) +

p k
D .
2η 2

Proof of Lemma 3.3


E Dk+1

Alg. 1

=

(3)

≤

(1 − p)Dk + p

n
4η 2 X
∇f (xk ) − ∇f (x∗ )
pn i=1

(1 − p)Dk + 8Lη 2 (f (xk ) − f ∗ ).
11

2

2

i

B.4

Proof of Lemma 3.4

Combining Lemmas 3.1 and 3.3 we obtain

E

h

xk+1 − x∗

2

+ Dk+1

(9)+(11)

i

(1 − µη) xk − x∗

≤

2

+ 2η(f ∗ − f (xk )) + η 2 E

h

gk

2

i

+(1 − p)Dk + 8Lη 2 (f (xk ) − f ∗ )
(10)

2

(1 − µη) xk − x∗ + (1 − p)Dk + (2η − 8Lη 2 )(f ∗ − f (xk ))


p k
2
k
∗
+η 4L(f (x ) − f ) + 2 D
2η

p k
2
(1 − µη) xk − x∗ + 1 −
D + (2η − 12Lη 2 )(f ∗ − f (xk )).
2

≤

=
Now we use the fact that η ≤
E

C
C.1

h

xk+1 − x∗

1
6L
2

and obtain the desired inequality:

+ Dk+1

i

≤

(1 − µη) xk − x∗

2


p k
D .
+ 1−
2

Proofs for Algorithm 2 (L-Katyusha)
Proof of Lemma 4.1

To upper bound the variance of g k we first uses its definition
E

h

g k − ∇f (xk )

2

i

Alg. 2

E

(19)

≤

(3)

2L f (wk ) − f (xk ) − ∇f (xk ), wk − xk

≤

C.2

h



∇fi (xk ) − ∇fi (wk ) − E ∇fi (xk ) − ∇fi (wk )
h
i
2
E ∇fi (xk ) − ∇fi (wk )

=



2

i

.

Proof of Lemma 4.2

We start with the definition of z k+1
Alg. 2

z k+1 =

which implies

η k
Lg

g k , z k+1 − x∗

1 
η 
ησxk + z k − g k ,
1 + ησ
L

= ησ(xk − z k+1 ) + (z k − z k+1 ), which further implies that

=
=

≤

L k
µ xk − z k+1 , z k+1 − x∗ +
z − z k+1 , z k+1 − x∗
η

µ k
2
2
2
x − x∗ − xk − z k+1 − z k+1 − x∗
2

L  k
2
2
2
+
z − x∗ − z k − z k+1 − z k+1 − x∗
2η
µ k
L  k
2
2
x − x∗ +
z − x∗ − (1 + ησ) z k+1 − x∗
2
2η
12

2



−

L
z k − z k+1
2η

2

.

C.3

Proof of Lemma 4.3

=
Alg. 2

=

=
=
(3)

≥

≥
=

L
2
z k+1 − z k + g k , z k+1 − z k
2η


1
L
2
θ1 (z k+1 − z k ) + g k , θ1 (z k+1 − z k )
θ1 2ηθ1


L
1
2
y k+1 − xk + g k , y k+1 − xk
θ1 2ηθ1


1
L
2
y k+1 − xk + ∇f (xk ), y k+1 − xk + g k − ∇f (xk ), y k+1 − xk
θ1 2ηθ1




1 L k+1
L
1
2
2
y
− xk + ∇f (xk ), y k+1 − xk +
− 1 y k+1 − xk + g k − ∇f (xk ), y k+1 − xk
θ1 2
2 ηθ1




L
1
1
2
f (y k+1 ) − f (xk ) +
− 1 y k+1 − xk + g k − ∇f (xk ), y k+1 − xk
θ1
2 ηθ1


1
ηθ1
2
g k − ∇f (xk )
f (y k+1 ) − f (xk ) −
θ1
2L(1 − ηθ1 )


1
θ2
2
g k − ∇f (xk )
f (y k+1 ) − f (xk ) −
,
θ1
2L
2

where the last inequality uses the Young’s inequality in the form of ha, bi ≥ − kak
2β −
β=
C.4

ηθ1
L(1−ηθ1 ) ,

βkbk2
2

for

which concludes the proof.

Proof of Lemma 4.4

From the definition of wk+1 in Algorithm 2 we have

 Alg. 2
E f (wk+1 ) = (1 − p)f (wk ) + pf (y k ).
The rest of proof follows from the definition of W k (17).

13

(21)

C.5

Proof of Lemma 4.5

Combining all the previous lemmas together, we obtain
f∗

(4)

≥

=
Alg. 2

=

(2)

≥

(15)

≥

(16)

≥

(14)

≥

=

f (xk ) + ∇f (xk ), x∗ − xk +

µ k
x − x∗
2

2

µ k
2
x − x∗ + ∇f (xk ), x∗ − z k + z k − xk
2
θ2
µ k
(1 − θ1 − θ2 )
2
f (xk ) +
x − x∗ + ∇f (xk ), x∗ − z k +
∇f (xk ), xk − wk +
∇f (xk ), xk − y k
2
θ1
θ1
θ2
(1 − θ1 − θ2 )
f (xk ) +
∇f (xk ), xk − wk +
(f (xk ) − f (y k ))
θ1
θ1
i
hµ
2
xk − x∗ + g k , x∗ − z k+1 + g k , z k+1 − z k
+E
2
θ2
(1 − θ1 − θ2 )
f (xk ) +
∇f (xk ), xk − wk +
(f (xk ) − f (y k ))
θ1
θ1




L
1
2
z k − z k+1
Z k + E g k , z k+1 − z k +
+E Z k+1 −
1 + ησ
2η
θ2
(1 − θ1 − θ2 )
f (xk ) +
∇f (xk ), xk − wk +
(f (xk ) − f (y k ))
θ1
θ1





1
1
θ2
2
+E Z k+1 −
Zk + E
f (y k+1 ) − f (xk ) −
g k − ∇f (xk )
1 + ησ
θ1
2Lθ1
θ
(1
−
θ
−
θ
)
2
1
2
f (xk ) +
∇f (xk ), xk − wk +
(f (xk ) − f (y k ))
θ1
θ1




 θ2

1
1
k+1
k
k+1
k
k
k
k
k
k
+E Z
−
Z +E
f (y
) − f (x ) −
f (w ) − f (x ) − ∇f (x ), w − x
1 + ησ
θ1
θ1
(1 − θ1 − θ2 )
θ2
1
f (xk ) +
Z k − (f (wk ) − f (xk ))
(f (xk ) − f (y k )) −
θ1
1 + ησ
θ1



1
k+1
k
k+1
f (y
) − f (x ) ,
+E Z
+
θ1
f (xk ) +

where in the second inequality we use also convexity of f (x).
xk

Alg. 2

=

z k − xk

=

θ1 z k + θ2 wk + (1 − θ1 − θ2 )y k
θ2 k
1 − θ1 − θ2 k
(x − wk ) +
(x − y k ).
θ1
θ1

After rearranging we get


θ2
1
Z k + (1 − θ1 − θ2 )Y k + (f (wk − f ∗ )) ≥ E Z k+1 + Y k+1 .
1 + ησ
θ1
Using definition of W k we get


E Z k+1 + Y k+1 ≤

1
p
Z k + (1 − θ1 − θ2 )Y k +
Wk.
1 + ησ
(1 + θ1 )

(22)

Finally, using Lemma 4.4 we get


E Z k+1 + Y k+1 + W k+1 ≤
=

1
p
Z k + (1 − θ1 − θ2 )Y k +
W k + (1 − p)W k + θ2 (1 + θ1 )Y k
1 + ησ
(1 + θ1 )


1
pθ1
k
k
Z + (1 − θ1 (1 − θ2 ))Y + 1 −
Wk,
1 + ησ
1 + θ1

which concludes the proof.

14

