A score function for Bayesian cluster analysis
John Noble
Lukasz Rajkowski

arXiv:1905.10209v1 [stat.OT] 24 May 2019

May 27, 2019
Abstract
We propose a score function for Bayesian clustering. The function is parameter free and
captures the interplay between the within cluster variance and the between cluster entropy
of a clustering. It can be used to choose the number of clusters in well-established clustering
methods such as hierarchical clustering or K-means algorithm.

1

Introduction

Many clustering methods generate a family of clusterings that depend on some user-defined
parameters. The most prominent example is the K-means algorithm, where the investigator
has to specify the number of clusters. Similarly, in hierarchical clustering, a whole family
of clusterings is obtained, starting from the finest partition into singletons and ending in
the coarsest clustering, i.e. a single cluster. Again, the investigator chooses the number of
clusters based on the dendrogram.
All these methods come with a variety of suggestions how to choose the optimal number
of clusters. Some of these are rather heuristic in nature, while others have deep theoretical
foundations. For the K-means algorithm these include the elbow method or average silhouette method (Rousseeuw [1987]). Another solution is to use a score statistic (a function
which is intended to measure the quality of a clustering) and among different clusterings
proposed by a given method choose the one that maximises the score statistic. Constructing score statistics is not a trivial task; one of the most popular choices is the gap statistic
(Tibshirani et al. [2001]).
In this article we propose a new score statistic. It is derived as a limit of the first order
approximation to the posterior probability (up to the norming constant) in a Nonparametric
Bayesian Mixture Model with the inverse Wishart distribution as a base measure for the
within group covariance matrices and the Gaussian distribution as a base measure for the
cluster means and the component measure. In order to derive the limit we assume that the
data is an independent sample from some ‘input’ probability distribution on the observation
space; this gives a method of assessing the compatibility of the partitions of the observation
space to the input distribution. The score function is obtained by taking the empirical
measure as the input distribution and tweaking it slightly so that it is well defined on all
possible data clusterings.

1.1

Contribution and Results

Our main contribution is the formulation of a novel score function for clusterings, which is
motivated theoretically and performs well on analysed datasets. Suppose that we have a
sequence of observations x1 , . . . , xn ∈ Rd and we believe that it consists of several groups and
within every group the data is distributed according to some Gaussian distribution (with

1

unknown mean and covariance matrix). The goal is to construct a simple function that
measures how well a given clustering of the dataset corresponds to the assumption of being
Gaussially
following: for I ⊂ [n] we define
Pdistributed within clusters.
P Our proposition is the
t
1
1
xI = |I|
i∈I xi and V̂x (I) = |I|
i∈I (xi −x[n] )(xi −x[n] ) and for the notational simplicity

denote V̂x := V̂x ([n]). For x = (x1 , . . . , xn ) and I – a partition of [n] = {1, 2, . . . , n} let
D(x, I) := −

 V̂
 X |I| |I|
1 X |I|
x
ln det
+ V̂x (I) +
ln
.
2 I∈I n
|I|
n
n
I∈I

(1.1)

It should be noted that if x is a realisation of a random independent sample X1 , . . . , Xn
from some distribution P on X , then the components of the formula (1.1) can be treated as
empirical estimates of relevant probabilities or the conditional covariance matrices. This is
actually how (1.1) is obtained; we investigate the details in Section 3. This remark may be
also convenient when dealing with large datasets where the exact computation of (1.1) could
be time consuming. In such case we can approximate the variance components of (1.1) by
using the random samples from clusters.

2
2.1

Score functions and the main formula
Basic definitions

We start our presentation with a formal definition of a score function, intended to measure
the quality of the data clustering.
Notation. For n ∈ N let [n] = {1, . . . , n} and let S
Πn be the set of all partitions of [n].
n
Let X = Rd be the observation space. Let O = ∞
n=1 X × Πn be the set of all possible
finite sequences of observations and their partitions and let R = R ∪ {−∞, ∞}.
Definition. A clustering score function is any function S : O → R.
Definition. Let S be a score function and let F be a family of functions from X to X .
We say that S is robust to F if for every x = (x1 , . . . , xn ) ∈ X n and I, J ∈ Πn and
every f ∈ F we have S(x,
 I) ≤ S(x, J ) if and only if S(f (x), I) ≤ S(f (x), J ), where
f (x) = f (x1 ), . . . , f (xn ) .
Hence robustness to F means that if we apply any function f ∈ F to all observations, the
optimal clustering indicated by the score function will not alter. If no prior knowledge about
the clustering structure is available, a natural demand from a score function is to be robust
to linear isomorphisms of X . In particular, it should be robust to scaling of the axes since
it would be strange if the result of applying the score function would depend on the units
used to measure the observation. For the similar reasons, we expect a good score function
to be robust to translations.
Note, that on the other hand the robustness to all linear transformation would be undesirable
– in particular, moving all points to the origin is a linear transformation and we do not expect
any clusters to be seen after applying it.
Notation. Let A and B be two partitions of the same set. We say that A is finer than B if
for every A ∈ A there exist B ∈ B such that A ⊂ B. Equivalently, we say that B is coarser
than A and we write A  B.
Definition. Let S be a clustering score function. We say that it is non-increasing if for
every x ∈ X n and I, J ∈ Πn such that I  J we have S(x, I) ≤ S(x, J ). If −S is
non-increasing then S is non-decreasing.
Clearly, no non-decreasing score function would be good for clustering purposes as it would
assign the highest score to the clustering into one full cluster, regardless of the data. Similarly, a non-increasing function gives the highest score to the partition of singletons. It
seems desirable for this two tendencies to interplay and it is theoretically appealing to find
increasing and decreasing parts in a given score function.

2

2.2

Properties of the D score function

Notation. To facilitate the notation in the remaining part of the text we use |Σ| to denote
the determinant of a square matrix Σ.
Definition. With the notation presented in Section 1.1 we define
DΣ (x, I) := −

X |I| |I|
1 X |I|
Σ
ln
+ V̂x (I) +
ln
.
2 I∈I n
|I|
n
n
I∈I

(2.1)

and then D(x, I) = DV̂x (x, I) (which is equivalent to (1.1)). Moreover, we use D0 to denote
DΣ with Σ being a matrix of zeroes.
Property 1. Let x1 , . . . , xn ∈ X such that x1 , . . . , xn span X . Let x = (x1 , . . . , xn ). Then
|D(x, I)| < ∞ for any I ∈ Πn .
Proof. For any v ∈ Rd
vt

X
i∈I

(xi − bxI )(xi − bxI )t

!

v=

X
i∈I

v t (xi − bxI )

2

≥0

(2.2)

P
and hence i∈I (xi − bxI )(xi − bxI )t is non-negative definite. Moreover, it follows from the
assumptions that V̂x is positive definite. A sum of non-negative and positive definite matrix
is positive definite, so its determinant is positive. Therefore all the summands in (1.1) are
finite and the proof follows.
Property 2. The score function D is robust to translations and linear isomorphisms.
Proof. It is easy to check that for any x ∈ X n , I ∈ Πn and any translation T we have
D(x, I) = D T (x), I and hence robustness to translations.
Let L : X → X be a linear automorphism, defined by L(x) = Ax, where A is a n × n
invertible matrix. Then
X |I| |I|

1 X
1 X |I|
1
ln
AV̂x At +
ln
=
D L(x), I = −
A(xi − xI )(xi − xI )t At +
2 I∈I n
|I|
|I| i∈I
n
n
I∈I

X |I| |I|

1 X
1
1 X |I|
ln A
V̂x +
ln
=
(xi − xI )(xi − xI )t At +
2 I∈I n
|I|
|I| i∈I
n
n
I∈I
 X |I| |I|
1 X |I| 
1 X
1
=−
ln |A|
V̂x +
ln
=
(xi − xI )(xi − xI )t |At | +
2 I∈I n
|I|
|I| i∈I
n
n
I∈I
=−

= D(x, I) − ln |A|,

(2.3)

which clearly implies robustness to linear isomorphisms.
P
|I|
|I|
Property 3. (a)
I∈I n ln n is increasing
P
(b) − I∈I |I|
ln V̂x (I) is decreasing
n
P
Σ
ln |I|
is increasing
(c) − I∈I |I|
n

Proof. The proof of parts (a) and (b) follow from Proposition 6 by taking the empirical
measure instead of P . Part (c) follows from (a) because
−

X |I|
X |I| |I|
Σ
ln
ln
+ d ln n − ln |Σ|.
=d
n
|I|
n
n
I∈I
I∈I

3

(2.4)

3

The derivation

In this section we give the theoretical foundations for considering the function D as clustering
score function. We present a general formulation of a Bayesian Mixture Model and then we
concentrate on the case where the data within clusters are distributed as Gaussians.
We analyse the asymptotics of the formula for the (unnormalised) posterior in this model.
In this way we concentrate on scoring the partitions of the observation space rather than
the data themselves. However, it is easy to switch to the score statistic by considering an
empirical counterpart of P instead of P ; this yields D0 (cf. (2.1)). The general form of (2.1)
is constructed to prevent the function D0 from assigning an infinite score to clusterings with
very small clusters (of size less than the dimension of the observation space); on the other
hand when the clusters are large enough, then D approximates D0 .

3.1

Bayesian Mixture Models

Let Θ ⊂ Rp be the parameter space and {Gθ : θ ∈ Θ} be a family of probability measures on
the observation space Rd . Consider a prior distribution π on
Let ν be a probability distriPΘ.
m
bution on the m-dimensional simplex ∆m = {p = (pi )m
:
i=1
i=1 pi = 1 and pi ≥ 0 for i ≤ m}
(where m ∈ N ∪ {∞}). Let
p = (pi )m
i=1

∼

θ = (θi )m
i=1

iid

x = (x1 , . . . , xn ) | p, θ

iid

∼

∼

ν
π
Pm

i=1

(3.1)
pi Gθi .

This is a Bayesian Mixture Model. If Gθ a Gaussian distribution for all θ ∈ Θ, we say
that (3.1) defines a Bayesian Mixture of Gaussians. In this case a convenient choice of the
parameter space is Θ = Rd × Sd+ , where Sd+ is the space of positive definite d × d matrices.
Then for θ = (µ, Λ) the distribution Gθ is the multivariate normal distribution N (µ, Λ). A
conjugate prior distribution π on Θ is the Normal-inverse-Wishart distribution, which is
given by
Λ ∼ W −1 (η0 + d + 1, η0 Σ0 )
(3.2)
µ | Λ ∼ N (µ0 , Λ/κ0 )
Here W −1 denotes the inverse Wishart distribution and the hyperparameters are κ0 , η0 > 0,
µ0 ∈ Rd and Σ0 ∈ S + . This prior is listed in Gelman et al. [2013] with a slightly different
hyperparameters, but we made this modification to obtain
EΛ
V(µ)

= Σ0 ,
= E V(µ | Λ) + VE (µ | Λ) = E Λ/κ0 + V(µ0 ) = Σ0 /κ0 ,

(3.3)

which gives a nice interpretation of the hyperparameters.
Formula (3.1) can model data clustering; clusters are defined by deciding which Gθi generated a given data point. In order to formally define the clusters, we need to rewrite (3.1)
as
p = (pi )m
∼ ν
i=1
θ = (θi )m
i=1

iid

φ = (φ1 , . . . , φn ) | p, θ
xi | p, θ, φ

iid

∼

∼
∼

π
Pm

i=1

Gφi

pi δθi
independently for all i ≤ n.

(3.4)

Then the clusters are the classes of abstraction of the equivalence relation i ∼ j ≡ φi = φj . In
this way the distribution ν on the m dimensional simplex generates a probability distribution
Pν,n on the partitions of set [n] into at most m subsets.
Q
iid
Example 3.1. Let V1 , V2 , . . . ∼ Beta(1, α), p1 = V1 , pk = Vk k−1
i=1 (1 − Vi ) for k > 1. Let
ν to be the distribution of p = (p1 , p2 , . . .). The probability on the space of partitions of [n]

4

that ν generates is the Generalized Polya Urn Scheme (Blackwell et al. [1973]) also known
as the Chinese Restaurant Process (Aldous [1985]) with the probability weight given by
Pν,n (I) =

α|I| Y
(|I| − 1)!,
α(n) I∈I

(3.5)

where α(n) = α(α + 1) . . . (α + n − 1).

Lemma 3.2. Let ν be a probability distribution on ∆m that generates a probability Pν,n on
the partitions of [n]. Then for every partition I of [n]
Z
X
Y |I|
Pν,n (I) =
pψ(I) dν(p)
(3.6)
∆m

1−1

ψ : I → [m]

I∈I

where the ,,middle sum” ranges over all injective functions from I to [m] (with the convention
[∞] = N).
Proof. If |I| > m then both sides of (3.6) are 0. We now assume that |I| ≤ m. Let us go
m
back to (3.4) and suppose that the weights p = (pi )m
i=1 and the atoms θ = (θi )i=1 are fixed.
iid Pm
We need to know what is the probability that φ = (φ1 , . . . , φn ) | p, θ ∼ i=1 pi δθi induces
a partition I. This would mean that for every I ∈ I all the values φi for i ∈ I are equal
to θj for some j ≤ m; let j = ψ(I). The values ψ(I) must be different for different I ∈ I,
otherwise I would not be generated. The probability of the sequence (φ1 , . . . , φn ) where
Q
|I|
φi = θψ(I) for i ∈ I is equal to I∈I pψ(I) . Since any assignment of clusters to atoms is
P
Q
|I|
valid, so for fixed p the probability of I is equal to
1−1
I∈I pψ(I) . Since p ∼ ν is
ψ : I → [m]

random, we have to integrate it out and (3.6) follows.

Let Pν,n be the probability distribution on the space of partitions generated by ν. We can
formulate (3.1) as follows: firstly we generate the partition of observations into clusters, and
then for every cluster we sample actual observations from the relevant marginal distribution.
Formally, (3.1) is equivalent to
I
xI := (xi )i∈I | I

∼
∼

Pν,n
f|I| independently for all I ∈ I

(3.7)

iid

where for θ ∼ π, k ∈ N and u = (u1 , . . . , uk ) | θ ∼ Gθ , fk is the marginal density of u, i.e.
fk (u1 , . . . , uk ) :=

Z

π(θ)

Θ

k
Y

gθ (ui )dθ.

(3.8)

i=1

(gθ is the density of Gθ ). We stress the fact that the independent sampling on the ‘lower’ level
of (3.7) relates to the independence between clusters (conditioned on the random partition);
within one cluster the observations are (marginally) dependent. To make the notation more
concise we define
Y
f (x | I) :=
f|I| (xI ).
(3.9)
I∈I

Then (3.7) becomes

I
x|I

∼
∼

Pν,n
f (· | I).

(3.10)

The further analysis requires the exact formula for fk ; in our case it is straightforward to
compute since π and Gθ are conjugate. We state the result here for the reader’s convenience.

5

iid

Proposition 1. Let θ = (µ, Λ) have the distribution given by (3.2) and let u = (u1 , . . . , uk ) | θ ∼
N (µ, Λ). Then the marginal distribution of u is given by
1/2

|η0 Σ0 |ν0 /2 κ0 Γd ν2k

fk (u) =
π dk/2 κk 1/2 Γd ν20



· det (Σ(u))−νk /2 ,

(3.11)

where Γd is the multivariate Gamma function and
νk = η0 + d + 1 + k, κk = κ0 + k
Σ(u) = η0 Σ0 +

k
X
i=1

(ui − u)(ui − uI )t +

and

(3.12)

κ0 k
(u − µ0 )(u − µ0 )t .
κk

(3.13)

Proof. The proof follows from Murphy [2007], equation (266).

3.2

The Induced Partition

Throughout this section P is some fixed probability distribution on Rd .
Definition 3.3. We say that a family A of P -measurable subsets of Rd is a P -partition if

S
• P
A∈A A = 1
• P (A1 ∩ A2 ) = 0 for all A1 , A2 ∈ A, A1 6= A2 .

iid

Notation. Let A be a P -partition of the observation space. Let X1 , X2 , . . . ∼ P and for
n ∈ N let InA = {JnA : A ∈ A} where JnA = {i ≤ n : Xi ∈ A} (if JnA = ∅, we do not include it
in InA ). We say that InA is induced by A.
Proposition 2. Let A be a P -partition of the observation space. Then InA is almost surely
a partition of [n].
Proof. The proof is straightforward and therefore omitted.
Let EP (A) = E P (X | X ∈ A) and VP (A) = VarP (X | X ∈ A), where X ∼ P . That means
EP (A) is the conditional expected value and VP (A) is the conditional covariance matrix of
X conditioned on the event X ∈ A. For a family A of sets with positive P measure let
X
X
VP (A) =
P (A) ln |VP (A)|,
HP (A) = −
P (A) ln P (A),
(3.14)
A∈A

A∈A

where | · | means determinant. Let
1
∆P (A) = − VP (A) − HP (A)
2

(3.15)

It turns out that basically (3.15) is (modulo constant) the first order approximation to the
logarithm of the posterior probability in Bayesian Mixture Model of the data clustering
defined by A, when the data comes as an iid sample from P .
p
Proposition 3. n Pν,n (InA ) · f (X1:n | InA ) ≈ n exp{∆P (A)}, where
∆P (A) = −

X
1 X
P (A) ln |VP (A)| +
P (A) ln P (A)
2 A∈A
A∈A

Proof. The result follows from Proposition 4 and Proposition 5.

6

(3.16)

It should be noted that Proposition 3 does not depend on the form of the prior on probability
measures. This prior is responsible for the ‘entropy‘ part of (3.16).
The final goal is not to score the partitions of the observation space but clusterings of the
data. A natural
P idea is to replace the distribution P in (3.15) by its empirical counterpart.
Let P̂n = n1 i≤n δxi be the empirical probability of x. This is how D0 is obtained.

The function D0 would P
not be a good score statistic, because if J contains a cluster J
ˆ x (J ) = ∞. To
of size less than d then j∈J (xj − xJ )(xj − xJ )t is singular and hence ∆
circumvent this, one could add some positive definite matrix to the within-group covariance
matrix – in this way the relevant determinant will always be greater than zero. Since we
would like to avoid any arbitrary constants in thePscore function, a natural idea is to use
the covariance matrix of the whole dataset, V̂x = i≤n (xi − x)(xi − x)t . This operation is
also motivated by considering the adaptive model, where the strength of prior distribution is
increasing linearly with the number of observations. The details of this approach are given
ˆ x significantly
in Section 4. On the other hand, we do not want this modification to affect ∆
when the sizes of clusters are large and the empirical covariance matrices are good estimates
of theoretical ones. Therefore we decide to decrease the importance of the modification
linearly with the cluster size. This gives (1.1), which is a well defined score statistic.

3.2.1

Auxiliary propositions

Proposition 4. Let P be a probability distribution
on Rd Q
and let A be a finite P -partition
p
a.s.
n
of the observation space. Then limn→∞ f (X1:n | InA ) = A∈A |VP (A)|P (A)

Before we present the proof of Proposition 4, we formulate an auxiliary lemma that concerns
the asymptotics of the function Γd .

an
∞
Notation. If (an )∞
n=1 and (bn )n=1 are real sequences, we write an ≈ bn if limn→∞ bn = 1.
We write an = o(bn ) if limn→∞ abnn = 0. Similarly, if a, b : R → R are real functions, we write

a(x) ≈ b(x) if limx→∞ a(x)
= 1 and a(x) = o b(x) if limx→∞ a(x)
= 0.
b(x)
b(x)

a
1
Lemma 3.4. Let α, β, a, b > 0. If an ≈ αn and bn − β = o nb then abnn ≈ (αn)β .

Proof. For sufficiently large n we have 1 < an < 2αna and − n1b < bn − β <
(2αna )

− 1b
n

− 1

1

1
,
nc

1

< an nb < abnn −β < annb < (2αna ) nc

Left- and right-hand side of (3.17) converge to 1, so

n
ab
an β bn −β
n
from (αn)
an
.
β =
αna

limn→∞ abnn −β

hence

(3.17)

= 1. The proof follows

p

Lemma 3.5. If xn ≈ λn and xn /n − λ = o n1a for some a > 0 then n Γd (xn ) ≈ (λ ne )λd .
√
Proof. Recall Stirling’s formula: Γ(x) ≈ 2πx( xe )x . It follows from Lemma 3.4 that

a

 x xn 1/n
√
 x xn /n  n λ
p
n
n
n
Γ(xn ) ≈
2πxn
= (2πxn )1/n
≈ λ
e
e
e

since n1/n ≈ 1. Note that for fixed t > 0 we have (xn − t) ≈ λn and as a result
s 
  
d
√
Y
p
j−1
n λd
n
n
n
Γd (xn ) = π d(d−1)/4
Γ xn −
.
≈ λ
2
e
j=1

7

(3.18)

(3.19)

Proof of Proposition 4. Note that |JnA | is a random variable with distribution Bin(n, P (A))
for all A ∈ A. Due to Law of Iterated Logarithm we have that almost surely |JnA |/n −

P (A) = o(n−1/2+ε ) for any ε > 0 and hence the assumptions of Lemma 3.5 are almost
surely satisfied, so
s 


P (A)d/2
|JnA | + n0 a.s. P (A) n
n
Γd
·
≈
.
(3.20)
2
2
e
P
Because A is finite and A∈A P (A) = 1, it means that
s
n

Y

Γd

A∈A



|JnA | + n0
2



a.s.

≈

Y

P (A)

P (A)

A∈A

!d/2

 n d/2
.
2e

(3.21)

By the strong law of large numbers we have that
a.s.

(xi − xA )(xi − xA )t /|JnA | ≈ VP (A)

for A ∈ A

(3.22)

and hence, by (3.13), for A ∈ A
X

Σ(XJnA ) /|JnA |d = Σ0 /|JnA | +
a.s.

≈

X

A
i∈Jn

A
i∈Jn

(xi − xA )(xi − xA )t /|JnA | +

a.s.
k0
(xA − µ0 )(xA − µ0 )t ≈
k0 + |JnA |

a.s.

(xi − xA )(xi − xA )t /|JnA | ≈ |VP (A)|
(3.23)

a.s.

a.s.

Hence |Σ(XJnA )| ≈ |JnA |d |VP (A)| ≈ nd P (A)d |VP (A)|. Using the Law of Iterated Logarithm and Lemma 3.4 again we get
q
a.s.
A
n
(3.24)
|Σ(XJnA )|−(|Jn |+n0 )/2 ≈ (P (A)P (A))−d/2 n−dP (A)/2 |VP (A)|−P (A)/2

which means
sY
n

A∈A

A |+n )/2
−(|Jn
0

|Σ(XJnA )|

a.s.

≈

Y

P (A)

P (A)

A∈A

!−d/2

n−d/2

Y

A∈A

|VP (A)|−P (A)/2 (3.25)

and therefore
p
n

f (X1:n | InA )

a.s.

≈

Y

P (A)

P (A)

A∈A

= (2e)−d/2

Y

A∈A

!d/2

 n d/2
2e

|VP (A)|−P (A)/2

Y

A∈A

P (A)

P (A)

!−d/2

n−d/2

Y

A∈A

|VP (A)|−P (A)/2 =

(3.26)

Proposition 5. Let P be a probability distribution on Rd and let A be a finite P -partition of
the observation space. Let Pν,n be a probability distribution
of [n], generated
p on the partitions
a.s. Q
by the probability distribution ν on ∆∞ . Then limn→∞ n Pν,n (InA ) = A∈A P (A)P (A) .
Proof. The proof is a direct consequence of the Law of Large Numbers and Theorem 3.8.

By (3.15), ∆P consists of two components: VP and HP . These two behave differently when
two clusters are joined; the variance component is increasing whereas the entropy component
is decreasing.

8

Proposition 6. Let A be a partition of Rd and let A, B ∈ A. Let C be a partition obtained
from A by joining A and B, i.e. C = A ∪ {A ∪ B} \ {A, B}. Then
(a) HP (A) ≥ HP (C)
(b) VP (A) ≤ VP (C).

Proof. Let C = A ⊔ B.
Part (a):
P (A) ln P (A) + P (B) ln P (B) − P (C) ln P (C) = P (A) ln

P (B)
P (A)
+ P (B) ln
≤ 0 (3.27)
P (C)
P (C)

and the proof follows. The last inequality in (3.27) comes from P (A), P (B) ≤ P (C).
Lemma 3.6. Let A ∩ B = ∅, C := A ∪ B. Then
P (A)VP (A) + P (B)VP (B)  P (C)VP (C)

(3.28)

where  is the Löwner partial order, i.e. M1  M2 iff M2 − M1 is non-negative definite.
Proof. Let e1 (A) = E X1A (X) and e2 (A) = E XX t 1A (X) where X ∼ P . Then
VP (A) =

e1 (A)e1 (A)t
e2 (A)
−
.
P (A)
P (A)2

(3.29)

Note that the functions P, e1 , e2 are additive, hence
P (C)VP (C) − P (A)VP (A) − P (B)VP (B) =

 
 

e1 (C)e1 (C)t
e1 (A)e1 (A)t
e1 (B)e1 (B)t
= e2 (C) −
− e2 (A) −
− e2 (B) −
=
P (C)
P (A)
P (B)
=

e1 (B)e1 (B)t
e1 (C)e1 (C)t
e1 (A)e1 (A)t
+
−
=
P (A)
P (B)
P (C)


t
e1 (A) + e1 (B) e1 (A) + e1 (B)
e1 (B)e1 (B)t
e1 (A)e1 (A)t
+
−
=
=
P (A)
P (B)
P (A) + P (B)


t
P (A)P (B)
e(B)
e(B)
e(A)
e(A)
=
−
−
.
(P (A) + P (B)) P (B)
P (A)
P (B)
P (A)

(3.30)

The last matrix in (3.30) is clearly non-negative definite and the proof follows.
Theorem 3.7. (Theorem 2.4.4 in Horn et al. [1990]) The function ln det(·) is convex on
the space of positive definite matrices.
Proof of part (b):
T heorem 3.7
P (A)
P (A)
P (B)
P (B)
≤
ln
ln |VP (A)| +
ln |VP (B)|
VP (A) +
VP (B) ≤
P (C)
P (C)
P (C)
P (C)
Lemma 3.6
≤
ln |VP (C)|
(3.31)
and the proof follows.

9

Theorem 3.8. Let Pν,n be a probability distribution on the partitions of [n], generated by
the probability distribution ν on ∆∞ . Fix K ∈ N and consider a sequence of partitions
(In )n∈N , where In = {In,1 , . . . , In,K } is a partition of [n] (it is possible that In,i = ∅ for
some i ≤ K). Assume that |In,k |/n → αk > 0 for k ≤ K. Then
lim

n→∞

K
Y
p
n
k
αα
Pn,ν (In ) =
k

(3.32)

k=1

Proof. Firstly note that for sufficiently large n we have |Ik,n | ≥ 1 for all k ≤ K. Then in
(3.6) we sum functions that depend on exactly K coordinates of p. Hence we can
PKexpress
(3.6) in the form of an integral on the K-dimensional set NK = {(p1 , . . . , pK ) :
k=1 pk =
1, ∀k≤K pk ∈ (0, 1)} as
Z
K
Y
|I
|
pk k,n dνK (p)
(3.33)
Pn,ν (In ) =
NK k=1

where νK is a measure on N

K

defined by
X

ν (pψ(1) , pψ(2) , . . . , pψ(K) ) ∈ A
νK (A) =

(3.34)

1−1

ψ : [K] → N

K

for A ⊂ N , where [K] = {1, 2, . . . , K}. Hence
v
uZ
K
u
Y
p
|I
|
n
n
pi k,n dνK (p) = kgn kn
Pn,ν (In ) = t

(3.35)

NK k=1

Q
|Ik,n |/n
where gn (p1 , . . . , pK ) = K
and k · kn is the norm in Ln (NK , νK ) space.
k=1 pk
Since νK is not a finite measure on NK , in the remaining part of the proof we will have to be
careful that the functions we are considering belong to the space Ln (NK , νK ) for sufficiently
large n.
Q
QK
αk
Let g(p1 , . . . , pK ) = K
k=1 pk and let h(p1 , . . . , pK ) =
k=1 pk . Note that
Z


h(p)dνK (p) = PK,ν {1}, {2}, . . . , {K}
≤ 1.
(3.36)
NK

Moreover for n > 1/ min αi we have g n (p) ≤ h(p) and therefore g ∈ Ln (NK , νK ) for n >
1/ min αi . Because g is bounded by 1 we get
Y α
(3.37)
αk k
kgkn → kgk∞ = sup g =
NK

Q

k≤K

αk
k≤K αk

(the fact that kgk∞ = supNK g =
follows easily from applying the Lagrange
multipliers).
We now prove that kgn − gkn → 0. It is not a straightforward consequence of the pointwise
convergence of gn to g since νK is not a finite measure on NK .
Clearly, (|Ik,n |/n − αk /2) → αk /2 > 0 and hence kgn g −1/2 − g 1/2 k∞ → 0 on NK .
Let N ∈ N be chosen so that for n > N we have kgn g −1/2 − g 1/2 k∞ < ε and nαk ≥ 2 for
k ≤ K. Then for n > N
Z
Z
n
n
kgn − gkn =
|gn − g| dνK (p) =
|gn g −1/2 − g 1/2 |n g n/2 dνK (p) ≤
K
K
N
Z
ZN
(3.38)
n
n/2
n
≤ǫ
g dνK (p) ≤ ǫ
hdνK (p) ≤ ǫn ,
NK

NK

hence kgn − gkn → 0. The result follows from the triangle inequality

kgn kn − kgk∞ ≤ kgn kn − kgkn + kgkn − kgk∞ ≤ kgn − gkn + kgkn − kgk∞ . (3.39)

10

Lemma 3.9.
Q Let αik > 0 for i ≤ K and
supNK g = k≤K αα
k .

PK

i=1

αi = 1. Let g(p1 , . . . , pK ) =

QK

k=1

k
pα
k . Then

Proof. As αi > 0 for i ≤ K, the function g is continuous and, because NK is compact in RK ,
it achieves its extreme values. Let
p̂ = (p̂1 , . . . , p̂K ) ∈ NK satisfy g(p̂K ) = supNK g. Clearly,
PK
K
p̂ ∈ ∆ . Indeed, otherwise s = i=1 p̂i < 1, p̂/s ∈ NK and g(p̂/s) = g(p̂)/s > g(p̂), which
contradicts the definition of p̂. Since g is nonnegative on ∆K and it is equal to 0 on the
boundary of ∆K , we know that p̂ is in the interior of ∆K . The function g is positive on the
interior of ∆K , so by considering the function ln(g) and using the Lagrange multipliers, we
gat that p̂ satisfies
αi
0 = (αi ln pi )′ + λ =
+λ
(3.40)
pi
PK
for i ≤ K and some λ ∈ R. Hence pi ’s are proportional to αi ’s, and because i=1 αi = 1,
we get that pˆi = αi and the proof follows.

4

Adaptive model

We now allow parameters of the model (3.2) to change with the number of observations.
More precisely, we perform a substitution η0 7→ λn =: ηn so that the expected value of the
within group precision matrix is fixed and increasingly concentrated on Σ0 . We investigate
the limit formula for the posterior as n goes to infinity. Note that in this case Σ|JnA | /n →
λΣ0 + VP (A).
Λ
µ|Λ

∼
∼

W −1 (ηn + d + 1, ηn Σ0 )
N (µ0 , Λ/κ0 )

(4.1)

Proposition 7. Let P be a probability distribution on Rd and let A be a finite P -partition
of the observation space. Then

Y
p
a.s.
λ
P (A)
n
|
f (X1:n | InA ) ≈ (2e)−(1+|A|λ)d/2
Σ0 +
VP (A)|− P (A)+λ /2
P (A) + λ
P (A) + λ
A∈A
(4.2)

Proof. Note that |JnA | is a random variable with distribution Bin(n, P (A)) for all A ∈ A.
Due to Law of Iterated Logarithm we have that almost surely |JnA |/n−P (A) = o(n−1/2+ε )
for any ε > 0 and hence the assumptions of Lemma 3.5 are almost surely satisfied, so

s 

 P (A)+λ d/2

A| + η
a.s.
|J
n
P
(A)
+
λ
n
n
n
Γd
·
≈
.
(4.3)
2
2
e
P
Because A is finite and A∈A P (A) = 1, it means that
!d/2
s

 A
 n (1+|A|λ)d/2
Y
Y
P (A)+λ
a.s.
|J
|
+
n
0
n
n
(4.4)
≈
Γd
P (A) + λ
.
2
2e
A∈A
A∈A
By the strong law of large numbers we have that

a.s.

(xi − xA )(xi − xA )t /|JnA | ≈ VP (A)

for A ∈ A

and hence, by (3.13), for A ∈ A
Σ(XJnA ) /|JnA |d = ηn Σ0 /|JnA | +

X

A
i∈Jn

(xi − xA )(xi − xA )t /|JnA | +

(4.5)

a.s.
k0
(xA − µ0 )(xA − µ0 )t ≈
k0 + |JnA |

X
a.s.
λ
λ
Σ0 +
Σ0 + VP (A)|
(xi − xA )(xi − xA )t /|JnA | ≈ |
≈
P (A)
P
(A)
A

a.s.

i∈Jn

11

(4.6)

d
a.s.a.s.
(A)
λ
Hence |Σ(XJnA )| ≈ ≈ nd P (A)+λ | P (A)+λ
Σ0 + P P(A)+λ
VP (A)|. Using the Law of Iterated
Logarithm and Lemma 3.4 again we get

q
−(P (A)+λ)d/2
a.s.
λ
P (A)
A
n
|
|Σ(XJnA )|−(|Jn |+ηn )/2 ≈ n(P (A)+λ)
Σ0 +
VP (A)|− P (A)+λ /2
P (A) + λ
P (A) + λ
(4.7)
and (4.2) follows.

5

Discussion

In this article we proposed a score function that can be used for choosing the number of
clusters in popular clustering methods. It is derived as a limit in a Bayesian Mixture Model
of Gaussians. We derived some of its properties, though there are some questions that
remain unanswered. For example, it is interesting to ask what assumptions on P should be
made to ensure that the supremum of possible values of the ∆ function is finite.

References
David J Aldous. Exchangeability and related topics. In École d’Été de Probabilités de
Saint-Flour XIII—1983, pages 1–198. Springer, 1985.
David Blackwell, James B MacQueen, et al. Ferguson distributions via pólya urn schemes.
The annals of statistics, 1(2):353–355, 1973.
Andrew Gelman, Hal S Stern, John B Carlin, David B Dunson, Aki Vehtari, and Donald B
Rubin. Bayesian data analysis. Chapman and Hall/CRC, 2013.
Roger A Horn, Roger A Horn, and Charles R Johnson. Matrix analysis. Cambridge university press, 1990.
Kevin P Murphy. Conjugate bayesian analysis of the gaussian distribution. def, 1(2σ2):16,
2007.
Peter J Rousseeuw. Silhouettes: a graphical aid to the interpretation and validation of
cluster analysis. Journal of computational and applied mathematics, 20:53–65, 1987.
Robert Tibshirani, Guenther Walther, and Trevor Hastie. Estimating the number of clusters
in a data set via the gap statistic. Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 63(2):411–423, 2001.

12

