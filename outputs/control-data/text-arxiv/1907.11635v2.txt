Subexponential-Time Algorithms for Sparse PCA
Yunzi Ding∗1 , Dmitriy Kunisky‡1 , Alexander S. Wein§1 , and Afonso S. Bandeira¶2

arXiv:1907.11635v2 [math.ST] 2 Mar 2020

1

Department of Mathematics, Courant Institute of Mathematical Sciences, New York
University, USA
2
Department of Mathematics, ETH Zurich, Switzerland

Abstract
We study the computational cost of recovering a unit-norm sparse principal component
x ∈ Rn planted in a random matrix, in either the Wigner or Wishart spiked model (observing
either W + λxx⊤ with W drawn from the Gaussian orthogonal ensemble, or N independent
samples from N (0,p
In + βxx⊤ ), respectively). Prior work has shown that when the signal-tonoise ratio (λ or β N/n, respectively) is a small constant and the fraction of nonzero entries
√
in the planted vector is kxk0 /n = ρ, it is possible to recover x in polynomial time if ρ . 1/ n.
While it is possible to recover x in exponential time under the weaker
√ condition ρ ≪ 1, it
is believed that polynomial-time recovery is impossible unless ρ . 1/ n. We√investigate the
precise amount of time required for recovery in the “possible but hard” regime 1/ n ≪ ρ ≪ 1 by
δ
exploring the power of subexponential-time
√ algorithms, i.e., algorithms running in time exp(n )
for some constant δ ∈ (0, 1). For any 1/ n ≪ ρ ≪ 1, we give a recovery algorithm with runtime
roughly exp(ρ2 n), demonstrating a smooth tradeoff between sparsity and runtime. Our family of
algorithms interpolates smoothly between two existing algorithms: the polynomial-time diagonal
thresholding algorithm and the exp(ρn)-time exhaustive search algorithm. Furthermore, by
analyzing the low-degree likelihood ratio, we give rigorous evidence suggesting that the tradeoff
achieved by our algorithms is optimal.

∗

Email: yding@nyu.edu. Partially supported by NSF grant DMS-1712730.
Email: kunisky@cims.nyu.edu. Partially supported by NSF grants DMS-1712730 and DMS-1719545.
§
Email: awein@cims.nyu.edu. Partially supported by NSF grant DMS-1712730 and by the Simons Collaboration
on Algorithms and Geometry.
¶
Email: bandeira@math.ethz.ch. Most of this work was done while ASB was with the Department of Mathematics
at the Courant Institute of Mathematical Sciences, and the Center for Data Science, at New York University; and
partially supported by NSF grants DMS-1712730 and DMS-1719545, and by a grant from the Sloan Foundation.
‡

Contents
1 Introduction
1.1 Spiked Matrix Models . . . . . . . . . . .
1.2 Principal Component Analysis . . . . . .
1.3 Sparse PCA . . . . . . . . . . . . . . . . .
1.4 Our Contributions . . . . . . . . . . . . .
1.5 Background on the Low-Degree Likelihood

. . . .
. . . .
. . . .
. . . .
Ratio

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

3
3
4
4
5
6

2 Main Results
8
2.1 The Wishart Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2 The Wigner Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
3 Proofs for Subexponential-Time Algorithms
15
3.1 The Wishart Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
3.2 The Wigner Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
4 Proofs for Low-Degree Likelihood Ratio Bounds
4.1 Low-Degree Likelihood Ratio for Spiked Models . .
4.2 Introduction and Estimates of Ad . . . . . . . . . .
4.3 The Wishart Model . . . . . . . . . . . . . . . . .
4.4 The Wigner Model . . . . . . . . . . . . . . . . . .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

25
25
26
30
35

Acknowledgments

37

References

37

A Chernoff Bounds

43

2

1

Introduction

1.1

Spiked Matrix Models

Since the foundational work of Johnstone [Joh01], spiked random matrix ensembles have been
widely studied throughout random matrix theory, statistics, and theoretical data science. These
models describe a deformation of one of several canonical random matrix distributions by a rankone perturbation or “spike,” intended to capture a signal corrupted by noise. Spectral properties
of these spiked models have received much attention in random matrix theory [BBP05, BS06,
Pau04, Péc06, FP07, CDMF09, BGN11, PRS13, KY13], leading to a theoretical understanding of
methods based on principal component analysis (PCA) for recovering the direction of the rank-one
spike [Joh01, JL04, Pau07, Nad08, JL09]. Spiked matrix models have also found more specific
applications to problems such as community detection in graphs (see, e.g., [McS01, Vu18, DAM16],
or [Moo17, Abb17] for surveys) and synchronization over groups (see, e.g., [Sin11, SS11, JMRT16,
PWBM16, PWBM18a]).
We will study two classical variants of the spiked matrix model: the Wigner and Wishart
models. The models differ in how noise is applied to the signal vector. In either case, let x ∈ Rn
be the signal vector (or “spike”). We will either have x deterministic with kxk2 = 1, or x ∈ Rn
random for each n with kxk2 → 1 in probability as n → ∞.
• Spiked Wigner Model. Let λ > 0. Observe Y = W + λxx⊤ , where W ∈ Rn×n is
drawn from the Gaussian orthogonal ensemble GOE(n), i.e., W is symmetric with entries
distributed independently as Wii ∼ N (0, 2/n) for all 1 ≤ i ≤ n, and Wij = Wji ∼ N (0, 1/n)
for all 1 ≤ i < j ≤ n.
• Spiked Wishart Model. Let β > 0 and N ∈ N. Observe N samples y (1) , y (2) , . . . , y (N ) ∈ Rn
drawn independently from N (0, In + βxx⊤ ). The ratio of dimension to number of samples
is denoted γ := n/N . We will focus on the high-dimensional regime where γ converges to a
P
(i) (i) ⊤ .
constant as n → ∞. We let Y denote the sample covariance matrix Y = N1 N
i=1 y y

Each of these planted models has a corresponding null model, given by sampling from the planted
model with either λ = 0 (Wigner) or β = 0 (Wishart).
We are interested in the computational feasibility of the following two statistical tasks, to be
performed given a realization of the data (either Y or {y (1) , . . . , y (N ) }) drawn from either the null
or planted distribution.
• Detection. Perform a simple hypothesis test between the planted model and null model.
We say that strong detection is achieved by a statistical test if both the type-I and type-II
errors tend to 0 as n → ∞.
• Recovery. Estimate the spike x given data drawn from the planted model. We say that
a unit-norm estimator x
b ∈ Rn achieves weak recovery if hb
x, xi2 remains bounded away from
1
zero with probability tending to 1 as n → ∞. (Note that we cannot hope to distinguish
between the planted models with signals x and −x.)
For high-dimensional inference problems such as the spiked Wigner and Wishart models, these two
tasks typically share the same computational profile: with a given computational time budget,
strong detection and weak recovery are possible in the same regions of parameter space.
1
We will also consider stronger notions of recovery: strong recovery is hx̂, xi2 → 1 as n → ∞ and exact recovery is
x̂ = x with probability 1 − o(1).

3

1.2

Principal Component Analysis

Simple algorithms for both detection and recovery are given by principal component analysis (PCA)
of the matrix Y . For detection, one computes and thresholds the maximum eigenvalue λmax (Y )
of Y , while for recovery one estimates x using the leading eigenvector vmax (Y ). Both the spiked
Wishart and Wigner models are known to exhibit a sharp transition in their top eigenvalue as the
model parameters vary. For the Wishart model, the celebrated “BBP transition” of Baik, Ben
Arous, and Péché [BBP05, BS06] states that the maximum eigenvalue of the sample covariance
matrix Y emerges from the Marchenko–Pastur-distributed bulk if and only if β 2 > γ. Similarly, in
the Wigner model, the maximum eigenvalue of Y emerges from the semicircular bulk if and only if
λ > 1 [FP07]. More formally, the following statements hold.
Theorem 1.1 ([FP07, BGN11]). Consider the spiked Wigner model Y = W + λxx⊤ with kxk = 1
and λ > 0 fixed. Then as n → ∞,
• if λ ≤ 1, λmax (Y ) → 2 almost surely, and hvmax (Y ), xi2 → 0 almost surely (where vmax
denotes the leading eigenvector);
• if λ > 1, λmax (Y ) → λ + λ−1 > 2 almost surely, and hvmax (Y ), xi2 → 1 − λ−2 > 0 almost
surely.
Theorem 1.2 ([BBP05, BS06, Pau04]). Let Y denote the sample covariance matrix in the spiked
Wishart model with kxk = 1, β > 0 fixed, and N = N (n) such that γ := n/N converges to a
constant γ̄ > 0 as n → ∞. Then as n → ∞,
√
• if β 2 ≤ γ̄, λmax (Y ) → (1 + γ̄)2 almost surely, and hvmax (Y ), xi2 → 0 almost surely (where
vmax denotes the leading eigenvector);
√
• if β 2 > γ̄, λmax (Y ) → (1 + β)(1 + γ̄/β) > (1 + γ̄)2 almost surely, and hvmax (Y ), xi2 →
(1 − γ̄/β 2 )(1 + γ̄/β) > 0 almost surely.
We define the signal-to-noise ratio (SNR) λ̂ by

λ
in the Wigner model,
λ̂ :=
√
β/ γ in the Wishart model.

(1)

Theorems 1.1 and 1.2 then characterize the performance of PCA in terms of λ̂. Namely, thresholding
the largest eigenvalue of Y succeeds at strong detection when λ̂ > 1 and fails when λ̂ ≤ 1; similarly,
the top eigenvector succeeds at weak recovery when λ̂ > 1 and fails when λ̂ ≤ 1. For some
distributions of x, including the spherical prior (x drawn uniformly from the unit sphere) and
√
the Rademacher prior (each entry xi drawn i.i.d. from Unif(±1/ n)), it is known that the PCA
threshold is optimal, in the sense that strong detection and weak recovery are statistically impossible
(for any test or estimator, regardless of computational cost) when λ̂ < 1 [OMH13, MRZ15, DAM16,
BMV+ 18, PWBM18b].

1.3

Sparse PCA

Sparse PCA, a direction initiated by Johnstone and Lu [JL04, JL09], seeks to improve performance
when the planted vector is known to be sparse in a given basis. While various sparsity assumptions

4

have been considered, a simple and illustrative one is to take x drawn from the sparse Rademacher
prior, denoted Xnρ , in which each entry xi is distributed independently as

1
ρ


with probability
√


ρn
2


ρ
xi = − 1
(2)
with probability
√


ρn
2




0 with probability 1 − ρ

for some known sparsity parameter ρ ∈ (0, 1], which may depend on n.2 The normalization ensures
kxk → 1 in probability as n → ∞.
Consider the Wishart model (we will see that the Wigner model shares essentially the same
behavior) in the regime λ̂ = Θ(1) with λ̂ < 1 (so that ordinary PCA fails at weak recovery). The
simple diagonal thresholding algorithm proposed by [JL09] estimates
the support of x by identifying
√
the largest diagonal entries of Y . Under the condition3 ρ . 1/ n log n, this has been shown [AW08]
to achieve exact support recovery, i.e., it exactly recovers the support of x with probability tending
to 1 as n → ∞ (and once the support is known, it is straightforward to recover x). The more
sophisticated covariance thresholding algorithm proposed by [KNV15] has been shown [DM14b] to
√
achieve exact support recovery when ρ . 1/ n.
On the other hand, given unlimited computational power, an exhaustive search over all possible
support sets of size ρn achieves exact support recovery under the much weaker assumption ρ .
1/ log(n) [PJ12, VL12, CMW13]. Similarly, strong detection and weak recovery are statistically
possible even when ρ is a sufficiently small constant (depending on β, γ) [BMV+ 18, PWBM18b],
and the precise critical constant ρ∗ (β, γ) is given by the replica formula from statistical physics
(see, e.g., [LKZ15a, LKZ15b, KXZ16, DMK+ 16, LM19, Mio17, EKJ17, EK18, EKJ18], or [Mio18]
for a survey). However, no polynomial-time algorithm is known to succeed (for any reasonable
√
notion of success) when ρ ≫ 1/ n, despite extensive work on algorithms for sparse PCA [dGJL05,
ZHT06, MWA06, dBG08, AW08, WTH09, BR13b, DM14a, KNV15, BPP18].
In fact, a growing body of theoretical evidence suggests that no polynomial-time algorithm can
√
succeed when ρ ≫ 1/ n [BR13a, CMW13, MW15, WBS16, HKP+ 17, BBH18, BB19]. Such evidence takes the form of reductions [BR13a, WBS16, BBH18, BB19] from the planted clique problem
(which is widely conjectured to be hard in certain regimes [Jer92, DM15, MPW15, BHK+ 19]), as
well as lower bounds against the sum-of-squares hierarchy of convex relaxations [MW15, HKP+ 17].
√
Thus, we expect sparse PCA to exhibit a large “possible but hard” regime when 1/ n ≪ ρ ≪ 1.
Statistical-to-computational gaps of this kind are believed to occur and have been studied extensively in many other statistical inference problems, such as community detection in the stochastic
block model [DKMZ11b, DKMZ11a] and tensor PCA [RM14, HSS15, HKP+ 17].

1.4

Our Contributions

√
In this paper, we investigate precisely how hard the “hard” region (1/ n ≪ ρ ≪ 1) is in sparse
PCA. We consider subexponential-time algorithms, i.e., algorithms with runtime exp(nδ+o(1) ) for
fixed δ ∈ (0, 1). We show a smooth tradeoff between sparsity (governed by ρ) and runtime (governed
by δ). More specifically, our results (for both the Wishart and Wigner models) are as follows.
• Algorithms. For any δ ∈ (0, 1), we give an algorithm with runtime exp(nδ+o(1) ) that achieves
exact support recovery, provided ρ ≪ n(δ−1)/2 .
2
3

We analyze our algorithms for a more general set of assumptions on x; see Definition 2.1.
We use A . B to denote A ≤ CB for some constant C, and use A ≪ B to denote A ≤ B/polylog(n).

5

• Lower bounds. Through an analysis of the low-degree likelihood ratio (see Section 1.5),
we give formal evidence suggesting that the above condition is essentially tight in the sense
that no algorithm of runtime exp(nδ+o(1) ) can succeed when ρ ≫ n(δ−1)/2 . (Our results are
sharper than the sum-of-squares lower bounds of [HKP+ 17] in that we pin down the precise
constant δ.)
δ
Our algorithm involves exhaustive search over subsets of [n] of cardinality ℓ ≈
√ n . The case ℓ = 1 is
diagonal thresholding (which is polynomial-time and succeeds when ρ . 1/ n log n) and the case
ℓ = ρn is exhaustive search over all possible spikes (which requires time exp(ρn1+o(1) ) and succeeds
when ρ . 1/(log n)). As ℓ varies in the range 1 ≤ ℓ ≤ ρn, our algorithm interpolates smoothly
√
between these two extremes. For a given ρ in the range 1/ n ≪ ρ ≪ 1, the smallest admissible
choice of ℓ is roughly ρ2 n, yielding an algorithm of runtime exp(ρ2 n1+o(1) ).
Our results extend to the case λ̂ ≪ 1, e.g., λ̂ = n−α for some constant α > 0. In this case,
provided ρ ≪ λ̂2 (which is information-theoretically necessary [PJ12, VL12, CMW13]), there is an
exp(nδ+o(1) )-time algorithm if ρ ≪ λ̂n(δ−1)/2 , and the low-degree likelihood ratio again suggests
√
that this is optimal. In other words, for a given ρ in the range λ̂/ n ≪ ρ ≪ λ̂2 , we can solve
sparse PCA in time exp(λ̂−2 ρ2 n1+o(1) ).
The analysis of our algorithm applies not just to the sparse Rademacher spike prior, but also
to a weaker set of assumptions on the spike that do not require all of the nonzero entries to have
the same magnitude. Our algorithm is guaranteed (with high probability) to exactly recover both
the support of x and the signs of the nonzero entries of x. Once the support is known, it is
straightforward to estimate x via the leading eigenvector of the appropriate submatrix.
In independent work [HSV19], a different algorithm was proposed and shown to have essentially
the same subexponential runtime as ours.

Remark 1.3. Certain problems besides sparse PCA have a similar smooth tradeoff between subexponential runtime requirements and statistical power. These include refuting random constraint
satisfaction problems [RRS17] and tensor PCA [BGG+ 16, BGL16, WEM19]. In contrast, other
problems have a sharp threshold at which they transition from being solvable in polynomial-time
to (conjecturally) requiring essentially exponential time: exp(n1−o(1) ). Examples of this behavior
can occur at the spectral transition at λ̂ = 1 in the spiked Wishart and Wigner matrix models (see
[BKW19, KWB19]) as well as at the Kesten–Stigum threshold in the stochastic block model (see
[DKMZ11b, DKMZ11a, HS17, Hop18]).

1.5

Background on the Low-Degree Likelihood Ratio

A sequence of recent work on the sum-of-squares hierarchy [BHK+ 19, HS17, HKP+ 17, Hop18] has
led to the development of a remarkably simple method for predicting the amount of computation
time required to solve statistical tasks. This method—which we will refer to as the low-degree
method—is based on analyzing the so-called low-degree likelihood ratio, and is believed to be intimately connected to the power of sum-of-squares (although formal implications have not been
established). We now give an overview of this method; see [Hop18, KWB19] for more details.
We will consider the problem of distinguishing two simple hypotheses Pn and Qn , which are
probability distributions on some domain Ωn = Rd(n) with d(n) = poly(n). The idea of the lowdegree method is to explore whether there is a low-degree polynomial fn : Ωn → R that can
distinguish Pn from Qn .
We call Qn the “null” distribution, which for us will always be i.i.d. Gaussian (see Definitions 2.4
and 2.17). Qn induces an inner product on L2 functions f : Ωn → R given by hf, giL2 (Qn ) =
EY ∼Qn [f (Y )g(Y )], and a norm kf k2L2 (Qn ) = hf, f iL2 (Qn ) . For D ∈ N, let R[Y ]≤D denote the
6

multivariate polynomials Ωn → R of degree at most D. For f : Ωn → R, let f ≤D denote the
orthogonal projection (with respect to h·, ·iL2 (Qn ) ) of f onto R[Y ]≤D . The following result then
relates the distinguishing power of low-degree polynomials (in a certain L2 sense) to the low-degree
likelihood ratio.
Theorem 1.4 ([HS17, HKP+ 17]). Let P and Q be probability distributions on Ω = Rd . Suppose P
dP
is defined. Then
is absolutely continuous with respect to Q, so that the likelihood ratio L = dQ
E
[f (Y )]
p Y ∼P
= kL≤D kL2 (Q) .
2
f ∈R[Y ]≤D \{0}
EY ∼Q [f (Y ) ]
max

(3)

(The proof is straightforward: the fraction on the left can be written as hf, LiL2 (Qn ) /kf kL2 (Qn ) , so
the maximizer is f = L≤D .) The left-hand side of (3) is a heuristic measure of how well degree-D
polynomials can distinguish P from Q: if this quantity is O(1) as n → ∞, this suggests that no
degree-D polynomial can achieve strong detection (and indeed this is made formal by Theorem 4.3 of
[KWB19]). The right-hand side of (3) is the norm of the low-degree likelihood ratio (LDLR), which
can be computed or bounded in many cases, making this heuristic a practical tool for predicting
computational feasibility of hypothesis testing.
The key assumption underlying the low-degree method is that, for many natural distributions
Pn and Qn , degree-D polynomials are as powerful as algorithms of runtime nΘ̃(D) . This is captured
by the following informal conjecture, which is based on [HS17, HKP+ 17, Hop18]; in particular, see
Hypothesis 2.1.5 of [Hop18].
Conjecture 1.5 (Informal). Suppose t : N → N. For “nice” sequences of distributions Pn and
≤D(n)
Qn , if kLn
kL2 (Qn ) remains bounded as n → ∞ whenever D(n) ≤ t(n) · polylog(n), then there
exists no sequence of functions fn : Ωn → {p, q} with fn computable in time nt(n) that strongly
distinguishes Pn and Qn , i.e., that satisfies
lim Qn [fn (Y ) = q] = lim Pn [fn (Y ) = p] = 1.

n→∞

n→∞

(4)

On a finer scale, it is conjectured [HS17, Hop18] that if for some ε > 0 we have D(n) ≥ log1+ε (n)
≤D(n)
and kLn
kL2 (Qn ) = O(1), then no polynomial-time algorithm can strongly distinguish Pn from
Qn . In practice, it seems that the converse of Conjecture 1.5 often holds as well, in the sense that if
≤D(n)
kLn
kL2 (Qn ) = ω(1) for some D(n) = t(n)/polylog(n), then there is an nt(n) -time distinguishing
algorithm (however, see Remark 2.16 for one caveat).
Calculations with the LDLR have been carried out for problems such as community detection
[HS17, Hop18], planted clique [BHK+ 19, Hop18], the spiked Wishart model [BKW19], the spiked
Wigner model [KWB19], and tensor PCA [HKP+ 17, Hop18, KWB19] (tensor PCA exhibits a
subexponential-time tradeoff similar to sparse PCA; see [KWB19]). In all of the above cases, the
low-degree predictions coincide with widely-conjectured statistical-versus-computational tradeoffs.
≤D(n)
Low-degree lower bounds of the form kLn
kL2 (Qn ) = O(1) are known to formally imply
lower bounds (in a particular sense) against a general class of spectral methods; see Theorem 4.4 of
[KWB19]. The low-degree predictions are also conjectured to coincide with the power of the sumof-squares hierarchy and are in particular connected to the pseudo-calibration approach [BHK+ 19];
see [HKP+ 17, RSS18, Hop18]. We refer the reader to Section 4 of [KWB19] for further discussion
of the implications (both formal and conjectural) of low-degree lower bounds.
Conjecture 1.5 is informal in the sense that we have not specified the meaning of “nice” Pn
and Qn . Roughly speaking, highly-symmetric high-dimensional problems are considered “nice” so
7

long as Pn and Qn have at least a small amount of noise in order to rule out brittle high-degree
algorithms such as Gaussian elimination. (In particular, we consider spiked Wigner and Wishart to
be “nice.”) Conjecture 2.2.4 of [Hop18] is one formal variant of the low-degree conjecture, although
it uses the more refined notion of coordinate degree and so does not apply to the calculations in
this paper.
We remark that if kLn kL2 (Qn ) = O(1) (the D = ∞) case, then it is statistically impossible
to strongly distinguish Pn and Qn ; this is a commonly-used second moment method (see e.g.,
[MRZ15, BMV+ 18, PWBM18b]) of which Conjecture 1.5 is a computationally-bounded analogue.
In this paper we give tight computational lower bounds for sparse PCA, conditional on Conjecture 1.5. Alternatively, one can view the results of this paper as a “stress test” for Conjecture 1.5:
we show that Conjecture 1.5 predicts a certain statistical-versus-computational tradeoff and this
indeed matches the best algorithms that we know.
Organization. The remainder of the paper is organized as follows. In Section 2, we present our
subexponential-time algorithms and our lower bounds based on the low-degree likelihood ratio. In
Section 3, we give proofs for the correctness of our algorithms. In Section 4, we give proofs for our
analysis of the low-degree likelihood ratio.
Notation. We use standard asymptotic notation O(·), Ω(·), Θ(·), always pertaining to the limit
n → ∞. We also use Õ(B) to mean O(B · polylog(n)) and Ω̃(B) to mean Ω(B/polylog(n)).
Also recall that f (n) = o(g(n)) means f (n)/g(n) → 0 as n → ∞ and f (n) = ω(g(n)) means
f (n)/g(n) → ∞ as n → ∞. An event occurs with high probability if it occurs with probability
1 − o(1). We sometimes use the shorthand A . B to mean A ≤ CB for an absolute constant C,
and the shorthand A ≪ B to mean A ≤ B/polylog(n).

2

Main Results

In the analysis of our algorithms, we consider the spiked Wishart and Wigner models with signal
x satisfying the following properties.
Definition 2.1. For ρ ∈ (0, 1] and A ≥ 1, a vector x ∈ Rn is called (ρ, A)-sparse if
• kxk2 = 1 and kxk0 = ρn, and
• for any i ∈ supp(x),

1
√
A ρn

≤ |xi | ≤

√A .
ρn

Here we have used the standard notations supp(x) = {i ∈ [n] : xi 6= 0} and kxk0 = |supp(x)|. We
assume that ρ (which may depend on n) is chosen so that ρn is an integer.
Remark 2.2. A lower bound on |xi | is essential for exact support recovery, since we cannot hope
to distinguish tiny nonzero entries of x from zero entries. The upper bound on |xi | is a technical
condition that is likely not essential, and is only used for recovery in the Wishart model (Theorem 2.10).
In our calculations of the low-degree likelihood ratio, we instead assume the signal x is drawn
from the sparse Rademacher distribution, defined as follows.

8

Definition 2.3. The sparse Rademacher prior Xnρ with sparsity ρ ∈ (0, 1] is the distribution on
Rn whereby x ∼ Xnρ has i.i.d. entries distributed as

√
 +1/ ρn with probability ρ/2,
√
−1/ ρn with probability ρ/2,
(5)
xi =

0
with probability 1 − ρ.
Note that x ∼ Xnρ has kxk2 → 1 in probability as n → ∞.

2.1

The Wishart Model

We first present our results for the Wishart model. Our algorithms and results for the Wigner
model are essentially identical and can be found in Section 2.2.
Definition 2.4 (Spiked Wishart model). The spiked Wishart model with parameters n, N ∈ N+ ,
β ≥ 0, and planted signal x ∈ Rn is defined as follows.
• Under Pn = Pn,N,β , we observe N independent samples y (1) , . . . , y (N ) ∼ N (0, In + βxx⊤ ).
• Under Qn = Qn,N , we observe N independent samples y (1) , . . . , y (N ) ∼ N (0, In ).
We will sometimes specify a prior Xn for x, in which case Pn first draws x ∼ Xn and then draws
y (1) , . . . , y (N ) as above.
Detection. We first consider the detection problem, where the goal is to determine whether the
given data {y (i) } was drawn from Pn or Qn .
Algorithm 1: Detection in the spiked Wishart model
Input: Data {y (i) }1≤i≤N , parameters ρ ∈ (0, 1], β ≥ 0, A ≥ 1, ℓ ∈ N+
1 PN
(i) (i) ⊤
1: Compute the sample covariance matrix: Y ← N
i=1 y y
2: Specify the search set: In,ℓ ← {v ∈ {−1, 0, 1}n : kvk0 = ℓ}
3: Compute the test statistic: T ← maxv∈In,ℓ v ⊤ Y v
βℓ
4: Compute the threshold: T ∗ ← ℓ(1 + 2A2 ρn )
5: if T ≥ T ∗ then
6:
return p
7: else
8:
return q
9: end if
P
(i) 2
The detection algorithm is motivated by the fact that v ⊤ Y v = N1 N
i=1 hv, y i . Under the planted
(i)
⊤
(i)
2
model Pn , y ∼ N (0, In + βxx ) and thus hv, y i ∼ N (0, ℓ + βhv, xi ) for any fixed v ∈ In,ℓ ;
as a result, if v correctly “guesses” ℓ entries of x with correct signs (up to a global flip), then the
contribution of hv, xi2 to the variance of hv, y (i) i will cause v ⊤ Y v to be large.
Remark 2.5 (Runtime). The runtime of Algorithm 1 is dominated by exhaustive search over In,ℓ
during Step 3, when we compute T . Since |In,ℓ | = nℓ 2ℓ ≤ (2n)ℓ , the runtime is nO(ℓ) . If ℓ = ⌈nδ ⌉
δ
for a constant δ > 0, then the runtime is nO(n ) = exp(nδ+o(1) ).

9

Theorem 2.6 (Wishart detection). Consider the spiked Wishart model with a (ρ, A)-sparse signal
x, and let γ = n/N . Let {y (i) }N
i=1 be drawn from either Pn or Qn , and let fn be the output of
Algorithm 1. Suppose


β
1
β
.
(6)
ρ ≤ min 1, 2
2
A
25A γ log n
Let ℓ be any integer in the interval




β A2
25A4 γ 2
ρn ,
ρ n log n, min 1, 2
ℓ∈
β2
A
β

(7)

which is nonempty due to (6). Then, the total failure probability of Algorithm 1 satisfies


β 2 ℓ2
Pn [fn = q] + Qn [fn = p] ≤ 2 exp −
≤ 2n−25ℓ/48 ,
48A4 γ ρ2 n
where the last inequality follows from (7).
Remark 2.7. Since the runtime is nO(ℓ), for the best possible runtime we should choose ℓ as small
as possible, i.e.,


25A4 γ 2
ρ n log n .
ℓ=
β2
Remark 2.8. We are primarily interested in the regime n → ∞ with γ = Θ(1), A = Θ(1),
ρ = n−τ for a constant τ ∈ (0, 1), and either β = n−α for a constant α > 0, or β = Θ(1) with
√
λ̂ := β/ γ < 1 (in which case α := 0). In this case, the requirement (6) reads ρ ≤ Ω(λ̂2 / log n)
(or, in other words, τ > 2α), which is information-theoretically necessary up to log factors [PJ12,
−2 2
VL12, CMW13]. Choosing ℓ as in Remark 2.7 yields an algorithm of runtime nO(1+λ̂ ρ n log n) =
poly(n) + exp(n2α−2τ +1+o(1) ).
Remark 2.9. For S ⊆ [n], let YS denote the corresponding principal submatrix of Y (i.e., restrict
to the rows and columns whose indices lie in S). An alternative detection algorithm would be to
threshold the test statistic
T ′ := max λmax (YS ),
S∈([n]
ℓ )
i.e., the largest eigenvalue of any ℓ × ℓ principal submatrix. One can obtain similar guarantees for
this algorithm as for Algorithm 1.
Recovery. We now turn to the problem of exactly recovering the support and signs of x, given
data drawn from Pn . The goal is to output a vector x̄ ∈ {−1, 0, 1}n such that sign(x̄) = ±sign(x)
where sign(x)i = sign(xi ) and

 1 if xi > 0,
−1 if xi < 0,
sign(xi ) =

0 if xi = 0.

Note that we can only hope to recover sign(x) up to a global sign flip, because xx⊤ = (−x)(−x)⊤ .

10

Algorithm 2: Recovery of supp(x) and sign(x) in the spiked Wishart model
Input: Data {y (i) }1≤i≤N , parameters ρ ∈ (0, 1], β ≥ 0, A ≥ 1, ℓ ∈ N+
1: N̄ ← ⌊N/2⌋
PN̄ (i) (i) ⊤ ′′
,Y ←
2: Compute sample covariance matrices: Y ′ ← 1
i=1 y y
N̄
3: Specify the search set: In,ℓ ← {v ∈ {−1, 0, 1}n : kvk0 = ℓ}
4: Compute the initial estimate: v ∗ ← argmaxv∈In,ℓ v ⊤ Y ′ v
5: Compute the refined estimate: z ← (Y ′′ − I)v ∗
6: for j = 1 to n do
βℓ
7:
x̄j ← sign(zj ) · 1{|zj | > 2√3A
}
2 ρn
8: end for
Output: x̄

1
N̄

P2N̄

i=N̄ +1 y

(i) y (i) ⊤

For technical reasons, we divide our N samples into two subsamples of size N̄ = ⌊N/2⌋ (with
one sample discarded if N is odd) and produce two independent sample covariance matrices Y ′ and
Y ′′ . The first step of the algorithm is similar to the detection algorithm: by exhaustive search, we
find the vector v ∗ ∈ In,ℓ maximizing v ⊤ Y ′ v. In the course of proving that the algorithm succeeds,
we will show that v ∗ has nontrivial correlation with x. The second step is to recover the support
(and signs) of x by thresholding z = (Y ′′ − I)v ∗ . Note that z discards (i.e., does not depend on)
the columns of Y ′′ that do not lie in supp(v ∗ ); since supp(v ∗ ) has substantial overlap with supp(x),
this serves to amplify the signal.
Theorem 2.10 (Wishart support and sign recovery). Consider the planted spiked Wishart model
Pn with an arbitrary (ρ, A)-sparse signal x, and let γ = n/N . Suppose
 4

A β 1
β
.
(8)
ρ ≤ min 1,
8
25A
400γ log n
Let ℓ be any integer in the interval




25A8
β
10000A4 γ 2
ρn ,
ρ n log n, min 1,
ℓ∈
β2
25A8
β

(9)

which is nonempty due to (8). Then the failure probability of Algorithm 2 satisfies


ℓ
β2
≤ 6n−3/2 ,
1 − Pn [sign(x̄) = ±sign(x)] ≤ 6 exp −
6400A4 γ ρ2 n
where the last inequality follows from (9).
Remark 2.11. As for detection, the runtime of Algorithm 2 is nO(ℓ) , and we can minimize this by
choosing


10000A4 γ 2
ℓ=
ρ
n
log
n
.
β2
Once we obtain supp(x) using Algorithm 2, it is straightforward to estimate x (up to global
sign flip) using the leading eigenvector of the appropriate submatrix. This step of the algorithm
requires only polynomial time.

11

Theorem 2.12 (Wishart recovery). Consider the planted spiked Wishart model Pn with an arbitrary (ρ, A)-sparse signal x, and let γ = n/N . Suppose we have access (e.g., via Algorithm 2) to
PN (i) (i) ⊤
P
(i)
(i) and Y = P Y P ⊤ = 1
.
I = supp(x) ⊂ [n]. Write PI = i∈I ei e⊤
I
I
i , yI = PI y
i=1 yI yI
I
N
Let x̃ denote the unit-norm eigenvector corresponding to the maximum
eigenvalue
of
Y
.
Then,
I
√
2(1+β) γρ
,
1),
there exists an absolute constant C > 0 such that, for any ǫ ∈ [
Cβ




C 2 β 2 nǫ2
Pn hx̃, xi2 ≤ 1 − ǫ ≤ 2 exp −
≤ 2 exp(−n).
4(1 + β)2 γρ

Remark 2.13. In the regime we are interested in, n → ∞ with A = O(1), β = O(1), and (8) is
satisfied. In this case, the conclusion of Theorem 2.12 gives hx̃, xi2 > 1 − o(1) with high probability.

Low-degree likelihood. Now, we turn to controlling the low-degree likelihood ratio (LDLR) (see
Section 1.5) to provide rigorous evidence that the above algorithms are optimal. In this section
we take a fully Bayesian approach, and assume that the planted signal x is drawn from the sparse
√
Rademacher prior Xnρ . Recall that the signal-to-noise ratio is defined as λ̂ := β/ γ.
As discussed in Section 1.5, we will determine the behavior of kL≤D
n k in the limit n → ∞:
Ω̃(D)
≤D
-time algorithms. We allow the parameters
if kLn k = O(1), this suggests hardness for n
D, ρ, β, γ to depend on n, which we sometimes emphasize by writing, e.g., ρn . For Dn = o(n),
p
our results suggest hardness for nΩ̃(D) -time algorithms whenever λ̂ < 1 and ρ ≫ λ̂ Dn /n. This
is essentially tight, matchingpPCA (which succeeds when λ̂ > 1) and our algorithm with ℓ = Dn
(which succeeds when ρ ≪ λ̂ Dn /n) (however, see Remark 2.16 below for one caveat).
Theorem 2.14 (Boundedness of LDLR for large ρ). Under the spiked Wishart model with spike
prior X = Xnρ , suppose Dn = o(n). If one of the following holds for sufficiently large n:
(a) lim supn→∞ λ̂n < 1 and
ρn ≥ max 1,

s

1
6 log(1/λ̂n )

√
(b) lim supn→∞ λ̂n < 1/ 3 and
ρn ≥ λ̂n

r

!r

Dn
, or
n

Dn
,
n

(10)

(11)

then, as n → ∞, kL≤D
n,N,β,X k = O(1).
The following result on divergence of the LDLR serves as a sanity check: we show that kL≤D
n k
Ω̃(D)
indeed diverges in the regime where we know that a n
-time algorithm exists.
Theorem 2.15 (Divergence of LDLR for small ρ). Under the spiked Wishart model with spike
prior X = Xnρ , suppose Dn = ω(1) and Dn = o(n). If one of the following holds:
(a) lim inf n→∞ λ̂n > 1, or
√
(b) lim supn→∞ λ̂n < 1, | log λ̂n | = o( Dn ) and for sufficiently large n,
r
Dn
2
ρn < C λ̂n log (1/λ̂n )
n
where C is an absolute constant,
12

then, as n → ∞, kL≤D
n,N,β,X k = ω(1).
Remark 2.16. There is one regime where the above results give some unexpected behavior. Recall
first that optimal Bayesian inference for sparse PCA can be performed in time nO(ρn) by computing
the likelihood ratio. Thus if kL≤D
n k = O(1) for some Dn ≫ ρn, this suggests that the problem is
information-theoretically impossible; from our results above, there are regimes where this occurs (and
indeed the problem is information-theoretically impossible), yet kL≤D
n k = ω(1) for some larger Dn
(which incorrectly suggests that there should be an algorithm). This is analogous to a phenomenon
where the second moment of the (non-low-degree) likelihood ratio kLn k can sometimes diverge even
when strong detection is impossible (see, e.g. [BMNN16, BMV+ 18, PWBM18b]). Luckily, this issue
never occurs for us in the regime of interest Dn ≪ ρn, and therefore does not prevent our results
from being tight. Note also that none of these observations contradict Conjecture 1.5.

2.2

The Wigner Model

We now state our algorithms and results for the Wigner model. These are very similar to the
Wishart case, so we omit some of the discussion.
Definition 2.17 (Spiked Wigner model). The spiked Wigner model with parameters n ∈ N+ ,
λ ≥ 0, and planted signal x ∈ Rn is defined as follows.
• Under Pn = Pn,λ , we observe the matrix Y = W + λxx⊤ , where W ∼ GOE(n).
• Under Qn , we observe the matrix Y ∼ GOE(n).
Algorithm 3: Detection in the spiked Wigner model
Input: Data Y , parameters ρ ∈ (0, 1], λ > 0, A ≥ 1, ℓ ∈ N+
1: Specify the search set: In,ℓ ← {v ∈ {−1, 0, 1}n : kvk0 = ℓ}
2: Compute the test statistic: T ← maxv∈In,ℓ v ⊤ Y v
λℓ2
3: Compute the threshold: T ∗ ← 2A
2 ρn
4: if T ≥ T ∗ then
5:
return p
6: else
7:
return q
8: end if
Remark 2.18 (Runtime). As in the Wishart case (see Remark 2.5), the runtime is nO(ℓ) . The
same holds for Algorithm 4 below.
Theorem 2.19 (Wigner detection). Consider the spiked Wigner model with an arbitrary (ρ, A)sparse signal x. Let Y be drawn from either Pn or Qn , and let fn be the output of Algorithm 3.
Suppose
1
λ2
.
(12)
ρ≤
36A4 log n
Let ℓ be any integer in the interval
ℓ∈




36A4 2
ρ
n
log
n,
ρn
,
λ2
13

(13)

which in nonempty due to (12). Then the total failure probability of Algorithm 3 satisfies


λ2 ℓ2
≤ 2n−9ℓ/8 ,
Pn [fn = q] + Qn [fn = p] ≤ 2 exp −
32A4 ρ2 n
where the last inequality follows from (13).
Algorithm 4: Recovery of supp(x) and sign(x) in the spiked Wigner model
Input: Data Y , parameters ρ ∈ (0, 1], λ > 0, A ≥ 1, ℓ ∈ N+
1: Sample W̃ ∼ GOE(n)
√
√
2: Compute independent data matrices: Y ′ ← (Y + W̃ )/ 2 and Y ′′ ← (Y − W̃ )/ 2
3: Specify the search set: In,ℓ ← {v ∈ {−1, 0, 1}n : kvk0 = ℓ}
4: Compute the initial estimate: v ∗ ← argmaxv∈In,ℓ v ⊤ Y ′ v
5: Compute the refined estimate: z ← Y ′′ v ∗
6: for j = 1 to n do
7:
x̄j = sign(zj ) · 1{|zj | > 4Aλℓ
2 ρn }
8: end for
Output: x̄
For technical reasons, our first step is to fictitiously “split” the data into two independent copies
Y ′ and Y ′′ . Note that
W + W̃
λ
Y ′ = √ xx⊤ + √
2
2

λ
W − W̃
and Y ′′ = √ xx⊤ + √
.
2
2

Since W ′ := W√+2W̃ and W ′′ := W√−2W̃ are independent GOE(n) matrices, Y ′ and Y ′′ are distributed
as independent observations
√ drawn from Pn with the same planted signal x and with effective
signal-to-noise ratio λ̄ = λ/ 2.
Theorem 2.20 (Wigner support and sign recovery). Consider the planted spiked Wishart model
Pn with an arbitrary (ρ, A)-sparse signal x. Suppose
ρ≤

1
λ2
.
4
338A log n

(14)

Let ℓ be any integer in the interval



338A4 2
ℓ∈
ρ n log n, ρn ,
λ2

(15)

which is nonempty due to (14). Then the failure probability of Algorithm 4 satisfies


ℓ
λ2
≤ 4n−169/144 ,
1 − Pn [supp(x̄) = supp(x), sign(x̄) = ±sign(x)) ≤ 4 exp −
288A4 ρ2 n
where the last inequality follows from (15).
As in the Wishart case, once we have recovered the support, there is a standard polynomial-time
spectral method to estimate x.

14

Theorem 2.21 (Wigner recovery). Consider the planted spiked Wigner model Pn with an arbitrary
(ρ, A)-sparse
signal x. Suppose we have access (e.g., via Algorithm 4) to I = supp(x) ⊂ [n]. Write
P
⊤
PI = i∈I ei e⊤
the unit-norm eigenvector corresponding to the
i and YI = PI Y PI . Let x̃ denote
√
maximum eigenvalue of YI . Then for any ǫ ∈ ( 4
Pn



2ρ
λ , 1),




p 2
n 
λǫ − 4 2ρ
hx̃, xi ≤ 1 − ǫ ≤ 4 exp −
.
16
2



√
Remark 2.22. In the regime we are interested in, n → ∞ with (14) satisfied, so that ρ/λ → 0.
In this case, the conclusion√ of Theorem 2.21 gives hx̃, xi2 > 1 − o(1) with high probability, upon
choosing for example ǫ = 8 λ2ρ .
We also have the following results on the behavior of the low-degree likelihood ratio.
Theorem 2.23 (Boundedness of LDLR for large ρ). Under the spiked Wigner model with prior
X = Xnρ , suppose Dn = o(n). If one of the following holds for sufficiently large n:
(a) lim supn→∞ λn < 1 and
ρn ≥ max 1,

s

1
6 log(1/λn )

√
(b) lim supn→∞ λn < 1/ 3 and
ρ n ≥ λn

r

!r

Dn
, or
n

Dn
,
n

(16)

(17)

then, as n → ∞, kL≤D
n,λ,X k = O(1).
Theorem 2.24 (Divergence of LDLR for small ρ). Under the spiked Wigner model with prior
X = Xnρ , suppose Dn = ω(1) and Dn = o(n). If one of the following holds:
(a) lim inf n→∞ λn > 1, or
√
(b) lim supn→∞ λn < 1, | log λn | = o( Dn ) and for sufficiently large n,
r
Dn
2
ρn < Cλn log (1/λn )
n
where C is an absolute constant,
then, as n → ∞, kL≤D
n,λ,X k = ω(1).

3
3.1

Proofs for Subexponential-Time Algorithms
The Wishart Model

Proof of Theorem 2.6 (Detection). Under Qn , for any fixed v ∈ In,ℓ we have v ⊤ y (i) ∼ N (0, ℓ) for
i ∈ [N ] and
N
1 X ⊤ (i) 2 (d) ℓ 2
(v y ) =
χ ,
v⊤ Y v =
N
N N
i=1

15

where χ2N is a chi-squared random variable with N degrees of freedom, i.e., the sum of the squares
of N standard gaussians. Using Corollary A.3, we union bound over v ∈ In,ℓ for any t ∈ (0, 12 ):


Qn [T ≥ ℓ(1 + t)] ≤ |In,ℓ | Pr χ2N ≥ N (1 + t)

 

n ℓ
N t2
≤
2 · exp −
3
ℓ


N t2
≤ exp ℓ log(2n) −
.
3
Under the condition

N t2 ≥ 4ℓ log(2n)

we have

(18)




1
2
Qn [T ≥ ℓ(1 + t)] ≤ exp − N t .
12
Meanwhile, under Pn , when v = v̄ correctly guesses ℓ entries and their signs in the support of x
(which requires ℓ ≤ ρn), for any i ∈ [N ] we have
v̄ ⊤ y (i) ∼ N (0, v̄ ⊤ (In + βxx⊤ )v̄) = N (0, ℓ + βhv̄, xi2 ).

Therefore,
v̄ ⊤ Y v̄ =

N
1 X ⊤ (i) 2 (d) 1
(v̄ y ) =
(ℓ + βhv̄, xi2 )χ2N
N
N
i=1

where hv̄, xi2 ≥

ℓ2
A2 ρn

. As a result, by Corollary A.3,
h
i
Pn [T < ℓ(1 + t)] ≤ Pn v̄ ⊤ Y v̄ < ℓ(1 + t)


1
2 2
(ℓ + βhv̄, xi )χN < ℓ(1 + t)
= Pr
N



βℓ − A2 ρnt
2
= Pr χN < N 1 −
βℓ + A2 ρn

2 !
N βℓ − A2 ρnt
,
≤ exp −
3
βℓ + A2 ρn

the last inequality requiring

1
βℓ − A2 ρnt
≤ .
βℓ + A2 ρn
2
1
To satisfy t ∈ (0, 2 ), (18) and (19) at the same time, we choose
0≤

t=

(19)

βℓ
.
2A2 ρn

Under the condition
β
βℓ
≤ρ≤
√
2
A n
5A2 γ

s

ℓ
,
n log n

(20)

which is equivalent to the interval for ℓ given in (7), thresholding the statistic T at ℓ(1 + t) succeeds
at distinguishing Pn and Qn with total error probability


β 2 ℓ2
Qn [T ≥ ℓ(1 + t)] + Pn [T < ℓ(1 + t)] ≤ 2 exp −
,
48A4 γ ρ2 n

which completes the proof.

16

Proof of Theorem 2.10 (Support and Sign Recovery). First, we give a high-probability lower bound
on hv ∗ , xi. From the analysis of the detection algorithm, we know that under the condition (20),


β 2 ℓ2
1 − 2 exp −
48A4 γ ρ2 n
 


βℓ
∗⊤ ′ ∗
≤ Pn ℓ 1 +
≤v Y v
2A2 ρn


 
ℓ2
1
βℓ
∗
2 2
∗
2
≤ Pn ℓ 1 +
≤ (ℓ + βhv , xi )χN̄ , hv , xi <
2A2 ρn
3A2 ρn
N̄
 


βℓ
ℓ2
1
∗
2 2
∗
2
+ Pn ℓ 1 +
(ℓ
+
βhv
,
xi
)χ
,
hv
,
xi
≥
≤
N̄
2A2 ρn
3A2 ρn
N̄






 
βℓ2
1
ℓ2
βℓ
2
∗
2
ℓ+
≤
χN̄ + Pn hv , xi ≥
,
≤ Pn ℓ 1 +
2A2 ρn
3A2 ρn
3A2 ρn
N̄
where

 
Pn ℓ 1 +

βℓ
2A2 ρn



1
≤
N̄



βℓ2
ℓ+
3A2 ρn



χ2N̄







βℓ
= Pn
≥ N̄ 1 +
2βℓ + 6A2 ρn

2 !
βℓ
N̄
≤ exp −
3 2βℓ + 6A2 ρn


β2
ℓ2
≤ exp −
,
384A4 γ ρ2 n
χ2N̄



hence we have the lower bound






ℓ2
β 2 ℓ2
β2
ℓ2
∗
2
≥ 1 − 2 exp −
− exp −
Pn hv , xi ≥
3A2 ρn
48A4 γ ρ2 n
384A4 γ ρ2 n


β2
ℓ2
≥ 1 − 3 exp −
.
384A4 γ ρ2 n
We now fix v ∗ satisfying the above lower bound on hv ∗ , xi2 . From this point onward, we will
only use the second copy Y ′′ of our data; note that, crucially, Y ′′ is independent from v ∗ . To
simplify the notation, we will write y (1) , . . . , y (N̄ ) instead of y (N̄ +1) , . . . , y (2N̄ ) for the samples
√ used
to form Y ′′ . We now adopt an equivalent representation of the observations: y (i) = u(i) + βw(i) x,
where u(i) ∼ N (0, In ) and w(i) ∼ N (0, 1) are independent random gaussian vectors and scalars,
respectively. Substituting this into z = (Y ′′ − I)v ∗ yields
N̄
1 X
(aij + bij + cij + dij + eij )
zj =
N̄ i=1

where, for i ∈ [N̄ ] and j ∈ [n],

aij = (w(i) )2 βxj hv ∗ , xi
(i)

bij = ((uj )2 − 1)vj∗
X (i) (i)
cij =
uj uk vk∗
k6=j

(i)

dij = uj w(i)

p

βhv ∗ , xi
p
eij = hu(i) , v ∗ iw(i) βxj ,
17

with E(aij ) = βxj hv ∗ , xi and E(bij ) = E(cij ) = E(dij ) = E(eij ) = 0. We will show separate union
bounds for these five contributions to zj . In the following, we fix the constant µ = 1/20.
Union bound for aij .
√

For all j ∈ supp(x),

ℓ
A2 βℓ
1
βℓ
A Aℓ
≤β √ √ √
,
≤ |βxj hv ∗ , xi| ≤ β √ √ =
A ρn 3A ρn
ρn ρn
ρn
3A2 ρn

so by Corollary A.3,




N̄
X
1
βℓ 
βℓ
1 2
1
∗

log Pn
aij − βxj hv , xi > µ 2
χ −1 >µ 2 ·
= log Pr
A ρn
A ρn |βxj hv ∗ , xi|
N̄ i=1
N̄ N̄


1 2
µ
χ −1 > 4
≤ log Pr
A
N̄ N̄
2
µ N̄
≤− 8.
3A
Therefore, we may union bound over j ∈ supp(x):


N̄
X
1
βℓ
aij − βxj hv ∗ , xi ≤ µ 2 , for all j ∈ [n]
Pn 
A ρn
N̄ i=1


N̄
X
X
βℓ
1
aij − βxj hv ∗ , xi > µ 2 
≥1−
Pn 
A ρn
N̄ i=1
j∈supp(x)


µ2 N̄
≥ 1 − exp log n −
3A8


µ2 n
≥ 1 − exp − 8
.
7A γ

(21)

We have bij nonzero only when j ∈ supp(v ∗ ). For such j, by Corollary A.3,




N̄
X
βℓ 
βℓ
1 2
1

bij > µ 2
χ −1 >µ 2
log Pn
= log Pr
A ρn
A ρn
N̄ i=1
N̄ N̄


N̄
βℓ 2
≤−
.
µ 2
3
A ρn

Union bound for bij .

Therefore,



N̄
X
1
βℓ
Pn 
bij > µ 2 
A
ρn
N̄
i=1
j∈supp(v∗ )

 !
N̄
βℓ 2
≥ 1 − exp log ℓ −
µ 2
3
A ρn


µ 2 β 2 ℓ2
,
≥ 1 − exp −
12A4 γ ρ2 n


N̄
X
βℓ
1
Pn 
bij ≤ µ 2 , for all j ∈ [n] ≥ 1 −
A ρn
N̄ i=1

18

X



(22)

under the condition
µβ
1
ρ≤ √
√
A
12 2 γ

s

ℓ
.
n log ℓ

Union bound for cij and dij . In the following, let u, u′ denote independent samples from
N (0, IN̄ ). Note that
X (i)
(i)
ũj :=
uk vk∗ ∼ N (0, ℓ˜j ), where ℓ̃j = ℓ − 1{j ∈ supp(v ∗ )},
k6=j

(i)

(i)

and ũj is independent from uj . Therefore,
N̄
X

cij =

N̄
X

(i) (i) (d)
uj ũj =

i=1

i=1

q

ℓ̃j hu, u′ i.

Therefore, by Lemma A.1, for the cij we have


"
√ #
N̄
n
X
X
βℓ
β
ℓN̄
1
′
cij ≤ µ 2 , for all j ∈ [n] ≥ 1 −
Pr |hu, u i| > µ 2
Pn 
A ρn
A ρn
N̄ i=1
j=1

√ !2 
1
ℓN̄ 
β
≥ 1 − 2n exp −
µ 2
A ρn
4N̄


µ2 β 2 ℓ
≥ 1 − exp −
.
16A4 γ ρ2 n

(23)

The last inequality holds under the condition
√
1 µ2 β 2 ℓ
β ℓ
≤ ,
≥ log(2n),
µ 2
A ρn
2 16A4 γ ρ2 n
which follows from

s
√
2µβ ℓ
µβ
ℓ
≤ρ≤
.
√
2
2
A n
5A γ n log n

Meanwhile,
N̄
X
i=1

(d)

dij =

p

βhv ∗ , xihu, u′ i.

Therefore, as for the cij , for the dij we have


√


n
N̄
X
X
β N̄
βℓ
1
′
Pr |hu, u i| > µ 3 √
dij ≤ µ 2 , for all j ∈ [n] ≥ 1 −
Pn 
A ρn
A ρn
N̄ i=1
j=1
 √
2 !
β N̄
1
µ 3√
≥ 1 − 2n exp −
A ρn
4N̄


µ2 β 1
.
≥ 1 − exp −
16A6 γ ρ
19

(24)

The last inequality holds under the condition
√
µ2 β 1
β
1
≥ log(2n),
µ 3√ ≤ ,
A ρn
2 16A6 γ ρ
which follows from

Union bound for eij .

4µ2 β
µ2 β
≤
ρ
≤
.
A6 n
17A6 γ log n
We have
N̄
X
i=1

(d)

eij = xj

p

βℓhu, u′ i,

which is only nonzero for j ∈ supp(x). Therefore,


N̄
X
βℓ
1
eij ≤ µ 2 , for all j ∈ [n] ≥ 1 −
Pn 
A ρn
N̄ i=1

X



√

βℓN̄
Pr |hu, u i| > µ 2
A ρn|xj |
j∈supp(x)
√


X
βℓN̄
′
≥1−
Pr |hu, u i| > µ 4 √
A ρn
j∈supp(x)
 √
2 !
1
βℓN̄
µ 4√
≥ 1 − 2n exp −
A ρn
4N̄


µ2 β ℓ
≥ 1 − exp −
.
16A8 γ ρ
′



(25)

The last inequality holds under the condition
√
1
µ2 β ℓ
βℓ
≥ log(2n),
µ 4√ ≤ ,
A ρn
2 16A8 γ ρ
which follows from

4µ2 βℓ
µ2 βℓ
≤
ρ
≤
.
A8 n
17A8 γ log n

Final steps. Now, combining all of the union bounds and conditions from (21), (22), (23), (24)
and (25), assuming that β, γ = Θ(1) and that ω(1) ≤ ℓ(n) ≤ o(n/ log n), under the condition
s


ℓ
β
ℓ
β
≤ρ≤
(26)
max 1,
√
8
2
25A
n
100A γ n log n

20

which is equivalent to the regime for ℓ given in (9) that we are considering, we have




βℓ
ℓ2
∗
∗
2
Pn for some j, |zj − βxj hv , xi| >
+ Pn hv , xi ≤
4A2 ρn
3A2 ρn


N̄
X
βℓ
1
aij − βxj hv ∗ , xi > µ 2 
≤ Pn for some j,
A ρn
N̄ i=1


N̄
X
βℓ
1
bij > µ 2 
+ Pn for some j,
A ρn
N̄ i=1


N̄
X
1
βℓ
+ Pn for some j,
cij > µ 2 
A ρn
N̄ i=1


N̄
X
βℓ
1
dij > µ 2 
+ Pn for some j,
A ρn
N̄ i=1


N̄
X
βℓ
1
eij > µ 2 
+ Pn for some j,
A ρn
N̄ i=1


ℓ2
∗
2
+ Pn hv , xi ≤
3A2 ρn




ℓ
ℓ2
β2
β2
+ exp −
≤ 5 exp −
6400A4 γ ρ2 n
384A4 γ ρ2 n


2
β
ℓ
≤ 6 exp −
.
4
2
6400A γ ρ n
√ βℓ
3A2 ρn

for j ∈ supp(x) and |βxj hv ∗ , xi| = 0 for j ∈
/ supp(x), we conclude that,


β2
with probability at least 1 − 6 exp − 6400A4 γ ρ2ℓn , for every j ∈ [n],
Since |βxj hv ∗ , xi| ≥

j ∈ supp(x)

if and only if

βℓ
|zj | ≥ √
,
2 3A2 ρn

and
completing the proof.

sign(zj ) = sign(xj hv ∗ , xi),

Proof of Theorem 2.12 (Full Recovery). By a result in the analysis of covariance matrix estimation
for subgaussian distributions ([Ver10], Remark 5.51), there exists an absolute constant
0such
 C2 >
√
C δ2 N
that, for any δ ∈ [ ργ/C, 1), the following holds with probability at least 1 − 2 exp − ρ
:
kYI − (PI + βxx⊤ )k ≤ δkPI + βxx⊤ k = δ(1 + β).

Whenever this is true, by the definition of spectral norm we have
h
i
δ(1 + β) ≥ x̃⊤ YI − (PI + βxx⊤ ) x̃
= kYI k − (1 + βhx̃, xi2 )

≥ (1 − δ)(1 + β) − (1 + βhx̃, xi2 ),
21

which is equivalent to hx̃, xi2 ≥ 1 − ǫ upon taking δ =

βǫ
2(1+β) .

Thus, for any ǫ ∈ (

√
2(1+β) ργ
, 1),
Cβ



C 2 β 2 nǫ2
,
Pr hx̃, xi ≤ 1 − ǫ ≤ 2 exp −
4(1 + β)2 γρ
which completes the proof.

3.2





2

The Wigner Model
2

Proof of Theorem 2.19 (Detection). For simplicity we denote t = 2Aλℓ2 ρn . Under Pn , when v̄ correctly guesses ℓ entries in the support of x with correct signs (which requires ℓ ≤ ρn),
v̄ ⊤ Y v̄ = v̄ ⊤ W v̄ + λhv̄, xi2 ,
where v̄ ⊤ W v̄ ∼ N (0, ℓ2 /n). Note that
λhv̄, xi2 ≥

λℓ2
= 2t.
A2 ρn

Therefore, a standard Gaussian tail bound gives
h
i
Pn [T < t] ≤ Pn v̄ ⊤ Y v̄ < t


≤ Pr N (0, ℓ2 /n) > t

2 !
n
λℓ2
≤ exp − 2
2ℓ
2A2 ρn


λ2 ℓ2
.
= exp − 4 2
8A ρ n
Under Qn , for each fixed v ∈ In,ℓ, we have
v ⊤ Y v ∼ N (0, 2ℓ2 /n).
By the same tail bound,
Qn

h

Now, by a union bound over v ∈ In,ℓ ,



i

nt2
v Y v ≥ t ≤ exp − 2
4ℓ
⊤



.


nt2
Qn [T ≥ t] ≤ |In,ℓ | exp − 2
4ℓ
 


n ℓ
nt2
=
2 exp − 2
ℓ
4ℓ


nt2
≤ exp ℓ log(2n) − 2 .
4ℓ


Under the condition
nt2
8ℓ2

≥ ℓ log(2n) ⇐ ρ <

22

λ
6A2

s

ℓ
,
n log n

which is equivalent to the interval for ℓ given in (13), we have




λ2 ℓ 2
nt2
.
Qn [T ≥ t] ≤ exp − 2 = exp −
8ℓ
32A4 ρ2 n
Therefore, by thresholding T at t, under the condition
s
ℓ
λ
ℓ
≤ρ≤
,
2
n
6A
n log n

(27)

we can distinguish Pn and Qn with total failure probability at most






λ2 ℓ2
λ2 ℓ2
λ2 ℓ2
Pn [T < t] + Qn [T ≥ t] ≤ exp − 4 2
+ exp −
≤ 2 exp −
,
8A ρ n
32A4 ρ2 n
32A4 ρ2 n
completing the proof.
Proof of Theorem 2.20 (Support and Sign Recovery). First, we show that v ∗ has significant overlap
with the support of x. From the analysis
 of the detection algorithm, provided (27) holds, with
λ̄2 ℓ2
probability at least 1 − 2 exp − 32A4 ρ2 n we have
λ̄ℓ2
≤ v ∗ ⊤ Y ′ v ∗ = λ̄hv ∗ , xi2 + v ∗ ⊤ W ′ v ∗ .
2A2 ρn

where v ∗ ⊤ W ′ v ∗ ∼ N (0, 2ℓ2 /n). Therefore, for n sufficiently large,
 

 



λ̄2 ℓ2
λ̄ℓ2
ℓ2
2
∗
2
≥ 1 − 2 exp −
1 − Pr N (0, 2ℓ /n) ≥
Pn hv , xi ≥
4A2 ρn
32A4 ρ2 n
4A2 ρn




λ̄2 ℓ2
λ̄2 ℓ2
≥ 1 − 2 exp −
−
exp
−
32A4 ρ2 n
64A4 ρ2 n


λ̄2 ℓ2
≥ 1 − 3 exp −
.
64A4 ρ2 n
We now fix v ∗ satisfying the above lower bound on hv ∗ , xi2 . From this point onward, we will only
use the second copy Y ′′ of our data; it is important here that Y ′′ is independent from v ∗ . We will
that x is successfully recovered by thresholding the entries of z = Y ′′ v ∗ . Entrywise, we have
′′ ∗
zi = λ̄xi hv ∗ , xi + e⊤
i W v .

For all i ∈ supp(x),

ℓ
λ̄ℓ
1
=
.
|λ̄xi hv ∗ , xi| ≥ λ̄ √ ·
√
A ρn 2A ρn
2A2 ρn

For simplicity we denote s =
N (0, ℓ/n) and therefore

λ̄ℓ
2A2 ρn

′′ ∗
2
and µ = 13 . Note that for all i ∈ [n], e⊤
i W v ∼ N (0, kvk /n) =



h
i
nµ2 s2
′′ ∗
.
W
v
|
≥
µs
≤
2
exp
−
Pn |e⊤
i
2ℓ

23

(28)

By a union bound over all i ∈ [n],


h
i
nµ2 s2
⊤
′′ ∗
Pn |ei W v | ≤ µs for all i ≥ 1 − 2n exp −
2ℓ


nµ2 s2
≥ 1 − exp log(2n) −
2ℓ


2
2
nµ s
≥ 1 − exp −
4ℓ


ℓ
λ̄2
= 1 − exp −
144A4 ρ2 n

under the condition

nµ2 s2
λ̄
≥ log(2n) ⇐ ρ ≤
4ℓ
13A2

s

λ
ℓ
= √
n log n
13 2A2

s

ℓ
,
n log n

which, combined with (27), is equivalent membership in the interval for ℓ that we are considering
per (15). Therefore, with probability at least






λ̄2 ℓ2
λ̄2
ℓ
λ2
ℓ
1 − 3 exp −
− exp −
≥ 1 − 4 exp −
64A4 ρ2 n
144A4 ρ2 n
288A4 ρ2 n
for all j ∈ [n],

j ∈ supp(x)

if and only if

|zj | ≥

s
2

and
sign(zj ) = sign(xj hv ∗ , xi).

Thus, we find that thresholding the entries of z at s/2 successfully recovers the support and signs
of x, completing the proof.
Proof of Theorem 2.21 (Full Recovery). Since YI x̃ = λmax (YI )x̃, we must have supp(x̃) ⊂ I. Denote WI = PI W PI⊤ and W̄I the ℓ × ℓ submatrix of WI with rows and columns indexed by I (the
only nonzero rows and columns). Now, the variational description of the leading eigenvector yields
x̃⊤ WI x̃ + λhx̃, xi2 = x̃⊤ YI x̃ ≥ x⊤ YI x = x⊤ WI x + λ.
Therefore,
1
1 ⊤
(x̃ WI x̃ − x⊤ WI x) ≥ 1 − (λmax (W̄I ) + λmax (−W̄I )).
λ
λ
√
⊤
Note that W̄I has the same law as (Ḡ + Ḡ )/ 2n, where Ḡ is an ρn√× ρn matrix whose
entries are
√
√
4 2ρ
2nλǫ
independent standard normal random variables. Now, for any ǫ > λ , we have 4 > 2 ρn, a
standard singular value estimate for Gaussian matrices (see [Ver10], Corollary 5.35) gives




Pn hx̄, xi2 ≤ 1 − ǫ ≤ Pr λmax (W̄I ) + λmax (−W̄I ) ≥ λǫ


λǫ
≤ 2 Pr λmax (W̄I ) ≥
2
#
"
√
2nλǫ
≤ 2 Pr σmax (Ḡ) ≥
4



p 2
n
λǫ − 4 2ρ
,
≤ 4 exp −
16
hx̃, xi2 ≥ 1 −

which conclues the proof.

24

4
4.1

Proofs for Low-Degree Likelihood Ratio Bounds
Low-Degree Likelihood Ratio for Spiked Models

We begin by giving expressions for the norm of the low-degree likelihood ratio (LDLR) for the
spiked Wigner and Wishart models. These expressions are derived in [KWB19] and [BKW19],
respectively.
Lemma 4.1 (D-LDLR for spiked Wigner model [KWB19]). Let L≤D
n,λ,X denote the degree-D likelihood ratio for the spiked Wigner model with parameters n, λ and spike prior X . Then,
"D
#
n
d
X
1
2
kL≤D
λ2 hv (1) , v (2) i2
(29)
n,λ,X k = (1) E
d! 2
v ,v(2) ∼Xn
d=0

where v (1) , v (2) are drawn independently from Xn .

Lemma 4.2 (D-LDLR for spiked Wishart model [BKW19]). Let L≤D
n,N,β,X denote the degree-D
likelihood ratio for the spiked Wishart model with parameters n, N, β and spike prior X . Define
ϕN (x) := (1 − 4x)−N/2

ϕN,k (x) :=

k
X
d=0

X

xd

(30)
N 
Y

dP
1 ,...,dN i=1
di =d


2di
,
di

so that ϕN,k (x) is the Taylor series of ϕN around x = 0 truncated to degree k. Then,
!#
"
β 2 hv (1) , v (2) i2
≤D
2
ϕN,⌊D/2⌋
kLn,N,β,X k2 =
E
4
v(1) ,v(2) ∼Xn


!d


⌊D/2⌋
N
2 hv (1) , v (2) i2
X  X Y

2d
β
i 

=
,
E

di 
4
v(1) ,v(2) ∼Xn
d=0

(31)

(32)

dP
1 ,...,dN i=1
di =d

where v (1) , v (2) are drawn independently from Xn .

We consider a signal x drawn from the sparse Rademacher prior, Xn = Xnρ . The goal of this
section is to prove upper and lower bounds on the LDLR expressions in (29) and (32) as n → ∞,
for certain regimes of the parameters (λ, ρ for the Wigner model and β, γ, ρ for the Wishart model).
These bounds are obtained in several steps. First, we treat the moment terms
Ad := (nρ)2d

E

v(1) ,v(2) ∼Xnρ

hv (1) , v (2) i2d

(33)

from (29) and (32) in Section 4.2, with upper bounds given in Lemmas 4.4 and 4.5 and a lower
bound given in Lemma 4.6. We then give a precise estimate in Lemma 4.7 of the coefficient

N 
X Y
2di
di
dP
1 ,...,dN i=1
di =d

in the LDLR (32) of the Wishart model. Finally, by combining the above bounds, we show regimes
of parameters under which the LDLR either remains bounded or diverges as n → ∞. This yields
the proofs of Theorems 2.14 and 2.15 for the Wishart model, and Theorems 2.23 and 2.24 for the
Wigner model.
25

4.2

Introduction and Estimates of Ad

In this section, we carry out combinatorial estimates of the moments Ad defined in (33), which
appear in the LDLR expressions (29) and (32). We give upper bounds (Lemmas 4.4 and 4.5) and
a lower bound (Lemma 4.6) on these moments.
For independent v (1) ,P
v (2) drawn from the sparse Rademacher prior Xnρ , hv (1) , v (2) i has the same
distribution as Sn,ρ = n1 ni=1 si,ρ for i.i.d. si,ρ with

 +1/ρ with probability ρ2 /2,
si,ρ =
−1/ρ with probability ρ2 /2,
(34)

2
0
with probability 1 − ρ ,
and kth moment (for k > 0) given by

Esk =
i,ρ



0
ρ2−k

for k odd,
for k even.

(35)

Therefore, the moments of hv (1) , v (2) i have the combinatorial description
E

v(1) ,v(2) ∼Xnρ

2d
hv (1) , v (2) i2d = n−2d ESn,ρ

= n−2d

X

Esi1 ,ρ si2 ,ρ . . . si2d ,ρ

i1 ,...,i2d ∈[n]

=n

−2d

X

a1P
,...,an ≥0
ai =d

=n

−2d

X

a1P
,...,an ≥0
ai =d

=n

−2d

X

a1P
,...,an ≥0
ai =d





2a1

2d
· · · 2an



2a1

2d
· · · 2an

 Y

2a1

2d
· · · 2an

E

Y

2a

sj,ρj

aj >0

ρ2−2aj

aj >0



ρ2|{i:

ai >0}|−2d

.

(36)

Recall, from (33), that
Ad = (nρ)2d

E

v(1) ,v(2) ∼Xnρ

hv (1) , v (2) i2d =

X

a1P
,...,an ≥0
ai =d



2a1

2d
· · · 2an



ρ2|{i:

ai >0}|

.

Fix d ≤ Dn , and let 1 ≤ k ≤ d be the number of positive numbers among
P the {ai }. Suppose
d = wk + r, where 0 ≤ r < k. For positive integers b1 , b2 , . . . , bk such that
bi = d, we claim that

 

2d
2d
≤
=: M (k),
(37)
2b1 · · · 2bk
2w · · · 2w 2(w + 1) · · · 2(w + 1)
and that equality holds if and only if {bi } consists of r copies of (w + 1) and k − r copies of
w. This follows from the simple fact that, for any 1 ≤ i, j ≤ k such that bi ≥ bj + 2, we have
(2bi )!(2bj )! > (2(bi − 1))!(2(bj + 1))!, and therefore the left-hand side of the above inequality
becomes strictly larger as we “unbalance” the bi by replacing bi and bj with bi − 1 and bj + 1. As a
result, the left-hand side is maximized if and only if the maximum and minimum of {b1 , b2 , . . . , bk }
26

differ by at most 1. Now, since the total number of positive integer solutions to

d−1
k−1 , we have

d  
X
n d − 1 2k
Ad ≤
ρ M (k).
k
k−1

Pk

i=1 bi

= d is

(38)

k=1

Before proceeding to bounds on the Ad , we introduce the following result, which will be useful
in several estimates in this section.
Lemma 4.3. Suppose Dn = o(n). Then, for sufficiently large n,
1 −Dn2 /n
n(n − 1) · · · (n − Dn + 1)
>
e
.
D
n n
2

Proof. By Stirling’s formula, as n → ∞ and n − Dn → ∞ (which is ensured by Dn = o(n)),
√
2πn( ne )n
n(n − 1) · · · (n − Dn + 1)
n!
p
=
∼
n n−Dn Dn
n Dn
(n − Dn )!nDn
2π(n − Dn )( n−D
n
e )
n−Dn

1
Dn
∼ Dn 1 +
.
e
n − Dn
Here the relation ∼ means that the quotient of the quantities on either side tends to 1 as n → ∞.
Since Dn = o(n), for large enough n such that Dn < n3 , we have
"

n−Dn #


Dn
Dn
1
= −Dn + (n − Dn ) log 1 +
log Dn 1 +
e
n − Dn
n − Dn


Dn
Dn2
≥ −Dn + (n − Dn )
−
n − Dn 2(n − Dn )2
D2
≥ − n,
n
and the lemma follows.
Lemma 4.4 (First upper bound on Ad ). In the setting of the spiked Wishart or Wigner model
with sparse Rademacher prior Xnρ , suppose Dn = o(n). If for some µ > 0 we have
 r r
1
Dn
,
ρ ≥ max 1,
6µ
n
then for sufficiently large n and for any 1 ≤ d ≤ Dn , Ad defined by (33) satisfies
Ad ≤
where

d
X
k=1

2

µd+ dn

G(k) ≤ 2de

2

µd+ dn

G(d) = 2de

 
n (2d)! 2d
ρ ,
d 2d

 

n d − 1 2k
G(k) :=
ρ M (k).
k
k−1

27

(39)

Proof. Fix 1 ≤ d ≤ Dn . Recall that the first inequality in (39) is a restatement of (38). By a
simple comparison argument, we observe that M (k) is monotone increasing with respect to k. For
any 1 ≤ k < d2 , we have
n  d−1 2k+2
M (k + 1)
G(k + 1)
(n − k)(d − k) 2 (n − d)( d2 ) d
k+1
k ρ
=
ρ ≥
>1
≥


n d−1 2k
G(k)
k(k + 1)
k(k + 1) n
k k−1 ρ M (k)
q
if ρ ≥ Dnn . Therefore,
 X
X
d
G(k) ≤
G(k).
2
d
d
k≥ 2

k< 2

d
2

≤ k < d, by (37),

 

2d
2d
(2d)!
M (k) = max
=
= d−k 2k−d ,
b1P
,...,bk >0 2b1 · · · 2bk
24 2
2 ··· 2 4 ··· 4

Meanwhile, for

bi =d

since the maximum is attained when {bi } has (d − k) occurrences of 2 and (2k − d) occurrences of
1. As a result, with the help of Lemma 4.3,
n d−1 2k
ρ M (k)
G(k)

= k nk−1
2d
G(d)
d ρ M (d)
n d 
1
· k nk
≤
(6ρ2 )d−k
d
d!
d!
1
·
·
≤
(6ρ2 )d−k (n − k)(n − k − 1) . . . (n − d + 1)k! k!(d − k)!
 2
2
1
d!
2ed /n
1
≤
· d−k ·
2
d−k
(6ρ )
n
k!
(d − k)!
 2 d−k
1
2
d
·
.
≤ 2ed /n
2
6ρ n
(d − k)!
q
q
2
D
d
Thus for ρ ≥ 6µn
≥ 6µn
we have 6ρd2 n ≤ µd and
d
X
k=1

G(k) ≤ (⌊d/2⌋ + 1)

X

k≥ 2d

2 /n

G(k) ≤ 2ded

X (µd)d−k
2
G(d) ≤ 2deµd+d /n G(d),
(d
−
k)!
d

k≥ 2

completing the proof.
Lemma 4.5 (Second upper bound on Ad ). In the setting of the spiked Wishart
or Wigner model
√
with sparse Rademacher prior Xnρ , suppose Dn = o(n). If for some µ < 1/ 3 we have
r
Dn
ρ≥µ
,
n
then for sufficiently large n and for any 11 ≤ d ≤ Dn ,
 




d
X
√ d2 /n 11e d/2 −2d n (2d)! 2d
√ d2 /n 11e d/2 −2d
µ G(d) = de
µ
ρ ,
G(k) . de
Ad ≤
d 2d
30
30
k=1

where Ad is defined in (33) and G(k) is defined as in Lemma 4.4.
28

Proof. As in the proof of Lemma 4.4, for sufficiently large n and for any 1 ≤ d ≤ Dn and any
1 ≤ k < d2 , we have


(n − k)(d − k) 2 (n − k)(d − k)
d
G(k + 1)
≥
ρ ≥
µ2
≥ µ2 .
G(k)
k(k + 1)
k(k + 1)
n
Therefore,
X

k< 2d

G(k) ≤ (µ−2 + µ−4 + · · · + µ−2(⌈d/2⌉−1) )G(⌈d/2⌉) . µ−2⌈d/2⌉

Meanwhile, for

d
2

X

G(k).

k≥⌈d/2⌉

≤ k < d,

G(k)
2
≤ 2ed /n
G(d)



d2
6ρ2 n

d−k

1
2
·
≤ 2ed /n
(d − k)!



d
6µ2

d−k

·

1
.
(d − k)!

Summing these quantities, we find (the last inequality requiring d ≥ 11)
P
X  d k 1
k≥d/2 G(k)
d2 /n
≤ 2e
·
G(d)
6µ2
k!
k≤d/2

d2 /n

. de
.



√ d2 /n
de

√ 2
. ded /n

d
6µ2



⌊d/2⌋

·

ed

6µ2 ⌊d/2⌋

11e
30

1
⌊d/2⌋!
⌊d/2⌋

⌊d/2⌋

µ−2⌊d/2⌋ .

Here we have used the fact that (d/6µ2 )k /k! is monotone increasing for 1 ≤ k ≤ d2 , since d/6µ2 >
d/2. Combining the two cases, we conclude that
P
Pd


(1 + µ−2⌈d/2⌉ ) k≥d/2 G(k) √ d2 /n 11e d/2 −2d
k=1 G(k)
µ ,
.
. de
G(d)
G(d)
30
completing the proof.
Lemma 4.6 (Lower bound on Ad ). In the settings of the spiked Wishart or Wigner models with
sparse Rademacher prior Xnρ , consider a series d = dn = o(n) with integers w = wn satisfying
wn | dn . Then, as d/w → ∞, Ad defined by (33) satisfies
1− 1 
 1 #d
 
√ " 
w
w
w
d
n (2d)! w
ρ2d .
2
Ad ≥ (1 − o(1))
d
2
2
neρ
(2w)!
d

Proof. To obtain a lower bound on Ad , we only consider the contribution to the sum from terms
{ai } with wd -many occurrences of w and (n − wd )-many occurrences of zero:



X 
2d
2d
n
(2d)!
−2d
2|{i: ai >0}|−2d
ρ w −2d =: Tn,d (w).
ρ Ad =
ρ
≥
d/w
2a1 · · · 2an
d/w [(2w)!]
a1P
,...,an ≥0
ai =d

29

Now, we calculate the ratio
2d
Tn,d (w)
2d
d!(n − d)!
ρ w −2d
= d
d
d
Tn,d (1)
( w )!(n − w )! [(2w)!] w

#d
"
d!/( wd )!
2

.
=
1
1
(n − wd ) n − wd − 1 · · · (n − d + 1) [(2w)!] w ρ2(1− w )

(40)

By Stirling’s formula, for w fixed and d sufficiently large,

√
d
d
d!
= (1 + o(1)) w(d/e)d− w w w .
(d/w)!
Meanwhile,



d
n−
w




d
d
n − − 1 · · · (n − d + 1) ≤ nd− w .
w

Plugging into (40) we get
√
Tn,d (w)
≥ (1 − o(1)) w
Tn,d (1)

" 
1− 1 
 1 #d
w
w
d
w
2
,
neρ2
(2w)!

completing the proof.

4.3

The Wishart Model

In this section, we first carry out an estimate on the extra coefficient occurring in the Wishart
LDLR (32) (Lemma 4.7), then use the bounds on Ad (Lemmas 4.4, 4.5 and 4.6) to prove the upper
bound (Theorem 2.14) and the lower bound (Theorem 2.15) on (32).
Lemma 4.7 (Bounds on coefficient in Wishart LDLR). Suppose Dn = o(N ). There exist absolute
constants c1 , c2 > 0 such that, for sufficiently large N , for any 1 ≤ d ≤ Dn ,
(2N )d
≤
d!


N 
X Y
2di
2
(2N )d
.
≤ c1 d3/2 ec2 d /N
d!
di

dP
1 ,...,dN i=1
di =d

Proof. For the lower bound, note that for any di ≥ 2,

  
 

2 2(di − 1)
2di
2di (2di − 1) 2(di − 1)
≥
=
,
di − 1
1
di
di − 1
d2i
P
so for any d1 , . . . , dN ≥ 0 such that
di = d,

N 
Y
2di
i=1

di

Summing over all of the {di }, we find

≥ 2d .

 

N 
X Y
N + d − 1 d (2N )d
2di
.
≥
2 ≥
d!
di
d

dP
1 ,...,dN i=1
di =d

30

(41)

For the upper bound, we separately consider those {di }’s with exactly k positive entries for each
k = 1, 2, . . . , d:
 X

N 
k 
d  
X Y
X Y
N
2di
2ci
=
k
di
ci
dP
1 ,...,dN i=1
di =d

c1P
,...,ck >0 i=1
ci =d

k=1

≤


d  
X
N
d−1
k−1

k

k=1

max


k 
Y
2ci

c1P
,...,ck >0
ci =d i=1

ci

.

P
Given any positive integers c1 , . . . , ck such that
ci = d, if there are two entries cj ≥ cℓ ≥ 2,
consider c̃j = cj + 1, c̃ℓ = cℓ − 1 and c̃i = ci for all i 6= j, ℓ. We have the comparison


Qk

2c̃i
i=1 c̃i

Qk
2ci
i=1 ci

=

4−

4−

2
cj +1
2
cl +1

> 1.


Q
i
Therefore, for fixed k, the product ki=1 2c
is maximized when {ci } is composed of (k − 1)
ci
occurrences of 1 and one occurrence of (d − k + 1). As a result,
 X



N 
d  
d
X Y
X
N
d−1
2di
k−1 2(d − k + 1)
≤
·2
=:
S(k).
k
di
k−1
d−k+1

dP
1 ,...,dN i=1
di =d

Since
S(k) =

k=1

k=1

 


 



2(d − k)
N
d−1
2(d − k + 1)
Nk d
· 2k
· 2k−1
≤
k! k
d−k
k
k−1
d−k+1

and Stirling’s formula gives

 k
k
,
k! &
e
 
d
dd
,
. k
k (d − k)d−k
k


2(d − k)
. 4d−k ,
d−k
 d
√
d
d! . d
,
e
substituting into (42) and denoting k = (1 − η)d, we find
S(k)



(2N )d
d!

−1




Ne k
dd
· k
· 22d−k ·
.
k
k (d − k)d−k


√
1
2d ηd
.
= d
N eη
(1 − η)2(1−η)d

Now, for η ∈ (0, 1], denote
h(η) :=



2d
N eη

η
31

1
.
(1 − η)2(1−η)

1
√
d



2N e
d

d !−1

(42)

Then,

"
#d
 X
N 
d
d
X Y
2di
3/2 (2N )
sup h(η) .
≤
S(k) ≤ d max S(k) . d
1≤k≤d
d!
di
η∈(0,1]

dP
1 ,...,dN i=1
di =d

(43)

k=1

The last step is to evaluate h(η). Note that
 
d
2d
[log h(η)] = log
− log(η) + 2 log(1 − η).
dη
N
Since

2d
N

≤

2Dn
N

= o(1) as n → ∞, for large N the unique maximizer of h has the form
η ∗ = η ∗ (N, d) =

(2 − o(1))d
,
N

and consequently
∗

sup h(η) = h(η ) =
η∈(0,1]



2
e(2 − o(1))

η ∗

(1 − η ∗ )−2(1−η

∗)

≤ e4η

∗ (1−η ∗ )

≤ ec2 d/N .

Substituting into (43) then completes the proof.
Proof of Theorem 2.14(a). Suppose (10) holds. Let µ = − log λ̂. In the setting of Lemma 4.7, note
that
lim

n→∞

Dn
= 0,
N




1
1
lim inf − log λ̂n = − log lim sup λ̂n
n→∞
2
2
n→∞

> 0.

For n large enough so that (c2 + γ1 ) DNn < − 21 log λ̂n , applying Lemma 4.4 in the expression of (32)
yields


#d
"


⌊D/2⌋
N
X  X Y
 β 2 hv (1) , v (2) i2
2d
i
≤D
2


kLn,N,β,X k2 =
E

4
di 
v(1) ,v(2) ∼Xnρ
d=0

.

⌊D/2⌋ 

X
d=1

≤
.
≤

⌊D/2⌋

X

(2N )d c2 d2 /N
e
d3/2
d!
5/2 c2 dD/N

2d

e

d=1
∞
X
d=1
∞
X

dP
1 ,...,dN i=1
di =d

 "

β2
4

d

µd+d2 /n

(nρ)−2d · 2de


  d
(2d)!
µ+D/n N
e
β2
4d (d!)2
n


d
(c + 1 ) D
d2 e 2 γ N eµ λ̂2
d2 (λ̂1/2 )d

d=1

= O(1),
where the last equality is by the assumption that lim supn→∞ λ̂n < 1.
32

#
 
n (2d)! 2d
ρ
d 2d

√
Proof of Theorem 2.14(b). If (11) holds, then λ̂n ≤ 1/ 3 for sufficiently large n. In the setting of
D
< 0.001. Then, substituting the estimates in Lemma 4.5 (taking
Lemma 4.7, suppose (c2 + γ1 ) N
µ = λ̂) into (32) gives
2
kL≤D
n,N,β,X k2

.

⌊D/2⌋ 

X

d=11


d
3/2 (2N ) c2 d2 /N

d

d!

e

"

β2
4

d

−2d

(nρ)

·

√

d2 /n

de

  

d 
11e d/2
(2d)!
−2
2 N
.
d e
λ̂
β
4d (d!)2
n
30
d=11
!d
r
∞
X
11e
0.001
3/2
e
d
.
30
⌊D/2⌋

X



11e
30

d
2

λ̂

−2d

#
 
n (2d)! 2d
ρ
d 2d

1 dD
2 (c2 + γ ) N

d=11

= O(1),

completing the proof.
Proof of Theorem 2.15(a). In (36), only counting the terms {ai } with d occurrences of 1 and (n−d)
occurrences of zero yields
 
(1) (2) 2d
−2d n (2d)!
.
(44)
hv , v i ≥ n
E
d 2d
v(1) ,v(2) ∼Xnρ
From Lemma 4.3 we have that when Dn = o(n), for sufficiently large n and for any 1 ≤ d ≤ Dn ,
 
1 −d2 /n nd
1 −dDn /n nd
n
n(n − 1) . . . (n − d + 1) nd
·
≥
e
≥
e
.
(45)
=
nd
d!
2
d!
2
d!
d
Substituting Lemma 4.7, (44) and (45) into (32) yields, for sufficiently large n,
2
kL≤D
n,N,β,X k2

≥

  2d
Dn 
X
(2N )d
β

&

d=1
Dn
X

d
1 
√ λ̂2n e−Dn /n
d

d=1

1
√
d

−2d


 
n (2d)!
d 2d

n
4d
d=1
 2 d
Dn
X
β
(2d)!
e−dDn /n
&
d
2
4 (d!)
γ

≥

d!

d=1
Dn
X

= ω(1),
since Dn = ω(1), lim inf n→∞ λ̂n > 1 and e−Dn /n → 1.

√
Lemma 4.8. Suppose ω(1) ≤ Dn ≤ o(n). If there exists a series of positive integers wn = o( Dn )
such that
1− 1 
 1

wn
wn
wn
Dn
2
> 1,
(46)
lim inf 2λ̂n
n→∞
2neρ2n
(2wn )!

≤Dn
k22 → ∞ as n → ∞.
then kLn,N,β,X

33

Proof. If (46) holds, we can choose an ǫ > 0 such that, for sufficiently large n,
2λ̂2n



Dn
2neρ2n

1−

1
wn



wn
(2wn )!



1
wn

> 1 + ǫ.

Let n satisfy the above inequality. Pick µ ∈ (0, 1) such that
1

µ1− wn (1 + ǫ) > 1,
which implies
2



µDn
2neρ2n

1−

1
wn



wn
(2wn )!



1
wn

>

γ
.
β2

(47)

In the sum (32), we only consider those d ∈ (µDn /2, ⌊Dn /2⌋) that are multiples of wn . By
Lemma 4.7, Lemma 4.6, and (47),


!d


N
 X Y 2di  β 2 hv (1) , v (2) i2


E

4
di 
v(1) ,v(2) ∼Xnρ
dP
1 ,...,dN i=1
di =d

(2N )d
&
d!

(2d)!
& d
4 (d!)2
1
&√ .
d

"


β2
4

d

Nγ
n

d

 d #
 
√
(2d)!
w
γ
n
n
(nρ)−2d
ρ2d ·
d
2
β2
d

Therefore
≤Dn
k22 &
kLn,N,β,X

X

µDn /2<d<⌊Dn
wn | d

1
√
d
/2⌋

!
r
r
Dn
µDn
1
&√
−
wn
2wn
2wn
√ √
1 − µ Dn
= √
wn
2
= ω(1),
completing the proof.
Proof of Theorem 2.15(b). For sufficiently large n, in Lemma 4.8 we choose
√
wn = ⌈log(1/λ̂n )⌉ = ⌈log( γ/β)⌉

√
which is o( Dn ). Reorganizing the terms, the condition (46) is satisfied if for sufficiently large n,
r


1
wn · 2wn 1/(wn −1) Dn wn /(wn −1)
ρn < 0.99 √
λ̂
.
(48)
n n
2e (2wn )!
34

Notice that
1
√
2e



wn · 2wn
(2wn )!

1/(wn −1)

= Θ(wn−2 )
= Θ(log2 (1/λ̂n )),

λ̂nwn /(wn −1) = λ̂n · λ̂n1/(⌈log(1/λ̂n )⌉−1)
= Θ(λ̂n ).

Therefore, there exists an absolute constant C such that, if
r
Dn
ρn < C
λ̂n log2 (1/λ̂n ),
n
≤Dn
then (48) is satisfied and the divergence of kLn,N,β,X
k22 follows from Lemma 4.8.

4.4

The Wigner Model

In this section, we use the bounds on Ad (Lemmas 4.4, 4.5 and 4.6) to prove the upper bound
(Theorem 2.23) and the lower bound (Theorem 2.24) on the Wigner LDLR (29).
Proof of Theorem 2.23(a). We only work with those n for which (16) holds. Let µ = − log λ. Note
that
Dn
= 0,
n→∞ N

lim



1
lim inf − log λ̂n
n→∞
2
For large enough n that

Dn
n




1
= − log lim sup λ̂n
2
n→∞

> 0.

< − 21 log λn , applying Lemma 4.4 in the expression of (29) yields

#
"D
n
 n d
X
1
≤Dn 2
λ2 hv (1) , v (2) i2d
k =
kLn,λ,X
E
ρ
(1)
(2)
d!
2
v ,v ∼Xn d=0
 
Dn
X
1  n 2 d
−2d
µd+d2 /n n (2d)! 2d
.
λ
(nρ)
· 2de
ρ
d! 2
d 2d
.

d=1
Dn
X
d=1

d(2d)! µ+Dn /n 2 d
(e
λ )
4d (d!)2

Dn
X
1
√ (λ1/2 )d
.
d
d=1

= O(1),

where the last equation is by the assumption lim supn→∞ λn < 1.
√
Proof of Theorem 2.23(b). By the assumption (17), for sufficiently large n we have λn ≤ 1/ 3.
Now, Theorem 2.23(b) immediately follows from Lemma 4.5 (taking µ = λ), since for large enough

35

n that

Dn
n

< 0.001, we have
≤Dn 2
k
kLn,λ,X

 


Dn
X
√ d2 /n 11e d/2 −2d n (2d)! 2d
1  n 2 d
−2d
(nρ)
· de
λ
.
λ
ρ
d! 2
30
d 2d
d=11


∞ √
X
d(2d)! d2 /n 11e d/2
.
e
4d (d!)2
30
d=11
!d
r
∞
X
11e
Dn /n
e
.
30
d=11
!d
r
∞
X
11e
0.001
.
e
30
d=11

= O(1),

completing the proof.
Proof of Theorem 2.24(a). Substituting (44) and (45) into (29) yields
≤Dn 2
kLn,λ,X
k2

 
Dn
X
1  n 2 d −2d n (2d)!
λ
·n
≥
d! 2
d 2d
&

d=1
Dn
X
d=1

(2d)! 2d −dDn /n
λ e
4d (d!)2

Dn
d
X
1 
√ λ2 e−Dn /n
&
d
d=1

Dn
X
1
√
≥
d
d=1

= ω(1),

since Dn = ω(1), lim inf n→∞ λn > 1 and e−Dn /n → 1.

√
Lemma 4.9. Suppose ω(1) ≤ Dn ≤ o(n). If there exists a series of positive integers wn = o( Dn )
such that

 1 
 1
wn
Dn 1− wn
wn
2
lim inf 2λn
>1
(49)
2
n→∞
neρn
(2wn )!

≤Dn 2
k2 → ∞ as n → ∞.
then kLn,λ,X

Proof. If (49) holds, we can choose an ǫ > 0 such that for sufficiently large n,
2λ2n



Dn
neρ2n

1−

1
wn



wn
(2wn )!



1
wn

Let n satisfy the above inequality. Pick µ ∈ (0, 1) such that
1

µ1− wn (1 + ǫ) > 1.
36

> 1 + ǫ.

In the sum (29) we only consider those d > µDn that are multiples of wn . For each of them,
Lemma 4.6 gives
" 
1− 1 
 
 1 #d
 n d
wn
w
1
d
n
(2d)!
1  n 2 d
w
n
λn Ehv (1) , v (2) i2d &
λ2n · n−2d
2
d! 2
d! 2
neρ2
(2wn )!
d 2d
"
1− 1 

 1 #d
wn
wn
w
µD
(2d)! n(n − 1) · · · (n − d + 1)
n
n
2
·
·
2λ
≥ d
4 (d!)2
nd
neρ2
(2wn )!
1
&√ .
d

Therefore,
≤Dn 2
k2 &
kLn,λ,X

X

µDn <d<Dn
wn | d

1
√
d

r

1
&√
wn

Dn
−
wn
√
Dn
√
= (1 − µ)
wn
= ω(1),

r

µDn
wn

!

completing the proof.
Proof of Theorem 2.24(b). For sufficiently large n, in Lemma 4.9 we choose the positive integer
wn = ⌈log(1/λn )⌉,

√
≤Dn 2
k2 follows from the condition (49), which is implied by
which is o( Dn ). The divergence of kLn,λ,X
the following sufficient condition: for sufficiently large n,
r


1
wn · 2wn 1/(w−1) Dn wn /(wn −1)
ρn < 0.99 √
λ
.
(50)
n n
e (2wn )!
Similar to the proof of Lemma 4.8, notice that
1
√
e



wn · 2wn
(2wn )!



1
w−1

= Θ(wn−2 ) = Θ(log2 (1/λn ));

λnwn /(wn −1) = λn · λn1/(⌈log(1/λn )⌉−1) = Θ(λn ),

Thus there exists an absolute constant C such that, if
r
Dn
λn log2 (1/λn ),
ρn < C
n
≤Dn 2
k2 follows from Lemma 4.9.
then (50) is satisfied and the divergence of kLn,λ,X

Acknowledgments
We thank Samuel B. Hopkins, Philippe Rigollet, and Eliran Subag for helpful discussions.

37

References
[Abb17]

Emmanuel Abbe. Community detection and stochastic block models: recent developments. The Journal of Machine Learning Research, 18(1):6446–6531, 2017.

[AW08]

Arash A Amini and Martin J Wainwright. High-dimensional analysis of semidefinite
relaxations for sparse principal components. In 2008 IEEE International Symposium
on Information Theory, pages 2454–2458. IEEE, 2008.

[BB19]

Matthew Brennan and Guy Bresler. Optimal average-case reductions to sparse PCA:
From weak assumptions to strong hardness. arXiv preprint arXiv:1902.07380, 2019.

[BBH18]

Matthew Brennan, Guy Bresler, and Wasim Huleihel. Reducibility and computational lower bounds for problems with planted sparse structure. arXiv preprint
arXiv:1806.07508, 2018.

[BBP05]

Jinho Baik, Gérard Ben Arous, and Sandrine Péché. Phase transition of the largest
eigenvalue for nonnull complex sample covariance matrices. The Annals of Probability,
33(5):1643–1697, 2005.

[BGG+ 16]

Vijay VSP Bhattiprolu, Mrinalkanti Ghosh, Venkatesan Guruswami, Euiwoong Lee,
and Madhur Tulsiani. Multiplicative approximations for polynomial optimization over
the unit sphere. In Electronic Colloquium on Computational Complexity (ECCC),
volume 23, page 1, 2016.

[BGL16]

Vijay Bhattiprolu, Venkatesan Guruswami, and Euiwoong Lee. Sum-of-squares certificates for maxima of random tensors on the sphere. arXiv preprint arXiv:1605.00903,
2016.

[BGN11]

Florent Benaych-Georges and Raj Rao Nadakuditi. The eigenvalues and eigenvectors
of finite, low rank perturbations of large random matrices. Advances in Mathematics,
227(1):494–521, 2011.

[BHK+ 19]

Boaz Barak, Samuel Hopkins, Jonathan Kelner, Pravesh K Kothari, Ankur Moitra,
and Aaron Potechin. A nearly tight sum-of-squares lower bound for the planted clique
problem. SIAM Journal on Computing, 48(2):687–735, 2019.

[BKW19]

Afonso S Bandeira, Dmitriy Kunisky, and Alexander S Wein. Computational hardness
of certifying bounds on constrained PCA problems. arXiv preprint arXiv:1902.07324,
2019.

[BMNN16]

Jess Banks, Cristopher Moore, Joe Neeman, and Praneeth Netrapalli. Informationtheoretic thresholds for community detection in sparse networks. In Conference on
Learning Theory, pages 383–416, 2016.

[BMV+ 18]

Jess Banks, Cristopher Moore, Roman Vershynin, Nicolas Verzelen, and Jiaming Xu.
Information-theoretic bounds and phase transitions in clustering, sparse PCA, and
submatrix localization. IEEE Transactions on Information Theory, 64(7):4872–4894,
2018.

[BPP18]

Guy Bresler, Sung Min Park, and Madalina Persu. Sparse PCA from sparse linear
regression. In Advances in Neural Information Processing Systems, pages 10942–
10952, 2018.
38

[BR13a]

Quentin Berthet and Philippe Rigollet. Computational lower bounds for sparse PCA.
arXiv preprint arXiv:1304.0828, 2013.

[BR13b]

Quentin Berthet and Philippe Rigollet. Optimal detection of sparse principal components in high dimension. The Annals of Statistics, 41(4):1780–1815, 2013.

[BS06]

Jinho Baik and Jack W Silverstein. Eigenvalues of large sample covariance matrices
of spiked population models. Journal of multivariate analysis, 97(6):1382–1408, 2006.

[CDMF09]

Mireille Capitaine, Catherine Donati-Martin, and Delphine Féral. The largest eigenvalues of finite rank deformation of large wigner matrices: convergence and nonuniversality of the fluctuations. The Annals of Probability, 37(1):1–47, 2009.

[CMW13]

T Tony Cai, Zongming Ma, and Yihong Wu. Sparse PCA: Optimal rates and adaptive
estimation. The Annals of Statistics, 41(6):3074–3110, 2013.

[DAM16]

Yash Deshpande, Emmanuel Abbe, and Andrea Montanari. Asymptotic mutual information for the binary stochastic block model. In 2016 IEEE International Symposium
on Information Theory (ISIT), pages 185–189. IEEE, 2016.

[dBG08]

Alexandre d’Aspremont, Francis Bach, and Laurent El Ghaoui. Optimal solutions
for sparse principal component analysis. Journal of Machine Learning Research,
9(Jul):1269–1294, 2008.

[dGJL05]

Alexandre d’Aspremont, Laurent El Ghaoui, Michael I Jordan, and Gert R Lanckriet.
A direct formulation for sparse PCA using semidefinite programming. In Advances
in neural information processing systems, pages 41–48, 2005.

[DKMZ11a] Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborová. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic
applications. Physical Review E, 84(6):066106, 2011.
[DKMZ11b] Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborová. Inference and phase transitions in the detection of modules in sparse networks. Physical
Review Letters, 107(6):065701, 2011.
[DM14a]

Yash Deshpande and Andrea Montanari. Information-theoretically optimal sparse
PCA. In 2014 IEEE International Symposium on Information Theory, pages 2197–
2201. IEEE, 2014.

[DM14b]

Yash Deshpande and Andrea Montanari. Sparse PCA via covariance thresholding. In
Advances in Neural Information Processing Systems, pages 334–342, 2014.

[DM15]

Yash Deshpande and Andrea Montanari. Improved sum-of-squares lower bounds for
hidden clique and hidden submatrix problems. In Conference on Learning Theory,
pages 523–562, 2015.

[DMK+ 16]

Mohamad Dia, Nicolas Macris, Florent Krzakala, Thibault Lesieur, and Lenka Zdeborová. Mutual information for symmetric rank-one matrix estimation: A proof of
the replica formula. In Advances in Neural Information Processing Systems, pages
424–432, 2016.

39

[EK18]

Ahmed El Alaoui and Florent Krzakala. Estimation in the spiked wigner model:
A short proof of the replica formula. In 2018 IEEE International Symposium on
Information Theory (ISIT), pages 1874–1878. IEEE, 2018.

[EKJ17]

Ahmed El Alaoui, Florent Krzakala, and Michael I Jordan. Finite size corrections and likelihood ratio fluctuations in the spiked wigner model. arXiv preprint
arXiv:1710.02903, 2017.

[EKJ18]

Ahmed El Alaoui, Florent Krzakala, and Michael I Jordan. Fundamental limits of
detection in the spiked wigner model. arXiv preprint arXiv:1806.09588, 2018.

[FP07]

Delphine Féral and Sandrine Péché. The largest eigenvalue of rank one deformation
of large wigner matrices. Communications in mathematical physics, 272(1):185–228,
2007.

[HKP+ 17]

Samuel B Hopkins, Pravesh K Kothari, Aaron Potechin, Prasad Raghavendra, Tselil
Schramm, and David Steurer. The power of sum-of-squares for detecting hidden
structures. In 2017 IEEE 58th Annual Symposium on Foundations of Computer
Science (FOCS), pages 720–731. IEEE, 2017.

[Hop18]

Samuel Hopkins. Statistical Inference and the Sum of Squares Method. PhD thesis,
Cornell University, 2018.

[HS17]

Samuel B Hopkins and David Steurer. Bayesian estimation from few samples: community detection and related problems. arXiv preprint arXiv:1710.00264, 2017.

[HSS15]

Samuel B Hopkins, Jonathan Shi, and David Steurer. Tensor principal component
analysis via sum-of-square proofs. In Conference on Learning Theory, pages 956–1006,
2015.

[HSV19]

Guy Holtzman, Adam Soffer, and Dan Vilenchik. A greedy anytime algorithm for
sparse PCA. arXiv preprint arXiv:1910.06846, 2019.

[Jer92]

Mark Jerrum. Large cliques elude the Metropolis process. Random Structures &
Algorithms, 3(4):347–359, 1992.

[JL04]

Iain M. Johnstone and Arthur Yu Lu. Sparse principal components analysis. Unpublished manuscript, 2004.

[JL09]

Iain M Johnstone and Arthur Yu Lu. On consistency and sparsity for principal components analysis in high dimensions. Journal of the American Statistical Association,
104(486):682–693, 2009.

[JMRT16]

Adel Javanmard, Andrea Montanari, and Federico Ricci-Tersenghi. Phase transitions in semidefinite relaxations. Proceedings of the National Academy of Sciences,
113(16):E2218–E2223, 2016.

[Joh01]

Iain M Johnstone. On the distribution of the largest eigenvalue in principal components analysis. The Annals of statistics, 29(2):295–327, 2001.

[KNV15]

Robert Krauthgamer, Boaz Nadler, and Dan Vilenchik. Do semidefinite relaxations
solve sparse PCA up to the information limit? The Annals of Statistics, 43(3):1300–
1322, 2015.
40

[KWB19]

Dmitriy Kunisky, Alexander S Wein, and Afonso S Bandeira. Notes on computational hardness of hypothesis testing: Predictions using the low-degree likelihood
ratio. arXiv preprint arXiv:1907.11636, 2019.

[KXZ16]

Florent Krzakala, Jiaming Xu, and Lenka Zdeborová. Mutual information in rankone matrix estimation. In 2016 IEEE Information Theory Workshop (ITW), pages
71–75. IEEE, 2016.

[KY13]

Antti Knowles and Jun Yin. The isotropic semicircle law and deformation of wigner
matrices. Communications on Pure and Applied Mathematics, 66(11):1663–1749,
2013.

[LKZ15a]

Thibault Lesieur, Florent Krzakala, and Lenka Zdeborová. MMSE of probabilistic
low-rank matrix estimation: Universality with respect to the output channel. In
2015 53rd Annual Allerton Conference on Communication, Control, and Computing
(Allerton), pages 680–687. IEEE, 2015.

[LKZ15b]

Thibault Lesieur, Florent Krzakala, and Lenka Zdeborová. Phase transitions in sparse
PCA. In 2015 IEEE International Symposium on Information Theory (ISIT), pages
1635–1639. IEEE, 2015.

[LM00]

Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional
by model selection. Annals of Statistics, pages 1302–1338, 2000.

[LM19]

Marc Lelarge and Léo Miolane. Fundamental limits of symmetric low-rank matrix
estimation. Probability Theory and Related Fields, 173(3-4):859–929, 2019.

[McS01]

Frank McSherry. Spectral partitioning of random graphs. In Proceedings 2001 IEEE
International Conference on Cluster Computing, pages 529–537. IEEE, 2001.

[Mio17]

Léo Miolane. Fundamental limits of low-rank matrix estimation: the non-symmetric
case. arXiv preprint arXiv:1702.00473, 2017.

[Mio18]

Léo Miolane. Phase transitions in spiked matrix estimation: information-theoretic
analysis. arXiv preprint arXiv:1806.04343, 2018.

[Moo17]

Cristopher Moore. The computer science and physics of community detection: Landscapes, phase transitions, and hardness. arXiv preprint arXiv:1702.00467, 2017.

[MPW15]

Raghu Meka, Aaron Potechin, and Avi Wigderson. Sum-of-squares lower bounds for
planted clique. In Proceedings of the forty-seventh annual ACM symposium on Theory
of computing, pages 87–96. ACM, 2015.

[MRZ15]

Andrea Montanari, Daniel Reichman, and Ofer Zeitouni. On the limitation of spectral
methods: From the gaussian hidden clique problem to rank-one perturbations of
gaussian tensors. In Advances in Neural Information Processing Systems, pages 217–
225, 2015.

[MW15]

Tengyu Ma and Avi Wigderson. Sum-of-squares lower bounds for sparse PCA. In
Advances in Neural Information Processing Systems, pages 1612–1620, 2015.

[MWA06]

Baback Moghaddam, Yair Weiss, and Shai Avidan. Spectral bounds for sparse PCA:
Exact and greedy algorithms. In Advances in neural information processing systems,
pages 915–922, 2006.
41

[Nad08]

Boaz Nadler. Finite sample approximation results for principal component analysis:
A matrix perturbation approach. The Annals of Statistics, 36(6):2791–2817, 2008.

[OMH13]

Alexei Onatski, Marcelo J Moreira, and Marc Hallin. Asymptotic power of sphericity
tests for high-dimensional data. The Annals of Statistics, 41(3):1204–1231, 2013.

[Pau04]

Debashis Paul. Asymptotics of the leading sample eigenvalues for a spiked covariance
model. Preprint, 2004.

[Pau07]

Debashis Paul. Asymptotics of sample eigenstructure for a large dimensional spiked
covariance model. Statistica Sinica, pages 1617–1642, 2007.

[Péc06]

Sandrine Péché. The largest eigenvalue of small rank perturbations of hermitian
random matrices. Probability Theory and Related Fields, 134(1):127–173, 2006.

[PJ12]

Debashis Paul and Iain M Johnstone. Augmented sparse principal component analysis
for high dimensional data. arXiv preprint arXiv:1202.1242, 2012.

[PRS13]

Alessandro Pizzo, David Renfrew, and Alexander Soshnikov. On finite rank deformations of wigner matrices. In Annales de l’IHP Probabilités et statistiques, volume 49,
pages 64–94, 2013.

[PWBM16]

Amelia Perry, Alexander S Wein, Afonso S Bandeira, and Ankur Moitra. Optimality
and sub-optimality of PCA for spiked random matrices and synchronization. arXiv
preprint arXiv:1609.05573, 2016.

[PWBM18a] Amelia Perry, Alexander S Wein, Afonso S Bandeira, and Ankur Moitra. Messagepassing algorithms for synchronization problems over compact groups. Communications on Pure and Applied Mathematics, 71(11):2275–2322, 2018.
[PWBM18b] Amelia Perry, Alexander S Wein, Afonso S Bandeira, and Ankur Moitra. Optimality
and sub-optimality of PCA I: Spiked random matrix models. The Annals of Statistics,
46(5):2416–2451, 2018.
[RM14]

Emile Richard and Andrea Montanari. A statistical model for tensor PCA. In Advances in Neural Information Processing Systems, pages 2897–2905, 2014.

[RRS17]

Prasad Raghavendra, Satish Rao, and Tselil Schramm. Strongly refuting random
CSPs below the spectral threshold. In Proceedings of the 49th Annual ACM SIGACT
Symposium on Theory of Computing, pages 121–131. ACM, 2017.

[RSS18]

Prasad Raghavendra, Tselil Schramm, and David Steurer. High-dimensional estimation via sum-of-squares proofs. arXiv preprint arXiv:1807.11419, 2018.

[Sin11]

Amit Singer. Angular synchronization by eigenvectors and semidefinite programming.
Applied and computational harmonic analysis, 30(1):20–36, 2011.

[SS11]

Amit Singer and Yoel Shkolnisky. Three-dimensional structure determination from
common lines in cryo-EM by eigenvectors and semidefinite programming. SIAM
journal on imaging sciences, 4(2):543–572, 2011.

[Ver10]

Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices.
arXiv preprint arXiv:1011.3027, 2010.
42

[VL12]

Vincent Vu and Jing Lei. Minimax rates of estimation for sparse PCA in high dimensions. In Artificial intelligence and statistics, pages 1278–1286, 2012.

[Vu18]

Van Vu. A simple SVD algorithm for finding hidden partitions. Combinatorics,
Probability and Computing, 27(1):124–140, 2018.

[WBS16]

Tengyao Wang, Quentin Berthet, and Richard J Samworth. Statistical and computational trade-offs in estimation of sparse principal components. The Annals of
Statistics, 44(5):1896–1930, 2016.

[WEM19]

Alexander S Wein, Ahmed El Alaoui, and Cristopher Moore. The Kikuchi hierarchy
and tensor PCA. arXiv preprint arXiv:1904.03858, 2019.

[WTH09]

Daniela M Witten, Robert Tibshirani, and Trevor Hastie. A penalized matrix decomposition, with applications to sparse principal components and canonical correlation
analysis. Biostatistics, 10(3):515–534, 2009.

[ZHT06]

Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis.
Journal of computational and graphical statistics, 15(2):265–286, 2006.

A

Chernoff Bounds

In this section, we present two Chernoff-type concentration inequalities used in our proofs.
Lemma A.1 (Local Chernoff bound Gaussian inner products). Let u(1) , u(2) ∈ RN be independent
samples from N (0, IN ). Then, for any 0 < t ≤ N/2,


h
i
t2
(1) (2)
.
Pr |hu , u i| ≥ t ≤ 2 exp −
4N
(1) (2)
(1) (2)
Proof. Since
by symmetry
 (1)
 hu , u i and −hu , u i have the same distribution, it suffices to
(2)
bound Pr hu , u i ≥ t for 0 < t ≤ N/2. By Markov’s inequality on the moment generating
function, for any µ > 0,

h

(1)

Pr hu

,u

(2)

i

(1)

Eeµhu ,u
i≥t ≤
eµt

(2) i

= e−µt (Eeµx1 x2 )N ,

where x1 , x2 are independent samples from N (0, 1). We compute
 2

ZZ
1
x
y2
1
µx1 x2
exp − + µxy −
Ee
=
dx dy = (1 − µ2 )− 2 .
2π
2
2
R2
2 /2

Take µ = t/N ∈ (0, 21 ]. Note that 1 − z ≥ e−3z/2 on z ∈ (0, 14 ], and so 1 − µ2 ≥ e−3µ
h

(1)

Pr hu

(2)

,u

i



t2
i ≥ t ≤ exp −
N



and the result follows.
The following result may be found in [LM00].

43

· exp



3N µ2
4





t2
= exp −
,
4N

. Hence

Lemma A.2 (Chernoff bound for χ2 distribution). For all 0 < z < 1,

Similarly, for all z > 1,

 1

1
log Pr χ2k ≤ zk ≤ (1 − z + log z).
k
2
 1

1
log Pr χ2k ≥ zk ≤ (1 − z + log z).
k
2

Corollary A.3. For all 0 < t ≤ 1/2,



t2
1
log Pr |χ2k − k| ≥ kt ≤ − .
k
3

Proof. It is easy to check that for t ∈ (0, 1/2],

t2
3
t2
−t + log(1 + t) ≤ − .
3
t + log(1 − t) ≤ −

Therefore, by Lemma A.2,

 1

 1


1
log Pr |χ2k − k| ≥ kt = log Pr χ2k ≥ (1 + t)k + log Pr χ2k ≤ (1 − t)k
k
k
k
1
1
≤ (−t + log(1 + t)) + (t + log(1 − t))
2
2
t2
≤− ,
3
completing the proof.

44

