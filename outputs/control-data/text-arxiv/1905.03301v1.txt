arXiv:1905.03301v1 [physics.data-an] 8 May 2019

A new method to determine multi-angular reflectance factor
from lightweight multispectral cameras with sky sensor in a
target-less workflow applicable to UAV
Klaus Schneider-Zappa,∗, Manuel Cubero-Castana , Dai Shia , Christoph Strechaa
a Pix4D,

EPFL Innovation Park, Building F, 1015 Lausanne, Switzerland

Abstract
A new physically based method to estimate hemispheric-directional reflectance factor
(HDRF) from lightweight multispectral cameras that have a downwelling irradiance
sensor is presented. It combines radiometry with photogrammetric computer vision to
derive geometrically and radiometrically accurate data purely from the images, without
requiring reflectance targets or any other additional information apart from the imagery.
The sky sensor orientation is initially computed using photogrammetric computer vision and revised with a non-linear regression comprising radiometric and photogrammetryderived information. It works for both clear sky and overcast conditions. A groundbased test acquisition of a Spectralon target observed from different viewing directions
and with different sun positions using a typical multispectral sensor configuration for
clear sky and overcast showed that both the overall value and the directionality of the
reflectance factor as reported in the literature were well retrieved. An RMSE of 3 % for
clear sky and up to 5 % for overcast sky was observed.
Keywords: reflectance; multi-angular; hemispheric-directional reflectance factor
(HDRF); multispectral camera; downwelling irradiance sensor

1. Introduction
Image-based remote sensing is a widely used tool, from Earth science disciplines
like oceanography or geology to dedicated applications like mineral exploration or
precision agriculture. Traditionally, space-borne and air-borne sensors were used to
retrieve reflectance factors in several spectral bands. Satellites have the advantage of
large spacial coverage, e.g. to predict maize yield on the whole USA (Sakamoto et al.,
2014) or to monitor forest clearance in Madagascar (Page et al., 2002). However, they
are limited by low resolution, long revisiting times, no available data under cloud cover,
and high costs to operate and maintain. Air-borne platforms allow better flexibility in
sensors, naturally increasing spectral and spatial resolution, but are also too costly to
allow short revisiting times (Matese et al., 2015).
∗ Principal

corresponding author
Email address: klaus.schneider-zapp@pix4d.com (Klaus Schneider-Zapp)

Preprint submitted to Elsevier

10th May 2019

Recently, unmanned aerial vehicles (UAVs) have become popular sensing platforms. They allow timely measurements, have high resolution and can be operated
under cloud cover. Light multi- and hyper-spectral sensors which are suitable to be
flown on UAVs have become available in the last years (Colomina and Molina, 2014).
Especially in precision agriculture, where the temporal evolution needs to be tracked,
UAVs have become an important monitoring tool, which is used to estimate plant properties like leaf area index (Lelong et al., 2008) or biomass (Bendig et al., 2015), to
detect diseases (Nebiker et al., 2016), or to predict yields (Geipel et al., 2014; Nebiker
et al., 2016).
Radiometric accuracy is of prime importance for accurate results. Artefacts due to
incorrect radiometry can lead to misinterpretation. Traditional methods from satellites
are not readily applicable to UAV-based data due to different spacial scales, illumination conditions (satellites only yield data without cloud cover), and instrument constraints. To date, most researchers use the empirical linear method to calibrate their
sensor with known reflectance targets (Jakob et al., 2017; Wang and Myint, 2015).
Hakala et al. (2010) derived the reflectance factor by using a known reflectance target which must be visible in every single image; this is impractical for any larger
area or frequent measurements as required by precision agriculture monitoring for instance. Honkavaara and Khoramshahi (2018) use an optimisation procedure to derive
radiometric parameters from reflectance targets. However, as illumination conditions
can change rapidly under cloud cover, its applicability is quite limited, since the reflectance target has to be measured repeatedly for every change in illumination. Furthermore, for practical data acquisition of larger areas, a simple workflow is required. This
is especially true for applications like precision agriculture, where operators may not
be experts and it is important to reduce any human intervention as much as possible.
Multispectral cameras with a “sky sensor” that captures the downwelling irradiance
have recently become commercially available. With the correct camera calibration,
those sensors can be used to derive reflectance factors without the need of an empirical
calibration procedure for every acquisition (Cubero-Castan et al., 2018). Traditional
correction methods like Li et al. (2016); Colby (1991) which are based on work for
satellites or airborne measurements only work for clear sky, as they neglect the contribution of scattered skylight. The International Commission on Illumination (Commission Internationale de l’Eclairage, CIE) published a standard sky model (Darula and
Kittler, 2002) that describes the sky illuminance for a broad range of sky conditions,
from overcast to clear sky. It has recently been used for estimating the illuminance on
tilted areas (Sola et al., 2016). However, due to the large number of parameters, it is not
easily usable for UAV data. Especially for overcast sky, where the illuminance can be a
quickly changing function of time, an estimation only from the images is problematic.
The desired outcome for most multispectral acquisitions are generally reflectance
maps, or in some case the computation of vegetation indices based on those reflectance
maps. By definition, reflectance is a multi-angular quantity (Schaepman-Strub et al.,
2006) and reflectance maps are an estimation of these quantities at a given viewing
angle (normally nadir). Methods have been designed to correct for this angular effect.
Such corrections are specifically required for time series analysis (Roujean et al., 1992).
Models based on multi-angular acquisition are used to recover different parameters,
like the canopy structure of forests (Widlowski et al., 2004) or advanced vegetation
2

indexes like Leaf Area Index or foliage clumping index (Chen et al., 2005; Schaepman,
2007). An advantage of UAV acquisitions is that measurements are multi-angular:
a good overlap between different frames is usually required, resulting in measuring
all scene materials with different viewing angles for each acquisition. This allows to
perform an analysis similar to what was previously only possible with satellite data
while still using all advantages of UAVs.
In this paper, we describe a new physically based method to derive the hemisphericaldirectional reflectance factor (HDRF) with a multispectral camera and a sky sensor.
The proposed method works in both clear sky and overcast conditions by describing
both the direct and scattered sunlight contributions to the illumination, and takes into
account the local surface orientation as well as the sky sensor orientation. All required
parameters are estimated solely from the multispectral images by combining photogrammetry and radiometry.
First we quickly review radiometric corrections for image acquisition. Then we
describe our new algorithm to take the sky sensor data into account for estimating
HDRF in a target-less workflow. We present the data acquisition and processing of the
verification experiment. Finally we discuss the results and show that our new method
is able to correctly retrieve HDRF in both overcast and clear sky conditions.
2. Review of radiometry of imaging
The basics of radiometry and remote sensing are treated comprehensively in textbooks, e.g. McCluney (2014); Jähne (2012); Schott (2007). An overview of reflectance
quantities is found in Schaepman-Strub et al. (2006). Here we only quickly review
the most important concepts required for this study. Only the optical and near-infrared
spectrum are considered. In this spectral range, the observed signal is dominated by
reflected sunlight.
Consider a sun-illuminated object on a field observed by a multispectral camera
with a sky sensor aboard a UAV. A typical height is around 100 m. The object receives
direct solar irradiance (directional), and scattered radiance (skylight and light scattered
in clouds) from the entire hemisphere (Schott, 2007). If there are no high objects in
the field, light reflected by adjacent objects and multiple scattering can be neglected.
Due to the low flying height, atmospheric scattering on the path between the object of
interest and the camera (light from the object that is scattered away from the camera
and light scattered in the atmosphere into the camera) can be neglected and the downwelling irradiance reaching the UAV is practically identical to the one on the ground
(Schläpfer and Nieke, 2005). The instantaneous field of view of one pixel of a typical
multispectral camera is very small (e.g. for the Parrot Sequoia it is 0.055◦ horizontally
and vertically), thus the solid angle is considered infinitesimal. The quantity of interest
is the hemispherical-directional reflectance factor (HDRF), which is defined as
R

R(θi , ϕi , 2π; θr , ϕr ) =

fr (θi , ϕi , θr , ϕr )Li (θi , ϕi ) dΩi

2π

(1/π)

R

Li (θi , ϕi ) dΩi

2π

3

,

(1)

where
fr =

dLr
dEi

(2)

is the bi-directional
reflectance distribution function (BRDF), Li the incoming spectral
R
radiance, Ei = Li dΩi the incoming spectral irradiance, Lr the reflected spectral radi2π

ance, θi and ϕi the zenith and azimuth angles of the incoming direct sunlight, θr and
ϕr the zenith and azimuth angles of observation, and Ω solid angle (Schaepman-Strub
et al., 2006).
We divide the incoming radiance Li into direct sunlight Ed with angles θi and
ϕi (through a hypothetical surface perpendicular to the beam direction) and diffuse
scattered sunlight Ls . Assume that the diffuse part is isotropic (i.e. independent of
the angles). Then LsR is a constant and the diffuse irradiance received from the whole
hemisphere is Es = Ls dΩ = πLs . Let
2π

ε=

Ed
Ed + πLs

(3)

be the direct sunlight ratio, i.e. the ratio of direct irradiance to total irradiance. With
this definition, we can re-write Schaepman-Strub et al. (2006), eq. (12)–(13), as
R(θi , ϕi , 2π; θr , ϕr ) = επ fr (θi , ϕi , θr , ϕr ) + (1 − ε)

Z

fr (θi , ϕi , θr , ϕr ) dΩi .

(4)

2π

For a Lambertian reflector, the equation simplifies to R = π fr .
A pixel of an optical camera with focal length f and aperture number k = f /a
at distance d from a Lambertian source of spectral radiance Lλ receives the spectral
irradiance
Eλ0 = t

π cos4 θ
Lλ ,
4k2 (1 + m)2

(5)
0

0

f
where θ is the observation angle, m = xx = ff+d
+d = d the projection scale, and t the
transmissivity of the optical system (a function of wavelength and observation angle)
(Jähne, 2012, Sect. 3.5). A solid state imaging sensor such as a complementary metaloxide-semiconductor (CMOS) sensor can be modelled as

Z∞

p = p0 + KτApix

ηEλ0 dλ ,

(6)

0

where p is the pixel digital number (DN), p0 the dark current (signal obtained without
any radiance, caused by thermal stimulation), K the gain, τ the exposure time, Apix the
area of the pixel, η the quantum yield, and λ wavelength (Jähne, 2012, Sect. 6.4). Real
sensors may have additional effects. Electron leakage from the pixel well can result in
a loss of signal. For CMOS sensors with an electronic global shutter, light leakage into
the hold cell can lead to an additional signal.
4

Introducing the effective radiance for the band of interest
Z∞

L=

SLλ dλ

(7)

0

with the spectral sensitivity of the sensor S (which is composed of the spectral variance
of both the transmissivity t and the quantum yield η) we can combine eq. (5) and eq. (6)
as
p = p0 +

τKsv
1
πL ,
k2 (1 + m)2

(8)

where s is the (wavelength-independent) sensitivity and v vignetting (composed of
cos4 θ and the angle-dependent part of the transmissivity t).
3. HDRF estimation from multispectral camera with sky sensor
This section describes our model to estimate HDRF by combining radiometry with
photogrammetric computer vision. It is applicable to any calibrated multispectral camera with sky sensor where the sky sensor is designed such that the orientation of the
light-sensitive area is well-defined and the sensitivity does not change with angle, and
which is rigidly connected to the camera body. Initially, the scene is reconstructed with
photogrammetric computer vision. This results, among others, in the camera orientation and position in full 3D for each capture, and a geo-referenced digital surface model
of the scene which includes the elevation and local surface normal of each point. The
lens distortion is also optimised during this process and automatically corrected when
projecting the images. It is assumed that the different bands of the multispectral camera
and the sky sensor are mechanically rigidly connected and the captures of the different
bands and the sky sensor are synchronised. This allows to use the known constraints
between the cameras for the different bands and the sky sensor to improve the speed
and geometric accuracy. We assume that the sky sensor measures the sun radiation at
the same spectral band as the camera. It is important that the orientation of the sky
sensor is precise, since small errors in the angle between the sky sensor and the sun ray
direction lead to significant errors in reflectance, especially when the angle between the
sky sensor normal and the sun ray is large in clear sky conditions. This is illustrated
by fig. 1 which shows the theoretical relative error in reflectance caused by an error
in the angle between sun and sky sensor due to an incorrect sky sensor orientation. A
major novelty of our method is the precise estimation of the sky sensor pose due to the
combination of photogrammetry and radiometry, which is a prerequisite for obtaining
reliable reflectance values.
As discussed above, the sunlight consists of direct sunlight Ed (referring to an area
perpendicular to the sun direction) and diffuse scattered light Ls (fig. 2) and we assume
that the scattered light is isotropic. Let κ be the angle between the normal of the sky
sensor and the direct sunlight beam. This angle is computed from the photogrammetrically derived sky sensor pose and the sun ray direction. If the sky sensor is perfectly

5

relative error (%)

Orientation error:

angle sun - sky sensor (°)

nl
igh
t

Figure 1: Theoretical error in reflectance due to an incorrect sky sensor orientation as a function of the angle
between sky sensor and sun for an orientation error of 5.0◦ (red), 2.0◦ (orange), 1.0◦ (cyan), 0.5◦ (dark blue),
and 0.1◦ (green). The direct sunlight ratio is ε = 0.9 and the sun zenith angle is 39.1◦ .

su

nadir

di
re

ct

sun

igh
t

ϑ

κ

su
re

ct

sky sensor

di

scattering

nl

sun

β
ϑ

object

sky sensor
Figure 2: Geometry of sun radiation on the object and on the sky sensor.

6

horizon

horizontal (ϑ = 0) and measures the full hemisphere, the observed irradiance is
Z

Esun,meas = Ed cos κ + Ls

dΩ = Ed cos κ + Es .

(9)

2π

In case κ > π/2, the sky sensor does not see any direct sunlight any more and the
term Ed cos κ should be forced to zero. If the sky sensor is inclined with respect to
the horizon, ϑ 6= 0, then even a sensor with perfect hemispherical sensitivity will not
capture the complete scattered irradiance; the energy outside the field of view (the grey
area in fig. 2 (right)) is missing:
Esun,meas = Ed cos κ + Ls Ωv = Ed cos κ +

Ωv
Es
π

(10)

with the projected solid angle of view

Rπ





π/2−ϑ
R

t(θ , ϕ) cos θ sin θ dθ dϕ +

ϕ=0 θ =0
Ωv =
π/2
 Rπ
R





2π
R
R π/2

t(θ , ϕ) cos θ sin θ dθ dϕ

, 0 ≤ ϑ ≤ π/2

ϕ=π θ =0

t(θ , ϕ) cos θ sin θ dθ dϕ

,
, π/2 < ϑ ≤ π

ϕ=0 θ =ϑ −π/2

(11)
where ts (θ , ϕ) is the directional transmissivity of the sky sensor. Note that the integral
is written in the coordinate system of the sky sensor; the term cos θ describes the
projection of the incoming radiance onto the sky sensor. Only the fraction ζ = Ωπv of
the real scattered sun irradiance is measured. For an ideal sky sensor with constant
transmissivity ts = 1 over the whole hemisphere, we can evaluate
the integral to Ωv =

π 21 + 12 cos2 θ for 0 ≤ ϑ ≤ π/2 and Ωv = π 12 − 12 cos2 θ for π/2 < ϑ ≤ π.
Using the direct sunlight ratio ε, we can compute the total irradiance received by a
hypothetical area perpendicular to the sun direction:
Esun,tot = Ed + Es =

Esun,meas
.
ε cos κ + ζ (1 − ε)

(12)

If we know the direct light ratio ε, we can estimate the sun irradiation on the object of
interest:
Esun = Esun,meas

ε cos β + (1 − ε)
.
ε cos κ + ζ (1 − ε)

(13)

As mentioned before, the local surface normal of each point is known from the photogrammetric reconstruction, therefore β can be readily computed without any additional
information.
Analogous to a camera pixel, the sky sensor measures a DN
Z∞

I = I0 + τs Ks ss

SEsun,meas dλ ,

(14)

0

7

where I0 is the dark current, Ks the gain, τs the exposure time, and ss the sensitivity of
the sky sensor (the sky sensor is like a camera with only one pixel with an IFOV of the
whole hemisphere). The spectral sensitivity S is assumed to be the same as the one of
corresponding camera.
In a local coordinate system with the axes east, north, up, the direction from the
sky sensor towards the sun is d~sun = (sin ϕi , cos ϕi , cot θi ). As mentioned before, the
direction of the sky sensor d~sensor is known from the photogrammetric reconstruction.
Then the cosine of the angle between the sun and the sensor is computed from the two
vectors.
If the reflectance does not vary too much with the incident angles, we can approximate the integral in eq. (4) and the equation simplifies to R = π fr = πLr /Ei . By solving
eq. (8) for L and inserting that and eq. (13), and using eq. (14) to compute DN for the
sunshine sensor, we can write the HDRF as a function of the measured quantities:
R=

(p − p0 )k2 (1 + m)2 τs Ks ss ε cos κ + ζ (1 − ε)
.
τKsv
I − I0 ε cos β + (1 − ε)

(15)

The direct sunlight ratio is not known a priori, thus it must be estimated. Our
method derives it directly from the measurements without requiring additional knowledge by fitting the angle-dependence of the sky sensor measurements. If there are
no scattered clouds with sunny spots (e.g. scattered cumulus), the total downwelling
irradiance Esun,tot is a smooth function of time, and for sufficiently small times it can
be parametrised in a simple way. We can use measurements of the irradiance Esun,meas,i
and angles κi to least-square fit ε and the parameters of the parametrisation:

2
Esun,meas,i
1 N
χ 2 = 2 ∑ Esun,tot −
→ min .
(16)
N i=0
ε cos κi + ζ (1 − ε)
The nominal relative angle between camera and sky sensor typically has an uncertainty of a few degrees, because the lenses always have a slight misalignment due to
manufacturing tolerances, temperature changes, or shocks on the camera. Therefore,
we coupled our optimisation with the photogrammetric computation to be able to optimise the relative angle between the camera and the sky sensor. The total downwelling
irradiance was modelled as a polynomial. The result of the fit is the direct sunlight ratio ε, the total downwelling irradiance, and optionally a revised estimate of the relative
angle.
4. Data acquisition and processing
To test our new HDRF retrieval method, we conducted test experiments by acquiring images of a target of known reflectance under different viewing angles and sun to
sky sensor angles, in order to compare the retrieved HDRF with its literature values.
The ground experiment avoids additional uncertainties that would be introduced by
UAVs and allows more rigorous testing of our new method.
The acquisitions were performed with the Parrot Sequoia multispectral camera1 ,
1 https://www.parrot.com/business-solutions-us/parrot-professional/parrot-sequoia/

8

diffuser
black cone
bandpass filter
photodiode
Figure 3: Schematic of one band of the Sequoia sky sensor. A diffuser diffuses the incoming light. The
bandpass filter is identical to the one used for the CMOS sensor. The setup is repeated for each band in a
2 × 2 grid.

Table 1: Details of sequoia bands.

Band name
Green
Red
Red edge
NIR

Wavelength (nm)
550
660
735
790

FWHM (nm)
40
40
10
40

which is composed of 4 CMOS sensors with different narrow-band filters in the visible and near infrared domain; the spectral characterisation is detailed in table 1. The
camera comes with a sky sensor (Parrot call it “sunshine sensor”) which measures the
downwelling irradiance at the same spectral bands as the cameras. Each band has its
own sensing setup which is sketched in fig. 3. The four bands are arranged in a 2 × 2
grid in the same way as the cameras. Each band is composed of a diffuser, a spectral
filter and a photo diode. The diffuser diffuses the incoming light in order to provide
a well-defined area for the projection of the incoming ray and to avoid any change in
sensitivity of the photo diode with incoming ray angle. The bandpass filter is identical
to the one used for the camera and placed above the photodiode. The spectral width of
the bandpass filter is small compared to the spectral change in sensitivity of the photodiode and of a pixel of the CMOS sensor, therefore the spectral response shape of the
sky sensor is virtually identical to the one of the camera.
In order to be able to retrieve reflectance without the need for a reflectance target,
each camera unit is calibrated in the factory. This includes independent measurements
of each band for both sky sensor and camera. Cubero-Castan et al. (2018) have shown
a ground-based verification measurement to assess the precision and accuracy of this
camera/sky sensor combination. It involved a calibration target composed of 42 different grey patches (6 rows by 7 columns) whose true reflectance factors were measured
using a spectro-radiometer. An accuracy of 2.5 % for all 4 bands was observed.
Since the red edge band overlaps with a water absorption band, we computed the
absorption due to water vapour using HITRAN data (Gordon et al., 2017), assuming
a homogeneous layer of 100 m thickness (a typical flying height) with 100 % relative
humidity at 300 K and 1013 hPa as a bad-case scenario, and a perfectly Gaussian sensitivity distribution with the centre and FWHM of each band. The resulting absorbances
are 0.02 %, 0.1 %, 1.1 %, and 0.5 %, for the green, red, red edge, and NIR bands, respectively. Even the absorption for the red edge band of just above 1 % is below the
measurement accuracy of the camera.
As reflectance target, we used a 10 cm square white Spectralon R , which is a sintered

9

Figure 4: Rendering of the 3D model of one acquisition taken in clear sky condition as produced by
Pix4Dmapper. In the centre is the box with the spectralon target. The position and orientation of the image
captures is shown as green pyramids. GCPs are shown as green nails, the green, red, and blue arrows show
the orientation of the North, East, and Up axis, respectively, as given by connecting appropriate GCPs.

polytetrafluoroethylene (PTFE) material manufactured by Labsphere and widely used
as standard target. Its reflectance, close to 0.99, can be traced back to a NIST standard.
Bruegge et al. (2001) have acquired a BRF measurement of Spectralon and made it
available online.
The acquisitions have been performed under different light conditions (clear sky
and overcast sky), at an open space at École Polytechnique Fédérale de Lausanne
(EPFL), Lausanne, Switzerland (46◦ 310 6.200 N, 6◦ 330 59.400 E), which is flat and without
any 3D structures close by, in order to prevent reflected light from adjacent objects. The
synthetic scene was comprised of a Spectralon target fixed on top of a box (65 cm ×
45 cm × 25 cm). The surrounding was a planar concrete place. The sky sensor was
mechanically rigidly fixed to the camera, with a known relative angle of (175◦ , 0◦ , 0◦ )
in (x, y, z) directions of the camera reference (i.e. nearly opposite). This is the configuration used for the Sequoia aboard the senseFly eBee, a widely used UAV. The
camera/sky sensor combination was fixed on a stick to avoid shadows by the operator.
Several Ground Control Points (GCPs) were acquired with an RTK (Real Time Kinematic) GPS (Javad, Triumph - LS, accuracy 3 cm absolute) to georeference the scene.
A precise georeferencing is required to compute the correct relative angle between the
sun and the sky sensor. Images were taken at different angles, all around the box, in order to cover well the space of possible sun to sky sensor angles and observation angles.
Figure 4 shows the 3D scene reconstruction of the setup for one of the measurements,
together with a visualisation of the position and orientation of the acquired images.
This also visualises the different observation angles used.
With clear sky, the irradiance is dominated by direct sunlight. Hence, the computed HDRF can be compared to the laboratory-derived Spectralon BRF measurement
by Bruegge et al. (2001) with the irradiance incident angle equal to the sun zenith
angle. Those database measurements were made with a Laser in a laboratory setting.
Originally, the data were used for space-borne measurements, which comprise only
direct sunlight since there is no atmosphere. This differs from our setup with clear sky,
which also comprises a small diffuse part. That diffuse component will make a small
10

Figure 5: Visualisation of the bias correction procedure (here for the NIR data of the 12pm acquisition). A
line (red line) was fitted into the uncorrected multi-grey target data using the nadir views (blue dots). The
resulting function was applied to each data point. Green stars represent the nadir multi-grey target data after
the bias correction.

additional uniform contribution to the reflectance but not disturb the angular response.
Thus the error made by neglecting the diffuse part for estimating the directionality is
small. Measuring the true values in-situ was not feasible, because it is very costly and
difficult to conduct. The database contains the BRF of Spectralon measured with illumination angles of 8◦ , 40◦ , 45◦ , 50◦ , and 55◦ . In order to compare to those values, we
needed to perform our measurements at sun zenith angles covered by the database.
The overcast condition is dominated by diffuse light, therefore for the comparison,
one needs to integrate over all incident angles. The available data from Bruegge et al.
(2001) is very sparse, only comprising five zenith angles. Since all values are constant
within error, we approximated it with a constant function in order to do the integration.
Clear sky acquisitions were performed on 21 September 2017, at 11am, 12pm,
3pm, and 4pm local time, with zenith angles of 53◦ , 49◦ , 51◦ , and 55◦ , respectively, to
match the laboratory measurements for 50◦ and 55◦ . Overcast sky acquisitions were
performed on the 11 January 2018 at 9:30am local time.
Photogrammetric and radiometric image processing was done with a development
version of Pix4Dmapper 4.32 . Image orientation was computed with advanced aerial
triangulation (AAT) and bundle block adjustment (BBA), the project was georeferenced, a point cloud was generated, and the images were radiometrically corrected to
HDRF. The sky sensor orientation was computed from the photogrammetrically derived camera orientation using known relative angles which were adjusted during the
direct sunlight regression. Besides HDRF, the outputs also included the observation
angles for each image and the direct sunlight ratio. Further evaluation steps and plotting were done with Python.
The Sequoia camera is designed and calibrated to work with vegetation, which
has typical reflectance of up to 0.5, but not for materials with a reflectance as high as
Spectralon. In clear sky, this high reflectance brings the camera to its limits: all green
frames and part of the red frames are saturated. Due to that and the way the auto2 https://pix4d.com/product/pix4dmapper-photogrammetry-software/

11

Downwelling irradiance (a.u.)

Downwelling irradiance (a.u.)

Angle sky sensor - sun (degree)

(a) Clear sky

(b) Overcast sky

Figure 6: Evolution of measured downwelling irradiance (here for the red band) with respect to the relative
angle between the sky sensor and the sun for clear sky (a) and with respect to time for overcast sky (b),
both in the same arbitrary irradiance units. For clear sky (a), the irradiance fit (red line) is compared to the
at-sensor irradiance measurement (blue dots). For overcast sky (b), the at-object irradiance (i.e. corrected for
angle effects) (red line) is compared to both the at-sensor irradiance measurement (blue dots) and the ground
reference measurement (grey line).

exposure of the cameras works, we needed to manually set the exposure time of all
bands to its lowest possible value (between 0.3 ms and 0.023 ms). However, with such
an extreme setting, the camera calibration does not seems to be valid any more, since
we observed radiometric artefacts in the form of a bias in radiance (see also CuberoCastan et al. (2018)). Therefore we removed the bias using a separate known target
composed of 42 different grey patches (6 rows by 7 columns) (“multi-grey target”)
placed close to the Spectralon. The true reflectance of each patch was measured with
the spectro-radiometer viewing nadir. Images taken nadir were selected and the HDRF
was estimated for each patch. Then a linear regression between the estimated and the
true HDRF was ran. This fit resulted in a linear transformation that was used to correct
the reflectance for all Spectralon measurements. The procedure is visualised in fig. 5.
This was possible for most of the clear sky acquisitions.
Except if mentioned otherwise, all four bands behave similarly. Thus by default,
we show detailed results only for one band and report a summary for all bands.
5. Results and discussion
5.1. Direct sunlight ratio estimation
One of the key components to obtain accurate HDRF is the direct sunlight ratio
ε. Depending on the sky condition, the typical behaviour of the sky illumination is
different. In this study, we limit ourselves to clear sky and overcast sky.
The relative angle between the camera and the sky sensor was optimised for clear
sky conditions. For overcast sky, the angle dependency of the downwelling irradiance
is very low and the available data do not constrain the angle well enough to adjust it in
the fit, but small inaccuracies of the relative angles also do not have an effect.

12

Figure 7: Evolution of direct sunlight ratio with wavelength in clear sky conditions for each of the 4 acquisitions.

Figure 6 shows the regression for clear sky and overcast for the red band. For
clear sky, the downwelling irradiance is high and since most of the irradiance is direct
sunlight, a strong relationship between measured irradiance and relative angle between
the sky sensor and the sun is observed. The fit characterised well the angle dependency
of the measurement. With angles higher than 90◦ , the sky sensor is only exposed to
scattered light (flat line with between 5 % to 10 % of the total irradiance). For short
times like the acquisitions performed for this study, the total downwelling irradiance is
approximately constant in time. Thus the total downwelling irradiance retrieved from
the fit can be used for the whole acquisition.
For overcast conditions, the downwelling irradiance is relatively low due to absorption in the clouds. Light that is coming to the ground has been scattered many times
in the clouds and therefore is nearly entirely diffuse. As expected, the fitted direct
sunlight ratio ε is close to 0 (0.06 for Green, 0.04 for Red and 0.07 for Red Edge and
NIR). The total downwelling irradiance is a smoothly changing irregular function of
time, because the thickness of the cloud layer changes over time. Therefore the sky
sensor correction has to be computed per frame.
We placed an additional sky sensor beside the target to compare the temporal evolution of the reconstructed at-object downwelling irradiance with a real measurement
(grey line in fig. 6 (b)). For most measurements, the at-sensor irradiance is below
the ground reference, while nadir measurements fit nearly perfectly. As discussed in
section 3, if the sky sensor is not pointing zenith, it measures irradiance only from a
fraction of the sky, which is taken into account in our downwelling irradiance computation. The corrected at-object downwelling irradiance (red line in fig. 6 (b)) matches
better with the real temporal change. Both reference and corrected irradiances decrease
between 35:55 and 38:25 and increase between 38:55 and 40:25. A slight bias is visible
between the measurement and the reference that could be attributed to a limitation of
our overcast model. In real conditions, the scattered light is not uniformly distributed
over the hemisphere as assumed in our model. The implication of this misestimation
for the HDRF computation is explained in section 5.2.2.
On both weather conditions, the total downwelling irradiance is well retrieved and
used to obtain the HDRF of Spectralon.

13

In clear sky, the direct sunlight ratio is expected to vary with wavelength: blue
light is scattered more than red light (it is dominated by Rayleigh scattering, whose
cross section is proportional λ −4 , which is why the sky is blue). The estimated direct
sunlight ratio as a function of wavelength is shown in fig. 7. The value for the green
band at 12pm is not shown because the sun angle fitting algorithm did not converge for
this particular measurement. A clear wavelength dependency with increasing values for
increasing wavelength is observed, which corresponds well to the expected behaviour.
No clear relationship of the direct sunlight ratio with respect to the sun zenith angle
is observed, as the range of acquired sun zenith angle is too small to notice such relationship.
5.2. Spectralon HDRF estimation
Three metrics were computed to assess the quality of the multi-angular HDRF estimation: the Pearson correlation coefficient (PCC), the Root Mean Square Error
s
1 N 2
(17)
RMSE =
∑ Ei
N i=1
and the Standard Deviation Error
s
1 N
SDE =
∑ (Ei − E)2 ,
N − 1 i=1

(18)

where the error Ei is defined as the difference in HDRF between the result of our retrieval method and the literature value (HDRF from the database for clear sky, constant
across all viewing angles for overcast) and E denotes the mean error. The RMSE is
computed to estimate the global accuracy of the computation. The SDE is a measure
for the spread of the error for the different data points and is used to assess the validity
of the method: if the SDE is in the same order of magnitude as the RMSE, the error is
random. If it is much lower, there is an approximately a constant offset.
Different Spectralon targets have slightly different characteristics. We quantified
the error introduced by the difference between our Spectralon and the Spectralon used
by Bruegge et al. (2001) by comparing the directional-hemispherical reflectance factor
(DHRF) with 8◦ incident angle. The value at 632.8 nm for the unit used by Bruegge
et al. (2001) is equal to 0.991 (as measured with a laser). The respective value for
our Spectralon (derived from the DHRF provided with the calibration certificate from
Labsphere) is equal to 0.9896, resulting in a difference of around 0.1 %. This is way
beyond the measurement uncertainty of our setup and can be neglected.
We also investigated the spectral variation of the four different bands. From the
8◦ DHRF data provided by Labsphere, the spectral variation is around 0.1 %. Chrien
and Bruegge (1998) measured the spectral difference in BRF for incident angles of 55◦
and 50◦ and found differences of up to 2.5 % for high viewing angles (50◦ ), which is
equal to the measurement accuracy of the cameras. This last error is the main source
of difference and explains a possible bias of computed reflectance between different
bands.

14

(b) HDRF as a function of the viewing
angle

(a) Comparison with the literature values

Figure 8: Comparison of the estimated HDRF of Spectralon with the laboratory measurement by Bruegge
et al. (2001) for the NIR band of the measurement at 12pm. (a) true vs. the estimated values with the ideal
1:1 case shown as grey line. (b) Visualisation of the HDRF as a function of the viewing angle in solar
coordinates, with the literature BRF values in the background and the values retrieved by our measurement
as coloured dots. The concentric circles and dotted lines represent the θ and ϕ viewing angles, respectively
(θ between 0◦ and 50◦ , and ϕ between 0◦ and 360◦ ).

5.2.1. Clear sky data analysis
For the clear sky data acquisition, the HDRF of Spectralon is computed using Esun ,
the total downwelling irradiance projected onto the object of interest (cf. eq. (13)). It is
then compared with the BRF database. We use the data with an incident angle of 55◦
for the acquisitions at 11am and 4pm (solar zenith angle at 53◦ and 55◦ , respectively)
and an incident angle of 50◦ for the acquisitions at 12pm and 3pm (solar zenith angle
at 49◦ and 51◦ , respectively).
The three metrics used to characterise the quality of the multi-angular HDRF retrieval are summarised in table 2 for all acquisitions except the 11am acquisition where
the multi-grey target was not available in the frames. For this particular acquisition,
only PCC and SDE were computed.
A strong correlation between the reference and the computed HDRF was found,
with a correlation coefficient between 0.81 and 0.94 for Red, between 0.87 and 0.97
for Red Edge and between 0.83 and 0.93 for NIR. This shows that the bi-directionality
of the Spectralon is well retrieved. The RMSE is varying per acquisition but is quite
constant per band when considering a single acquisition. It is between 1.6 % and 6.1 %
which corresponds to the errors both of the model and the camera. Even for the measurement with largest bias (the one at 3pm), the HDRF estimation is strongly correlated
with the true value, with a correlation coefficient between 0.94 and 0.97. The SDE
is ranging between 1.0 % and 1.7 %. These values will be used as reference for the
overcast acquisition analysis.
The band that correlates best with the laboratory measurement is red edge, for most
of the acquisitions. This might be due to the small bandwidth that allowed us to acquire
the data at a higher exposure time, where the camera performs better.

15

Table 2: The Root Mean Square Error (RMSE) and the Standard Deviation Error (SDE) between the estimated and true HDRF for the clear sky acquisitions.

Time - Solar Zenith Angle
11am - 53◦

12pm - 49◦

3pm - 51◦

4pm - 55◦

Band
Red
Red edge
NIR
Red
Red edge
NIR
Red
Red edge
NIR
Red
Red edge
NIR

RMSE (%)
1.79
1.60
1.60
6.04
4.42
5.39
1.61
2.05
3.20

SDE (%)
1.29
1.32
1.51
1.58
1.18
1.52
1.04
0.97
1.26
1.62
1.42
1.69

PCC
0.91
0.92
0.90
0.89
0.91
0.86
0.94
0.97
0.93
0.81
0.87
0.83

Table 3: The Root Mean Square Error (RMSE) and the Standard Deviation Error (SDE) for the overcast
acquisition, both for using the sky sensor fixed to the camera and for using the ground sky sensor to compute
the HDRF.

Band
Green
Red
Red edge
NIR

embedded sky sensor
RMSE (%)
SDE (%)
3.65
3.63
4.54
3.36
4.25
3.63
5.04
3.67

ground sky sensor
RMSE (%)
SDE (%)
1.37
0.92
3.46
1.20
2.46
1.05
3.59
1.14

A visualisation of the estimated HDRF with respect to the literature laboratory
measurement is presented in fig. 8 for the NIR band of the 12pm acquisition. ϕ =
180◦ and θ = 50◦ represent the direct solar reflection. As discussed above, the bidirectionality of the reflectance is well recovered and matches the laboratory measurements.
5.2.2. Overcast sky data analysis
For the overcast sky acquisition, the HDRF of Spectralon is expected to be constant
for all viewing angles. That is characteristic for an almost Lambertian surface like
Spectralon.
In addition to the setup for the clear sky acquisition, an additional sky sensor is
located close to the target to measure the at-target downwelling irradiance. This allows
to compute an HDRF using the measured at-target downwelling irradiance instead of
using the at-sensor downwelling irradiance and back-computing the at-target downwelling irradiance. While this is not a typical use case for UAV acquisitions, it allows
us to better evaluate the influence of the sky model and the angle correction on the
HDRF estimation.
Figure 9 shows the estimated HDRF for both approaches. The RMSE and SDE for

16

(a) Estimated HDRF as a function of
time

(b) HDRF as a function of the viewing
angle

Figure 9: Comparison of the computed HDRF of Spectralon with the laboratory measurement as reference.
(a) Estimated HDRF as a function of time, with blue dots showing values retrieved using the sky sensor fixed
on the camera and orange dots using the ground sky sensor. (b) Visualisation of the HDRF as a function
of viewing angles, with the reference value in the background and the retrieved values using the ground
sunshine sensor as coloured dots.

both evaluations is shown in table 3. The PCC could not be computed because the true
HDRF is constant across the viewing angles.
The true HDRF is well retrieved, with RMSE ranging from 1.3 % to 5.0 %. Looking
at the SDE, the at-ground measurement yields better results, decreasing SDE from
3.5 % for using the sky sensor fixed on the camera to 1.0 % for using the ground sky
sensor, which is closer to the SDE computed for clear sky. This is attributed to the
assumption of a uniform diffuse illumination. Due to moving clouds with different
thickness, the real illumination is irregular. If the sky sensor is oriented nadir and
measures the whole hemisphere, the effective downwelling light is well retrieved. Once
the sky sensor is orientated at high angles, a certain portion of the sky is not seen and
its contribution to the total downwelling irradiance must be estimated using the value
measured in the visible part of the sky. However the real angular distribution of the
downwelling irradiance is difficult to model without measuring the cloud distribution
(for example by mapping the clouds). This is expected to be less problematic for real
flight datasets, since images are typically acquired more or less nadir.
6. Conclusions
We presented a new physically based method to estimate the HDRF from images of
lightweight multispectral cameras with a downwelling irradiance sensor aboard UAVs
which combines radiometry with photogrammetric computer vision to derive geometrically and radiometrically accurate data purely from the images. It does not require
calibration targets, works both in clear sky and overcast conditions, and allows to capture the directionality of the reflectance factor. The method works for any calibrated
multispectral camera with a sky sensor that is rigidly connected to the camera. It was

17

tested with a ground acquisition of a well-characterised Spectralon target. We used
the Parrot Sequoia with the downwelling irradiance sensor fixed opposite the camera,
a configuration typically found on UAVs, and acquired data with different orientation
all around the target to obtain different viewing angles in clear sky and overcast conditions. A good agreement of the HDRF estimated with our method for the different
viewing directions with literature values was found, with an RMSE of 3.0 % for clear
sky and 2.7 % for overcast sky. For overcast sky, the assumption of isotropic scattered
light was found to lead to a small additional error for non-nadir measurements, which
increases the error to a maximum of 5 %. Our measurement shows that our method
is capable of retrieving correct HDRF in various conditions. Future research should
check the performance for real flight data and study the dependency of heterogeneity
of scene materials or terrains slope in the retrieved HDRF.
References
References
Bendig, J., Yu, K., Aasen, H., Bolten, A., Bennertz, S., Broscheit, J., Gnyp, M. L.,
Bareth, G., 2015. Combining UAV-based plant height from crop surface models,
visible, and near infrared vegetation indices for biomass monitoring in barley. International Journal of Applied Earth Observation and Geoinformation 39, 79–87.
Bruegge, C., Chrien, N., Haner, D., 2001. A spectralon BRF data base for MISR calibration applications. Remote Sensing of Environment 76, 354–366.
Chen, J., Menges, C., Leblanc, S., 2005. Global mapping of foliage clumping index
using multi-angular satellite data. Remote Sensing of Environment 97 (4), 447–457.
Chrien, N., Bruegge, C., Apr. 1998. BRF measurements made for MISR OBC. techreport MISR SCIENCE DFM #153, Jet propulsion Laboratory.
Colby, J. D., 1991. Topographic normalization in rugged terrain. Photogrammetric Engineering and Remote Sensing 57 (5), 531–537.
Colomina, I., Molina, P., 2014. Unmanned aerial systems for photogrammetry and
remote sensing: A review. ISPRS Journal of Photogrammetry and Remote Sensing
92, 79 – 97.
Cubero-Castan, M., Schneider-Zapp, K., Bellomo, M., Shi, D., Rehak, M., Strecha, C.,
Sep. 2018. Assessment of the radiometric accuracy in a targetless workflow using
pix4d software. 2018 9th Workshop on Hyperspectral Image and Signal Processing:
Evolution in Remote Sensing (WHISPERS).
Darula, S., Kittler, R., 09 2002. CIE general sky standard defining luminance distributions. In: Proc. Conf. eSim 2002 The Canadian conference on building energy
simulation.

18

Geipel, J., Link, J., Claupein, W., 2014. Combined spectral and spatial modeling of
corn yield based on aerial images and crop surface models acquired with an unmanned aircraft system. Remote Sensing 6 (11), 10335.
Gordon, I., Rothman, L., Hill, C., Kochanov, R., Tan, Y., Bernath, P., Birk, M., Boudon,
V., Campargue, A., Chance, K., Drouin, B., Flaud, J.-M., Gamache, R., Hodges, J.,
Jacquemart, D., Perevalov, V., Perrin, A., Shine, K., Smith, M.-A., Tennyson, J.,
Toon, G., Tran, H., Tyuterev, V., Barbe, A., Csaszar, A., Devi, V., Furtenbacher, T.,
Harrison, J., Hartmann, J.-M., Jolly, A., Johnson, T., Karman, T., Kleiner, I., Kyuberis, A., Loos, J., Lyulin, O., Massie, S., Mikhailenko, S., Moazzen-Ahmadi, N.,
Muller, H., Naumenko, O., Nikitin, A., Polyansky, O., Rey, M., Rotger, M., Sharpe,
S., Sung, K., Starikova, E., Tashkun, S., Auwera, J. V., Wagner, G., Wilzewski, J.,
Wcislo, P., Yu, S., Zak, E., 2017. The HITRAN2016 molecular spectroscopic database. Journal of Quantitative Spectroscopy and Radiative Transfer 203, 3–69.
Hakala, T., Suomalainen, J., Peltoniemi, J. I., 2010. Acquisition of bidirectional reflectance factor dataset using a micro unmanned aerial vehicle and a consumer camera.
Remote Sensing 2 (3), 819–832.
Honkavaara, E., Khoramshahi, E., 2018. Radiometric correction of close-range spectral
image blocks captured using an unmanned aerial vehicle with a radiometric block
adjustment. Remote Sensing 10 (2).
Jähne, B., 2012. Digital Image Processing, 7th Edition. Springer, Heidelberg.
Jakob, S., Zimmermann, R., Gloaguen, R., 2017. The need for accurate geometric and
radiometric corrections of drone-borne hyperspectral data for mineral exploration:
MEPHySTo – a toolbox for pre-processing drone-borne hyperspectral data. Remote
Sensing 9 (1).
Lelong, C. C. D., Burger, P., Jubelin, G., Roux, B., Labbé, S., Baret, F., 2008. Assessment of unmanned aerial vehicles imagery for quantitative monitoring of wheat crop
in small plots. Sensors (8), 3557–3585.
Li, D. H., Lou, S., Lam, J. C., Wu, R. H., 2016. Determining solar irradiance on inclined planes from classified CIE (international commission on illumination) standard skies. Energy 101, 462 – 470.
Matese, A., Toscano, P., Di Gennaro, S. F., Genesio, L., Vaccari, F. P., Primicerio,
J., Belli, C., Zaldei, A., Bianconi, R., Gioli, B., 2015. Intercomparison of UAV, aircraft and satellite remote sensing platforms for precision viticulture. Remote Sensing
7 (3), 2971–2990.
McCluney, W. R., 2014. Introduction to radiometry and photometry, 2nd Edition.
Artech House, Boston.
Nebiker, S., Lack, N., Abächerli, M., Läderach, S., 2016. Light-weight multispectral
UAV sensors and their capabilities for predicting grain yield and detecting plant
diseases. The International Archives of the Photogrammetry, Remote Sensing and
Spatial Information Sciences XLI-B1.
19

Page, S. E., Siegert, F., Rieley, J. O., Boehm, H.-D. V., Jaya, A., Limin, S., 2002.
The amount of carbon released from peat and forest fires in indonesia during 1997.
Nature 420 (6911), 61–65.
Roujean, J.-L., Leroy, M., Deschamps, P.-Y., 1992. A bidirectional reflectance model of
the earth’s surface for the correction of remote sensing data. Journal of Geophysical
Research: Atmospheres 97 (D18), 20455–20468.
Sakamoto, T., Gitelson, A. A., Arkebauer, T. J., 2014. Near real-time prediction of U.S.
corn yields based on time-series MODIS data. Remote Sensing of Environment 147,
219–231.
Schaepman, M. E., 2007. Spectrodirectional remote sensing: From pixels to processes.
International Journal of Applied Earth Observation and Geoinformation 9 (2), 204–
223.
Schaepman-Strub, G., Schaepman, M., Painter, T., Dangel, S., Martonchik, J., 2006.
Reflectance quantities in optical remote sensing – definitions and case studies. Remote Sensing of Environment 103 (1), 27 – 42.
Schläpfer, D., Nieke, J., 2005. Operational simulation of at sensor radiance sensitivity
using the modo/modtran4 environment. In: Proceedings EARSeL Fourth Workshop
on Imaging Spectroscopy, Warsaw, Poland. pp. 611–619.
Schott, J. R., 2007. Remote sensing. The chain approach., 2nd Edition. Oxford university press, Oxford.
Sola, I., González-Audícana, M., Álvarez Mozos, J., 2016. Multi-criteria evaluation of
topographic correction methods. Remote Sensing of Environment 184, 247 – 262.
Wang, C., Myint, S. W., May 2015. A simplified empirical line method of radiometric
calibration for small unmanned aircraft systems-based remote sensing. IEEE Journal
of Selected Topics in Applied Earth Observations and Remote Sensing 8 (5), 1876–
1885.
Widlowski, J., Pinty, B., Gobron, N., Verstraete, M., Diner, D., Davis, A., 2004. Canopy structure parameters derived from multi-angular remote sensing data for terrestrial carbon studies. Climatic Change 67 (2-3), 403–415.

20

