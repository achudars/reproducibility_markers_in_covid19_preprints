1

Deep Multi-modal Object Detection and Semantic
Segmentation for Autonomous Driving: Datasets,
Methods, and Challenges

arXiv:1902.07830v4 [cs.RO] 8 Feb 2020

Di Feng∗,1,2,† , Christian Haase-Schütz∗,3,4 , Lars Rosenbaum1 , Heinz Hertlein3 , Claudius Gläser1 , Fabian Timm1 ,
Werner Wiesbeck4 , Klaus Dietmayer2

Abstract—Recent advancements in perception for autonomous
driving are driven by deep learning. In order to achieve robust
and accurate scene understanding, autonomous vehicles are
usually equipped with different sensors (e.g. cameras, LiDARs,
Radars), and multiple sensing modalities can be fused to exploit
their complementary properties. In this context, many methods
have been proposed for deep multi-modal perception problems.
However, there is no general guideline for network architecture
design, and questions of “what to fuse”, “when to fuse”, and
“how to fuse” remain open. This review paper attempts to systematically summarize methodologies and discuss challenges for
deep multi-modal object detection and semantic segmentation in
autonomous driving. To this end, we first provide an overview of
on-board sensors on test vehicles, open datasets, and background
information for object detection and semantic segmentation in
autonomous driving research. We then summarize the fusion
methodologies and discuss challenges and open questions. In the
appendix, we provide tables that summarize topics and methods.
We also provide an interactive online platform to navigate each
reference: https://boschresearch.github.io/multimodalperception/.
Keywords—multi-modality, object detection, semantic segmentation, deep learning, autonomous driving

RGB Image

0.96

0.99

Map

Radar Points

LiDAR Points

0.96

0.8
0.94

0.99
0.98

Vehicle

Person

Road sign

Traffic light

Fig. 1: A complex urban scenario for autonomous driving. The
driverless car uses multi-modal signals for perception, such as
RGB camera images, LiDAR points, Radar points, and map
information. It needs to perceive all relevant traffic participants
and objects accurately, robustly, and in real-time. For clarity,
only the bounding boxes and classification scores for some
objects are drawn in the image. The RGB image is adapted
from [4].

I. I NTRODUCTION
Significant progress has been made in autonomous driving
since the first successful demonstration in the 1980s [1] and
the DARPA Urban Challenge in 2007 [2]. It offers high
potential to decrease traffic congestion, improve road safety,
and reduce carbon emissions [3]. However, developing reliable
autonomous driving is still a very challenging task. This is
because driverless cars are intelligent agents that need to
perceive, predict, decide, plan, and execute their decisions in
the real world, often in uncontrolled or complex environments,
such as the urban areas shown in Fig. 1. A small error in the
system can cause fatal accidents.
Perception systems in driverless cars need to be (1). accurate: they need to give precise information of driving envi∗

Di Feng and Christian Haase-Schütz contributed equally to this work.
Driver Assistance Systems and Automated Driving, Corporate Research,
Robert Bosch GmbH, 71272 Renningen, Germany.
2 Institute of Measurement, Control and Microtechnology, Ulm University,
89081 Ulm, Germany.
3 Engineering Cognitive Systems, Automated Driving, Chassis Systems
Control, Robert Bosch GmbH, 74232 Abstatt, Germany.
4 Institute of Radio Frequency Engineering and Electronics, Karlsruhe
Institute of Technology, 76131 Karlsruhe, Germany.
† Corresponding author: Di.Feng@de.bosch.com
1

ronments; (2). robust: they should work properly in adverse
weather, in situations that are not covered during training
(open-set conditions), and when some sensors are degraded or
even defective; and (3). real-time: especially when the cars are
driving at high speed. Towards these goals, autonomous cars
are usually equipped with multi-modal sensors (e.g. cameras,
LiDARs, Radars), and different sensing modalities are fused
so that their complementary properties are exploited (cf. Sec.
II-A). Furthermore, deep learning has been very successful
in computer vision. A deep neural network is a powerful
tool for learning hierarchical feature representations given a
large amount of data [5]. In this regard, many methods have
been proposed that employ deep learning to fuse multi-modal
sensors for scene understanding in autonomous driving. Fig. 2
shows some recently published methods and their performance
on the KITTI dataset [6]. All methods with the highest performance are based on deep learning, and many methods that fuse
camera and LiDAR information produce better performance
than those using either LiDAR or camera alone. In this paper,
we focus on two fundamental perception problems, namely,
object detection and semantic segmentation. In the rest of
this paper, we will call them deep multi-modal perception

©2020 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing
this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this
work in other works. DOI: 10.1109/TITS.2020.2972974.

2

0.9

HDNET ContFuse F-PointNet
AVOD
PointRCNN

0.8

PIXOR

MV3D(LiDAR)
VoxelNet

AVOD-FPN

MV3D

F-PC_CNN

LiDAR+Camera

Camera

A3DODWTDA
TopNet-HighRes

0.7

Average Precision

B. Contributions

LiDAR

PointPillars

TopNet-DecayRate

3D-FCN
(inference time>5s)

0.6
BirdNet

0.5

Pseudo-LiDAR

0.4

0.3
0.2
A3DODWTDA(image)

MonoFusion

OFT-NET

0.1
3D-SSMFCNN

0.0
0

100

200

300

400

500

600

700

800

Reported Runtime (ms)

Fig. 2: Average precision (AP) vs. runtime. Visualized are deep
learning approaches that use LiDAR, camera, or both as inputs
for car detection on the KITTI bird’s eye view test dataset.
Moderate APs are summarized. The results are mainly based
on the KITTI leader-board [6] (visited on Apr. 20, 2019). On
the leader-board only the published methods are considered.
unless mentioned otherwise.
When developing methods for deep multi-modal object
detection or semantic segmentation, it is important to consider
the input data: Are there any multi-modal datasets available
and how is the data labeled (cf. Tab. II)? Do the datasets
cover diverse driving scenarios (cf. Sec. VI-A1)? Is the data
of high quality (cf. Sec. VI-A2)? Additionally, we need to
answer several important questions on designing the neural
network architecture: Which modalities should be combined
via fusion, and how to represent and process them properly
(“What to fuse” cf. Sec. VI-B1)? Which fusion operations and
methods can be used (“How to fuse” cf. Sec. VI-B2)? Which
stage of feature representation is optimal for fusion (“When
to fuse” cf. Sec. VI-B2)?

A. Related Works
Despite the fact that many methods have been proposed
for deep multi-modal perception in autonomous driving, there
is no published summary examining available multi-modal
datasets, and there is no guideline for network architecture
design. Yin et al. [7] summarize 27 datasets for autonomous
driving that were published between 2006 and 2016, including the datasets recorded with a single camera alone or
multiple sensors. However, many new multi-modal datasets
have been released since 2016, and it is worth summarizing
them. Ramachandram et al. [8] provide an overview on deep
multi-modal learning, and mention its applications in diverse
research fields, such as robotic grasping and human action
recognition. Janai et al. [9] conduct a comprehensive summary
on computer vision problems for autonomous driving, such
as scene flow and scene construction. Recently, Arnold et
al. [10] survey the 3D object detection problem in autonomous
driving. They summarize methods based on monocular images
or point clouds, and briefly mention some works that fuse
vision camera and LiDAR information.

To the best of our knowledge, there is no survey that
focuses on deep multi-modal object detection (2D or 3D)
and semantic segmentation for autonomous driving, which
makes it difficult for beginners to enter this research field.
Our review paper attempts to narrow this gap by conducting a
summary of newly-published datasets (2013-2019), and fusion
methodologies for deep multi-modal perception in autonomous
driving, as well as by discussing the remaining challenges and
open questions.
We first provide background information on multi-modal
sensors, test vehicles, and modern deep learning approaches
in object detection and semantic segmentation in Sec. II. We
then summarize multi-modal datasets and perception problems
in Sec. III and Sec. IV, respectively. Sec. V summarizes
the fusion methodologies regarding “what to fuse”, “when to
fuse” and “how to fuse”. Sec. VI discusses challenges and
open questions when developing deep multi-modal perception
systems in order to fulfill the requirements of “accuracy”,
“robustness” and “real-time”, with a focus on data preparation
and fusion methodology. We highlight the importance of
data diversity, temporal and spatial alignment, and labeling
efficiency for multi-modal data preparation. We also highlight the lack of research on fusing Radar signals, as well
as the importance of developing fusion methodologies that
tackle open dataset problems or increase network robustness.
Sec. VII concludes this work. In addition, we provide an
interactive online platform for navigating topics and methods
for each reference. The platform can be found here: https:
//boschresearch.github.io/multimodalperception/.
II. BACKGROUND
This section provides the background information for deep
multi-modal perception in autonomous driving. First, we
briefly summarize typical automotive sensors, their sensing
modalities, and some vehicles for test and research purposes. Next, we introduce deep object detection and semantic
segmentation. Since deep learning has most-commonly been
applied to image-based signals, here we mainly discuss imagebased methods. We will introduce other methods that process
LiDAR and Radar data in Sec. V-A. For a more comprehensive
overview on object detection and semantic segmentation, we
refer the interested reader to the review papers [11], [12]. For a
complete review of computer vision problems in autonomous
driving (e.g. optical flow, scene reconstruction, motion estimation), cf. [9].
A. Sensing Modalities for Autonomous Driving
1) Visual and Thermal Cameras: Images captured by visual
and thermal cameras can provide detailed texture information
of a vehicle’s surroundings. While visual cameras are sensitive
to lighting and weather conditions, thermal cameras are more
robust to daytime/nighttime changes as they detect infrared
radiation that relates to heat from objects. However, both types
of cameras however cannot directly provide depth information.

3

2) LiDARs: LiDARs (Light Detection And Ranging) give
accurate depth information of the surroundings in the form
of 3D points. They measure reflections of laser beams which
they emit with a certain frequency. LiDARs are robust to
different lighting conditions, and less affected by various
weather conditions such as fog and rain than visual cameras.
However, typical LiDARs are inferior to cameras for object
classification since they cannot capture the fine textures of
objects, and their points become sparse with distant objects.
Recently, flash LiDARs were developed which can produce detailed object information similar to camera images. Frequency
Modulated Continuous Wave (FMCW) LiDARs can provide
velocity information.
3) Radars: Radars (Radio Detection And Ranging) emit
radio waves to be reflected by an obstacle, measures the
signal runtime, and estimates the object’s radial velocity by
the Doppler effect. They are robust against various lighting
and weather conditions, but classifying objects via Radars
is very challenging due to their low resolution. Radars are
often applied in adaptive cruise control (ACC) and traffic jam
assistance systems [13].
4) Ultrasonics: Ultrasonic sensors send out high-frequency
sound waves to measure the distance to objects. They are
typically applied for near-range object detection and in low
speed scenarios, such as automated parking [13]. Due to
the sensing properties, Ultrasonics are largely affected by air
humidity, temperature, or dirt.
5) GNSS and HD Maps: GNSS (Global Navigation Satellite Systems) provide accurate 3D object positions by a global
satellite system and the receiver. Examples of GNSS are GPS,
Galileo and GLONASS. First introduced to automotive as
navigation tools in driver assistance functions [13], currently
GNSS is also used together with HD Maps for path planning
and ego-vehicle localization for autonomous vehicles.
6) IMU and Odometers: Unlike sensors discussed above
which capture information in the external environment (i.e.
“exteroceptive sensors”), Inertial Measurement Units (IMU)
and odometers provide vehicles’ internal information (i.e.
“proprioceptive sensors”) [13]. IMU measure the vehicles’
accelerations and rotational rates, and odometers the odometry.
They have been used in vehicle dynamic driving control
systems since the 1980s. Together with the exteroceptive
sensors, they are currently used for accurate localization in
autonomous driving.
B. Test Vehicle Setup
Equipped with multiple sensors introduced in Sec. II-A,
many autonomous driving tests have been conducted. For
example, the Tartan Racing Team developed an autonomous
vehicle called “Boss” and won the DARPA Urban Challenge
in 2007 (cf. Fig. 3(a)) [2]. The vehicle was equipped with a
camera and several Radars and LiDARs. Google (Waymo) has
tested their driverless cars in more than 20 US cities driving
8 million miles on public roads (cf. Fig. 3(b)) [14]; BMW
has tested autonomous driving on highways around Munich
since 2011 [15]; Daimler mounted a stereo camera, two mono
cameras, and several Radars on a Mercedes Benz S-Class car

(a)

(b)

Fig. 3: (a) The Boss autonomous car at DARPA 2007 [2], (b)
Waymo self-driving car [14].

to drive autonomously on the Bertha Benz memorial route in
2013 [16]. Our interactive online platform provides a detailed
description for more autonomous driving tests, including Uber,
Nvidia, GM Cruise, Baidu Apollo, as well as their sensor
setup.
Besides driving demonstrations, real-world datasets are crucial for autonomous driving research. In this regard, several
research projects use data vehicles with multi-modal sensors to
build open datasets. These data vehicles are usually equipped
with cameras, LiDARs and GPS/IMUs to collect images, 3D
point clouds, and vehicle localization information. Sec. III
provides an overview of multi-modal datasets in autonomous
driving.
C. Deep Object Detection
Object detection is the task of recognizing and localizing
multiple objects in a scene. Objects are usually recognized
by estimating a classification probability and localized with
bounding boxes (cf. Fig. 1). Deep learning approaches have
set the benchmark on many popular object detection datasets,
such as PASCAL VOC [17] and COCO [18], and have been
widely applied in autonomous driving, including detecting
traffic lights [19]–[22], road signs [23]–[25], people [26]–[28],
or vehicles [29]–[33], to name a few. State-of-the-art deep
object detection networks follow one of two approaches: the
two-stage or the one-stage object detection pipelines. Here we
focus on image-based detection.
1) Two-stage Object Detection: In the first stage, several
class-agnostic object candidates called regions of interest
(ROI) or region proposals (RP) are extracted from a scene.
Then, these candidates are verified, classified, and refined in
terms of classification scores and locations. OverFeat [34] and
R-CNN [35] are among pioneering works that employ deep
learning for object detection. In these works, ROIs are first
generated by the sliding window approach (OverFeat [34])
or selective search (R-CNN [35]) and then advanced into a
regional CNN to extract features for object classification and
bounding box regression. SPPnet [36] and Fast-RCNN [37]
propose to obtain regional features directly from global feature
maps by applying a larger CNN (e.g. VGG [38], ResNet [39],
GoogLeNet [40]) on the whole image. Faster R-CNN [41]
unifies the object detection pipeline and adopts the Region
Proposal Network (RPN), a small fully-connected network, to
slide over the high-level CNN feature maps for ROI generation

4

Region Proposal Network (RPN)
Objectness
classification
Bounding box
regression

Faster-RCNN Header network
ROI generation

Bounding box
refinement

Input image
Pre-processing Network
(ResNet, VGG, GoogLeNet etc.)

Object class
probability score
For each ROI

Fig. 4: The Faster R-CNN object detection network. It consists
of three parts: a pre-processing network to extract highlevel image features, a Region Proposal Network (RPN) that
produces region proposals, and a Faster-RCNN head which
fine-tunes each region proposal.

(cf. Fig. 4). Following this line, R-FCN [42] proposes to
replace fully-connected layers in an RPN with convolutional
layers and builds a fully-convolutional object detector.
2) One-stage Object Detection: This method aims to map
the feature maps directly to bounding boxes and classification
scores via a single-stage, unified CNN model. For example,
MultiBox [43] predicts a binary mask from the entire input
image via a CNN and infers bounding boxes at a later
stage. YOLO [44] is a more complete unified detector which
regresses the bounding boxes directly from the CNN model.
SSD [45] handles objects with various sizes by regressing
multiple feature maps of different resolution with small convolutional filters to predict multi-scale bounding boxes.
In general, two-stage object detectors like Faster-RCNN
tend to achieve better detection accuracy due to the region
proposal generation and refinement paradigm. This comes with
the cost of higher inference time and more complex training.
Conversely, one-stage object detectors are faster and easier to
be optimized, yet under-perform compared to two-stage object
detectors in terms of accuracy. Huang et al. [46] systematically evaluate the speed/accuracy trade-offs for several object
detectors and backbone networks.

pixel-wise semantic segmentation for multiple classes including road, car, bicycle, column-pole, tree, sky, etc; [52] and [63]
concentrate on road segmentation; and [51], [64], [65] deal
with instance segmentation for various traffic participants.
Similar to object detection introduced in Sec. II-C, semantic
segmentation can also be classified into two-stage and onestage pipelines. In the two-stage pipeline, region proposals are
first generated and then fine-tuned mainly for instance-level
segmentation (e.g. R-CNN [66], SDS [67], Mask-RCNN [64]).
A more common way for a semantic segmentation is the
one-stage pipeline based on a Fully Convolutional Network
(FCN) originally proposed by Long et al. [68]. In this work,
the fully-connected layers in a CNN classifier for predicting
classification scores are replaced with convolutional layers
to produce coarse output maps. These maps are then upsampled to dense pixel labels by backwards convolution (i.e.
deconvolution). Kendall et al. [62] extend FCN by introducing an encoder-decoder CNN architecture. The encoder
serves to produce hierarchical image representations with a
CNN backbone such as VGG or ResNet (removing fullyconnected layers). The decoder, conversely, restores these lowdimensional features back to original resolution by a set of
upsampling and convolution layers. The restored feature maps
are finally used for pixel-label prediction.
Global image information provides useful context cues for
semantic segmentation. However, vanilla CNN structures only
focus on local information with limited receptive fields. In
this regard, many methods have been proposed to incorporate
global information, such as dilated convolutions [69], [70],
multi-scale prediction [71], as well as adding Conditional
Random Fields (CRFs) as post-processing step [72].
Real-time performance is important in autonomous driving
applications. However, most works only focus on segmentation accuracy. In this regard, Siam et al. [73] made a
comparative study on the real-time performance among several
semantic segmentation architectures, regarding the operations
(GFLOPs) and the inference speed (fps).
III. M ULTI - MODAL DATASETS

D. Deep Semantic Segmentation
The target of semantic segmentation is to partition a scene
into several meaningful parts, usually by labeling each pixel in
the image with semantics (pixel-level semantic segmentation)
or by simultaneously detecting objects and doing per-instance
per-pixel labeling (instance-level semantic segmentation). Recently, panoptic segmentation [47] is proposed to unify pixellevel and instance-level semantic segmentation, and it starts to
get more attentions for autonomous driving [48]–[50]. Though
semantic segmentation was first introduced to process camera
images, many methods have been proposed for segmenting
LiDAR points as well (e.g. [51]–[56]).
Many datasets have been published for semantic segmentation, such as Cityscape [57], KITTI [6], Toronto City [58],
Mapillary Vistas [59], and ApolloScape [60]. These datasets
advance the deep learning research for semantic segmentation
in autonomous driving. For example, [54], [61], [62] focus on

Most deep multi-modal perception methods are based on
supervised learning. Therefore, multi-modal datasets with labeled ground-truth are required for training such deep neural
networks. In the following, we summarize several real-world
datasets published since 2013, regarding sensor setups, recording conditions, dataset size and labels (cf. Tab. II). Note that
there exist some virtual multi-modal datasets generated from
game engines. We will discuss them in Sec. VI-A1.
A. Sensing Modalities
All reviewed datasets include RGB camera images. In
addition, [6], [60], [74]–[89] provide LiDAR point clouds,
and [90]–[92] thermal images. The KAIST Multispectral
Dataset [93] provides both thermal images and LiDAR data.
Bus data is included additionally in [87]. Only the very
recently nuScenes [89], Oxford Radar RobotCar [85] and
Astyx HiRes2019 Datasets [94] provide Radar data.

5

B. Recording Conditions

1
1.4M

Car

C. Dataset Size
The dataset size ranges from only 1,569 frames up to over
11 million frames. The largest dataset with ground-truth labels
that we have reviewed is the nuScenes Dataset [89] with nearly
1,4M frames. Compared to the image datasets in the computer
vision community, the multi-modal datasets are still relatively
small. However, the dataset size has grown by two orders of
magnitudes between 2014 and 2019 (cf. Fig. 5(b)).
D. Labels
Most of the reviewed datasets provide ground-truth labels
for 2D object detection and semantic segmentation tasks [60],
[75], [88], [90]–[93]. KITTI [75] also labels tracking, optical
flow, visual odometry, and depth for various computer vision
problems. BLV3D [80] provides labels for tracking, interaction
and intention. Labels for 3D scene understanding are provided
by [60], [75], [79]–[84], [89].
Depending on the focus of a dataset, objects are labeled
into different classes. For example, [90] only contains label
for people, including distinguishable individuals (labeled as
“Person”), non-distinguishable individuals (labeled as “People”), and cyclists; [60] classifies objects into five groups, and
provides 25 fine-grained labels, such as truck, tricycle, traffic

Person
0.8

Cyclist

# of image frames

Normalized proportion

Even though the KITTI dataset [75] is widely used for
autonomous driving research, the diversity of its recording
conditions is relatively low: it is recorded in Karlsruhe - a
mid-sized city in Germany, only during daytime and on sunny
days. Other reviewed datasets such as [60], [78], [79], [82],
[87]–[89] are recorded in more than one location. To increase
the diversity of lighting conditions, [60], [80]–[82], [82], [84],
[86], [88]–[92] collect data in both daytime and nighttime, and
[93] considers various lighting conditions throughout the day,
including sunrise, morning, afternoon, sunset, night, and dawn.
The Oxford Dataset [74] and the Oxford Radar RobotCar
Dataset [85] are collected by driving the car around the
Oxford area during the whole year. It contains data under
different weather conditions, such as heavy rain, night, direct
sunlight and snow. Other datasets containing diverse weather
conditions are [60], [86], [88], [89]. In [95], LiDAR is used as
a reference sensor for generating ground-truth, hence we do
not consider it a multi-modal dataset. However the diversity
in the recording conditions is large, ranging from dawn to
night, as well as reflections, rain and lens flare. The crossseason dataset [96] emphasizes the importance of changes
throughout the year. However, it only provides camera images
and labels for semantic segmentation. Similarly, the visual
localization challenge and the corresponding benchmark [97]
cover weather and season diversity (but no new multi-modal
dataset is introduced). The recent Eurocity dataset [88] is
the most diverse dataset we have reviewed. It is recorded in
different cities from several European countries. All seasons
are considered, as well as weather and daytime diversity. To
date, the dataset is camera-only and other modalities (e.g.
LiDARs) are announced.

0.6

0.4

0.2

144K
15K

200K

8.9K

0

(a)

(b)

Fig. 5: (a). Normalized percentage of objects of car, person,
and cyclist classes in KAIST Multispectral [93], KITTI [6],
Apolloscape [60] (E: easy, M: moderate, and H: hard refer
to the number of moveable objects in the frame - details can
be found in [60]), and nuScene dataset [89]. (b). Number of
camera image frames in several datasets. An increase by two
orders of magnitude of the dataset size can be seen.
cone, and trash can. The Eurocity dataset [88] focuses on
vulnerable road-users (mostly pedestrian). Instead of labeling
objects, [77] provides a dataset for place categorization. Scenes
are classified into forest, coast, residential area, urban area and
indoor/outdoor parking lot. [78] provides vehicle speed and
wheel angles for driving behavior predictions. The BLV3D
dataset [80] provides unique labeling for interaction and intention.
The object classes are very imbalanced. Fig. 5(a) compares
the percentage of car, person, and cyclist classes from four
reviewed datasets. There are much more objects labeled as
car than person or cyclist.
IV. D EEP M ULTI - MODAL P ERCEPTION P ROBLEMS FOR
AUTONOMOUS D RIVING
In this section, we summarize deep multi-modal perception
problems for autonomous driving based on sensing modalities
and targets. An overview of the existing methods is shown in
Tab. III and Tab. IV. An accuracy and runtime comparison
among several methods is shown in Tab. V and Tab. VI.
A. Deep Multi-modal Object Detection
1) Sensing Modalities: Most existing works combine RGB
images from visual cameras with 3D LiDAR point clouds
[98]–[116]. Some other works focus on fusing the RGB
images from visual cameras with images from thermal cameras
[91], [117]–[119]. Furthermore, Mees et al. [120] employ a
Kinect RGB-D camera to fuse RGB images and depth images;
Schneider et al. [61] generate depth images from a stereo
camera and combine them with RGB images; Yang et al. [121]
and Cascas et al. [122] leverage HD maps to provide prior
knowledge of the road topology.
2) 2D or 3D Detection: Many works [61], [91], [99]–
[101], [106], [108], [109], [111], [117]–[120], [123] deal
with the 2D object detection problem on the front-view 2D
image plane. Compared to 2D detection, 3D detection is
more challenging since the object’s distance to the ego-vehicle

6

needs to be estimated. Therefore, accurate depth information
provided by LiDAR sensors is highly beneficial. In this regard,
some papers including [98], [102]–[105], [107], [113], [115]
combine RGB camera images and LiDAR point clouds for
3D object detection. In addition, Liang et al. [116] propose
a multi-task learning network to aid 3D object detection. The
auxiliary tasks include camera depth completion, ground plane
estimation, and 2D object detection. How to represent the
modalities properly is discussed in section V-A.
3) What to detect: Complex driving scenarios often contain
different types of road users. Among them, cars, cyclists, and
pedestrians are highly relevant to autonomous driving. In this
regard, [98], [99], [106], [108], [110] employ multi-modal
neural networks for car detection; [101], [108], [109], [117]–
[120] focus on detecting non-motorized road users (pedestrians
or cyclists); [61], [91], [100], [102]–[105], [111], [115], [116]
detect both.
B. Deep Multi-modal Semantic Segmentation
Compared to the object detection problem summarized in
Sec. IV-A, there are fewer works on multi-modal semantic
segmentation: [92], [119], [124] employ RGB and thermal
images, [61] fuses RGB images and depth images from a
stereo camera, [125]–[127] combine RGB, thermal, and depth
images for semantic segmentation in diverse environments
such as forests, [123] fuses RGB images and LiDAR point
clouds for off-road terrain segmentation and [128]–[132] for
road segmentation. Apart from the above-mentioned works for
semantic segmentation on the 2D image plane, [125], [133]
deal with 3D segmentation on LiDAR points.
V. M ETHODOLOGY
When designing a deep neural network for multi-modal
perception, three questions need to be addressed - What to
fuse: what sensing modalities should be fused, and how to
represent and process them in an appropriate way; How to
fuse: what fusion operations should be utilized; When to fuse:
at which stage of feature representation in a neural network
should the sensing modalities be combined. In this section,
we summarize existing methodologies based on these three
aspects.
A. What to Fuse
LiDARs and cameras (visual cameras, thermal cameras) are
the most common sensors for multi-modal perception in the
literature. While the interest in processing Radar signals via
deep learning is growing, only a few papers discuss deep
multi-modal perception with Radar for autonomous driving
(e.g. [134]). Therefore, we focus on several ways to represent
and process LiDAR point clouds and camera images separately, and discuss how to combine them together. In addition,
we briefly summarize Radar perception using deep learning.
1) LiDAR Point Clouds: LiDAR point clouds provide both
depth and reflectance information of the environment. The
depth information of a point p
can be encoded by its Cartesian
coordinates [x, y, z], distance x2 + y2 + z2 , density, or HHA

features (Horizontal disparity, Height, Angle) [66], or any
other 3D coordinate system. The reflectance information is
given by intensity.
There are mainly three ways to process point clouds. One
way is by discretizing the 3D space into 3D voxels and assigning the points to the voxels (e.g. [29], [113], [135]–[137]).
In this way, the rich 3D shape information of the driving
environment can be preserved. However, this method results in
many empty voxels as the LiDAR points are usually sparse and
irregular. Processing the sparse data via clustering (e.g. [100],
[106]–[108]) or 3D CNN (e.g. [29], [136]) is usually very
time-consuming and infeasible for online autonomous driving.
Zhou et al. [135] propose a voxel feature encoding (VFE) layer
to process the LiDAR points efficiently for 3D object detection. They report an inference time of 225 ms on the KITTI
dataset. Yan et al. [138] add several sparse convolutional layers
after the VFE to convert the sparse voxel data into 2D images,
and then perform 3D object detection on them. Unlike the
common convolution operation, the sparse convolution only
computes on the locations associated with input points. In
this way, they save a lot of computational cost, achieving an
inference time of only 25 ms.
The second way is to directly learn over 3D LiDAR points in
continuous vector space without voxelization. PointNet [139]
and its improved version PointNet++ [140] propose to predict
individual features for each point and aggregate the features
from several points via max pooling. This method was firstly
introduced in 3D object recognition and later extended by Qi et
al. [105], Xu et al. [104] and Shin et al. [141] to 3D object detection in combination with RGB images. Furthermore, Wang
et al. [142] propose a new learnable operator called Parametric
Continuous Convolution to aggregate points via a weighted
sum, and Li et al. [143] propose to learn a χ transformation
before applying transformed point cloud features into standard
CNN. They are tested in semantic segmentation or LiDAR
motion estimation tasks.
A third way to represent 3D point clouds is by projecting
them onto 2D grid-based feature maps so that they can be
processed via 2D convolutional layers. In the following, we
distinguish among spherical map, camera-plane map (CPM),
as well as bird’s eye view (BEV) map. Fig. 6 illustrates
different LiDAR representations in 2D.
A spherical map is obtained by projecting each 3D point
onto a sphere, characterized by azimuth and zenith angles.
It has the advantage of representing each 3D point in a
dense and compact way, making it a suitable representation
for point cloud segmentation (e.g. [51]). However, the size
of the representation can be different from camera images.
Therefore, it is difficult to fuse them at an early stage. A
CPM can be produced by projecting the 3D points into the
camera coordinate system, provided the calibration matrix. A
CPM can be directly fused with camera images, as their sizes
are the same. However, this representation leaves many pixels
empty. Therefore, many methods have been proposed to upsample such a sparse feature map, e.g. mean average [111],
nearest neighbors [144], or bilateral filter [145]. Compared
to the above-mentioned feature maps which encode LiDAR
information in the front-view, a BEV map avoids occlusion

7

(a) RGB camera image

(e) LiDAR spherical map

(b) LiDAR sparse depth map

(c) LiDAR dense depth map

(d) LiDAR dense intensity map

(f) LiDAR BEV density map

Fig. 6: RGB image and different 2D LiDAR representation
methods. (a) A standard RGB image, represented by a pixel
grid and color channel values. (b) A sparse (front-view)
depth map obtained from LiDAR measurements represented
on a grid. (c) Interpolated depth map. (d) Interpolation of
the measured reflectance values on a grid. (e) Interpolated
representation of the measured LiDAR points (surround view)
on a spherical map. (f) Projection of the measured LiDAR
points (front-facing) to bird’s eye view (no interpolation).
problems because objects occupy different space in the map.
In addition, the BEV preserves the objects’ length and width,
and directly provides the objects’ positions on the ground
plane, making the localization task easier. Therefore, the
BEV map is widely applied to 3D environment perception.
For example, Chen et al. [98] encode point clouds by height,
density and intensity maps in BEV. The height maps are
obtained by dividing the point clouds into several slices. The
density maps are calculated as the number of points within
a grid cell, normalized by the number of channels. The
intensity maps directly represent the reflectance measured
by the LiDAR on a grid. Lang et al. [146] argue that the
hard-coded features for BEV representation may not be
optimal. They propose to learn features in each column of
the LiDAR BEV representation via PointNet [139], and feed
these learnable feature maps to standard 2D convolution
layers.
2) Camera Images: Most methods in the literature employ
RGB images from visual cameras or one type of infrared
images from thermal cameras (near-infrared, mid-infrared,
far-infrared). Besides, some works extract additional sensing
information, such as optical flow [120], depth [61], [125],
[126], or other multi-spectral images [91], [125].
Camera images provide rich texture information of the
driving surroundings. However, objects can be occluded and
the scale of a single object can vary significantly in the camera
image plane. For 3D environment inference, the bird’s eye

view that is commonly used for LiDAR point clouds might
be a better representation. Roddick et al. [147] propose a
Orthographic Feature Transform (OFT) algorithm to project
the RGB image features onto the BEV plane. The BEV feature
maps are further processed for 3D object detection from
monocular camera images. Lv et al. [130] project each image
pixel with the corresponding LiDAR point onto the BEV plane
and fuse the multi-modal features for road segmentation. Wang
et al. [148] and their successive work [149] propose to convert
RGB images into pseudo-lidar representation by estimating
the image depth, and then use state-of-the-art BEV LiDAR
detector to significantly improve the detection performance.
3) Processing LiDAR Points and Camera Images in Deep
Multi-modal Perception: Tab. III and Tab. IV summarize
existing methods to process sensors’ signals for deep multimodal perception, mainly LiDAR points and camera images.
From the tables we have three observations: (1). Most works
propose to fuse LiDAR and camera features extracted from 2D
convolution neural networks. To do this, they project LiDAR
points on the 2D plane and process the feature maps through
2D convolutions. Only a few works extract LiDAR features
by PointNet (e.g. [104], [105], [128]) or 3D convolutions
(e.g. [123]); (2). Several works on multi-modal object detection cluster and segment 3D LiDAR points to generate 3D
region proposals (e.g. [100], [106], [108]). Still, they use a
LiDAR 2D representation to extract features for fusion; (3).
Several works project LiDAR points on the camera-plane or
RGB camera images on the LiDAR BEV plane (e.g. [130],
[131], [150]) in order to align the features from different
sensors, whereas many works propose to fuse LiDAR BEV
features directly with RGB camera images (e.g. [98], [103]).
This indicates that the networks implicitly learn to align
features of different viewpoints. Therefore, a well-calibrated
sensor setup with accurate spatial and temporal alignment is
the prerequisite for accurate multi-modal perception, as will
be discussed in Sec. VI-A2.
4) Radar Signals: Radars provide rich environment information based on received amplitudes, ranges, and the Doppler
spectrum. The Radar data can be represented by 2D feature
maps and processed by convolutional neural networks. For
example, Lombacher et al. employ Radar grid maps made by
accumulating Radar data over several time-stamps [151] for
static object classification [152] and semantic segmentation
[153] in autonomous driving. Visentin et al. show that CNNs
can be employed for object classification in a post-processed
range-velocity map [154]. Kim et al. [155] use a series
of Radar range-velocity images and convolutional recurrent
neural networks for moving objects classification. Moeness et
al. [156] feed spectrogram from Time Frequency signals as 2D
images into a stacked auto-encoders to extract high-level Radar
features for human motion recognition. The Radar data can
also be represented directly as “point clouds” and processed
by PointNet++ [140] for dynamic object segmentation [157].
Besides, Woehler et al. [158] encode features from a cluster
of Radar points for dynamic object classification. Chadwick et
al. [134] first project Radar points on the camera plane to build
Radar range-velocity images, and then combine with camera
images for distant vehicle detection.

8

C. When to Fuse

Expert Network i

Expert Network j

Addition

Gating Network
Combined
features of
experts

Fig. 7: An illustration of the Mixture of Experts fusion method.
Here we show the combined features which are derived from
the output layers of the expert networks. They can be extracted
from the intermediate layers as well.

B. How to Fuse
This section summarizes typical fusion operations in a deep
neural network. For simplicity we restrict our discussion to two
sensing modalities, though more still apply. Denote Mi and M j
M
as two different modalities, and flMi and fl j their feature maps
in the l th layer of the neural network. Also denote Gl (·) as a
mathematical description of the feature transformation applied
in layer l of the neural network.
1) Addition or Average Mean: This join operation
adds


Mj
Mi
the feature maps element-wise, i.e. fl = Gl−1 fl−1 + fl−1 ,
or calculates the average mean of the feature maps.
2) Concatenation:
Combines feature maps by fl =

Mi _ M j
Gl−1 fl−1 fl−1 . The feature maps are usually stacked along
their depth before they are advanced to a convolution layer. For
a fully connected layer, these features are usually flattened into
vectors and concatenated along the rows of the feature maps.
3) Ensemble: This operation ensembles feature
 maps

Mi
from different sensing modalities via fl = Gl−1 fl−1
∪


Mj
Gl−1 fl−1 . As will be introduced in the following sections
(Sec. V-C4 and Sec. V-C5), ensembles are often used to fuse
ROIs in object detection networks.
4) Mixture of Experts: The above-mentioned fusion operations do not consider the informativeness of a sensing modality
(e.g. at night time RGB camera images bring less information
than LiDAR points). These operations are applied, hoping that
the network can implicitly learn to weight the feature maps.
In contrast, the Mixture of Experts (MoE) approach explicitly
models the weight of a feature map. It is first introduced
in [159] for neural networks and then extended in [120], [126],
[160]. As Fig. 7 illustrates, the feature map of a sensing
modality is processed by its domain-specific network called
“expert”. Afterwards, the outputs of multiple expert networks
are averaged with the weights wMi , wM j predicted by a gating
network which takes the combined features output by the
expert networks as inputs h via a simple fusion operation such
as concatenation:


M
Mi
fl = Gl wMi · fl−1
+ wM j · fl−1j , with wMi + wM j = 1. (1)

Deep neural networks represent features hierarchically and
offer a wide range of choices to combine sensing modalities at
early, middle, or late stages (Fig. 8). In the sequel, we discuss
the early, middle, and late fusions in detail. For each fusion
scheme, we first give mathematical descriptions using the same
notations as in Sec. V-B, and then discuss their properties.
Note that there exists some works that fuse features from the
early stage till late stages in deep neural networks (e.g. [161]).
For simplicity, we categorize this fusion scheme as “middle
fusion”. Compared to the semantic segmentation where multimodal features are fused at different stages in FCN, there exist
more diverse network architectures and more fusion variants
in object detection. Therefore, we additionally summarize the
fusion methods specifically for the object detection problem.
Finally, we discuss the relationship between the fusion operation and the fusion scheme.
Note that we do not find conclusive evidence from the
methods we have reviewed that one fusion method is better
than the others. The performance is highly dependent on
sensing modalities, data, and network architectures.
1) Early Fusion: This method fuses the raw or preM
Mi
processed sensor data. Let us define fl = fl−1
⊕ fl−1j as a fusion
operation introduced in Sec. V-B. For a network that has L + 1
layers, an early fusion scheme can be described as:
!




M
fL = GL GL−1 · · · Gl · · · G2 G1 ( f0Mi ⊕ f0 j )
, (2)
with l = [1, 2, · · · , L]. Early fusion has several pros and cons.
First, the network learns the joint features of multiple modalities at an early stage, fully exploiting the information of the
raw data. Second, early fusion has low computation requirements and a low memory budget as it jointly processes the
multiple sensing modalities. This comes with the cost of model
inflexibility. As an example, when an input is replaced with a
new sensing modality or the input channels are extended, the
early fused network needs to be retrained completely. Third,
early fusion is sensitive to spatial-temporal data misalignment
among sensors which are caused by calibration error, different
sampling rate, and sensor defect.
2) Late Fusion: This fusion scheme combines decision
outputs of each domain specific network of a sensing modality.
It can be described as:




M
Mj
M
M 
Mi
Mi Mi 
i
⊕GL j GL−1
· · · G1 j ( f 0 j ) .
f L = GM
L GL−1 · · · G1 ( f 0 )
(3)
Late fusion has high flexibility and modularity. When a
new sensing modality is introduced, only its domain specific
network needs to be trained, without affecting other networks.
However, it suffers from high computation cost and memory
requirements. In addition, it discards rich intermediate features
which may be highly beneficial when being fused.
3) Middle Fusion: Middle fusion is the compromise of
early and late fusion: It combines the feature representations
from different sensing modalities at intermediate layers. This
enables the network to learn cross modalities with different
feature representations and at different depths. Define l ? as

9

(a) Early Fusion

(b) Late Fusion

(c) Middle Fusion
fusion in one layer

Modality

Network output

(d) Middle Fusion
deep fusion

Intermediate layers

Fusion operation

(e) Middle Fusion
short-cut fusion

Fig. 8: An illustration of early fusion, late fusion, and several middle fusion methods.

the layer from which intermediate features begin to be fused. Chen et al. [98] use LiDAR BEV maps to generate region
The middle fusion can be executed at this layer only once:
proposals. For each ROI, the regional features from the LiDAR



 BEV maps are fused with those from the LiDAR front-view
Mj
Mj Mj 
Mi Mi 
i
fL = GL · · · Gl ? +1 GM
·
·
·
G
(
f
)
⊕G
·
·
·
G
(
f
)
. maps as well as camera images via deep fusion. Compared
1
0
1
0
l?
l?
(4) to object detections from LiDAR point clouds, camera images
Alternatively, they can be fused hierarchically, such as by deep have been well investigated with larger labeled dataset and better 2D detection performance. Therefore, it is straightforward
fusion [98], [162]:
to exploit the predictions from well-trained image detectors
Mj
Mi
when doing camera-LiDAR fusion. In this regard, [104],
?
fl +1 = fl ? ⊕ fl ? ,
(5)
[105], [107] propose to utilize a pre-trained image detector to
M
j
?
i
fk+1 = GM
k ( f k ) ⊕ Gk ( f k ), ∀k : k ∈ {l + 1, · · · , L} .
produce 2D bounding boxes, which build frustums in LiDAR
point clouds. Then, they use these point clouds within the
or “short-cut fusion” [92]:
frustums for 3D object detection. Fig. 9 shows some exemplary
M
fl+1 = flMi ⊕ fl j ,
fusion architectures for two-stage object detection networks.
Mj
Mi
Tab. III summarizes the methodologies for multi-modal object
(6)
fk+1 = fk ⊕ fk? ⊕ fk? ,
detection.
∀k : k ∈ {l + 1, · · · , L} ; ∃k? : k? ∈ {1, · · · , l − 1} .
5) Fusion Operation and Fusion Scheme: Based on the
Although the middle fusion approach is highly flexible, it is papers that we have reviewed, feature concatenation is the
not easy to find the “optimal” way to fuse intermediate layers most common operation, especially at early and middle stages.
given a specific network architecture. We will discuss this Element-wise average mean and addition operations are adchallenge in detail in Sec. VI-B3.
ditionally used for middle fusion. Ensemble and Mixture of
4) Fusion in Object Detection Networks: Modern multi- Experts are often used for middle to decision level fusion.
modal object detection networks usually follow either the
two-stage pipeline (RCNN [35], Fast-RCNN [37], FasterVI. C HALLENGES AND O PEN Q UESTIONS
RCNN [41]) or the one-stage pipeline (YOLO [44] and
SSD [45]), as explained in detail in Sec. II-C. This offers
As discussed in the Introduction (cf. Sec. I), developing
a variety of alternatives for network fusion. For instance, the deep multi-modal perception systems is especially challenging
sensing modalities can be fused to generate regional proposals for autonomous driving because it has high requirements in acfor a two-stage object detector. The regional multi-modal curacy, robustness, and real-time performance. The predictions
features for each proposal can be fused as well. Ku et al. [103] from object detection or semantic segmentation are usually
propose AVOD, an object detection network that fuses RGB transferred to other modules such as maneuver prediction
images and LiDAR BEV images both in the region proposal and decision making. A reliable perception system is the
network and the header network. Kim et al. [109] ensemble the prerequisite for a driverless car to run safely in uncontrolled
region proposals that are produced by LiDAR depth images and complex driving environments. In Sec. III and Sec. V
and RGB images separately. The joint region proposals are we have summarized the multi-modal datasets and fusion
then fed to a convolutional network for final object detection. methodologies. Correspondingly, in this section we discuss the

10

LiDAR BEV map

LiDAR BEV map

Projection
and pooling
3D ROI

Projection
and pooling

3D ROI
generation

3D ROI

Projection
and pooling

3D ROI
LiDAR front view map
Projection
and pooling

3D object
detection

Fusion

Fusion

3D Anchors

3D ROI
generation

Fusion

3D object
detection

Projection
and pooling
3D ROI

3D ROI

RGB camera image

RGB camera image

Projection
and pooling

Projection
and pooling

(a) MV3D

(b) AVOD
LiDAR frustum proposal
LiDAR front view map
2D ROI
generation
2D ROI
Point Cloud
Segmentation

3D object
detection

Fusion
(ensemble)

2D ROI

RGB camera image

RGB camera image

2D ROI

Projection
and pooling

2D object
detection

2D ROI

Object class

2D ROI
generation

2D object detection

(c) Frustum PointNet

(d) Ensemble Proposals

Fig. 9: Examplary fusion architectures for two-stage object detection networks. (a). MV3D [98]; (b). AVOD [103]; (c). Frustum
PointNet [105]; (d). Ensemble Proposals [109].

TABLE I: AN OVERVIEW OF CHALLENGES AND OPEN QUESTIONS
Topics
Data diversity

•
•

Challenges
Relative small size of training dataset.
Limited driving scenarios and conditions, limited sensor
variety, object class imbalance.

•
•
•

Multi-modal data preparation
Data quality

•
•

Labeling errors.
Spatial and temporal misalignment of different sensors.

•
•
•

“What to fuse”

•
•

Too few sensing modalities are fused.
Lack of studies for different feature representations.

•
•
•
•

Fusion methodology

“How to fuse”

•
•

Lack of uncertainty quantification for each sensor channel.
Too simple fusion operations.

•
•
•
•

“When to fuse”

•
•

Others

Fusion architecture is often designed by empirical results.
No guideline for optimal fusion architecture design.
Lack of study for accuracy/speed or memory/robustness
trade-offs.

•
•
•

Open Questions
Develop more realistic virtual datasets.
Finding optimal way to combine real- and virtual data.
Increasing labeling efficiency through cross-modal labeling, active learning, transfer learning, semi-supervised
learning etc. Leveraging lifelong learning to update networks with continual data collection.
Teaching network robustness with erroneous and noisy
labels.
Integrating prior knowledge in networks.
Developing methods (e.g. using deep learning) to
automatically register sensors.
Fusing multiple sensors with the same modality.
Fusing more sensing modalities, e.g. Radar, Ultrasonic,
V2X communication.
Fusing with physical models and prior knowledge, also
possible in the multi-task learning scheme.
Comparing different feature representation w.r.t informativeness and computational costs.
Uncertainty estimation via e.g. Bayesian neural networks
(BNN).
Propagating uncertainties to other modules, such as tracking and motion planning.
Anomaly detection by generative models.
Developing fusion operations that are suitable for
network pruning and compression.
Optimal fusion architecture search.
Incorporating requirements of computation time or memory as regularization term.
Using visual analytics tool to find optimal fusion
architecture.

Evaluation metrics

•

Current metrics focus on comparing networks’ accuracy.

•

Metrics to quantify the networks’ robustness should
be developed and adapted to multi-modal perception
problems.

More network architectures

•

Current networks lack temporal cues and cannot guarantee prediction consistency over time.
They are designed mainly for modular autonomous
driving.

•

Using Recurrent Neural Network (RNN) for sequential
perception.
Multi-modal end-to-end learning or multi-modal directperception.

•

•

11

remaining challenges and open questions for multi-modal data
preparation and network architecture design. We focus on how
to improve the accuracy and robustness of the multi-modal
perception systems while guaranteeing real-time performance.
We also discuss some open questions, such as evaluation
metrics and network architecture design. Tab. I summarizes
the challenges and open questions.

(a) “collaborative labeling”

Human
annotator

Weakly human-labeled LiDAR
data: one click per object

Pre-trained LiDAR
detector (F-PointNet)

Fine-tuned full object labels
(class and bounding box)

+

LiDAR data labeled by
human annotaters

(b) “collaborative training”
LiDAR data labeled by a pre-trained SegNet

A. Multi-modal Data Preparation
1) Data Diversity: Training a deep neural network on a
complex task requires a huge amount of data. Therefore, using
large multi-modal datasets with diverse driving conditions, object labels, and sensors can significantly improve the network’s
accuracy and robustness against changing environments. However, it is not an easy task to acquire real-world data due
to cost and time limitations as well as hardware constraints.
The size of open multi-modal datasets is usually much smaller
than the size of image datasets. As a comparison, KITTI [6]
records only 80,256 objects whereas ImageNet [163] provides 1,034,908 samples. Furthermore, the datasets are usually
recorded in limited driving scenarios, weather conditions, and
sensor setups (more details are provided in Sec. III). The
distribution of objects is also very imbalanced, with much
more objects being labeled as car than person or cyclist
(Fig. 5). As a result, it is questionable how a deep multi-modal
perception system trained with those public datasets performs
when it is deployed to an unstructured environment.
One way to overcome those limitations is by data augmentation via simulation. In fact, a recent work [164] states that
the most performance gain for object detection in the KITTI
dataset is due to data augmentation, rather than advances
in network architectures. Pfeuffer et al. [111] and Kim et
al. [110] build augmented training datasets by adding artificial
blank areas, illumination change, occlusion, random noises,
etc. to the KITTI dataset. The datasets are used to simulate
various driving environment changes and sensor degradation.
They show that trained with such datasets, the network accuracy and robustness are improved. Some other works aim at
developing virtual simulators to generate varying driving conditions, especially some dangerous scenarios where collecting
real-world data is very costly or hardly possible. Gaidon et
al. [165] build a virtual KITTI dataset by introducing a real to
virtual cloning method to the original KITTI dataset, using the
Unity Game Engine. Other works [166]–[171] generate virtual
datasets purely from game engines, such as GTA-V, without a
proxy of real-world datasets. Griffiths and Boehm [172] create
a purely virtual LiDAR only dataset. In addition, Dosovitskiy et al. [173] develop an open-source simulator that can
simulate multiple sensors in autonomous driving and Hurl et
al. [174] release a large scale, virtual, multi-modal dataset with
LiDAR data and visual camera. Despite many available virtual
datasets, it is an open question to which extend a simulator can
represent real-world phenomena. Developing more realistic
simulators and finding the optimal way to combine real and
virtual data are important open questions.
Another way to overcome the limitations of open datasets
is by increasing the efficiency of data labeling. When building

Semantic labels from
a pre-trained SegNet

Registrating LiDAR data
with semantic labels
Training a LiDAR point SegNet

Fig. 10: Two examples of increasing data labeling efficiency in
LiDAR data. (a) Collaborative labeling LiDAR points for 3D
detection [175]: the LiDAR points within each object are firstly
weakly-labeled by human annotators, and then fine-tuned
by a pre-trained LiDAR detector based on the F-PointNet.
(b) Collaborative training a semantic segmentation network
(SegNet) for LiDAR points [133]: To boost the training data,
a pre-trained image SegNet can be employed to transfer the
image semantics.
a multi-modal training dataset, it is relatively easy to drive
the test vehicle and collect many data samples. However, it
is very tedious and time-consuming to label them, especially
when dealing with 3D labeling and LiDAR points. Lee et
al. [175] develop a collaborative hybrid labeling tool, where
3D LiDAR point clouds are firstly weakly-labeled by human
annotators, and then fine-tuned by pre-trained network based
on F-PointNet [105]. They report that the labeling tool can
significantly reduce the “task complexity” and “task switching”, and have a 30× labeling speed-up (Fig. 10(a)). Piewak et
al. [133] leverage a pre-trained image segmentation network
to label LiDAR point clouds without human intervention. The
method works by registering each LiDAR point with an image
pixel, and transferring the image semantics predicted by the
pre-trained network to the corresponding LiDAR points (cf.
Fig. 10(b)). In another work, Mei et al. [176] propose a
semi-supervised learning method to do 3D point segmentation
labeling. With only a few manual labels together with pairwise spatial constraints between adjacent data frames, a lot of
objects can be labeled. Several works [177]–[179] propose to
introduce active learning in semantic segmentation or object
detection for autonomous driving. The networks iteratively
query the human annotator some most informative samples in
an unlabeled data pool and then update the networks’ weights.
In this way, much less labeled training data is required while
reaching the same performance and saving human labeling
efforts. There are many other methods in the machine learning
literature that aim to reduce data labeling efforts, such as transfer learning [180], domain adaptation [181]–[185], and semisupervised learning [186]. How to efficiently label multi-modal
data in autonomous driving is an important and challenging
future work, especially in scenarios where the signals from
different sensors may not be matched (e.g. due to the distance
some objects are only visible by visual camera but not by
LiDAR). Finally, as there can always be new driving scenarios
that are different from the training data, it is an interesting

12

(a)

in an end-to-end fashion.

(b)

1
0.9

𝐦𝐀𝐏

𝐦𝐀𝐏𝐛𝐚𝐬𝐞𝐥𝐢𝐧𝐞

0.8

B. Fusion Methodology

0.7
0.6

Labeling with random noises

0.5
0.4

0.3
Labeling with biases

0.2

Labeling with random noises

0.1
0
0

50

Noises δ [px]

100

Labeling with biases

Fig. 11: (a) An illustration for the influence of label quality
on the performance of an object detection network [196]. The
network is trained on labels which are incrementally disturbed.
The performance is measured by mAP normalized to the
performance trained on the undisturbed dataset. The network is
much more robust against random labeling errors (drawn from
a Gaussian distribution with variance σ ) than biased labeling
(all labels shifted by σ ) cf. [194], [195]. (b) An illustration of
the random labeling noises and labeling biases (all bounding
boxes are shifted in the upper-right direction).
research topic to leverage lifelong learning [187] to update the
multi-modal perception network with continual data collection.
2) Data Quality and Alignment: Besides data diversity and
the size of the training dataset, data quality significantly affects
the performance of a deep multi-modal perception system as
well. Training data is usually labeled by human annotators to
ensure the high labeling quality. However, humans are also
prone to making errors. Fig. 11 shows two different errors in
the labeling process when training an object detection network.
The network is much more robust against labeling errors when
they are randomly distributed, compared to biased labeling
from the use of a deterministic pre-labeling. Training networks
with erroneous labels is further studied in [188]–[191]. The
impact on weak or erroneous labels on the performance of
deep learning based semantic segmentation is investigated
in [192], [193]. The influence of labelling errors on the
accuracy of object detection is discussed in [194], [195].
Well-calibrated sensors are the prerequisite for accurate and
robust multi-modal perception systems. However, the sensor
setup is usually not perfect. Temporal and spatial sensing misalignments might occur while recording the training data or deploying the perception modules. This could cause severe errors
in training datasets and degrade the performance of networks,
especially for those which are designed to implicitly learn
the sensor alignment (e.g. networks that fuse LiDAR BEV
feature maps and front view camera images cf. Sec. V-A3).
Interestingly, several works propose to calibrate sensors by
deep neural networks: Giering et al. [197] discretize the spatial
misalignments between LiDAR and visual camera into nine
classes, and build a network to classify misalignment taking
LiDAR and RGB images as inputs; Schneider et al. [198]
propose to fully regress the extrinsic calibration parameters
between LiDAR and visual camera by deep learning. Several multi-modal CNN networks are trained on different decalibration ranges to iteratively refine the calibration output. In
this way, the feature extraction, feature matching, and global
optimization problems for sensor registration could be solved

1) What to Fuse: Most reviewed methods combine RGB
images with thermal images or LiDAR 3D points. The networks are trained and evaluated on open datasets such as
KITTI [6] and KAIST Pedestrian [93]. These methods do
not specifically focus on sensor redundancy, e.g. installing
multiple cameras on a driverless car to increase the reliability
of perception systems even when some sensors are defective.
How to fuse the sensing information from multiple sensors
(e.g. RGB images from multiple cameras) is an important open
question.
Another challenge is how to represent and process different
sensing modalities appropriately before feeding them into
a fusion network. For instance, many approaches exist to
represent LiDAR point clouds, including 3D voxels, 2D BEV
maps, spherical maps, as well as sparse or dense depth maps
(more details cf. Sec. V-A). However, only Pfeuffer et al. [111]
have studied the pros and cons for several LiDAR front-view
representations. We expect more works to compare different
3D point representation methods.
In addition, there are very few studies for fusing LiDAR
and camera outputs with signals from other sources such
as Radars, ultrasonics or V2X communication. Radar data
differs from LiDAR data and it requires different network
architecture and fusion schmes. So far, we are not aware of
any work fusing Ultrasonic sensor signals in deep multi-modal
perception, despite its relevance for low-speed scenarios. How
to fuse these sensing modalities and align them temporally and
spatially are big challenges.
Finally, it is an interesting topic to combine physical constraints and model-based approaches with data-driven neural
networks. For example, Ramos et al. [199] propose to fuse
semantics and geometric cues in a Bayesian framework for
unexpected objects detections. The semantics are predicted by
a FCN network, whereas the geometric cues are provided by
model-based stereo detections. The multi-task learning scheme
also helps to add physical constraints in neural networks. For
example, to aid 3D object detection task, Liang et al. [116]
design a fusion network that additionally estimate LiDAR
ground plane and camera image depth. The ground plane
estimation provides useful cues for object locations, while
the image depth completion contributes to better cross-modal
representation; Panoptic segmentation [47] aims to achieve
complete scene understanding by jointly doing semantic segmentation and instance segmentation.
2) How to Fuse: Explicitly modeling uncertainty or informativeness of each sensing modality is important safe
autonomous driving. As an example, a multi-modal perception system should show higher uncertainty against adverse
weather or detect unseen driving environments (open-world
problem). It should also reflect sensor’s degradation or defects
as well. The perception uncertainties need to be propagated
to other modules such as motion planning [200] so that
the autonomous vehicles can behave accordingly. Reliable
uncertainty estimation can show the networks’ robustness (cf.

13

LiDAR

Camera

Fusion network

Object located

Yes (Prob. = 90%)

Object classified

“Pedestrian”
(Prob. = 60%)

Camera signal

Uncertain

LiDAR signal

Certain

Detection outputs

Night drive

Fig. 12: The importance of explicitly modeling and propagating uncertainties in a multi-modal object detection network.
Ideally, the network should produce reliable prediction probabilities (object classification and localization). It should e.g.
depict high uncertainty for camera signals during a night drive.
Such uncertainty information is useful for the decision making
modules, such as maneuver planning or emergency braking
systems.
Fig 12). However, most reviewed papers only fuse multiple
sensing modalities by a simple operation (e.g. addition and
average mean, cf. Sec. V-B). Those methods are designed to
achieve high average precision (AP) without considering the
networks’ robustness. The recent work by Bijelic et al. [112]
uses dropout to increase the network robustness in foggy
images. Specifically, they add pixel-wise dropout masks in different fusion layers so that the network randomly drops LiDAR
or camera channels during training. Despite promising results
for detections in foggy weather, their method cannot express
which sensing modality is more reliable given the distorted
sensor inputs. To the best of our knowledge, only the gating
network (cf. Sec. V-B) explicitly models the informativeness
of each sensing modality.
One way to estimate uncertainty and to increase network
robustness is Bayesian Neural Networks (BNNs). They assume
a prior distribution over the network weights and infer the posterior distribution to extract the prediction probability [201].
There are two types of uncertainties BNNs can model. Epistemic uncertainty illustrates the models’ uncertainty when
describing the training dataset. It can be obtained by estimating the weight posterior by variational inference [202],
sampling [203]–[205], batch normalization [206], or noise
injection [207]. It has been applied to semantic segmentation [208] and open-world object detection problems [209],
[210]. Aleatoric uncertainty represents observation noises inherent in sensors. It can be estimated by the observation
likelihood such as a Gaussian distribution or Laplacian distribution. Kendall et al. [211] study both uncertainties for
semantic segmentation; Ilg et al. [212] propose to extract
uncertainties for optical flow; Feng et al. [213] examine
the epistemic and aleatoric uncertainties in a LiDAR vehicle
detection network for autonomous driving. They show that
the uncertainties encode very different information. In the
successive work, [214] employ aleatoric uncertainties in a 3D
object detection network to significantly improve its detection
performance and increase its robustness against noisy data.
Other works that introduce aleatoric uncertainties in object
detectors include [215]–[218]. Although much progress has
been made for BNNs, to the best of our knowledge, so far they
have not been introduced to multi-modal perception. Furthermore, few works have been done to propagate uncertainties

in object detectors and semantic segmentation networks to
other modules, such as tracking and motion planning. How
to employ these uncertainties to improve the robustness of an
autonomous driving system is a challenging open question.
Another way that can increase the networks’ robustness
is generative models. In general, generative models aim at
modeling the data distribution in an unsupervised way as
well as generating new samples with some variations. Variational Autoencoders (VAEs) [219] and Generative Adversarial
Networks (GANs) [220] are the two most popular deep
generative models. They have been widely applied to image
analysis [221]–[223], and recently introduced to model Radar
data [224] and road detection [225] for autonomous driving.
Generative models could be useful for multi-modal perception
problems. For example, they might generate labeled simulated
sensor data, when it is tedious and difficult to collect in the real
world; they could also serve to detect situations where sensors
are defect or an autonomous car is driving into a new scenario
that differs from those seen during training. Designing specific
fusion operations for deep generative models is an interesting
open question.
3) When to Fuse: As discussed in Sec. V-C, the choice of
when to fuse the sensing modalities in the reviewed works
is mainly based on intuition and empirical results. There
is no conclusive evidence that one fusion scheme is better
than the others. Ideally, the “optimal” fusion architecture
should be found automatically instead of by meticulous engineering. Neural network structure search can potentially
solve the problem. It aims at finding the optimal number of
neurons and layers in a neural network. Many approaches
have been proposed, including the bottom-up construction
approach [226], pruning [227], Bayesian optimization [228],
genetic algorithms [229], and the recent reinforcement learning
approach [230]. Another way to optimize the network structure is by regularization, such as l1 regularization [231] and
stochastic regularization [232], [233].
Furthermore, visual analytics techniques could be employed
for network architecture design. Such visualization tools can
help to understand and analyze how networks behave, to
diagnose the problems, and finally to improve the network
architecture. Several methods have been proposed for understanding CNNs for image classification [234], [235]. So far,
there has been no research on visual analytics for deep multimodal learning problems.
4) Real-time Consideration: Deep multi-modal neural networks should perceive driving environments in real-time.
Therefore, computational costs and memory requirements
should be considered when developing the fusion methodology. At the “what to fuse” level, sensing modalities should
be represented in an efficient way. At the “how to fuse”
level, finding fusion operations that are suitable for network
acceleration, such as pruning and quantization [236]–[239],
is an interesting future work. At the “when to fuse” level,
inference time and memory constraints can be considered as
regularization term for network architecture optimization.
It is difficult to compare the inference speed among the
methods we have reviewed, as there is no benchmark with
standard hardware or programming languages. Tab. V and

14

Tab. VI summarize the inference speed of several object detection and semantic segmentation networks on the KITTI test
set. Each method uses different hardware, and the inference
time is reported only by the authors. It is an open question how
these methods perform when they are deployed on automotive
hardware.
C. Others
1) Evaluation Metrics: The common way to evaluate object
detection methods is mean average precision (mAP) [6], [240].
It is the mean value of average precision (AP) over object
classes, given a certain intersection over union (IoU) threshold
defined as the geometric overlap between predictions and
ground truths. As for the pixel-level semantic segmentation,
metrics such as average precision, false positive rate, false
negative rate, and IoU calculated at pixel level [57] are often
used. However, these metrics only summarize the prediction
accuracy to a test dataset. They do not consider how sensor
behaves in different situations. As an example, to evaluate the
performance of a multi-modal network, the IoU thresholds
should depend on object distance, occlusion, and types of
sensors.
Furthermore, common evaluation metrics are not designed
specifically to illustrate how the algorithm handles open-set
conditions or in situations where some sensors are degraded or
defective. There exist several metrics to evaluate the quality of
predictive uncertainty, e.g. empirical calibration curves [241]
and log predictive probabilities. The detection error [242] measures the effectiveness of a neural network in distinguishing
in- and out-of-distribution data. The Probability-based Detection Quality (PDQ) [243] is designed to measure the object
detection performance for spatial and semantic uncertainties.
These metrics can be adapted to the multi-modal perception
problems to compare the networks’ robustness.
2) More Network Architectures: Most reviewed methods
are based on CNN architectures for single frame perception.
The predictions in a frame are not dependent on previous
frames, resulting in inconsistency over time. Only a few works
incorporate temporal cues (e.g. [122], [244]). Future work is
expected to develop multi-modal perception algorithms that
can handle time series, e.g. via Recurrent Neural Networks.
Furthermore, current methods are designed to propagate results
to other modules in autonomous driving, such as localization,
planning, and reasoning. While the modular approach is the
common pipeline for autonomous driving, some works also try
to map the sensor data directly to the decision policy such as
steering angles or pedal positions (end-to-end learning) [245]–
[247], or to some intermediate environment representations
(direct-perception) [248], [249]. Multi-modal end-to-end learning and direct perception can be potential research directions
as well.
VII. C ONCLUSION AND D ISCUSSION
We have presented our survey for deep multi-modal object
detection and segmentation applied to autonomous driving. We
have provided a summary of both multi-modal datasets and fusion methodologies, considering “what to fuse”, “how to fuse”,

and “when to fuse”. We have also discussed challenges and
open questions. Furthermore, our interactive online tool allows
readers to navigate topics and methods for each reference. We
plan to frequently update this tool.
Despite the fact that an increasing number of multi-modal
datasets have been published, most of them record data from
RGB cameras, thermal cameras, and LiDARs. Correspondingly, most of the papers we reviewed fuse RGB images
either with thermal images or with LiDAR point clouds. Only
recently has the fusion of Radar data been investigated. This
includes nuScene dataset [89], the Oxford Radar RobotCar
Dataset [85], the Astyx HiRes2019 Dataset [94], and the
seminal work from Chadwick et al. [134] that proposes to fuse
RGB camera images with Radar points for vehicle detection.
In the future, we expect more datasets and fusion methods
concerning Radar signals.
There are various ways to fuse sensing modalities in neural
networks, encompassing different sensor representations, cf.
Sec. V-A, fusion operations cf. Sec. V-B, and fusion stages,
cf. Sec. V-C. However, we do not find conclusive evidence
that one fusion method is better than the others. Additionally,
there is a lack of research on multi-modal perception in openset conditions or with sensor failures. We expect more focus
on these challenging research topics.
ACKNOWLEDGMENT
We thank Fabian Duffhauss for collecting literature and
reviewing the paper. We also thank Bill Beluch, Rainer Stal,
Peter Möller and Ulrich Michael for their suggestions and
inspiring discussions.
R EFERENCES
[1] E. D. Dickmanns and B. D. Mysliwetz, “Recursive 3-d road and relative
ego-state recognition,” IEEE Trans. Pattern Anal. Mach. Intell., no. 2,
pp. 199–213, 1992.
[2] C. Urmson et al., “Autonomous driving in urban environments: Boss
and the urban challenge,” J. Field Robotics, vol. 25, no. 8, pp. 425–466,
2008.
[3] R.
Berger,
“Autonomous
driving,”
Think
Act,
2014.
[Online]. Available: http://www.rolandberger.ch/media/pdf/Roland
Berger TABAutonomousDrivingfinal20141211
[4] G. Neuhold, T. Ollmann, S. R. Bulò, and P. Kontschieder, “The
Mapillary Vistas dataset for semantic understanding of street scenes,”
in Proc. IEEE Conf. Computer Vision, Oct. 2017, pp. 5000–5009.
[5] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol.
521, no. 7553, p. 436, 2015.
[6] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous
driving? the KITTI vision benchmark suite,” in Proc. IEEE Conf.
Computer Vision and Pattern Recognition, 2012.
[7] H. Yin and C. Berger, “When to use what data set for your self-driving
car algorithm: An overview of publicly available driving datasets,” in
IEEE 20th Int. Conf. Intelligent Transportation Systems, 2017, pp. 1–8.
[8] D. Ramachandram and G. W. Taylor, “Deep multimodal learning: A
survey on recent advances and trends,” IEEE Signal Process. Mag.,
vol. 34, no. 6, pp. 96–108, 2017.
[9] J. Janai, F. Güney, A. Behl, and A. Geiger, “Computer vision
for autonomous vehicles: Problems, datasets and state-of-the-art,”
arXiv:1704.05519 [cs.CV], 2017.
[10] E. Arnold, O. Y. Al-Jarrah, M. Dianati, S. Fallah, D. Oxtoby, and
A. Mouzakitis, “A survey on 3d object detection methods for autonomous driving applications,” IEEE Trans. Intell. Transp. Syst., pp.
1–14, 2019.
[11] L. Liu et al., “Deep learning for generic object detection: A survey,”
arXiv:1809.02165 [cs.CV], 2018.

15

[12] A. Garcia-Garcia, S. Orts-Escolano, S. Oprea, V. Villena-Martinez, and
J. Garcia-Rodriguez, “A review on deep learning techniques applied to
semantic segmentation,” Applied Soft Computing, 2017.
[13] K. Bengler, K. Dietmayer, B. Farber, M. Maurer, C. Stiller, and
H. Winner, “Three decades of driver assistance systems: Review and
future perspectives,” IEEE Intell. Transp. Syst. Mag., vol. 6, no. 4, pp.
6–22, 2014.
[14] Waymo. (2017) Waymo safety report: On the road to fully self-driving.
[Online]. Available: https://waymo.com/safety
[15] M. Aeberhard et al., “Experience, results and lessons learned from
automated driving on Germany’s highways,” IEEE Intell. Transp. Syst.
Mag., vol. 7, no. 1, pp. 42–57, 2015.
[16] J. Ziegler et al., “Making Bertha drive – an autonomous journey on a
historic route,” IEEE Intell. Transp. Syst. Mag., vol. 6, no. 2, pp. 8–20,
2014.
[17] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
and A. Zisserman, “The PASCAL Visual Object Classes
Challenge
2007
(VOC2007)
Results,”
http://www.pascalnetwork.org/challenges/VOC/voc2007/workshop/index.html.
[18] T.-Y. Lin et al., “Microsoft COCO: Common objects in context,” in
Proc. Eur. Conf. Computer Vision. Springer, 2014, pp. 740–755.
[19] M. Weber, P. Wolf, and J. M. Zöllner, “DeepTLR: A single deep
convolutional network for detection and classification of traffic lights,”
in IEEE Intelligent Vehicles Symp., 2016, pp. 342–348.
[20] J. Müller and K. Dietmayer, “Detecting traffic lights by single shot
detection,” in 21st Int. Conf. Intelligent Transportation Systems. IEEE,
2016, pp. 342–348.
[21] M. Bach, S. Reuter, and K. Dietmayer, “Multi-camera traffic light
recognition using a classifying labeled multi-bernoulli filter,” in IEEE
Intelligent Vehicles Symp., 2017, pp. 1045–1051.
[22] K. Behrendt, L. Novak, and R. Botros, “A deep learning approach to
traffic lights: Detection, tracking, and classification,” in IEEE Int. Conf.
Robotics and Automation, 2017, pp. 1370–1377.
[23] Z. Zhu, D. Liang, S. Zhang, X. Huang, B. Li, and S. Hu, “Traffic-sign
detection and classification in the wild,” in Proc. IEEE Conf. Computer
Vision and Pattern Recognition, 2016, pp. 2110–2118.
[24] H. S. Lee and K. Kim, “Simultaneous traffic sign detection and
boundary estimation using convolutional neural network,” IEEE Trans.
Intell. Transp. Syst., 2018.
[25] H. Luo, Y. Yang, B. Tong, F. Wu, and B. Fan, “Traffic sign recognition
using a multi-task convolutional neural network,” IEEE Trans. Intell.
Transp. Syst., vol. 19, no. 4, pp. 1100–1111, 2018.
[26] S. Zhang, R. Benenson, M. Omran, J. Hosang, and B. Schiele,
“Towards reaching human performance in pedestrian detection,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 40, no. 4, pp. 973–986, 2018.
[27] L. Zhang, L. Lin, X. Liang, and K. He, “Is Faster R-CNN doing well for
pedestrian detection?” in Proc. Eur. Conf. Computer Vision. Springer,
2016, pp. 443–457.
[28] X. Chen, K. Kundu, Y. Zhu, H. Ma, S. Fidler, and R. Urtasun,
“3d object proposals using stereo imagery for accurate object class
detection,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 5, pp.
1259–1272, 2018.
[29] B. Li, “3d fully convolutional network for vehicle detection in point
cloud,” in IEEE/RSJ Int. Conf. Intelligent Robots and Systems, 2017,
pp. 1513–1518.
[30] B. Li, T. Zhang, and T. Xia, “Vehicle detection from 3d lidar using
fully convolutional network,” in Proc. Robotics: Science and Systems,
Jun. 2016.
[31] X. Chen, K. Kundu, Z. Zhang, H. Ma, S. Fidler, and R. Urtasun,
“Monocular 3d object detection for autonomous driving,” in Proc. IEEE
Conf. Computer Vision and Pattern Recognition, 2016, pp. 2147–2156.
[32] J. Fang, Y. Zhou, Y. Yu, and S. Du, “Fine-grained vehicle model recognition using a coarse-to-fine convolutional neural network architecture,”
IEEE Trans. Intell. Transp. Syst., vol. 18, no. 7, pp. 1782–1792, 2017.
[33] A. Mousavian, D. Anguelov, J. Flynn, and J. Košecká, “3d bounding
box estimation using deep learning and geometry,” in Proc. IEEE Conf.
Computer Vision and Pattern Recognition, 2017, pp. 5632–5640.
[34] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun,
“OverFeat: Integrated recognition, localization and detection using
convolutional networks,” in Int. Conf. Learning Representations, 2013.
[35] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature
hierarchies for accurate object detection and semantic segmentation,”
in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2014,
pp. 580–587.
[36] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep
convolutional networks for visual recognition,” IEEE Trans. Pattern
Anal. Mach. Intell., vol. 37, no. 9, pp. 1904–1916, 2015.

[37] R. Girshick, “Fast R-CNN,” in Proc. IEEE Conf. Computer Vision,
2015, pp. 1440–1448.
[38] K. Simonyan and A. Zisserman, “Very deep convolutional networks
for large-scale image recognition,” arXiv:1409.1556 [cs.CV], 2014.
[39] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for
image recognition,” in Proc. IEEE Conf. Computer Vision and Pattern
Recognition, 2016, pp. 770–778.
[40] C. Szegedy et al., “Going deeper with convolutions,” in Proc. IEEE
Conf. Computer Vision and Pattern Recognition, 2015, pp. 1–9.
[41] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards realtime object detection with region proposal networks,” in Advances in
Neural Information Processing Systems, 2015, pp. 91–99.
[42] J. Dai, Y. Li, K. He, and J. Sun, “R-FCN: Object detection via regionbased fully convolutional networks,” in Advances in Neural Information
Processing Systems, 2016, pp. 379–387.
[43] C. Szegedy, A. Toshev, and D. Erhan, “Deep neural networks for object
detection,” in Advances in Neural Information Processing Systems,
2013, pp. 2553–2561.
[44] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only
look once: Unified, real-time object detection,” in Proc. IEEE Conf.
Computer Vision and Pattern Recognition, 2016, pp. 779–788.
[45] W. Liu et al., “SSD: Single shot multibox detector,” in Proc. Eur. Conf.
Computer Vision. Springer, 2016, pp. 21–37.
[46] J. Huang et al., “Speed/accuracy trade-offs for modern convolutional
object detectors,” in Proc. IEEE Conf. Computer Vision and Pattern
Recognition, vol. 4, 2017.
[47] A. Kirillov, K. He, R. Girshick, C. Rother, and P. Dollár, “Panoptic
segmentation,” in Proc. IEEE Conf. Computer Vision and Pattern
Recognition, 2018.
[48] A. Kirillov, R. Girshick, K. He, and P. Dollár, “Panoptic feature
pyramid networks,” in Proc. IEEE Conf. Computer Vision and Pattern
Recognition, 2019, pp. 6399–6408.
[49] Y. Xiong et al., “Upsnet: A unified panoptic segmentation network,”
in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2019,
pp. 8818–8826.
[50] L. Porzi, S. R. Bulo, A. Colovic, and P. Kontschieder, “Seamless
scene segmentation,” in Proc. IEEE Conf. Computer Vision and Pattern
Recognition, 2019, pp. 8277–8286.
[51] B. Wu, A. Wan, X. Yue, and K. Keutzer, “SqueezeSeg: Convolutional
neural nets with recurrent CRF for real-time road-object segmentation
from 3d lidar point cloud,” in IEEE Int. Conf. Robotics and Automation,
May 2018, pp. 1887–1893.
[52] L. Caltagirone, S. Scheidegger, L. Svensson, and M. Wahde, “Fast
lidar-based road detection using fully convolutional neural networks,”
in IEEE Intelligent Vehicles Symp., 2017, pp. 1019–1024.
[53] Q. Huang, W. Wang, and U. Neumann, “Recurrent slice networks for
3d segmentation of point clouds,” in Proc. IEEE Conf. Computer Vision
and Pattern Recognition, 2018, pp. 2626–2635.
[54] A. Dewan, G. L. Oliveira, and W. Burgard, “Deep semantic classification for 3d lidar data,” in IEEE/RSJ Int. Conf. Intelligent Robots and
Systems, 2017, pp. 3544–3549.
[55] A. Dewan and W. Burgard, “DeepTemporalSeg: Temporally consistent semantic segmentation of 3d lidar scans,” arXiv preprint
arXiv:1906.06962, 2019.
[56] A. Milioto, I. Vizzo, J. Behley, and C. Stachniss, “RangeNet++: Fast
and Accurate LiDAR Semantic Segmentation,” in IEEE/RSJ Int. Conf.
Intelligent Robots and Systems, 2019.
[57] M. Cordts et al., “The Cityscapes dataset for semantic urban scene
understanding,” in Proc. IEEE Conf. Computer Vision and Pattern
Recognition, 2016, pp. 3213–3223.
[58] S. Wang et al., “TorontoCity: Seeing the world with a million eyes,”
in Proc. IEEE Conf. Computer Vision, 2017, pp. 3028–3036.
[59] G. Neuhold, T. Ollmann, S. Rota Bulò, and P. Kontschieder, “The
mapillary vistas dataset for semantic understanding of street scenes,”
in Proc. IEEE Conf. Computer Vision, 2017. [Online]. Available:
https://www.mapillary.com/dataset/vistas
[60] X. Huang et al., “The ApolloScape dataset for autonomous driving,” in
Workshop Proc. IEEE Conf. Computer Vision and Pattern Recognition,
2018, pp. 954–960.
[61] L. Schneider et al., “Multimodal neural networks: RGB-D for semantic
segmentation and object detection,” in Scandinavian Conf. Image
Analysis. Springer, 2017, pp. 98–109.
[62] V. Badrinarayanan, A. Kendall, and R. Cipolla, “SegNet: A deep
convolutional encoder-decoder architecture for image segmentation,”
IEEE Trans. Pattern Anal. Mach. Intell., no. 12, pp. 2481–2495, 2017.

16

[63] M. Teichmann, M. Weber, M. Zoellner, R. Cipolla, and R. Urtasun,
“MultiNet: Real-time joint semantic reasoning for autonomous driving,” in IEEE Intelligent Vehicles Symp., 2018.
[64] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask R-CNN,” in
Proc. IEEE Conf. Computer Vision, 2017, pp. 2980–2988.
[65] J. Uhrig, E. Rehder, B. Fröhlich, U. Franke, and T. Brox, “Box2Pix:
Single-shot instance segmentation by assigning pixels to object boxes,”
in IEEE Intelligent Vehicles Symp., 2018.
[66] S. Gupta, R. Girshick, P. Arbeláez, and J. Malik, “Learning rich features
from RGB-D images for object detection and segmentation,” in Proc.
Eur. Conf. Computer Vision. Springer, 2014, pp. 345–360.
[67] B. Hariharan, P. Arbeláez, R. Girshick, and J. Malik, “Simultaneous
detection and segmentation,” in Proc. Eur. Conf. Computer Vision.
Springer, 2014, pp. 297–312.
[68] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
for semantic segmentation,” in Proc. IEEE Conf. Computer Vision and
Pattern Recognition, 2015, pp. 3431–3440.
[69] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,
“DeepLab: Semantic image segmentation with deep convolutional nets,
atrous convolution, and fully connected CRFs,” IEEE Trans. Pattern
Anal. Mach. Intell., vol. 40, no. 4, pp. 834–848, 2018.
[70] A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, “ENet: A
deep neural network architecture for real-time semantic segmentation,”
arXiv:1606.02147 [cs.CV], 2016.
[71] A. Roy and S. Todorovic, “A multi-scale CNN for affordance segmentation in RGB images,” in Proc. Eur. Conf. Computer Vision. Springer,
2016, pp. 186–201.
[72] S. Zheng et al., “Conditional random fields as recurrent neural networks,” in Proc. IEEE Conf. Computer Vision, 2015, pp. 1529–1537.
[73] M. Siam, M. Gamal, M. Abdel-Razek, S. Yogamani, M. Jagersand, and
H. Zhang, “A comparative study of real-time semantic segmentation for
autonomous driving,” in Workshop Proc. IEEE Conf. Computer Vision
and Pattern Recognition, 2018, pp. 587–597.
[74] W. Maddern, G. Pascoe, C. Linegar, and P. Newman, “1 year, 1000
km: The Oxford RobotCar dataset,” Int. J. Robotics Research, vol. 36,
no. 1, pp. 3–15, 2017.
[75] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:
The KITTI dataset,” Int. J. Robotics Research, 2013.
[76] J.-L. Blanco-Claraco, F.-Á. Moreno-Dueñas, and J. González-Jiménez,
“The Málaga urban dataset: High-rate stereo and lidar in a realistic
urban scenario,” Int. J. Robotics Research, vol. 33, no. 2, pp. 207–214,
2014.
[77] H. Jung, Y. Oto, O. M. Mozos, Y. Iwashita, and R. Kurazume, “Multimodal panoramic 3d outdoor datasets for place categorization,” in
IEEE/RSJ Int. Conf. Intelligent Robots and Systems, 2016, pp. 4545–
4550.
[78] Y. Chen et al., “Lidar-video driving dataset: Learning driving policies
effectively,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2018, pp. 5870–5878.
[79] A. Patil, S. Malla, H. Gang, and Y.-T. Chen, “The H3D dataset for
full-surround 3D multi-object detection and tracking in crowded urban
scenes,” in IEEE Int. Conf. Robotics and Automation, 2019.
[80] X. Jianru et al., “BLVD: Building a large-scale 5D semantics benchmark for autonomous driving,” in IEEE Int. Conf. Robotics and
Automation, 2019.
[81] R. Kesten et al. (2019) Lyft level 5 AV dataset 2019. [Online].
Available: https://level5.lyft.com/dataset/
[82] M.-F. Chang et al., “Argoverse: 3D tracking and forecasting with rich
maps,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition,
June 2019.
[83] (2019) PandaSet: Public large-scale dataset for autonomous driving.
[Online]. Available: https://scale.com/open-datasets/pandaset
[84] (2019) Waymo open dataset: An autonomous driving dataset. [Online].
Available: https://www.waymo.com/open
[85] D. Barnes, M. Gadd, P. Murcutt, P. Newman, and I. Posner, “The
Oxford radar RobotCar dataset: A radar extension to the Oxford
RobotCar dataset,” arXiv preprint arXiv: 1909.01300, 2019. [Online].
Available: https://arxiv.org/pdf/1909.01300
[86] Q.-H. Pham et al., “A*3D Dataset: Towards autonomous driving in
challenging environments,” arXiv preprint arXiv: 1909.07541, 2019.
[87] J. Geyer et al. (2019) A2D2: AEV autonomous driving dataset.
[Online]. Available: https://www.audi-electronics-venture.de/aev/web/
en/driving-dataset.html
[88] M. Braun, S. Krebs, F. B. Flohr, and D. M. Gavrila, “EuroCity Persons:
A novel benchmark for person detection in traffic scenes,” IEEE Trans.
Pattern Anal. Mach. Intell., pp. 1–1, 2019.

[89] H. Caesar et al., “nuScenes: A multimodal dataset for autonomous
driving,” arXiv preprint arXiv:1903.11027, 2019.
[90] S. Hwang, J. Park, N. Kim, Y. Choi, and I. So Kweon, “Multispectral
pedestrian detection: Benchmark dataset and baseline,” in Proc. IEEE
Conf. Computer Vision and Pattern Recognition, 2015, pp. 1037–1045.
[91] K. Takumi, K. Watanabe, Q. Ha, A. Tejero-De-Pablos, Y. Ushiku, and
T. Harada, “Multispectral object detection for autonomous vehicles,”
in Proc. Thematic Workshops of ACM Multimedia, 2017, pp. 35–43.
[92] Q. Ha, K. Watanabe, T. Karasawa, Y. Ushiku, and T. Harada, “MFNet:
Towards real-time semantic segmentation for autonomous vehicles with
multi-spectral scenes,” in IEEE/RSJ Int. Conf. Intelligent Robots and
Systems, 2017, pp. 5108–5115.
[93] Y. Choi et al., “KAIST multi-spectral day/night data set for autonomous
and assisted driving,” IEEE Trans. Intell. Transp. Syst., vol. 19, no. 3,
pp. 934–948, 2018.
[94] M. Meyer and G. Kuschk, “Automotive radar dataset for deep learning
based 3d object detection,” in Proceedings of the 16th European Radar
Conference, 2019.
[95] D. Kondermann et al., “Stereo ground truth with error bars,” in 12th
Asian Conf. on Computer Vision. Springer, 2014, pp. 595–610.
[96] M. Larsson, E. Stenborg, L. Hammarstrand, T. Sattler, M. Pollefeys,
and F. Kahl, “A cross-season correspondence dataset for robust semantic segmentation,” in Proc. IEEE Conf. Computer Vision and Pattern
Recognition, 2019.
[97] T. Sattler et al., “Benchmarking 6DOF outdoor visual localization in
changing conditions,” in Proc. IEEE Conf. Computer Vision and Pattern
Recognition, 2018, pp. 8601–8610.
[98] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3D object
detection network for autonomous driving,” in Proc. IEEE Conf.
Computer Vision and Pattern Recognition, 2017, pp. 6526–6534.
[99] A. Asvadi, L. Garrote, C. Premebida, P. Peixoto, and U. J. Nunes,
“Multimodal vehicle detection: fusing 3d-lidar and color camera data,”
Pattern Recognition Lett., 2017.
[100] S.-I. Oh and H.-B. Kang, “Object detection and classification by
decision-level fusion for intelligent vehicle systems,” Sensors, vol. 17,
no. 1, p. 207, 2017.
[101] J. Schlosser, C. K. Chow, and Z. Kira, “Fusing lidar and images for
pedestrian detection using convolutional neural networks,” in IEEE Int.
Conf. Robotics and Automation, 2016, pp. 2198–2205.
[102] Z. Wang, W. Zhan, and M. Tomizuka, “Fusing bird view lidar point
cloud and front view camera image for deep object detection,” in IEEE
Intelligent Vehicles Symp., 2018.
[103] J. Ku, M. Mozifian, J. Lee, A. Harakeh, and S. Waslander, “Joint 3d
proposal generation and object detection from view aggregation,” in
IEEE/RSJ Int. Conf. Intelligent Robots and Systems, Oct. 2018, pp.
1–8.
[104] D. Xu, D. Anguelov, and A. Jain, “PointFusion: Deep sensor fusion for
3D bounding box estimation,” in Proc. IEEE Conf. Computer Vision
and Pattern Recognition, 2018.
[105] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum PointNets
for 3d object detection from RGB-D data,” in Proc. IEEE Conf.
Computer Vision and Pattern Recognition, 2018.
[106] X. Du, M. H. Ang, and D. Rus, “Car detection for autonomous vehicle:
Lidar and vision fusion approach through deep learning framework,”
in IEEE/RSJ Int. Conf. Intelligent Robots and Systems, 2017, pp. 749–
754.
[107] X. Du, M. H. Ang Jr., S. Karaman, and D. Rus, “A general pipeline for
3d detection of vehicles,” in IEEE Int. Conf. Robotics and Automation,
2018.
[108] D. Matti, H. K. Ekenel, and J.-P. Thiran, “Combining lidar space
clustering and convolutional neural networks for pedestrian detection,”
in 14th IEEE Int. Conf. Advanced Video and Signal Based Surveillance,
2017, pp. 1–6.
[109] T. Kim and J. Ghosh, “Robust detection of non-motorized road users
using deep learning on optical and lidar data,” in IEEE 19th Int. Conf.
Intelligent Transportation Systems, 2016, pp. 271–276.
[110] J. Kim, J. Koh, Y. Kim, J. Choi, Y. Hwang, and J. W. Choi, “Robust
deep multi-modal learning based on gated information fusion network,”
in Asian Conf. Computer Vision, 2018.
[111] A. Pfeuffer and K. Dietmayer, “Optimal sensor data fusion architecture
for object detection in adverse weather conditions,” in Proc. 21st Int.
Conf. Information Fusion. IEEE, 2018, pp. 2592–2599.
[112] M. Bijelic, F. Mannan, T. Gruber, W. Ritter, K. Dietmayer, and F. Heide,
“Seeing through fog without seeing fog: Deep sensor fusion in the
absence of labeled training data,” in Proc. IEEE Conf. Computer Vision,
2019.

17

[113] V. A. Sindagi, Y. Zhou, and O. Tuzel, “MVX-Net: Multimodal voxelnet
for 3D object detection,” in IEEE Int. Conf. Robotics and Automation,
2019.
[114] J. Dou, J. Xue, and J. Fang, “SEG-VoxelNet for 3D vehicle detection
from rgb and lidar data,” in IEEE Int. Conf. Robotics and Automation.
IEEE, 2019, pp. 4362–4368.
[115] Z. Wang and K. Jia, “Frustum convnet: Sliding frustums to aggregate
local point-wise features for amodal 3D object detection,” in IEEE/RSJ
Int. Conf. Intelligent Robots and Systems. IEEE, 2019.
[116] M. Liang, B. Yang, Y. Chen, R. Hu, and R. Urtasun, “Multi-task multisensor fusion for 3D object detection,” in Proc. IEEE Conf. Computer
Vision and Pattern Recognition, 2019, pp. 7345–7353.
[117] J. Wagner, V. Fischer, M. Herman, and S. Behnke, “Multispectral
pedestrian detection using deep fusion convolutional neural networks,”
in 24th Eur. Symp. Artificial Neural Networks, Computational Intelligence and Machine Learning, 2016, pp. 509–514.
[118] S. W. Jingjing Liu, Shaoting Zhang and D. Metaxas, “Multispectral
deep neural networks for pedestrian detection,” in Proc. British Machine Vision Conf., Sep. 2016, pp. 73.1–73.13.
[119] D. Guan, Y. Cao, J. Liang, Y. Cao, and M. Y. Yang, “Fusion of
multispectral data through illumination-aware deep neural networks for
pedestrian detection,” Information Fusion, vol. 50, pp. 148–157, 2019.
[120] O. Mees, A. Eitel, and W. Burgard, “Choosing smartly: Adaptive
multimodal fusion for object detection in changing environments,” in
IEEE/RSJ Int. Conf. Intelligent Robots and Systems, 2016, pp. 151–
156.
[121] B. Yang, M. Liang, and R. Urtasun, “HDNET: Exploiting HD maps
for 3D object detection,” in Proc. 2nd Annu. Conf. Robot Learning,
2018, pp. 146–155.
[122] S. Casas, W. Luo, and R. Urtasun, “IntentNet: Learning to predict
intention from raw sensor data,” in Proc. 2nd Annu. Conf. Robot
Learning, 2018, pp. 947–956.
[123] D.-K. Kim, D. Maturana, M. Uenoyama, and S. Scherer, “Seasoninvariant semantic segmentation with a deep multimodal network,” in
Field and Service Robotics. Springer, 2018, pp. 255–270.
[124] Y. Sun, W. Zuo, and M. Liu, “RTFNet: Rgb-thermal fusion network
for semantic segmentation of urban scenes,” IEEE Robotics and Automation Letters, 2019.
[125] A. Valada, G. L. Oliveira, T. Brox, and W. Burgard, “Deep multispectral semantic scene understanding of forested environments using
multimodal fusion,” in Int. Symp. Experimental Robotics. Springer,
2016, pp. 465–477.
[126] A. Valada, J. Vertens, A. Dhall, and W. Burgard, “AdapNet: Adaptive
semantic segmentation in adverse environmental conditions,” in IEEE
Int. Conf. Robotics and Automation, 2017, pp. 4644–4651.
[127] A. Valada, R. Mohan, and W. Burgard, “Self-supervised model adaptation for multimodal semantic segmentation,” Int. J. Computer Vision,
2018.
[128] F. Yang, J. Yang, Z. Jin, and H. Wang, “A fusion model for road
detection based on deep learning and fully connected CRF,” in 13th
Annu. Conf. System of Systems Engineering. IEEE, 2018, pp. 29–36.
[129] L. Caltagirone, M. Bellone, L. Svensson, and M. Wahde, “Lidar-camera
fusion for road detection using fully convolutional neural networks,”
Robotics and Autonomous Systems, vol. 111, pp. 125–131, 2019.
[130] X. Lv, Z. Liu, J. Xin, and N. Zheng, “A novel approach for detecting
road based on two-stream fusion fully convolutional network,” in IEEE
Intelligent Vehicles Symp., 2018, pp. 1464–1469.
[131] F. Wulff, B. Schäufele, O. Sawade, D. Becker, B. Henke, and
I. Radusch, “Early fusion of camera and lidar for robust road detection
based on U-Net FCN,” in IEEE Intelligent Vehicles Symp., 2018, pp.
1426–1431.
[132] Z. Chen, J. Zhang, and D. Tao, “Progressive lidar adaptation for road
detection,” IEEE/CAA Journal of Automatica Sinica, vol. 6, no. 3, pp.
693–702, 2019.
[133] F. Piewak et al., “Boosting lidar-based semantic labeling by crossmodal training data generation,” in Workshop Proc. Eur. Conf. Computer Vision, 2018.
[134] S. Chadwick, W. Maddern, and P. Newman, “Distant vehicle detection
using radar and vision,” in IEEE Int. Conf. Robotics and Automation,
2019.
[135] Y. Zhou and O. Tuzel, “VoxelNet: End-to-end learning for point cloud
based 3d object detection,” in Proc. IEEE Conf. Computer Vision and
Pattern Recognition, 2018.
[136] M. Engelcke, D. Rao, D. Z. Wang, C. H. Tong, and I. Posner,
“Vote3Deep: Fast object detection in 3d point clouds using efficient
convolutional neural networks,” in IEEE Int. Conf. Robotics and
Automation, 2017, pp. 1355–1361.

[137] S. Shi, Z. Wang, X. Wang, and H. Li, “Part − A2 Net: 3d part-aware
and aggregation neural network for object detection from point cloud,”
arXiv preprint arXiv:1907.03670, 2019.
[138] Y. Yan, Y. Mao, and B. Li, “Second: Sparsely embedded convolutional
detection,” Sensors, vol. 18, no. 10, p. 3337, 2018.
[139] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “PointNet: Deep learning on
point sets for 3d classification and segmentation,” in Proc. IEEE Conf.
Computer Vision and Pattern Recognition, Jul. 2017, pp. 77–85.
[140] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “PointNet++: Deep hierarchical
feature learning on point sets in a metric space,” in Advances in Neural
Information Processing Systems, 2017, pp. 5099–5108.
[141] K. Shin, Y. P. Kwon, and M. Tomizuka, “RoarNet: A robust 3D
object detection based on region approximation refinement,” in IEEE
Intelligent Vehicles Symp., 2018.
[142] S. Wang, S. Suo, M. Wei-Chiu, A. Pokrovsky, and R. Urtasun, “Deep
parametric continuous convolutional neural networks,” in Proc. IEEE
Conf. Computer Vision and Pattern Recognition, 2018, pp. 2589–2597.
[143] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen, “PointCNN: Convolution on χ -transformed points,” in Advances in Neural Information
Processing Systems, 2018, pp. 826–836.
[144] A. Asvadi, L. Garrote, C. Premebida, P. Peixoto, and U. J. Nunes,
“DepthCN: Vehicle detection using 3d-lidar and ConvNet,” in IEEE
20th Int. Conf. Intelligent Transportation Systems, 2017.
[145] C. Premebida, L. Garrote, A. Asvadi, A. P. Ribeiro, and U. Nunes,
“High-resolution lidar-based depth mapping using bilateral filter,” in
IEEE 19th Int. Conf. Intelligent Transportation Systems, Nov. 2016,
pp. 2469–2474.
[146] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom,
“PointPillars: Fast encoders for object detection from point clouds,” in
Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2018.
[147] T. Roddick, A. Kendall, and R. Cipolla, “Orthographic feature transform for monocular 3d object detection,” in Proc. British Machine
Vision Conf., 2019.
[148] Y. Wang, W.-L. Chao, D. Garg, B. Hariharan, M. Campbell, and
K. Weinberger, “Pseudo-lidar from visual depth estimation: Bridging
the gap in 3d object detection for autonomous driving,” in Proc. IEEE
Conf. Computer Vision and Pattern Recognition, 2019.
[149] Y. You et al., “Pseudo-lidar++: Accurate depth for 3d object detection
in autonomous driving,” arXiv preprint arXiv:1906.06310, 2019.
[150] M. Liang, B. Yang, S. Wang, and R. Urtasun, “Deep continuous fusion
for multi-sensor 3d object detection,” in Proc. Eur. Conf. Computer
Vision, 2018, pp. 641–656.
[151] K. Werber et al., “Automotive radar gridmap representations,” in IEEE
MTT-S Int. Conf. Microwaves for Intelligent Mobility, 2015, pp. 1–4.
[152] J. Lombacher, M. Hahn, J. Dickmann, and C. Wöhler, “Potential of
radar for static object classification using deep learning methods,” in
IEEE MTT-S Int. Conf. Microwaves for Intelligent Mobility, 2016, pp.
1–4.
[153] J. Lombacher, K. Laudt, M. Hahn, J. Dickmann, and C. Wöhler,
“Semantic radar grids,” in IEEE Intelligent Vehicles Symp., 2017, pp.
1170–1175.
[154] T. Visentin, A. Sagainov, J. Hasch, and T. Zwick, “Classification of
objects in polarimetric radar images using cnns at 77 ghz,” in 2017
IEEE Asia Pacific Microwave Conference (APMC). IEEE, 2017, pp.
356–359.
[155] S. Kim, S. Lee, S. Doo, and B. Shim, “Moving target classification
in automotive radar systems using convolutional recurrent neural networks,” in 26th Eur. Signal Processing Conf. IEEE, 2018, pp. 1482–
1486.
[156] M. G. Amin and B. Erol, “Understanding deep neural networks performance for radar-based human motion recognition,” in IEEE Radar
Conf., 2018, pp. 1461–1465.
[157] O. Schumann, M. Hahn, J. Dickmann, and C. Wöhler, “Semantic segmentation on radar point clouds,” in Proc. 21st Int. Conf. Information
Fusion. IEEE, 2018, pp. 2179–2186.
[158] C. Wöhler, O. Schumann, M. Hahn, and J. Dickmann, “Comparison
of random forest and long short-term memory network performances
in classification tasks using radar,” in Sensor Data Fusion: Trends,
Solutions, Applications. IEEE, 2017, pp. 1–6.
[159] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton, “Adaptive
mixtures of local experts,” Neural Computation, vol. 3, pp. 79–87,
1991.
[160] D. Eigen, M. Ranzato, and I. Sutskever, “Learning factored representations in a deep mixture of experts,” in Workshop Proc. Int. Conf.
Learning Representations, 2014.

18

[161] M. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger, and A. J.
Davison, “Codeslam: learning a compact, optimisable representation
for dense visual slam,” in Proc. IEEE Conf. Computer Vision and
Pattern Recognition, 2018, pp. 2560–2568.
[162] J. Wang, Z. Wei, T. Zhang, and W. Zeng, “Deeply-fused nets,”
arXiv:1605.07716 [cs.CV], 2016.
[163] O. Russakovsky et al., “ImageNet large scale visual recognition challenge,” Int. J. Computer Vision, vol. 115, no. 3, pp. 211–252, 2015.
[164] J. Ngiam et al., “Starnet: Targeted computation for object detection in
point clouds,” arXiv preprint arXiv:1908.11069, 2019.
[165] A. Gaidon, Q. Wang, Y. Cabon, and E. Vig, “Virtual worlds as proxy for
multi-object tracking analysis,” in Proc. IEEE Conf. Computer Vision
and Pattern Recognition, 2016.
[166] S. R. Richter, V. Vineet, S. Roth, and V. Koltun, “Playing for data:
Ground truth from computer games,” in Proc. Eur. Conf. Computer
Vision, 2016, pp. 102–118.
[167] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez, “The
SYNTHIA dataset: A large collection of synthetic images for semantic
segmentation of urban scenes,” in Proc. IEEE Conf. Computer Vision
and Pattern Recognition, 2016, pp. 3234–3243.
[168] S. R. Richter, Z. Hayder, and V. Koltun, “Playing for benchmarks,” in
Proc. IEEE Conf. Computer Vision, Oct. 2017, pp. 2232–2241.
[169] X. Yue, B. Wu, S. A. Seshia, K. Keutzer, and A. L. SangiovanniVincentelli, “A lidar point cloud generator: from a virtual world to
autonomous driving,” in Proc. ACM Int. Conf. Multimedia Retrieval.
ACM, 2018, pp. 458–464.
[170] M. Wrenninge and J. Unger, “Synscapes: A photorealistic synthetic
dataset for street scene parsing,” arXiv preprint arXiv:1810.08705,
2018.
[171] P.-H. Huang, K. Matzen, J. Kopf, N. Ahuja, and J.-B. Huang, “Deepmvs: Learning multi-view stereopsis,” in Proc. IEEE Conf. Computer
Vision and Pattern Recognition, 2018.
[172] D. Griffiths and J. Boehm, “SynthCity: A large scale synthetic point
cloud,” in ArXiv preprint, 2019.
[173] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun,
“CARLA: An open urban driving simulator,” in Proc. 1st Annu. Conf.
Robot Learning, 2017, pp. 1–16.
[174] B. Hurl, K. Czarnecki, and S. Waslander, “Precise synthetic image and
lidar (presil) dataset for autonomous vehicle perception,” arXiv preprint
arXiv:1905.00160, 2019.
[175] J. Lee, S. Walsh, A. Harakeh, and S. L. Waslander, “Leveraging pretrained 3d object detection models for fast ground truth generation,” in
21st Int. Conf. Intelligent Transportation Systems. IEEE, Nov. 2018,
pp. 2504–2510.
[176] J. Mei, B. Gao, D. Xu, W. Yao, X. Zhao, and H. Zhao, “Semantic
segmentation of 3d lidar data in dynamic scene using semi-supervised
learning,” IEEE Trans. Intell. Transp. Syst., 2018.
[177] R. Mackowiak, P. Lenz, O. Ghori, F. Diego, O. Lange, and C. Rother,
“CEREALS – cost-effective region-based active learning for semantic
segmentation,” in Proc. British Machine Vision Conf., 2018.
[178] S. Roy, A. Unmesh, and V. P. Namboodiri, “Deep active learning for
object detection,” in Proc. British Machine Vision Conf., 2018, p. 91.
[179] D. Feng, X. Wei, L. Rosenbaum, A. Maki, and K. Dietmayer, “Deep
active learning for efficient training of a lidar 3d object detector,” in
IEEE Intelligent Vehicles Symp., 2019.
[180] S. J. Pan, Q. Yang et al., “A survey on transfer learning,” IEEE Trans.
Knowl. Data Eng., vol. 22, no. 10, pp. 1345–1359, 2010.
[181] V. M. Patel, R. Gopalan, R. Li, and R. Chellappa, “Visual domain
adaptation: A survey of recent advances,” IEEE Signal Process. Mag.,
vol. 32, no. 3, pp. 53–69, 2015.
[182] Y. Chen, W. Li, X. Chen, and L. V. Gool, “Learning semantic
segmentation from synthetic data: A geometrically guided input-output
adaptation approach,” in Proc. IEEE Conf. Computer Vision and Pattern
Recognition, 2019, pp. 1841–1850.
[183] Y. Chen, W. Li, C. Sakaridis, D. Dai, and L. Van Gool, “Domain
adaptive faster r-cnn for object detection in the wild,” in Proc. IEEE
Conf. Computer Vision and Pattern Recognition, 2018, pp. 3339–3348.
[184] K.-H. Lee, G. Ros, J. Li, and A. Gaidon, “Spigan: Privileged adversarial
learning from simulation,” in Proc. Int. Conf. Learning Representations,
2019.
[185] J. Tremblay et al., “Training deep networks with synthetic data:
Bridging the reality gap by domain randomization,” in Workshop Proc.
IEEE Conf. Computer Vision and Pattern Recognition, 2018, pp. 969–
977.
[186] D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling, “Semisupervised learning with deep generative models,” in Advances in
Neural Information Processing Systems, 2014, pp. 3581–3589.

[187] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter,
“Continual lifelong learning with neural networks: A review,” Neural
Networks, 2019.
[188] Y. Wang et al., “Iterative learning with open-set noisy labels,” in Proc.
IEEE Conf. Computer Vision and Pattern Recognition, 2018, pp. 8688–
8696.
[189] M. Ren, W. Zeng, B. Yang, and R. Urtasun, “Learning to reweight
examples for robust deep learning,” in Int. Conf. Machine Learning,
2018.
[190] L. Jiang, Z. Zhou, T. Leung, L.-J. Li, and L. Fei-Fei, “MentorNet:
Learning data-driven curriculum for very deep neural networks on
corrupted labels,” in Int. Conf. Machine Learning, 2018, pp. 2309–
2318.
[191] X. Ma et al., “Dimensionality-driven learning with noisy labels,” in
Int. Conf. Machine Learning, 2018, pp. 3361–3370.
[192] A. Zlateski, R. Jaroensri, P. Sharma, and F. Durand, “On the importance
of label quality for semantic segmentation,” in Proc. IEEE Conf.
Computer Vision and Pattern Recognition, 2018, pp. 1479–1487.
[193] P. Meletis and G. Dubbelman, “On boosting semantic street scene
segmentation with weak supervision,” in IEEE Intelligent Vehicles
Symp., 2019.
[194] C. Haase-Schütz, H. Hertlein, and W. Wiesbeck, “Estimating labeling
quality with deep object detectors,” in IEEE Intelligent Vehicles Symp.,
June 2019, pp. 33–38.
[195] S. Chadwick and P. Newman, “Training object detectors with noisy
data,” in IEEE Intelligent Vehicles Symp., June 2019, pp. 1319–1325.
[196] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for
dense object detection,” IEEE Trans. Pattern Anal. Mach. Intell., 2018.
[197] M. Giering, V. Venugopalan, and K. Reddy, “Multi-modal sensor
registration for vehicle perception via deep neural networks,” in IEEE
High Performance Extreme Computing Conf., 2015, pp. 1–6.
[198] N. Schneider, F. Piewak, C. Stiller, and U. Franke, “RegNet: Multimodal sensor registration using deep neural networks,” in IEEE
Intelligent Vehicles Symp., 2017, pp. 1803–1810.
[199] S. Ramos, S. Gehrig, P. Pinggera, U. Franke, and C. Rother, “Detecting
unexpected obstacles for self-driving cars: Fusing deep learning and
geometric modeling,” in IEEE Intelligent Vehicles Symp., 2017, pp.
1025–1032.
[200] H. Banzhaf, M. Dolgov, J. Stellet, and J. M. Zöllner, “From footprints
to beliefprints: Motion planning under uncertainty for maneuvering
automated vehicles in dense scenarios,” in 21st Int. Conf. Intelligent
Transportation Systems. IEEE, 2018, pp. 1680–1687.
[201] D. J. C. MacKay, “A practical Bayesian framework for backpropagation
networks,” Neural Computation, vol. 4, no. 3, pp. 448–472, 1992.
[202] G. E. Hinton and D. Van Camp, “Keeping the neural networks simple
by minimizing the description length of the weights,” in Proc. 6th
Annu. Conf. Computational Learning Theory. ACM, 1993, pp. 5–13.
[203] Y. Gal, “Uncertainty in deep learning,” Ph.D. dissertation, University
of Cambridge, 2016.
[204] A. Graves, “Practical variational inference for neural networks,” in
Advances in Neural Information Processing Systems, 2011, pp. 2348–
2356.
[205] S. Mandt, M. D. Hoffman, and D. M. Blei, “Stochastic gradient descent
as approximate Bayesian inference,” J. Machine Learning Research,
vol. 18, no. 1, pp. 4873–4907, 2017.
[206] M. Teye, H. Azizpour, and K. Smith, “Bayesian uncertainty estimation
for batch normalized deep networks,” in Int. Conf. Machine Learning,
2018.
[207] J. Postels, F. Ferroni, H. Coskun, N. Navab, and F. Tombari, “Samplingfree epistemic uncertainty estimation using approximated variance
propagation,” in Proc. IEEE Conf. Computer Vision, 2019.
[208] A. Kendall, V. Badrinarayanan, and R. Cipolla, “Bayesian SegNet:
Model uncertainty in deep convolutional encoder-decoder architectures
for scene understanding,” in Proc. British Machine Vision Conf., 2017.
[209] D. Miller, L. Nicholson, F. Dayoub, and N. Sünderhauf, “Dropout
sampling for robust object detection in open-set conditions,” in IEEE
Int. Conf. Robotics and Automation, 2018.
[210] D. Miller, F. Dayoub, M. Milford, and N. Sünderhauf, “Evaluating
merging strategies for sampling-based uncertainty techniques in object
detection,” in IEEE Int. Conf. Robotics and Automation, 2018.
[211] A. Kendall and Y. Gal, “What uncertainties do we need in Bayesian
deep learning for computer vision?” in Advances in Neural Information
Processing Systems, 2017, pp. 5574–5584.
[212] E. Ilg et al., “Uncertainty estimates and multi-hypotheses networks for
optical flow,” in Proc. Eur. Conf. Computer Vision, 2018.

19

[213] D. Feng, L. Rosenbaum, and K. Dietmayer, “Towards safe autonomous
driving: Capture uncertainty in the deep neural network for lidar 3d
vehicle detection,” in 21st Int. Conf. Intelligent Transportation Systems,
Nov. 2018, pp. 3266–3273.
[214] D. Feng, L. Rosenbaum, F. Timm, and K. Dietmayer, “Leveraging
heteroscedastic aleatoric uncertainties for robust real-time lidar 3d
object detection,” in IEEE Intelligent Vehicles Symp., 2019.
[215] G. P. Meyer, A. Laddha, E. Kee, C. Vallespi-Gonzalez, and C. K.
Wellington, “Lasernet: An efficient probabilistic 3d object detector for
autonomous driving,” in Proc. IEEE Conf. Computer Vision and Pattern
Recognition, 2019, pp. 12 677–12 686.
[216] S. Wirges, M. Reith-Braun, M. Lauer, and C. Stiller, “Capturing object
detection uncertainty in multi-layer grid maps,” in IEEE Intelligent
Vehicles Symp., 2019.
[217] M. T. Le, F. Diehl, T. Brunner, and A. Knol, “Uncertainty estimation
for deep neural object detectors in safety-critical applications,” in IEEE
21st Int. Conf. Intelligent Transportation Systems. IEEE, 2018, pp.
3873–3878.
[218] D. Feng, L. Rosenbaum, C. Gläser, F. Timm, and K. Dietmayer, “Can
we trust you? on calibration of a probabilistic object detector for
autonomous driving,” arXiv:1909.12358 [cs.RO], 2019.
[219] D. P. Kingma and M. Welling, “Auto-encoding variational Bayes,” in
Int. Conf. Learning Representations, 2014.
[220] I. Goodfellow et al., “Generative adversarial nets,” in Advances in
Neural Information Processing Systems, 2014, pp. 2672–2680.
[221] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther,
“Autoencoding beyond pixels using a learned similarity metric,” in Int.
Conf. Machine Learning, 2016, pp. 1558–1566.
[222] A. Deshpande, J. Lu, M.-C. Yeh, M. J. Chong, and D. A. Forsyth,
“Learning diverse image colorization,” in Proc. IEEE Conf. Computer
Vision and Pattern Recognition, 2016, pp. 2877–2885.
[223] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image
translation with conditional adversarial networks,” in Proc. IEEE Conf.
Computer Vision and Pattern Recognition, 2017, pp. 5967–5976.
[224] T. A. Wheeler, M. Holder, H. Winner, and M. J. Kochenderfer, “Deep
stochastic radar models,” in IEEE Intelligent Vehicles Symp., 2017, pp.
47–53.
[225] X. Han, J. Lu, C. Zhao, S. You, and H. Li, “Semi-supervised and
weakly-supervised road detection based on generative adversarial networks,” IEEE Signal Process. Lett., 2018.
[226] J. L. Elman, “Learning and development in neural networks: The
importance of starting small,” Cognition, vol. 48, no. 1, pp. 71–99,
1993.
[227] J. Feng and T. Darrell, “Learning the structure of deep convolutional
networks,” in Proc. IEEE Conf. Computer Vision, 2015, pp. 2749–2757.
[228] D. Ramachandram, M. Lisicki, T. J. Shields, M. R. Amer, and G. W.
Taylor, “Structure optimization for deep multimodal fusion networks
using graph-induced kernels,” in 25th Eur. Symp. Artificial Neural
Networks, Computational Intelligence and Machine Learning, 2017.
[229] D. Whitley, T. Starkweather, and C. Bogart, “Genetic algorithms and
neural networks: Optimizing connections and connectivity,” Parallel
computing, vol. 14, no. 3, pp. 347–361, 1990.
[230] B. Zoph and Q. V. Le, “Neural architecture search with reinforcement
learning,” arXiv:1611.01578 [cs.LG], 2016.
[231] P. Kulkarni, J. Zepeda, F. Jurie, P. Pérez, and L. Chevallier, “Learning
the structure of deep architectures using l1 regularization,” in Proc.
British Machine Vision Conf., 2015.
[232] C. Murdock, Z. Li, H. Zhou, and T. Duerig, “Blockout: Dynamic
model selection for hierarchical deep networks,” in Proc. IEEE Conf.
Computer Vision and Pattern Recognition, 2016, pp. 2583–2591.
[233] F. Li, N. Neverova, C. Wolf, and G. Taylor, “Modout: Learning multimodal architectures by stochastic regularization,” in 12th IEEE Int.
Conf. Automatic Face & Gesture Recognition, 2017, pp. 422–429.
[234] A. Bilal, A. Jourabloo, M. Ye, X. Liu, and L. Ren, “Do convolutional
neural networks learn class hierarchy?” IEEE Trans. Vis. Comput.
Graphics, vol. 24, no. 1, pp. 152–162, 2018.
[235] M. Liu, J. Shi, Z. Li, C. Li, J. Zhu, and S. Liu, “Towards better analysis
of deep convolutional neural networks,” IEEE Trans. Vis. Comput.
Graphics, vol. 23, no. 1, pp. 91–100, 2017.
[236] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing
deep neural networks with pruning, trained quantization and huffman
coding,” in Int. Conf. Learning Representations, 2015.
[237] A. G. Howard et al., “MobileNets: Efficient convolutional neural
networks for mobile vision applications,” arXiv:1704.04861 [cs.CV],
2017.

[238] Y. Cheng, D. Wang, P. Zhou, and T. Zhang, “A survey of model
compression and acceleration for deep neural networks,” IEEE Signal
Process. Mag., 2017.
[239] L. Enderich, F. Timm, L. Rosenbaum, and W. Burgard, “Learning
multimodal fixed-point weights using gradient descent,” in 27th Eur.
Symp. Artificial Neural Networks, Computational Intelligence and
Machine Learning, 2019.
[240] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman, “The PASCAL visual object classes (VOC) challenge,” Int. J.
Computer Vision, vol. 88, no. 2, pp. 303–338, 2010.
[241] A. P. Dawid, “The well-calibrated Bayesian,” J. American Statistical
Association, vol. 77, no. 379, pp. 605–610, 1982.
[242] S. Liang, Y. Li, and R. Srikant, “Enhancing the reliability of out-ofdistribution image detection in neural networks,” in Int. Conf. Learning
Representations, 2017.
[243] D. Hall, F. Dayoub, J. Skinner, P. Corke, G. Carneiro, and
N. Sünderhauf, “Probability-based detection quality (PDQ): A probabilistic approach to detection evaluation,” arXiv:1811.10800 [cs.CV],
2018.
[244] W. Luo, B. Yang, and R. Urtasun, “Fast and furious: Real time endto-end 3d detection, tracking and motion forecasting with a single
convolutional net,” in Proc. IEEE Conf. Computer Vision and Pattern
Recognition, 2018, pp. 3569–3577.
[245] M. Bojarski et al., “End to end learning for self-driving cars,”
arXiv:1604.07316 [cs.CV], 2016.
[246] G.-H. Liu, A. Siravuru, S. Prabhakar, M. Veloso, and G. Kantor,
“Learning end-to-end multimodal sensor policies for autonomous navigation,” in Proc. 1st Annu. Conf. Robot Learning, 2017, pp. 249–261.
[247] M. Bansal, A. Krizhevsky, and A. Ogale, “Chauffeurnet: Learning
to drive by imitating the best and synthesizing the worst,” in Proc.
Robotics: Science and Systems, 2019.
[248] A. Sauer, N. Savinov, and A. Geiger, “Conditional affordance learning
for driving in urban environments,” in Proc. 2st Annu. Conf. Robot
Learning, 2018, pp. 237–252.
[249] C. Chen, A. Seff, A. Kornhauser, and J. Xiao, “DeepDriving: Learning
affordance for direct perception in autonomous driving,” in Proc. IEEE
Conf. Computer Vision, 2015, pp. 2722–2730.
[250] B. Yang, W. Luo, and R. Urtasun, “PIXOR: Real-time 3d object
detection from point clouds,” in Proc. IEEE Conf. Computer Vision
and Pattern Recognition, 2018, pp. 7652–7660.
[251] Ö. Erkent, C. Wolf, C. Laugier, D. S. González, and V. R. Cano,
“Semantic grid estimation with a hybrid bayesian and deep neural
network approach,” in 2018 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS). IEEE, 2018, pp. 888–895.
[252] S. Gu, T. Lu, Y. Zhang, J. Alvarez, J. Yang, and H. Kong, “3D lidar+
monocular camera: an inverse-depth induced fusion framework for
urban road detection,” IEEE Transactions on Intelligent Vehicles, 2018.
[253] Y. Cai, D. Li, X. Zhou, and X. Mou, “Robust drivable road region detection for fixed-route autonomous vehicles using map-fusion images,”
Sensors, vol. 18, no. 12, p. 4158, 2018.

Di Feng (Member, IEEE) is currently pursuing
his doctoral degree in the Corporate Research of
Robert Bosch GmbH, Renningen, in cooperation
with the Ulm University. He finished his master’s
degree with distinction in electrical and computer
engineering at the Technical University of Munich.
During his studies, he was granted the opportunity
to work in several teams with reputable companies
and research institutes such as BMW AG, German
Aerospace Center (DLR), and Institute for Cognitive
Systems (ICS) at Technical University of Munich.
He received the bachelor’s degree in mechatronics with honor from Tongji
University. His current research is centered on robust multi-modal object
detection using deep learning approach for autonomous driving. He is also
interested in robotic active learning and exploration through tactile sensing
and cognitive systems.

20

Christian Haase-Schütz (Member, IEEE) is currently pursuing his PhD degree at Chassis Systems
Control, Robert Bosch GmbH, Abstatt, in cooperation with the Karlsruhe Institute of Technology. Before joining Bosch, he finished his master’s degree in physics at the Friedrich-AlexanderUniversity Erlangen-Nuremberg. He did his thesis
with the Center for Medical Physics. During his
master studies he was granted a scholarship by
the Bavarian state to visit Huazhong University of
Science and Technology, Wuhan, China, from March
2015 till July 2015. He received his bachelor’s degree in physics from the
University of Stuttgart in 2013, where he did his thesis with the Max Planck
Institute for Intelligent Systems. His professional experience includes work
with ETAS GmbH, Stuttgart, and Andreas Stihl AG, Waiblingen. His current
research is centered on multi-modal object detection using deep learning
approaches for autonomous driving. He is further interested in challenges
of AI systems in the wild. Christian Haase-Schütz is a member of the IEEE
and the German Physical Society DPG.

Lars Rosenbaum received his Dipl.-Inf. (M.S.) and
the Dr. rer. nat. (Ph.D.) degrees in bioinformatics
from the University of Tuebingen, Germany, in
2009 and 2013, respectively. During this time he
was working on machine learning approaches for
computer-aided molecular drug design and analysis
of metabolomics data. In 2014, he joined ITK Engineering in Marburg, Germany, working on driver
assistance systems. Since 2016, he is a research engineer at Corporate Research, Robert Bosch GmbH
in Renningen, Germany, where he is currently doing
research on machine learning algorithms in the area of perception for
automated driving.

Heinz Hertlein (Member, IEEE) received the
Dipl.-Inf. degree (diploma in computer science)
from the Friedrich-Alexander-University ErlangenNuremberg, Germany, in 1999, and the Dr.-Ing.
(Ph.D.) degree from the same university in 2010 for
his research in the field of biometric speaker recognition. From 2002, he was working on algorithms and
applications of multi modal biometric pattern recognition at the company BioID in Erlangen-Tennenlohe
and Nuremberg. From 2012, he was appointed at the
University of Hertfordshire in the UK, initially as a
Postdoctoral Research Fellow and later as a Senior Lecturer. He was teaching
in the fields of signal processing and pattern recognition, and his research
activities were mainly focused on biometric speaker and face recognition.
Since 2015, he is employed at Chassis Systems Control, Robert Bosch GmbH
in Abstatt, Germany, where he is currently working in the area of perception
for autonomous driving.

Claudius Gläser was born in Gera, Germany in
1982. He received his Diploma degree in Computer
Science from the Technical University of Ilmenau,
Germany, in 2006, and the Dr.-Ing. degree (equivalent to PhD) from Bielefeld University, Germany,
in 2012. From 2006 he was a Research Scientist
at the Honda Research Institute Europe GmbH in
Offenbach/Main, Germany, working in the fields of
speech processing and language understanding for
humanoid robots. In 2011 he joined the Corporate
Research of Robert Bosch GmbH in Renningen,
Germany, where he developed perception algorithms for driver assistance
and highly automated driving functions. Currently, he is Team Lead for
perception for automated driving and manages various related projects. His
research interests include environment perception, multimodal sensor data
fusion, multi-object tracking, and machine learning for highly automated
driving.

Fabian Timm studied Computer Science at the
University of Lübeck, Germany. In 2006 he did
his diploma thesis at Philips Lighting Systems in
Aachen, Germany. Afterwards he started his PhD
also at Philips Lighting Systems in the field of
Machine Vision and Machine Learning and finished
it in 2011 at the University of Lübeck, Institute
for Neuro- and Bioinformatics. In 2012 he joined
corporate research at Robert Bosch GmbH and
worked on industrial image processing and machine
learning. Afterwards he worked in the business unit
at Bosch and developed new perception algorithms, such as pedestrian and
cyclist protection only with a single radar sensor. Since 2018 he leads the
research group ”automated driving perception and sensors” at Bosch corporate
research. His main research interests are machine and deep learning, signal
processing, and sensors for automated driving.

Werner Wiesbeck (Fellow, IEEE) received the
Dipl.-Ing. (M.S.) and the Dr. -Ing. (Ph.D.) degrees in
electrical engineering from the Technical University
Munich, Germany, in 1969 and 1972, respectively.
From 1972 to 1983, he was with product responsibility for mm-wave radars, receivers, direction finders,
and electronic warfare systems in industry. From
1983 to 2007, he was the Director of the Institut für
Höchstfrequenztechnik und Elektronik, University of
Karlsruhe. He is currently a Distinguished Senior
Fellow at the Karlsruhe Institute of Technology. His
research topics include antennas, wave propagation, radar, remote sensing,
wireless communication, and ultra wide band technologies.
He has authored and co-authored several books and over 850 publications,
is a supervisor of over 90 Ph.D. students, a responsible supervisor of over
600 Diploma-/Master theses, and holds over 60 patents. He is an Honorary
Life Member of IEEE GRS-S and a member of the Heidelberger Academy
of Sciences and Humanities, and the German Academy of Engineering and
Technology. He was a recipient of a number of awards, including the IEEE
Millennium Award, the IEEE GRS Distinguished Achievement Award, the
Honorary Doctorate (Dr. h. c.) from the University Budapest/Hungary, the
Honorary Doctorate (Dr.-Ing. E. h.) from the University Duisburg/Germany,
the Honorary Doctorate (Dr. -Ing. E. h.) from Technische Universität Ilmenau,
and the IEEE Electromagnetics Award in 2008. He is the Chairman of the
GRS-S Awards Committee. He was the Executive Vice President of IEEE
GRS-S (1998-1999) and the President of IEEE GRS-S (2000-2001). He has
been a general chairman of several conferences.

Klaus Dietmayer (Member, IEEE) was born in
Celle, Germany in 1962. He received his Diploma
degree in 1989 in Electrical Engineering from the
Technical University of Braunschweig (Germany),
and the Dr.-Ing. degree (equivalent to PhD) in 1994
from the University of Armed Forces in Hamburg
(Germany). In 1994 he joined the Philips Semiconductors Systems Laboratory in Hamburg, Germany
as a research engineer. Since 1996 he became a
manager in the field of networks and sensors for automotive applications. In 2000 he was appointed to a
professorship at the University of Ulm in the field of measurement and control.
Currently he is Full Professor and Director of the Institute of Measurement,
Control and Microtechnology in the school of Engineering and Computer
Science at the University of Ulm. Research interests include information
fusion, multi-object tracking, environment perception, situation understanding
and trajectory planning for autonomous driving. Klaus Dietmayer is member
of the IEEE and the German society of engineers VDI / VDE.

TABLE II: OVERVIEW OF MULTI-MODAL DATASETS
Name

Sensing Modalities

Year (published)
2019

Labelled
(benchmark)
3D bounding boxes

Recording area

Size

Categories / Remarks

Link

Astyx HiRes2019 [94]

Radar, Visual camera,
3D LiDAR

n.a.

500 frames (5000 annotated
objects)

Car, Bus, Cyclist, Motorcyclist,
Person, Trailer, Truck

https://www.astyx.com/development/
astyx-hires2019-dataset.html

A2D2 [87]

Visual cameras (6); 3D
LiDAR (5); Bus data

2019

2D/3D bounding
boxes, 2D/3D instance
segmentation

Gaimersheim,
Ingolstadt,
Munich

40k frames (semantics), 12k
frames (3D objects), 390k
frames unlabeled

Car, Bicycle, Pedestrian, Truck,
Small vehicles, Traffic signal,
Utility vehicle, Sidebars, Speed
bumper, Curbstone, Solid line,
Irrelevant signs, Road blocks,
Tractor, Non-drivable street, Zebra
crossing, Obstacles / trash, Poles,
RD restricted area, Animals, Grid
structure, Signal corpus, Drivable
cobbleston, Electronic traffic,
Slow drive area, Nature object,
Parking area, Sidewalk, Ego car,
Painted driv. instr., Traffic guide
obj., Dashed line, RD normal
street, Sky, Buildings, Blurred
area, Rain dirt

https://www.audi-electronics-venture.de/
aev/web/en/driving-dataset.html

A*3D Dataset [86]

Visual cameras (2); 3D
LiDAR

2019

3D bounding boxes

Singapore

39k frames, 230k objects

Car, Van, Bus, Truck, Pedestrians,
Cyclists, and Motorcyclists;
Afternoon and night, wet and dry

https://github.com/I2RDL2/ASTAR-3D

EuroCity Persons [88]

Visual camera;
Announced: stereo,
LiDAR, GNSS and
intertial sensors

2019

2D bounding boxes

12 countries in
Europe, 27 cities

47k frames, 258k objects

Pedestrian, Rider, Bicycle,
Motorbike, Scooter, Tricycle,
Wheelchair, Buggy, Co-Rider;
Highly diverse: 4 seasons, day and
night, wet and dry

https://eurocity-dataset.tudelft.nl/eval/
overview/home

Oxford RobotCar [74], [85]

2016: Visual cameras
(fisheye & stereo), 2D
& 3D LiDAR, GNSS,
and inertial sensors;
2019: Radar, 3D Lidar
(2), 2D LiDAR (2),
visual cameras (6),
GNSS and inertial
sensors

2016, 2019

no

Oxford

2016: 11, 070, 651 frames
(stereo), 3, 226, 183 frames
(3D LiDAR); 2019: 240k
scans (Radar), 2.4M frames
(LiDAR)

Long-term autonomous driving.
Various weather conditions,
including heavy rain, night, direct
sunlight and snow.

http://robotcar-dataset.robots.ox.ac.uk/
downloads/, http://ori.ox.ac.uk/datasets/
radar-robotcar-dataset

Waymo Open Dataset [84]

3D LiDAR (5), Visual
cameras (5)

2019

3D bounding box,
Tracking

n.a.

200k frames, 12M objects
(3D LiDAR), 1.2M objects
(2D camera)

Vehicles, Pedestrians, Cyclists,
Signs

https://waymo.com/open/

Lyft Level 5 AV Dataset
2019 [81]

3D LiDAR (5), Visual
cameras (6)

2019

3D bounding box

n.a.

55k frames

Semantic HD map included

https://level5.lyft.com/dataset/

Argoverse [82]

3D LiDAR (2), Visual
cameras (9, 2 stereo)

2019

3D bounding box,
Tracking, Forecasting

Pittsburgh,
Pennsylvania,
Miami, Florida

113 scenes, 300k trajectories

Vehicle, Pedestrian, Other Static,
Large Vehicle, Bicycle, Bicyclist,
Bus, Other Mover, Trailer,
Motorcyclist, Moped, Motorcycle,
Stroller, Emergency Vehicle,
Animal, Wheelchair, School Bus;
Semantic HD maps (2) included

https://www.argoverse.org/data.html

PandaSet [83]

3D LiDAR (2), Visual
cameras (6), GNSS
and inertial sensors

2019

3D bounding box

San Francisco,
El Camino Real

Announced: 60k frames
(camera), 20k frames
(LiDAR), 125 scenes

28 classes, 37 semantic
segmentation labels; Solid state
LiDAR

https://scale.com/open-datasets/pandaset

21

TABLE II: OVERVIEW OF MULTI-MODAL DATASETS
Name

Sensing Modalities

Year (published)

Labelled
(benchmark)

Recording area

Size

Categories / Remarks

Link

nuScenes dataset [89]

Visual cameras (6), 3D
LiDAR, and Radars
(5)

2019

3D bounding box

Boston,
Singapore

1000 scenes, 1.4M frames
(camera, Radar), 390k
frames (3D LiDAR)

25 Object classes, such as Car /
Van / SUV, different Trucks,
Buses, Persons, Animal, Traffic
Cone, Temporary Traffic Barrier,
Debris, etc.

https://www.nuscenes.org/download

BLVD [80]

Visual (Stereo)
camera, 3D LiDAR

2019

3D bounding box,
Tracking, Interaction,
Intention

Changshu

120k frames,
249, 129 objects

Vehicle, Pedestrian, Rider during
day and night

https://github.com/VCCIV/BLVD/

H3D dataset [79]

Visual cameras (3), 3D
LiDAR

2019

3D bounding box

San Francisco,
Mountain View,
Santa Cruz, San
Mateo

27, 721 frames,
1, 071, 302 objects

Car, Pedestrian, Cyclist, Truck,
Misc, Animals, Motorcyclist, Bus

https:
//usa.honda-ri.com/hdd/introduction/h3d

ApolloScape [60]

Visual (Stereo)
camera, 3D LiDAR,
GNSS, and inertial
sensors

2018, 2019

2D/3D pixel-level
segmentation, lane
marking, instance
segmentation, depth

Multiple areas in
China

143, 906 image frames,
89, 430 objects

Rover, Sky, Car, Motobicycle,
Bicycle, Person, Rider, Truck,
Bus, Tricycle, Road, Sidewalk,
Traffic Cone, Road Pile, Fence,
Traffic Light, Pole, Traffic Sign,
Wall, Dustbin, Billboard,
Building, Bridge, Tunnel,
Overpass, Vegetation

http://apolloscape.auto/scene.html

DBNet Dataset [78]

3D LiDAR, Dashboard
visual camera, GNSS

2018

Driving behaviours
(Vehicle speed and
wheel angles)

Multiple areas in
China

Over 10k frames

In total seven datasets with
different test scenarios, such as
seaside roads, school areas,
mountain roads.

http://www.dbehavior.net/

KAIST multispectral dataset
[93]

Visual (Stereo) and
thermal camera, 3D
LiDAR, GNSS, and
inertial sensors

2018

2D bounding box,
drivable region, image
enhancement, depth,
and colorization

Seoul

7, 512 frames,
308, 913 objects

Person, Cyclist, Car during day
and night, fine time slots (sunrise,
afternoon,...)

http://multispectral.kaist.ac.kr

Multi-spectral Object
Detection dataset [91]

Visual and thermal
cameras

2017

2D bounding box

University
environment in
Japan

7, 512 frames, 5, 833 objects

Bike, Car, Car Stop, Color Cone,
Person during day and night

https://www.mi.t.u-tokyo.ac.jp/static/
projects/mil multispectral/

Multi-spectral Semantic
Segmentation dataset [92]

Visual and thermal
camera

2017

2D pixel-level
segmentation

n.a.

1569 frames

Bike, Car, Person, Curve,
Guardrail, Color Cone, Bump
during day and night

https://www.mi.t.u-tokyo.ac.jp/static/
projects/mil multispectral/

Multi-modal Panoramic 3D
Outdoor (MPO) dataset [77]

Visual camera,
LiDAR, and GNSS

2016

Place categorization

Fukuoka

650 scans (dense),
34200 scans (sparse)

No dynamic objects

http:
//robotics.ait.kyushu-u.ac.jp/∼kurazume/
research-e.php?content=db#d08

KAIST multispectral
pedestrian [90]

Visual and thermal
camera

2015

2D bounding box

Seoul

95, 328 frames,
103, 128 objects

Person, People, Cyclist during day
and night

https://sites.google.com/site/
pedestrianbenchmark/home

KITTI [6], [75]

Visual (Stereo)
camera, 3D LiDAR,
GNSS, and inertial
sensors

2012,
2013, 2015

2D/3D bounding box,
visual odometry, road,
optical flow, tracking,
depth, 2D instance and
pixel-level
segmentation

Karlsruhe

7481 frames (training)
80.256 objects

Car, Van, Truck, Pedestrian,
Person (sitting), Cyclist, Tram,
Misc

http://www.cvlibs.net/datasets/kitti/

The Málaga Stereo and
Laser Urban dataset [76]

Visual (Stereo)
camera, 5× 2D
LiDAR (yielding 3D
information), GNSS
and inertial sensors

2014

no

Málaga

113, 082 frames, 5, 654.6 s
(camera); > 220, 000 frames,
5, 000 s (LiDARs)

n.a.

https:
//www.mrpt.org/MalagaUrbanDataset

22

TABLE III: SUMMARY OF MULTI-MODAL OBJECT DECTECTION METHODS
Reference

Sensors

Obj Type

Sensing Modality Representations
and Processing
LiDAR BEV maps, RGB image.
Each processed by a ResNet with
auxiliary tasks: depth estimation
and ground segmentation

Network
Pipeline
Faster
R-CNN

How to generate Region
Proposals (RP) a
Predictions with fused
features

When to
fuse
Before RP

Fusion Operation and
Method
Addition, continuous fusion
layer

Fusion
Levelb
Middle

Dataset(s) used

Liang et
al., 2019
[116]

LiDAR, visual
camera

3D Car, Pedestrian,
Cyclist

Wang et al.,
2019 [115]

LiDAR, visual
camera

3D Car, Pedestrian,
Cyclist, Indoor
objects

LiDAR voxelized frustum (each
frustum processed by the PointNet),
RGB image (using a pre-trained
detector).

R-CNN

Pre-trained RGB image
detector

After RP

Using RP from RGB image
detector to build LiDAR
frustums

Late

KITTI, SUN-RGBD

Dou et al.,
2019 [114]

LiDAR, visual
camera

3D Car

LiDAR voxel (processed by
VoxelNet), RGB image (processed
by a FCN to get semantic features)

Two
stage
detector

Predictions with fused
features

Before RP

Feature concatenation

Middle

KITTI

Sindagi et
al., 2019
[113]

LiDAR, visual
camera

3D Car

LiDAR voxel (processed by
VoxelNet), RGB image (processed
by a pre-trained 2D image
detector).

One
stage
detector

Predictions with fused
features

Before RP

Feature concatenation

Early,
Middle

KITTI

Bijelic et
al., 2019
[112]

LiDAR, visual
camera

2D Car in foggy
weather

Lidar front view images (depth,
intensity, height), RGB image. Each
processed by VGG16

SSD

Predictions with fused
features

Before RP

Feature concatenation

From
early to
middle
layers

Self-recorded datasets
focused on foggy weather,
simulated foggy images
from KITTI

Chadwick
et al., 2019
[134]

Radar, visual
camera

2D Vehicle

Radar range and velocity maps,
RGB image. Each processed by
ResNet

One
stage
detector

Predictions with fused
features

Before RP

Addition, feature
concatenation

Middle

Self-recorded

Liang et
al., 2018
[150]

LiDAR, visual
camera

3D Car, Pedestrian,
Cyclist

LiDAR BEV maps, RGB image.
Each processed by ResNet

One
stage
detector

Predictions with fused
features

Before RP

Addition, continuous fusion
layer

Middle

KITTI, self-recorded

Du et al.,
2018 [107]

LiDAR, visual
camera

3D Car

LiDAR voxel (processed by
RANSAC and model fitting), RGB
image (processed by VGG16 and
GoogLeNet)

R-CNN

Pre-trained RGB image
detector produces 2D
bounding boxes to crop
LiDAR points, which are
then clustered

Before and
at RP

Ensemble: use RGB image
detector to regress car
dimensions for a model
fitting algorithm

Late

KITTI, self-recorded data

Kim et al,
2018 [110]

LiDAR, visual
camera

2D Car

LiDAR front-view depth image,
RGB image Each input processed
by VGG16

SSD

SSD with fused features

Before RP

Feature concatenation,
Mixture of Experts

Middle

KITTI

Yang et al.,
2018 [121]

LiDAR, HD-map

3D Car

LiDAR BEV maps, Road mask
image from HD map. Inputs
processed by PIXOR++ [250] with
the backbone similar to FPN

One
stage
detector

Detector predictions

Before RP

Feature concatenation

Early

KITTI, TOR4D
Dataset [250]

Pfeuffer et
al., 2018
[111]

LiDAR, visual
camera

Multiple 2D objects

LiDAR spherical, and front-view
sparse depth, dense depth image,
RGB image. Each processed by
VGG16

Faster
R-CNN

RPN from fused features

Before RP

Feature concatenation

Early,
Middle,
Late

KITTI

Casas et
al., 2018
[122]c

LiDAR, HD-map

3D Car

sequential LiDAR BEV maps,
sequential several road topology
mask images from HD map. Each
input processed by a base network
with residual blocks

One
stage
detector

Detector predictions

Before RP

Feature concatenation

Middle

self-recorded data

Guan et al.,
2018 [119]

visual camera,
thermal camera

2D Pedestrian

RGB image, thermal image. Each
processed by a base network built
on VGG16

Faster
R-CNN

RPN with fused features

Before and
after RP

Feature concatenation,
Mixture of Experts

Early,
Middle,
Late

KAIST Pedestrian Dataset

KITTI, self-recorded

23

TABLE III: SUMMARY OF MULTI-MODAL OBJECT DECTECTION METHODS
Sensors

Obj Type

Sensing Modality Representations
and Processing

Network
Pipeline

How to generate Region
Proposals (RP) a

When to
fuse

Fusion Operation and
Method

Fusion
Levelb

Dataset(s) used

Shin et al.,
2018 [141]

LiDAR, visual
camera

3D Car

LiDAR point clouds, (processed by
PointNet [139]); RGB image
(processed by a 2D CNN)

R-CNN

A 3D object detector for
RGB image

After RP

Using RP from RGB image
detector to search LiDAR
point clouds

Late

KITTI

Schneider
et al., 2017
[61]

Visual camera

Multiple 2D objects

RGB image (processed by
GoogLeNet), depth image from
stereo camera (processed by NiN
net)

SSD

SSD predictions

Before RP

Feature concatenation

Early,
Middle,
Late

Cityscape

Takumi et
al., 2017
[91]

Visual camera,
thermal camera

Multiple 2D objects

RGB image, NIR, FIR, FIR image.
Each processed by YOLO

YOLO

YOLO predictions for each
spectral image

After RP

Ensemble: ensemble final
predictions for each YOLO
detector

Late

self-recorded data

Chen et al.,
2017 [98]

LiDAR, visual
camera

3D Car

LiDAR BEV and spherical maps,
RGB image. Each processed by a
base network built on VGG16

Faster
R-CNN

A RPN from LiDAR BEV
map

After RP

average mean, deep fusion

Early,
Middle,
Late

KITTI

Asvadi et
al., 2017
[99]

LiDAR, visual
camera

2D Car

LiDAR front-view dense-depth
(DM) and reflectance maps (RM),
RGB image. Each processed
through a YOLO net

YOLO

YOLO outputs for LiDAR
DM and RM maps, and
RGB image

After RP

Ensemble: feed engineered
features from ensembled
bounding boxes to a
network to predict scores
for NMS

Late

KITTI

Oh et al.,
2017 [100]

LiDAR, visual
camera

2D Car, Pedestrian,
Cyclist

LiDAR front-view dense-depth map
(for fusion: processed by VGG16),
LiDAR voxel (for ROIs:
segmentation and region growing),
RGB image (for fusion: processed
by VGG16; for ROIs: segmentation
and grouping)

R-CNN

LiDAR voxel and RGB
image separately

After RP

Association matrix using
basic belief assignment

Late

KITTI

Wang et al.,
2017 [102]

LiDAR, visual
camera

3D Car, Pedestrian

LiDAR BEV map, RGB image.
Each processed by a
RetinaNet [196]

One
stage
detector

Fused LiDAR and RGB
image features extracted
from CNN

Before RP

Sparse mean manipulation

Middle

KITTI

Ku et al.,
2017 [103]

LiDAR, visual
camera

3D Car, Pedestrian,
Cyclist

LiDAR BEV map, RGB image.
Each processed by VGG16

Faster
R-CNN

Fused LiDAR and RGB
image features extracted
from CNN

Before and
after RP

Average mean

Early,
Middle,
Late

KITTI

Xu et al.,
2017 [104]

LiDAR, visual
camera

3D Car, Pedestrian,
Cyclist, Indoor
objects

LiDAR points (processed by
PointNet), RGB image (processed
by ResNet)

R-CNN

Pre-trained RGB image
detector

After RP

Feature concatenation for
local and global features

Middle

KITTI, SUN-RGBD

Qi et al.,
2017 [105]

LiDAR, visual
camera

3D Car, Pedestrian,
Cyclist, Indoor
objects

LiDAR points (processed by
PointNet), RGB image (using a
pre-trained detector)

R-CNN

Pre-trained RGB image
detector

After RP

Feature concatenation

Middle,
Late

KITTI, SUN-RGBD

Du et al.,
2017 [106]

LiDAR, visual
camera

2D Car

LiDAR voxel (processed by
RANSAC and model fitting), RGB
image (processed by VGG16 and
GoogLeNet)

Faster
R-CNN

First clustered by LiDAR
point clouds, then
fine-tuned by a RPN of
RGB image

Before RP

Ensemble: feed LiDAR RP
to RGB image-based CNN
for final prediction

Late

KITTI

Matti et al.,
2017 [108]

LiDAR, visual
camera

2D Pedestrian

LiDAR points (clustering with
DBSCAN) and RGB image
(processed by ResNet)

R-CNN

Clustered by LiDAR point
clouds, then size and ratio
corrected on RGB image.

Before and
at RP

Ensemble: feed LiDAR RP
to RGB image-based CNN
for final prediction

Late

KITTI

Kim et al.,
2016 [109]

LiDAR, visual
camera

2D Pedestrian,
Cyclist

LiDAR front-view depth image,
RGB image. Each processed by
Fast R-CNN network [37]

Fast
R-CNN

Selective search for LiDAR
and RGB image separately.

At RP

Ensemble: joint RP are fed
to RGB image based CNN.

Late

KITTI

Mees et al.,
2016 [120]

RGB-D camera

2D Pedestrian

RGB image, depth image from
depth camera, optical flow. Each
processed by GoogLeNet

Fast
R-CNN

Dense multi-scale sliding
window for RGB image

After RP

Mixture of Experts

Late

RGB-D People Unihall
Dataset, InOutDoor RGB-D
People Dataset.

24

Reference

TABLE III: SUMMARY OF MULTI-MODAL OBJECT DECTECTION METHODS
Reference

Sensors

Obj Type

Sensing Modality Representations
and Processing

Network
Pipeline

How to generate Region
Proposals (RP) a

When to
fuse

Fusion Operation and
Method

Fusion
Levelb

Dataset(s) used

Wagner et
al., 2016
[117]

visual camera,
thermal camera

2D Pedestrian

RGB image, thermal image. Each
processed by CaffeeNet

R-CNN

ACF+T+THOG detector

After RP

Feature concatenation

Early,
Late

KAIST Pedestrian Dataset

Liu et al.,
2016 [118]

Visual camera,
thermal camera

2D Pedestrian

RGB image, thermal image. Each
processed by NiN network

Faster
R-CNN

RPN with fused (or
separate) features

Before and
after RP

Feature concatenation,
average mean, Score fusion
(Cascaded CNN)

Early,
Middle,
Late

KAIST Pedestrian Dataset

Schlosser et
al., 2016
[101]

LiDAR, visual
camera

2D Pedestrian

LiDAR HHA image, RGB image.
Each processed by a small ConvNet

R-CNN

Deformable Parts Model
with RGB image

After RP

Feature concatenation

Early,
Middle,
Late

KITTI

a
b

25

c

For one-stage detector, we refer region proposals to be the detection outputs of a network.
Some methods compare multiple fusion levels. We mark the fusion level with the best reported performance in bold.
Besides object detection, this paper also proposes intention prediction and trajectory prediction up to 3s in the unified network (multi-task prediction).

TABLE IV: SUMMARY OF MULTI-MODAL SEMANTIC SEGMENTATION METHODS
a

Reference

Sensors

Semantics

Sensing Modality Representations

Fusion Operation and Method

Fusion Level

Chen et al., 2019
[132]

LiDAR, visual camera

Road segmentation

RGB image, altitude difference image. Each
processed by a CNN

Feature adaptation module, modified
concatenation.

Middle

KITTI

Dataset(s) used

Valada et al., 2019
[127]

Visual camera, depth
camera, thermal camera

Multiple 2D objects

RGB image, thermal image, depth image. Each
processed by FCN with ResNet backbone
(Adapnet++ architecture)

Extension of Mixture of Experts

Middle

Six datasets,
including
Cityscape, Sun
RGB-D, etc.

Sun et al., 2019
[124]

Visual camera, thermal
camera

Multiple 2D objects in
campus environments

RGB image, thermal image. Each processed by a
base network built on ResNet

Element-wise summation in the encoder
networks

Middle

Datasets published
by [92]

Caltagirone et al.,
2019 [129]

LiDAR, visual camera

Road segmentation

LiDAR front-view depth image, RGB image. Each
input processed by a FCN

Feature concatenation (For early and late
fusion), weighted addition similar to gating
network (for middle-level cross fusion)

Early, Middle, Late

KITTI

Erkent et al., 2018
[251]

LiDAR, visual camera

Multiple 2D objects

LiDAR BEV occupancy grids (processed based on
Bayesian filtering and tracking), RGB image
(processed by a FCN with VGG16 backbone)

Feature concatenation

Middle

KITTI,
self-recorded

Lv et al., 2018
[130]

LiDAR, visual camera

Road segmentation

LiDAR BEV maps, RGB image. Each input
processed by a FCN with dilated convolution
operator. RGB image features are also projected
onto LiDAR BEV plane before fusion

Feature concatenation

Middle

KITTI

Wulff et al., 2018
[131]

LiDAR, visual camera

Road segmentation.
Alternatives: freespace,
ego-lane detection

LiDAR BEV maps, RGB image projected onto
BEV plane. Inputs processed by a FCN with UNet

Feature concatenation

Early

KITTI

Kim et al., 2018
[123]

LiDAR, visual camera

2D Off-road terrains

LiDAR voxel (processed by 3D convolution), RGB
image (processed by ENet)

Addition

Early, Middle, Late

self-recorded data

Guan et al., 2018
[119]b

Visual camera, thermal
camera

2D Pedestrian

RGB image, thermal image. Each processed by a
base network built on VGG16

Feature concatenation, Mixture of Experts

Early, Middle, Late

KAIST Pedestrian
Dataset

Yang et al., 2018
[128]

LiDAR, visual camera

Road segmentation

LiDAR points (processed by PointNet++), RGB
image (processed by FCN with VGG16 backbone)

Optimizing Conditional Random Field
(CRF)

Late

KITTI

Gu et al., 2018
[252]

LiDAR, visual camera

Road segmentation

LiDAR front-view depth and height maps
(processed by a inverse-depth histogram based line
scanning strategy), RGB image (processed by a
FCN).

Optimizing Conditional Random Field

Late

KITTI

Cai et al., 2018
[253]

Satellite map with route
information, visual
camera

Road segmentation

Route map image, RGB image. Images are fused
and processed by a FCN

Overlaying the line and curve segments in
the route map onto the RGB image to
generate the Map Fusion Image (MFI)

Early

self-recorded data

Ha et al., 2017
[92]

Visual camera, thermal
camera

Multiple 2D objects in
campus environments

RGB image, thermal image. Each processed by a
FCN and mini-inception block

Feature concatenation, addition (“short-cut
fusion”)

Middle

self-recorded data

Valada et al., 2017
[126]

Visual camera, thermal
camera

Multiple 2D objects

RGB image, thermal image, depth image. Each
processed by FCN with ResNet backbone

Mixture of Experts

Late

Cityscape, Freiburg
Multispectral
Dataset, Synthia

Schneider et al.,
2017 [61]

Visual camera

Multiple 2D Objects

RGB image, depth image

Feature concatenation

Early, Middle, Late

Cityscape

Schneider et al.,
2017 [61]2

Visual camera

Multiple 2D Objects

RGB image (processed by GoogLeNet), depth
image from stereo camera (processed by NiN net)

Feature concatenation

Early, Middle, Late

Cityscape

Valada et al., 2016
[125]

Visual camera, thermal
camera

Multiple 2D objects in
forested environments

RGB image, thermal image, depth image. Each
processed by the UpNet (built on VGG16 and
up-convolution)

Feature concatenation, addition

Early, Late

self-recorded data
26

a
b

Some methods compare multiple fusion levels. We mark the fusion level with the best reported performance in bold.
They also test the methods for object detection problem with different network architectures (see Table III).

TABLE V: PERFORMANCE AND RUNTIME FOR 3D OBJECT DETECTION ON KITTI TEST SET
Reference

Liang et al., 2019 [116]
Wang et al., 2019 [115]
Sindagi et al., 2019 [113]
Shin et al., 2018 [141]
Du et al., 2018 [107]
Liang et al., 2018 [150]
Ku et al., 2017 [103]
Qi et al., 2017 [105]
Chen et al., 2017 [98]

Moderate

Car
Easy

Hard

76.75 %
76.51 %
72.7 %
73.04 %
73.80 %
66.22 %
71.88 %
70.39 %
62.35 %

86.81 %
85.88 %
83.2 %
83.71 %
84.33 %
82.54 %
81.94 %
81.20 %
71.09 %

68.41
68.08
65.12
59.16
64.83
64.04
66.38
62.19
55.12

%
%
%
%
%
%
%
%
%

Moderate

Pedestrian
Easy

45.61 %
42.81 %
44.89 %
-

52.37 %
50.80 %
51.21 %
-

Hard

Moderate

Cyclist
Easy

Hard

Runtime

41.49 %
40.88 %
40.23 %
-

64.68 %
52.18 %
56.77 %
-

79.58 %
64.00 %
71.96 %
-

57.03 %
46.61 %
50.39 %
-

0.08 s
0.47 s
0.5 s
0.06 s
0.1 s
0.17 s
0.36 s

Environment

GPU
GPU
GPU
GPU
GPU
GPU
GPU
GPU

@ 2.5 Ghz (Python)
@ 2.5 Ghz (Python + C/C++)
Titan X (not Pascal)
@ 2.5 Ghz (Matlab + C/C++)
@ 2.5 Ghz (Python)
Titan X (Pascal)
@ 3.0 Ghz (Python)
@ 2.5 Ghz (Python + C/C++)

TABLE VI: PERFORMANCE AND RUNTIME FOR ROAD SEGMENTATION (URBAN) ON KITTI TEST SET
Method

MaxF

Chen et al., 2019 [132]
Caltagirone et al., 2019 [129]
Gu et al., 2018 [252]
Lv et al., 2018 [130]
Yang et al., 2018 [128]

97.03
96.03
95.22
94.48
91.40

AP
%
%
%
%
%

94.03
93.93
89.31
93.65
84.22

PRE
%
%
%
%
%

97.19
96.23
94.69
94.28
89.09

REC
%
%
%
%
%

96.88
95.83
95.76
94.69
93.84

FPR
%
%
%
%
%

1.54
2.07
2.96
3.17
6.33

FNR
%
%
%
%
%

3.12
4.17
4.24
5.31
6.16

%
%
%
%
%

Runtime

Environment

0.16 s
0.15 s
0.07 s
-

GPU
GPU
CPU
GPU Titan X
GPU

25

