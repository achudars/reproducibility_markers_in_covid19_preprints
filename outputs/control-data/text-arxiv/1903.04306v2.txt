arXiv:1903.04306v2 [math.ST] 20 Sep 2019

Consistency of the maximum likelihood and variational
estimators in a dynamic stochastic block model
Léa Longepierre and Catherine Matias
Sorbonne Université, Université Paris Diderot, Centre National de la Recherche Scientifique,
Laboratoire de Probabilités, Statistique et Modélisation,
4 place Jussieu, 75252 PARIS Cedex 05, FRANCE.
{lea.longepierre,catherine.matias}@sorbonne-universite.fr
Abstract
We consider a dynamic version of the stochastic block model, in which the nodes are partitioned into latent
classes and the connection between two nodes is drawn from a Bernoulli distribution depending on the classes
of these two nodes. The temporal evolution is modeled through a hidden Markov chain on the nodes memberships. We prove the consistency (as the number of nodes and time steps increase) of the maximum likelihood and
variational estimators of the model parameters, and obtain upper bounds on the rates of convergence of these estimators. We also explore the particular case where the number of time steps is fixed and connectivity parameters
are allowed to vary.

Keywords: maximum likelihood estimation, dynamic network, dynamic stochastic block model, variational
estimation, temporal network

1 Introduction
Random graphs are a suitable tool to model and describe interactions in many kinds of datasets such as biological,
ecological, social or transport networks. Here we are interested in time-evolving networks, which is a powerful tool
for modeling real-world phenomena, where the role or behaviour of the nodes in the network and the relationships
between them are allowed to change over time. Indeed, it is important to take into account the evolutionary
behaviour of the graphs, instead of just studying separate snapshots as static graphs. We focus on graphs evolving
in discrete time and refer to Holme [2015] for an introduction to dynamic networks.
A myriad of dynamic graph models has been introduced in the past few years, see for instance Zhang et al.
[2017]. We focus here on those which are based on the (static) stochastic block model [SBM, Holland et al., 1983]
in which the nodes are partitioned into classes. In the SBM, class memberships of the nodes are represented by
latent variables and the connection between two nodes is drawn from a distribution depending on the classes of
these two nodes (a Bernoulli distribution in the case of binary graphs). A first dynamic version of the SBM with
discrete time is proposed in Yang et al. [2011]. There, the nodes are partitioned into Q classes and the graphs
are binary or weighted. The nodes are allowed to change membership over time, and these changes are governed
by independent Markov chains with values in the Q classes, while the connection probabilities are constant over
time. Xu and Hero [2014] introduce a state-space model on the logit of the connection probabilities for dynamic
(binary) networks with connection probabilities and group memberships varying over time. Unfortunately, their
model presents parameter identifiability issues [Matias and Miele, 2017]. Xu [2015] proposes a stochastic block
transition model in which the presence or absence of an edge between two nodes at a particular time affects the
presence or absence of such an edge at a future time. There, the nodes can change classes over time, new nodes can
enter the network, and the connection probabilities are allowed to vary over time. The model in Matias and Miele
[2017] and in Becker and Holzmann [2018] is quite similar to that of Yang et al. [2011] except that it allows
the connection probabilities to vary and the latter is moreover nonparametric. Bartolucci et al. [2018] extend the
model of Yang et al. [2011] to deal with different forms of reciprocity in directed graphs, by directly modeling
dyadic relations and with the assumption that the dyads are conditionally independent given the latent variables.
Paul and Chen [2016] and Han et al. [2015] study multi-graph SBM, arising in settings including dynamic networks and multi-layer networks where each layer corresponds to a type of edge. In these two models, the nodes
memberships stay constant over the layers. Pensky [2019], Pensky et al. [2019] study a dynamic SBM for undirected and binary edges where both connection probabilities and group memberships vary over time, assuming that
the connection probabilities between groups are a smooth function of time. Xing et al. [2010] and Ho et al. [2011]
1

introduce dynamic versions of the mixed-membership stochastic block model, allowing each actor to carry out
different roles when interacting with different peers. Zreik et al. [2016] introduce the dynamic random subgraph
model, given a known decomposition of the graph into subgraphs, in which the latent class membership depends
on the subgraph membership and the edges are categorical variables, their types being sampled from a distribution
depending on the latent classes of the two nodes. There, a state-space model is used to characterize the temporal
evolution of the latent classes proportions.
As far as estimation is concerned, different methods of inference are proposed to estimate groups and model
parameters. The maximum likelihood estimator (MLE) is not tractable in the SBM, thus neither in its dynamic
versions. Variational methods are rather popular to approximate that MLE [Xing et al., 2010, Ho et al., 2011,
Han et al., 2015, Paul and Chen, 2016, Zreik et al., 2016, Matias and Miele, 2017, Bartolucci et al., 2018]. Yang et al.
[2011] rely on Gibbs sampling and simulated annealing. Pensky et al. [2019] propose an estimator of the connection probabilities matrix at each time step by a discrete kernel-type method and obtain a clustering of the nodes
thanks to spectral clustering on this estimated matrix. They also give an estimator for the number of clusters.
Spectral clustering algorithms are also used by Han et al. [2015] on the mean graph over time and by Liu et al.
[2018] who use eigenvector smoothing to get some similarity across time periods (and allow the number of classes
to be unknown and possibly varying over time).
Some theoretical results on the convergence of the procedures have been proven, mainly for static graphs. In
the static SBM, Celisse et al. [2012] prove the consistency of the MLE and variational estimates as the number of
nodes increases, and Bickel et al. [2013] establish their asymptotic normality. Mariadassou and Matias [2015] have
a different approach and give sufficient conditions for the groups posterior distribution to converge to a Dirac mass
located at the actual groups configuration, for every parameter in a neighborhood of the true one. Rohe et al. [2011]
give asymptotic results on the normalized graph Laplacian and its eigenvectors for the spectral clustering algorithm,
allowing the number of clusters to grow with the number of nodes. They also provide bounds on the number of
misclustered nodes, requiring an assumption on the degree distribution. Lei and Rinaldo [2015] prove consistency
for the recovery of communities in the spectral clustering on the adjacency matrix, with milder conditions on the
degrees, and also extend this result to degree corrected stochastic block models. Klopp et al. [2017] derive oracle
inequalities for the connection probabilities estimator and obtain minimax estimation rates, including the sparse
case where the density of edges converges to zero as the number of nodes increase thus extending previous results
of Gao et al. [2015]. Gaucher and Klopp [2019] propose a bound on the risk of the maximum likelihood estimator
of network connection probabilities, and show that it is minimax optimal in the sparse graphon model.
In the dynamic setting, fewer theoretical results have been established. Pensky [2019] derives a penalized
least squares estimator of the connection probabilities adaptive to the number of blocks and which does not require knowledge of the number of classes Q. She shows that it satisfies an oracle inequality. Under the additional
assumption that at most n0 nodes change groups between two time steps, this estimator attains minimax lower
bounds for the risk. She also introduces a dynamic graphon model and shows that the estimators (that do not
require knowledge of a degree of smoothness of the graphon function) are minimax optimal within a logarithmic
factor of the number of time steps. Based on the same dynamic SBM with at most n0 nodes changing groups
between two time steps, Pensky et al. [2019] give an upper bound for the (non asymptotic) error of their estimators
of the connection probabilities matrix and group memberships (and also an estimator for the number of clusters).
Han et al. [2015] show consistency (as the number of time steps increases but the number of nodes is fixed) of
two estimators of the class memberships for dynamic SBM (and more generally multi-graph SBM) in which the
nodes memberships are constant over time but the connection probabilities are allowed to vary and the considered
graphs are binary and symmetric. They show that the spectral clustering (on the mean graph over time) estimator
of the class memberships is consistent under some stationarity and ergodicity conditions on the connection probabilities. They also prove that the MLE of the class memberships is consistent (i.e. that the fraction of misclustered
nodes converges to 0) in the general case (without any structure on the connection probabilities), provided certain
sufficient conditions are satisfied. In their multi-layer model, Paul and Chen [2016] give minimax rates of misclassification under certain conditions on the growth of the types of relations, number of nodes and number of classes,
extending the result of Han et al. [2015].
Here, we consider a dynamic version of the binary SBM as in Yang et al. [2011], where each node is allowed
to change group membership at each time step according to a Markov chain, independently of other nodes. We
prove the consistency of the connectivity parameter MLE and, under some additional conditions, of the transition
matrix MLE, when the number of nodes and of time steps are increasing. We also give upper bounds on the
rates of convergence of these estimators. While these upper bounds are known to be non optimal in the static
case where asymptotic normality is obtained with classical parametric rates of convergence [Bickel et al., 2013],
these are the first to be established in a dynamic setting for the MLE. As already mentioned, the log-likelihood is
intractable (except for very small values of the number of nodes n and the number of time steps T ), as it requires to
sum over QnT terms. Thus, while its consistency remains an important result, the estimator cannot be computed. A
2

possible alternative is to rely on a variational estimator to approximate the MLE [see for instance Matias and Miele,
2017]. We also establish the consistency of the variational estimator of the connectivity parameter and under some
additional assumptions, that of the variational estimator of the transition matrix and obtain the same upper bounds
on the rates of convergence as for the MLE. In the particular case where the number of time steps T is fixed, we
also consider the model of Matias and Miele [2017], in which the connection probabilities are allowed to vary over
time and generalise these results with only the number of nodes increasing. When T = 1, we not only recover
the results of Celisse et al. [2012] but extend these by giving rates of convergence. Unlike the model studied in
Han et al. [2015] and Paul and Chen [2016], the node memberships in our model evolve over time. Our context is
different from Pensky [2019] that focuses on least squares estimate.
This article is organized as follows. Section 2 introduces our model and notation. More precisely, Section 2.1 describes the dynamic stochastic block model as introduced in Yang et al. [2011], Section 2.2 gives the
assumptions we make on the model parameters, Section 2.3 describes the dynamic stochastic block model as in
Matias and Miele [2017] for the finite time case and Section 2.4 states the expression of the likelihood of this
model to define the MLE. Section 3 establishes the consistency and upper bounds of the rates of convergence for
the MLE of the connection probabilities in Section 3.1 and of the transition matrix in Section 3.2. Section 4 is
dedicated to variational estimators: Section 4.1 and 4.2 establish the consistency of the variational estimators of
the connection probabilities and transition matrix, respectively, along with upper bounds of the associated rates of
convergence. All the proofs of the main results are postponed to Section 5, except those for the fixed T case that
are in Appendix A, while the more technical proofs are deferred to Appendix B.

2 Model and notation
2.1 Dynamic stochastic block model
We consider a set of n vertices, forming a sequence of binary undirected graphs with no self-loops at each time
t = 1, . . . , T . The case of a set of directed graphs, with or without self-loops, may be handled similarly. These
vertices are assumed to be split into Q latent classes, and we denote by Zit the label of the i-th vertex at time t.
Letting Zi = (Zi1 , . . . , ZiT ), we assume that the {Zi }1≤i≤n are independent and identically distributed (iid) and each Zi
is a homogeneous and stationary Markov chain with transition probabilities
P(Zit+1 = l | Zit = q) = γql ,

∀1 ≤ q, l ≤ Q

where Γ = (γql )1≤q,l≤Q is a stochastic matrix, i.e. with nonnegative coefficients and with each row summing to
1. We let α = (α1 , . . . , αQ ) the stationary distribution of the Markov chain. For any i ∈ ~1, n, the probability
distribution of Zi is then
Pθ (Zi ) = α

Zi1

T −1
Y

γZit Zit+1 .

t=1

We will also denote Z t = (Z1t , . . . , Znt ) and Z 1:T = (Z 1 , . . . , Z T ) = (Zit )1≤t≤T,1≤i≤n .
Consider X t = {Xit j }1≤i, j≤n the symmetric binary adjacency matrix of the graph at time t such that for every
nodes 1 ≤ i, j ≤ n, we have Xiit = 0 and Xit j = X tji . Each X t follows a stochastic block model so that, conditional on
the latent groups {Zit }1≤i≤n , the {Xit j }1≤i, j≤n are independent Bernoulli random variables
Xit j | Zit = q, Z tj = l ∼ B(πql )
2

where (πql )1≤q,l≤Q ∈ [0, 1]Q are the connectivity parameters. More precisely, conditional on the whole sequence
of latent groups {Zit }1≤t≤T,1≤i≤n , the graphs X 1:T = X 1 , . . . , X T are assumed to be independent, each X t having a
distribution depending only on {Zit }1≤i≤n . The model is thus parameterized by θ = (Γ, π), with Γ = (γql )1≤q,l≤Q and
π = (πql )1≤q,l≤Q . Note that π is a symmetric matrix in the undirected setup. We denote by Pθ (resp. Eθ ) the probability distribution (resp. expectation) of all the random variables {Zit , Xit j }t≥1;i, j≥1 , under the parameter value θ. In the
∗
following, we assume that we observe {Xit j }1≤i, j,≤n, 1≤t≤T and we denote by θ∗ = (Γ∗ , π∗ ) = ((γql
)1≤q,l≤Q , (π∗ql )1≤q,l≤Q )
the true parameter value, with corresponding probability distribution Pθ∗ and expectation Eθ∗ , and by α∗ = (α∗q )1≤q≤Q
the (true) stationary distribution corresponding to the transition matrix Γ∗ . We also let 1A denote the indicator function of the set A and Ac the complementary set of A in the ambient set. For any integer M ≥ 1, the set ~1, M is the
set of integers between 1 and M. For any finite set A, let |A| denote its cardinality. For any configuration z1:T , we
denote Nq (zt ) (resp. Nql (z1:T )) the number of nodes assigned to class q by the configuration zt (resp. the number of
3

transitions from class q to class l in configuration z1:T ), that is
Nq (zt ) = |{i ∈ ~1, n; zti = q}|

and

Nql (z1:T ) =

T −1 X
n
X

1zti =q,zt+1
.
i =l

(1)

t=1 i=1

We also define for any two parameters θ = (Γ, π) and θ′ = (Γ′ , π′ ) the following distances
kπ − π′ k∞ = max |πql − π′ql |
1≤q,l≤Q

and

′
kΓ − Γ′ k∞ = max |γql − γql
|.
1≤q,l≤Q

2.2 Assumptions
The assumptions we make on the model parameters are the following.
1. For every 1 ≤ q , q′ ≤ Q, there exists some l ∈ ~1, Q such that πql , πq′ l .
2. There exists some 0 < δ < 1/Q such that for any (q, l) ∈ ~1, Q2 , we have γql ∈ [δ, 1 − δ].
3. There exists some ζ > 0 such that for any (q, l) ∈ ~1, Q2, we have πql ∈ [ζ, 1 − ζ].
Assumption 1 is necessary for identifiability of the model. Indeed, if it does not hold, we cannot distinguish
between classes q and q′ . Assumption 2 ensures that each Markov chain Zi is irreducible, aperiodic and recurrent.
This assumption could be weakened at the cost of technicalities. In particular, it implies that the stationary distribution α exists. Moreover, Assumption 2 also implies that for any q ∈ ~1, Q, we have αq ∈ [δ, 1 − δ]. Note that
this can be seen as an equivalent of Assumption 2 in Celisse et al. [2012] (on the probability distribution of the
class memberships) in the dynamic case. Celisse et al. [2012] however also have an additional assumption that is
an empirical version of this assumption (which states that the observed class proportions are bounded away from
0) that is true with high probability. We do not make such an assumption and use the fact that the probability of
this event converges to 1. Assumption 3 is technical and could also be weakened with additional technicalities. For
example, Celisse et al. [2012] also consider the case πql ∈ {0, 1} (i.e. πql ∈ {0, 1} ∪ [ζ, 1 − ζ]) whereas we do not.
The whole parameter set defined by these constraints is denoted by Θ. In the following, we assume that θ∗ ∈ Θ.
In what follows, we work up to label permutation on the groups. Indeed, as in any latent group model, the
parameters can only be recovered up to label switching on the latent groups. We then define the following notation
for any permutation σ ∈ SQ with SQ the set of permutations on ~1, Q


θσ = (Γσ , πσ ) = (γσ(q)σ(l) )1≤q,l≤Q , (πσ(q)σ(l) )1≤q,l≤Q .

2.3 Finite time case
If the number of time steps T is fixed, it is possible to let the connection probabilities vary over time. We then
consider this case, the connection parameter now being π1:T = (π1 , . . . , πT ) with πt = (πtql )1≤q,l≤Q for every t ∈
~1, T  and πtql = Pθ (Xit j = 1 | Zit = q, Z tj = l) for any (t, q, l) ∈ ~1, T  × ~1, Q2 . Note that this is the more general
model of Matias and Miele [2017], in which the model parameter is θ = (Γ, π1:T ). Moreover, we introduce the
following Assumptions 1’ and 3’ that are alternate versions of Assumptions 1 and 3 respectively for the finite time
case.
1’. For every t ∈ ~1, T , for every 1 ≤ q , q′ ≤ Q, there exists some l ∈ ~1, Q such that πtql , πtq′ l .
3’. There exists some ζ > 0 such that for every t ∈ ~1, T , for any (q, l) ∈ ~1, Q2, we have πtql ∈ [ζ, 1 − ζ].
Assumption 1’ (resp. Assumption 3’) expresses that for every t ∈ ~1, T , πt satisfies Assumption 1 (resp. Assumption 3). We also introduce the following additional assumption, which ensures (together with Assumption 1’) that
the model is identifiable (up to a label permutation). See Matias and Miele [2017].
1
2
4. For every q ∈ ~1, Q, for every t1 , t2 ∈ ~1, T , πtqq
= πtqq
≔ πqq and {πqq ; q ∈ ~1, Q} are Q distinct values.

Assumption 4 states that the diagonal of π does not change over time, and that its values are distinct. We denote
by ΘT the set of parameters satisfying Assumptions 1’, 2, 3’ and 4. As before, we assume in the following that
θ∗ ∈ ΘT in the fixed T case. We also define as before for any π1:T and π′1:T the distance
kπ1:T − π′1:T k∞ =

max

(q,l,t)∈~1,Q2 ×~1,T 

4

|πtql − π′tql |.

2.4 Likelihood
The conditional log-likelihood and the log-likelihood write
ℓc (θ; Z 1:T ) = log Pθ (X 1:T | Z 1:T ) =


1:T
and ℓ(θ) = log Pθ (X ) = log 

T
X
t=1

log Pθ (X t | Z t ) =

X

e

ℓc (θ;z1:T )

T
X
X

t=1 1≤i< j≤n

Pθ (Z

1:T

1:T

=z

z1:T ∈~1,QnT

Xit j log πZit Z tj + (1 − Xit j ) log(1 − πZit Z tj )



) ,

(2)

respectively. We then denote the maximum likelihood estimator (MLE) by
θ̂ = (Γ̂, π̂) = argmax ℓ(θ).
θ∈Θ

In the next section, we study separately the consistency of the connectivity parameter estimator π̂ and that of the
transition matrix estimator Γ̂.

3 Consistency of the maximum likelihood estimate
3.1 Connectivity parameter
We first prove the consistency of the maximum likelihood estimator of the connectivity parameter π = (πql )1≤q,l≤Q
when the number of nodes and time steps increase. We denote the normalized log-likelihood by
Mn,T (Γ, π) =

2
2
ℓ(θ) =
log Pθ (X 1:T )
n(n − 1)T
n(n − 1)T

and introduce the quantities, for any A = (aql )1≤q,l≤Q ∈ A the set of Q × Q stochastic matrices,
X
X
aqq′ all′ [π∗ql log πq′ l′ + (1 − π∗ql ) log(1 − πq′ l′ )]
M(π, A) =
α∗q α∗l
1≤q,l≤Q

1≤q′ ,l′ ≤Q

and M(π) = sup M(π, A) = M(π, Āπ ),

(3)

A∈A

where Āπ = argmaxA∈A M(π, A). It is worth noticing that M(π), which will be the limiting value for Mn,T (Γ, π)
when n and T increase (see below), does not depend on Γ.
Theorem 1. For any sequence {rn,T }n,T ≥1 increasing to infinity, if log(T ) = o(n), we have for all ǫ > 0
!
ǫrn,T
−−−−−−→ 0.
Pθ∗ sup Mn,T (Γ, π) − M(π) > √
n n,T →+∞
(Γ,π)∈Θ
We then conclude on the consistency of the maximum likelihood estimator of the connection probabilities with
the following corollary. Note that we also obtain an upper bound of the rate of convergence of this estimator.
Corollary 1. For any sequence {rn,T }n,T ≥1 increasing to infinity such that rn,T = o(n1/4 ) and if log(T ) = o(n), we
have for every ǫ > 0
!
ǫrn,T
∗
Pθ∗ min kπ − π̂σ k∞ > 1/4 −−−−−→ 0.
n,T →∞
σ∈SQ
n
We want to get equivalent consistency results if the number of time steps T is fixed and only the number of
nodes n increases. In that case, denoting by θ̂ = (Γ̂, π̂1:T ) the MLE of θ, we have the following Corollary that is the
equivalent of Corollary 1.
Corollary 2. If the number of time steps T is fixed, we have for every ǫ > 0 and for any sequence {rn }n≥1 increasing
to infinity such that rn = o(n1/4 )
!
ǫrn
−−−−→ 0,
k
>
Pθ∗ min kπ∗1:T − π̂1:T
σ ∞
σ∈SQ
n1/4 n→∞
t
denoting π̂1:T
σ = (π̂σ )t∈~1,T  .

5

This result states that minσ∈SQ kπ∗1:T − π̂1:T
σ k∞ converges to 0 in Pθ∗ -probability as n increases, i.e. the MLE of
the connection probabilities is consistent up to label switching, and gives an upper bound of the rate of convergence
of the MLE of the connection probabilities. The particular case when T = 1 is then a stronger result than that of
Celisse et al. [2012] where no rate of convergence is given.
Remark 1. Note that in Corollaries 1 and 2, the results still hold for any sequences rn,T and rn increasing to
infinity, respectively. However, we are interested in sequences increasing slowly to infinity, giving the strongest
results, namely the smallest lower bounds. Indeed, whenever these assumptions are not satisfied, the lower bounds
appearing in the inequalities are larger, and the results may even become trivial.

3.2 Latent transition matrix
We now prove that the MLE for the transition matrix Γ is consistent when the number of nodes and time steps
increase.
Lemma 1. Any critical point θ̆ = (Γ̆, π̆) of the likelihood function ℓ(·) is such that Γ̆ satisfies the fixed point equation


PT −1 Pn
Pθ̆ Zit = q, Zit+1 = l | X 1:T
t=1
i=1


.
(4)
∀(q, l) ∈ ~1, Q2 , γ̆ql =
PT −1 Pn
t
1:T
t=1
i=1 Pθ̆ Zi = q | X
There are two different possible cases for the MLE θ̂

• Either θ̂ is a critical point of the likelihood function. Then Γ̂ satisfies equation (4).
• Or θ̂ is not a critical point (this can happen if it belongs to the boundary of Θ) and we assume that there exists
Γ̆ such that (Γ̆, π̂) ∈ Θ and (Γ̆, π̂) satisfies equation (4) (at least for n and T large enough). We then choose as
our estimator (Γ̆, π̂). By an abuse of notation, we will denote this estimator θ̂ = (Γ̂, π̂) and call it MLE in the
following.
In what follows, for any fixed configuration z1:T , any θ ∈ Θ and any ǫ > 0, we consider the event
(
)
Pθ (Z 1:T , z1:T | X 1:T )
E(z1:T , θ, ǫ) ≔
.
>
ǫ
Pθ (Z 1:T = z1:T | X 1:T )
The following result establishes that asymptotically, any estimator that correctly estimates the transition probability
matrix π also recovers the group memberships. This result is similar to Theorem 1 in Mariadassou and Matias
[2015].
Theorem 2. For any estimator θ̆ ∈ Θ (at least for n and T large enough), if log(T ) = o(n), there exist some positive
constants C, C1 , C2 , C3 , C4 such that for any ǫ > 0, for any positive sequence {yn,T }n,T ≥1 such that log(1/yn,T ) =
o(n), any η ∈ (0, δ) and for n and T large enough, we have



Pθ∗ E(Z 1:T , θ̆, ǫyn,T ) ≤ QT exp(−2η2 n) + Pθ∗ kπ̆ − π∗ k∞ > vn,T

"
#
"
#




(log(nT ))2


2
exp − (δ − η) C1 n + C2 log(nT ) − C4 log(ǫyn,T ) + exp − C3
+ CnT 
+ 3n log(nT ) 
,


2


nv
n,T

whenever {vn,T }n,T ≥1 is a sequence decreasing to 0 such that vn,T

p
= o( log(nT )/n).

Theorem
3. If log(T ) = o(n), for any ǫ > 0 and {rn,T }n,T ≥1 any sequence increasing to infinity such that rn,T =
p
o nT/ log n , we have for any σ ∈ SQ


p

log n 
∗
 ≤ Q2 (3Q + 1)Pθ∗ kπ̂σ − π∗ k∞ > vn,T  + o(1)
Pθ∗ kΓ̂σ − Γ k∞ > ǫrn,T √
nT
p
with {vn,T }n,T ≥1 a sequence decreasing to 0 such that vn,T = o( log(nT )/n).

Corollary 3. Assume that log(T ) = o(n) and minσ∈SQ kπ̂σ −π∗ k∞ = oPθ∗ (vn,T ) with {vn,T }n,T ≥1 a sequence decreasing
p
to 0 such that vn,T = o( log(nT )/n). Then for any ǫ > 0 and {rn,T }n,T ≥1 any sequence increasing to infinity such
p
that rn,T = o nT/ log n , we have the convergence


p

log n 
∗
 −−−−−→ 0.

Pθ∗  min kΓ̂σ − Γ k∞ > ǫrn,T √
n,T →∞
σ∈SQ
nT
6

Remark 2. Note that the upper bound obtained in Corollary 1 on the rate of convergence in probability of π̂
does not ensure that minσ∈SQ kπ̂σ − π∗ k∞ = oPθ∗ (vn,T ) holds. While the latter has never been established (to our
knowledge), it is a reasonable assumption.
We want an equivalent result than that of Corollary 3 when the number of time steps T is fixed, and the
connection probabilities are varying over time (the connection parameter being π = π1:T = (π1 , . . . , πT ) with
πt = (πtql )q,l ). For that, we are going to need an equivalent of Theorem 2 in that case.
Theorem 4. For any fixed T ≥ 2, for any estimator θ̆ ∈ ΘT (at least for n large enough), there exist some positive
constants C, C1 , C2 , C3 , C4 such that for any ǫ > 0, for any positive sequence {yn }n≥1 such that log(1/yn ) = o(n),
any η ∈ (0, δ) and for n large enough, we have




Pθ∗ E(Z 1:T , θ̆, ǫyn ) ≤ QT exp(−2η2 n) + Pθ∗ kπ̆1:T − π∗1:T k∞ > vn
(
"
#
"
#)
(log(nT ))2
2
+ CnT exp − (δ − η) C1 n + C2 log(nT ) − C4 log(ǫyn ) + exp − C3
+ 5n log(nT ) ,
nv2n
p
whenever {vn }n≥1 is a sequence decreasing to 0 such that vn = o( log(n)/n).
The following corollary gives the expected result.

∗1:T
Corollary 4. Let the number of time steps T ≥ 2 be fixed. Assume that minσ∈SQ kπ̂1:T
k∞ = oPθ∗ (vn ) with
σ −π
p
{vn }n≥1 a sequence decreasing to 0 such
 p that vn  = o( log(n)/n). Then for any ǫ > 0 and {rn }n≥1 any sequence
increasing to infinity such that rn = o n/ log n , we have the convergence


p

log n 
∗
Pθ∗  min kΓ̂σ − Γ k∞ > ǫrn √  −−−−→ 0.
n→∞
σ∈SQ
n

The proof of Corollary 4 is the same as that of Corollary 3, but relying on Theorem 4 instead of Theorem 2 and
is therefore omitted.

Remark 3. As in Remark 1 for Corollaries 1 and 2, the results of Corollaries 3 and 4 still hold for sequences rn,T
and rn increasing to infinity at any rate.

4 Variational estimators
In practice, we cannot compute the MLE except for very small values of n and T , because it involves a summation
over all the QnT possible latent configurations. We cannot either use the Expectation-Maximization (EM) algorithm
to approximate it because it involves the computation of the conditional distribution of the latent variables given
the observations which is not tractable. A common solution is to use the Variational Expectation-Maximization
(VEM) algorithm that optimizes a lower bound of the log-likelihood (see for example Daudin et al. [2008]). Let
us denote Ziqt = 1Zit =q for every t, i and q. Using the same approach as in Matias and Miele [2017] for the VEM
algorithm in the dynamic SBM, we consider a variational approximation of the conditional distribution of the
1:T
latent variable
given the observed variable X 1:T in the class of probability distributions parameterized by
 Z
χ = (τ, η) = {τtiq }t,i,q , {ηtiql }t,i,q,l of the form

 T −1
 t Ziqt Zilt+1 

Q
n
T
n 


Y
Y
Y



Y 1 Ziq1  Y Y  ηiql 
1:T
1
t−1
t


Qχ (Z ) =
Qχ (Zi )
,
Qχ (Zi | Zi ) =
)
(τ
 t 




iq






τ


iq
i=1
i=1
t=2
q=1
t=1 1≤q,l≤Q
h i
h
i
i.e. with Qχ such that EQχ Ziqt Zilt+1 = ηtiql and EQχ Ziqt = τtiq . Notice that Qχ (Zit+1 = l | Zit = q) = ηtiql /τtiq =
P
ηtiql / qQ′ =1 ηtiqq′ . The quantity to optimize in the VEM algorithm is then
h
i
J(χ, θ) = ℓ(θ) − KL(Qχ , Pθ (·|X 1:T )) = EQχ log Pθ (X 1:T , Z 1:T ) + H(Qχ )
with KL(·, ·) denoting the Kullback-Leibler divergence and H(·) denoting the entropy. Define
χ̂(θ) = (τ̂(θ), η̂(θ)) = argmax J(χ, θ),
χ∈[0,1]T 2 n2 Q3

and the variational estimator of θ
θ̃ = (Γ̃, π̃) = argmax J(χ̂(θ), θ).
θ∈Θ

Moreover, we denote χ̃ = (τ̃, η̃) = χ̂(θ̃) = (τ̂(θ̃), η̂(θ̃)). In practice, the VEM algorithm is an iterative algorithm that
maximizes the function J alternatively with respect to χ and θ in order to find θ̃.
7

4.1 Connectivity parameter
Theorem 5. For any sequence {rn,T }n,T ≥1 increasing to infinity, if log(T ) = o(n), we have for all ǫ > 0
!
ǫrn,T
2
Pθ∗ sup
−→ 0.
J(χ̂(θ), θ) − M(π) > √
n n,T →+∞
θ∈Θ n(n − 1)T
We conclude on the consistency of the connection probabilities variational estimators as n and T increase
thanks to the following corollary.
Corollary 5. For any sequence {rn,T }n,T ≥1 increasing to infinity such that rn,T = o(n1/4), we have for any ǫ > 0
!
ǫrn,T
1
Pθ∗ min kπ̃σ − π∗ k∞ > 1/4 −−−−−→ 0.
n,T →∞
σ∈SQ
2
n
We have the equivalent following corollary for a fixed number of time steps.
Corollary 6. If the number of time steps T is fixed, we have for every ǫ > 0 and for any sequence {rn }n≥1 increasing
to infinity such that rn = o(n1/4 )
!
ǫrn
1
∗1:T
−
π
k
>
Pθ∗ min kπ̃1:T
−−−−→ 0.
∞
σ
σ∈SQ
2
n1/4 n→∞
Remark 4. As for Corollaries 1 to 4, the results of Corollaries 5 and 6 still hold for any sequences rn,T and rn
increasing to infinity.

4.2 Latent transition matrix
We now prove that Γ̃ is consistent when the number of nodes and time steps increase.
Lemma 2. Any critical point (χ̆, θ̆) of the function J(·, ·) is such that Γ̆ satisfies the fixed-point equation
Pn PT −1 t
i=1 t=1 η̆iql
2
∀(q, l) ∈ ~1, Q , γ̆ql = Pn PT −1 t .
i=1 t=1 τ̆iq
We assume that (χ̃, θ̃) is a critical point of J(·, ·). Then we have the fixed-point equation
Pn PT −1 t
i=1 t=1 η̂iql (θ̃)
2
.
∀(q, l) ∈ ~1, Q , γ̃ql = Pn PT −1 t
i=1 t=1 τ̂iq (θ̃)

(5)

(6)

The following proposition gives the consistency and a rate of convergence of this estimator, under an assumption
on the rate of convergence of π̃.
Theorem
6. If log(T ) = o(n), for any ǫ > 0 and {rn,T }n,T ≥1 any sequence increasing to infinity such that rn,T =
p
o nT/ log n and for any σ ∈ SQ

with {vn,T }n,T ≥1



p

log n 
∗
 ≤ 2Q2 (3Q + 1)Pθ∗ kπ̃σ − π∗ k∞ > vn,T  + o(1)
Pθ∗ kΓ̃σ − Γ k∞ > ǫrn,T √
nT
p
a sequence decreasing to 0 such that vn,T = o( log(nT )/n).

Corollary 7. Assume that log(T ) = o(n) and minσ∈SQ kπ̃σ −π∗ k∞ = oPθ∗ (vn,T ) with {vn,T }n,T ≥1 a sequence decreasing
p
to 0 such that vn,T = o( log(nT )/n). Then for any ǫ > 0 and {rn,T }n,T ≥1 any sequence increasing to infinity such
p
that rn,T = o nT/ log n , we have the convergence


p

log n 
∗
 −−−−−→ 0.
Pθ∗  min kΓ̃σ − Γ k∞ > ǫrn,T √
n,T →∞
σ∈SQ
nT

The proof of Corollary 7 is the same as that of Corollary 3, using Theorem 6 instead of Theorem 3 and is
therefore omitted.
When the number of time steps T is fixed and the connection probabilities can vary over time, we have the
following Corollary that is the equivalent of Corollary 7.
8

∗1:T
Corollary 8. Let the number of time steps T ≥ 2 be fixed. Assume that minσ∈SQ kπ̃1:T
k∞ = oPθ∗ (vn ) with
σ −π
p
{vn }n≥1 a sequence decreasing to 0 such
that
v
=
o(
log(n)/n).
Then
for
any
ǫ
>
0
and
{r
n
n }n≥1 any sequence
p
increasing to infinity such that rn = o n/ log n , we have the convergence



p

log n 
∗
Pθ∗  min kΓ̃σ − Γ k∞ > ǫrn √  −−−−→ 0.
n→∞
σ∈SQ
n

The proof of Corollary 8 is the same as that of Corollary 7, but relying on Theorem 4 instead of Theorem 2 and
is therefore omitted.
Remark 5. As for Corollaries 1 to 6, the results of Corollaries 7 and 8 still hold for any sequences rn,T and rn
increasing to infinity.

5 Proofs of main results
5.1 Proof of Theorem 1
The proof follows the lines of the proof of Theorem 3.6 in Celisse et al. [2012]. Nonetheless, our result is sharper
as we establish an upper bound of the rate of convergence (in probability) of the normalised likelihood. We fix
some θ ∈ Θ and introduce the quantities
ẑ1:T = argmax log Pθ (X 1:T | Z 1:T = z1:T ),

(7)

z1:T ∈~1,QnT



Z̃ 1:T = argmax Eθ∗ log Pθ (X 1:T | Z 1:T = z1:T ) Z 1:T .

(8)

z1:T ∈~1,QnT

Note that Z̃ 1:T is a random variable that depends on Z 1:T and that
ẑ1:T = argmax

T
X

z1:T ∈~1,QnT t=1

!
log Pθ (X t | Z t = zt ) = argmax log Pθ (X 1 | Z 1 = z), . . . , argmax log Pθ (X T | Z T = z) .
z∈~1,Qn

(9)

z∈~1,Qn



Similarly, for any t ∈ ~1, T , we have Z̃ t = argmaxz∈~1,Qn Eθ∗ log Pθ (X t | Z t = z) | Z t .
We bound the difference between Mn,T (Γ, π) and M(π) by introducing three intermediate terms so that we can
write, for any sequence {rn,T }n,T ≥1 and any ǫ > 0
!
ǫrn,T
Pθ∗ sup Mn,T (Γ, π) − M(π) > √
n
θ∈Θ
!
ǫrn,T
2
2
log Pθ (X 1:T ) −
log Pθ (X 1:T | Z 1:T = ẑ1:T ) > √
≤Pθ∗ sup
n(n − 1)T
3 n
θ∈Θ n(n − 1)T
!


ǫrn,T
2
2
+ Pθ∗ sup
log Pθ (X 1:T | Z 1:T = ẑ1:T ) −
Eθ∗ log Pθ (X 1:T | Z 1:T = Z̃ 1:T ) Z 1:T > √
n(n − 1)T
3 n
θ∈Θ n(n − 1)T
!


ǫrn,T
2
(10)
Eθ∗ log Pθ (X 1:T | Z 1:T = Z̃ 1:T ) Z 1:T − M(π) > √ .
+ Pθ∗ sup
n(n
−
1)T
3 n
θ∈Θ
In the following, we prove separately the convergence (in Pθ∗ -probability) to zero of the three terms of this sum
(while controlling for the rate of these convergences). Before starting, let us remark that we have
log Pθ (X 1:T | Z 1:T = z1:T ) =

T
X
X

t=1 1≤i< j≤n

Xit j log πzti ztj + (1 − Xit j ) log(1 − πzti ztj )

T

 X
X
π∗Z t Z t log πzti ztj + (1 − π∗Z t Z t ) log(1 − πzti ztj ).
and Eθ∗ log Pθ (X 1:T | Z 1:T = z1:T ) Z 1:T =
t=1 1≤i< j≤n

i

i

j

j

In particular, for every t ∈ ~1, T , we have
ẑt =

argmax

X

z=(z1 ,...,zn )∈~1,Qn 1≤i< j≤n

Z̃ t =

argmax

X

z=(z1 ,...,zn )∈~1,Qn 1≤i< j≤n

Xit j log πzi z j + (1 − Xit j ) log(1 − πzi z j ),
π∗Z t Z t log πzi z j + (1 − π∗Z t Z t ) log(1 − πzi z j ).
i

i

j

9

j

(11)

(12)

First term of the right-hand side of (10).

We let

2
2
log Pθ (X 1:T ) −
log Pθ (X 1:T | Z 1:T = ẑ1:T )
n(n − 1)T
n(n − 1)T
T
X
2
log Pθ (X t | X 1:t−1 ) − log Pθ (X t | Z t = ẑt ) .
≤
n(n − 1)T t=1

T1 ≔

(13)

Lemma 3. For every t ∈ ~1, T , we have
log Pθ (X t |X 1:t−1 ) − log Pθ (X t |Z t = ẑt ) ≤ log Pθ (Z t = ẑt |X 1:t−1 ) .
Going back to (13) and applying Lemma 3, we get
T1 ≤

T
T
X
X
2
2
log Pθ (Z t = ẑt | X 1:t−1 ).
log Pθ (Z t = ẑt | X 1:t−1 ) = −
n(n − 1)T t=1
n(n − 1)T t=1

Now, using classical dependency rules in directed acyclic graphs [see for e.g. Lauritzen, 1996] combined with
Assumption 2, we get
T1 ≤ −
≤−

T
X
2
log
n(n − 1)T t=1

2
n(n − 1)T

T
X

log

t=1

X

Pθ (Z t = ẑt | Z t−1 = zt−1 )Pθ (Z t−1 = zt−1 | X 1:t−1 )

X

δn Pθ (Z t−1 = zt−1 | X 1:t−1 ) ≤ −

zt−1 ∈~1,Qn

zt−1 ∈~1,Qn

T
X
2
2
n log δ =
log(1/δ).
n(n − 1)T t=1
n−1

√
√
This implies that Pθ∗ (supθ∈Θ T 1 > ǫrn,T /(3 n)) = 0 as soon as ǫrn,T / n ≥ 6 log(1/δ)/(n −√1). Then for any
sequence {rn,T }n,T ≥1 increasing to infinity, for any ǫ > 0, we have that Pθ∗ (supθ∈Θ T 1 > ǫrn,T /(3 n)) → 0 as n and
T increase.
Second term of the right-hand side of (10).
T 2 (Z 1:T ) ≔

Let us denote



2
2
log Pθ (X 1:T | Z 1:T = ẑ1:T ) −
Eθ∗ log Pθ (X 1:T |Z 1:T = Z̃ 1:T ) Z 1:T .
n(n − 1)T
n(n − 1)T

For the sake of clarity, we study this term on the event {Z 1:T = z∗1:T } where z∗1:T ∈ ~1, QnT is a fixed configuration.
This event induces the definition of Z̃ 1:T following Equation (8) as


Z̃ 1:T = argmax Eθ∗ log Pθ (X 1:T | Z 1:T = z1:T ) Z 1:T = z∗1:T ,
z1:T ∈~1,QnT

or equivalently for every t ∈ ~1, T ,
Z̃ t =

argmax

X

z=(z1 ,...,zn )∈~1,Qn 1≤i< j≤n

π∗z∗t z∗t log πzi z j + (1 − π∗z∗t z∗t ) log(1 − πzi z j ).
i

i

j

j

By definition of ẑ1:T and Z̃ 1:T respectively, we have the two inequalities
log Pθ (X 1:T | Z 1:T = ẑ1:T ) ≥ log Pθ (X 1:T | Z 1:T = Z̃ 1:T )
and





Eθ∗ log Pθ (X 1:T | Z 1:T = Z̃ 1:T ) Z 1:T = z∗1:T ≥ Eθ∗ log Pθ (X 1:T | Z 1:T = ẑ1:T ) Z 1:T = z∗1:T ,

implying the lower and upper bounds



log Pθ (X 1:T | Z 1:T = Z̃ 1:T ) − Eθ∗ log Pθ (X 1:T | Z 1:T = Z̃ 1:T ) Z 1:T = z∗1:T


≤ log Pθ (X 1:T | Z 1:T = ẑ1:T ) − Eθ∗ log Pθ (X 1:T | Z 1:T = Z̃ 1:T ) Z 1:T = z∗1:T


≤ log Pθ (X 1:T | Z 1:T = ẑ1:T ) − Eθ∗ log Pθ (X 1:T | Z 1:T = ẑ1:T ) Z 1:T = z∗1:T .
10

Taking the absolute value gives us an upper bound for T 2 (z∗1:T )
T 2 (z∗1:T ) ≤



2
log Pθ (X 1:T | Z 1:T = z1:T ) − Eθ∗ log Pθ (X 1:T | Z 1:T = z1:T ) Z 1:T = z∗1:T .
z1:T ∈{ẑ1:T ,Z̃ 1:T } n(n − 1)T
max

Using Equations (11) and (12), we then obtain the following upper bound for T 2 (z∗1:T )
∗1:T

T 2 (z

)≤

max

z1:T ∈{ẑ1:T ,Z̃ 1:T }



T
X
X
 πzti ztj 
2
t
∗

 .
(X − πz∗t z∗t ) log 
i j
n(n − 1)T t=1 1≤i< j≤n i j
1 − πzti ztj 

We use the following concentration result to conclude.

Lemma 4. Let ǫ, β > 0 and {xn,T }n,T ≥1 a sequence of positive real numbers. We let P∗θ∗ (·) denote the probability
conditional on {Z 1:T = z∗1:T } under parameter θ∗ , i.e. P∗θ∗ (·) = Pθ∗ (·|Z 1:T = z∗1:T ). Denoting Λ = 2 log[(1−ζ)/ζ] > 0
we have for any θ ∈ Θ




T


X
X
 πzti ztj 
2

∗
t
∗ 
 > ǫ 
(Xi j − πz∗t z∗t ) log 
Pθ∗  sup
sup

i j
1 − πzti ztj
z1:T ∈~1,QnT π∈[ζ,1−ζ]Q2 n(n − 1)T t=1 1≤i< j≤n
p



 (1 + β)Λ
Λ xn,T /2
(Λ/2)xn,T
+ √
+ (1/β + 1/3)
> ǫ  + 2e−xn,T
≤P∗θ∗  √
n(n − 1)T/2
n(n − 1)T/2
n(n − 1)T/2
≤12Ω/(n(n−1)T )>ǫ + 2e−xn,T

(14)

p
√
with Ω = (1 + β)Λ n(n − 1)T/2 + Λ n(n − 1)T xn,T /4 + (1/β + 1/3)(Λ/2)xn,T .

Let us choose xn,T = log(n) in the above lemma. For any ǫ > 0, for any sequence {rn,T }n,T ≥1 increasing to
infinity, we have for n and T large enough
ǫrn,T
2Ω
.
√ ≥
3 n n(n − 1)T

Then for n and T large enough, the first term in the right-hand side of inequality (14) is equal to 0 and we have
!
ǫrn,T
2
∗1:T
∗
Pθ∗ sup T 2 (z ) > √ ≤
n
3 n
θ∈Θ
! X
!
ǫrn,T
ǫrn,T
2
∗
∗1:T
1:T
Pθ∗ sup T 2 (z ) > √ Pθ∗ (Z 1:T = z∗1:T ) ≤ .
and Pθ∗ sup T 2 (Z ) > √ ≤
n
3 n
3 n
θ∈Θ
θ∈Θ
z∗1:T
Third term of the right-hand side of (10).
T 3 (Z 1:T ) ≔
=

2
n(n − 1)T

Let us denote


Eθ∗ log Pθ (X 1:T | Z 1:T = Z̃ 1:T ) Z 1:T − M(π)

T


X
2
Eθ∗ log Pθ (X t | Z t = Z̃ t ) Z t − M(π, Āπ ) .
n(n − 1)T t=1

For any fixed configuration zt ∈ ~1, Qn , analogous to Equation (12), we write


X
π∗Z t Z t log πzti ztj + (1 − π∗Z t Z t ) log(1 − πzti ztj )
Eθ∗ log Pθ (X t | Z t = zt ) Z t =
1≤i< j≤n

i

i

j

j

1 X ∗
π t t log πzti ztj + (1 − π∗Z t Z t ) log(1 − πzti ztj )
i j
2 1≤i, j≤n Zi Z j
X
X 

1
=
π∗ql log πq′ l′ + (1 − π∗ql ) log(1 − πq′ l′ ) 1{Zit =q,Z tj =l,zti =q′ ,ztj =l′ }
2 1≤q,l,q′ ,l′ ≤Q 1≤i, j≤n
X


1
=
Cqq′ (Z t , zt )Cll′ (Z t , zt ) π∗ql log πq′ l′ + (1 − π∗ql ) log(1 − πq′ l′ ) ,
2 1≤q,l,q′ ,l′ ≤Q
=

where Cqq′ (Z t , zt ) = |{i ∈ ~1, n; Zit = q, zti = q′ }| is the (random variable) number of nodes classified in group q
in the current (random) configuration Z t , while they belong to group q′ in (deterministic) configuration zt . Recall
11

that Nq (zt ) is the number of nodes assigned to class q by the configuration zt and let us denote atqq′ = aqq′ (Z t , zt ) =
Cqq′ (Z t , zt )/Nq (Z t ) the (random) proportion of vertices from class q in Z t attributed to class q′ by zt . We write


X

Nq (Z t )Nl (Z t ) t t  ∗
2
Eθ∗ log Pθ (X t | Z t = zt ) Z t =
aqq′ all′ πql log πq′ l′ + (1 − π∗ql ) log(1 − πq′ l′ )
n(n − 1)
n(n − 1)
1≤q,l,q′ ,l′ ≤Q
≔ Φt (At , π),

with At = (atqq′ )1≤q,q′ ≤Q .
Now extending these notations to the case where zt = Z̃ t , we let Ãt = (ãtqq′ )1≤q,q′ ≤Q where ãtqq′ = aqq′ (Z t , Z̃ t ).
We remark that the definition of Z̃ t implies that Ãt = argmaxAt ∈At (Z 1:T ) Φt (At , π) with At (Z 1:T ) the (random) subset
of stochastic matrices defined for every t ∈ ~1, T  by
Q
X
o
n
nql = Nq (Z t ) .
At (Z 1:T ) = A = (nql /Nq (Z t ))1≤q,l≤Q ; nql ∈ ~0, Nq (Z t ),
l=1

Let us also denote Ātπ = argmaxA∈At (Z 1:T ) M(π, A). Then
sup T 3 (Z 1:T ) ≤
θ∈Θ

≤

sup
π∈[ζ,1−ζ]Q2

sup
π∈[ζ,1−ζ]Q2

T
1X t t
Φ (Ã , π) − M(π, Āπ )
T t=1

T
T
1X
1X t t
sup M(π, Ātπ ) − M(π, Āπ ) .
Φ (Ã , π) − M(π, Ātπ ) +
T t=1
T t=1 π∈[ζ,1−ζ]Q2

(15)

We start by stating a concentration lemma on the random variable Nq (Z t ) for any q ∈ ~1, Q and any t ∈ ~1, T .
Lemma 5. For any θ ∈ Θ and any η ∈ (0, δ), let
(
)
Nq (zt )
1:T
nT
Ωη (θ) ≔ z ∈ ~1, Q ; ∀t ∈ ~1, T , ∀q ∈ ~1, Q,
≥ αq − η .
n


Then Pθ Z 1:T ∈ Ωη (θ) ≥ 1 − QT exp(−2η2 n).

Building on the previous concentration lemma, the following one gives the convergence in Pθ∗ -probability of
the second term in the right-hand side of (15).
Lemma 6. For any ǫ > 0, any η ∈ (0, δ) and {rn,T }n,T ≥1 any positive sequence,


T
 1 X



ǫr
n,T
t
Pθ∗ 
sup M(π, Āπ ) − M(π, Āπ ) > √  ≤ QT exp −2η2 n + 1n≤6c √n/[ǫrn,T (δ−η)]
T t=1 π∈[ζ,1−ζ]Q2
6 n

(16)

with c = 6(1 − δ)2 (1 − ζ) log(1/ζ)Q4 .

Then taking any η ∈ (0, δ), for any ǫ > 0, for any sequence {rn,T }n,T ≥1 increasing to infinity, we have the
following inequality for n and T large enough
√
6c n
,
(17)
rn,T >
ǫ(δ − η)n
implying that the probability in Lemma 6 converges to 0 as n and T increase for any ǫ > 0, as long as log T = o(n).
Now, for the first term in the right-hand side of (15), note that we have for every π and every t
( t t
Φ (Ã , π) ≥ Φt (Ātπ , π) because Ãt = argmaxA∈At Φt (A, π)
M(π, Ātπ) ≥ M(π, Ãt ) because Ātπ = argmaxA∈At M(π, A).
Then, either M(π, Ātπ ) ≤ Φt (Ãt , π) and
0 ≤ Φt (Ãt , π) − M(π, Ātπ ) ≤ Φt (Ãt , π) − M(π, Ãt )
or M(π, Ātπ ) ≥ Φt (Ãt , π) and
0 ≤ M(π, Ātπ ) − Φt (Ãt , π) ≤ M(π, Ātπ ) − Φt (Ātπ , π).
12

In both cases, we get that Φt (Ãt , π) − M(π, Ātπ ) ≤ supA∈A Φt (A, π) − M(π, A) for every t and π, thus obtaining the
upper bound
T
T
1X
1X t t
sup sup Φt (At , π) − M(π, At ) .
Φ (Ã , π) − M(π, Ātπ ) ≤
T t=1
T t=1 π∈[ζ,1−ζ]Q2 At ∈A

sup
π∈[ζ,1−ζ]Q2

Letting
sup

∆(ζ) =

sup

π∈[ζ,1−ζ] π∗ ∈[ζ,1−ζ]

|π∗ log π + (1 − π∗ ) log(1 − π)| ∈ (0, +∞)

and recalling that 0 ≤ aql ≤ 1 (for every q, l ∈ ~1, Q) for every A = (aql )1≤q,l≤Q ∈ A, we have
sup
π∈[ζ,1−ζ]

≤

Q2

sup Φt (At , π) − M(π, At )

At ∈A

sup

!


Nq (Z t )Nl (Z t )
− α∗q α∗l atqq′ atll′ π∗ql log πq′ l′ + (1 − π∗ql ) log(1 − πq′ l′ )
n(n − 1)

X

sup

t
π∈[ζ,1−ζ]Q2 A ∈A 1≤q,l,q′ ,l′ ≤Q

≤ ∆(ζ)Q2

Nq (Z t )Nl (Z t )
− α∗q α∗l .
n(n − 1)

X

1≤q,l≤Q

Finally, we bound the first term of the right-hand-side of (15) as follows
sup
π∈[ζ,1−ζ]Q2

T
T
X 1X
Nq (Z t )Nl (Z t )
1X t t
Φ (Ã , π) − M(π, Ātπ ) ≤ ∆(ζ)Q2
− α∗q α∗l .
T t=1
T
n(n
−
1)
t=1
1≤q,l≤Q

(18)

Applying Markov’s Inequality, we obtain


Pθ∗ 

sup

π∈[ζ,1−ζ]Q2




T
T

 1 X
Nq (Z t )Nl (Z t )
ǫrn,T  X
ǫrn,T
1X t t
t
∗
∗
Φ (Ã , π) − M(π, Āπ ) > √  ≤
− αq αl >
Pθ∗ 
√ 
4
T t=1
T t=1
n(n − 1)
6 n
6∆(ζ)Q n
q,l
"
#
√
T
Nq (Z t )Nl (Z t )
6∆(ζ)Q4 n X 1 X
Eθ ∗
− α∗q α∗l
≤
ǫrn,T
T t=1
n(n − 1)
q,l
"
#
√
Nq (Z 1 )Nl (Z 1 )
6∆(ζ)Q4 n X
∗ ∗
∗
≤
Eθ
− αq αl .
ǫrn,T
n(n − 1)
q,l

The following lemma gives an upper bound of the expectation appearing in the previous inequality, for any q, l ∈
~1, Q.
Lemma 7. For any q, l ∈ ~1, Q and any t ∈ ~1, T , we have the following inequality
Eθ ∗

"

#
√
Nq (Z t )Nl (Z t )
2 n
∗ ∗
− αq αl ≤
.
n(n − 1)
n−1

This leads to


Pθ∗ 

sup

π∈[ζ,1−ζ]Q2


T
ǫrn,T  12∆(ζ)Q6 n
1X t t
t
Φ (Ã , π) − M(π, Āπ ) > √  ≤
.
T t=1
ǫrn,T (n − 1)
6 n

Then for any ǫ > 0, for any sequence {rn,T }n,T ≥1 increasing to infinity, we have the convergence


Pθ∗ 

sup

π∈[ζ,1−ζ]Q2


T
√ 
1X t t
t
Φ (Ã , π) − M(π, Āπ ) > ǫrn,T /(6 n) −−−−−→ 0.
n,T →∞
T t=1

We proved the convergence to 0 of the three terms in the right-hand side of (10) for any sequence {rn,T }n,T ≥1
increasing to infinity and as long as log T = o(n). This gives the expected result and concludes the proof.

13

5.2 Proof of Corollary 1
To prove this corollary, we establish the following lemma that allows us to obtain a rate of convergence of π̂ to π∗
from a rate of convergence of Mn,T to M. Note that this lemma is a bit more general than what we need and gives
an equivalent result when the number of time steps T is fixed, which will be useful for Corollary 2.
Lemma 8. Let {Fn,T }n,T ≥1 be any random functions on the set Θ (resp. ΘT ) and M (resp. MT ) defined as before.
Assume that there exists a sequence {vn,T }n,T ≥1 (resp. {vn }n≥1 ) a sequence decreasing to 0 such that for every ǫ > 0,
we have the following convergence as n, T → ∞ (resp. n → ∞)
!
Pθ∗ sup Fn,T (Γ, π) − M(π) > ǫvn,T −−−−−→ 0
n,T →∞

(Γ,π)∈Θ





resp. Pθ∗  sup

(Γ,π)∈ΘT





Fn,T (Γ, π1:T ) − MT (π1:T ) > ǫvn  −−−−→ 0 .
n→∞

If for any n and T , θ̂ = (Γ̂, π̂) (resp. θ̂ = (Γ̂, π̂1:T )) is defined as the maximizer of Fn,T on the set Θ, (resp. ΘT ) we
have the following convergence
!
√
Pθ∗ min kπ̂σ − π∗ k∞ > ǫ vn,T −−−−−→ 0
n,T →∞

σ∈SQ

resp. Pθ∗

min kπ̂1:T
σ1:T
σ1 ,...,σT ∈SQ

−π

∗1:T

!
!
√
k∞ > ǫ vn −−−−→ 0
n→∞

with π̂1:T
= (π̂tσt )t∈~1,T  .
σ1:T
2
The result of Corollary 1 is then a direct consequence of Theorem 1 (choosing the sequence {rn,T
}n,t≥1 ) and
Lemma 8 applied with Fn,T = Mn,T .


5.3 Proof of Theorem 2
The proof follows the lines of the proof of Theorem 3.8 in Celisse et al. [2012]. Nonetheless, our result is sharper
as we will establish an upper bound of the rate of convergence (in probability) of the quantity at stake. For any
ǫ > 0, any sequence {yn,T }n,T ≥1 and η ∈ (0, δ), we write
X
Pθ∗ (E(z∗1:T , θ̆, ǫyn,T ); Z 1:T = z∗1:T ) ≤ Pθ∗ (Z 1:T ∈ Ωcη (θ∗ ))
Pθ∗ (E(Z 1:T , θ̆, ǫyn,T )) =
z∗1:T ∈~1,QnT

+

X

z∗1:T ∈Ωη (θ∗ )


  1:T

 Pθ̆ Z , z∗1:T | X 1:T



1:T
∗1:T
 Pθ∗ Z 1:T = z∗1:T
Pθ∗ 
>
ǫy
Z
=
z

n,T

Pθ̆ Z 1:T = z∗1:T | X 1:T

(19)

with Ωη (θ∗ ) as defined in Lemma 5. We will establish that there exist some positive constants C, C1 , C2 , C3 , C4 such
that for any fixed configuration z∗1:T ∈ Ωη (θ∗ ), any ǫ > 0, any positive sequence {yn,T }n,T ≥1 such that log(1/yn,T ) =
o(n) and n and T large enough, we have
#


Pθ̆ (Z 1:T , z∗1:T | X 1:T )
∗
1:T
∗1:T
1:T
∗1:T
∗ kπ̆ − π k∞ > vn,T | Z
P
=
z
≤
P
Z
=
z
>
ǫy
θ
n,T
Pθ̆ (Z 1:T = z∗1:T | X 1:T )

"
#
"
#




(log(nT ))2


2
+ CnT 
exp − (δ − η) C1 n + C2 log(nT ) + C4 log(1/(ǫyn,T )) + exp − C3
+ 3n log(nT ) 
.


2


nv
θ∗

"

(20)

n,T

Combined with (19) and applying Lemma 5, this gives the desired result. So now we focus on establishing (20).
In what follows, we consider a fixed configuration z∗1:T ∈ Ωη (θ∗ ) and introduce the Hamming distance between
∗1:T
z
and any other configuration z1:T defined as
kz1:T − z∗1:T k0 =

T X
n
X

1zti ,z∗ti .

t=1 i=1

We let P∗θ∗ (·) denote the probability conditional on {Z 1:T = z∗1:T } under parameter θ = θ∗ , i.e. P∗θ∗ (·) = Pθ∗ (· | Z 1:T =
z∗1:T ). In the following, we will often use the fact that the variables {Xit j } are independent under P∗θ∗ (with mean
14

value π∗z∗t z∗t ) so that we can rely on Hoeffding’s Inequality. We introduce a sequence {vn,T }n,T ≥1 decreasing to 0 and
i

j

Ωn,T the event defined as

Ωn,T = {kπ̆ − π∗ k∞ ≤ vn,T }.
We bound the probability of interest in (20) by splitting it on the two complementary events Ωn,T and Ωcn,T . For
any ǫ > 0 and any positive sequence {yn,T }n,T ≥1
"

P∗θ∗

#
"(
#
)


Pθ̆ (Z 1:T , z∗1:T | X 1:T )
Pθ̆ (Z 1:T , z∗1:T | X 1:T )
∗
∗
c
.
+
P
≤
P
Ω
∩
Ω
>
ǫy
>
ǫy
∗
∗
n,T
n,T
n,T
θ
θ
n,T
Pθ̆ (Z 1:T = z∗1:T | X 1:T )
Pθ̆ (Z 1:T = z∗1:T | X 1:T )

(21)

Thus, the proof of (20) boils down to establishing the desired upper bound on the second term appearing in the
right-hand side of (21). We have
P∗θ∗

"(

#
)
Pθ̆ (Z 1:T , z∗1:T | X 1:T )
> ǫyn,T ∩ Ωn,T
Pθ̆ (Z 1:T = z∗1:T | X 1:T )
nT
X
X
≤

P∗θ∗

r=1 z1:T ;kz1:T −z∗1:T k0 =r

by using the bound (Q − 1)r
each value of r). Then,

nT 
r

"(

#
)
Pθ̆ (Z 1:T = z1:T | X 1:T )
r
r+1
> ǫyn,T /(Q (nT ) ) ∩ Ωn,T ,
Pθ̆ (Z 1:T = z∗1:T | X 1:T )

≤ Qr (nT )r on the number of terms in the sum over {z1:T ; kz1:T − z∗1:T k0 = r} (for

#
)
Pθ̆ (Z 1:T , z∗1:T | X 1:T )
∩
Ω
>
ǫy
n,T
n,T
Pθ̆ (Z 1:T = z∗1:T | X 1:T )
#
)
"(
nT
X
X
P (Z 1:T = z1:T | X 1:T )
∩
Ω
>
log(ǫy
)
−
r
log
Q
−
(r
+
1)
log(nT
)
≤
P∗θ∗ log θ̆ 1:T
n,T
n,T
Pθ̆ (Z = z∗1:T | X 1:T )
r=1 1:T 1:T ∗1:T
P∗θ∗

"(
z

≤

nT
X

;kz

−z

k0 =r

X

P∗θ∗

r=1 z1:T ;kz1:T −z∗1:T k0 =r

#
"(
)
P (Z 1:T = z1:T | X 1:T )
,
log θ̆ 1:T
∩
Ω
>
−
log(1/(ǫy
))
−
3r
log(nT
)
n,T
n,T
Pθ̆ (Z = z∗1:T | X 1:T )

(22)

as long as nT ≥ Q. For any configuration z1:T such that kz1:T − z∗1:T k0 = r, we denote by r(1), . . . , r(T ) the number
P
of differences between the two configurations at each time step t ∈ ~1, T , i.e. r(t) = kzt −z∗t k0 such that r = t r(t).
Moreover, for any parameter π, we define Dn,T (z1:T , π) the subset of indexes (i, j, t) ∈ ~1, n2 × ~1, T  such that
i < j for which the parameter π differs between the configuration z∗1:T and z1:T , namely
n
o
Dn,T (z1:T , π) ≔ (i, j, t) ∈ In,T ; πzti ztj , πz∗ti z∗tj ,

with In,T = {(i, j, t) ∈ ~1, n2 × ~1, T ; i < j} the set of indexes over which we sum to compute the conditional
log-likelihood. In what follows, we abbreviate to D∗ (resp. D̆), the set Dn,T (z1:T , π∗ ) (resp. Dn,T (z1:T , π̆)). Next
lemma gives a decomposition of the main term at stake in (22).
Lemma 9. We have the decomposition
log

Pθ̆ (Z 1:T = z1:T | X 1:T )
= U1 + U2 − U3 ,
Pθ̆ (Z 1:T = z∗1:T | X 1:T )

where

1 − π∗zt zt
π∗zt zt
X 
i j
i j

t
t

U1 ≔
 Xi j log ∗ + (1 − Xi j ) log
π
1 − π∗
(i, j,t)∈D∗

U2 ≔

X

(i, j,t)∈D∗ ∪D̆

U3 ≔

X

(i, j,t)∈D∗ ∪D̆

∗t
z∗t
i zj


T −1 X
n
n
 X
γ̆zt zt+1
ᾰz1 X
 +
log i i
log i +

ᾰz∗1
γ̆z∗t z∗t+1
∗t

z∗t
i zj



(π̆zti ztj − π∗zt zt )(Xit j − π∗zt zt ) 

i j
i j 


log 1 +

π∗t t (1 − π∗t t )
zi z j

i

t=1 i=1

i

(23)

i

(24)

zi z j



(π̆z∗ti z∗tj − π∗z∗t z∗t )(Xit j − π∗z∗t z∗t ) 


i
j
i
j
log 1 +
 .
π∗∗t ∗t (1 − π∗∗t ∗t )
zi z j

i=1

zi z j

15

(25)

Combining (22) and Lemma 9, we obtain








X P (Z 1:T = z1:T | X 1:T )




θ̆

>
ǫy
P∗θ∗ 
∩
Ω

n,T
n,T





z1:T ,z∗1:T Pθ̆ (Z 1:T = z∗1:T | X 1:T )

≤

nT
X

X

r=1 z1:T ;kz1:T −z∗1:T k0 =r



P∗θ∗ U1 + U2 − U3 > − log(1/(ǫyn,T )) − 3r log(nT ) ∩ Ωn,T .

We then decompose


P∗θ∗ U1 + U2 − U3 > − log(1/(ǫyn,T )) − 3r log(nT ) ∩ Ωn,T



≤P∗θ∗ U1 + U2 − U3 > − log(1/(ǫyn,T )) − 3r log(nT ) ∩ Ωn,T ∩ |U3 | ≤ r log(nT )



+ P∗θ∗ Ωn,T ∩ |U3 | > r log(nT )





≤P∗θ∗ U1 + U2 > − log(1/(ǫyn,T )) − 4r log(nT ) ∩ Ωn,T + P∗θ∗ Ωn,T ∩ |U3 | > r log(nT )





≤P∗θ∗ U1 > − log(1/(ǫyn,T )) − 5r log(nT ) + P∗θ∗ Ωn,T ∩ |U2 | > r log(nT )



+ P∗θ∗ Ωn,T ∩ |U3 | > r log(nT ) .

(26)

(27)
1:T

We handle these three terms separately in the following. From now on, we consider a configuration z
P
kz1:T − z∗1:T k0 = r = t r(t).

such that

First term in the right-hand side of (27). Recall that U1 is given by (23). We can further decompose this term


π∗zt zt 1 − π∗z∗t z∗t 
X 
i j
i j 


t
∗
(Xi j − π ∗t ∗t ) log
U1 =

∗
∗ 
zi z j

π
∗t ∗t 1 − π t t
∗
zi z j
zi z j
(i, j,t)∈D


T −1 X
n
n
1 − π∗zt zt  X
π∗zt zt
X
X 
γ̆zt zt+1
ᾰz1i
i j
i j 
 +

∗
∗
π ∗t ∗t log
+
(1
−
π
+
log i i .
+
log
∗t ∗t ) log

∗
∗
z
z
zi z j


i j
πz∗t z∗t
1 − πz∗t z∗t
ᾰz∗1i
γ̆z∗ti z∗t+1
t=1 i=1
(i, j,t)∈D∗
i=1
i
i

j

i

For n and T large enough such that Γ̆ ∈ [δ, 1 − δ]
[δ, 1 − δ]Q ), we have
n
X

log

i=1

ᾰz1i
ᾰz∗1i

+

T −1 X
n
X
t=1 i=1

log

γ̆zti zt+1
i
γ̆z∗ti z∗t+1
i

=

n
X
i=1

Q2

(implying for the corresponding stationary distribution ᾰ ∈

1{z1i ,z∗1i } log

≤ r(1) log

j

ᾰz1i
ᾰz∗1i
T −1

+

T −1 X
n
X
t=1 i=1

∗t ∗t+1 log
1{(zti ,zt+1
)}
i ),(zi ,zi

γ̆zti zt+1
i
γ̆z∗ti z∗t+1
i

1−δ X
1−δ
1−δ
+
≤ 2r log
.
[r(t) + r(t + 1)] log
δ
δ
δ
t=1

To handle the term U1 , we need to lower bound the cardinality of the set D∗ . This is the purpose of Lemma 10
which is a generalization of Proposition B.4 in Celisse et al. [2012]. This can be done for all the configurations
z1:T and all the configurations z∗1:T that belong to some Ωη (θ).
Lemma 10. For any η ∈ (0, δ), any parameter θ ∈ Θ, any configuration z1:T and any z∗1:T ∈ Ωη (θ) such that
kz1:T − z∗1:T k0 = r, we have
(δ − η)2
Dn,T (z1:T , π) ≥
nr.
4
Combining Lemma 10 with the previous bound, we get that

 n
T −1 X
n
X

X
γ̆zti zt+1
ᾰz1i
1−δ
8
1−δ
2r
i
∗ −1 
 ≤ ∗ log
≤
−−−−−→ 0.
(28)
+
log
log
(|D |)  log
2
∗1
∗t ∗t+1
ᾰ
γ̆
|D
|
δ
δ n→+∞
n(δ
−
η)
zi
zi zi
t=1 i=1
i=1

We also have

(|D∗ |)−1


1 − π∗zt zt
π∗t t
X 
i j
π∗ log zi z j + (1 − π∗ ) log
∗t ∗t
∗
∗
z
z
 z∗ti z∗tj
i
j
π
1−π

(i, j,t)∈D∗



 ≤
−k(π∗ql , π∗q′ l′ )
 q,l,q′ ,lmax
′ ;π∗ ,π∗
∗t
ql
q′ l ′

∗t
z∗t
i zj

z∗t
i zj

with k(x, y) = x log(x/y) + (1 − x) log[(1 − x)/(1 − y)] for (x, y) ∈ (0, 1)2. The function k is positive for every (x, y)
such that x , y, hence, introducing the notation K ∗ = minq,l,q′ ,l′ ;π∗ql ,π∗q′ l′ k(π∗ql , π∗q′ l′ )/2,
max

q,l,q′ ,l′ ;π∗ql ,π∗q′ l′

−k(π∗ql , π∗q′ l′ ) ≔ −2K ∗ < 0.
16

So, by (28), we have for n large enough






T −1 X
n
n
π∗zt zt
1 − π∗zt zt  X

X 
X
t t+1 
1
γ̆
ᾰ


z
z
z



i
j
i
j
i i
i


∗ −1
∗
∗


≤ −K ∗ .
(|D |) 
+
log
log

πz∗ti z∗tj log π∗ + (1 − πz∗ti z∗tj ) log 1 − π∗  +




∗1
∗t
∗t+1
ᾰ
γ̆
(i, j,t)∈D∗
zi
zi zi 
z∗t z∗t
z∗t z∗t
t=1 i=1
i=1
i

j

This leads to

P∗θ∗ (U1

> u) ≤

P∗θ∗

i

j





π∗t t 1 − π∗z∗t z∗t 
 X 

i j 

(X t − π∗ ) log zi z j
 − |D∗ |K ∗ > u
∗t

 i j


∗
∗
z∗t
z
i j
πz∗t z∗t 1 − πzt zt 
(i, j,t)∈D∗
i

j

i j

for any u > 0 and large enough n. Moreover, thanks to Hoeffding’s Inequality and Assumption 3,




π∗zt zt 1 − π∗z∗t z∗t 

 X 
i j 
i j



∗
∗
∗ 
∗
∗
t
 > u + |D |K 
(Xi j − π ∗t ∗t ) log
Pθ∗ (U1 > u) ≤Pθ∗ 
∗
∗
z
z


i j
πz∗t z∗t 1 − πzt zt 
(i, j,t)∈D∗
i j
i j
#
" 2
u + |D∗ |2 K ∗2 + 2u|D∗ |K ∗
≤ exp −
|D∗ |Cζ
"
#
"
#
"
#
∗ 2 ∗2
|D | K + 2u|D∗ |K ∗
2uK ∗
|D∗ |K ∗2
= exp −
exp −
,
≤ exp −
|D∗ |Cζ
Cζ
Cζ

where Cζ is a constant depending on ζ. Finally using Lemma 10, we have
"
#
"
#


 2K ∗
|D∗ |K ∗2
∗
Pθ∗ U1 > − log(1/(ǫyn,T )) − 5r log(nT ) ≤ exp log(1/(ǫyn,T )) + 5r log(nT )
exp −
Cζ
Cζ
"
#
"
#
∗

 2K
(δ − η)2 K ∗2
≤ exp log(1/(ǫyn,T )) + 5r log(nT )
exp −nr
.
Cζ
4Cζ
Second term in the right-hand side of (27). We have


(π̆zti ztj − π∗zt zt )(Xit j − π∗zt zt ) 

X
i j
i j 
 ≤

U2 ≔
log 1 +

∗
∗
π
(1
−
π
)
zt zt
zt zt
∗
(i, j,t)∈D ∪D̆

i j

i j

(π̆zti ztj − π∗zt zt )(Xit j − π∗zt zt )

X

i j

i j

π∗zt zt (1 − π∗zt zt )

(i, j,t)∈D∗ ∪D̆

i j

.

i j

For any q, l, q′, l′ ∈ ~1, Q, we introduce the sets

′ ∗t
′
Fqlq′ l′ = Fqlq′ l′ (z1:T , z∗1:T ) ≔ {(i, j, t) ∈ In,T ; zti = q, ztj = l, z∗t
i = q , zj = l }

Fql = Fql (z1:T ) ≔ ∪1≤q′ ,l′ ≤Q Fqlq′ l′ = {(i, j, t) ∈ In,T ; zti = q, ztj = l}

Gqlq′ l′ = Gqlq′ l′ (z1:T , z∗1:T , π∗ , π̆) ≔ (D∗ ∪ D̆) ∩ Fqlq′ l′
∗
′
∗
′ ∗t
∗t )}
= {(i, j, t) ∈ In,T ; zti = q, ztj = l, z∗t
i = q , z j = l and (πzt zt , πz∗t z∗t or π̆zti ztj , π̆z∗t
i zj
i

i j

1:T

∗1:T

∗

j

∗

Gql = Gql (z , z , π , π̆) ≔ (D ∪ D̆) ∩ Fql
= {(i, j, t) ∈ In,T ; zti = q, ztj = l and (π∗zt zt , π∗z∗t z∗t or π̆zti ztj , π̆z∗ti z∗tj )}.
i

i j

j

Then we bound
|U2 | ≤
≤
≤

X

π̆ql − π∗ql

π∗ (1
1≤q,l≤Q ql
X

π∗ql )
(i, j,t)∈D∗ ∪D̆

π̆ql − π∗ql

π∗ (1
1≤q,l≤Q ql
X

−
−
−

(Xit j − π∗ql )1zti =q,ztj =l ≤

X

(Xit j

X

(Xit j − π∗z∗t z∗t ) +

π∗ql ) (i, j,t)∈G
ql

π̆ql − π∗ql

π∗ (1
1≤q,l≤Q ql

X

π∗ql ) (i, j,t)∈G
ql

−

π∗z∗t z∗t )
i j

i

+

j

X

π∗ (1
1≤q,l≤Q ql
π̆ql − π∗ql

π∗ (1
1≤q,l≤Q ql
X

π̆ql − π∗ql

X

−
−

X

(π∗∗t ∗t
π∗ql ) (i, j,t)∈G zi z j
ql

π̆ql − π∗ql

π∗ (1
1≤q,l≤Q ql

−

X

(X t
π∗ql ) (i, j,t)∈G i j
ql

X

(π∗′ ′
π∗ql ) q′ ,l′ q l

− π∗ql )

− π∗ql )

− π∗ql )|Gqlq′ l′ | .

(29)

For every u > 0, we thus have
P∗θ∗ (Ωn,T









X
X π̆ql − π∗ql






∗
t
−
π
(X
∩ {|U2 | > u}) ≤P∗θ∗ 
∗t ∗t ) > u/2 ∩ Ωn,T 

i
j
zi z j
∗ (1 − π∗ )




π

1≤q,l≤Q ql
ql (i, j,t)∈Gql








X
X π̆ql − π∗ql






(π∗q′ l′ − π∗ql )|Gqlq′ l′ | > u/2
∩ Ωn,T  .
+ P∗θ∗ 
∗
∗





π
(1
−
π
)

1≤q,l≤Q ql
ql 1≤q′ ,l′ ≤Q
17

(30)

We start by dealing with the first term of the right-hand side of (30). Notice that on the event Ωn,T , we have
(π̆ql − π∗ql )/(π∗ql (1 − π∗ql )) ≤ vn,T /ζ 2 for every q, l ∈ ~1, Q. The next lemma establishes that any set Dn,T (z1:T , π)

is included in a larger set, whose cardinality is bounded. In particular, the random set D̆ is included in a larger
deterministic subset.
Lemma 11. Let z1:T and z∗1:T denote two configurations such that kz1:T − z∗1:T k0 = r. Then for any parameter
π = (πql )1≤q,l≤Q , we have
o
n
1:T
∗t
) ≤ 2nr.
Dn,T (z1:T , π) ⊂ Dn,T (z1:T ) ≔ (i, j, t) ∈ ~1, n2 × ~1, T ; (zti , ztj ) , (z∗t
i , z j ) and Dn,T (z
As the set Gql is random (because D̆ is random), we write



∗





π̆
−
π
X
X
ql


ql



∗
t
)
−
π
(X
>
u/2
P∗θ∗ 
∩
Ω
∗t
∗t

n,T

ij
zi z j




1≤q,l≤Q π∗ql (1 − π∗ql ) (i, j,t)∈Gql




 X
 X
2 
X
X

uζ


∗ 
∗
t
∗
 ≤
≤Pθ∗ 
(Xi j − πz∗t z∗t ) >
P
∗ 
θ

i j
2vn,T 
1≤q,l≤Q (i, j,t)∈Gql

D⊂Dn,T (z1:T )

X

1≤q,l≤Q (i, j,t)∈Fql ∩D

(Xit j

−

π∗z∗t z∗t )
i j


uζ 2 
>
,
2vn,T 

where now D is a deterministic set. By a union bound and Hoeffding’s inequality, we have for any D ⊂ Dn,T (z1:T )




 X
 X
2 
2 
X


uζ
uζ


t
∗
2
∗
t
∗
∗ 
 ≤Q max Pθ∗ 

(Xi j − πz∗t z∗t ) >
(Xi j − πz∗t z∗t ) >
Pθ∗ 


i
j
i j
1≤q,l≤Q
2vn,T
2vn,T 
(i, j,t)∈Fql ∩D
1≤q,l≤Q (i, j,t)∈Fql ∩D


 2u2 ζ 4 1 
2

 .
≤2Q exp − 2 4
4vn,T Q |D|

This leads to

∗


 X π̆ql − πql

P∗θ∗ 


1≤q,l≤Q π∗ql (1 − π∗ql )

For the second term of (30),
|D∗ ∪ D̆|) that




 X
P∗θ∗ 


1≤q,l≤Q

 X
≤P∗θ∗ 








X
 2u2 ζ 4 1 



∗
t
2

(Xi j − πz∗t z∗t ) > u/2
∩ Ωn,T  ≤
2Q exp − 2 4


i j

4vn,T Q |D| 

(i, j,t)∈Gql
D⊂Dn,T (z1:T )


2nr
X
X
 2u2 ζ 4 1 
2
≤
2Q exp − 2 4 
4vn,T Q k
k=1 D⊂Dn,T (z1:T );|D|=k


2nr
X
 2u2 ζ 4 1 

≤ 2Q2
(2nr)k exp − 2 4
4vn,T Q 2nr
k=1



u2 ζ 4 
2

≤ 2Q exp − 2 4  (2nr)2nr+1 .
4vn,T Q nr
X

we get from a union bound and from Lemma 11 (that gives an upper bound for









∗
∗
′ l′ | > u/2 ∩ Ωn,T 
)|G
−
π
(π
′
′
qlq

ql


π∗ql (1 − π∗ql ) 1≤q′ ,l′ ≤Q q l


X
uζ 2 
∗
∗
(πq′ l′ − πql )|Gqlq′ l′ | >

2vn,T 
1≤q,l≤Q 1≤q′ ,l′ ≤Q


!
 X
2 

uζ 2
uζ
2
∗ 
2 ∗
∗
∗


,
≤Q max Pθ∗ 
(πq′ l′ − πql )|Gqlq′ l′ | >
 ≤ Q Pθ∗ 2nr >
1≤q,l≤Q
2vn,T Q2 
2vn,T Q2
1≤q′ ,l′ ≤Q
(π̆ql − π∗ql )

X

because |π∗q′ l′ − π∗ql | ≤ 1, implying that
X
q′ ,l′

(π∗q′ l′ − π∗ql )|Gqlq′ l′ | ≤

X
q′ ,l′

|Gqlq′ l′ | = |Gql | = |Fql ∩ (D∗ ∪ D̆)| ≤ |Dn,T (z1:T )| ≤ 2nr.

Finally, we have the following upper bound for the second term of (27)


!
 rζ 4 (log(nT ))2 


ζ 2 log(nT )
2
∗
2nr+1
2 ∗


.
Pθ∗ Ωn,T ∩ |U2 | > r log(nT ) ≤ 2Q exp −
+ Q Pθ∗ vn,T >
 (2nr)
4Q2 n
4Q4 v2n,T n 
18

Third term in the right-hand side of (27). We want to bound (in probability) the last term U3 . Distinguishing
between the cases where Xit j = 0 and Xit j = 1, we have


(π̆z∗ti z∗tj − π∗z∗t z∗t )(Xit j − π∗z∗t z∗t ) 

X

i
j
i
j
U3 ≔
log 1 +

∗
∗
πz∗t z∗t (1 − πz∗t z∗t )
(i, j,t)∈D∗ ∪D̆
i j
i j





(π̆z∗ti z∗tj − π∗z∗t z∗t ) 
(π̆z∗ti z∗tj − π∗z∗t z∗t ) 


X 
i j 
i j 





t
t
 + Xi j log 1 +
(1 − Xi j ) log 1 −
=

∗
∗




(1
−
π
π
∗t ∗t )
∗t ∗t
zi z j
zi z j
(i, j,t)∈D∗ ∪D̆





X
X 
(π̆ql − π∗ql ) 
(π̆ql − π∗ql ) 



 1 ∗t ∗t .

t
t
 + Xi j log 1 +
=
(1 − Xi j ) log 1 −
 zi =q,z j =l
∗) 
∗
(1
−
π
π
ql
ql
∗
1≤q,l≤Q
(i, j,t)∈D ∪D̆

For any (q, l) ∈ ~1, Q2 , we further introduce the sets

∗t
∗
Fql
= ∪1≤q′ ,l′ ≤Q Fq′ l′ ql = {(i, j, t) ∈ In,T ; z∗t
i = q, z j = l}

∗t
∗
G∗ql = ∪1≤q′ ,l′ ≤Q Gq′ l′ ql = (D∗ ∪ D̆) ∩ Fql
= {(i, j, t) ∈ D∗ ∪ D̆; z∗t
i = q, z j = l}.

Centering the Xit j (under the distribution P∗θ∗ ), we get





X
X 
(π̆ − π∗ql ) 
(π̆ − π∗ql ) 
 + (X t − π∗ ) log 1 + ql
 1 ∗t ∗t
(π∗ − X t ) log 1 − ql
U3 =


 zi =q,z j =l
 ql
ql
ij
ij
∗) 
∗
(1
−
π
π
ql
ql
1≤q,l≤Q (i, j,t)∈D∗ ∪D̆





X
X 
(π̆ql − π∗ql ) 
(π̆ql − π∗ql ) 




 1 ∗t ∗t
∗
∗
 + πql log 1 +
+
(1 − πql ) log 1 −
 zi =q,z j =l
∗) 
∗
(1
−
π
π
ql
ql
1≤q,l≤Q (i, j,t)∈D∗ ∪D̆



 
X  
(π̆ql − π∗ql )  X
(π̆ql − π∗ql ) 





=
(Xit j − π∗ql )
 − log 1 −
log 1 +

∗
∗) 
π
(1
−
π
∗
ql
ql
(i, j,t)∈Gql
1≤q,l≤Q





∗
X
(π̆ql − π∗ql ) 
(π̆ql − πql ) 




 .
∗
∗ 
∗
 + πql log 1 +
+
|Gql | (1 − πql ) log 1 −

∗) 
∗
(1
−
π
π
ql
ql
1≤q,l≤Q

Then, on the event Ωn,T and for n and T large enough such that |(π̆ql −π∗ql )/(1 −π∗ql)| ≤ 1/2 and |(π̆ql −π∗ql )/π∗ql | ≤ 1/2
for every q and l, using the fact that | log(1 + x)| ≤ 2|x| for x ∈ [−1/2, 1/2], we have
|U3 | ≤4

vn,T X
ζ 1≤q,l≤Q

X

(i, j,t)∈G∗ql

(Xit j − π∗ql ) + 4

vn,T X
|G∗ |.
ζ 1≤q,l≤Q ql

Then, for every u > 0,
P∗θ∗

Ωn,T



 ∗  X
∩ {|U3 | > u} ≤Pθ∗ 


X

1≤q,l≤Q (i, j,t)∈G∗ql

(Xit j

−

π∗ql )





X


uζ 
uζ
∗
∗
 + Pθ∗ vn,T
>
|Gql | >  .
8vn,T 
8
1≤q,l≤Q

For the first term of (31), using Hoeffding’s inequality as before,




 X

 X
2nr
 X
X
X
X
uζ
uζ




∗ 
∗
t
 ≤
P∗θ∗ 
P
(Xit j − π∗ql ) >
−
π
)
(X
>
∗ 


θ
ij
ql





8vn,T
8vn,T 
∗
k=1 D⊂Dn,T (z1:T );|D|=k
1≤q,l≤Q (i, j,t)∈G∗ql
1≤q,l≤Q (i, j,t)∈D∩Fql




u2 ζ 2
2
2nr+1
 .

≤2Q (2nr)
exp − 2 4 2
8 Q vn,T nr
For the second term of (31), we use




X


uζ 
uζ
 ≤ P∗θ∗ vn,T >
.
P∗θ∗ vn,T
|G∗ql | >
8
16nr
1≤q,l≤Q

Finally, we have the following upper bound for the third term of (27)


!
 r(log(nT ))2ζ 2 


log(nT )ζ
2
2nr+1
∗
∗


.
exp − 2 4 2
Pθ∗ Ωn,T ∩ |U3 | > r log(nT ) ≤ 2Q (2nr)
 + Pθ∗ vn,T >
16n
8 Q vn,T n 
19

(31)

Combining the 3 bounds on the right-hand-side of (27).


P∗θ∗ U1 + U2 − U3 > − log(1/(ǫyn,T )) − 3r log(nT ) ∩ Ωn,T


"
#
"
#
 rζ 4 (log(nT ))2 

 2K ∗
(δ − η)2 K ∗2
2
2nr+1

≤ exp log(1/(ǫyn,T )) + 5r log(nT )
exp −nr
+ 2Q (2nr)
exp −
Cζ
4Cζ
4Q4 v2n,T n 


!
!
 r(log(nT ))2 ζ 2 
ζ 2 log(nT )
2
2nr+1
2 ∗
−
 + P∗θ∗ vn,T > log(nT )ζ .
+
2Q
(2nr)
exp
+ Q Pθ∗ vn,T >
16n
4Q2 n
82 Q4 v2n,T n
p
Now
 we choose the sequencevn,T such that vn,T = o( log(nT )/n) which is sufficient to imply that the quantities
∗
Pθ∗ vn,T > ζ 2 log(nT )/(4Q2n) and P∗θ∗ vn,T > log(nT )ζ/(16n) vanish as n and T increase. For large enough values
of n and T and with C1 , C2 , C3 , C4 and κ positive constants only depending on Q, ζ and K ∗ , we then have


P∗θ∗ U1 + U2 − U3 > − log(1/(ǫyn,T )) − 3r log(nT ) ∩ Ωn,T


"
#
"
#
 rζ 4 (log(nT ))2 

 2K ∗
(δ − η)2 K ∗2
2
2nr+1


≤ exp log(1/(ǫyn,T )) + 5r log(nT )
exp −nr
+ 2Q (2nr)
exp −
Cζ
4Cζ
4Q4 v2n,T n


 r(log(nT ))2 ζ 2 
2
2nr+1


+ 2Q (2nr)
exp − 2 4 2
8 Q vn,T n 
#
"
#
"
(log(nT ))2r
2
.
(32)
≤ exp − (δ − η) C1 nr + C2 log(nT )r + C4 log(1/(ǫyn,T )) + κ exp 3nr log(nT ) − C3
nv2n,T
Let us introduce
h
i
unT = exp −(δ − η)2C1 n + C2 log(nT ) + C4 log(1/(ǫyn,T ))




(log(nT ))2
 .

+
3n
log(nT
)
wnT = exp −C3
nv2n,T

Now! we go back to (26). Noticing that the number of configurations z1:T such that kz1:T − z∗1:T k0 = r is equal to
nT
(Q − 1)r , we have
r
(
! X
!
)
!
nT
nT
X
Pθ̆ (Z 1:T , z∗1:T | X 1:T )
nT
nT
r r
P∗θ∗
≤
+
(Q
−
1)
u
∩
Ω
(Q − 1)r κwrnT
>
ǫy
n,T
n,T
nT
r
r
Pθ̆ (Z 1:T = z∗1:T | X 1:T )
r=1
r=1


nT
≤[1 + QunT ] − 1 + κ [1 + QwnT ]nT − 1 .
p
Finally, notice that as long as log T = o(n) and log(1/yn,T ) = o(n) (resp. as long as vn,T = o( log(nT )/n)), we
have nT unT (resp. nT wnT ) converges to 0. Then we obtain for some universal positive constant C and large enough
n and T
!
)
(
Pθ̆ (Z 1:T , z∗1:T | X 1:T )
∗
> ǫyn,T ∩ Ωn,T ≤ CnT (unT + wnT ).
Pθ∗
Pθ̆ (Z 1:T = z∗1:T | X 1:T )
This leads directly to inequality (20).


5.4 Proof of Theorem 3
∗
with Γ̂ as defined by the fixed
We fix some σ ∈ SQ and study the convergence in Pθ∗ −probability of γ̂σ(q)σ(l) to γql
point equation (4), i.e.


PT −1 Pn
t
t+1
= l | X 1:T
t=1
i=1 Pθ̂σ Zi = q, Zi
γ̂σ(q)σ(l) =

.

PT −1 Pn
t
1:T
t=1
i=1 Pθ̂σ Zi = q | X

First, let us denote

Aq,l =
Bq =

T −1 n

XX


1
Pθ̂σ Zit = q, Zit+1 = l | X 1:T ,
n(T − 1) t=1 i=1
T −1 n

XX


1
Pθ̂σ Zit = q | X 1:T .
n(T − 1) t=1 i=1
20

Then we can write the quantity at stake as
∗
=
γ̂σ(q)σ(l) − γql

!
∗
Aq,l − α∗q γql
Aq,l
1
1
∗
∗
=
− γql
+ α∗q γql
− ∗
Bq
Bq
Bq αq

to obtain the following upper bound on the probability of interest






p
p
p
∗

 Aq,l − α∗q γql
 ∗ ∗ 1
log n 
log n 
log n 
1
ǫ
ǫ
∗
 + Pθ∗ αq γql
 .
−
> rn,T √
> rn,T √
Pθ∗  γ̂σ(q)σ(l) − γql > ǫrn,T √
 ≤ Pθ∗ 
Bq
2
Bq α∗q
2
nT
nT
nT
(33)
First term of the right-hand side of (33). For the first term in (33), for any 0 < λ < δ (implying λ < α∗q for any
q ∈ ~1, Q),
p
p




∗
∗



 Aq,l − α∗q γql
 Aq,l − α∗q γql
log n 
log n
ǫ
ǫ
∗
> rn,T √
> rn,T √
Bq ≥ αq − λ Pθ∗ Bq ≥ α∗q − λ
Pθ∗ 
 =Pθ∗ 
Bq
2
Bq
2
nT
nT

p

∗ ∗



 Aq,l − αq γql
log n
ǫ
∗
Bq < αq − λ Pθ∗ Bq < α∗q − λ
> rn,T √
+ Pθ∗ 
Bq
2
nT
p






log n ∗
ǫ
∗
∗
(34)
(αq − λ) + Pθ∗ Bq < α∗q − λ .
≤Pθ∗  Aq,l − αq γql > rn,T √
2
nT
√ !
log n
∗ ∗
∗
First, we upper bound the probability Pθ Aq,l − αq γql > ǫrn,T √nT for any ǫ > 0, using the following lemma.
Lemma
12. If log(T ) = o(n), for any ǫ > 0, for any sequence {rn,T }n,T ≥1 increasing to infinity such that rn,T =
p
o nT/ log n and any η ∈ (0, δ), we have for any σ ∈ SQ


p
T −1 X
n
X



log n 

1
∗
∗
1:T
t+1
t

 ≤ Pθ∗ kπ̂σ − π∗ k∞ > vn,T + o(1)
− αq γql > ǫrn,T √
Pθ̂σ Zi = q, Zi = l | X
Pθ∗ 
n(T − 1) t=1 i=1
nT
p

with vn,T a sequence decreasing to 0 such that vn,T = o log(nT )/n .
PQ
PQ ∗
Then, for the second term of (34), notice that Bq = l=1
Aq,l and l=1
γql = 1. We then have, if log(T ) = o(n)
p

and vn,T = o log(nT )/n , using Lemma 12 again,
 Q

Q
X
 X






∗
∗
∗
∗
∗

Pθ∗ Bq < αq − λ =Pθ∗ Bq − αq < −λ = Pθ∗  (Aq,l − αq γql ) < −λ ≤
< −λ/Q
Pθ∗ Aq,l − α∗q γql
l=1

≤

Q
X
l=1

l=1




∗
> λ/Q ≤ QPθ∗ kπ̂σ − π∗ k∞ > vn,T + o(1).
Pθ∗ Aq,l − α∗q γql


p

p
Finally, for the first term of (33), if yn,T is such that 1/yn,T = o nT/ log(n) , if vn,T = o log(nT )/n and as
long as log(T ) = o(n), we obtain
p


∗
 Aq,l − α∗q γql
log n 
ǫ

 ≤ (Q + 1)Pθ∗ kπ̂σ − π∗ k∞ > vn,T  + o(1).
> rn,T √
Pθ∗ 
(35)
Bq
2
nT

Second term of the right-hand side of (33). For the second term of (33), we split it on two complementary
events as before. For any 0 < λ < δ, we have

p
p






 ∗ ∗ 1
 ∗ ∗ 1
log n 
log n
ǫ
ǫ
1
1
∗



− ∗ > rn,T √
− ∗ > rn,T √
Bq ≥ αq − λ Pθ∗ Bq ≥ α∗q − λ
Pθ∗ αq γql
 =Pθ∗ αq γql
Bq αq
2
Bq αq
2
nT
nT


p



log n
1
ǫ
 ∗ ∗ 1
∗
Bq < αq − λ Pθ∗ Bq < α∗q − λ
− ∗ > rn,T √
+ Pθ∗ αq γql
Bq αq
2
nT
p





 ∗ ∗ 1
log n
1
ǫ
∗

Bq ≥ αq − λ Pθ∗ Bq ≥ α∗q − λ
− ∗ > rn,T √
≤Pθ∗ αq γql
Bq αq
2
nT


∗
(36)
+ Pθ∗ Bq < αq − λ .
21

We already gave an upper bound on the second term in the right-hand side of (36). Let us give one for the first
term. Notice that as α∗q ≥ δ and if Bq ≥ α∗q − λ ≥ δ − λ > 0, we have by the mean value theorem
1
1
1
−
≤
Bq − α∗q .
Bq α∗q
(δ − λ)2
We can then
(36), as long
 first term in the right-hand side of
 as log(T ) = o(n), for {yn,T }n,T ≥1 such that
 pwrite for the
p
1/yn,T = o nT/ log n and with vn,T such that vn,T = o log(nT )/n , still using Lemma 12

p





log n
ǫ
1
∗ 1
Pθ∗ α∗q γql
− ∗ > rn,T √
Bq ≥ α∗q − λ Pθ∗ Bq ≥ α∗q − λ
Bq αq
2
nT

 Q

p
p

2
 X

log n 
log n 
(δ − λ)2 ǫ
(δ − λ) ǫ
∗
∗
∗




(Aq,l − αq γql ) >
≤Pθ∗  Bq − αq >
 = Pθ∗ 
∗ rn,T √
∗ rn,T √
2α∗q γql
2α∗q γql
nT
nT
l=1

p

Q
X

log n 
(δ − λ)2 ǫ
∗
 ≤ QPθ∗ kπ̂σ − π∗ k∞ > vn,T  + o(1).
>
≤
Pθ∗  Aq,l − α∗q γql
r
∗ γ∗ Q n,T √
2α
nT
q ql
l=1

We finally obtain for the second term of the right-hand side of (33)


p
 ∗ ∗ 1
log n 
1
ǫ

 ≤ 2QPθ∗ kπ̂σ − π∗ k∞ > vn,T  + o(1).
∗
Pθ αq γql
−
> rn,T √
Bq α∗q
2
nT

(37)

We conclude the proof by summing the upper bounds obtained in (35) and (37)
p



log n 
∗
 ≤(3Q + 1)Pθ∗ kπ̂σ − π∗ k∞ > vn,T  + o(1)
> ǫrn,T √
Pθ∗  γ̂σ(q)σ (l) − γql
nT
p
√
√
p
P
∗
and by noticing that Pθ∗ (kΓ̂σ − Γ∗ k∞ > ǫrn,T log n/ nT ) ≤ 1≤q,l≤Q Pθ∗ (|γ̂σ(q)σ(l) − γql
| > ǫrn,T log n/ nT ).



5.5 Proof of Corollary 3

Denoting by σn,T the permutation minimizing the distance between π̂ (permuted) and π∗ for every (n, T ) ∈ ~1, n ×
~1, T , i.e. σn,T = argminσ∈SQ kπ̂σ − π∗ k∞ , we apply Theorem 3 to θ̂σn,T in order to get




p
p


log n 
log n 
∗
∗
Pθ∗  min k Γ̂σ − Γ k∞ > ǫrn,T √
 ≤Pθ∗ k Γ̂σn,T − Γ k∞ > ǫrn,T √

σ∈SQ
nT
nT
!
2
∗
≤Q (3Q + 1)Pθ∗ min kπ̂σ − π k∞ > vn,T + o(1) −−−−−→ 0,
σ∈SQ

n,T →∞



5.6 Proof of Theorem 5
We use the following lemma, that states that the quantity we optimize in the VEM algorithm and the log-likelihood
are asymptotically equivalent.
Lemma 13. We have the following inequality Pθ∗ -a.s.
sup
θ∈Θ

2
2 log(1/δ)
2
J(χ̂(θ), θ) −
ℓ(θ) ≤
.
n(n − 1)T
n(n − 1)T
n−1

We have that for any ǫ > 0, for n and T large enough,
Pθ∗ sup
θ∈Θ

!
!
ǫrn,T
2
2
2 log(1/δ) ǫrn,T
J(χ̂(θ), θ) −
ℓ(θ) > √
> √
≤ Pθ∗
=0
n(n − 1)T
n(n − 1)T
n−1
n
n

We then conclude by combining this result with Theorem 1.



5.7 Proof of Corollary 5
This is a direct consequence of Theorem 5 and Lemma 8 applied with the functions Fn,T =
22

2
n(n−1)T J(χ̂(·), ·).



5.8 Proof of Theorem 6
This proof is quite similar to that of Theorem 3. We fix some σ ∈ SQ and study the convergence in Pθ∗ −probability
∗
of γ̃σ(q)σ(l) to γql
with Γ̃ as defined by the fixed point equation (5), i.e.
Pn PT −1
i=1

t=1

η̂tiql (θ̃σ )

i=1

t=1

τ̂tiq (θ̃σ )

γ̃σ(q)σ(l) = Pn PT −1

First, let us denote

Aq,l

n T −1

.

n T −1

XX
XX
1
1
=
η̂tiql (θ̃σ ) =
Q
(Z t = q, Zit+1 = l),
n(T − 1) i=1 t=1
n(T − 1) i=1 t=1 χ̂(θ̃σ ) i

Bq =

n T −1

n T −1

XX
XX
1
1
τ̂tiq (θ̃σ ) =
Q
(Z t = q).
n(T − 1) i=1 t=1
n(T − 1) i=1 t=1 χ̂(θ̃σ ) i

Then we can write the quantity at stake as
∗
γ̃σ(q)σ(l) − γql
=

!
∗
Aq,l − α∗q γql
Aq,l
1
1
∗
∗
− γql
=
+ α∗q γql
− ∗ .
Bq
Bq
Bq αq

We follow the line of the proof of Theorem 3, using Lemma 14 below instead of Lemma 12 in order to obtain the
result.
p

Lemma 14. For any ǫ > 0, for any sequence {rn,T }n,T ≥1 increasing to infinity such that rn,T = o nT/ log n and
any η ∈ (0, δ), we have for any σ ∈ SQ


p
n X
T −1
X

log n 

1
t
t+1
∗
∗

 ≤ 2Pθ∗ kπ̃σ − π∗ k∞ > vn,T + o(1)
Pθ∗ 
Qχ̂(θ̃σ ) (Zi = q, Zi = l) − αq γql > ǫrn,T √
n(T − 1) i=1 t=1
nT

p
with vn,T a sequence decreasing to 0 such that vn,T = o( log(nT )/n).



Acknowledgement
Work partly supported by the grant ANR-18-CE02-0010 of the French National Research Agency ANR (project
EcoNet).

References
F. Bartolucci, M. F. Marino, and S. Pandolfi. Dealing with reciprocity in dynamic stochastic block models. Comput.
Stat. Data Anal., 123(C):86–100, 2018.
A.-K. Becker and H. Holzmann. Nonparametric identification in the dynamic stochastic block model. arXiv
e-prints, page arXiv:1811.00934, Nov. 2018.
P. Bickel, D. Choi, X. Chang, and H. Zhang. Asymptotic normality of maximum likelihood and its variational
approximation for stochastic blockmodels. Ann. Statist., 41(4):1922–1943, 08 2013.
S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic Theory of Independence.
OUP Oxford, 2013.
S. Bubeck. Jeux de bandits et fondations du clustering. PhD thesis, Université Lille 1, 2010.
A. Celisse, J.-J. Daudin, and L. Pierre. Consistency of maximum-likelihood and variational estimators in the
stochastic block model. Electron. J. Statist., 6:1847–1899, 2012.
23

K.-M. Chung, H. Lam, Z. Liu, and M. Mitzenmacher. Chernoff-Hoeffding bounds for Markov chains: generalized
and simplified. In C. Dürr and T. Wilke, editors, 29th International Symposium on Theoretical Aspects of
Computer Science (STACS 2012), volume 14 of Leibniz International Proceedings in Informatics (LIPIcs), pages
124–135, Dagstuhl, Germany, 2012. Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik.
J.-J. Daudin, F. Picard, and S. Robin. A mixture model for random graphs. Statistics and Computing, 18(2):
173–183, Jun 2008.
C. Gao, Y. Lu, and H. H. Zhou. Rate-optimal graphon estimation. Ann. Statist., 43(6):2624–2652, 12 2015.
S. Gaucher and O. Klopp. Maximum likelihood estimation of sparse networks with missing observations. Technical
report, manuscript, 2019.
Q. Han, K. Xu, and E. Airoldi. Consistent estimation of dynamic and multi-layer block models. In International
Conference on Machine Learning, pages 1511–1520, 2015.
Q. Ho, L. Song, and E. P. Xing. Evolving cluster mixed-membership blockmodel for time-varying networks. In
Proceedings of the 18th International Conference on Artificial Intelligence and Statistics (AISTATS), volume 15
of JMLR: W&CP, San Diego, CA, USA., 2011.
P. W. Holland, K. B. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Social Networks, 5(2):109 –
137, 1983.
P. Holme. Modern temporal network theory: a colloquium. The European Physical Journal B, 88(9):234, 2015.
O. Klopp, A. B. Tsybakov, and N. Verzelen. Oracle inequalities for network models and sparse graphon estimation.
Ann. Statist., 45(1):316–354, 2017.
S. L. Lauritzen. Graphical models, volume 17 of Oxford Statistical Science Series. The Clarendon Press, Oxford
University Press, New York, 1996.
J. Lei and A. Rinaldo. Consistency of spectral clustering in stochastic block models. Ann. Statist., 43(1):215–237,
2015.
F. Liu, D. Choi, L. Xie, and K. Roeder. Global spectral clustering in dynamic networks. Proceedings of the
National Academy of Sciences, 115(5):927–932, 2018.
M. Mariadassou and C. Matias. Convergence of the groups posterior distribution in latent or stochastic block
models. Bernoulli, 21(1):537–573, 2015.
P. Massart. Concentration inequalities and model selection, volume 1896 of Lecture Notes in Mathematics.
Springer, Berlin, 2007.
C. Matias and V. Miele. Statistical clustering of temporal networks through a dynamic stochastic block model. J.
R. Stat. Soc. Ser. B. Stat. Methodol., 79(4):1119–1141, 2017.
S. Paul and Y. Chen. Consistent community detection in multi-relational data through restricted multi-layer
stochastic blockmodel. Electron. J. Statist., 10(2):3807–3870, 2016.
M. Pensky. Dynamic network models and graphon estimation. Ann. Statist., 47(4):2378–2403, 08 2019. doi:
10.1214/18-AOS1751.
M. Pensky, T. Zhang, et al. Spectral clustering in the dynamic stochastic block model. Electronic Journal of
Statistics, 13(1):678–709, 2019.
K. Rohe, S. Chatterjee, and B. Yu. Spectral clustering and the high-dimensional stochastic blockmodel. Ann.
Statist., 39(4):1878–1915, 2011.
E. P. Xing, W. Fu, and L. Song. A state-space mixed membership blockmodel for dynamic network tomography.
Ann. Appl. Stat., 4(2):535–566, 2010.
K. Xu. Stochastic block transition models for dynamic networks. In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics (AISTATS), volume 38 of JMLR: W&CP, San Diego, CA, USA.,
2015.
24

K. S. Xu and A. O. Hero. Dynamic stochastic blockmodels for time-evolving social networks. IEEE Journal of
Selected Topics in Signal Processing, 8(4):552–562, 2014.
T. Yang, Y. Chi, S. Zhu, Y. Gong, and R. Jin. Detecting communities and their evolutions in dynamic social
networks– a Bayesian approach. Machine Learning, 82(2):157–189, Feb 2011.
X. Zhang, C. Moore, and M. E. J. Newman. Random graph models for dynamic networks. The European Physical
Journal B, 90(10):200, 2017.
R. Zreik, P. Latouche, and C. Bouveyron. The dynamic random subgraph model for the clustering of evolving
networks. Computational Statistics, 2016.

25

A Proofs of main results for the finite time case
A.1 Proof of Corollary 2
When the number of time steps is fixed and the connection probabilities vary over time, the conditional loglikelihood is
ℓcT (θ; Z 1:T ) =

T
X
X

t=1 1≤i< j≤n

Xit j log πtZ t Z t + (1 − Xit j ) log(1 − πtZ t Z t )
i

i

j

j

and the likelihood ℓT (θ) is defined as in (2) with ℓcT (·) instead of ℓc (·). The maximum likelihood estimator is then
θ̂ = (Γ̂, π̂1:T ) = argmax ℓT (θ).
θ∈ΘT

As before, we denote the normalized log-likelihood Mn,T (Γ, π1:T ) = 2/(n(n−1)T )ℓT (θ). We introduce the following
limiting quantity
MT (π1:T ) =

T
T
1X
1X
M(πt ) =
sup M(πt , A).
T t=1
T t=1 A∈A

We follow the lines of the proof of Theorem 1 in order to prove that we have for any sequence yn → +∞, for all
ǫ>0



ǫyn 
Pθ∗  sup Mn,T (Γ, π1:T ) − MT (π1:T ) > √  −→ 0.
(38)
n n→+∞
(Γ,π1:T )∈ΘT

√
Choosing yn = rn2 , we then use Lemma 8 to conclude that, as rn2 / n = o(1) by assumption, for any ǫ > 0,
!
∗1:T
1/4
−−−−→ 0.
min kπ̂1:T
−
π
k
>
ǫr
/n
Pθ∗
∞
n
σ1:T
n→∞

σ1 ,...,σT ∈SQ

In particular, for every t ∈ ~1, T , π̂t converges in Pθ∗ -probability to π∗t up to label switching. Then, let us prove
that on the event {minσ1 ,...,σT ∈SQ kπ̂1:T − π∗1:T
k ≤ ǫrn n−1/4 } (whose probability converges to 1), for n large enough,
σ1:T ∞
t
the permutation σ minimizing the distance between π∗t and π̂tσt is the same for every t ∈ ~1, T . We consider n
large enough such that ǫrn n−1/4 < min1≤q,l≤Q |π∗qq − π∗ll |/4. Denoting by σ1m , . . . , σTm the permutations (depending
on n) minimizing kπ̂1:T − π∗1:T
k , we have that, for any 1 ≤ t , t′ ≤ T , if some q, l ∈ ~1, Q are such that
σ1:T ∞
t
t′
σm (q) = σm (l), then
′
π̂tσtm (q)σtm (q) = π̂tσt′ (l)σt′ (l) = π̂tσt′ (l)σt′ (l)
m

m

m

m

and on the event we consider
′

′

′

′

′

∗t
∗t
∗t
∗t
t
t
∗t
∗t
t
t
∗t
|π∗t
qq − πll | = |πqq − πll | = |πqq − π̂σtm (q)σtm (q) + π̂σt′ (l)σt′ (l) − πll | ≤ |πqq − π̂σtm (q)σtm (q) | + |π̂σt′ (l)σt′ (l) − πll |
m

m

m

m

≤ 2ǫrn n−1/4 < min |π∗qq − π∗ll |/2,
1≤q,l≤Q

implying that q = l. This means that on this event, the permutation σtm minimizing the distance between π∗t and
π̂tσt is the same for every t ∈ ~1, T . We can conclude that
Pθ∗ min

σ∈SQ

kπ̂1:T
σ

−π

∗1:T

k∞ > ǫrn /n

1/4

!

= 1 − Pθ∗ min

σ∈SQ

kπ̂1:T
σ

−π

∗1:T

k∞ ≤ ǫrn /n

1/4

!

−−−−→ 0.
n→∞



A.2 Proof of Theorem 4
First, let us introduce some notations, as in the proof of Theorem 2. For any fixed configuration z∗1:T ∈ Ωη , we
define for any configuration z1:T and any parameter θ


Dn,T (z1:T , π1:T ) ≔ (i, j, t) ∈ In,T ; πtzt zt , πtz∗t z∗t
i j

26

i

j

and for any 1 ≤ t ≤ T



Dtn,T (zt , πt ) ≔ (i, j) ∈ ~1, n2; i < j and πtzt zt , πtz∗t z∗t ,
i j

∗

i

j

∗1:T

1:T

and as before, we abbreviate to D (resp. D̆), the set Dn,T (z , π ) (resp. Dn,T (z1:T , π̆1:T )). We also introduce
for any q, l, q′ , l′ ∈ ~1, Q the quantities Fqlq′ l′ , Fql , Gqlq′ l′ and Gql as before, accordingly to this definition of
Dn,T (z1:T , π1:T ). Finally, we introduce for any t ∈ ~1, T  and q, l, q′ , l′ ∈ ~1, Q the quantities
′
′ ∗t
∗t
t
t
t
t ∗t
2
t
Fqlq
′ l′ = F qlq′ l′ (z , z ) ≔ {(i, j) ∈ ~1, n ; i < j and zi = q, z j = l, zi = q , z j = l }
t
t
t
t
2
t
Fql
= Fql
(zt ) ≔ ∪1≤q′ ,l′ ≤Q Fqlq
′ l′ = {(i, j) ∈ ~1, n ; i < j and zi = q, z j = l}

t
Gtqlq′ l′ = Gtqlq′ l′ (zt , z∗t , π∗t , π̆t ) ≔ (D∗t ∪ D̆t ) ∩ Fqlq
′ l′

t
t
∗t
′
∗t
′ ∗t
= {(i, j) ∈ ~1, n2; i < j and zti = q, ztj = l, z∗t
i = q , z j = l and (πzt zt , πz∗t z∗t or π̆zt zt , π̆z∗t z∗t )}
i

i j

Gtql

=

Gtql (zt , z∗t , π∗t , π̆t )
2

= {(i, j) ∈ ~1, n ; i

∗t

t
≔ (D ∪ D̆ ) ∩ Fql
< j and zti = q, ztj =

j

i j

i

j

t

∗t
t
t
l and (π∗t
zt zt , πz∗t z∗t or π̆zt zt , π̆z∗t z∗t )}.
i j

i

j

i j

i

j

Note that we can get an equivalent of Lemma 10 with a similar proof that gives that for any configuration z∗1:T in
Ωη , for any configuration z1:T and any θ ∈ ΘT ,
γ2
nr.
4
In the same way, we have an equivalent of Lemma 11 (with a similar proof) that gives that for any zt and z∗t two
configurations at time t such that kzt − z∗t k0 = r(t) and any parameter πt = (πtql )1≤q,l≤Q , we have
n
o
∗t
t
t
Dtn,T (zt , πt ) ⊂ Dtn,T (zt ) ≔ (i, j) ∈ ~1, n2 × ~1, T ; (zti , ztj ) , (z∗t
(39)
i , z j ) and Dn,T (z ) ≤ 2nr(t).
Dn,T (z1:T , π1:T ) ≥

Going back to the proof of Theorem 4, we follow the line of that of Theorem 2, with a few changes. We get
the same decomposition as in equation (26), replacing π by π1 , . . . , πT in the definitions of U1 , U2 and U3 , and
replacing the event Ωn,T by Ωn = {kπ̂1:T − π∗1:T k∞ ≤ vn }. For U1 , the proof does not change. For U2 , we write
(instead of (29))
|U2 | ≤

X

(i, j,t)∈D∗ ∪D̆

≤

T
X
X

≤

T
X
X

π̆tql − π∗t
ql

X

∗t
π∗t
ql (1 − πql )

1≤q,l≤Q

π̆tql − π∗t
ql

π∗t (1
t=1 1≤q,l≤Q ql

−

π∗t
ql ) (i, j)∈Gt
ql

π̆tql − π∗t
ql

π∗t (1
t=1 1≤q,l≤Q ql

−

X

t
t
(Xit j − π∗t
ql )1zi =q,z j =l

X

(Xit j

X

−

π∗t
ql ) q′ ,l′ (i, j)∈Gt ′ ′
qlq l

π∗t
ql )

≤

π̆tql − π∗t
ql

T
X
X

π∗t (1
t=1 1≤q,l≤Q ql

(Xit j − π∗t
q′ l′ ) +

T
X
X

−

X

X

(Xit j
π∗t
ql ) q′ ,l′ (i, j)∈Gt ′ ′
qlq l
π̆tql − π∗t
ql

π∗t (1
t=1 1≤q,l≤Q ql

−

X

(π∗t
q′ l′
π∗t
)
ql q′ ,l′

− π∗t
ql )

t
− π∗t
ql )|G qlq′ l′ | .

For every u > 0, we thus have
P∗θ∗ ({|U2 | > u} ∩ Ωn ) ≤

T
X
t=1


t
∗t




 X π̆ql − πql

∗ 

Pθ∗ 
∗t


1≤q,l≤Q π∗t
ql (1 − πql )


X

X

1≤q′ ,l′ ≤Q (i, j)∈Gtqlq′ l′


t
∗t
T


X
 X π̆ql − πql

+
P∗θ∗ 
∗t


1≤q,l≤Q π∗t
ql (1 − πql )
t=1

X

1≤q′ ,l′ ≤Q

(π∗t
q′ l′








u


∗t
t
(Xi j − πq′ l′ ) >
∩ Ωn 


2T 










u 

t
− π∗t
)|G
|
>
∩
Ω
′
′

n
 .
ql
qlq l

2T 


(40)

∗t
∗t
2
We start by dealing with the first term of (40). Notice that on the event Ωn , we have π̆tql − π∗t
ql /(πql (1−πql )) ≤ vn /ζ

for every q, l ∈ ~1, Q. As the set Gtql is random (because D̆t is random), we write for every t ∈ ~1, T , using (39),




t
∗t






π̆
−
π
X
X
X


ql
ql
u




∗t
t
∗ 

∩ Ωn 
(Xi j − πq′ l′ ) >
Pθ∗ 

∗t (1 − π∗t )



π
2T



1≤q,l≤Q ql

ql 1≤q′ ,l′ ≤Q (i, j)∈Gt

ql




 X
 X
2 
2 
X
X
X
X
X



uζ
uζ


 ≤

≤P∗θ∗ 
(Xit j − π∗t
P∗θ∗ 
(Xit j − π∗t
′ l′ ) >
′ l′ ) >
q
q





2T
v
2T
v
n
n
′ ′
′ ′
t
t
t
t
1≤q,l≤Q 1≤q ,l ≤Q (i, j)∈Gql

D⊂Dn,T (z )

27

1≤q,l≤Q 1≤q ,l ≤Q (i, j)∈Fql ∩D

where now D is a deterministic set. By a union bound and Hoeffding’s inequality, we have for any D ⊂ Dtn,T (zt )
P∗θ∗


 X



X

X

(Xit j

t ∩D
1≤q,l≤Q 1≤q′ ,l′ ≤Q (i, j)∈Fql

−

π∗t
q′ l′ )




 X
uζ 2 
2
∗
 ≤Q max Pθ∗ 
>
 ′ ′
1≤q,l≤Q
2T vn 
1≤q ,l ≤Q
≤2Q2 exp −

This leads to, for the first term of (40),


t
∗t

T

X

 X (π̆ql − πql )

∗ 

Pθ∗ 
∗t


1≤q,l≤Q π∗t

ql (1 − πql )
t=1
≤

T
X

X

t=1 D⊂Dtn,T (zt )

≤2Q

2

T
X
t=1

2Q2 exp −

X

X

1≤q′ ,l′ ≤Q (i, j)∈Gtql

X

(Xit j

t ∩D
(i, j)∈Fql

!
2u2 ζ 4 1
.
4T 2 v2n Q4 |D|








u


∗t
t
(Xi j − πq′ l′ ) >
∩ Ωn 


2T 




! X
T 2nr(t)
X
2u2 ζ 4 1
≤
2
2
4
4T vn Q |D|
t=1 k=1

−

π∗t
q′ l′ )

X

D⊂Dtn,T (zt );|D|=k

2Q2 exp −



uζ 2 

>
2T vn Q2 

2u2 ζ 4 1
4T 2 v2n Q4 k

!

!
u2 ζ 4
u2 ζ 4
2nr(t)+1
2
exp − 2 2 4
(2nr(t))
≤ 2Q T exp − 2 2 4
(2nr)2nr+1 .
4T vn Q nr(t)
4T vn Q nr
!

For the second term of (40), we get from a union bound and from (39) that




T




X
X
X (π̆tql − π∗t


u
ql )



∗ 
∗t
∗t
t
Pθ∗ 
(πq′ l′ − πql )|Gqlq′ l′ | >
∩ Ωn 

∗t
∗t



2T 

1≤q,l≤Q πql (1 − πql ) 1≤q′ ,l′ ≤Q
t=1


!
T
 X
X
uζ 2
uζ 2 

2
∗
∗ 
2
∗t
∗t
t

.
max Pθ∗ 
≤Q
(πq′ l′ − πql )|Gqlq′ l′ | >
 ≤ Q T Pθ∗ 2nr >
1≤q,l≤Q
2T vn Q2 
2vn T Q2
t=1
1≤q′ ,l′ ≤Q

Finally, we have the following upper bound for U2
P∗θ∗

!
!


ζ 2 log(nT )
rζ 4 (log(nT ))2
2nr+1
2
∗
2
(2nr)
+ Q T Pθ∗ vn >
.
Ωn ∩ |U2 | > r log(nT ) ≤2Q T exp −
4Q4 T 2 v2n n
4Q2 T n

t
∗t
t ∗t
∗t
′ ′
For the third term U3 , denoting G∗t
ql = ∪1≤q ,l ≤Q G ql = {(i, j) ∈ D ∪ D̆ ; zi = q, z j = l}, we have






(π̆tql − π∗t
(π̆tql − π∗t
 ∗t




ql ) 
ql ) 

t
t
∗t
(πql − Xi j ) log 1 −
 + (Xi j − πql ) log 1 +
U3 =
 1z∗ti =q,z∗tj =l
∗t 
∗t
(1 − πql )
πql
1≤q,l≤Q (i, j,t)∈D∗ ∪D̆





X
X 
(π̆tql − π∗t
(π̆tql − π∗t




ql ) 
ql ) 


∗t
∗
 + πql log 1 +
+
 1z∗ti =q,z∗tj =l
(1 − πql ) log 1 −
∗t 
∗t
(1 − πql )
πql
1≤q,l≤Q (i, j,t)∈D∗ ∪D̆



 
T
X
X  
(π̆tql − π∗t
(π̆tql − π∗t

 X

ql ) 
ql ) 





(Xit j − π∗t
=
 − log 1 −

log 1 +
ql )
∗t
∗t 
πql
(1 − πql ) 
∗t
t=1 1≤q,l≤Q
X

X

(i, j)∈Gql

+

T
X
t=1






X
(π̆tql − π∗t
(π̆tql − π∗t





ql ) 
ql ) 

∗t
∗t
∗t 
 + πql log 1 −
|Gql | (1 − πql ) log 1 +
.
∗t
∗t 
πql
(1 − πql ) 
1≤q,l≤Q

∗t
t
∗t
∗t
Then, we have on the event Ωn and for n large enough such that |(π̆tql −π∗t
ql )/πql | ≤ 1/2 and |(π̆ql −πql )/(1−πql )| ≤ 1/2
for every q and l, using the fact that | log(1 + x)| ≤ 2|x| for x ∈ [−1/2, 1/2],

|U3 | ≤

T
T
X
X
X
vn X
vn X
4
+
4
)
(Xit j − π∗t
|G∗t
ql
ql |.
ζ
ζ
∗t
t=1
t=1
1≤q,l≤Q
1≤q,l≤Q
(i, j)∈Gql

Then, for every u > 0,
P∗θ∗ (Ωn ∩ {|U3 | > u}) ≤

T
X
t=1


 X

P∗θ∗ 


X




 X
T
X

uζ
uζ 

∗t
t
∗ 
∗t

 .
(Xi j − πql ) >
P ∗ vn
|G | >
+
8vn T  t=1 θ  1≤q,l≤Q ql
8T 
∗t

1≤q,l≤Q (i, j)∈Qql

28

(41)

For the first term of (41), using Hoeffding’s inequality as before,


T 2nr(t)
 X
T
X X
X
X
X


≤
>
uζ/(8v
T
)
(Xit j − π∗t
)
P∗θ∗ 

n
ql


t
∗t
t
t=1

t=1 k=1 D⊂Dn,T (z );|D|=k

q,l (i, j)∈Gql

≤2Q2 T exp −


X

P∗θ∗ 


X

∗t
q,l (i, j)∈Fql
∩D

u2 ζ 2
(2nr)2nr+1 ,
82 T 2 Q4 v2n nr
!




>
uζ/(8v
T
)
(Xit j − π∗t
)

n
ql

and for the second term of (41),
T
X
t=1

P∗θ∗




 X
uζ 
uζ 
∗t
∗
vn

.
|G
|
>
≤
T
P
v
>
∗

n
θ
ql

8T 
16T nr
q,l

Finally, we have the following upper bound for U3

!
!


ζ log(nT )
rζ 2 (log(nT ))2
(2nr)2nr+1 + T P∗θ∗ vn >
P∗θ∗ Ωn ∩ |U3 | > r log(nT ) ≤2Q2 T exp − 2 2 4 2
.
16T n
8 T Q vn n
p
which is sufficient to imply that the quantities
Now
 we choose the sequence
 vn such that vn = o( log n/n)

∗
2
2
∗
Pθ∗ vn > ζ log(nT )/(4Q T n) and Pθ∗ vn > ζ log(nT )/(16T n) vanish as n increases and we gather the three upper
bounds. For large enough values of n and with C1 , C2 , C3 , C4 and κ positive constants only depending on Q, ζ, K ∗
and T , we then have


P∗θ∗ U1 + U2 − U3 > − log(1/(ǫyn )) − 3r log(nT ) ∩ Ωn
"
!
#
"
#
2K ∗
(δ − η)2 K ∗2
rζ 4 (log(nT ))2
≤ exp (log(1/(ǫyn )) + 5r log(nT ))
(2nr)2nr+1
exp −nr
+ 2Q2 T exp −
Cζ
4Cζ
4Q4 T 2 v2n n
!
rζ 2 (log(nT ))2
(2nr)2nr+1
+ 2Q2 T exp − 2 2 4 2
8 T Q vn n
#
"
h
i
(log(nT ))2 r
2
≤ exp −(δ − η) C1 nr + C2 log(nT )r + C4 log(1/(ǫyn )) + κ exp 5nr log(nT ) − C3
nv2n

Then, introducing

h
i
unT = exp −(δ − η)2C1 n + C2 log(nT ) + C4 log(1/(ǫyn ))
"
#
(log(nT ))2
wnT = exp −C3
+
5n
log(nT
)
,
nv2n
we conclude as in the proof of Theorem 2, noticing
that nT unT (resp. nT wnT ) converges to 0 as n increases as long
p

as log(1/yn) = o(n) (resp. as long as vn = o( log(n)/n)).

A.3 Proof of Corollary 6

As in the proof of Theorem 5, using the convergence in Equation (38) and Lemma 13, we obtain for any ǫ > 0
!
ǫrn2
2
T 1:T
J(χ̂(θ), θ) − M (π ) > √ −→ 0.
Pθ∗ sup
n n→+∞
θ∈Θ n(n − 1)T
We then conclude by using Lemma 8 applied with Fn,T =

2
n(n−1)T

J(χ̂(·), ·).



B Proofs of technical lemmas
B.1 Proof of Lemma 1
As in the proof of Lemma E.2 from Celisse et al. [2012], we use the method of Lagrange multipliers to find the
fixed-point equation of the critical point. Recall that θ = (Γ, π) and let us denote the likelihood L(Γ, π) ≔ exp ℓ(θ) =
Pθ (X 1:T ) and the conditional likelihood Lc (z1:T , π) = Pθ (X 1:T | Z 1:T = z1:T ). Recall the definition of Nql (z1:T ) in (1)
and that
n
Y N (z1:T ) Y
Pθ (Z 1:T = z1:T ) =
γqlql
α1z1 .
1≤q,l≤Q

29

i=1

i

We compute the derivative of the Lagrangian with respect to each parameter γql .
∂
∂γql



 Q


Q
X

X


∂ X
1:T
1:T
1:T
L(Γ, π) +

 + λq






λ
L
(z
,
π)P
(Z
=
z
)
γ
−
1
=
m

c
θ

mk





 ∂γql 
1:T
m=1
k=1
z

=

X

Lc (z1:T , π)

z1:T

Nql (z1:T )
Pθ (Z 1:T = z1:T ) + λq
γql

T −1 n

X X X

1:T
1:T
1:T

Lc (z , π)Pθ (Z = z )1zti =q,zt+1
+ λq γql 

i =l
t=1 i=1 z1:T
T −1 n

X

1  X
t+1
1:T
t

Pθ (X , Zi = q, Zi = l) + λq γql  .
=
γql t=1 i=1
1
=
γql

At the critical point θ̆ = (γ̆, π̆), we obtain that for each (q, l) ∈ ~1, Q2 we have
γ̆ql ∝

T −1 X
n
X
t=1 i=1

where ∝ means ’proportional to’. The constraint
γ̆ql =

Pθ̆ (X 1:T , Zit = q, Zit+1 = l)

PT −1 Pn

P

l

γql = 1 gives the normalizing term and we obtain

1:T
, Zit = q, Zit+1
i=1 Pθ̆ (X
PT −1 Pn
1:T , Z t = q)
t=1
i=1 Pθ̆ (X
i

t=1

= l)

=

PT −1 Pn

t
t+1
= l | X 1:T )
i=1 Pθ̆ (Zi = q, Zi
.
PT −1 Pn
t
1:T )
t=1
i=1 Pθ̆ (Zi = q | X

t=1



B.2 Proof of Lemma 2
We can write the quantity to optimize
h
i
J(χ, θ) =EQχ log Pθ (X 1:T , Z 1:T ) + H(Qχ )
h
i
h
i
h
i
=EQχ log Pθ (X 1:T | Z 1:T ) + EQχ log Pθ (Z 1:T ) − EQχ log Qχ (Z 1:T )
 T

X X

t
t
=EQχ 
Xi j log πZit Z tj + (1 − Xi j ) log(1 − πZit Z tj )
t=1 i< j
 n

 n

n X
T −1
n X
T −1
X
X
X

X

1
t
t+1
+ EQχ  log αZi1 +
log γZit Zit+1  − EQχ  log Qχ (Zi ) +
log Qχ (Zi | Zi )
i=1

=

T XX
X
t=1 i< j q,l

+

Q
n X
X

i=1 t=1

i=1

i=1 t=1

h
i
τtiq τtjl Xit j log πql + (1 − Xit j ) log(1 − πql )

τ1iq log αq +

i=1 q=1

n XX
T −1
X
i=1 q,l t=1

ηtiql log γql −

Q
n X
X
i=1 q=1

τ1iq log τ1iq −

n X
T −1 X
X

ηtiql log

i=1 t=1 q,l

ηtiql
τtiq

.

(42)

Using this expression, we can obtain directly the expected fixed-point equation for the variational estimator of the
transition probability from q to l.


B.3 Proof of Lemma 3
We rely on the notation introduced in the proof of Theorem 1. For any t ∈ ~1, T , using classical dependency rules
in directed acyclic graphs and the expression (9) of ẑt , we write
X
Pθ (X t | Z t = zt )Pθ (Z t = zt | X 1:t−1 )
log Pθ (X t | X 1:t−1 ) = log
zt



X


t
t
1:t−1
t
t
t

≤ log Pθ (X | Z = ẑ )
Pθ (Z = z | X
) = log Pθ (X t | Z t = ẑt )
zt

30

and thus
log Pθ (X t | X 1:t−1 ) − log Pθ (X t | Z t = ẑt ) ≤ 0.

(43)

Using Bayes’ rule, we have
log Pθ (X t | X 1:t−1 ) = log Pθ (X t , Z t | X 1:t−1 ) − log Pθ (Z t | X 1:t ).
Taking the expectation of this quantity with respect to any distribution Q on Z t , we obtain
h
i


log Pθ (X t | X 1:t−1 ) = EQ log Pθ (X t , Z t | X 1:t−1 ) + KL Q; Pθ (Z t | X 1:t ) + H(Q)
h
i
≥ EQ log Pθ (X t , Z t | X 1:t−1 ) + H(Q)
h
i
h
i
≥ EQ log Pθ (X t | Z t ) + EQ log Pθ (Z t | X 1:t−1 ) + H(Q),

h
i


where KL Q; Pθ (Z t | X 1:t ) = EQ log Q(Z t ) − log Pθ (Z t | X 1:t ) is a Kullback-Leibler divergence (thus non negative)


and H(Q) = −EQ log Q(Z t ) is the entropy of Q.
Taking now Q as the Dirac distribution located on ẑt , we have H(Q) = 0 and
log Pθ (X t | X 1:t−1 ) ≥ log Pθ (X t | Z t = ẑt ) + log Pθ (Z t = ẑt | X 1:t−1 ).

(44)

Now, combining Inequalities (43) and (44), we obtain
log Pθ (Z t = ẑt | X 1:t−1 ) ≤ log Pθ (X t | X 1:t−1 ) − log Pθ (X t | Z t = ẑt ) ≤ 0,
giving the expected result.



B.4 Proof of Lemma 4
To prove this lemma, we first establish a control of the expectation of the random variable appearing in the statement.
Lemma 15. We have the following inequality for z∗1:T and z1:T any configurations and any θ ∈ Θ

 s


T X


X
πzti ztj 

2
2

 Z 1:T = z∗1:T  ≤
sup
(Xit j − π∗z∗t z∗t ) log 
Λ
Eθ∗ 



i
j
t
t
1 − πz z
n(n − 1)T
1:T
nT
Q2 n(n − 1)T
(z

,π)∈~1,Q ×[ζ,1−ζ]

t=1 i< j

i j

with Λ = 2 log[(1 − ζ)/ζ].

We now turn to the proof of Lemma 4. Let us first recall Talagrand’s inequality [see for e.g. Massart, 2007,
page 170, Equation (5.50)].
Theorem (Talagrand’s inequality). Let {Yit j }1≤i< j≤n,1≤t≤T denote independent and centered random variables. Define
T
X X
∀g ≔ {gti j }1≤i< j≤n,1≤t≤T ∈ G, S n,T (g) =
Yit j gti j ,
1≤i< j≤n t=1

where G ⊂ Rn(n−1)T/2 . Let us further assume that there exist b > 0 and σ2 > 0 such that |Yit j gti j | ≤ b for every
P P
(i, j, t) ∈ ~1, n2 × ~1, T  and any g ∈ G and supg∈G i< j t Var(Yit j gti j ) ≤ σ2 . Then, for every β > 0 and x > 0, for
any finite set {g1 , . . . , g2n(n−1)T/2 } of elements of G, we have
!
"
#
√
−1
−1
2
(45)
P
max
S n,T (g) ≥ E
max
S n,T (g) (1 + β) + 2σ x + b(β + 3 )x ≤ e−x .
g∈{g1 ,...,g2n(n−1)T/2 }

g∈{g1 ,...,g2n(n−1)T/2 }

First, notice that argmin̟∈[ζ,1−ζ] log(̟/(1 − ̟)) = ζ and argmax̟∈[ζ,1−ζ] log(̟/(1 − ̟)) = 1 − ζ so that we
have




T X


X
 πzti ztj 
2

∗
t
∗ 
 > ǫ 
(Xi j − πz∗t z∗t ) log 
Pθ∗ 
sup

i j
n(n − 1)T t=1 i< j
1 − πzti ztj
(z1:T ,π)∈~1,QnT ×[ζ,1−ζ]Q2




t
T X


X
̟


2


i,
j
 > ǫ 
≤ P∗θ∗ 
(Xit j − π∗z∗t z∗t ) log 
max


i j
1 − ̟ti, j
̟∈{ζ,1−ζ}n(n−1)T/2 n(n − 1)T
t=1 i< j
31

with ̟ ≔ {̟ti, j }1≤i< j≤n,1≤t≤T . The set {ζ, 1 − ζ}n(n−1)T/2 is finite, of size 2n(n−1)T/2 . Let us now apply Talagrand’s
2

inequality to our setup. Note that for every (i, j, t) ∈ ~1, n2 × ~1, T , for any π ∈ [ζ, 1 − ζ]Q , we have


 πzti ztj 
 ≤ log[(1 − ζ)/ζ] = Λ
(Xit j − π∗z∗t z∗t ) log 
i j
1 − πzti ztj 
2

almost surely thanks to Assumption
3, and with
p Λ as defined in Lemma 15. Combining this result with Lemma 15
√
and writing Ω = (1 + β)Λ n(n − 1)T/2 + n(n − 1)T (Λ/2)2 xn,T + (1/β + 1/3)(Λ/2)xn,T , we have for any ǫ > 0,
for any β > 0, applying Talagrand’s inequality with b = Λ/2 and σ2 = n(n − 1)T/2(Λ/2)2,




T X


X
πzti ztj 

2

 > ǫ 
P∗θ∗ 
(Xit j − π∗z∗t z∗t ) log 
sup


i j
n(n − 1)T t=1 i< j
1 − πzti ztj
(z1:T ,π)∈~1,QnT ×[ζ,1−ζ]Q2




T X


X
 ̟ti, j 
2

∗
t
∗ 
 > ǫ 
(Xi j − πz∗t z∗t ) log 
≤Pθ∗ 
max

i j
1 − ̟ti, j
̟∈{ζ,1−ζ}n(n−1)T/2 n(n − 1)T
t=1 i< j




T X


X
 ̟ti, j 
2
2

∗
t
∗ 
 ≤
(Xi j − πz∗t z∗t ) log 
Ω
≤Pθ∗ ǫ <
max
t
i j
1 − ̟i, j
n(n − 1)T
̟∈{ζ,1−ζ}n(n−1)T/2 n(n − 1)T
t=1 i< j




t
T X


X
̟


2
2


i,
j

 >
(Xit j − π∗z∗t z∗t ) log 
Ω
+ P∗θ∗ 
max
t

i j
1
−
̟
n(n
−
1)T
̟∈{ζ,1−ζ}n(n−1)T/2 n(n − 1)T
i,
j
t=1 i< j
!
2
Ω > ǫ + 2e−xn,T ≤ 1ǫ<2Ω/(n(n−1)T ) + 2e−xn,T .
≤P∗θ∗
n(n − 1)T


B.5 Proof of Lemma 5
For any η ∈ (0, δ), Hoeffding’s inequality [see for example Theorem 2.8 from Boucheron et al., 2013] gives that


!
n


Nq (Z t )
1X

t
1Zi =q < αq − η
≥ αq − η = 1 − Pθ ∃t ∈ ~1, T , ∃q ∈ ~1, Q;
Pθ ∀t ∈ ~1, T , ∀q ∈ ~1, Q,
n
n i=1
≥ 1−

which concludes the proof.

Q X
T
X
q=1 t=1





exp −2η2 n ≥ 1 − QT exp −2η2 n ,



B.6 Proof of Lemma 6
First notice that argmaxA∈A M(π, A) may not be unique, it is in fact a closed subset of A. However, we choose a
fixed element Āπ in this subset in the following. Letting ǫ > 0 and η ∈ (0, δ) and using Lemma 5, we can split the
probability as





T
T

X
 1 X





1
ǫr
ǫr


n
n
t
t
∗
Pθ∗ 
sup M(π, Āπ ) − M(π, Āπ ) > √  ≤Pθ∗ 
sup M(π, Āπ ) − M(π, Āπ ) > √ 
∩ Ωη (θ )


T
T t=1 π∈[ζ,1−ζ]Q2
2
6 n
6 n
t=1 π∈[ζ,1−ζ]Q


+ QT exp −2η2 n ,
recalling that

(
)
Nq (zt )
Ωη (θ) ≔ z1:T ∈ ~1, QnT ; ∀t ∈ ~1, T , ∀q ∈ ~1, Q,
≥ αq − η .
n

√ 
P
We thus want to bound the quantity Pθ∗ T −1 Tt=1 supπ∈[ζ,1−ζ]Q2 M(π, Ātπ ) − M(π, Āπ ) > ǫrn /(6 n) on the event
n
o
Z 1:T ∈ Ωη (θ∗ ) , which means bounding


T

 1 X
ǫrn
1:T
∗
t

sup M(π, Āπ ) − M(π, Āπ ) > √ Z ∈ Ωη (θ ) .
Pθ∗ 
T t=1 π∈[ζ,1−ζ]Q2
6 n
32

Let us denote for any matrix P of size m × n the norm kPk∞ = max(i, j)∈~1,m×~1,n |Pi j |. Then note that, for any matrix
2
Ă with coefficients in [0, 1], for any π ∈ [ζ, 1 − ζ]Q , using Assumption 2 and 3,
X

 X
|āqq′ āll′ − ăqq′ ăll′ | sup |π∗ql log πq′ l′ + (1 − π∗ql ) log(1 − πq′ l′ )|
M(π, Āπ ) − M(π, Ă) ≤
α∗q α∗l
π∈[ζ,1−ζ]Q2

q′ ,l′

q,l

≤ 2(1 − δ)2 (1 − ζ) log(1/ζ)

XX
q,l q′ ,l′

|āqq′ āll′ − ăqq′ ăll′ |

≤ 2(1 − δ)2 (1 − ζ) log(1/ζ)Q4 2kĂ − Āπ k∞ ≔ ckĂ − Āπ k∞
with c = 4(1 − δ)2 (1 − ζ) log(1/ζ)Q4 . On the event Ωη (θ∗ ) we then have


T
 1 X
ǫrn 
t

sup M(π, Āπ) − M(π, Āπ ) > √ 
Pθ∗ 
T t=1 π∈[ζ,1−ζ]Q2
6 n


T
 1 X

ǫr
n
=1 − Pθ∗ 
sup M(π, Ātπ ) − M(π, Āπ ) ≤ √ 
T t=1 π∈[ζ,1−ζ]Q2
6 n





ǫrn 
t

≤1 − Pθ∗ ∀t ∈ ~1, T , sup
M(π, Āπ ) − M(π, Āπ ) ≤ √ 
6 n
Q2
π∈[ζ,1−ζ]

!


ǫrn
2
≤1 − Pθ∗ ∀t ∈ ~1, T , ∀π ∈ [ζ, 1 − ζ]Q , M(π, Āπ ) − M(π, Ātπ ) ≤ √
6 n
!


ǫrn
2
≤1 − Pθ∗ ∀t ∈ ~1, T , ∀π ∈ [ζ, 1 − ζ]Q , ∃Ă ∈ At (Z 1:T ); M(π, Āπ ) − M(π, Ă) ≤ √
6 n
!
ǫrn
2
≤1 − Pθ∗ ∀t ∈ ~1, T , ∀π ∈ [ζ, 1 − ζ]Q , ∃Ă ∈ At (Z 1:T ); kĂ − Āπ k∞ <
√ .
6c n
2

We√then show that for any ǫ > 0, for every t ∈ ~1, T  and every π ∈ [ζ, 1 −√ζ]Q , for any n such that n >
t 1:T
6c n/[ǫrn (δ − η)], there
√ exists some Ă ∈ A (Z ) such that kĂ − Āπ k∞ < ǫrn /(6c n), i.e. such that for every q, l,
|ăql − āql | < ǫrn /(6c n). For every 1 ≤ q ≤ Q, we can√ construct Ăq· = (ăq1 , . . . , ăqQ ) as follows.
√ On the event
Ωη (θ∗ ), for every q ∈ ~1, Q, for any n such that n > 6c n/[ǫrn (δ − η)], we have Nq (Z t )ǫrn /(6c n) > 1 for every
t ∈ ~1, T . We then construct (n̆ql )1≤l≤Q as follows and take ăql = n̆ql /Nq (Z 1:T ) for every l ∈ ~1, Q.
t
t
• for l = 1 choose n̆q1 as the closest integer to Nq (Z t )āq1
√ . It is in the interval (Nq (Z )āq1 − 1, Nq (Zt )āq1 + 1)
t
t
so we have |āq1 − n̆q1 /Nq (Z )| < 1/Nq (Z ) < ǫrn /(6c n). Moreover, note that 0 ≤ n̆q1 ≤ Nq (Z ) because
0 ≤ Nq (Z t )āq1 ≤ Nq (Z t ).

• Repeat for l = 2, . . . , Q
– if
– if

Pl−1

l′ =1 (Nq (Z

Pl−1

l′ =1 (Nq (Z

t
t

)āql′ − n̆ql′ ) ≥ 0 choose n̆ql as the closest bigger (or equal) integer to Nq (Z t )āql .

)āql′ − n̆ql′ ) < 0 choose n̆ql as the closest smaller (or equal) integer to Nq (Z t )āql .

t
t
t
1:T
As before,
)<
ql − 1, Nq (Z )āql + 1) so we have |āql − n̆ql /Nq (Z )| < 1/Nq (Z
√ n̆ql is in the interval (Nq (Z )ā
t
t
t
ǫrn /(6c n). Moreover 0 ≤ n̆ql ≤ Nq (Z ) because 0 ≤ Nq (Z )āql ≤ Nq (Z ). We also have (by induction)
l
X
l′ =1

t

(Nq (Z )āql′

 l−1

X

t

− n̆ql′ ) = 
Nq (Z )āql′ − n̆ql′  + Nq (Z t )āql − n̆ql < 1.
l′ =1

PQ
PQ
PQ
In the end, we have | l=1
(Nq (Z t )āql − n̆ql )| < 1 i.e. |Nq (Z t ) − l=1
n̆ql | < 1, meaning that l=1
n̆ql = Nq (Z t ),
√
P
Q
t 1:T
t
both Nq (Z ) and l=1 n̆ql being integers. Then, if n > 6c n/[ǫrn (δ − η)], there exists Ă ∈ A (Z ) such that
√
kĂ − Āπ k∞ < ǫrn /(6c n). This leads to


T
 1 X
ǫrn 
M(π, Ātπ ) − M(π, Āπ ) > √  ≤ QT exp(−2η2 n) + 1 − 1n>6c √n/[ǫrn (δ−η)]
Pθ∗ 
T t=1
6 n

which concludes the proof.

33



B.7 Proof of Lemma 7
We can upper bound the expectation as follows
"
#
"
!#
!
1
1
Nq (Z 1 )Nl (Z 1 )
Nq (Z 1 )
∗ ∗
∗ Nl (Z )
∗ Nl (Z )
∗
Eθ ∗
− αq αl =Eθ∗
− αq
+ αq
− αl
n(n − 1)
n
n−1
n−1
"
#
#
"
Nq (Z 1 )
Nl (Z 1 )
Nl (Z 1 )
+ α∗q Eθ∗
− α∗q
− α∗l
≤Eθ∗
n
n−1
n−1
v
s 
t 

!2 
!
#
"
2
1 )2
 Nl (Z 1 )

 Nq (Z 1 )
N
(Z

l

∗
∗
∗



− αq  Eθ∗
− αl .
≤ Eθ∗ 
+ αq Eθ∗ 
2
n
n−1
(n − 1)
We have for any q ∈ ~1, Q
 X

X
i X
h
∗
∗2
α∗q +
α∗2
Eθ∗ 1Zi1 =q 1Z 1j =q =
Eθ∗ Nq (Z 1 )2 =
q = nαq + n(n − 1)αq .
i

i, j

i, j

This implies that

!2 
#
"

 Nq (Z 1 )
Nq (Z 1 )2
1 ∗
1 ∗ n − 1 ∗2
∗ 
∗


∗
E 
− αq  = Eθ
α − α∗2
− α∗2
q = αq (1 − αq ),
q = αq +
n
n
n q
n
n2
θ∗

and identically

#
!2 
"

 Nl (Z 1 )

1 ∗2
n
1 ∗ n
n ∗2
Nl (Z 1 )2
∗ 


∗
∗
Eθ 
α∗l −
− αl  = Eθ
αl =
αl =
αl
− α∗l .
+ α∗2
l −2
2
2
n−1
(n − 1)
n−1
(n − 1)
n−1
n−1
n−1

This leads to
r
# s
!
"

Nq (Z 1 )Nl (Z 1 )
n
n ∗2
1 ∗
1 ∗ n
∗
∗
∗
∗
∗ +
− αq αl ≤
αq (1 − α∗q )
α
+
α
α
−
α
α
Eθ ∗
q
q
q
l
l
n(n − 1)
n
n−1
n−1
n−1
(n − 1)2
s
√
r
1
1
n
n
+
≤
2
≤
+
,
2
2
n−1
n−1
(n − 1)
(n − 1)

(46)

using the fact that 0 ≤ α∗q ≤ 1 for every q ∈ ~1, Q.



B.8 Proof of Lemma 8
We first consider the case when T → ∞, and π is constant over time. We use the following lemma.

Lemma 16. For any θ ∈ Θ, we have for ǫ small enough (0 < ǫ < min1≤q,q′ ≤Q max1≤l≤Q |π∗ql − π∗q′ l |/2)
min kπσ − π∗ k∞ > ǫ =⇒ M(π∗ ) − M(π) >

σ∈SQ

2δ2 2
ǫ .
Q2

This gives an upper bound on the probability of interest
!
!
2δ2
√
Pθ∗ min kπ̂σ − π∗ k∞ > ǫ vn,T ≤ Pθ∗ M(π∗ ) − M(π̂) > 2 ǫ 2 vn,T .
σ∈SQ
Q
By definition of θ̂ = (Γ̂, π̂), we write
M(π∗ ) = Fn,T (Γ̂, π∗ ) + M(π∗ ) − Fn,T (Γ̂, π∗ ) ≤ Fn,T (Γ̂, π̂) + M(π∗ ) − Fn,T (Γ̂, π∗ ),
implying that
h
i h
i
M(π∗ ) − M(π̂) ≤ Fn,T (Γ̂, π̂) − M(π̂) + M(π∗ ) − Fn,T (Γ̂, π∗ ) .

We then obtain the following upper bound, that converges to 0 as n and T increase by assumption,
!
!
!
δ2
δ2
√
Pθ∗ min kπ̂σ − π∗ k∞ > ǫ vn,T ≤Pθ∗ Fn,T (Γ̂, π̂) − M(π̂) > 2 ǫ 2 vn,T + Pθ∗ M(π∗ ) − Fn,T (Γ̂, π∗ ) > 2 ǫ 2 vn,T .
σ∈SQ
Q
Q
When the number of time steps T is fixed and π is allowed to vary over time, the proof is almost the same.
√
∗1:T
Indeed, minσ1 ,...,σT ∈SQ kπ̂1:T
k∞ > ǫ vn means that there exists t ∈ ~1, T  such that minσt ∈SQ kπ̂tσt − π∗t k∞ >
1:T − π
σ
√
ǫ vn and we can apply Lemma 16 to this π̂t to obtain that M(π∗t ) − M(π̂t ) > 2ǫ 2 δ2 vn /Q2 . This implies that
MT (π∗1:T ) − MT (π̂1:T ) > 2ǫ 2 δ2 vn /(T Q2 ), which allows to conclude in the same way as before.

34

B.9 Proof of Lemma 9
We have
log

Pθ̆ (Z 1:T = z1:T | X 1:T )
P (X 1:T | Z 1:T = z1:T )
P (Z 1:T = z1:T )
= log θ̆ 1:T 1:T
+ log θ̆ 1:T
1:T
∗1:T
1:T
∗1:T
Pθ̆ (Z = z
|X )
Pθ̆ (X | Z = z )
Pθ̆ (Z = z∗1:T )


T
T −1 X
n
n
X
X 
X
1 − π̆zti ztj  X
π̆zti ztj
γ̆zt zt+1
ᾰz1i

 +
t
t
Xi j log
=
+
+ (1 − Xi j ) log
log i i .
log

π̆ ∗t ∗t
1 − π̆ ∗t ∗t
ᾰ ∗1
γ̆ ∗t ∗t+1
zi z j

t=1 1≤i< j≤n

zi z j

zi

i=1

t=1 i=1

zi zi

We decompose this sum as



n
T −1 X
n
T
π∗zt zt
1 − π∗zt zt  X
X
ᾰz1i
γ̆zti zit+1
Pθ̆ (Z 1:T = z1:T | X 1:T ) X X  t
i j
i j 
 +
t
Xi j log
log
log
)
log
+
(1
−
X
+
=
log

i
j
∗
∗
1:T
∗1:T
1:T
πz∗t z∗t
1 − πz∗t z∗t  i=1
ᾰz∗1i
γ̆z∗ti z∗t+1
Pθ̆ (Z = z
| X ) t=1 1≤i< j≤n 
t=1 i=1
i
i j
i j


∗
∗
T
X X 
π̆ t t π ∗t ∗t
1 − π̆zti ztj 1 − πz∗ti z∗tj 
 .
 X t log zi z j zi z j + (1 − X t ) log
(47)
+

 i j
i
j
∗
∗
∗t
∗t
∗t
∗t
π
π̆
1
−
π
1
−
π̆
zi z j
zt zt zi z j
zt zt
t=1 1≤i< j≤n
i j

i j

In the first sum of the right-hand side of (47), the terms are different from zero only for triplets (i, j, t) in D∗ .
Similarly in the last sum, the terms are different from zero for triplets (i, j, t) in D∗ ∪ D̆. As a consequence, we
obtain


T −1 X
n
n
1 − π∗zt zt  X
π∗zt zt
X
X 
γ̆zt zt+1
ᾰz1i
Pθ̆ (Z 1:T = z1:T | X 1:T )
i j 
i j
 +

t
t
Xi j log
log
+
(1
−
X
)
log
+
log i i
=
log

ij
∗
∗
1:T
∗1:T
1:T


πz∗t z∗t
1 − πz∗t z∗t
ᾰz∗1i
γ̆z∗ti z∗t+1
Pθ̆ (Z = z
| X ) (i, j,t)∈D∗
i
t=1 i=1
i=1
i j
i j


∗
∗
π
1
−
π
∗t 
X 
1 − π̆zti ztj
π̆ t t ∗t ∗t

z∗t
i zj 
X t log zi z j zi z j + (1 − X t ) log
 .
+
 i j
i
j
∗
∗
π t t π̆z∗t z∗t
1 − π t t 1 − π̆z∗t z∗ 
zi z j

(i, j,t)∈D∗ ∪D̆

i

zi z j

j

i

j

We now write the last sum in the right-hand side as


∗
∗
X 
π̆zti ztj πz∗ti z∗tj
1 − π̆zti ztj 1 − πz∗ti z∗tj 

t
Xit j log

∗ π̆ ∗t ∗t + (1 − Xi j ) log 1 − π∗ 1 − π̆ ∗t ∗t 

π
t t
t t
z
z
z
z
zi z j
zi z j
i j
i j
(i, j,t)∈D∗ ∪D̆
  




 
π̆zti ztj − π∗zt zt 
π∗z∗t z∗t 
1 − π∗z∗t z∗t 
π̆zti ztj − π∗zt zt 
 
 

X 




 t  

i j 
i j 
i
j
i
j





 + log
 + log
 + (1 − Xit j ) log 1 −
=
Xi j log 1 +
.




∗
∗









∗t
∗t
∗t
∗t
π
π̆
1
−
π
1
−
π̆


zi z j
zi z j 
zt zt
zt zt
(i, j,t)∈D∗ ∪D̆
i j

i j

Distinguishing between the cases where Xit j = 1 and Xit j = 0, we obtain



∗
∗

1 − π̆zti ztj 1 − πz∗ti z∗tj 
π̆zti ztj πz∗ti z∗tj

X t log
t

 i j
∗ π̆ ∗t ∗t + (1 − Xi j ) log 1 − π∗ 1 − π̆ ∗t ∗t 
π
t t
t t
z
z
z
z
zi z j
zi z j
i j
i j
(i, j,t)∈D∗ ∪D̆




(π̆zti ztj − π∗zt zt )(Xit j − π∗zt zt ) 
(π̆z∗ti z∗tj − π∗z∗t z∗t )(Xit j − π∗z∗t z∗t ) 


X
X
i j
i j 
i j
i j 

 .


 −
=
log 1 +
log 1 +


π∗t t (1 − π∗t t )
π∗∗t ∗t (1 − π∗∗t ∗t )
X

zi z j

(i, j,t)∈D∗ ∪D̆

zi z j

zi z j

(i, j,t)∈D∗ ∪D̆

zi z j

In the end, we decompose



T −1 X
n
n
1 − π∗zt zt  X
π∗zt zt
X
X 
γ̆zt zt+1
ᾰz1i
Pθ̆ (Z 1:T = z1:T | X 1:T )
i j 
 +
X t log i j + (1 − X t ) log
+
log i i
=
log
log


ij
ij
∗
∗
1:T
∗1:T
1:T


πz∗t z∗t
1 − πz∗t z∗t
ᾰz∗1i
γ̆z∗ti z∗t+1
Pθ̆ (Z = z
| X ) (i, j,t)∈D∗
t=1 i=1
i=1
i
i j
i j


∗
t
∗
t
t
(
π̆
−
π
)(X
−
π
)


X
zi z j

ij
zti ztj
zti ztj 

+
log 1 +

∗
∗
π t t (1 − π t t )
(i, j,t)∈D∗ ∪D̆

−

which gives the result.

X

(i, j,t)∈D∗ ∪D̆

zi z j

zi z j



(π̆z∗ti z∗tj − π∗z∗t z∗t )(Xit j − π∗z∗t z∗t ) 


i
j
i
j
log 1 +
 ,
∗
∗
π ∗t ∗t (1 − π ∗t ∗t )
zi z j

35

zi z j

B.10 Proof of Lemma 10
We first notice that
Dn,T (z1:T , π) =

T
o
o
1 n
1X n
(i, j) ∈ ~1, n2; πzti ztj , πz∗ti z∗tj .
(i, j, t) ∈ ~1, n2 × ~1, T ; πztiztj , πz∗ti z∗tj =
2
2 t=1

For every t ∈ ~1, T , we can apply Proposition B.4. from Celisse et al. [2012], as their Assumption (A4) is required
to hold only for z∗t (see proof) and is valid on Ωη (θ) with the constant δ − η. We obtain

We conclude by noticing that

PT

n
o
(δ − η)2
(i, j) ∈ ~1, n2; πzti ztj , πz∗ti z∗tj ≥
nr(t).
2

t=1

r(t) = r.

B.11 Proof of Lemma 11
The inclusion of the sets is straightforward. Now we have
o
n
n
o
∗t
(i, j, t) ∈ ~1, n2 × ~1, T ; πztiztj , πz∗ti z∗tj ≤ (i, j, t) ∈ ~1, n2 × ~1, T ; (zti , ztj ) , (z∗t
i , zj )
o
n
o
n
+ (i, j, t) ∈ ~1, n2 × ~1, T ; ztj , z∗tj
≤ (i, j, t) ∈ ~1, n2 × ~1, T ; zti , z∗t
i
≤2

T
X
t=1

nr(t) ≤ 2nr.

B.12 Proof of Lemma 12
First, let us decompose the quantity at stake as follows


p
T −1 X
n
X




log
n
1
∗
∗
1:T
t+1
t

Pθ∗ 
− αq γql > ǫrn,T √
Pθ̂σ Zi = q, Zi = l | X
n(T − 1) t=1 i=1
nT 


p
T −1 X
n
X

 Nql (Z 1:T )

log n 
ǫ
1
1:T
t+1
t


≤Pθ∗ 
−
P Z = q, Zi = l | X
> rn,T √
n(T − 1) t=1 i=1 θ̂σ i
n(T − 1)
2
nT
p


 Nql (Z 1:T )
log n 
ǫ
∗
∗
 ,
− αq γql > rn,T √
+ Pθ∗ 
n(T − 1)
2
nT

(48)

and upper bound the two terms in the right-hand side of (48). For the first one we will follow the proof of Theorem
3.9 from Celisse et al. [2012]. Let z1:T denote a fixed configuration. We work on the set {Z 1:T = z1:T } and write
V1 (z1:T ) ≔
≤

T −1 X
n
X

 Nql (z1:T )
1
Pθ̂σ Zit = q, Zit+1 = l | X 1:T −
n(T − 1) t=1 i=1
n(T − 1)

T −1 X
n
X


Nql (z1:T )
1
−
Pθ̂σ Zit = q, Zit+1 = l | X 1:T 1(zti ,zt+1
)=(q,l)
i
n(T − 1) t=1 i=1
n(T − 1)

+

≤

T −1 n

XX


1
Pθ̂σ Zit = q, Zit+1 = l | X 1:T 1(zti ,zt+1
i ),(q,l)
n(T − 1) t=1 i=1
T −1 n

XX


1
1 − Pθ̂σ (Zit , Zit+1 ) = (zti , zit+1 ) | X 1:T 1(zti ,zt+1
i )=(q,l)
n(T − 1) t=1 i=1
T −1 n

XX


1
Pθ̂σ (Zit , Zit+1 ) , (zti , zit+1 ) | X 1:T 1(zti ,zt+1
i ),(q,l)
n(T − 1) t=1 i=1


≤ 2Pθ̂σ Z 1:T , z1:T | X 1:T .
+

36

Then


p
p

 

 
log n 
log n 1:T 
ǫ
ǫ
1:T
1:T




Z 
Pθ∗ V1 (Z ) > rn,T √
 =Eθ∗ Pθ∗ V1 (Z ) > rn,T √
2
2
nT
nT
p


X


 ǫ

log n 1:T
  1:T
1:T 
1:T
1:T
Z = z  Pθ∗ Z 1:T = z1:T
Pθ∗ Pθ̂σ Z , z | X
≤
> rn,T √
4
nT
z1:T




p
X

 Pθ̂σ Z 1:T , z1:T | X 1:T


log n 1:T
ǫ
1:T

Z = z  Pθ∗ Z 1:T = z1:T
Pθ∗ 
≤
 > rn,T √
1:T
1:T
1:T
4
Pθ̂σ Z = z | X
nT
z1:T

≤QT exp(−2η2 n) + Pθ∗ kπ̂σ − π∗ k∞ > vn,T



√

 4 nT 
2



+ CnT exp −(δ − η) C1 n + C2 log(nT ) + C4 log 
p
ǫrn,T log n




(log(nT ))2
 ,

+
3n
log(nT
)
(49)
+ CnT exp −C3
nv2n,T

where the last inequality comes from Theorem 2 where the bound is uniform with respect to z1:T .
Now, for the second term of (48), we use the following lemma.

Lemma
√ c1 , c2 > 0 such that for any ǫ > 0, for any sequence {rn,T }n,T ≥1 , we have, as long as
p 17. There∗ exist
nT ) < 1,
ǫrn,T log n/(2α∗q γql


p


 Nql (Z 1:T )
log n 
ǫ
∗
∗
2
 ≤ c1 exp −c2 ǫ 2 rn,T
− αq γql > rn,T √
Pθ∗ 
.
(50)
n(T − 1)
2
nT
We
the assumption
p
√ the two upper bounds obtained in (49) and (50) in order to conclude,
p then combine
∗
nT ) < 1 being satisfied for n and T large enough because rn,T = o( nT/ log n). We obtain
ǫrn,T log n/(2α∗q γql
p

the expected result, using the fact that log(T ) = o(n), that rn,T increases to infinity and that vn,T = o log(nT )/n ,


T −1 X
n
X





1
t
t+1
1:T
∗
∗

− αq γql > ǫyn,T  ≤ Pθ∗ kπ̂σ − π∗ k∞ > vn,T + o(1).
Pθ̂σ Zi = q, Zi = l | X
Pθ∗ 
n(T − 1) t=1 i=1


B.13 Proof of Lemma 13
We have the following inequalities by definition of ẑ1:T , J(χ, θ) and χ̂(θ) and because the Kullback-Leibler divergence is non-negative
J(ẑ1:T , θ) ≤ J(χ̂(θ), θ) ≤ ℓ(θ) ≤ ℓc (θ, ẑ1:T ),
1:T

(51)

1:T

with J(ẑ , θ) = ℓ(θ) − KL(δẑ1:T , Pθ (·|X )). We write this Kullback-Leibler divergence (from Pθ (·|X
δẑ1:T , with χ = (τ, η) such that τtiq = ẑtiq and ηtiql = ẑtiq ẑilt+1 ) as follows

1:T

) to Qχ =

KL(δẑ1:T , Pθ (·|X 1:T )) = − log Pθ (ẑ1:T |X 1:T ).
We then obtain
J(ẑ1:T , θ) = log Pθ (X 1:T ) + log Pθ (ẑ1:T |X 1:T ) = Pθ (X 1:T |ẑ1:T ) + log Pθ (ẑ1:T )
=ℓc (θ; ẑ1:T ) +

n
X
i=1

log αẑ1i +

n X
T
X

t.
log γẑt−1
i ẑi

i=1 t=2

Combined with (51), this leads to the following inequality for any parameter θ ∈ Θ
|J(χ̂(θ), θ) − ℓ(θ)| ≤ J(ẑ1:T , θ) − ℓc (θ, ẑ1:T ) ≤ −
We can conclude that
sup
θ∈Θ

n
X
i=1

log αẑ1i −

n X
T
X
i=1 t=2

t ≤ nT log(1/δ).
log γẑt−1
i ẑi

2
2
2 log(1/δ)
J(χ̂(θ), θ) −
ℓ(θ) ≤
.
n(n − 1)T
n(n − 1)T
n−1

37



B.14 Proof of Lemma 14
This proof is quite similar to that of Lemma 12. For any ǫ > 0, let us write


p
n X
T −1
X

log n 
1
t
t+1
∗
∗


Q
(Z = q, Zi = l) − αq γql > ǫrn,T √
Pθ∗ 
n(T − 1) i=1 t=1 χ̂(θ̃σ ) i
nT


p
n X
T −1
1:T
X


N
(Z
)
log
n
ǫ
1
ql

Qχ̂(θ̃σ ) (Zit = q, Zit+1 = l) −
> rn,T √
≤Pθ∗ 
n(T − 1) i=1 t=1
n(T − 1)
2
nT 


p
 Nql (Z 1:T )
log n 
ǫ
∗
∗
− αq γql > rn,T √
+ Pθ∗ 

n(T − 1)
2
nT

and upper bound the two probabilities in the right-hand side of this inequality. We already proved in Lemma 12
that the second term converges to 0 thanks to the assumptions on the sequence {rn,T }n,T ≥1 . For the first term, let
z1:T denote a fixed configuration. Let us work on the set {Z 1:T = z1:T } and use the same method as in the proof of
Lemma 12,
n T −1

XX
1
Q
(Z t = q, Zit+1 = l) =
n(T − 1) i=1 t=1 χ̂(θ̃σ ) i
n T −1

n T −1

XX
XX
1
1
+
,
Qχ̂(θ̃σ ) (Zit = q, Zit+1 = l)1zti =q,zt+1
Q
(Z t = q, Zit+1 = l)1(zti ,zt+1
i =l
i ),(q,l)
n(T − 1) i=1 t=1
n(T − 1) i=1 t=1 χ̂(θ̃σ ) i
leading to
n X
T −1
X
Nql (z1:T )
1
Qχ̂(θ̃σ ) (Zit = q, Zit+1 = l) −
≤2Qχ̂(θ̃σ ) (Z 1:T , z1:T ).
n(T − 1) i=1 t=1
n(T − 1)

Then we obtain


p
n X
T −1
X

Nql (Z 1:T )
log n 
ǫ
1
t
t+1


Pθ∗ 
Q
(Z = q, Zi = l) −
> rn,T √
n(T − 1) i=1 t=1 χ̂(θ̃σ ) i
n(T − 1)
2
nT

p

X




log n 1:T
ǫ
1:T
1:T
1:T
Pθ∗ Qχ̂(θ̃σ ) (Z , z ) > rn,T √
≤
Z = z  Pθ∗ Z 1:T = z1:T .
4
nT
1:T
z

For each z1:T , we use the following lemma.

Lemma 18. Denoting P̃σ (·) = Pθ̃σ (Z 1:T = · | X 1:T ), we have the following inequality for any configuration z1:T
1:T

Qχ̂(θ̃σ ) (z

1:T

) − P̃σ (z

) ≤

r



1
− log P̃σ (z1:T ) .
2

This gives us

p



log n 1:T
ǫ
1:T
1:T
1:T
Z = z 
Pθ∗ Qχ̂(θ̃σ ) (Z , z ) > rn,T √
4
nT
p




log n 1:T
ǫ
1:T
1:T
1:T
1:T
1:T
Z = z 
≤Pθ∗  Qχ̂(θ̃σ ) (Z , z ) − P̃σ (Z , z ) > rn,T √
8
nT
p




log n 1:T
ǫ
Z = z1:T 
+ Pθ∗ P̃σ (Z 1:T , z1:T ) > rn,T √
8
nT


p
p

r

 ǫ



log n 1:T
log n 1:T
1
ǫ

1:T 
1:T
1:T
1:T
1:T
Z = z  + Pθ∗ P̃σ (Z , z ) > rn,T √
Z = z 
≤Pθ∗  − log P̃σ (z ) > rn,T √
2
8
8
nT
nT

p




 2 2




 ǫ rn,T log n  1:T
log n 1:T
ǫ
1:T
1:T
1:T
1:T
1:T
1:T
Z = z  .
≤Pθ∗ P̃σ (Z , z ) > 1 − exp −
 Z = z  + Pθ∗ P̃σ (Z , z ) > rn,T √
32nT
8
nT
(52)
38

Noticing that the assumptions on {rn,T }n,T ≥1 imply that


 2 2

 ǫ rn,T log n 

 = o(n)

− log 1 − exp −
32nT 



p

log n 
 = o(n),

− log rn,T √
nT

and

we can conclude by applying the result of Theorem 2 with the estimator θ̃σ = (Γ̃σ , π̃σ ) for both terms of the
right-hand side of (52).


B.15 Proof of Lemma 15
The proof follows the lines of the proof of Lemma C.3. from Celisse et al. [2012]. Let E∗θ∗ [·] denote the expectation
given Z 1:T = z∗1:T , i.e. E∗θ∗ [·] = Eθ∗ [· | Z 1:T = z∗1:T ]. Introducing a ghost sample {X̃it j }i, j,t that is independent of
{Xit j }i, j,t and has the same distribution, we write



T X

X
 πzti ztj  
2

t
∗
 

(X − πz∗t z∗t ) log 
E
sup
 1:T
i j
n(n − 1)T t=1 i< j i j
1 − πzti ztj  
(z ,π)∈~1,QnT ×[ζ,1−ζ]Q2


 T






X X
πzti ztj 


2



 {X t }  

∗ 
t
t
E
(X
−
X̃
)
log
=E∗θ∗ 
sup
∗ 

i,
j,t




i
j
θ
i
j
i
j






t
t
n(n
−
1)T
1
−
π

(z1:T ,π)∈~1,QnT ×[ζ,1−ζ]Q2
zi z j
t=1 i< j
 




T X




X
 πzti ztj 

2
 ∗ 


t
t
t


≤E∗θ∗ 
E
sup
(X
−
X̃
)
log
{X
}



∗

i, j,t 




θ
i
j
i
j
i
j






t t
1
−
π
2 n(n − 1)T


1:T
nT
Q
z
z
(z ,π)∈~1,Q ×[ζ,1−ζ]
i j
t=1 i< j




T X

X
 πzti ztj  
2

t
t
∗

  ,

(Xi j − X̃i j ) log 
≤Eθ∗ ,X,X̃ 
sup
1 − πzt zt  
1:T
nT
Q2 n(n − 1)T
≔E∗θ∗

,π)∈~1,Q ×[ζ,1−ζ]

(z

t=1 i< j

i j

where E∗θ∗ ,X,X̃ [·] denotes the expectation with respect to {X, X̃} = {Xit j , X̃it j }i, j,t under the true parameter θ∗ and given
Z 1:T = z∗1:T . At this point, we notice that, if {ǫit j }i, j,t ≔ ǫ are n2 T independent Rademacher variables, then the
random variables
Eǫ

T X
X

ǫit j (Xit j

t=1 i< j

−


 πzti ztj


X̃it j ) log 

1 − πzti ztj

follow the same distribution, which implies that


E∗θ∗,X,X̃ 

sup

(z1:T ,π)∈~1,QnT ×[ζ,1−ζ]Q2

and

T X
X
t=1 i< j

(Xit j

−


 πzti ztj


X̃it j ) log 

1 − πzti ztj







T X
X
πzti ztj  

2

 
Eǫ
ǫit j (Xit j − X̃it j ) log 
 
t zt
n(n − 1)T
1
−
π
z
i j
t=1 i< j



T X

X
 πzti ztj  
2

t
t
∗
  .
(Xi j − X̃i j ) log 
= Eθ∗ ,X,X̃ 
sup
1 − πzt zt  
1:T
nT
Q2 n(n − 1)T
(z

As a consequence, we have





,π)∈~1,Q ×[ζ,1−ζ]

t=1 i< j

i j




T X

X
πzti ztj  

2

 
Eǫ
ǫit j (Xit j − X̃it j ) log 
E ≤E∗θ∗ ,X,X̃ 
sup
 
t zt
n(n − 1)T
1
−
π
z
(z1:T ,π)∈~1,QnT ×[ζ,1−ζ]Q2
i j
t=1 i< j



T X

X
 πzti ztj  
2

t
t
∗ 
 
sup
ǫi j Xi j log 
Eǫ
≤Eθ∗ 
n(n − 1)T
1 − πzti ztj  
(z1:T ,π)∈~1,QnT ×[ζ,1−ζ]Q2
t=1 i< j



T X

X
 πzti ztj  
2

∗ 
t
t
 
+ Eθ∗ 
sup
ǫi j X̃i j log 
Eǫ
n(n − 1)T
1 − πzti ztj  
(z1:T ,π)∈~1,QnT ×[ζ,1−ζ]Q2
t=1 i< j



T X

X
πzti ztj  

2

  .
≤2E∗θ∗ 
ǫit j Xit j log 
Eǫ
sup
1 − πzt zt  
1:T
nT
Q2 n(n − 1)T
(z

,π)∈~1,Q ×[ζ,1−ζ]

t=1 i< j

39

i j

Then using Jensen’s inequality, Assumption 3 and the bound Varǫ (ǫit j Xit j ) ≤ 1, we get
v


u
u 
2 

t

T X
X
t zt


π



z


2



i j


 
E ≤2E∗θ∗ 
Eǫ 
sup
ǫit j Xit j log 


(z1:T ,π)∈~1,QnT ×[ζ,1−ζ]Q2 n(n − 1)T
1 − πzti ztj  
t=1 i< j
v

u
 T


t

X X
 πzti ztj 
2
∗ 
t
t

Varǫ 
ǫi j Xi j log 
≤2Eθ∗ 
sup
(z1:T ,π)∈~1,QnT ×[ζ,1−ζ]Q2 n(n − 1)T
1 − πzti ztj 
t=1 i< j
s

 π  r n(n − 1)T 

2
2


 ≤
sup log
Λ,
≤2E∗θ∗ 
n(n − 1)T π∈[ζ,1−ζ]
1−π
2
n(n − 1)T

where Λ = 2 log[(1 − ζ)/ζ], concluding the proof.



B.16 Proof of Lemma 16
We assume that minσ∈SQ kπσ − π∗ k∞ > ǫ. Without loss of generality, assume that the permutation (or one of the
permutations) minimizing this distance is the identity. Let us write, using the fact that IQ the identity matrix of size
Q maximizes in A (over the set of Q × Q stochastic matrices) the quantity M(π∗ , A) (see the proof of Theorem 3.6
in Celisse et al. [2012]) and denoting (āqq′ )q,q′ ∈~1,Q the coefficients of Āπ (thus depending on π),


X
X
X
1 − π∗ql  X
π∗ql

 =
āqq′ āll′ π∗ql log
M(π∗ ) − M(π) =
α∗q α∗l
+ (1 − π∗ql ) log
āqq′ āll′ K(π∗ql , πq′ l′ )
α∗q α∗l
′ l′
′ l′
π
1
−
π
q
q
q′ ,l′
q,l
q′ ,l′
q,l

denoting K(p1 , p2 ) = p1 log(p1 /p2 ) + (1 − p1 ) log[(1 − p1 )/(1 − p2 )] > 0 the Kullback-Leibler divergence from
a Bernoulli distribution with parameter p2 to a Bernoulli distribution with parameter p1 . For every q, there exists
q′ ≔ f (q) such that āqq′ ≥ 1/Q because Āπ is a stochastic matrix. Using Assumption 2, we obtain
M(π∗ ) − M(π) ≥

δ2 X
δ2 X
K(π∗ql , π f (q) f (l) ) ≥ 2
2(π∗ql − π f (q) f (l) )2
2
Q q,l
Q q,l

thanks to a result on Kullback-Leibler divergence for Bernoulli distributions (see for instance Bubeck [2010],
Chapter 10, Section 2, Lemma 10.3). We then want to show that there exist q, l such that |π∗ql − π f (q) f (l) | > ǫ.
• If f is a permutation, the assumption minσ∈SQ kπσ − π∗ k∞ > ǫ gives the expected result.
• If f is not a permutation, it is not injective and there exist q1 , q2 such that f (q1 ) = f (q2 ). Thanks to
Assumption 1, take l0 ∈ ~1, Q such that |πq1 l0 − πq2 l0 | = maxl∈~1,Q |πq1 l − πq2 l | > 0. Then
|π∗q1 l0 − π f (q1 ) f (l0 ) | + |π f (q2 ) f (l0 ) − π∗q2 l0 | ≥ |π∗q1 l0 − π f (q1 ) f (l0 ) + π f (q2 ) f (l0 ) − π∗q2 l0 | = |π∗q1 l0 − π∗q2 l0 | > 0
leading to either |π∗q1 l0 − π f (q1 ) f (l0 ) | ≥ |π∗q1 l0 − π∗q2 l0 |/2 > ǫ or |π∗q2 l0 − π f (q2 ) f (l0 ) | ≥ |π∗q1 l0 − π∗q2 l0 |/2 > ǫ, using the
fact that ǫ < min1≤q,q′ ≤Q max1≤l≤Q |π∗ql − π∗q′ l |/2.
So, as there exist q, l such that |π∗ql − π f (q) f (l) | > ǫ, we have
M(π∗ ) − M(π) >

2δ2 2
ǫ .
Q2


B.17 Proof of Lemma 17
For any node i ∈ ~1, n, the Markov chain {Zit }t≥1 is geometrically ergodic because its transition matrix Γ satisfies
Doeblin’s condition thanks to Assumption 2. For any z ∈ ~1, Q, let us denote δz the Dirac mass at z. There exists
a positive constant A and some r ∈ (0, 1) such that ∀q ∈ ~1, Q and ∀t ≥ 1, we have
δq Γt − α

TV

≤ Art ,

where k · kT V is the total variation norm. This leads to
δq Γt − α

TV

=

1
δq Γt − α
2

1

=

40

1 X t
|Γ (q, l) − αl | ≤ Art .
2 l∈~1,Q

We now consider the Markov chain {Z t = (Z1t , . . . , Znt )}t≥1 of the n nodes evolving through time. Note that it is
irreducible and aperiodic. Moreover, its transition matrix is given by Pn = Γ⊗n , the n-th Kronecker power of Γ and
its stationary distribution is α⊗n . For any z = (z1 , . . . , zn ) ∈ ~1, Qn, let us denote µn,z = ⊗ni=1 δzi . For every t ≥ 1,
we can decompose
µn,z Ptn − α⊗n

TV



=


n
⊗ δzi (Γ⊗n )t − α⊗n

i=1


1 n 
=
⊗ δzi Γt − α⊗n
2 i=1

1



=


n
⊗ δzi (Γt )⊗n − α⊗n

i=1

TV

1
=
2 (z′ ,...,z′ )∈~1,Qn
X
n

1

n
Y
i=1

TV


n 
= ⊗ δzi Γt − α⊗n

Γt (zi , z′i ) −

i=1

n
Y

TV

αz′i .

i=1

We use
n
Y
i=1

Γt (zi , z′i ) −

n
Y
i=1

So, reorganizing the terms, we write
µn,z Ptn − α⊗n

TV





n 
i−1
n


X
Y
 h
iY



t ′
t
′

′
′

αz′i =
α
.
(µ
Γ
)
Γ
(z
,
z
)
−
α


z
z
z
i
z

k
i


j
i
k




i=1

j=1

k=i+1





n
i−1
n 


Y
X
X


1
Y ′  t
′
t ′
′


Γ
(z
,
z
)
−
α
(µ
Γ
)
α
≤



i
z
z
z
z

k
i

i
j
k


2 (z′ ,...,z′ )∈~1,Qn i=1 

 j=1
k=i+1
n

1

n
1 XX

X

X

Γt (zi , z′i ) − αz′i

≤

2

≤

1 XX t
Γ (zi , z′i ) − αz′i ≤ nArt .
2 i=1 z′

i=1

z′1

n

αz′1 . . .

z′i−1

αz′i−1

z′i

X

Γt (zi+1 , z′i+1 ) . . .

z′i+1

X

Γt (zn , z′n )

z′n

i

Let us recall the definition of an ǫ-mixing time. For any Markov transition matrix M over the set X with stationary
distribution α, for any ǫ > 0, the ǫ-mixing time of the Markov chain is defined as
τ(ǫ) = min{t ≥ 1; max kδ x M t − αkT V ≤ ǫ}.
x∈X

Denoting by τn (ǫ) the ǫ-mixing time of the Markov chain {Z t }t≥1 , we thus obtain
τn (ǫ) ≤

log(nA/ǫ)
.
log(1/r)

Now, we introduce a new Markov chain Y = {Y t }t≥1 , that is defined by
Y t = (Z t , Z t+1 )

∀t ≥ 1.

Notice that it is irreducible and aperiodic, with stationary distribution ρ defined for every state (qt1 , . . . , qtn , q1t+1 , . . . , qnt+1 )
by
ρ(qt1 ,...,qtn ,qt+1
= αqt1 . . . αqtn γqt1 qt+1
. . . γqtn qt+1
.
,...,qt+1
n )
n
1
1
It is easily seen that for any ǫ > 0, its ǫ-mixing time τY,n (ǫ) equals τn (ǫ) + 1. We apply Theorem 3 from Chung et al.
P
∗
[2012], for any η ≤ 1/8, considering the weight function f (Y t ) = ni=1 for every t ≥ 1 (of expectation nα∗q γql
under
√
p
P
T
−1
∗ ∗
1:T
t
the stationary distribution). Then Nql (Z ) = t=1 f (Y ), and denoting ǫn,T = ǫrn,T log n/(2αq γql nT ), we
obtain that there exist c1 , c2 > 0 such that for any ǫ > 0, as long as ǫn,T ≤ 1
p




 Nql (Z 1:T )
log n 
ǫ
∗
∗
∗
− αq γql > rn,T √
Pθ∗ 
(T − 1)
 =Pθ∗ Nql (Z 1:T ) > (1 + ǫn,T )nα∗q γql
n(T − 1)
2
nT


∗
(T − 1)
+ Pθ∗ Nql (Z 1:T ) < (1 − ǫn,T )nα∗q γql

 2
∗


(T − 1) 
 ǫn,T nα∗q γql
 ≤ c exp −c ǫ 2 r2 .
≤c1 exp −
1
2

n,T
72τY,n (η)
41



B.18 Proof of Lemma 18
For any configuration z1:T ,
1:T

Qχ̂(θ̃σ ) (z

1:T

) − P̃σ (z

) ≤ Qχ̂(θ̃σ ) − P̃σ

TV

≤

r

1
KL(Qχ̂(θ̃σ ) , P̃σ ) ≤
2

r

1
KL(δz1:T , P̃σ ) ≤
2

r



1
− log P̃σ (z1:T ) ,
2

the third inequality being true because by definition Qχ̂(θ̃σ ) minimizes KL(·, P̃σ ) over the set of variational distributions.


42

