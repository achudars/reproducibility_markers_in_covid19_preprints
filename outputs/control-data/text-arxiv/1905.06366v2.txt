arXiv:1905.06366v2 [math.OC] 18 May 2020

Equivalence and invariance of the chi and Hoffman
constants of a matrix
Javier F. Peña∗

Juan C. Vera†

Luis F. Zuluaga‡

May 19, 2020

Abstract
We show that the following two condition measures of a full column rank matrix
A ∈ Rm×n are identical: the chi constant and a signed Hoffman constant. This identity
is naturally suggested by the evident invariance of the chi constant under sign changes of
the rows of A. We also show that similar equivalence and invariance properties extend
to variants of the chi and Hoffman constants that depend only on the linear subspace
A(Rn ) := {Ax : x ∈ Rn } ⊆ Rm . Finally, we show similar identities between the chi
constants and signed versions of Renegar’s and Grassmannian condition measures.
AMS Subject Classification: 65K10, 65F22, 90C25 90C57
Keywords: Condition measures, invariance, weighted least squares, linear inequalities

1

Introduction

We show a novel equivalence between the following two condition measures of a matrix that
play central roles in numerical linear algebra and in convex optimization: the chi measure [3,
5, 6, 29, 30] and the Hoffman constant [9, 12, 14, 34]. We also show some similar equivalences
for some variants of these constants.
Let A ∈ Rm×n be a full column rank matrix. The chi constant χ(A) and its variant
χ(A) arise in the analysis of weighted least squares problems [4, 6, 7, 13]. In particular,
χ(A) plays a central role in the analysis of Vavasis and Ye’s interior-point algorithm for
linear programming [19, 33]. A remarkable feature of Vavasis and Ye’s algorithm is its sole
dependence on the matrix A defining the primal and dual constraints.
The Hoffman constant H(A) is associated to Hoffman’s Lemma [9, 12], a fundamental
error bound for systems of linear constraints of the form Ax ≤ b. The Hoffman constant
and other similar error bounds are used to establish the convergence rate of a wide variety
of optimization algorithms [2, 8, 10, 15, 16, 18, 20, 23, 24, 34, 34].
∗

Tepper School of Business, Carnegie Mellon University, USA, jfp@andrew.cmu.edu
Department of Econometrics and Operations Research, Tilburg University, The Netherlands,
j.c.veralizcano@uvt.nl
‡
Department of Industrial and Systems Engineering, Lehigh University, USA, luis.zuluaga@lehigh.edu
†

1

As we discuss in Section 2, the chi constant χ(A) and its variant χ(A) can be seen as
measures of worst behavior of a canonical solution mapping for the following weighted least
squares problems
b 7→ arg min(Ax − b)T D(Ax − b)
x∈Rn

where D is a diagonal matrix with positive diagonal entries.
Similarly, the Hoffman constant H(A) and its variant H(A) can be seen as measures of
worst behavior of a canonical solution mapping for the following system of linear inequalities
b 7→ {x ∈ Rn : Ax ≤ b}.
It is not immediately obvious that there should be a relationship between the chi and
Hoffman constants. Nonetheless, it is known that H(A) ≤ χ(A) and that χ(A) can be
arbitrarily larger [11,26]. Thus an equivalence between the constants H(A) and χ(A) appears
impossible. The main goal of this paper is to show that this apparent impossibility can
be attributed to and rectified via a canonical sign invariance property of χ(A) detailed in
equation (2) below. Namely, the constant χ(A) does not change when the signs of some of
the rows of A are flipped as the solution mapping (7) satisfies this sign invariance property.
On the other hand, the constant H(A) does not satisfy this sign invariance property and
thus H(A) and χ(A) cannot be identical. Our main result (Theorem 1) shows that χ(A) and
H(A) become identical after properly tweaking H(A) to ensure the sign invariance property.
A similar type of invariance consideration yields identities between the variants χ(A) and
H(A). Our developments can be further extended to obtain analogous identities between the
four measures χ(A), χ(A), H(A), H(A) and the following two popular condition measures
for systems of linear inequalities: Renegar’s distance to ill-posedness R(A) [27] and the
Grassmannian condition measure G(A) [1].
The above developments are similar in spirit to results previously derived by Tunçel [32],
by Todd, Tunçel, and Ye [31], and by Ho and Tunçel [11]. These articles compare various
condition measures for linear programming including the chi and Hoffman constants. However, there are two major differences between our developments and theirs. First, most of
the results in [11, 31, 32] provide only inequalities and hence are weaker than our identities
concerning the chi and Hoffman constants. Second, the articles [11, 31, 32] do not deal with
Renegar’s and Grassmannian condition measures but instead relate the chi and Hoffman constants with Ye’s condition measure [35] for polyhedra of the form {AT y : y ≥ 0, kyk1 = 1}.
Hence we deliberately chose not to discuss Ye’s condition measure in this paper. However,
we note that our results can be extended to identities involving Ye’s condition measure by
drawing on the recent work by Peña and Roshchina [25].
To formally state the sign invariance property, we rely on the following convenient notation. Let S ⊆ Rm×m denote the set of signature matrices defined as follows
S := {Diag(s) : s ∈ {−1, 1}m }.

(1)

The constant χ(A) satisfies the following sign invariance property:
χ(A) = χ(SA) for all S ∈ S .

(2)

Our main result states that χ(A) and H(A) become identical if we take a suitable closure of
H(A) to ensure the sign invariance property.
2

Theorem 1. Let A ∈ Rm×n be a full column-rank matrix. Then
χ(A) = max H(SA).
S∈S

(3)

A similar type of invariance property relates the measures χ(A) and χ(A). The construction of χ(A) depends only on the subspace A(Rn ). Thus χ(A) readily satisfies the following
invariance under right multiplication by non-singular matrices
χ(A) = χ(AR) for all R ∈ Rn×n non-singular.
In analogy to Theorem 1, the measures χ(A) and χ(A) become identical if we take a suitable
closure of χ(A) to ensure the same invariance under right multiplication by non-singular
matrices (see Proposition 1):
χ(A) =

min

R∈Rm×m
non-singular

kARk · χ(AR).

Furthermore, the same kind of identity holds for the measures H(A) and H(A) (see Proposition 3):
kARk · H(AR).
H(A) = min
m×m
R∈R
non-singular

In particular, identity (3) in Theorem 1 readily extends to the measures χ(A) and H(A) as
follows (see Corollary 1):
χ(A) = max H(SA).
(4)
S∈S

Our proof of Theorem 1 will actually show the following stronger identity when all rows of
A are non-zero (see Theorem 2):
χ(A) =

max

S∈S
SA(Rn )∩Rm 6=∅
++

H(SA).

This stronger identity in turn yields some interesting connections with Renegar’s distance
to ill-posedness R(A) [27, 28] and the Grassmannian condition number of G(A) [1]. More
precisely, in Section 4 we show the following identity analogous to (3) (see Proposition 5):
χ(A) =

max

S∈S
SA(Rn )∩Rm 6=∅
++

1
R(SA)

(5)

and the following identity analogous to (4) (see Corollary 2):
χ(A) =

max

S∈S
SA(Rn )∩Rm
++ 6=∅

G(SA).

(6)

The main sections of the paper are organized as follows. Section 2 recalls the construction
of the chi constants χ(A), χ(A) as well as the Hoffman constants H(A), H(A) and some of
their main properties. Our presentation deliberately follows separate but similar formats
for χ(A), χ(A) and for H(A), H(A). Section 3 presents the proof of Theorem 1. To do so,
3

we state and prove the stronger Theorem 2. Finally, Section 4 recalls the construction of
Renegar’s condition measure R(A) and of the Grassmannian condition measure G(A). This
section also proves identities (5) and (6).
Throughout the paper whenever we encounter an Euclidean space Rd we implicitly assume
d
that it is endowed
√ with the Euclidean norm defined by the canonical inner product in R ,
d
that is, kuk := uTu for all u ∈ R . Likewise, whenever we encounter a space of matrices
Rp×d we implicitly assume that it is endowed with the operator norm, that is,
kAk = max kAxk
x∈Rd
kxk≤1

for all A ∈ Rp×d .

2

Definition and properties of the chi and Hoffman
constants

This section recalls the construction and main properties of the constants χ(A), χ(A) and
H(A), H(A). These constants can be seen as condition measures for two fundamental problems in scientific computing, namely weighted least squares and linear inequalities.

2.1

Weighted least squares

Let D ⊆ Rm×m denote the set of diagonal matrices in Rm×m with positive diagonal entries.
That is,
D := {Diag(d) : d ∈ Rm
++ },
m
m
where Rm
++ ⊆ R denotes the set of vectors in R with positive entries.
Suppose A ∈ Rm×n . Given D ∈ D, consider the weighted least squares problem

min (Ax − b)T D(Ax − b).

x∈Rn

(7)

When A is full column-rank, it is easy to see that the solution to (7) is precisely A†D b where
A†D is the following weighted pseudo-inverse of A [6, 29]:
A†D = (AT DA)−1 AT D.
2.1.1

(8)

Condition measures χ(A) and χ(A)

Suppose A ∈ Rm×n is full column-rank. The condition measure χ(A) is defined as the following worst-case characteristic of the family of solution mappings A†D : Rm → Rn constructed
via (8):
χ(A) := max kA†D k.
(9)
D∈D

Consider the following alternative reformulation of the weighted least-squares problem (7)
in the subspace A(Rn ):
minn (y − b)T D(y − b).
(10)
y∈A(R )

4

The solution to (10) is evidently the D-projection of b onto A(Rn ). Once again, it is easy to
see that if A is full column-rank then the D-projection onto A(Rn ) is
A(AT DA)−1 AT D = AA†D .
The condition measure χ(A) is defined as the following worst-case characteristic of the family
of solution mappings AA†D : Rm → A(Rn ):
χ(A) := max kAA†D k = max kA(AT DA)−1 AT Dk.
D∈D

D∈D

(11)

Although it is not immediately evident, the constants χ(A) and χ(A) are finite for any fullrank matrix A ∈ Rm×n . This fact was independently shown by Ben-Tal and Teboulle [3],
Dikin [5], Stewart [29], and Todd [30]. The constants χ(A) and χ(A) arise in and play a key
role in weighted least-squares problems [4, 6, 7] and in linear programming [11, 31–33].
We record some alternative expressions for χ(A) and χ(A) that are closely related to the
constructions of H(A) and H(A) discussed below. First, observe that
χ(A) = max
D∈D

max

b∈Rm ,x∈Rn
Ax6=b

kx − A†D (b)k
.
kAx − bk

Second, observe that
χ(A) = max
D∈D

2.1.2

max

b∈Rm ,y∈A(Rn )
y6=b

ky − AA†D (b)k
.
ky − bk

Properties of χ(A) and χ(A)

Suppose A ∈ Rm×n is full column-rank and D ∈ D. By construction, the solution mappings
A†D and AA†D satisfy the following property: For S ∈ S then (SA)†D = A†D S. In particular
k(SA)†D k = kA†D k and k(SA)(SA)†D k = kAA†D k. Therefore (9) and (11) imply that the
constants χ(A) and χ(A) satisfy the following sign invariance property:
χ(A) = χ(SA) and χ(A) = χ(SA), for all S ∈ S .
Furthermore, the quantity χ(A) depends only on the subspace A(Rn ) which evidently satisfies
A(Rn ) = AR(Rn ) for all non-singular R ∈ Rn×n . Therefore, the constant χ(A) is invariant
under multiplication by non-singular matrices, that is,
χ(A) = χ(AR), for all non-singular R ∈ Rn×n .

(12)

The constant χ(A) is not invariant under multiplication by singular matrices. Proposition 1
shows that χ(A) is the closure of χ(A) under this kind of invariance.
Proposition 1. Suppose A ∈ Rm×n is full column-rank. Then χ(A) ≤ kAk · χ(A) and
χ(A) = χ(A) when the columns of A are orthonormal. In particular,
χ(A) =

min

R∈Rn×n
non-singular

5

kARk · χ(AR).

(13)

Proof. Since kAA†D k ≤ kAk · kA†D k, the construction (9) and (11) of χ(A) and χ(A) readily
implies that
χ(A) ≤ kAk · χ(A).
(14)

Next, we show that χ(A) = χ(A) when the columns of A are orthonormal. To that end,
observe that if the columns of A are orthonormal then kAxk = kxk for all x ∈ Rn . In
particular, if the columns of A are orthonormal then kAA†D k = kA†D k for all D ∈ D. Thus (9)
and (11) imply that χ(A) = χ(A).
Finally, from (12) and (14) it follows that χ(A) = χ(AR) ≤ kARk · χ(AR) for all
R ∈ Rm×m non-singular. Thus (13) follows.
In the special case when m = n and A ∈ Rn×n is non-singular it is easy to see that
χ(A) = kA−1 k.

We will rely on the following related characterization of χ(A) from [6]. The same characterization is also stated and proved in [36] by adapting a technique from [31]. In the statement
below for A ∈ Rm×n and J ⊆ [m] := {1, . . . , m} the matrix AJ ∈ RJ×n denotes the |J| × n
submatrix of A defined by the rows of A indexed by J.
Proposition 2. Suppose A ∈ Rm×n has full column-rank. Then
χ(A) =

2.2

max

J ⊆[m]
AJ non-singular

kA−1
J k =

max

max kvk.

J ⊆[m]
v∈RJ
AJ non-singular kAT vk=1
J

(15)

Linear inequalities

Suppose A ∈ Rm×n . Consider the feasibility problem
Ax ≤ b.

(16)

PA (b) := {x ∈ Rn : Ax ≤ b}.

(17)

The solution of (16) is the set

Observe that PA (b) 6= ∅ if and only if b ∈ A(Rn ) + Rm
+.
2.2.1

Condition measures H(A) and H(A)

Suppose A ∈ Rm×n is a nonzero matrix. The condition measure H(A) is defined as the
following worst-case characteristic of the solution mapping PA : Rm ⇒ Rn constructed
via (17):
dist(x, PA (b))
.
(18)
H(A) = max
b∈A(Rn )+Rm
+ k(Ax − b)+ k
x∈Rn \PA (b)

Here and throughout the paper, dist(u, S) denotes the following point-to-set distance for all
u ∈ Rd and S ⊆ Rd :
dist(u, S) = inf ku − vk.
v∈S

6

The constant H(A) can be equivalently defined as the smallest constant depending only on
n
A such that the following error bound holds for all b ∈ A(Rn ) + Rm
+ and all x ∈ R :
dist(x, PA (b)) ≤ H(A) · k(Ax − b)+ k.
Again, it is not immediately evident that H(A) is finite. This fact was shown by Hoffman
in his seminal paper [12]. Other proofs of this fundamental result can be found in [9, 26, 34].
After Hoffman’s initial work, the literature in error bounds has developed extensively [17,
18, 21–23, 37]. Error bounds play a key role in optimization and variational analysis. In
particular, error bounds are widely used to established the convergence rate of a variety of
algorithms [2, 8, 10, 15, 16, 18, 20, 23, 24, 34].
Consider the following reformulation of (16) in the subspace A(Rn ):
y ≤ b, y ∈ A(Rn ).

(19)

The solution of (19) is the set
n
(b − Rm
+ ) ∩ A(R ) = APA (b).

Define H(A) as the following worst-case characteristic of the solution mapping APA : Rm ⇒
A(Rn ):
dist(y, APA (b))
.
(20)
H(A) =
max
m
n
b∈A(R )+R+
k(y − b)+ k
y∈A(Rn )\APA (b)

The constant H(A) can be equivalently defined as the smallest constant depending only on
the subspace A(Rn ) such that the following error bound holds for all b ∈ A(Rn ) + Rm
+ and
v ∈ A(Rn ) + b
m
dist(v, (A(Rn ) + b) ∩ Rm
+ ) ≤ H(A) · dist(v, R+ ).
2.2.2

Properties of H(A) and H(A)

By construction, H(A) depends only on A(Rn ) and thus is invariant under multiplication by
non-singular matrices, i.e.,
H(A) = H(AR), for all non-singular R ∈ Rn×n .

(21)

On the other hand, H(A) is not invariant under multiplication by non-singular matrices.
Proposition 3 shows that H(A) is the closure of H(A) under this kind of invariance.
Proposition 3. Suppose A ∈ Rm×n is a nonzero matrix. Then H(A) ≤ kAk · H(A) and
H(A) = H(A) when the nonzero columns of A are orthonormal. In particular,
H(A) =

min

R∈Rn×n
non-singular

kARk · H(AR).

(22)

Proof. This proof is similar to the proof of Proposition 1. Observe that dist(Ax, APA (b)) ≤
kAk · dist(x, PA (b)) for all x ∈ Rn because kAx − Auk ≤ kAk · kx − uk for all x, u ∈ Rn .
Hence (18) and (20) imply that
H(A) ≤ kAk · H(A).
7

(23)

We next show that H(A) = H(A) when the nonzero columns of A are orthonormal. For ease
of exposition, consider first the case when all columns of A are nonzero and orthonormal. In
this case it is easy to see that y ∈ A(Rn ) if and only if y = Ax for some unique x ∈ Rn with
kyk = kxk. Therefore dist(y, APA (b)) = dist(x, PA (b)) for all y = Ax ∈ A(Rn ). From (18)
and (20) it follows that H(A) = H(A).
Next consider the more general
 case when some columns of A are zero. Without loss of

generality assume that A = Ã 0 for some Ã ∈ Rm×k with nonzero orthonormal columns
for some k < n. Since the columns of Ã are orthonormal, we have H(Ã) = H(Ã). To
finish, it suffices to show that H(A) = H(Ã) and H(A) = H(Ã). Indeed, H(A) = H(Ã)
holds because A(Rn ) = Ã(Rk ) and APA (b) = ÃPÃ (b). On the other hand, for x ∈ Rn let
x̃ ∈ Rk denote the subvector of first k entries of x. Then Ax = Ãx̃ for all x ∈ Rn and thus
PA (b) = PÃ (b) × Rn−k . Hence
H(A) =

max

b∈A(Rn )+Rm
+
x∈Rn \PA (b)

dist(x, PA (b))
=
k(Ax − b)+ k

max

b∈Ã(Rk )+Rm
+
x̃∈Rn \PA (b)

dist(x̃, PÃ (b))
= H(Ã).
k(Ãx̃ − b)+ k

Finally from (21) and (23) it follows that H(A) = H(AR) ≤ kARk·H(AR) for all R ∈ Rm×m
non-singular. Thus (22) follows.
We will also rely on the following two properties of H(A). First, in the special case when
n
m
m
A(Rn ) ∩ Rm
++ 6= ∅ or equivalently A(R ) + R+ = R we have [26, Corollary 1]
kvk.
H(A) = max
m

(24)

v∈R+

kAT vk=1

Second, for general A ∈ Rm×n we have the following related characterization of H(A) discussed in [26] but that can be traced back to [14, 34, 36].
Proposition 4. Suppose A ∈ Rm×n is full column-rank. Then
H(A) =

max

max kvk =

J ⊆[m]
v∈RJ
+
AJ (Rn )∩RJ
++ 6=∅ kAT vk=1
J

max

J ⊆[m]
AJ non-singular

max kvk.
v∈RJ
+

(25)

kAT vk=1
J

Observe both the similarity and subtle difference between the right-most expressions in
the characterization (15) of χ(A) in Proposition 2 and the characterization (25) of H(A) in
Proposition 4: the first maximum is taken over the same collection of sets J in both (15)
and (25) whereas the second maximum is taken over v ∈ RJ in (15) and over v ∈ RJ+ in (25).

3

Proof of Theorem 1

We will prove the following stronger version of Theorem 1.
Theorem 2. Let A ∈ Rm×n be a full column-rank matrix. Then
χ(A) = max H(SA) = H(A),
S∈S

8

(26)

where A ∈ R2m×n is the column-wise concatenation of A and −A, that is,


A
A=
.
−A

(27)

Furthermore, if all rows of A are nonzero then (26) can be sharpened to
χ(A) =

max

S∈S
SA(Rn )∩Rm 6=∅
++

H(SA).

(28)

Proof. From (15) in Proposition 2 and (25) in Proposition 4 it immediately follows that
H(A) ≤ χ(A). Thus the sign invariance of χ(A) readily yields
χ(A) = max χ(SA) ≥ max H(SA).
S∈S

S∈S

To prove the reverse inequality we rely on (15) and (25) again. Suppose Jˆ ⊆ [m] is such
that AJˆ is non-singular and
χ(A) = kA−1
k = max kvk.
Jˆ
ˆ
v∈RJ
kAT vk=1
Jˆ

ˆ

Thus χ(A) = kv̂k for some v̂ ∈ RJ such that kATJˆv̂k = 1. Choose Ŝ ∈ S such that
ˆ
Ŝii = sign(vi ) for each i ∈ Jˆ and let u := ŜJˆv̂ ∈ RJ+ . Observe that (ŜA)J = ŜJˆAJˆ is
nonsingular and
k(ŜA)TJˆuk = kATJˆŜJˆuk = kATJˆvk = 1.
Therefore

max H(SA) ≥ H(ŜA) ≥
S∈S

max

ˆ
w∈RJ
+
T
k(ŜA) wk=1
Jˆ

kwk ≥ kuk = kv̂k = χ(A).

Thus the first identity in (26) is established. Next, Proposition 2 and Proposition 4 imply
that for all S ∈ S
χ(A) = χ(A) ≥ H(A) ≥ H(SA).

The second inequality follows because all rows of SA are rows of A as well. Hence by taking
the maximum over S ∈ S and applying the first identity in (26), we obtain the second
identity in (26).
When all rows of A are non-zero, it follows that Aṽ has all nonzero entries for an arbitrarily small perturbation ṽ of v̂. Therefore the matrix Ŝ ∈ S above can be chosen so that
ˆ
both ŜJˆv̂ ∈ RJ+ and ŜAT ṽ ∈ Rm
++ . Thus the sharper identity (28) follows.

Corollary 1. Let A ∈ Rm×n be a full column-rank matrix. Then
χ(A) = max H(SA) = H(A),
S∈S

where A is as in (27). Furthermore, if all rows of A are nonzero then
χ(A) =

max

S∈S
SA(Rn )∩Rm
++ 6=∅

9

H(SA).

Proof. This is an immediate consequence of Theorem 2, Proposition 1 and Proposition 3.
We note that when A ∈ Rm×n is full column-rank but some rows of A ∈ Rm×n are zero,
then the following amended version of (28) holds for the submatrix Ã ∈ Rℓ×n obtained after
deleting the zero rows from A:
χ(Ã) =

max

S∈S
S Ã(Rn )∩Rℓ
++ 6=∅

H(S Ã).

The construction of χ(A) and H(A) enables us to rewrite the latter identity as follows
χ(A) =

4

max

S∈S
S Ã(Rn )∩Rℓ
++ 6=∅

H(SA).

Renegar’s and Grassmannian condition numbers

Suppose A ∈ Rm×n is such that A(Rn ) ∩ Rm
++ 6= ∅. This property can be equivalently stated
n
m
m
as A(R ) + R+ = R , that is, for all b ∈ Rm the system of linear inequalities
Ax ≤ b
is feasible. In his seminal paper on condition measures for optimization [27], Renegar defined
the distance to infeasibility of A as the smallest perturbation that can be made on A so that
this property is lost. That is
R(A) := inf{k∆Ak : (A + ∆A)(Rn ) ∩ Rm
++ = ∅}.
Renegar also defined kAk/R(A) as a condition number of A.
We have the following characterization of χ(A) in terms R(A) analogous to that in
Theorem 2.
Proposition 5. Let A ∈ Rm×n be a full column-rank matrix. If A(Rn ) ∩ Rm
++ 6= ∅ then
H(A) = 1/R(A). Consequently, if all rows of full column-rank matrix A ∈ Rm×n are
nonzero then
1
.
(29)
χ(A) =
max
S∈S
R(SA)
SA(Rn )∩Rm 6=∅
++

Proof. When A(Rn )∩Rm
++ 6= ∅, the distance to ill-posedness R(A) has the following property
similar in spirit to Proposition 2 and Proposition 4 (see [28, Theorem 3.5]):
1
kAT vk.
= max
v∈Rm
R(A)
+

(30)

kvk=1

From (24) and (30) it follows that H(A) = 1/R(A) when A(Rn ) ∩ Rm
++ 6= ∅. The latter
condition and (28) in turn imply (29) if all rows of A are nonzero.

10

Ameluxen and Burgisser [1] proposed a condition number via the Grassmannian manifold
of linear subspaces of Rm of some fixed dimension. This condition number can be seen as a
variant of Renegar’s condition measure that depends only on A(Rn ) akin to the variants χ(A)
and H(A) of χ(A) and H(A) respectively. We next recall the description of the Grassmannian
condition number proposed by Ameluxen and Burgisser [1]. First, define the Grassmannian
distance dist(L, L′ ) between two linear subspaces L, L′ ⊆ Rm of the same dimension as
dist(L, L′ ) := kΠL − ΠL′ k,
where ΠL and ΠL′ denote the orthogonal projection matrices onto L and L′ respectively.
n
Suppose A ∈ Rm×n satisfies A(Rn ) ∩ Rm
++ 6= ∅. Let L := A(R ) and define the Grassmannian condition number of A as follows
G(A) :=

min{dist(L, L′ )

:

dim(L′ )

1
.
= dim(L) and L ∩ Rm
++ = ∅}

Since G(A) depends only on A(Rn ), it automatically satisfies the following invariance
property just as χ(A) and H(A) do: For all non-singular R ∈ Rm×m
G(AR) = G(A).

(31)

The pair of quantities 1/R(A), G(A) are related to each other in the same way the pairs of
quantities χ(A), χ(A) and H(A), H(A) are. More precisely, we have the following analogue
of Proposition 1 and Proposition 3.
Proposition 6. Suppose A ∈ Rm×n is a nonzero matrix and A(Rn ) ∩ Rm
++ 6= ∅. Then
G(A) ≤ kAk/R(A) and G(A) = 1/R(A) when the non-zero columns of A are orthonormal.
Consequently, if A ∈ Rm×n is a nonzero matrix
G(A) =

min
m×m

R∈R
non-singular

kARk
.
R(AR)

(32)

Proof. Suppose A ∈ Rm×n and A(Rn ) ∩ Rm
++ 6= ∅. Then the inequality G(A) ≤ 1/R(A)
follows from [1, Theorem 1.4] and the identity G(A) = 1/R(A) when the nonzero columns
of A are orthonormal follows from [1, Theorem 1.3]. The latter two facts and (31) in turn
imply (32) when A ∈ Rm×n is a nonzero matrix.
We conclude with the following characterization of χ(A) in terms G(A) analogous to that
in Corollary 1.
Corollary 2. Suppose A ∈ Rm×n is a full column-rank matrix and all rows of A are nonzero.
Then
G(SA).
(33)
χ(A) =
max
S∈S
SA(Rn )∩Rm
++ 6=∅

Proof. This is an immediate consequence of Proposition 1, Proposition 5, and Proposition 6.

11

References
[1] D. Amelunxen and P. Bürgisser. A coordinate-free condition number for convex programming. SIAM J. on Optim., 22(3):1029–1041, 2012.
[2] A. Beck and S. Shtern. Linearly convergent away-step conditional gradient for nonstrongly convex functions. Mathematical Programming, 164:1–27, 2017.
[3] A. Ben-Tal and M. Teboulle. A geometric property of the least squares solution of linear
equations. Linear Algebra and its Applications, 139:165–170, 1990.
[4] E. Bobrovnikova and S. Vavasis. Accurate solution of weighted least squares by iterative
methods. SIAM Journal on Matrix Analysis and Applications, 22(4):1153–1174, 2001.
[5] I. Dikin. On the speed of an iterative process. Upravlyaemye Sistemi, 12(1):54–60, 1974.
[6] A. Forsgren. On linear least-squares problems with diagonally dominant weight matrices.
SIAM Journal on Matrix Analysis and Applications, 17(4):763–788, 1996.
[7] A. Forsgren and G. Sporre. On weighted linear least-squares problems related to interior
methods for convex quadratic programming. SIAM Journal on Matrix Analysis and
Applications, 23(1):42–56, 2001.
[8] D. Garber. Fast rates for online gradient descent without strong convexity via Hoffman’s
bound. arXiv preprint arXiv:1802.04623, 2018.
[9] O. Güler, A. Hoffman, and U. Rothblum. Approximations to solutions to systems of
linear inequalities. SIAM Journal on Matrix Analysis and Applications, 16(2):688–696,
1995.
[10] D. Gutman and J. Peña. The condition number of a function relative to a set. To
Appear in Math. Program., 2020.
[11] J. Ho and L. Tunçel. Reconciliation of various complexity and condition measures for
linear programming problems and a generalization of Tardos’ theorem. In Foundations
Of Computational Mathematics, pages 93–147. World Scientific, 2002.
[12] A. Hoffman. On approximate solutions of systems of linear inequalities. Journal of
Research of the National Bureau of Standards, 49(4):263–265, 1952.
[13] P. Hough and S. Vavasis. Complete orthogonal decomposition for weighted least squares.
SIAM Journal on Matrix Analysis and Applications, 18(2):369–392, 1997.
[14] D. Klatte and G. Thiere. Error bounds for solutions of linear equations and inequalities.
Zeitschrift für Operations Research, 41(2):191–214, 1995.
[15] S. Lacoste-Julien and M. Jaggi. On the global linear convergence of Frank-Wolfe optimization variants. In Advances in Neural Information Processing Systems (NIPS),
2015.
12

[16] D. Leventhal and A. Lewis. Randomized methods for linear constraints: Convergence
rates and conditioning. Math. Oper. Res., 35:641–654, 2010.
[17] X. Luo and Z. Luo. Extension of Hoffman’s error bound to polynomial systems. SIAM
Journal on Optimization, 4(2):383–392, 1994.
[18] Z. Luo and P. Tseng. Error bounds and convergence analysis of feasible descent methods:
a general approach. Annals of Operations Research, 46(1):157–178, 1993.
[19] R. Monteiro and T. Tsuchiya. A variant of the Vavasis-Ye layered-step interior-point
algorithm for linear programming. SIAM Journal on Optimization, 13(4):1054–1079,
2003.
[20] I. Necoara, Y. Nesterov, and F. Glineur. Linear convergence of first order methods for
non-strongly convex optimization. Math. Program., 175:69–107, 2019.
[21] H. Ngai. Global error bounds for systems of convex polynomials over polyhedral constraints. SIAM Journal on Optimization, 25(1):521–539, 2015.
[22] T. Nguyen. A stroll in the jungle of error bounds. arXiv preprint arXiv:1704.06938,
2017.
[23] J. S. Pang. Error bounds in mathematical programming. Math. Program., 79:299–332,
1997.
[24] J. Peña and D. Rodrı́guez. Polytope conditioning and linear convergence of the FrankWolfe algorithm. Mathematics of Operations Research, 44(1):1–18, 2019.
[25] J. Peña and V. Roshchina. A data-independent distance to infeasibility for linear conic
systems. SIAM J. on Optim., 30(2):1049–1066, 2020.
[26] J. Peña, J. Vera, and L. Zuluaga. New characterizations of Hoffman constants for
systems of linear constraints. To Appear in Math. Program., 2020.
[27] J. Renegar. Incorporating condition measures into the complexity theory of linear
programming. SIAM J. on Optim., 5:506–524, 1995.
[28] J. Renegar. Linear programming, complexity theory and elementary functional analysis.
Math. Program., 70:279–351, 1995.
[29] G. Stewart. On scaled projections and pseudoinverses. Linear Algebra and its Applications, 112:189–193, 1989.
[30] M. Todd. A Dantzig-Wolfe-like variant of Karmarkar’s interior-point linear programming algorithm. Operations Research, 38(6):1006–1018, 1990.
[31] M. Todd, L. Tunçel, and Y. Ye. Characterizations, bounds, and probabilistic analysis of
two complexity measures for linear programming problems. Mathematical Programming,
90(1):59–69, 2001.
13

[32] L. Tunçel. On the condition numbers for polyhedra in Karmarkar’s form. Operations
Research Letters, 24(4):149–155, 1999.
[33] S. Vavasis and Y. Ye. A primal-dual interior point method whose running time depends
only on the constraint matrix. Mathematical Programming, 74(1):79–120, 1996.
[34] P. Wang and C. Lin. Iteration complexity of feasible descent methods for convex optimization. Journal of Machine Learning Research, 15(1):1523–1548, 2014.
[35] Y. Ye. Toward probabilistic analysis of interior-point algorithms for linear programming.
Math. of Oper. Res., 19:38–52, 1994.
[36] S. Zhang. Global error bounds for convex conic problems. SIAM Journal on Optimization, 10(3):836–851, 2000.
[37] Z. Zhou and A. So. A unified approach to error bounds for structured convex optimization problems. Mathematical Programming, 165(2):689–728, 2017.

14

