Collective Learning

arXiv:1912.02580v2 [cs.LG] 26 May 2021

Francesco Farina
franc.farina@unibo.it
Department of Electrical, Electronic and Information Engineering
Alma Mater Studiorum - Università di Bologna
Bologna, Italy

Abstract
In this paper, we introduce the concept of collective learning (CL) which exploits
the notion of collective intelligence in the field of distributed semi-supervised learning.
The proposed framework draws inspiration from the learning behavior of human beings,
who alternate phases involving collaboration, confrontation and exchange of views with
other consisting of studying and learning on their own. On this regard, CL comprises
two main phases: a self-training phase in which learning is performed on local private
(labeled) data only and a collective training phase in which proxy-labels are assigned
to shared (unlabeled) data by means of a consensus-based algorithm. In the considered
framework, heterogeneous systems can be connected over the same network, each with
different computational capabilities and resources and everyone in the network may take
advantage of the cooperation and will eventually reach higher performance with respect
to those it can reach on its own. An extensive experimental campaign on an image
classification problem emphasizes the properties of CL by analyzing the performance
achieved by the cooperating agents.

1

Introduction

The notion of collective intelligence has been firstly introduced in [Engelbart, 1962] and
widespread in the sociological field by Pierre Lévy in [Lévy and Bononno, 1997]. By borrowing the words of Lévy, collective intelligence “is a form of universally distributed intelligence,
constantly enhanced, coordinated in real time, and resulting in the effective mobilization of
skills”. Moreover, “the basis and goal of collective intelligence is mutual recognition and
enrichment of individuals rather than the cult of fetishized or hypostatized communities”.
In this paper, we aim to exploit some concepts borrowed from the notion of collective
intelligence in a distributed machine learning scenario. In fact, by cooperating with each
other, machines may exhibit performance higher than those they can obtain by learning on
their own. We call this framework collective learning (CL).
Distributed systems1 have received a steadily growing attention in the last years and
1 When talking about distributed systems, the word distributed can be used with different meanings.
Here, we refer to those networks composed by peer agents, without any central coordinator.

1

they still deserve a great consideration. In fact, nowadays the organization of computational power and data naturally calls for the distributed systems. It is extremely common to
have heterogeneous computing units connected together in possibly time-varying networks.
Both computational power and data can be shared or kept private. Moreover, there can
be additional globally available resources (such as cloud -stored data). Last but not least,
preserving the privacy of the local data of the networked systems must be a key requirement.
This network structure requires the design of tailored distributed algorithms that may let
agents benefit from the communication capabilities at disposal, exploit the available computational power and take advantage both of shared and private resources without affecting
privacy preservation.
The learning framework we want to address in CL is the one of semi-supervised learning.
In particular, we consider problems in which private data at each node are labeled, while
shared (and cloud) data are unlabeled. This captures a key challenge in today’s learning
problems. In fact, while unlabeled data can be easy to retrieve, labeled data are often
expensive to obtain (both in terms of time and money) and can be unshareable (due, e.g.,
to privacy restrictions or trade secrets). Thus, one typically has few local labeled samples
and a huge number of (globally available) unlabeled ones. Hybrid problems in which also
shared labeled and private unlabeled data are available can be easily included in the proposed
framework.
In order to perform CL in the above set-up, we propose an algorithmic idea that is now
briefly described. First of all, in order to take advantage of the possible peculiarities and
heterogeneity of all the agents in the network, each agent can use a custom architecture for
its learning function. The algorithm starts with an initial preliminary phase, called selftraining, in which agents independently train their local learning functions on their private
labeled data. Then, the algorithm proceeds with the collective training phase, by iterating
through the shared (unlabeled) set. For each unlabeled data, each agent makes a prediction
of the corresponding label. Then, by using a weighted average of the predictions of its
neighbors (as in consensus-based algorithms), it produces a local proxy-label for the current
data and uses such a label to train the local learning function. Weights for the predictions
coming from the neighbors are assigned by evaluating the performance on local validation
sets. During the collective training phase, the local labeled dataset are reviewed from time
to time in order to give more importance to the local correctly labeled data.
We want to emphasize right now that addressing the theoretical properties of the proposed algorithm is beyond the scope of this paper and will be subject to future investigation.
Rather, in this work, we present the CL framework for distributed semi-supervised learning, and we provide some experimental results in order to emphasize the features and the
potential of the proposed algorithm.
The paper is organized as follows. The relevant literature for CL is reported in the next
section. Then, the problem set-up is formalized and the proposed CL algorithm is presented
in details. Finally, an extensive numerical analysis is performed on an image classification
problem to evaluate the performance of CL.

2

Related work

The literature related to this paper can be divided in two main groups: works addressing
distributed systems and those involving widely known machine learning techniques that are
2

strictly related to CL.
A vast literature has been produced for dealing with distributed systems in different fields,
including computer science, control, estimation, cooperative robotics and learning. Many
problems arising in these fields can be cast as optimization problems and need to be solved in
a distributed fashion via tailored algorithms. Many of them are based on consensus protocols,
which allows to reach agreement in multi-agent systems [Olfati-Saber et al., 2007] and have
been widely studied under various network structures and communication protocols [Bullo
et al., 2009, Kar and Moura, 2009, Garin and Schenato, 2010, Liu et al., 2011, Kia et al.,
2015]. On the optimization side, depending on the nature of the optimization problem to
be solved, various distributed algorithms have been developed. Convex problems have been
studied within a very large number of frameworks [Boyd et al., 2006, Nedic and Ozdaglar,
2009, Zhu and Martı́nez, 2012, Ram et al., 2010, Nedic et al., 2010, Wei and Ozdaglar, 2012,
Farina and Notarstefano, 2020], while nonconvex problems have been originally addressed
via the distributed stochastic gradient descent [Tsitsiklis et al., 1986] and have received
recent attention in [Bianchi and Jakubowicz, 2013, Di Lorenzo and Scutari, 2016, Tatarenko
and Touri, 2017, Notarnicola et al., 2018, Farina et al., 2019a]. In this paper, we consider a
different set-up with respect to the one usually found in the above distributed optimization
algorithms. In fact, each agent has its own learning function, and hence a local optimization
variable that is not related with the ones of other agents. Thus, there is no explicit coupling
in the optimization problem. As it will be shown in the next sections, the collective training
phase of CL is heavily based on consensus algorithms, but agreement is sought on data and
not on decision variables. Other relevant algorithms and frameworks specifically designed for
learning with non-centralized systems include the recent works on distributed learning from
constraints [Farina et al., 2019b], federated learning [Konečnỳ et al., 2015, McMahan et al.,
2016, Konečnỳ et al., 2016, Smith et al., 2017] and many other frameworks [Dean et al.,
2012, Low et al., 2012, Kraska et al., 2013, Li et al., 2014, Chen et al., 2015, Meng et al.,
2016, Chen et al., 2016]. Except [Farina et al., 2019b] and some papers on federated learning,
most of these works, however, look for data/model distribution and parallel computation.
They usually do not deal with fully distributed systems, because a central server is required
to collect and compute the required parameters.
Machine learning techniques related to CL are, mainly, those involving proxy labeling
operations on unsupervised data (in semi-supervised learning scenarios). In fact, there exist
many techniques in which fictitious labels are associated to unsupervised data, based on the
output of one (or more) models that have been previously trained on supervised data only.
Co-training [Blum and Mitchell, 1998, Nigam and Ghani, 2000, Chen et al., 2011] exploit two
(or more) views of the data, i.e., different feature sets representing the same data in order to
let models produce labeled data for each other. Similarly, in democratic co-learning [Zhou
and Goldman, 2004] different training algorithms on the same views are exploited, by leveraging off the fact that different learning algorithms have different inductive biases. Labels on
unsupervised data are assigned by using the voted majority. Tri-training [Zhou and Li, 2005]
is similar to democratic co-learning, but only three independently trained models are used.
In self-training [McClosky et al., 2006, Rosenberg et al., 2005] and pseudo-labeling [Wu and
Yap, 2006, Lee, 2013] a single model is first trained on supervised data, then it assigns labels
to unsupervised data and uses them for training. Moreover, strictly related to this work are
the concepts of ensemble learning [Tumer and Ghosh, 1996, Dietterich, 2000, Wang et al.,
2003, Rokach, 2010, Deng and Platt, 2014] in which an ensemble of models is used to make
3

better predictions, transfer learning [Bengio, 2012, Weiss et al., 2016] and distillation [Hinton et al., 2015] in which models are trained by using other models, and learning with ladder
networks [Rasmus et al., 2015] and noisy labels [Natarajan et al., 2013, Liu and Tao, 2016,
Han et al., 2018].
To sum up, we point out that this paper utilizes some of the above concepts both from
distributed optimization and machine learning. In particular, we exploit consensus protocols
and proxy labeling techniques in order to produce collective intelligence from networked
machines.

3

Problem setup

In this section the considered problem setup is presented. First, we describe the structure of
the network over which agents in the network communicate. Then, the addressed distributed
learning setup is described.

3.1

Communication network structure

We consider a network composed by N agents, which is modeled as a time-varying directed
graph G k = (V, E k , W k ), where V = {1, . . . , N } is the set of agents, E k ⊆ V × V is the set
k
of directed edges connecting the agents at time k and W k = [wij
] is the weighted adjacency
k
matrix associated to E . the elements of which satisfy
k
(i) wii
> 0 for all i ∈ V,
k
(ii) wij
≥ 0 if and only if (j, i) ∈ E k ,

(iii)

PN

j=1

k
wij
= 1 (i.e., W k is row stochastic),

for all k = 0, 1, . . . . We assume the time-varying graph G k is jointly strongly connected, i.e.,
there exists K > 0 such that the graph G k ∪ G k+1 ∪ · · · ∪ G k+K is strongly connected for all k
k
(see Figure 1 for a graphical representation). We denote by Ni,in
the set of in-neighbors of
k
node i at iteration k (including node i itself), i.e., Ni = {j | (j, i) ∈ E k } ∪ {i}. Similarly we
k
define the set of out-neighbors of node i at time k as Ni,out
. The joint-strong connectivity
of the graph sequence is a typical assumption and it is needed to guarantee the spread of
information among all the agents.

Gk

G k+1

G k+2

Figure 1: Graphical representation of a time varying graph for which the joint graph G k ∪
G k+1 ∪ G k+2 is strongly connected.

4

3.2

Learning setup

We consider a semi-supervised learning scenario. Each agent i is equipped with a set of Mi
r
d
i
private labeled data points Di = {(xri , yir )}M
r=1 , where xi ∈ R is the r-th data of node i
r
(with d being the dimension of the input space), and yi the corresponding label. The set
Di is divided in a training set and a validation set. The training set consists of the first
i
mi < Mi samples and is defined as Di,T = {(xri , yir )}m
r=1 , while the validation set is defined
M
i
as Di,V = {(xri , yir )}r=m
. Besides, all agents have access to a shared dataset consisting of
i +1
r
d
s
ms unlabeled data, Ds = {xrs }m
r=1 , with xs ∈ R . The goal of each agent is to learn a certain
local function fi (θi ; x) (representing a local classifier, regressor, etc.), where we denote by
θi ∈ Rni the learnable parameters of fi and by x a generic input data. Notice that we are
not making any assumption on the local functions fi . In fact, in general, ni 6= nj for any i
and j. A graphical representation of the considered learning setup is given in Figure 2.
In the experiments, we will consider as a metric to evaluate the actual performance of
the agents the accuracy computed on a shared test set Dtest . Such a dataset is intended for
test purposes only and cannot be used to train the local classifiers.

s
Ds = {xrs }m
r=1

i
fi , θi ,
i
Di = {(xri , yir )}M
r=1

Figure 2: Graphical representation of the considered distributed semi-supervised learning
set-up.

4

Collective Learning

In this section, we present in details our algorithmic idea for CL. For the sake of exposition,
let us consider a problem in which all agents want to learn the same task through their local
functions fi . Multi-task problems can be directly addressed in the same way, at the price of
a more involved notation.
Each agent in the network is equipped with a local private learning function fi . The
structure of each fi can be arbitrary and different from one agent to the other. For example,
f1 can be a shallow neural network with d input units, f2 a deep one with many hidden

5

layers, f3 a CNN and so on. As said, the functions fi are private for each agent, and,
consequently, their learnable parameters θi should not be shared.
Collective learning consists of two main phases:
1. a preliminary phase (referred to as self-training), in which each agent trains its local
learning function by using only its private (labeled) data contained in the local training
set Di,T ;
2. a collective training phase, in which agents collaborate in order for collective intelligence to emerge.

4.1

Self-training

This first preliminary phase does not require any communication in the network since each
agent tries to learn fi from its private labeled training set Dt,T . It allows agents to perform
the successive collective training phase after exploiting their private supervised data.
Define Φi (fi (θi ; x), y) as the loss function associated by agent i to a generic datum (x, y).
Then, the optimization problem to be addressed by agent i in this phase is
X
minimize Ψi (θi ; Di,T ) =
Φi (fi (θi ; x), y).
(1)
θi
(x,y)∈Di,T

Usually, such a problem is solved (meaning that a stationary point is found) by iteratively
updating θi . Rules for updating θi usually depends on (sub)gradients of Ψi or, in stochastic
methods, on the ones computed on batches of data from Di,T .
Consider for simplicity the particular case in which a batch consisting of only one datum is used at each iteration. Most of the current state-of-art algorithms usable in this
set-up, starting from the classical SGD [Bottou, 2010] to Adagrad [Duchi et al., 2011],
Adadelta [Zeiler, 2012] and Adam [Kingma and Ba, 2014], can be implicitly written as
algorithms in which θi is updated by computing
θih+1 = Ui (θih ; xhi , yih ),

for h = 1, . . . , mi ,

(2)

where θi0 is some initial condition and Ui (θih ; xhi , yih ) denotes the implicit update rule given
the current estimate of the parameter θih and the data (xhi , yih ) ∈ Di,T chosen at iteration h.
We leave the update rule implicit, since, depending on the architecture of its own classifier,
the available computational power and other factors, each agent can choose the more appropriate way to perform a training step on the current data. As an example, in the classical
SGD, the update rule reads θih+1 = θih − αh ∇Φi (fi (θih ; xhi ), yih ) where αh is a stepsize and
∇ denotes the gradient operator.
In order to approach a stationary point of problem (1), the procedure in (2) typically
needs to be repeated multiple times, i.e., one needs to iterate over the set Di,T multiple
times. We call (2) an epoch of the training procedure. Moreover, we denote by STi,e (θ̂i ) the
value of θi obtained after a self-training phase started from θ̂i and carried out for e epochs.
We assume that the locally available data at each node are relatively few, so that the
performance that can be reached on the test set Dtest by solving (1) are intrinsically lower
than the ones that can be reached by training on a larger and more representative dataset.

6

4.2

Collective training

This is the main phase of collective learning. It resembles the typical human cooperative
behavior that is at the heart of collective intelligence. Algorithmically speaking, this phase
exploits the communication among the agents in the network and uses the shared (unlabeled)
data Ds .
4.2.1

Learning from shared data

In order to learn from shared (unlabeled) data, agents are asked to produce at each iteration
proxy-labels for each point in Ds . In general, at each iteration, a batch from the set Ds
is drawn and processed. To fix the ideas, consider the case in which,, at each iteration k,
a single sample xks is drawn from the set Ds . Each node produces a prediction zik for the
k
sample xks , by computing zik = fi (θik ; xks ), and broadcasts it to its out-neighbors j ∈ Ni,out
.
k
With the received predictions, each node i produces a proxy-label ŷs,i (which we call local
collective label ) for the data xks , by converting the weighted average of its own predictions
k
and the ones of its in-neighbors into a label. Finally, it uses ŷs,i
as the label associated to
k
k
xs to update θi . Summarizing, for all k = 1, 2, . . . , each node i draws xks from Ds and then
it computes
zik = fi (θik ; xks )


X
k
k k
ŷs,i
= lbl 
wij
zj

(3)
(4)

k
j∈Ni,in

k
θik+1 = Ui (θik ; xks , ŷs,i
)

(5)

where we denoted by lbl[·] the operator converting its argument into a label. For example,
in a binary classification problem the lbl operator could be a simple thresholding one, i.e.,
lbl[x] = 0 if x < 0.5 and lbl[x] = 1 if x ≥ 0.5.
Note that the labeling procedure adopted in this phase highly resembles the human
behavior. When unlabeled data are seen, their labels are guessed by resorting to the opinion
of neighboring agents.
4.2.2

Weights computation

k
Let us now elaborate on the choice of the weights wij
. Clearly, they must account for the
expertise and quality of prediction of each agent with respect to the others. In particular, we
use as performance index the accuracy computed on the local validation sets Di,V . Let us
call aki the accuracy obtained at iteration t by agent i, and, in order to possibly accentuate
the differences between the nodes, let us define

ãki = exp(γaki )
with γ ≥ 0. Then, the weights of the weighted adjacency matrix W k are computed as
( ãk
j
k
, if j ∈ Ni,in
,
k
wij
= dki
0,
otherwise,
7

(6)

(7)

P
PN
k
= 1 and weights are
where dki = m∈N k ãkm . By doing so, we guarantee that j=1 wij
i,in
assigned proportionally to the performance of each neighboring agent. Moreover, agents
are capable to locally compute the weights to assign to their neighbors, since only locally
available information is required.
The value of ãki may not be computed at every iteration k. In fact, it is very unlikely
that it changes too much from one iteration to another. Thus, we let agents update their
local performance indexes every Ti,E > 0 iterations. In the iterations in which the scores
are not updated, they are assumed to be the same as in the previous iteration.
Notice that one can think to different rules for the computation of the weights in the
adjacency matrix. For example one can use the F1 score or some other metric in place of
the accuracy or assign weights with a different criterion. As a guideline, however, we point
out that the weights should always depend on performance of the agents on some (possibly
common) task. Moreover, the local validation sets should be sufficiently equally informative
in order to evaluate agents on an fairly equally difficult task. For example, when available,
a common validation set could be used.
4.2.3

Review step

By taking again inspiration from the human behavior, the collective training phase also
includes a review step which is to be performed occasionally by each node (say every Ti,R > 0
iterations for each i). Similarly to humans that occasionally review what they have already
learned from reliable sources (e.g., books, articles,. . . ), agents in the network will review
the data in the local set Di,T (which are correctly labeled). Formally, every Ti,R iterations,
node i performs a training epoch on the local data set Di , i.e., it modifies step (5) as
k
θ̂ik+1 = Ui (θik ; xks , ŷs,i
)

(8)

θik+1

(9)

=

STi,1 (θ̂ik+1 ).

As it will be shown next, the frequency of the review step plays a crucial role in the learning
procedure. A too high frequency tends to produce a sort of overfitting behavior, while too
low one makes agent forget their reliable data.

4.3

Remarks

Before proceeding with the experimental results, a couple of remarks should be done. The
framework presented so far is quite general and can be easily implemented over networks
consisting of various heterogeneous systems. In fact, each agent is allowed to use a custom
structure for the local function fi . This accounts for, e.g., different systems with different
computational capabilities. More powerful units can use more complex models, while those
with lower potential will use simpler ones. Clearly, there will be units that will intrinsically
perform better with respect to the others, but, at the same time agents starting with low
performance (e.g., due to low representative local labeled datasets) will eventually reach
higher performance by collaborating with more accurate units. Finally, CL is intrinsically
privacy-preserving since each agent shares with its neighbors only predictions on shared
data. Thus, it is not possible to infer anything about the internal architecture or private
data of each node, since they are never exposed.

8

5

Experimental results

Consider an image classification problem in which each agent has a certain number of private
labeled images and a huge amount of unlabeled ones is available from some common source
(for example the internet). In this setup, we select the Fashion-MNIST dataset [Xiao et al.,
2017] to perform an extensive numerical analysis, and CL is implemented in Python by
combining TensorFlow [Abadi et al., 2016] with the distributed optimization features provided by DISROPT [Farina et al., 2020]. The Fashion-MNIST dataset, consists of 70, 000
28x28 greyscale images of clothes. Each image is associated with a label from 1 to 10, which
corresponds to the type of clothes depicted in the image. The dataset is divided in a training
set D with 60, 000 samples and a test set Dtest with 10, 000 samples.
Next, we first consider a simple communication network and perform a Montecarlo analysis to show the influence of some of the algorithmic and problem-dependent parameters
involved in CL. Then, we compare CL with other non-distributed methods and, finally, an
example with a bigger and time-varying network is provided. The accuracy computed on
Dtest is picked as performance metric and the samples in D are used to build the local sets
Di and the shared set Ds in CL.

5.1

Montecarlo analysis

Consider a simple scenario in which 4 agents cooperates over a fixed network (represented
as a complete graph, depicted in Figure 3) to learn to correctly classify clothes’ images. To
mimic heterogeneous agents, the local learning functions fi of the 4 agents are as follows.
1. f1 is represented as convolutional neural networks (CNN) consisting of (i) a convolutional layer with 32 filters, kernel size of 3x3 and ReLU activation combined with a
maxpool layer with pool size of 2x2; (ii) a convolutional layer with 64 filters, kernel size
of 3x3 and ReLU activation combined with a maxpool layer with pool size of 2x2; (iii)
a convolutional layer with 32 filters, kernel size of 3x3, ReLU activation and flattened
output; (iv) a dense layer with 64 units and ReLU activation; (v) an output layer with
10 output units and softmax activation.
2. f2 is represented as a neural network with 2 hidden layers (HL2) consisting of 500
and 300 units respectively, with ReLU activation, and an output layer with 10 output
units and softmax activation.
3. f3 is represented as a neural network with 1 hidden layer (HL1) of 300 units, with
ReLU activation, and an output layer with 10 output units and softmax activation.
4. f4 is a shallow network (SHL) with 10 output units with softmax activation.
Next, the role of the algorithmic and problem-dependent parameters involved in CL is
studied. In particular, we study the performance of the algorithm in terms of the accuracy
on the test set by varying: (i) the size of the local training sets, (ii) the review step frequency,
and (iii) the parameter γ in the weights’ computation. In all the next simulations, we use
|Di,V | = 100 with the samples composing such set randomly picked at each run from the
set D. Moreover, we use the Adam update rule in (2) and (5), and a batch size equal to 10
both in the self-training and in the collective training phases.

9

3
2
4
1
Figure 3: Communication network.
5.1.1

Influence of the local training set size

The number of private labeled samples locally available at each agent is clearly expected to
play a crucial role in the performance achieved by each agent. In order to show this, we
consider |Di,T | ∈ {100, 300, 500, 2000}. For each value we perform a Montecarlo simulation
consisting of 20 runs. In each run, we randomly pick the samples in each Di,T from D (along
with those in each Di,V ). The remaining samples of D are then unlabeled and put in the set
Ds . Then, the algorithm is ran for 3 epochs over Ds with the weights computation and the
review step performed every Ti,E = 100 and Ti,R = 300 iterations respectively and γ = 100
in (6). The results are depicted in Figure 4, and two things stand out. First, a higher
number of private labeled samples leads to an higher accuracy on the test set Dtest . Second,
as the number of local samples increases, the variance of the performance tends to decrease.
Moreover, it can be seen that all the network architectures reach almost the same accuracy
for |Di,T | ∈ {100, 300}, while for |Di,T | ∈ {1000, 2000}, the shallow network is overcome by
the other three.

Agent 1
CNN

Accuracy
over Dtest

1

|Di,T | =100

|Di,T | =300

|Di,T | =1000

|Di,T | =2000

0 0.5 1 1.5 2 2.5 3

0 0.5 1 1.5 2 2.5 3

0 0.5 1 1.5 2 2.5 3

0 0.5 1 1.5 2 2.5 3

Epochs over Ds

Epochs over Ds

Epochs over Ds

Epochs over Ds

0.9
0.8
0.7

Agent 2
HL2

Accuracy
over Dtest

0.6
1
0.9
0.8
0.7

Agent 3
HL1

Accuracy
over Dtest

0.6
1
0.9
0.8
0.7

Agent 4
SHL

Accuracy
over Dtest

0.6
1
0.9
0.8
0.7
0.6

Figure 4: Influence of the local training set size: evolution of the accuracy over Dtest along
the algorithm evolution for the 4 agents, for Di,T ∈ {100, 300, 1000, 2000}. The solid line
represents the average accuracy over 20 simulations, while the shaded region denotes the
average +/− two times the the standard deviation.
10

5.1.2

Influence of the review step frequency

Also the frequency with which the review step is performed (which is inversely proportional
to the magnitude of Ti,R ) influences the performance of the agents. In fact, as humans need to
review things from time to time (but not to rarely), also here a too high value for Ti,R leads to
a performance decay. We perform a Montecarlo simulation for Ti,R ∈ {50, 200, 2000, 5000}.
For each value we run 20 instances of the algorithm in which we build each set Di,T with
300 random samples from D (along with those in each Di,V ). The remaining samples of D
are then unlabeled and put in the set Ds . Then, the algorithm is ran for 3 epochs over Ds
with |Di,T | = 1000 for all i and the weights computation performed every Ti,E = 100 with
γ = 100 in (6). The results are reported in Figure 5. A higher time interval between two
review steps, produce a higher variance and also leads to a lower accuracy. This is due to the
fact that if the review step is performed too rarely, agents tends to forget their knowledge
on labeled data and start to learn from wrongly labeled samples. Then, when the review
occurs they seems to increase again their accuracy. On the contrary, a too high frequency
of review step produces a slightly overfitting behavior over the private labeled data. This
can be appreciated by comparing in Figure 5 the cases for Ti,R = 50 and Ti,R = 200. It can
be seen that the accuracy on the test set for Ti,R = 200 is higher with respect to the one for
Ti,R = 50. A more overfitting behavior can be seen by further reducing Ti,R .
Ti,R =50

Ti,R =200

Ti,R =2000

Ti,R =5000

0 0.5 1 1.5 2 2.5 3

0 0.5 1 1.5 2 2.5 3

0 0.5 1 1.5 2 2.5 3

0 0.5 1 1.5 2 2.5 3

Epochs over Ds

Epochs over Ds

Epochs over Ds

Epochs over Ds

Agent 1
CNN

Accuracy
over Dtest

1
0.9
0.8
0.7

Agent 2
HL2

Accuracy
over Dtest

0.6
1
0.9
0.8
0.7

Agent 3
HL1

Accuracy
over Dtest

0.6
1
0.9
0.8
0.7

Agent 4
SHL

Accuracy
over Dtest

0.6
1
0.9
0.8
0.7
0.6

Figure 5: Influence of the review step frequency: evolution of the accuracy computed over
Dtest along the collective training phase for the 4 agents, for Ti,R ∈ {50, 200, 2000, 5000}.
The solid line represents the average accuracy over 20 simulations, while the shaded region
denotes the average +/− two times the the standard deviation.

5.1.3

Influence of γ

The last parameter we study is γ in (6). A small value of γ means that small differences
in the local performances results in small differences in the weights. On the contrary, a
11

Agent 3
HL1

Accuracy
over Dtest

Agent 4
SHL

Accuracy
over Dtest

Agent 2
HL2

Accuracy
over Dtest

Agent 1
CNN

Accuracy
over Dtest

high value produce a weight near to 1 for the best agent in the neighborhood. In the
extreme case when γ = 0 all agents have in the neighborhood are assigned the same weight,
independently of their performance. In Figure 6 the results for a Montecarlo simulation for
γ ∈ {0, 1, 10, 100, 1000} are reported, where we use |Di,T | = 500, Ti,R = 500 and Ti,E = 100.
In this setup, the best accuracy is obtained with γ ∈ {1, 10, 100} with a slightly higher
standard deviation for γ = 100. When γ = 0 (i.e., when employing a uniform weighing), the
accuracies tend to reach a satisfactory value and, then, start to decrease. This is probably
due to the fact that all agents has the same importance and hence, in this case, all of them
seems to obtain the performance of the worst of them. Finally, when γ = 1000 there is a
substantial performance degradation. This should be caused by the fact that, in the first
iterations, there is an agent which is slightly better than the others and leads all the others
towards its (wrong) solution.
It is worth mentioning that the influence of γ on the performance may vary, depending
on the considered setup. For example, if we consider Di,T = 300 for all i, the results are
depicted in Figure 7. A choice of γ in the range [1, 100] seems to still work well, while for
γ = 1000 a steady state is reached. Moreover, for bigger generic communication graphs a
choice of γ too small may not work at all, due to, e.g., having a lot of intrinsically lowperformant neighbors whose weight in the creation of the proxy label tends to produce a lot
of wrong labels.
1
0.9
0.8
0.7
0.6
0.5
1
0.9
0.8
0.7
0.6
0.5
1
0.9
0.8
0.7
0.6
0.5
1
0.9
0.8
0.7
0.6
0.5

γ =0

γ =1

γ =10

γ =100

γ =1000

0 0.5 1 1.5 2 2.5 3

0 0.5 1 1.5 2 2.5 3

0 0.5 1 1.5 2 2.5 3

0 0.5 1 1.5 2 2.5 3

0 0.5 1 1.5 2 2.5 3

Epochs over Ds

Epochs over Ds

Epochs over Ds

Epochs over Ds

Epochs over Ds

Figure 6: Influence of γ (for |Di,T | = 500): evolution of the accuracy computed over Dtest
along the collective training phase for the 4 agents, for γ ∈ {0, 1, 10, 100, 1000}. The solid
line represents the average accuracy over 20 simulations, while the shaded region denotes
the average +/− two times the the standard deviation.

12

Accuracy
over Dtest

Agent 3
HL1

Accuracy
over Dtest

Accuracy
over Dtest

Agent 2
HL2

Agent 4
SHL

Accuracy
over Dtest

Agent 1
CNN

1
0.9
0.8
0.7
0.6
0.5
1
0.9
0.8
0.7
0.6
0.5
1
0.9
0.8
0.7
0.6
0.5
1
0.9
0.8
0.7
0.6
0.5

γ =0

γ =1

γ =10

γ =100

γ =1000

0 0.5 1 1.5 2 2.5 3

0 0.5 1 1.5 2 2.5 3

0 0.5 1 1.5 2 2.5 3

0 0.5 1 1.5 2 2.5 3

0 0.5 1 1.5 2 2.5 3

Epochs over Ds

Epochs over Ds

Epochs over Ds

Epochs over Ds

Epochs over Ds

Figure 7: Influence of γ (for |Di,T | = 300): evolution of the accuracy computed over Dtest
along the collective training phase for the 4 agents, for γ ∈ {0, 1, 10, 100, 1000}. The solid
line represents the average accuracy over 20 simulations, while the shaded region denotes
the average +/− two times the the standard deviation.

5.2

Comparison with non-cooperative methods

In this section we compare the results obtained by CL in the presented setup when |Di,T | =
500, γ = 100, Ti,E = 100 and Ti,R = 300 for all i with those obtained by using other
(non-cooperative) methods. In particular, we consider the following two approaches.
(ST) Independently train each learning function over a dataset with the same size of the
local private training dataset used in CL, i.e., with 500 samples.
(FS) Assume that the entire training dataset of Fashion-MNIST is available, and independently train each learning function over the entire dataset D.
These approaches gives two benchmarks. On one side, the performance obtained by the
four learning function with ST coincides with those that can be achieved by the agents by
performing the self-training phase only and without cooperating. On the other side, the performance obtained with FS, i.e., in a fully supervised case, represents the best performance
that can be achieved by the selected learning architectures. In order for CL to be worth for
the agents it should lead to better performance with respect to ST and approach as much
as possible those obtained by FS.
To compare CL, ST and FS we perform a Montecarlo simulation consisting of 100 runs
of each of the three approaches. In each run, the sets Di and Ds for CL have been created
randomly as in the previous sections, and CL is run for 3 epochs over Ds . Similarly, the
500 samples for training each function of ST are randomly drawn from D at each run. The
13

three approaches are compared in terms of the obtained accuracy on the test set Dtest and
the results are reported in Table 1.

Architecture/agent
CNN
HL2
HL1
SHL

CL
Mean
Std
0.8149 0.0060
0.8144 0.0051
0.8153 0.0049
0.8065 0.0050

ST
Mean
0.7663
0.7670
0.7728
0.7498

FS
Std
0.0127
0.0171
0.0112
0.0077

Mean
0.9021
0.8476
0.8465
0.8406

Std
0.0043
0.0058
0.0062
0.0028

Table 1: Comparison with non-cooperative methods: mean and standard deviation of the
accuracy on the test set for CL, ST and FS.
It can be seen that CL reaches an higher accuracy (with a lower standard deviation)
with respect to ST, thus confirming the benefits obtained through cooperation. The target
performance of FS, however are not reached. On this regard we want to point out that
the comparison with FS is a bit unfair, since the amount of usable information (in terms
of labeled samples) is extremely different. However, it can be shown that with an higher
number of samples in Di,T an accuracy near to FS can be reached. For example, from
Figure 4, it is clear that with Di,T = 2000, the learning functions HL2 and HL1 already
matches (via CL) the accuracy of FS.

5.3

Example with a larger, time-varying communication network

In this section we perform an experiment with a larger network consisting of 30 agents. Each
agent is equipped with a learning function randomly chosen from those introduced in the
previous section (CNN, HL2, HL1, SHL). In particular, there are 5 CNNs, 8 HL2s, 11 HL1s
and 6 SHLs. Agents in the network communicate across a time-varying (random) graph that
changes every 10 iterations. Each graph is generated according to an Erdős-Rènyi random
model with connectivity parameter p = 0.1 (see Figure 8 for an illustrative example). Each
agent is equipped with |Di,T | = 300 training samples randomly picked from the FashionMNIST training set. Moreover, we select Ti,R = 200, Ti,E = 100 and γ = 10. We run a
simulation in this setup for 3 epochs over the shared set Ds and the results at the end of
the simulation are reported in Figure 9 in terms of the accuracy on the test set Dtest . It can
be seen that all the agents reach an accuracy between 0.8 and 0.86. Moreover, in the last
iterations, some of them also outperform the target accuracy of FS obtained in the previous
section (for HL2, HL1 and SHL). Agents equipped with the CNN, on the other side seems
to be unable to reach the accuracy of FS in this setup.

6

Conclusions

In this paper we presented the collective learning framework to deal with semi-supervised
learning problems in a distributed set-up. The proposed algorithm allows heterogeneous
interconnected agents to cooperate for the purpose of collectively training their local learning
functions. The algorithmic idea draws inspiration from the notion of collective intelligence
and the related human behavior. The obtained experimental results show the potential of

14

CNNs

HL2s

HL1s

SHLs

Figure 8: Example with a larger, time-varying communication network: graph example.
Agents with CNN

Agents with HL1

Agents with HL2

Agents with SHL

0.88

Accuracy
on Dtest

0.86
0.84
0.82
0.8
0.78

2.8

2.85

2.9
Epochs on Ds

2.95

3

2.8

2.85

2.9
Epochs on Ds

2.95

3

2.8

2.85

2.9
Epochs on Ds

2.95

3

2.8

2.85

2.9
Epochs on Ds

2.95

3

Figure 9: Example with a larger, time-varying communication network: evolution of the
accuracy obtained by the various agents over Dtest (in the last part of the third epoch).
Each subplot groups together agents equipped with the same learning function.
the proposed scheme and call for a thorough theoretical analysis of the collective learning
framework.

References
Martı́n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean,
Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow:
A system for large-scale machine learning. In 12th {USENIX} Symposium on Operating
Systems Design and Implementation ({OSDI} 16), pages 265–283, 2016.

15

Yoshua Bengio. Deep learning of representations for unsupervised and transfer learning.
In Proceedings of ICML Workshop on Unsupervised and Transfer Learning, pages 17–36,
2012.
Pascal Bianchi and Jérémie Jakubowicz. Convergence of a multi-agent projected stochastic gradient algorithm for non-convex optimization. IEEE Transactions on Automatic
Control, 58(2):391–405, 2013.
Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training.
In Proceedings of the eleventh annual conference on Computational learning theory, pages
92–100. ACM, 1998.
Léon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings
of COMPSTAT’2010, pages 177–186. Springer, 2010.
Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized gossip
algorithms. IEEE/ACM Transactions on Networking (TON), 14(SI):2508–2530, 2006.
Francesco Bullo, Jorge Cortes, and Sonia Martinez. Distributed control of robotic networks:
a mathematical approach to motion coordination algorithms, volume 27. Princeton University Press, 2009.
Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, and Rafal Jozefowicz. Revisiting
distributed synchronous sgd. arXiv preprint arXiv:1604.00981, 2016.
Minmin Chen, Kilian Q Weinberger, and John Blitzer. Co-training for domain adaptation.
In Advances in neural information processing systems, pages 2456–2464, 2011.
Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing
Xu, Chiyuan Zhang, and Zheng Zhang. Mxnet: A flexible and efficient machine learning
library for heterogeneous distributed systems. arXiv preprint arXiv:1512.01274, 2015.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew
Senior, Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks.
In Advances in neural information processing systems, pages 1223–1231, 2012.
Li Deng and John C Platt. Ensemble deep learning for speech recognition. In Fifteenth
Annual Conference of the International Speech Communication Association, 2014.
Paolo Di Lorenzo and Gesualdo Scutari. Next: In-network nonconvex optimization. IEEE
Transactions on Signal and Information Processing over Networks, 2(2):120–136, 2016.
Thomas G Dietterich. Ensemble methods in machine learning. In International workshop
on multiple classifier systems, pages 1–15. Springer, 2000.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online
learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):
2121–2159, 2011.
Douglas C Engelbart. Augmenting human intellect: A conceptual framework. Menlo Park,
CA, 1962.
16

Francesco Farina and Giuseppe Notarstefano. Randomized block proximal methods for
distributed stochastic big-data optimization. IEEE Transactions on Automatic Control,
2020. doi: 10.1109/TAC.2020.3027647.
Francesco Farina, Andrea Garulli, Antonio Giannitrapani, and Giuseppe Notarstefano. A
distributed asynchronous method of multipliers for constrained nonconvex optimization.
Automatica, 103:243–253, 2019a.
Francesco Farina, Stefano Melacci, Andrea Garulli, and Antonio Giannitrapani. Asynchronous distributed learning from constraints. IEEE transactions on neural networks
and learning systems, 31(10):4367–4373, 2019b.
Francesco Farina, Andrea Camisa, Andrea Testa, Ivano Notarnicola, and Giuseppe Notarstefano. Disropt: a python framework for distributed optimization. IFAC-PapersOnLine, 53
(2):2666–2671, 2020.
Federica Garin and Luca Schenato. A survey on distributed estimation and control applications using linear consensus algorithms. In Networked control systems, pages 75–107.
Springer, 2010.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and
Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely
noisy labels. In Advances in Neural Information Processing Systems, pages 8527–8537,
2018.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.
arXiv preprint arXiv:1503.02531, 2015.
Soummya Kar and José MF Moura. Distributed consensus algorithms in sensor networks
with imperfect communication: Link failures and channel noise. IEEE Transactions on
Signal Processing, 57(1):355–369, 2009.
Solmaz S Kia, Jorge Cortés, and Sonia Martinez. Dynamic average consensus under limited control authority and privacy requirements. International Journal of Robust and
Nonlinear Control, 25(13):1941–1966, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980, 2014.
Jakub Konečnỳ, Brendan McMahan, and Daniel Ramage. Federated optimization: Distributed optimization beyond the datacenter. arXiv preprint arXiv:1511.03575, 2015.
Jakub Konečnỳ, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha
Suresh, and Dave Bacon. Federated learning: Strategies for improving communication
efficiency. arXiv preprint arXiv:1610.05492, 2016.
Tim Kraska, Ameet Talwalkar, John C Duchi, Rean Griffith, Michael J Franklin, and
Michael I Jordan. Mlbase: A distributed machine-learning system. In Cidr, volume 1,
pages 2–1, 2013.

17

Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method
for deep neural networks. In Workshop on Challenges in Representation Learning, ICML,
volume 3, page 2, 2013.
Pierre Lévy and Robert Bononno. Collective intelligence: Mankind’s emerging world in
cyberspace. Perseus books, 1997.
Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. Scaling distributed machine
learning with the parameter server. In 11th {USENIX} Symposium on Operating Systems
Design and Implementation ({OSDI} 14), pages 583–598, 2014.
Bo Liu, Wenlian Lu, and Tianping Chen. Consensus in networks of multiagents with switching topologies modeled as adapted stochastic processes. SIAM Journal on Control and
Optimization, 49(1):227–253, 2011.
Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting.
IEEE Transactions on pattern analysis and machine intelligence, 38(3):447–461, 2016.
Yucheng Low, Danny Bickson, Joseph Gonzalez, Carlos Guestrin, Aapo Kyrola, and
Joseph M Hellerstein. Distributed graphlab: a framework for machine learning and data
mining in the cloud. Proceedings of the VLDB Endowment, 5(8):716–727, 2012.
David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing.
In Proceedings of the main conference on human language technology conference of the
North American Chapter of the Association of Computational Linguistics, pages 152–159.
Association for Computational Linguistics, 2006.
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communicationefficient learning of deep networks from decentralized data.
arXiv preprint
arXiv:1602.05629, 2016.
Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkataraman,
Davies Liu, Jeremy Freeman, DB Tsai, Manish Amde, Sean Owen, et al. Mllib: Machine
learning in apache spark. The Journal of Machine Learning Research, 17(1):1235–1241,
2016.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisy labels. In Advances in neural information processing systems, pages 1196–
1204, 2013.
A Nedic and A Ozdaglar. Distributed subgradient methods for multi-agent optimization.
IEEE Transactions on Automatic Control, 54(1):48–61, 2009.
Angelia Nedic, Asuman Ozdaglar, and Pablo A Parrilo. Constrained consensus and optimization in multi-agent networks. IEEE Transactions on Automatic Control, 55(4):
922–938, 2010.
Kamal Nigam and Rayid Ghani. Analyzing the effectiveness and applicability of co-training.
In Cikm, volume 5, page 3, 2000.

18

Ivano Notarnicola, Ying Sun, Gesualdo Scutari, and Giuseppe Notarstefano. Distributed bigdata optimization via block-iterative gradient tracking. arXiv preprint arXiv:1808.07252,
2018.
Reza Olfati-Saber, J Alex Fax, and Richard M Murray. Consensus and cooperation in
networked multi-agent systems. Proceedings of the IEEE, 95(1):215–233, 2007.
S Sundhar Ram, Angelia Nedić, and Venugopal V Veeravalli. Distributed stochastic subgradient projection algorithms for convex optimization. Journal of optimization theory and
applications, 147(3):516–545, 2010.
Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semisupervised learning with ladder networks. In Advances in neural information processing
systems, pages 3546–3554, 2015.
Lior Rokach. Ensemble-based classifiers. Artificial Intelligence Review, 33(1-2):1–39, 2010.
Chuck Rosenberg, Martial Hebert, and Henry Schneiderman. Semi-supervised self-training
of object detection models. In 2005 Seventh IEEE Workshops on Applications of Computer
Vision (WACV/MOTION’05)-Volume 1, volume 1, pages 29–36. IEEE, 2005.
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multitask learning. In Advances in Neural Information Processing Systems, pages 4424–4434,
2017.
Tatiana Tatarenko and Behrouz Touri. Non-convex distributed optimization. IEEE Transactions on Automatic Control, 62(8):3744–3757, 2017.
John Tsitsiklis, Dimitri Bertsekas, and Michael Athans. Distributed asynchronous deterministic and stochastic gradient optimization algorithms. IEEE transactions on automatic
control, 31(9):803–812, 1986.
Kagan Tumer and Joydeep Ghosh. Error correlation and error reduction in ensemble classifiers. Connection science, 8(3-4):385–404, 1996.
Haixun Wang, Wei Fan, Philip S Yu, and Jiawei Han. Mining concept-drifting data streams
using ensemble classifiers. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 226–235. AcM, 2003.
Ermin Wei and Asuman Ozdaglar. Distributed alternating direction method of multipliers.
In 2012 IEEE 51st IEEE Conference on Decision and Control (CDC), pages 5445–5450.
IEEE, 2012.
Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A survey of transfer learning.
Journal of Big data, 3(1):9, 2016.
Kui Wu and Kim-Hui Yap. Fuzzy svm for content-based image retrieval: a pseudo-label
support vector machine framework. IEEE Computational Intelligence Magazine, 1(2):
10–16, 2006.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms, 2017.
19

Matthew D Zeiler. Adadelta:
arXiv:1212.5701, 2012.

an adaptive learning rate method.

arXiv preprint

Yan Zhou and Sally Goldman. Democratic co-learning. In 16th IEEE International Conference on Tools with Artificial Intelligence, pages 594–602. IEEE, 2004.
Zhi-Hua Zhou and Ming Li. Tri-training: Exploiting unlabeled data using three classifiers.
IEEE Transactions on Knowledge & Data Engineering, (11):1529–1541, 2005.
Minghui Zhu and Sonia Martı́nez. On distributed convex optimization under inequality and
equality constraints. IEEE Transactions on Automatic Control, 57(1):151–164, 2012.

20

