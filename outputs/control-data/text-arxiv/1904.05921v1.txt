arXiv:1904.05921v1 [q-fin.MF] 11 Apr 2019

Deep-learning based numerical BSDE method
for barrier options
Bing Yu∗, Xiaojing Xing†, Agus Sudjianto‡
April 15, 2019

Abstract
As is known, an option price is a solution to a certain partial differential
equation (PDE) with terminal conditions (payoff functions). There is
a close association between the solution of PDE and the solution of a
backward stochastic differential equation (BSDE). We can either solve
the PDE to obtain option prices or solve its associated BSDE. Recently a
deep learning technique has been applied to solve option prices using the
BSDE approach. In this approach, deep learning is used to learn some
deterministic functions, which are used in solving the BSDE with terminal
conditions. In this paper, we extend the deep-learning technique to solve
a PDE with both terminal and boundary conditions. In particular, we
will employ the technique to solve barrier options using Brownian motion
bridges.

1

Introduction

A barrier option is a type of derivative where the payoff depends on whether
the underlying asset has breached a predetermined barrier price. For a simple
barrier case, an analytical pricing formula is available (see [1]). Because barrier
options have additional conditions built in, they tend to have cheaper premiums
than comparable options without barriers. Therefore, if a trader believes the
barrier is unlikely to be reached, they may prefer to buy a knock-out barrier
option for a lower premium. There are different methods to solve option prices,
ranging from an analytical solution, solving PDE numerically, and Monte Carlo
simulations. Recently, a different approach using machine learning has been
proposed.
Using machine learning to solve PDE was studied in [2]. In this work, a new
method was proposed for solving parabolic partial differential equations with
terminal conditions, which we will call the standard framework hereafter. In this
∗ Corporate

Model Risk, Wells Fargo, bing.yu@wellsfargo.com
Model Risk, Wells Fargo
‡ Corporate Model Risk, Wells Fargo
† Corporate

1

new method, the PDE is formulated as a stochastic control problem through
a Feymann-Kac formula. In this formulation, a connection between PDE for
option prices and BSDE is made. The option price is obtained by solving the
BSDE rather than solving PDE. The solution to the BSDE is represented by
two deterministic functions. One innovation (shown in [2]) is the use of a neural
network and deep-learning technique to learn these deterministic functions. The
mathematical foundation of this approach is based on a Kolmogorov-Arnold
representation theorem. This theorem states that any continuous function can
be approximated by a finite composition of continuous functions of a single
variable. Cybenko (see[3]) found that a feed-forward neural network is a natural
realization of the theorem and he provided a concrete implementation using a
sigmoid function.
In addition to [2], [4] extended the method to solve fully non-linear PDE and
second-order backward stochastic differential equation. Other works related to
this deep-learning method include [5] and [6]. In [5], a different way of simulating
the processes in the forward-backward stochastic differential equation (FBSDE)
is proposed. Rather than using a neural network to approximate the derivative
of a PDE solution, the network is used to directly approximate the PDE solution
and the derivative is calculated using automatic differentiation. A number of
different choices for building the neural network and learning structure and
two new types of structures are proposed in [6] . These problems are in the
framework of a PDE with some terminal conditions. These PDE can be solved
by an equivalent BSDE.
In the aforementioned standard framework, the PDE solved has no boundary conditions. There are some works on PDE with free boundary conditions.
In these works, a BSDE is replaced by a reflected BSDE (RBSDE). A penalty
term is added to the loss function to take into account the free boundary condition in order to solve the RBSDE. Again, machine learning can be used in
solving these problems. This approach is used in [7] to solve American options.
Bermudan Swaptions is solved by exercising the option at a boundary in [8]. In
our work, we consider barrier options. We treat boundary conditions of barrier
options differently. Rather than using RBSDE with a penalty function or exercise options at a boundary, we incorporated the boundary conditions as terminal
conditions. To our best knowledge, this approach has not previously been done.
In this paper, we organize as follows. In section 2, we present the standard
framework, which is designed for Cauchy problem. In section 3, we describe how
we extend the standard framework to handle barrier options, which corresponds
to a Cauchy-Dirichlet problem. In section 4, we present numerical considerations
and the results we obtained from our experiments. Finally, we make some
concluding remarks in section 5.

2

2

Basic method to solve BSDE by machine learning

We briefly introduced the deep-learning-based numerical BSDE algorithm proposed in [2]. We start from an FBSDE, which is first proposed in [9].
ˆ t
ˆ t
σs (Xs )dWs
bs (Xs )ds +
Xt = X0 +
0

0

Yt = h(XT ) +

ˆ

T

fs (Xs , Ys , Zs )ds −

t

ˆ

T

Zs dWs

t

Here, {Ws }0<s<T is a Brownian motion and h(XT ) is the terminal condition.
The pair (Y, Z)0<t<T solves the BSDE. It is known that there exists a deterministic function u = u(t, x) such that Yt = u(t, Xt ), Zt = ∇u(t, Xt )σt (Xt ) and
u(t, x) solves a quasi-linear PDE. For both the forward and backward process,
we can use Euler scheme to approximate:
Xti+1 ≈ Xti + bti (Xti )(ti+1 − ti ) + σti (Xti )(Wti+1 − Wti )

(1)

Yti+1 ≈ Yti − fti (Xti , Yti , Zti )(ti+1 − ti ) + Zti (Wti+1 − Wti )

(2)

Note that we have made the backward process to be forward; this is a commonly
used technique in treating FBSDEs. This set of equations has the following
interpretation on a given path: Xti is the underlying price; Yti is the option
price and Zti is related to the delta at time ti .
In the deep-learning-based numerical BSDE algorithm, a neural network
structure is used to approximate the term Zti at each time step with parameter
θ. Starting from an underlying price X0 at time 0 and an initial guess Y0, Z0 ,
we use equations (1) and (2) to calculate Xti+1 and Yti+1 at every time step
until the terminal time T . At terminal time, the loss is given by l(θ, Y0 , Z0 ) =
E[(YT − h(XT ))2 ]. A stochastic gradient descent method is used to minimize
the loss function by iterating to the optimal value of Y0 ,Z0 and θ. Note that the
success of this idea relies on the fact that the neural network can approximate
non-linear function Zt (Xt ) well; this is guaranteed by the work of Cybenko (see
[3]).
This framework solves a PDE with Cauchy conditions (standard framework).
However, there are certain types of options that will correspond to a PDE
with Cauchy-Dirichlet conditions. For example, a barrier option is a CauchyDirichlet PDE problem. In this paper, we would consider extending the standard
framework to handle this case.

3

3

Extension of basic method to solve barrier option

Barrier options are options where the payoff depends on whether the underlying
asset’s price reaches a certain level during a certain period of time. These barrier options can be classified as either knock-out options or knock-in options. A
knock-out option ceases to exist when the underlying asset price reaches a certain barrier. A knock-in option comes into existence only when the underlying
asset price reaches a barrier. An up-and-out call is a regular call option that
ceases to exist if the asset price reaches a barrier level, B, that is higher than the
current asset price. An up-and-in call is a regular call option that comes into
existence only if the barrier is reached. The down-in and down-out option are
similarly defined. Under the Black-Scholes Framework, assuming constant coefficient, it is not hard to derive an analytical solution for these kinds of barrier
options. Therefore, we will use an analytical solution as the benchmark.
We would like to start from the most general form of a Cauchy-Dirichlet
problem. As is well known, the Feynman-Kac formula has provided a way of
translating the problem of a partial differential equation into a probabilistic
problem. A Dirichlet condition needs to be translated in a probabilistic way by
stopping the underlying diffusion process as it exits from a domain.
Theorem 1. (See [10] Chapter 4). Let W be a Brownian motion. Assume
process Xt satisfies:
ˆ t
ˆ t
σs (Xs )dWs
bs (Xs )ds +
Xt = X0 +
0

0

where b and σ satisfy some usual regularity and boundedness conditions. Let D
be a bounded domain in real space and define τ t,x = inf {s > t, Xst,x ∈
/ D} as
the first exit time from domain D by process X started from (t, x). Assume the
boundary ∂D is smooth. Assume functions r, g : [0, T ] × D̄ → R are continuous.
Then the solution u(t, x) of class C 1,2 of the following PDE

1
2

t < T, x ∈ D
 ∂t u(t, x) + b(t, x)∂x u(t, x) + 2 σ(t, x) ∂xx u(t, x) − r(t, x)u(t, x) = 0,
u(T, x) = g(T, x)
x ∈ D̄


u(t, x) = g(t, x)
(t, x) ∈ [0, T ] × ∂D
(3)
can be expressed by the following probabilistic representation
−
u(t, x) = E[g(τ t,x ∧ T, Xτt,x
t,x ∧T )e

´ τ t,x ∧T
t

r(s,Xst,x )ds

].

Remark 1. The domain D is assumed to be bounded. In fact, it is enough for
the boundary to be compact.
u(t, x) is an average over all paths start at (t, x). If the path never exits the
domain D (i.e., τ t,x ≥ T ), we use the value at terminal g(T, XTt,x). If the path
4

exits the domain (i.e., τ t,x < T ), we use the boundary value at the point the
path exits, (i.e., g(τ t,x , Xτt,x
t,x )). We can write it this way:
u(t, x)

= E[g(T, XTt,x)e−

´T
t

−
+ E[g(τ t,x , Xτt,x
t,x )e

r(s,Xst,x )ds
´ τ t,x
t

|τ t,x ≥ T ]P (τ t,x ≥ T )

r(s,Xst,x )ds

|τ t,x < T ]P (τ t,x < T ).

(4)

An up-out call barrier option is a special case of the above general case
with domain, terminal condition, and boundary condition specifically defined.
In up-out call option pricing, domain is D = {x < B}, terminal condition
is g(T, x) = (x − K)+ 1{x<B} , and boundary condition is g(t, x) = 0 when
x ≥ B. We can also assume constant drift b, volatility σ, and interest rate r
for simplicity. Then, the second term in equation (4) can be dropped since the
boundary condition is zero; the probabilistic representation then becomes:
= E[g(T, XTt,x)e−r(T −t) |τ t,x ≥ T ]P (τ t,x ≥ T )

u(t, x)

(5)

To calculate this probabilistic representation, we write it into conditional expectation of the terminal value XTt,x . By using Lemma 2 shown in the Appendix,
we have:
u(t, x)

= E{g(T, XTt,x)e−r(T −t) P (τ t,x ≥ T |XTt,x)}

(6)

The term P (τ t,x ≥ T |XTt,x) is the probability that, given the underlying value
at time t and T , the underlying process never crosses the barrier in between. We
can apply a commonly used technique in dealing with the simulation of a stopped
process - a Brownian motion bridge. Given the start point and end point of a
geometric Brownian motion, the probability that the process never exceeds a
certain level in between can be explicitly given as stated in the following Lemma.
Lemma 1. (Brownian Motion Bridge). Assume Xt follows dXt /Xt = bdt +
σdWt and define
ξ(y) = exp[−

2 ∗ ln(y/Xt) ∗ ln(y/Xt+∆t)
],
σ 2 ∆t

then
(1) If B > max(Xt , Xt+∆t ),then P [maxt<s<t+∆t Xs < B] = 1 − ξ(B)
(2)If B < min(Xt , Xt+∆t ),then P [mint<s<t+∆t Xs > B] = 1 − ξ(B)
Taking the use of Lemma 1 and plugging in the terminal condition for barrier
option, the probabilistic representation becomes:

u(t, x) =

E{[(XTt,x

+

− K) 1{X t,x <B} e

−r(T −t)

T

5

][1 − e

−

t,x
2ln(B/x)∗ln(B/X
)
T
σ2 (T −t)

]}

(7)

The only random variable in the expectation is the underlying at terminal XTt,x
and, therefore, we can view this as a vanilla option with a relatively complicated
payoff. Then, we can use the standard framework to solve this problem. The
BSDE we use in our algorithm is:
XT = x +

ˆ

T

bXs ds +
t

Yt = h(XT ) −

ˆ

T

σXs dWs

t

ˆ

T

rYs ds −

ˆ

T

Zs dWs

t

t

− 2ln(B/x)ln(B/X)

σ2 (T −t)
where h(X) = [(X − K)+ 1{X<B} ][1 − e
]. The Euler scheme and
loss function described in the standard framework (Section 2) can be applied.
For completeness, the PDE corresponding to this BSDE is:

(

∂t u(t, x, x0 ) + b∂x u(t, x, x0 ) + 21 σ 2 ∂xx u(t, x, x0 ) − ru(t, x, x0 ) = 0
u(T, x, x0 ) = h(x, x0 )

where h(x, x0 ) = [(x − K)+ 1{x<B} ][1 − e
u(t, x0 , x0 ) is the price we want.

4
4.1

−

2ln(B/x0 )ln(B/x)
σ2 (T −t)

]. Note that the value

Numerical consideration
Choosing basic neural network structure

Similar to other works mentioned before, a neural network is used to approximate the term Zti at each time step. The structure of the neural net can significantly influence the convergence. In the original work in [2], at each time step,
the parameter set θti are different (i.e., building N different neural networks for
the N time steps):
(8)
Zti = netθti (Xti ), i = 0...N.
In our work, we choose to use a type of merged neural network, as proposed
by [6]. Rather than building N neural networks, we use one neural network for
all the N time steps, so θ does not need subscript ti . We also need to add one
additional dimension to the underlying; that is, the neural network also needs to
take the time to maturity as an input for the merged neural network structure
to work:
(9)
Zti = netθ (Xti , T − ti ), i = 0...N.
Finally, we choose to use Elu as the base function and use one hidden layer
with 20 neurons. Using more hidden layers or using more neurons in a hidden
layer does not apparently improve the convergence. Learning rates are chosen
differently in each test. All weights in the network are initialized using normal
or uniform random variable without pre-training. We can achieve quite accurate

6

results by using this structure in most cases. However, for some isolated cases,
a batch normalization technique is needed to improve the convergence of the
algorithm. We discuss details on improving the convergence later.
To improve computational efficiency, we use variance discretization rather
than the commonly used time discretization. The number of time steps needed
will roughly depend on the total variance of the model; in our case, it is the
square of volatility times maturity time, σ 2 T . For cases where volatility is
small, we can take a larger time step; in cases where volatility is large, we
take a smaller time step to improve accuracy. As a rule of thumb, we use the
σ2 T
following formula to determine time step timestep# = max(80, 0.025
).
To improve efficiency further, we use different learning rates. At the beginning of the training, we use larger learning rates to search for solutions that are
close to the optimal point. When solutions become relatively stable, we decrease
the learning rate to zoom in on the optimal solution.
Finally, we used a stopping criteria to stop the training when a change of
the last 50 iteration average price is less than a certain small amount. We
also insist on a minimum iteration number that training needs to run and a
maximum iteration number where the program should stop.

4.2
4.2.1

Test result summary
Test overview

We ran the numerical experiment for a wide range of inputs for an up-out call
option. Namely, we vary the length of maturity, underlying price, volatility,
and barrier value. For simplicity, we set the interest rate and drift to both
be zero. In total, we tested 72 cases (see Table 1). For all tests, we used the
tensorflow Adam optimizer for the stochastic gradient descent method. We ran
these 72 cases in three different settings, as shown in Table 2. In the first setting,
we used brutal force of 8,000 iterations to ensure convergence. However, some
isolated results are still not converged. In the second setting, we introduced
batch normalization at every layer to improve convergence. In the last setting,
we only use batch normalization at the input layer. In the following section, we
will discuss the results in detail.

Option type
Up-out call

Table 1: Barrier Option Information
Strike Underlying Maturity Rate Drift
23
17,22,27,32
0.5, 2
0
0

7

Volatility
0.4,0.8,1.2

Barrier
40,60,100

Layers
Neurons
Base function
Initial learning rate
LR decay factor
LR decay frequency
Maximum iteration
Stopping criteria
Minimum iteration
Time step
Batch size
Batch normalization
4.2.2

Table 2: Setting of tests
Test 1 setting
Test 2 setting
Test 3 setting
3
d+20
Elu
0.01
0.02
0.02
0.5
0.5
0.5
1,500
500
1,000
8,000
1,500
3,000
No
Price change<0.002 Price change<0.005
8,000
750
1,500
max(80,Var/0.025)
512
No
At every layer
At input layer

Test result

In this section, we present the results for 72 cases tested. In test 1, we ran 8,000
iterations. As Table 3 indicates, on average the results differ from analytical
solutions by 4.5 cents and relative differences of 1.21%. In approximately 50%
of cases, the pricing difference is less than a penny, while there is a relative
error of 0.4%. Some isolated cases, shown in Table 4, have large differences. To
improve convergence, we applied batch normalization technique.
Batch normalization technique is proposed in [11]. When we have a neural
network, the change in the input distribution at each layer presents a problem
because parameters need to continuously adapt to a new distribution. This is
a phenomenon known as covariate shift. Eliminating internal covariate shift
provides faster training and batch normalization is the mechanism to do so.
Batch normalization accomplishes this via a normalization step that fixes the
means and variances of input distribution. We would like to note that, when
adding a batch normalization technique in tests to improve performance in our
test, we add additional trainable variables; these trainable parameters within
batch normalization are different among different time steps (i.e., the network
will still depend on ti ).
Zti = netθ,βti (Xti , T − ti ), i = 0...N

(10)

Here βti are the parameters introduced in batch normalization.
Test 2, shown in Table 3, are results of applying batch normalization at
every layer. Comparing the results from test 1 and those from test 2, we can
see significant improvement in both absolute differences and relative differences
when batch normalization is applied. In addition, those isolated points where
they failed to converge in test 1 now converge nicely, with less than 1% relative
differences, as shown in Table 4.
When doing batch normalization, we can choose to apply it at every layer or
we can apply it only at the input layer. Whether we apply batch normalization
8

at each layer or just the input layer, the results are similar, as shown in Table
3. However, computation efficiency is very different as we will show later.

Statistics
Average
STD
25% quantile
Median
75% quantile

Maturity
0.5
0.5
0.5
0.5
2
2
4.2.3

Table 3: Grid test result - error statistics
Test 1 rel Test 2 rel Test 3 rel Test 1 abs
1.21%
0.57%
0.56%
0.0452
2.56%
0.54%
0.53%
0.1399
0.24%
0.21%
0.13%
0.0017
0.41%
0.39%
0.48%
0.0043
1.04%
0.70%
0.76%
0.0174

Table 4: Result of isolated test cases
Underlying Volatility Barrier Test 1 rel error
17
1.2
100
18%
22
0.8
100
4.09%
27
0.8
100
6.76%
32
0.8
100
9.03%
22
0.4
100
4.02%
27
0.4
100
6.42%

Test 2 abs
0.0099
0.0116
0.0017
0.0059
0.0133

Test 2 rel error
0.65%
0.34%
0.28%
0.39%
0.18%
0.15%

Improving the efficiency

As we have mentioned before, we can apply batch normalization at every layer
or apply it only at the input layer. When we apply it at each layer, we increase
the trainable variable but less iterations are required to achieve convergence.
On the other hand, if we apply it only at the input layer, we have less trainable
parameters but more iterations are needed to achieve convergence. However, the
overall time needed to achieve convergence is less in the latter case, as shown
in Table 5. The overall running time for applying batch normalization at only
the input layer is almost three times faster.
Table 5: Efficiency Results
Indicator
Test 2 result Test 3 result
Average iteration step needed
1000
1670
Time consumed per 200 training
15-20s
4-5s
iterations
Approximate running time per
200s
75s
case (including building time)

9

Test 3 abs
0.0086
0.0093
0.0013
0.0047
0.0132

5

Conclusion

In this work, we solved a PDE with boundary conditions, using barrier options
as a concrete example. In this problem, the diffusion domain is restricted by
a barrier. By viewing the terminal condition probabilistically (i.e., including a
breaching probability of barrier), we are able to recast this problem into the standard framework, namely a PDE with terminal conditions. This PDE is solved
by its equivalent BSDE using a machine learning technique. We have completed
extensive testing using a wide range of market conditions and achieved good results when comparing with known analytical results. In some isolated cases, the
batch normalization technique is needed to improve learning.

6

Appendix

For completeness, we present a technique Lemma that was used in section 3 for
transition from equation (5) to (6).
Lemma 2. Assume X is a random variable, A is an event. Then E[f (X)|A]P (A) =
E[f (X)P (A|X)] for any function f (·).
Proof. First starting from the left side, we have E[f (X)|A]P (A) = E[1A E[f (X)|A]] =
E[E[f (X)1A |A]] = E[f (X)1A ]. Then, starting from the right side, E[f (X)P (A|X)] =
E[f (X)E[1A |X]] = E[E[f (X)1A |X]] = E[f (X)1A ]. We arrive at same quantity
from both side; thus, the statement is proved.

References
[1] John C. Hull. Options, Futures and other Derivatives, 9th Edition. 2015.
[2] Weinan E, Jiequn Han, Arnulf Jantzen. Deep learning-based numerical
methods for high-dimensional parabolic partial differential equations and
backward stochastic differential equations. Communications in Mathematics and Statistics, 2017.
[3] G. Cybenko. Approximation by superposition of a sigmoidal function.
Mathematics of Control, Signal and System, 1989.
[4] Chritian Beck, Weinan E, Arnulf Jantzen. Machine learning approximation
algorithms for high dimensional fully nonlinear partial differential equations and second order backward stochastic differential equations. Archive
1709.05963, 2017.
[5] Maziar Raissi. Forward backward stochastic neural networks: Deep learning of high-dimensional partial differential equations. Arxiv1804.07010,
2018.
[6] Quentin Chan-Wai-Nam, Joseph Mikael, Xavier Warin. Machine learning
for semi linear PDEs. arXiv:1809.07609v1, 2018.
10

[7] Masaaki Fujii, Akihiko Takahashi, Masayuki Takahashi. Asymptotic expansion as prior knowledge in deep learning method for high dimensional
BSDEs. Arxiv 1710.07030, 2017.
[8] Haojie Wang, Han Chen, Agus Sudjianto, Richard Liu, Qi Shen. Deep
learning-based BSDE solver for LIBOR market model with application to
Bermudan swaption pricing and hedging. Arxiv 1807.06622, 2018.
[9] Etienne Pardous, Shige Peng. Adapted solution of a backward stochastic
differential equation. System & Control Letters, 1990.
[10] Emmanuel Gobet. Monte Carlo Method and Stochastic Processes: from
Linear to non-Linear. Chapman and Hall/CRC; 1 edition, 2016.
[11] Sergey Ioffe, Christian Szegedy. Batch normalization: Accelerating deep
network training by reducing internal covariate shift. Proceedings of Machine Learning Research, 2015.

11

