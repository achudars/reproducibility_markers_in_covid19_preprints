Image Alignment in Unseen Domains
via Domain Deep Generalization
Thanh-Dat Truong1 , Khoa Luu2 , Chi Nhan Duong3 , Ngan Le4 , Minh-Triet Tran1

arXiv:1905.12028v2 [cs.CV] 31 May 2019

1

University of Science, VNU-HCM, Vietnam
ttdat@selab.hcmus.edu.vn, tmtriet@fit.hcmus.edu.vn
2
University of Arkansas, USA
khoaluu@uark.edu
3
Concordia University, Canada
dcnhan@ieee.org
4
Carnegie Mellon University, USA
thihoanl@andrew.cmu.edu

Abstract. Image alignment across domains has recently become one of the realistic and popular topics in the research community. In this problem, a deep
learning-based image alignment method is usually trained on an available largescale database. During the testing steps, this trained model is deployed on unseen
images collected under different camera conditions and modalities. The delivered
deep network models are unable to be updated, adapted or fine-tuned in these
scenarios. Thus, recent deep learning techniques, e.g. domain adaptation, feature transferring, and fine-tuning, are unable to be deployed. This paper presents
a novel deep learning based approach5 to tackle the problem of across unseen
modalities. The proposed network is then applied to image alignment as an illustration. The proposed approach is designed as an end-to-end deep convolutional
neural network to optimize the deep models to improve the performance. The proposed network has been evaluated in digit recognition when the model is trained
on MNIST and then tested on unseen MNIST-M dataset. Finally, the proposed
method is benchmarked in image alignment problem when training on RGB images and testing on Depth and X-Ray images.

1

Introduction

Image alignment has an important role in numerous medical applications, e.g clinical
track of events, clinical diagnostics, surgical procedure tracking, etc. Image alignment
of medical images can be considered as a solved problem in the context of both training and testing images are collected under the same conditions. However, in numerous
medical applications, due to the privacy, the deployment context or the limitations of
the image collecting process, testing images are unavailable during the training steps.
Indeed, there is a need to develop a class of image alignment methods that only requires
publicly available images, e.g. RGB images, in training, and then generalizes them in
unseen domains, e.g. depth images or X-ray images as shown in Figure 1.
5

The implementation of this work will be publicly available.

2

Thanh-Dat Truong, Khoa Luu, Chi Nhan Duong, Ngan Le, Minh-Triet Tran

Fig. 1: The Proposed Image Alignment Method across Image Modalities.

In order to approach the problem of image alignment across domains, domain adaptation [3] [16] has recently received significant attention. In this method, the knowledge
from the source domain will be learned and adapted to target domains. During the testing time, the trained model will be tested only in the target domains. However, domain
adaptation requires data in target domains in the training process. In many real-world
applications, it is impossible to observe training samples from new unseen domains in
the training process. In such scenarios, domain adaptation is unable to be re-trained or
fine-tuned with the inputs in new unseen domains. Therefore, domain adaptation cannot
be applied in these problems since the new unseen target domains are unavailable.
1.1

Contributions of this Work

In this paper, we present a novel Deep learning based Generalized Image Alignment
(DeGIA) approach to image alignment across domains that can be applied in various medical applications. The proposed DeGIA requires only single source domain for
training. It is then able to well generalize testing images in new unseen domains for the
task of image alignment. The proposed approach is designed in an end-to-end fashion
and easily integrated with any CNN-based deep network deployed for medical image
alignment. Through the experiments on RGB, Depth and X-Ray images, our proposal
consistently helps to improve performance of medical image alignment.
To the best of our knowledge, this is the first time an image alignment approach
across domains is introduced and achieves the improvement results. The proposed image alignment method is also potentially applied in many other medical applications.

2

Related Work

Medical Image Alignment Image alignment for medical images has been early developed. They are feature-based methods, e.g. SIFT [10], SURF [1], popularly adopted for
general computer vision applications, especially in image alignment. Xiahai et. al. [19]
presented a conception-normal vector information to evaluate the similarity between
two images. Jingfan et. al. [2] introduced an unsupervised image alignment for medical
images. The registration network is trained based on feedbacks from the discrimination
network designed to judge whether a pair of registered images is aligned.

Image Alignment in Unseen Domains via Domain Deep Generalization

Deep Generative Flow

Pre-processing

Pre-processed
Data

Training Set

3

Generalized Template
Features

Generating New
Samples

Deep Network

Discriminative Template
Features

Post-processing

DeGIA

Fig. 2: The proposed Deep learning based Generalized Image Alignment (DeGIA)
method

Domain Adaptation has recently become one of the most popular research topics in
computer vision and machine learning [3] [15] [13] [17] [16]. Tzeng et al. [16] proposed a framework for Unsupervised domain adaptation based on adversarial learning
objectives. Liu et al. [9] presented Coupled Generative Adversarial Network to learn
a joint distribution of multi-domain images. Ganin et al. [3] proposed to incorporate
both classification and domain adaptation to a unified network so that both tasks can be
learned together.
Domain Generalization Domain Generalization aims to learn an adversarially robust
model that can well generalize in the most testing scenarios. M. Ghifary et al. [4] proposed Multi-Task Autoencoder that learns to transform the original image into analogs
in multiple related domains and makes features more robust to variations across domains. Meanwhile, MMD-AAE [7] tries to learn a feature representation by jointly optimizing a multi-domain autoencoder regularized via the Maximum Mean Discrepancy
(MMD) distance. K. Muandet et al.[11] proposed a kernel-based algorithm for minimizing the differences in the marginal distributions of multiple domains. Y. Li et al. [8]
presented an end-to-end conditional invariant deep domain generalization approach to
leverage deep neural networks for domain-invariant representation learning. R. Volpi
et al. introduced Adversarial Data Augmentation (ADA) [18] to generalize to unseen
domains.

3

The Proposed Method

In this section, we introduce a novel Deep learning based Generalized Image Alignment (DeGIA) approach that is able to simultaneously learn the discriminative template
features for landmark points and generalize these template features so that they can
represent the features in different domains.
The proposed deep network architecture is designed in an end-to-end fashion as
shown in Figure 2 with two main components: (1) Discriminative Template Feature Extraction with CNN architecture; and (2) Deep Generative Flows for generalizing these
template features. While the former is to generate template features of landmark points
as much discriminative as possible to maximize the alignment precision, the latter aims
to maintain the consistent structures of these features against the domain shifting to enhance the robustness of these features. Both of them are simultaneously optimized in
DeGIA to generalize images in new unseen domains.
As a result, without seeing any data from other target domains, our learned model is
still able to efficiently extract the generalized template features for all landmark points

4

Thanh-Dat Truong, Khoa Luu, Chi Nhan Duong, Ngan Le, Minh-Triet Tran

and robustly detect them in different domains with high accuracy. The data in source
domain can be considered as RGB hand images that are easy to collect, while the target
domains consist of depth hand pose and X-ray images.
3.1

Discriminative Template Feature Extraction

Mathematically, image alignment can be formulated as a regression problem. In particular, the model receives an input image and then a regression function is built to detect
Nlp landmark points. Let X, Y are input images and the corresponding ground-truths,
M is a model, θ is a set of parameters, and Ȳ is detected landmark points. The image
alignment problem can be formulated as Ȳ = M(X; θ). To learn the model, we use
mean squared error (MSE) as the objective learning function of the model as in Eqn.
(1).
`MSE (X, Y; M) = ||Ȳ − Y||22 = ||M(X; θ) − Y||22

(1)

The model M generally can be any deep network structure, e.g LeNet, AlexNet, ResNet,
etc. The model learns to extract Discriminative Template Features that can be generalized to new unseen domains via the generative component in the DeGIA framework.
3.2

Generalized Template Features

Since pixel values in the image space are quite sensitive to the domain shifting, rather
than directly learning and generalizing features in the original image space, we propose
to employ these processes in a learned latent space where each data point is represented
as density Gaussian distributions. With these distributions, the structure of features can
be efficiently maintained even when shifted among different feature domains. By this
way, the learned features can be robustly generalized. The mapping process from the
image space to the latent space
the deep generative flow.
S is named as src
,
Σ
In particular, let PZsrc = c N (µsrc
c ) be C Gaussian distributions with differc
src
src
src
} of C classes.
}
and
covariances
{Σ1src , Σ2src , ..., ΣC
ent means {µsrc
,
µ
,
..,
µ
1
2
C
Then a generative flow is formulated as learning a function F that maps an image x in
the image space of the source domain to its latent representation z in the latent space Z
src
(x) can be indirectly estimated via PZsrc (z):
such that the density function PX
src
psrc
X (x, c; θ1 ) = pZ (z, c; θ1 )

∂F(z, c; θ1 )
∂x

(2)

where pX (x, c) defines the density distributions of a sample of class label c ∈ C (C is
the number of classes) in the image space; similarity, pZ (z, c; θ1 ) defines the density
1)
distributions of a sample of class label c in the latent space. ∂F (z,c;θ
denotes the
∂x
Jacobian matrix with respect to x. We adopt the structure of [12] for F and learn its
parameters by maximizing the log-likelihood as in Eqn. (2).
θ1∗ = arg max
θ1

X

log pX (xi , c; θ1 )

(3)

i

Then, let ρ be the distance between a new unseen distribution PX and the distrisrc
bution of source domain PX
in latent space. The goal of generalization process is to

Image Alignment in Unseen Domains via Domain Deep Generalization

5

learn the Generalized Template Features such that even when the new unseen distribusrc
tion PX is deviated from PX
a distance ρ, these learned features for landmarks can
still minimize the alignment loss `(X, Y; M, θ) as follows,
arg min
θ

sup
src )≤ρ
P :d(PX ,PX

E [`(X, Y; M, θ)]

(4)

In the next section, our DeGIA is presented to solved Eqn. (4) for generalization.
3.3

Deep Learning Based Generalized Image Alignment

The proposed DeGIA consists of two main components: (1) Discriminative Template
Feature and (2) Generalized Template Features. Firstly, given a training dataset, parameters {θ1 , θ} of F and the network M are updated according to the loss function as
follows,
`(X, Y, C; M, F, θ, θ1 ) = `MSE (M(X; θ), Y) − log pX (X, C; θ1 )

(5)

where the first term is the specific task loss for M and the second term is the loglikelihood of F. Notice that during the optimization process in DeGIA, when discriminative features are updated, the generative flow also requires to be updated accordingly.
Therefore, both alignment and log-likelihood losses are included in the objective function as in Eqn. (5). Secondly, applying the Lagrangian relaxation to Eqn. (4):
h
i
src
2
arg min sup E [`(X, Y, C; M, F, θ, θ1 )] − α · d(PX , PX
) + ||M(X) − M(Xsrc
X )||2 ) (6)
θ1

PX

src
where d(·, ·) is the distance between probability distributions; PX
and PX are the
density distributions of the source and current expanded domains, respectively.
In order to learn the model that optimizes Eqn. (6), we employ a learning process
with two phases: (1) Minimization phase to optimize M and F; and (2) Maximization
phase to approximate PX . For the minimization phase, we use samples from training
set to minimize the objective `(X, Y, C; M, F, θ, θ1 ). Meanwhile, the maximization
phase generates new samples of PX by maximizing Eqn. (7).
i
h
2
x = arg max{`(x, y, c; M, F, θ, θ1 ) − α · d(Px , Pxsrc ) + ||M(x) − M(xsrc
x )||2 }
x

(7)

In this way, new generated samples not only well generalize to unseen domains but also
“hard” samples under the current models due to maximizing loss of the network. Then,
new generated samples are added to the training set to update M and F in the next
training steps. This process continues until convergence.
In order to better generalize, instead of pre-defining mean and corvariances of the deep
generative flow, we design learnable functions Gm (c) and Gstd (c) that map given label
c to corresponding mean and corvariance. To make more robust, we add a noise signal
n, specifically,
µc = γGm (c) + λHm (n)
Σc = Gstd (c)

(8)

where Hm (n) is a flexible shifting range, γ and λ are hyper-parameters that control the
separation of the Gaussian distributions between different classes and the contribution
of Hm (n) to µc .

6

Thanh-Dat Truong, Khoa Luu, Chi Nhan Duong, Ngan Le, Minh-Triet Tran

4

Experiments

This section presents experimental results of DeGIA on benchmark datasets. Firstly,
we shows the effectiveness of our proposed methods with ablative experiments in Sec.
4.1. In these experiments, MNIST is used as the only single source training set and
MNIST-M plays a role as an unseen test set. As the achieved results, our proposals help
consistently to improve performance of deep networks. Finally, we do evaluate DeGIA
on hand pose alignment task trained on RGB NYU Hand Pose images [14] and tested
Depth NYU Hand Pose images [14] and X-Ray images [5].
4.1

Ablation Study

Table 1: Ablative experiment results (%) on effectiveness of the parameters λ, α and β
that control the distribution separation and shitting range. MNIST is used as the only
training set, MNIST-M is used as the unseen testing set.
α
β(%)
0.1 1.0
1% 10% 20% 30%
Pure-CNN
99.06
MNIST
DeGIA 99.43 99.06 99.24 99.40 99.39 99.06 99.40 99.36 99.06 99.42
Pure-CNN
56.47
MNIST-M
DeGIA 57.53 62.94 56.69 60.13 57.62 62.94 62.32 60.54 62.94 58.52
Dataset

Methods

0.0

λ
0.1

1.0

0.01

First of all, we evaluate our proposed methods on MNIST (Fig. 3(A)) and MNISTM (Fig. 3(B)). This experiment aims to measure the effectiveness of hyper-parameters

Fig. 3: Some example images used in our experiments: (A) MNIST Dataset, (B)
MNIST-M Dataset, (C) NYU RGB Hand Poses, (D) NYU Depth Hand Poses and (E)
X-Ray Images.

Image Alignment in Unseen Domains via Domain Deep Generalization

7

to our proposed method. The training process just takes MNIST as a source training domain. Meanwhile, MNIST-M will be a testing domain (an unseen domain). We choose
LeNet [6] as deep network classifier, learning rate and batch size are set to 0.0001 and
128, respectively.
As we mentioned in Sec. 3.3, there are two alternating phases in the training process:
(1) Minimization phase optimizes the M and F and (2) Maximization (perturb) phase
approximates PX . For the maximization phase, we randomly select β percent of the
number of training images to explore new samples in unseen domains. We consider the
effect of percentage (β) of new generated samples, specifically, we do evaluate β ∈
{1%, 10%, 20%, 30%}. Generalized template feature extraction is handled by a set of
scale parameters, i.e γ and λ, are control numbers of distribution separation and shitting
range as shown in Eqn. (8). As the results shown in Table 1, our proposed approach
helps to improve the classifiers significantly, specifically, the accuracy of MNIST-M
has been improved 6.47%. Since the testing phase just uses the discriminative template
feature extraction branch, the inference time of DeGIA will be the same with the standalone CNN.

4.2

Deep learning based Generalized Hand Pose Alignment

In this experiment, we aim for improving Hand Pose Alignment trained on NYU Hand
Pose RGB images (Fig. 3(C)) [14] and tested on NYU Hand Pose Depth (Fig. 3(D))
and X-Ray images (Fig. 3(E)) [5]. In our experiment, we select Nlp = 17 of a hand
as landmark points. We use features of the deep network (i.e LeNet, AlexNet, VGG,
ResNet, DenseNet) forward to a fully connected layer to detect Nlp = 17 landmark
points, all images were resized to 256 × 256. In the maximization phase, we randomly
select 500 images to generate new samples. We compare our method DeGIA against
stand-alone network (Pure-CNN). As the results in Table 2, our DeGIA robustly detects
landmark points in new unseen domains (i.e depth and X-Ray images) and achieves
state-of-the-art results.

Table 2: Experimental Results (MSE) on Hand Alignment with various common deep
network structures.
Networks
LeNet
AlexNet
VGG
ResNet
DenseNet

Methods
Pure-CNN
DeGIA
Pure CNN
DeGIA
Pure CNN
DeGIA
Pure CNN
DeGIA
Pure CNN
DeGIA

RGB
0.0506
0.0345
0.0105
0.0102
0.0083
0.0195
0.0982
0.0295
0.0194
0.0162

Depth
0.0697
0.0534
0.0484
0.0401
0.0822
0.0709
0.2596
0.1065
0.0958
0.0863

X-Ray
0.1266
0.0735
0.0472
0.0336
0.0981
0.0284
0.1447
0.1140
0.1221
0.1214

8

5

Thanh-Dat Truong, Khoa Luu, Chi Nhan Duong, Ngan Le, Minh-Triet Tran

Conclusions

This paper has presented a novel DeGIA approach to medical image alignment across
domains. The proposed method requires only a single source domain for training and
well generalizes in unseen domains. It is designed within an end-to-end CNN to extract
discriminative template features and generalized template features to robustly detect
landmarks. It has been benchmarked on numerous databases across domains, including
MNIST, MNIST-M, NYU RGB and Depth hand pose images, and X-Ray hand images, and consistently achieved improvement performance results. The proposed image
alignment method is also potentially applied in many other medical applications.

References
1. Bay, H., Ess, A., Tuytelaars, T., Van Gool, L.: Speeded-up robust features (surf). Comput.
Vis. Image Underst. 110(3), 346–359 (Jun 2008)
2. Fan, J., Cao, X., Xue, Z., Yap, P.T., Shen, D.: Adversarial similarity network for evaluating image alignment in deep learning based registration. In: Frangi, A.F., Schnabel, J.A.,
Davatzikos, C., Alberola-López, C., Fichtinger, G. (eds.) MICCAI. pp. 739–746 (2018)
3. Ganin, Y., Lempitsky, V.: Unsupervised domain adaptation by backpropagation. In: Bach,
F., Blei, D. (eds.) Proceedings of the 32nd International Conference on Machine Learning.
Proceedings of Machine Learning Research, vol. 37, pp. 1180–1189. PMLR, Lille, France
(07–09 Jul 2015), http://proceedings.mlr.press/v37/ganin15.html
4. Ghifary, M., Bastiaan Kleijn, W., Zhang, M., Balduzzi, D.: Domain generalization for object
recognition with multi-task autoencoders. In: ICCV (December 2015)
5. Halabi, S.S., Prevedello, L.M., Kalpathy-Cramer, J., Mamonov, A.B., Bilbily, A., Cicero,
M., Pan, I., Pereira, L.A., Sousa, R.T., Abdala, N., Kitamura, F.C., Thodberg, H.H., Chen,
L., Shih, G., Andriole, K., Kohli, M.D., Erickson, B.J., Flanders, A.E.: The rsna pediatric
bone age machine learning challenge. Radiology 290(2), 498–503 (2019)
6. Lecun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document
recognition. Proceedings of the IEEE 86(11), 2278–2324 (Nov 1998)
7. Li, H., Jialin Pan, S., Wang, S., Kot, A.C.: Domain generalization with adversarial feature
learning. In: CVPR (June 2018)
8. Li, Y., Tian, X., Gong, M., Liu, Y., Liu, T., Zhang, K., Tao, D.: Deep domain generalization
via conditional invariant adversarial networks. In: ECCV (September 2018)
9. Liu, M.Y., Tuzel, O.: Coupled generative adversarial networks. In: Lee, D.D., Sugiyama, M.,
Luxburg, U.V., Guyon, I., Garnett, R. (eds.) NIPS, pp. 469–477 (2016)
10. Lowe, D.G.: Distinctive image features from scale-invariant keypoints. IJCV 60(2), 91–110
(Nov 2004)
11. Muandet, K., Balduzzi, D., Schlkopf, B.: Domain generalization via invariant feature representation. In: ICML. vol. 28, pp. 10–18 (2013)
12. Nhan Duong, C., Gia Quach, K., Luu, K., Le, N., Savvides, M.: Temporal non-volume preserving approach to facial age-progression and age-invariant face recognition. In: The IEEE
International Conference on Computer Vision (ICCV) (Oct 2017)
13. Sener, O., Song, H.O., Saxena, A., Savarese, S.: Learning transferrable representations for
unsupervised domain adaptation. In: Proceedings of the 30th International Conference on
Neural Information Processing Systems. NIPS’16 (2016)
14. Tompson, J., Stein, M., Lecun, Y., Perlin, K.: Real-time continuous pose recovery of human
hands using convolutional networks. ACM Transactions on Graphics 33 (August 2014)

Image Alignment in Unseen Domains via Domain Deep Generalization

9

15. Tzeng, E., Hoffman, J., Darrell, T., Saenko, K.: Simultaneous deep transfer across domains
and tasks pp. 4068–4076 (2015)
16. Tzeng, E., Hoffman, J., Saenko, K., Darrell, T.: Adversarial discriminative domain adaptation. In: CVPR (July 2017)
17. Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., Darrell, T.: Deep domain confusion: Maximizing for domain invariance. CoRR (2014)
18. Volpi, R., Namkoong, H., Sener, O., Duchi, J.C., Murino, V., Savarese, S.: Generalizing to
unseen domains via adversarial data augmentation. NIPS (2018)
19. Zhuang, X., Gu, L., Xu, J.: Medical image alignment by normal vector information. In:
Computational Intelligence and Security. pp. 890–895. Springer Berlin Heidelberg, Berlin,
Heidelberg (2005)

