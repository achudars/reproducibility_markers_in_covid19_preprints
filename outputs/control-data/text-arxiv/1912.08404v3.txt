Collective Entity Alignment via Adaptive Features
Weixin Zeng†

arXiv:1912.08404v3 [cs.AI] 1 Apr 2020

†

Xiang Zhao

∗† §

Jiuyang Tang†

§

Xuemin Lin‡

Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, China
‡
The University of New South Wales, Australia
§
Collaborative Innovation Center of Geospatial Technology, China

Abstract—Entity alignment (EA) identifies entities that refer
to the same real-world object but locate in different knowledge
graphs (KGs), and has been harnessed for KG construction and
integration. When generating EA results, current solutions treat
entities independently and fail to take into account the interdependence between entities. To fill this gap, we propose a collective
EA framework. We first employ three representative features,
i.e., structural, semantic and string signals, which are adapted
to capture different aspects of the similarity between entities in
heterogeneous KGs. In order to make collective EA decisions, we
formulate EA as the classical stable matching problem, which is
further effectively solved by deferred acceptance algorithm. Our
proposal is evaluated on both cross-lingual and mono-lingual EA
benchmarks against state-of-the-art solutions, and the empirical
results verify its effectiveness and superiority.
Index Terms—Entity alignment; Stable matching

I. I NTRODUCTION
Over recent years, a large number of knowledge graphs
(KGs) have been constructed, whereas none of them can
reach full coverage. These KGs, however, usually contain
complementary contents, making it compelling to study the
integration of heterogeneous KGs. To incorporate the knowledge from target KGs into the source KG, an indispensable
step is entity alignment (EA). For each entity in the source
KG, EA aims to discover its equivalent entities in target KGs.
Most state-of-the-art EA strategies assume that equivalent
entities in different KGs have similar neighbouring information. Consequently, they harness representation learning
frameworks, e.g., TransE [2], graph convolutional network
(GCN) [14], recurrent skipping networks (RSNs) [7], to model
structural feature. When generating EA results for entities
in test set, these embedding-based solutions handle source
entities separately and return a list of ranked target entities
for each source entity with confidence (similarity) scores. The
top ranked target entity is aligned to source entity.
Compared with conventional methods [5], embedding-based
EA methods require less human involvement in feature engineering and can be easily scaled to large KGs. Additionally,
unlike typical entity resolution (ER) approaches [3], [9] that
operate on relational tables, EA centers on graph-structured
data, i.e., KGs.
State-of-the-art solutions treat entities independently when
determining EA results. However, there is often additional
interdependence between different EA decisions, i.e., a target
* Corresponding

Author: Xiang Zhao (xiangzhao@nudt.edu.cn)

entity is less likely to be the match to a source entity if it
is aligned to another source entity with higher confidence.
Sufficiently modelling such collective signals would reduce
mismatches and lead to higher aligning accuracy. Noteworthy
is that here we highlight the overlook of collective signals
during decision making process, while we acknowledge the
use of structural (collective) information as a useful feature
during feature generation process.
In order to address the shortages of current EA solutions,
we establish CEA, a collective entity alignment framework.
CEA first exploits structural, semantic and string-level features
to capture different aspects of the similarity between entities
in source and target KGs. These features are representative
and generally available, which are effectively modelled and
adapted by our proposed feature generation strategies. Then
to capture the interdependence between EA decisions, we formulate EA as the classic stable matching (marriage) problem
(SMP) [6], with entity preference lists constructed by using
these features. The problem is further addressed by deferred
acceptance algorithm (DAA) [10] with high effectiveness and
efficiency. We empirically evaluate CEA on both cross-lingual
and mono-lingual EA tasks against 11 state-of-the-art methods,
and the comparative results demonstrate its superiority.
II. P ROBLEM D EFINITION

AND

R ELATED W ORK

Problem definition.
Given two KGs, source KG G1 =
(E1 , R1 , T1 ) and target KG G2 = (E2 , R2 , T2 ), where E,
R and T represent entities, relations and triples, respectively,
denote the seed entity pairs as S = {(u, v)|u ∈ E1 , v ∈
E2 , u ↔ v}, where ↔ represents equivalence, EA task can
be defined as discovering equivalent target entities for sources
entities in the test set.
EA using structural information. Relying on structural
information, most efforts on EA harness KG embedding for
aligning due to its simplicity, generality, and ability of dealing
with large-scale data. Initially, they utilize KG representation
methods to encode structural information and embed KGs
into individual low-dimensional spaces. Then the embedding
spaces are aligned using seed entity pairs. Some methods [7],
[11] directly project entities in different KGs into the same
embedding space by fusing the training corpus first. In accordance to distance in unified embedding space, equivalent
elements in different KGs can be aligned.

EA using multiple sources of information. In addition
to structural information, existing research also investigates
to incorporate attribute information, e.g., attribute types [11],
[14], and attribute values [13]. Besides entity attributes and
description, some propose to integrate the more generally
available information of entity names [15], [17], [18] to
provide multiple views. Note that these methods unify multiple
views of entities at representation-level; in contrast, CEA
aggregates features at outcome-level.
III. F EATURE G ENERATION
Three generally available features are adapted to tackle EA.
Structural information. In this work, following [14], we
harness GCN [8] to encode the neighbourhood information of
entities as real-valued vectors. In the interest of space, we leave
out the implementation details, which can be found in [14].
Given the learned structural embedding matrix Z, the
similarity between two entities can be calculated by cosine
similarity. We denote the structural similarity matrix as Ms ,
where rows represent source entities, columns denote target
entities and each element in the matrix denote structural
similarity score between a pair of source and target entities.

u, we retrieve its corresponding row entry in M, and rank the
elements in a descending order. The top ranked target entity
is considered as the match.
Nevertheless, this way of generating EA pairs fails to
consider the interdependence between different EA decisions.
To adequately model such coherence, we formulate EA as the
stable matching problem [6]. Concretely, it is proved that for
any two sets of members with the same size, each of whom
provides a ranking of the members in the opposing set, there
exists a bijection of the two sets such that no pair of two
members from the opposite side would prefer to be matched
to each other rather than their assigned partners [4]. This set of
pairing is also called a stable matching. As input to the stable
matching process, the preference list of an entity is defined as
the set of entities in opposing side ranked by similarity scores
in a descending order.

Semantic information. Entity name can be exploited both
from the semantic and string similarity level. We choose
averaged word embeddings to capture the semantic meaning
on account of its simplicity and generality. It does not require
special training corpus, and can represent semantics in a
concise and straightforward manner. For a KG, the name
embeddings of all entities can be denoted in matrix form as N
and cosine similarity is harnessed to capture entity proximity.
We denote semantic similarity matrix as Mn .
String information. Current methods mainly capture semantic information of entities, as semantics can be easily
encoded as embeddings, facilitating the fusion of different feature representations [17], [18]. Plus, semantic feature can also
work well in cross-lingual scenarios by referring to external
sources, e.g., pre-trained multilingual word embeddings.
In our work, we contend that string information, which
has been largely overlooked by existing embedding-based EA
literature, is also beneficial, since: (1) string similarity is
especially useful in tackling mono-lingual EA task and crosslingual EA task where KG pair is very close (e.g., English and
German); and (2) string similarity does not rely on external
resources and is not affected by out-of-vocabulary problem
which tends to restrain the effectiveness of semantic information. In particular, we adopt Levenshtein distance, a string
metric for measuring the difference between two sequences.
We denote the corresponding string similarity matrix as Ml .
IV. C OLLECTIVE E NTITY A LIGNMENT
EA via stable matching. After obtaining feature-specific
similarity matrices, we combine them with equal weights
and generate the fused matrix M. Then EA results can be
determined in an independent fashion that has been adopted
by state-of-the-art methods. Specifically, for each source entity

Fig. 1. EA as SMP using DAA.

Algorithmic solution. To solve SMP, we adopt deferred
acceptance algorithm (DAA) [10], which works as follows:
(1) In the first round, each source entity proposes to the
target entity it prefers most, and then the target entity is
provisionally matched to the source entity it most prefers
so far, and that source entity is likewise provisionally
matched to the target entity.
(2) In each subsequent round, each unmatched source entity
proposes to the most-preferred target entity to which it has
not yet proposed (regardless of whether the target entity
is already matched), and then each target entity accepts
the proposal if it is currently not matched or if it prefers
the new proposer over its current match (in this case, it
rejects its provisional match and becomes unmatched).
(3) The process of (2) is repeated until every source entity is
settled, i.e., find its match.
Example 1: The example in Figure 1 illustrates the algorithm. (1) Suppose there are three source entities u1 , u2 , u3
and three target entities v1 , v2 , v3 ; (2) Given the fused similarity matrix M, where the values in an entity’s corresponding
row/column denote its preferences (larger value is preferred).
Source entities first make proposals to target entities. Both u1

and u2 propose to v1 and u3 proposes to v2 . Since v1 prefers
u1 to u2 , it accepts u1 and rejects u2 , and v2 accepts its only
proposer u3 ; (3) In the next round, u2 is the only source entity
that is unmatched and it proposes to its second most preferable
entity v2 . Although v2 is temporally matched to u3 , it accepts
u2 ’s offer and rejects u3 since it prefers u2 to u3 ; (4) In the
final round, the only unmatched source entity u3 proposes to
v3 and they form a match.

TABLE I
R ESULTS ON DBP15K.

This algorithm guarantees that every entity gets matched.
At the end, there cannot be a source entity and a target entity
both unmatched, as this source entity must have proposed to
this target entity at some point (since a source entity will
eventually propose to every target entity, if necessary) and,
being proposed to, the target entity would necessarily be
matched (to a source entity) thereafter. Also, the “marriages”
are stable. Let u and v both be matched, but not to each other.
Upon completion of the algorithm, it is not possible for both
u and v to prefer each other over their current partners.
Discussion. Theoretically, EA can also be formed as the
maximum weighted bipartite matching problem that requires
the elements from two sets to be paired one-to-one and to
have the largest sum of pairwise utilities (similarity scores).
In this way, the classic Hungarian algorithm may be applied,
which however could be computationally prohibitive for EA
(O(n4 )). Nevertheless, stable matching is more desirable,
since it produces a matching result where no participants have
incentives to deviate from and requires much less time.
V. E XPERIMENTS
Experimental setting. Following previous works, we adopt
DBP15K [11] and SRPRS [7] as evaluation datasets. Regarding parameter setting, in entity name representation, we
utilize the fastText embedding as word embedding and the
multilingual word embeddings are obtained from MUSE1 . The
source code of CEA is publicly available2 .
We utilize the accuracy of alignment results as evaluation
metric. It is defined as the number of correctly aligned
source entities divided by the total number of source entities.
11 state-of-the-art EA methods are adopted for comparison,
which can be divided into two groups, methods merely using
structural information and methods using information external
to structural information.
Evaluation results. From Tables I and II, it reads that our
model CEA consistently outperforms all baselines. The superiority can be attributed to the fact that: (1) three representative
sources of information, i.e, structural, semantic and string-level
features, are leveraged to offer more comprehensive signals for
EA; and (2) source entities are aligned collectively, which can
avoid frequently appearing situations where multiple source
entities are aligned to the same target entity.
In specific, solutions in the first group merely harness
structural information for aligning. MTransE obtains the worst
1 https://github.com/facebookresearch/MUSE
2 https://github.com/DexterZeng/CEA

ZH-EN

JA-EN

FR-EN

MTransE [2]
IPTransE ⋆ [19]
BootEA [12]
RSNs ◦ [7]
MuGNN [1]
NAEA [20]

0.308
0.406
0.629
0.581
0.494
0.650

0.279
0.367
0.622
0.563
0.501
0.641

0.244
0.333
0.653
0.607
0.495
0.673

GCN-Align [14]
JAPE [11]
RDGCN [11]
HGCN [16]
GM-Align [17]
CEA

0.413
0.412
0.708
0.720
0.679
0.787

0.399
0.363
0.767
0.766
0.740
0.863

0.373
0.324
0.886
0.892
0.894
0.972

⋆

1

The results of ⋆-marked methods are obtained
from [12]. We run the source codes of ◦-marked
methods to attain the results. The rest are from
their original papers.

results. IPTransE achieves better performance than MTransE
as it adopts relational path for learning structural embedding,
and utilizes an iterative framework to augment training set.
Additionally, RSNs enhances the results by taking into account of long-term relational dependencies between entities,
which can capture more structural signals for alignment, while
BootEA devises a carefully designed alignment-oriented KG
embedding framework, with one-to-one constrained bootstrapping strategy.
On DBP15K, MuGNN also outperforms IPTransE since it
utilizes a multi-channel graph neural network that captures
different levels of structural information. Nevertheless, its
results are still inferior to BootEA and RSNs. NAEA attains
the highest results within this group, as its neighbourhoodaware attentional representation method can make better use
of KG structures and learn more comprehensive structural
representation. Nevertheless, we implement their source codes
on SRPRS and the corresponding results are much worse, as
shown in Table II.
TABLE II
R ESULTS ON SRPRS.
EN-FR

EN-DE

DBP-WD

DBP-YG

MTransE ⋆
IPTransE ⋆
BootEA ⋆
RSNs ⋆
MuGNN
NAEA

0.251
0.255
0.313
0.347
0.131
0.195

0.312
0.313
0.442
0.487
0.245
0.321

0.223
0.231
0.323
0.388
0.151
0.215

0.246
0.227
0.313
0.400
0.175
0.211

GCN-Align ⋆
JAPE ⋆
RDGCN
HGCN
GM-Align
CEA

0.155
0.256
0.672
0.670
0.627
0.962

0.253
0.320
0.779
0.763
0.677
0.971

0.177
0.219
0.827
0.823
0.815
0.998

0.193
0.233
0.846
0.822
0.827
0.999

1

The results of ⋆-marked methods are obtained from [7]. We
run the source codes of the rest methods to attain the results.

Noteworthily, the overall performance on SRPRS are worse
than DBP15K, as the KGs in DBP15K are much denser than

those in SRPRS and contain more structural information [7].
On SRPRS, where KGs are with real-life degree distributions,
RSNs achieves the best results since the long-term relational
dependencies it captures are less sensitive to entity degrees.
This is verified by the fact that the results of RSNs exceed
BootEA, in contrast to results on DBP15K.
Within the second group, taking advantage of attribute
information, GCN-Align and JAPE outperform MTransE on
DBP15K, whereas on SRPRS, GCN-Align achieves worse
results than MTransE and JAPE attains similar accuracy
scores to MTransE. This reveals that attribute information is
quite noisy and might not guarantee consistent performance.
RDGCN, HGCN and GM-Align outperform all the other
methods except for CEA, as they harness entity name information as the inputs to their overall framework for learning entity
embeddings, and the final entity representation encodes both
structural and semantic signals, providing a more comprehensive view for alignment. Nevertheless, CEA outperforms these
name-based methods by a large margin, since we aggregate
features on outcome-level instead of representation-level.
Notably, on mono-lingual datasets, CEA advances the accuracy to almost 1.000. This is because entity names in DBpedia,
YAGO and Wikidata are nearly identical, where string-level
feature is extremely effective. In contrast, although semantic
information is also useful, not all words in entity names
can find corresponding entries in external word embeddings,
which hence limits its effectiveness. The fact that a simple
string-level feature can achieve ground truth results on current
benchmarks also encourages us to build more challenging
mono-lingual EA datasets, which is left for future work.
TABLE III
A BLATION RESULTS ON SRPRS AND DBP15KZH-EN

CEA
w/o C
w/o Ms
w/o Mn
w/o Ml

EN-FR

EN-DE

DBP-WD

DBP-YG

ZH-EN

0.962
0.914
0.915
0.947
0.782

0.971
0.925
0.968
0.967
0.863

0.998
0.986
0.998
0.998
0.915

0.999
0.994
0.998
0.998
0.937

0.787
0.701
0.622
0.507
0.778

Ablation study. We perform ablation study to gain insights
into the components of CEA. The results on SRPRS and
DBP15KZH-EN are presented in Table III
We first examine the contribution of the collective scheme.
As can be observed from Table III, without collective EA, the
performance drops on all datasets, revealing the significance
of considering the interdependence between EA decisions.
We then test the feasibility of our proposed features. On
cross-lingual datasets, removing structural information consistently brings performance drop, showing its stable effectiveness across all language pairs. In comparison, semantic
information plays a more important role on distantly-related
language pairs, e.g., Chinese–English, whereas string-level
feature is significant for aligning closely-related language
pairs, e.g., English–French. On mono-lingual datasets, removing structural or semantic information does not hurt the

accuracy, while pruning string-level feature results in a considerable accuracy drop. This unveils that string-level feature
is quite useful on datasets where entity names are similar.
VI. C ONCLUSION
In this paper, we have investigated the problem of EA
for heterogeneous knowledge fusion. Existing EA methods
overlook the opportunity to boost performance by constructing
a collective alignment solution. In response, we propose to
incorporate multiple adaptive features and resort to stable
matching for collective EA. The resultant method is experimentally verified to be superior to state-of-the-art options.
Acknowledgment. This work was partially supported by
NSFC under grants Nos. 61872446, 61902417, and 71971212,
and NSF of Hunan province under grant No. 2019JJ20024.
R EFERENCES
[1] Y. Cao, Z. Liu, C. Li, Z. Liu, J. Li, and T. Chua. Multi-channel graph
neural network for entity alignment. In ACL, pages 1452–1461, 2019.
[2] M. Chen, Y. Tian, M. Yang, and C. Zaniolo. Multilingual knowledge
graph embeddings for cross-lingual knowledge alignment. In IJCAI,
pages 1511–1517, 2017.
[3] S. Das, P. S. G. C., A. Doan, J. F. Naughton, G. Krishnan, R. Deep,
E. Arcaute, V. Raghavendra, and Y. Park. Falcon: Scaling up handsoff crowdsourced entity matching to build cloud services. In SIGMOD,
pages 1431–1446, 2017.
[4] J. Doerner, D. Evans, and A. Shelat. Secure stable matching at scale.
In SIGSAC Conference, pages 1602–1613, 2016.
[5] L. A. Galárraga, N. Preda, and F. M. Suchanek. Mining rules to align
knowledge bases. In AKBC@CIKM, pages 43–48, 2013.
[6] D. Gale and L. S. Shapley. College admissions and the stability of
marriage. The American Mathematical Monthly, 69(1):9–15, 1962.
[7] L. Guo, Z. Sun, and W. Hu. Learning to exploit long-term relational
dependencies in knowledge graphs. In ICML, pages 2505–2514, 2019.
[8] T. N. Kipf and M. Welling. Semi-supervised classification with graph
convolutional networks. CoRR, abs/1609.02907, 2016.
[9] S. Mudgal, H. Li, T. Rekatsinas, A. Doan, Y. Park, G. Krishnan, R. Deep,
E. Arcaute, and V. Raghavendra. Deep learning for entity matching: A
design space exploration. In SIGMOD, pages 19–34, 2018.
[10] A. E. Roth. Deferred acceptance algorithms: history, theory, practice,
and open questions. Int. J. Game Theory, 36(3-4):537–569, 2008.
[11] Z. Sun, W. Hu, and C. Li. Cross-lingual entity alignment via joint
attribute-preserving embedding. In ISWC, pages 628–644, 2017.
[12] Z. Sun, W. Hu, Q. Zhang, and Y. Qu. Bootstrapping entity alignment
with knowledge graph embedding. In IJCAI, pages 4396–4402, 2018.
[13] B. D. Trisedya, J. Qi, and R. Zhang. Entity alignment between
knowledge graphs using attribute embeddings. In AAAI, pages 297–
304, 2019.
[14] Z. Wang, Q. Lv, X. Lan, and Y. Zhang. Cross-lingual knowledge graph
alignment via graph convolutional networks. In EMNLP, pages 349–357,
2018.
[15] Y. Wu, X. Liu, Y. Feng, Z. Wang, R. Yan, and D. Zhao. Relation-aware
entity alignment for heterogeneous knowledge graphs. In IJCAI, pages
5278–5284, 2019.
[16] Y. Wu, X. Liu, Y. Feng, Z. Wang, and D. Zhao. Jointly learning entity
and relation representations for entity alignment. In EMNLP, pages
240–249, 2019.
[17] K. Xu, L. Wang, M. Yu, Y. Feng, Y. Song, Z. Wang, and D. Yu. Crosslingual knowledge graph alignment via graph matching neural network.
In ACL, pages 3156–3161, 2019.
[18] Q. Zhang, Z. Sun, W. Hu, M. Chen, L. Guo, and Y. Qu. Multi-view
knowledge graph embedding for entity alignment. In IJCAI, pages 5429–
5435, 2019.
[19] H. Zhu, R. Xie, Z. Liu, and M. Sun. Iterative entity alignment via joint
knowledge embeddings. In IJCAI, pages 4258–4264, 2017.

[20] Q. Zhu, X. Zhou, J. Wu, J. Tan, and L. Guo. Neighborhood-aware
attentional representation for multilingual knowledge graphs. In IJCAI,
pages 1943–1949, 2019.

