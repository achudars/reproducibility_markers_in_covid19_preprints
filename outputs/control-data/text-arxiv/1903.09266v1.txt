SUBMITTED TO ENTROPY

1

Reduction of Markov Chains using a
Value-of-Information-Based Approach

arXiv:1903.09266v1 [cs.IT] 21 Mar 2019

Isaac J. Sledge and José C. Príncipe
Abstract—In this paper, we propose an approach to obtain reduced-order models of Markov chains.
Our approach is composed of two information-theoretic processes. The first is a means of comparing pairs
of stationary chains on different state spaces, which is done via the negative Kullback-Leibler divergence
defined on a model joint space. Model reduction is achieved by solving a value-of-information criterion
with respect to this divergence. Optimizing the criterion leads to a probabilistic partitioning of the states
in the high-order Markov chain. A single free parameter that emerges through the optimization process
dictates both the partition uncertainty and the number of state groups. We provide a data-driven means of
choosing the ‘optimal’ value of this free parameter, which sidesteps needing to a priori know the number of
state groups in an arbitrary chain.
Index Terms—Markov chains, value of information, aggregation, model reduction, dynamics reduction, information theory
1 Introduction

Markov models have seen a widespread adoption in a variety of disciplines. Part of their appeal is that the application and simulation of such models is rather efficient, provided that the corresponding state-space has a small to
moderate size. Dealing with large state spaces is often troublesome, in comparison, as it may not be possible to adequately simulate the underlying models. Such large-scale spaces are frequently encountered in reinforcement learning,
for instance [1–5].
A means of rendering the simulation of large-scale models tractable is crucial for many applications. One way
for doing this is to reduce the overall size of the Markov-chain state space by aggregation [6]. Aggregation entails
either explicitly or implicitly defining and utilizing a function to partition nodes in the probability transition graph associated with the large-scale chain. Groups of nodes, which are related by the their inter-state transition probabilities
and have strong interactions, are combined and treated as a single aggregated node in a new graph. This results in a
lower-order chain with a reduced state space. A stochastic matrix for the lower-order chain is then specified, which
describes the transitions from one super-state to another. This stochastic matrix should roughly mimic the dynamics
of the original chain despite the potential loss in information incurred from the state combination.
There are a variety of methods for aggregating Markov chains, as we review in the next section. In this paper,
we develop and analyze a novel approach for aggregating Markov chains, which is composed of two informationtheoretic processes [7]. The first process entails quantifying the dissimilarity of nodes in the original and reducedorder probability transition graphs, despite the difference in state space sizes. The second process involves iteratively
partitioning similar nodes without explicit knowledge of the number of groups.
For the first process, we adopt the reasonable view that nodes in a pair of chains are dissimilar if their associated
rows of the stochastic matrix are sufficiently distinct. We employ an information-theoretic measure, the negative
Kullback-Leibler divergence [8], to gauge the distinctiveness and hence identify candidate nodes in the original chain
for aggregation. This divergence assesses the overlap between probability distributions. It coincides with the DonskerVaradhan rate function appearing in the large-deviations theory of Markov chains [9, 10] and measures the distance
between two Markov chains defined on the same discrete state space.
In the aggregation process that we consider, a reduced-order model is constructed on a discrete state space of
a different size than the original model. To facilitate assessing the negative Kullback-Leibler divergence between
the original and reduced-order models, we construct a so-called joint model that incorporates details from both the
original and reduced-order models. This model encodes the salient properties of the lower-order transition matrix and
is of the proper dimensionality to compare against rows of the original transition matrix. A byproduct of using this
Isaac J. Sledge a Research Engineer with the Advanced Signal Processing and Automated Target Recognition Branch of the US Naval Surface
Warfare Center (NSWC-PCD), Panama City, FL 32407, USA (email: isaac.j.sledge@navy.mil).
José C. Príncipe is the Don D. and Ruth S. Eckis Chair and Distinguished Professor with both the Department of Electrical and Computer Engineering
and the Department of Biomedical Engineering, University of Florida, Gainesville, FL 32611, USA (email: principe@cnel.ufl.edu). He is the director
of the Computational NeuroEngineering Laboratory (CNEL) at the University of Florida.
The work of the authors was funded by grants N00014-15-1-2013 (Jason Stack), N00014-14-1-0542 (Marc Steinberg), and N00014-19-WX-00636
(Marc Steinberg) from the US Office of Naval Research. The first author was additionally supported by in-house laboratory independent research (ILIR)
grant N00014-19-WX-00687 (Frank Crosby) from the US Office of Naval Research, a University of Florida Research Fellowship, a Robert C. Pittman
Research Fellowship, and an ASEE Naval Research Enterprise Fellowship while at the University of Florida. This authors would like to thank Sean P.
Meyn for his suggestion to use the value of information for aggregating Markov chains.

SUBMITTED TO ENTROPY

2

joint model is that we can sidestep considering all possible liftings of the reduced-order models to the original space
by averaging their dynamics according to a given distribution [11, 12]. Our approach therefore avoids having to solve
an additional optimization problem, which is a boon when aggregating large-state-space chains.
The problem of finding an aggregated Markov chain that captures much of the dynamics in the original chain
can be posed as a cost function that uses the above divergence. For the second process, we consider the use of an
information-theoretic criterion known as the value of information [13–15] to efficiently partition the probability
transition graph. The value of information is a constrained, modified-free-energy-difference criterion that describes
the maximum benefit associated with a given quantity of information in order to minimize average losses [16, 17]. It
is an optimal, non-linear conversion between information, in the Shannon sense [18], and either costs or utilities, in
the von-Neumann-Morgenstern sense [19].
In the context of aggregating Markov chains, the value of information describes the change in the distortion between the high- and low-order transition models that occurs from potentially modifying the number of state groups
and elements of those groups. The number of groups is implicitly determined by the bounded information that a given
row of the original chain’s transition matrix shares with a corresponding row of the reduced-order chain’s transition
matrix. Low information bound amounts lead to small numbers of groups with many states per group. A potentially
good qualitative partitioning is often observed in such cases, as the reduced-order chain is parsimonious. Higher information bound amounts can lead to large numbers of groups with fewer states per group. The partitioning of the
original chain can be over-complete, as related states may be unnecessarily split to yield a lower free energy.
Optimizing the value of information in a grouped-coordinate-descent manner yields a single free parameter that
represents the effect of the information bound. Increasing this parameter from some base value yields a hierarchy of
partitions. Each element of this hierarchy corresponds to a partition with an increasing information bound amount
and hence a potentially increasing number of state groups. Finer-scale group structure in the transition matrix is captured as the parameter value rises. After some value, however, there are diminishing returns on the quality of the
aggregation results. Determining the ‘optimal’ value, in a completely data-driven fashion, is hence crucial. To find
such values for arbitrary Markov chains, we apply perturbation theory. In particular, we calculate the underestimation error of the information constraint in the value of information that occurs when considering finite-state chains.
We then augment the value-of-information criterion by subtracting out this overestimation. Finally, we determine a
lower bound for the free parameter that minimizes the underestimation error. The corresponding aggregation process
empirically avoids fitting more to the noise than the structure in the stochastic matrix of the high-order model.
As a part of our treatment of the value of information, we furnish convergence and convergence-rate proofs to
demonstrate the optimality of the criterion for the aggregation problem.
The remainder of this short paper is organized as follows. We begin, in section 2, with a survey of aggregation
techniques for Markov chains. Our approach is given in section 3. In section 3.1, we introduce our notation and some
fundamental concepts for binary-partition-based aggregation. In section 3.1.1, we introduce the concept of a joint
model so that the differently-sized transition matrices of the original and reduced-order chains can be compared. We
outline, in section 3.1.2, how this joint model facilitates the formulation of an minimum-dissimilarity aggregation
optimization problem. Properties of this problem are analyzed for general divergence measures. At the end of section
3.1.2, we discuss practical issues associated with this initial optimization problem, which motivates the use of the
value of information. We show, in sections 3.2.1 and 3.2.2, how this information-theoretic criterion can be applied to
probabilistically partition transition matrices. We also cover how the criterion can be efficiently solved, how to construct the reduced-order transition matrices after partitioning, and how to find an upper bound on the free parameter
that emerges from optimizing this information-theoretic criterion. Lastly, in section 3.2.3, we furnish a bound on the
expected performance of this criterion.
In section 4, we assess the empirical capabilities of the value of information for Markov chain aggregation. We
begin by covering our experimental protocols in section 4.1. In section 4.2, we present our simulation results for
series of synthetic datasets. We first assess the performance of our value-of-information-based reduction for manuallyselected and perturbation-theory-derived multiplier values. We also comment on the convergence properties. The
appropriateness of the Shannon information constraint over an entropy constraint is additionally investigated in these
sections. Discussions of these results are given at the end of this section. We summarize these findings in the broader
context of our theoretical results in section 5. Additionally, we outline directions for future research.
2 Literature Review

A variety of Markov model aggregation techniques have been proposed over the years. Some of the earliest work
exploited the strong-weak interaction structure of nearly completely decomposable Markov chains to obtain reducedorder approximations [20, 21]. Both uncontrolled [22, 23] and controlled Markov chains [24–26] have been extensively studied in the literature.
The aggregation of nearly completely decomposable Markov chains have been investigated by Courtois [27]
and other researchers [28–30]. Courtois developed an aggregation procedure that yields an approximation of the

SUBMITTED TO ENTROPY

3

steady-state probability distribution with respect to a parameter that represents the weak interaction between state
groups. This process was later augmented to provide more accurate approximations [31]. It was also combined with
various iterative schemes, like the Gauss-Seidel method, to improve the speed of convergence [32–35]. Years later,
Phillips and Kokotovic presented a singular perturbation interpretation of Courtois’ aggregation approach [36]. They
developed a similarity transformation that converts the system into a singularly perturbed form, whose slow model
coincides with the aggregated matrix found by Courtois’ approach. The use of singular perturbation has also been
considered by other researchers [37–39].
There are additional approaches that have been developed. A few are worth noting here, as they resemble our contributions in various ways [11, 12, 40–43]. For example, Deng and Huang [41] used the Kullback-Leibler divergence
as a cost function to obtain a low-rank approximation of the original transition matrix via nuclear-norm regularization. This preserved the cardinality of the state space. Here, we employ the negative Kullback-Leibler divergence as
a means of measuring the change in the original and modified chains. We, however, consider a modified chain that
is of a reduced order, not the same order. This change should provide more tangible benefits for the simulation of
large-scale systems.
Another scheme that is related to ours is that of Vidyasagar [43]. Vidyasagar investigated an information-theoretic
metric, the variation of information, between distributions on sets with different cardinalities. Actually computing
the metric that he proposed turns out to be computationally intractable for large-scale systems, however. He therefore
considered an efficient greedy approximation for finding an upper bound of the distance and studied its use for optimal order reduction. He demonstrated that the optimal reduced-order distribution a set of of a particular cardinality is
obtained by projecting the original distribution. That is, the reduced-order distribution should have maximal entropy.
This condition is equivalent to to requiring that the partition function induces the minimum information loss. In our
work, the metric that we consider is tractable for different-cardinality sets. The partitioning process is not, however,
which motivates the use of the value of information for efficiently finding approximate partitions. An advantage of
using the value of information is that it directly minimizes the information loss, as it relies on a Shannon information
constraint that quantifies the mutual dependence of the high-order and low-order chain states.
In [11, 12, 40], Deng et al. and Geiger et al. developed two-step, information-theoretic approaches for Markov
chain aggregation. In the first step, the optimal model reduction problem is solved on the reduced space defined by
a fixed partition function. In the second step, Deng et al. [11, 40] select an optimal partition function according to a
non-convex relaxation of the bi-partition problem, while Geiger et al. [12] find an approximate partition using the
information-bottleneck method. In both works, the distortion between the original and reduced-order models was
assessed via Kullback-Leibler divergence. The authors defined an optimization-based lifting procedure so that both
chains would have the same cardinality. The lifting employed by Geiger et al. incorporates one-step transition probabilities of the original chain, which minimizes information loss. They obtained a tight bound for lumpable chains.
Deng et al. lift based only on the stationary distribution of the original chain, which maximizes the redundancy of the
aggregated Markov chain. Here, we consider the formation of an joint model based on a similar approach to Deng
et al.: we form a probabilistically weighted average of the entries from the original stochastic matrix. However, the
formulation of this joint model occurs naturally versus being defined as an optimization problem.
There are other topical differences between these approaches. For example, Geiger et al. [12], through the use of
the information bottleneck, attempt to compress the original-model states into reduced-model states, in a lossy way,
while keeping as much information about the original transition probabilities as possible. Optimizing the value of
information achieves a similar effect, albeit in a different manner. It limits the information lost during quantization by
both bounding the divergence between the original and reduced-order models and simultaneously maximizing the
mutual dependence between the states in both models. Despite this similar effect, the value of information has practical advantages. We prove that the dynamical system underlying the partitioning process undergoes phase changes,
for certain values of the criterion’s single free-parameter, where a new state-group emerges in the reduced-order
model. Between critical values of the free parameter, no phase changes occur, which implies that only a finite number of distinct values must be considered. For the information bottleneck, investigators would have to sweep over
many parameter values, often far more than we consider, and repeatedly solve the aggregation problem. Using an
information-bottleneck scheme can hence be computationally prohibitive for large-scale Markov chains.
We further enhance the practicality of the value of information by deriving an expression for the ‘optimal’ freeparameter value. This value performs a second-order minimization of the estimation error associated with the Shannonmutual-information term in the value of information. Empirically, using this value causes the partitioning process
to fit more to the structure of well-defined state groups in the original model than outlier states. It also tends to yield
parsimonious partitions that neither over- nor under-quantize the state space.
Our motivation for considering a value-of-information-based methodology arose from our use of this criterion in reinforcement learning. We have previously applied this criterion, in [13–15], for resolving the explorationexploitation dilemma in Markov-decision-process-based reinforcement learning. In our experiments on a variety
of complicated application domains, we found that the value of information would consistently outperform exist-

SUBMITTED TO ENTROPY

4

ing search heuristics. We originally attributed this improved learning rate solely to a systematic partitioning of the
state space. That is, groups of states would be partitioned, according to their action-value function magnitude, and
assigned the same action. The problem of determining an action that works well for an entire group of related states is
easier than doing the same for each state individually. However, it is our hypothesis that there is an aggregation of the
Markov chains underlying the Markov decision processes. The aggregation theory developed in this paper represents
a necessary first step to showing that the criterion can perform reinforcement learning on a simpler Markov decision
process whose dynamics roughly mirror those of the original problem.
We are not the first to consider the aggregation of Markov chains that appear in Markov-decision-process-based
reinforcement learning, though [1–5]. Aldhaheri and Khalil [2] focused on the optimal control of nearly completely
decomposable Markov chains. They adapted Howard’s policy-iteration algorithm to work on an aggregated model.
They showed that they could provide optimal control that minimizes the average cost over an infinite horizon. Sun
et al. [4] employed time aggregation to reduce the state space for complicated Markov decision processes. They
divided the original process into segments, by certain states, to form an embedded Markov decision process. Value
iteration is then executed on this lower-order model. In [5], Jia provided a polynomial-time means of aggregating
states of a Markov decision process when the optimal value function is known. For approximate value functions,
he showed how to apply ordinal optimization to uncover a good state reduction with a high probability of being the
correct aggregation. A commonality of these works is that they are model-based: they assume that the transition
probabilities are explicitly known. Our previous work [14, 15], however, focused on model-free learning, where these
probabilities are not available a priori. Model reduction according to a value-of-information-based should therefore
occur implicitly during the exploration process if the concepts we develop as part of our aggregation theory extend to
Markov decision processes.
3 Methodology

Our approach for aggregating Markov chains can be described as follows. Given a stochastic matrix of transition
probabilities between states, we seek to partition this matrix to produce a reduced-size matrix which we refer to as
an aggregated stochastic matrix. The aggregated stochastic matrix has an equivalent graph-based interpretation, as it
characterizes the edge weights of an undirected graph. The vertices in this matrix correspond to states of a reducedorder chain. There is a one-to-many mapping of a vertex from the aggregated stochastic matrix to the vertices of
the original transition-matrix graph for the high-order chain. Edges of the aggregated stochastic matrix codify the
transition probability between pairs of states in the low-order model.
There are many possible aggregated stochastic matrices that can be formed for a given Markov chain. We would
like to find a matrix that yields the least total distortion for some measure, particularly the negative Kullback-Leibler
divergence. Due to the different sizes of the original transition matrix and the aggregated stochastic matrix, though,
directly applying this divergence is not possible. While we could re-define the Kullback-Leibler divergence for probability vectors with different cardinalities, we have opted to instead transform the aggregated stochastic matrices so
that they are of the same size as the original transition matrix. We specify how to construct a so-called joint model
that encodes all of the dynamics of the reduced-order chain. We provide an straightforward objective function for
constructing a binary partition of the original transition matrix to uncover the optimal aggregated stochastic matrix.
The objective function that we specify leads to another issue: finding the optimal aggregated stochastic matrix
is not trivial due to the binary-valuedness of the one-to-many mappings. It can quickly become computationally
intractable as the size of the state space rises. To make our aggregation approach more computationally efficient,
we relax the binary assumption by considering an alternate objective function, which is based on the value of information. Optimization of the value of information yields a probabilistic partitioning process for finding part of the
aggregated stochastic matrix. A single parameter associated with this function dictates both the uniformity of the
probabilistic partitions and the number of state groups that emerge. In the limits of the parameter value, the solution
of the value of information approaches the global solutions of the original objective function. A hierarchy of possible
partitions, each with a different number of groups, are produced for other parameter values; these are approximate
solutions of the binary-partition-based objective function.
3.1 Aggregating Markov Chains
3.1.1 Preliminaries

For our approach, we consider a first-order, homogeneous Markov chain defined on a finite state space. Our analyses of such chains focus on graph-based transition abstractions.
Definition 3.1. The transition model of a first-order, homogeneous Markov chain is a weighted, directed graph

Rπ given by the three-tuple (Vπ , Eπ , Π) with the following elements
(i) A set of n vertices Vπ = vπ1 ∪ . . . ∪ vπn representing the states of the Markov chain.
(ii) A set of n×n edge connections Eπ ⊂ Vπ ×Vπ between reachable states in the Markov chain.

SUBMITTED TO ENTROPY

5

Π ∈ R9×9
+

ϑ2,1:9

ψ2,1:9

π1,1:9

0.000 0.086 0.050 0.143 0.169 0.120 0.138 0.141 0.150

0.000 0.346 0.346

1.000 0.000 0.000

π2,1:9

0.081 0.000 0.036 0.131 0.193 0.150 0.149 0.113 0.144

0.000 0.000 0.000

1.000 0.000 0.000

π3,1:9

0.053 0.041 0.000 0.130 0.186 0.136 0.141 0.141 0.167

0.000 0.000 0.000

1.000 0.000 0.000

π4,1:9

0.125 0.122 0.107 0.000 0.076 0.063 0.034 0.223 0.245

0.530 0.000 0.654

0.000 1.000 0.000

π5,1:9

0.124 0.150 0.128 0.064 0.000 0.036 0.037 0.224 0.234

Φ ∈ R3×3
+
g(π5,1:9, ϕ2,1:3)
g(π5,1:3, ϑ2,1:9)

0.000 0.530 0.469

ϕ1,1:3

0.346 0.000 0.653

ϕ2,1:3

0.346 0.654 0.000

ϕ3,1:3

=

0.000 0.000 0.000

×

0.000 1.000 0.000

π6,1:9

0.110 0.146 0.117 0.065 0.045 0.000 0.033 0.234 0.247

π7,1:9

0.125 0.142 0.120 0.035 0.045 0.033 0.000 0.240 0.257

0.000 0.000 0.000

0.000 1.000 0.000

π8,1:9

0.092 0.078 0.087 0.167 0.200 0.168 0.174 0.000 0.030

0.469 0.653 0.000

0.000 0.000 1.000

π9,1:9

0.090 0.092 0.094 0.168 0.192 0.162 0.171 0.027 0.000

0.000 0.000 0.000

0.000 0.000 0.000

ϑ1,1:9
>

Θ

ϑ3,1:9
∈ R3×9
+

0.000 1.000 0.000

0.000 0.000 1.000

ψ1,1:9
>

ψ3,1:9

Ψ ∈ {0, 1}3×9

Figure 3.1: Depiction of the comparison process for exact, binary aggregation of a nine-state Markov chain. The transition matrix Φ associated with
the low-order, three-state Markov chain cannot be directly compared to the transition matrix Π of the high-order, nine-state chain for general
measures g. For example, we may want to compare the fifth row of Π, π5,1:9 , which is highlighted in green, with the second row of Φ, ϕ2,1:3 :
g(π5,1:9 , ϕ2,1:3 ), which is highlighted in purple. To facilitate this comparison, we consider a joint model whose accumulation matrix Θ is of the
proper size for comparison against Π. Θ when multiplied with the binary partition matrix Ψ equals the low-order transition matrix Φ.
It can be seen that the accumulation matrix Θ of the joint model encodes all of the dynamics of Φ. This relationship ensures that g(π5,1:9 , ϑ2,1:9 )
is actually comparing the dynamics of Π and Φ. Θ has been automatically padded with zero entries, by way of the exact aggregation process
developed in this section, to ensure that it is of the same size as Π. For this example, only the first, fourth, and eighth entries of any row in Π are
relevant for the comparison of the entries in Φ. Such entries lead to the maximal preservation of the information between Π and Φ when using the
negative Kullback-Leibler divergence for the measure g.

(iii) A stochastic transition matrix Π ∈ Rn×n
+ . Here, [Π]i,j = πi,j represents the non-negative transition
probability between states i and j. We impose the constraint that the probability of experiencing a state transition is independent of time.
The subscripts on the vertices and edges represent the dependence on the matrix Π.
Throughout, we assume that all Markov chains are irreducible and aperiodic. As a consequence, there is a unique
invariant probability distribution γ associated with the chain such that γ > Π = γ > . We will sometimes write this
distribution as γ(Π) to denote to which matrix the distribution is associated.
We are interested in comparing pairs of Markov chains. A means to do this is by considering given rows of the
stochastic transition matrix. We represent the ith row of Π by πi,1:n = [πi,1 , . . . , πi,n ]. πi,1:n is a probability vector
describing the chance of transitioning from state vπi to any possible next states. We assume that πi,j = 0 if and only if
there is no directed edge from state vπi to state vπj and hence no chance of transitioning between these states.
If a pair of transition models for different Markov chains, Rπ and Rϕ , have the same number of states, then they
can be compared according to a measure g : Rn+ × Rn+ → R+ acting on πi,1:n and ϕi,1:n ∀i. Here, we take this measure to be the negative Kullback-Leibler divergence; the theory that follows is applicable to many general measures,
though.
Definition 3.2. Let Rπ = (Vπ , Eπ , Π) and Rϕ = (Vϕ , Eϕ , Φ) be transition models of two Markov chains

over n states. The negative relative entropy, or negative Kullback-Leibler divergence,
between a given set of
Pn
states between these two chains is a function given by g(πi,1:n , ϕi,1:n ) = j=1 γi πi,j log(πi,j /ϕi,j ), where
γ is the invariant probability distribution associated with Rπ . The divergence rate is finite provided that Π is
absolutely continuous with respect to Φ.
Since we are considering the problem of chain aggregation, the state spaces will be different. One chain Rπ will have
n states while another Rϕ will have m states, with m < n. The dimensionalities of given rows in the corresponding
transition matrices will hence not be equivalent, which precludes a direct comparison using conventional measures.
To facilitate the application of measures to Rπ and Rϕ when they have different discrete state spaces, we consider
construction of a joint model Rϑ . This joint model defines a joint state space composed of Vπ and Vϕ . It consequently
possesses a weighting matrix Θ with the same number of columns as Π, which is outlined in definition 3.4 and illustrated in figure 3.1.
The joint model relies on the specification of a binary partition function ψ : Z+ → Z+ , which is given in definition 3.3. This function provides a one-to-many mapping between states in Vπ and Vϕ and hence can be seen as a
means of delineating which states of the original chain should be combined.
Definition 3.3. Let Rπ = (Vπ , Eπ , Π) and Rϕ = (Vϕ , Eϕ , Φ) be transition models of two Markov chains

over n and m states, respectively. A binary partition function ψ is a surjective mapping between two state index
sets, Z1:n and Z1:m , such that ψ −1 (Z1:m ) is a partition of Z1:n . That is, ψ −1 (j) ⊂ Z1:n is not empty, ψ −1 (1) ∪
. . . ∪ ψ −1 (m) = Z1:n , and ψ −1 (j) ∩ ψ −1 (k) = ∅, for j 6= k.

SUBMITTED TO ENTROPY

6

?.???
?
ϑ 2,7 = ?.??
vπ4

0.1
0.1 30
12

vπ7

43
0.1 22
0.18
3
0.1
19
0.1

vπ2

ψ −1(2) = {4, 5, 6, 7}

2
0.02
9
0.01

5
16
0. 264
0.

vϕ1
vπ1

4
65
0. 653
0.

ϑ1,6 = ?.???
?.???

ϑ3,11 = ?.???
?.???

vπ9

ψ −1(1) = {1, 2, 3}

?.???
ϑ1,4 = ?.???

vπ8

vϕ3

vϕ3

ψ −1(3) = {8, 9}

vπ9

(Vπ , Eπ , Π), Π ∈ R9×9

?.???
ϑ3,12 = ?.???

(Vϕ, Eϕ, Φ), Φ ∈ R3×3

(Vϑ, Eϑ, Θ), Θ ∈ R3×9

(a) Original (left) and reduced-order (right) transition model

vϕ2

?.???
ϑ2,9 = ?.???
vπ3

vϕ1

vπ1

0.
0. 186
25
8

vπ8

?.???
?.???

vπ6

?.???
?
ϑ 1,5 = ?.??
vπ2

72
0.1 8
2
0.1

0.094
0.
0.0 159
9
0.1 7
80

93
0.0
44
0.1
5
09
0. 166
0.

ϑ2,8 =

vπ5

vπ7

vϕ2

30
0.5 46
0.3

0.
3
0. 46
46
9

0.0
0.0 79
77

9
02
0. 032
0.

0.0
8
0.1 5
2
0.9
0. 090
14
9

vπ3

0.026
vπ5
0.0305
03
0. 30
0
0.

vπ6

0.136
0.1 0.126
0.
0. 1210.1 40
11
28
2

0.148
0.140

?.???
ϑ2,10 = ?.???

vπ4

0.
0. 159
12
7

1
13
0. 24
1
0.

6
17
0. 49
2
0.

0.052
0.045

2
05
0. 045
0.

0.1
0.2 69
41

6
0.17
4
0.14

(b) Joint model

Figure 3.2: Depictions of the various models when using binary-valued partitions for the transition matrices in figure 3.1. In (a), we show the
transition model for a high-order, nine-state Markov chain (right) and its low-order, three-state transition model representation (left) after the
aggregation process. The numbers along the edges represent the probabilities of transitioning to and from pairs of states. In (b), we show the joint
model. The vertices of the joint model represent states in both the high-order and low-order chains. The edges between state pairs in both the highand low-order chains, which are depicted using dashed lines, are removed. In the joint model, edges are inserted to connect states in high-order chain
with those in the low-order chain, thereby providing the state aggregation. Note that the edge weights in the joint model are unknown a priori and
must be uncovered.

It can be seen that a partition of a state index set induces a binary
Ppartition matrix [Ψ]i,j = ψi,j , where
/ ψ −1 (j). Thus, [Ψ]1:n,k = i∈ψ−1 (k) ei , where ei is the ith unit vector.
ψi,j = 1 if i ∈ ψ −1 (j) and ψi,j = 0 if i ∈
Pm
The set of all binary partition matrices is given by {Ψ ∈ Rn×m
|[Ψ]i,j = ψi,j ∈ {0, 1}, j=1 ψi,j = 1}.
+
Definition 3.4. Let Rπ = (Vπ , Eπ , Π) and Rϕ = (Vϕ , Eϕ , Φ) be transition models of two Markov chains over

n and m states, respectively, where m < n. Rϑ = (Vϑ , Eϑ , Θ) is a joint model, with m+n states, that is defined by
(i) A vertex set Vϑ = Vπ ∪ Vϕ , which is the union of all state vertices in Rπ and Rϕ . For simplicity, we
assume that the vertex set for the intermediate transition model is indexed such that the first m nodes are from
Rϕ and the remaining n nodes are from Rπ .
(ii) An edge set Eϑ ⊂ Vϕ ×Vπ , which are one-to-many mappings from the states in the original transition
model Rπ to the reduced-order transition model Rϕ .
>
>
>
(iii) A weighting matrix Θ, which is such that Θ ∈ Rm×n
, Θ = [ϑ>
1,1:n , ϑ2,1:n , . . . , ϑm,1:n ] . The partition
+
function ψ provides
Pa relationship between the stochastic matrices Φ and Θ of
PRϕ and Rϑ , respectively. This
is given by ϕj,k = i∈ψ−1 (k) ϑj,k ∀j, k, or, rather, ΘΨ = Φ, where [Ψ]1:n,k = i∈ψ−1 (k) ei .

An illustration of the joint model relative to the other models is provided in figure 3.2.
3.1.2 Partitioning Process and State Aggregation

For any given transition model Rπ , we would like to find, by way of the joint model Rϑ , another transition model
Rϕ with fewer states that resembles the dynamics encoded by Rπ . We therefore seek a Rϑ with a weighting matrix Θ
that has the least total distortion with respect to the transition matrix Π of Rπ for some partition. In this section, we
specify how to find Rϕ .
Before we can define the notion of least total distortion, we must first specify the concept of the total distortion of
Θ with respect to Π.
Definition 3.5. Let Rπ = (Vπ , Eπ , Π), Rϕ = (Vϕ , Eϕ , Φ), and Rϑ = (Vϑ , Eϑ , Θ) be transition models of two

Markov chains over n and m states and the joint model over n+m states, respectively. The total distortion between
Π and Θ and hence Π and Φ is
!
n
X
q(Rπ , Rϕ ) = minΘ∈Rm×n
p(i)g(πi,1:n , ϑψ(i),1:n ) Rϑ ∈ Rπϕ
+

i=1

for some unit-sum weights p(i). We can take these weights to be the invariant distribution of the original Markov
chain, i.e., p(i) = γi .
For this objective function we have the constraint that Rπϕ must be a member of the set of all joint models
for Rπ and Rϕ .
It can be seen from definition 3.5 that the total distortion is over the set of all possible binary partitions. We, however,
seek the best binary partition. Best, in this context, means that it would yield an Rϕ with the least total distortion to
Rπ . It hence would lead to a lower-order model Rϕ that most resembles Rπ according to the chosen measure g.

SUBMITTED TO ENTROPY

7

Definition 3.6. Let Rπ = (Vπ , Eπ , Π) and Rϕ = (Vϕ , Eϕ , Φ) be transition models of two Markov chains
>
>
>
over n and m states, respectively. The accumulation matrix Φ = [ϕ>
1,1:m , ϕ2,1:m , . . . , ϕm,1:m ] for Rϕ that
n
n
achieves the least total distortion to Π of Rπ , according to g : R+ × R+ → R+ , is given by
!
X
ei .
arg minΨ∈Rn×m , Φ∈Rm×m q(Rπ , Rϕ ) [Ψ]1:n,k =
+

+

i∈ψ −1 (k)

Here, q : Rn×n
× Rm×m
→ R+ is the total distortion.
+
+
At least one minimizer exists for both assessing total distortion and least total distortion. This is because both are
continuous functions operating on closed and bounded sets and hence, according to the Weierstrass extreme value
theorem, obtain both a maximum and minimum on those sets.
From definition 3.6, we can now specify the optimization problem of aggregating a Markov chain described. This
problem can be solved in a two-step process. The first step entails finding the optimal partition that leads to the least
total distortion between the original chain Π and Φ, as described by Θ. The second step involves constructing the
corresponding low-order transition matrix Φ from Θ and Ψ.
Definition 3.7. Let Rπ = (Vπ , Eπ , Π) and Rϕ = (Vϕ , Eϕ , Φ) be transition models of two Markov chains
over n and m states, respectively. The optimal reduced-order transition model Rϕ with respect to the original
model Rπ can be found as follows
(i) Optimal partitioning: Find a binary partition matrix Ψ that leads to the least total distortion between the
models Rπ and Rϕ . As well, find the corresponding weighting matrix Θ that satisfies
!
n
X
X
arg minΨ∈Rn×m , Θ∈Rm×n
p(i)g(πi,1:n , ϑψ(i),1:n ) Rϑ ∈ Rπϕ , [Ψ]1:n,k =
ei .
+

+

i=1

i∈ψ −1 (k)

Solving this problem has the effect of partitioning the n vertices of the relational matrix Rπ into m groups.
(ii) P
Transition matrix construction: Obtain the transition matrix for Rϕ from the following expression:
ϕj,k = i∈ψ−1 (k) ϑj,k using the optimal weights Θ and the binary partition matrix Ψ from step (i).
It is important to notice for the first step in definition 3.7 that there is no efficient way to find a Rϕ with least total
distortion to Rπ . This is due to the binary nature of the partitions, which leads to a problem with an NP-hard computational complexity. For practical problems, which may contain thousands or even millions of states, this aggregation
procedure will not be tractable. A more efficient alternative is therefore required.
3.2 Approximately Aggregating Markov Chains
3.2.1 Preliminaries

A straightforward way to make the aggregation problem more efficient is by approximating the least total distortion optimization given in definition 3.6. This can be effectuated by relaxing the constraint that the state-state
assignments specified by the partition matrix are binary. Instead, each state from the high-order chain can have a
chance to map to states in the low-order chain. Such changes lead to the notion of a probabilistic partition matrix.
Definition 3.8. Let Rπ = (Vπ , Eπ , Π) and Rϕ = (Vϕ , Eϕ , Φ) be transition models of two Markov chains
over n and m states, respectively. A probabilistic partition function ψ is a surjective mapping between two state
index sets, Z1:n and Z1:m , such that ψ −1 (Z1:m ) is a partition of Z1:n , which has a given probabilistic chance of
m×n
occurring. That is, ψ −1 (j) ⊂ Z1:n × Rn+ is not empty and where ψ −1 (1) ∪ . . . ∪ ψ −1 (m) = Zm
, with
1:n × R+
the real-valued responses being non-negative and summing to one.
The probabilistic partition of a state index set induces a probabilistic partition matrix [Ψ]i,j = ψi,j , where
ψi,j = ζ if i ∈ ψ −1 (j) occurs with probability ζ. The set of allP
probabilistic partition matrices for the two chains
m
specified above is given by {Ψ ∈ Rn×m
|[Ψ]
=
ψ
∈
[0,
1],
i,j
i,j
+
j=1 ψi,j = 1}.
An example of a probabilistic partitioning is given in figure 3.3.
As before, we will partition and compare the dynamics for pairs of chains according to rows of the corresponding
stochastic transition matrices. We will, therefore, still encounter issues when trying to compare transition models Rπ
and Rϕ with differing state spaces. We again consider the construction of a joint model Rϑ to avoid this issue. The
only difference between this joint model and the one defined for binary-valued partitions is that the weighting matrix
has a different form. The connectivity of the joint-space graph can hence be different.
Definition 3.9. Let Rπ = (Vπ , Eπ , Π) and Rϕ = (Vϕ , Eϕ , Φ) be transition models of two Markov chains over
n and m states, respectively, where m < n. Rϑ = (Vϑ , Eϑ , Θ) is a joint model, with m+n states, that is defined by
(i) A vertex set Vϑ = Vπ ∪ Vϕ , which is the union of all state vertices in Rπ and Rϕ .
(ii) An edge set Eϑ ⊂ Vϕ ×Vπ , which are one-to-many mappings from the states in the original transition
model Rπ to the reduced-order transition model Rϕ .

SUBMITTED TO ENTROPY

8

Π ∈ R9×9
+
π1,1:9

0.000 0.049 0.197 0.165 0.185 0.115 0.091 0.106 0.088

π2,1:9

0.050 0.000 0.179 0.153 0.181 0.140 0.126 0.084 0.084

π3,1:9

0.197 0.176 0.000 0.038 0.056 0.176 0.150 0.093 0.110

π4,1:9

0.209 0.191 0.049 0.000 0.046 0.142 0.175 0.088 0.097

ϑ2,1:9

Φ ∈ R4×4
+
0.100 0.419 0.252 0.226

π5,1:9

0.205 0.197 0.062 0.040 0.000 0.120 0.154 0.111 0.108

π6,1:9

0.136 0.163 0.178 0.133 0.129 0.000 0.040 0.130 0.087

g(π2,1:9, ϕ1,1:4)
g(π2,1:9, ϑ1,1:9)

ϕ1,1:4

0.327 0.130 0.309 0.232

ϕ2,1:4

0.239 0.380 0.117 0.262

ϕ3,1:4

0.237 0.320 0.284 0.157

ϕ4,1:4

=

ϑ4,1:9

ψ2,1:9

ψ4,1:9

0.027 0.202 0.118 0.141

0.758 0.045 0.096 0.099

0.028 0.186 0.148 0.121

0.688 0.063 0.086 0.160

0.186 0.037 0.184 0.144

0.063 0.709 0.076 0.150

0.157 0.029 0.141 0.104

0.032 0.828 0.043 0.095

0.181 0.036 0.141 0.140
0.127 0.137 0.024 0.132

×

0.063 0.687 0.105 0.143
0.056 0.057 0.809 0.076

π7,1:9

0.101 0.137 0.195 0.152 0.154 0.038 0.000 0.131 0.090

0.109 0.168 0.025 0.144

0.076 0.047 0.801 0.074

π8,1:9

0.148 0.114 0.130 0.096 0.140 0.152 0.165 0.000 0.051

0.095 0.097 0.128 0.035

0.055 0.065 0.043 0.835

π9,1:9

0.136 0.128 0.170 0.117 0.150 0.113 0.125 0.057 0.000

0.086 0.105 0.088 0.035

0.127 0.112 0.143 0.616

ϑ1,1:9
>

ϑ3,1:9

U Π ∈ R4×9
+

ψ1,1:9

ψ3,1:9

4×9
Ψ > ∈ R+

Figure 3.3: Depiction of the comparison process for approximate, probabilistic aggregation of a nine-state Markov chain. Here, we want to compare
the second row of the high-order transition matrix Π with the first row of a potential low-order stochastic matrix Φ. We show the transition matrix Π
associated with a nine-state Markov chain on the left; four state clusters are visible along the main diagonal. The corresponding low-order transition
matrix Φ for a four-state chain is given in the right. As before, comparisons between Π and Φ occur by comparing rows of Π with rows of ΘΨ. Φ
can be found via the joint model weight matrix Θ = U > Π and the probabilistic partition Ψ: Φ = U > ΠΨ. The least expected distortion between the
high-order Π and low-order Φ transition matrices is determined by way of Θ and Ψ.
When performing exact aggregation, the dynamics of Φ are directly encoded in Θ. Ψ is only used to determine which columns of Θ can be ignored.
For approximate aggregation, the dynamics of Φ are split between Θ and Ψ. This is because each state in the high-order model can have have the
chance to map to multiple states in the low-order model.

(iii) A weighting matrix Θ ∈ Rm×n
. The partition function ψ provides
+
Pna relationship between the stochastic matrices Φ and Θ of Rϕ and Rϑ , respectively. This is given by ϕi,j = k=1 ϑk,j ψk,i ∀i, j, or, rather,
Φ = ΘΨ, where Ψ ∈ Rn×m
is the probabilistic partition matrix.
+

An illustration of this joint model is given in figure 3.4 for the stochastic matrix presented in figure 3.3. Unlike the
joint model for binary-valued partitions, using probabilistic partitions allows for each state in the high-order chain to
map to multiple states in the low-order chain.
3.2.2 Partitioning Process and State Aggregation

For any transition model Rπ we, again, would like to find a joint model Rϑ that facilitates the construction of
another transition model Rϕ . Rϕ should have fewer states than Rπ while still possessing similar intra-group transition dynamics. Since we are now considering probabilistic partitions, we instead seek a Θ with the least expected
distortion to Π to ensure that the dynamics of Φ largely match those of Π. Definition 3.5 is hence modified as follows.
Definition 3.10. Let Rπ = (Vπ , Eπ , Π), Rϕ = (Vϕ , Eϕ , Φ), and Rϑ = (Vϑ , Eϑ , Θ) be transition models of two

Markov chains over n and m states and the joint model over n+m states, respectively. The least expected
distortion between Π and Θ and hence Π and Φ is
!
Pm
n X
m
X
0 ≤ ϑi,k , ψP
ϑ
=
1,
i,k ≤ 1,
i,k
k=1
q(Rπ , Rϕ ) = minΨ∈Rn×m , Θ∈Rm×n
γi ψi,j g(πi,1:n , ϑj,1:n )
.
m
+
+
k=1 ψi,k = 1
i=1 j=1

There are few constraints on the probabilistic partitions in definition 3.10, which can make finding viable solutions difficult. To address this issue, we impose that the partitions should minimize the information loss associated
with the state quantization process. That is, the mutual dependence between states in the high-order and low-order
chains should be maximized with respect to a supplied upper bound. Simultaneously, the least expected distortion, for
this supplied bound, should be achieved.
Aggregating Markov chains in this fashion can be done via a two-step process similar to that in definition 3.7.
Definition 3.11. Let Rπ = (Vπ , Eπ , Π) and Rϕ = (Vϕ , Eϕ , Φ) be transition models of two Markov chains

over n and m states, respectively. The optimal reduced-order transition model Rϕ with respect to the original
model Rπ can be found as follows:
(i) Optimal partitioning: Find a probabilistic partition matrix Ψ that leads to the least expected distortion
between the models Rπ and Rϕ . As well, find the corresponding weighting matrix Θ that satisfies
!
Pm
Pn
n X
m
X
j=1 αj
i=1 ψi,j log(ψi,j /γi ) ≤ r,
arg minΨ∈Rn×m , Θ∈Rm×n
γi ψi,j g(πi,1:n , ϑj,1:n )
Pm
Pm
+
+
0 ≤ ϑi,k , ψi,k ≤ 1, k=1 ϑi,k = 1, k=1 ψi,k = 1
i=1 j=1

SUBMITTED TO ENTROPY

9

ϑ1,x =

0.
2
0. 29
32
0

0.4
0.4 62
35

0.084
0.128
0.
0. 088
13
6

0.2
0.3 20
26

0.
0. 040
03
8

0.0
0.1 84
14

0.1
5
0.1 3
91

0.1
7
0.1 9
76

0.
0. 093
13
0
0.08
8
0.09
6

?.???
ϑ1,x = ?.???

(Vπ , Eπ , Π), Π ∈ R9×9

ϑ1,x = ?.???
?.???

ϑ1,x =

?.???
?.???

vϕ2

ϑ1,x = ?.???
?.???
?.???
ϑ1,x =
?.???

ϑ1,x =

?.???
?.???

ϑ1,x =

(a) Original (left) and reduced-order (right) transition model

vϕ4

vϕ3
vπ6

ϑ1,x = ?.???
?.???
?.???
ϑ1,x = ?.???

vπ5

(Vϕ, Eϕ, Φ), Φ ∈ R4×4

?.??
?
?.??
?

,5 =

?.??
?
?.??
?

x=

?.???
ϑ 1,x = ?.???

0.344
0.394

4
0.15
4
0.15

ϑ1
ϑ1,

vϕ1

?.?
??
?.?
??

,x =

vπ7

??
?.? ?
= ?.??

0.150
0.178

vπ3

vπ4

ϑ1

?.???
?.???

ϑ 1,x

0
0.12
9
0.12

vπ5

vϕ3

vϕ2

0.145
0.133

ϑ1,x =

vπ9

??
?.? ?
= ?.??

vπ6

0.2
7
0.3 3
53

?.???
?.???

ϑ 1,x

vπ7

0.1
3
0.1 0
52

?
?.??
= ?.???

0.09
0
0.12
5

ϑ1,x =

vπ8

?.???
ϑ 1,x = ?.???

vπ1

ϑ 1,x

vϕ4

0.307
0.332

0.091
0.101

0.142
0.133

vπ4

0.120
0.129

0.0
0.0 56
62

vπ9

97
0.0 17
0.1

26
0.1 7
3
0.1

vπ3

0.038
0.049

vπ1

?.???
?.???
?.???
ϑ1,x = ?.???

ϑ1,x = ?.???
?.???

vπ2

vϕ1

06
0.1 8
4
0.1

0.0
5
0.0 7
51

vπ8

?.???
?.???

ϑ1,x =

0.04
9
0.05
0

vπ2

?.???
?.???

4×9
(Vϑ, Eϑ, Θ); Θ ∈ R3×9
+ , P ∈ R+
(b) Joint model

Figure 3.4: Depictions of the various models for the transition matrices in figure 3.3 when using probabilistic partitions. In (a), we show the
transition model for a high-order, nine-state Markov chain (right) and its low-order, four-state transition model representation (left) after the
approximate aggregation process. In (b), we show the joint model defined by Θ = U > Π and a relatively low value of β for this example. As before,
each edge in both chains is removed and mappings between states in the two chains are established. For probabilistic partitions, each state in the
high-order chain have the chance to map to more than one state in the low-order chain. This contrasts with the binary-valued partition case, where
each state in the high-order chain could only be associated with a single state in the low-order chain.

Pn
for some positive value of r; r has an upper bound of − i=1 γi log(γi ). The variables α, γ, and ψ all have
probabilistic interpretations: αj = p(vϕj ) and γi = p(vπi ) correspond to marginal probabilities of states vϕj and
vπi , while ψi,j = p(vϕj |vπi ) is the conditional probability of state vπi mapping to state vϕj .
(ii)P
Transition matrix construction: Obtain the transition matrix for Rϕ from the following expression:
n
ϕi,j = k=1 ϑk,j ψk,i using the optimal weights Θ and the probabilistic partition matrix Ψ from step (i).

The optimization problem presented in definition 3.11 trades off between the minimum expected distortion and the
information contained by the states in the low-order chain Rφ about those in the original, high-order chain Rπ after
partitioning. It is hence describing the value of quantizing the high-order model by a certain amount [16, 17]; this
is the value of information formulated for Markov chains, which is, itself, an analogue of rate-distortion theory [18].
Coarsely quantizing Π, as dictated by the parameter r, leads to a parsimonious low-order stochastic matrix Φ that may
not greatly resemble the dynamics of Π. Finely quantizing Π, again determined by r, yields a Φ that is similar to the
high-order model’s transition matrix Π yet may contain many redundant details. P
Pn
m
In the value of information, the role of the Shannon mutual information term j=1 αj i=1 ψi,j log(ψi,j /γi )
is to impose a certain level of randomness, or uncertainty, in the partition matrix to ensure that the entries can be
non-binary.
Pn PmA similar effect could be achieved by considering a Shannon entropy constraint on the partition matrix
− i=1 j=1 ψi,j log(ψi,j ). However, a Shannon entropy constraint is rather non-restrictive on the entries of the
partition matrix: there is the potential that a given row ψ1:n,j ∈ Rn+ could be a duplicate of another, thereby overinflating the number of states in the reduced-order chain and leading to a poor aggregation. We have found, empirically, that Shannon mutual information does not share this defect, except when all states have a uniform chance of
being grouped together in every group. This is because we are bounding the informational overlap between the original and aggregated states. Coincident partitions often violate this bound. In the Shannon-entropy case, however, we
are only bounding the uncertainty on the entries of the partition, so there is no direct constraint between the original
and aggregated states.
Definitions 3.8, 3.9, and 3.11 provide a means of approximating the computationally intractable aggregation
process outlined in the previous section. Actually solving the constrained optimization problem in definition 3.11
can be efficiently performed in a few ways. Here, we opt to form the Lagrangian and differentiate it. This provides an
expectation-maximization-like procedure for specifying the probabilistic partitions Ψ.
Proposition 3.1. For a transition model Rπ = (Vπ , Eπ , Π) over n states and a joint model Rϑ = (Vϑ , Eϑ , Θ)
and m+n states, the Lagrangian of the relevant terms for the minimization problem given in definition 3.11 is
F (Ψ, α; Π, Θ, γ) = E[E[g(Π, Θ)|Ψ]|γ]−E[DKL (γkΨ)]/β, or, rather,
!
!
n X
m
m
n
X
1 X X
F (Ψ, α; Π, Θ, γ) =
γi ψi,j g(πi,1:n , ϑj,1:n ) −
αj
ψi,j log(ψi,j /γi ) .
β j=1 i=1
i=1 j=1

Here, β ≥ 0 is a Lagrange multiplier that emerges from the Shannon mutual information constraint in the value of

SUBMITTED TO ENTROPY

10

information.
Probabilistic partitions [Ψ]i,j = ψi,j , which are local solutions of ∇F (Ψ, α; Π, Θ, γ) = 0, can be found by the
following expectation-maximization-based alternating updates
!, m
!
n
X
X
−βg(πi,1:n ,ϑj,1:n )
−βg(πi,1:n ,ϑp,1:n )
αj ←
γi ψi,j ,
ψi,j ← αj e
αp e
,
i=1

p=1

which are iterated until convergence.

Proposition 3.2 shows that the alternating optimization updates in proposition 3.1 yield monotonic decreases in the
modified free-energy associated with the value of information. Global convergence to solutions can therefore be
obtained. Proposition 3.3 bounds the approximation error as a function of the number of alternating-optimization
iterations. Linear-speed convergence to solutions is hence obtained, which coincides with the interpretation of the
updates as an expectation-maximization-type algorithm.
Proposition 3.2. Let Rπ = (Vπ , Eπ , Π) and Rϕ = (Vϕ , Eϕ , Φ) be transition models of two Markov chains
∗
over n and m states, respectively, where m < n. If [Ψ∗ ]i,j = ψi,j
is an optimal probabilistic partition and
∗
∗
[α ]j = αj an optimal marginal probability vector, then, for the updates in proposition 3.1, we have that:
(i) The approximation error is non-negative
!
!
Pm
n
∗ −βg(πi,1:n ,ϑj,1:n )
X
j=1 αj e
(k)
(k)
∗
∗
F (Ψ , α ; Π, Θ, γ) − F (Ψ , α ; Π, Θ, γ) =
γi log Pm
≥ 0.
(k) −βg(πi,1:n ,ϑj,1:n )
i=1
j=1 αj e

(ii) The modified free energy monotonically decreases F (Ψ(k) , α(k) ; Π, Θ, γ) ≥ F (Ψ(k+1) , α(k+1) ; Π, Θ, γ)
across all iterations k.
(iii) For any K ≥ 1, we have the following bound for the sum of approximation errors
!
!
K
n X
m
∗
X
X
ψi,j
(k)
(k)
∗
∗
∗
F (Ψ , α ; Π, Θ, γ) − F (Ψ , α ; Π, Θ, γ) ≤
γi ψi,j log
.
(1)
ψi,j
i=1 j=1
k=1

Here, F (Ψ, α; Π, Θ, γ) = E[E[g(Π, Θ)|Ψ]|γ]−E[DKL (γkΨ)]/β is the Lagrangian.

Proposition 3.3. Let Rπ = (Vπ , Eπ , Π) and Rϕ = (Vϕ , Eϕ , Φ) be transition models of two Markov chains
∗
over n and m states, respectively, where m < n. If [Ψ∗ ]i,j = ψi,j
is an optimal probabilistic partition and
∗
∗
[α ]j = αj an optimal marginal probability vector, then, the approximation error
!
!
n m
∗
ψi,j
1 XX
∗
∗
∗
(k)
(k)
γi ψi,j log
F (Ψ , α ; Π, Θ, γ) − F (Ψ , α ; Π, Θ, γ) ≤
.
(1)
k i=1 j=1
ψ
i,j

falls off as a function of the inverse of the iteration count k. Here, the constant factor of the error bound is a
Kullback-Leibler divergence between the initial partition matrix Ψ(1) and the global-best partition matrix Ψ∗ .
As shown in proposition 3.1, a Lagrange multiplier β is introduced to account for the mutual information constraint. The effects of β are as follows. As β tends to zero, minimizing the Lagrangian is approximately same as
minimizing the negative Shannon information. The information loss associated with the quantization process takes
precedence, albeit at the expense of a potentially poor reconstruction. In this case, there are few state clusters defined
by the partition; that is, there are few rows in Θ. Every state in the high-order transition model Rπ has an almost uniform chance to map to each state in the low-order model Rϕ . The alternating updates from proposition 3.1 yield a
global minimizer of the value of information, which follows from the convexity of the dual criterion and the Picarditeration theory of Zangwill [44].
As β is increased, the probabilistic partitions become more binary. Higher probabilities are therefore assigned for
a state in Rπ to map to either a small set of states or a single state in Rϕ . This is because the effects of the Shannon
information term are increasingly ignored in favor of achieving the minimum expected distortion. The value of information problem given in definition 3.11 therefore approaches the binary aggregation problem from definition 3.7. An
increasing number of clusters are formed by the partition matrix, which increases the number of rows in the weighting
matrix Θ and hence Φ. When β tends to infinity, we obtain a completely binary partition matrix. We hence recover the
least total distortion function given in definition 3.6. This binary partition can contain as many clusters as states in the
high-order model Rπ ; that is, no aggregation may be performed, so Rϕ is typically equal to Rπ .
The number of state clusters in the high-order chain, or, rather, the number of distinct rows of the weight matrix
Θ, does not increase continuously as a function of β. Instead, it increases only for certain critical values of β where a
bifurcation occurs in the underlying gradient flow of the Lagrangian. Critical values of β can be explicitly determined
when using the negative Kullback-Leibler divergence by looking at the second derivative of the Lagrangian at Θ.

SUBMITTED TO ENTROPY

11

β ∗ = 0.119

β = 0.794

π1,1:9

0.000 0.020 0.127 0.115 0.106 0.178 0.156 0.148 0.149

0.000 0.020 0.127 0.115 0.106 0.178 0.156 0.148 0.149

0.000 0.020 0.127 0.115 0.106 0.178 0.156 0.148 0.149

π2,1:9

0.018 0.000 0.133 0.122 0.113 0.178 0.158 0.151 0.127

0.018 0.000 0.133 0.122 0.113 0.178 0.158 0.151 0.127

0.018 0.000 0.133 0.122 0.113 0.178 0.158 0.151 0.127

π3,1:9

0.198 0.226 0.000 0.020 0.035 0.085 0.063 0.049 0.324

0.198 0.226 0.000 0.020 0.035 0.085 0.063 0.049 0.324

0.198 0.226 0.000 0.020 0.035 0.085 0.063 0.049 0.324

π4,1:9

0.186 0.215 0.021 0.000 0.016 0.105 0.077 0.062 0.319

0.186 0.215 0.021 0.000 0.016 0.105 0.077 0.062 0.319

0.186 0.215 0.021 0.000 0.016 0.105 0.077 0.062 0.319

π5,1:9

0.168 0.197 0.035 0.015 0.000 0.118 0.088 0.074 0.304

0.168 0.197 0.035 0.015 0.000 0.118 0.088 0.074 0.304

0.168 0.197 0.035 0.015 0.000 0.118 0.088 0.074 0.304

π6,1:9

0.209 0.228 0.064 0.076 0.087 0.000 0.029 0.035 0.271

0.209 0.228 0.064 0.076 0.087 0.000 0.029 0.035 0.271

0.209 0.228 0.064 0.076 0.087 0.000 0.029 0.035 0.271

π7,1:9

0.219 0.241 0.057 0.066 0.078 0.035 0.000 0.013 0.291

0.219 0.241 0.057 0.066 0.078 0.035 0.000 0.013 0.291

0.219 0.241 0.057 0.066 0.078 0.035 0.000 0.013 0.291

π8,1:9

0.220 0.244 0.046 0.056 0.069 0.045 0.014 0.000 0.305

0.220 0.244 0.046 0.056 0.069 0.045 0.014 0.000 0.305

0.220 0.244 0.046 0.056 0.069 0.045 0.014 0.000 0.305

π9,1:9

0.098 0.091 0.136 0.129 0.125 0.151 0.136 0.135 0.000

0.098 0.091 0.136 0.129 0.125 0.151 0.136 0.135 0.000

0.098 0.091 0.136 0.129 0.125 0.151 0.136 0.135 0.000

1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 0.000

1.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000

1.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000

ψ1,1:9

0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000

0.000 0.000 1.000 1.000 1.000 1.000 1.000 1.000 0.000

0.000 0.000 0.514 0.512 0.508 0.509 0.509 0.509 0.000

ψ2,1:9

0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000

0.000 0.000 0.481 0.479 0.475 0.488 0.488 0.488 0.000

ψ3,1:9

Ψ> ∈ R3×9
+

0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000

ψ4,1:9

Π ∈ R9×9
+

β = 0.095

Ψ

>

∈ R2×9
+

Ψ> ∈ R4×9
+

Figure 3.5: An illustration of the phase change property when the Lagrange multiplier β is increased above three critical values. For 0 ≤ β < 0.095,
all of the states in the original chain are grouped together. As β is slightly increased beyond this upper threshold, a new state group emerges, as we
highlight on the left-hand side of the figure. For any 0.095 ≤ β < 0.119 only two state groups are formed. As β is increased to β ≥ 0.119 and
β ≥ 0.794 three and four state groups are formed, respectively; these results are shown in the middle and right-hand side of the figure. The ‘optimal’
value of β, predicted by our perturbation-theory results, is close to β = 0.119. This yields a parsimonious aggregation where the state-groups are
compact and well separated. For β ≥ 0.794, the original chain is over-partitioned: near-coincident clusters are defined in Ψ. The value of
information hence starts to fit more to the noise in the state transitions than to the well-defined state groupings as β is increased beyond the next
critical point after the ‘optimal’ value.

Proposition 3.4. Let Rπ = (Vπ , Eπ , Π) and Rϕ = (Vϕ , Eϕ , Φ) be transition
Pn models of two Markov chains
over n and m states, respectively, where m < n. Let g(πi,1:n , ϑi,1:n ) = j=1 γi πi,j log(πi,j /ϑi,j ), where
Rϑ = (Vϑ , Eϑ , Θ) is the joint model. The following hold
(i) The transition matrix Φ of aP
low-order Markov chain over states m is given by Φ = ΘΨ, where
n
Θ = U > Π. Here, [U ]i,j = γi ψi,j / k=1 γk ψk,j for the probabilistic partition matrix [Ψ]i,j = ψi,j found using
the updates in proposition 3.1.
(ii) Suppose that we have a low-order chain over m states with a transition matrix Φ and weight matrix Θ
given by (i). For some β0 , suppose Θβ0 , the matrix Θ for that value of β0 , satisfies
the following inequality
Pm
m×n
2
2
>
d
/d
F
(Ψ,
α;
Π,
Θ
+Q,
γ)|
>
0.
Here,
Q
∈
R
is
a
matrix
such
that
β0
=0
+
k=1 qk,1:n qk,1:n = 1 and
Pn
2
2
j=1 qi,j = 0 ∀i. A critical value βc , βc = minβ>β0 (d /d F (Ψ, α; Π, Θβ +Q, γ)|=0 ≤ 0), occurs
whenever the minimum eigenvalue of the matrix
!
!
n
n
X
X
diag
ψk,j πi,1:n /ϑ2k,1:n − β
ψk,j (πi,1:n /ϑ2k,1:n )(πi,1:n /ϑ2k,1:n )>
i=1

i=1

is zero. The number of rows in Θ and columns in Ψ needs to be increased for β > βc .

Proposition 3.4 illustrates a major advantage of the value of information cost function for partitioning Markov
chains: the number of states in a low-order model does not need to be manually specified. It is dictated implicitly by
the value of the Lagrange multiplier β that captures the effects of favoring information retainment over achieving a
minimal expected distortion. This automatic increase in the number of state groups is depicted in figure 3.5.
Choosing a good value for β is crucial for practical problems. There are a variety of ways to do this. One such
approach entails applying perturbation theory to obtain an upper bound on β. More specifically, it is known that
measurements of Shannon mutual information are always, on average, improperly estimated when considering finite
samples [45]. That is, for finitely sized state spaces, the probability distributions that comprise the mutual information
expression are approximating, thereby leading to errors that propagate into the aggregation process. Our approach
therefore entails modeling this perturbation error and removing it from the value of information. This leads to a
modified criterion for which a value of β can be determined that minimizes the estimation error and better fits to the
structure of the transition matrix. Such values typically correspond to the beginning of an asymptotic region of the
original value of information expression where favoring a minimum expected distortion over information loss leads to
negligible improvements.
Proposition 3.5. Let Rπ = (Vπ , Eπ , Π) and Rϕ = (Vϕ , Eϕ , Φ) be transition models of two Markov chains over

n and m states, respectively, where m < n. Rϑ = (Vϑ , Eϑ , Θ) is a joint model, with m+n states. The system-

SUBMITTED TO ENTROPY

12

atic underestimation of the information cost of the Shannon mutual information term in definition 3.11 can be
second-order minimized by solving the following optimization problem
!
Pm
Pn
Pn Pm
j=1 αj
i=1 ψi,j log(ψi,j /γi )
i=1
j=1 γi ψi,j g(πi,1:n , ϑi,1:n ) ≤ r,
Pm Pn
minΨ∈Rn×m , Θ∈Rm×n
P
Pm
2
+
+
+ j=1 i=1 γi ψi,j
/2nlog(2)αj 0 ≤ ϑi,k , ψi,k ≤ 1, m
k=1 ϑi,k = 1,
k=1 ψi,k = 1

where β = 2 j=1 αj i=1 ψi,j log(ψi,j /γi )/2n.
This corrected version of the value of information
rescaled
slope compared to the original, where a lower bound
Pmhas aP
n
on the rescaling is given by log(2)/β −log(2)2 j=1 αj i=1 ψi,j log(ψi,j /γi ) /2βn.
Pm

Pn

3.2.3 Expected Aggregation Performance

The preceding theory outlines how Markov chains can be aggregated by trading off between expected distortion
and expected relative entropy. We have shown that global-optimal solutions can be uncovered. However, we have not
bounded the aggregation quality of those solutions for arbitrary problems; such bounds are important for understanding how our approach will behave in general.
Toward this end, we quantify the relationship between stationary distributions of the original and reduced-order
stochastic matrix for nearly-completely-decomposable systems. Many practical examples of Markov chains are
typically nearly-completely-decomposable: there are groups of states that possess similar transition dynamics where
the chance to jump between states within the group is higher than states outside of the group.
Definition 3.12. The transition model of a first-order, homogeneous, nearly-completely-decomposable Markov

chain is a weighted, directed graph Rπ given by the three-tuple (Vπ , Eπ , Π) with the following elements
(i) A set of n vertices Vπ = vπ1 ∪ . . . ∪ vπn representing the states of the Markov chain.
(ii) A set of n×n edge connections Eπ ⊂ Vπ ×Vπ between reachable states in the Markov chain.
(iii) A stochastic transition matrix Π ∈ Rn×n
+ . Here, [Π]i,j = πi,j represents the non-negative transition
probability between states i and j. We impose the constraint that the probability of experiencing a state transition is independent of time. Moreover, for a block-diagonal matrix Π∗ with zeros along the diagonal, we have
that Π = Π∗ +εC. Here, Π∗ ∈ Rn×n
is a completely-decomposable stochastic matrix with m indecomposable
+
sub-matrix blocks Π∗i of order ni .
 ∗

Π1 0
0 ...
0
 0 Π∗2 0 . . .
0 


∗
0
∗
0 Π3 . . .
0 
Π =

 ..
..
..
.. 
..
 .
.
. 
.
.
Π∗m
Pni
P Pnj
Since Π and Π∗ are both stochastic, the matrix
C ∈ Rn×n
must satisfy k=1
cpi ,ki = − j6=i q=1
cpi ,qj
+
P
ni
|cpi ,ki |) = 1. Additionally, the maximum
degree
of
∀pi , for blocks Π∗i and Π∗j . That is, maxpi ( k=1
P Pnj coupling
between sub-systems Π∗i and Π∗j , given by the perturbation factor ε, must obey ε = maxi ( i6=j q=1
πpi ,qj ).
0

0

0

...

Proposition 3.6. Let Rπ = (Vπ , Eπ , Π) be a transition model of a Markov chain with n states, where Π ∈ Rn×n
+

is nearly completely decomposable into m Markov sub-chains.
m×m
(i) The associated
found by solving the value of information
Pnlow-order
Pnj stochastic matrix
Pni Φ ∈ R+
i
is given by ϕi,j = pi =1 qj =1 πpi ,qi γpi / qi γqi , where pi , qi represent state indices p = 1, . . . , ni
associated with block i, while qj represents a state index q = 1, . . . , nj into block j. The variable γpi = γpi (Π)
denotes the invariant-distribution
probability of state p in block i of Π.
Pn
(ii) Suppose that γpi / qii γqi = vp∗i (1i ) is approximated by the entries of the first left-eigenvector v ∗ (1i )
for block i of Π∗ . We then have that
!
nj
ni
X
X
∗
γ
vpi (1i )
πpi ,qi − γ(Π)Ψ ∼ O(ε2 )
pi =1

qj =1

1

where the first term is the invariant distribution of the low-order matrix γ(Φ), under the simplifying assumption, and Ψ ∈ Rn×m
is the probabilistic partition matrix found by solving the value of information.
+

The preceding proposition elucidates the behavior of the value-of-information aggregation results: a reduced Markov
chain will have similar long-run dynamics as a projected version of the original Markov chain. This result is made
possible by the work of Simon and Ando [20]. They proved that, for nearly-completely-decomposable chains, there
are two types of dynamics that influence the stationary distribution: short and long term. In the short term, each
completely-decomposable block evolves almost independently towards a local equilibrium, as if the system was

SUBMITTED TO ENTROPY

13

completely decomposable. In the long run, the entire aggregated chain moves toward the steady state defined by the
first left-eigenvalue of the original stochastic matrix. The equilibrium states attained for each block of the original
stochastic matrix are approximately the same as those for the short-run dynamics.
More specifically, the local-equilibrium states for the short-term may dynamics be closely approximated by the
steady-state vectors of the sub-systems for the completely decomposable stochastic matrix Π∗ . The macro-transition
probability between blocks Πi and Πj of Π remain, in the long term, more or less constant in time and are approximately equal to Φ. Hence, the elements of the steady-state probability vector γ1:n (1i ), where γ1:n (1i )(Φ−In×n ) = 0,
are so-called macro-variables that yield good approximations to the steady-state probabilities of being in any one
state of block Πi . The so-called micro-variables γpi (1i ) = γi (1)vp∗i (1I ) are good approximations to the steady-state
probabilities vpi (1i ) of being in any particular state p of block i. That is, as we showed in the previous section, both
the macro- and micro-variables have an `1 -norm error that is a square of the perturbation factor compared to those for
the original stochastic matrix. The aggregated chain will thus possess similar long-term dynamics as the original.
4 Simulations

In the previous section, we provided an information-theoretic criterion, the value of information, for quantifying the effects of quantizing stochastic matrices associated with Markov chains. We also provided a first-order approach for optimizing this criterion, which provides a mechanism for simultaneously partitioning and aggregating
chain states. In this section, we assess the empirical performance of this criterion. The aims of our simulations are
multi-fold. First, we ascertain how well the value of information reduces the complexity of Markov chains when they
possess either simple or complex state-transition dynamics. We also discuss various facets of the criterion within the
context of these results. We then gauge how well the results for the ‘optimal’ free-parameter value, as predicted via
perturbation theory, align with the ground truth. We also illustrate that using Shannon mutual information, versus
Shannon entropy, as a constraint for the expected-distortion objective function, avoids returning coincident partitions.
4.1 Simulation Protocols

For each of the examples that follow, we adopted the following simulation protocols for value-of-informationbased aggregation. We initialized the aggregation process with a partition matrix of all ones, Ψ = [1]9×1 , signifying
that each state belongs to a single group. This is the global optimal solution of Markov chain aggregation for both the
binary- and probabilistic-partition cases. For the latter case, it coincides with a parameter value β of zero for the value
of information. We then found the subsequent critical values of β and increased the column count of the partition
matrix Ψ. We determined which state group would be further split and modified both the new column and an existing
column of Ψ to randomly allocate the appropriate states. This initialization process bootstraps the quantization for
the new cluster and typically achieves convergence in only a few iterations. It also permits the value of information to
reliably track the global minimizer for the binary-partition aggregation problem case as β increases.
For certain problems, a priori specifying a fixed amount of partition updates may not permit finding a steady-state
solution. We therefore run the alternating updates until no entries of the partition matrix change across two iterations.
4.2 Simulation Results and Analyses
4.2.1 Value-of-Information Aggregation
Aggregation Performance. We establish the performance of value-of-information aggregation through two
examples. The first, shown in figure 4.1, corresponds to a Markov chain with nine states and four state groups with
strong intra-group interactions and weak inter-group interactions. This is a relatively simple aggregation problem.
The second example, presented in figure 4.2, is of a nine-state Markov chain with a single dominant state group and
six outlying states with near-equal transition probabilities. This is a more challenging problem than the first, as the
outlying states cannot be reliably combined without adversely impacting the mutual dependence. In both cases, the
transition probabilities were randomly generated through knowledge of a limit distribution γ.
In figures 4.1 and 4.2, we provide partitions and aggregated Markov chains for four critical values of the free parameter β. The ‘optimal’ value of β, as predicted by our perturbation-theory formulation of the value of information,
leads to four and seven state groups for the first and second examples, respectively. The associated partitions align
with an inspection of the dynamics of the stochastic matrix: the partitions separate states that are more likely to transition to each other from those that are not. The ‘optimal’ aggregated stochastic matrix encodes this behavior well. The
remaining aggregated chains do too for their respective partitions, as they all mimic the interaction dynamics of the
original chain for the given state groups. However, those partitions for ‘non-optimal’ βs either over- or under-quantize
the chain states, which is illustrated by the plot of expected distortion E[E[g(Π, Θ)|Ψ]|γ] versus the critical values
of β; these plots are given in figure 4.3. That is, for critical βs before the ‘optimal’ value, there is a steep drop in the
distortion, while the remaining βs only yield modest decreases. The ‘optimal’ value of β for both examples, in contrast, lies at the ‘knee’ of this curve, which is where the expected-distortion minimization, minΨ E[E[g(Π, Θ)|Ψ]|γ],

β = 0.0217 (m = 2)

β = 0.0848 (m = 3)
0.199 0.192 0.177 0.127 0.121 0.027 0.030 0.013 0.114
0.186 0.193 0.173 0.129 0.121 0.031 0.033 0.017 0.118

0.191 0.192 0.214 0.126 0.113 0.015 0.018 0.000 0.132

0.191 0.192 0.214 0.126 0.113 0.015 0.018 0.000 0.132

0.110 0.115 0.102 0.173 0.146 0.082 0.085 0.070 0.116

0.110 0.115 0.102 0.173 0.146 0.082 0.085 0.070 0.116

Π ∈ R9×9
+

0.199 0.192 0.177 0.127 0.121 0.027 0.030 0.013 0.114
0.186 0.193 0.173 0.129 0.121 0.031 0.033 0.017 0.118

0.108 0.111 0.093 0.149 0.178 0.093 0.095 0.081 0.092
0.030 0.035 0.015 0.105 0.117 0.223 0.219 0.207 0.047

0.108 0.111 0.093 0.149 0.178 0.093 0.095 0.081 0.092
0.030 0.035 0.015 0.105 0.117 0.223 0.219 0.207 0.047

0.033 0.038 0.018 0.107 0.118 0.216 0.219 0.201 0.050

0.033 0.038 0.018 0.107 0.118 0.216 0.219 0.201 0.050

0.017 0.022 0.000 0.101 0.113 0.231 0.228 0.249 0.039

0.017 0.022 0.000 0.101 0.113 0.231 0.228 0.249 0.039

0.125 0.133 0.135 0.147 0.113 0.046 0.050 0.034 0.218

0.125 0.133 0.135 0.147 0.113 0.046 0.050 0.034 0.218

0.000 0.000 0.000 0.000 0.000 1.000 1.000 1.000 0.000

0.663 0.337

0.000 0.000 0.000 0.000 0.000 1.000 1.000 1.000 0.000

0.663 0.045 0.292

1.000 1.000 1.000 1.000 1.000 0.000 0.000 0.000 1.000

0.143 0.857

1.000 1.000 1.000 1.000 1.000 0.000 0.000 0.000 0.000

0.145 0.741 0.114

Φ ∈ R2×2
+

0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000

0.218 0.130 0.652

Ψ> ∈ R3×9
+

3×3
Φ ∈ R+

Ψ> ∈ R2×9
+

Π ∈ R9×9
+

14

β ∗ = 0.1015 (m = 4)

β = 0.4816 (m = 5)

0.199 0.192 0.177 0.127 0.121 0.027 0.030 0.013 0.114

0.199 0.192 0.177 0.127 0.121 0.027 0.030 0.013 0.114

0.186 0.193 0.173 0.129 0.121 0.031 0.033 0.017 0.118

0.186 0.193 0.173 0.129 0.121 0.031 0.033 0.017 0.118

0.191 0.192 0.214 0.126 0.113 0.015 0.018 0.000 0.132

0.191 0.192 0.214 0.126 0.113 0.015 0.018 0.000 0.132

0.110 0.115 0.102 0.173 0.146 0.082 0.085 0.070 0.116

0.110 0.115 0.102 0.173 0.146 0.082 0.085 0.070 0.116

Π ∈ R9×9
+

Π ∈ R9×9
+

SUBMITTED TO ENTROPY

0.108 0.111 0.093 0.149 0.178 0.093 0.095 0.081 0.092
0.030 0.035 0.015 0.105 0.117 0.223 0.219 0.207 0.047

0.108 0.111 0.093 0.149 0.178 0.093 0.095 0.081 0.092
0.030 0.035 0.015 0.105 0.117 0.223 0.219 0.207 0.047

0.033 0.038 0.018 0.107 0.118 0.216 0.219 0.201 0.050

0.033 0.038 0.018 0.107 0.118 0.216 0.219 0.201 0.050

0.017 0.022 0.000 0.101 0.113 0.231 0.228 0.249 0.039

0.017 0.022 0.000 0.101 0.113 0.231 0.228 0.249 0.039

0.125 0.133 0.135 0.147 0.113 0.046 0.050 0.034 0.218

0.125 0.133 0.135 0.147 0.113 0.046 0.050 0.034 0.218

1.000 1.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000

0.571 0.245 0.062 0.121

1.000 1.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000

0.571 0.127 0.118 0.062 0.121

0.000 0.000 0.000 1.000 1.000 0.000 0.000 0.000 0.000

0.260 0.433 0.203 0.104

0.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000

0.173 0.327 0.146 0.146 0.197

0.000 0.000 0.000 0.000 0.000 1.000 1.000 1.000 0.000

0.071 0.221 0.663 0.045

0.000 0.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000

0.178 0.149 0.312 0.152 0.210

0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000

0.259 0.218 0.130 0.393

0.000 0.000 0.000 0.000 0.000 1.000 1.000 1.000 0.000

0.071 0.105 0.116 0.663 0.045

Ψ> ∈ R4×9
+

Φ ∈ R4×4
+

0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000

0.218 0.147 0.113 0.130 0.393

Ψ

>

∈ R5×9
+

5×5
Φ ∈ R+

Figure 4.1: Value-of-information-based aggregation for a 9-state Markov chain with four discernible state groups. We show the original stochastic
matrix Π ∈ R9×9 with the partitions Ψ ∈ Rm×9 overlaid for four critical values of β. We also show the resulting aggregation Θ ∈ Rm×m , which,
in each case, approximately mimics the dynamics of the original stochastic matrix.

is roughly balanced against the competing objective of state-mutual-dependence maximization with respect to some
bound, E[DKL (γkΨ)] ≤ r.
For both examples, we aggregated at critical values of β where the number of state groups increases. We also
considered non-critical values of β between two phase changes; a thousand Monte Carlo trials were conducted for
random βs. For each of these trials, the partitions produced between two related critical values were virtually identical, up to a permutation of the rows. Only minute, arithmetic-error-attributed differences were encountered. Such
results illustrate the validity of our theory: only a finite number of critical values for β need to be used for reducing
finite-cardinality stochastic matrices.
Convergence. In figure 4.3, we furnished plots of the decrease in the expected distortion, E[E[g(Π, Θ(k) )|Ψ(k) ]|γ]
across each iteration k = 1, 2, . . . This provides a means of gauging the per-iteration solution improvement and hence
convergence. We also provided plots of the partition matrix cross-entropy for consecutive iterations, E[−log(Ψ(k−1) )|Ψ(k) ].
The partition cross-entropy is a bounded measure of change between consecutive partitions and captures how greatly
the partition changed across a single update. Taken together, they offer alternate views of the aggregation improvement during intermediate stages of the dynamics reduction process. In either example, the average of these quantities
across the Monte Carlo trials exhibits a nearly-linear decrease in their respective quantities before plateauing, regardless the critical value of β. This finding suggests rapid convergence to the global solution, which was anticipated
from our convergence analysis. That is, due to how we initialize the partitions, we are roughly ensuring that they are

SUBMITTED TO ENTROPY

15

β = 0.0636 (m = 3)

0.214 0.154 0.145 0.141 0.072 0.063 0.063 0.062 0.086

0.214 0.154 0.145 0.141 0.072 0.063 0.063 0.062 0.086

0.155 0.214 0.136 0.096 0.062 0.068 0.068 0.072 0.129

0.155 0.214 0.136 0.096 0.062 0.068 0.068 0.072 0.129

0.112 0.098 0.214 0.119 0.100 0.092 0.092 0.089 0.083

0.112 0.098 0.214 0.119 0.100 0.092 0.092 0.089 0.083

0.137 0.089 0.147 0.214 0.111 0.085 0.085 0.076 0.055

0.137 0.089 0.147 0.214 0.111 0.085 0.085 0.076 0.055

Π ∈ R9×9
+

Π ∈ R9×9
+

β = 0.0349 (m = 2)

0.031 0.017 0.115 0.087 0.214 0.167 0.167 0.148 0.055
0.000 0.006 0.097 0.039 0.162 0.214 0.214 0.193 0.075

0.000 0.006 0.097 0.039 0.162 0.214 0.214 0.193 0.075

0.006 0.019 0.099 0.034 0.144 0.194 0.194 0.214 0.096

0.006 0.019 0.099 0.034 0.144 0.194 0.194 0.214 0.096

0.076 0.121 0.119 0.050 0.081 0.108 0.108 0.121 0.214

0.076 0.121 0.119 0.050 0.081 0.108 0.108 0.121 0.214

0.000 0.000 0.000 0.000 1.000 1.000 1.000 1.000 0.000

0.753 0.247

1.000 0.000 1.000 1.000 0.000 0.000 0.000 0.000 0.000

0.473 0.183 0.343

1.000 1.000 1.000 1.000 0.000 0.000 0.000 0.000 1.000

0.350 0.650

0.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000

0.301 0.338 0.361

Ψ> ∈ R2×9
+

Φ ∈ R2×2
+

0.000 0.000 0.000 0.000 1.000 1.000 1.000 1.000 0.000

0.159 0.087 0.753

Ψ> ∈ R3×9
+

3×3
Φ ∈ R+

β = 0.1130 (m = 4)

β ∗ = 2.2404 (m = 7)

0.214 0.154 0.145 0.141 0.072 0.063 0.063 0.062 0.086

0.214 0.154 0.145 0.141 0.072 0.063 0.063 0.062 0.086

0.155 0.214 0.136 0.096 0.062 0.068 0.068 0.072 0.129

0.155 0.214 0.136 0.096 0.062 0.068 0.068 0.072 0.129

0.112 0.098 0.214 0.119 0.100 0.092 0.092 0.089 0.083

0.112 0.098 0.214 0.119 0.100 0.092 0.092 0.089 0.083

0.137 0.089 0.147 0.214 0.111 0.085 0.085 0.076 0.055

Π ∈ R9×9
+

Π ∈ R9×9
+

0.000 0.006 0.097 0.039 0.162 0.214 0.214 0.193 0.075

0.031 0.017 0.115 0.087 0.214 0.167 0.167 0.148 0.055
0.000 0.006 0.097 0.039 0.162 0.214 0.214 0.193 0.075

0.031 0.017 0.115 0.087 0.214 0.167 0.167 0.148 0.055
0.000 0.006 0.097 0.039 0.162 0.214 0.214 0.193 0.075

0.137 0.089 0.147 0.214 0.111 0.085 0.085 0.076 0.055
0.031 0.017 0.115 0.087 0.214 0.167 0.167 0.148 0.055
0.000 0.006 0.097 0.039 0.162 0.214 0.214 0.193 0.075

0.000 0.006 0.097 0.039 0.162 0.214 0.214 0.193 0.075

0.000 0.006 0.097 0.039 0.162 0.214 0.214 0.193 0.075

0.006 0.019 0.099 0.034 0.144 0.194 0.194 0.214 0.096

0.006 0.019 0.099 0.034 0.144 0.194 0.194 0.214 0.096

0.076 0.121 0.119 0.050 0.081 0.108 0.108 0.121 0.214

0.076 0.121 0.119 0.050 0.081 0.108 0.108 0.121 0.214

1.000 0.000 1.000 1.000 0.000 0.000 0.000 0.000 0.000

0.473 0.108 0.343 0.075

1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000

0.214 0.154 0.145 0.141 0.072 0.188 0.086

0.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000

0.387 0.214 0.270 0.129

0.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000

0.155 0.214 0.136 0.096 0.062 0.208 0.129

0.000 0.000 0.000 0.000 1.000 1.000 1.000 1.000 0.000

0.159 0.012 0.753 0.076

0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000

0.112 0.098 0.214 0.119 0.100 0.273 0.083

0.245 0.121 0.419 0.214

0.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000

0.137 0.089 0.147 0.214 0.111 0.247 0.055

Φ ∈ R4×4
+

0.000 0.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000

0.031 0.017 0.115 0.087 0.214 0.482 0.055

0.000 0.000 0.000 0.000 0.000 1.000 1.000 1.000 0.000

0.002 0.010 0.098 0.037 0.156 0.614 0.082

0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000

0.076 0.121 0.119 0.050 0.081 0.338 0.214

Ψ> ∈ R7×9
+

7×7
Φ ∈ R+

0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000

Ψ

>

∈ R4×9
+

Figure 4.2: Value-of-information-based aggregation for a 9-state Markov chain with one discernible state group and 6 outlying states. We show the
original stochastic matrix Π ∈ R9×9 with the partitions Ψ ∈ Rm×9 overlaid for four critical values of β. We also show the resulting aggregation
Θ ∈ Rm×m , which, in each case, approximately mimics the dynamics of the original stochastic matrix.

in close proximity to the next global optima Ψ∗ , up to a permutation of the rows. The DKL (Ψ∗ kΨ(1) ) term in the
approximation error bound dominates over the k −1 term in this situation and hence that few changes in Ψ are needed.
To assess the convergence stability of the aggregation process, we performed a thousand Monte Carlo trials on
both examples. In only a very small fraction of the trials did the partitions deviate from those presented in figures
4.1 and 4.2 by more than a simple permutation. Such occurrences were largely due to a degenerate initialization of
a new partition column whereby no states would be associated with that new state group. Imposing a constraint that
a new group must contain at least a single state fixed this issue and led to consistent partitions being produced. The
expectation-maximization-based procedure for solving the value of information was then able to discover global
optima well in just a few iterations; the optima often were binary partitions like those presented in figures 4.1 and 4.2.
Avoiding coincident Partitions. The results for the preceding examples indicate that the Shannon-information
constraint, E[DKL (γkΨ(k) )] ≤ r, has the potential to yield non-coincident partitions. We now demonstrate using two
additional Markov chains that using a Shannon-entropy penalty, E[−log(Ψ(k) )] ≤ r, is more likely to return partitions
with duplicated rows. This over-inflates the state-group cardinality, leading to aggregations with redundant details.
Both of these examples are for Markov chains with nine states. The first example, shown in the top left-hand corner of figure 4.4, contains three state groups with a high chance to both jump to states in different groups and jump to
states within a group. Moreover, many of the rows in the matrix are the same. We anticipate that coincident partitions

SUBMITTED TO ENTROPY

16

0

6

6

4

4

2

2

m

0.15

m=4
0

Iteration

2

0 0.1 0.2 0.3

0.5

1

1

0
6

4

4

0.5

1

0.1

Exp. Distortion E[E[g(Π, Θ )|Ψ ]|γ]
Cross Entropy E[−log(Ψ(k−1) )|Ψ(k) ]
(k)

(k)

0
6

4

4

2

2

0

4

0 0.1 0.2 0.3

0.5

1

m=7
1

0

6

6

4

4

2

2
0.2

0.5

0.25

m=4

2
0.1 0.15

m=3
1

0.25 0.3 0.35 0.2

2
2

0.5

6

6

0.2

m=5

6

0

8

0.15 0.2 0.25 0.3

4

m=2

0.5

Iteration

6

m=3
1

m

Iteration

8

0.5

Iteration

m=2
0

0.25

0.5

0

1

0.1

Exp. Distortion E[E[g(Π, Θ )|Ψ ]|γ]
Cross Entropy E[−log(Ψ(k−1) )|Ψ(k) ]
(k)

(k)

(a)
(b)
Figure 4.3: Expected distortion (blue curves) and cross-entropy (red curves) plots for the aggregation results in figure 4.1, shown in (a), and figure
4.2, shown in (b). For both (a) and (b), the large, left-most plot shows the expected distortion as a function of the number of state groups m after
convergence has been achieved. The ‘knee’ of the plot in (a) is given for m = 4, while for (b) it is at m = 7. These ‘knee’ regions correspond to the
‘optimal’ number of state groups as returned by our perturbation-theoretic criterion. They indicate where there are diminishing returns for including
more aggregated state groups. The smaller four plots in (a) and (b) highlight the change in the expected distortion and cross-entropy as a function of
the number of alternating-update iterations. These plots highlight a rapid stabilization of the update process.

will easily materialize due to these properties. The second example is given in the bottom left-hand corner of figure
4.4. It contains two state groups with weakly-interacting intra-group dynamics and strong inter-group dynamics. Each
group has highly distinct transition probabilities. We hence expect that returning coincident partitions will be more
difficult than in the first case. As before, the transition probabilities for each matrix were randomly generated through
knowledge of a limit distribution.
Partitions for to nine state groups are presented in figures 4.4. The partitions in the middle column of figure 4.4 are
the results for the Shannon-information constraint, while those in right column are for the Shannon-entropy constraint.
The Shannon-information case quantizes the data in the manner we would expect for both examples: each state is,
more or less, assigned to its own group so that the original stochastic matrix is exactly recovered. There are hence
no degenerate clusters. For the Shannon-entropy case, three coincident clusters formed for the first problem and this
value of β. Two states from the first state group were incorrectly viewed as being equivalent. Two states from the second group and three states from the third group were also improperly treated, leading to further coincident partitions.
Four degenerate clusters thus emerged and the original stochastic matrix could not be recovered; the Kullback-Leibler
distortion for this value of β and other βs illustrate this. For the second problem, every state in the second state group
was considered equal. Seven degenerate groups were thus created, leading to a stochastic matrix with a very different
invariant distribution and hence long-term dynamics than the original.
For these examples, we considered the same number of groups as states to highlight the severity of the coincident
partition issue when using an uncertainty constraint. Coincident clusters were also observed when the group count
was below the number of states.
4.2.2 Value-of-Information Aggregation Discussions

We have illustrated that the value of information criterion provides an effective mechanism for dynamics reduction of Markov chains for these examples. Consistently stable partitions of the transition probabilities are produced by
optimizing this criterion. Such partitions induce reduced-order chains that do not have duplicate state groups and are
often parsimonious representations. We have additionally demonstrated that only a finite number of free-parameter
values need to be considered for this purpose, the ‘optimal’ value of which can be discerned in a data-driven fashion.
Aggregation Performance: Binary Partitions. In the previous section, we relaxed the binary-valued constraint
on the partition matrices to avoid exactly solving a potentially computationally-intractable problem. However, our
aggregation results for the first two examples indicate that either binary or nearly-binary partition matrices may still
be returned when solving the value of information. The reason for this is the interplay between the expected distortion
and the Shannon-mutual-information constraint: while the latter does not explicitly preclude their formation, the
former naturally favors binary partitions.
More specifically, non-binary partitions will always have less Shannon mutual information than binary partitions.
This is is because the conditional entropy of the states in the original and aggregated chains increases more quickly
than the marginal entropy, which is due to the additional uncertainty in the non-binary partitions. Hence, for a given

SUBMITTED TO ENTROPY

17

Π ∈ R9×9
+

β = 1.6253

β = 3.0164

0.000 0.020 0.093 0.107 0.097 0.172 0.188 0.153 0.169

0.062 0.938 0.000 0.000 0.000 0.000 0.000 0.000 0.000

0.000 0.000 0.000 0.000 0.000 0.036 0.982 0.951 0.995

0.023 0.000 0.086 0.101 0.091 0.176 0.195 0.155 0.173

0.000 0.000 0.000 0.000 0.000 0.978 0.003 0.009 0.009

0.000 0.000 0.708 0.083 0.708 0.000 0.000 0.001 0.000

0.183 0.145 0.000 0.030 0.028 0.161 0.186 0.118 0.149

0.000 0.000 0.195 0.771 0.068 0.000 0.000 0.001 0.000

0.000 0.000 0.010 0.007 0.010 0.000 0.000 0.000 0.000

0.217 0.177 0.031 0.000 0.048 0.136 0.167 0.096 0.129

0.000 0.000 0.000 0.001 0.000 0.009 0.007 0.875 0.101

0.000 0.000 0.057 0.042 0.057 0.000 0.000 0.000 0.000

0.185 0.150 0.027 0.045 0.000 0.162 0.175 0.113 0.141

0.000 0.000 0.000 0.000 0.000 0.003 0.934 0.007 0.064

0.000 0.000 0.145 0.103 0.145 0.000 0.000 0.000 0.000

0.261 0.231 0.124 0.101 0.129 0.000 0.059 0.049 0.046

0.938 0.062 0.000 0.000 0.000 0.000 0.000 0.000 0.000

0.000 0.000 0.000 0.000 0.000 0.964 0.018 0.046 0.005

0.262 0.235 0.132 0.115 0.128 0.054 0.000 0.049 0.026

0.000 0.000 0.229 0.070 0.731 0.000 0.000 0.000 0.000

0.065 0.065 0.000 0.000 0.000 0.000 0.000 0.000 0.000

0.286 0.249 0.112 0.088 0.110 0.060 0.065 0.000 0.030

0.000 0.000 0.000 0.000 0.000 0.010 0.056 0.109 0.825

0.000 0.000 0.079 0.765 0.079 0.000 0.000 0.002 0.000

0.283 0.251 0.127 0.106 0.124 0.051 0.032 0.027 0.000

0.000 0.000 0.576 0.159 0.201 0.000 0.000 0.000 0.000

0.935 0.935 0.000 0.000 0.000 0.000 0.000 0.000 0.000

Ψ:

Pm

j=1 αj

Pn

i=1 ψi,j log(ψi,j /γi )

≤r

Ψ:−

Pn Pm
i=1

j=1 ψi,j log(ψi,j )

≤r

0.000 0.022 0.087 0.048 0.164 0.178 0.213 0.157 0.132

0.007 0.009 0.000 0.000 0.000 0.003 0.000 0.001 1.000

0.000 0.000 0.000 0.000 0.277 0.277 0.277 0.277 0.277

0.024 0.000 0.090 0.064 0.164 0.173 0.212 0.150 0.122

0.005 0.006 0.000 0.000 0.000 0.017 0.000 0.995 0.000

0.013 0.013 0.012 0.111 0.000 0.000 0.000 0.000 0.000

0.073 0.069 0.000 0.046 0.178 0.165 0.193 0.151 0.125

0.004 0.005 0.000 0.000 1.000 0.002 0.000 0.000 0.000

0.023 0.023 0.022 0.204 0.000 0.000 0.000 0.000 0.000

0.041 0.049 0.047 0.000 0.176 0.179 0.209 0.161 0.137

0.015 0.016 1.000 0.000 0.000 0.000 0.000 0.000 0.000

0.000 0.000 0.000 0.000 0.253 0.253 0.253 0.253 0.253

0.161 0.145 0.208 0.201 0.000 0.066 0.083 0.059 0.077

0.004 0.004 0.000 0.000 0.000 0.910 0.001 0.002 0.000

0.039 0.039 0.037 0.348 0.000 0.000 0.000 0.000 0.000

0.195 0.172 0.216 0.229 0.073 0.000 0.039 0.023 0.054

0.003 0.003 0.000 0.000 0.000 0.006 0.997 0.000 0.000

0.000 0.000 0.000 0.000 0.239 0.239 0.239 0.239 0.239

0.187 0.169 0.202 0.214 0.074 0.031 0.000 0.049 0.074

0.910 0.039 0.000 0.000 0.000 0.000 0.000 0.000 0.000

0.012 0.012 0.881 0.098 0.000 0.000 0.000 0.000 0.000

0.189 0.164 0.217 0.227 0.073 0.025 0.068 0.000 0.037

0.050 0.033 0.000 1.000 0.000 0.000 0.000 0.000 0.000

0.000 0.000 0.000 0.000 0.229 0.229 0.229 0.229 0.229

0.166 0.139 0.188 0.202 0.099 0.062 0.106 0.038 0.000

0.003 0.920 0.000 0.000 0.000 0.062 0.001 0.001 0.000

0.912 0.912 0.048 0.238 0.001 0.001 0.001 0.001 0.001

Π ∈ R9×9
+

β = 0.8325

β = 2.5205

Figure 4.4: Value-of-information-based aggregation for a 9-state Markov chain with three discernible state groups (top row) and two discernible
state groups (bottom row). The left-most column shows the original stochastic matrices Π ∈ R9×9 . The middle column gives the partitions
Ψ ∈ R9×9 found when using a Shannon mutual information constraint for the expected-distortion objective function. The right-most column gives
the partitions Ψ ∈ R9×9 found when using a Shannon entropy constraint for the expected-distortion objective function. When using Shannon
entropy, several columns of the partition matrix are duplicated for high values of β, leading to an incorrect aggregation of states.

upper bound on the Shannon information, if a binary partition can be formed for that bound, then a corresponding
non-binary one can also be formed. The minimization of the distortion term, however, impedes the formation of nonbinary partitions. In the binary case, provided that the partition reflects the underlying structure of the transitions,
only related probability vectors will be compared to each other. Vastly different rows and columns of the stochastic
and joint stochastic matrices will not factor into the expected distortion, since the state-assignment probability will
be zero if the partition encodes well the underlying transition structure. Making highly non-binary state-group assignments can raise the expected distortion: the Kullback-Leibler divergence between two, possibly very distinct,
probability vectors may be multiplied by a non-zero state-assignment probability.
This behavior contrasts with the use of a conditional Shannon entropy equality constraint on the entries of the partition matrix. Such a constraint directly imposes that the partition matrix should have a given amount of uncertainty,
potentially at the expense of a worse distortion. Non-binary partitions hence can be more readily constructed.
Aggregation Performance: ‘Optimal’ State Group Count. We considered a perturbation-theoretic approach for
determining the ‘optimal’ number of state groups. The approach operates on the assumption that, for finitely-sized
stochastic matrices, there is an error in estimating the marginal distribution of the original states. This poor estimate
leads to a systematic error in evaluating the Shannon-information term, which we quantified in a second-order sense.
In the case of binary partitions, a second-order correction of this error introduces a penalty in the value of information
for using more aggregated state groups than can be resolved for a particular finitely-sized state space. Values for the
free parameter were returned that, for our examples, aligned well with a balance between the expected distortion of
the aggregation and the mutual dependence between states in the original and aggregated chains.
As shown in our experiments, the value of information monotonically decreases for an increasing number of state
groups. The second-order-corrected version shares this trait, as it is a slope-rescaled version of the original value of information. Ideally, we would like to further transform this slope-scaled value-of-information curve so that it possesses
an extremum where both terms of the objective function are balanced. This would lend further credence to the notion
that such a free parameter value, and hence the number of state groups, for any stochastic matrix is ‘optimal’. In our
upcoming work, we will demonstrate how to perform this transformation. We will show that the value of information
can be written, in some cases, as a variational problem involving two Shannon-mutual-information terms. Applying
the same perturbation-theoretic arguments to this version of the value of information reveals that the corrected criterion is monotonically increasing up to a certain point, after which it is monotonically non-increasing and often is

SUBMITTED TO ENTROPY

18

strictly decreasing. This inflexion point corresponds to the ‘optimal’ parameter value determined here. This value
minimizes the mutual dependence between the original and aggregated states while simultaneously retraining as
much information about the original transition dynamics as possible.
5 Conclusions

In this paper, we have provided a novel, two-part information-theoretic approach for aggregating Markov chains.
The first part of our approach is aimed at assessing the distortion between original- and reduced-order chains according to the Kullback-Leibler divergence between rows of their corresponding stochastic matrices. The discrete nature
of the graphs precludes the direct comparison of the transition probabilities according to this divergence, which
motivated our construction of a joint transition model. This joint model encodes all of the information of the reducedorder Markov chain and is of the proper size to compare against the original Markov chain. The second part of our
approach addresses how to combine states in the original chain, according to the Kullback-Leibler divergence, by solving a value-of-information criterion. This criterion aggregates states together if doing so reduces the total expected
distortion between the low- and high-order chains and simultaneously maximizes the bounded mutual dependence between states in the high- and low-order chains. It thus attempts to find a low-order Markov chain that most resembles
the global and local transition structure of the original, high-order Markov chain.
The value of information provides a principled and optimal trade-off between the quality of the aggregation, as
measured by the total expected distortion, and the complexity of it, as measured by the state mutual dependence
according to Shannon mutual information. The complexity constraint has dual roles. The first is that it explicitly
dictates the number of states in the low-order chain. We proved that changing the value of a variable associated with
this constraint causes the aggregation process to undergo phase transitions where new groupings emerge by splitting
an existing state cluster. The second role of the constraint is that it relaxes the condition that the partition matrices
must be strictly binary. This relaxation permits the formulation of an efficient procedure for approximately solving the
aggregation problem. While the same effect could be achieved with a Shannon entropy constraint, it has the tendency
to yield coincident partitions. This over-inflates the number of states in the reduced-order model.
We applied our approach to a series of Markov chains. Our simulation results showed that our value-of-information
scheme achieved equal or better performance compared to a range of different aggregation techniques. A practical
advantage of our methodology is that we have derived a data-driven expression for the ‘optimal’ value for a parameter
associated with the state mutual dependence constraint. This expression was based upon correcting the underestimation of the Shannon information term for finitely-sized stochastic matrices. Employing this expression fits the
partitions more to the structure of the data than to the noise, ensuring that it tends not to over-cluster states in the original chain. It also frees investigators from having to supply the number of state groups. Many existing aggregation
approaches rely on the manual specification of the state-group count, in contrast; a reasonable number of state groups
may not be immediately evident for certain problems, which complicates their effective application.
As we noted at the beginning of the paper, our emphasis is on understanding the effects of the value of information
when it is applied to resolve the exploration-exploitation dilemma in reinforcement learning. In particular, we seek to
address the question of if the value of information is implicitly aggregating the Markov chains underlying the Markov
decision process during exploration. Toward this end, our next step will be to show that hidden Markov models can be
reduced in a value-of-information-based manner. Much like our work here, this will entail defining a joint model that
allows for comparisons between pairs of hidden Markov models with different state spaces. We will need to construct
the joint model so that it is Markovian, which will ensure that the theory in this paper applies with few modifications.
Following this, for the Markov-decision-process case, we will need to show that a lumpable partition of the state space
can be defined by the value of information, where the partition is bounded by the state-transition and cost effects.
States in the same partition will then be viewed as a single state in a reduced-order, aggregated Markov chain. An
aggregated Markov decision process with average cost on the aggregated Markov chain can then be obtained. As
a part of this effort, we will also quantify the local-neighborhood performance difference between this aggregated
Markov decision process and the optimal one.
References

[1] E. F. Arruda and M. D. Fragoso, “Standard dynamic programming applied to time aggregated Markov decision
processes,” in Proceedings of the IEEE Conference on Decision and Control (CDC), Shanghai, China, December
15-18 2009, pp. 2576–2580. Available: http://dx.doi.org/10.1109/CDC.2009.5400692
[2] R. W. Aldhaheri and H. K. Khalil, “Aggregation of the policy iteration method for nearly completely
decomposable Markov chains,” IEEE Transactions on Automatic Control, vol. 36, no. 2, pp. 178–187, 1991.
Available: http://dx.doi.org/10.1109/9.67293
[3] Z. Ren and B. H. Krogh, “Markov decision processes with fractional costs,” IEEE Transactions on Automatic
Control, vol. 50, no. 5, pp. 646–650, 2005. Available: http://dx.doi.org/10.1109/TAC.2005.846520
[4] T. Sun, Q. Zhao, and P. B. Luh, “Incremental value iteration for time-aggregated Markov decision processes,”
IEEE Transactions on Automatic Control, vol. 52, no. 11, pp. 2177–2182, 2007. Available:
http://dx.doi.org/10.1109/TAC.2007.908359

SUBMITTED TO ENTROPY

19

[5] Q. S. Jia, “On state aggregation to approximate complex value functions in large-scale Markov decision
processes,” IEEE Transactions on Automatic Control, vol. 56, no. 2, pp. 333–334, 2011. Available:
http://dx.doi.org/10.1109/TAC.2010.2052697
[6] M. Aoki, “Some approximation methods for estimation and control of large scale systems,” IEEE Transactions on
Automatic Control, vol. 23, no. 2, pp. 173–182, 1978. Available: http://dx.doi.org/10.1109/TAC.1978.1101705
[7] J. C. Príncipe, Information Theoretic Learning. New York City, NY, USA: Springer-Verlag, 2010.
[8] Z. Rached, F. Alajaji, and L. L. Campbell, “The Kullback-Leibler divergence rate between Markov sources,” IEEE
Transactions on Information Theory, vol. 50, no. 5, pp. 917–921, 2004. Available:
http://dx.doi.org/10.1109/TIT.2004.826687
[9] M. D. Donsker and S. R. S. Varadhan, “Asymptotic evaluation of certain Markov process expectations for large
time I.” Communications of Pure and Applied Mathematics, vol. 28, no. 1, pp. 1–47, 1975. Available:
http://dx.doi.org/10.1002/cpa.3160280102
[10] ——, “Asymptotic evaluation of certain Markov process expectations for large time II.” Communications of Pure
and Applied Mathematics, vol. 28, no. 2, pp. 279–301, 1975. Available:
http://dx.doi.org/10.1002/cpa.3160280206
[11] K. Deng, P. G. Mehta, and S. P. Meyn, “Optimal Kullback-Leibler aggregation via spectral theory of Markov
chains,” IEEE Transactions on Automatic Control, vol. 56, no. 12, pp. 2793–2808, 2011. Available:
http://dx.doi.org/10.1109/TAC.2011.2141350
[12] B. C. Geiger, T. Petrov, G. Kubin, and H. Koeppl, “Optimal Kullback-Leibler aggregation via inforamtion
bottleneck,” IEEE Transactions on Automatic Control, vol. 60, no. 4, pp. 1010–1022, 2015. Available:
http://dx.doi.org/10.1109/TAC.2014.2364971
[13] I. J. Sledge and J. C. Príncipe, “An analysis of the value of information when exploring stochastic, discrete
multi-armed bandits,” Entropy, vol. 20, no. 3, pp. 155(1–34), 2018. Available:
http://dx.doi.org/10.3390/e20030155
[14] ——, “Analysis of agent expertise in Ms. Pac-Man using value-of-information-based policies,” IEEE Transactions
on Computational Intelligence and Artificial Intelligence in Games, 2018, (accepted, in press). Available:
http://dx.doi.org/10.1109/TG.2018.2808201
[15] I. J. Sledge, M. S. Emigh, and J. C. Príncipe, “Guided policy exploration for Markov decision processes using an
uncertainty-based value-of-information criterion,” IEEE Transactions on Neural Networks and Learning Systems,
vol. 29, no. 6, pp. 2080–2098, 2018. Available: http://dx.doi.org/10.1109/TNNLS.2018.2812709
[16] R. L. Stratonovich, “On value of information,” Izvestiya of USSR Academy of Sciences, Technical Cybernetics,
vol. 5, no. 1, pp. 3–12, 1965.
[17] R. L. Stratonovich and B. A. Grishanin, “Value of information when an estimated random variable is hidden,”
Izvestiya of USSR Academy of Sciences, Technical Cybernetics, vol. 6, no. 1, pp. 3–15, 1966.
[18] T. M. Cover and J. A. Thomas, Elements of Information Theory. New York, NY, USA: John Wiley and Sons,
2006.
[19] J. von Neumann and O. Morgenstern, Theory of Games and Economic Behavior. Princeton, NJ, USA: Princeton
University Press, 1953.
[20] H. A. Simon and A. Ando, “Aggregation of variables in dynamic systems,” Econometrica, vol. 29, no. 2, pp.
111–138, 1961. Available: http://dx.doi.org/10.2307/1909285
[21] P. J. Courtois, “Error analysis in nearly-completely decomposable stochastic systems,” Econometrica, vol. 43,
no. 4, pp. 691–709, 1975. Available: http://dx.doi.org/10.2307/1913078
[22] A. A. Pervozvanskii and I. N. Smirnov, “Stationary-state evaluation for a complex system with slowly varying
couplings,” Kibernetika, vol. 3, no. 1, pp. 45–51, 1974. Available: http://dx.doi.org/10.1007/BF01071538
[23] V. G. Gaitsgori and A. A. Pervozvanskii, “Aggregation of states in a Markov chain with weak interaction,”
Kibernetika, vol. 4, no. 1, pp. 91–98, 1975. Available: http://dx.doi.org/10.1007/BF01069471
[24] D. Teneketzis, S. H. Javid, and B. Sridhar, “Control of weakly-coupled Markov chains,” in Proceedings of the
IEEE Conference on Decision and Control (CDC), Albuquerque, NM, USA, December 10-12 1980, pp. 137–142.
Available: http://dx.doi.org/10.1109/CDC.1980.272033
[25] F. Delebecque and J.-P. Quadrat, “Optimal control of Markov chains admitting strong and weak interactions,”
Automatica, vol. 17, no. 2, pp. 281–296, 1981. Available: http://dx.doi.org/10.1016/0005-1098(81)90047-9
[26] Q. Zhang, G. Yin, and E. K. Boukas, “Controlled Markov chains with weak and strong interactions,” Journal of
Optimization Theory and Applications, vol. 94, no. 1, pp. 169–194, 1997. Available:
http://dx.doi.org/10.1023/A:1022667905086
[27] P. J. Courtois, Decomposability, Instabilities, and Saturation in Multiprogramming Systems. New York, NY,
USA: Academic Press, 1977.
[28] R. W. Aldhaheri and H. K. Khalil, “Aggregation and optimal control of nearly completely decomposable Markov
chains,” in Proceedings of the IEEE Conference on Decision and Control (CDC), Tampa, FL, USA, December
13-15 1989, pp. 1277–1282. Available: http://dx.doi.org/10.1109/CDC.1989.70343
[29] G. Kotsalis and M. Dahleh, “Model reduction of irreducible Markov chains,” in Proceedings of the IEEE
Conference on Decision and Control (CDC), Maui, HI, USA, December 9-12 2003, pp. 5727–5728. Available:
http://dx.doi.org/10.1109/CDC.2003.1271917
[30] S. Dey, “Reduced-complexity filtering for partially observed nearly completely decomposable Markov chains,”
IEEE Transactions on Signal Processing, vol. 48, no. 12, pp. 3334–3344, 2000. Available:
http://dx.doi.org/10.1109/78.886997
[31] H. Vantilborgh, “Aggregation with an error of O(2 ),” Journal of the ACM, vol. 32, no. 1, pp. 162–190, 1985.
Available: http://dx.doi.org/10.1145/2455.214107
[32] W.-L. Cao and W. J. Stewart, “Iterative aggregation/disaggregation techniques for nearly uncoupled Markov
chains,” Journal of the ACM, vol. 32, no. 3, pp. 702–719, 1985. Available: http://dx.doi.org/10.1145/3828.214137

SUBMITTED TO ENTROPY

20

[33] J. R. Koury, D. F. McAllister, and W. J. Stewart, “Iterative methods for computing stationary distributions of
nearly completely decomposable Markov chains,” SIAM Journal on Algebraic and Discrete Methods, vol. 5, no. 2,
pp. 164–186, 1984. Available: http://dx.doi.org/10.1137/0605019
[34] G. P. Barker and R. J. Piemmons, “Convergent iterations for computing stationary distributions of Markov chains,”
SIAM Journal on Algebraic and Discrete Methods, vol. 7, no. 3, pp. 390–398, 1986. Available:
http://dx.doi.org/10.1137/0607044
[35] T. Dayar and W. J. Stewart, “On the effects of using the Grassman-Taksar-Heyman method in iterative
aggregation-disaggregation,” SIAM Journal on Scientific Computing, vol. 17, no. 1, pp. 287–303, 1996. Available:
http://dx.doi.org/10.1137/0917021
[36] R. Phillips and P. Kokotovic, “A singular perturbation approach to modeling and control of markov chains,” IEEE
Transactions on Automatic Control, vol. 26, no. 5, pp. 1087–1094, 1981. Available:
http://dx.doi.org/10.1109/TAC.1981.1102780
[37] G. Peponides and P. Kokotovic, “Weak connections, time scales, and aggregation of nonlinear systems,” IEEE
Transactions on Automatic Control, vol. 28, no. 6, pp. 729–735, 1983. Available:
http://dx.doi.org/10.1109/TAC.1983.1103300
[38] J. Chow and P. Kokotovic, “Time scale modeling of sparse dynamic networks,” IEEE Transactions on Automatic
Control, vol. 30, no. 8, pp. 714–722, 1985. Available: http://dx.doi.org/10.1109/TAC.1985.1104055
[39] J. A. Filar, V. Gaitsgory, and A. B. Haurie, “Control of singularly perturbed hybrid stochastic systems,” IEEE
Transactions on Automatic Control, vol. 46, no. 2, pp. 179–180, 2001. Available:
http://dx.doi.org/10.1109/9.905686
[40] K. Deng, Y. Sun, P. G. Mehta, and S. P. Meyn, “An information-theoretic framework to aggregate a Markov chain,”
in Proceedings of the American Control Conference (ACC), St. Louis, MO, USA, June 10-12 2009, pp. 731–736.
Available: http://dx.doi.org/10.1109/ACC.2009.5160607
[41] K. Deng and D. Huang, “Model reduction of Markov chains via low-rank approximation,” in Proceedings of the
American Control Conference (ACC), Montréal, Canada, June 27-29 2012, pp. 2651–2656. Available:
http://dx.doi.org/10.1109/ACC.2012.6314781
[42] M. Vidyasagar, “Reduced-order modeling of Markov and hidden Markov processes via aggregation,” in
Proceedings of the IEEE Conference on Decision and Control (CDC), Atlanta, GA, USA, December 15-17 2010,
pp. 1810–1815. Available: http://dx.doi.org/10.1109/CDC.2010.5717206
[43] ——, “A metric between probability distributions on finite sets of different cardinalities and applications to order
reduction,” IEEE Transactions on Automatic Control, vol. 57, no. 10, pp. 2464–2477, 2012. Available:
http://dx.doi.org/10.1109/TAC.2012.2188423
[44] W. I. Zangwill, Nonlinear Programming: A Unified Approach. Upper Saddle River, NJ, USA: Prentice-Hall,
1969.
[45] A. Treves and S. Panzeri, “The upward bias in meausres of information derived from limited data samples,”
Neural Computation, vol. 7, no. 2, pp. 399–407, 1995. Available: http://dx.doi.org/10.1162/neco.1995.7.2.399
[46] T. Kato, Perturbation Theory for Linear Operators. New York, NY, USA: Springer-Verlag, 1966.

SUBMITTED TO ENTROPY

21

A Appendix
Proposition 3.1. For a transition model Rπ = (Vπ , Eπ , Π) over n states and a joint model Rϑ = (Vϑ , Eϑ , Θ)
and m+n states, the Lagrangian of the relevant terms for the minimization problem given in definition 3.11 is
F (Ψ, α; Π, Θ, γ) = E[E[g(Π, Θ)|Ψ]|γ]−E[DKL (γkΨ)]/β, or, rather,
!
!
n X
m
m
n
X
1 X X
F (Ψ, α; Π, Θ, γ) =
γi ψi,j g(πi,1:n , ϑj,1:n ) −
αj
ψi,j log(ψi,j /γi ) .
β j=1 i=1
i=1 j=1

Here, β ≥ 0 is a Lagrange multiplier that emerges from the Shannon mutual information constraint in the value of
information.
Probabilistic partitions [Ψ]i,j = ψi,j , which are local solutions of ∇ϑj,1:n F (Ψ, α; Π, Θ, γ) = 0, can be found
by the following expectation-maximization-based alternating updates
!, m
!
n
X
X
−βg(πi,1:n ,ϑj,1:n )
−βg(πi,1:n ,ϑp,1:n )
αj ←
γi ψi,j ,
ψi,j ← αj e
αp e
,
p=1

i=1

which are iterated until convergence.

Proof: We can convert the constrained value-of-information into an unconstrained problem using the theory
of Lagrange multipliers. There are five different constraints for which we need to account,
!
!
n X
m
m
n
X
1 X X
F (Ψ, α; Π, Θ, γ) =
γi ψi,j g(πi,1:n , ϑj,1:n ) −
αj
ψi,j log(ψi,j /γi )
β j=1 i=1
i=1 j=1
!
!
!
!
n X
m
n X
m
n X
m
n X
m
X
X
X
X
+
κi (1−ψi,j ) +
ωi (1−ϑj,i ) −
ξi ψi,j −
µi ϑj,i
i=1 j=1

i=1 j=1

i=1 j=1

i=1 j=1

The first constraint corresponds to the Shannon mutual information term. The remaining constraints ensure that
entries from Θ and Ψ correspond to valid probabilities.
We can derive the update for the probabilistic partition matrix Ψ by differentiating the Lagrangian and setting it to zero
!
!
∂
ψi,j
γi g(πi,1:n , ϑj,1:n ) − βγi
+ κi + ωi − ξi − µi = 0.
F (Ψ, α; Π, Θ, γ) = γi log
−
∂ψi,j
αj
β
Pn
Pn Pm
j,1:n )
Solving for ψi,j yields ψi,j = r=1P
γr ψr,j e−βg(πi,1:n ,ϑP
/ r=1 p=1 γr ψr,j e−βg(πi,1:n ,ϑp,1:n ) , where κi
n
n
and ωi have been selected such that i=1 ψi,j = 1 and i=1 ϑi,j = 1, respectively. It is apparent that the update
for the marginal probabilities α is encoded in this update for Ψ.
We now can show that the update for the marginal probabilities is optimal. For a fixed probabilistic partition
matrix Ψ, the following inequality
!
!
n X
m
m
n X
X
X
ψi,j
ψi,j
γi ψi,j log Pn
γi ψi,j log
≥
αj
r=1 γr ψj,r
i=1 j=1
i=1 j=1
Pn
holds with equality
if and only if αj = i=1 γi ψi,j
is a consequence
applying the divergence
Presult
Pof
Pn P
P.nThis
n
m
m
inequality to i=1 j=1 γi ψi,j log(ψi,j /αj ) − i=1 j=1 γi ψi,j log(ψi,j / r=1 γr ψj,r ). That is,
!
!
!
Pn
n X
m
n X
m
n X
m
X
X
X
γr ψj,r
ψi,j
ψi,j
r=1
γi ψi,j log
−
γi ψi,j log Pn
=
γi ψi,j log
αj
αj
r=1 γr ψj,r
i=1 j=1
i=1 j=1
i=1 j=1
!
!
n
m
m
XX
X
≥
γi ψi,j −
αj
i=1 j=1

≥ 0,

j=1

Pn
where equality to zero is only obtained if and only if αj = i=1 γi ψi,j . This implies that, for a fixed Ψ, the
update for α globally solves the problem minα F (Ψ, α; Π, Θ, γ), establishing its optimality.
We now demonstrate that the probabilistic partition update is optimal. For a fixed marginal probability vector

SUBMITTED TO ENTROPY

22

α, the following inequality
−

n
X
i=1

γi log

m
X

αj e

−βg(πi,1:n ,ϑj,1:n )

j=1

!

n X
m
X

ψi,j
≤
γi ψi,j log
αj
i=1 j=1
n X
m
X

=

1
−
β

n X
m
X
i=1 j=1

ψi,j e−βg(πi,1:n ,ϑj,1:n )
≤
γi ψi,j log
αj
i=1 j=1

holds with equality if and only if ψi,j = αj e−βg(πi,1:n ,ϑj,1:n ) /
showing that
!
n X
m
X
ψi,j e−βg(πi,1:n ,ϑj,1:n )
γi ψi,j log
αj
i=1 j=1
=

!

n X
m
X

i=1 j=1
n X
m
X

γi ψi,j log
γi ψi,j log

i=1 j=1

Pm

p=1

ψi,j g(πi,1:n , ϑj,1:n )
!

αp e−βg(πi,1:n ,ϑp,1:n ) . This follows from

Pm

−βg(πi,1:n ,ϑp,1:n )
p=1 αp e
P
m
αj e−βg(πi,1:n ,ϑj,1:n ) p=1 αp e−βg(πi,1:n ,ϑp,1:n )

ψi,j

αj e−βg(πi,1:n ,ϑj,1:n ) /

n X
m
X

ψ
Pi,j
m

!

p=1

!

αp e−βg(πi,1:n ,ϑp,1:n )
!

!

1
−βg(πi,1:n ,ϑp,1:n )
α
e
p=1 p
i=1 j=1
!
m
n X
X
1
γi ψi,j log Pm
≥0+
−βg(πi,1:n ,ϑp,1:n )
p=1 αp e
i=1 j=1
!
n
X
1
≥
γi log Pm
−βg(πi,1:n ,ϑp,1:n )
p=1 αp e
i=1
+

γi ψi,j log Pm

Here, we used
step. As well,
have that the last step can be
Pn the divergence
Pminequality in the second-to-lastP
Pwe
n
n
written as i=1 γi log(1/ p=1 αp e−βg(πi,1:n ,ϑp,1:n ) ) = − i=1 γi log( j=1 αj e−βg(πi,1:n ,ϑj,1:n ) ), where
the right-hand side is the desired expression. Substituting the update for Ψ in the original inequality leads to
equivalency. Hence, for a fixed α, the update for Ψ globally solves minΨ F (Ψ, α; Π, Θ, γ).

Proposition 3.2. Let Rπ = (Vπ , Eπ , Π) and Rϕ = (Vϕ , Eϕ , Φ) be transition models of two Markov chains
∗
over n and m states, respectively, where m < n. If [Ψ∗ ]i,j = ψi,j
is an optimal probabilistic partition and
∗
∗
[α ]j = αj an optimal marginal probability vector, then, for the updates in proposition 3.1, we have that:
(i) The approximation error is non-negative
!
!
Pm
n
∗ −βg(πi,1:n ,ϑj,1:n )
X
α
e
j
j=1
≥ 0.
F (Ψ(k) , α(k) ; Π, Θ, γ) − F (Ψ∗ , α∗ ; Π, Θ, γ) =
γi log Pm
(k) −βg(πi,1:n ,ϑj,1:n )
i=1
j=1 αj e

(ii) The modified free energy monotonically decreases F (Ψ(k) , α(k) ; Π, Θ, γ) ≥ F (Ψ(k+1) , α(k+1) ; Π, Θ, γ)
across all iterations k.
(iii) For any K ≥ 1, we have the following bound for the sum of approximation errors
!
!
K
n X
m
∗
X
X
ψ
i,j
∗
F (Ψ(k) , α(k) ; Π, Θ, γ) − F (Ψ∗ , α∗ ; Π, Θ, γ) ≤
γi ψi,j
log
.
(1)
ψi,j
i=1 j=1
k=1

Here, F (Ψ, α; Π, Θ, γ) = E[E[g(Π, Θ)|Ψ]|γ]−E[DKL (γkΨ)]/β is the Lagrangian.

Proof: Parts (i) and (ii) follow immediately from proposition 3.1. For part (iii), we have the following equality expressions for iterations k and k+1 of the expectation-maximization updates
!
!
(k+1)
(k)
n X
m
n X
m
X
X
ψi,j
αj e−g(πi,1:n ,ϑj,1:n )/β
∗
∗
γi ψj,i log
=
γi ψj,i log
(k)
(k) Pm
(k) −g(πi,1:n ,ϑp,1:n )/β
ψi,j
ψi,j
i=1 j=1
i=1 j=1
p=1 αp e
!

=

F (Ψ(k) , α(k) ; Π, Θ, γ) − F (Ψ∗ , α∗ ; Π, Θ, γ)
+

n X
m
X
i=1 j=1

∗
γi ψi,j
log

!
(k)
αj∗ e−g(πi,1:n ,ϑj,1:n )/β
1 αj
Pm
.
(k) α∗
∗ −g(πi,1:n ,ϑp,1:n )/β
ψ
j
p=1 αp e
i,j

SUBMITTED TO ENTROPY

23

We obtain that the last term in P
the last inequality expression is non-negative, since, from part (i), we have that
m
∗
ψi,j
= αj∗ e−g(πi,1:n ,ϑj,1:n )/β / p=1 αp∗ e−g(πi,1:n ,ϑp,1:n )/β . We thus have that.
!
!
!
(k+1)
n X
n X
m
m
∗ α(k)
X
X
ψi,j
ψi,j
j
(k)
(k)
∗
∗
∗
∗
= F (Ψ , α ; Π, Θ, γ) − F (Ψ , α ; Π, Θ, γ) +
γi ψj,i log
γi ψi,j log
∗
(k)
(k)
ψi,j
ψi,j αj
i=1 j=1
i=1 j=1
!
!
(k)
n X
m
X
αj∗ ψi,j
(k)
(k)
∗
∗
∗
≥ F (Ψ , α ; Π, Θ, γ) − F (Ψ , α ; Π, Θ, γ) +
.
γi ψi,j 1 − (k)
∗
αj ψi,j
i=1 j=1
Pn Pm
(k)
(k) ∗
(k)
(k) ∗
∗
Since αj∗ ψi,j /αj ψi,j
= 1, it can be seen that i=1 j=1 γi ψi,j
(1 − αj∗ ψi,j /αj ψi,j
) = 0. We hence recover
the following inequality
!
!
(k+1)
n X
m
X
ψi,j
(k)
(k)
∗
∗
∗
γi ψj,i log
≥ F (Ψ , α ; Π, Θ, γ) − F (Ψ , α ; Π, Θ, γ) ,
(k)
ψi,j
i=1 j=1
which holds for any k = 1, 2, . . . Summing up this inequality up to K, we have that
!
!
(K+1)
K
n X
m
X
X
ψi,j
(k)
(k)
∗
∗
∗
F (Ψ , α ; Π, Θ, γ) − F (Ψ , α ; Π, Θ, γ) ≤
γi ψi,j log
(1)
ψi,j
i=1 j=1
k=1
!
(K+1)
n X
m
X
ψi,j
∗
≤
γi ψi,j
−1
(1)
ψi,j
i=1 j=1
from which we get the desired inequality, since log(ψi,j

(K+1)

/ψi,j ) = log(ψi,j
(1)

(K+1)

∗
∗
/ψi,j
) + log(ψi,j
/ψi,j ).
(1)



Proposition 3.3. Let Rπ = (Vπ , Eπ , Π) and Rϕ = (Vϕ , Eϕ , Φ) be transition models of two Markov chains
∗
over n and m states, respectively, where m < n. If [Ψ∗ ]i,j = ψi,j
is an optimal probabilistic partition and
∗
∗
[α ]j = αj an optimal marginal probability vector, then, the approximation error
!
!
n X
m
∗
X
ψ
1
i,j
∗
.
F (Ψ∗ , α∗ ; Π, Θ, γ) − F (Ψ(k) , α(k) ; Π, Θ, γ) ≤
γi ψi,j
log
(1)
k i=1 j=1
ψ
i,j

falls off as a function of the inverse of the iteration count k. Here, the constant factor of the error bound is a
Kullback-Leibler divergence between the initial partition matrix Ψ(1) and the global-best partition matrix Ψ∗ .
Proof: From propositions 3.2(ii) and 3.2(iii), we get that

!

k F (Ψ∗ , α∗ ; Π, Θ, γ) − F (Ψ(k) , α(k) ; Π, Θ, γ)

≤

K
X

k=1

!

F (Ψ(k) , α(k) ; Π, Θ, γ) − F (Ψ∗ , α∗ ; Π, Θ, γ) .

The desired inequality follows after dividing both sides by k and substituting the bound obtained in proposition
3.2(iii) on the right-hand side of the inequality.

Proposition 3.4. Let Rπ = (Vπ , Eπ , Π) and Rϕ = (Vϕ , Eϕ , Φ) be transition models of two Markov chains
Pn
over n and m states, respectively, where m < n. Let g(πi,1:n , ϑi,1:n ) = j=1 γi πi,j log(πi,j /ϑi,j ), where
Rϑ = (Vϑ , Eϑ , Θ) is the joint model. The following hold:
(i) The transition matrix Φ of aP
low-order Markov chain over states m is given by Φ = ΘΨ, where
n
Θ = U > Π. Here, [U ]i,j = γi ψi,j / k=1 γk ψk,j for the probabilistic partition matrix [Ψ]i,j = ψi,j found using
the updates in proposition 3.1.
(ii) Suppose that we have a low-order chain over m states with a transition matrix Φ and weight matrix Θ
given by (i). For some β0 , suppose Θβ0 , the matrix Θ for that value of β0 , satisfies
the following inequality
Pm
m×n
2
2
>
d
/d
F
(Ψ,
α;
Π,
Θ
+Q,
γ)|
>
0.
Here,
Q
∈
R
is
a
matrix
such
that
qk,1:n
qk,1:n = 1 and
β
=0
+
0
k=1
Pn
2
2
j=1 qi,j = 0 ∀i. A critical value βc , βc = minβ>β0 (d /d F (Ψ, α; Π, Θβ +Q, γ)|=0 ≤ 0), occurs
whenever the minimum eigenvalue of the matrix
!
!
n
n
X
X
2
2
2
>
diag
ψk,j πi,1:n /ϑk,1:n − β
ψk,j (πi,1:n /ϑk,1:n )(πi,1:n /ϑk,1:n )
i=1

i=1

is zero. The number of rows in Θ and columns in Ψ needs to be increased once β > βc .

Proof: For part (i), we first substitute the partition matrix update into the Lagrangian F (Ψ, α; Π, Θ, γ). After

SUBMITTED TO ENTROPY

24

some simplification and when ignoring irrelevant terms, we find that
∂
∂
F (Ψ, α; Π, Θ, γ) =
∂ϑj,i
∂ϑj,i

n
X

γi log

i=1

m
X

αj e

!!

−βg(πi,1:n ,ϑj,1:n )

j=1

.

Pn
Setting
to zero and solving, we arrive atP
ϑj,i = r=1 γr ψr,j πr,i . Due to the
Pn this expression
Pn
Pnconditions that
n
ϑ
=
π
=
1
∀j,
we
thus
arrive
at
ϑ
=
γ
u
,
where
u
=
ψ
/
j,i
r,j
r,j
i=1 j,i
i=1 i,j
r=1 r r,j
p=1 γp ψp,j . It is
apparent that the entries of Θ are non-negative. As well, every entry in Φ is a convex combination of a column in
Θ. Therefore, Φ is a probabilistic transition
Pnmatrix. Pm
For part (ii), the second variation of i=1 γi log( j=1 αj e−βg(πi,1:n ,ϑj,1:n ) ) at the optimal weighting matrix
Θ∗ is given by
∆2 F (Ψ, α; Π, Θ∗ , γ) = d2 /d2 F (Ψ, α; Π, Θ∗β0 +Q, γ)|=0
!
m X
n
n
X
X
>
∗
2
=
γi ψi,j qj,1:n diag
ψk,j πi,1:n /(ϑk,1:n )
j=1 i=1

i=1

−β
+β
≥ minj min-eig
−β
≥ 0.

n
X

ψk,j (πi,1:n /(ϑ∗k,1:n )2 )(πi,1:n /(ϑ∗k,1:n )2 )>

i=1

n
X

m
X

q=1
i=1
m
n
XX

!!

γψ
Pn i i,q
(πi,1:n /ϑ∗k,1:n )> qq,1:n
γ
ψ
r=1 r r,q

>
γi ψi,j qj,1:n

j=1 i=1
n
X

diag

n
X

!2 !

ψk,j πi,1:n /(ϑ∗k,1:n )2

i=1

ψk,j (πi,1:n /(ϑ∗k,1:n )2 )(πi,1:n /(ϑ∗k,1:n )2 )>

i=1

qj,1:n

!!

!

qj,1:n

!!

Here, we have used the fact that ∆2 F (Ψ, α; Π, Θ∗ , γ) is continuous and strictly positive for β0 < β. We have
also used the spectral theorem [46] to obtain a lower bound on the lowest eigenvalue for a self-adjoint operator.
Equality to zero, ∆2 F (Ψ, α; Π, Θ∗ , γ) = 0, can thus only be obtained for β < βc if and only if
!
!!
n
n
X
X
2
2
2
>
min-eig diag
ψk,j πi,1:n /ϑk,1:n − β
ψk,j (πi,1:n /ϑk,1:n )(πi,1:n /ϑk,1:n )
= 0.
i=1

i=1

At such a point β, a bifurcation in d /d
occurs, for a finite perturbation term Q, and
the minimum is no longer stable. That is, there is a bifurcation on a solution branch that is fixed by the algebraic
group of all permutations on m < n symbols, Sym(m), which follows from the equivariant branching lemma and
the Smoller-Wasserman theorem; this bifurcation is symmetry breaking. The equivariant branching lemma gives
explicit bifurcating directions of the m branching solutions, each of which has symmetry Sym(m − 1). The
branches are hence associated with a Θ and Ψ of different cardinalities m.

2

2

F (Ψ, α; Π, Θ∗β0+Q, γ)|=0

Proposition 3.5. Let Rπ = (Vπ , Eπ , Π) and Rϕ = (Vϕ , Eϕ , Φ) be transition models of two Markov chains over
n and m states, respectively, where m < n. Rϑ = (Vϑ , Eϑ , Θ) is a joint model, with m+n states. The systematic underestimation of the information cost of the Shannon mutual information term in definition 3.11 can be
second-order minimized by solving the following optimization problem
!
Pm
Pn
Pn Pm
j=1 αj
i=1 ψi,j log(ψi,j /γi )
i=1
j=1 γi ψi,j g(πi,1:n , ϑi,1:n ) ≤ r,
Pm Pn
minΨ∈Rn×m , Θ∈Rm×n
P
Pm
2
+
+
+ j=1 i=1 γi ψi,j
/2nlog(2)αj 0 ≤ ϑi,k , ψi,k ≤ 1, m
k=1 ϑi,k = 1,
k=1 ψi,k = 1

where β = 2

Pm

j=1

αj

Pn

i=1

ψi,j log(ψi,j /γi )

/2n.

Proof: Let us assume that we approximate the marginal distribution γi0 by γi0 = γi +δγi , where γi is the true

marginal, with zero P
average over
Pnall possible realizations. There is hence an underestimation of the Shannon
m
mutual information j=1 αj i=1 ψi,j log(ψi,j /γi ), which can be found by taking the multi-order Taylor expansion about γi ,
!
m
n
X
X
ψi,j
αj
ψi,j log
γi
j=1
i=1
γi +δγi

SUBMITTED TO ENTROPY
m
X

25
n
X

ψi,j
=
αj
ψi,j log
γi
j=1
i=1
m
X

n
X

ψi,j
≥
αj
ψi,j log
γi
j=1
i=1
m
X

n
X

ψi,j
≥
αj
ψi,j log
γi
j=1
i=1

!
!
!

∞
X
1
(−1)p
+
p−1
p(p−1)
log(2)α
j
p=2

m X
n X
m
X

n
X

γw ψw,s
ψr,j δγr
r=1
j=1 w=1 s=1
!!
m X
n
n X
m
X
γw ψw,s X 2
ψr,j γr
αj
r=1
j=1 w=1 s=1

+

1
2plog(2)

+

Pm
Pn
1
2 j=1 αj i=1 ψi,j log(ψi,j /γi ) .
2plog(2)

!p !

For the last and second-to-last steps, weP
considered
only the second-order
Taylor series expansion term. For the
Pn P
n Pm
m
log(ψi,j /αj )
last step, we used thePfollowing
equality
ρ
ψ
/α
=
, which is
j
i=1
j=1 i,j i,j
i=1
j=1 ρi,j 2
Pn
m
αj
ψi,j log(ψi,j /γi )
j=1
i=1
bounded below by 2
. Here, ρi,j is the joint distribution
of
the random variables. It
Pm
Pn
can be seen that the Shannon mutual information term is underestimated by 2 j=1 αj i=1 ψi,j log(ψi,j /γi ) /2plog(2)
bits. The bound on the second-order Taylor expansion hence has a rescaled slope.
Plugging the augmented Shannon information term into the value of information Lagrangian, in place of the
original Shannon mutual information constraint, and solving yields following update for Ψ,
!p
n
m X
∞
X
X
αj
(−1)p
ψi,j =
γi ψi,s
ψr,j δγr
exp − βlog(2)g(πi,1:n , ϑj,1:n ) +
z
pαjp
r=1
j,s=1 p=2
!p−1 !
n X
m
n
X
X
1
γi δγw ψw,s
ψr,j δγr
−
(p−1)γi αjp−1 w=1 s=1
r=1

where z is a normalization factor that ensures the entries
Ψ are
Pof
Pnprobabilities. Considering only the secondm
order terms from the Taylor expansion and using β = 2 j=1 αj i=1 ψi,j log(ψi,j /γi )/2n leads to a second-order
minimization of the underestimation of information.

Proposition 3.6. Let Rπ = (Vπ , Eπ , Π) be a transition model of a Markov chain with n states, where Π ∈ Rn×n
+

is nearly completely decomposable into m Markov sub-chains.
m×m
(i) The associated
found by solving the value of information
Pnlow-order
Pnj stochastic matrix
Pni Φ ∈ R+
i
is given by ϕi,j = pi =1 qj =1 πpi ,qi γpi / qi γqi , where pi , qi represent state indices p = 1, . . . , ni
associated with block i, while qj represents a state index q = 1, . . . , nj into block j. The variable γpi = γpi (Π)
denotes the invariant-distribution
probability of state p in block i of Π.
Pn
(ii) Suppose that γpi / qii γqi = vp∗i (1i ) is approximated by the entries of the first left-eigenvector v ∗ (1i )
for block i of Π∗ . We then have that
!
nj
ni
X
X
∗
γ
vpi (1i )
πpi ,qi − γ(Π)Ψ ∼ O(ε2 )
pi =1

qj =1

1

where the first term is the invariant distribution of the low-order matrix γ(Φ), under the simplifying assumption, and Ψ ∈ Rn×m
is the probabilistic partition matrix found by solving the value of information.
+

Proof: For part (i), we note that the aggregated stochastic matrix for nearly-completely decomposable chains,
obtained via Θ = U > Π and Φ = ΘΨ, satisfies the implicit equation

ϕ1:n,j =

nk
m X
X

ϑj,pi πpi ,1:n

k=1 pi =1

ψp ,j γp
e−βg(πpi ,1:n ,ϑj,1:n )
where uj,pi = Pm Pniq i
and ψpi ,j = Pm Pnq
.
−βg(πsq ,1:n ,ϑq,1:n )
sq =1 ψsq ,q γsq
q=1
sq =1 e
q=1

In what follows, we want to assess the form of the aggregated stochastic matrix Φ when it contains m state
groups. However, β dictates the number of state groups in a manner that is dependent on the original stochastic
matrix Π. We therefore simply consider what happens when β is infinite and note that the same expression for Φ
can be obtained for finite βs. In the former case, we have that
Ig(πp ,1:n ,ϑj,1:n ) = minj 0 g(πpi ,1:n ,ϑj0 ,1:n ) γpi
limβ→∞ uj,pi = Pm Pnq i
sq =1 Ig(πsq ,1:n ,ϑq,1:n ) = minq 0 g(πsq ,1:n ,ϑq0 ,1:n ) γsq
q=1
0

e−βg(πpi ,1:n ,ϑj,1:n )−minj g(πpi ,1:n ,ϑj0 ,1:n )
limβ→∞ ψpi ,j = Pm −βg(π
0
pi ,1:n ,ϑq,1:n )−minj g(πpi ,1:n ,ϑj 0 ,1:n )
q=1 e

SUBMITTED TO ENTROPY

26

where I is the indicator function. Due to the nearly-completely decomposable structure of the Markov chain,
Ig(πpi ,1:n ,ϑj,1:n )=minj 0 g(πpi ,1:n ,ϑj0 ,1:n ) γpi = γpi ∀j. Hence,
limβ→∞ ϕi,j = (limβ→∞ uj,pi )Π(limβ→∞ ψpi ,j ) =

pi

Part (ii) follows from the work of Courtois [21].

nj
X
γ
Pnpi i
πpi ,qi .
qi γqi q =1
=1

ni
X

j

