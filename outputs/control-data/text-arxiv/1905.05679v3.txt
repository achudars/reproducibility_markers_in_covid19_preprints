List-Decodable Linear Regression

arXiv:1905.05679v3 [cs.DS] 30 May 2019

Sushrut Karmalkar∗

Adam R. Klivans †

Pravesh K. Kothari‡

May 31, 2019

Abstract
We give the ﬁrst polynomial-time algorithm for robust regression in the list-decodable
setting where an adversary can corrupt a greater than 1/2 fraction of examples.
For any α < 1, our algorithm takes as input a sample {(x i , y i )}i 6 n of n linear equations where
αn of the equations satisfy y i  hx i , ℓ ∗ i + ζ for some small noise ζ and (1 − α)n of the equations
are arbitrarily chosen. It outputs a list L of size O(1/α) - a ﬁxed constant - that contains an ℓ that
is close to ℓ ∗ .
Our algorithm succeeds whenever the inliers are chosen from a certiﬁably anti-concentrated
8
distribution D. In particular, this gives a (d/α)O(1/α ) time algorithm to ﬁnd a O(1/α) size list
when the inlier distribution is standard Gaussian. For discrete product distributions that are
anti-concentrated only in regular directions, we give an algorithm that achieves similar guarantee
under the promise that ℓ ∗ has all coordinates of the same magnitude. To complement our result,
we prove that the anti-concentration assumption on the inliers is information-theoretically
necessary.
Our algorithm is based on a new framework for list-decodable learning that strengthens the
“identiﬁability to algorithms” paradigm based on the sum-of-squares method.
In an independent and concurrent work, Raghavendra and Yau [RY19] also used the Sumof-Squares method to give a similar result for list-decodable regression.

∗ University

of Texas at Austin. Supported by NSF Award CNS-1414023
of Texas at Austin. Supported by NSF Award CCF-1717896
‡ Princeton University and Institute for Advanced Study. Supported by Schmidt Foundation Fellowship and Avi
Wigderson’s NSF Award CCF-1412958.
† University

Contents
1 Introduction
1.1 Our Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1
2

2 Overview of our Technique

5

3 Preliminaries
9
3.1 Pseudo-distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3.2 Sum-of-squares proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4 Algorithm for List-Decodable Robust Regression
11
4.1 List-Decodable Regression for Boolean Vectors . . . . . . . . . . . . . . . . . . . . . . 15
5 Certifiably Anti-Concentrated Distributions

17

6 Information-Theoretic Lower Bounds for List-Decodable Regression

19

References

21

A Polynomial Approximation for Core-Indicator

25

B Brute-force search can generate a exp(d) size list

28

1 Introduction
In this work, we design algorithms for the problem of linear regression that are robust to training
sets with an overwhelming (≫ 1/2) fraction of adversarially chosen outliers.
Outlier-robust learning algorithms have been extensively studied (under the name robust statistics) in mathematical statistics [Tuk75, MMY06, Hub11, HRRS11]. However, the algorithms resulting from this line of work usually run in time exponential in the dimension of the data [Ber06].
An inﬂuential line of recent work [KLS09, ABL13, DKK+ 16b, LRV16, CSV17, KS17a, KS17b, HL17,
DKK+ 17, DKS17, KKM18] has focused on designing eﬃcient algorithms for outlier-robust learning.
Our work extends this line of research. Our algorithms work in the “list-decodable learning”
framework. In this model, a majority of the training data (a 1 − α fraction) can be adversarially
corrupted leaving only an α ≪ 1/2 fraction of “inliers”. Since uniquely recovering the underlying
parameters is information-theoretically impossible in such a setting, the goal is to output a list (with
an absolute constant size) of parameters, one of which matches the ground truth. This model was
introduced in [BBV08] to give a discriminative framework for clustering. More recently, beginning
with [CSV17], various works [DKS18, KS17a] have considered this as a model of “untrusted” data.
There has been phenomenal progress in developing techniques for outlier-robust learning with
a small (≪ 1/2)-fraction of outliers (e.g. outlier “ﬁlters” [DKK+ 16a, DKK+ 17, CDG19, DKK+ 18b],
separation oracles for inliers [DKK+ 16a] or the sum-of-squares method [KS17b, HL17, KS17a,
KKM18]). In contrast, progress on algorithms that tolerate the signiﬁcantly harsher conditions
in the list-decodable setting has been slower. The only prior works [CSV17, DKS18, KS17a] in this
direction designed list-decodable algorithms for mean estimation via problem-speciﬁc methods.
In this paper, we develop a principled technique to give the ﬁrst eﬃcient list-decodable learning
algorithm for the fundamental problem of linear regression. Our algorithm takes a corrupted set
of linear equations with an α ≪ 1/2 fraction of inliers and outputs a O(1/α)-size list of linear
functions, one of which is guaranteed to be close to the ground truth (i.e., the linear function that
correctly labels the inliers). A key conceptual insight in this result is that list-decodable regression
information-theoretically requires the inlier-distribution to be “anti-concentrated”. Our algorithm
succeeds whenever the distribution satisﬁes a stronger “certiﬁable anti-concentration” condition
that is algorithmically “usable’. This class includes the standard gaussian distribution and more
generally, any spherically symmetric distribution with strictly sub-exponential tails.
Prior to our work1, the state-of-the-art outlier-robust algorithms for linear regression [KKM18,
DKS19, DKK+ 18a, PSBR18] could handle only a small (< 0.1)-fraction of outliers even under strong
assumptions on the underlying distributions.
List-decodable regression generalizes the well-studied [DV89, JJ94, FS10, YCS13, BWY14,
CYC14, ZJD16, SJA16, LL18] and easier problem of mixed linear regression: given k “clusters” of
examples that are labeled by one out of k distinct unknown linear functions, ﬁnd the unknown
set of linear functions. All known techniques for the problem rely on faithfully estimating certain
moment tensors from samples and thus, cannot tolerate the overwhelming fraction of outliers in
1There’s a long line of work on robust regression algorithms (see for e.g. [BJKK17, KP19]) that can tolerate corruptions
only in the labels. We are interested in algorithms robust against corruptions in both examples and labels.

1

the list-decodable setting. On the other hand, since we can take any cluster as inliers and treat
rest as outliers, our algorithm immediately yields new eﬃcient algorithms for mixed linear regression. Unlike all prior works, our algorithms work without any pairwise separation or bounded
condition-number assumptions on the k linear functions.
List-Decodable Learning via the Sum-of-Squares Method Our algorithm relies on a strengthening of the robust-estimation framework based on the sum-of-squares (SoS) method. This paradigm
has been recently used for clustering mixture models [HL17, KS17a] and obtaining algorithms for
moment estimation [KS17b] and linear regression [KKM18] that are resilient to a small (≪ 1/2) fraction of outliers under the mildest known assumptions on the underlying distributions. At the heart
of this technique is a reduction of outlier-robust algorithm design to just ﬁnding “simple” proofs
of unique “identiﬁability” of the unknown parameter of the original distribution from a corrupted
sample. However, this principled method works only in the setting with a small (≪ 1/2) fraction
of outliers. As a consequence, the work of [KS17a] for mean estimation in the list-decodable setting
relied on “supplementing” the SoS method with a somewhat problem-dependent technique.
As an important conceptual contribution, our work yields a framework for list-decodable
learning that recovers some of the simplicity of the general blueprint. Central to our framework is
a general method of rounding by votes for “pseudo-distributions” (see Section 2) in the setting with
≫ 1/2 fraction outliers. Our rounding builds on the work of [KS19] who developed such a method
to give a simpler proof of the list-decodable mean estimation result of [KS17a].
Prior results discussed above hold for any underlying distribution that has upper-bounded
low-degree moments and such bounds are “captured” within the SoS system. Such conditions
are called as “certiﬁed bounded moment” inequalities. An important contribution of this work
is to formalize anti-concentration inequalities within the SoS system and prove such inequalities
for natural distribution families. Unlike bounded moment inequalities, there is no canonical
encoding within SoS for such statements. We choose an encoding that allows proving certiﬁed anticoncentration for a distribution by showing the existence of a certain approximating polynomial.
This allows showing certiﬁed anti-concetration via a modular approach relying on a beautiful line
of works that construct “weighted ” polynomial approximators [Lub07].
We believe that our framework for list-decodable estimation and our formulation of certiﬁed
anti-concentration condition will likely have further applications in outlier-robust learning.

1.1 Our Results
We ﬁrst deﬁne our model for generating samples for list-decodable regression.
Model 1.1 (Robust Linear Regression). For 0 < α < 1 and ℓ ∗ ∈ d with kℓ ∗ k2 6 1, let LinD (α, ℓ ∗ )
denote the following probabilistic process to generate n noisy linear equations S  {hx i , ai  yi |
1 6 i 6 n} in variable a ∈ d with αn inliers I and (1 − α)n outliers O:
1. Construct I by choosing αn i.i.d. samples x i ∼ D and set yi  hx i , ℓ ∗ i + ζ for additive noise
ζ,

2

2. Construct O by choosing the remaining (1 − α)n equations arbitrarily and potentially adversarially w.r.t the inliers I.
Note that α measures the “signal” (fraction of inliers) and can be ≪ 1/2. The bound on the
norm of ℓ ∗ is without any loss of generality. For the sake of exposition, we will restrict to ζ  0 for
most of this paper and discuss (see Remarks 1.6 and 4.4) how our algorithms can tolerate additive
noise.
An η-approximate algorithm for list-decodable regression takes input a sample from LinD (α, ℓ ∗ )
and outputs a constant (depending only on α) size list L of linear functions such that there is some
ℓ ∈ L that is η-close to ℓ ∗ .
One of our key conceptual contributions is to identify the strong relationship between anticoncentration inequalities and list-decodable regression. Anti-concentration inequalities are wellstudied [Erd45, TV12, RV08] in probability theory and combinatorics. The simplest of these
inequalities upper bound the probability that a high-dimensional random variable has zero projections in any direction.
Definition 1.2 (Anti-Concentration). A d -valued zero-mean random variable Y has a δ-anticoncentrated distribution if [hY, vi  0] < δ.
In Proposition 2.4, we provide a simple but conceptually illuminating proof that anticoncentration is suﬃcient for list-decodable regression. In Theorem 6.1, we prove a sharp converse and show that anti-concentration is information-theoretically necessary for even noiseless
list-decodable regression. This lower bound surprisingly holds for a natural distribution: uniform
distribution on {0, 1} d and more generally, uniform distribution on [q]d for [q]  {0, 1, 2 . . . , q}.
Our lower bound, in fact, shows the impossibility of even the “easier” problem of mixed linear
regression on this distribution.
Theorem 1.3 (See Proposition 2.4 and Theorem 6.1). There is a (ineﬃcient) list-decodable regression
algorithm for LinD (α, ℓ ∗ ) with list size O( α1 ) whenever D is α-anti-concentrated. Further, there exists
a distribution D on d that is (α + ε)-anti-concentrated for every ε > 0 but there is no algorithm for
α
∗
2 -approximate list-decodable regression for LinD (α, ℓ ) that returns a list of size < d.
To handle additive noise of variance ζ 2 , we need a control of [|hx, vi| 6 ζ]  1(|hx, vi| 6 δ).
For our eﬃcient algorithms, in addition, we need that the anti-concentration property to have
a low-degree “sum-of-squares” certiﬁcate. SoS is a proof system that reasons about polynomial
inequalities. Since the “core indicator” 1(|hx, vi| 6 δ) is not a polynomial, we phrase certiﬁed
anti-concentration in terms of an approximating polynomial p for the core indicator.
For this section, we will use "low-degree sum-of-squares proof" informally and encourage the
reader to think of certiﬁed anti-concentration as a stronger version of anti-concentration that the
SoS method can reason about.
Definition 1.4 (Certiﬁable Anti-Concentration). A random variable Y has a k-certiﬁably (C, δ)-anticoncentrated distribution if there is a univariate polynomial p satisfying p(0)  1 such that there
is a degree k sum-of-squares proof of the following two inequalities:
3

1. ∀ v, hY, vi 2 6 δ 2 hY, vi 2 implies (p(hY, vi) − 1)2 6 δ 2 .
2. ∀ v, kv k22 6 1 implies

p 2 (hY, vi) 6 Cδ.

We are now ready to state our main result.
Theorem 1.5 (List-Decodable Regression). For every α, η > 0 and a k-certiﬁably (C, α 2 η2 /10C)-anticoncentrated distribution D on d , there exists an algorithm that takes input a sample generated according
to LinD (α, ℓ ∗ ) and outputs a list L of size O(1/α) such that there is an ℓ ∈ L satisfying kℓ − ℓ ∗ k2 < η with
probability at least 0.99 over the draw of the sample. The algorithm needs a sample of size n  (kd)O(k) and
2
runs in time n O(k)  (kd)O(k ) .
Remark 1.6 (Tolerating Additive Noise). For additive noise (not necessarily independent across
samples) of variance ζ 2 in the inlier labels, our algorithm, in the same running time and sample
complexity, outputs a list of size O(1/α) that contains an ℓ satisfying kℓ − ℓ ∗ k2 6 αζ + η. Since we
normalize ℓ ∗ to have unit norm, this guarantee is meaningful only when ζ ≪ α.

Remark 1.7 (Exponential Dependence on 1/α). List-decodable regression algorithms immediately
yield algorithms for mixed linear regression (MLR) without any assumptions on the components.
The state-of-the-art algorithms for MLR with gaussian components [LL18, SJA16] has an exponential dependence on k  1/α in the running time in the absence of strong separation/condition
number assumptions. Liang and Liu [LL18] (see Page 10 of their paper) use the relationship to
learning mixtures of k gaussians (with an exp(k) lower bound [MV10]) to hint at the impossibility of algorithms with polynomial dependence on 1/α for MLR and thus, also for list-decodable
regression.
Certifiably anti-concentrated distributions In Section 5, we show certiﬁable anti-concentration
of some well-studied families of distributions. This includes the standard gaussian distribution
and more generally any anti-concentrated spherically symmetric distribution with strictly subexponential tails. We also show that simple operations such as scaling, applying well-conditioned
linear transformations and sampling preserve certiﬁable anti-concentration. This yields:
Corollary 1.8 (List-Decodable Regression for Gaussian Inliers). For every α, η > 0 there’s an algorithm
for list-decodable regression
for
the model LinD (α, ℓ ∗ ) with D  N(0, Σ) with
λ max
(Σ)/λ min (Σ)  O(1)




that needs n  (d/αη)

O

1
α4 η4

samples and runs in time n

O

1
α4 η4

 (d/αη)

O

1
α8 η8

.

We note that certiﬁably anti-concentrated distributions are more restrictive compared to the
families of distributions for which the most general robust estimation algorithms work [KS17b,
KS17a, KKM18]. To a certain extent, this is inherent. The families of distributions considered
in these prior works do not satisfy anti-concentration in general. And as we discuss in more
detail in Section 2, anti-concentration is information-theoretically necessary (see Theorem 1.3) for
list-decodable regression. This surprisingly rules out families of distributions that might appear
natural and “easy”, for example, the uniform distribution on {0, 1} n .
We rescue this to an extent for the special case when ℓ ∗ in the model Lin(α, ℓ ∗ ) is a "Boolean
vector", i.e., has all coordinates of equal magnitude. Intuitively, this helps because while the the
4

uniform distribution on {0, 1} n (and more generally, any discrete product distribution) is badly
anti-concentrated in sparse directions, they are well anti-concentrated [Erd45] in the directions
that are far from any sparse vectors.
As before, for obtaining eﬃcient algorithms, we need to work with a certiﬁed version (see
Deﬁnition 4.5) of such a restricted anti-concentration condition. As a speciﬁc Corollary (see
Theorem 4.6 for a more general statement), this allows us to show:
Theorem 1.9 (List-Decodable Regression for Hypercube Inliers). For every α, η > 0 there’s an ηapproximate algorithm for list-decodable regression for the model LinD (α, ℓ ∗ ) with D is uniform on {0, 1} d
that needs n  (d/αη)

O(

1
)
α4 η4

samples and runs in time n

O(

1
)
α4 η4

 (d/αη)

O(

1
)
α8 η8

.

In Section 4.1, we obtain similar results for general product distributions. It is an important open
problem to prove certiﬁed anti-concentration inequalities for a broader family of distributions.
Concurrent Work In an independent and concurrent work, Raghavendra and Yau obtained
similar results for list-decodable linear regression based on the sum-of-squares method [RY19].

2 Overview of our Technique
In this section, we give a bird’s eye view of our approach and illustrate the important ideas in our
n
algorithm for list-decodable regression. Thus, given a sample S  {(x i , yi )}i1
from LinD (α, ℓ ∗ ),
we must construct a constant-size list L of linear functions containing an ℓ close to ℓ ∗ .
Our algorithm is based on the sum-of-squares method. We build on the “identiﬁability to
algorithms” paradigm developed in several prior works [BM16, BKS15, MSS16, KS17b, HL17,
KS17a, KKM18] with some important conceptual diﬀerences.
An inefficient algorithm Let’s start by designing an ineﬃcient algorithm for the problem. This
may seem simple at the outset. But as we’ll see, solving this relaxed problem will rely on some
important conceptual ideas that will serve as a starting point for our eﬃcient algorithm.
Without computational constraints, it is natural to just return the list L of all linear functions ℓ
that correctly labels all examples in some S ⊆ S of size αn. We call such an S, a large, soluble set.
True inliers I satisfy our search criteria so ℓ ∗ ∈ L. However, it’s not hard to show (Proposition B.1 )
that one can choose outliers so that the list so generated has size exp(d) (far from a ﬁxed constant!).
A potential ﬁx is to search instead for a coarse soluble partition of S, if it exists, into disjoint
S1 , S2 , . . . , S k and linear functions ℓ 1 , ℓ 2 , . . . , ℓ k so that every |S i | > αn and ℓ i correctly computes
the labels in S i . In this setting, our list is small (k 6 1/α). But it is easy to construct samples S for
which this fails because there are coarse soluble partitions of S where every ℓ i is far from ℓ ∗ .
Anti-Concentration It turns out that any (even ineﬃcient) algorithm for list-decodable regression
provably (see Theorem 6.1) requires that the distribution of inliers2 be suﬃciently anti-concentrated:
2As in the standard robust estimation setting, the outliers are arbitrary and potentially adversarially chosen.

5

Definition 2.1 (Anti-Concentration). A d -valued random variable Y with mean 0 is δ-anticoncentrated3 if for all non-zero v, [hY, vi  0] < δ. A set T ⊆ d is δ-anti-concentrated if
the uniform distribution on T is δ-anti-concentrated.
As we discuss next, anti-concentration is also suﬃcient for list-decodable regression. Intuitively,
this is because anti-concentration of the inliers prevents the existence of a soluble set that intersects
signiﬁcantly with I and yet can be labeled correctly by ℓ , ℓ ∗ . This is simple to prove in the special
case when S admits a coarse soluble partition.
Proposition 2.2. Suppose I is α-anti-concentrated. Suppose there exists a partition S1 , S2 , . . . , S k ⊆ S
such that each |S i | > αn and there exist ℓ 1 , ℓ 2 , . . . , ℓ k such that y j  hℓ i , x j i for every j ∈ S i . Then, there
is an i such that ℓ i  ℓ ∗ .
Proof. Since k 6 1/α, there is a j such that |I ∩ S j | > α|I|. Then, hx i , ℓ j i  hx i , ℓ ∗ i for every
i ∈ I ∩ S j . Thus, i∼I [hx i , ℓ j − ℓ ∗ i  0] > α. This contradicts anti-concentration of I unless
ℓ j − ℓ ∗  0.

The above proposition allows us to use any soluble partition as a certiﬁcate of correctness for
the associated list L. Two aspects of this certiﬁcate were crucial in the above argument: 1) largeness:
each S i is of size αn - so the generated list is small, and, 2) uniformity: every sample is used in
exactly one of the sets so I must intersect one of the S i s in at least α-fraction of the points.
Identifiability via anti-concentration For arbitrary S, a coarse soluble partition might not exist.
So we will generalize coarse soluble partitions to obtain certiﬁcates that exist for every sample S
and guarantee largeness and a relaxation of uniformity (formalized below). For this purpose, it
is convenient to view such certiﬁcates as distributions µ on > αn size soluble subsets of S so any
collection C ⊆ 2S of αn size sets corresponds to the uniform distribution µ on C.
To precisely deﬁne uniformity, let Wi (µ)  S∼µ [1(i ∈ S)] be the “frequency of i”, that is,
probability that the ith sample is chosen to be in a set drawn according to µ. Then, the uniform
distribution µ on any coarse soluble k-partition satisﬁes Wi  1k for every i. That is, all samples
Í
i ∈ S are uniformly used in such a µ. To generalize this idea, we deﬁne i Wi (µ)2 as the distance
to uniformity of µ. Up to a shift, this is simply the variance in the frequencies of the points
in S used in draws from µ. Our generalization of a coarse soluble partition of S is any µ
Í
that minimizes i Wi (µ)2 , the distance to uniformity, and is thus maximally uniform among all
distributions supported on large soluble sets. Such a µ can be found by convex programming.
The following claim generalizes Proposition 2.2 to derive the same conclusion starting from
any maximally uniform distribution supported on large soluble sets.
Proposition 2.3. For a maximally uniform µ on αn size soluble subsets of S,

Í

Í

i∈I

S∼µ [1 (i

∈ S)] > α|I|.

The proof proceeds by contradiction (see Lemma 4.3). We show that if i∈I Wi (µ) 6 α|I|, then
we can strictly reduce the distance to uniformity by taking a mixture of µ with the distribution
that places all its probability mass on I. This allow us to obtain an (ineﬃcient) algorithm for
list-decodable regression establishing identiﬁability.
3Deﬁnition 1.4 diﬀers slightly to handle list-decodable regression with additive noise in the inliers.

6

Proposition 2.4 (Identiﬁability for List-Decodable Regression). Let S be sample from Lin(α, ℓ ∗ ) such
20
that I is δ-anti-concentrated for δ < α. Then, there’s an (ineﬃcient) algorithm that ﬁnds a list L of size α−δ
∗
such that ℓ ∈ L with probability at least 0.99.
20
,
Proof. Let µ be any maximally uniform distribution over αn size soluble subsets of S. For k  α−δ
let S1 , S2 , . . . , S k be independent samples from µ. Output the list L of k linear functions that
correctly compute the labels in each S i .
Í
To see why ℓ ∗ ∈ L, observe that |S j ∩ I |  i∈I 1(i ∈ S j ) > α|I|. By averaging, [|S j ∩
α−δ
α+δ
I| > α+δ
2 |I|] > 2 . Thus, there’s a j 6 k so that |S j ∩ I| > 2 |I| with probability at least
20

α−δ > 0.99. We can now repeat the argument in the proof of Proposition 2.2 to conclude
1 − (1 − α−δ
2 )
that any linear function that correctly labels S j must equal ℓ ∗ .


An efficient algorithm Our identiﬁability proof suggests the following simple algorithm: 1) ﬁnd
any maximally uniform distribution µ on soluble subsets of size αn of S, 2) take O(1/α) samples
S i from µ and 3) return the list of linear functions that correctly label the equations in S i s. This is
ineﬃcient because searching over distributions is NP-hard in general.
To make this into an eﬃcient algorithm, we start by observing that soluble subsets S ⊆ S of size
αn can be described by the following set of quadratic equations where w stands for the indicator
of S and ℓ, the linear function that correctly labels the examples in S.

Ín





i1 w i  αn 





2
 ∀i ∈ [n].



wi  wi 
A w,ℓ :

∀i ∈ [n]. w i · (yi − hx i , ℓi)  0 








2


kℓ
k
6
1



(2.1)

Our eﬃcient algorithm searches for a maximally uniform pseudo-distribution on w satisfying
(2.1). Degree k pseudo-distributions (see Section 3 for precise deﬁnitions) are generalization of distributions that nevertheless “behave” just as distributions whenever we take (pseudo)-expectations
(denoted by ˜ ) of a class of degree k polynomials. And unlike distributions, degree k pseudodistributions satisfying4 polynomial constraints (such as (2.1)) can be computed in time n O(k) .
For the sake of intuition, it might be helpful to (falsely) think of pseudo-distributions µ̃ as
simply distributions where we only get access to moments of degree 6 k. Thus, we are allowed
to compute expectations of all degree 6 k polynomials with respect to µ̃. Since Wi (µ̃)  ˜ µ̃ w i
are just ﬁrst moments of µ̃, our notion of maximally uniform distributions extends naturally to
pseudo-distributions. This allows us to prove an analog of Proposition 2.3 for pseudo-distributions
and gives us an eﬃcient replacement for Step 1.
Proposition 2.5. For any maximally uniform µ̃ of degree > 2,

Í

i∈I

˜ µ̃ [w i ] > α|I|  α Íi∈[n] ˜ µ̃ [w i ] .

For Step 2, however, we hit a wall: it’s not possible to obtain independent samples from µ̃ given
only low-degree moments.
4See Fact 3.3 for a precise statement.

7

Rounding by Votes To circumvent this hurdle, our algorithm departs from rounding strategies
for pseudo-distributions used in prior works and instead “rounds” each sample to a candidate
linear function. While a priori, this method produces n diﬀerent candidates instead of one, we
will be able to extract a list of O( α1 ) size that contains the true vector from them. This step will
crucially rely on anti-concentration properties of I.
˜ µ̃ [w i ℓ]
whenever ˜ µ̃ [w i ] , 0 (set v i to zero, otherwise). This is simply
Consider the vector v i 
˜ µ̃ [w i ]

the (scaled) average, according to µ̃, of all the linear functions ℓ that are used to label the sets S of
size αn in the support of µ̃ whenever i ∈ S. Further, v i depends only on the ﬁrst two moments of
µ̃.
We think of v i s as “votes”cast by the ith sample for the unknown linear function. Let us focus
our attention on the votes v i of i ∈ I - the inliers. We will show that according to the distribution
proportional to ˜ [w], the average ℓ 2 distance of v i from ℓ ∗ is at max η:

Í

Õ
1
˜ [w i ]kv i − ℓ ∗ k2 < η .
˜
i∈I [w i ]

(⋆)

i∈I

Before diving into (⋆), let’s see how it gives us our eﬃcient list-decodable regression algorithm:
1. Find a pseudo-distribution µ̃ satisfying (2.1) that minimizes distance to uniformity
Í ˜
2
i µ̃ [w i ] .
2. For O( α1 ) times, independently choose a random index i ∈ [n] with probability proportional
to ˜ µ̃ [w i ] and return the list of corresponding v i s.

Step 1 above is a convex program - it minimizes a norm subject on the convex set of pseudodistributions - and can be solved in polynomial time. Let’s analyze step 2 to see why the algorithm
inequality, conditioned on i ∈ I, kv i − ℓ ∗ k2 6 2η with probability
works. Using (⋆) and Markov’s
Í

> 1/2. By Proposition 2.5,

˜
Íi∈I [w i ]
i∈[n] ˜ [w i ]

> α so i ∈ I with probability at least α. Thus in each iteration

of step 2, with probability at least α/2, we choose an i such that v i is 2η-close to ℓ ∗ . Repeating
O(1/α) times gives us the 0.99 chance of success.

(⋆) via anti-concentration As in the information-theoretic argument, (⋆) relies on the anticoncentration of I. Let’s do a quick proof for the case when µ̃ is an actual distribution µ.
Proof of (⋆) for actual distributions µ. Observe that µ is a distribution over (w, ℓ) satisfying (2.1).
Recall that w indicates a subset S ⊆ S of size αn and w i  1 iﬀ i ∈ S. And ℓ ∈ d satisﬁes all the
equations in S.
Í
Í
By Cauchy-Schwarz, i k µ [w i ℓ] − µ [w i ]ℓ ∗ k 6 µ [ i∈I w i kℓ − ℓ ∗ k]. Next, as in Proposition 2.2, since I is η-anti-concentrated, and for all S such that |I ∩ S| > η|I|, ℓ − ℓ ∗  0. Thus,
any such S in the support of µ contributes 0 to the expectation above. We will now show that the
contribution from the remaining terms is upper bounded by η. Observe that since kℓ − ℓ ∗ k 6 2,

Í
Í
∗
∗
∗

µ [ i∈I w i kℓ − ℓ k]  µ [1 |S ∩ I| < η|I| w i kℓ − ℓ k]  µ [ i∈S∩I kℓ − ℓ k] 6 2η|I|.
8

SoSizing Anti-Concentration The key to proving (⋆) for pseudo-distributions is a sum-of-squares
(SoS) proof of anti-concentration inequality: x∼I [hx, vi  0] 6 η in variable v. SoS is a restricted
system for proving polynomial inequalities subject to polynomial inequality constraints. Thus, to
even ask for a SoS proof we must phrase anti-concentration as a polynomial inequality.
To do this, let p(z) be a low-degree polynomial approximator for the function 1 (z  0).
Then, we can hope to “replace” the use of the inequality x∼I [hx, vi  0] 6 η ≡ x∼I [1(hx, vi 
0)] 6 η in the argument above by x∼I [p(hx, vi)] 6 η. Since polynomials grow unboundedly for
large enough inputs, it is necessary for the uniform distribution on I to have suﬃciently light-tails
to ensure that x∼I p(hx, vi) is small. In Lemma A.1, we show that anti-concentration and strictly
sub-exponential tails are suﬃcient to construct such a polynomial.
We can ﬁnally ask for a SoS proof for x∼I p(hx, vi) 6 η in variable v. We prove such certiﬁed
anti-concentration inequalities for broad families of inlier distributions in Section 5.

3 Preliminaries
In this section, we deﬁne pseudo-distributions and sum-of-squares proofs. See the lecture notes
[BS16] for more details and the appendix in [MSS16] for proofs of the propositions appearing here.
Let x  (x1 , x2 , . . . , x n ) be a tuple of n indeterminates and let [x] be the set of polynomials
with real coeﬃcients and indeterminates x1 , . . . , x n . We say that a polynomial p ∈ [x] is a
sum-of-squares (sos) if there are polynomials q 1 , . . . , q r such that p  q 12 + · · · + q 2r .

3.1 Pseudo-distributions
Pseudo-distributions are generalizations of probability distributions. We can represent a discrete
(i.e., ﬁnitely supported) probability distribution over n by its probability mass function D : n →
Í
 such that D > 0 and x∈supp(D) D(x)  1. Similarly, we can describe a pseudo-distribution by its
mass function by relaxing the constraint D > 0 to passing certain low-degree non-negativity tests.
Concretely, a level-ℓ pseudo-distribution is a ﬁnitely-supported function D : n →  such that
Í
Í
2
x D(x)  1 and
x D(x) f (x) > 0 for every polynomial f of degree at most ℓ/2. (Here, the
summations are over the support of D.) A straightforward polynomial-interpolation argument
shows that every level-∞-pseudo distribution satisﬁes D > 0 and is thus an actual probability
distribution. We deﬁne the pseudo-expectation of a function f on d with respect to a pseudodistribution D, denoted ˜ D(x) f (x), as
˜ D(x) f (x) 

Õ
x

D(x) f (x) .

(3.1)

The degree-ℓ moment tensor of a pseudo-distribution D is the tensor D(x) (1, x1 , x2 , . . . , x n )⊗ℓ .
In particular, the moment tensor has an entry corresponding to the pseudo-expectation of all
monomials of degree at most ℓ in x. The set of all degree-ℓ moment tensors of probability
distribution is a convex set. Similarly, the set of all degree-ℓ moment tensors of degree d pseudodistributions is also convex. Unlike moments of distributions, there’s an eﬃcient separation oracle
for moment tensors of pseudo-distributions.
9

Fact 3.1 ([Sho87, Par00, Nes00, Las01]). For any n, ℓ ∈ , the following set has a n O(ℓ) -time weak
separation oracle (in the sense of [GLS81]):



˜ D(x) (1, x1 , x2 , . . . , x n )⊗d | degree-d pseudo-distribution D over n

(3.2)

.

This fact, together with the equivalence of weak separation and optimization [GLS81] allows
us to eﬃciently optimize over pseudo-distributions (approximately)—this algorithm is referred to
as the sum-of-squares algorithm. The level-ℓ sum-of-squares algorithm optimizes over the space of
all level-ℓ pseudo-distributions that satisfy a given set of polynomial constraints (deﬁned below).
Definition 3.2 (Constrained pseudo-distributions). Let D be a level-ℓ pseudo-distribution over n .
Let A  { f1 > 0, f2 > 0, . . . , f m > 0} be a system of m polynomial inequality constraints. We say
that D satisﬁes the system of constraints A at degree r, denoted D r A, if for every S ⊆ [m] and every
Í
Î
sum-of-squares polynomial h with deg h + i∈S max{deg f i , r}, ˜ D h · i∈S f i > 0.
A (without specifying the degree) if D 0 A holds. Furthermore, we say that
We write D
ℓ

Î

D r A holds approximately if the above inequalities are satisﬁed up to an error of 2−n · kh k· i∈S k f i k,
where k·k denotes the Euclidean norm5 of the coﬃcients of a polynomial in the monomial basis.
We remark that if D is an actual (discrete) probability distribution, then we have D A if and
only if D is supported on solutions to the constraints A. We say that a system A of polynomial
constraints is explicitly bounded if it contains a constraint of the form {kx k 2 6 M}. The following
fact is a consequence of Fact 3.1 and [GLS81],
Fact 3.3 (Eﬃcient Optimization over Pseudo-distributions). There exists an (n + m)O(ℓ) -time algorithm
that, given any explicitly bounded and satisﬁable system6 A of m polynomial constraints in n variables,
outputs a level-ℓ pseudo-distribution that satisﬁes A approximately.

3.2 Sum-of-squares proofs
Let f1 , f2 , . . . , f r and 1 be multivariate polynomials in x. A sum-of-squares proof that the constraints
{ f1 > 0, . . . , f m > 0} imply the constraint {1 > 0} consists of polynomials (p S )S⊆[m] such that
1

Õ

S⊆[m]

p S · Πi∈S f i .

(3.3)

We say that this proof has degree ℓ if for every set S ⊆ [m], the polynomial p S Πi∈S f i has degree at
most ℓ. If there is a degree ℓ SoS proof that { f i > 0 | i 6 r} implies {1 > 0}, we write:
{ f i > 0 | i 6 r}

ℓ

{1 > 0} .

(3.4)

For all polynomials f , 1 : n →  and for all functions F : n → m , G : n → k , H : p → n
such that each of the coordinates of the outputs are polynomials of the inputs, we have the following
inference rules:
{ f > 0, 1 > 0} A
,
A ℓ { f + 1 > 0}

A

ℓ

ℓ

{ f > 0}, A
A

ℓ+ℓ′

ℓ′

{1 > 0}

{ f · 1 > 0}
ℓ

(addition and multiplication)

5The choice of norm is not important here because the factor 2−n swamps the eﬀects of choosing another norm.
6Here, we assume that the bitcomplexity of the constraints in A is (n + m)O(1) .

10

A

ℓ

B, B

A

ℓ·ℓ′

ℓ′

C

(transitivity)

C

{F > 0}

{F(H) > 0}

ℓ

{G > 0}

ℓ·deg(H)

{G(H) > 0}

(substitution)

.

Low-degree sum-of-squares proofs are sound and complete if we take low-level pseudodistributions as models. Concretely, sum-of-squares proofs allow us to deduce properties of
pseudo-distributions that satisfy some constraints.
Fact 3.4 (Soundness). If D r A for a level-ℓ pseudo-distribution D and there exists a sum-of-squares
proof A r ′ B, then D r·r ′+r ′ B.
If the pseudo-distribution D satisﬁes A only approximately, soundness continues to hold if
we require an upper bound on the bit-complexity of the sum-of-squares A r ′ B (number of bits
required to write down the proof). In our applications, the bit complexity of all sum of squares
proofs will be n O(ℓ) (assuming that all numbers in the input have bit complexity n O(1) ). This
bound suﬃces in order to argue about pseudo-distributions that satisfy polynomial constraints
approximately.
The following fact shows that every property of low-level pseudo-distributions can be derived
by low-degree sum-of-squares proofs.
Fact 3.5 (Completeness). Suppose d > r ′ > r and A is a collection of polynomial constraints with degree
Í
at most r, and A ⊢ { ni1 x 2i 6 B} for some ﬁnite B.
Let {1 > 0} be a polynomial constraint. If every degree-d pseudo-distribution that satisﬁes D

satisﬁes D

r′

{1 > 0}, then for every ε > 0, there is a sum-of-squares proof A

d

{1 > −ε}.

r

A also

We will use the following Cauchy-Schwarz inequality for pseudo-distributions:
Fact 3.6 (Cauchy-Schwarz for Pseudo-distributions). Let f , 1 be polynomials
q of degree
q at most d in
indeterminate x ∈ d . Then, for any degree d pseudo-distribution µ̃, ˜ µ̃ [ f 1] 6 ˜ µ̃ [ f 2 ] ˜ µ̃ [1 2 ].
The following fact is a simple corollary of the fundamental theorem of algebra:

Fact 3.7. For any univariate degree d polynomial p(x) > 0 for all x ∈ ,

x
d



p(x) > 0 .

This can be extended to univariate polynomial inequalities over intervals of .
Fact 3.8 (Fekete and Markov-Lukács, see [Lau09]). For any univariate degree d polynomial p(x) > 0
x 
for x ∈ [a, b], {x > a, x 6 b} d p(x) > 0 .

4 Algorithm for List-Decodable Robust Regression
In this section, we describe and analyze our algorithm for list-decodable regression and prove our
ﬁrst main result restated here.
11

Theorem 1.5 (List-Decodable Regression). For every α, η > 0 and a k-certiﬁably (C, α 2 η2 /10C)-anticoncentrated distribution D on d , there exists an algorithm that takes input a sample generated according
to LinD (α, ℓ ∗ ) and outputs a list L of size O(1/α) such that there is an ℓ ∈ L satisfying kℓ − ℓ ∗ k2 < η with
probability at least 0.99 over the draw of the sample. The algorithm needs a sample of size n  (kd)O(k) and
2
runs in time n O(k)  (kd)O(k ) .
We will analyze Algorithm 1 to prove Theorem 1.5.

Ín





i1 w i  αn 






2


∀
i
∈
[n].
w

w
i


i




A w,ℓ : ∀ i ∈ [n]. w i · (yi − hx i , ℓi)  0




Õ




2


ℓ
6
1


i




i
6
d



(4.1)

Algorithm 1 (List-Decodable Regression).
Given: Sample S of size n drawn according to Lin(α, n, ℓ ∗) with inliers I, η > 0.
Output: A list L ⊆ d of size O(1/α) such that there exists a ℓ ∈ L satisfying kℓ − ℓ ∗ k2 < η.
Operation:
1. Find a degree O(1/α 4 η4 ) pseudo-distribution µ̃ satisfying A w,ℓ that minimizes
k ˜ [w]k2 .

2. For each i ∈ [n] such that ˜ µ̃ [w i ] > 0, let v i 

˜ µ̃ [w i ℓ]
˜ µ̃ [w i ]

. Otherwise, set v i  0.

3. Take J be a random multiset formed by union of O(1/α) independent draws of
i ∈ [n] with probability

˜ [w i ]
αn .

4. Output L  {v i | i ∈ J} where J ⊆ [n].
Our analysis follows the discussion in the overview. We start by formally proving (⋆).
Lemma 4.1. For any t > k and any S so that I ⊆ S is k-certiﬁably (C, α 2 η2 /4C)-anti-concentrated,
w,ℓ
A w,ℓ t

(

n
α 2 η2
1 Õ
w i kℓ − ℓ ∗ k22 6
|I|
4
i∈I

)

ℓ

Proof. We start by observing: A w,ℓ 2 kℓ − ℓ ∗ k22 6 2.
Since I is (C, αη/2C)-anti-concentrated, there exists a univariate polynomial p such that ∀ i:
{w i hx, ℓ − ℓ ∗ i  0}
and

k
ℓ



p(w i hx i , ℓ − ℓ ∗ i)  1 ,

12

(4.2)



Using (4.2), we have:

A w,ℓ

w,ℓ
t+2



Using (4.3) and A w,ℓ
A w,ℓ

w,ℓ
t+2

n 1 Õ
|I|

i∈I

kℓ k 2 6 1

k
ℓ

(

α 2 η2
1 Õ
p(hx i , ℓ − ℓ ∗ i)2 6
|I|
4
i∈I

w,ℓ
t+2

1 − p 2 (w i hx i , ℓ − ℓ ∗ i)  0
2
w





)

(4.3)

.

1 − w i p 2 (hx i , ℓ − ℓ ∗ i)  0 .

w 2i  w i , we thus have:

w i kℓ − ℓ ∗ k22 

6

1 Õ
1 Õ
w i kℓ − ℓ ∗ k22 w i p 2 (hx i , ℓ − ℓ ∗ i) 
w i kℓ − ℓ ∗ k22 p 2 (hx i , ℓ − ℓ ∗ i)
|I|
|I|
i∈I

1 Õ

|I|

i∈I

α 2 η2 o
kℓ − ℓ ∗ k22 p 2 (hx i , ℓ − ℓ ∗ i) 6
.
4

i∈I


As a consequence of this lemma, we can show that a constant fraction of the v i for i ∈ I
constructed in the algorithm are close to ℓ ∗ .
Lemma 4.2. For any µ̃ of degree k satisfying A w,ℓ ,
Proof. By Lemma 4.1, we have:
We also have:

w,ℓ
A w,ℓ 2



w,ℓ
A w,ℓ k

n

1
|I|

Ín

1
|I|

i∈I

Í

i∈I

˜ [w i ] · kv i − ℓ ∗ k2 6

w i kℓ −

ℓ ∗ k22

6

w 2i − w i  0 for any i. This yields:

w,ℓ
A w,ℓ k

(

α 2 η2
4

n
α 2 η2
1 Õ
kw i ℓ − w i ℓ ∗ k22 6
|I|
4
i∈I

o

α
2 η.

.

)

Í
Since µ̃ satisﬁes A w,ℓ , taking pseudo-expectations yields: I1 i∈I ˜ kw i ℓ − w i ℓ ∗ k22 6
By Cauchy-Schwarz for pseudo-distributions (Fact 3.6), we have:
1Õ ˜
k [w i ℓ] − ˜ [w i ]ℓ ∗ k2
I
i∈I

Using v i 

˜ [w i ℓ]
˜ [w i ]

!2

6

α 2 η2
4 .

α 2 η2
1 Õ ˜
k [w i ℓ] − ˜ [w i ]ℓ ∗ k22 6
.
I
4
i∈I

if ˜ [w i ] > 0 and 0 otherwise, we have:

1
I

Í

i∈I , ˜ [w i ]>0

˜ [w i ] · kv i − ℓ ∗ k2 6

α
2 η.



Next, we formally prove that maximally uniform pseudo-distributions satisfy Proposition 2.5.

Í
Lemma 4.3. For any µ̃ of degree > 4 satisfying A w,ℓ that minimizes k ˜ [w]k2 , i∈I ˜ µ̃ [w i ] > α 2 n.
13

Í

1 ˜
[w]. Then, u is a non-negative vector satisfying i∼[n] u i  1.
Proof. Let u  αn
Í
Í
Let wt(I)  i∈I u i and wt(O)  i<I u i . Then, wt(I) + wt(O)  1.
We will show that if wt(I) < α, then there’s a pseudo-distribution µ̃′ that satisﬁes A w,ℓ and
has a lower value of k ˜ [w]k2 . This is enough to complete the proof.
To show this, we will “mix” µ̃ with another pseudo-distribution satisfying A w,ℓ . Let µ̃∗ be the
actual distribution supported on single (w, ℓ) - the indicator 1I and ℓ ∗ . Thus, ˜ µ̃∗ w i  1 iﬀ i ∈ I
and 0 otherwise. µ̃∗ clearly satisﬁes A w,ℓ . Thus, any convex combination (mixture) of µ̃ and µ̃∗
also satisﬁes A w,ℓ .
Let µ̃ λ  (1 − λ)µ̃ + λ µ̃∗ . We will show that there is a λ > 0 such that k ˜ µ̃ λ [w]k2 < k ˜ [w]k2 .
We ﬁrst lower bound ku k22 in terms of wt(I) and wt(O). Observe that for any ﬁxed values of
1
wt(I) and wt(O), the minimum is attained by the vector u that ensures u i  αn
wt(I) for each i ∈ I
1
and u i  (1−α)n wt(O).
2

This gives ku k >



wt(I)

αn

2



1 − wt(I)
αn +
(1 − α)n

Next, we compute the the ℓ 2 norm of u ′ 

1
αn

2

(1 − α)n 

 α 
1 
.
· wt(I) + (1 − wt(I))2
αn
1−α

˜ µ̃ w as:
λ

ku ′ k22  (1 − λ)2 ku k 2 +

wt(I)
λ2
+ 2λ(1 − λ)
.
αn
αn

wt(I)
λ2
+ 2λ(1 − λ)
αn
αn

2
wt(I)
α  λ2
−2λ + λ
+
· wt(I)2 + (1 − wt(I))2
+ 2λ(1 − λ)
6
αn
1−α
αn
αn

Thus, ku ′ k 2 − ku k 2  (−2λ + λ 2 )ku k 2 +

 α 


λ 
(2 − λ) · wt(I)2 + (1 − wt(I))2
− λ − 2(1 − λ)wt(I)
αn
1−α

λ(2 − λ) 
2
2 α
>
wt(I) + (1 − wt(I))
− wt(I)
αn
1−α

Rearranging, ku k 2 − ku ′ k 2 >

α
Now, whenever wt(I) < α, wt(I)2 + (1 − wt(I))2 1−α
− wt(I) > 0. Thus, we can choose a small
2
′
2
enough λ > 0 so that ku k − ku k > 0.



Lemma 4.3 and Lemma 4.2 immediately imply the correctness of our algorithm.
Proof of Main Theorem 1.5. First, since D is k-certiﬁably (C, αη/4C)-anti-concentrated, Lemma 5.5
implies taking > n  (kd)O(k) samples ensures that I is k-certiﬁably (C, αη/2C)-anti-concentrated
with probability at least 1 − 1/d. Let’s condition on this event in the following.
Let µ̃ be a pseudo-distribution of degree t satisfying A w,ℓ and minimizing k ˜ [w]k2 . Such a
pseudo-distribution exists as can be seen by just taking the distribution with a single-point support
w where w i  1 iﬀ i ∈ I.
14

From Lemma 4.2, we have:
rescaling, we obtain:

1
|I|

Í

i∈I

˜ [w i ] · kv i − ℓ ∗ k2 6

α
2 η.

1 Õ ˜ [w i ]
1α
· kv i − ℓ ∗ k2 6
η.
|I|
Z
Z2

Let Z 

1
αn

Í

i∈I

˜ [w i ]. By a
(4.4)

i∈I

Using Lemma 4.3, Z > α. Thus,
1 Õ ˜ [w i ]
· kv i − ℓ ∗ k2 6 η/2 .
|I|
Z

(4.5)

i∈I

˜ [w ]

Let i ∈ [n] be chosen with probability αni . Then, i ∈ I with probability Z > α. By Markov’s
inequality applied to (4.5), with 21 conditioned on i ∈ I, kv i − ℓ ∗ k2 < η. Thus, in total, with
probability at least α/2, kv i − ℓ ∗ k2 6 η. Thus, the with probability at least 0.99 over the draw of the
random set J, the list constructed by the algorithm contains an ℓ such that kℓ − ℓ ∗ k2 6 η.
Let us now account for the running time and sample complexity of the algorithm. The sample
size for the algorithm is dictated by Lemma 5.5 and is (kd)O(k) , which for our choice of p goes as
(kd)O(k) . A pseudo-distribution satisfying A w,ℓ and minimizing k ˜ [w]k2 can be found in time
2
n O(k)  (kd)O(k ) . The rounding procedure runs in time at most O(nd).

Remark 4.4 (Tolerating Additive Noise). To tolerate independent additive noise, our algorithm and
analysis change minimally. For an additive noise of variance ζ 2 ≪ α 2 η2 in the inliers, we modify
A
by replacing the constraint ∀ i, w i · (yi − hx i , ℓi)  0 by ∀ i, ±w i · (yi − hx i , ℓi) 6 4ζ. And
Ínw,ℓ
Ín
i1 w i  αn to
i1 w i  (α/2)n.
This means that instead of searching for a subsample of size αn that has a exact solution ℓ, we
search for a subsample of size α/2n where there’s a solution ℓ with an additive error of at most 2ζ.
With additive noise of variance ζ 2 , it is easy to check that there’s a subset of 1/2 fraction of inliers
that satisﬁes this property. Thus, A w,ℓ is feasible.
Our analysis remains exactly the same except for one change in the proof of Lemma 4.1. We start
from a distribution that is (C, αηζ/100C)-certiﬁably anti-concentrated. And instead of inferring
that p(w i (yi − hx i , ℓi))  1, we use that whenever ±(yi − hx i , ℓi) 6 4ζ, p 2 ((yi − hx i , ℓi)) > 1 − 4ζ.

4.1 List-Decodable Regression for Boolean Vectors
In this section, we show algorithms for list-decodable regression when the distribution on the
inliers satisﬁes a weaker anti-concentration condition. This allows us to handle more general
inlier distributions including the product distributions on {±1} d , [0, 1]d and more generally any
product domain. We however require that the unknown linear function be “Boolean”, that is, all
its coordinates be of equal magnitude.
We start by deﬁning the weaker anti-concentration inequality. Observe that if v ∈ d satisﬁes
v 3i  d1 v i for every i, then the coordinates of v are in {0, ± √1 }.
d

15

Definition 4.5 (Certiﬁable Anti-Concentration for Boolean Vectors). A d valued random variable
Y is k-certiﬁably (C, δ)-anti-concentrated in Boolean directions if there is a univariate polynomial p satisfying p(0)  1 such that there is a degree k sum-of-squares proof of the following two inequalities:
for all x 2 6 δ 2 , (p(x) − 1)2 6 δ2 and for all v such that v 3i  d4 v i for all i, kv k 2 Y p(hY, vi)2 6 Cδ.
We can now state the main result of this section.
Theorem 4.6 (List-Decodable Regression in Boolean Directions). For every α, η, there’s a algorithm
that takes input a sample generated according to LinD (α, n, ℓ ∗) in d for D that is k-certiﬁably (C, αη/10C)-

n

anti-concentrated in Boolean directions and ℓ ∗ ∈ ± √1
ℓ∗ k

there’s an ℓ ∈ L satisfying kℓ −

d

od

and outputs a list L of size O(1/α) such that

< η with probability at least 0.99 over the draw of the sample. The

algorithm requires a sample of size n > (d/αη)

O(

1
)
α2 η2

2

and runs in time n O(k)  (d/αη)O(k ) .

The only diﬀerence in our algorithm and rounding is that instead of the constraint set A w,ℓ , we
will work with Bw,ℓ that has an additional constraint ℓ 2i  d1 for every i. Our algorithm is exactly
the same as Algorithm 1 replacing A w,ℓ by Bw,ℓ .

Ín



i1 w i  αn 






2



∀
i
∈
[n],
w

w
i 




i
Bw,ℓ : ∀ i ∈ [n],
w i · (yi − hx i , ℓi)  0 







1


2



 ∀in ∈ [d],
ℓi 

d 

(4.6)

We will use the following fact in our proof of Theorem 4.6.
Lemma 4.7. If a, b satisfy a 2  b 2  d2 , then, (a − b)3  d1 (a − b)

Proof. (a − b)3  a 3 − b 3 − 3a 2 b + 3ab 2  d1 (a − b − 3b + 3a)  d4 (a − b).



Proof of Theorem 4.6. The proof remains the same as in the previous section with one additional
step. First, we can obtain the analog of Lemma 4.1 with a few quick modiﬁcations to the proof.
Then, Lemma 4.2 follows from modiﬁed Lemma 4.1 as in the previous section. And the proof of
Lemma 4.3 remains exactly the same. We can then put the above lemmas together just as in the
proof of Theorem 1.5.
We now describe the modiﬁcations to obtain the analog of Lemma 4.1. The key additional step
in the proof of the analog of Lemma 4.1 which follows immediately from Lemma 4.7.



∀i

ℓ 2i

1

d



ℓ
4



(ℓ i −

ℓ ∗i )3

4
 (ℓ i − ℓ ∗i )
d



This allows us to replace the usage of certiﬁable anti-concentration by certiﬁable anticoncentration for Boolean vectors and derive:



∀i

ℓ 2i

2

d



ℓ
4

(

α 2 η2
1 Õ
p(hx i , ℓ − ℓ ∗ i)2 6
|I|
4
i∈I

)

The rest of the proof of Lemma 4.1 remains the same.


16

5 Certifiably Anti-Concentrated Distributions
In this section, we prove certiﬁable anti-concentration inequalities for some basic families of distributions. We ﬁrst formally state the deﬁnition of certiﬁed-anti-concentration.
Definition 5.1 (Certiﬁable Anti-Concentration). A p
d -valued zero-mean random variable Y has a
(C, δ)-anti-concentrated distribution if [|hY, vi| 6 δ
hY, vi 2] 6 Cδ.
Y has a k-certifiably (C, δ)-anti-concentrated distribution if there is a univariate polynomial p
satisfying p(0)  1 such that
1.
2.





hY, vi 2 6 δ 2 hY, vi 2
kv k22 6 1

v
k



kv k22

v
k



(p(hY, vi) − 1)2 6 δ 2 .

p 2 (hY, vi) 6 Cδ .

We will say that such a polynomial p “witnesses the certiﬁable anti-concentration of Y”. We
will use the phrases “Y has a certiﬁably anti-concentrated distribution” and “Y is a certiﬁably
anti-concentrated random variable” interchangeably.
As one would like, the deﬁnition above is scale invariant:
Lemma 5.2 (Scale invariance). Let Y be a k-certifiably (C, δ)-anti-concentrated random variable. Then,
so is cY for any c , 0.
Proof. Let p be the polynomial that witnesses the certiﬁable anti-concentration of Y. Then, observe
that q(z)  p(z/c) satisﬁes the requirements of the deﬁnition for cY.

Lemma
anti-concentration of gaussians). For every 0.1 > δ > 0, there is a

 5.3 (Certiﬁed
kO

log2 (1/δ)
δ2

such that N(0, I) is k-certifiably (2, 2δ)-anti-concentrated.

Proof. Lemma A.1 yields that there exists an univariate even polynomial p of degree k as above such
that for all v, whenever |hx, vi| 6 δ, p(hx, vi) 6 2δ, and whenever kv k 2 6 1, x∼N(0,I) p(hx, vi)2 6
2δ. Since p is even, p(z)  21 (p(z) + p(−z)) and thus, any monomial in p(z) with non-zero coeﬃcient
must be of even degree. Thus, p(z)  q(z 2 ) for some polynomial q of degree k/2.
The ﬁrst property above for p implies that whenever z ∈ [0, δ], p(z) 6 2δ. By Fact 3.8, we obtain

v 
2j
that: hx, vi 2 6 δ 2 k p(hx, vi)2 6 δ . Next, observe that for any j, x∼N(0,I) hx, vi 2 j  (2j)!!· kv k2 .
Thus, kv k22 x∼N(0,I) p 2 (hx, vi) is a univariate polynomial F in kv k22 . The second property above
thus implies that F(kv k22) 6 Cδ whenever kv k22 6 1. By another application of Fact 3.8, we obtain:

v 
2
kv k22 6 1 k

x∼N(0,I) p(hx, vi) 6 2δ .
We say that Y is a spherically symmetric random variable over d if for every orthogonal matrix
R, RY has the same distribution as Y. Examples include the standard gaussian random variable
and uniform (Haar) distribution on d−1 . Our argument above for the case of standard gaussian
extends to any distribution that is spherically symmetric and has suﬃciently light tails.

17

Lemma 5.4 (Certiﬁed anti-concentration of spherically symmetric, light-tail distributions). Suppose
Y is a d -valued,
random variable such that for any k ∈ (0, 2), for all t and for all v,
p spherically symmetric
2/k
hY, vi 2] 6 Ce −t /C and for all η > 0, x∼D [|x| < ησ] 6 Cη, for some absolute constant
[hv, Yi > t
C > 0. Then, for d  O



log(4+k)/(2−k) (1/δ)
δ2/(2−k)



, Y is d-certifiably (10C, δ)-anti-concentrated.

Lemma 5.5 (Certiﬁed anti-concentration under sampling). Let D be k-certifiably (C, δ)-anticoncentrated, subexponential and unit covariance distribution. Let S be a collection of n independent

samples from D. Then, for n > Ω (kd log(d))O(k) , with probability at least 1 − 1/d, the uniform distribution on S is (2C, δ)-anti-concentrated.
Proof. Let p be the degree k polynomial that witnesses the certiﬁable anti-concentration of D.
Let Y be the random variable with distribution D ′, the uniform distribution on n i.i.d. samples from D. We will show that p also witnesses that k-certiﬁable (4C, δ/2)-anti-concentration
of Y. To this end it is suﬃcient to take enough samples such that the following holds.

2
2
2
2

D [p (hY, vi)] − D ′ [p (hY, vi)] > D [p (hY, vi)]/2 < 1/d. Observe that p (hY, vi) may be
written as hc(Y)c(Y)T , m(v)m(v)T i where c(Y) are the coeﬃcients of p(hY, vi) and m(v) is the vector
containing monomials. The dot product above is the usual trace inner product between matrices.

Thus, it is suﬃcient to show that  k D′ c(Y)c(Y)T − D c(Y)c(Y)T kF2 > k D c(Y)c(Y)T kF2 /4 <
1/d. Since p was a univariate polynomial of degree k in d dimensional variables, there are at most
d 2k entries in total, and each entry is at most a degree 2k polynomial of subexponential random
variables in d variables. Using standard concentration results for polynomials of subexponential
therein). We
random variables (for instance Theorem 1.2 from [GSS19] and the references
 see

that each entry satisﬁes 

D c(Y)i c(Y) j −

D′



c(Y)i c(Y) j > ε 6 exp −Ω



nε
(c(Y) i c(Y) j )2

 1/2k

. An

application of a union bound, squaring the term inside and replacing ε 2 by (c(Y)i c(Y) j )2 /4 gives


us 

Í

d 2k
i, j1

D c(Y)i c(Y) j −

D′

c(Y)i c(Y) j

2

>k

c(Y)c(Y)T kF2 /4



6 d 2k exp −Ω



n

d O(k)

 1/2k

.

Hence, setting n  O((kd log(d))O(k) ) ensures that with probability at least 1 − 1/d, the distribution D ′ is (2C, δ)-anti-concentrated.



We say that a d × d matrix A is C ′-well-conditioned if all singular values of A are within a factor
of C ′ of each other.
Lemma 5.6 (Certiﬁed anti-concentration under linear transformations). Let Y be k-certifiably (C, δ)anti-concentrated random variable over d . Let A be any C ′-well-conditioned linear transformation. Then,
AY is k-certifiably (C, C ′2 δ)-anti-concentrated.
Proof. Let kAk be the largest singular value of A. Let p be a polynomial that witnesses the
certiﬁable anti-concentration of Y. Let q(z)  p(z/kAk). We will prove that q witnesses the
k-certiﬁable (C, C ′2 δ)-anti-concentration of AY.


v
hAY, vi 2 6 δ 2 hAY, vi 2 .
Towards this,
observe that: hY, vi 2 6 δ 2 hY, vi 2
2


v
hY, (AT v)/kAki 2 6 δ 2 hY, (AT v)/kAki 2 k (p(hY, (AT v)/kAki) − 1)2 6 δ 2 ,
18



v
k

This is the same as hAY, vi 2 6 δ 2 hAY, vi 2
Now, for w  (AT v)/kAk and any unit vector v,



Thus,





kw k22 6 1

kAT v k22 6 kAk 2

v
k



v
k



kAT v k22 /kAk22

kAT v k22

v
k

kAT v k22 6 kAk 2 , and thus, kv k22 6 1





(q(hAY, vi) − 1)2 6 δ 2 , where q  p(x/kAk).

p 2 (hAY, vi/kAk) 6 Cδ ,

q 2 (hAY, vi) 6 C kAk22 δ .

Using

q 2 (hAY, vi) 6 CC ′2 δ .

kv k22



kv k22 6 1

v
2



Lemma 5.7 (Certiﬁable Anti-Concentration in Boolean Directions). Fix C > 0. Let Y be a d valued
product random variable satisfying:
1. Identical Coordinates: Yi are identically distributed for every 1 6 i 6 d.
2. Anti-Concentration For every v ∈

n

0, ± √1
d

od

3. Light tails For every v ∈ d−1 , [|hY, vi| > t

, [|hY, vi| 6 δ

p

p

hY, vi 2] 6 Cδ.

hY, vi 2] 6 exp(−t 2 /C).

Then, Y is k-certifiably (C, δ)-anti-concentrated for k  O



log2 (1/δ)
δ2



.

Proof. We use the p from Lemma A.1. Observe that every monomial of even degree 2k for any
k ∈ , Y∼D hY, vi 2k is a symmetric polynomial in v with non-zero coeﬃcients only on even-degree
monomials in v. This follows by noting that the coordinates of D are independent and identically
distributed and p is an even function. It is a fact that all symmetric polynomials in v can be
2i
expressed as polynomials in the “power-sum” polynomials kv k2i
for i 6 2t. However, since

1
1
2i
2i
2
2
v i ∈ 0, d for i > 1, kv k2i  d i−1 kv k2 . Hence a polynomial in kv k2i is also a univariate polynomial
in kv k22 . Since these are polynomial inequalities, they are also sum-of-squares proofs of these
inequalities.
The observation above implies kv k22 Y p(hY, vi)2  kv k22 · F(kv k22) for some degree k univariate
polynomial F. Since Since F is a univariate polynomial and kv k22 6 1 is an “interval constraint” by
kvk22



applying Fact 3.8, we get: 2t
kv k22 F(kv k22) 6 Cδ . Recalling the fact that kv k22
kv k22 · F(kv k22), this completes the proof.

Y

p(hY, vi)2 



6 Information-Theoretic Lower Bounds for List-Decodable Regression
In this section, we show that list-decodable regression on LinD (α, ℓ ∗ ) information-theoretically
requires that D satisfy α-anti-concentration: x∼D [hx, vi  0] < α for any non-zero v.
Theorem 6.1 (Main Lower Bound). For every q, there is a distribution D on d satisfying x∼D [hx, vi 
1
-approximate list-decodable regression algorithm for LinD ( 1q , ℓ ∗ ) that can
0] 6 1q such that there’s no 2q
output a list of size < d.
Remark 6.2 (Impossibility of Mixed Linear Regression on the Hypercube). Our construction for the
case of q  2 actually shows the impossibility of the well-studied and potentially easier problem
of noiseless mixed linear regression on the uniform distribution on {0, 1} n . This is because R i is, by
construction, obtained by using one of e i or 1− e i to label each example point with equal probability.
19

Theorem 6.1 is tight in a precise way. In Proposition 2.4, we proved that whenever D satisﬁes
x∼D [hx, vi  0] < 1q , there is an (ineﬃcient) algorithm for exact list-decodable regression algorithm
for LinD ( 1q , ℓ ∗ ). Note that our lower bound holds even in the setting where there is no additive
noise in the inliers.
Somewhat surprisingly, our lower bound holds for extremely natural and well-studied
distributions - uniform distribution on {0, 1} n and more generally, uniform distribution on
{0, 1, . . . , q − 1} d  [q]d for any q. We can easily determine a tight bound on the anti-concentration
of both these distributions.
Lemma 6.3. For any non-zero v ∈ d , x∼{0,1} n hx, vi  0 6

1
2

and x∼[q]d [hx, vi  0] 6 q1 .

Note that this is tight for any v  e i , the vector with 1 in the ith coordinates and 0s in all others.
Proof. Fix any v. Without loss of generality, assume that all coordinates of v are non-zero. If not,
we can simply work with the uniform distribution on the sub-hypercube corresponding to the
non-zero coordinates of v.
Let S ⊆ {0, 1} n ([q]d , respectively) be the set of all x ∈ {0, 1} n ([q]d , respectively) such that
hx, vi  0. Then, observe that for any x ∈ S, and any i, x (i) obtained by ﬂipping the ith bit
(changing the ith coordinate to any other value) of x cannot be in S. Thus, S is an independent set
in the graph on {0, 1} n (in [q]d , respectively) with edges between pairs of points with hamming
distance 1.
It is a standard fact [Wik] that the maximum independent set in the d-hypercube is of size
exactly 2d−1 and in the q-ary Hamming graph [q]d is of size q d−1 . Thus, x∼{0,1} d [hx, vi  0] 6 21
and x∼[q]d [hx, vi  0] 6 1q .



To prove our lower bound, we give a family of d distributions on labeled linear equations, R i
for 1 6 i 6 d that satisfy the following:
1. The examples in each are chosen from uniform distribution on [q]d ,
2.

1
q

fraction of the samples are labeled by e i in R i , and,

3. for any i, j, R i and R j are statistically indistinguishable.
1
-approximate list-decoding algorithm must produce a list of
Thus, given samples from R i , any 2q
size at least d.
Our construction and analysis of R i is simple and exactly the same in both the cases. However
it is somewhat easier to understand for the case of the hypercube (q  2). The following simple
observation is the key to our construction.

Lemma 6.4. For 1 6 i 6 d, let R i be the distribution on linear equations induced by the following sampling
method: Sample x ∼ {0, 1} d , choose a ∼ {0, 1} uniformly at random and output: (x, hx, (1 − a)e i i). Then,
R i  R j for any i, j 6 d.

20

Proof. The proof follows by observing that R i when viewed as a distribution on d+1 is same as
the uniform distribution on {0, 1} d+1 and thus independent of i.

The argument immediately generalizes to [q]d and yields:
Lemma 6.5. For 1 6 i 6 d, let R i be the distribution on linear equations induced by the following sampling
method: Sample x ∼ [q]d , choose a ∼ {0, 1} uniformly at random and output: (x, (hx, e i i + a) mod q).
Then, R i  R j for any i, j 6 d.
In this case, we interpret the 1/q fraction of the samples where a  0 as the inliers. Observe
that these are labeled by a single linear function e i in any R i . Thus, they form a valid model in
LinD (α, ℓ ∗ ) for α  1/q.
Since the linear functions deﬁned by e i on [q]d , when normalized to have unit norm, have a
pairwise Euclidean distance of at least 1/q, we immediately obtain a proof of Theorem 6.1.

Acknowledgement
We thank Surbhi Goel for pointing out a bug in an earlier version of the paper. P.K. thanks David
Steurer for illuminating discussions on list-decodable robust estimation via SoS.

References
[ABL13]

Pranjal Awasthi, Maria-Florina Balcan, and Philip M. Long, The power of localization for
efficiently learning linear separators with malicious noise, CoRR abs/1307.8371 (2013). 1

[BBV08]

Maria-Florina Balcan, Avrim Blum, and Santosh Vempala, A discriminative framework
for clustering via similarity functions, STOC, ACM, 2008, pp. 671–680. 1

[Ber06]

Thorsten Bernholt, Robust estimators are hard to compute, Tech. report, Technical Report/Universität Dortmund, SFB 475 Komplexitätsreduktion in Multivariaten Datenstrukturen, 2006. 1

[BJKK17]

Kush Bhatia, Prateek Jain, Parameswaran Kamalaruban, and Purushottam Kar, Consistent robust regression, Advances in Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long
Beach, CA, USA, 2017, pp. 2107–2116. 1

[BKS15]

Boaz Barak, Jonathan A. Kelner, and David Steurer, Dictionary learning and tensor
decomposition via the sum-of-squares method [extended abstract], STOC’15—Proceedings
of the 2015 ACM Symposium on Theory of Computing, ACM, New York, 2015, pp. 143–
151. MR 3388192 5

[BM16]

Boaz Barak and Ankur Moitra, Noisy tensor completion via the sum-of-squares hierarchy,
COLT, JMLR Workshop and Conference Proceedings, vol. 49, JMLR.org, 2016, pp. 417–
445. 5
21

[BS16]

Boaz Barak and David Steurer, Proofs, beliefs, and algorithms through the lens of sum-ofsquares, 2016, Lecture notes in preparation, available on http://sumofsquares.org.
9

[BWY14]

Sivaraman Balakrishnan, Martin J. Wainwright, and Bin Yu, Statistical guarantees for the
EM algorithm: From population to sample-based analysis, CoRR abs/1408.2156 (2014). 1

[CDG19]

Yu Cheng, Ilias Diakonikolas, and Rong Ge, High-dimensional robust mean estimation
in nearly-linear time, Proceedings of the Thirtieth Annual ACM-SIAM Symposium on
Discrete Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019, 2019,
pp. 2755–2771. 1

[CSV17]

Moses Charikar, Jacob Steinhardt, and Gregory Valiant, Learning from untrusted data,
STOC, ACM, 2017, pp. 47–60. 1

[CYC14]

Yudong Chen, Xinyang Yi, and Constantine Caramanis, A convex formulation for mixed
regression with two components: Minimax optimal rates, Proceedings of The 27th Conference on Learning Theory, COLT 2014, Barcelona, Spain, June 13-15, 2014, 2014,
pp. 560–604. 1

[DGJ+ 09]

Ilias Diakonikolas, Parikshit Gopalan, Ragesh Jaiswal, Rocco A. Servedio, and
Emanuele Viola, Bounded independence fools halfspaces, 50th Annual IEEE Symposium
on Foundations of Computer Science, FOCS 2009, October 25-27, 2009, Atlanta, Georgia, USA, 2009, pp. 171–180. 26

[DKK+ 16a] Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart, Robust estimators in high dimensions without the computational intractability,
FOCS, IEEE Computer Society, 2016, pp. 655–664. 1
[DKK+ 16b] Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Zheng Li, Ankur Moitra,
and Alistair Stewart, Robust estimators in high dimensions without the computational intractability, CoRR abs/1604.06443 (2016). 1
[DKK+ 17]

Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and
Alistair Stewart, Robustly learning a gaussian: Getting optimal error, efficiently, CoRR
abs/1704.03866 (2017). 1

[DKK+ 18a] Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li 0001, Jacob Steinhardt,
and Alistair Stewart, Sever: A robust meta-algorithm for stochastic optimization, CoRR
abs/1803.02815 (2018). 1
[DKK+ 18b] Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart, Robustly learning a gaussian: Getting optimal error, efficiently, Proceedings
of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA
2018, New Orleans, LA, USA, January 7-10, 2018, 2018, pp. 2683–2702. 1

22

[DKS17]

Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart, Learning geometric concepts
with nasty noise, CoRR abs/1707.01242 (2017). 1

[DKS18]

, List-decodable robust mean estimation and learning mixtures of spherical gaussians,
Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing,
STOC 2018, Los Angeles, CA, USA, June 25-29, 2018, 2018, pp. 1047–1060. 1

[DKS19]

Ilias Diakonikolas, Weihao Kong, and Alistair Stewart, Efficient algorithms and lower
bounds for robust linear regression, Proceedings of the Thirtieth Annual ACM-SIAM
Symposium on Discrete Algorithms, SODA 2019, San Diego, California, USA, January
6-9, 2019 (Timothy M. Chan, ed.), SIAM, 2019, pp. 2745–2754. 1

[DV89]

Richard D. De Veaux, Mixtures of linear regressions, Comput. Statist. Data Anal. 8 (1989),
no. 3, 227–245. MR 1028403 1

[Erd45]

P. Erdös, On a lemma of littlewood and offord, Bull. Amer. Math. Soc. 51 (1945), no. 12,
898–902. 3, 5

[EY07]

Alexandre Eremenko and Peter Yuditskii, Uniform approximation of sgn x by polynomials
and entire functions, J. Anal. Math. 101 (2007), 313–324. MR 2346548 26

[EY08]

, An extremal problem for a class of entire functions, C. R. Math. Acad. Sci. Paris
346 (2008), no. 15-16, 825–828. MR 2441914 26

[EY11]

, Polynomials of the best uniform approximation to sgn(x) on two intervals, J. Anal.
Math. 114 (2011), 285–315. MR 2837087 26

[FS10]

Susana Faria and Gilda Soromenho, Fitting mixtures of linear regressions, J. Stat. Comput.
Simul. 80 (2010), no. 1-2, 201–225. MR 2757044 1

[GLS81]

M. Grötschel, L. Lovász, and A. Schrĳver, The ellipsoid method and its consequences in
combinatorial optimization, Combinatorica 1 (1981), no. 2, 169–197. MR 625550 10

[GSS19]

Friedrich Götze, Holger Sambale, and Arthur Sinulis, Concentration inequalities for polynomials in α-sub-exponential random variables, arXiv e-prints (2019), arXiv:1903.05964. 18

[HL17]

Sam B. Hopkins and Jerry Li, Mixture models, robustness, and sum of squares proofs, 2017.
1, 2, 5

[HRRS11]

Frank R Hampel, Elvezio M Ronchetti, Peter J Rousseeuw, and Werner A Stahel, Robust
statistics: the approach based on influence functions, vol. 114, John Wiley & Sons, 2011. 1

[Hub11]

Peter J Huber, Robust statistics, International Encyclopedia of Statistical Science,
Springer, 2011, pp. 1248–1251. 1

[JJ94]

Michael I. Jordan and Robert A. Jacobs, Hierarchical mixtures of experts and the em
algorithm, Neural Computation 6 (1994), no. 2, 181–214. 1
23

[KKM18]

Adam R. Klivans, Pravesh K. Kothari, and Raghu Meka, Efficient algorithms for outlierrobust regression, Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9
July 2018., 2018, pp. 1420–1430. 1, 2, 4, 5

[KLS09]

Adam R. Klivans, Philip M. Long, and Rocco A. Servedio, Learning halfspaces with
malicious noise, Journal of Machine Learning Research 10 (2009), 2715–2740. 1

[KP19]

Sushrut Karmalkar and Eric Price, Compressed sensing with adversarial sparse noise via
l1 regression, SOSA@SODA (Jeremy T. Fineman and Michael Mitzenmacher, eds.),
OASICS, vol. 69, Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, 2019, pp. 19:1–
19:19. 1

[KS17a]

Pravesh K. Kothari and Jacob Steinhardt, Better agnostic clustering via relaxed tensor
norms, 2017. 1, 2, 4, 5

[KS17b]

Pravesh K. Kothari and David Steurer, Outlier-robust moment-estimation via sum-ofsquares, CoRR abs/1711.11581 (2017). 1, 2, 4, 5

[KS19]

, List-decodable mean estimation made simple, Manuscript, 2019. 2

[Las01]

Jean B. Lasserre, New positive semidefinite relaxations for nonconvex quadratic programs,
Advances in convex analysis and global optimization (Pythagorion, 2000), Nonconvex
Optim. Appl., vol. 54, Kluwer Acad. Publ., Dordrecht, 2001, pp. 319–331. MR 1846160
10

[Lau09]

Monique Laurent, Sums of squares, moment matrices and optimization over polynomials,
Emerging applications of algebraic geometry, Springer, 2009, pp. 157–270. 11

[LL18]

Yuanzhi Li and Yingyu Liang, Learning mixtures of linear regressions with nearly optimal
complexity, Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9 July
2018., 2018, pp. 1125–1144. 1, 4

[LRV16]

Kevin A. Lai, Anup B. Rao, and Santosh Vempala, Agnostic estimation of mean and
covariance, FOCS, IEEE Computer Society, 2016, pp. 665–674. 1

[Lub07]

Doron S Lubinsky, A Survey of Weighted Approximation for Exponential Weights, arXiv
Mathematics e-prints (2007), math/0701099. 2, 26

[MMY06]

RARD Maronna, R Douglas Martin, and Victor Yohai, Robust statistics, John Wiley &
Sons, Chichester. ISBN, 2006. 1

[MSS16]

Tengyu Ma, Jonathan Shi, and David Steurer, Polynomial-time tensor decompositions with
sum-of-squares, FOCS, IEEE Computer Society, 2016, pp. 438–446. 5, 9

[MV10]

Ankur Moitra and Gregory Valiant, Settling the polynomial learnability of mixtures of
gaussians, FOCS, IEEE Computer Society, 2010, pp. 93–102. 4

24

[Nes00]

Yurii Nesterov, Squared functional systems and optimization problems, High performance
optimization, Appl. Optim., vol. 33, Kluwer Acad. Publ., Dordrecht, 2000, pp. 405–440.
MR 1748764 10

[Par00]

Pablo A Parrilo, Structured semidefinite programs and semialgebraic geometry methods in
robustness and optimization, Ph.D. thesis, California Institute of Technology, 2000. 10

[PSBR18]

Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar,
Robust estimation via robust gradient estimation, CoRR abs/1802.06485 (2018). 1

[RV08]

Mark Rudelson and Roman Vershynin, The Littlewood-Offord problem and invertibility of
random matrices, Adv. Math. 218 (2008), no. 2, 600–633. MR 2407948 3

[RY19]

Prasad Raghavendra and Morris Yau, List decodable learning via sum of squares,
Manuscript, 2019. 1, 5

[Sho87]

N. Z. Shor, Quadratic optimization problems, Izv. Akad. Nauk SSSR Tekhn. Kibernet.
(1987), no. 1, 128–139, 222. MR 939596 10

[SJA16]

Hanie Sedghi, Majid Janzamin, and Anima Anandkumar, Provable tensor methods for
learning mixtures of generalized linear models, AISTATS, JMLR Workshop and Conference
Proceedings, vol. 51, JMLR.org, 2016, pp. 1223–1231. 1, 4

[Tuk75]

John W. Tukey, Mathematics and the picturing of data, 523–531. MR 0426989 1

[TV12]

Terence Tao and Van Vu, The Littlewood-Offord problem in high dimensions and a conjecture
of Frankl and Füredi, Combinatorica 32 (2012), no. 3, 363–372. MR 2965282 3

[Wei]

Eric
W.
Weisstein,
Hermite
number
http://mathworld.wolfram.com/HermiteNumber.html. 28

[Wik]

Wikipedia, Singleton bound, https://en.wikipedia.org/wiki/Singleton_bound. 20

[YCS13]

Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi, Alternating Minimization for
Mixed Linear Regression, arXiv e-prints (2013), arXiv:1310.3745. 1

[ZJD16]

Kai Zhong, Prateek Jain, and Inderjit S. Dhillon, Mixed linear regression with multiple
components, NIPS, 2016, pp. 2190–2198. 1

A

from

mathworld,

Polynomial Approximation for Core-Indicator

The main result of this section is a low-degree polynomial approximator for the function 1(|x| < δ)
with respect to all distributions that have strictly sub-exponential tails.
Lemma A.1. Let D be a distribution on  with mean 0, variance σ2 6 1 and satisfying:
1. Anti-Concentration: For all η > 0, x∼D [|x| < ησ] 6 Cη, and,
25

2. Tail bound: [|x| > tσ] 6 e −

t 2/k
C

for k < 2 and all t,

for some C > 1. Then, for any δ > 0, there is a d  O



log(4+k)/(2−k) (1/δ)
δ2/(2−k)



 Õ

polynomial q(x) of degree d such that q(0)  1, q(x)  1 ± δ for all |x| 6 δ and σ 2 ·



1



δ2/(2−k)

x∼D



and an even



q 2 (x) 6 10Cδ.

Before proceeding to the proof, we note that the bounds on the degree above are tight up to
poly logarithmic factors for the gaussian distribution.
Lemma A.2. For every polynomial p of degree d such that p(0)  1,
there is a polynomial p∗ of degree d such that p∗ (0)  1 and

x∼N(0,1)

x∼N(0,1) [p

p∗2 (x)  Θ

2 (x)]



√1
d



Ω

.



√1
d



. Further,

Our construction of the polynomial is based on standard techniques in approximation theory for
constructing polynomial approximators for continuous functions over an interval. Most relevant
for us are various works of Eremenko and Yuditskii [EY08, EY11, EY07] and Diakonikolas, Gopalan,
Jaiswal, Servedio and Viola [DGJ+ 09] on such constructions for the sign function on the interval
[−1, a] ∪ [a, 1] for a > 0. We point the reader to the excellent survey of this beautiful line of work
by Lubinsky [Lub07].
Fact A.3 (Theorem 3.5 in [DGJ+ 09]). Let 0 < η < 0.1, then there exist constants C, c such that for
a : η2 /C log(1/η) and K  4c log(1/η)/a + 2 < O(log2 (1/η)/η2 )
there is a polynomial p(t) of degree K satisfying
1. p(t) > sign(t) > −p(−t) for all t ∈ .
2. p(t) ∈ [sign(t), sign(t) + η] for t ∈ [−1/2, −2a] ∪ [0, 1/2].
3. p(t) ∈ [−1, 1 + η] for t ∈ (−2a, 0)
4. |p(t)| 6 2 · (4t)K for all t > 21 .
We will also rely on the following elementary integral estimate.
Lemma A.4 (Tail Integral).

∫









x 2/k 2d
L 2/k
exp −
x dx < exp −
((L)4d + (16kd)kd ) .
C
C
[L,∞]

∫∞

2

∫∞

2

Proof. We ﬁrst prove the claim for k  1. Let y  x − L. The, L e −x x 2d dx  0 e −(y+L) (y + L)2d dy.
2d
2d 2d
2d
We now use that y 2 + L 2 6 (y + L)2 for all
∫ ∞ y >2 0 and (y + L)
∫ ∞ 6 22 (y + L ) to upper bound
2
2
the integral above by: e −L L 2d + 22d e −L 0 e −y y 2d . Using 0 e −y y 2d < (4d)d gives a bound of
e −L (L 2d + (8d)d ).
∫∞
2
For larger k, we substitute y  x 1/k and write the integral in question as L1/k e −y y 2kd−(k−1) dy.
2

2/k

Applying the calculation from the above special case, this integral is upper bounded by: e −L (L 4d +
(16kd)kd ).

26



Proof of Lemma A.1. Let p(x) be the degree d < O

L log2 (1/δ)
δ



polynomial from Fact A.3. We then

construct a polynomial q(x) that will be close to 0 in the range [δ, L] and [−L, −δ] and close to 1 in
the range [−δ, δ]. Our polynomial q is obtained by shifting and appropriately scaling two copies
of p.


x
x
+ p −(a + 4L
) −1
p a + 4L
q(x) 
p (a) + p (−a) − 1

Then, q(0)  1. It further satisﬁes:

p

1. q(x) ∈ [0, C δ/L] for x ∈ [δ, L] ∪ [−L, δ].

p

2. q(x) ∈ [1 − C δ/L, 1 +
3. q(x) ∈ [0, 1 +

p

p

δ/L] for x ∈ [−δ, δ].

δ/L] for x ∈ [−3δ, −δ] ∪ [δ, 3δ].

4. |q(x)| < 4 · (4x)t for |x| > L
We now prove the bound the p 2 . We do this by providing upper bounds on the contributions


to σ2 · x∼D q 2 (σx) from the disjoint sets with diﬀerent guarantees below. Since we are going to
evaluate q(σx) the intervals will be scaled by σ. The contributions from the regions σ1 [δ, L] and
1
σ [−δ, δ] can be naively upper bounded by the maximum value that the polynomial can take here
times the probability of landing in these regions. The ﬁrst of these contributes σ · Lδ · (L − δ) 6 δ,



and using anticoncentration, the second region contributes 1 +
1
σ [δ, 3δ]



q 2

can be bounded similarly to get an upper bound of 2 1 +

use Lemma A.4 to upper bound the contribution to
σ2 C′

∫





δ
L

q 2
δ
L

  2/k !

 

((L/σ)4d + (16kd)kd )

  2/k

1
L
L
− ·
. exp 2d + 4d log
σ
C σ

Since d  O



L log2 (1/δ)
δ



1
C

· ( Lσ )2/k < 2 log(1/δ).

, k < 2, and σ < 1 we can now choose L 
log

2+3k/(2−k)

(1/δ)

σ 2 δ 6 4δ. To ﬁnish, we

p 2 from the tail:

1
x 2/k
L
dx . σ 2+d 4d exp − ·
q 2 (σx) exp −
1
C
C σ
σ [L,∞]

We choose L satisfying 10d log(d) + 4d log( Lσ ) −

· 2Cδ 6 4Cδ. The region



!

+ kd log(16kd) .

C100 log3 (1/δ)
δ

 k/(2−k)

to satisfy

the inequality above and to get d .
. When k  1 we get d  Õ(1/δ 2 ). Since σ < 1 in
δ1+k/(2−k)
all the above calculations, we get our result by re-scaling δ.

We now complete the proof of Lemma A.2.

Íd

Proof of Lemma A.2. Any polynomial p of degree d can be written as p(x) 
denote the hermite polynomials of degree i, satisfying x∼N(0,1) h i  0 and

27

α i h i (x) where h i
2
x∼N(0,1) [h i (x)]  1.

i1

Since p(0)  1, using Cauchy-Schwartz inequality, we obtain:
2

x∼N(0,1)

[p (x)] ·

d
Õ
i1

h 2i (0)



d
Õ

α 2i

i1

Further, observe that for the polynomial p∗ (x) 
(2i−1)!!

Using that h2i (0)  √

(2i)!

x∼N(0,1)

[p 2 (x)] >



!

d
Õ

·

i1

Í 12
i

h i (0)

h 2i (0)

Í

i

!

>

d
Õ
i1

α i h i (0)

!2

>1

h i (0)h i (x), the above inequality is tight.

and h i (0)  0 if i is odd, (see, for e.g., [Wei]), we have:

x∼N(0,1)

p∗2 (x) 

d/2
Õ
(2i)!
i1

22i i!2

! −1



d
Õ
i1

h 2i (0)

d/2  
Õ
2i
i1

i

! 2 −1
d/2
©Õ (2i − 1)!! ª
­
®
p
(2i)!
i1
«
¬
! −1
! −1
d/2
Õ 1
1

! −1
·

22i

Θ

i1

√

i

Θ

 √  −1
d

.



B Brute-force search can generate a exp(d) size list
In the following, we write e i to denote the vector with 1 in the ith coordinate and 0s in all others.
Proposition B.1. There exists a distribution D on d and a model LinD (α, ℓ ∗ ) such that for every α < 1/2,
with probability at least 1 − 1/d over the draw of a n-size sample S from LinD (α, ℓ ∗ ), there exists a collection
Sol ⊆ {S ⊆ S | |S|  αn} of size exp(d) and unit length vectors ℓ S for every S ∈ Sol such that ℓ S satisfies
all equations in S and for every S , S′ ∈ Sol, kℓ S − ℓ S′ k2 > 0.1.
√
® d be the all-ones vector
Proof. Let D be the√uniform distribution on e1 , e2 , . . . , e d ∈ d . Let ℓ ∗ : 1/
in d scaled by 1/ d and let d samples be drawn from the uncorrupted distribution. These give
αn
us our inliers, I  {(x i , yi )}i1
. For the outliers, choose the following multiset O : 1/α − 1 copies
√
√
of {(e i , j) | i ∈ [d], j ∈ {±1/ d}}. This is a sample set of size 2d/α. Any a ∈ {±1/ d} d is a valid
candidate for a solution for this data. This is because for any such a, Ia : {(e i , a i ) | i ∈ [d]} ⊂ S
satisﬁes the following
1. Ia ⊂ S, |Ia |  d 

α
2 |S|

and

2. for any (x, y) ∈ Ia , y  hx, ai.
The Gilbert–Varshamov bound from coding theory now tells us that there are at least Ω(exp(Ω(d)))
{0, 1} vectors√in d dimensions that pairwise have a hamming distance of
√ 0.1 · d. This transfers to
the set {±1/ d} to give us that there are Ω(exp(Ω(d))) vectors in {±1/ d} that are pairwise 0.1
apart in 2-norm.



28

