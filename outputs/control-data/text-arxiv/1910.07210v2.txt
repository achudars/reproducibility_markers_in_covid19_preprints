On Learning Paradigms for the
Travelling Salesman Problem

arXiv:1910.07210v2 [cs.LG] 31 Oct 2019

1

Chaitanya K. Joshi1 , Thomas Laurent2 , and Xavier Bresson1
School of Computer Science and Engineering, Nanyang Technological University, Singapore
2
Department of Mathematics, Loyola Marymount University
{chaitanya.joshi, xbresson}@ntu.edu.sg, tlaurent@lmu.edu

Abstract
We explore the impact of learning paradigms on training deep neural networks
for the Travelling Salesman Problem. We design controlled experiments to train
supervised learning (SL) and reinforcement learning (RL) models on fixed graph
sizes up to 100 nodes, and evaluate them on variable sized graphs up to 500 nodes.
Beyond not needing labelled data, our results reveal favorable properties of RL
over SL: RL training leads to better emergent generalization to variable graph sizes
and is a key component for learning scale-invariant solvers for novel combinatorial
problems. 1

1

Introduction

The Travelling Salesman Problem (TSP) is one of the most intensively studied combinatorial optimization problems in the Operations Research community and is the backbone of industries such
as transportation, logistics and scheduling. Being an NP-hard graph problem, finding optimal TSP
solutions is intractable at large scales above thousands of nodes. In practice, the Concorde TSP solver
[1] uses carefully handcrafted heuristics to find approximate solutions up to tens of thousands of
nodes. Unfortunately, powerful OR solvers such as Concorde are problem-specific; their development
for new problems requires significant time and specialized knowledge.
An alternate approach by the Machine Learning community is to develop generic learning algorithms
which can be trained to solve any combinatorial problem from problem instances themselves [15, 3].
Using 2D Euclidean TSP as a representative of practical combinatorial problems, recent learningbased approaches [10, 7, 13] have leveraged advances in graph representation learning [5, 6, 16, 12,
8, 14] to operate directly on the problem’s graph structure and perform competitively with Concorde
on problem instances of fixed/trivially small graphs.
A key design choice for scaling these approaches to real-world problem sizes is the learning paradigm:
supervised learning (SL) or reinforcement learning (RL). As noted in [3], the performance of SLbased models depends on the availability of large sets of optimal or high-quality instance-solution
pairs. Although RL is known to be less sample efficient than SL, it does not require labelled instances.
As long as a problem can be formulated via a reward signal for making sequential decisions, an
autoregressive policy can be trained via RL. Hence, most recent work on TSP has defaulted to
training autoregressive RL models to minimize the tour length [2, 10, 13]. In contrast, [9] showed that
non-autoregressive SL models with sufficient labelled data (generated using Concorde) outperform
state-of-the-art RL approaches on fixed graph sizes. However, their non-autoregressive architecture
show poor ‘zero-shot’ generalization performance compared to autoregressive models when evaluated
on instances of different sizes than those used for training.
1

Code available: https://github.com/chaitjo/learning-paradigms-for-tsp

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

In this paper, we perform controlled experiments on the learning paradigm for autoregressive TSP
models2 with an emphasis on emergent generalization to variable graph sizes, especially those larger
than training graphs. We find that both SL and RL learn to solve TSP on fixed graph sizes very close
to optimal, with SL models achieving state-of-the-art results for TSP20-TSP100. Interestingly, RL
emerges as the superior learning paradigm for zero-shot generalization to variable and large-scale
TSP instances up to TSP500.
Our findings suggest that learning guided by sparse reward functions trains policies which identify
more general motifs and patterns for TSP. In contrast, policies trained to imitate optimal solutions
overfit to specific graph sizes. We contribute to existing literature on neural combinatorial optimization
by empirically exploring the impact of learning paradigms for TSP, and believe that RL will be a key
component towards building scale-invariant solvers for new combinatorial problems beyond TSP.

2

Experiments

Intuitively, learning from variable graph sizes is a straightforward way of building more robust and
scale-invariant solvers. In our experiments, we chose to focus on learning from graphs of fixed sizes
because we want to study the impact of the learning paradigm on emergent generalization in the
extreme case; where generalization is measured as performance on smaller or larger instances of the
same combinatorial problem that the model was trained on.
Model Setup We follow the experimental setup of [13]3 to train autoregressive Graph Attentionbased models for TSP20, TSP50 and TSP100, and evaluate on instances from TSP20 up till TSP500.
We train two otherwise equivalent variants of the model: an RL model trained with REINFORCE [19]
and a greedy rollout baseline; and an SL model trained to minimize a Cross Entropy Loss between
the model’s predictions and optimal targets at each step, similar to supervised Pointer Networks [18].
We use the model architecture and optimizer specified by [13] for both approaches. Optimal TSP
datasets from [9] are used to train the SL models whereas training data is generated on the fly for RL.
See Appendix A for detailed descriptions of training setups.
Evaluation We measure performance on held-out test sets of 10, 000 instances of TSP20, TSP50
and TSP100 from [9], as well as 1, 000 test instances of TSP150, TSP200, TSP300, TSP400 and
TSP500 generated using Concorde. We use the average predicted tour length and the average
optimality gap (percentage ratio of the predicted tour length relative to the optimal solution) over the
test sets as performance metrics. We evaluate both models in three search settings: greedy search,
sampling from the learnt policy (1, 280 solutions), and beam search (with beam width 1, 280).

3

Results

Performance on training graph sizes Table 1 presents the performance of SL and RL models
for various TSP sizes. In general, we found that both SL and RL models learn solvers close to
optimal for TSP20, TSP50 and TSP100 when trained on the problem size. In the greedy setting, RL
models clearly outperform SL models. As we sample or perform beam search, SL models obtains
state-of-the-art results for all graph sizes, showing significant improvement over RL models as well
as non-autoregressive SL models from [9]; e.g. TSP100 optimality gap of 0.38% for TSP100 SL
model using beam search vs. 2.85% for TSP100 RL model vs. 1.39% reported in [9].
Generalization to variable graph sizes RL clearly results in better zero-shot generalization to
problem sizes smaller or larger than training graphs. The different generalization trends for SL and
RL models can be visualized in Figure 1 and are highlighted below:
• Both TSP20 models do not generalize well to TSP100, but RL training leads to better
performance; e.g. in the greedy setting, TSP100 optimality gap of 32.25% for TSP20 SL
model vs. 15.21% for TSP20 RL model.
2
We select the autoregressive RL model [13] as it naturally fits the sequential nature of TSP and can easily
be extended to the SL setting . In contrast, it is not trivial to extend the non-autoregressive SL [9] model to the
RL setting.
3
We modify their codebase: https://github.com/wouterkool/attention-learn-to-route

2

Table 1: Test set performance of SL and RL models trained on various TSP sizes. In the Type column,
SL: Supervised Learning, RL: Reinforcement Learning. In the Decoder column, G: Greedy search,
S: Sampling, BS: Beam search
Model

Type

Decoder

Concorde

Tour Len.

TSP20
Opt. Gap.

Time

Tour Len.

TSP50
Opt. Gap.

Time

Tour Len.

TSP100
Opt. Gap.

Time

3.831

0.000%

(1m)

5.692

0.00%

(2m)

7.764

0.00%

(3m)

Greedy search
TSP20 Model
TSP50 Model
TSP100 Model

SL
SL
SL

G
G
G

3.847
4.177
5.696

0.437%
9.052%
48.712%

(1s)
(1s)
(1s)

6.219
5.951
6.643

9.254%
4.557%
16.717%

(1s)
(1s)
(1s)

10.269
8.519
8.589

32.255%
9.721%
10.616%

(5s)
(5s)
(5s)

TSP20 Model
TSP50 Model
TSP100 Model

RL
RL
RL

G
G
G

3.846
3.885
4.362

0.412%
1.434%
13.881%

(1s)
(1s)
(1s)

5.946
5.809
5.971

4.473%
2.053%
4.911%

(1s)
(1s)
(1s)

8.946
8.177
8.146

15.221%
5.312%
4.912%

(5s)
(5s)
(5s)

TSP20 Model
TSP50 Model
TSP100 Model

SL
SL
SL

S
S
S

3.831
3.834
4.380

0.000%
0.082%
14.348%

(6m)
(6m)
(6m)

5.992
5.694
5.751

5.268%
0.041%
1.038%

(26m)
(26m)
(26m)

11.115
8.135
7.862

43.151%
4.776%
1.259%

(1.5h)
(1.5h)
(1.5h)

TSP20 Model
TSP50 Model
TSP100 Model

RL
RL
RL

S
S
S

3.834
3.841
3.951

0.099%
0.278%
3.141%

(6m)
(6m)
(6m)

5.805
5.726
5.847

1.987%
0.598%
2.730%

(26m)
(26m)
(26m)

9.733
7.990
7.972

25.357%
2.901%
2.688%

(1.5h)
(1.5h)
(1.5h)

TSP20 Model
TSP50 Model
TSP100 Model

SL
SL
SL

BS
BS
BS

3.831
3.831
4.138

0.000%
0.000%
8.036%

(4m)
(4m)
(4m)

5.750
5.692
5.703

1.017%
0.000%
0.193%

(30m)
(30m)
(30m)

9.928
7.905
7.794

27.859%
1.807%
0.385%

(2h)
(2h)
(2h)

TSP20 Model
TSP50 Model
TSP100 Model

RL
RL
RL

BS
BS
BS

3.831
3.833
3.922

0.000%
0.067%
2.390%

(4m)
(4m)
(4m)

5.795
5.714
5.824

1.808%
0.388%
2.330%

(30m)
(30m)
(30m)

9.166
7.986
7.986

18.048%
2.847%
2.855%

(2h)
(2h)
(2h)

Sampling, 1280 solutions

Beam search, 1280 width

15
10

15

5

0

0

30

40

50

60

TSP Size

70

80

(a) Greedy Search

90

100

Model
TSP20 Model (GAT,SL)
TSP20 Model (GAT,RL)
TSP50 Model (GAT,SL)
TSP50 Model (GAT,RL)
TSP100 Model (GAT,SL)
TSP100 Model (GAT,RL)

20

10

5

20

25

Model
TSP20 Model (GAT,SL)
TSP20 Model (GAT,RL)
TSP50 Model (GAT,SL)
TSP50 Model (GAT,RL)
TSP100 Model (GAT,SL)
TSP100 Model (GAT,RL)

20

Optimality Gap (%)

20

Optimality Gap (%)

25

Model
TSP20 Model (GAT,SL)
TSP20 Model (GAT,RL)
TSP50 Model (GAT,SL)
TSP50 Model (GAT,RL)
TSP100 Model (GAT,SL)
TSP100 Model (GAT,RL)

Optimality Gap (%)

25

15
10
5

20

30

40

50

60

TSP Size

70

80

90

100

0

20

(b) Sampling

30

40

50

60

TSP Size

70

80

90

100

(c) Beam Search

Figure 1: Zero-shot generalization trends on TSP20-TSP100 for various search settings.
• The TSP50 RL model generalizes to TSP20 and TSP100 better than the TSP50 SL model,
except when performing beam search; e.g. in the sampling setting, TSP100 optimality gap
of 4.77% for TSP50 SL model vs. 2.90% for TSP50 RL model.
• In all search settings, the TSP100 SL model shows extremely poor generalization to TSP20
compared to the TSP100 RL model; e.g. in the beam search setting, TSP20 optimality gap
of 8.03% for TSP100 SL model vs. 2.39% for TSP100 RL model.
TSP solution visualizations for various models are available in Appendix D.
Performance on large-scale instances We further evaluate all SL and RL models on TSP instances
with up to 500 nodes to determine the upper limits of emergent generalization. During sampling/beam
search, we only sample/search for 250 solutions instead of 1, 280 as in previous experiments due to
time and memory constraints.
Figure 2 shows that RL training leads to relatively better generalization than SL in all search settings.
Although no model is able to solve large instances close to optimal, generalization performance
breaks down smoothly, suggesting that an incremental fine-tuning process using RL might be the
key to learning solvers for large-scale TSP. Additionally, sampling leads to worse performance than
greedy search for larger instances. This may be due to the probability distributions at each decoding
step being close to uniform, i.e. the learnt policy is not confident about choosing the next node.
3

60
40
20

100

Model
TSP20 Model (GAT,SL)
TSP20 Model (GAT,RL)
TSP50 Model (GAT,SL)
TSP50 Model (GAT,RL)
TSP100 Model (GAT,SL)
TSP100 Model (GAT,RL)

80

Optimality Gap (%)

80

Optimality Gap (%)

100

Model
TSP20 Model (GAT,SL)
TSP20 Model (GAT,RL)
TSP50 Model (GAT,SL)
TSP50 Model (GAT,RL)
TSP100 Model (GAT,SL)
TSP100 Model (GAT,RL)

60
40
20

0

150

200

250

300

350

TSP Size

400

450

60
40
20

0

500

Model
TSP20 Model (GAT,SL)
TSP20 Model (GAT,RL)
TSP50 Model (GAT,SL)
TSP50 Model (GAT,RL)
TSP100 Model (GAT,SL)
TSP100 Model (GAT,RL)

80

Optimality Gap (%)

100

150

(a) Greedy Search

200

250

300

350

TSP Size

400

450

0

500

150

(b) Sampling

200

250

300

350

TSP Size

400

450

500

(c) Beam Search

Figure 2: Zero-shot generalization trends on TSP150-TSP500 for various search settings.

Stability and sample efficiency of learning paradigms Conventionally, RL is known to be unstable and less sample efficient than SL. As seen by Figure 3, we found training to be stable and
equally sample efficient in our experiments for both learning paradigms. SL and RL models require
approximately equal number of mini-batches to converge to stable states using a fixed learning rate
of 10−4 . As noted by [13], using larger learning rates with a decay schedule speeds up learning at the
cost of stability.

2
1

Learning
SL (Run 1)
SL (Run 2)
RL (Run 1)
RL (Run 2)

14
12
10
8
6
4

50000

100000

# of batches

(a) TSP20

150000

200000

20
15
10
5

2
0

Learning
SL (Run 1)
SL (Run 2)
RL (Run 1)
RL (Run 2)

25

Optimality Gap (%)

Optimality Gap (%)

3

16

Optimality Gap (%)

Learning
SL (Run 1)
SL (Run 2)
RL (Run 1)
RL (Run 2)

4

0

50000

100000

# of batches

(b) TSP50

150000

200000

0

50000

100000

# of batches

150000

200000

(c) TSP100

Figure 3: Validation optimality gap (using greedy search) vs. number of training mini-batches for SL
and RL models. The difference in validation optimality gap is prominent in the greedy setting, but
reduces considerably as we sample or use beam search.

Impact of RL baseline and graph encoder In Appendix B, we replace the rollout baseline for REINFORCE with a critic baseline similar to [2]. We find that RL models follow similar generalization
trends to variable sized instances, regardless of the choice of baseline used.
Further, in Appendix C, we replace the Graph Transformer encoder with the Graph Convolutional
Network encoder from [9]. We find that choice of graph encoder has negligible impact on performance,
and better generalization in RL models is due to the learning paradigm.

4

Conclusion

This paper investigates the choice of learning paradigms for the Travelling Salesman Problem.
Through controlled experiments, we find that both supervised learning and reinforcement learning
can train models which perform close to optimal on fixed graph sizes. Evaluating models on variable
sized instances larger than those seen in training reveals a threefold advantage of RL training: (1)
training does not require expensive labelled data; (2) learning is as stable and sample-efficient as SL;
and most importantly, (3) RL training leads to better emergent generalization on variable sized graphs.
Finding (3) has broader implications on building scale-invariant solvers for practical combinatorial
problems beyond TSP.
Future work shall explore more detailed evaluations of learning paradigms for other problems, as
well as performing fine-tuning/transfer learning for generalization to large-scale instances.
4

Acknowledgement
XB is supported in part by NRF Fellowship NRFF2017-10.

References
[1] D. L. Applegate, R. E. Bixby, V. Chvatal, and W. J. Cook. The traveling salesman problem: a
computational study. Princeton university press, 2006.
[2] I. Bello, H. Pham, Q. V. Le, M. Norouzi, and S. Bengio. Neural combinatorial optimization
with reinforcement learning. In International Conference on Learning Representations, 2017.
[3] Y. Bengio, A. Lodi, and A. Prouvost. Machine learning for combinatorial optimization: a
methodological tour d’horizon. arXiv preprint arXiv:1811.06128, 2018.
[4] X. Bresson and T. Laurent. An experimental study of neural networks for variable graphs. In
International Conference on Learning Representations, 2018.
[5] J. Bruna, W. Zaremba, A. Szlam, and Y. Lecun. Spectral networks and locally connected
networks on graphs. In International Conference on Learning Representations, 2014.
[6] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs
with fast localized spectral filtering. In Advances in neural information processing systems,
pages 3844–3852, 2016.
[7] M. Deudon, P. Cournut, A. Lacoste, Y. Adulyasak, and L.-M. Rousseau. Learning heuristics
for the tsp by policy gradient. In International Conference on the Integration of Constraint
Programming, Artificial Intelligence, and Operations Research, pages 170–181. Springer, 2018.
[8] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In
Advances in Neural Information Processing Systems, pages 1024–1034, 2017.
[9] C. K. Joshi, T. Laurent, and X. Bresson. An efficient graph convolutional network technique for
the travelling salesman problem. arXiv preprint arXiv:1906.01227, 2019.
[10] E. Khalil, H. Dai, Y. Zhang, B. Dilkina, and L. Song. Learning combinatorial optimization
algorithms over graphs. In Advances in Neural Information Processing Systems, pages 6348–
6358, 2017.
[11] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
[12] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks.
In International Conference on Learning Representations, 2017.
[13] W. Kool, H. van Hoof, and M. Welling. Attention, learn to solve routing problems! In
International Conference on Learning Representations, 2019.
[14] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein. Geometric
deep learning on graphs and manifolds using mixture model cnns. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pages 5115–5124, 2017.
[15] K. A. Smith. Neural networks for combinatorial optimization: a review of more than a decade
of research. INFORMS Journal on Computing, 11(1):15–34, 1999.
[16] S. Sukhbaatar, A. Szlam, and R. Fergus. Learning multiagent communication with backpropagation. In Advances in Neural Information Processing Systems, pages 2244–2252, 2016.
[17] P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio. Graph Attention
Networks. International Conference on Learning Representations, 2018.
[18] O. Vinyals, M. Fortunato, and N. Jaitly. Pointer networks. In Advances in Neural Information
Processing Systems, pages 2692–2700, 2015.
[19] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229–256, 1992.

5

A

Experimental Setup

Table 2 presents an overview of our experimental setup for training autoregressive SL and RL models
on fixed graphs. We use the model architecture specified in [13] for both approaches: The graph
encoder is a 3-layer Graph Transformer [17] with 128-dimensional embeddings/hidden states and 8
attention heads per layer. The node and graph embeddings produced by the encoder are provided
to an autoregressive Attention-based decoder which outputs TSP tours node-by-node. Both models
are trained using the Adam optimizer [11] with a fixed learning rate of 10−4 for 100 epochs with
mini-batches of size 512, where the epoch size is 1, 000, 000.
Table 2: Training setup for comparing autoregressive SL and RL models.
Parameter
Reinforcement Learning
Supervised Learning
100
1, 000, 000
512
Random

100
1, 000, 000
512
Fixed set of 1M

Graph Transformer
3
128
128
512
8
706, 608

Graph Transformer
3
128
128
512
8
706, 608

REINFORCE, rollout baseline
Baseline update after validation
Adam
10−4 (fixed)

Cross Entropy Loss
Teacher Forcing
Adam
10−4 (fixed)

Training Epochs
Epoch Size
Batch Size
Graph Generation
Graph Encoder
Encoder Layers
Embedding Dimension
Hidden Dimension
Feed-forward Dimension
Attention Heads
Number of Parameters
Loss Function
Other Tricks
Optimizer
Learning Rate

B

Replacing Rollout with Critic Baseline

In Figure 4, we compare the generalization trends for RL models trained with rollout baselines (as
in [13]) and critic baselines (as in [2]). The critic network architecture uses the same 3-layer graph
encoder architecture as the main model, after which the node embeddings are averaged and provided
to an MLP with one hidden layer of 128-units and ReLU activation, and a single output. All other
experimental settings are kept consistent across both approaches, as in Table 2. We find that RL
models follow similar generalization trends to variable sized instances, regardless of the choice of
baseline used to train them.

15
10
5
0

25

Model
TSP20 Model (GAT,RL)
TSP20 Model (GAT,RL-AC)
TSP50 Model (GAT,RL)
TSP50 Model (GAT,RL-AC)
TSP100 Model (GAT,RL)
TSP100 Model (GAT,RL-AC)

20

Optimality Gap (%)

20

Optimality Gap (%)

25

Model
TSP20 Model (GAT,RL)
TSP20 Model (GAT,RL-AC)
TSP50 Model (GAT,RL)
TSP50 Model (GAT,RL-AC)
TSP100 Model (GAT,RL)
TSP100 Model (GAT,RL-AC)

15
10
5

20

30

40

50

60

TSP Size

70

80

(a) Greedy Search

90

100

0

Model
TSP20 Model (GAT,RL)
TSP20 Model (GAT,RL-AC)
TSP50 Model (GAT,RL)
TSP50 Model (GAT,RL-AC)
TSP100 Model (GAT,RL)
TSP100 Model (GAT,RL-AC)

20

Optimality Gap (%)

25

15
10
5

20

30

40

50

60

TSP Size

70

80

(b) Sampling

90

100

0

20

30

40

50

60

TSP Size

70

80

(c) Beam Search

Figure 4: Zero-shot generalization trends on TSP20-TSP100 for various search settings.

6

90

100

C

Replacing GAT Graph Encoder with GCN

To further isolate the impact of learning paradigms on performance and generalization, we swap
the Graph Transformer encoder used in [13] with the Graph Convolutional Network encoder from
[9], keeping the autoregressive decoder the same. Our GCN architecture consists of 3 graph convolution layers with 128-dimensional embeddings/hidden states. We use residual connections, batch
normalization and edge gates between each layer, as described in [4].
Figure 5 plots performance of SL and RL models using GAT and GCN as graph encoders. We find
negligible impact on performance for both learning paradigms despite GCN models have half the
number of parameters as GAT models (315, 264 for GCN vs. 708, 608 for GAT).4 Generalization
to instances sizes different from training graphs indicates that better emergent generalization in RL
models is chiefly due to the learning paradigm, and is independent of the choice of graph encoder.

20

30
20

10

10

0

0

30

40

50

60

TSP Size

70

80

90

100

(a) TSP20 Model, Greedy Search
12

6
4

0

60

TSP Size

70

80

90

100

(d) TSP50 Model, Greedy Search
25

Optimality Gap (%)

Optimality Gap (%)

5
0

20

80

90

30

40

50

60

TSP Size

70

80

90

100

(g) TSP100 Model, Greedy Search

30

40

50

60

TSP Size

70

80

90

100

(c) TSP20 Models, Beam Search

Model
TSP50 Model (GAT,SL)
TSP50 Model (GAT,RL)
TSP50 Model (GCN,SL)
TSP50 Model (GCN,RL)

10

30

40

50

60

TSP Size

70

80

90

8
6
4

0

100

20

40

50

60

TSP Size

70

80

90

100

Model
TSP100 Model (GAT,SL)
TSP100 Model (GAT,RL)
TSP100 Model (GCN,SL)
TSP100 Model (GCN,RL)

20

10

30

(f) TSP50 Model, Beam Search
25

15

0

20

12

5

20

0

100

Model
TSP100 Model (GAT,SL)
TSP100 Model (GAT,RL)
TSP100 Model (GCN,SL)
TSP100 Model (GCN,RL)

20

10

20

2

25

15

70

(e) TSP50 Model, Sampling

Model
TSP100 Model (GAT,SL)
TSP100 Model (GAT,RL)
TSP100 Model (GCN,SL)
TSP100 Model (GCN,RL)

20

60

TSP Size

4

0

50

50

6

2

40

40

8

2

30

30

Model
TSP50 Model (GAT,SL)
TSP50 Model (GAT,RL)
TSP50 Model (GCN,SL)
TSP50 Model (GCN,RL)

10

Optimality Gap (%)

Optimality Gap (%)

12

8

20

20

(b) TSP20 Model, Sampling

Model
TSP50 Model (GAT,SL)
TSP50 Model (GAT,RL)
TSP50 Model (GCN,SL)
TSP50 Model (GCN,RL)

10

30

10

Optimality Gap (%)

20

Model
TSP20 Model (GAT,SL)
TSP20 Model (GAT,RL)
TSP20 Model (GCN,SL)
TSP20 Model (GCN,RL)

40

Optimality Gap (%)

30

50

Model
TSP20 Model (GAT,SL)
TSP20 Model (GAT,RL)
TSP20 Model (GCN,SL)
TSP20 Model (GCN,RL)

40

Optimality Gap (%)

40

Optimality Gap (%)

50

Model
TSP20 Model (GAT,SL)
TSP20 Model (GAT,RL)
TSP20 Model (GCN,SL)
TSP20 Model (GCN,RL)

Optimality Gap (%)

50

15
10
5

20

30

40

50

60

TSP Size

70

80

90

100

(h) TSP100 Model, Sampling

0

20

30

40

50

60

TSP Size

70

80

90

100

(i) TSP100 Model, Beam Search

Figure 5: Zero-shot generalization trends for various search settings for GAT and GCN models.

4

We did not experiment with larger GCN encoders as 3-layer GCNs took longer to train than 3-layer GATs
and had high GPU overhead due to the computation of edge features for fully-connected graphs.

7

D

Solution Visualizations

Figures 6, 7, 8 and 9 display prediction visualizations for samples from test sets of various problem
instances under various search settings. In each figure, the first panel shows the groundtruth TSP tour,
obtained using Concorde. Subsequent panels show the predicted tours from each model.
Concorde: 4.131

TSP20 Model (GAT,SL): 4.131

TSP20 Model (GAT,RL): 4.154

TSP50 Model (GAT,SL): 4.154

TSP50 Model (GAT,RL): 4.131

TSP100 Model (GAT,SL): 7.477

TSP100 Model (GAT,RL): 4.562

TSP100 Model (GAT,SL): 4.543

TSP100 Model (GAT,RL): 4.255

TSP100 Model (GAT,SL): 4.664

TSP100 Model (GAT,RL): 4.213

(a) TSP20 instance, Greedy search
Concorde: 4.132

TSP20 Model (GAT,SL): 4.132

TSP20 Model (GAT,RL): 4.132

TSP50 Model (GAT,SL): 4.143

TSP50 Model (GAT,RL): 4.204

(b) TSP20 instance, Sampling (1280 solutions)
Concorde: 4.102

TSP20 Model (GAT,SL): 4.096

TSP20 Model (GAT,RL): 4.096

TSP50 Model (GAT,SL): 4.096

TSP50 Model (GAT,RL): 4.096

(c) TSP20 instance, Beam search (1280 width)

Figure 6: Prediction visualizations from various models for TSP20 test instances.
Concorde: 6.232

TSP20 Model (GAT,SL): 6.432

TSP20 Model (GAT,RL): 6.727

TSP50 Model (GAT,SL): 6.287

TSP50 Model (GAT,RL): 6.577

TSP100 Model (GAT,SL): 6.930

TSP100 Model (GAT,RL): 6.432

TSP100 Model (GAT,SL): 5.691

TSP100 Model (GAT,RL): 5.788

TSP100 Model (GAT,SL): 5.772

TSP100 Model (GAT,RL): 5.932

(a) TSP50 instance, Greedy search
Concorde: 5.679

TSP20 Model (GAT,SL): 5.950

TSP20 Model (GAT,RL): 5.782

TSP50 Model (GAT,SL): 5.679

TSP50 Model (GAT,RL): 5.689

(b) TSP50 instance, Sampling (1280 solutions)
Concorde: 5.772

TSP20 Model (GAT,SL): 5.824

TSP20 Model (GAT,RL): 5.963

TSP50 Model (GAT,SL): 5.772

TSP50 Model (GAT,RL): 5.856

(c) TSP50 instance, Beam search (1280 width)

Figure 7: Prediction visualizations from various models for TSP50 test instances.

8

Concorde: 7.848

TSP20 Model (GAT,SL): 11.054

TSP20 Model (GAT,RL): 9.077

TSP50 Model (GAT,SL): 8.450

TSP50 Model (GAT,RL): 8.102

TSP100 Model (GAT,SL): 7.945

TSP100 Model (GAT,RL): 8.126

TSP100 Model (GAT,SL): 7.395

TSP100 Model (GAT,RL): 7.553

TSP100 Model (GAT,SL): 7.648

TSP100 Model (GAT,RL): 7.772

(a) TSP100 instance, Greedy search
Concorde: 7.346

TSP20 Model (GAT,SL): 10.589

TSP20 Model (GAT,RL): 9.020

TSP50 Model (GAT,SL): 7.541

TSP50 Model (GAT,RL): 7.538

(b) TSP100 instance, Sampling (1280 solutions)
Concorde: 7.595

TSP20 Model (GAT,SL): 9.490

TSP20 Model (GAT,RL): 8.894

TSP50 Model (GAT,SL): 7.725

TSP50 Model (GAT,RL): 7.872

(c) TSP100 instance, Beam search (1280 width)

Figure 8: Prediction visualizations from various models for TSP100 test instances.

Concorde: 10.664

TSP20 Model (GAT,SL): 15.704

TSP20 Model (GAT,RL): 13.854

TSP50 Model (GAT,SL): 12.922

TSP50 Model (GAT,RL): 12.037

TSP100 Model (GAT,SL): 12.664

TSP100 Model (GAT,RL): 11.410

TSP100 Model (GAT,SL): 12.104

TSP100 Model (GAT,RL): 11.398

TSP100 Model (GAT,SL): 11.315

TSP100 Model (GAT,RL): 11.473

(a) TSP200 instance, Greedy search
Concorde: 10.693

TSP20 Model (GAT,SL): 21.592

TSP20 Model (GAT,RL): 18.537

TSP50 Model (GAT,SL): 14.481

TSP50 Model (GAT,RL): 12.885

(b) TSP200 instance, Sampling (250 solutions)
Concorde: 10.851

TSP20 Model (GAT,SL): 14.689

TSP20 Model (GAT,RL): 14.249

TSP50 Model (GAT,SL): 13.880

TSP50 Model (GAT,RL): 11.935

(c) TSP200 instance, Beam search (250 width)

Figure 9: Prediction visualizations from various models for TSP200 test instances.

9

