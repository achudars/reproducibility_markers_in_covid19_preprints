arXiv:1909.02739v2 [math.ST] 2 Oct 2020

Generalization of the simplicial depth: no
vanishment outside the convex hull of the
distribution support
Giacomo Francisci1,2 , Alicia Nieto-Reyes2 , and Claudio
Agostinelli1
1

2

University of Trento
University of Cantabria
October 5, 2020
Abstract

The simplicial depth, like other relevant multivariate statistical
data depth functions, vanishes right outside the convex hull of the
support of the distribution with respect to which the depth is computed. This is problematic when it is required to differentiate among
points outside the convex hull of the distribution support, with respect to which the depth is computed, based on their depth values.
We provide the first two proposals to overcome this issue as well as
several corresponding estimators, which do not vanish right outside
the convex hull of the data. The properties of the proposals and of
the corresponding estimators are studied, theoretically and by means
of Monte Carlo simulations.
Keywords: Classification, consistency, empirical depth, halfspace
depth, multivariate statistical data depth, multivariate symmetry, vanishment outside the convex hull.

1

1

Introduction

Multivariate statistical data depth functions provide an order of the elements of a space on Rp , p ≥ 1, with respect to a probability distribution on
the space. Regina Liu [1990] introduced the simplicial depth as an instance
of depth that satisfies some good theoretical properties that later became
the constituting properties of the notion of statistical data depth [Zuo and
Serfling, 2000a]. The simplicial depth of a p-dimensional point x with respect to a distribution P is the probability that x is contained in a closed
simplex
whose vertices are drawn indipendently from P , namely d(x; P ) =
R
I(x ∈ 4[x1 , . . . , xp+1 ])dP (x1 ) · · · dP (xp+1 ); with 4[x1 , . . . , xp+1 ] denoting
the closed simplex with vertices on x1 , . . . , xp+1 and I the indicator function.
Given a random sample X1 , . . . , Xn from P , the simplicial
depth is estimated

n −1 P
by the sample simplicial depth dn (x; P ) = p+1
1≤i1 <···<ip+1 ≤n I(x ∈
4[Xi1 , . . . , Xip+1 ]). Consistency of the sample simplicial depth is established
in Liu [1990] and Arcones and Giné [1993]. As studied in [Liu, 1990], the
simplicial depth is invariant under affine transformations and vanishes at infinity and, if P is absolutely continuous and angularly symmetric (see Section
2 for the definition of angular symmetry), it obtaines its maximum value at
the center of angular symmetry and it is monotonically nondecreasing along
rays through this point. Since its introduction, the simplicial depth has been
broadly studied and applied in the literature; see, for instance, Arcones and
Giné [1993], Arcones et al. [1994], Li et al. [2012].
A particularity of the simplicial depth is that it provides zero depth value
to the elements of the space that are outside the convex hull of the distribution support with respect to which the depth is computed. Additionally, the
sample simplicial depth gives value zero to every point in the space outside
the convex hull of the sample. These points are called outsiders [Lange et al.,
2014]. Our main aim is to address these two aspects as they become problematic for those applications in which it is required to discriminate among
different elements of the space.
Let us first focus on the theoretical case of not being able to differentiate
among elements outside the convex hull of a distribution support, which is
problematic, for example, on applications found in medicine such as neurodegenerative diseases. For instance, let P (i) , i = 1, 2, 3, be three multivariate
distribution functions and let us denote by Si , i = 1, 2, 3, the convex hulls of
the corresponding supports. Suppose we aim to divide S2 into two groups:
the points that are deeper in P (1) than in P (3) and its complementary; while
2

Figure 1: Left plot: Example of of three compact supports where the set
S2 \(S1 ∪ S3 ) is non-empty. Right plot: 3000 random draws from each of two
bivariate normal distributions: mean (0, 0) as triangles and mean (2, 2) as
circles. The first 500 elements of each sample are depicted in color.
keeping the occurrence of ties to a minimum. If S2 \(S1 ∪ S3 ) is non-empty,
as illustrated in the left plot of Figure 1 in R2 , the simplicial depth gives zero
depth to each point in this set when computed with respect to either P (1) or
P (3) . This might be the case of a disease with different stages: P (1) refers to
patients in the early stage of the disease, P (3) to patients in the last stage of
the disease and P (2) to patients in an intermediate stage of the disease. In
particular, we would aim to know which of the patients in the intermediate
stage of the disease are in a stage more similar to the last stage of the disease
than to the early, or viceversa.
The above scenario can be particularized to P (2) being a mixture of two
distributions that result of incurring on an error on P (1) and on P (3) so that
S2 \(S1 ∪ S3 ) is non-empty; the error on P (1) not being necessarily equivalent
to the one on P (3) . If no error is incurred, we are in front of a theoretical supervised classification problem, where S2 \(S1 ∪ S3 ) is empty; and the
vanishing outside the convex hull of the distribution support would not be
problematic. However, if P (1) and P (3) are unknown, the depth is estimated
through its sample version which suffers from the problem of vanishing out3

side the convex hull of the data sample. In general, a supervised classification
problem consists of two samples X = {X1 , . . . , Xm } and Z = {Z1 , . . . , Zo },
drawn respectively from P (1) and P (3) and a third sample Y = {Y1 , . . . , Yn },
where some of the elements of Y are drawn from P (1) and the others from
P (3) . The aim is to classify each element of Y as being drawn from either
distribution P (1) or P (3) . The X and Z are know as training samples and
the Y as test sample. This is illustrated in the right plot of Figure 1 in R2 ,
where the X and Z are plotted respectively in red and blue and the Y in
grey. In the particular setting of the plot, P (1) and P (3) are both normally
distributed. It can be easily observed from the right plot of Figure 1 that
there are elements of Y which are simultaneously outside both convex hulls,
the one of X and the one of Z. If we were to classify the elements of Y using
a supervised classification methodology based on statistical depth [see for example, Li et al., 2012], we would like the depth value of each Yi , i = 1, . . . , n,
with respect to the empirical distribution associated to X, to differ from the
value obtained when computed with respect to the empirical distribution associated to Z. Making use of the sample simplicial depth, it is not possible
to classify the elements of Y that are simultaneously outside the convex hull
of X and Z because the sample simplicial depth of these points is zero when
computed with respect to either the empirical distribution associated to X
or Z.
Three of the most well-known depth functions are Tukey, simplicial and
spatial depth. Recently, Girard and Stupfler [2017] proves that the spatial
depth is always positive, while this is not the case for either Tukey or simplicial depth. An approach to solve the problematic is pursued in Einmahl
et al. [2015] for the sample version of the Tukey depth. The proposal there
has, however, several shortcomings: it requires of certain restrictive assumptions that do not include common distributions like the normal distribution,
used in the right plot of Figure 1, or distributions with compact support such
as the uniform distribution in left plot of Figure 1. Additionally, as stated
there, their methodology is not applicable to other instances of depth like the
simplicial depth. A recent attempt to improve the sample Tukey depth is presented in the pre-print Nagy and Dvořák [2019]. However, as described there,
the estimator requires large sample sizes with a resulting bivariate estimated
depth value instead of scalar. Additionally, it needs restrictive conditions,
such as elliptically symmetric distributions, for a reasonable behavior. For
the sample zonoid, an approach is presented in Mosler and Hoberg [2006],
where, for outsiders, it is suggested to use the Mahalanobis depth which is
4

positive everywhere. Alternatively, the projection depth [Zuo, 2003] and the
spatial depth [Vardi and Zhang, 2000, Serfling, 2002] can be used. Lange
et al. [2014] suggests to use k-nearest neighbors to classify the outsiders.
We provide a methodology that generalizes the simplicial depth resulting
in depth functions that do not vanish right outside the convex hull of the
distribution support. The methodology is based on the idea of enlargement,
which is performed in two different ways. The first approach is an enlargement of the simplex involved in the definition of the simplicial depth and the
second approach might be seen as an enlargement of the distribution with respect to which the depth is computed; which involves linear combinations of
independent random variables. As multivariate symmetry [Zuo and Serfling,
2000b] is a key concept in the notion of depth, we dedicate Section 2 to study
the conditions under which symmetry is inherited under affine combinations.
This has far reaching implications as affine combinations are widely applied
in statistics, for instance, in dimension reduction problems. Moreover, the
non-symmetry of an affine combination will imply the non-symmetry of the
original distribution, under certain assumptions. In Section 3 we introduce
the two different generalizations of the simplicial depth. The results concerning multivariate symmetry are exploited to derive useful properties for the
simplicial depth based on an enlargement of the distribution. We provide
different sample versions of them that do not vanish right outside the convex
hull of the data sample and study the consistency of these sample versions.
In Section 4 we present some Monte Carlo simulations where the two main
contributions are studied empirically, for distributions with overlapping and
non-overlapping convex-hulls. The first setting includes a missing data scenario. The second one is an innovative scenario in the statistical literature
that is helpful for studying the closeness to some fixed groups of elements of
other distinct group(s). For instance, the example of degenerative diseases
provided above. The proofs of the results stated in the next sections and
some further results are provided in the supplementary material.

2

Symmetry of random variables

We prove under which notions of symmetry the affine combinations of independent and symmetric random variables are symmetric. The most wellknown notions of symmetry in the literature are spherical, elliptical, central,
angular and halfspace symmetry, where each is a generalization of the previ5

ous one. According to Serfling [2004], a random vector X ∈ Rp is spherically
symmetric about a point µ ∈ Rp if X − µ and U (X − µ) are identically
distributed for any orthonormal matrix U . Different definitions of elliptical
symmetry are possible. We choose the weak version in Ley and Paindaveine [2011]: a random vector X in Rp is elliptically symmetric about a point
µ ∈ Rp if there exists a nonsingular matrix V such that V X is spherically
symmetric about V µ. According to Serfling [2004], a random vector X in Rp
is centrally symmetric about a point µ ∈ Rp if X − µ and µ − X are identically distributed. Note that the notions of spherical, elliptical and central
symmetry coincide for univariate random variables. The notion of angular
symmetry was introduced in Liu [1990]: a random vector X in Rp is angularly
symmetric about a point µ ∈ Rp if (X − µ)/ kX − µk and (µ − X)/ kX − µk
are identically distributed. This was generalized in Zuo and Serfling [2000b]
by defining a random vector X in Rp to be halfspace symmetric about µ if
P(X ∈ H) ≥ 21 for every closed halfspace H with µ on the boundary.
The next proposition states that all these notions of symmetry are preserved under translation and scalar multiplication.
Proposition 1 Let X be a random variable on Rp that is symmetric about
µ ∈ Rp with respect to either spherical, elliptical, central, angular or halfspace
symmetry. Then, for any λ ∈ R and b ∈ Rp , λX +b is symmetric about λµ+b
with respect to the same notion of symmetry.
If a distribution is spherically, elliptically or centrally symmetric, the
center of symmetry is unique. If the distribution is angular or halfspace
symmetric, the center is unique but for the degenerate case in which the
distribution on Rp , p > 1, has all its probability mass on a line with more
than one median [Zuo and Serfling, 2000b, Theorem 2.1, Lemma 2.3]. Note
that, when the center of symmetry is not unique, Proposition 1 remains valid
for each center of symmetry.
The next two results concern the inheritance under affine combinations
of spherical, elliptical and central symmetry.
Proposition 2 Let X1 , . . . , Xn be independent and identically distributed
random variables on Rp that are symmetric about µ ∈ Rp with respect to
either spherical,
For any λ1 , . . . , λn ∈ R and
Pn elliptical or central symmetry. P
p
b ∈ R then i=1 λi Xi + b is symmetric about ni=1 λi µ + b with respect to
the same notion of symmetry.

6

The above proposition is generalized below to non-identically distributed
random variables for the notions of spherical and central symmetry.
Proposition 3 Let X1 , . . . , Xn be independent random variables on Rp . For
p
either spherical or central symmetry, let Xi be symmetric about
Pn µi ∈ R for
p
all i = 1, . . . , n. For
Pn any λ1 , . . . , λn ∈ R and b ∈ R then i=1 λi Xi + b is
symmetric about i=1 λi µi + b with respect to the same notion of symmetry.
This result does not hold for elliptically symmetric distributions. To
see this consider two multivariate random variables X1 , X2 , which are elliptically symmetric with means µ1 , µ2 and covariance matrices Σ1 , Σ2 .
By Frahm [2004][Proposition 1, Theorem 2] they have characteristic functions ϕX1 (x) = exp(ix> µ1 ) φ1 (x> Σ1 x), ϕX2 (x) = exp(ix> µ2 ) φ2 (x> Σ2 x) for
some functions φ1 , φ2 . Now, the sum X1 + X2 has characteristic function
ϕX1 +X2 (x) = ϕX1 (x) ϕX2 (x), and it does not fall back in the previous form
unless φ1 (s) φ2 (t) = φ(s + t), for some function φ. This is true for normal
distributions, in which case φ1 , φ2 have exponential form, but is not true in
general. For elliptically symmetric non-identically distributed random variables, we have the following corollary of Proposition 3.
Corollary 4 Let X1 , . . . , Xn be as in Proposition 3, but with
Pn Xi elliptically
symmetric about µi ∈ RpP
for each i = 1, . . . , n. Then,
i=1 λi Xi + b is
n
centrally symmetric about i=1 λi µi + b for any λ1 , . . . , λn ∈ R and b ∈ Rp .
For a series of counterexamples on angular and halfspace symmetry related to the above results, see the supplementary material, Section S-II.

3

Generalization of the simplicial depth

The simplicial depth [Liu, 1990]
of a point x ∈ Rp with respect to a distribuR
tion P on Rp is d(x; P ) = I(x ∈ 4[x1 , . . . , xp+1 ])dP (x1 ) · · · dP (xp+1 ), with
4[x1 , . . . , xp+1 ] denoting the closed simplex with vertices on x1 , . . . , xp+1 and
I the indicator function. The objective of this section is to modified the
simplicial depth in a manner that the depth value of the points in Rp that
are outside the convex hull of the support of P is not necessarily zero. We
pursue it in two different manners: Definition 5 uses an enlargement of the
simplex and Definition 7 computes the simplicial depth with respect to a
transformation of the original distribution with respect to which the depth
is evaluated. This transformation is introduced below in Definition 6.
7

Figure 2: Simplex generated by the random draws of three independent standard normal distribution in R2 , X1 , X2 and X3 (solid line),
P and the enlarged
simplex for σ = 2 (dash line), which has Y1 := 2X1 −1/3 3j=1 Xj as a vertex.

Definition 5 Given σ > 1, the simplex enlarged σ-simplicial depth of a
point x ∈ Rp with respect to a distribution P on Rp is
Z
d4 (x; P ) := I(x ∈ 4σ [x1 , . . . , xp+1 ])dP (x1 ) · · · dP (xp+1 ),
where
Pp+14σ [x1 , . . . , xp+1 ] := 4[y1 , . . . , yp+1 ] with yi = σxi + (1 − σ)/(p +
1) j=1 xj for i = 1, . . . , p + 1.
We illustrate Definition 5 in Figure 2, where it is shown a simplex in R2 and
the corresponding enlarged simplex for σ = 2. If we allow σ to take value
1 in the definition, there is no enlargement of the simplex and we are left
with the simplicial depth. Furthermore, the case 0 < σ < 1 corresponds to a
reduction of the simplex, whereas for σ = 0 the simplex degenerates into its
centroid. The results presented in this paper are valid for σ > 0. We state
σ > 1, though, because our interest lies in this subset.
Definition 6 Given P a distribution on Rp and σ > 1, the σ-transformation
of P
P, Pσ , is the distribution of the random variable σX1 + (1 − σ)/(p +
1) p+1
j=1 Xj , where X1 , . . . , Xp+1 are independent and identically distributed
random variables with distribution P.
8

To illustrate the σ-transformation of P, Figure 2 contains the realizations of
three independent random variables with the standard normal distribution
P
in R2 , X1 , X2 and X3 , and the random variable σX1 + (1 − σ)/3 3j=1 Xj ,
with σ = 2, which is denoted by Y1 in the plot. If we allow σ to be 1 in
the definition, no transformation is performed on P , while for 0 ≤ σ < 1 is
equivalent to the one mentioned above.
Definition 7 Given σ > 1, the distribution enlarged σ-simplicial depth of a
point x ∈ Rp with respect to a distribution P on Rp is
Z
dσ (x; P ) := I(x ∈ 4[x1 , . . . , xp+1 ])dPσ (x1 ) · · · dPσ (xp+1 ).
Note that dσ (x; P ) is the simplicial depth of x with respect to Pσ .
Remark 8 The simplex enlarged σ-simplicial depth is a refinement of the
simplicial depth that makes use of dependent vertices in the simplex while the
distribution enlarged σ-simplicial depth benefits from independent vertices in
the simplex. This is easily observed by realizing that
Z
d4 (x; P ) = I(x ∈ 4[l(x1 , . . . , xp+1 )]dP (x1 ) · · · dP (xp+1 ) and
Z
dσ (x; P ) = I(x ∈ 4[s(x1 , . . . , x(p+1)2 )]dP (x1 ) . . . dP (x(p+1)2 ),
with
p+1
p+1
1−σ X
1−σ X
xj , . . . , σxp+1 +
xj ) and
l(x1 , . . . , xp+1 ) := (σx1 +
p + 1 j=1
p + 1 j=1
p+1
1−σ
1−σ X
xj , . . . , σx1+p2 +p +
s(x1 , . . . , x(p+1)2 ) := (σx1 +
p + 1 j=1
p+1

(p+1)2

X

xj ).

j=1+p2 +p

In particular the function s is obtained byPobserving that from Definition
p+1
6 every Y ∼ Pσ has the form σX1 + 1−σ
j=1 Xj where X1 , . . . , Xp+1 are
p+1
independent and identically distributed random variables with distribution P .
Before studying the theoretical properties of the two definitions of the σsimplicial depth, we examine in the next subsection the transfer of regularity
conditions from P to Pσ . For this we make use of some of the results of
Section 2. These conditions will be useful in proving many of the properties
of the distribution enlarged σ-simplicial depth.
9

3.1

Properties of Pσ

We first recall that a distribution P is smooth if P (L) = 0 for any hyperplane
L ⊂ Rp [Massé, 2004, Condition (S)].
Theorem 9 Let P be a distribution on Rp and σ > 1. If P is a continuous distribution (or respectively absolutely continuous or smooth), the σtransformation of P, Pσ , is a continuous distribution (or respectively absolutely continuous or smooth).
The following result concerns the transfer of symmetry from P to Pσ .
Theorem 10 Let P be a distribution on Rp and σ > 1. If P is either spherical, elliptical or centrally symmetric about µ ∈ Rp , then Pσ is symmetric
about µ with respect to the same notion of symmetry.
It is well-known that, if the mean of a spherically, elliptically or centrally
symmetric distribution exists, then it coincides with the center of symmetry.
By Theorem 10, if P is either spherical, elliptical or centrally symmetric
about µ ∈ Rp , and its mean exists, then µ is also the mean of Pσ . In a
similar way, if the covariance matrix of P exists, by Definition
 6, it is2 not
difficult to see that the covariance matrix of Pσ is given by σ 2 + 1−σ
Σ.
p+1
The following example is an illustration of these results.
Example 11 If P is normally distributed with mean
ma 
 µ and covariance
1−σ 2
2
trix Σ, P ∼ N(µ, Σ), then for all σ > 1, Pσ ∼ N µ, σ + p+1 Σ .
Equivalently, for all τ > 1, Pq

3.2

(1+ p1 )τ 2 − p1

∼ N (µ, τ 2 Σ).

Properties of the σ-simplicial depth

We study the properties of the simplex and distribution enlarged σ-simplicial
depths below. Proposition 12 examines them as a function of σ and Theorems
13 and 15 for a fixed σ.
Proposition 12 The simplex enlarged σ-simplicial depth as a function of
sigma is monotonically nondecreasing and continuous on the right. If computed with respect to a smooth distribution, the simplex and distribution enlarged σ-simplicial depths are continuous as a function of sigma. Moreover,
the distribution enlarged σ-simplicial depth is monotonically nondecreasing
when computed with respect to elliptical distributions.
10

The above result ensures that for any smooth P, when σ gets close to 1, the
σ-simplicial depths have a similar behavior to that of the simplicial depth,
and that their behavior varies in a continuous manner with σ. Additionally,
these functions can only have jump discontinuities and, in particular, they
can only have at most countably many of them. The below results show
that the σ-simplicial depths satisfy properties put forward in defining the
simplicial depth. Upper semicontinuity is important in the last two results
of this subsection, while the continuity of the depth assures similar depth
values for points close to each other.
Theorem 13 The simplex enlarged σ-simplicial depth is (i) affine invariant,
(ii) vanishes at infinity and is (iii) upper semicontinuous as a function of x.
Additionally, if computed with respect to a smooth distribution, it is also (iv)
continuous as a function of x.
The simplex enlarged σ-simplicial depth does not satisfy the maximality
at the center property nor it is monotonically decreasing along rays from
this point even for spherically symmetric distributions (see Section S-III of
the supplementary material for a counterexample). However, for p = 1 it is
monotone for the points outside the convex hull of the support.
Proposition 14 Let σ > 1, P a distribution function on R and S the convex
hull of the support of P. If S ⊂ [a, b], a ≤ b then for either x ≤ y ≤ a or
b ≤ y ≤ x, we have that d∆ (x; P ) ≤ d∆ (y; P ).
Theorem 15 The distribution enlarged σ-simplicial depth is (i) affine invariant, (ii) vanishes at infinity and is (iii) upper semicontinuous as a function of x. Additionally, if computed with respect to a smooth distribution, it is
also (iv) continuous as a function of x. Furthermore, if computed with respect
to a smooth distribution that is centrally symmetric about a point µ ∈ Rp ,
then the σ-simplicial depth satisfies the (v) maximality at the center property
and is (vi) monotone nonincreasing along rays through the center.
The simplicial depth satisfies (v) and (vi) for angularly symmetric distributions. It is not generally true that if P is angularly symmetric, then also Pσ
is. However, Theorem 10 ensures that if P is centrally symmetric about µ,
then Pσ is centrally symmetric about µ, which implies that Pσ is angularly
symmetric.

11

Given a statistical data depth, d, with respect to a distribution P on Rp
and α > 0, the associated depth trimmed region is {x ∈ Rp : d(x; P ) ≥ α}.
Upper semicontinuity of the σ-simplicial depths implies that the corresponding trimmed regions are closed. Further properties of the trimmed regions
for the σ-simplicial depths follow from Theorem 13, Theorem 15 and Zuo
and Serfling [2000c, Theorem 3.1].
Theorem 16 The depth trimmed regions based on the simplex and distribution enlarged σ-simplicial depth are (i) affine equivariant, (ii) nested and (iii)
compact. Moreover, for the distribution enlarged σ-simplicial depth, they are
(iv) connected if P is smooth and centrally symmetric.
The following corollary shows that there exists at least one point that
maximizes the σ-simplicial depths.
Corollary 17 There exists at least: (i) a µσ such that dσ (µσ ; P ) = supx∈Rp dσ (x; P )
and (ii) a µ4 such that d4 (µ4 ; P ) = supx∈Rp d4 (x; P ).

3.3

Definition and properties of the empirical σ-simplicial
depth

A sample X1 , . . . , Xn of random draws from a distribution P gives rise to the
empirical distribution Pn . We provide, in the following definitions, sample
versions for the simplex enlarged (Definition 18) and the distribution enlarged
(Definitions 19 and 21) σ-simplicial depth functions.
Definition 18 Let P be a given distribution on Rp−1 , σ > 1, n ≥ p and Pn
the empirical distribution associated to a sample X1 , . . . , Xn of random draws
taken from P. For any x ∈ Rp−1 ,
 −1
X
n
d4,n (x; P ) :=
I(x ∈ 4σ [Xi1 , . . . , Xip ]).
p
1≤i <···<i ≤n
1

p

Definition 19 Let P be a given distribution on Rp−1 , σ > 1, n ≥ p2 and
Pn the empirical distribution associated to a sample X1 , . . . , Xn of random
draws taken from P. Let k := k(n, p) be the greatest integer less than or equal
to n/p. Then, for any x ∈ Rp−1 ,

12

 −1
X
k
dσ,k (x; P ) :=
I(x ∈ 4[Yi1 , . . . , Yip ])
p
1≤i1 <···<ip ≤k
P
p
with Yi = σX1+(i−1)p + 1−σ
j=1 Xj+(i−1)p for i = 1, . . . , k.
p
Remark 20 Note that Y1 , . . . , Yk in Definition 19 are random draws from
Pσ each obtained as linear combinations of p random draws from P . Thus,
dσ,k (·; P ) = dk (·, Pσ ), with dk (·, Pσ ) denoting the sample simplicial depth based
on k random draws from the distribution Pσ . Taking σ = 1, we obtain
dσ,k (·; P ) = dk (·, P ), retrieving the sample simplicial depth, but based only
on k of the n random draws X1 , . . . , Xn .
A simple method for computing the sample simplicial depth of a point
x in Rp−1 is to compute all the possible combinations for the vertices of the
simplices and to check if x is inside the simplex with those vertices. For
the first
 part, the computational cost, with respect to n and p, is of order
n
O( p ). The second part can be performed by exploiting Equation (1.8)
in Liu [1990], e. g. by Gauss elimination, at a computational cost of order
O (p3 ). The sample σ-simplicial depths of Definition 18 and Definition 19

can be computed in the same way, but the second one requires only O( kp )
operations for the choice of the vertices. Faster algorithms for computing
the sample simplicial depth are available for p ≤ 5 [Rousseeuw and Ruts,
1996, Cheng and Ouyang, 2001]. Although the sample σ-simplicial depth
of Definition 19 is a computationally efficient estimator of the distribution
enlarged σ-simplicial depth, we recommend to make use of the full sample
X1 , . . . , Xn and, thus, we propose the alternative definition below. The computational cost for computing the vertices of this alternative sample depth
is of order O( (n−p2 )! p!n!((p−1)!)p ). For further insight on it, see the supplementary material, Section S-IV. Approximated algorithms for the computation
of the sample depths can be obtained by considering only an average over
a (random) fraction m of the simplices [Pokotylo et al., 2016]. For all the
sample versions proposed, this approximation can lead to a reduction of the
total computational costs to O(mp3 ). For instance, the sample σ-simplicial

depth of Definition 19 can be obtained as an average of m = kp simplices
from Definition 21.

13

Definition 21 Let P be a given distribution on Rp−1 , σ > 1, n ≥ p2 and
Pn the empirical distribution associated to a sample X1 , . . . , Xn of random
draws taken from P. Then, for any x ∈ Rp−1 ,
dσ,n (x; P ) :=

(n − p2 )! X
I(x ∈ 4[Yi1 ,j1,1 ,...,j1,p−1 , . . . , Yip ,jp,1 ,...,jp,p−1 ]),
n!
A

where A := {i1 , .. . , ip , j1,1, . . . , jp,p−1 ∈ {1, . . . , n} : all indexes differ}
Pp−1
and Yik ,jk,1 ,...,jk,p−1 := σ + 1−σ
Xik + 1−σ
l=1 Xjk,l for k = 1, . . . , p.
p
p
Taking σ = 1 in dσ,n (·; P ), we retrieve the ranking provided by the empirical
simplicial depth.
The next results study the properties of these estimators.
Proposition 22 For any distribution P on Rp and any x ∈ Rp , the function
[1, ∞) → [0, 1]
σ 7→ d4,n (x; P )
is monotonically nondecreasing for each realization of the random variables
X1 , . . . , Xn , drawn with distribution P.
The above result is not valid in general for either of the estimators of the
distribution enlarged σ-simplicial depth.
Proposition 23 For any distribution P on Rp , d4,n (·; P ) is a U -statistic
for the estimation of d4 (·; P ) while dσ,k (·; P ) and dσ,n (·; P ) are U -statistics
for the estimation of dσ (·; P ).
Thanks to Proposition 23, the theoretical results for the empirical σsimplicial depths can make use of the study of U-processes [Korolyuk and
Borovskich, 2013]. In particular, the sample depths converge almost surely
to their population counterparts. Moreover, since the set of simplices in Rp
forms a VC-class [Arcones and Giné, 1993], this convergence can be made
uniform over Rp , as it is shown in the next theorem. This result is fundamental in practical applications since it ensures that the empirical σ-depths
are a good approximation of their population counterparts as the sample size
increases.
Theorem 24 For any distribution P on Rp , we have that
14

supx∈Rp |d4,n (x; P ) − d4 (x; P )| −−−→ 0

almost surely,

n→∞

supx∈Rp dσ,k(n,p) (x; P ) − dσ (x; P ) −−−→ 0

almost surely and

n→∞

supx∈Rp |dσ,n (x; P ) − dσ (x; P )| −−−→ 0

almost surely.

n→∞

Using the uniform convergence of the sample simplicial depths to their
population counterpart, the convergence of the maximizer can also be established. The maximizer of the distribution enlarged σ-simplicial depth
corresponds, for centrally symmetric distributions, to the point of central
symmetry (Theorem 15). The following corollary ensures in particular that
the maximizers of dσ,k and dσ,n are a good approximation of this point as the
sample size increases.
Corollary 25 The following statement is satisfied for (d, dn ) equal to either
(d4 , d4,n ), (dσ , dσ,k ) or (dσ , dσ,n ).
For any distribution P on Rp such that d(·; P ) is uniquely maximized, we
have that
µn → µ almost surely
with µ denoting the maximization point of d(·; P ) and {µn }n a sequence of
points in Rp such that dn (µn ; P ) = supx∈Rp dn (x; P ).
In the following theorem we establish the asymptotic normality of the
empirical process defined by the introduced sample simplicial depths as a
function of x; `∞ (Rp ) refers to the space of all bounded functions f : Rp → R
and the convergence in law in `∞ (Rp ) (denoted by ) is in the sense of
Hoffmann-Jørgensen [Massé, 2002]. Furthermore, using the notation of Definition 21,
hx (Xi1 , . . . , Xip , Xj1,1 , . . . , Xjp,p−1 ) := I(x ∈ 4[Yi1 ,j1,1 ,...,j1,p−1 , . . . , Yip ,jp,1 ,...,jp,p−1 ]).
Theorem 26 For any distribution P on Rp ,
√
(i) { n (d∆,n (x; P ) − d∆ (x; P )) : x ∈ Rp }

n→∞

(ii) {

p
{(p + 1)G∆
P (x) : x ∈ R }

p
k(n, p) (dσ,k(n,p) (x; P ) − dσ (x; P )) : x ∈ Rp }

x ∈ Rp }
√
(iii) { n (dσ,n (x; P ) − dσ (x; P )) : x ∈ Rp }
15

n→∞

n→∞

{(p + 1)GPσ (x) :

{(p + 1)2 GσP (x) : x ∈ Rp }.

σ
G∆
P , GPσ and GP are centered Gaussian process with covariance function
Z
Z
Z
E[G(x)G(y)] = gx (z)gy (z) dP (z) − gx (z) dP (z) gy (z) dP (z)

R
where gxR(z) = I(x ∈ 4σ [x1 , . . . , xp , z]) dP (x1 ) . . . dP (xp ) in (i),
gx (z) = I(x
R ∈ 4[x1 , . . . , xp , z]) dPσ (x1 ) . . . dPσ (xp ) in (ii) and
1
gx (z) = p+1 hx (z, x2 , . . . , x(p+1)2 ) dP (x2 ) . . . dP (x(p+1)2 )
R
p
+ p+1
hx (x1 , . . . , xp2 +2p , z) dP (x1 ) . . . dP (xp2 +2p ) in (iii).
Theorem 26 implies that, roughly speaking, the errors in the approximation in Theorem 24 are asymptotically normal with mean 0 and variance and
covariance specified by E[G(x)G(y)], for x, y ∈ Rp . In the next section we
study the finite-sample performance of the empirical σ-depths by means of
Monte Carlo simulations.

4

Simulations

We consider four simulation settings, which include a missing data scenario
and non-overlapping convex hulls scenario.

4.1

Overlapping convex-hulls

We provide three simulations under this scenario.
Simulation 1. When a supervised classifier is based on depth functions, it
is well-known the problematic cause by the vanishing of the empirical depth
outside the convex hull of the data: if a datum has depth zero when computed
with respect to two distinct samples, it can not be clearly classified based
on its depth value. The families of depth functions, and their corresponding
sample versions, defined in this paper give a solution to this problem. Furthermore, the hypothesis required on the distribution for a good behavior
of these families of depth functions are easily checked, as only a symmetry
test is required [see, Ley and Paindaveine, 2011, and the references therein].
A prior solution for the sample depth was provided in Einmahl et al. [2015]
for the particular case of the sample halfspace depth, by introducing the socalled refined halfspace depth. Their proposal is based on refining the sample
halfspace depth outside and in proximity of the boundaries of the convex hull
of each unidimensional projection by means of extreme value statistics. However, it requires certain restrictive assumptions, which leave out the normal
16

and uniform distribution among others. It has further disadvantages which
include: (i) the non-affine invariance of the depth; (ii) the required hypothesis are not easy to be verified in practice; (iii) the need of selecting a constant
r regarding the size of the refinement and estimating a constant γ regarding the speed of decrease outside the central region; (iv) the performance
for small sample sizes is not appropriate to have good estimations for the
univariate projections and the probabilities in the tails; (v) the probability
in the tails of unidimensional projections is estimated parametrically using
the same function for every probability distribution.
Hereafter, we study empirically our proposal under a two-class classification scenario inspired by Einmahl et al. [2015, Section 3.2]. We classify the
elements with zero simplicial depth value with respect to two elliptical distributions that satisfy their assumptions on the distribution and two normal
distributions; both cases under differences on location, scale and simultane(1)
(1)
(1)
ous location and scale differences. Consider Sn = {X1 , . . . , Xn } samples
(2)
(2)
(2)
from a distribution P (1) and Sm = {X1 , . . . , Xm } samples from another
distribution P (2) of sizes n = m = 500. We train the linear DD-classifier as
(1)
(2)
in [Li et al., 2012] on the samples Sn and Sm using one of our proposed
depths to construct the DD-plot. For a more general classification algorithm
see Cuesta-Albertos et al. [2017], Hubert et al. [2017]. We generate 5000 new
observations (2500 from P (1) and 2500 from P (2) ). We compute the misclassification rates according to the linear classifier for the new samples and for
(1)
(2)
the new outsiders of both Sn and Sm . We repeat the above experiment 100
times and compute the mean and standard deviation of the misclassification
rates for each choice of P (1) and P (2) . In particular, we consider the following setting: (i) P (1) is a standard bivariate normal distribution and P (2) is a
bivariate normal distribution with different location (difference is 2 for both
coordinates) or scale (three times the original scale for both coordinates) or
both; (ii) P (1) and P (2) have elliptical density of the form (r0 ≈ 1.2481)

3
x2
3 4
6 −

+ y 2 < r02
 4π r0 (1 + r0 ) 2
4

2
2
3 x4 +y 2
f (x, y) =
x2
+ y 2 ≥ r02 .

 
4
3  32

2
 4π 1+ x +y2
4

and again P (2) differs in location (difference is 4 for both coordinates) or scale
(three times the original scale for both coordinates) or both. Simulations
from this density are obtained using an acceptance-rejection method with a
1 degree of freedom t-distribution as proposal.
17

In Figure 3 we illustrate the obtained results for the sample simplex enlarged σ-simplicial depth for different values of σ, in the range from from 1.2
to 25, and for the sample simplicial depth (σ = 1). Figure 4 shows the results
for the refined halfspace depth computed as in [Einmahl et al., 2015] by using
500 random projections following [Cuesta-Albertos and Nieto-Reyes, 2008].
Comparing Figure 3 and Figure 4 it is evident that the misclassification rates
obtained for the sample simplex enlarged σ-simplicial depth are dramatically
lower except when there is a big difference in location, in which case the refined halfspace depth is slightly worse. For the details, see Table 1 and Table
6 of the supplementary material, Section S-V.

Figure 3: Boxplots of 100 misclassification rates of the outsiders for d4,n (·, ·)
with σ ∈ {1.2, 1.5, 2, 3, 4, 5, 7, 10, 15, 25} and the sample simplicial depth (σ =
1). Linear DD-plot classifier.
A more extensive simulation study has been carried out and included in
the supplementary material, Section S-V. For instance, Table 2 includes the
misclassification rates for all the elements in the sample. Figure 10 and Tables
3 and 4 repeat the study for the sample distribution enlarged σ-simplicial
depth dσ,k (·, ·) while Table 5 focuses on dσ,n (·, ·). The classifier used so far
is a linear DD-classifier. To compare it with a more complex one, we have
redone the study with a polynomial up to degree 10 DD-classifier for the
sample simplex enlarged σ-simplicial depth (Figure 11 and Tables 8 and 9)
and for the sample distribution enlarged σ-simplicial depth dσ,k (·, ·) (Figure
18

Figure 4: Boxplots of 100 misclassification rates of the outsiders for the
refined halfspace depth with r ∈ {10, 20, 30, 40, 60, 80, 100, 120, 160, 200} and
the sample halfspace depth (r = 0). Linear DD-plot classifier.
12 and Tables 10 and 11).
We also highlight that although the proposed σ-simplicial depths will
still be 0 outside an enlarged support this does not poses any problems in
applications since it is always possible to choose σ in such a way that the
sample σ-simplicial depths assign positive value to the region of interest.
Our suggestion for the choice of σ is to take the smallest σ such that all
points have positive depth with respect to at least one of the two samples.
Only if there are still many ties it might be worth to choose a bigger σ. For
more details about this we refer to Section 4.2.
Simulation 2. We perform a simulation under the scenario of four bivariate independent normal distributions with identity covariance matrix. We
denote them by P (1) , P (2) , P (3) , P (4) . We take P (1) with mean (−4, 0), P (2)
with mean (−δ, 0) where δ ∈ (0, 4) and P (4) with mean (4, 0). We consider
two scenarios: (i) symmetric distributions: P (3) with mean (δ, 0); and (ii)
asymmetric distributions: P (3) with mean (2, 0). The perfect classifier would
result on closeness of P (2) to P (1) and of P (3) to P (4) .
We generate 500, training, samples from P (1) and another 500 from P (4)
and 2500, test, samples from P (2) and another 2500 from P (3) . The procedure is repeated 100 times for each δ ∈ {.1, .2, .3, . . . , 3.9}. The mean of the
19

Figure 5: Mean, as a function of δ, of misclassification rates over 100 times
using the sample simplicial depth and d4,n (·, ·) for σ ∈ {1.2, 1.5, 2, 3, 4, 5}.
On the left symmetric distributions are given, on the right asymmetric ones.
misclassification rates using the sample simplicial depth and the sample simplex enlarged σ-simplicial depth for different values of σ are computed and
plotted as a function of δ in Figure 5. Clearly, the sample simplex enlarged
σ-simplicial depth outperforms the sample simplicial depth for all values of
δ. Furthermore, the results are similar for any σ ≥ 1.5, which shows that
this procedure is stable with respect to the choice of σ.
Simulation 3. This is a simulation study where there are missing data. Let
(1)
P and P (2) be two bivariate independent normal distributions with identity
covariance matrix; and respective means (−2, 0) and (2, 0). Let δ ∈ (0, 2).
We consider two cases: (i) symmetric band: {(x, y) ∈ R2 : −δ ≤ x ≤ δ} and
(ii) asymmetric band: {(x, y) ∈ R2 : −δ ≤ x ≤ 0}.
We draw 500, samples from P (1) and another 500 from P (2) ; and keep as
training samples those that are outside the corresponding band. Then, we
draw additional samples from P (1) and P (2) until there are 100 additional
samples from each inside the band we are using. We keep these 200 samples
as test samples. The maximum depth classifier [Ghosh and Chaudhuri, 2005]
is used to compute the mean of the misclassification rates over 100 times for
each δ ∈ {.05, .1, .15, . . . , 1.95}. Figure 6 displays the curves resulting of com20

puting the sample simplicial and sample simplex enlarged σ-simplicial depth
for σ ∈ {1, 1.2, 1.5, 2, 3, 4, 5}. Additionally, it displays the curve resulting of
computing the sample simplicial depth with respect to all of the training
samples; and not just the training samples outside the band as with the
other seven curves.
In Figure 6, it happens that every curve takes a value approximately
.5 when δ = .05. Let us first study the symmetric case. In this case, this
is due to we have an extremely narrow band and then, the distribution of
the 100 elements of the test sample from P (1) has approximately the same
amount of elements with the abscissa coordinate larger than zero than with
it smaller than zero. The same occurs for P (2) . Thus, the elements of the
training sample are assigned indistinctly to either P (1) or P (2) . This changes
the moment that δ grows as it is more probable to find elements of the test
sample that belongs to P (1) with x < 0. Analogously for P (2) with x > 0. The
simplicial depth case is depicted in red. One can observe that, as delta grows,
the misclassification rate decreases from the 50% as expected but it then
increases back again. This is mainly due to three reasons: (i) The training
sample size decreases as δ grows. (ii) For small values of δ it is expected
to obtain samples drawn from P (1) on both regions of the complement of
the band. However, as δ grows the probability of this happening rapidly
decreases. (iii) When σ is small (1 for the simplicial depth) and δ is big,
there are points in the band that have zero depth with respect to P (1) and
with respect to P (2) . Thus, they are assigned randomly. This characteristic
of the simplicial depth is inherited by the σ-simplicial depth for small values
of σ. However, for σ ≥ 3 it is easily observed from the left plot in Figure
6, that our methodology is able to steadily overcome these issues. It is also
important to notice the outstanding results as they are even better than
computing the simplicial depth without missing data. The reason for this is
that there are elements of the test sample that are outside the convex hull
of P (1) and of P (2) . This elements have zero simplicial depth but positive
σ-simplicial depth if σ is not too small.
The asymmetric case is different, though. Let us first focus in the case
of the simplicial depth without missing data. In this setting we have drawn
training samples from two normal distributions that only differ in mean. The
best separator of these two distributions is the line x = 0. The elements of the
test sample (half from P (1) and half from P (2) ) have the x coordinate smaller
or equal than 0 and, therefore, almost every element of the test sample is
assigned to P (1) , resulting in a misclassification rate close to .5, independently
21

Figure 6: Mean, as a function of δ, of misclassification rates over 100 times
using the sample simplicial depth and d4,n (·, ·) for σ ∈ {1, 1.2, 1.5, 2, 3, 4, 5}.
Symmetric band (left) and asymmetric band (right).
of δ. The missing data case works as follows. The training sample drawn from
P (1) has a larger amount of elements with x coordinate larger than zero than
the training sample drawn from P (2) has with x coordinate smaller than −δ.
Thus, as before, the simplicial depth assigns most of the elements in the test
sample to P (1) . Similarly occurs for the σ-simplicial depth for small values
of σ. When σ increases, the σ-simplicial depth overcomes this issue because
the amount of simplices covering the band increases rapidly. The reason for
every curve taking a value approximately .5 when δ = .05, is similar to that
of the symmetric case, as there is an extremely narrow band.

4.2

Non-overlapping convex-hulls

Simulation 4. We perform a simulation under the simple scenario of four
independent uniform distributions over the intervals [−2, −1], (−1, 0), (0, 1)
and [1, 2]. We denote them by P (1) , P (2) , P (3) and P (4) , respectively. In this
experiment, we consider two sample sizes n = 100, 1000 for the random
draws from P (1) and P (4) . We use them to assign each of 2500 observations
generated from P (2) and 2500 from P (3) to either the group defined by P (1)
22

0.5
0.3
0.0

0.1

0.2

Misclassification Rate

0.4

0.5
0.4
0.3
0.2

Misclassification Rate

0.1
0.0
1

5

10

50

100

500

1000

1

5

10

50

500

1000

0.08
0.06
0.00

0.02

0.04

Misclassification Rate

0.06
0.04
0.00

0.02

Misclassification Rate

100

σ

0.08

σ

3.0

3.1

3.2

3.3

3.4

3.5

3.0

σ

3.1

3.2

3.3

3.4

3.5

σ

Figure 7: Median (solid line), 25% and 75% quantiles (dashed lines) and
whiskers of the boxplot (dotted lines) of the misclassification rates for different values of σ, sample sizes n = 100 (left plots) and n = 1000 (right plots).
The bottom plots are a zoomed version of the above plots.

23

or P (4) . Note that the perfect assignment would allocate closeness to P (1) to
each of the observations from P (2) and to P (4) to each of the observations
from P (3) . The assignment can be performed using any of the depth functions
we have proposed in this paper together with a depth based classifier. To
exemplify it, we make use of the proposed sample version of the simplex
enlarged σ-simplicial depth and the maximum depth classifier. Although
more complex classifiers exist in the literature, the maximum depth classifier
suffices to show the outperformance of the enlarged depth over the classical.
In Figure 7 we show the median (solid line), the 25% and 75% quantiles
(dashed lines) and the whiskers of the boxplot (dotted lines) of the misclassification rates when the procedure is repeated 1000 times for different values
of σ. The value σ = 1 is the case of the sample simplicial depth which has
about 50 percent of misclassification rate. The misclassification rates of the
sample simplex enlarged σ-simplicial depth rapidly decreases. For extremely
large values of σ, the misclassification rate increases due to every enlarged
simplex contains each of the points we aim to classify. The optimal sample
value of σ as displayed in the figure is obtained around 3.28 for n = 100 and
around 3.02 for n = 1000.
Although the theoretical properties for the σ-simplicial depth functions
are satisfied for any sigma, the choice of σ is relevant for applications. If we
make use of either the distribution or simplex enlarged σ-simplicial depth for
this task, the appropriate sigma is σ = 3. For the distribution enlarged σsimplicial depth, the reason is that dσ (x, P ) is equal to the simplicial depth
(1)
with respect Pσ and the support of P3 is the interval [−3, 0] while the
(4)
support of P3 is the interval [0, 3]. Larger values of σ provide larger supports
while smaller values of σ provide shorter supports. For the simplex enlarged
σ-simplicial depth notice that for σ = 3 the largest enlarged simplex of
elements from P (1) is the interval [−3, 0]; and equivalently for P (4) . For finite
n the intervals [−2, 1] and [1, 2] are not entirely covered by the converx hull of
the sample points, which makes the optimal σ slightly bigger than 3. Thus,
the best choice of σ is to take the smallest possible value such that each
observation that we want to assign to P (1) or P (4) has positive depth with
respect to P (1) or P (4) . This simple idea remains valid for distributions other
than P (1) and P (4) and can also be applied to multivariate distributions.

24

S-I

Proofs

Unless specified otherwise, P is the probability measure over the Borel sets
of Rp .
Proof of Proposition 1. The case of X being symmetric about µ with
respect to either spherical, elliptical or central symmetry, is addressed below
in the proofs of Proposition 2 and 3 (n = 1).
Let X be angularly symmetric about µ. According to Zuo and Serfling
[2000b, Theorem 2.2], angular symmetry is equivalent to P(u> (X −µ) ≥ 0) =
P(u> (µ − X) ≥ 0) for all u ∈ S p−1 . Denote Eu1 := [u> (X − µ) ≥ 0] and Eu2 :=
[u> (µ − X) ≥ 0] for any u ∈ S p−1 . Let λ > 0. Multiplying by λ on both sides
of the inequality in Eu1 and Eu2 and adding and subtracting b to X, we have
that Eu1 = [u> ((λX+b)−(λµ+b)) ≥ 0] and Eu2 = [u> ((λµ+b)−(λX+b)) ≥ 0].
Due to P(Eu1 ) = P(Eu2 ) and the arbitrary of u, we get that λX +b is angularly
symmetric about λµ+b. The proof follows analogously for λ < 0, as we obtain
Eu1 = [u> ((λµ+b)−(λX +b)) ≥ 0] and Eu2 = [u> ((λX +b)−(λµ+b)) ≥ 0]. If
λ = 0, λX +b is the degenerate random variable b, which, clearly, is angularly
symmetric about λµ + b = b.
Let X be halfspace symmetric about µ. According to Zuo and Serfling [2000b, Theorem 2.4], X is halfspace symmetric about µ if and only
if Med(u> X) = u> µ for all u ∈ S p−1 , with Med(·) denoting the median
function. For any λ ∈ R and b ∈ Rp , we have that Med(u> (λX + b)) =
λ Med(u> X) + u> b = λu> µ + u> b = u> (λµ + b), where the first equality is
thanks to the median is a homogeneous function. Thus, λX + b is halfspace
symmetric about λµ + b.
Proof of Proposition 2. Let Xi be elliptically symmetric about µ for
each i = 1, . . . , n. As X1 , . . . , Xn are identically distributed, there exists a
nonsingular matrix V such that V Xi is spherically symmetric about V µ for all
d
i = 1, . . . , n. Thus, U (V Xi − V µ) = (V Xi − V µ) for any orthonormal
matrix
P
U and i =P
1, . . . , n. Given λ ∈ R and b ∈ Rp , let us denote Y := ni=1 λi Xi +b
and µ̃ := ni=1 λi µ + b. Thus,
!
n
n
X
X
d
d
U (V Y − V µ̃) = U
λi (V Xi − V µ) =
λi U (V Xi − V µ)
i=1
d

=

n
X

i=1
d

λi (V Xi − V µ) = V Y − V µ̃,

i=1

25

for any orthonormal matrix U. Then, Y is elliptically symmetric about µ̃.
The cases of spherical and central symmetry are addressed below in the
proof of Proposition 3.
Proof
of PropositionP3. Given λ ∈ R and b ∈ Rp , let us denote Y :=
Pn
n
i=1 λi Xi + b and µ̃ :=
i=1 λi µi + b. Let Xi be spherically symmetric about
µi for i = 1, . . . , n. Then, for any orthonormal matrix U
!
n
n
n
X
X
X
d
d
d
d
U (Y −µ̃) = U
λi (Xi − µi ) =
λi U (Xi −µi ) =
λi (Xi −µi ) = Y −µ̃,
i=1

i=1

i=1

which implies that Y is spherically symmetric about µ̃.
According to Zuo and Serfling [2000b, Lemma 2.1], a random variable X
d
is centrally symmetric about µ ∈ Rp if and only if u> (X −µ) = u> (µ−X) for
all u ∈ S p−1 . Let Xi be centrally symmetric about µi for i = 1, . . . , n. Then,
d
d Pn
>
Y is centrally symmetric about µ̃ as u> (Y − µ̃) =
i=1 λi u (Xi − µi ) =
Pn
d
>
>
i=1 λi u (µi − Xi ) = u (µ̃ − Y ).
Proof of Corollary 4. The proof follows directly from Proposition 3 as
elliptical symmetry implies central symmetry.
Proof of Theorem 9. Recall that a distribution P on Rp (or a random
variable X ∼ P ) is continuous if P ({x}) = 0 for all x ∈ Rp and absolutely
continuous if P (A) = 0 for anyPLebesgue measure 0 set A ⊂ Rp . Since Pσ is
p+1
the distribution of σX1 + 1−σ
j=1 Xj where X1 , . . . , Xp+1 are independent
p+1
and identically distributed random variables with distribution P, it is enough
to show that a linear combination of continuous (or respectively absolutely
continuous or smooth) random variables is continuous (or respectively absolutely continuous or smooth). For this, it suffices to prove that: (i) if
X ∼ P is continuous (or respectively absolutely continuous or smooth), then
λX is continuous (or respectively absolutely continuous or smooth) for any
λ ∈ R \ {0}; and (ii) if X ∼ P and Y ∼ Q are continuous (or respectively
absolutely continuous or smooth), then the sum X + Y is continuous (or
respectively absolutely continuous or smooth).
For (i), observe that λX ∼ P λ where P λ (B) := P ( λ1 B) for all measurable
subsets B ⊂ Rp , while forR (ii), notice that X + Y has distribution P ∗ Q
such that (P ∗ Q)(B) = P (B + {y}) dQ(y) for B ⊂ Rp measurable. In
26

both cases, the result follows by considering sets B of a specific form: if P is
continuous take B = {x} for x ∈ Rp , if P is absolutely continuous consider
Lebesgue measure 0 sets and if P is smooth consider hyperplanes in Rp .
Proof of Theorem 10. It is a direct consequence of Proposition 2.
The next lemma shows that simplices are nested, which is required for
the proof of Proposition 12 and 22.
Lemma 27 For all x1 , . . . , xp+1 ∈ Rp and scalars σ∗ ≥ σ ≥ 1, we have that
4σ∗ [x1 , . . . , xp+1 ]) ⊃ 4σ [x1 , . . . , xp+1 ]).
Proof of Lemma 27 . Let y ∈ 4σ [x1 , . . . , xp+1 ], then 4σ [x1 , . . . , xp+1 ] =
4[y1 , . . . , yp+1 ] for
yi := σxi + (1 − σ)/(p + 1)

p+1
X

xj , with i = 1, . . . , p + 1.

(1)

j=1

Due to Liu [1990, Equation (1.8)], there exist α1 , . . . , αp+1 ≥ 0 with α1 +
· · · + αp+1 = 1 such that y = α1 y1 + · · · + αp+1 yp+1 ; which by (1) results in
!
p+1
p+1
p+1
X
1 X
1 X
y=σ
α i xi −
xi +
xi .
p
+
1
p
+
1
i=1
i=1
i=1
Multiplying and diving by σ ∗ the first term of the sum,
!
p+1
p+1
p+1 



X
X
σ
σ
1
1 X
∗
y=σ
αi xi −
xi +
xi
σ∗
σ ∗ p + 1 i=1
p + 1 i=1
i=1
!
p+1
p+1
p+1
X
X
1
1 X
∗
∗
=σ
α i xi −
xi +
xi
p + 1 i=1
p + 1 i=1
i=1
∗

(2)

with αi∗ := σσ∗ αi + σ σ−σ
for i = 1, . . . , p + 1. Observe that αi∗ ≥ 0 and
∗
Pp+1 ∗
∗ ∗
∗
∗
∗
i=1 αi = 1. We rewrite equation (2) as y = α1 y1 +· · ·+αp+1 yp+1 where yi =
P
p+1
∗
σ ∗ xi +(1−σ ∗ )/(p+1) j=1 xj for i = 1, 2, . . . , p+1. Thus, y ∈ 4[y1∗ , . . . , yp+1
].
∗
∗
The proof finishes because 4σ∗ [x1 , . . . , xp+1 ] = 4[y1 , . . . , yp+1 ] by the definition of enlarged simplex.
27

Proof of Proposition 12. The simplex enlarged simplicial depth is monotonically non-decreasing as a function of σ as a direct consequence of above
Lemma 27.
We prove next the continuity on the right and the continuity for smooth
P of the σ-simplicial depth. Let σ > 1 and {σn }∞
n=1 be a sequence of real
numbers that converges to σ. Note that in Definition 5 4 depends on σ,
p
4
R = 4σ and for any n ∈ N, x ∈ R and P on R , d4σn (x; P ) − d4σ (x; P ) ≤
|gn (x1 , . . . , xp+1 ) − g(x1 , . . . , xp+1 )| dP (x1 ) . . . dP (xp+1 ) where
g(x1 , . . . , xp+1 ) := I(x ∈ 4σ [x1 , . . . , xp+1 ]) and analogously for gn , n ≥ 1.
Clearly {gn }∞
n=1 is measurable and bounded by 1. Moreover, for (x1 , . . . , xp+1 )
fixed we have two possibilities: (i) if x is not on the boundary of 4σ [x1 , . . . , xp+1 ],
there exists an  > 0 and N ∈ N such that |σ − σn | <  and gn (x1 , . . . , xp+1 ) =
g(x1 , . . . , xp+1 ) for all n ≥ N ; (ii) if x is on the boundary of 4σ [x1 , . . . , xp+1 ],
limσn ↑σ I(x ∈ 4σn [x1 , . . . , xp+1 ]) = 0 and limσn ↓σ I(x ∈ 4σn [x1 , . . . , xp+1 ]) =
1. Therefore, if {σn }∞
n=1 converges from above to σ, the corresponding sequence of functions {gn }∞
n=1 convergence pointwise to g and because of the
Lebesgue’s dominated convergence theorem, the distribution enlarged σsimplicial depth is right continuous.
Let P be smooth. The set
{(x1 , . . . , xp+1 ) ∈ Rp × · · · × Rp : x ∈ ∂4σ [x1 , . . . , xp+1 ]} has measure 0.
Hence, hn converges pointwise to h almost everywhere. The result follows
again from Lebesgue’s dominated convergence theorem.
The proof of the continuity of the distribution enlarged σ-simplicial depth
for smooth P is similar. Each Pσ , and Pσn , is replaced by p + 1 P ’s, so
that the dependence on σ is not in the probability distribution but only
in the integrand (see Definition 6). For any n ∈ N, x ∈ R and P on Rp ,
R|dσn (x; P ) − dσ (x; P )| ≤
hn (x1 , . . . , x(p+1)2 ) − h(x1 , . . . , x(p+1)2 ) dP (x1 ) . . . dP (x(p+1)2 ) where
h(x1 , . . . , x(p+1)2 ) := I(x ∈ 4σ ) with


(p+1)2
p+1
X
X
1−σ
1−σ
xj , . . . , σx1+p(p+1) +
xj  ;
4σ := 4 σx1 +
p + 1 j=1
p+1
j=1+p(p+1)

and analogously for hn , n ≥ 1. The result follows again from Lebesgue’s
dominated convergence theorem and the almost sure pointwise convergence
of the sequence {hn }∞
n=1 to h.
We prove next the monotonicity of the distribution enlarged sigma simplicial depth. If P has density f (x) = κg((x − µ)> Σ−1 (x − µ)) from Zuo and
28

Serfling [2000c, Theorem 3.3] we see that dσ (x;P ) = h((x− µ)> Σ−1
σ (x − µ)),
1−σ 2
2
where h is a nonincreasing function and Σσ = σ + p+1 Σ. Therefore, for
any scalars σ∗ ≥ σ ≥ 1, dσ∗ (x; P ) ≥ dσ (x; P ).
Proof of Theorem 13. (i) The affine invariance property follows from Liu
[1990, Equation (1.8)] and the fact that, for any p × p nonsingular matrix A
and p dimensional vector b, Ay + b ∈ 4[Ay1 + b, . . . , Ayp+1 + b] if and only
if y ∈ 4[y1 , . . . , yp+1 ].
(ii) For the vanishing at infinity property, observe that
lim dσ (x; P ) ≤ lim sup dσ (x; P )
kxk→∞
Z
≤ lim sup I(x ∈ 4σ [x1 , . . . , xp+1 ]) dP (x1 ) . . . dP (xp+1 ) = 0,

kxk→∞

kxk→∞

because of Lebesgue’s dominated convergence theorem.
(iii) For the upper semicontinuity property, let x∗ ∈ Rp and note that
Z
lim sup dσ (x; P ) ≤ lim sup I(x ∈ 4σ [x1 , . . . , xp+1 ]) dP (x1 ) . . . dP (xp+1 )
∗
x→x∗
Z x→x
≤ I(x∗ ∈ 4σ [x1 , . . . , xp+1 ]) dP (x1 ) . . . dP (xp+1 )
= dσ (x∗ ; P ).
(iv) Finally, let P be smooth, then
Z
lim inf
dσ (x; P ) ≥ lim inf
I(x ∈ 4σ [x1 , . . . , xp+1 ]) dP (x1 ) . . . dP (xp+1 )
∗
∗
x→x

x→x

= dσ (x∗ ; P ).

Proof of Proposition 14. For any x ∈ R and P on R,
d∆ (x; P ) =

Z 
1+σ
1−σ
1−σ
1+σ
= I
x1 +
x2 ≤ x ≤
x1 +
x2 dP (x1 )dP (x2 )
2
2
2
2

Z 
1−σ
1+σ
1+σ
1−σ
+ I
x1 +
x2 ≤ x ≤
x1 +
x2 dP (x1 )dP (x2 )
2
2
2
2
Z
− I (x1 = x2 = x) dP (x1 )dP (x2 ).
29

For any x ∈ R, let us denote

+
S1 (x) := (x1 , x2 ) ∈ R2

+
S2 (x) := (x1 , x2 ) ∈ R2

−
S1 (x) := (x1 , x2 ) ∈ R2

−
S2 (x) := (x1 , x2 ) ∈ R2

:
:
:
:

1+σ
x1 +
2
1−σ
x1 +
2
1+σ
x1 +
2
1−σ
x1 +
2

1−σ
x2
2
1+σ
x2
2
1−σ
x2
2
1+σ
x2
2


≤x ,

≥x ,

≥x ,

≤x ,

S + (x) := S1+ (x) ∩ S2+ (x) and S − (x) := S1− (x) ∩ S2− (x). Then,
Z
Z
Z
d∆ (x; P ) =
dP (x1 )dP (x2 )+
dP (x1 )dP (x2 )−
S − (x)

S + (x)

dP (x1 )dP (x2 ).

{(x,x)}

We consider the case b ≤ y ≤ x. The proof for the case x ≤ y ≤ a is
analogous. Thus,
d∆ (y; P )−d∆ (x; P ) =
Z
Z
=
dP (x1 )dP (x2 ) +
dP (x1 )dP (x2 )
S + (y)\S2+ (x)
S − (y)\S1− (x)
Z
Z
−
dP (x1 )dP (x2 ) −
dP (x1 )dP (x2 )
{(y,y)}
S + (x)\S1+ (y)
Z
Z
−
dP (x1 )dP (x2 ) +
dP (x1 )dP (x2 ).
S − (x)\S2− (y)

{(x,x)}

Notice that S + (x) \ S1+ (y) and S − (x) \ S2− (y) have no intersection with S ×
S. Hence, their corresponding probability is zero. Furthermore {(y, y)} is
a subset of both S + (y) \ S2+ (x) and S − (y) \ S1− (x). All this implies that
d∆ (y; P ) − d∆ (x; P ) ≥ 0.
Proof of Theorem 15. As the distribution enlarged σ-simplicial depth
with respect to a distribution P is the simplicial depth with respect to the
corresponding distribution Pσ , properties (i) and (ii) follow from Liu [1990,
Equations (1.8) and (1.9), Theorem 1] and the, above, proof of Theorem
13. The proofs of properties (iii) and (iv) are similar to that of Theorem 13
(see also Liu [1990, Theorem 2]). Observe that thanks to Theorem 9, if P is
30

smooth, then Pσ is smooth. Finally, if P is centrally symmetric then, because
of Theorem 10, Pσ is centrally symmetric about the same point. Therefore,
properties (v) and (vi) follow from Liu [1990, Theorem 3].
Proof of Theorem 16. (i) is a consequence of the affine invariance of
the σ-simplicial depths (see (i) of Theorem 13 and Theorem 15, and (a) of
Zuo and Serfling [2000c, Theorem 3.1]). (ii) follows directly from (b) of Zuo
and Serfling [2000c, Theorem 3.1]. For the distribution enlarged σ-simplicial
depth, (iii) holds because point (iii) of Theorem 15 implies that the depth
trimmed regions are closed and point (ii) of Theorem 15 implies that the
depth trimmed regions are bounded. For the simplex enlarged σ-simplicial
depth, the proof is the same using Theorem 13. (iv) is a consequence of point
(vi) of Theorem 15 and point (c) of Zuo and Serfling [2000c, Theorem 3.1].

Proof of Corollary 17. This is a direct consequence of Theorem 16. For
the proof see Rousseeuw and Ruts [1999, Proposition 7].
Proof of Proposition 22. d∆,n (x; P ) is an average of indicators of the
form I(x ∈ ∆σ [x1 , . . . , xp+1 ]) for x1 , . . . , xp+1 ∈ Rp . Then, the result follows
from Lemma 27.
Proof of Proposition 23. For any distribution P on Rp−1 , d4,n (·; P ) and
dσ,k (·; P ) are clearly U -statistic of order p with symmetric kernels for the
estimation of d4 (·; P ) and dσ (·; P ), respectively. We see also that dσ,n (x; P )
is a U-statistics of order p2 with symmetric kernel Kx , given below, for the
estimation of dσ (·; P ). In fact, it can be written in the form
 −1
X
n
dσ,n (x; P ) =
p2
1≤l <···<l
1

Kx (Xl1 , . . . , Xlp2 ),
p2 ≤n

where
Kx (Xl1 , . . . , Xlp2 ) =

p!((p − 1)!)p X
hx (Xi1 , . . . , Xip , Xj1,1 , . . . , Xjp,p−1 ).
p2 !
A

A := A(l1 , . . . , lp2 ) is the set of all possible p2 -tuples (i1 , . . . , ip , j1,1 , . . . , jp,p−1 )
given by splitting the indices l1 , . . . , lp2 into p ordered groups, the first of size
31

p and the remaining p of size p − 1 and hx (Xi1 , . . . , Xip , Xj1,1 , . . . , Xjp,p−1 ) =
I(x ∈ 4[Yi1 ,j1,1 ,...,j1,p−1 , . . . , Yip ,jp,1 ,...,jp,p−1 ]).
Proof of Theorem 24.
By Proposition 23, the sample depths are Ustatistics for the estimation of their population counterpart. Moreover the
class of functions indexing each of them are collections of indicators of a
VC-class of sets (i. e. simplices in Rp−1 ). For the almost sure uniform convergence of d∆,n (·, P ) to d∆ (·, P ), observe that the only difference with the
classical simplicial depth is the rescaling of the simplices. Therefore, Arcones
and Giné [1993, Corollary 6.7] holds. The result follows from Corollary 3.3
therein. Due to dσ,k(n,p) (·, P ) = dk(n,p) (·, Pσ ) is the classical sample simplicial depth based on k independent random draws with distribution Pσ , its
almost sure uniform convergence to dσ (·, P ) follows from Arcones and Giné
[1993, Corollary 6.8]. Finally, the convergence of dσ,n (·, P ) to dσ (·, P ) follows
from Arcones and Giné [1993, Corollary 3.3] along with a slight modification of Corollary 6.7 there, in order to adapt it to the class of functions
{hx : x ∈ Rp−1 }, with hx is as in the proof of Proposition 23.
Proof of Corollary 25.
The σ-simplicial depths d∆ (·, P ) and dσ (·, P )
are upper semicontinuous and vanish at infinity because of Theorem 13 and
15, respectively. Furthermore, Theorem 24 implies that, for either of the
three cases of (d, dn ), dn (·; P ) converges uniformly almost surely to d(·; P ).
Therefore, the proof of the almost sure convergence of µn to µ is analogous
to that of Arcones and Giné [1993, Theorem 6.9].
Proof of Theorem 26. ItR follows from Arcones and Giné [1993, Theorem
4.9]. In particular, gx (z) = kx (x1 , . . . , xm−1 , z) dP (x1 ) . . . dP (xm−1 ), where
m = p + 1 for (i) and (ii), m = (p + 1)2 for (iii) and kx is the kernel of the
corresponding U-statistic. This gives directly gx for (i) and (ii), while for (iii)
Z
gx (z) = Kx (x1 , . . . , xp2 +p , z) dP (x1 ) . . . dP (xm−1 )
Z
1
hx (z, x2 , . . . , x(p+1)2 ) dP (x2 ) . . . dP (x(p+1)2 )
=
p+1
Z
p
+
hx (x1 , . . . , xp2 +2p , z) dP (x1 ) . . . dP (xp2 +2p )
p+1
where Kx is as in the proof of Proposition 23.
32

S-II

Counterexamples for Section 2

Propositions 2 and 3 do not apply for neither angular nor halfspace symmetry, as we show in the following counterexamples. These counterexamples
picture random variables that are angular, and consequently halfspace, symmetric but that are not central symmetric. Counterexample 28 provides a
distribution on R, where the notion of symmetric distribution is unique and
coincide with that of spherical, elliptical and central symmetry. On R, all
distributions are angular and halfspace symmetric about the distribution median. Thus, in this counterexample, the affine combination is still angular
and halfspace symmetric but the center of symmetry differs from the one
stated in the above two propositions.
Counterexample 28 Let X1 , X2 be two independent and identically distributed random variables on R with the exponential distribution of parameter 1, which is symmetric about log(2) with respect to angular and halfspace symmetry. X1 + X2 follows a gamma distribution of parameters (2, 1),
which is also symmetric with respect to these two notions. If Proposition
2 were to be satisfied, the center of symmetry of the distribution associated
to X1 + X2 would be 2 log(2); however, the center of angular and halfspace
symmetry is the median of this gamma distribution, which is the solution of
(m + 1)e−m = .5, 1.39 approximately.
The below counterexamples consider distributions on R2 . In each of them,
the affine combination of two angular, and halfspace, symmetric distributions
is neither angular, nor halfspace symmetric. Counterexamples 29 and 30
concern discrete distributions; there is no mass on the center of symmetry in
the first one but there is in the second.
Counterexample 29 Let X1 , X2 be two independent and identically distributed random variables on R2 with the discrete uniform distribution on
the set
{(−1, 0), (−1, −1), (3, 0), (3, 3)}.
This distribution is angular, and halfspace, symmetric about (0, 0) but the
distribution associated to the random variable X1 + X2 is neither angular nor
halfspace symmetric.
We have that PX1 +X2 (−2, −2) = PX1 +X2 (−2, 0) = PX1 +X2 (6, 0) = PX1 +X2 (6, 6) =
1/16 and PX1 +X2 (−2, −1) = PX1 +X2 (2, −1) = PX1 +X2 (2, 0) = PX1 +X2 (2, 2) =
33

PX1 +X2 (2, 3) = PX1 +X2 (6, 3) = 1/8. Selecting the lines y = 0 and x = 2, we
obtain that (2, 0) is the only possible center of symmetry. However, it is easy
to see that it is not the case by considering for instance the line y = x − 2.
Counterexample 30 Let X1 , X2 be two independent and identically distributed random variables on R2 with the discrete uniform distribution on
the set
{(0, 0), (−1, 0), (−2, −2), (3, 0), (4, 4)}.
This distribution is angular, and halfspace, symmetric about (0, 0) but the
distribution associated to the random variable X1 + X2 is neither angular nor
halfspace symmetric.
It suffices to notice that PX1 +X2 (0, 0) = PX1 +X2 (−2, 0) = PX1 +X2 (−4, −4) =
PX1 +X2 (6, 0) = PX1 +X2 (8, 8) = 1/25 and PX1 +X2 (−1, 0) = PX1 +X2 (−2, −2) =
PX1 +X2 (3, 0) = PX1 +X2 (4, 4) = PX1 +X2 (−3, −2) = PX1 +X2 (2, 0) = PX1 +X2 (3, 4) =
PX1 +X2 (1, −2) = PX1 +X2 (2, 2) = PX1 +X2 (7, 4) = 2/25 and consider the same
lines than in the previous counterexample.
The next paragraphs contain two counterexamples where the involved
random variables are continuous. The first one is a modification of Counterexample 29: the distribution is concentrated on open balls instead of discrete points. The support of the random variables involved in the second is,
however, the entire R2 .
Counterexample 31 Given  ∈ (0, 1/8), let X1 , X2 be two independent and
identically distributed random variables on R2 with density function
f (x, y) =


1
1 1


3
I 3 (x, y) + IB(3,3)
IB(−1,0)
(x, y) + IB(−1,−1)
(x, y) +
(x, y) ,
4 π2
9 B(3,0)

where B(a,b)
denotes the open ball with center (a, b) and radius . Then, taking
r1
r2
r1 +r2
into account that the Minkowski sum of B(a
and B(a
is B(a
,
1 ,b1 )
2 ,b2 )
1 +a2 ,b1 +b2 )
it is obtained that X1 + X2 has probability mass 1/16 in each of the open balls
2
2
6
6
2
4
4
4
4
B(−2,0)
, B(−2,−2)
, B(6,0)
, B(6,6)
, and 1/8 in B(−2,−1)
, B(2,−1)
, B(2,0)
, B(2,2)
, B(2,3)
6
and B(6,3)
. It follows that X1 + X2 is not halfspace, nor angular, symmetric.
If X1 + X2 were to be symmetric, its center would belong to the rectangle
R := {(x, y) ∈ R2 : 2 − 4 ≤ x ≤ 2 + 4, −6 ≤ y ≤ 6}, but considering,
for instance, the line y = 73/80x − 9/16, we reach a contradiction. See the
left plot of Figure 8 for an illustration.

34

Figure 8: Left: For  = 0.095, in black the discs with mass 1/8, in grey the
discs with mass 1/16 and in red the rectangle R . The line y = 73/80x−9/16
is plotted in blue. Right:
√ The support of the random variable (1 + 2λ)X1 −
λX2 − λX3 for λ = 2. The black points have mass 1/64, the red points
1/32.

35

Counterexample 32 Let X1 , X2 be two independent random variables on
R2 whose distribution is a mixture of four bivariate normal distributions with
equal weights, respective means µ1 = (−1, 0), µ2 = (−1, −1), µ3 = (3, 0) and
µ4 = (3, 3) and covariance matrices Σ1 = Σ2 = σ 2 I and Σ3 = Σ4 = 9 σ 2 I
for some σ > 0. It is not difficult to see that this distribution is angular,
and halfspace, symmetric about (0, 0), while X1 + X2 is neither angular nor
halfspace symmetric.
Note that X1 + X2 is a mixture of ten normal distribution with respective
1
and ω5 = ω6 = ω7 = ω8 = ω9 = ω10 = 81 ,
weights ω1 = ω2 = ω3 = ω4 = 16
means µ1 = (−2, 0), µ2 = (−2, −2), µ3 = (6, 0), µ4 = (6, 6), µ5 = (−2, −1),
µ6 = (2, −1), µ7 = (2, 0), µ8 = (2, 2), µ9 = (2, 3) and µ10 = (6, 3) and covariance matrices Σi = c σ 2 I with c = 2 for i ∈ {1, 2, 5}, c = 18 for i ∈ {3, 4, 10}
and c = 10 for i ∈ {6, . . . , 9}. In order to see that X1 , X2 are angular symmetric about 0, it is enough to consider all the straight lines through the origin
and see that the corresponding halfspaces have probability 1/2. To prove that
X1 + X2 is not angularly symmetric, we restrict ourselves to a single possible
candidate for center of symmetry by considering the straight lines parallel to
the axes and taking the two lines whose corresponding halfspaces have probability 1/2. Then, it suffices to observe that there exists another straight
line that passes through this point but which does not provide mass 1/2 to
its corresponding halfspaces. Therefore, there is no center of symmetry. For
simplicity all this computations were done numerically.
The next counterexample is a modification of Counterexample 29 to include the type of affine combinations on which we focus in Section 3, and
for which we study the symmetry under affine combinations on this section.
For a continuous version of this counterexample, just replace, as in Counterexample 31, the points in the support by uniformly distributed open balls
with center on these points, or consider, for instance, a mixture of normal
distributions with mean on these points, as in Counterexample 32.
Counterexample 33 Let X1 , X2 , X3 be three independent and identically
distributed random variables on R2 following the distribution given in Counterexample 29. Then, for any λ 6= 0, the distribution of (1 + 2λ)X1 − λX2 −
λX3 is neither halfspace, nor angular symmetric. The reasoning for that is
equivalent to the one provided in Counterexample 29. We illustrate it in the
right plot of Figure 8, where any of the halfspaces that has a black line as
border have at least probability mass 1/2 while one of the halfspaces with the
36

blue line as border has smaller probability mass: the one containing the point
(5, 0).
The next counterexample applies only to Proposition 3, as in this counterexample the random variables are not identically distributed.
Counterexample 34 Let X1 , X2 be two independent random variables on
R2 with the following distributions: PX1 (0, 0) = PX2 (0, 0) = 1/5, PX1 (−1, 0) =
PX1 (5, 0) = 2/5 and PX2 (0, 3) = PX2 (0, −7) = 2/5. X1 and X2 are both angular, and halfspace, symmetric about (0, 0); but X1 + X2 is neither angular
nor halfspace symmetric.
Let us see that X1 + X2 is not halfspace symmetric, and consequently
not angularly symmetric. Denoting Y := X1 + X2 , we have that PY (−1, 3) =
PY (4, 0) = PY (0, −4) = PY (5, −7) = 4/25, PY (−1, 0) = PY (0, 3) = PY (0, −7) =
PY (5, 0) = 2/25 and PY (0, 0) = 1/25. Considering the halfspaces in which
the plane is divided by the coordinate axes, it is easy to see that the origin
is the only possible center of symmetry. However, the upper halfspace determined by any other straight line in the first and third quadrant through the
origin has mass smaller than 1/2.

S-III

Counterexample for Section 3

Here it is shown that, in general, the simplex enlarged σ-simplicial depth does
not satisfy the maximality at the center property for spherically symmetric
distributions; and that it is not monotonically decreasing from the center of
symmetry.
Let p = 1 and σ > 1. As in the proof of Proposition 14, the simplex
enlarged σ-simplicial depth of x ∈ R is given by
d∆ (x; P ) =
Z
Z
dP (x1 )dP (x2 ) +
S + (x)

Z
dP (x1 )dP (x2 ) −

S − (x)

dP (x1 )dP (x2 )
{(x,x)}

where the sets


1−σ
1−σ
1+σ
1+σ
+
2
x1 +
x2 ≤ x ≤
x1 +
x2
S (x) := (x1 , x2 ) ∈ R :
2
2
2
2


1−σ
1+σ
1+σ
1−σ
−
2
S (x) := (x1 , x2 ) ∈ R :
x1 +
x2 ≤ x ≤
x1 +
x2
2
2
2
2
37

have only the point (x, x) in common. For c > 0 and 0 <  ≤ min (σ − 1, 2) c/σ,
let us consider the probability distribution P with density function given by
f (x) =


1 
I(−−c,−c+) (x) + I(−+c,c+) (x) .
4

(3)

Clearly, P is symmetric about 0. Note that S := (−−c, −c+)∪(−+c, c+)
is the support of f and that S × S = S++ ∪ S+− ∪ S−+ ∪ S−− , with S++ :=
(− + c, c + ) × (− + c, c + ), S+− := (− + c, c + ) × (− − c, −c + ), S−+ :=
(− − c, −c + ) × (− + c, c + ) and S−− := (− − c, −c + ) × (− − c, −c + )
disjoint sets. Then, the simplex enlarged σ-simplicial depth of x is
Z
Z
d∆ (x; P ) =
f (x1 )f (x2 ) dx1 dx2 +
f (x1 )f (x2 ) dx1 dx2
S − (x)

S + (x)

1
λ(S + (x) ∩ (S++ ∪ S+− ∪ S−+ ∪ S−− ))
4
1
+ λ(S − (x) ∩ (S++ ∪ S+− ∪ S−+ ∪ S−− ))
4
=

where λ denotes the Lebesgue measure. For 0 ≤ α < 1 we compare the depth
difference of the points αc and c, d∆ (αc; P ) − d∆ (c; P ). Since  ≤ (σ − 1)/σc
we have that S−+ is a subset of both S + (c) and S + (αc), and that S+− is a
subset of both S − (c) and S − (αc). On the contrary, since  ≤ 2/σc, S−− does
not belong to any of the sets S + (c), S − (c), S + (αc) and S − (αc). It follows
that
d∆ (αc; P )−d∆ (c; P ) =

1
λ((S + (αc) ∪ S − (αc)) ∩ S++ ) − λ((S + (c) ∪ S − (c)) ∩ S++ ) ,
4
which is smaller than 0. Thus, the simplex enlarged σ-simplicial depth is not
monotonically decreasing from 0. This is illustrated in Figure 9.

S-IV

Definition

The expression of dσ,n in Definition 21 involves more summation terms than
required; for ease of understanding. The statement below does not involve
unnecessary terms.
Let P be a given distribution on Rp−1 , σ > 1, n ≥ p2 and Pn the empirical
distribution associated to a sample X1 , . . . , Xn of random draws taken from
38

Figure 9: The sample simplicial and simplex enlarged σ-simplicial depth for
σ ∈ {2, 5} of the distribution P with density (3) for c = 2 and  = 0.5;
constructed with 104 samples drawn from P .
P. Then, for any x ∈ Rp−1 ,
X
dσ,n (x, P ) = a

I(x ∈ 4[Yi1 ,j1,1 ,...,j1,p−1 , . . . , Yip ,jp,1 ,...,jp,p−1 ]),

1≤i1 <···<ip ≤n
{j1,1 ,...,j1,p−1 },...,
{jp,1 ,...,jp,p−1 }
⊂{1,...,n}
all indexes differ
2

p

where a := (n−p )! p!n!((p−1)!) .
The summation above is over all possible choices of the X’s that give rise
to different simplices. It is obtained as follows.
1. Choose p2 among n random drawns to be the ones
 exploited for comn
puting the vertices of the simplex. There are p2 ways to do this.
2. Choose p among the p2 random drawns to be the Xik k = 1, . . . , p
2
which have coefficient σ + 1−σ
. There are pp ways to do this.
p
3. Split the remaining p(p − 1) random drawns into p groups of size p − 1.
That is, the groups {Xjk,1 , . . . , Xjk,p−1 } associated with Xik for k =
(p2 −p)!
1, . . . , p. There are ((p−1)!)
p ways to do this.
39

All together, we obtain

S-V

n
p2




p2 (p2 −p)!
p ((p−1)!)p

=

n!
.
(n−p2 )! p! ((p−1)!)p

Further simulations

Table 1 shows the results when we classify only the outsiders, obviously the
results are much better for the sample simplex enlarged σ-simplicial depth
than for the sample simplicial depth since in this last case points are assigned
randomly. The performance of our procedure is still better even when we
classify the whole sample as it is shown in Table 2. Table 2 and Table 7
also show that the sample simplex enlarged σ-simplicial depth outperforms
the refined halfspace depth when considering the whole sample. Comparing
Table 1 with Table 3 (see also Figure 3 in the main paper and Figure 10
here) it is clear that d4,n performs much better than dσ,k(n,p) . Moreover the
choice of σ is more relevant for dσ,k(n,p) . The gap is smaller, however, for
the whole sample case as it can be seen by comparing Table 2 with Table
4. Therefore dσ,k(n,p) is a good contender for large amounts of data due to
its higher efficiency. The results of Table 5 for dσ,n are similar to what it
happens in Tables 1 and 2 for d4,n . Tables 8, 9, 10 and 11 use up to degree
10 polynomial classifier as implemented in the ddalpha R package [Pokotylo
et al., 2016]. The results are similar to those using the linear classifier.

40

Misclassification rates for the outsiders
Bivariate normal
Bivariate elliptical
location
scale
location & location
scale
location &
scale
scale
simplicial 0.51 (0.06) 0.50 (0.07) 0.50 (0.07) 0.49 (0.05) 0.49 (0.07) 0.49 (0.08)
σ = 1.2 0.17 (0.06) 0.13 (0.05) 0.13 (0.05) 0.23 (0.05) 0.22 (0.07) 0.32 (0.09)
σ = 1.5 0.07 (0.04) 0.00 (0.01) 0.00 (0.01) 0.11 (0.05) 0.07 (0.05) 0.19 (0.08)
σ=2
0.06 (0.04) 0 (0)
0 (0)
0.07 (0.04) 0.02 (0.02) 0.12 (0.06)
σ=3
0.05 (0.04) 0 (0)
0 (0)
0.05 (0.03) 0.01 (0.01) 0.12 (0.06)
σ=4
0.05 (0.04) 0 (0)
0 (0)
0.05 (0.03) 0.01 (0.01) 0.11 (0.06)
σ=5
0.05 (0.04) 0 (0)
0 (0)
0.04 (0.03) 0.01 (0.01) 0.11 (0.06)
σ=7
0.04 (0.03) 0 (0)
0 (0)
0.05 (0.03) 0.01 (0.01) 0.11 (0.06)
σ = 10
0.04 (0.03) 0 (0)
0 (0)
0.04 (0.03) 0.01 (0.01) 0.11 (0.05)
σ = 15
0.05 (0.03) 0 (0)
0 (0)
0.04 (0.03) 0.01 (0.01) 0.11 (0.05)
σ = 25
0.05 (0.03) 0 (0)
0 (0)
0.04 (0.03) 0.01 (0.01) 0.11 (0.06)
σ = 50
0.04 (0.03) 0 (0)
0 (0)
0.04 (0.03) 0.01 (0.01) 0.11 (0.05)
σ = 102 0.04 (0.03) 0 (0)
0 (0)
0.04 (0.03) 0.01 (0.01) 0.11 (0.05)
3
σ = 10
0.05 (0.03) 0 (0)
0 (0)
0.04 (0.03) 0.01 (0.01) 0.11 (0.06)
σ = 104 0.05 (0.03) 0 (0)
0 (0)
0.04 (0.03) 0.01 (0.01) 0.11 (0.06)
5
σ = 10
0.05 (0.03) 0 (0)
0 (0)
0.04 (0.03) 0.01 (0.01) 0.11 (0.06)

Table 1:
Mean and standard deviation, in parenthesis, of
100 misclassification rates of the outsiders for d4,n (·, ·) with σ ∈
{1.2, 1.5, 2, 3, 4, 5, 7, 10, 15, 25, 50, 102 , 103 , 104 , 105 } and the sample simplicial
depth (σ = 1). Linear DD-plot classifier.

41

Misclassification rates
Bivariate normal
Bivariate elliptical
location
scale
location & location
scale
location &
scale
scale
simplicial 0.09 (0.00) 0.17 (0.01) 0.13 (0.01) 0.01 (0.00) 0.16 (0.01) 0.04 (0.00)
σ = 1.2 0.08 (0.00) 0.17 (0.01) 0.13 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
σ = 1.5 0.08 (0.00) 0.17 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
σ=2
0.08 (0.00) 0.17 (0.01) 0.12 (0.01) 0.00 (0.00) 0.14 (0.01) 0.03 (0.00)
σ=3
0.08 (0.00) 0.17 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
σ=4
0.08 (0.00) 0.16 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
σ=5
0.08 (0.00) 0.17 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
σ=7
0.08 (0.00) 0.16 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
σ = 10
0.08 (0.00) 0.17 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
σ = 15
0.08 (0.00) 0.17 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
σ = 25
0.08 (0.00) 0.17 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
σ = 50
0.08 (0.00) 0.17 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
2
σ = 10
0.08 (0.00) 0.17 (0.01) 0.12 (0.01) 0.00
0.15 (0.01) 0.03 (0.00)
(0.001)
σ = 103 0.08 (0.00) 0.17 (0.01) 0.12 (0.00) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
σ = 104 0.08 (0.00) 0.17 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
σ = 105 0.08 (0.00) 0.17 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)

Table 2:
Mean and standard deviation, in parenthesis, of 100 misclassification rates, using the whole sample, for d4,n (·, ·) with σ ∈
{1.2, 1.5, 2, 3, 4, 5, 7, 10, 15, 25, 50, 102 , 103 , 104 , 105 } and the sample simplicial
depth (σ = 1). Linear DD-plot classifier.

42

Figure 10: Boxplots of 100 misclassification rates of the outsiders for dσ,k (·, ·)
with σ ∈ {1.2, 1.5, 2, 3, 4, 5, 7, 10, 15, 25} and the sample simplicial depth (σ =
1). Linear DD-plot classifier.

43

σ
σ
σ
σ
σ
σ
σ
σ
σ
σ
σ

=1
= 1.2
= 1.5
=2
=3
=4
=5
=7
= 10
= 15
= 25

Misclassification rates for the outsiders
Bivariate normal
Bivariate elliptical
location
scale
location & location
scale
location &
scale
scale
0.50 (0.06) 0.50 (0.07) 0.51 (0.08) 0.50 (0.06) 0.50 (0.08) 0.50 (0.07)
0.38 (0.08) 0.37 (0.09) 0.37 (0.09) 0.42 (0.08) 0.42 (0.10) 0.44 (0.10)
0.17 (0.07) 0.13 (0.07) 0.13 (0.07) 0.27 (0.08) 0.26 (0.08) 0.32 (0.09)
0.08 (0.04) 0.01 (0.01) 0.01 (0.02) 0.13 (0.06) 0.11 (0.06) 0.21 (0.08)
0.06 (0.04) 0 (0)
0 (0)
0.07 (0.03) 0.02 (0.02) 0.14 (0.07)
0.06 (0.04) 0 (0)
0 (0)
0.05 (0.03) 0.01 (0.01) 0.13 (0.08)
0.07 (0.05) 0 (0)
0.00 (0.00) 0.05 (0.03) 0.01 (0.01) 0.13 (0.08)
0.08 (0.07) 0 (0)
0.00 (0.00) 0.05 (0.03) 0.01 (0.01) 0.13 (0.09)
0.10 (0.09) 0 (0)
0.00 (0.02) 0.05 (0.03) 0.01 (0.01) 0.14 (0.09)
0.15 (0.12) 0.00 (0.02) 0.02 (0.05) 0.05 (0.03) 0.01 (0.02) 0.16 (0.11)
0.23 (0.18) 0.04 (0.10) 0.07 (0.12) 0.08 (0.07) 0.06 (0.12) 0.17 (0.11)

Table 3: Mean and standard deviation, in parenthesis, of 100 misclassification rates of the outsiders for dσ,k (·, ·) with σ ∈ {1.2, 1.5, 2, 3, 4, 5, 7, 10, 15, 25}
and the sample simplicial depth (σ = 1). Linear DD-plot classifier.

σ
σ
σ
σ
σ
σ
σ
σ
σ
σ
σ

=1
= 1.2
= 1.5
=2
=3
=4
=5
=7
= 10
= 15
= 25

Misclassification rates
Bivariate normal
Bivariate elliptical
location
scale
location & location
scale
location &
scale
scale
0.10 (0.01) 0.19 (0.01) 0.15 (0.01) 0.02 (0.01) 0.18 (0.02) 0.05 (0.01)
0.09 (0.01) 0.18 (0.01) 0.13 (0.01) 0.01 (0.00) 0.16 (0.01) 0.04 (0.01)
0.08 (0.00) 0.17 (0.01) 0.13 (0.01) 0.01 (0.00) 0.15 (0.01) 0.03 (0.00)
0.08 (0.00) 0.17 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
0.08 (0.01) 0.17 (0.01) 0.13 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
0.08 (0.01) 0.18 (0.01) 0.13 (0.01) 0.00 (0.00) 0.16 (0.01) 0.03 (0.00)
0.08 (0.01) 0.18 (0.01) 0.13 (0.01) 0.00 (0.00) 0.16 (0.02) 0.03 (0.00)
0.09 (0.02) 0.19 (0.02) 0.14 (0.02) 0.00 (0.00) 0.18 (0.02) 0.03 (0.00)
0.10 (0.04) 0.20 (0.03) 0.15 (0.03) 0.00 (0.00) 0.20 (0.03) 0.03 (0.01)
0.14 (0.09) 0.24 (0.04) 0.18 (0.05) 0.00 (0.00) 0.23 (0.05) 0.04 (0.03)
0.20 (0.15) 0.28 (0.05) 0.22 (0.08) 0.01 (0.03) 0.27 (0.05) 0.08 (0.08)

Table 4:
Mean and standard deviation, in parenthesis, of 100
misclassification rates,using the whole sample, for dσ,k (·, ·) with σ ∈
{1.2, 1.5, 2, 3, 4, 5, 7, 10, 15, 25} and the sample simplicial depth (σ = 1). Linear DD-plot classifier.
44

Misclassification rates
Bivariate normal: difference in location & scale
All the sample points
Outsiders
σ=5

0.12 (0.01)

0 (0)

Table 5: Mean and standard deviation, in parenthesis, of 100 misclassification rates, of the outsiders and of all the sample points, for dσ,n (·, ·) with
σ = 5. Linear DD-plot classifier.

r
r
r
r
r
r
r
r
r
r
r

=0
= 10
= 20
= 30
= 40
= 60
= 80
= 100
= 120
= 160
= 200

Misclassification rates for the outsiders
Bivariate normal
Bivariate elliptical
location
scale
location & location
scale
location &
scale
scale
0.48 (0.06) 0.49 (0.07) 0.49 (0.07) 0.49 (0.05) 0.50 (0.08) 0.50 (0.07)
0.31 (0.16) 0.24 (0.28) 0.24 (0.27) 0.06 (0.04) 0.19 (0.32) 0.15 (0.12)
0.25 (0.18) 0.20 (0.27) 0.20 (0.26) 0.06 (0.04) 0.14 (0.29) 0.14 (0.10)
0.26 (0.19) 0.18 (0.27) 0.18 (0.26) 0.06 (0.04) 0.10 (0.24) 0.12 (0.10)
0.23 (0.18) 0.15 (0.24) 0.15 (0.25) 0.05 (0.03) 0.11 (0.25) 0.11 (0.07)
0.23 (0.18) 0.14 (0.24) 0.14 (0.23) 0.05 (0.03) 0.11 (0.22) 0.11 (0.06)
0.25 (0.19) 0.14 (0.22) 0.14 (0.23) 0.05 (0.04) 0.12 (0.21) 0.11 (0.06)
0.26 (0.18) 0.17 (0.25) 0.17 (0.25) 0.05 (0.04) 0.15 (0.11) 0.11 (0.06)
0.24 (0.17) 0.16 (0.21) 0.13 (0.19) 0.05 (0.03) 0.16 (0.23) 0.11 (0.06)
0.34 (0.18) 0.27 (0.23) 0.23 (0.22) 0.05 (0.03) 0.17 (0.19) 0.11 (0.06)
0.44 (0.11) 0.43 (0.15) 0.44 (0.17) 0.06 (0.04) 0.20 (0.24) 0.15 (0.09)

Table 6:
Mean and standard deviation, in parenthesis, of 100 misclassification rates of the outsiders for the refined halfspace depth with
r ∈ {10, 20, 30, 40, 60, 80, 100, 120, 160, 200} and the sample halfspace depth
(r = 0). Linear DD-plot classifier.

45

r
r
r
r
r
r
r
r
r
r
r

=0
= 10
= 20
= 30
= 40
= 60
= 80
= 100
= 120
= 160
= 200

Misclassification rates
Bivariate normal
Bivariate elliptical
location
scale
location & location
scale
location &
scale
scale
0.09 (0.00) 0.17 (0.01) 0.13 (0.01) 0.01 (0.00) 0.15 (0.01) 0.03 (0.00)
0.09 (0.01) 0.17 (0.01) 0.13 (0.01) 0.00 (0.00) 0.15 (0.02) 0.03 (0.00)
0.09 (0.01) 0.17 (0.01) 0.13 (0.01) 0.00 (0.00) 0.15 (0.02) 0.03 (0.00)
0.09 (0.01) 0.17 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.02) 0.03 (0.00)
0.09 (0.01) 0.17 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
0.09 (0.01) 0.17 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
0.09 (0.01) 0.17 (0.01) 0.13 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
0.09 (0.01) 0.17 (0.01) 0.13 (0.01) 0.00 (0.00) 0.17 (0.02) 0.03 (0.00)
0.09 (0.01) 0.17 (0.01) 0.13 (0.01) 0.00 (0.00) 0.20 (0.04) 0.03 (0.00)
0.10 (0.02) 0.17 (0.02) 0.13 (0.02) 0.00 (0.00) 0.32 (0.05) 0.03 (0.00)
0.13 (0.03) 0.22 (0.04) 0.18 (0.03) 0.00 (0.00) 0.42 (0.05) 0.03 (0.00)

Table 7: Mean and standard deviation, in parenthesis, of 100 misclassification rates,using the whole sample, for the refined halfspace depth with
r ∈ {10, 20, 30, 40, 60, 80, 100, 120, 160, 200} and the sample halfspace depth
(r = 0). Linear DD-plot classifier.

46

Figure 11: Boxplots of 100 misclassification rates of the outsiders for d4,n (·, ·)
with σ ∈ {1.2, 1.5, 2, 3, 4, 5, 7, 10, 15, 25} and the sample simplicial depth (σ =
1). Polynomial up to degree 10 DD-plot classifier.

47

σ
σ
σ
σ
σ
σ
σ
σ
σ
σ
σ
σ
σ
σ
σ
σ

=1
= 1.2
= 1.5
=2
=3
=4
=5
=7
= 10
= 15
= 25
= 50
= 102
= 103
= 104
= 105

Misclassification rates for the outsiders
Bivariate normal
Bivariate elliptical
location
scale
location & location
scale
location &
scale
scale
0.50 (0.06) 0.50 (0.07) 0.50 (0.07) 0.50 (0.06) 0.48 (0.09) 0.50 (0.07)
0.17 (0.05) 0.13 (0.05) 0.13 (0.05) 0.24 (0.06) 0.21 (0.06) 0.30 (0.08)
0.08 (0.04) 0.01 (0.01) 0.01 (0.01) 0.09 (0.04) 0.06 (0.05) 0.18 (0.08)
0.06 (0.04) 0 (0)
0.00 (0.00) 0.06 (0.04) 0.02 (0.02) 0.12 (0.06)
0.05 (0.04) 0 (0)
0 (0)
0.05 (0.03) 0.01 (0.01) 0.11 (0.06)
0.05 (0.04) 0 (0)
0 (0)
0.05 (0.04) 0.01 (0.01) 0.11 (0.06)
0.05 (0.04) 0 (0)
0 (0)
0.05 (0.03) 0.01 (0.01) 0.11 (0.06)
0.05 (0.04) 0 (0)
0.00 (0.00) 0.05 (0.04) 0.01 (0.01) 0.11 (0.07)
0.05 (0.05) 0.01 (0.10) 0.00 (0.00) 0.06 (0.05) 0.01 (0.01) 0.13 (0.08)
0.05 (0.07) 0 (0)
0 (0)
0.05 (0.04) 0.01 (0.01) 0.12 (0.07)
0.05 (0.04) 0 (0)
0.00 (0.00) 0.06 (0.05) 0.01 (0.02) 0.12 (0.07)
0.06 (0.05) 0.00 (0.01) 0.00 (0.00) 0.04 (0.03) 0.01 (0.04) 0.13 (0.09)
0.05 (0.06) 0.00 (0.01) 0.00 (0.01) 0.04 (0.03) 0.01 (0.02) 0.16 (0.10)
0.05 (0.04) 0 (0)
0 (0)
0.04 (0.03) 0.01 (0.01) 0.21 (0.11)
0.05 (0.04) 0 (0)
0 (0)
0.12 (0.07) 0.01 (0.01) 0.21 (0.11)
0.05 (0.04) 0 (0)
0 (0)
0.14 (0.06) 0.01 (0.01) 0.21 (0.11)

Table 8:
Mean and standard deviation, in parenthesis, of
100 misclassification rates of the outsiders for d4,n (·, ·) with σ ∈
{1.2, 1.5, 2, 3, 4, 5, 7, 10, 15, 25, 50, 102 , 103 , 104 , 105 } and the sample simplicial
depth (σ = 1). Polynomial up to degree 10 DD-plot classifier.

48

σ
σ
σ
σ
σ
σ
σ
σ
σ
σ
σ
σ
σ
σ
σ
σ

=1
= 1.2
= 1.5
=2
=3
=4
=5
=7
= 10
= 15
= 25
= 50
= 102
= 103
= 104
= 105

Misclassification rates
Bivariate normal
Bivariate elliptical
location
scale
location & location
scale
location &
scale
scale
0.09 (0.01) 0.17 (0.01) 0.13 (0.01) 0.02 (0.07) 0.16 (0.01) 0.03 (0.00)
0.08 (0.00) 0.17 (0.01) 0.13 (0.00) 0.02 (0.10) 0.15 (0.01) 0.03 (0.00)
0.08 (0.00) 0.17 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.01)
0.08 (0.00) 0.17 (0.01) 0.12 (0.00) 0.00 (0.00) 0.14 (0.01) 0.03 (0.00)
0.08 (0.00) 0.17 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
0.08 (0.00) 0.16 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
0.08 (0.00) 0.16 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
0.08 (0.00) 0.16 (0.01) 0.12 (0.00) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
0.08 (0.00) 0.17 (0.01) 0.12 (0.00) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
0.08 (0.00) 0.16 (0.01) 0.12 (0.00) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
0.08 (0.00) 0.16 (0.01) 0.12 (0.00) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
0.08 (0.00) 0.17 (0.01) 0.12 (0.00) 0.00 (0.00) 0.15 (0.01) 0.03 (0.01)
0.08 (0.00) 0.16 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
0.08 (0.00) 0.16 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
0.08 (0.01) 0.16 (0.01) 0.12 (0.00) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
0.08 (0.01) 0.16 (0.01) 0.12 (0.00) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)

Table 9:
Mean and standard deviation, in parenthesis, of 100
misclassification rates,using the whole sample, for d4,n (·, ·) with σ ∈
{1.2, 1.5, 2, 3, 4, 5, 7, 10, 15, 25, 50, 102 , 103 , 104 , 105 } and the sample simplicial
depth (σ = 1). Polynomial up to degree 10 DD-plot classifier.

49

Figure 12: Boxplots of 100 misclassification rates of the outsiders for dσ,k (·, ·)
with σ ∈ {1.2, 1.5, 2, 3, 4, 5, 7, 10, 15, 25} and the sample simplicial depth (σ =
1). Polynomial up to degree 10 DD-plot classifier.

50

σ
σ
σ
σ
σ
σ
σ
σ
σ
σ
σ

=1
= 1.2
= 1.5
=2
=3
=4
=5
=7
= 10
= 15
= 25

Misclassification rates for the outsiders
Bivariate normal
Bivariate elliptical
location
scale
location & location
scale
location &
scale
scale
0.50 (0.06) 0.49 (0.07) 0.49 (0.07) 0.50 (0.06) 0.51 (0.08) 0.51 (0.07)
0.37 (0.08) 0.37 (0.09) 0.37 (0.09) 0.45 (0.08) 0.42 (0.11) 0.45 (0.09)
0.17 (0.05) 0.13 (0.07) 0.13 (0.07) 0.29 (0.11) 0.27 (0.10) 0.32 (0.09)
0.08 (0.04) 0.01 (0.02) 0.01 (0.02) 0.19 (0.15) 0.11 (0.06) 0.21 (0.08)
0.06 (0.04) 0 (0)
0 (0)
0.07 (0.04) 0.03 (0.03) 0.14 (0.07)
0.06 (0.04) 0 (0)
0.00 (0.00) 0.05 (0.03) 0.01 (0.02) 0.13 (0.07)
0.07 (0.06) 0 (0)
0 (0)
0.05 (0.03) 0.01 (0.01) 0.13 (0.07)
0.09 (0.07) 0 (0)
0 (0)
0.05 (0.03) 0.01 (0.01) 0.14 (0.09)
0.12 (0.10) 0 (0)
0.00 (0.01) 0.05 (0.05) 0.01 (0.01) 0.15 (0.12)
0.17 (0.13) 0.01 (0.04) 0.02 (0.08) 0.06 (0.04) 0.01 (0.01) 0.19 (0.15)
0.21 (0.15) 0.02 (0.08) 0.09 (0.15) 0.08 (0.07) 0.05 (0.11) 0.21 (0.17)

Table 10: Mean and standard deviation, in parenthesis, of 100 misclassification rates of outsiders for dσ,k (·, ·) with σ ∈ {1.2, 1.5, 2, 3, 4, 5, 7, 10, 15, 25}
and the sample simplicial depth (σ = 1). Polynomial up to degree 10 DD-plot
classifier.

51

σ
σ
σ
σ
σ
σ
σ
σ
σ
σ
σ

=1
= 1.2
= 1.5
=2
=3
=4
=5
=7
= 10
= 15
= 25

Misclassification rates
Bivariate normal
Bivariate elliptical
location
scale
location & location
scale
location &
scale
scale
0.11 (0.06) 0.19 (0.01) 0.14 (0.01) 0.05 (0.11) 0.18 (0.01) 0.05 (0.01)
0.09 (0.01) 0.18 (0.01) 0.13 (0.01) 0.05 (0.13) 0.16 (0.01) 0.04 (0.00)
0.08 (0.00) 0.17 (0.01) 0.13 (0.01) 0.06 (0.16) 0.15 (0.01) 0.03 (0.00)
0.08 (0.00) 0.17 (0.01) 0.12 (0.01) 0.07 (0.17) 0.15 (0.01) 0.03 (0.00)
0.08 (0.01) 0.17 (0.01) 0.12 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
0.08 (0.01) 0.17 (0.01) 0.13 (0.01) 0.00 (0.00) 0.15 (0.01) 0.03 (0.00)
0.09 (0.01) 0.17 (0.01) 0.13 (0.01) 0.00 (0.00) 0.16 (0.01) 0.03 (0.00)
0.09 (0.02) 0.18 (0.02) 0.14 (0.02) 0.00 (0.00) 0.17 (0.02) 0.03 (0.00)
0.10 (0.04) 0.20 (0.02) 0.15 (0.03) 0.00 (0.00) 0.18 (0.03) 0.04 (0.01)
0.12 (0.06) 0.22 (0.04) 0.16 (0.04) 0.00 (0.00) 0.21 (0.04) 0.04 (0.02)
0.13 (0.07) 0.25 (0.04) 0.18 (0.04) 0.00 (0.02) 0.24 (0.04) 0.05 (0.03)

Table 11:
Mean and standard deviation, in parenthesis, of 100
misclassification rates,using the whole sample, for dσ,k (·, ·) with σ ∈
{1.2, 1.5, 2, 3, 4, 5, 7, 10, 15, 25} and the sample simplicial depth (σ = 1).
Polynomial up to degree 10 DD-plot classifier.

52

References
Arcones, M. A. and E. Giné (1993). Limit theorems for u-processes. The
Annals of Probability 21 (3), 1494–1542.
Arcones, M. A., C. Zhiqiang, and E. Giné (1994). Estimators related to uprocesses with applications to multivariate medians: asymptotic normality.
The Annals of Statistics 22 (3), 1460–1477.
Cheng, A. Y. and M. Ouyang (2001). On algorithms for simplicial depth. In
In: Proceedings 13th Canadian Conference on Computational Geometry,
Volume 1, pp. 53–56.
Cuesta-Albertos, J. A., M. Febrero-Bande, and M. O. de la Fuente (2017).
The ddg -classifier in the functional setting. Test 26 (1), 119–142.
Cuesta-Albertos, J. A. and A. Nieto-Reyes (2008). The random tukey depth.
Computational Statistics & Data Analysis 52 (11), 4979–4988.
Einmahl, J. H., J. Li, and R. Y. Liu (2015). Bridging centrality and extremity:
Refining empirical data depth using extreme value statistics. The Annals
of Statistics 43 (6), 2738–2765.
Frahm, G. (2004). Generalized elliptical distributions: theory and applications. Ph. D. thesis, Universität zu Köln.
Ghosh, A. K. and P. Chaudhuri (2005). On maximum depth and related
classifiers. Scandinavian Journal of Statistics 32 (2), 327–350.
Girard, S. and G. Stupfler (2017). Intriguing properties of extreme geometric
quantiles. REVSTAT: Statistical Journal 15 (1), 107–139.
Hubert, M., P. Rousseeuw, and P. Segaert (2017). Multivariate and functional
classification using depth and distance. Advances in Data Analysis and
Classification 11 (3), 445–466.
Korolyuk, V. S. and Y. V. Borovskich (2013). Theory of U-statistics, Volume
273. Springer Science & Business Media.
Lange, T., K. Mosler, and P. Mozharovskyi (2014). Fast nonparametric
classification based on data depth. Statistical Papers 55 (1), 49–69.
53

Ley, C. and D. Paindaveine (2011). Depth-based runs tests for multivariate
central symmetry. ECORE Discussion Papers 22.
Li, J., J. A. Cuesta-Albertos, and R. Y. Liu (2012). Dd-classifier: Nonparametric classification procedure based on dd-plot. Journal of the American
Statistical Association 107 (498), 737–753.
Liu, R. Y. (1990). On a notion of data depth based on random simplices.
The Annals of Statistics 18 (1), 405–414.
Massé, J.-C. (2002). Asymptotics for the tukey median. Journal of Multivariate Analysis 81 (2), 286–300.
Massé, J.-C. (2004). Asymptotics for the tukey depth process, with an application to a multivariate trimmed mean. Bernoulli 10 (3), 397–419.
Mosler, K. and R. Hoberg (2006). Data analysis and classification with the
zonoid depth. DIMACS Series in Discrete Mathematics and Theoretical
Computer Science 72, 49–59.
Nagy, S. and J. Dvořák (2019). Illumination depth. arXiv e-prints.
Pokotylo, O., P. Mozharovskyi, and R. Dyckerhoff (2016). Depth and depthbased classification with R-package ddalpha. arXiv:1608.04109.
Rousseeuw, P. J. and I. Ruts (1996). Algorithm as 307: Bivariate location
depth. Journal of the Royal Statistical Society. Series C (Applied Statistics) 45 (4), 516–526.
Rousseeuw, P. J. and I. Ruts (1999). The depth function of a population
distribution. Metrika 49, 213–244.
Serfling, R. (2002). A depth function and a scale curve based on spatial
quantiles. In Statistical Data Analysis Based on the L1-Norm and Related
Methods, pp. 25–38. Springer.
Serfling, R. (2004). Multivariate symmetry and asymmetry. Encyclopedia of
statistical sciences 8.
Vardi, Y. and C. Zhang (2000). The multivariate l1-median and associated
data depth. Proceedings of the National Academy of Sciences 97, 1423–
1426.
54

Zuo, Y. (2003). Projection-based depth functions and associated medians.
The Annals of Statistics 31 (5), 1460–1490.
Zuo, Y. and R. Serfling (2000a). General notions of statistical depth function.
Annals of statistics 28, 461–482.
Zuo, Y. and R. Serfling (2000b). On the performance of some robust nonparametric location measures relative to a general notion of multivariate
symmetry. Journal of Statistical Planning and Inference 84 (1-2), 55–79.
Zuo, Y. and R. Serfling (2000c). Structural properties and convergence results for contours of sample statistical depth functions. The Annals of
Statistics 28 (2), 483–499.

55

