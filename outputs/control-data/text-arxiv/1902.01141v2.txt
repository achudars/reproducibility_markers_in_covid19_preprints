A note on the geometry of the MAP partition in Conjugate
Exponential Bayesian Mixture Models
Lukasz Rajkowski∗
John Noble†

arXiv:1902.01141v2 [math.ST] 24 Jun 2019

June 25, 2019

Abstract
We investigate the geometry of the maximal a posteriori (MAP) partition in the Bayesian
Mixture Model where the component and the base distributions are chosen from conjugate
exponential families. We prove that in this case the clusters are separated by the contour
lines of a linear functional of the sufficient statistic. As a particular example, we describe
Bayesian Mixture of Normals with Normal-inverse-Wishart prior on the component mean
and covariance, in which the clusters in any MAP partition are separated by a quadratic
surface. In connection with results of Rajkowski [2018], where the linear separability of
clusters in the Bayesian Mixture Model with a fixed component covariance matrix was
proved, it gives a nice Bayesian analogue of the geometric properties of Fisher Discriminant
Analysis (LDA and QDA).
Keywords: Bayesian Mixture Models, Maximal a Posteriori Partition

1

Introduction

In the standard setting of Bayesian Mixture Models we assume that the target distribution
is a random mixture of distributions from some parametrized family. We assume that the
probabilities of components are sampled from a (perhaps infinitely dimensional) simplex
and the parameters of the component distribution are sampled independently for each component. A prominent example is the Dirichlet Process Mixture Model (Antoniak [1974]),
where the prior distribution on the probability weights is Sethuraman’s stick breaking process (Sethuraman [1994]).
A popular choice of the component distribution is multivariate Normal, which gives Normal Bayesian Mixture Model. There are two standard conjugate prior distributions on the
component mean and covariance matrix (Gelman et al. [2013], Chapter 3.6): Normal distribution on the mean with fixed component covariance matrix or Normal-inverse-Wishart
distribution (here the terminology is adopted from Murphy [2007]), where the component
covariance matrix follows the inverse-Wishart distribution and the component mean (conditioned on the component covariance matrix) is Normal. The exact specification of these
priors is given in Section 2.4.
Bayesian Mixture Models give a basis for cluster analysis. Indeed, one can translate the
distribution on component probabilities into a discrete prior distribution on the possible
data partitions – just like Sethuraman’s construction translates in to the Generalised Pólya
∗ Faculty
† Faculty

of Mathematics, Informatics and Mechanics, University of Warsaw
of Mathematics, Informatics and Mechanics, University of Warsaw

1

Urn Scheme (Blackwell et al. [1973]), also known as the Chinese Restaurant Process (Aldous
[1985]). The inference about clusters is based on the posterior distribution on the space
of partitions. Analysing partition which maximises the posterior probability (the MAP
partition) seems to be a natural choice.
In Rajkowski [2018] it is proved that in the Normal Bayesian Mixture Model, when the
component covariance matrix is fixed and the prior on the component mean is Normal,
the MAP partition is convex, i.e. the convex hulls of clusters are disjoint. An equivalent
formulation is: for every two clusters in the MAP there exists a hyperplane that separates
them.
Placing an inverse-Wishart prior on the cluster covariance structure, with covariances for
different clusters, independent of each other, gives better modelling possibilities, since it is
unusual for the covariance to be known a priori and the same for different clusters. It would
be of interest to characterise the boundaries of the MAP partition in this case. Since cluster
covariance structures are no longer fixed, we might expect quadratic boundaries, analogously
to the Fisher’s Quadratic Discriminant Analysis (Friedman et al. [2001]).
A natural generalisation of this hypothesis is to establish the separability result in the case
when the base and the component distributions in the Bayesian Mixture Model form a
conjugate exponential family. One may expect that the separability can be described in
terms of the sufficient statistics. This is indeed what happens and it is the goal of the article
to prove this.

2

Formal statement of the result

Naming conventions and notation. In order to facilitate the readership, we introduce
the following naming convention. To make a distinction between real numbers and vectors
or matrices, we denote the latter in bold, i.e. x, θ, Σ. We do the same to distinguish between
real-valued, and vector-valued functions. We use an upright bold font do denote a sequence
of vectors, e.g. x = (x1 , . . . , xn ). In such case, when I is a subset of [n] := {1, . . . , n}, we
define xI := (xi )i∈I .

2.1

Bayesian Mixture Models and the MAP partition

Let Θ ⊂ Rp be the parameter space and {gθ : θ ∈ Θ} be a family of probability densities on the observation space Rd . Consider a prior distribution on Θ given by its density π. P
Let P be a probability distribution on the m-dimensional simplex ∆m = {p =
m
(pi )i=1 : m
i=1 pi = 1 and pi ≥ 0 for i ≤ m} (where m ∈ N ∪ {∞}). Let
p = (pi )m
i=1

∼

θ = (θi )m
i=1

iid

x = (x1 , . . . , xn ) | p, θ

iid

∼

∼

P
π
Pm

i=1

(2.1)
pi gθi .

This is a Bayesian Mixture Model. It can model possible clusters within data; they are
defined by deciding which gθi generated a given data point. In order to formally define the
clusters, we need to rewrite (2.1) as
p = (pi )m
i=1

∼

θ = (θi )m
i=1

iid

φ = (φ1 , . . . , φn ) | p, θ
xi | p, θ, φ

iid

∼

∼
∼

P
π
Pm

i=1

gφi

(2.2)
pi δθi
independently for all i ≤ n.

Then the clusters are the classes of abstraction of the equivalence relation i ∼ j ≡ φi = φj .
In this way the distribution on m dimensional simplex generates a probability distribution

2

on the partitions of set [n] into at most m subsets. According to Pitman [2002] this leads
to an exchangeable partition, i.e. a random partition whose probability function is invariant
with respect to permutations of indices.
Definition 2.1. We say that Π is an exchangeable random partition of [n] if for every
partition I of [n] and permutation σ : [n] → [n],



(2.3)
P(Π = I) = P Π = {σ(i) : i ∈ I} : I ∈ I .
In order to indicate that Π is an exchangeable random partition of [n] we use a generic
notation Π ∼ ERPn . Moreover we use the notation pn (I) := P(Π = I).
iid

For θ ∼ π, k ∈ N and u = (u1 , . . . , uk ) | θ ∼ gθ let fk be the resulting marginal distribution
on u, i.e.
Z
k
Y
fk (u1 , . . . , uk ) :=
π(θ)
gθ (ui )dθ.
(2.4)
Θ

i=1

Let ERPn be the exchangeable probability distribution on the space of partitions generated
by P. We see that (2.1) is equivalent to
I
xI := (xi )i∈I | I

∼
∼

ERPn
f|I| independently for all I ∈ I.

(2.5)

We stress the fact that the independent sampling on the ‘lower’ level of (2.5) relates to the
independence between clusters (conditioned on the random partition); within one cluster
the observations are (marginally) dependent. To make the notation more concise we define
Y
f (x | I) :=
f|I| (xI ).
(2.6)
I∈I

Then (2.5) becomes
I
x|I

∼
∼

ERPn
f (· | I).

(2.7)

Example 2.2. Consider the Dirichlet Process Mixture Model (Antoniak [1974]). Let α > 0,
G0 be a probability measure on Θ with density π and DP (α, G0 ) be the Dirichlet Process
on Θ (Ferguson [1973]). The Dirichlet Process Mixture Model is defined by
G

∼

φ = (φ1 , . . . , φn ) | G
xi | G, φ

iid

∼
∼

DP (α, G0 )
G
gφi

(2.8)
independently for all i ≤ n.

Q
iid
Let V1 , V2 , . . . ∼ Beta(1, α), p1 = V1 , pk = Vk k−1
i=1 (1 − Vi ) for k > 1. By Sethuraman
[1994] by setting P to be the distribution of p = (p1 , p2 , . . .) we get that (2.8) is equivalent
to (2.1). The exchangeable random partition that P generates is the Generalized Polya Urn
Scheme (Blackwell et al. [1973]) or the Chinese Restaurant Process (Aldous [1985]) with the
probability weight given by
α|I| Y
(|I| − 1)!,
(2.9)
pn (I) = (n)
α
I∈I
where α(n) = α(α + 1) . . . (α + n − 1). Again, setting ERPn to be the Chinese Restaurant
Process with parameter α we get the equivalence between (2.8) and (2.7).
Definition 2.3. Let x = (x1 , . . . , xn ) ∈ (Rd )n . We say that a partition Î of [n] is a MAP
of x if for any other partition I of [n]:
pn (I)f (xI | I) ≤ pn (Î)f (xI | Î).

3

(2.10)

Notation. Here and below, argmaxa∈A φ(a) is the set of maximisers of function φ on the
set A (note that the maximiser may not be unique). Hence the MAP partition of x in a
Bayesian Mixture Model (2.7) can be defined by
Î ∈

pn (I)f (xI | I).

argmax

(2.11)

partitions I of [n]

or, equivalently, using (2.6)
Î ∈



argmax

ln pn (I) +

partitions I of [n]

X


ln f|I| (xI ) .

(2.12)

I∈I

2.2 Specification of the Exponential Family Bayesian Mixture
Models
In the paper we consider Exponential Family Bayesian Mixture Models in which the base
and the component distributions come from a conjugate exponential family.
Let X ⊆ Rd be the observation space and let Θ ⊆ Rp be the parameter space, which is an
open subset of Rp . Let T : X → Rp be a statistic, let h : X → R be a function with the
support of positive Lebesgue measure and let η : Θ → Rp . Let B : Θ → R. Suppose that
the family of component densities is given by
n
o
(2.13)
gθ (x) = h(x) · exp T (x)> η(θ) − B(θ) .
Let B(θ) = a> B(θ) where a ∈ Rq and B(θ) = [B1 (θ), . . . , Bq (θ)]> . Let H : Θ → R and let


 

Z
χ
Ω := (χ, τ ) ∈ Rp × Rq :
H(θ) · exp [η(θ)> , B(θ)> ]
dθ < ∞ .
(2.14)
τ
Θ
be a nonempty hyperparameter space. We define A : Rp × Rq → R as
Z

  
χ
>
>
A(χ, τ ) := ln
H(θ) · exp [η(θ) , B(θ) ]
dθ
τ
Θ

(2.15)

Definition. Canonical Exponential Family Bayesian Mixture Model is a Bayesian Mixture
Model in which the component density is given by (2.13) and the base density is

 

χ
π(θ) = πχ,τ (θ) = H(θ) · exp [η(θ)> , −B(θ)> ]
− A(χ, τ )
(2.16)
τ
for some (χ, τ ) ∈ Ω.
Let θ ∼ πχ,κ and u = (u1 , . . . , uk ) | θ ∼ gθ then it follows from the multiplication
rule
P
that (χu , τk ) ∈ Ω almost surely and θ | u ∼ πχu ,τk , where χu := χ + ki=1 T (ui ) and
τk := τ + ka. As the marginal density of u is the quotient of the joint density of (θ, u) and
the conditional density of θ | u, we get that
u ∼ fk (u) :=

k
Y

h(ui ) · exp {A(χu , τk ) − A(χ, τ )}

(2.17)

i=1

2.3

Main result

We now define what we mean by T -linear separation of clusters.
Definition 2.4. Let X be a family of subsets of Rd and L a family of real functions
on Rd . We say that X is separated by L if for every X, Y ∈ X , X 6= Y , there exist
LX,Y ∈ L such that LX,Y (x) ≥ 0 and LX,Y (y) < 0 for all x ∈ X, y ∈ Y . Moreover,
if L = {a> T (x) + b : a ∈ Rs , b ∈ R} for some function T : Rd → Rs , we say that X is
T -linearly separated from Y. If T (x) = x, we use the term linear separability for short.

4

(a) This family is linearly
separable.

(b)
This
family
is
quadratically, but not
linearly separable.

(c) This family is not
quadratically separable.

Figure 1: Illustration of the different types of separability.
Notation. For the notational convenience we will use the separability notions also with
respect to the sets of sequences in Rd . For example, if x1 , . . . , xn ∈ Rd and I, J are disjoint
subsets of [n] then the expression xI is linearly separated from xJ means that {xi : i ∈ I} is
linearly separated from {xj : j ∈ J}.
In Rajkowski [2018] it is proved that in the Normal Bayesian Mixture Model with Normal
distribution on the component mean and fixed covariance matrix, when the prior on the
space of partitions is the Chinese Restaurant Process, the convex hulls of the clusters in
the MAP partition are disjoint. Equivalently, the MAP is linearly separable. The following
theorem is a generalisation of this result to the case of arbitrary Exponential Family Bayesian
Mixture Model.
Theorem 2.5. Let x1 , . . . , xn ∈ Rd be pairwise distinct and let Î be the MAP partition
of x1 , . . . , xn in the Bayesian Mixture Model where the prior on component parameters is
given by (2.16). Then the family {xI : I ∈ Î} is T -linearly separable.
Proof. The proof is left for Section 4.

2.4

Example: Normal Bayesian Mixture Models

As a commonly used in practice example, we consider Normal Bayesian Mixture Models
in which the component distributions are multivariate Normal, so X = Rd . The mean is
uknown, but the covariance matrix may be treated as uknown, known or known up to a
scaling factor.
Notation. We use two standard notations to denote the determinant of a square matrix Λ:
det Λ and |Λ|. The latter may seem ambiguous as we also use the symbol | · | to denote the
cardinality of a set. However, the meaning of this symbol is always clear from the context.
Notation. To keep the notation precise, in the following we introduce the following convention: if Σ is a symmetric d × d matrix, then diag(Σ) is the diagonal of Σ, treated as
d-dimensional vector, and low(Σ)
is the ‘lower
triangular’ part of Σ, treated as a d(d−1)
2



dimensional vector, whose
Σ, where i > j.

2.4.1

(i−1)(i−2)
2

+ j -th coordinate is equal to (i, j)-th coefficient of

Normal-inverse-Wishart

In this case both the mean and the covariance matrix are unknown. The parameter space
d
d
is therefore equal to Θ = Rd × S+
, where S+
is the space of all positive definite, d × d

5

matrices. This can be naturally interpreted as an open subset of Rp , where p =
For θ = (µ, Λ) ∈ Θ the component distribution is x | θ ∼ N (µ, Λ) and
Λ
µ|Λ

∼
∼

d(d−1)
2

W −1 (ν0 + d + 1, ν0 Σ0 )
N (µ0 , Λ/κ0 ),

+ d.

(2.18)

where W −1 is the inverse-Wishart distribution. Here the hyperparameters are κ0 , ν0 > 0,
µ0 ∈ Rd and Σ0 ∈ S + . This model is listed in Gelman et al. [2013] with a slightly different
hyperparameters, but we made this modification to obtain
E (V(x | µ, Λ))
V(E (x | µ, Λ))

= E Λ = Σ0 ,
= V(µ) = E V(µ | Λ) + VE (µ | Λ) = E Λ/κ0 + V(µ0 ) = Σ0 /κ0 ,
(2.19)
which is consistent with the remaining two priors.
The conditional densities are given by

x | µ, Λ ∼ (2π)−d/2 |Λ|−1/2 exp − 12 (x − µ)> Λ−1 (x − µ)

d/2
µ | Λ ∼ (2π)−d/2 κ0 |Λ|−1/2 exp − 12 κ0 (µ − µ0 )> Λ−1 (µ − µ0 )
ν
+d+1
 0


−1
ν0 +2d+2
2
|ν0 Σ2 |
2
exp − 12 ν0 tr(Σ0 Λ−1 )
Λ ∼
Γd ν0 +d+1
|Λ|−
2
2d
(2.20)
The density of x | θ can be expressed as (2.13) by placing



 1
diag(Λ−1 )
− 2 diag(xx> )
−d/2
−1
>
h(x) = (2π)
, T (x) =  − low(xx )  , η(θ) =  low(Λ )  ,
Λ−1 µ
x
1
1
B(θ) = ln |Λ| + µt Λ−1 µ
2
2

(2.21)

We get (2.16) by placing a = [1, 1]> ,


1
2


ln |Λ|
B(θ) = 1 t −1 ,
µΛ µ
2




ν0 + 2d + 3
τ =
,
κ0


− 12 diag(ν0 Σ0 + µ0 µ0> )
>
χ =  −low(ν0 Σ0 + µ0 µ0 ) 
κ0 µ0


(2.22)

and
H(θ) = (2π)−d/2 ,

d
ν0 + d + 1 |ν0 Σ0 |
A(χ, τ ) = − ln κ0 −
ln
+ ln Γd
2
2
2d




ν0 + d + 1
.
2
(2.23)

Corollary 2.6. Let x1 , . . . , xn ∈ Rd be pairwise distinct and let Î be the MAP partition of
x1 , . . . , xn in the Normal Bayesian Mixture Model where the prior on component parameters
is given by (2.18). Then the family {xI : I ∈ Î} is quadratically separable, i.e. every two
clusters are separated by a quadratic surface.
Proof. It follows from Theorem 2.5 and the formula for the sufficient statistic T in this
model, (2.21).

2.4.2

Normal (fixed covariance)

Here the component covariance matrix is assumed to be known a priori; the component
mean is unknown and this is the parameter on which the prior distribution is set, i.e.
θ = µ, Θ = Rd and x | µ ∼ N (µ, Σ0 ). The prior is
µ

∼

N (µ0 , Ψ0 )

6

(2.24)

The hyperparameters are µ0 ∈ Rd and Ψ0 , Σ0 ∈ S + . This prior is also listed in Gelman et
al. [2013]. Clearly
E (V(x | µ)) = Σ0 ,

V(E (x | µ)) = V(µ) = Ψ0 .

(2.25)

The conditional densities are given by
x|µ
µ

∼
∼


(2π)−d/2 |Σ0 |−1/2 exp − 21 (x − µ)> Σ−1
0 (x − µ)
(2π)−d/2 |Ψ0 |−1/2 exp − 21 (µ − µ0 )> Ψ−1
0 (µ − µ0 )

The density of x | θ can be expressed as (2.13) by placing


1 > −1
−d/2
−1/2
h(x) = (2π)
|Σ0 |
exp − x Σ0 x , T (x) = Σ−1
η(θ) = µ,
0 x,
2
1
B(θ) = µ> Σ−1
0 µ
2


diag(Σ−1
0 )
We get (2.16) by placing a =
−1
low(Σ0 )

diag(µµ> )
,
>
low(µµ )

1
B(θ) =



2

τ =


diag(Ψ−1
0 )
,
−1
low(Ψ0 )

and
H(θ) = (2π)−d/2 ,

A(χ, τ ) =

χ = Ψ−1
0 µ0

1
1
ln |Ψ0 | + µ0> Ψ0 µ0 .
2
2

(2.26)

(2.27)

(2.28)

(2.29)

Corollary 2.7. Let x1 , . . . , xn ∈ Rd be pairwise distinct and let Î be the MAP partition of
x1 , . . . , xn in the Normal Bayesian Mixture Model where the prior on component parameters
is given by (2.18). Then the family {xI : I ∈ Î} is linearly separable, i.e. every two clusters
are separated by a hyperplane.
Proof. It follows from Theorem 2.5 and the formula for the sufficient statistic T in this
model, (2.21) and the fact that Σ0 is an invertible matrix.

2.4.3

Normal-inverse-Gamma

In Normal-inverse-Gamma model we assume that the base covariance matrix and the component covariance matrix are known up to some scaling factor λ ∼ G −1 (β0 + 1, β0 ). Hence
the parameter is θ = (µ, λ), the parameter space is Θ = Rd × R ≡ Rd+1 and
λ
µ|λ
x | µ, λ

∼
∼
∼

G −1 (β0 + 1, β0 )
N (µ0 , λΨ0 )
N (µ, λΣ0 )

(2.30)

Here the hyperparameters are β0 > 0, µ0 ∈ Rd and Ψ0 , Σ0 ∈ S + . With this prior
E (V(x | µ, λ))
V(E (x | µ, λ))

= E λΣ0 = Σ0 ,
= V(µ) = E V(µ | λ) + VE (µ | λ) = E λΨ0 + V(µ0 ) = Ψ0 .

(2.31)

The conditional densities are given by
x | µ, λ
µ|λ
λ

∼
∼
∼

 1
(x − µ)> Σ−1
(2π)−d/2 |Σ0 |−1/2 λ−d/2 exp − 2λ
0 (x − µ)
−d/2
−1/2 −d/2
1
(2π)
|Ψ0 |
λ
exp − 2λ (µ − µ0 )> Ψ−1
0 (µ − µ0 )
β0β0 +1 Γ(β0 )−1 λ−(β0 +2) exp {−β0 /λ}

7

(2.32)

The density of x | θ can be expressed as (2.13) by placing
 1 > −1 
− 2 x Σ0 x
h(x) ≡ (2π)−d/2 |Σ0 |−1/2 , T (x) =
,
Σ−1
0 x


η(θ) =


1/λ
,
µ/λ

(2.33)

1
d
B(θ) = ln λ + µ> Σ−1
0 µ/λ
2
2


d/2
−1
We get (2.16) by placing a = diag(Σ0 )
low(Σ−1
0 )

ln λ
B(θ) =  21 diag(µµ> )/λ ,
low(µµ> )/λ





β0 + 2

τ = diag(Ψ−1
0 ) ,
−1
low(Ψ0 )


χ=

−β0
Ψ−1
0 µ0


(2.34)

and

1
1
ln |Ψ0 | + µ0> Ψ0 µ0
(2.35)
2
2
Corollary 2.8. Let x1 , . . . , xn ∈ Rd be pairwise distinct and let Î be the MAP partition of
x1 , . . . , xn in the Normal Bayesian Mixture Model where the prior on component parameters
is given by (2.18). Then the family {xI : I ∈ Î} is elliptically separable, i.e. every two
clusters are separated by a multidimensional ellipse.
H(θ) = (2πλ)−d/2 ,

A(χ, τ ) =

Proof. It follows from Theorem 2.5 and the formula for the sufficient statistic T in this
model, (2.21).

2.4.4

Comparison of the models

If we assume that the component parameters in the Normal Bayesian Mixture Model are
distributed by (2.24) then we assume that the covariance matrix in each component is equal
to Σ0 which is known to us. The results of Rajkowski [2018] imply that the misspecification
of this hyperparameter may lead to serious inference issues regarding the number of clusters,
at least as far as the MAP partition is concerned. In the light of these findings, (2.18)
seems to be a safer choice of the prior for the component parameters. In this case the
covariance matrix is chosen independently for each component according to the inverseWishart distribution. Note that although the Normal-inverse-Wishart prior gives more
flexibility in terms of the component covariances, it imposes some modelling restriction,
namely the expected within and between group covariance matrices are proportional, as is
shown by (2.19). This does not affect the fixed-covariance model, cf. (2.25).
This is the reason for which we propose the Normal-inverse-Gamma prior. It is not listed in
Gelman et al. [2013] and we were not able to find any reference to it in the literature. It only
allows a 1-parameter variation of the covariance function, but no restrictions are imposed
on the within-group means, unlike the Normal-inverse-Wishart prior. At the same time, by
allowing the component covariance matrix to scale between clusters can be a remedy to the
drawbacks of fixed covariance prior that were pointed out in Rajkowski [2018].
As a final point we note that Normal-inverse-Gamma prior is a generalisation of the Normal
prior in the sense that (2.30) becomes (2.24) as β0 → ∞. Analogously, Normal-inverseWishart prior is a quasi-extension of the Normal prior, since as η0 → ∞, (2.18) converges
to (2.24), but with Ψ0 = Σ0 /κ0 .

3

Discussion of potential applications

We proved a separability result concerning the MAP partition in the Exponential Family
Bayesian Mixture Models. In particular, we proved linear or quadratic separability of the

8

MAP partition in most popular Normal Bayesian Mixture Models. Apart from an aesthetic
analogy to the properties of Fisher Discriminant Analysis, the benefits of such result may
be twofold.
In Rajkowski [2018] the linear separability of the MAP partition is crucial for establishing
the existence of ‘limits’ of the MAP partitions when the prior on partitions is the Chinese
Restaurant Process and the data is independently and identically distributed with some
‘input distribution’. The limit is related to the partitions of observation space which maximises a given functional ∆ (which depends only on the hypeerparameter Σ0 and the input
distribution). The linear separability is important for two reasons: firstly, it is possible to
consider the limits of sequences of convex sets and secondly: it is possible to apply the
Uniform Law of Large Numbers for the family of convex sets. Theorem 2.5 should enable
an analogous approach for the Normal-inverse-Wishart and Normal-inverse-Gamma priors
on the component parameters.
The other kind of application is more practical; Theorem 2.5 shows that the search for an
MAP partition may be restricted to situations where clusters are quadratically separated.
The space of such partitions is still far too large for an exhaustive search, but may help in
finding a partition whose score approximates the MAP score.

4
4.1

Proofs
Proof of Theorem 2.5

Lemma 4.1. Let L be a family of real functions on Rd . Let x1 , . . . , xn ∈ Rd and let Iˆ be
the MAP partition for x1 , . . . , xn in some Bayesian Mixture Model, given by (2.5). If for
any U ⊂ [n], k, l ∈ N such that k + l = |U| and Iˆk,U such that

Iˆk,U ∈ argmax ln fk (xI ) + ln fl (xU \I )
(4.1)
I⊂U :|I|=k

observations xIˆk,U and xU \Iˆk,U are separated by L then {xI : I ∈ Î} is separated by L.
Proof. Firstly note that by (2.11)
Î ∈

argmax



ln pn (I) +

partitions I of [n]

X


ln f|I| (xI ) .

(4.2)

I∈I

Assume that the assumptions of Lemma 4.1 hold. Suppose that Î is not separated by L.
ˆ Jˆ ∈ Î such that x ˆ and x ˆ are not separated by L. Let U = Iˆ ∪ Jˆ
Then there exist I,
I
J
ˆ
˜ Moreover let Ĩ be a partition of [n] obtained
and k = |I|. Let I˜ = Ik,U and J˜ = U \ I.
ˆ Jˆ by I,
˜ J,
˜ i.e. Ĩ = Î \ {I,
ˆ J}
ˆ ∪ {I,
˜ J}.
˜ Note that pn (Î) = pn (Ĩ) (we have
by replacing I,
ˆ = |I|
˜ and |J|
ˆ = |J|,
˜ so we use the exchangeability of Π). Moreover x ˆ and x ˆ are not
|I|
I
J
separated by L so by the assumptions of Lemma 4.1

Iˆ ∈
/ argmax ln fk (xI ) + ln fl (xU \I )
(4.3)
I⊂U :|I|=k

and hence
ln fk (xI˜) + ln fl (xJ˜) > ln fk (xIˆ) + ln fl (xJˆ).

(4.4)

This means that
ln pn (Ĩ) +

X

ln f|I| (xI ) > ln pn (Î) +

I∈Ĩ

X
I∈Î

which contradicts the definition of Î and the proof follows.

9

ln f|I| (xI ),

(4.5)

Lemma 4.2. Let V ⊆ RD be a convex set.
P Let α : V → R be a strictly concave function,
z1 , . . . , zk+l ∈ RD are pairwise distinct. If i∈I zi ∈ V for every I ⊆ [k + l] such that |I| = k
and
X 
Jˆ ∈ argmin α
(4.6)
zi
I⊂[k+l] : |I|=k

i∈I

then zJˆ and z[k+l]\Jˆ are linearly separable.
P
Proof. Consider the set of all possible sums of k distinct vectors zi , i.e. Sk = { i∈I zi : I ⊂
[n], |I| = k} and let ŝk ∈ argmins∈Sk α(s). Since α is strictly concave, then ŝk is a vertex
of conv Sk . This means that there exist a vector v0 ∈ Rd such that ŝk ∈ argmaxs∈Sk hs, v0 i
(cf. Moszynska [2006], Corollary 3.3.6), where h·, ·i is a standard Euclidean scalar product.
I can also choose v0 so that hzi , v0 i are are all different (because we are dealing with a
discrete set). Let z(1) , . . . , z(k+l) be a decreasing ordering of vectors zi ‘in the direction v0 ’,
i.e. {z(1) , . . . , z(k+l) } = {z1 , . . . , zk+l } and hz(i) , v0 i > hz(j) , v0 i if i < j. Note that
DX
E X
zi , v 0 =
hzi , v0 i
(4.7)
i∈I

i∈I

ˆ and {zi : i ∈
ˆ are linearly
and therefore Iˆ = {z(1) , . . . , z(k) }. Thus the sets {zi : i ∈ I}
/ I}
D
separated by the hyperplane {u ∈ R : hu, v0 i = hz(k) + z(k+1) , v0 i/2}.
Remark 4.3. Let us assume the notation of Section 2.2. Then the set Ω is a convex set
and the function A : Ω → R is strictly convex.
Proof. It is a well known property of exponential families in canonical form (cf. Diaconis
and Ylvisaker [1979]).
Proof of Theorem 2.5. Let U ⊆ [n], k, l ∈ N and Iˆk,U be as in Lemma 4.1. Plugging the
formula (2.17) into (4.1) gives:
X
Iˆk,U = argmax
ln h(xi ) + A(χxI , τk ) − A(χ, τ )+
I⊂U : |I|=k

+

i∈I

X


ln h(xi ) + A(χxU \I , τl ) − A(χ, τ ) =

i∈U \I

= argmax
I⊂U : |I|=k

X


ln h(xi ) + A(χxI , τk ) + A(χxU \I , τl ) − 2A(χ, τ ) =

(4.8)

i∈U



= argmax A(χxI , τk ) + A(χxU \I , τl )
I⊂U : |I|=k





P
Let ti = T (xi ), a and let t0 = χ, τ and tU = i∈U ti . By Remark 4.3, A is a strictly
convex function. Hence the functions α(t) = A(t0 + t) and α(t) = A(t0 + tU − t) are also
strictly convex and so is their sum, α(t) = α(t) + α(t). By (4.8) we get
X 
Iˆk,U = argmax α
ti
(4.9)
I⊂U : |I|=k

i∈I

Therefore by Lemma 4.2 we obtain that tIˆk,U and tU \Iˆk,U are linearly separable (i.e. in terms
of the base functions). Obviously this yields T -linear separability of xIˆk,U and xU \Iˆk,U and
the proof follows.

10

References
David J Aldous. Exchangeability and related topics. In École d’Été de Probabilités de
Saint-Flour XIII—1983, pages 1–198. Springer, 1985.
Charles E Antoniak. Mixtures of dirichlet processes with applications to bayesian nonparametric problems. The annals of statistics, pages 1152–1174, 1974.
David Blackwell, James B MacQueen, et al. Ferguson distributions via pólya urn schemes.
The annals of statistics, 1(2):353–355, 1973.
Persi Diaconis and Donald Ylvisaker. Conjugate priors for exponential families. The Annals
of statistics, pages 269–281, 1979.
Thomas S Ferguson. A bayesian analysis of some nonparametric problems. The annals of
statistics, pages 209–230, 1973.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning,
volume 1. Springer series in statistics New York, NY, USA:, 2001.
Andrew Gelman, Hal S Stern, John B Carlin, David B Dunson, Aki Vehtari, and Donald B
Rubin. Bayesian data analysis. Chapman and Hall/CRC, 2013.
Maria Moszynska. Selected topics in convex geometry. Springer, 2006.
Kevin P Murphy. Conjugate bayesian analysis of the gaussian distribution. def, 1(2σ2):16,
2007.
Jim Pitman. Combinatorial stochastic processes. Technical report, Technical Report 621,
Dept. Statistics, UC Berkeley, 2002. Lecture notes for St. Flour course, 2002.
Lukasz Rajkowski. Analysis of the maximal a posteriori partition in the Gaussian Dirichlet
Process Mixture Model. Bayesian Analysis, 2018.
Jayaram Sethuraman. A constructive definition of Dirichlet priors. Statistica sinica, pages
639–650, 1994.

11

