HodgeNet: Graph Neural Networks for Edge Data

arXiv:1912.02354v1 [eess.SP] 5 Dec 2019

T. Mitchell Roddenberry and Santiago Segarra

Abstract—Networks and network processes have emerged as
powerful tools for modeling social interactions, disease propagation, and a variety of additional dynamics driven by relational
structures. Recently, neural networks have been generalized to
process data on graphs, thus being able to learn from the aforementioned network processes achieving cutting-edge performance
in traditional tasks such as node classification and link prediction.
However, these methods have all been formulated in a way suited
only to data on the nodes of a graph. The application of these
techniques to data supported on the edges of a graph, namely flow
signals, has not been explored in detail. To bridge this gap, we
propose the use of the so-called Hodge Laplacian combined with
graph neural network architectures for the analysis of flow data.
Specifically, we apply two graph neural network architectures to
solve the problems of flow interpolation and source localization.

I. I NTRODUCTION
As relational, structured data becomes increasingly ubiquitous, network science and graph analytics grow in importance
for modeling data in many domains [1]–[5]. Recently, deep
learning techniques have been generalized to data supported
on the nodes of graphs, enabling cutting-edge results in node
classification, graph classification, and link prediction [6]. The
main theme of these approaches is the application of a graph
shift operator that generalizes the time delay or shift operator
in classical signal processing. By learning polynomials of
the graph shift operator coupled with nonlinear activation
functions, neural networks can be generalized to the graph
domain.
In many relevant settings, the data of interest is supported
most naturally on the edges of a graph, such as a flow
– modeling the transfer of mass, energy, or information –
through a network. Recently, Hodge theory has been applied
to higher-order graph structures (represented by simplicial
complexes) on which such data is supported. In this context,
the spectral properties of the Hodge Laplacian have been
leveraged to solve the problems of flow denoising [7], flow
interpolation [8], and higher-order network topology inference [9]. In the current paper, we propose to combine these
advances in graph signal processing (GSP) for flow data along
with the recent successes of graph neural networks (GNNs) to
solve inverse problems on graph flow data, specifically flow
interpolation and source localization.
Related work. Several problems related to signals defined
on higher-order graph structures have recently been considered. Flow denoising through low-pass graph filters based on
Research was sponsored by the Army Research Office and was accomplished under Cooperative Agreement Number W911NF-19-2-0269. The
views and conclusions contained in this document are those of the authors and
should not be interpreted as representing the official policies, either expressed
or implied, of the Army Research Office or the U.S. Government. The U.S.
Government is authorized to reproduce and distribute reprints for Government
purposes notwithstanding any copyright notation herein. Both authors are
with the Department of Electrical and Computer Engineering, Rice University.
Emails: mitch@rice.edu, segarra@rice.edu

the Hodge Laplacian has been studied in [7]. Furthermore,
[8] proposed interpolation and sampling methods for flow
data under priors on the signal characteristics with respect to
the Hodge Laplacian, addressing the same flow interpolation
problem discussed in Section III-A. Most recently, [9] and
[10] employed these tools to tackle the problem of topology
inference from data on abstract simplicial complexes. Outside
of the context of signal processing, the Hodge Laplacian
for graphs has enabled novel data science approaches. These
include a spectral method for pairwise ranking in sports
analytics [11] and the formal characterization of random walks
in (higher-order) simplicial complexes [12].
Contributions. The primary contribution of this paper is the
first integration of discrete Hodge theory with deep learning
techniques by incorporating the Hodge Laplacian in GNN
architectures. Additionally, our methods provide a prior-free
approach to the problem of flow interpolation, in contrast
with [8]. Finally, we use the aggregation GNN architecture
introduced by [13] to solve a source localization problem for
flow data.
II. BACKGROUND
A. Graphs, graph signals, and graph signal processing
A graph is a data structure consisting of a set of nodes V
connected by a set of edges E ⊆ V×V, denoted by G = (V, E).
An undirected graph has an edge set consisting of unordered
tuples, i.e., (i, j) ∈ E ⇐⇒ (j, i) ∈ E. For convenience,
we will indicate the cardinality of the node and edge sets as
N := |V| and E := |E|, respectively. Graphs can be compactly
represented by their adjacency matrix A ∈ RN ×N , where for
some indexing of V with the integers {1, 2, . . . , N }, we have
Aij = 1 if (i, j) ∈ E and Aij = 0, otherwise. An alternative
representation is the incidence matrix B ∈ RN ×E , where for
the same indexing of V and a labeling of E with {ei }E
i=1 we
have


−1 ej = (i, k) for some k ∈ V,
Bij = 1
(1)
ej = (k, i) for some k ∈ V,

0
otherwise.
Notice that this assigns an inherent direction or orientation to
each edge ej . For undirected graphs, this choice is arbitrary
but does not affect the subsequent definitions of other relevant
graph matrices in a meaningful way.
While the inherent structure of a graph is often of interest,
GSP offers tools for the analysis of signals supported by
graphs [14], [15]. In the same way that a discrete-time signal
is a function from the integers to the real numbers, a graph
signal maps nodes to real numbers, x : V → R. For some node
labeling with the integers {1, 2, . . . , N }, a graph signal x can
be conveniently represented as a vector x ∈ RN . Extending

the analogy with discrete-time signal processing, a graph filter
g is defined as a polynomial of a graph shift operator S [16]
g?x=

K−1
X

g(k)S k x.

(2)

k=0

A common choice of graph shift operator is the graph
Laplacian, defined as
L0 = D − A,

(3)

where D = diag(A1) is the diagonal matrix of node degrees.
The eigenvectors of the graph Laplacian (ordered in terms
of increasing associated eigenvalues) provide an orthonormal
basis of frequencies that is specific to the graph at hand.
Consequently, the coefficients of a signal projected onto the
eigenbasis of the graph Laplacian can be interpreted as a
measurement of the smoothness of said signal in terms of
local uniformity. We refer to [14], [15] for a more complete
discussion of GSP basics.
B. Graph neural networks
GNNs use tools from GSP combined with traditional convolutional neural network architectures to process data on graphs.
Convolutional neural networks for images are typically composed of many layers, each consisting of a localized filtering
operation followed by a nonlinear activation function [17]. As
laid out in [18] and [19], these layers can be generalized to
graphs simply by replacing the convolution operation with a
graph filter as defined in (2). Moreover, one can restrict the
order K of the filter to constrain the filter to operate locally
on each node. By then applying a nonlinear function, we form
a graph convolutional layer. Architectures following this form
have achieved state of the art results in tasks such as node
classification, graph classification, and link prediction [18],
[19]. We defer discussions of our proposed architectures to
Section IV.
C. Flow signals and the Hodge Laplacian
So far, our discussion has considered signals on the nodes
of a graph, i.e., x : V → R. We define flow signals as signals
on the edges of a graph f : E → R that have the property of
skew-symmetry:
f (i, j) = −f (j, i), ∀(i, j) ∈ E.

(4)

Similar to graph signals, for some edge labeling with the
integers {1, 2, . . . , E} and a choice of edge orientations, a flow
signal f can be represented as a vector f ∈ RE . It is important
to note that the following discussion does not depend on the
chosen edge labeling and orientations.
As pointed out by [7], a tempting shift operator for flow
signals is the linegraph Laplacian, formed by treating each
edge in the original graph as a ‘node’ and treating incidence
between edges in the original graph as (unweighted) ‘edges.’
One would then consider the graph Laplacian matrix of this
construction, which we refer to as LLG , and apply standard
GSP methods. Similar to the graph Laplacian, the spectral
characteristics of LLG relate to the local uniformity (smoothness) of flow signals.

An alternative graph shift operator for flow signals is the
Hodge Laplacian, defined in terms of the graph incidence
matrix B [cf. (1)]
L1 = B > B.
(5)
In contrast with the smoothness measured by L0 and LLG , the
Hodge Laplacian captures conservatism of flow signals [9].
Conservative flows are referred to as cyclic since they are
naturally composed of cycles, or loops, in the graph. Nonconservative flows are referred to as gradient since they are
induced by differences in potentials on the nodes of the graph.
That is, for a purely gradient flow f , there is a vector of node
potentials Φ such that f = B > Φ. This distinction is formally
captured in the Hodge decomposition for graphs [7], [9].
Theorem 1 The space of edge-flows admits an orthogonal
decomposition into a cyclic component and a gradient component

RE = ker (L1 ) ⊕ Im B > .
(6)
In (6), ker (L1 ) is a basis for the space of cyclic edge flows,
and Im B > is a basis for the space of gradient edge flows.
A more general version of this decomposition exists for flow
signals on higher-order simplicial complexes, but we focus on
the case for simple graphs here.
III. P ROBLEM STATEMENT
To motivate our proposed methods, we introduce two problems in the analysis of flow data on graphs. The first is a
regression problem for the purpose of flow interpolation, while
the second is a graph-level signal classification problem.
A. Flow interpolation
A common task in image processing is inpainting, where
some pixels of an image are missing, and we wish to use
the surrounding image information to predict their values. We
specify an analogous problem for flow signals next.
Problem 1 (Flow interpolation) Given a graph G = (V, E)
and an observation of a flow signal f : E → R over a subset
Ω ⊆ E, infer the value of f over the unobserved edges E \ Ω.
For convenience, we indicate the flow over the observed set
as fΩ , and the flow over the unobserved set as fE\Ω . We refer
to the inferred flow as fb.
In the context of GSP, Problem 1 has already been addressed
in [8], where a flow-conservation prior is placed on f , and fb is
recovered via a convex optimization procedure that minimizes
the gradient component of the recovered flow. Our aim in
applying GNNs to this problem is to remove the dependence
on a prior, alleviating problems that arise when the model does
not match the true flow, as illustrated in Section V-A.
B. Source localization
In many scenarios, flow on a graph is mainly induced by one
or several sources. For instance, in an urban setting, the flow of
traffic in the afternoon would mostly be induced by the exodus
of cars from the city center to surrounding neighborhoods. In
the source localization task, we aim to recover the source of
an observed flow signal.

Problem 2 (Source localization) Given a graph G = (V, E)
with a known partition of V into communities C1 , C2 , . . . , Ck ,
as well as a flow signal f induced by an unknown source
node v ∈ V, recover the community 1 ≤ j ≤ k in which v is
contained, i.e., {j | v ∈ Cj }.
Problem 2 is a simpler version of the typical source localization problem where one aims to recover the node that a graph
signal originated from. In this coarser alternative, we seek to
infer the community in which the origin node is located.
IV. H ODGE N ET
For solving the flow interpolation and source localization
problems, we propose to apply GNN architectures using the
Hodge Laplacian as a shift operator. Although the following
architectures are distinct from one another, we loosely refer
to them under the umbrella term HodgeNet to indicate the use
of the Hodge Laplacian as a constituent element of the neural
networks.
A. Hodge RNN for flow interpolation
Inspired by the GNN introduced in [20], we propose a
recurrent neural architecture that receives a sequence of aggregated (partially observed) flow signals as input and outputs
a prediction of the flow at the unobserved edges. This sequence
is formed by repeatedly applying the Hodge Laplacian to the
masked flow signal, which updates a hidden state at each step,
from which the output is computed. More precisely, the input
xk ∈ RE , hidden state Hk ∈ RE×F (F is a parameter
indicating the dimension of the feature space), and output
ok ∈ RE at the kth step are given by
L1
xk−1 ,
λ1

Hk = σ xk u> + Hk−1 V ,
ok = σ (Hk w) ,
xk =

(7)
(8)
(9)

for trainable weights u, w ∈ RF , V ∈ RF ×F , and nonlinear
function σ(·) applied elementwise. We initialize the hidden
state as H0 = 0 and the initial input x0 = fΩ is the masked
flow signal with value 0 in the unobserved entries. In (7), the
Hodge Laplacian is normalized by its maximum eigenvalue λ1 .
We terminate this recursive process after some pre-specified
number of layers K.
Recalling the skew-symmetry property of flow signals (4),
we wish to maintain this architecture’s equivariance to the
arbitrary choice of edge orientations. That is, if we denote
the output of this architecture by oK (f , L1 ), for some input
flow f with associated Hodge Laplacian L1 , a diagonal “edgeflipping” matrix F ∈ RE×E with diagonal entries in {−1, 1}
should obey
oK (F f , F L1 F ) = F oK (f , L1 ) .

(10)

That is, if the orientation for an edge is flipped, the output of
the Hodge RNN using the reoriented Hodge Laplacian should
take the same value at that edge, except with opposite sign,
reflecting the skew-symmetry property of flows [cf. (4)]. This
is characterized by the following proposition.

Proposition 1 (Equivariance of Hodge RNN) The output
oK of the Hodge RNN described by (7)-(9) is equivariant to
the arbitrary choice of edge orientations as in (10) if and
only if σ(·) is an odd function.
See Appendix A for the proof. Guided by Proposition 1,
we select σ in (8) and (9) to be the (elementwise) softthresholding operator with a trainable threshold τ , i.e.,
σ(x; τ ) = sign(x)[|x| − τ ]+ . In this way, σ is odd while
maintaining an evident functional similarity to the popular
ReLU nonlinearity.
For the purpose of flow interpolation, the RNN architecture
is trained by artificially masking known flow signals and
minimizing the mean-squared error between the ground truth
and output flow over the masked edges. That is, for some
observation set Ω and a set of artificially masked edges Ψ ⊂ Ω,
we aim to find parameters u, V , w that minimize L, defined
as:
 2
1 
f − oK (fΩ\Ψ , L1 ) Ψ 2 ,
(11)
L=
|Ψ|
where we use xS ∈ R|S| to refer to the vector of elements of
x corresponding to the set S. This procedure can be applied
both to a large set of historical flow signals or to a single
partially observed flow signal by training on multiple artificial
masks Ψ applied to the observed edges.
B. Hodge aggregation GNN for source localization
For the purpose of graph-level signal classification, we adapt
the aggregation GNN (AGNN) proposed by [13]. Similar to
the RNN approach, we repeatedly apply the Hodge Laplacian
normalized by its largest eigenvalue L1 /λ1 to a signal f ,
yielding a real-valued sequence observed at a fixed set of edges
in the graph. More specifically, for K observed edges indicated
by a fixed binary row-selection matrix C ∈ {0, 1}K×E ,
the aggregation sampling method yields the multi-channel
sequence G ∈ RK×E given by
"    
 E−1 #
2
L1
L1
L1
G = C f,
f,
f, . . . ,
f . (12)
λ1
λ1
λ1
We then apply a standard 1D-CNN with ReLU nonlinearities
to the sequence G [17]. Once the sequence has been reduced
to a sufficiently low dimension, it is fed into a fully-connected
layer followed by a softmax operation for classification. We
denominate the output of this architecture AGGθ (f , L1 ), for
input flow f with associated Hodge Laplacian L1 , and CNN
filter parameters collected in θ.
An analogous version of Proposition 1 for the Hodge AGNN
with ReLU nonlinearities can be stated as follows.
Proposition 2 (Invariance of Hodge AGNN) For a given
set of parameters θ, the output of the Hodge AGNN satisfies
the following:
i) AGGθ is invariant to the arbitrary choice of edge
orientations outside of the edges selected by C, i.e.,
AGGθ (F f , F L1 F ) = AGGθ (f , L1 ) if Fii = 1 for all edges
i selected by C.
ii) For an arbitrary set of CNN parameters θ and edge-flipping
matrix F , there exists a rotated set of CNN parameters θ0 such
that AGGθ (f , L1 ) = AGGθ0 (F f , F L1 F ) for all f ∈ RE .

B

10−2

1

Hodge RNN
Linegraph RNN
Optimization
Kriging

C

0.8
Accuracy

Hodge RNN
Linegraph RNN
Optimization
Kriging
MSE

MSE

A

10−2

0.6
0.4
Hodge Laplacian
Linegraph Laplacian
Graph Laplacian (Nodespace)

0.2

10−3

10−3
0

100
200
300
400
Number of training examples

500

0

100
200
300
400
Number of training examples

500

0

0

200

400

600

800

1,000

Epochs

Fig. 1. Performance of RNN architectures in flow interpolation task when trained on differently-sized synthetic sets of flow signals compared to kriging and
optimization procedures for (A) conservative flows and (B) smooth gradient flows. (C) Convergence of test accuracy of aggregation GNN in source localization
task for Hodge, linegraph, and nodespace shift operators. The shaded regions indicate the estimated standard deviation in performance over 10 training runs.

TABLE I
F LOW INTERPOLATION PERFORMANCE FOR THE RNN ARCHITECTURES ,
THE CONVEX OPTIMIZATION PROCEDURE , AND SPATIAL KRIGING .
Method
Hodge RNN
Linegraph RNN
ConvOpt
Kriging

PSNR (dB)
20.5
18.7
31.0
14.8

See Appendix B for the proof. The first part of Proposition 2
reveals that even if after training we flip the orientations of the
edges not selected by C then the output of the Hodge AGNN
remains invariant. More importantly, the second part of the
proposition states that the ultimate performance of the neural
network does not depend on the edge orientations chosen
before training, if the parameters θ are randomly initialized
according to a spherically symmetric distribution, e.g., an i.i.d.
multivariate gaussian distribution. Notice that this result is
expected and appealing, since the orientation of the edges
is arbitrary and an architecture whose performance depends
on the choice of these orientations would be undesirable in
practice.
V. N UMERICAL EXPERIMENTS
In this section, we demonstrate the proposed methods as
applied to the flow interpolation and source localization problems1 . To highlight the superior modeling capabilities of the
Hodge Laplacian for flow data, we compare the performance
of our proposed architectures to similar models using standard
graph shift operators, such as the linegraph Laplacian. That is,
we replace L1 with LLG in (7)-(9) and (12).
A. Flow interpolation
For the flow interpolation task, we apply the RNN architecture to the Anaheim traffic flow dataset [21]. The flow signal is
artificially masked to make 10% of edges unobserved, and the
rest of the edges in the graph are observed, i.e., |Ω| = 0.9E
[cf. Problem 1].
The RNN architecture is applied using both the Hodge
Laplacian as a shift operator and the linegraph Laplacian,
with the training performed on the observed set of edges as
1 Code

for these experiments can be found at github.com/tmrod/hodgenet

described in Section IV-A. Since the linegraph Laplacian does
not encode the orientation of flow signals, we apply it to
the absolute value of the flow signal, i.e., f is replaced by
|f |. These are compared to the convex optimization approach
proposed by [8], as well as a spatial kriging approach similar
to [22], where the graph is embedded in space using spectral
drawing [23], and the flow signals are interpolated via a
Gaussian process regression. Again, this does not capture flow
orientation, so we apply it to the absolute value of the flow
signal.
The results are shown in Table I. Clearly, the convex
optimization approach outperforms all methods, which is due
to the flow-conservation prior enforced by the method holding strongly. However, among the prior-free approaches, the
Hodge RNN model performs the best. This can be attributed
to the spectral properties of the Hodge Laplacian matching
intuitive properties of traffic flows (i.e., conservatism), as
opposed to the spectral properties of the linegraph Laplacian,
which capture smoothness in the sense of uniformity.
In the previous setting, we only had access to a single
partially observed flow signal over a graph. To understand how
the RNN for flow interpolation can improve with access to
historical flow data, we generate a synthetic dataset by adding
cyclic Gaussian noise (that is, a Gaussian random vector in
RE that has been projected into ker(L1 )) to the original
Anaheim flow signal, as well as a small amount of smooth
gradient noise. This procedure is repeated to yield multiple
flow signals and the RNN architectures are trained on these
datasets. The convex optimization and kriging baselines do
not learn from multiple flow signals, but we also evaluate
their performance on this data for the sake of comparison.
The results of this experiment for increasing training set size
is shown in Figure 1A. The behavior is as expected: The
optimization approach has a very accurate prior, and thus
outperforms the prior-free approaches. Additionally, both RNN
architectures improve with an increasing amount of training
data, with the Hodge RNN consistently outperforming the
model based on the linegraph Laplacian LLG .
To emphasize the versatility of the RNN approaches (due
to the lack of a strong prior), we generate a set of synthetic
(smooth) gradient flows using a procedure similar to the one
previously described. In this case, the convex optimization
procedure experiences a model mismatch since the flowconservation prior does not hold. This is illustrated in Fig-

ure 1B, where the optimization approach performs poorly
and both RNN approaches improve with more historical data.
Interestingly, kriging performs comparatively better in this
setting, which could be explained by a smooth gradient flow
corresponding to a set of smooth node potentials [9].
B. Source localization
For the source localization problem, we consider flows
induced by a smoothly varying vector of node potentials,
which is concentrated in a localized region of the graph. To
model this, the set of node potentials is obtained by applying
a diffusion graph process D(A) to an impulse signal at some
source node v. These node potentials then induce a flow via
the incidence matrix B. More precisely, the observed flow is
modeled by
f = B > D(A)δv + ,
(13)
where δv is a graph signal taking value 1 at node v and 0
elsewhere, and  is normally distributed additive noise. Notice
that this signal model is purely gradient, aside from the added
noise.
Following the experiments in [13], we consider a graph
drawn from a planted partition model with p = 0.8, q = 0.2,
and k = 5 communities of 20 nodes each. A set of 10000
training signals is then generated by randomly choosing a
diffusion time 1 ≤ ti ≤ 20 uniformly at random for each
signal indexed by i ∈ {1, 2, . . . , 10000}, then applying the
t
diffusion operator Dti (A) = (A/λ1 ) i to an impulse δvi
as in (13), where vi is randomly chosen among the nodes
of highest degree in each community. Here, λ1 denotes the
largest eigenvalue of A. The flow signal fi is then generated
as in (13). The signal is then categorically labeled according
to the community from which the impulse originated, e.g., if
vi ∈ Cj , then the label for fi is j [cf. Problem 2].
An AGNN is then trained on these signals, and tested on
a set of 2000 signals generated in the same way. Operating
directly in the edge space, L1 and LLG are used as shift
operators, once again using the absolute value of the flow
signal when operating with LLG . Additionally, we consider
an architecture that first determines the latent vector of node
potentials Φ = D(A)δv , and then uses the graph Laplacian
in the node space as an aggregation operator, similar to [13].
More precisely, we replace f by the estimated node potentials
b = (B > )† f and L1 by L0 in (12).
Φ
The performance of these architectures on the test set is
shown in Figure 1C. Among the three approaches, the Hodge
AGNN is superior in both accuracy and speed of convergence,
confirming our intuition that it favorably models flow signals.
VI. D ISCUSSION
In this work, we have considered the application of GNNs to
flow signals supported on the edges of a graph. To accomplish
this, we use the Hodge Laplacian as a shift operator due to
its favorable properties for modeling flows, which stem from
the spectral decomposition into bases for cyclic and gradient
flows. We then demonstrated that this operator outperforms
other graph shift operators in the tasks of flow interpolation
and source localization.
Our ultimate goal is to develop a global and rigorous
understanding of methods for processing and learning from

data on higher-order networks. As a first step towards this
objective, in this paper we kept a (non higher-order) graph as
our domain, but considered the case where the data of interest
is supported on the edges as opposed to the nodes of the
graph. The discussed use of the Hodge Laplacian in a neural
architecture opens up the possibility of applying GNNs to data
supported on arbitrarily high-order networks, allowing us to go
beyond systems modeled by strictly pairwise interactions.
R EFERENCES
[1] M. E. J. Newman, Networks: An Introduction. Oxford University Press,
USA, Mar. 2010.
[2] S. H. Strogatz, “Exploring complex networks,” Nature, vol. 410, no.
6825, pp. 268–276, Mar. 2001.
[3] M. O. Jackson, Social and Economic Networks. Princeton university
press, 2010.
[4] S. Segarra, M. Eisen, and A. Ribeiro, “Authorship attribution through
function word adjacency networks,” IEEE Trans. Signal Process.,
vol. 63, no. 20, pp. 5464–5478, Oct 2015.
[5] J. D. Medaglia, W. Huang, S. Segarra, C. Olm, J. Gee, M. Grossman,
A. Ribeiro, C. T. McMillan, and D. S. Bassett, “Brain network efficiency
is influenced by the pathologic source of corticobasal syndrome,”
Neurology, vol. 89, no. 13, pp. 1373–1381, 2017.
[6] M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst,
“Geometric deep learning: Going beyond euclidean data,” IEEE Signal
Process. Mag., vol. 34, pp. 18–42, Jul. 2017.
[7] M. T. Schaub and S. Segarra, “Flow smoothing and denoising: Graph
signal processing in the edge-space,” in IEEE Global Conf. Signal and
Info. Process. (GlobalSIP), Nov. 2018, pp. 735–739.
[8] J. Jia, M. Schaub, S. Segarra, and A. Benson, “Graph-based semisupervised & active learning for edge flows,” in Intl. Conf. Knowl. Disc.
Data Min. (KDD), Aug. 2019, pp. 761–771.
[9] S. Barbarossa and S. Sardellitti, “Topological signal processing over
simplicical complexes,” IEEE Trans. Signal Process. (submitted), 2019.
[10] S. B. S. Sardellitti and E. Ceci, “Learning from signals defined over
simplicial complexes,” Jun. 2018, pp. 51–55.
[11] X. Jiang, L.-H. Lim, Y. Yao, and Y. Ye, “Statistical ranking and
combinatorial Hodge theory,” Mathematical Programming, vol. 127,
no. 1, pp. 203–244, Mar. 2011.
[12] M. Schaub, A. Benson, P. Horn, G. Lippner, and A. Jadbabaie, “Random
walks on simplicial complexes and the normalized Hodge 1-Laplacian,”
SIAM Review (SIREV) (to appear), Jun. 2020.
[13] F. Gama, A. Marques, G. Leus, and A. Ribeiro, “Convolutional neural
network architectures for signals supported on graphs,” IEEE Trans.
Signal Process., vol. 67, no. 4, pp. 1034–1049, Feb. 2019.
[14] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst, “The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular
domains,” IEEE Signal Process. Mag., vol. 30, no. 3, pp. 83–98, Apr.
2013.
[15] A. Ortega, P. Frossard, J. Kovacevic, J. M. F. Moura, and P. Vandergheynst, “Graph signal processing: Overview, challenges, and applications,” Proc. IEEE, vol. 106, no. 5, pp. 808–828, May 2018.
[16] S. Segarra, A. G. Marques, and A. Ribeiro, “Optimal graph-filter design
and applications to distributed linear network operators,” IEEE Trans.
Signal Process., vol. 65, no. 15, pp. 4117–4131, Aug 2017.
[17] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press,
2016.
[18] M. Defferard, X. Bresson, and P. Vandergheynst, “Convolutional neural
networks on graphs with fast localized spectral filtering,” in Adv. Neural
Info. Process. Syst., Dec. 2016, pp. 3844–3852.
[19] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
convolutional networks,” in Intl. Conf. Learning Representations (ICLR),
Apr. 2017.
[20] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
“The graph neural network model,” IEEE Trans. Neural Netw., vol. 20,
no. 1, pp. 61–80, 2009.
[21] B. Stabler, H. Bar-Gera, and E. Sall, “Transportation networks for research core team,” https://github.com/bstabler/TransportationNetworks.
[22] X. Wang and K. Kockelman, “Forecasting network data: Spatial interpolation of traffic counts from Texas data,” Transportation Res. Record:
J. Transportation Res. Board, no. 2105, pp. 100–108, 2009.
[23] Y. Koren, “Drawing graphs by eigenvectors: Theory and practice,”
Computers & Mathematics with Applications, vol. 49, no. 11, pp. 1867–
1888, 2005.

A PPENDIX A
P ROOF OF P ROPOSITION 1
Let F be an edge-flipping matrix where
(
=0
i 6= j
Fij
.
∈ {−1, 1} i = j

(14)

Applying this matrix to a flow signal f with associated Hodge
Laplacian L1 yields f 0 = F f , L01 = F L1 F . Substituting
these into (7) yields
x00 = f 0 = F f = F x0
L1
L1
L0
F x00 = F
F F x0 = F x1
x01 = 1 x00 = F
λ1
λ1
λ1
..
.
0
xK = F xK ,

i
j

(15)

following from the fact that F F = I. That is, changing the
edge orientation of the original flow signal changes the edge
orientations of each element in the sequence of flow signals
{xi }K
i=0 .
Applying a similar argument to (8), we get that

Hk0 = σ F xk u> + Hk−1 V , 1 ≤ k ≤ K.
(16)
Clearly, if σ(·) is an odd function applied elementwise, (16)
can be rewritten as

Hk0 = F σ xk u> + Hk−1 V = F Hk , 1 ≤ k ≤ K. (17)
Applying this to (9) then yields
0
o0K = σ (HK
w) = σ (F HK w) = F σ (HK w) = F oK ,
(18)
proving the “if” part of Proposition 1. The “only if” part
follows from a similar argument applied in reverse.

A PPENDIX B
P ROOF OF P ROPOSITION 2
Again, let F be an edge-flipping matrix as described
by (14). Additionally, let C be the row-selection matrix for
the Hodge AGNN.
Proof of i) The first part of the Proposition 2 is true if
the aggregated sequence G is equal to the sequence G0
formed by the flipped flow signal. So, for any flow signal f
with associated Hodge Laplacian L1 , as well as their flipped
counterparts f 0 = F f , L01 = F L1 F , we have that
 0 
  0
L1
L1
0
0
0
G =C f ,
f ,...,
f0
λ1
λ1

 
 

L1
L1
= C F f, F
F F f, . . . , F
FFf
λ1
λ1
"
 
 E−1 #
L1
L1
= C F f, F
f, . . . , F
f
λ1
λ1
= CF C > G.

CF C > G. Notice that the matrix CF C > is itself an edgeflipping matrix, restricted to the edges selected by C (and
thus, the edges measured in G).
We split the parameters θ into {θi }`i=1 , where θj indicates
the set of filters for the jth convolutional layer. Consider the
1st layer of the CNN, with input G and output Y . The features
(rows) of Y can be written as
!
X
ij
j
i
Y =σ
G ∗ θ1 ,
(20)

(19)

If Fii = 1 for the edges selected by C, then CF C > = I.
Under these conditions, G0 = G, as desired.
Proof of ii) As shown previously, applying an edge-flipping
matrix to a flow signal and its associated Hodge Laplacian
in the AGNN architecture yields an aggregated signal G0 =

where Y is the jth feature (row) of Y , Gi is the ith feature
(row) of G, and θ1ij is a 1D convolutional filter. Clearly, we
can obtain the same output from G0 by changing the sign of
selected filters θ1 , i.e.,
!


X
ij
j
0i
>
Y =σ
G ∗ [CF C ]ii θ1
.
(21)
i

So, for a flow signal f , Hodge Laplacian L1 , and CNN
parameters θ, AGGθ (f , L1 ) = AGGθ0 (F f , F L1 F ), where
θ10ij = [CF C > ]ii θ1ij , with the same parameters beyond the
first layer, as desired.
Note that the first part of Proposition 2 is a special case of
this, where CF C > = I → θ0 = θ.

