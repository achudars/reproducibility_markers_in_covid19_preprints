code::proof: Prepare for most weather
conditions

arXiv:1910.06964v1 [stat.OT] 14 Oct 2019

Charles T. Gray[0000−0002−9978−011X]⋆
La Trobe University, Melbourne charlestigray@gmail.com

Abstract Computational tools for data analysis are being released daily
on repositories such as the Comprehensive R Archive Network1 . How
we integrate these tools to solve a problem in research is increasingly
complex and requiring frequent updates. In this manuscript we propose
a toolchain walkthrough, an opinionated documentation of a scientific
workflow. As a practical complement to our proof-based argument (Gray
and Marwick, arXiv, 2019) for reproducible data analysis, here we focus
on the practicality of setting up a research compendia with unit tests
as a measure of code::proof, a reproducible research compendia that
provides a measure of confidence in computational algorithms.
Keywords: Metaresearch · Metaprogramming · Statistical computing.

1

The Kafkaesque dystopia of DevOps

In Franz Kafka’s 1925 novel The Trial [14], the fictional character Josef K.
is prosecuted for crimes that are not clear, in proceedings brought forth by an
unidentified authority. For the diligent scientist attempting to answer a mathematical question computationally, such as measuring the efficacy of a statistical
estimator via simulation, the process of implementing a scientific workflow to
achieve this aim can be a Kafkaesque tour of computational tools and systems.
The scientist may feel as if they are locked in a dystopia, tested repeatedly for
practices in which they have not been trained, such as shell scripts and computational architecture. Whilst there are detailed guides for specific computational
tools, it is hard to tell what is still relevant, as code frequently slides into obsolescence [22], and identify the optimal place to begin [34]. Significant cultural
barriers continue to exist in programming fora; for example, only one in seventeen contributors to Stack Overflow2 identify as women [6].
⋆

1

Thank you to Ben Marwick, Hien Nguyen, Emily Kothe, James Goldie, Mathew Ling,
J.D. Long, Kate Smith-Miles, Greg Wilson, Kerrie Mengersen, Jacinta Holloway,
Alex Hayes, Noam Ross, Rowland Mosbergen, Luke Prendergast, Dale Maschette,
Elio Campitelli, Thomas Lumley, and Daniel S. Katz for advising on particular
aspects of this manuscript.
As in the companion manuscript [8], we focus on R packages, but the reader is invited
to consider these as examples rather than definitive guidance. The same arguments
hold for other languages, such as Python, and associated tools.

For many an unfortunate scientist, the dystopian experience is not confined
to the DevOps, the developmental operations of preparation for the implementation of an algorithm [13]. Just as Josef K. was tried multiple times, the labours
of the scientist attempting to answer a mathematical question computationally
have only just begun. Analogous to how a string will knot with mathematical
predictability when jostled [2], an algorithm will reliably require debugging, the
process of identifying and correcting code, either to incorporate a new feature, or
to correct an error. This scientist finds themselves part of the first generation of
research software engineers (RSEs), who use computational tools in disciplinespecific research practices [35]. By virtue of pioneering, RSEs are inadvertently
cast as metaresearchers3, developing new methodologies for scientific technologies that hitherto did not exist [15]. With the aim of mitigating the dystopia
of DevOps and debugging for RSEs, this manuscript proposes a toolchain walkthrough, an opinionated [21] documentation of a scientific workflow, towards a
measure of code::proof, a good enough [34] effort to provide computational
confidence through reproducible research compendia with unit tests.

2

Toolchain walkthrough

We define a toolchain as a collection of computational tools and commands that
forms a scientific workflow to achieve a specific research objective, such as test the
efficacy of a statistical estimator in a particular context. The term walkthrough,
we borrow from video game terminology [5], and is defined as a guide for other
players of the game. Various walkthrough formats exist to optimise the narrative
enjoyment of the gamer. For example, the Universal Hint System [26] interface
provides the gamer with ever more revealing hints without spoiling other parts
of the game. Next generation walkthroughs see in-game modifiers, in games such
as World of Warcraft, where these provide an option for on-screen boss-specific
warnings [25].
We define toolchain walkthrough as an opinionated [21] documentation of
a scientific workflow, where opinionated is a term appropriated from software
engineering that acknowledges that software guides the user to certain choices.
In this manuscript, we describe a workflow for building a research compendium
that is opinionated in privileging reproducibility. As with the hint systems of
gaming, a workflow can and must be tailored to the skill and background of the
user. Thus toolchain walkthroughs can be extended and adapted for different
disciplines.
3

Visit the discussion on metaresearch and RSEs on the research compendium associated with this manuscript as an example of why this paper, and its companion [8],
have so many acknowledgements. Canonical literature is not yet established in the
field of RSE, and thus leaders of RSE projects, such as Alex Hayes’ maintenance of
the broom:: [23]. This has propelled Hayes rapidly to the level of expert, by virtue of
the pioneering collaborative structure of the package, where hundreds of statistical
modellers contribute integrated code.

Toolchain walkthroughs have not only intrinsic value in terms of solving
the intended research problem, but also extrinsic value, pedagogically and from
a developmental perspective. Frequently those who are undertaking research
software engineering on statistical projects are not the most senior member of the
team; in the case of university faculty, these are often also lecturers and service
teachers. There is value in seeing the minutiae of what the footsoldiers of research
development undertake and how they instruct others. This can inform as to what
skillsets are required in graduate courses, or are required for those who wish to
optimise scientific workflow for researchers. Much of what is being implemented
right now, in workflows recommended in texts such as R Packages [29] and
Advanced R [30], is being adopted from existing software engineering principles.
Toolchain walkthroughs can contribute to the literature on the adoption of these
procedures in a research context, in addition to programming fora and blog posts.
Blog posts and programming fora, as well as printed texts, are inevitably
bound for obsolescence [22]. Vignettes, tool-specific long-form documentation [29],
focus on one tool in the chain. As a counterpoint to the inadvertently implied
redundancy of the academic manuscript in the theoretical companion manuscript [8], here we consider if the ephemerality of most-recent publications, and
the chronological nature of academic publishing, may serve the breakneck speed
of research development. The toolchain walkthrough provides a documentation
of a specific scientific workflow constructed by an expert, or expert in training, in
the field. Indeed an expert in training is perhaps best placed, as by virtue of inexperience must research in order to solve the problem. The challenge above, say,
the standard one might expect from a blog post, is to provide a good enough 4 [34]
effort to avoid questionable research practices [7] that privilege, say, convention
over optimal scientific methodology.

3

Two research compendia case studies

For concrete examples of the benefits of adopting software research engineering principals in mathematical science, we consider two in-development research
compendia, varameta:: and simeta::. The primary purpose of these packages
is to provide a comparative analysis of estimators for the variance of the sample
median when quartiles are provided, rather than a measure of standard deviation, within the meta-analytic context. However, by structuring the packages
as such, rather than within a single script file, there is scope for solving similar
problems.
3.1

The varameta:: package; a comparative analysis

In contemporary meta-analytic computational tools, such as the R package
metafor:: [27], a measure of both an effect and its variance are required to
estimate the population parameters of interest.
4

As opposed to ofttimes unattainable or impractical best practices [33] in scientific
computing.

However, not all studies report a variance of effect; particularly when scientists suspect an underlying asymmetry in the distribution of the observed data,
prompting them to report quartiles, rather than sample standard deviations.
One solution to this is to approximate estimators for mean and variance from
quartiles [3,12,28]. We wish to explore the comparative efficacy of an estimator
for the variance of the sample median derived from the estimator of [24]:
var(m) ≈

1
4nf (ν)2

where m denotes the sample median, n the sample size, ν the population median,
and f the population probability density function.
However, in an experimental setting, we do not know the true distribution,
nor the true population median. Thu, our method proposes that we assume
a distribution, and estimate the parameters of that characterized the assumed
distribution from the sample size and sample quartiles. We provide estimators
derived for different distributions, to assess the efficacy of this analysis framework. One of which is the exponential distribution, which this manuscript will
focus on.
If we assume that f is an exponential probability density function, with
unknown rate parameter λ, then we can estimate this rate parameter via the
sample median. Since the true median is given by log 2/λ, we can estimate the
rate parameter,
λ ≈ log 2/m,
(1)
via the sample median, m.
Each proposed estimator requires a different set of reported values as inputs
and different calculations. It is notable that a most optimal estimation method
for the problem above is generally unknown. For example, in the comparative
analysis Wan et al. [28], it was shown that the performance of different estimators
varied with the simulated sample sizes.
Thus, there is merit to providing not only the practical functionality of our
proposed solution, but also the existing solutions. By structuring this comparative analysis as a reproducible research compendium we achieve practical improvements on a self-contained computational script file. Via roxygen::ised [32]
documentation, estimators are provided in a modular fashion, with a devoted
script file for each estimator that is easily sourced from the package environment.
In addition to the advantage of debugging a single script file, the comparative
analysis also serves a practical purpose, providing a characterisation of the functionality of each estimator.
To compare these estimators for the variance of the sample median, we undertook coverage probability simulations. Here, the coverage probability refers to
the probability that the true parameter of interest falls within its constructed
confidence interval. In order to do so, we require simulated meta-analytic data,
which has the added complexity of a random effect that governs the variation
between studies. To solve this with confidence in the implementation of computational algorithms and mathematical derivations, we structure this as a package.

In addition to building code::proof, by separating the simulation component,
we begin to develop a computational solution to not only solving this problem,
but the testing of any estimator for the variance of the sample mean or median.
3.2

The simeta:: package

A coverage probability simulation repeats several trials with the same simulation
meta-parameters where the differing factor is the random sampling of data. In
order to separate simulation meta-parameters from trial-level parameters, and
delineate this algorithm, we begin by considering a single trial from a standard
coverage probability simulation.
3.3

Coverage probability simulation

Each trial draws a random sample, for example rnorm(n = 100, mean = 3, sd
= 0.2) will produce 100 values drawn randomly from a normal distribution with
mean 3 and standard deviation 0.2. From this sample, we calculate summary
statistics. Using these summary statistics, we can compute an estimate of the
parameter of interest ν̂, and its variance γ̂. With these
√ estimates, we can produce
a (1 − α) × 100% confidence interval ν̂ ± z1−α/2 γ̂, where za = Φ−1 (a) is the
ath quantile of the standard normal distribution, and Φ is the standard normal
distribution function. Given we set the parameters for the random sample drawn,
we know the true parameter, ν. Thus we can ask, does ν fall within the confidence
interval produced? We summarise the steps of a trial as an algorithm:
1. Draw a random sample from the distribution that is characterised by the
parameter of interest, ν;
2. Calculate summary statistics from the random sample;
3. Calculate an estimate of ν from the summary statistics;
4. Construct a confidence interval using the parameter estimate;
5. Check if ν falls within the confidence interval.
A coverage probability simulation performs multiple trials and returns the
proportion p ∈ [0, 1] of confidence intervals for ν that contain the generative
parameter value.
3.4

Simulating meta-analysis data

For a meta-analysis simulation, however, these steps are significantly more involved. And with this complexity, as we shall see, nesting, of the algorithm, the
advantages of the package structure begin to become apparent. In a single script
file, it is hard to find at which step of the algorithm that the code has failed. In
addition to human error introduced into code, there are also practical considerations. For example, the random effects maximum likelihood model, method
= REML, employed by metafor::rma [27] does not always converge on estimates

for the effect and its variance, in which case a fixed effects model, method = FE,
can be employed to produce parameter estimates.
The other point of complexity is in the sampling of meta-analytic data. As
meta-analytic data is a collection of summary statistics for K studies of control
and intervention samples, the first step of a coverage probability simulation trial,
1. Draw a random sample from the distribution that is characterised by the
parameter of interest, ν,
requires several substeps. For the kth (k ∈ {1, . . . , K}) study, we assume there
is variation γk associated with that study, and, in particular, the control, with
parameter νkC , and intervention, with parameter νkI , samples with ratio, ρ =
νkC /νkI .
Let us consider a practical example from the estimators provided in the
comparative analysis, varameta::. Our estimator of interest is the variance of
the log-ratio of sample medians for control, ν C , and intervention, ν I groups. Since
our focus is on building the research compendium to undertake this analysis,
rather than the estimators in question, we will take the simplest case, where
there is one parameter λ associated with the distribution of interest. Let us
assume an underlying exponential distribution: Exponential(λ).
At the simulation level, which is to say, across all trials, we set λ, the parameter of the distribution of interest. Also at the simulation level, we define a
ratio ρ := ν C /ν I of interest for the population medians, where ρ = 1 would
indicate no true difference between control and intervention groups. We assume
that the log-ratio of sample medians log(mIk /mC
k ) for the kth study, can be
characterised in terms of the log-ratio of populations medians log(ν C /ν I ), with
some error γ ∼ N (0, τ 2 ) association with that study, as well as sampling error,
ε ∼ N (0, σ 2 ),
I
C
log(mIk /mC
k ) = log(ν /ν ) + γk + εk .

Since the underlying distribution is exponential, we need to find λJk for J ∈
{C, I} in order to sample n values x1 , . . . , xn ∼ Exponential(λJk ). We also know
the median of the exponential distribution with rate parameter λ is given by
log 2/λ. Then, assuming the sampling error will be attained through the random
computational process, we have
I
C
log(mIk /mC
k ) = log(ν /ν ) + γk
I
C
I
=⇒ log(λC
k ) − log(λk ) = log(λ ) − log(λ ) + γk

I
C
I
=⇒ log(λC
k ) − log(λk ) = (log(λ ) + γk /2) − (log(λk ) − γk /2)

If we then split the random effect associated with the variation between
studies γk equally, and divide the terms by experimental group J ∈ {C, I}, we
obtain the following system for the control C and intervention I groups’ kth
parameter, λJk .

C
λC
k = λ exp(γk /2)

λIk = λI exp(−γk /2)
1. Draw a measure of variation for the kth study from N (0, τ 2 ) and calculate
λI from fixed values, the ratio of medians, ρ, and the control group’s rate
parameter λC ;
I
2. Calculate the rate parameters for the control, λC
k , and intervention, λk ,
groups for the kth study;
3. Draw a random samples of size nJk from Exponential(λJk ), for J ∈ {C, I}.
The sample size nJk for the Jth group of the kth study can also be sampled,
I
by assuming Nk := nC
k + nk and drawing Nk from a uniform distribution
Uniform(a, b), where the minimum a, and maximum b, reflect knowledge about
the domain of interest. The proportion of Nk given to nIk can be drawn from a
beta distribution. But we shall omit the derivations of these sampling distributions, in the interests of brevity.
In the sampling steps that have been outlined, there are random values
drawn, but there are also set simulation-level parameters. We may wish to see
how our estimator performs for different numbers of studies, K, different expected variability between the studies, τ 2 , and whether or not there is a difference
between the control and intervention groups, ρ.
And finally, if we consider other distributions, with a mix of symmetric, say,
normal or Cauchy distribution, and asymmetric, say, exponential or log-normal,
we require different derivations for the sampling parameters.
3.5

Complexity and formalised analysis structures

Via the modular nature of a research compendium R package, we can separate
each layer of the algorithm into functions. We can produce automated unit tests
for these functions that, at the very least, check that each component of the
algorithm returns an output of expected type. We cannot automate the mathematical derivations, but we can produce an algorithm structure that provides
far more computational confidence in implementation than a single script file in
which the entire algorithm is nested.
However, structuring an analyses in research compendia is more challenging
than simply coding directly into a .R script. Thus, there is benefit to outlining
the computational workflow. We now turn to the practical toolchain walkthrough
for establishing these analyses as research compendia. We may not be able to
prepare for all errors, but we can aim to weather most problems that arise in
the computational implementation of mathematical algorithms.

4

Research compendia toolchain walkthrough

We now aim to provide a practical guide to computational research compendia for the comparative analysis, varameta::, and the simulation algorithm,

simeta::, that supports it. As this is a first effort at a toolchain walkthrough,
there will likely be aspects that are overlooked or underdeveloped.
4.1

DevOps

The DevOps section of this toolchain walkthrough aims to cover computational
tools, why they were chosen, as well as some guidance as to how to source them.
Intended audience. A toolchain walkthrough is a documentation of a specific
scientific workflow created by a scientist who utilised this workflow for research.
We begin by identifying the audience targeted who may benefit from detailing
the minutiae of this process. We do not seek to generalise, but rather to provide
a workflow that reflects the author’s knowledge of good enough practices in
scientific computing for this task, optimised for efficiency, scientific rigour, and,
in the spirit of the gaming walkthrough: fun.
This toolchain walkthrough assumes an R user whose expertise is not primarily in computing, but rather a researcher who employs R for analysis in a discipline such as statistics, psychology, archaeology, or ecology. We make an effort to
cover some of the less familiar aspects of computational workflow, such as shell
commands, that might be considered trivial to a formally trained computer scientist.
Although many R users have gaps in their formal computational science
education, researchers who utilise R are often implementing complex algorithms,
such as the one outlined in Section 3.2, which describes the simulation of metaanalysis data for coverage probability simulation.
Burn it down. This section only applies for work that has already begun.
However, this is often the case for the development of a scientific project. We
frequently have work that begin as small scripts, that develop in complexity and
requirements.
In recognition of the ofttimes overwhelming density of resources, we list a few
bash shell commands here that are particularly useful for moving files around
when setting up an analysis as a research compendium. We enclose user input
in <> and describe the utility of the command after #. A directory is colloquially
referred to as a folder. These can be executed from a terminal.
. # here
.. # up one
cd <directory path> # change location of .
ls -a # list files in .
cp <file> <toplace> # copy
mv <file> <toplace> # move or rename
rm -rf <directory> # remove directory and its contents
locate <partoffilename> # find a file
mkdir <directory> # create a directory

How to code. The R software environment can be downloaded from R: The R Project for Statistical Computin
There are several excellent resources for getting started with programming with
R. We list an opinionated selection here, chosen for clarity and enjoyment, all of
which are freely available online:
– Learning Statistics with R by Danielle Navarro [20],
– R for Data Science by Grolemund Garrett and Hadley Wickham [9],
– R Cookbook by J.D. Long and Paul Teetor [17].
We now assume a working knowledge of the R programming language, as the
intended audience of this toolchain workflow are researchers who have a working
level of programming proficiency in R.
Where to code. In this toolchain walkthrough, we emphasise cross-platform
open-source software. There is, of course, the immediate benefit of accessibility.
Furthermore, open-source invites an evolutionary development community where
many can contribute small solutions that integrate to solve larger problems.
RStudio is an integrated development environment for writing in the statistical
language R. RStudio is cross-platform in that it can be installed on Windows,
Macintosh, and Linux operating systems. There are many further advantages
to this widely-used environment. For example, the citr:: add-in [1] modifies
RStudio to enable a connection to the open-source reference manager Zotero.
Another example is the datapasta:: [19] add-in that enables copy-paste of
tables into R-formatted script.
4.2

Create compendium architecture

As varameta:: is a research compendium containing comparative analyses and
simeta:: a package to provide simulation tools, the creation process for these
two compendia are different.
We make use of two R packages, rrtools:: [18] and usethis:: [31], to assist
in automating these tasks.
Compendiumise varameta::.
1. Open RStudio and close project via the toolbar File menu,
2. In the Console, set the working directory to desired location; e.g.,
> getwd()
[1] "/home/charles"
> setwd("Documents/repos/")
> getwd()
[1] "/home/charles/Documents/repos",
3. and rrtools::use compendium("varameta"),

4. and update DESCRIPTION file with author, title, etc.,
5. Create analysis file structure with rrtools::use analysis().
For varameta::, we will have several reproducible documents that will form
the basis of the analysis, as well as figures to contribute to the associated publication. The final step above automates the creation of a directory structure for
a paper, figures, data, and templates.
Compendiumise simeta::. In this case, the file structure is less involved,
however the testing structure will be need to be considerably more robust because
of the complexity of the simulation algorithm described in Section 3.2:
1. Create a package with usethis::create package(),
2. Switch to the package directory with usethis::project activate().
4.3

Common steps across both packages

1. Set open source licence, with
usethis::use mit license(name = "Charles Gray");
this ‘simple and permissive’ choice of licence [31] serves the purpose of a
comparative analysis of estimators,
2. Set up documentation for functions with usethis::use roxygen md(),
3. Set up data for internal datasets and examples with usethis::use data().
Connecting to GitHub. There are benefits to implementing a version control
system, such as via the Git language and GitHub online repository archive, beyond the ability to trace work back to an earlier iteration [4]. The added benefit,
arguably even greater benefit, is that of collaborative science. Storing work on
GitHub allows for instantaneous sharing of code and analyses, and collaborative
work with advanced project planning features, enabling other scientists to make
very specific comments on work in progress.
Data ethics and further considerations. In the case of varameta::’s estimators for meta-analysing medians, and simeta:: for simulating meta-analysis
estimators, there are no ethics in data considerations beyond ensuring contributors are recognised and credited for their work by time of publication. For some
disciplines, sharing geographic locations might be an ethical consideration, say,
in preventing fossil hunters from exploiting palaeontology sites [11]. Personal
details, must, too be considered, that might inadvertently identify people and
violate privacy considerations. Furthermore, various allowances might need to
be made for institutional workflow. We note these here as a possible considerations, but as our case studies do not have such requirements, we now consider
our research compendia instantiated.
However, as this algorithm has significant complexity, we need to include
unit tests to provide confidence in our results , as we argue in the companion
computational metamathematics manuscript [8], which motivates the practical
steps laid out here.

5

Testing

We now expand in a practical sense on unit testing, which, in the theoretical
companion manuscript, we describe ‘the software engineering tool that provides
a key piece of the correspondence between scientific claim and programming’ [8].
It is in this manuscript that we sought to answer the question: why test? In this
toolchain walkthrough, we will focus on the practical implementation of first
unit tests.
5.1

What is a test?

Tests are collected in contexts. Each test comprises congruous expectation functions.
In the head of the ‘bug hunt’ context (under context("bug hunt")), we find
the loading of packages. A seed is then set for reproducibility of errors. The first
test, "metasim runs for different n", tests the simeta::metasim() function for different orders of magnitude of trials. As each trial samples new data,
this is the most direct way to test the scalability of the function for large datasets. We then follow up with a test that checks that the exponential distribution
can be passed to all levels in the algorithm.
context("bug hunt")
set.seed(38)
library(tidyverse)
library(metasim)
test_that("metasim runs for different n", {
expect_is(metasim(), ’data.frame’)
expect_is(metasim(trials = 100) , "data.frame")
# expect_is(metasim(trials = 1000) , "data.frame")
})
test_that("exponential is parsed throughout", {
# check sample
expect_equal(
sim_sample(10, rdist = "exp",
par = list(rate = 3)) %>% length, 10)
# check samples
...
5.2

Non-empty thing of expected type

Simply asking ‘does a function produce the expected output? ’, induces a surprising
number of considerations. To illustrate this, we return to our case studies.

Testing a collection of estimators in varameta::. In the interests of mathematical and computational brevity, we focus on one distributional example: the
simple case of the exponential distribution, which is characterised by a single
parameter. We return to the estimator of the rate λ̂ := log 2/m derived for the
exponential distribution, as discussed in Section 3.4 and defined in Equation (1),
explicitly coded in R.
function(n, median) {
# Estimate parameters.
lambda <- log(2) / median
# Approximate the standard error of the sample median.
1 / (2 * sqrt(n) * dexp(median, rate = lambda))
}
We create a context file, tests/testthat/test-exponential.R and provide
a short context description in the first line of the script.
context("exponential estimator")
As a starting point, we can write unit tests to automate a check that this
function returns non-empty thing of expected type. We arbitrarily choose values,
a sample size of 10, and a proposed sample median of 4, for instance. The function
should return a numeric double value, and should be positive.
test_that("non-empty thing of expected type, for fixed values", {
# returns numeric
expect_type(g_exp(10, 4), "double")
# returns positive number
expect_gt(g_exp(10, 4), 0)
})
In addition to choosing explicit values, we can also randomly sample the
sample size n, and sample median m. To ensure reproducibility of these testing
results on any machine, we set a random seed, passing set.seed an arbitrary
numeric value.
set.seed(39) # ensures reproducibility of test results
# sample fuzz testing parameters
n <- sample(seq(2, 100), 1)
m <- runif(1, 1, 100)

We can then use these random fuzz values [16] to produce analogous unit
tests for non-empty thing of expected type.
test_that("non-empty thing of expected type, for random values", {
expect_type(g_exp(n, m), "double")
expect_gt(g_exp(n, m), 0)
})
We can extend these tests to cover expected input errors. For example, we
wish this function to fail when passed negative numbers. The sample size cannot
be less than or equal to 0, and due to the logarithm, the function only works for
positive sample medians. Here, we include the fixed and randomised values in
the same test.
test_that("negative numbers throw an error", {
expect_error(g_exp(-3, 4))
expect_error(g_exp(3, -4))
# with fuzz testing
expect_error(g_exp(-n, m))
expect_error(g_exp(n, -m))
})
Running all tests in a context tells us if the function is behaving as expected.
The more tests we write, the more confidence we will have that our function
behaves as we intended it to.
==> Testing R file using ’testthat’
Loading varameta
| OK F W S | Context
|
8
| exponential estimator
Results
OK:
Failed:
Warnings:
Skipped:

8
0
0
0

Test complete
There is a tradeoff with tests, in terms of time taken by updating the tests
themselves. Here a test requires updating from an expected output of a numeric
vector, to a dataframe. The function that is being tested.
==> Testing R file using ’testthat’
Loading simeta

|
|

OK F W S | Context
6 1
| bug hunt [7.1 s]

test-bug-hunt.R:21: failure: exponential is parsed throughout
sim_stats(rdist = "exp", par = list(rate = 3)) inherits from
‘tbl_df/tbl/data.frame‘ not ‘numeric‘.

Results
Duration: 7.1 s
OK:
Failed:
Warnings:
Skipped:

6
1
0
0

Test complete
Testing a nested algorithm in simeta::. Our other case study provides an
example of a nested algorithm. In addition to ensuring each function returns a
non-empty thing of expected type, we can automate checks that the functions
form a toolchain. In the first place, it is helpful to know that our functions
continue to form a toolchain under default settings.
We begin by setting our context. In this case, as we are running our functions
on default settings, we do not require randomly sampled fuzz parmeters.
context("default pipeline")
We now check that the algorithm runs ‘upwards’, by running a test from most
granular function in the algorithm to most nested. We could write a similiarly
inverted test, from most nested function, downwards to most granular.
test_that("work upwards through algorithm", {
expect_is(sim_n(), "data.frame")
expect_gt(sim_n() %>% nrow(), 1)
# sim_df calls sim_n
expect_is(sim_df(), "data.frame")
expect_is(sim_stats(), "data.frame")
# metasim calls metatrial
expect_is(metatrial(), "data.frame")
expect_is(singletrial(), "data.frame") # alternate trial
expect_is(metasim(trials = 3), "data.frame")
# metasims calls sim_df & metasim
expect_is(metasims(
single_study = FALSE,
trials = 3,

progress = FALSE
),
"sim_ma")
})
Now, if this test fails, we will know the combination of functions fails at some
point in the nested algorithm. We follow this upwards test with a series of small
tests for each function set to defaults to identify at which point in the pipeline
where the algorithm fails, if the ‘work upwards’ test fails.
# test each component on defaults
test_that("sim_n", {
expect_is(sim_n(), "data.frame")
})
test_that("sim_df", {
expect_is(sim_df(), "data.frame")
})
test_that("metatrial", {
# metasim calls metatrial
expect_is(metatrial(), "data.frame")
})
test_that("singletrial", {
expect_is(singletrial(), "data.frame") # alternate trial
})
test_that("metasim", {
expect_is(metasim(trials =
})

3), "data.frame")

test_that("metasims", {
expect_is(metasims(
single_study = FALSE,
trials = 3,
progress = FALSE
),
"list")
})
And we can now run all tests, for a starting point of automating checks that
our algorithm runs on default settings.
==> Testing R file using ’testthat’

Loading simeta
| OK F W S | Context
| 14
| default pipeline [28.7 s]
Results
Duration: 28.7 s
OK:
Failed:
Warnings:
Skipped:

14
0
0
0

Test complete
To demonstrate how informative testing can be in identifying where an algorithm breaks, we now modify the simeta::metasim function to return a character string, "error". Testing the default pipeline reveals where the algorithm is
broken. Debugging is where the advantage of testing is exposed, and thus, arguably the requirement for testing increases with complexity of algorithm. Detailed
output have been omitted for brevity.
==> Testing R file using ’testthat’
Loading simeta
| OK F W S | Context
| 10 4
| default pipeline [32.3 s]
test-default-pipeline.R:12: failure: work upwards through algorithm
metasim(trials = 3) inherits from ‘character‘ not ‘data.frame‘.
test-default-pipeline.R:14: error: work upwards through algorithm
Argument 1 must have names
...
test-default-pipeline.R:43: failure: metasim
metasim(trials = 3) inherits from ‘character‘ not ‘data.frame‘.
test-default-pipeline.R:47: error: metasims
Argument 1 must have names
...

Results
Duration: 32.3 s

OK:
Failed:
Warnings:
Skipped:

10
4
0
0

Test complete
From this output, we can see not only where the algorithm fails, but also
what other functions fail because of a reliance on the elements that have failed.
5.3

Test-driven development

As we build new features into our package, such as checking that the singletrial setting works in the simulation function from simeta::, we can focus on
a writing new tests that ensure our feature works within the ecosystem of our
algorithm as expected. We can develop our algorithm from a testing setting,
rather than focusing on rewriting functions and script files.
Another overview check that we can incorporate is from the covr:: package [10]. Using covr::package coverage(), we can check what proportion of
lines of code have been tested in each function.
For the varmeta:: package, at the time of writing, we have the following test
coverage.
varameta Coverage: 90.00%
R/g_cauchy.R: 44.44%
R/g_norm.R: 71.43%
R/hozo_se.R: 92.31%
R/bland_mean.R: 100.00%
R/bland_se.R: 100.00%
R/effect_se.R: 100.00%
R/g_exp.R: 100.00%
R/g_lnorm.R: 100.00%
R/hozo_mean.R: 100.00%
R/wan_mean_C1.R: 100.00%
R/wan_mean_C2.R: 100.00%
R/wan_mean_C3.R: 100.00%
R/wan_se_C1.R: 100.00%
R/wan_se_C2.R: 100.00%
R/wan_se_C3.R: 100.00%
This is enables us to target specific functions that may require further testing.
Testing lines of code is somewhat a blunt instrument, as we are not ensuring tests
for every combination of inputs. However, test coverage is still an informative
measure of software reliability. For example, here we see not all code in the g *
estimators have been checked.
These notes on testing are not intended to be comprehensive, but only aim to
give the user an starting point for the initialisation of summarising an analysis

in a reproducible research compendia, with an informative level of automated
checks. Given only one quarter of packages on the largest R package repository
CRAN have unit tests at all [8], it is arguable that there is much further scope
for discussion and development with respect to the adoption of automated tests
in reproducible research compendia.

6

Prepare for most weather conditions

Computational proof may be unachievable, however, a measure of code::proof
can be attained by structuring research compendia in a standardised reproducible format, such as produced by rrtools:: [18]. Perhaps we cannot prove our
software in the traditional mathematical sense [8]. However, we could consider
building confidence in the mathematics that we implement computationally, like
waterproofing our shoes. If we step in a big enough puddle, our feet are still
going to get wet, but at least we have prepared to weather most of the problems
associated with the implementation of statistical algorithms.

References
1. Aust, F.: Citr: ’RStudio’ add-in to insert markdown citations (2018),
https://github.com/crsh/citr, r package version 0.3.0
2. Belmonte,
A.:
The
tangled
web
of
self-tying
knots.
Proceedings
of
the
National
Academy
of
Sciences
104(44),
17243–
https://doi.org/10.1073/pnas.0708150104,
17244
(Oct
2007).
https://www.pnas.org/content/104/44/17243
3. Bland, M.: Estimating Mean and Standard Deviation from the
Sample Size, Three Quartiles, Minimum, and Maximum. International
Journal of Statistics in Medical Research 4(1), 57–64–64 (Jan 2014),
http://lifescienceglobal.com/pms/index.php/ijsmr/article/view/2688
4. Bryan, J.: Excuse me, do you have a moment to talk about version control? PeerJ
PrePrints 5, e3159 (2017)
5. Consalvo, M.: Zelda 64 and Video Game Fans: A Walkthrough
of Games, Intertextuality, and Narrative. Television & New Media
4(3),
321–334
(Aug
2003).
https://doi.org/10.1177/1527476403253993,
https://doi.org/10.1177/1527476403253993
6. Ford, D., Smith, J., Guo, P.J., Parnin, C.: Paradise Unplugged: Identifying Barriers for Female Participation on Stack Overflow. In: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on
Foundations of Software Engineering. pp. 846–857. FSE 2016, ACM,
New York, NY, USA (2016). https://doi.org/10.1145/2950290.2950331,
http://doi.acm.org/10.1145/2950290.2950331, event-place: Seattle, WA,
USA
7. Fraser, H., Parker, T., Nakagawa, S., Barnett, A., Fidler, F.: Questionable research practices in ecology and evolution. PLOS ONE
13(7), e0200303 (Jul 2018). https://doi.org/10.1371/journal.pone.0200303,
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0200303,
bibtex*[shortjournal=PLOS ONE]

8. Gray, C.T., Marwick, B.: Truth, Proof, and Reproducibility: There’s
no counter-attack for the codeless. arXiv:1907.05947 [math] (Jul 2019),
http://arxiv.org/abs/1907.05947, arXiv: 1907.05947
9. Grolemund,
G.,
Wickham,
H.:
R
for
Data
Science
(2017),
https://r4ds.had.co.nz/
10. Hester,
J.:
covr:
Test
Coverage
for
Packages
(2018),
https://CRAN.R-project.org/package=covr
11. Hopkin,
M.:
Palaeontology
journal
will
’fuel
black
market’.
https://doi.org/10.1038/445234b,
Nature
445,
234–235
(Jan
2007).
https://www.nature.com/articles/445234b
12. Hozo, S.P., Djulbegovic, B., Hozo, I.: Estimating the mean and variance
from the median, range, and the size of a sample. BMC Medical Research
Methodology 5(1),
13 (Apr 2005). https://doi.org/10.1186/1471-2288-5-13,
https://doi.org/10.1186/1471-2288-5-13
13. Httermann, M.: DevOps for Developers. Apress (Oct 2012), google-Books-ID:
JfUAkB8AA7EC
14. Kafka, F.: The Trial (Apr 2005), http://www.gutenberg.org/ebooks/7849
15. Katz, D.S., McHenry, K.: Super RSEs: Combining research and service in three dimensions of Research Software Engineering (Jul 2019),
https://danielskatzblog.wordpress.com/2019/07/12/
16. Klees, G., Ruef, A., Cooper, B., Wei, S., Hicks, M.: Evaluating Fuzz
Testing. In: Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security. pp. 2123–2138. CCS ’18, ACM,
New York, NY, USA (2018). https://doi.org/10.1145/3243734.3243804,
http://doi.acm.org/10.1145/3243734.3243804, event-place: Toronto, Canada
17. Long, J.J., Teetor, P.: R Cookbook, 2nd Edition (2019), https://rc2e.com/
18. Marwick, B.: rrtools: Creates a reproducible research compendium (2018),
https://github.com/benmarwick/rrtools
19. McBain, M., Carroll, J.: Datapasta: R tools for data copy-pasta (2018),
https://CRAN.R-project.org/package=datapasta, r package version 3.0.0
20. Navarro, D.: Learning statistics with R: A tutorial for psychology
students
and
other
beginners.
(Version
0.6.1)
(2019),
https://learningstatisticswithr.com/book/
21. Parker,
H.:
Opinionated
analysis
development.
preprint
(2017).
https://doi.org/10.7287/peerj.preprints.3210v1
22. Ragkhitwetsagul, C., Krinke, J., Paixao, M., Bianco, G., Oliveto, R.: Toxic Code
Snippets on Stack Overflow. IEEE Transactions on Software Engineering pp. 1–1
(2019). https://doi.org/10.1109/TSE.2019.2900307
23. Robinson, D., Hayes, A.: broom: Convert Statistical Analysis Objects into Tidy
Tibbles (2019), https://CRAN.R-project.org/package=broom
24. Serfling, R.J.: Approximation Theorems of Mathematical Statistics. John Wiley &
Sons (Sep 2009), google-Books-ID: enUouJ4EHzQC
25. skyisup:
Deadly
Boss
Mods
Addon
Guide
(2019),
https://www.wowhead.com/deadly-boss-mods-addon-guide
26. UHS: Universal Hint System: Not your ordinary walkthrough. Just the hints you
need. (2019), http://www.uhs-hints.com/
27. Viechtbauer, W.: Conducting meta-analyses in R with the <span
class=”nocase”>metafor</span> package. Journal of Statistical Software
36(3), 1–48 (2010), http://www.jstatsoft.org/v36/i03/

28. Wan, X., Wang, W., Liu, J., Tong, T.: Estimating the sample
mean and standard deviation from the sample size, median, range
and/or
interquartile
range.
BMC
Medical
Research
Methodology
14(1),
135
(Dec
2014).
https://doi.org/10.1186/1471-2288-14-135,
https://doi.org/10.1186/1471-2288-14-135
29. Wickham, H.: R Packages: Organize, Test, Document, and Share Your Code.
O’Reilly Media (2015), https://books.google.com.au/books?id=DqSxBwAAQBAJ,
bibtex*[lccn=2015472811]
30. Wickham, H.: Advanced R. Routledge, Boca Raton, FL, 1 edition edn. (Sep 2014)
31. Wickham, H., Bryan, J.: usethis: Automate Package and Project Setup (2019),
https://CRAN.R-project.org/package=usethis
32. Wickham, H., Danenberg, P., Eugster, M.: Roxygen2: in-line documentation for R
(2019), https://github.com/klutometis/roxygen, r package version 6.1.1.9000
33. Wilson, G., Aruliah, D.A., Brown, C.T., Chue Hong, N.P., Davis, M., Guy,
R.T., Haddock, S.H.D., Huff, K.D., Mitchell, I.M., Plumbley, M.D., Waugh, B.,
White, E.P., Wilson, P.: Best Practices for Scientific Computing. PLoS Biology 12(1), e1001745 (Jan 2014). https://doi.org/10.1371/journal.pbio.1001745,
https://dx.plos.org/10.1371/journal.pbio.1001745
34. Wilson, G., Bryan, J., Cranston, K., Kitzes, J., Nederbragt, L., Teal, T.K.:
Good enough practices in scientific computing. PLOS Computational Biology 13(6), e1005510 (Jun 2017). https://doi.org/10.1371/journal.pcbi.1005510,
http://dx.plos.org/10.1371/journal.pcbi.1005510
35. Wyatt, C.: Research Software Engineers Association (2019), https://rse.ac.uk/

