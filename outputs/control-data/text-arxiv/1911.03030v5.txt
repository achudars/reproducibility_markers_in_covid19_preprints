Certified Data Removal from Machine Learning Models

Chuan Guo 1 Tom Goldstein 2 Awni Hannun 2 Laurens van der Maaten 2

arXiv:1911.03030v5 [cs.LG] 14 Aug 2020

Abstract
Good data stewardship requires removal of data
at the request of the data‚Äôs owner. This raises the
question if and how a trained machine-learning
model, which implicitly stores information about
its training data, should be affected by such a removal request. Is it possible to ‚Äúremove‚Äù data
from a machine-learning model? We study this
problem by defining certified removal: a very
strong theoretical guarantee that a model from
which data is removed cannot be distinguished
from a model that never observed the data to begin
with. We develop a certified-removal mechanism
for linear classifiers and empirically study learning settings in which this mechanism is practical.

inference attacks (Yeom et al., 2018; Carlini et al., 2019) are
unsuccessful on data that was removed from the model. We
emphasize that certified removal is a very strong notion of
removal; in practical applications, less constraining notions
may equally fulfill the data owner‚Äôs expectation of removal.
We develop a certified-removal mechanism for L2 regularized linear models that are trained using a differentiable convex loss function, e.g., logistic regressors. Our
removal mechanism applies a Newton step on the model
parameters that largely removes the influence of the deleted
data point; the residual error of this mechanism decreases
quadratically with the size of the training set. To ensure
that an adversary cannot extract information from the small
residual (i.e., to certify removal), we mask the residual using an approach that randomly perturbs the training loss
(Chaudhuri et al., 2011). We empirically study in which
settings the removal mechanism is practical.

1. Introduction
Machine-learning models are often trained on third-party
data, for example, many computer-vision models are trained
on images provided by Flickr users (Thomee et al., 2016).
When a party requests that their data be removed from such
online platforms, this raises the question how such a request
should impact models that were trained prior to the removal.
A similar question arises when a model is negatively impacted by a data-poisoning attack (Biggio et al., 2012). Is it
possible to ‚Äúremove‚Äù data from a model without re-training
that model from scratch?
We study this question in a framework we call certified removal, which theoretically guarantees that an adversary cannot extract information about training data that was removed
from a model. Inspired by differential privacy (Dwork,
2011), certified removal bounds the max-divergence between the model trained on the dataset with some instances
removed and the model trained on the dataset that never
contained those instances. This guarantees that membership1

Department of Computer Science, Cornell University, New
York, USA 2 Facebook AI Research, New York, USA. Correspondence to: Chuan Guo <cg563@cornell.edu>, Laurens van der
Maaten <lvdmaaten@gmail.com>.
Proceedings of the 37 th International Conference on Machine
Learning, Online, PMLR 119, 2020. Copyright 2020 by the author(s).

2. Certified Removal
Let D be a fixed training dataset and let A be a (randomized)
learning algorithm that trains on D and outputs a model
h ‚àà H, that is, A : D ‚Üí H. Randomness in A induces a
probability distribution over the models in the hypothesis
set H. We would like to remove a training sample, x ‚àà D,
from the output of A.
To this end, we define a data-removal mechanism M that
is applied to A(D) and aims to remove the influence of
x. If removal is successful, the output of M should be
difficult to distinguish from the output of A applied on
D \ x. Given  > 0, we say that removal mechanism M
performs -certified removal (-CR) for learning algorithm
A if ‚àÄT ‚äÜ H, D ‚äÜ X , x ‚àà D:
e‚àí ‚â§

P (M (A(D), D, x) ‚àà T )
‚â§ e .
P (A(D \ x) ‚àà T )

(1)

This definition states that the ratio between the likelihood
of (i) a model from which sample x was removed and (ii) a
model that was never trained on x to begin with is close to
one for all models in the hypothesis set, for all possible data
sets, and for all removed samples. Note that although the
definition requires that the mechanism M is universally applicable to all training data points x ‚àà D, it is also allowed
to be data-dependent, i.e., both the training set D and the

Certified Data Removal from Machine Learning Models

data point to be removed x are given as inputs to M .
We also define a more relaxed notion of (, Œ¥)-certified removal for Œ¥ > 0 if ‚àÄT ‚äÜ H, D ‚äÜ X , x ‚àà D:
P (M (A(D), D, x) ‚àà T ) ‚â§ e P (A(D \ x) ‚àà T ) + Œ¥, and
P (A(D \ x) ‚àà T ) ‚â§ e P (M (A(D), D, x) ‚àà T ) + Œ¥.
Conceptually, Œ¥ upper bounds the probability for the maxdivergence bound in Equation 1 to fail.
A trivial certified-removal mechanism M with  = 0 completely ignores A(D) and evaluates A(D\x) directly, that is,
it sets M (A(D), D, x) = A(D \ x). Such a removal mechanism is impractical for many models, as it may require
re-training the model from scratch every time a training
sample is removed. We seek mechanisms M with removal
cost O(n) or less, with small constants, where n = |D| is
the training set size.
Insufficiency of parametric indistinguishability. One
alternative relaxation of exact removal is by asserting
that the output of M (A(D), D, x) is sufficiently close to
that of A(D \ x). It is easy to see that a model satsifying such a definition still retains information about x.
Consider a linear regressor trained on the dataset D =
{(e1 , 1), (e2 , 2), . . . , (ed , d)} where ei ‚Äôs are the standard
basis vectors for Rd . A regressor that is initialized with
zeros, or that has a weight decay penalty, will place a nonzero weight on wi if (ei , i) is included in D, and a zero
weight on wi if not. In this case, an approximate removal
algorithm that leaves wi small but non-zero still reveals that
(ei , i) appeared during training.
Relationship to differential privacy. Our formulation of
certified removal is related to that of differential privacy
(Dwork, 2011) but there are important differences. Differential privacy states that:
‚àÄT ‚äÜ H, D, D0 : e‚àí ‚â§

P (A(D) ‚àà T )
‚â§ e ,
P (A(D0 ) ‚àà T )

(2)

where D and D0 differ in only one sample. Since D and
D \ x only differ in one sample, it is straightforward to
see that differential privacy of A is a sufficient condition
for certified removal, viz., by setting removal mechanism
M to the identity function. Indeed, if algorithm A never
memorizes the training data in the first place, we need not
worry about removing that data.
Even though differential privacy is a sufficient condition,
it is not a necessary condition for certified removal. For
example, a nearest-neighbor classifier is not differentially
private but it is trivial to certifiably remove a training sample
in O(1) time with  = 0. We note that differential privacy
is a very strong condition, and most differentially private

models suffer a significant loss in accuracy even for large 
(Chaudhuri et al., 2011; Abadi et al., 2016). We therefore
view the study of certified removal as analyzing the tradeoff between utility and removal efficiency, with re-training
from scratch and differential privacy at the two ends of the
spectrum, and removal in the middle.

3. Removal Mechanisms
We focus on certified removal from parametric models, as
removal from non-parametric models (e.g., nearest-neighbor
classifiers) is trivial. We first study linear models with
strongly convex regularization before proceeding to removal
from deep networks.
3.1. Linear Classifiers
Denote by D = {(x1 , y1 ), . . . , (xn , yn )} the training set
of n samples, ‚àÄi : xi ‚àà Rd , with corresponding targets
yi . We assume learning algorithm A tries to minimize the
regularized empirical risk of a linear model:
L(w; D) =

n
X
i=1

`(w> xi , yi ) +

Œªn
kwk22 ,
2

where `(w> x, y) is a convex loss that is differentiable everywhere. We denote w‚àó = A(D) = argminw L(w; D) as it
is uniquely determined.
Our approach to certified removal is as follows. We first
define a removal mechanism that, given a training data point
(x, y) to remove, produces a model w‚àí that is approximately equal to the unique minimizer of L(w; D \ (x, y)).
The model produced by such a mechanism may still contain information about (x, y). In particular, if the gradient ‚àáL(w‚àí ; D \ (x, y)) is non-zero, it reveals information
about the removed data point. To hide this information, we
apply a sufficiently large random perturbation to the model
parameters at training time. This allows us to guarantee
certified removal; the values of  and Œ¥ depend on the approximation quality of the removal mechanism and on the
distribution from which the model perturbation is sampled.
Removal mechanism. We assume without loss of generality that we aim to remove the last training sample,
(xn , yn ). Specifically, we define an efficient removal
mechanism that approximately minimizes L(w; D0 ) with
D0 = D \ (xn , yn ). First, denote the loss gradient at sample
(xn , yn ) by ‚àÜ = Œªw‚àó +‚àá`((w‚àó )> xn , yn ) and the Hessian
of L(¬∑; D0 ) at w‚àó by Hw‚àó = ‚àá2 L(w‚àó ; D0 ). We consider
the Newton update removal mechanism M :
‚àí1
w‚àí = M (w‚àó , D, (xn , yn )) := w‚àó + Hw
‚àó ‚àÜ,

(3)

which is a one-step Newton update applied to the gradient
‚àí1
influence of the removed point (xn , yn ). The update Hw
‚àó‚àÜ

Certified Data Removal from Machine Learning Models

is also known as the influence function of the training point
(xn , yn ) on the parameter vector w‚àó (Cook and Weisberg,
1982; Koh and Liang, 2017).
The computational cost of this Newton update is dominated
by the cost of forming and inverting the Hessian matrix. The
Hessian matrix for D can be formed offline with O(d2 n)
cost. The subsequent Hessian inversion makes the removal
mechanism O(d3 ) at removal time; the inversion can leverage efficient linear algebra libraries and GPUs.
To bound the approximation error of this removal mechanism, we observe that the quantity ‚àáL(w‚àí ; D0 ), which we
refer to hereafter as the gradient residual, is zero only when
w‚àí is the unique minimizer of L(¬∑; D0 ). We also observe
that the gradient residual norm, k‚àáL(w‚àí ; D0 )k2 , reflects
the error in the approximation w‚àí of the minimizer of
L(¬∑; D0 ). We derive an upper bound on the gradient residual
norm for the removal mechanism (cf. Equation 3).
Theorem 1. Suppose that ‚àÄ(xi , yi ) ‚àà D, w ‚àà Rd :
k‚àá`(w> xi , yi )k2 ‚â§ C. Suppose also that `00 is Œ≥-Lipschitz
and kxi k2 ‚â§ 1 for all (xi , yi ) ‚àà D. Then:
‚àí1
k‚àáL(w‚àí ; D0 )k2 = k(HwŒ∑ ‚àí Hw‚àó )Hw
‚àó ‚àÜk2
‚àí1
2
‚â§ Œ≥(n ‚àí 1)kHw
‚àó ‚àÜk2 ‚â§

(4)
2

4Œ≥C
,
‚àí 1)

u := ‚àáLb (wÃÉ; D0 ) =

n‚àí1
X

‚àá`(wÃÉ> xi , yi ) + Œª(n ‚àí 1)wÃÉ + b.

i=1

(5)
We assume that AÃÉ can produce a gradient residual u with
kuk2 ‚â§ 0 for some pre-specified bound 0 that is independent of the perturbation vector b.
Let fA (¬∑) and fAÃÉ (¬∑) be the density functions over the model
parameters produced by A and AÃÉ, respectively. We bound
the max-divergence between fA and fAÃÉ for any solution wÃÉ
produced by approximate minimizer AÃÉ.
Theorem 2. Suppose that b is drawn from a distribution
with density function p(¬∑) such that for any b1 , b2 ‚àà Rd
1)
satisfying kb1 ‚àí b2 k2 ‚â§ 0 , we have that: e‚àí ‚â§ p(b
p(b2 ) ‚â§
e . Then e‚àí ‚â§

fAÃÉ (wÃÉ)
fA (wÃÉ)

‚â§ e for any wÃÉ produced by AÃÉ.

Achieving certified removal. We can use Theorem 2 to
prove certified removal by combining it with the gradient
residual norm bound 0 from Theorem 1. The security
parameters  and Œ¥ depend on the distribution from which
b is sampled. We state the guarantee of (, Œ¥)-certified
removal below for two suitable distributions p(b).

Œª2 (n

0

where HwŒ∑ denotes the Hessian of L(¬∑; D ) at the parameter
‚àí1
vector wŒ∑ = w‚àó + Œ∑Hw
‚àó ‚àÜ for some Œ∑ ‚àà [0, 1].
Loss perturbation. Obtaining a small gradient norm
k‚àáL(w‚àí ; D0 )k2 via Theorem 1 does not guarantee certified removal. In particular, the direction of the gradient
residual may leak information about the training sample that
was ‚Äúremoved.‚Äù To hide this information, we use the loss
perturbation technique of Chaudhuri et al. (2011) at training
time. It perturbs the empirical risk by a random linear term:

Lb (w; D) =

AÃÉ. This implies the gradient residual is:

n
X

 Œªn
` w> xi , yi +
kwk22 + b> w,
2
i=1

with b ‚àà Rd drawn randomly from some distribution. We
analyze how loss perturbation masks the information in the
gradient residual ‚àáLb (w‚àí ; D0 ) through randomness in b.
Let A(D0 ) be an exact minimizer1 for Lb (¬∑; D0 ) and let
AÃÉ(D0 ) be an approximate minimizer of Lb (¬∑; D0 ), for example, our removal mechanism applied on the trained model.
Specifically, let wÃÉ be an approximate solution produced by
1
Our result can be modified to work with approximate loss
minimizers by incurring a small additional error term.

Theorem 3. Let A be the learning algorithm that returns
the unique optimum of the loss Lb (w; D) and let M be
the Newton update removal mechanism (cf., Equation 3).
Suppose that k‚àáL(w‚àí ; D0 )k2 ‚â§ 0 for some computable
bound 0 > 0. We have the following guarantees for M :
(i) If b is drawn from a distribution with density p(b) ‚àù

e‚àí 0 kbk2 , then M is -CR for A;
(ii) If b ‚àº N (0, c0 /)d with c > 0, then M is (, Œ¥)-CR
2
for A with Œ¥ = 1.5 ¬∑ e‚àíc /2 .
The distribution in (i) is equivalent to sampling a direction
uniformly over the unit sphere and sampling a norm from
0
the Œì(d,  ) distribution (Chaudhuri et al., 2011).
3.2. Practical Considerations
Least-squares and logistic regression. The certified removal mechanism described above can be used with leastsquares and logistic regression, which are ubiquitous in
real-world applications of machine learning.
Least-squares regression assumes ‚àÄi : yi ‚àà R and uses the
2
loss function `(w> xi , yi ) = w> xi ‚àí yi . The Hessian
of this loss function is ‚àá2 `(w> xi , yi ) = xi x>
i , which is
independent of w. In particular, the gradient residual from
Equation 4 is exactly zero, which makes the Newton update
in Equation 3 an -certified removal mechanism with  = 0

Certified Data Removal from Machine Learning Models

(loss perturbation is not needed). This is not surprising since
the Newton update assumes a local quadratic approximation
of the loss, which is exact for least-squares regression.
Logistic regression assumes ‚àÄi : yi ‚àà {‚àí1, +1} and
 uses
the loss function `(w> xi , yi ) = ‚àí log œÉ yi w> xi , where
1
œÉ(¬∑) denotes the sigmoid function, œÉ(x) = 1+exp(‚àíx)
. The
loss gradient and Hessian are given by:

‚àá`(w> xi , yi ) = œÉ(yi w> xi ) ‚àí 1 yi xi

‚àá2 `(w> xi , yi ) = œÉ(yi w> xi ) 1 ‚àí œÉ(yi w> xi ) xi x>
i .
Under the assumption that kxi k2 ‚â§ 1 for all i, it is
straightforward to show that k‚àá`(w> xi , yi )k2 ‚â§ 1 and
that `00 (w> xi , yi ) is Œ≥-Lipschitz with Œ≥ = 1/4. This allows
us to apply Theorem 1 to logistic regression.

Multiple removals. The worst-case gradient residual
norm after T removals can be shown to scale linearly in
T . We can prove this using induction on T . The base
case, T = 1, is proven above. Suppose that the gradient
residual after T ‚â• 1 removals is uT with kuT k2 ‚â§ T 0 ,
where 0 is the gradient residual norm bound for a single removal. Let D(T ) be the training dataset with T
data points removed. Consider the modified loss function
(T )
Lb (w; D(T ) ) = Lb (w; D(T ) ) ‚àí u>
T w and let wT be the
approximate solution after T removals. Then wT is an ex(T )
act solution of Lb (w; D(T ) ), hence, the argument above
(T )
can be applied to Lb (w; D(T ) ) to show that the Newton
update approximation wT +1 has gradient residual u0 with
norm at most 0 . Then:
(T )

Data-dependent bound on gradient norm. The bound
in Theorem 1 contains a constant factor of 1/Œª2 and may
be too loose for practical applications on smaller datasets.
Fortunately, we can derive a data-dependent bound on the
gradient residual that can be efficiently computed and that is
much tighter in practice. Recall that the Hessian of L(¬∑; D0 )
has the form:
Hw = (X ‚àí )> Dw X ‚àí + Œª(n ‚àí 1)Id ,
where X ‚àí ‚àà R(n‚àí1)√ód is the data matrix corresponding
to D0 , Id is the identity matrix of size d√ód, and Dw is a
diagonal matrix containing values:

(Dw )ii = `00 w> xi , yi .
By Equation 4 we have that:
‚àí1
k‚àáL(w‚àí ; D0 )k2 = k(HwŒ∑ ‚àí Hw‚àó )Hw
‚àó ‚àÜk2
‚àí1
= k(X ‚àí )> (DwŒ∑ ‚àí Dw‚àó )X ‚àí Hw
‚àó ‚àÜk2
‚àí1
‚â§ kX ‚àí k2 kDwŒ∑ ‚àí Dw‚àó k2 kX ‚àí Hw
‚àó ‚àÜk2 .

The term kDwŒ∑ ‚àí Dw‚àó k2 corresponds to the maximum
singular value of a diagonal matrix, which in turn is the
maximum value among its diagonal entries. Given the Lipschitz constant Œ≥ of the second derivative `00 , we can thus
bound it as:
‚àí1
kDwŒ∑ ‚àí Dw‚àó k2 ‚â§ Œ≥kwŒ∑ ‚àí w‚àó k2 ‚â§ Œ≥kHw
‚àó ‚àÜk2 .

The following corollary summarizes this derivation.
‚àí1
Corollary 1. The Newton update w‚àí = w‚àó + Hw
‚àó ‚àÜ satisfies:
‚àí1
‚àí ‚àí1
k‚àáL(w‚àí ; D0 )k2 ‚â§ Œ≥kX ‚àí k2 kHw
Hw‚àó ‚àÜk2 ,
‚àó ‚àÜk2 kX

where Œ≥ is the Lipschitz constant of `00 .
The bound in Corollary 1 can be easily computed from the
‚àí1
‚àí
Newton update Hw
, the
‚àó ‚àÜ and the spectral norm of X
latter admitting efficient algorithms such as power iterations.

u0 = ‚àáw Lb (w; D(T ) ) = ‚àáw Lb (w; D(T ) ) ‚àí uT
‚áí 0 = ‚àáw Lb (w) ‚àí uT ‚àí u0 .
Thus the gradient residual for Lb (w; D(T ) ) after T + 1
removals is uT +1 := uT + u0 and its norm is at most
(T + 1)0 by the triangle inequality.
Batch removal. In certain scenarios, data removal may
not need to occur immediately after the data‚Äôs owner requests removal. This potentially allows for batch removals
in which multiple training samples are removed at once for
improved efficiency. The Newton update removal mechanism naturally supports this extension. Assume without loss
of generality that the batch of training data to be removed is
Dm = {(xn‚àím+1 , yn‚àím+1 ), . . . , (xn , yn )}. Define:
‚àÜ(m) = mŒªw‚àó +

n
X

‚àá`((w‚àó )> xj , yj )

j=n‚àím+1
(m)
Hw ‚àó

2

‚àó

= ‚àá L(w ; D \ Dm ).

The batch removal update is:
h
i‚àí1
(m)
w(‚àím) = w‚àó + Hw‚àó
‚àÜ(m) .

(6)

We derive bounds on the gradient residual norm for batch
removal that are similar Theorem 1 and Corollary 1.
Theorem 4. Under the same regularity conditions of Theorem 1, we have that:
k‚àáL(w(‚àím) ; D \ Dm )k2 ‚â§ Œ≥(n ‚àí m)

h

(m)

Hw‚àó

i‚àí1

2

‚àÜ(m)
2

4Œ≥m2 C 2
‚â§ 2
.
Œª (n ‚àí m)

Certified Data Removal from Machine Learning Models

Algorithm 1 Training of a certified removal-enabled model.
1: Input: Dataset D, loss `, parameters œÉ, Œª > 0.
2: Sample b ‚àº N (0, œÉ)d .
P
3: Return: argminw‚ààRd ni=1 `(w> xi ,yi )+Œªnkwk22 +b> w.

ing time, i.e., before the data to be removed is presented,
and only the inverse needs to be computed at removal time.
When computing the data-dependent bound, a similar tech‚àí1
nique can be used for calculating the term kX ‚àí Hw
‚àó ‚àÜk2
‚Äì which involves the product of the (n ‚àí 1) √ó d data matrix X ‚àí with a d-dimensional vector. We can reduce the
online component of this computation to O(d3 ) by forming the SVD of X offline and applying online down-dates
(Gu and Eisenstat, 1995) to form the SVD of X ‚àí by solving an eigen-decomposition problem on a d √ó d matrix. It
can be shown that this technique reduces the computation
‚àí1
of kX ‚àí Hw
‚àó ‚àÜk2 to involve only d √ó d matrices and ddimensional vectors, which enables the online computation
cost to be independent of n.

Algorithm 2 Repeated certified removal of data batches.
1: Input: Dataset D, loss `, parameters , Œ¥, œÉ, Œª > 0.
2:
Lipschitz constant Œ≥ of `00 .
3:
Solution w computed by Algorithm 1.
4:
Sequence of batches of training sample
5:
indices to be removed: B1 , B2 , . . .
6: Gradient
p residual bound Œ≤ ‚Üê 0.
2 log(1.5/Œ¥).
7: c ‚Üê P
n
8: K ‚Üê i=1 xi x>
i .
9: X ‚Üê [x1 |x2 | ¬∑ ¬∑ ¬∑ |xn ]> .
10: for j = 1, 2, . . . do
P
11:
‚àÜ ‚Üê |Bj |Œªw + i‚ààBj ‚àá`(w> xi , yi ).
P
2
>
12:
H ‚Üê i:i‚ààB
/ 1 ,B2 ,...,Bj ‚àá `(w xi , yi ).
13:
X ‚Üê remove
P rows(X, Bj ).
14:
K ‚Üê K ‚àí i‚ààBj xi x>
i .
p
15:
Œ≤ ‚Üê Œ≤ + Œ≥ kKk2 ¬∑ kH ‚àí1 ‚àÜk2 ¬∑ kXH ‚àí1 ‚àÜk2 .
16:
if Œ≤ > œÉ/c then
17:
Re-train from scratch using Algorithm 1.
18:
else
19:
w ‚Üê w + H ‚àí1 ‚àÜ.
20:
end if
21: end for
Corollary 2. The Newton update w(‚àím)
h
i‚àí1
(m)
Hw‚àó
‚àÜ(m) satisfies:
kL(w(‚àím) ; D \ Dm )k2 ‚â§
h
i‚àí1
(m)
Œ≥kX (‚àím) k2 Hw‚àó
‚àÜ(m)

Pseudo-code. We present pseudo-code for training
removal-enabled models and for the (, Œ¥)-CR Newton update mechanism. During training (Algorithm 1), we add
a random linear term to the training loss by sampling a
Gaussian noise vector b. The choice of œÉ determines a
‚Äúremoval budget‚Äù according to Theorem 3: the maximum
gradient residual norm that can be incurred is œÉ/c. When
optimizing the training loss, any optimizer with convergence
guarantee for strongly convex loss functions can be used to
find the minimizer in Algorithm 1. We use L-BFGS (Liu
and Nocedal, 1989) in our experiments as it was the most
efficient of the optimizers we tried.

= w‚àó +

h
i‚àí1
(m)
X (‚àím) Hw‚àó
‚àÜ(m)
2

During removal (line 19 in Algorithm 2), we apply the
batch Newton update (Equation 6) and compute the gradient
residual norm bound using Corollary 2 (line 15 in Algorithm
2). The variable Œ≤ accumulates the gradient residual norm
over all removals. If the pre-determined budget of œÉ/c
is exceeded, we train a new removal-enabled model from
scratch using Algorithm 1 on the remaining data points.
,

2

where X (‚àím) is the data matrix for D \ Dm and Œ≥ is the
Lipschitz constant of `00 .
Interestingly, the gradient residual bound in Theorem 4
scales quadratically with the number of removals, as opposed to linearly when removing examples one-by-one. This
increase in error is due to a more crude approximation of
the Hessian, that is, we compute the Hessian only once at
the current solution w‚àó rather than once per removed data.
Reducing online computation. The Newton update requires forming and inverting the Hessian. Although the
O(d3 ) cost of inversion is relatively limited for small d and
inversion can be done efficiently on GPUs, the cost of forming the Hessian is O(d2 n), which may be problematic for
large datasets. However, the Hessian can be formed at train-

3.3. Non-Linear Models
Deep learning models often apply a linear model to features
extracted by a network pre-trained on a public dataset like
ImageNet (Ren et al., 2015; He et al., 2017; Zhao et al.,
2017; Carreira and Zisserman, 2017) for vision tasks, or
from language model trained on public text corpora (Devlin
et al., 2019; Dai et al., 2019; Yang et al., 2019; Liu et al.,
2019) for natural language tasks. In such setups, we only
need to worry about data removal from the linear model that
is applied to the output of the feature extractor.
When feature extractors are trained on private data as well,
we can use our certified-removal mechanism on linear models that are applied to the output of a differentially-private
feature extraction network (Abadi et al., 2016).
Theorem 5. Suppose Œ¶ is a randomized learning algorithm that is (DP , Œ¥DP )-differentially private, and the out-

Certified Data Removal from Machine Learning Models

Dataset
Removal setting
Removal time

MNIST (¬ß4.1)

LSUN (¬ß4.2)

SST (¬ß4.2)

SVHN (¬ß4.3)

CR Linear
0.04s

Public Extractor + CR Linear
0.48s

Public Extractor + CR Linear
0.07s

DP Extractor + CR Linear
0.27s

15.6s

124s

61.5s

1.5h

Training time

Table 1. Summary of removal and training times observed in our experiments. For LSUN and SST, the public extractor is trained on
a public dataset and hence removal is only applied to the linear model. For SVHN, removal is applied to a linear model that operates
on top of a differentially private feature extractor. In all cases, using the Newton update to (certifiably) remove data is several orders of
magnitude faster than re-training the model from scratch.
<latexit sha1_base64="xfyl4e7TxNt7cmGMK5ZbuRU4wQ4=">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0EYI2lhGMDGQHGFus0mW7O6du3tCOPInbCwUsfXv2Plv3CRXaOKDgcd7M8zMixLBjfX9b6+wsrq2vlHcLG1t7+zulfcPmiZONWUNGotYtyI0THDFGpZbwVqJZigjwR6i0c3Uf3hi2vBY3dtxwkKJA8X7nKJ1Uqtj+EDiVdAtV/yqPwNZJkFOKpCj3i1/dXoxTSVTlgo0ph34iQ0z1JZTwSalTmpYgnSEA9Z2VKFkJsxm907IiVN6pB9rV8qSmfp7IkNpzFhGrlOiHZpFbyr+57VT278MM66S1DJF54v6qSA2JtPnSY9rRq0YO4JUc3croUPUSK2LqORCCBZfXibNs2rgV4O780rtOo+jCEdwDKcQwAXU4Bbq0AAKAp7hFd68R+/Fe/c+5q0FL585hD/wPn8AkhuPpQ==</latexit>

95

95

95

85
Œª = 10‚àí4

80

Œª = 10‚àí3
Œª = 10‚àí2
Œª = 10‚àí1

75
10

‚àí2

10

‚àí1

90
85
Œª = 10‚àí4

80

Œª = 10‚àí3
Œª = 10‚àí2
Œª = 10‚àí1

75

10
œÉ

0

10

1

10

2

7est Accuracy

7est Accuracy

7est Accuracy

90

10

‚àí3

10

‚àí2

10

‚àí1

0

10
Œµ

10

1

10

2

10

3

10

= 0.01

<latexit sha1_base64="NTC3SikE07hq8VhsAGpuLIoEIno=">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0EYI2lhGMDGYHGFus5cs2d07dveEEPIvbCwUsfXf2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wsrq2vlHcLG1t7+zulfcPmibJNGUNmohEtyI0THDFGpZbwVqpZigjwR6i4c3Uf3hi2vBE3dtRykKJfcVjTtE66bFjeF/ilV8NuuWKX/VnIMskyEkFctS75a9OL6GZZMpSgca0Az+14Ri15VSwSamTGZYiHWKftR1VKJkJx7OLJ+TEKT0SJ9qVsmSm/p4YozRmJCPXKdEOzKI3Ff/z2pmNL8MxV2lmmaLzRXEmiE3I9H3S45pRK0aOINXc3UroADVS60IquRCCxZeXSfOsGrjE7s4rtes8jiIcwTGcQgAXUINbqEMDKCh4hld484z34r17H/PWgpfPHMIfeJ8/cGqQFw==</latexit>

= 0.1

= 10

90
85
80
75

4

=1
<latexit sha1_base64="KxabckYkoAwGJzQXep4i6uuBE3A=">AAAB8HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0EYI2lhGMDGSHGFvs5cs2d07dueEEPIrbCwUsfXn2Plv3CRXaOKDgcd7M8zMi1IpLPr+t1dYWV1b3yhulra2d3b3yvsHTZtkhvEGS2RiWhG1XArNGyhQ8lZqOFWR5A/R8GbqPzxxY0Wi73GU8lDRvhaxYBSd9Nixoq/oVeB3yxW/6s9AlkmQkwrkqHfLX51ewjLFNTJJrW0HforhmBoUTPJJqZNZnlI2pH3edlRTxW04nh08ISdO6ZE4Ma40kpn6e2JMlbUjFblORXFgF72p+J/XzjC+DMdCpxlyzeaL4kwSTMj0e9IThjOUI0coM8LdStiAGsrQZVRyIQSLLy+T5lk18KvB3Xmldp3HUYQjOIZTCOACanALdWgAAwXP8ApvnvFevHfvY95a8PKZQ/gD7/MHAreP3w==</latexit>

<latexit sha1_base64="iVZ1HvEFassixQIymUo0vP5jAcA=">AAAB8nicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoBuh6MZlBfuA6VAyaaYNzWNIMkIZ+hluXCji1q9x59+YtrPQ1gMhh3Pu5d574pQzY33/2yutrW9sbpW3Kzu7e/sH1cOjtlGZJrRFFFe6G2NDOZO0ZZnltJtqikXMaSce3838zhPVhin5aCcpjQQeSpYwgq2Twp5hQ4Fv/Lof9Ks1982BVklQkBoUaParX72BIpmg0hKOjQkDP7VRjrVlhNNppZcZmmIyxkMaOiqxoCbK5ytP0ZlTBihR2j1p0Vz93ZFjYcxExK5SYDsyy95M/M8LM5tcRzmTaWapJItBScaRVWh2PxowTYnlE0cw0cztisgIa0ysS6niQgiWT14l7Yt64NeDh8ta47aIowwncArnEMAVNOAemtACAgqe4RXePOu9eO/ex6K05BU9x/AH3ucP4ViQUQ==</latexit>

Œª = 10‚àí4
Œª = 10‚àí3

<latexit sha1_base64="XODDCn4YQcuyDxlrzavRuRPWHQQ=">AAAB8XicbVBNSwMxEJ2tX7V+VT16CRbBU8mKUC9C0YvHCvYD26Vk02wbmmSXJCuUpf/CiwdFvPpvvPlvTNs9aOuDgcd7M8zMCxPBjcX42yusrW9sbhW3Szu7e/sH5cOjlolTTVmTxiLWnZAYJrhiTcutYJ1EMyJDwdrh+Hbmt5+YNjxWD3aSsECSoeIRp8Q66bFn+FCSax/jfrmCq3gOtEr8nFQgR6Nf/uoNYppKpiwVxJiujxMbZERbTgWblnqpYQmhYzJkXUcVkcwE2fziKTpzygBFsXalLJqrvycyIo2ZyNB1SmJHZtmbif953dRGV0HGVZJapuhiUZQKZGM0ex8NuGbUiokjhGrubkV0RDSh1oVUciH4yy+vktZF1cdV//6yUr/J4yjCCZzCOfhQgzrcQQOaQEHBM7zCm2e8F+/d+1i0Frx85hj+wPv8AXN2kBk=</latexit>

= 100

Œª = 10‚àí2
Œª = 10‚àí1

10‚àí2 10‚àí1 100 101 102 103 104 105
ExSected # Rf SuSSRrted 5emRvals

Figure 1. Linear logistic regression on MNIST. Left: Effect of L2 -regularization parameter, Œª, and standard deviation of the objective
perturbation, œÉ, on test accuracy. Middle: Effect of  on test accuracy when supporting 100 removals. Right: Trade-off between accuracy
and supported number of removals at  = 1. At a given , higher Œª and œÉ values reduce test accuracy but allow for many more removals.

puts of Œ¶ are used in a linear model by minimizing Lb
and using a removal mechanism that guarantees (CR , Œ¥CR )certified removal. Then the entire procedure guarantees
(DP +CR , Œ¥DP +Œ¥CR )-certified removal.
The advantage of this approach over training the entire network in a differentially private manner (Abadi et al., 2016)
is that the (removal-enabled) linear model can be trained
using a much smaller perturbation, which may greatly boost
the accuracy of the final model (see Section 4.3).

4. Experiments
We test our certified removal mechanism in three settings: (1) removal from a standard linear logistic
regressor, (2) removal from a linear logistic regressor that uses a feature extractor pre-trained on public data, and (3) removal from a non-linear logistic regressor by using a differentially private feature extractor. Code reproducing the results of our experiments
is publicly available from https://github.com/
facebookresearch/certified-removal. Table
1 summarizes the training and removal times measured in
our experiments.
4.1. Linear Logistic Regression
We first experiment on the MNIST digit classification
dataset. For simplicity, we restrict to the binary classification problem of distinguishing between digits 3 and 8,
and train a regularized logistic regressor using Algorithm 1.

Removal is performed using Algorithm 2 with Œ¥ = 1e-4.
Effects of Œª and œÉ. Training a removal-enabled model using Algorithm 1 requires selecting two hyperparameters: the
L2 -regularization parameter, Œª, and the standard deviation,
œÉ, of the sampled perturbation vector b. Figure 1 shows the
effect of Œª and œÉ on test accuracy and the expected number of removals supported before re-training. When fixing
the supported number of removals at 100 (middle plot), the
value of œÉ is inversely related to  (cf. line 16 of Algorithm
2), hence higher  results in smaller œÉ and improved accuracy. Increasing Œª enables more removals before re-training
(left and right plots) because it reduces the gradient residual norm, but very high values of Œª negatively affect test
accuracy because the regularization term dominates the loss.
Tightness of the gradient residual norm bounds. In Algorithm 2 , we use the data-dependent bound from Corollaries 1 and 2 to compute a per-data or per-batch estimate
of the removal error, as opposed to the worst-case bound
in Theorems 1 and 4. Figure 2 shows the value of different bounds as a function of the number of removed points.
We consider two removal scenarios: single point removal
and batch removal with batch size m = 10. We observe
three phenomena: (1) The worst-case bounds (light blue and
light green) are several orders of magnitude higher than the
data-dependent bounds (dark blue and dark green), which
means that the number of supported removals is several
orders of magnitude higher when using the data-dependent
bounds. (2) The cumulative sum of the gradient residual

Certified Data Removal from Machine Learning Models

For SST, we extract features using a pre-trained RoBERTa
(Liu et al., 2019) language model. At removal time, we use
Algorithm 2 with  = 1 and Œ¥ = 1e-4 in both experiments.

106

Gradient Residual Norm

104

Result on LSUN. We reduce the 10-way LSUN classification task to 10 one-versus-all tasks and randomly subsample
the negative examples to ensure the positive and negative
classes are balanced in all binary classification problems.
Subsampling benefits removal since a training sample does
not always need to be removed from all 10 classifiers.

102

100

10‚àí2

10‚àí4
0

200

Worst-case single
(Theorem 1)
Data-dependent single
(Corollary 1)

400
600
# of Removals

800

Worst-case batch
(Theorem 3)
Data-dependent batch
(Corollary 2)

1000

True value

Figure 2. Linear logistic regression on MNIST. Gradient residual norm (on log scale) as a function of the number of removals.

norm bounds is approximately linear for both the single and
batch removal data-dependent bounds. (3) There remains
a large gap between the data-dependent norm bounds and
the true value of the gradient residual norm (dashed line),
which suggests that the utility of our removal mechanism
may be further improved via tighter analysis.
Gradient residual norm and removal difficulty. The
data-dependent bound is governed by the norm of the up‚àí1
date Hw
‚àó ‚àÜ, which measures the influence of the removed
point on the parameters and varies greatly depending on the
training sample being removed. Figure 3 shows the training
samples corresponding to the 10 largest and smallest values
‚àí1
of kHw
‚àó ‚àÜk2 . There are large visual differences between
these samples: large values correspond to oddly-shaped 3s
and 8s, while small values correspond to ‚Äúprototypical‚Äù digits. This suggests that removing outliers is harder, because
the model tends to memorize their details and their impact
on the model is easy to distinguish from other samples.
4.2. Non-Linear Logistic Regression using
Public, Pre-Trained Feature Extractors
We consider the common scenario in which a feature extractor is trained on public data (i.e., does not require removal),
and a linear classifier is trained on these features using nonpublic data. We study two tasks: (1) scene classification
on the LSUN dataset and (2) sentiment classification on the
Stanford Sentiment Treebank (SST) dataset. We subsample
the LSUN dataset to 100K images per class (i.e., n = 1M).
For LSUN, we extract features using a ResNeXt-101 model
(Xie et al., 2017) trained on 1B Instagram images (Mahajan
et al., 2018) and fine-tuned on ImageNet (Deng et al., 2009).

Figure 4 (left) shows the relationship between test accuracy
and the expected number of removals on LSUN. The value
of (Œª, œÉ) is shown next to each point, with the left-most
point corresponding to training a regular model that supports
no removal. At the cost of a small drop in accuracy (from
88.6% to 83.3%), the model supports over 10, 000 removals
before re-training is needed. As shown in Table 1, the
computational cost for removal is more than 250√ó smaller
than re-training the model on the remaining data points.
Result on SST. SST is a sentiment classification dataset
commonly used for benchmarking language models (Wang
et al., 2019). We use SST in the binary classification task
of predicting whether or not a movie review is positive.
Figure 4 (right) shows the trade-off between accuracy and
supported number of removals. The regular model (leftmost point) attains a test accuracy of 89.0%, which matches
the performance of competitive prior work (Tai et al., 2015;
Wieting et al., 2016; Looks et al., 2017). As before, a large
number of removals is supported at a small loss in test
accuracy; the computational costs for removal are 870√ó
lower than for re-training the model.
4.3. Non-linear Logistic Regression using
Differentially Private Feature Extractors
When public data is not available for training a feature extractor, we can train a differentially private feature extractor
on private data (Abadi et al., 2016) and apply Theorem 5 to
remove data from the final (removal-enabled) linear layer.
This approach has a major advantage over training the entire
model using the approach of (Abadi et al., 2016) because
the final linear layer can partly correct for the noisy features
produced by the private feature extractor.
We evaluate this approach on the Street View House Numbers (SVHN) digit classification dataset. We compare it to
a differentially private CNN2 trained using the technique
of Abadi et al. (2016). Since the CNN is differentially private, certified removal is achieved trivially without applying
any removal. For a fair comparison, we fix Œ¥ = 1e-4 and
train (DP /10, Œ¥)-private CNNs for a range of values of DP .
2
We use a simple CNN with two convolutional layers with 64
filters of size 3 √ó 3 and 2 √ó 2 max-pooling.

Certified Data Removal from Machine Learning Models

Top 10

Bottom 10
Figure 3. MNIST training digits sorted by norm of the removal update kH‚àí1
w‚àó ‚àÜk2 . The samples with the highest norm (top) appear
to be atypical, making it harder to undo their effect on the model. The samples with the lowest norm (bottom) are prototypical 3s and 8s,
and hence are much easier to remove.

<latexit sha1_base64="kBFh0xpOxXxGNqNfdD6R7fllkBo=">AAAB+3icbVDJSgNBEO2JW4xbjEcvjUGIoGEmEPQY9OIxglkgGUJPpyZp0rPQXSMJQ37FiwdFvPoj3vwbO8tBEx8UPN6roqqeF0uh0ba/rczG5tb2TnY3t7d/cHiUPy40dZQoDg0eyUi1PaZBihAaKFBCO1bAAk9CyxvdzfzWEygtovARJzG4ARuEwhecoZF6+UIXYYyIacmBq+olrV5Me/miXbbnoOvEWZIiWaLey391+xFPAgiRS6Z1x7FjdFOmUHAJ01w30RAzPmID6BgasgC0m85vn9Jzo/SpHylTIdK5+nsiZYHWk8AznQHDoV71ZuJ/XidB/8ZNRRgnCCFfLPITSTGisyBoXyjgKCeGMK6EuZXyIVOMo4krZ0JwVl9eJ81K2bHLzkOlWLtdxpElp+SMlIhDrkmN3JM6aRBOxuSZvJI3a2q9WO/Wx6I1Yy1nTsgfWJ8/HMGTKA==</latexit>

(1e-4, 20)

86

<latexit sha1_base64="Q0eObet+ZhWs0r2EhFdnPaezVXQ=">AAAB+3icbVDLSgNBEJyNrxhfazx6GQxCBA27OajHoBePEcwDkiXMTjrJkNkHM72SsORXvHhQxKs/4s2/cZLsQaMFDUVVN91dfiyFRsf5snJr6xubW/ntws7u3v6BfVhs6ihRHBo8kpFq+0yDFCE0UKCEdqyABb6Elj++nfutR1BaROEDTmPwAjYMxUBwhkbq2cUuwgQR07ILF5fn1Dmb9eySU3EWoH+Jm5ESyVDv2Z/dfsSTAELkkmndcZ0YvZQpFFzCrNBNNMSMj9kQOoaGLADtpYvbZ/TUKH06iJSpEOlC/TmRskDraeCbzoDhSK96c/E/r5Pg4NpLRRgnCCFfLhokkmJE50HQvlDAUU4NYVwJcyvlI6YYRxNXwYTgrr78lzSrFdepuPfVUu0miyNPjskJKROXXJEauSN10iCcTMgTeSGv1sx6tt6s92VrzspmjsgvWB/fFqyTJA==</latexit>

7est Accuracy

7est Accuracy

(1e-5, 5)
<latexit sha1_base64="M5sHhFU7xVeCKnqkv0ilOMeztW0=">AAAB/HicbVDLSgNBEJyNrxhfqzl6GQxCBA27QdBj0IvHCOYByRJmJ51kyOyDmV4xLPFXvHhQxKsf4s2/cZLsQaMFDUVVN91dfiyFRsf5snIrq2vrG/nNwtb2zu6evX/Q1FGiODR4JCPV9pkGKUJooEAJ7VgBC3wJLX98PfNb96C0iMI7nMTgBWwYioHgDI3Us4tdhAdETMsunJ2f0qpzMu3ZJafizEH/EjcjJZKh3rM/u/2IJwGEyCXTuuM6MXopUyi4hGmhm2iIGR+zIXQMDVkA2kvnx0/psVH6dBApUyHSufpzImWB1pPAN50Bw5Fe9mbif14nwcGll4owThBCvlg0SCTFiM6SoH2hgKOcGMK4EuZWykdMMY4mr4IJwV1++S9pViuuU3Fvq6XaVRZHnhySI1ImLrkgNXJD6qRBOJmQJ/JCXq1H69l6s94XrTkrmymSX7A+vgGKDJNe</latexit>

(2e-4, 30)
<latexit sha1_base64="hqxDPS1xKJXDvu0qblSzeuksDr0=">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmCnoMevEYwSyQDKGnU5M06VnorhHDEH/FiwdFvPoh3vwbO8tBEx8UPN6roqqeF0uh0ba/rczK6tr6RnYzt7W9s7uX3z9o6ChRHOo8kpFqeUyDFCHUUaCEVqyABZ6Epje8mfjNB1BaROE9jmJwA9YPhS84QyN184UOwiMipqUKnF2c0nP7ZNzNF+2yPQVdJs6cFMkctW7+q9OLeBJAiFwyrduOHaObMoWCSxjnOomGmPEh60Pb0JAFoN10evyYHhulR/1ImQqRTtXfEykLtB4FnukMGA70ojcR//PaCfpXbirCOEEI+WyRn0iKEZ0kQXtCAUc5MoRxJcytlA+YYhxNXjkTgrP48jJpVMqOXXbuKsXq9TyOLDkkR6REHHJJquSW1EidcDIiz+SVvFlP1ov1bn3MWjPWfKZA/sD6/AGNIJNg</latexit>

85

87
86

(1e-5, 5)
<latexit sha1_base64="kBFh0xpOxXxGNqNfdD6R7fllkBo=">AAAB+3icbVDJSgNBEO2JW4xbjEcvjUGIoGEmEPQY9OIxglkgGUJPpyZp0rPQXSMJQ37FiwdFvPoj3vwbO8tBEx8UPN6roqqeF0uh0ba/rczG5tb2TnY3t7d/cHiUPy40dZQoDg0eyUi1PaZBihAaKFBCO1bAAk9CyxvdzfzWEygtovARJzG4ARuEwhecoZF6+UIXYYyIacmBq+olrV5Me/miXbbnoOvEWZIiWaLey391+xFPAgiRS6Z1x7FjdFOmUHAJ01w30RAzPmID6BgasgC0m85vn9Jzo/SpHylTIdK5+nsiZYHWk8AznQHDoV71ZuJ/XidB/8ZNRRgnCCFfLPITSTGisyBoXyjgKCeGMK6EuZXyIVOMo4krZ0JwVl9eJ81K2bHLzkOlWLtdxpElp+SMlIhDrkmN3JM6aRBOxuSZvJI3a2q9WO/Wx6I1Yy1nTsgfWJ8/HMGTKA==</latexit>

85

(1e-4, 10)
<latexit sha1_base64="/dmzMgyleY0gGBvb1IYYTmeciYM=">AAAB/HicbVDLSgNBEJyNrxhfqzl6GQxCBA27QdBj0IvHCOYByRJmJ51kyOyDmV4xLPFXvHhQxKsf4s2/cZLsQaMFDUVVN91dfiyFRsf5snIrq2vrG/nNwtb2zu6evX/Q1FGiODR4JCPV9pkGKUJooEAJ7VgBC3wJLX98PfNb96C0iMI7nMTgBWwYioHgDI3Us4tdhAdETMsunJ2fUtc5mfbsklNx5qB/iZuREslQ79mf3X7EkwBC5JJp3XGdGL2UKRRcwrTQTTTEjI/ZEDqGhiwA7aXz46f02Ch9OoiUqRDpXP05kbJA60ngm86A4UgvezPxP6+T4ODSS0UYJwghXywaJJJiRGdJ0L5QwFFODGFcCXMr5SOmGEeTV8GE4C6//Jc0qxXXqbi31VLtKosjTw7JESkTl1yQGrkhddIgnEzIE3khr9aj9Wy9We+L1pyVzRTJL1gf34iFk10=</latexit>

84

(2e-4, 10)

83

(5e-4, 40)

84

90

<latexit sha1_base64="5KpZfeq6EjwjRbYBwicWuFxeKUs=">AAAB/HicbVDLSgNBEJyNrxhfqzl6GQxCBA27QdBj0IvHCOYByRJmJ51kyOyDmV4xLPFXvHhQxKsf4s2/cZLsQaMFDUVVN91dfiyFRsf5snIrq2vrG/nNwtb2zu6evX/Q1FGiODR4JCPV9pkGKUJooEAJ7VgBC3wJLX98PfNb96C0iMI7nMTgBWwYioHgDI3Us4tdhAdETMtVODs/pa5zMu3ZJafizEH/EjcjJZKh3rM/u/2IJwGEyCXTuuM6MXopUyi4hGmhm2iIGR+zIXQMDVkA2kvnx0/psVH6dBApUyHSufpzImWB1pPAN50Bw5Fe9mbif14nwcGll4owThBCvlg0SCTFiM6SoH2hgKOcGMK4EuZWykdMMY4mr4IJwV1++S9pViuuU3Fvq6XaVRZHnhySI1ImLrkgNXJD6qRBOJmQJ/JCXq1H69l6s94XrTkrmymSX7A+vgGKEpNe</latexit>

80
70
60
Non-private Baseline
DP Extractor + CR Linear
DP Extractor + DP Linear

50

82

<latexit sha1_base64="f8zNMLsnKCsvsags7OgDRqCvcQs=">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmRPQY9OIxglkgGUJPp5I06VnorhGHIf6KFw+KePVDvPk3dpaDRh8UPN6roqqeF0mh0ba/rMzK6tr6RnYzt7W9s7uX3z9o6jBWHBo8lKFqe0yDFAE0UKCEdqSA+Z6Elje+nvqte1BahMEdJhG4PhsGYiA4QyP18oUuwgMipqVzOKue0qp9Munli3bZnoH+Jc6CFMkC9V7+s9sPeexDgFwyrTuOHaGbMoWCS5jkurGGiPExG0LH0ID5oN10dvyEHhulTwehMhUgnak/J1Lma534nun0GY70sjcV//M6MQ4u3VQEUYwQ8PmiQSwphnSaBO0LBRxlYgjjSphbKR8xxTiavHImBGf55b+kWSk7dtm5rRRrV4s4suSQHJESccgFqZEbUicNwklCnsgLebUerWfrzXqft2asxUyB/IL18Q2TTpNk</latexit>

(1e-3, 50)
<latexit sha1_base64="lsr/Yvdsmx7W68nEoF+SPbUw3dw=">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmInoMevEYwSyQDKGnU5M06VnorhHDEH/FiwdFvPoh3vwbO8tBEx8UPN6roqqeF0uh0ba/rczK6tr6RnYzt7W9s7uX3z9o6ChRHOo8kpFqeUyDFCHUUaCEVqyABZ6Epje8mfjNB1BaROE9jmJwA9YPhS84QyN184UOwiMipiUHzs5P6YV9Mu7mi3bZnoIuE2dOimSOWjf/1elFPAkgRC6Z1m3HjtFNmULBJYxznURDzPiQ9aFtaMgC0G46PX5Mj43So36kTIVIp+rviZQFWo8Cz3QGDAd60ZuI/3ntBP0rNxVhnCCEfLbITyTFiE6SoD2hgKMcGcK4EuZWygdMMY4mr5wJwVl8eZk0KmXHLjt3lWL1eh5HlhySI1IiDrkkVXJLaqROOBmRZ/JK3qwn68V6tz5mrRlrPlMgf2B9/gCNF5Ng</latexit>

0

SST

(1e-6, 0)

88

Test Accuracy

<latexit sha1_base64="Q0eObet+ZhWs0r2EhFdnPaezVXQ=">AAAB+3icbVDLSgNBEJyNrxhfazx6GQxCBA27OajHoBePEcwDkiXMTjrJkNkHM72SsORXvHhQxKs/4s2/cZLsQaMFDUVVN91dfiyFRsf5snJr6xubW/ntws7u3v6BfVhs6ihRHBo8kpFq+0yDFCE0UKCEdqyABb6Elj++nfutR1BaROEDTmPwAjYMxUBwhkbq2cUuwgQR07ILF5fn1Dmb9eySU3EWoH+Jm5ESyVDv2Z/dfsSTAELkkmndcZ0YvZQpFFzCrNBNNMSMj9kQOoaGLADtpYvbZ/TUKH06iJSpEOlC/TmRskDraeCbzoDhSK96c/E/r5Pg4NpLRRgnCCFfLhokkmJE50HQvlDAUU4NYVwJcyvlI6YYRxNXwYTgrr78lzSrFdepuPfVUu0miyNPjskJKROXXJEauSN10iCcTMgTeSGv1sx6tt6s92VrzspmjsgvWB/fFqyTJA==</latexit>

87

89

LSUN

(1e-6, 0)
88

2000 4000 6000 8000 10000
ExSected # Rf 6uSSRrted 5emRvals

81

(5e-4, 10)
<latexit sha1_base64="v3nkArFRn06y/E8xLgKVibJtCV0=">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmKHoMevEYwSyQDKGnU5M06VnorhHDEH/FiwdFvPoh3vwbO8tBEx8UPN6roqqeF0uh0ba/rczK6tr6RnYzt7W9s7uX3z9o6ChRHOo8kpFqeUyDFCHUUaCEVqyABZ6Epje8mfjNB1BaROE9jmJwA9YPhS84QyN184UOwiMipqULODs/pY59Mu7mi3bZnoIuE2dOimSOWjf/1elFPAkgRC6Z1m3HjtFNmULBJYxznURDzPiQ9aFtaMgC0G46PX5Mj43So36kTIVIp+rviZQFWo8Cz3QGDAd60ZuI/3ntBP0rNxVhnCCEfLbITyTFiE6SoD2hgKMcGcK4EuZWygdMMY4mr5wJwVl8eZk0KmXHLjt3lWL1eh5HlhySI1IiDrkkVXJLaqROOBmRZ/JK3qwn68V6tz5mrRlrPlMgf2B9/gCOuZNh</latexit>

0

40

200
400
600
800 1000 1200
ExSected # Rf 6uSSRrted 5emRvals

Figure 4. Linear models trained on public feature extractors. Trade-off between
test accuracy and the expected number of supported removals (at  = 1) on LSUN (left)
and SST (right). The setting of (Œª, œÉ) is shown next to each point. The number of
supported removals rapidly increases when accuracy is slightly sacrificed.

By the union bound for group privacy (Dwork, 2011), the
resulting models support up to then (DP , Œ¥)-CR removals.
To measure the effectiveness of Theorem 5 for certified data
removal, we also train an (DP /10, Œ¥/10)-differentially private CNN and extract features from its penultimate linear.
We use these features in Algorithm 1 to train 10 one-versus9
all classifiers with total failure probability of at most 10
Œ¥.
Akin to the experiments on LSUN, we subsample the negative examples in each of the binary classifiers to speed up
removal. The expected contribution to  from the updates
is set to CR ‚âà DP /10, hence achieving (, Œ¥)-CR with
 = DP + CR ‚âà DP + DP /10 after 10 removals.
Figure 5 shows the relationship between test accuracy and
 for both the fully private and the Newton update removal
methods. For reference, the dashed line shows the accuracy obtained by a non-private CNN that does not support
removal. For smaller values or , training a private feature
extractor (blue) and training the linear layer using Algorithm 1 attains much higher test accuracy than training a
fully differentially private model (orange). In particular, at
 ‚âà 0.1, the fully differentially private baseline‚Äôs accuracy
is only 22.7%, whereas our approach attains a test accuracy
of 71.2%. Removal from the linear model trained on top
of the private extractor only takes 0.27s, compared to more
than 1.5 hour when re-training the CNN from scratch.

0

1

2
3
Œµ = ŒµDP + ŒµCR

4

5

6

Figure 5. Using -DP features. Trade-off between  and test accuracy on SVHN of models
that support 10 removals. Dashed line shows
non-private model accuracy.

5. Related Work
Removal of specific training samples from models has been
studied in prior work on decremental learning (Cauwenberghs and Poggio, 2000; Karasuyama and Takeuchi, 2009;
Tsai et al., 2014) and machine unlearning (Cao and Yang,
2015). Ginart et al. (2019) studied the problem of removing
data from k-means clusterings. These studies aim at exact
removal of one or more training samples from a trained
model: their success measure is closeness to the optimal parameter or objective value. This suffices for purposes such
as quickly evaluating the leave-one-out error or correcting
mislabeled data, but it does not provide a formal guarantee
of statistical indistinguishability. Our work leverages differential privacy to develop a more rigorous definition of data
removal. Concurrent work (Bourtoule et al., 2019) presents
an approach that allows certified removal with  = 0.
Our definition of certified removal uses the same notion of
indistinguishability as that of differential privacy. Many
classical machine learning algorithms have been shown
to support differentially private versions, including PCA
(Chaudhuri et al., 2012), matrix factorization (Liu et al.,
2015), linear models (Chaudhuri et al., 2011), and neural
networks (Abadi et al., 2016). Our removal mechanism
can be viewed on a spectrum of noise addition techniques
for preserving data privacy, balancing between computation time for the removal mechanism and model utility. We
hope to further explore the connections between differen-

Certified Data Removal from Machine Learning Models

tial privacy and certified removal in follow-up work to design certified-removal algorithms with better guarantees and
computational efficiency.

6. Conclusion
We have studied a mechanism that quickly ‚Äúremoves‚Äù data
from a machine-learning model up to a differentially private guarantee: the model after removal is indistinguishable
from a model that never saw the removed data to begin with.
While we demonstrate that this mechanism is practical in
some settings, at least four challenges for future work remain. (1) The Newton update removal mechanism requires
inverting the Hessian matrix, which may be problematic.
Methods that approximate the Hessian with near-diagonal
matrices may address this problem. (2) Removal from models with non-convex losses is unsupported; it may require
local analysis of the loss surface to show that data points
do not move the model out of a local optimum. (3) There
remains a large gap between our data-dependent bound and
the true gradient residual norm, necessitating a tighter analysis. (4) Some applications may require the development of
alternative, less constraining notions of data removal.

References
Abadi, M., Chu, A., Goodfellow, I. J., McMahan, H. B.,
Mironov, I., Talwar, K., and Zhang, L. (2016). Deep
learning with differential privacy. In Proceedings of the
2016 ACM SIGSAC Conference on Computer and Communications Security, Vienna, Austria, October 24-28,
2016, pages 308‚Äì318.
Biggio, B., Nelson, B., and Laskov, P. (2012). Poisoning
attacks against support vector machines. pages 1467‚Äì
1474.
Bourtoule, L., Chandrasekaran, V., Choquette-Choo, C.,
Jia, H., Travers, A., Zhang, B., Lie, D., and Papernot, N.
(2019). Machine unlearning. In arXiv 1912.03817.
Cao, Y. and Yang, J. (2015). Towards making systems forget
with machine unlearning. In 2015 IEEE Symposium on
Security and Privacy, SP 2015, San Jose, CA, USA, May
17-21, 2015, pages 463‚Äì480.
Carlini, N., Liu, C., Kos, J., Erlingsson, U., and Song, D.
(2019). The secret sharer: Measuring unintended neural
network memorization & extracting secrets. In USENIX
Security Symposium, pages 267‚Äì284.
Carreira, J. and Zisserman, A. (2017). Quo vadis, action
recognition? A new model and the kinetics dataset. CoRR,
abs/1705.07750.
Cauwenberghs, G. and Poggio, T. A. (2000). Incremental
and decremental support vector machine learning. In Advances in Neural Information Processing Systems 13, Papers from Neural Information Processing Systems (NIPS)
2000, Denver, CO, USA, pages 409‚Äì415.
Chaudhuri, K., Monteleoni, C., and Sarwate, A. D. (2011).
Differentially private empirical risk minimization. Journal of Machine Learning Research, 12:1069‚Äì1109.
Chaudhuri, K., Sarwate, A. D., and Sinha, K. (2012). Nearoptimal differentially private principal components. In
Advances in Neural Information Processing Systems 25:
26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States., pages
998‚Äì1006.
Cook, R. D. and Weisberg, S. (1982). Residuals and influence in regression. New York: Chapman and Hall.
Dai, Z., Yang, Z., Yang, Y., Carbonell, J. G., Le, Q. V.,
and Salakhutdinov, R. (2019). Transformer-xl: Attentive
language models beyond a fixed-length context. In Proceedings of the 57th Conference of the Association for
Computational Linguistics, ACL 2019, Florence, Italy,
July 28- August 2, 2019, Volume 1: Long Papers, pages
2978‚Äì2988.

Certified Data Removal from Machine Learning Models

Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Li,
F. (2009). Imagenet: A large-scale hierarchical image
database. In 2009 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR 2009),
20-25 June 2009, Miami, Florida, USA, pages 248‚Äì255.

Looks, M., Herreshoff, M., Hutchins, D., and Norvig, P.
(2017). Deep learning with dynamic computation graphs.
In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings.

Devlin, J., Chang, M., Lee, K., and Toutanova, K. (2019).
BERT: pre-training of deep bidirectional transformers
for language understanding. In Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Minneapolis, MN, USA,
June 2-7, 2019, Volume 1 (Long and Short Papers), pages
4171‚Äì4186.

Mahajan, D., Girshick, R. B., Ramanathan, V., He, K.,
Paluri, M., Li, Y., Bharambe, A., and van der Maaten,
L. (2018). Exploring the limits of weakly supervised
pretraining. In Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14,
2018, Proceedings, Part II, pages 185‚Äì201.

Dwork, C. (2011). Differential privacy. Encyclopedia of
Cryptography and Security, pages 338‚Äì340.
Ginart, A., Guan, M. Y., Valiant, G., and Zou, J. (2019).
Making AI forget you: Data deletion in machine learning.
CoRR, abs/1907.05012.
Gu, M. and Eisenstat, S. C. (1995). Downdating the singular value decomposition. SIAM J. Matrix Anal. Appl.,
16(3):793‚Äì810.
He, K., Gkioxari, G., DollaÃÅr, P., and Girshick, R. B. (2017).
Mask R-CNN. In IEEE International Conference on
Computer Vision, ICCV 2017, Venice, Italy, October 2229, 2017, pages 2980‚Äì2988.
Karasuyama, M. and Takeuchi, I. (2009). Multiple incremental decremental learning of support vector machines.
In Advances in Neural Information Processing Systems
22: 23rd Annual Conference on Neural Information Processing Systems 2009. Proceedings of a meeting held 7-10
December 2009, Vancouver, British Columbia, Canada.,
pages 907‚Äì915.
Koh, P. W. and Liang, P. (2017). Understanding black-box
predictions via influence functions. In Proceedings of
the 34th International Conference on Machine Learning,
ICML 2017, Sydney, NSW, Australia, 6-11 August 2017,
pages 1885‚Äì1894.
Liu, D. C. and Nocedal, J. (1989). On the limited memory
bfgs method for large scale optimization. Math. Program.,
45(1-3):503‚Äì528.
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.
(2019). Roberta: A robustly optimized BERT pretraining
approach. CoRR, abs/1907.11692.
Liu, Z., Wang, Y.-X., and Smola, A. (2015). Fast differentially private matrix factorization. In Proceedings of the
9th ACM Conference on Recommender Systems, pages
171‚Äì178. ACM.

Ren, S., He, K., Girshick, R. B., and Sun, J. (2015). Faster
R-CNN: towards real-time object detection with region
proposal networks. In Advances in Neural Information
Processing Systems 28: Annual Conference on Neural
Information Processing Systems 2015, December 7-12,
2015, Montreal, Quebec, Canada, pages 91‚Äì99.
Tai, K. S., Socher, R., and Manning, C. D. (2015). Improved semantic representations from tree-structured long
short-term memory networks. In Proceedings of the 53rd
Annual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference
on Natural Language Processing of the Asian Federation
of Natural Language Processing, ACL 2015, July 26-31,
2015, Beijing, China, Volume 1: Long Papers, pages
1556‚Äì1566.
Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni,
K., Poland, D., Borth, D., and Li, L.-J. (2016). Yfcc100m:
The new data in multimedia research. Communications
of the ACM, 59(2):64‚Äì73.
Tsai, C., Lin, C., and Lin, C. (2014). Incremental and
decremental training for linear classification. In The 20th
ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ‚Äô14, New York, NY,
USA - August 24 - 27, 2014, pages 343‚Äì352.
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and
Bowman, S. R. (2019). GLUE: A multi-task benchmark
and analysis platform for natural language understanding.
In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
2019.
Wieting, J., Bansal, M., Gimpel, K., and Livescu, K. (2016).
Towards universal paraphrastic sentence embeddings. In
4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,
Conference Track Proceedings.
Xie, S., Girshick, R. B., DollaÃÅr, P., Tu, Z., and He, K. (2017).
Aggregated residual transformations for deep neural networks. In 2017 IEEE Conference on Computer Vision

Certified Data Removal from Machine Learning Models

and Pattern Recognition, CVPR 2017, Honolulu, HI, USA,
July 21-26, 2017, pages 5987‚Äì5995.
Yang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov,
R., and Le, Q. V. (2019). Xlnet: Generalized autoregressive pretraining for language understanding. CoRR,
abs/1906.08237.
Yeom, S., Giacomelli, I., Fredrikson, M., and Jha, S. (2018).
Privacy risk in machine learning: Analyzing the connection to overfitting. In CSF.
Zhao, H., Shi, J., Qi, X., Wang, X., and Jia, J. (2017). Pyramid scene parsing network. In 2017 IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 2017,
Honolulu, HI, USA, July 21-26, 2017, pages 6230‚Äì6239.

Certified Data Removal from Machine Learning Models

A. Appendix
We present proofs for theorems stated in the main paper.
Theorem 1. Suppose that ‚àÄ(xi , yi ) ‚àà D, w ‚àà Rd : k‚àá`(w> xi , yi )k2 ‚â§ C. Suppose also that `00 is Œ≥-Lipschitz and
kxi k2 ‚â§ 1 for all (xi , yi ) ‚àà D. Then:
‚àí1
k‚àáL(w‚àí ; D0 )k2 = k(HwŒ∑ ‚àí Hw‚àó )Hw
‚àó ‚àÜk2
‚àí1
2
‚â§ Œ≥(n ‚àí 1)kHw
‚àó ‚àÜk2 ‚â§

4Œ≥C 2
,
Œª2 (n ‚àí 1)

‚àí1
where HwŒ∑ denotes the Hessian of L(¬∑; D0 ) at the parameter vector wŒ∑ = w‚àó + Œ∑Hw
‚àó ‚àÜ for some Œ∑ ‚àà [0, 1].

Proof. Let G(w) = ‚àáL(w; D0 ) denote the gradient at w of the empirical risk on the reduced dataset D0 . Note that
G : Rd ‚Üí Rd is a vector-valued function. By Taylor‚Äôs Theorem, there exists some Œ∑ ‚àà [0, 1] such that:
‚àí1
G(w‚àí ) = G(w‚àó + Hw
‚àó ‚àÜ)
‚àí1
‚àí1
= G(w‚àó ) + ‚àáG(w‚àó + Œ∑Hw
‚àó ‚àÜ)Hw‚àó ‚àÜ.
‚àí1
0
Since G is the gradient of L(¬∑; D0 ), the quantity ‚àáG(w‚àó + Œ∑Hw
‚àó ‚àÜ) is exactly the Hessian of L(¬∑; D ) evaluated at the
‚àí1
‚àó
point wŒ∑ = w + Œ∑Hw‚àó ‚àÜ. Thus:
‚àí1
G(w‚àí ) = G(w‚àó ) + HwŒ∑ Hw
‚àó‚àÜ
‚àí1
= (G(w‚àó ) + ‚àÜ) + HwŒ∑ Hw
‚àó‚àÜ ‚àí ‚àÜ
‚àí1
‚àí1
= 0 + HwŒ∑ Hw
‚àó ‚àÜ ‚àí Hw‚àó Hw‚àó ‚àÜ
‚àí1
= (HwŒ∑ ‚àí Hw‚àó )Hw
‚àó ‚àÜ.

This gives:
‚àí1
kG(w‚àí )k2 = k(HwŒ∑ ‚àí Hw‚àó )Hw
‚àó ‚àÜk2
‚àí1
‚â§ kHwŒ∑ ‚àí Hw‚àó k2 kHw
‚àó ‚àÜk2 .

Using the Lipschitz-ness of `00 , we have for every i:
k‚àá2 `(wŒ∑> xi , yi ) ‚àí ‚àá2w `((w‚àó )> xi , yi )k2 = k[`00 (wŒ∑> xi , yi ) ‚àí `00 ((w‚àó )> xi , yi )]xi x>
i k2
‚â§ |`00 (wŒ∑> xi , yi ) ‚àí `00 ((w‚àó )> xi , yi )| ¬∑ kxi k22
‚â§ Œ≥kwŒ∑ ‚àí w‚àó k2
=
‚â§

since kxi k2 ‚â§ 1

‚àí1
Œ≥kŒ∑Hw
‚àó ‚àÜk2
‚àí1
Œ≥kHw‚àó ‚àÜk2 .

As a result, we can conclude that:
kHwŒ∑ ‚àí Hw‚àó k2 ‚â§

n‚àí1
X

‚àá2 `(wŒ∑> xi , yi ) ‚àí ‚àá2 `((w‚àó )> xi , yi )

i=1

2

‚àí1
‚â§ Œ≥(n ‚àí 1)kHw
‚àó ‚àÜk2 .
‚àí1
2
Combining these results leads us to conclude that kG(w‚àí )k2 ‚â§ Œ≥(n ‚àí 1)kHw
‚àó ‚àÜk2 .
‚àí1
0
We can simplify this bound by analyzing kHw
‚àó ‚àÜk2 . Since L(¬∑; D ) is Œª(n‚àí1)-strongly convex, we get kHw‚àó k2 ‚â• Œª(n‚àí1),
‚àí1
1
hence kHw‚àó k2 ‚â§ Œª(n‚àí1) . Recall that

‚àÜ = Œªw‚àó + ‚àá` (w‚àó )> xn , yn .

Certified Data Removal from Machine Learning Models

Since w‚àó is the global optimal solution of the loss L(¬∑; D), we obtain the condition:
0 = ‚àáL(w‚àó ; D) =

n
X


‚àá` (w‚àó )> xi , yi + Œªnw‚àó .

i=1

Using the norm bound k‚àá`(w> x, y)k2 ‚â§ C and re-arranging the terms, we obtain:
Pn
k i=1 ‚àá`((w‚àó )> xi , yi )k2
C
‚àó
‚â§ .
kw k2 =
Œªn
Œª
Using this and the same norm bound, we observe:
k‚àÜk2 ‚â§ Œªkw‚àó k2 + k‚àá`((w‚àó )> xn , yn )k2 ‚â§ 2C,
from which we obtain:
‚àí1
‚àí1
kHw
‚àó ‚àÜk2 ‚â§ kHw‚àó k2 k‚àÜk2 ‚â§

2C
,
Œª(n ‚àí 1)

which leads to the desired bound.
Theorem 2. Suppose that b is drawn from a distribution with density function p(¬∑) such that for any b1 , b2 ‚àà Rd satisfying
p(b1 )
‚â§ e . Then:
kb1 ‚àí b2 k2 ‚â§ 0 , we have that: e‚àí ‚â§ p(b
2)
e‚àí ‚â§

fAÃÉ (wÃÉ)
‚â§ e ,
fA (wÃÉ)

for any solution wÃÉ produced by AÃÉ.
Proof. Let p be the density function of b and let gAÃÉ be the density functions of the gradient residual under optimizer AÃÉ.
Consider the density functions qA and qAÃÉ of z = b ‚àí u under optimizers A and AÃÉ. We obtain:
Z
qAÃÉ (z) =
gAÃÉ (v)p(z + v)dv
Zv
=
gAÃÉ (v)p(z + v)dv since gAÃÉ has no support elsewhere
v:kvk2 ‚â§0
Z
‚â§
gAÃÉ (v)e p(z)dv since kvk2 ‚â§ 0
v:kvk2 ‚â§0

= e p(z)
= e qA (z)
where the last step follows since the gradient residual u under A is 0. To complete the proof, note that the value of wÃÉ is
completely determined by z = b‚àíu. Indeed, any w satisfying Equation 5 is an exact solution of the strongly convex loss
Lb (w) ‚àí u> w and, hence, must be unique. This gives:
Z
fAÃÉ (wÃÉ) = fAÃÉ (wÃÉ|z)qAÃÉ (z)dz
Zz
= fA (wÃÉ|z)qAÃÉ (z)dz since wÃÉ is governed by z
Zz
‚â§ fA (wÃÉ|z)e qA (z)dz
z

= e fA (wÃÉ).
In the above, note that while fAÃÉ and fA are not the same in general, their difference is governed entirely by z: given a fixed
z, the conditional density of wÃÉ is the same under both density functions.
Using a similar approach as above, we can also show that fAÃÉ (wÃÉ) ‚â• e‚àí fA (wÃÉ).

Certified Data Removal from Machine Learning Models

Theorem 3. Let A be the learning algorithm that returns the unique optimum of the loss Lb (w; D) and let M be the
Newton update removal mechanism (cf., Equation 3). Suppose that k‚àáL(w‚àí ; D0 )k2 ‚â§ 0 for some computable bound
0 > 0. We have the following guarantees for M :


(i) If b is drawn from a distribution with density p(b) ‚àù e‚àí 0 kbk2 , then M is -CR for A;
(ii) If b ‚àº N (0, c0 /)d with c > 0, then M is (, Œ¥)-CR for A with Œ¥ = 1.5 ¬∑ e‚àíc

2

/2

.

Proof. The proof involves bounding the density ratio of b1 and b2 when kb1 ‚àí b2 k2 ‚â§ 0 and then invoking Theorem 2.
(i)

p(b1 )
p(b2 )





= e‚àí 0 (kbb1 k2 ‚àíkbb2 k2 ) ‚â§ e 0 (kbb1 ‚àíbb2 k2 ) ‚â§ e . The reverse direction can be obtained similarly.

(ii) The proof of Theorem 3.22 in (Dwork, 2011) applies using ‚àÜ2 (f ) = 0 , giving that with probability at least 1 ‚àí Œ¥, we
1)

have that e‚àí ‚â§ p(b
p(b2 ) ‚â§ e . Applying Theorem 2 gives the desired (, Œ¥)-CR guarantee.

Theorem 4. Under the same regularity conditions of Theorem 1, we have that:
h
i‚àí1
(m)
k‚àáL(w(‚àím) ; D \ Dm )k2 ‚â§ Œ≥(n ‚àí m) Hw‚àó
‚àÜ(m)

2

‚â§
2

4Œ≥m2 C 2
.
Œª2 (n ‚àí m)

Proof. The proof is almost identical to that of Theorem 1, except that there are n ‚àí m terms in the Hessian and ‚àÜ(m) now
scales linearly with m.
Theorem 5. Suppose Œ¶ is a randomized learning algorithm that is (DP , Œ¥DP )-differentially private, and the outputs of Œ¶
are used in a linear model by minimizing Lb and using a removal mechanism that guarantees (CR , Œ¥CR )-certified removal.
Then the entire procedure guarantees (DP +CR , Œ¥DP +Œ¥CR )-certified removal.
Proof. Let Œ¶ be the randomized algorithm that learns a feature extractor from the data D and let ¬µ(S) = P (Œ¶(D) ‚àà S) be
the induced probability measure over the space, ‚Ñ¶, of all possible feature extractors. Let D0 = D \ x be the dataset with x
removed and let ¬µ0 (¬∑) be the corresponding probability measure for Œ¶(D0 ).
Since Œ¶ is (DP , Œ¥DP )-DP, for any S ‚äÜ ‚Ñ¶, we have that:
¬µ(S) = P (Œ¶(D) ‚àà S) ‚â§ eDP P (Œ¶(D0 ) ‚àà S) = eDP ¬µ0 (S),
with probability 1 ‚àí Œ¥DP . In particular, this shows that ¬µ is absolutely continuous w.r.t. ¬µ0 and therefore admits a RadonNikodym derivative g. Furthermore, g is (almost everywhere w.r.t. ¬µ0 ) bounded by eDP . Indeed, suppose that there exists a
set S ‚äÜ ‚Ñ¶ with ¬µ0 (S) > 0 such that g ‚â• eDP + Œ± on S for some Œ± ‚â• 0, then:
Z
¬µ(S) =
g(S) d¬µ0 ‚â• (eDP + Œ±)¬µ0 (S) ‚â• ¬µ(S) + Œ±¬µ0 (S),
S

which is a contradiction unless Œ± = 0.
Finally, for any œÜ ‚àà ‚Ñ¶ such that Œ¶(D) = œÜ, let A(D, œÜ) be the learning algorithm that trains a model on D using the feature
extractor œÜ. Suppose that M is an (CR , Œ¥CR )-CR mechanism, then by Fubini‚Äôs Theorem:
Z
P (M (A(D, Œ¶(D)), D, x) ‚àà T ) =
P (M (A(D, œÜ), D, x) ‚àà T ) d¬µ
‚Ñ¶
Z
‚â§
eCR P (A(D0 , œÜ) ‚àà T ) d¬µ
‚Ñ¶
Z
=
eCR P (A(D0 , œÜ) ‚àà T ) ¬∑ g d¬µ0
‚Ñ¶
Z
‚â§
eDP +CR P (A(D0 , œÜ) ‚àà T ) d¬µ0
‚Ñ¶

= eDP +CR P (A(D0 , Œ¶(D0 )) ‚àà T ),
with probability at least 1 ‚àí Œ¥DP ‚àí Œ¥CR . The lower bound can be shown in a similar fashion.

