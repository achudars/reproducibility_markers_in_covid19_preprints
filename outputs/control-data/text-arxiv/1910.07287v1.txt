1

Continuous-Domain Assignment Flows

arXiv:1910.07287v1 [math.DS] 16 Oct 2019

F. S A V A R I N O and C. S C H N Ö R R
Image and Pattern Analysis Group, Heidelberg University, Heidelberg, Germany
email: fabrizio.savarino@iwr.uni-heidelberg.de, schnoerr@math.uni-heidelberg.de
URL: https://ipa.math.uni-heidelberg.de

Assignment flows denote a class of dynamical models for contextual data labeling (classification) on
graphs. We derive a novel parametrization of assignment flows that reveals how the underlying information geometry induces two processes for assignment regularization and for gradually enforcing
unambiguous decisions, respectively, that seamlessly interact when solving for the flow. Our result
enables to characterize the dominant part of the assignment flow as a Riemannian gradient flow with
respect to the underlying information geometry. We consider a continuous-domain formulation of the
corresponding potential and develop a novel algorithm in terms of solving a sequence of linear elliptic PDEs subject to a simple convex constraint. Our result provides a basis for addressing learning
problems by controlling such PDEs in future work.

Key Words: image labeling, image segmentation, information geometry, replicator equation, evolutionary
dynamics, assignment flow.

Contents
1. Introduction
2. Preliminaries
2.1. Basic Notation
2.2. Assignment Manifold and Flow
2.2.1. Assignment Manifold
2.2.2. Assignment Flow
2.3. Functional Analysis
2.3.1. Sobolev Spaces
2.3.2. Weak Convergence Properties, Variational Inequalities
3. A Novel Representation of the Assignment Flow
3.1. Non-Potential Flow
3.2. S-parametrization
4. Continuous-Domain Variational Approach
4.1. Well-Posedness
4.2. Fixed Boundary Conditions
4.3. Numerical Algorithm and Example
4.4. A PDE Characterizing Optimal Assignment Flows
5. Conclusion
References

2
3
3
3
4
5
6
6
7
8
8
13
15
16
17
19
20
22
23

2

F. Savarino and C. Schnörr
1 Introduction

Deep networks are omnipresent in many disciplines due to their unprecedented predictive power
and the availability of software for training that is easy to use. However, this rapid development
during recent years has not improved our mathematical understanding in the same way, so far
[Ela17]. The ‘black box’ behaviour of deep networks and systematic failures [ARP+ 19], the
lack of performance guarantees and reproducibility of results, raises doubts if a purely datadriven approach can deliver the high expectations of some of its most passionate proponents
[CDH16]. ‘Mathematics of deep networks’, therefore, has become a focal point of research.
Initiated maybe by [HZRS16] and mathematically substantiated and promoted by, e.g. [HR17,
E17], attempts to understand deep network architectures as discretized realizations of dynamical
systems has become a fruitful line of research. Adopting this viewpoint, we introduced a dynamical system – called assignent flow – for contextual data classification and image labeling
on graphs [ÅPSS17]. We refer to [Sch19] for a review of recent work including parameter estimation (learning) [HSPS19], adaption of data prototypes during assignment [ZZPS19a], and
learning prototypes from low-rank data representations and self-assignment [ZZPS19b].
Two key properties of the assignment flow are smoothness and gradual enforcement of unambigious classification in a single process, solely induced by adopting an elementary statistical
manifold as state space that is natural for classification tasks, and the corresponding information geometry [AN00]. This differs from traditional variational approaches to image labeling
[LS11, CCP12] that enjoy convexity but are inherently nonsmooth and require postprocessing
to achieve unambigous decisions. We regard nonsmoothness as a major barrier to the design of
hierarchical architectures for data classification.
The assignment flow combines by composition (rather than by addition) separate local processes at each vertex of the underlying graph and nonlocal regularization. Each local process
for label assignment is governed by an ODE, the replicator equation [HS03, San10], whereas
regularization is accomplished by nonlocal geometric averaging of the evolving assignments. It
is well known [HS03] that if the affinity measure which defines the replicator equation and hence
governs label selection can be derived as gradient of a potential, then the replicator equation is
just the corresponding Riemannian gradient flow induced by the Fisher-Rao metric. The geometric regularization of assignments performed by the assignment flow yields an affinity measure
for which the (non-)existence of a corresponding potential is not immediate, however.
Contribution and Organization. The objective of this paper is to clarify this situation. After
collecting background material in Section 2, we prove that no potential exists that enables to
characterize the assignment flow as Riemannian gradient flow (Section 3.1). Next, we provide a
novel parametrization of the assignment flow by separating a dominant component of the flow,
called S-flow, that completely determines the remaining component and hence essentially characterizes the assignment flow (Section 3.2). The S-flow does correspond to a potential, under an
additional symmetry assumption with respect to the weights that parametrize the regularization
properties of the assignment flow through (weighted) geometric averaging. This potential can
be decomposed into two components that make explicit the two interacting processes mentioned
above: regularization of label assignments and gradually enforcing unambigous decisions. We
point out again that this is a direct consequence of the ‘spherical geometry’ (positive curvature)
underlying the assignment flow.

Continuous-Domain Assignment Flow

3

Based on this result, we consider the corresponding continuous-domain variational formulation in Section 4. We prove well-posedness which is not immediate due to nonconvexity, and
we propose an algorithm that computes a locally optimal assignment by solving a sequence of
simple linear PDEs, with changing right-hand side and subject to a simple convex constraint. A
numerical example demonstrates that our PDE-based approach reproduces results obtained with
solving the original formulation of the assignment flow using completely different numerical
techniques [ZSPS19]. We hope that the simplicity of our PDE-approach and the direct connection to a smooth geometric setting will stimulate future work on learning, from an optimal control
point-of-view [EHL19, LT19]. We conclude by a formal derivation of a PDE that characterizes
global minimizers of the nonconvex objective function (Section 4.4) and by outlining future research in Section 5.

2 Preliminaries
2.1 Basic Notation
We denote the standard basis of Rn by
Bn := {e1 , . . . , en }.

(2.1)

| · | applied to a finite set denotes its cardinality, i.e. |Bn | = n. We set [n] = {1, 2, . . . , n} for
n ∈ N and 1n = (1, 1, . . . , 1)> ∈ Rn . The symbols
I = [n],

J = [c],

n, c ∈ N

(2.2)

will specifically index data points and classes (labels), respectively. k · k denotes the Euclidean
vector norm and the Frobenius matrix norm induced by the inner product kAk = hA, Ai1/2 =
tr(A> A)1/2 . All other norms will be indicated by a corresponding subscript. For a given matrix
A ∈ Rn×c , Ai , i ∈ [n] denote the row vectors, Aj , j ∈ [c] denote the column vectors, and
A> ∈ Rc×n the transpose matrix. Sn+ denotes the set of all symmetric n × n matrices with
nonnegative entries.
∆n = {p ∈ Rn+ : h1n , pi = 1}

(2.3)

denotes the probability simplex. There will be no danger to confuse it with the Laplacian differential operator ∆ that we use without subscript. For strictly positive vectors p > 0, we efficiently
denote componentwise subdivision by vp . Likewise, we set pv = (p1 v1 , . . . , pn vn )> . The exponential function applies componentwise to vectors (and similarly for log) and will always be
denoted by ev = (ev1 , . . . , evn )> , in order not to confuse it with the exponential maps (2.18).
Strong and weak convergence of a sequence (fn ) is written as fn → f and fn * f , respectively. ψS denotes the indicator function of some set S: ψS (i) = 1 if i ∈ S and ψS (i) = 0
otherwise. δC denotes the indicator function from the optimization point of view: δC (f ) = 0 if
f ∈ C and δC (f ) = +∞ otherwise.

2.2 Assignment Manifold and Flow
We sketch the assignment flow as introduced by [ÅPSS17] and refer to the recent survey [Sch19]
for more background and a review of recent related work.

4

F. Savarino and C. Schnörr

2.2.1 Assignment Manifold
Let (F, dF ) be a metric space and
Fn = {fi ∈ F : i ∈ I},

|I| = n.

(2.4)

|J | = c.

(2.5)

given data. Assume that a predefined set of prototypes
F∗ = {fj∗ ∈ F : j ∈ J },
is given. Data labeling denotes the assignments
fj∗ → fi

j → i,

(2.6)

of a single prototype fj∗ ∈ F∗ to each data point fi ∈ Fn . The set I is assumed to form the vertex
set of an undirected graph G = (I, E) which defines a relation E ⊂ I × I and neighborhoods
Ni = {k ∈ I : ik ∈ E} ∪ {i},

(2.7)

where ik is a shorthand for the unordered pair (edge) (i, k) = (k, i). We require these neighborhoods to satisfy the relations
k ∈ Ni

⇔

i ∈ Nk ,

∀i, k ∈ I.

(2.8)

The assignments (labeling) (2.6) are represented by matrices in the set
W∗ = {W ∈ {0, 1}n×c : W 1c = 1n }

(2.9)

with unit vectors Wi , i ∈ I, called assignment vectors, as row vectors. These assignment vectors
are computed by numerically integrating the assignment flow below (2.28), in the following
elementary geometric setting. The integrality constraint of (2.9) is relaxed and vectors
Wi = (Wi1 , . . . , Wic )> ∈ S,

i ∈ I,

(2.10)

that we still call assignment vectors, are considered on the elementary Riemannian manifold
S = {p ∈ ∆c : p > 0}

(S, g),

(2.11)

with
1S =

1
1c ∈ S,
c

(barycenter)

(2.12)

tangent space
T0 = {v ∈ Rc : h1c , vi = 0}

(2.13)

and tangent bundle T S = S × T0 , orthogonal projection
Π0 : Rc → T0 ,

Π0 = I − 1S 1>

(2.14)

and the Fisher-Rao metric
gp (u, v) =

X uj v j
,
pj

p ∈ S,

u, v ∈ T0 .

(2.15)

j∈J

Based on the linear map
Rp : Rc → T0 ,

Rp = Diag(p) − pp> ,

p∈S

(2.16)

5

Continuous-Domain Assignment Flow
satisfying
Rp = Rp Π0 = Π0 Rp ,

(2.17)

exponential maps and their inverses are defined as
v

Exp : S × T0 → S,

(p, v) 7→ Expp (v) =

pe p
v ,
hp, e p i

q
q 7→ Exp−1
p (q) = Rp log ,
p

Exp−1
p : S → T0 ,
expp : T0 → S,

expp = Expp ◦Rp ,
q
exp−1
p (q) = Π0 log .
p

exp−1
p : S → T0 ,

(2.18 a)
(2.18 b)
(2.18 c)
(2.18 d)

Applying the map expp to a vector in Rc = T0 ⊕ R1 does not depend on the constant component
of the argument, due to (2.17).
Remark 2.1 The map Exp corresponds to the e-connection of information geometry [AN00],
rather than to the exponential map of the Riemannian connection. Accordingly, the affine geodesics
(2.18 a) are not length-minimizing. But they provide an close approximation [ÅPSS17, Prop. 3]
and are more convenient for numerical computations.
The assignment manifold is defined as
(W, g),

W = S × · · · × S.

(n = |I| factors)

(2.19)

We identify W with the embedding into Rn×c
W = {W ∈ Rn×c : W 1c = 1n and Wij > 0 for all i ∈ [n], j ∈ [c]}.

(2.20)

Thus, points W ∈ W are row-stochastic matrices W ∈ Rn×c with row vectors Wi ∈ S, i ∈ I
that represent the assignments (2.6) for every i ∈ I. We set
T0 := T0 × · · · × T0

(n = |I| factors).

(2.21)

Due to (2.20), the tangent space T0 can be identified with
T0 = {V ∈ Rn×c : V 1c = 0}.

(2.22)

Thus, Vi ∈ T0 for all row vectors of V ∈ Rn×c and i ∈ I. All mappings defined above factorize
in a natural way and apply row-wise, e.g. ExpW = (ExpW1 , . . . , ExpWn ) etc.

2.2.2 Assignment Flow
Based on (2.4) and (2.5), the distance vector field
DF ;i = dF (fi , f1∗ ), . . . , dF (fi , fc∗ )

>

,

i∈I

(2.23)

is well-defined. These vectors are collected as row vectors of the distance matrix
DF ∈ Sn+ .

(2.24)

6

F. Savarino and C. Schnörr

The likelihood map and the likelihood vectors, respectively, are defined as
1

Li : S → S,

Li (Wi ) = expWi




1
Wi e− ρ DF ;i
− DF ;i =
,
1
ρ
hWi , e− ρ DF ;i i

i ∈ I,

(2.25)

where the scaling parameter ρ > 0 is used for normalizing the a-prior unknown scale of the
components of DF ;i that depends on the specific application at hand.
A key component of the assignment flow is the interaction of the likelihood vectors through
geometric averaging within the local neighborhoods (2.7). Specifically, using weights
X
wik = 1,
(2.26)
ωik > 0 for all k ∈ Ni , i ∈ I with
k∈Ni

the similarity map and the similarity vectors, respectively, are defined as
 X

wik Exp−1
,
Si : W → S,
Si (W ) = ExpWi
Wi Lk (Wk )

i ∈ I.

(2.27)

k∈Ni

If ExpWi were the exponential map of the Riemannian (Levi-Civita) connection, then the argument inside the brackets of the right-hand side would just be the negative Riemannian gradient
with respect to Wi of the center of mass objective function comprising the points Lk , k ∈ Ni ,
i.e. the weighted sum of the squared Riemannian distances between Wi and Lk [Jos17, Lemma
6.9.4]. In view of Remark 2.1, this interpretation is only approximately true mathematically, but
still correct informally: Si (W ) moves Wi towards the geometric mean of the likelihood vectors
Lk , k ∈ Ni . Since ExpWi (0) = Wi , this mean is equal to Wi if the aforementioned gradient
vanishes.
The assignment flow is induced by the locally coupled system of nonlinear ODEs
Ẇ = RW S(W ),
Ẇi = RWi Si (W ),

W (0) = 1W ,
Wi (0) = 1S ,

(2.28 a)
i ∈ I,

(2.28 b)

where 1W ∈ W denotes the barycenter of the assignment manifold (2.19). The solution curve
W (t) ∈ W is numerically computed by geometric integration [ZSPS19] and determines a labeling W (T ) ∈ W∗ for sufficiently large T after a trivial rounding operation.
2.3 Functional Analysis
We record background material that will be used in Section 4.
2.3.1 Sobolev Spaces
We list few basic definitions and fix the corresponding notation [Zie89, ABM14]. Throughout
this section Ω ⊂ Rd denotes an open bounded domain.
We denote the inner product and the norm of functions f, g ∈ L2 (Ω) by
Z
1/2
(f, g)Ω =
f gdx,
kf kΩ = (f, f )Ω .
(2.29)
Ω

Functions f1 and f2 are equivalent and identified whenever they merely differ pointwise on a
Lebesque-negligible set of measure zero. f1 and f2 then are said to be equal a.e. (almost everywhere). H 1 (Ω) = W 1,2 (Ω) denotes the Sobolev space of functions f with square-integrable

7

Continuous-Domain Assignment Flow

weak derivatives Dα f up to order one. H 1 (Ω) is a Hilbert space with inner product and norm
denoted by
 X
1/2
X
(f, g)1;Ω =
(Dα f, Dα g)Ω ,
kf k1;Ω =
kDα f k2Ω
.
(2.30)
|α|≤1

|α|≤1

Lemma 2.2 ([Zie89, Cor. 2.1.9]) If Ω is connected, u ∈ H 1 (Ω) and Du = 0 a.e. on Ω, then u
is equivalent to a constant function on Ω.
The closure in H 1 (Ω) of the set of test functions Cc∞ (Ω) that are compactly supported on Ω, is
the Sobolev space
H01 (Ω) = Cc∞ (Ω) ⊂ H 1 (Ω).

(2.31)

1

It contains all functions in H (Ω) whose boundary values on ∂Ω (in the sense of traces) vanish.
The space H 1 (Ω; Rc ), 2 ≤ c ∈ N contains vector-valued functions f whose component functions fi , i ∈ [c] are in H 1 (Ω). For notational efficiency, we denote the norm of f ∈ H 1 (Ω; Rc )
by
X
1/2
kfi k21;Ω
kf k1;Ω =
(2.32)
i∈[c]

as in the scalar case (2.30). It will be clear from the context if f is scalar- or vector-valued.
The compactness theorem of Rellich-Kondrakov [ABM14, Thm. 5.3.3] says that the canonical embedding
H01 (Ω) ,→ L2 (Ω)
is compact, i.e. every bounded subset in
the vector-valued case

H01 (Ω)

(2.33)
2

is relatively compact in L (Ω). This extends to

H01 (Ω; Rc ) ,→ L2 (Ω; Rc )
H01 (Ω; Rc )

H01 (Ω)

× H01 (Ω)
−1

since
is isomorphic to
× ···
dual space of H01 (Ω) is commonly denoted by H
0
H −1 (Ω; Rc ) = H01 (Ω; Rc ) .

(2.34)
2

c

and likewise
 for L (Ω; R ). The
(Ω) = H01 (Ω) . Accordingly, we set

2.3.2 Weak Convergence Properties, Variational Inequalities
We list few further basic facts [Zei85, Prop. 38.2], [ABM14, Prop. 2.4.6].
Proposition 2.3 The following assertions hold in a Banach space X.
(a) A closed convex subset C ⊂ X is weakly closed, i.e. a sequence (fn )n∈N ⊂ C that weakly
converges to f implies f ∈ C.
(b) If X is reflexive (in particular, if X is a Hilbert space), then every bounded sequence in X
has a weakly convergent subsequence.
(c) If fn weakly converges to f , then (fn )n∈N is bounded and
kf kX ≤ lim inf kfn kX .
n→∞

(2.35)

8

F. Savarino and C. Schnörr

The following theorem states conditions for minimizers of the functional to satisfy a corresponding variational inequality.
Theorem 2.4 ([Zei85, Thm. 46.A(a)]) Let F : C → R be a functional on the convex nonempty
set C of a real locally convex space X, and let b ∈ X 0 be a given element. Suppose the Gateauxderivative F 0 exists on C. Then any solution f of

min F (f ) − hb, f iX 0 ×X ,
(2.36)
f ∈C

satisfies the variational inequality
hF 0 (f ) − b, h − f iX 0 ×X ≥ 0,

for all h ∈ C.

(2.37)

3 A Novel Representation of the Assignment Flow
Let J : W → R be a smooth function on the assignment manifold (2.19) and denote the Riemannian gradient of J at W ∈ W induced by the Fisher-Rao metric (2.15) by grad J(W ) ∈ T0 .
In view of the embedding (2.20), we can also compute the Euclidean gradient of J denoted by
∂ J(W ) ∈ Rn×c . These two gradients are related by [ÅPSS17, Prop. 1]
grad J(W ) = RW ∂ J(W ),

W ∈ W,

(3.1)

where RW : Rn×c → T0 is the product map obtained by applying RWi from (2.16) to every row
vector indexed by i ∈ I. This relation raises the natural question: Is there a potential J such
that the assignment flow (2.28) is a Riemannian gradient descent flow with respect to J, i.e. does
RW S(W ) = − grad J(W ) hold?
We next show that such a potential does not exist in general (Section 3.1). However, in Section 3.2, we derive a novel representation by decoupling the assignment flow into two separate
flows, where one flow steers the other and in this sense dominates the assignment flow. Under the
additional assumption that the weights ωij of the similarity map S(W ) in (2.27) are symmetric,
we show that the dominating flow is a Riemannian gradient flow induced by a potential. This
result is the basis for the continuous-domain formulation of the assignment flow studied in the
subsequent sections.

3.1 Non-Potential Flow
We next show (Theorem 3.4) that under some mild assumptions on DF (2.24) which are always
fulfilled in practice, no potential J exists that induces the assignment flow. In order to prove this
result, we first derive some properties of the mapping exp given by (2.18 c) as well as explicit
expressions of the differential dS(W ) of the similarity map (2.27) and its transpose dS(W )>
with respect to the standard Euclidean structure on Rn×c .
Lemma 3.1 The following properties hold for expp and its inverse (2.18 c), (2.18 d).
(1) For every p ∈ S the map expp : Rc → S can be expressed by
v 7→ expp (v) =

pev
.
hp, ev i

(3.2)

9

Continuous-Domain Assignment Flow

Its restriction to T0 , expp : T0 → S, is a diffeomorphism. The differential of expp and exp−1
p
at v ∈ T0 and q ∈ S, respectively, are given by
hui
for all u ∈ T0 .
(3.3)
d expp (v)[u] = Rexpp (v) [u] and d exp−1
p (q)[u] = Π0
q
−1
(2) Let p, q ∈ S. Then Exp−1
p (q) = Rp expp (q).
(3) Let q ∈ S. If the linear map Rq from (2.16) is restrictedhto iT0 , then Rq : T0 → T0 is a linear

isomorphism with inverse given by (Rq |T0 )−1 (u) = Π0
c

u
q

for all u ∈ T0 .

c

(4) If R is viewed as an abelian group, then exp : R × S → S given by (v, p) 7→ expp (v)
defines a Lie-group action, i.e.
expp (v + u) = expexpp (u) (v) and

expp (0) = p

for all v, u ∈ T0 and p ∈ S. (3.4)

Furthermore, the following identities follow for all p, q, a ∈ S and v ∈ Rc

expp (v) = expq v + exp−1
q (p)
exp−1
q (p)
−1
expq (a)

=
=

− exp−1
p (q)
−1
expp (a) −

(3.5 a)
(3.5 b)

exp−1
p (q).

(3.5 c)

Proof (1): We have Expp (v + λp) = Expp (v) for every p ∈ S, v ∈ T0 and λ ∈ R, as a simple
computation using definition (2.18 a) of Expp directly shows. Therefore, for every v ∈ T0
expp (v) = Expp (Rp v) = Expp (pv − hv, pip) = Expp (pv) =

pev
.
hp, ev i

(3.6)

If we restrict expp to T0 , then an inverse is explicitly given by (2.18 c). The differentials (3.3)
result from a standard computation.
and exp−1
given in
(2): The formula is a direct consequence of the formulas for Exp−1
p
p
(2.18 b) and (2.18 c), together with the fact (2.17).
(3): Fix any p ∈ S and set vq := exp−1
p (q) for q ∈ S. Since expp : T0 → S is a diffeomorphism, the differential d expp (vq ) : T0 → T0 is an isomorphism. By (3.3), we have
Rq [u] = Rexpp (vq ) [u] = d expp (vq )[u] for all u ∈ T0 , showing that Rq is an isomorphism
with the corresponding inverse.
(4): Properties (3.4) defining the group action are directly verified using (3.2). Now, suppose
p, q, a ∈ S and v ∈ Rc are arbitrary. Since expq : T0 → S is a diffeomorphism, we have
p = expq exp−1
q (p) and by the group action property

 (v) = exp v + exp−1 (p) ,
expp (v) = exp
(3.7)
−1
q
q
expq

expq (p)

which proves (3.5 a). To show (3.5 b), set va := exp−1
p (a) and substitute this vector into (3.5 a).
Applying exp−1
to
both
sides
then
gives
q

−1
−1
−1
expq (a) = exp−1
expp (va ) = va + exp−1
(3.8)
q
q (p) = expp (a) + expq (p).
Setting a = q in this equation, we obtain (3.5 b) from
−1
−1
0 = exp−1
q (q) = expp (q) + expq (p).
−1
Using exp−1
q (p) = − expp (q) in (3.8) yields (3.5 c).

(3.9)

10

F. Savarino and C. Schnörr

Lemma 3.2 The i-th component of the similarity map S(W ) defined by (2.27) can equivalently
be expressed as
X


1
Si (W ) = exp1S
ωij exp−1
(W
)
−
D
for all i ∈ I and W ∈ W. (3.10)
j
F
;j
1S
ρ
j∈Ni


Proof Consider the expression Exp−1
Wi Lj (Wj ) in the sum of the definition (2.27) of Si (W ).
Using (3.2) and (3.5 a), the likelihood (2.25) can be expressed as



 1
1
D
.
(3.11)
Lj (Wj ) = expWj − DF ;j = expWi exp−1
(W
)
−
F
;j
j
Wi
ρ
ρ
In the following, we set
Vk = exp−1
1S (Wk )

for all k ∈ I.

(3.12)

With this and (3.5 c), we have
−1
−1
exp−1
Wi (Wj ) = exp1S (Wj ) − exp1S (Wi ) = Vj − Vi .

The two previous identities and Lemma 3.1(2) give
i
h
h

i
1
−1
Exp−1
= RWi exp−1
Wi Lj (Wj ) = RWi expWi Lj (Wj )
Wi (Wj ) − DF ;j
ρ
h
i
1
= RWi Vj − Vi − DF ;j .
ρ

(3.13)

(3.14 a)
(3.14 b)

The sum over the neighboring nodes Ni in the definition (2.27) of Si (W ) can therefore be rewritten as
h
i
X
X

1
ωij RWi Vj − Vi − DF ;j
(3.15 a)
ωij Exp−1
Wi Lj (Wj ) =
ρ
j∈Ni
j∈Ni
i
h

X
1
(3.15 b)
= RWi − Vi +
ωij Vj − DF ;j ,
ρ
j∈Ni

where we used
we then have

P

j∈Ni ωij = 1 for the last equation. Setting Yi :=

P



1
ω
V
−
D
,
ij
j
F
;j
j∈Ni
ρ





Si (W ) = ExpWi RWi − Vi + Yi = expWi − Vi + Yi = exp1S Yi ,

(3.16)

where the last equality again follows from (3.5 a) together with the definition (3.12) of Vi .
Lemma 3.3 The i-th component of the differential of the similarity map S(W ) ∈ W is given by


X
Xj
dSi (W )[X] =
ωij RSi (W )
for all X ∈ T0 and i ∈ I.
(3.17)
Wj
j∈Ni

Furthermore, the i-th component of the adjoint differential dS(W )> : T0 → T0 with respect to
the standard Euclidean inner product on T0 ⊂ Rn×c is given by


X
RSj (W ) Xj
>
dSi (W ) [X] =
ωji Π0
for every X ∈ T0 and i ∈ I.
(3.18)
Wi
j∈Ni

Continuous-Domain Assignment Flow
11


P
1
∈ Rc
Proof Define the map Fi : W → Rc by Fi (W ) := j∈Ni ωij exp−1
1S (Wj ) − ρ DF ;j
for all W ∈ W. Let γ : (−ε, ε) → W be a smooth curve, with ε > 0, γ(0) = W and γ̇(0) = X.
By (3.3), we then have
hX i
X
X
d
d
j
dFi (W )[X] = Fi (γ(t)) t=0 =
ωij exp−1
ωij Π0
. (3.19)
1S (γj (t)) t=0 =
dt
dt
Wj
j∈Ni

j∈Ni

Due to Lemma 3.2, we can express the i-th component of the similarity map as Si (W ) =
exp1S Fi (W ) . Therefore, the differential of Si is given by




dSi (W )[X] = d exp1S (Fi (W )) dFi (W )[X] = Rexp1 (Fi (W )) dFi (W )[X]
(3.20 a)
S
hX i
hX
h X ii
X
j
j
ωij RSi (W )
=
,
(3.20 b)
= RSi (W )
ωij Π0
Wj
Wj
j∈Ni

j∈Ni

where we used RSi (W ) Π0 = RSi (W ) from (2.17) to obtain the last equation.
Now let W ∈ W and X, Y ∈ T0 be arbitrary. By assumption on the neighborhood structure
(2.7), we have j ∈ Ni if and only if i ∈ Nj , i.e. ψNi (j) = ψNj (i). Since RSi (W ) ∈ Rc×c is a
symmetric matrix, we obtain
D
E XD
E XX
D
hX i E
j
, Yi
(3.21 a)
dS(W )[X], Y =
dSi (W )[X], Yi =
ωij RSi (W )
Wj
i∈I

=

XX

ψNi (j)ωij

i∈I j∈I

=

XX
j∈I i∈Nj

i∈I j∈Ni

E XX
D
RSi (W ) [Yi ] E
j
, RSi (W ) [Yi ] =
ψNj (i)ωij Xj ,
Wj
Wj

DX

(3.21 b)

i∈I j∈I

iE X D
hR
iE
D
hR
X
Si (W ) [Yi ]
Si (W ) [Yi ]
=
Xj ,
ωij Π0
.
ωij Xj , Π0
Wj
Wj
j∈I

(3.21 c)

i∈Nj

On the other hand, we have
D
E D
E XD
E
dS(W )[X], Y = X, dS(W )> [Y ] =
Xj , dSj (W )> [Y ] .

(3.22)

j∈I

Because (3.21) and 3.22 hold for all X, Y ∈ T0 , the formula for dSi (W )> [X] is proven.
Theorem 3.4 Suppose c ≥ 3 and there exists a node i0 ∈ I such that the distance vector DF ;i0
is not constant: DF ;i0 ∈
/ R1. Then no potential J : W → R exists satisfying RW S(W ) =
− grad J(W ), i.e. the assignment flow (2.28) is not a Riemannian gradient descent flow.
Proof By (2.17), we have RW S(W ) = RW Π0 S(W ) and RW : T0 → T0 is a linear isomorphism (Lemma 3.1(3)). Therefore, the question of existence of a potential J : W → R for the
assignment flow (2.28) can be transferred to the Euclidean setting by applying (RW |T0 )−1 to
both sides of the equation RW S(W ) = grad J(W ), i.e.
RW S(W ) = − grad J(W ) = −RW ∂ J(W )

⇔

Π0 S(W ) = −Π0 ∂ J(W ) ∈ T0 .
(3.23)
If such a potential J exists, then the negative Hessian of J is given by

− Π0 Hess J(W ) = d − Π0 ∂ J (W ) = d(Π0 ◦ S)(W ) = Π0 dS(W ) = dS(W ), (3.24)
where the last equation follows from dS(W ) : T0 → T0 . Furthermore, Hess J(W ) and therefore

12

F. Savarino and C. Schnörr

also dS(W ) must be symmetric with respect to the Euclidean scalar product on T0 . Hence, in
order to prove that a potential cannot exist, we show that dS(W ) is not symmetric at every point
W ∈ W. To this end, we construct a W ∈ W and X ∈ T0 with dS(W )[X] − dS(W )> [X] 6= 0.
It suffices to show
dSi (W )[X] − dSi (W )> [X] 6= 0 for some row index i ∈ I.

(3.25)

To simplify notation, we write Di instead of DF ;i in the remainder of the proof. Due to the
hypothesis, we have
Di = DF ;i 6= R1.

(3.26)

Let k, l ∈ [c] be indices such that
Dik = min Dir
r∈[c]

Dil = max Dir .

and

r∈[c]

(3.27)

Relation (3.26) implies Dik < Dil and
1

1

e− ρ Dik > e− ρ Dil .

(3.28)

Define
u = ek − el ∈ T0 ,

ek , el ∈ Bc .

(3.29)

Since c ≥ 3, there is also a point
p∈S

with p 6= 1S

and

pk = pl ,

(3.30)

e.g. by choosing 0 < α < 1c and setting pk = pl = α and pr = (c − 2)−1 (1 − 2α) for r 6= k, l.
With these choices, we define the point
1 
Dj
W p ∈ W,
Wjp = expp
for all j ∈ I.
(3.31)
ρ

p
1
Also, set v := exp−1
1S (p). Then Wj = exp1S v + ρ Dj by (3.5 a) and Lemma 3.2 implies
 X
1 
p
Si (W ) = exp1S
ωij exp−1
Dj
(3.32 a)
(W
)
−
1S
j
ρ
j∈N (i)
 X

= exp1S
ωij v = expc (v) = p,
(3.32 b)
j∈N (i)

for all i ∈ I. Now, define
u

X ∈ T0

with

Xku

=

(
u ∈ T0 ,
Xju

if k = i

= 0, if k 6= i.

Using the expressions for dSi (W P ) and dSi (W p )> from Lemma 3.3, we obtain
hR
h Xu i
ui
Si (W p ) Xi
i
−
ω
Π
dSi (W p )[X u ] − dSi (W p )> [X u ] = ωii RSi (W p )
ii 0
Wip
Wip
i
h
i
h
u
Rp u
(3.32)
= ωii Rp
− ωii Π0
1
expp ( ρ Di )
expp ( ρ1 Di )
 hu 1 i
h e− ρ1 Di
i
1
(3.2)
= ωii hp, e ρ Di i Rp e− ρ Di − Π0
Rp u .
p
p

(3.33)

(3.34 a)
(3.34 b)
(3.34 c)

13

Continuous-Domain Assignment Flow
1
ρ Di

i > 0, we only have to check the expression inside the brackets. As for the first
Since ωii hp, e
term, using (2.16), we have
hu 1 i
1
1
Rp e− ρ Di = ue− ρ Di − u, e− ρ Di p.
(3.35 a)
p

1
1
Setting a := he− ρ Di , 1S i1 − e− ρ Di , we obtain for the second term
Π0

h e− ρ1 Di
p

i
i
h 1
1
1
1
Rp u = Π0 e− ρ Di u − hu, pie− ρ Di = e− ρ Di u − hu, e− ρ Di i1S + hu, pia.
(3.35 b)

Thus, the term inside the brackets reads
Rp

h1
p

1
Di
−ρ

ue

i

− Π0

h e− ρ1 Di
p

i
1
1
Rp u = − u, e− ρ Di p + hu, e− ρ Di i1S − hu, pia

1
= hu, e− ρ Di i 1S − p − hu, pia.

(3.36 a)
(3.36 b)

(3.29) and (3.30) imply
1

1

1

hu, e− ρ Di i = e− ρ Dik − e− ρ Dil > 0

and hu, pi = pk − pl = 0

(3.37)

such that we can conclude


1
1
1
hu, e− ρ Di i 1S − p − hu, pia = (e− ρ Dik − e− ρ Dil ) 1S − p 6= 0.

(3.38)

This proves (3.25) and consequently the theorem.
3.2 S-parametrization
Even though Theorem 3.4 says that no potential exists for the assignment flow in general, we
reveal in this section a ‘hidden’ potential flow under an additional assumption. To this end, we
decouple the assignment flow into two components and show that one component depends on
the second one. The dominating second one, therefore, provides a new parametrization of the
assignment flow. Assuming symmetry of the averaging matrix defined below by (3.39), the dominating flow becomes a Riemannian gradient descent flow. The corresponding potential defined
on a continuous domain will be studied in subsequent sections.
For notational efficiency, we collect all weights (2.26) into the averaging matrix
(
ωij if j ∈ Ni ,
ω
n×n
ω
Ω ∈R
with Ωij := ψNi (j)ωij =
, for i, j ∈ I.
(3.39)
0
else
Ωω encodes the spatial structure of the graph and the weights. For an arbitrary matrix M ∈ Rn×c ,
the average of its row vectors using the weights indexed by the neighborhood Ni is given by
X
X
> ω
ωik Mk =
Ωω
(3.40)
ik Mk = M Ωi .
k∈Ni

k∈I

Thus, all row vector averages are given as row vectors of the matrix Ωω M .
We now introduce a new representation of the assignment flow.

14

F. Savarino and C. Schnörr

Proposition 3.5 The assignment flow (2.28) is equivalent to the system
Ẇ = RW S
S˙ = RS [ΩS]

with W (0) = 1W

(3.41 a)

with S(0) = S(1W ).

(3.41 b)

Remark 3.6 We observe that the flow W (t) is completely determined by S(t). In the following,
we refer to the dominating part (3.41 b) as the S-flow.
Proof Let W (t) be a solution of the assignment flow, i.e. Ẇi = RWi Si (W ) for all i ∈ I. Set
S(t) := S(W (t)). Then (3.41 a) is immediate from the assumption on W . Using the expression
for dSi (W ) from Lemma 3.3 gives
h Ẇ i
X
d
j
S˙ i = S(W )i = dSi (W )[Ẇ ] =
ωij RSi (W )
.
dt
Wj

(3.42)

j∈Ni

Since W solves the assignment flow and RSi (W ) = RSi (W ) Π0 by (2.17) with ker(Π0 ) = R1c ,
it follows using the explicit expresssion (2.16) of RSi (W ) that
RSi (W )

h Ẇ i
j

Wj

= RSi (W )

hR

Wj Sj (W )

i

Wj


= RSi (W ) Sj (W ) .

h
i
= RSi (W ) Sj (W ) − hWj , Sj (W )i1c

(3.43 a)
(3.43 b)

Back-substitution of this identity into (3.42), pulling the linear map RSi (W ) out of the sum and
keeping Si (W ) = S i in mind, results in
i
hX


>
S˙ i = RSi (W ) Sj (W ) = RS i
(3.44)
ωij S j = RS i [S Ωω
i ] for all i ∈ I.
j∈Ni

Collecting these vectors as row vectors of the matrix S˙ gives (3.41 b).
Remark 3.7 Henceforth, we write S for the S-flow S to stress the underlying connection to the
assignment flow and to simplify the notation.
We next show that the S-flow which essentially determines the assignment flow (Remark 3.7)
becomes a Riemannian descent flow under the additional assumption that the averaging matrix
(3.39) is symmetric.
Proposition 3.8 Suppose the weights defining the similarity map in (2.27) are symmetric, i.e.
(Ωω )> = Ωω . Then the S-flow (3.41 b) is a Riemannian gradient decent flow Ṡ = − grad J(S),
induced by the potential
1
J(S) := − hS, Ωω Si, S ∈ W.
(3.45)
2
Proof Let γ : (−ε, ε) → W, ε > 0, be any smooth curve with γ̇(0) = V ∈ Rn×c and γ(0) = S.
d
By the symmetry of Ωω , we have h∂ J(S), V i = dJ(S)[V ] = dt
J(γ(t)) t=0 = −hΩω S, V i
n×c
ω
for all V ∈ R
. Therefore, ∂ J(S) = −Ω S. Thus, the Riemannian gradient is given by
grad J(S) = RS [∂ J(S)] = −RS [Ωω S].

15

Continuous-Domain Assignment Flow
Consider
LG = In − Ωω ,

(3.46)

n×n

ω

where In ∈ R
is the identity matrix. Since In = Diag(Ω 1n ) by (2.26) is the degree
matrix of the symmetric averaging matrix Ωω , LG can be regarded as Laplacian (matrix) of
the underlying undirected weighted graph G = (V, E)1 . For the analysis of the S-flow it will be
convenient to rewrite the potential (3.45) accordingly.
Proposition 3.9 Under the assumption of Proposition 3.8, the potential (3.45) can be written in
the form
1
1X X
1
1
ωij kSi − Sj k2 − kSk2 .
(3.47)
J(S) = hS, LG Si − kSk2 =
2
2
4
2
i∈I j∈Ni

The matrix LG is symmetric, positive semidefinite and LG 1n = 0.
Proof We have J(S) = − 21 hS, (Ωω − In )Si + hS, Si = 21 (hS, LG Si − kSk2 ). Thus, we focus
on the sum of (3.47).
First, note that kSj − Si k22 = hSj , Sj − Si i + hSi , Si − Sj i. Since ψNi (j) = ψNj (i) and
ωij = ωji by assumption, we have
XX
X
X
ωij hSj , Sj − Si i =
ψNi (j)ωij hSj , Sj − Si i =
ψNj (i)ωji hSj , Sj − Si i
i∈I j∈Ni

i,j∈I

i,j∈I

(3.48 a)
=

XX

ωji hSj , Sj − Si i =

j∈I i∈Nj

XX

ωij hSi , Si − Sj i, (3.48 b)

i∈I j∈Ni

where the last equality follows by renaming the indices i and j. Thus, using (2.26),
XX
XX
XX
ωij kSi − Sj k22 =
ωij hSj , Sj − Si i +
ωij hSi , Si − Sj i
i∈I j∈Ni

i∈I j∈Ni

=2

XX

(3.49 a)

i∈I j∈Ni

ωij hSi , Si − Sj i = 2

i∈I j∈Ni

E
XD
X
Si , Si −
ωij Sj
i∈I

j∈Ni

(3.49 b)
X
=2
hSi , (LS)i i = 2hS, LSi.

(3.49 c)

i∈I

The properties of LG follow from the symmetry of Ωω , nonnegativity of the quadratic form (3.49)
and definition (3.46).

4 Continuous-Domain Variational Approach
In this section, we study a continuous-domain variational formulation of the potential of Proposition 3.9. We confine ourselves to uniform weights (2.26) and neighborhoods (2.7) that only
1

For undirected graphs, the graph Laplacian is commonly defined by the weighted adjacency
matrices with diagonal entries 0, whereas Ωω
ii = ωii > 0. The diagonal entries do not affect the
quadratic form (3.47), however.

16

F. Savarino and C. Schnörr

contain the nearest neighbors of each vertex i, such that LG becomes the discretized ordinary
Laplacian. As a result, we consider the problem to minimize the functional
Eα : H 1 (M; Rc ) → R,
Z
Eα (S) :=
kDS(x)k2 − αkS(x)k2 dx ,

(4.1 a)
α > 0.

(4.1 b)

M

Throughout this section, M ⊂ R2 is a simply-connected bounded open subset in the Euclidean
plane. Parameter α controls the interaction between regularization and enforcing integrality when
S(x), x ∈ M is restricted to values in the probability simplex.
We prove well-posedness for vanishing (Section 4.1) and Dirichlet boundary conditions (Section 4.2), respectively, and specify explicitly the set of minimizers in the former case. The gradient descent flow corresponding to the latter case, initialized by means of given data and with
parameter value α = 1, may be seen as continuous-domain extension of the assignment flow,
that is parametrized according to (3.5) and operates at the smallest spatial scale in terms of the
size |Ni | of uniform neighborhoods (2.7) (in the discrete formulation (2.28): nearest neighbor
averaging). We illustrate this by a numerical example (Section 4.3), based on discretizing (4.1)
and applying an algorithm that mimics the S-flow and converges to a local minimum of the
non-convex functional (4.1), by solving a sequence of convex programs.
We point out that M could be turned into a Riemannian manifold using a metric that reflects
images features (edges etc.), as was proposed with the Laplace-Beltrami framework for image
denoising [KMS00]. In this work we focus on the essential point, however, that distinguishes
image denoising from image labeling, i.e. the interaction of the two terms (4.1) that essentially
is a consequence of the information geometry of the assignment manifold W (2.19).

4.1 Well-Posedness
Based on (2.3) we define the closed convex set
D 1 (M) = {S ∈ H 1 (M; Rc ) : S(x) ∈ ∆c a.e. in M}.

(4.2)

and focus on the variational problem
inf

S∈D 1 (M)

Eα (S),

(4.3)

with Eα given by (4.1). Eα is smooth but nonconvex. We specify the set of minimizers (Prop. 4.2).
Recall notation (2.1).
Lemma 4.1 Let p ∈ ∆c . Then kpk = 1 if and only if p ∈ Bc .
Proof The ‘if’ statement is obvious. As for the ‘only if’, suppose p 6∈ Bc , i.e. pi < 1 for all
i ∈ [c]. Then p2i < pi and kpk2 < kpk1 = 1.
Proposition 4.2 The functional Eα : D 1 (M) → R given by (4.1) is lower bounded,
Eα (S) ≥ −α Vol(M) > −∞,

∀S ∈ D 1 (M).

(4.4)

Continuous-Domain Assignment Flow
This lower bound is attained at some point in
(
{Se1 , . . . , Sec },
if α > 0,
arg min Eα (S) =
{Sp : M → ∆ : p ∈ ∆}, if α = 0,
S∈D 1 (M)

17

(4.5)

where, for any p ∈ ∆, Sp denotes the constant vector field x 7→ Sp (x) = p.
Proof Let p ∈ ∆. Then kpk2 ≤ kpk1 = 1. It follows for S ∈ D 1 (M) that
Eα (S) ≥ −αkSk2M ≥ −αk1kM = −α Vol(M),

(4.6)

which is (4.4).
We next show that the right-hand side of (4.5) specifies minimizers of Eα . For any p ∈ ∆,
the constant vector field Sp is contained in D 1 (M). Consider specifically Sei , i ∈ [c]. Since
kSei (x)k = kei k = 1 and DSei ≡ 0, the lower bound is attained, Eα (Sei ) = −α Vol(M),
and the functions {Se1 , . . . , Sec } minimize Eα , for every α ≥ 0. If α = 0, then the constant
functions Sp are minimizers as well, for any p ∈ ∆, since then
Eα (Sp ) = kDSp k2M = 0 = −0 · Vol(M).

(4.7)

We conclude by showing that no minimizers other than (4.5) exist. Let S∗ ∈ D 1 (M) be
another minimizer of Eα with Eα (S∗ ) = −α Vol(M). We distinguish the two cases α = 0 and
α > 0.
If α = 0, then S∗ satisfies (4.7) and kDS∗ k2M = 0. Since kDS∗;i kM ≤ kDS∗ kM = 0 for
every i ∈ [c], S∗ is constant by Lemma 2.2, i.e. a p ∈ ∆ exists such that S∗ = Sp a.e.
If α > 0, then using the equation Eα (S∗ ) = −α Vol(M) and kS∗ (x)k2 ≤ 1 gives
α Vol(M) ≤ kDS∗ k2M + α Vol(M) = kDS∗ k21;M − Eα (S∗ ) = αkS∗ k2M
≤ αk1kM = α Vol(M),

(4.8 a)
(4.8 b)

which shows kDS∗ kM = 0 and hence by Lemma 2.2 again S∗ = Sp for some p ∈ ∆. The
preceding inequalities also imply Vol(M) = kS∗ k2M , i.e. kS∗ (x)k = 1 a.e. By Lemma 4.1, we
conclude S∗ = Sp with p ∈ Bc , that is S∗ ∈ {Se1 , . . . , Sec }.
Proposition 4.2 highlights the effect of the concave term of the objective Eα (4.1): labelings are
enforced in the absence of data. Below, the latter are taken into account (i) by imposing non-zero
boundary conditions and (ii) by initalizing a corresponding gradient flow (Section 4.3).
4.2 Fixed Boundary Conditions
In this section, we consider the case where boundary conditions are imposed by restricting the
feasible set of problem (4.3) to

A1g (M) = {S ∈ D1 (M) : S − g ∈ H01 (M; Rc )} = g + H01 (M; Rc ) ∩ D1 (M)
(4.9)
for some fixed g that prescribes simplex-valued boundary values (in the trace sense). As intersection of a closed affine subspace and a closed convex set, A1g (M) is closed convex.
Weak lower semicontinuity is a key property for proving the existence of minimizers. In the
case of Eα (4.1) this is not immediate, due to the lack of convexity.

18

F. Savarino and C. Schnörr

Proposition 4.3 The functional Eα given by (4.1) is weak sequentially lower semicontinuous
on A1g (M), i.e. for any sequence (Sn )n∈N ⊂ A1g (M) weakly converging to S ∈ A1g (M), the
inequality
Eα (S) ≤ lim inf Eα (Sn )

(4.10)

n→∞

holds.
Proof Let Sn * S converge weakly in A1g (M) ⊂ H01 (M; Rc ). Then, by Prop. 2.3(c),
kSk1;M ≤ lim inf kSn k1;M .

(4.11)

n→∞

Since S, Sn ∈ A1g (M), we also have (Sn − g) * (S − g) in H01 (M; Rc ) by (4.9) and
consequently Sn → S strongly in L2 (M; Rc ) due to (2.34). Taking into account (4.11) and
lim inf n→∞ ksn kM = limn→∞ kSn kM = kSkM , we obtain

Eα (S) = kSk21;M − (1 + α)kSk2M ≤ lim inf kSn k21;M + lim inf − (1 + α)kSn k2M
n→∞

n→∞

(4.12 a)
≤ lim inf Eα (Sn ).

(4.12 b)

n→∞

We are now prepared to show that Eα attains its minimal value on A1g (M), following the basic
proof pattern of [Zei85, Ch. 38].
Theorem 4.4 Let Eα be given by (4.1). There exists S∗ ∈ A1g (M) such that
Eα∗ := Eα (S∗ ) =

inf

S∈A1g (M)

Eα (S).

(4.13)

Proof Let (Sn )n∈N ⊂ A1g (M) be a minimizing sequence such that
lim Eα (Sn ) = Eα∗ .

(4.14)

n→∞

Then there exists some sufficiently large n0 ∈ N such that
1 + Eα∗ ≥ Eα (Sn ) = kSn k21;M − (1 + α)kSn k2M ,

∀n ≥ n0 .

(4.15)

Since Sn (x) ∈ ∆ for a.e. x ∈ M, we have kSn k2M ≤ Vol(M) and hence obtain
kSn k21;M ≤ 1 + Eα∗ + (1 + α)kSn k2M ≤ 1 + Eα∗ + (1 + α) Vol(M),

∀n ≥ n0 . (4.16)

Thus the sequence (Sn )n∈N ⊂ H 1 (M; Rc ) is bounded and, by Prop. 2.3(c), we may extract a
weakly converging subsequence Snk * S∗ ∈ H 1 (M; Rc ). Since A1g (M) ⊂ H 1 (M; Rc ) is
closed convex, Prop. 2.3(a) implies S∗ ∈ A1g (M). Consequently, by Prop. 4.3 and (4.14),
Eα (S∗ ) ≤ lim inf Eα (Snk ) = lim Eα (Snk ) = Eα∗
k→∞

k→∞

which implies Eα (S∗ ) = Eα∗ , i.e. S∗ ∈ A1g (M) minimizes Eα .

(4.17)

19

Continuous-Domain Assignment Flow
4.3 Numerical Algorithm and Example
We consider the variational problem (4.13)
Z
inf
kDSk2 − αkSk2 dx,
1
S∈Ag (M)

(4.18)

M

for some fixed g specifying the boundary values S|∂M = g|∂M , and the problem to compute a
local minimum numerically using an optimization scheme that mimics the S-flow of Proposition
3.5.
Based on (4.9), we rewrite the problem in the form
n
o
2
2
1
inf
kD(g
+
f
)k
−
αkg
+
f
k
dx
+
δ
(g
+
f
)
(4.19 a)
D (M)
M
M
f ∈H01 (M;Rc )
n
o

2
2
1 (M) (g + f )
=
inf
kDf
k
+
2hDg,
Df
i
−
α
kf
k
+
2hg,
f
i
+
δ
+ C,
M
M
D
M
M
1
f ∈H0 (M;Rc )

(4.19 b)
where the last constant C collects terms not depending on f . We discretize the problem as
follows. f becomes a vector f ∈ Rc n with n = |I| subvectors fi ∈ Rc , i ∈ I or alternatively with c = |J | subvectors f j , j ∈ J . The inner product hg, f iM is replaced by hg, f i =
P
P
j
j
i∈[n] hgi , fi i =
j∈[c] hg , f i. We keep the symbols f, g for simplicity and indicate the discretized setting by the subscript n as introduced next.
D becomes a gradient matrix Dn that estimates the gradient of each subvector f j separately,
such that
Ln f := Dn> Dn f

(4.20)
j

is the basic discrete 5-point stencil Laplacian applied to each subvector f . The feasible set
D1 (M) is replaced by the closed convex set
Dn := {f ≥ 0 : h1c , fi i = 1, ∀i ∈ I}.
Thus the discretized problem reads
n
o
inf kDn f k2 + 2hLn g − αg, f i − αkf k2 + δDn (g + f ) .
f

(4.21)

(4.22)

Having computed a local minimum f∗ , the corresponding local minimum of (4.18) is S∗ =
g + f∗ .
In order to compute f∗ , we applied the proximal forward-backward scheme
o
n
1
f (k+1) = arg min kDn f k2 +2hLn g−α(g+f (k) ), f i+
kf −f (k) k2 +δDn (g+f ) , k ≥ 0,
f
2τk
(4.23)
(0)
with proximal parameters τk , k ∈ N and initialization fi , i ∈ I specified further below. The
iterative scheme (4.23) is a special case of the PALM algorithm [BST14, Sec. 3.7]. Ignoring
the proximal term, each problem (4.23) amounts to solve c (discretized) Dirichlet problems with
the boundary values of g j , j ∈ [c] imposed, and with right-hand sides that change during the
iteration since they depend on f (k) . The solutions (f j )(k) , j ∈ J to these Dirichlet problems
depend on each other, however, through the feasible set (4.21). At each iteration k, problem (4.23)
can be solved by convex programming. The proximal parameters τk act as stepsizes such that the
sequence f (k) does not approach a local minimum too rapidly. Then the interplay between the

20

F. Savarino and C. Schnörr

Figure 4.1. Evaluation of the numerical scheme (4.23) that mimics the S-flow of Proposition 3.5. Parameter values: α = 1, τk = τ = 10, ∀k. Top, from left to right: Ground
truth, noisy input data f (0) , iterate f (100) and f∗ resulting from f (100) by a trivial rounding step. S (k) = f (k) + g differs from f (k) by the boundary values corresponding to
the noisy input data. Inspecting the values of f (100) close to the boundary shows that
the influence of boundary noise is minimal. Bottom, from left to right: The iterates
f (10) , f (20) , f (30) , f (40) . Taking into account rounding as post-processing step, the sequence f (k) quickly converges after rounding to a reasonable partition. About 50 more
iterations are required to fix the values at merely few hundred remaining pixels. Slight
rounding of the geometry of the components of the partition, in comparison to ground
truth, corresponds to using uniform weights (2.26) for the assignment flow.
linear form that adapts during the iteration and the regularizing effect of the Laplacians can find
a labeling (partition) corresponding to a good local optimum.
As for g, we chose gi = Li (1S ), i ∈ I at boundary vertices i and gi = 0 at every interior
(0)
vertex i. Consequently, with the initialization fi = Li (1S ), i ∈ I at interior vertices (the
(k)
boundary values of f are zero), the sequence S = g + f (k) mimics the S-flow of Proposition
3.5 where the given data also show up in the initialization S(0) only.
Figure 4.1 provides an illustration using an experiment adopted from [ÅPSS17, Fig. 6], originally designed to evaluate the performance of geometric regularization of label assignments
through the assignment flow in an unbiased way. Parameter values are specified in the caption.
The result confirms that the continuous-domain formulations discussed above represent the assignment flow at the smallest spatial scale.

4.4 A PDE Characterizing Optimal Assignment Flows
Proposition 4.5 Let S∗ solve the variational problem (4.18). Then S∗ satisfies the variational
inequality
hDS∗ , DS − DS∗ iM − αhS∗ , S − S∗ iM ≥ 0,

∀S ∈ A1g (M).

(4.24)

21

Continuous-Domain Assignment Flow
Proof Functional Eα given by (4.18) is Gateaux-differentiable with derivative

hE 0 (α)(S∗ ), SiH −1 (M;Rc )×H01 (M;Rc ) = 2 hDS∗ , SiM − αhS∗ , SiM .

(4.25)

The assertion follows from applying Theorem 2.4.
We conclude this section by deriving a PDE corresponding to (4.24), that a minimizer S∗ is
supposed to satisfy in the weak sense. The derivation is formal in that we adopt the unrealistic
regularity assumption
S∗ ∈ A2g (M),

(4.26)

A2g (M)

with
defined analogous to (4.9). While this will hold for the continuous-domain linear
problems corresponding to (4.23) at each step k of the iteration and for sufficiently smooth ∂M,
it will not hold in the limit k → ∞, since we expect (and wish) S∗ to become discontinuous,
contrary to the regularity assumption (4.26) and the continuity implied by the Sobolev embedding
theorem for M ⊂ Rd with d = 2. Nevertheless, since the PDE provides another interpretation of
the assignment flow, we state it – see (4.32) below – and hope it will stimulate further research.
In view of the assumption (4.26), set
S∗ = g + f∗ ,

f∗ ∈ H02 (M; Rc ).

(4.27)

Inserting S∗ and S = g + h, h ∈ H01 (M; Rc ), into (4.24) and partial integration gives
h−∆S∗ − αS∗ , h − f∗ iM ≥ 0,

(4.28)

where ∆S∗ = (∆S∗;1 , . . . , ∆S∗;c )> applies componentwise. Using the shorthands
να (S∗ ) = −∆S∗ − αS∗ ,

(4.29 a)

µα (S∗ ) = να (S∗ ) − hνα (S∗ ), S∗ iR2 1c ,

(4.29 b)

where hνα (S∗ ), S∗ iR2 denotes the function x 7→ να (S∗ )(x), S∗ (x) , x ∈ M, we have
hµα (S∗ ), S∗ iM = 0

(4.30 a)

since h1c , S∗ (x)i = 1 for a.e. x, and
hµα (S∗ ), SiM = hνα (S∗ ), h − f∗ iM ≥ 0,

(4.30 b)

which is (4.28). Since S(x) ≥ 0 a.e. in M and may have arbitrary support, we deduce from
the inequality hµα (S∗ ), SiM ≥ 0 and from the self-duality of the nonnegative orthant Rc+ that
µα (S∗ ) ≥ 0 a.e. in M. Since also S∗ ≥ 0 a.e., this implies that equation (4.30 a) holds pointwise
a.e. in M:
µα (S∗ )(x)S∗ (x) = να (S∗ )(x)S∗ (x) − να (S∗ )(x)S∗ (x) S∗ (x) = 0

a.e. in M. (4.31)

Substituting να (S∗ ) we deduce that a minimizer S∗ = g + f∗ characterized by the variational
inequality (4.24) weakly satisfies the PDE
RS∗ (−∆S∗ − αS∗ ) = 0,

(4.32)

where RS∗ defined by (2.16) applies RS∗ (x) to vector (−∆S∗ − αS∗ )(x) at every x ∈ M.

22

F. Savarino and C. Schnörr

Remark 4.6 (Comments)
(1) We point out that computing a vector field S∗ satisfying (4.24) is difficult in practice, due to
the nonconvexity of problem (4.18). On the other hand, the algorithm proposed in Section 4.3
in the result illustrated by Figure 4.1 shows that good suboptima can be computed by merely
solving a sequence of simple problems.
(2) As already pointed out at the beginning of this section, the derivation of the PDE (4.32)
is merely a formal one, due to the unrealistic regularity assumption (4.26). In fact, since
ker RS∗ (x) = R1c , equation (4.32) says that S∗ is constant up to a set of measure zero.
While the numerical result (Fig. 4.1) clearly reflects this, the discontinuity of S∗ conflicts
with assumption (4.26).
5 Conclusion
We presented a novel parametrization of the assignment flow for contextual data classification on
graphs. The dominating part of the flow admits the interpretation as Riemannian gradient flow
with respect to the underlying information geometry, unlike the original formulation of the assignment flow. A decomposition of the corresponding potential by means of a non-local graph
Laplacian makes explicit the interaction of two processes: regularization of label assignments and
gradual enforcement of unambiguous decisions. The assignment flow combines these aspects in
a seamless way, unlike traditional approaches where solutions to convex relaxations require postprocessing. It is remarkable that this behaviour is solely induced by the underlying information
geometry.
We studied a continuous-domain variational formulation as counterpart of the discrete formulation restricted to a local discrete Laplacian (nearest neighbor interaction). A numerical algorithm in terms of a sequence of simple linear elliptic problems reproduces results that were
obtained with the original formulation of the assignment flow using completely different numerics (geometric ODE integration). This illustrates the derived mathematical relations.
We outline three attractive directions of further research.
• We clarified in Section 4 that the inherent smooth setting of the assignment flow (2.28) translates under suitable assumptions to the sequence of linear (discretized) elliptic PDE problems
(4.23) together with a simple convex constraint. We did not touch on the limit problem, however. More mathematical work is required here, cf. Remark 4.6.
Since the assignment flow returns image partitions when applied to image features on a grid
graph, the situation reminds us of the Mumford-Shah functional [MS89] and its approximation
by a sequence of Γ-converging smooth elliptic problems [AT90]. Likewise, one may regard
the concave second term of (4.18) together with the convex constraint S ∈ A1g as a vectorvalued counterpart of the basic nonnegative double-well potential of scalar phase-field models
for binary segmentation [Ste91, CT18]. In these works, too, nonsmooth limit cases result
from Γ-converging simpler problems.
• Adopting the viewpoint of evolutionary dynamics [HS03] on label assignment, the assignment
flow may be characterized as spatially coupled replicator dynamics. To the best of our knowledge, our paper [ÅPSS17] seems to be the first one that used information theory to formulate
this spatial coupling. Some consequences of the geometry were elaborated in the present paper
and discussed above.

Continuous-Domain Assignment Flow

23

We point out that the literature on evolutionary dynamics in general, and specifically on
the replicator equation, is vast. We merely point out few works on models involving the
replicator equation and spatial interaction in physics [TC04, dB13], applied mathematics
[NPB11, BPN14], including extensions to scenarios with an infinite number of strategies
(as opposed to selecting from a finite set of labels) – see [AFMS18] and references therein.
In this context, our work might stimulate researchers working on spatially extended evolutionary dynamics in various scientific disciplines. In particular, generalizing our approach
to continuous-domain integro-differential models seem attractive that conform to the assigment flow with non-local interactions (i.e. with larger neighborhoods |Ni |, i ∈ I) and the
underlying geometry.
• Last but not least, our work may support a better understanding of learning with networks.
Our preliminary work on learning the weights (2.26) using the linearized assignment flow
[HSPS19] on a single graph (‘layer’) revealed the model expressiveness of this limited scenario, on the one hand, and that subdividing complex learning tasks in this way avoids ‘black
box behaviour’, on the other hand. We hope that the continuous-domain perspective developed
in this paper in terms of sequences of linear PDEs will support our further understanding of
learning with hierarchical ‘deeper’ architectures.
Acknowledgements
Financial support by the German Science Foundation (DFG), grant GRK 1653, is gratefully acknowledged. This work has also been stimulated by the Heidelberg Excellence Cluster STRUCTURES, funded by the DFG under Germany’s Excellen Strategy EXC-2181/1 - 390900948.
References
[ABM14] H. Attouch, G. Buttazzo, and G. Michaille, Variational Analysis in Sovolev and BV
Spaces: Applications to PDEs and Optimization, 2nd ed., SIAM, 2014.
[AFMS18] L. Ambrosio, M. Fornasier, M. Morandotti, and G. Savaré, Spatially Inhomogeneous
Evolutionary Games, CoRR abs/1805.04027 (2018).
[AN00] S.-I. Amari and H. Nagaoka, Methods of Information Geometry, Amer. Math. Soc.
and Oxford Univ. Press, 2000.
[ÅPSS17] F. Åström, S. Petra, B. Schmitzer, and C. Schnörr, Image Labeling by Assignment,
Journal of Mathematical Imaging and Vision 58 (2017), no. 2, 211–238.
[ARP+ 19] V. Antun, F. Renna, C. Poon, B. Adcock, and A. C. Hansen, On Instabilities
of Deep Learning in Image Reconstruction - Does AI Come at a Cost?, CoRR
abs/1902.05300 (2019).
[AT90] L. Ambrosio and V. M. Tortorelli, Approximation of Functional Depending on Jumps
by Elliptic Functional via Γ-Convergence, Comm. Pure Appl. Math. 43 (1990),
no. 8, 999–1036.
[BPN14] A. S. Bratus, V. P. Posvyanskii, and A. S. Novozhilov, Replicator equations and
Space, Math. Modelling Natural Phenomena 9 (2014), no. 3, 47–67.
[BST14] J. Bolte, S. Sabach, and M. Teboulle, Proximal Alternating Linearized Minimization
for Nonconvex and Nonsmooth Problems, Math. Progr., Ser. A 146 (2014), no. 1-2,
459–494.
[CCP12] A. Chambolle, D. Cremers, and T. Pock, A Convex Approach to Minimal Partitions,
SIAM J. Imag. Sci. 5 (2012), no. 4, 1113–1158.
[CDH16] P. V. Coveney, E. R. Dougherty, and R. R. Highfield, Big Data Need Big Theory too,
Phil. Trans. R. Soc. Lond. A 374 (2016), 20160153.

24

F. Savarino and C. Schnörr

[CT18] R. Cristoferi and M. Thorpe, Large Data Limit for a Phase Transition Model with
the p-Laplacian on Point Clouds, Europ. J. Appl. Math. (2018), 1–47.
[dB13] Russ deForest and A. Belmonte, Spatial Pattern Dynamics due to the Fitness Gradient Flux in Evolutionary Games, Physical Review E 87 (2013), no. 6, 062138.
[E17] Weinan E, A Proposal on Machine Learning via Dynamical Systems, Comm. Math.
Statistics 5 (2017), no. 1, 1–11.
[EHL19] W. E, J. Han, and Q. Li, A Mean-Field Optimal Control Formulation of Deep Learning, Res. Math. Sci. 6 (2019), no. 10, 41 pages.
[Ela17] M. Elad, Deep, Deep Trouble: Deep Learning’s Impact on Image Processing, Mathematics, and Humanity, SIAM News (2017).
[HR17] E. Haber and L. Ruthotto, Stable Architectures for Deep Neural Networks, Inverse
Problems 34 (2017), no. 1, 014004.
[HS03] J. Hofbauer and K. Siegmund, Evolutionary Game Dynamics, Bull. Amer. Math.
Soc. 40 (2003), no. 4, 479–519.
[HSPS19] R. Hühnerbein, F. Savarino, S. Petra, and C. Schnörr, Learning Adaptive Regularization for Image Labeling Using Geometric Assignment, Proc. SSVM, Springer,
2019.
[HZRS16] K. He, X. Zhang, S. Ren, and J. Sun, Deep Residual Learning for Image Recognition,
Proc. CVPR, 2016.
[Jos17] J. Jost, Riemannian Geometry and Geometric Analysis, 7th ed., Springer-Verlag
Berlin Heidelberg, 2017.
[KMS00] R. Kimmel, R. Malladi, and N. Sochen, Images as Embedded Maps and Minimal
Surfaces: Movies, Color, Texture, and Volumetric Images, Int. J. Comp. Vision
39 (2000), no. 2, 111–129.
[LS11] J. Lellmann and C. Schnörr, Continuous Multiclass Labeling Approaches and Algorithms, SIAM J. Imag. Sci. 4 (2011), no. 4, 1049–1096.
[LT19] G.-H. Liu and E. A. Theodorou, Deep Learning Theory Review: An Optimal Control
and Dynamical Systems Perspective, CoRR abs/1908.10920 (2019).
[MS89] D. Mumford and J. Shah, Optimal Approximations by Piecewise Smooth Functions
and Associated Variational Problems, Comm. Pure Appl. Math. 42 (1989), 577–
685.
[NPB11] A. Novozhilov, V. P. PosvNovozh, and A. S. Bratus, On the Reaction–Diffusion
Replicator Systems: Spatial Patterns and Asymptotic Behaviour, Russ. J. Numer.
Anal. Math. Modelling 26 (2011), no. 6, 555–564.
[San10] W. H. Sandholm, Population Games and Evolutionary Dynamics, MIT Press, 2010.
[Sch19] C. Schnörr, Assignment Flows, Variational Methods for Nonlinear Geometric Data
and Applications (P. Grohs, M. Holler, and A. Weinmann, eds.), Springer (in
press), 2019.
[Ste91] P. Sternberg, Vector-Valued Local Minimizers of Nonconvex Variational Problems,
Rocky-Mountain J. Math. 21 (1991), no. 2, 799–807.
[TC04] A. Traulsen and J. C. Claussen, Similarity-Based Cooperation and Spatial Segregation, Phys. Rev. E 70 (2004), no. 4, 046128.
[Zei85] E. Zeidler, Nonlinear Functional Analysis and its Applications, vol. 3, Springer, 1985.
[Zie89] W. P. Ziemer, Weakly Differentiable Functions, Springer, 1989.
[ZSPS19] A. Zeilmann, F. Savarino, S. Petra, and C. Schnörr, Geometric Numerical Integration
of the Assignment Flow, CoRR abs/1810.06970, Inverse Problems: in press (2019).
[ZZPS19a] A. Zern, M. Zisler, S. Petra, and C. Schnörr, Unsupervised Assignment Flow: Label
Learning on Feature Manifolds by Spatially Regularized Geometric Assignment,
CoRR abs/1904.10863 (2019).
[ZZPS19b] M. Zisler, A. Zern, S. Petra, and C. Schnörr, Unsupervised Labeling by Geometric
and Spatially Regularized Self-Assignment, Proc. SSVM, Springer, 2019.

