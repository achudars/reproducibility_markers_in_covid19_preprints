1

Learning Orthogonal Projections in Linear Bandits

arXiv:1906.10981v3 [cs.LG] 24 Oct 2019

Qiyu Kang, and Wee Peng Tay, Senior Member, IEEE

that the independent d-armed bandit problem can be viewed as
a special case of the general linear stochastic bandit problem,
where the set of d available decisions serves as a standard
orthonormal basis of Rd [14].
The linear stochastic bandit model has been successfully
applied to some real-world problems like personalized news
article recommendation [18], advertisement selection [19], and
information retrieval [20]. For example, in news recommendation applications, typical features, including the news’ topic
categories, the users’ race, gender, location, etc., can be treated
as components in each arm or decision vector, while the users’
clicks are the rewards. A plausible recommendation scheme
is to get as many clicks as possible. Another application is in
sequential clinical trials [21]–[23] whose aims are to balance
the correct identification of the best treatment (exploration) and
the effectiveness of the treatment during the trials (exploitation).
Index Terms—Linear bandit, multi-armed bandit, orthogonal
The
classical sequential clinical trials containing d different
projection, discrimination aware
drugs can be modeled as an independent d-armed bandit
problem. However, this may be impractical as treatments may
I. I NTRODUCTION
utilize multiple drugs simultaneously. A more feasible and
ULTI-ARMED bandit (MAB) problems, introduced by general way is to model the treatment decision as a set of
Robbins [1] model the exploration and exploitation trade- mixed drugs instead. An arm corresponds to a mixed drugs
off of sequential decision making under uncertainty. In its treatment with specific dosages of each kind. The reward at
most basic paradigm, at each time step, the decision maker each time step is the curative efficacy after applying the mixed
is given d decisions or arms from which he is supposed to drug treatment to a patient at each trial.
However, in some scenarios, the decision maker is more
select one, and as a response, he observes a stochastic reward
interested
in some criterion other than maximizing the cumuafter each decision-making. The arm chosen at each time step
lative
reward
in the standard linear stochastic bandit model.
is based on the information gathered at all the previous time
One
example
is
a discrimination-aware movie recommendation
steps. Therefore, in order to maximize the expected cumulative
system.
To
avoid
racially discriminatory recommendations, a
reward, exploitation of the current empirically best arm and
user’s
race
should
not play any role in the recommendation
exploration of less frequently chosen arms should be balanced
system.
However,
the
observed reward (number of clicks) may
carefully. Stochastic independence of rewards is assumed for
be
biased
by
racial
factors.
A black user may have a history
different arms in some works like [2]–[10]. In other works like
of
following
a
particular
black
actor, but it does not mean
[11]–[15], reward dependence between arms is assumed, which
that
he
or
she
should
always
be
recommended movies with
allows the decision maker to gather information for more than
black
actors.
For
example,
Netflix
last year angered black
one arm at each time step. One such specific assumption is
subscribers
with
targeted
posters
containing
black actors no
that arms are vectors containing numeric elements, and the
matter
how
minor
their
roles
in
the
film
are
[24].
In principle,
expected reward of choosing each arm is an inner product of
to
avoid
discrimination,
protected
or
sensitive
attributes
such
the arm vector with an unknown parameter vector [14]–[17].
as
race
or
gender
should
not
be
included
in
recommendation
In [14], [15], the authors proposed effective strategies that
balance exploration and exploitation based on the optimism- algorithms. Nonetheless, discarding such attributes directly
in-the-face-of-uncertainty principle. This principle maintains and modeling the reward as a linear function of the other
a high probability confidence set for the estimated parameter unprotected attributes may introduce system bias during the
vector, and at each time step, the decision maker chooses an arm learning process. In another example, some clinical trials seek
and a parameter vector from the decision set and the confidence to maximize the curative effect on one disease of a mixed drugs
set respectively, so that their inner product is maximized. Note treatment, while at the same time the patients who have this
disease may concurrently have another disease. These patients
This research is supported in part by the Singapore Ministry of Education
need to take other drugs that have a positive or negative impact
Academic Research Fund Tier 2 grant MOE2018-T2-2-019.
on the targeted disease. One typical example is the treatment
The authors are with the School of Electrical and Electronic Engineering,
Nanyang Technological University, Singapore. Email: kang0080@e.ntu.edu.sg, of hypertension in chronic kidney disease. Hypertension is
wptay@ntu.edu.sg.
present in more than 80% of the patients with chronic kidney
Abstract—In a linear stochastic bandit model, each arm is a
vector in an Euclidean space and the observed return at each
time step is an unknown linear function of the chosen arm at that
time step. In this paper, we investigate the problem of learning
the best arm in a linear stochastic bandit model, where each
arm’s expected reward is an unknown linear function of the
projection of the arm onto a subspace. We call this the projection
reward. Unlike the classical linear bandit problem in which the
observed return corresponds to the reward, the projection reward
at each time step is unobservable. Such a model is useful in
recommendation applications where the observed return includes
corruption by each individual’s biases, which we wish to exclude
in the learned model. In the case where there are finitely many
arms, we develop a strategy to achieve O(|D| log n) regret, where
n is the number of time steps and |D| is the number of arms.
In the case where each arm is chosen from an infinite compact
set, our strategy achieves O(n2/3 (log n)1/2 ) regret. Experiments
verify the efficiency of our strategy.

M

2

disease, and drugs like ACEi and ARB targeted for reducing by projecting both the decision arm and context vectors into a
proteinuria may also have an effect on blood pressure [25]. subspace.
To study the effect of a mixed drug treatment targeted for
In the stochastic multi-objective multi-armed bandit
controlling hypertension, the decision maker is supposed to (MOMAB) problem [30]–[33], the reward of making one
discount the impact of drugs like ACEi and ARB in the decision decision is a vector rather than the scalar reward in the
process.
standard multi-armed bandit problem. Because of the possible
In this paper, we propose a linear stochastic bandit for- conflicting objectives, a set of Pareto optimal arms [31] [34]
mulation that maximizes the (expected) cumulative reward is considered in the MOMAB problem instead of a single
over a subspace of decision attributes, based on the reward best arm. Scalarization techniques can be used to transform
observed for the full space. Specifically, each arm is projected a MOMAB problem to a single objective MAB, and an arm
orthogonally onto a target subspace U. The reward is then belonging to the Pareto optimal set is regarded as a best arm in
decomposed into two components, one of which is due to U, a particular scalarization function. In our work, we consider the
and the other is due to U⊥ . We call the first component the decomposition of the reward where the decomposition is similar
projection reward and the second component the corruption. to a two-objective MAB applied with a linear scalarization
We develop a strategy that achieves1 O(|D| log n) cumulative function [31]. However, the difference between our model and
projection regret when there are finitely many |D| arms and MOMAB with linear scalarization is that we can only observe
where n is the number of time steps. In the case where the the sum of the (desired) projection reward and (undesired)
arms are drawn from an infinite compact set, we achieve a corruption at each time step rather than a reward vector
cumulative projection regret of O(n2/3 (log n)1/2 ). Here, the consisting of the two components as in MOMAB. Furthermore,
projection regret at each time step is defined as the difference our objective is to minimize the cumulative projection regret
between the current expected projection reward of making rather than minimizing the three types of regret defined in [31].
one decision and the oracle best expected projection reward.
This algorithm is based on the t -greedy policy [2], which
B. Our Contributions
is a simple and well-known algorithm for the standard finite
In this paper, we study the orthogonal projection problem
multi-armed bandit problem.
in linear bandits, where our aim is to maximize the expected
projection reward, which is an unknown linear function of a
A. Related Work
projection of the chosen arm onto a target subspace U ⊆ Rd .
Two categories of work are related to our problem of interest: In the case where there are finitely many arms, we develop a
1) the linear stochastic bandit model; and 2) the multi-objective strategy to achieve O(|D| log n) cumulative projection regret,
where n is the number of time steps and |D| is the number
Pareto bandit problem.
In the linear stochastic bandit model, the decision maker of arms. In the case where the arm is chosen from an
predicts the reward of an arm in Rd based on the given context infinite compact set, our strategy achieves O(n2/3 (log n)1/2 )
vector of this decision. In [26], the author proposed an algorithm cumulative projection regret.
In the linear stochastic bandit literature, the best cumulative
based on least squares estimation and high√probability confibound for the case of a compact decision set
dence bounds, and showed that it has O( dn log3/2 (n|D|)) regret upper
√
regret upper bound for the case of finitely many decisions, is O(d n polylog(n)) [14], [15], where polylog(n) means
where |D| is the number of arms. The work [22] extended a polynomial in log n. If a further smoothness assumption
[26] to the problem with an arbitrary
√ compact set of arms same as that in [15]√is made, we show that it is possible
and presented a policy with O(d n log3/2 n) regret upper to achieve the O(d n) projection regret upper bound in
bound. References [14], [15] further improved the regret our problem
√ formulation. However, the existence of a policy
upper bound using smaller confidence sets established using with O(d n polylog(n)) cumulative projection regret for the
martingale techniques. In [27], the authors proposed a linear general compact decision set remains an open question for our
bandit formulation with hidden features where the reward of problem. We verify our proposed policies on both synthetic
choosing one decision is the sum of two components, one of simulations and experiments on a wine quality dataset [35].
which is a linear function of the observable features, and the
The rest of this paper is organized as follows. In Section II,
other is a linear function of the unobservable features. They we present our system model and assumptions. In Section III,
applied a upper confidence bound (UCB)-type linear bandit we introduce our strategies and prove that they achieve
policy with a coordinate descent [28], [29] algorithm in which sublinear cumulative projection regret. In Section IV, we
estimating the hidden features and the unknown coefficients present simulations and experiments to verify the performance
jointly over time is achieved. Our work is different from the of our strategies. Section V concludes the paper.
above-mentioned papers in that we seek to maximize only the
Notations: We use E c to denote the complement of the
cumulative projection reward rather than the reward observed event E. The indicator function 1A (ω) = 1 if and only if
for the full space. Furthermore, the projection reward is formed ω ∈ A. I is the identity matrix. For x, y ∈ Rd , let x| be the
transpose p
of x , and hx, yi = x| y. We use kxk2 to denote the
1 For non-negative functions f (n) and g(n), we write f (n) = O(g(n)) if
L
to denote the weighted L2 -norm
p2 -norm hx, xi and kxkAd×d
lim supn→∞ f (n)/g(n) < ∞, f (n) = o(g(n)) if limn→∞ f (n)/g(n) =
hx,
Axi
for
any
A
∈
R
is a positive definite matrix.
0, and f (n) = Θ(g(n)) if 0 < lim inf n→∞ f (n)/g(n) ≤
lim supn→∞ f (n)/g(n) < ∞.
We use U(a, b) to denote the continuous uniform distribution

3

whose density function has support [a, b], and N (0, ϑ2 ) to
denote the Gaussian distribution with variance ϑ2 and mean 0.
II. S YSTEM M ODEL
d

Let D ⊆ R be a compact set of decisions or arms from
which the decision maker has to choose an arm Xt at each
time step t. The observed return rt after choosing Xt is given
by
rt = hXt , θi + ηt ,

(1)

where θ ∈ Rd is a fixed but unknown parameter and ηt is
a zero mean independent and identically distributed (i.i.d.)
random noise.
In the standard linear bandit problem, the performance of
a policy is measured by the difference between the decision
maker’s cumulative reward and the cumulative reward achieved
by the oracle policy with knowledge of θ. Formally, the goal
of the decision maker is to minimize the cumulative regret
over n time steps defined by
R(n) =

n
X


hX ∗ , θi − E [hXt , θi] ,

(2)

t=1

where X ∗ = arg maxx∈D hx, θi is the optimal arm in the
standard linear bandit problem.
In our orthogonal projection linear bandit model, we consider
a decomposition of the observed return into two components.
Let U, whose dimension is not more than d, be a subspace of
the Euclidean vector space Rd . Let U⊥ = {q ∈ Rd : hq, pi =
0, ∀p ∈ U} be the orthogonal complement of U. It is a standard
result [36] that Rd = U⊕U⊥ , and each w ∈ Rd can be written
uniquely as a sum p + q, where p ∈ U and q ∈ U⊥ . We define
the linear orthogonal projection operator as PU : Rd 7→ U such
that PU (w) = p. The operator PU can be represented by a
d × d matrix whose rank is the dimension of U. The expected
observed return can therefore be decomposed as
hXt , θi = hPU (Xt ) + PU⊥ (Xt ), PU (θ) + PU⊥ (θ)i
= hPU (Xt ), PU (θ)i + hPU⊥ (Xt ), PU⊥ (θ)i.

(3)

We call hPU (Xt ), PU (θ)i the projection reward on the subspace
U, which is denoted by rU,t . The other part of the observed
return, hPU⊥ (Xt ), PU⊥ (θ)i, is called the corruption. Different
from the standard linear bandit model, where the reward can
be observed (with random perturbation), in our orthogonal
projection linear bandit model, we do not directly observe the
projection reward rU,t . Furthermore, the cumulative projection
regret is defined as
RU (n) =

n
X
t=1

than the cumulative regret R(n), base on the observed returns
r1 , . . . , rn−1 .
For a concrete illustration, consider again the discrimination
prevention problem in a movie recommendation system described in Section I. An arm is a movie to be recommended to
a specific set of users and is represented by the features of the
movie. The features include the movie genre, movie length,
music types used, ages and races of the actors or actresses, etc.
Suppose that each arm is a d dimensional vector, where some of
its elements like the ages and races may lead to discriminatory
recommendations and are “protected”. To eliminate the effect
of those features when performing the recommendations, the
system should consider only the non-protected dimensions. In
[37], the authors proposed to control the discrimination effect
in a linear regression model, where PU in their formulation
can also be viewed as a projection matrix.
We use span(D) to denote the set of finite linear combinations of arms in D. In the ideal case, if at each time step t the
return rt is available without the random noise ηt , and if D
is specified by linear inequality constraints, the problem then
degenerates to a linear programming problem that finds x ∈ D
to maximize
hPU (x), PU (θ)i
= hPU (x), θi
= PU (x), Pspan(D) (θ) + Pspan(D)⊥ (θ)
= PU (x), Pspan(D) (θ) + PU (x), Pspan(D)⊥ (θ) .

(6)

However, since Pspan(D)⊥ (θ) is unobservable even in the ideal
case, and arg maxx∈D PU (x), Pspan(D) (θ) is not equal to
XU∗ (θ) in (5) when PU (x), Pspan(D)⊥ (θ) 6= 0, a linear
cumulative projection regret is therefore inevitable in the worst
case. In order to get a sublinear cumulative regret, further
assumptions are required.
A possible assumption is PU (x), Pspan(D)⊥ (θ) = 0 for
all x and θ. This is equivalent to saying U ⊆ span(D). To
see this, note that this assumption means that Pspan(D)⊥ (θ) ∈
kernel(PU ) = PU⊥ = PU⊥ . It follows that span(D)⊥ ⊆ U⊥ ,
which is equivalent to U ⊆ span(D). We make the following
assumption throughout this paper.
Assumption 1. We have U ⊆ span(D), the dimension of
span(D) is k, where dim(U) ≤ k ≤ d, and kPU (θ)k2 > 0.
Furthermore, the decision maker has access to k linearly
independent arms in D.

We use Dk ⊆ D to denote the set containing the k linearly

hPU (XU∗ (θ)), PU (θ)i − E [hPU (Xt ), PU (θ)i] , independent arms in Assumption 1. The condition kPU (θ)k2 >
0 guarantees θ ∈
/ U⊥ . We also make another assumption which
(4) is standard in the literature for the linear bandit problem.

where
XU∗ (θ) = arg max hPU (x), PU (θ)i

(5)

x∈D

is the best projection arm. The objective in our model is
to minimize the cumulative projection regret RU (n) rather

Assumption 2. We have kθk2 ≤ S, and maxx∈D kxk2 ≤ Z,
where S and Z are positive finite constants. The random
variables ηt are drawn i.i.d. from a zero-mean
sub-Gaussian


2 2
distribution with parameter ϑ, i.e., E eξηt ≤ eξ ϑ /2 for all
ξ ∈ R.

4

III. S TRATEGIES AND R EGRET A NALYSIS

B. Regret Analysis
1) Finitely Many Arms:

A. Greedy Projection Strategy
The decision set D may contain finitely or infinitely many
arms. In this section, we present a strategy with two settings,
the first is applicable for the finite-arm case, and the other is
applicable for the infinite-arm case. In the following, we use
θ̂t to denote the L2 regularized least square estimate of θ with
parameter λ > 0, after the decision making at time step t:
θ̂t = Vt−1

t
X

ri Xi ,

(7)

Theorem 1. Suppose Assumptions 1 and 2 hold, D contains
finitely many arms,


24dZ 2 ϑ2
α > max
, 10
(10)
minx6=XU∗ ∆2x δDk
where ∆x = hPU (XU∗ (θ)) − PU (x), PU (θ)i, and δDk is a
constant that depends on Dk . Then GENTRY has cumulative
projection regret of order O(|D| log n), where n is the number
of time steps and |D| is the number of arms.

i=1

Pt
where Vt = λI + i=1 Xi Xi| . We propose a Greedy projEctioN sTRategY (GENTRY) that accounts for the corruption in
the observed return to achieve sublinear cumulative projection
regret. GENTRY is based on the t -greedy policy [2], which
is a simple and well-known algorithm for the standard finite
multi-armed bandit problem. At each time step t, the t -greedy
policy chooses with probability 1 − t the arm with the highest
empirical average reward, and with probability t a random arm.
Since in our problem, we focus our attention on the projection
reward, GENTRY chooses with probability 1 − t the arm with
the highest empirical average projection reward defined as:
D
E
r̄t−1 (x) = PU (x), PU (θ̂t−1 ) ,
(8)
for each arm x ∈ D. Another difference is, at each time step
t, GENTRY chooses with probability t a random arm from
Dk rather than D. We define two different settings of t as
follows:




αk
αk
and it = min 1, 1/3 ,
ft = min 1,
(9)
t
t
to handle the finite- and infinite-arm cases, respectively. The
hyperparameter α > 0 is a fixed positive constant. The
GENTRY strategy is summarized in Algorithm 1.
Algorithm 1 GENTRY
Inputs: Set t to ft or it according to whether the problem
is finite-arm or infinite-arm. Set hyperparameters α > 0
and λ > 0.
1: Set t = 1.
2: loop
3:
Set t using (9).
4:
With probability 1−t , choose arg maxx∈D r̄t−1 (x) and
with probability t choose a random arm from Dk .
5:
Update r̄(xt ) using (8).
6:
Set t = t + 1.
7: end loop

Theorem 1 shows that if we choose α to be sufficiently
large, GENTRY achieves order optimal regret.
Proof: For any arm x ∈ D, let the random variable Nt (x)
denote the total number of time steps within P
the first t time
n
steps that the arm x was chosen, i.e., Nn (x) = t=1 1{Xt =x} .
Since

n 
X
E [RU (n)] = E
hPU (XU∗ (θ)), PU (θ)i − hPU (Xt ), PU (θ)i
t=1

=

X

E [Nn (x)] ∆x

x∈D

=

X
x∈D

∆x

n
X

P (Xt = x) ,

(11)

t=1

where ∆x = hPU (XU∗ (θ)) − PU (x), PU (θ)i is the projection
regret of arm x, it is sufficient to show that the probability
P (Xt = x) = O(t−1 ) for all suboptimal x ∈ D\{XU∗ }. We
have
t
P (Xt = x) ≤ + (1 − t )P (r̄t−1 (x) ≥ r̄t−1 (XU∗ (θ))) ,
k
(12)
and
P (r̄t−1 (x) ≥ r̄t−1 (XU∗ (θ)))
∆x
= P (r̄t−1 (x) ≥ r̄t−1 (XU∗ (θ)), r̄t−1 (x) ≥ hPU (x), PU (θ)i +
)
2


∆x
∗
+ P r̄t−1 (x) ≥ r̄t−1 (XU (θ)), r̄t−1 (x) < hPU (x), PU (θ)i +
2


∆x
≤ P r̄t−1 (x) ≥ hPU (x), PU (θ)i +
2


∆x
+ P r̄t−1 (XU∗ (θ)) ≤ hPU (XU∗ (θ)), PU (θ)i −
.
2
(13)

We next bound the first term on the right-hand side of (13).
The analysis for the second term is similar.
From the proof of Theorem 2 in [14], we have that with
probability at least 1 − δ, for all t ≥ 0 and x ∈ Rd ,
s
!


D
E
1+t
1/2
x, θ̂t − θ ≤ kxkV−1 ϑ d log
+λ S .
In the following theorems, we give a sufficient condition
t
δ
on the hyperparameter α for the strategy to achieve O(log n)
(14)
for the finite-arm case. We also show that our infinite-arm
p
strategy achieves O(n2/3 (log n)1/2 ) regret for the infinite-arm
Let βt
=
ϑ 3d log(1 + t) + λ1/2 S. Since
case. The empirical impact of α is further studied in Section IV. hPU (x), PU (θ)i = hPU (x), θi for the orthogonal projection
Parameter λ can be set as a moderate value like Z 2 .
operator PU , by setting δ = t12 , we have

5

Because PU (x) ∈ span(D), using (17), we obtain
s
ϑ


d log

1+t
δ



+ λ1/2 S = ϑ

p

d log (t2 (t + 1)) + λ1/2 S

< βt ,
so that for all x ∈ D\{XU∗ }, it follows from (14) that


P r̄t (x) − hPU (x), PU (θ)i > βt kPU (x)kV−1
t
D
E
≤ P PU (x), θ̂t − hPU (x), θi >


 p
1/2
2
ϑ d log (t (t + 1)) + λ S kPU (x)kV−1 .




∆x
≤ βt kPU (x)kV−1
t
2
!

2
kPU (x)k22
∆x
≤
≤P
2βt
λD
t,min


2
4βt kPU (x)k22
= P λD
≤
t,min
∆2x


4βt2 Z 2
k
≤ P λD
≤
t,min
minx6=XU∗ ∆2x

P

(18)

t

Consider the function
1
(15)
≤ 2.
X
t
f : {y : y ∈ span(D), kyk22 = 1} 7→ y| (
xx| )y. (19)
We then have


x∈Dk
∆x
P r̄t (x) ≥ hPU (x), PU (θ)i +
2

We have f (y) > 0 since Dk contains k linearly independent
∆x ∆x
> βt kPU (x)kV−1 arms. We also have δDk := inf f (y) > 0 since f (y) is a
= P r̄t (x) ≥ hPU (x), PU (θ)i +
t
2
2
continuous function on a compact set. Define the event


∆x


·P
> βt kPU (x)kV−1
t
4βt2 Z 2
2


, for all x ∈ Dk .
δDk Nt (x) >
∆x ∆x
minx6=XU∗ ∆2x
≤ βt kPU (x)kV−1
+ P r̄t (x) ≥ hPU (x), PU (θ)i +
t
2
2


Under this event, using the last equality in (17), we obtain
∆x
4βt2 Z 2
·P
≤ βt kPU (x)kV−1
k
λD
t
2
t,min > minx6=X ∗ ∆2x . Therefore, we have
U


∆x
1
≤ βt kPU (x)kV−1 .
(16)
≤ 2 +P


t
t
2
4βt2 Z 2
k
P λD
≤
t,min
minx6=XU∗ ∆2x
We next show that when the hyperparameter α is sufficiently


X
large,
4βt2 Z 2


≤
P Nt (x) ≤
∆x
minx6=XU∗ ∆2x δDk
x∈Dk
P
≤ βt kPU (x)kV−1 = O(t−1 ).

t
2
X 
4βt2 Z 2
≤
Ñ
(x)
≤
,
(20)
P
t
Since
minx6=XU∗ ∆2x δDk
X
x∈Dk
Vt = λI +
Nt (x)xx|
x∈D

= λPspan(D)⊥ (I) + λPspan(D) (I) +

X

Nt (x)xx| ,

x∈D

it can be shown by induction that the eigenvectors of Vt can
be divided into two groups, one in which all the eigenvectors
are in span(D), and another in which all the eigenvectors
are in span(D)⊥ . Let λD
t,min be the smallest eigenvalue of Vt
Dk
in
span(D)
and
λ
t,min be the smallest eigenvalue of (λI +
P
|
N
(x)xx
)
in span(D). If we define
t
x∈Dk
y1 =

arg min

hy, Vt yi,

y∈span(D),kyk22 =1

|
λD
t,min = y1 Vt y1

X

Nt (x)xx| )y1

x∈Dk

≥
=

min

y∈span(D),kyk22 =1
k
λD
t,min

y| (λI +

t
1 X
P Ñt (x) ≤
i
2k i=1

!

For t ≥ t0 = dαke, t = ft =

t
1 X
≤ exp −
i
10k i=1
αk
t ,

!
(21)

we then obtain

0

we then have
≥ y1| (λI +

where Ñt (x) ≤ Nt (x) is the number of times that arm x ∈ Dk
was chosen randomly during the first t time steps. From the
proof of Theorem 3 in [2] (where Bernstein’s inequality [38]
was used), we have

X

Nt (x)xx| )y

x∈Dk

(17)

t
t
t
1 X
1 X
1 X
i =
i +
i
2k i=1
2k i=1
2k
i=t0 +1
Z t+1
α
1
αk
≥ +
dx
2
2k t0 +1 t
α α
t+1
≥ + log
2
2
αk + 2
α
(t + 1)e
= log
,
2
αk + 2

(22)

6

4β 2 Z 2

t
where e is Euler’s number. If α2 log (t+1)e
αk+2 ≥ minx6=X ∗ ∆2x δDk ,
U
from (20), we have


4βt2 Z 2
k
P λD
≤
t,min
minx6=XU∗ ∆2x


X
4βt2 Z 2
≤
P Ñt (x) ≤
minx6=XU∗ ∆2x δDk
x∈Dk


X
α
(t + 1)e
≤
P Ñt (x) ≤ log
2
αk + 2
x∈Dk
!
t
X
1 X
≤
P Ñt (x) ≤
i
2k i=1
x∈Dk
!
t
X
1 X
i
≤
exp −
10k i=1
x∈Dk
α

αk + 2 10
,
(23)
≤k
(t + 1)e

or 2) when Xt is chosen as arg maxx∈D r̄t−1 (x). We denote
these two event as Et and Etc respectively.
E [RU (n)]
" n
#
X
∗
=E
hPU (XU ) − PU (Xt ), PU (θ)i
t=1

=

n
X

t E [ hPU (XU∗ ) − PU (Xt ), PU (θ)i | Et ]

t=1

+ (1 − t )E [ hPU (XU∗ ) − PU (Xt ), PU (θ)i | Etc ]
n
X
≤
(2ZSt + E [ hPU (XU∗ ) − PU (Xt ), PU (θ)i | Etc ]) ,
t=1

(26)

where the inequality follows from Assumption 1, kPU (x)k2 ≤
kxk2 for all x, and the Cauchy-Schwarz inequality.
We next derive an upper bound for the second term
E [ hPU (XU∗ ) − PU (Xt ), PU (θ)i | Etc ] in the sum on the rightwhere the third inequality follows from (22), the penultimate hand side of (26). c
Under the event Et , we have
inequality follows fromp(21), and the last inequality from (22).
1/2
Recall that βt = ϑ 3d log(t + 1) + λ S. Therefore, if
hPU (XU∗ ) − PU (Xt ), PU (θ)i
D
E
24dZ 2 ϑ2
, 10}, when t is sufficiently large, the
α > max{ min
2
x6=X ∗ ∆x δDk
= PU (XU∗ ) − PU (Xt ), PU (θ̂t−1 )
U
2 2
4βt Z
D
E
condition α2 log (t+1)e
αk+2 ≥ minx6=X ∗ ∆2x δDk is satisfied. We have
U
+ PU (XU∗ ) − PU (Xt ), PU (θ) − PU (θ̂t−1 )


D
E
∆x
≤ PU (XU∗ ) − PU (Xt ), PU (θ) − PU (θ̂t−1 ) ,
(27)
P r̄t (x) ≥ hPU (x), PU (θ)i +
2


1
∆x
where the inequality follows from Xt = arg maxx∈D r̄t−1 (x).
≤ 2 +P
≤ βt kPU (x)kV−1
t
We use the same βt defined in the proof of Theorem 1. For
t
2


2 2
any
t ≥ 2, let the event
1
4βt Z
k
≤ 2 + P λD
n
o
t,min ≤
t
minx6=XU∗ ∆2x
∗
∗
−1 βt−1
F
=
hP
(X
−
X
),
P
(θ)i
≥
kP
(X
−
X
)k
.
t
U
t
U
U
t
U
U
V
α


t−1
1
αk + 2 10
≤ 2 +k
From (14) and (27), since hPU (x), PU (θ)i = hPU (x), θi for
t
(t + 1)e
the orthogonal projection operator PU , we have
−1
= o(t ),
(24)
P (Ft+1 )
D

where the first inequality follows from (16), the second
E
∗
∗
inequality from (18) and the last inequality from (23).
≤ P PU (XU − Xt+1 ), θ − θ̂t ≥ kPU (XU − Xt+1 )kV−1 βt
t
A
similar
argument
shows
that
D
E
∆
=
o(t−1 ). Then, ≤ P P (X ∗ − X ), θ − θ̂
P r̄t (x) ≤ hPU (XU∗ ), PU (θ)i − 2x
U
t+1
t
U
using (12) and (13), we have

 s


α
1+t
−1
−1
1/2
∗
P (Xt = x) ≤ + o(t ) = O(t ),
(25)
−1
d
log
≥
kP
(X
−
X
)
k
ϑ
+
λ
S
U
t+1
U
Vt
t
t2
From (11), we conclude that
1
≤ 2,
(28)
t
E [RU (n)] = O(|D| log n).
where the final inequality follows from the concentration
The proof of Theorem 1 is now complete.
inequality (14) with δ = t12 . We then have
2) Infinitely Many Arms:


c
E hPU (XU∗ ) − PU (Xt+1 ), PU (θ)i Et+1


Theorem 2. Suppose Assumptions 1 and 2 hold, and
c
= E hPU (XU∗ − Xt+1 ), PU (θ)i Et+1
, Ft+1 P (Ft+1 )
D is a compact set containing infinitely many arms.
h
i
c
c
+ E hPU (XU∗ − Xt+1 ), PU (θ)i1Ft+1
Et+1
Then,
 GENTRY
n √ ohas cumulative projection regret of order
h
i
O max k, d n2/3 (log n)1/2 , where n is the number of
2ZS
c
≤ 2 + E kPU (XU∗ − Xt+1 )kV−1 βt Et+1
(29)
time steps.
t
t
"
#
s
2ZS
1
Proof: The contribution to the projection regret at time
c
≤ 2 + E 2Zβt
Et+1
(30)
D
t
λt,min
step t comes from two cases: 1) when Xt is chosen randomly,

7

"
#
s
2ZS
1
≤ 2 + E 2Zβt
t
λD
t,min
v "
#
u
u
2ZS
1
≤ 2 + 2Zβt tE D
t
λt,min
v "
#
u
u
2ZS
1
t
≤ 2 + 2Zβt E Dk ,
t
λt,min

(31)

(32)

where
•
•

(29) follows from (28);
(30) follows because kPU (XU∗ − Xt )k2V−1 ≤ kPU (XU∗ −
t

Xt )k22 /λD
t,min ;
• (31) follows from Jensen’s inequality; and
• (32) follows from (17).


1
We next show that E Dk
= O(t−2/3 ). We first show
λt,min


2/3
2/3
k
P λD
≤ O(e−t ). Similar to what we have done
t,min < t
in (20), define the following event:
n
o
α
Nt (x) > t2/3 , for all x ∈ Dk .
2
Under this event, from the last equality in (17), we get
α 2/3
δDk . Therefore, we have
2t

α 2/3 
k
P λD
≤
t δDk
t,min
2

X 
α
≤
P Nt (x) ≤ t2/3
2
x∈Dk

X 
α
≤
P Ñt (x) ≤ t2/3 ,
2

k
λD
t,min

>

(33)

where Ñt (x) is the number of times that arm x ∈ Dk was
chosen randomly during the first t time steps. Recall from (21)
that
!

t
1 X
≤ exp −
i
10k i=1



For t ≥ t0 = (αk)3 , t = it =
0

αk
t1/3

where the penultimate inequality follows from (17).
From (32) and (37), when t is sufficiently large, we have


c
E hPU (XU∗ ) − PU (Xt+1 ), PU (θ)i Et+1
v "
#
u
u
2ZS
1
t
≤ 2 + 2Zβt E Dk
t
λt,min
s
3 −1
2ZS
(38)
≤ 2 + 2Zβt
t 3
t
αδDk
Finally, from (26), we have

x∈Dk

t
1 X
P Ñt (x) ≤
i
2k i=1

when t is sufficiently large. It follows from (33) to (35) that

α 2/3 
k
P λD
≤
t δDk
t,min

X 2
α
≤
P Ñt (x) ≤ t2/3 ,
2
x∈Dk
!
t
X
1 X
≤
P Ñt (x) ≤
i ,
2k i=1
x∈Dk

 α
(36)
≤ k exp − t2/3
10
Now, we can conclude that when t is sufficiently large,
"
#
1
E Dk
λt,min
"
#

α 2/3
1
α 2/3 
Dk
k
λt,min > t δDk P λD
=E
t δDk
t,min >
Dk
2
2
λt,min
#
"


α 2/3
α 2
1
Dk
k
λt,min ≤ t δDk P λD
t 3 δDk
+E
t,min ≤
Dk
2
2
λt,min
 α

 k
2t−2/3  Dk
α
≤
P λt,min > t2/3 δDk + exp − t2/3
αδDk
2
λ
10
−2/3
3t
≤
,
(37)
αδDk

E [RU (n)]
n
X
≤
(2ZSt + E [ hPU (XU∗ ) − PU (Xt ), PU (θ)i | Etc ])
t=1

s
!
2ZSαk
3 −1
3
t
+ C1
+ 2Zβt
≤
αδDk
t1/3
t=1
s
!
Z n
C2 k
d log x − 1
≤
+ 6Zϑ
x 3 dx + C3
αδDk
x1/3
1

n √ o

= O max k, d n2/3 (log n)1/2 ,
n
X

!
(34)

and we obtain

t
t
t
1 X
1 X
1 X
i =
i +
i
2k i=1
2k i=1
2k
i=t0 +1
Z t+1
1 0
1
αk
≥
t +
dx
2k
2k t0 +1 t1/3
α3 k 2
3α 2/3 3α 3 3
≥
+
t −
(α k + 2)2/3 .
2
4
4
α
≥ t2/3 ,
(35)
2

(39)

where C1 , C2 and C3 are constants. The proof of Theorem 2
is now complete.
C. Additional Smoothness Assumption
In this subsection, we show that if an additional smoothness
assumption similar to that in [15] is made, we can achieve better
cumulative projection regret order than O(n2/3 (log n)1/2 ) for
the infinite-arm case.

8

Assumption 3. There exists J ∈ R+ such that for all θ1 ,
θ2 ∈ Rd ,
kPU (XU∗ (θ1 )

−

XU∗ (θ2 ))k2

PU (θ1 )
PU (θ1 )
.
≤J
−
kPU (θ1 )k kPU (θ1 )k 2
(40)

This is to say the projection of the decision set D and all
θ ∈ Rd onto the subspace U satisfies the SBAR(J) condition
[15], [39]. For example, if the decision set D is a ball or
an elliposid, it satisfies the condition
 (40).
h In thei proof of
1/2
k
Theorem 2, we bound (27) with O βt (E 1/λD
] =
t,min )
−1/3

O(t
log t). With the additional Assumption 3, we can
improve this bound, and further get a tighter upper bound
for the cumulative projection regret in the following result.

"
2

≤λ S E
2

Proof. The inequalities (26) and (27) from the proof of
Theorem 2 still hold. We have
E [hPU (XU∗ ) − PU (Xt ), PU (θ)i | Etc ]
hD
E
i
≤ E PU (XU∗ ) − PU (Xt ), PU (θ) − PU (θ̂t−1 ) Etc
h
i
≤ E kPU (XU∗ ) − PU (Xt )k2 PU (θ) − PU (θ̂t−1 )
Etc
2
"
PU (θ)
PU (θ̂t−1 )
PU (θ) − PU (θ̂t−1 )
≤E J
−
kPU (θ)k kPU (θ̂t−1 )k
2


2
2J PU (θ) − PU (θ̂t−1 )


2
≤ E
Etc 
kPU (θ)k


2J PU (θ − θ̂t−1 )


= E

kPU (θ)k

2

2

(41)

where the first inequality follows from (27), the second inequality follows from Cauchy-Schwarz inequality, the third inequality
follows from Assumption 3, the penultimate inequality follows
from Lemma 3.5 in [15], and the last equality follows from
independence.


2
We next bound E PU (θ − θ̂t )
. From (7), we have,
2

θ̂t =

λI +

t
X

!−1
Xi Xi|

i=1

=

λI +

t
X

2

ri Xi

≤λ S E
"
≤ λ2 S 2 E

Xi Xi|

t
X
(hXi , θi + ηi )Xi ,
i=1



2

≤ E  PU (λVt−1 θ)

2
2

+

t
X
i=1

1

2

PU (Vt−1 Xi )ηi




2

+ ζE

1

2

≤λ S E

#

1

2

≤λ S E

" i=1
t
X

#
PU (Vt−1 Xi )| PU (Vt−1 Xi )
#
tr


PU (Vt−1 )Vt−1 (Xi Xi| )

"
+ ζE tr

PU (Vt−1 )Vt−1 (

t
X

!#
Xi Xi|

i=1

#


+ ζE dλmax (PU (Vt−1 ))
#

"
+ dζE

2
(λD
t,min )

"

t
X

i=1

2
(λD
t,min )

"
2

#

2
(λD
t,min )

#

k
2
(λD
t,min )

#

λD
t,min
"

+ dζE

1
1
k
λD
t,min

#
,

(42)

where the second inequality has used the
 condition that ηt
are i.i.d. and the standard result that E ηt2 ≤ ζ where ζ is
a constant depending on ϑ since ηt has the zero-mean subGaussian distribution with parameter ϑ.
Using a similar argument as that in the proof of Theorem 2,
we can show
"
#
"
#
1
1
−1/2
E Dk
= O(t
), and E
= O(t−1 ).
k
2
λt,min
(λD
)
t,min
(43)
# proof steps are skipped here for brevity. Finally, using
The
Etc(26), (26) and (42), we have
E [RU (n)]
n
X
≤
2ZSst + E [ hPU (XU∗ ) − PU (Xt ), PU (θ)i | Etc ]
t=1
n
X

dO(t−1/2 )

t=1
√
= O(d n).

(44)

The proof of Theorem 3 is now complete.
IV. S IMULATIONS AND P ERFORMANCE E VALUATION
In this section, we verify the efficiency of our strategy by
performing simulations on synthetic data and a wine quality
dataset [35]. We compare against the following strategies:
1) the Uncertainty Ellipsoid (UE) policy [15], which chooses
p
arg max r̄t−1 (x) + α log t min{d log t, |D|}kxkV−1
x∈D

i=1

!−1

i=1

which yields

E PU (θ − θ̂t )


t
X

1

2

"
+ ζE

2
(λD
t,min )

"

≤

2


1

2

≤λ S E
2

#

2
(λD
t,min )

"

2

Theorem 3. Suppose Assumptions 1 to 3 hold and D is a
compact set withn infinitely
o many arms. Then, GENTRY with
√
t = st := min 1, αk
has cumulative projection regret of
t
√
order O(d n), where n is the number of time steps.

1

2

t−1

at each time step t;
2) the CorrUpted GReedy StratEgy (CURSE)
D by replacE
ing arg maxx∈D r̄t−1 (x) with arg maxx∈D x, θ̂t−1 in
GENTRY, i.e., the corruption in the observed return is
not accounted for; and
3) the caRelEss GReEdy sTrategy (REGRET) where the Xi
in GENTRY are all replaced by PU (Xi ). To avoid any
doubts, the full REGRET is shown in Algorithm 2.
In each simulation, we perform 2000 trials, each with 104
time steps. To compare the projection regret performance

9

Inputs: Set t to
or it according to the number of arms;
set hyper parameter α > 0 and λ > 0.
1: loop
2:
Update t .
3:
With probability 1 − t , choose
D
E
arg max PU (x), PU (θ̃t−1 ) ,
x∈D

where
θ̃t =

t
X

!−1
|

PU (Xi )PU (Xi ) + λI

i=1

t
X

ri PU (Xi ),

i=1

and withDprobability t choose
E a random arm from Dk .
4:
Update PU (x), PU (θ̃t−1 ) . Set t = t + 1.
5: end loop

of different strategies, we compute the average empirical
cumulative projection regret over all the trials. For convenience,
this average is referred to as the average cumulative projection
regret.
A. Experiments on Synthetic Data
In this section, we compare the performance of different
strategies using synthetic data. We use three settings in the
simulations:
(a) For the finite-arm case, in each trial, we generate the
decision set D ⊆ Rd containing K arms, in which each
arm is associated with a d-dimension feature vector. Each
dimension of the feature vector is drawn i.i.d. from the
uniform distribution U(−1, 1). Each dimension of the
ground-truth parameter θ is also drawn from U(−1, 1).
We next generate the orthogonal projection matrix PU =
A(A| A)−1 A| , where A is a d × u (u < d) matrix
whose elements are generated randomly from U(−1, 1).
The noise ηt at each time step is drawn i.i.d. from the
Gaussian distribution N (0, ϑ2 ) with variance ϑ2 . The
decision set D, parameter θ and projection matrix PU are
fixed in each trial.
(b) We use the same setting as in setting (a) except that the
projection matrix PU in this setting is a diagonal matrix
whose (i, i) entry is 1 for i = 1, · · · , u and 0 otherwise.
This means that the last d − u dimensions of θ are the
protected features.
(c) For the infinite-arm case, in each trial, the decision set
D is limited to a convex set for ease of computation.
Specifically, we use the convex set D:
(
)
d
X
D= x:
x(i) log x(i) ≤ 5 and x(i) ≥ 0 ∀ i ,
i=1

(45)
where x(i) is the i-th entry of x, 0 log 0 := 0, and θ, PU
and ηt are generated the same way as in setting (a).
In the following, GENTRY, CURSE, and REGRET use the
setting  = ft described in Section III when in setting (a) and

(b) for the finite-arm case. Accordingly,  = it is used when
in setting (c) for the infinite-arm case. In all simulations, the
parameter λ is set to 1.

average cumulative projection regret

ft

GENTRY
CURSE
REGRET
UE

20000
15000
10000
5000
0
0.001 0.01

0.1

1

10

30

100

(a) Using setting (a) with d = 10, K = 45, and u = 5

average cumulative projection regret

Algorithm 2 REGRET

20000
15000

GENTRY
CURSE
REGRET
UE

10000
5000
0
0.001 0.01

0.1

1

10

30

100

(b) Using setting (c) with d = 4, and u = 2
Fig. 1. Regret comparison between different strategies with ϑ = 0.5 and
varying α.

1) Varying α: All the strategies depend on the hyperparameter α, with a larger α corresponding to more exploration
on average. We do simulations using setting (a) with d = 10,
K = 45, ϑ = 0.5, u = 5, and setting (c) with d = 4, ϑ = 0.5,
u = 2. Figs. 1a and 1b show how the average cumulative
projective regret at time step 104 changes with different α in
each strategy. We observe the following:
• We note that a moderate α is optimal for each strategy.
When α is too large, the strategy explores too frequently,
leading to a large projection regret as expected. On the
other hand, a small α results in little exploration, and
good arms cannot be found efficiently.
• In Fig. 1a, GENTRY with α = 1 outperforms all the other
benchmark strategies. This is because the other strategies
do not achieve an asymptotically unbiased estimation of
the projection reward. Fig. 1b shows similar results.
In the following simulations, for a fair comparison, we
tune the parameter α for each strategy to achieve the best
average asymptotic cumulative projection regret for that strategy.
Specifically, we set α = 1 for all the strategies except UE,

3000
2000
1000
0

100
percentage of trials

GENTRY
CURSE
REGRET
UE

4000

0

2000

4000 6000
time steps t

8000

10000

(a) Average cumulative projection regret

80
60
40

GENTRY
CURSE
REGRET
UE

5000
4000
3000
2000
1000
0

100
percentage of trials

average cumulative projection regret

which is given a α = 0.1 when using setting (a) and (b). When
using setting (c), we set α = 0.01 for all the strategies except
REGRET, which is given a α = 0.1.

average cumulative projection regret

10

0

2000

4000 6000
time steps t

8000

10000

(a) Average cumulative projection regret

80
60
40
20
0

20

GENTRY

CURSE

REGRET
policy

UE

(b) Percentage of trials where the best decision is found

0

GENTRY

CURSE

REGRET
policy

UE

Fig. 3. Performance comparison between different strategies using setting (b)
with d = 10, K = 45, ϑ = 0.5 and u = 5.

(b) Percentage of trials where the best decision is found

2) Results and Analysis: In the simulations for the finitearm case using settings (a) and (b), we set d = 10, K = 45,
ϑ = 0.5 and u = 5. The simulation results are shown in Figs. 2
and 3. We observe the following:
• The average cumulative projection regrets of different
strategies are shown in Figs. 2a and 3a. We see that
GENTRY has obvious sublinear cumulative projection
regret performance. The other benchmark strategies all
suffer from a linearly increasing cumulative projection
regret. This verifies that if our objective is to maximize
the cumulative projection reward instead of the cumulative
return (including the corruption), GENTRY is more
appropriate.
• Figs. 2b and 3b show the percentage of trials in which
the optimal arm is chosen for more than 90% of the time
in the last 200 time steps. We see that GENTRY finds
the optimal arm in most trials, and outperforms the other
strategies by a significant margin.
In the simulations for the infinite-arm case using setting (c),
we set d = 4, ϑ = 0.5 and u = 2. From Fig. 4, we

also observe that GENTRY has obvious sublinear cumulative
projection regret performance. The other benchmark strategies
have linearly increasing cumulative projection regret. This
verifies the efficiency of GENTRY for the infinite-arm case.

average cumulative projection regret

Fig. 2. Performance comparison between different strategies using setting (a)
with d = 10, K = 45, ϑ = 0.5 and u = 5.

GENTRY
CURSE
REGRET
UE

5000
4000
3000
2000
1000
0

0

2000

4000 6000
time steps t

8000

10000

Fig. 4. Average cumulative projection regret using setting (c) with d = 4,
u = 2 and ϑ = 0.5.

11

B. Experiments on Wine Quality Dataset

average cumulative projection regret

We next compare the performance of different strategies
using the wine quality dataset, which contains 11-dimension
description vectors of 4898 white wines and their ratings (scores
between 0 and 10). In each trial, we randomly select 200
wines with ratings 4, 5, 6, 7 and 8, as the decision set D,
since among all the wines there are only 5 wines with ratings
larger than 8 and 20 wines with ratings less than 4. Each
dimension of the wine description vector is a physicochemical
characteristic like volatile acidity, chlorides, or density. Due
to privacy and logistic issues [35], only the physicochemical
characteristics and ratings are available (e.g., there is no data
about grape types, wine brand, wine selling price, etc.). We
add one additional feature drawn i.i.d. from U(0, 1) as the
protected feature. The corresponding rating of each wine is
then corrupted by subtracting 4 times this protected feature
value from the original rating. In this experiment, we take
the original rating as the projection reward. Finally, we add a
constant 1 as the constant feature to each description vector.
If we put the protected feature as the final dimension of each
wine, then d = 13, and the projection matrix PU is defined as
a diagonal matrix whose (i, i) entry is 1 for i = 1, · · · , 12, and
0 for i = 13. As there are finitel many arms, we use  = ft in
this experiment. The results are shown in Fig. 5, from which
we observe that GENTRY outperforms all the other strategies.

GENTRY
CURSE
REGRET
UE

4000
3000
2000
1000
0

0

2000

4000 6000
time steps t

8000

10000

Fig. 5. Average cumulative projection regret using the wine quality dataset.

V. C ONCLUSION
We have formulated the orthogonal projection problem in
the linear stochastic bandit model, where the objective is to
maximize the cumulative projection reward over a subspace of
decision attributes based on observed returns that consist of
the projection reward with corruption. Our proposed GENTRY
achieves sublinear projection regret for the finite- and infinitearm cases. Experiments verify the efficiency of our strategy. Our
formulation and strategy are useful in avoiding discrimination
in recommendation systems and in mixed drug treatment trials.
In this paper, we have assumed that the target subspace is
known beforehand when decomposing the reward. However,

in practice, we may not know what is a suitable projection
subspace a priori. It is of interest in future research to develop
methods to learn this subspace in conjunction with the best
arm using possibly additional side-information.
R EFERENCES
[1] H. Robbins, “Some aspects of the sequential design of experiments,”
Bull. Amer. Math. Soc., vol. 58, no. 5, pp. 527–535, Jul. 1952.
[2] P. Auer, N. Cesa-Bianchi, and P. Fischer, “Finite–time analysis of the
multiarmed bandit problem,” Mach. Learn., vol. 47, no. 2, pp. 235–256,
May 2002.
[3] T. L. Lai and H. Robbins, “Asymptotically efficient adaptive allocation
rules,” Advances Appl. Math., vol. 6, no. 1, pp. 4–22, Mar. 1985.
[4] J.-Y. Audibert, R. Munos, and C. Szepesvári, “Exploration–exploitation
tradeoff using variance estimates in multi-armed bandits,” Theoretical
Comput. Sci., vol. 410, no. 19, pp. 1876–1902, Oct. 2009.
[5] R. Agrawal, “Sample mean based index policies by O(log n) regret for
the multi-armed bandit problem,” Advances Appl. Probability, vol. 27,
no. 4, pp. 1054–1078, Dec. 1995.
[6] A. N. Burnetas and M. N. Katehakis, “Optimal adaptive policies for
sequential allocation problems,” Advances Appl. Math., vol. 17, no. 2,
pp. 122–142, Jun. 1996.
[7] Q. Kang and W. P. Tay, “Task recommendation in crowdsourcing based on
learning preferences and reliabilities,” arXiv preprint arXiv:1807.10444,
2018.
[8] Y. Liu and M. Liu, “An online learning approach to improving the
quality of crowd-sourcing,” IEEE/ACM Trans. Netw., vol. 25, no. 4, pp.
2166–2179, Aug 2017.
[9] S. Klos nÃl’e MÃijller, C. Tekin, M. van der Schaar, and A. Klein,
“Context-aware hierarchical online learning for performance maximization
in mobile crowdsourcing,” IEEE/ACM Trans. Netw., vol. 26, no. 3, pp.
1334–1347, June 2018.
[10] Q. Kang and W. P. Tay, “Sequential multi-class labeling in crowdsourcing,”
IEEE Trans. Knowl. Data Eng., vol. 31, no. 11, pp. 2190 – 2199, Nov.
2019.
[11] S. Pandey, D. Chakrabarti, and D. Agarwal, “Multi-armed bandit problems
with dependent arms,” in Proc. Int. Conf. Mach. Learn., Oregon, USA,
Jun. 2007, pp. 721–728.
[12] E. L. Presman, Sonin, I. N, and Sonin, Sequential Control with Incomplete
Information. London, UK: Academic Press, 1990.
[13] A. Goldenshluger, A. Zeevi et al., “WoodroofeâĂŹs one-armed bandit
problem revisited,” Ann. Appl. Probability, vol. 19, no. 4, pp. 1603–1633,
Nov. 2009.
[14] Y. Abbasi-Yadkori, D. Pál, and C. Szepesvári, “Improved algorithms for
linear stochastic bandits,” in Proc. Advances Neural Inf. Process. Syst.,
Granada, Spain, Dec. 2011, pp. 2312–2320.
[15] P. Rusmevichientong and J. N. Tsitsiklis, “Linearly parameterized bandits,”
Math. Oper. Res., vol. 35, no. 2, pp. 395–411, May 2010.
[16] Y. Gai, B. Krishnamachari, and R. Jain, “Combinatorial network
optimization with unknown variables: Multi-armed bandits with linear
rewards and individual observations,” IEEE/ACM Trans. Netw., vol. 20,
no. 5, pp. 1466–1478, Oct 2012.
[17] S. Agrawal and N. Goyal, “Thompson sampling for contextual bandits
with linear payoffs,” in Proc. Int. Conf. Mach. Learn., Georgia, USA,
Jun. 2013, pp. 127–135.
[18] L. Li, W. Chu, J. Langford, and R. E. Schapire, “A contextual-bandit
approach to personalized news article recommendation,” in Proc. Int.
Conf. World Wide Web, New York, USA, Apr. 2010, pp. 661–670.
[19] N. Abe, A. W. Biermann, and P. M. Long, “Reinforcement learning with
immediate rewards and linear hypotheses,” Algorithmica, vol. 37, no. 4,
pp. 263–293, Dec. 2003.
[20] Y. Yue and C. Guestrin, “Linear submodular bandits and their application
to diversified retrieval,” in Proc. Advances Neural Inf. Process. Syst.,
Granada, Spain, Dec. 2011, pp. 2483–2491.
[21] S. S. Villar, J. Bowden, and J. Wason, “Multi-armed bandit models for
the optimal design of clinical trials: Benefits and challenges,” Statist.
Sci., vol. 30, no. 2, p. 199, May 2015.
[22] V. Dani, T. P. Hayes, and S. M. Kakade, “Stochastic linear optimization
under bandit feedback,” in Proc. Annu. Conf. Learn. Theory, Helsinki,
Finland, Jul. 2008, pp. 355–366.
[23] V. Kuleshov and D. Precup, “Algorithms for multi-armed bandit problems,”
arXiv preprint arXiv:1402.6028, 2014.
[24] Why netflix features black actors in promos to black users. Accessed:
Jun. 25, 2019. [Online]. Available: https://www.wired.com/story/
why-netflix-features-black-actors-promos-to-black-users

12

[25] R. D. Toto, “Treatment of hypertension in chronic kidney disease,”
Seminars Nephrology, vol. 25, no. 6, pp. 435–439, Nov. 2005.
[26] P. Auer, “Using confidence bounds for exploitation-exploration trade-offs,”
J. Mach. Learn. Res., vol. 3, no. 11, pp. 397–422, Nov. 2002.
[27] H. Wang, Q. Wu, and H. Wang, “Learning hidden features for contextual
bandits,” in Proc. Int. Conf. Inform. Knowl. Manag., Indianapolis, USA,
Oct. 2016, pp. 1633–1642.
[28] A. Uschmajew, “Local convergence of the alternating least squares
algorithm for canonical tensor approximation,” J. Matrix Anal. Appl.,
vol. 33, no. 2, pp. 639–652, Jun. 2012.
[29] J. Friedman, T. Hastie, and R. Tibshirani, “Regularization paths for
generalized linear models via coordinate descent,” J. Statist. Softw.,
vol. 33, no. 1, pp. 1–22, Aug. 2010.
[30] M. M. Drugan, A. Nowé, and B. Manderick, “Pareto upper confidence
bounds algorithms: an empirical study,” in Proc. IEEE Symp. Adaptive
Dynamic Programming Reinforcement Learn., Orlando, USA, Dec. 2014,
pp. 1–8.
[31] M. M. Drugan and A. Nowe, “Designing multi-objective multi-armed
bandits algorithms: A study,” in Proc. Int. Joint Conf. Neural Netw.,
Dallas, USA, Aug 2013, pp. 1–8.
[32] S. Q. Yahyaa, M. M. Drugan, and B. Manderick, “Annealing-pareto multiobjective multi-armed bandit algorithm,” in Proc. IEEE Symp. Adaptive
Dynamic Programming Reinforcement Learn., Orlando, USA, Dec. 2014,
pp. 1–8.
[33] ——, “The scalarized multi-objective multi-armed bandit problem: An
empirical study of its exploration vs. exploitation tradeoff,” in Proc. Int.
Joint Conf. Neural Netw., Beijing, China, Jul. 2014, pp. 2290–2297.
[34] E. Zitzler, L. Thiele, M. Laumanns, C. M. Fonseca, and V. Da Fonseca Grunert, “Performance assessment of multiobjective optimizers: An
analysis and review,” IEEE Trans. Evol. Comput., vol. 139, no. 2, pp.
117–132, May 2002.
[35] P. Cortez, A. Cerdeira, F. Almeida, T. Matos, and J. Reis, “Modeling wine
preferences by data mining from physicochemical properties,” Decision
Support Syst., vol. 47, no. 4, pp. 547–553, May 2009.
[36] S. J. Leon, Linear Algebra with Applications. Upper Saddle River, NJ:
Pearson, 2010.
[37] T. Calders, A. Karim, F. Kamiran, W. Ali, and X. Zhang, “Controlling
attribute effect in linear regression,” in Proc. Int. Conf. Data Mining,
Dallas, USA, Dec. 2013, pp. 71–80.
[38] D. Pollard, Convergence of Stochastic Processes. Berlin, DE: Springer,
1984.
[39] E. S. Polovinkin, “Strongly convex analysis,” Sbornik: Math., vol. 187,
no. 2, p. 259, Feb. 1996.

