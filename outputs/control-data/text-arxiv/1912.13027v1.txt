All-or-Nothing Phenomena:
From Single-Letter to High Dimensions

arXiv:1912.13027v1 [math.ST] 30 Dec 2019

Galen Reeves

∗

Jiaming Xu†

Ilias Zadik

‡

January 1, 2020

Abstract
We consider the linear regression problem of estimating a p-dimensional vector β from n observai.i.d.
tions Y = Xβ + W , where βj ∼ π for a real-valued distribution π with zero mean and unit variance,
i.i.d.
i.i.d.
Xij ∼ N (0, 1), and Wi ∼ N (0, σ 2 ). In the asymptotic regime where n/p → δ and p/σ 2 → snr for
two fixed constants δ, snr ∈ (0, ∞) as p → ∞, the limiting (normalized) minimum mean-squared error
(MMSE) has been characterized by the MMSE of an associated single-letter (additive Gaussian scalar)
channel.
In this paper, we show that if the MMSE function of the single-letter channel converges to a step
function, then the limiting MMSE of estimating β in the linear regression problem converges to a step
function which jumps from 1 to 0 at a critical threshold. Moreover, we establish that the limiting meansquared error of the (MSE-optimal) approximate message passing algorithm also converges to a step
function with a larger threshold, providing evidence for the presence of a computational-statistical gap
between the two thresholds.

1

Introduction

Consider the classical linear regression model
Y = Xβ + W

(1)

i.i.d.

i.i.d.

where X ∈ Rn×p with Xij ∼ N (0, 1), β ∈ Rp with βj ∼ π for a distribution π with zero mean and unit
i.i.d.

variance, and W ∈ Rn with Wi ∼ N (0, σ 2 ). We are interested in estimating β from observation of (X, Y ).
b
For a given estimator β(X,
Y ), the normalized mean squared-error of estimating β is given by


 
2
1
MSE βb := E β − βb .
p
 
b or equivalently,
Let MMSE denote the minimum of MSE βb among all possible estimators β,
MMSE :=

i
1 h
2
E kβ − E[β | X, Y ]k .
p

(2)

∗ G. Reeves is with the Department of Electrical and Computer Engineering and the Department of Statistical Science, Duke
University, Durham, NC 27708 USA; e-mail: galen.reeves@duke.edu. G. Reeves is is supported by NSF Grants CCF-1718494
and CCF-1750362.
† J. Xu is with the Fuqua School of Business, Duke University, Durham, NC 27708 USA; e-mail: jiamingxu.868@duke.edu.
J. Xu is is supported by NSF Grants IIS-1932630, CCF-1850743, and CCF-1856424.
‡ I. Zadik is with the Center for Data Science, New York University, New York, NY 10011 USA; e-mail: zadik@nyu.edu. I.
Zadik is supported by a CDS-Moore-Sloan postdoctoral fellowship.
This paper was presented at the 2019 IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive
Processing (CAMSAP), Guadeloupe, French West Indies.

1

In this paper, we focus on the asymptotic regime:
n
→δ
p

and

p
→ snr,
σ2

as p → ∞,

(3)

for two fixed constants
δ, snr ∈

 (0, ∞).
 Note that δ is the under-sampling ratio and snr is the signal-to-noise
ratio in view of E kXβk2 /E kW k2 = p/σ 2 .
Recent work [1–3] proves that under certain structural assumptions in terms of (π, δ, snr), the limiting
MMSE in the asymptotic regime (3) is characterized by the replica-symmetric (RS) formula through a single
letter channel
√
(4)
y = sβ0 + N,
where s > 0, β0 ∼ π and N ∼ N (0, 1) are independent. However, often the RS formula is too complicated
to extract structural behavior of the limiting MMSE.
In this work, we propose generic conditions under which the limiting MMSE exhibits an all-or-nothing
phenomena. More precisely, consider a family (πǫ , δǫ , snrǫ ) indexed by a positive parameter ǫ where πǫ has
finite entropy Hǫ := H(πǫ ). We show that if the MMSE of the single letter channel (4) as a function of
s converges to a step function as ǫ → 0, then the limiting MMSE of the linear regression model (1) also
converges to a step function, which jumps from 1 to 0 at a critical threshold δǫ = δǫ,MMSE , where
δǫ,MMSE :=

2Hǫ
.
log(1 + snrǫ )

(5)

In other words, an all-or-nothing phenomena in the single letter channel implies an all-or-nothing phenomena
in the high-dimensional linear regression model. Moreover, we establish that the limiting MSE of the (MSEoptimal) approximate message passing (AMP) algorithm also converges to a step function, which jumps from
1 to 0 at a larger threshold δǫ = δǫ,AMP , where
δǫ,AMP :=

2Hǫ (1 + snrǫ )
.
snrǫ

(6)
i.i.d.

An important application of our general result is the binary linear regression model where βj ∼ Bern(ǫ).
In this case, we show that the MMSE function of the single letter channel converges to a step function as
the sparsity ǫ → 0. Then we obtain from our general result that the limiting MMSE of the binary linear
2ǫ log(1/ǫ)
.
regression model converges to a step function which jumps from 1 to 0 at the critical threshold δǫ = log(1+snr
ǫ)
This coincides with the all-or-nothing phenomenon established in [4] for the binary linear regression model
where β is chosen uniformly at random from the set of binary k-sparse vectors, in the highly sparse and high
√
signal-to-noise ratio regime where k/ p → 0 and k/σ 2 is above a sufficiently large constant. Furthermore,
we deduce from our general result that the limiting MSE of the (MSE-optimal) AMP converges to a step
ǫ)
. This coincides with the
function which jumps from 1 to 0 at the critical threshold δǫ = 2ǫ log(1/ǫ)(1+snr
snrǫ
computational threshold for a number of computationally efficient methods in the literature such as LASSO
or Orthogonal Matching Pursuit. In particular, our result adds to the existing evidence for the presence of a
computational-statistical gap between the two thresholds (see [5,6] for an extended discussion and literature
review on the presence of this computational-statistical gap).

2
2.1

Preliminaries
The Replica Symmetric Formulas

To describe the RS formulas, we first define the mutual information and MMSE functions for the single letter
channel (4):
√
(7)
I(s) := I(β0 ; sβ0 + N ), s > 0
2

M (s) := mmse(β0 |

√
sβ0 + N ),

s>0

(8)

where β0 ∼ π and N ∼ N (0, 1) are independent. Both of these functions are non-negative and the unit
variance assumption on π means that for any s > 0, (see [7] for details)
1
s
log(1 + s) ≤ ,
2
2
1
M (s) ≤
≤ 1.
1+s
I(s) ≤

(9)
(10)

Moreover, the I-MMSE relation for the single-letter channel [7] states the derivative of the mutual information
is one half the MMSE, that is I ′ (s) = 12 M (s).
Next, we define the potential function F : [0, ∞) → [0, ∞) according to
δ  s 
F (s) := I(s) + φ
,
2 δ snr

(11)

where φ(x) = x − log x − 1, and δ, snr are respectively the undersampling ratio and the signal-to-noise ratio
of our original model. Note that φ(x) is convex and non-negative on (0, ∞).
Lemma 1. All stationary points of F (s) lie on the open interval between δ snr /(1 + snr) and δ snr.
Proof. By differentiation with respect to s and the I-MMSE relation for the single-letter channel [7], we have
that for any s > 0
F ′ (s) ∝ M (s) + 1/ snr −δ/s.
The fact that M (s) > 0 for all s implies that F ′ (s) is strictly positive for all s ≥ δ snr, and thus F (s) is
strictly increasing on [δ snr, ∞). Alternatively, the fact that M (s) < 1 for all s > 0 implies that F ′ (s) is
strictly negative for all s ≤ δ snr /(1 + snr), and thus F (s) is strictly decreasing on (0, δ snr /(1 + snr)].
In view of Lemma 1, the minimum of the potential function and the smallest and largest minimizers can
be defined as follows:
F ∗ := min F (s),

(12)

s

∗

∗

(13)

∗

∗

(14)

s := min{s : F (s) = F },

s := max{s : F (s) = F }.

Note that s∗ = s∗ if and only if the minimum is attained at a unique point.
Proposition 2 (RS MMSE [2,3,8]). For any (δ, snr, π) for which (snr, π) satisfies the single-crossing property
[2] and π has finite fourth moment 1 , the mutual information and MMSE satisfy
1
I(β; X, Y ) = F ∗ ,
p
i
1 h
2
lim sup E kβ − E[β | y, X]k ≤ M (s∗ ),
p→∞ p
i
1 h
2
lim inf E kβ − E[β | y, X]k ≥ M (s∗ ),
p→∞ p
lim

p→∞

(15)
(16)
(17)

where the limits are taken as (n = np , p, σ 2 = σp2 ) scale to infinity with p → +∞, n/p → δ and p/σ 2 → snr.
1A

different set of assumptions on (δ, snr, π) for which the Proposition 2 holds can be found in [3, 8]

3

Next, we turn to the family of approximate message passing (AMP) [9, 10] algorithms and specifically
to the case of MMSE-AMP which is proven in to be optimal among AMP algorithms in minimizing the
MSE of the recovery problem of interest [11]. For simplicity from now on when we say AMP we refer to the
MMSE-AMP algorithm. It turns out that a related formula to the one given in Proposition 2 describes the
asymptotic MSE associated with AMP.
The smallest stationary point is defined as
sAMP := inf{s : F ′ (s) = 0}.

(18)

It is rather straightforward to check that sAMP is attained by some positive value s and therefore it is a
stationary point of F (s). In particular, by Lemma 1 we have sAMP ∈ (δ snr /(1 + snr), δ snr).
For the next result, for T ∈ N let βbAMP,T (Y, X) be the output of the AMP estimator [11, Section II.C]
with input data (Y, X) after T iterations.

Proposition 3 (AMP, [10, 11]). For any (δ, snr, π) where π has a finite fourth moment, AMP satisfies


2
1
b
lim lim
E β − βAMP,T (Y, X)
= M (sAMP )
(19)
T →+∞ p→+∞ p

where the inner limit is taken as (n = np , p, σ 2 = σp2 ) scale to infinity with p → +∞, n/p → δ and p/σ 2 → snr.
Remark 1. The results stated above imply that AMP is optimal whenever s∗ = s∗ = sAMP .
Remark 2. For a proof of Proposition 3 we refer the reader to the statement and proof of [11, Theorem 6].

3

Main Results

Let us consider now a family of coefficient distributions (πǫ )ǫ>0 indexed by a positive-valued parameter ǫ > 0.
We assume throughout the section that for each ǫ > 0 the distributions πǫ has zero mean, unit variance and
finite entropy Hǫ . Our results are all based on the following assumption on the family πǫ .
Assumption 1. Let (πǫ )ǫ>0 be a family of distributions with unit variance and finite entropy Hǫ . The
MMSE function Mǫ (s) of the single letter channel, as defined in (8), for πǫ coefficient distribution is assumed
to converge pointwise to a step function as ǫ → 0 in the following sense
(
1, t ∈ [0, 1)
lim Mǫ (2Hǫ t) =
(20)
ǫ→0
0, t ∈ (1, ∞).
Remark 3. It can be straightforwardly checked using the I-MMSE relation for the single-letter channel [7]
that the rescaling in the argument of Mǫ by twice the entropy term, i.e. by 2Hǫ , is necessary for the
convergence of Mǫ to the
R ∞step function. To see why, observe that Mǫ (s), using the I-MMSE relation, satisfies
the integral constraint 0 Mǫ (2Hǫ t) dt = 1. Thus, convergence to a step function at a threshold other than
2Hǫ would violate this constraint.
Remark 4. As we establish later in the section, Assumption 1 is satisfied for the family of (normalized)
Bernoulli distributions with probability ǫ. See Fig. 1 for a graphical illustration. We expect the assumption
to hold under greater generality.
We now present our two main results assuming the family of distributions (πǫ )ǫ>0 satisfies Assumption
1.
Theorem 4. Let (πǫ )ǫ>0 satisfy Assumption 1. Given a number r ∈ (0, 1) ∪ (1, ∞), let (δǫ , snrǫ , πǫ )ǫ>0 be a
family of triplets such that
lim

ǫ→0

δǫ
δǫ,MMSE
4

= r.

(21)

Iǫ (2Hǫ t)/Hǫ

Mǫ (2Hǫ t)
ǫ = 1e-01
ǫ = 1e-02

1

1

ǫ = 1e-04
ǫ = 1e-08
ǫ = 1e-16
ǫ = 1e-32

ǫ = 1e-01
ǫ = 1e-02
ǫ = 1e-04
ǫ = 1e-08
ǫ = 1e-16
ǫ = 1e-32

0

0

0

1

0

1

t

t

Figure 1: Single letter mutual information and MMSE functions corresponding to the Bernoulli(ǫ) distribution normalized to unit variance.
Then, the minimizers of the RS potential function exhibit the all-or-nothing behavior in the small-ǫ limit
depending on whether r is greater than or less than one:
r ∈ (0, 1)

=⇒

r ∈ (1, ∞)

=⇒

lim Mǫ (s∗ǫ ) = 1

(22)

lim Mǫ (s∗ǫ ) = 0.

(23)

ǫ→0

ǫ→0

Combining Theorem 4 with Proposition 2 we obtain the following Corollary.
Corollary 5 (All-or-nothing MMSE behavior). Let r ∈ (0, 1)∪(1, ∞). For any family of triplets (δǫ , snrǫ , πǫ )ǫ>0 ,
suppose that for any ǫ > 0, (snrǫ , πǫ ) satisfies the single-crossing property, πǫ has finite fourth moment,
(πǫ )ǫ>0 satisfies Assumption 1, and (21) holds. Then it holds that
(
i
1, r ∈ [0, 1)
1 h
2
lim lim E kβ − E[β | X, Y ]k =
(24)
ǫ→0 p→∞ p
0, r ∈ (1, ∞).
where the inner limits are taken as (n = np , p, σ 2 = σp2 ) scale to infinity with p → +∞, n/p → δǫ and
p/σ 2 → snrǫ .
We next present our second main result.
Theorem 6. Let (πǫ )ǫ>0 satisfy Assumption 1. Given a number r ∈ (0, 1) ∪ (1, ∞), let (δǫ , snrǫ , πǫ ) be such
that
lim

ǫ→0

δǫ
δǫ,AMP

= r.

(25)

Then, the smallest stationary point sAMP exhibits the all-or-nothing behavior in the small-ǫ limit depending
on whether r is greater than or less than one:

r ∈ (0, 1)
=⇒
lim Mǫ sAMP
=1
(26)
ǫ
ǫ→0

= 0.
(27)
r ∈ (1, ∞)
=⇒
lim Mǫ sAMP
ǫ
ǫ→0

5

For the result, for T ∈ N let βbAMP,T (Y, X) the output of the AMP estimator [11, Section II.C] with input
data (X, Y ) after T iterations. Combining Theorem 6 with Proposition 3 we obtain the following Corollary
on the performance of AMP.
Corollary 7 (All-or-nothing AMP behavior). Let r ∈ (0, 1)∪(1, ∞). For any family of triplets (δǫ , snrǫ , πǫ )ǫ>0 ,
suppose that each πǫ has a finite fourth moment, the family (πǫ )ǫ>0 satisfies Assumption 1, and (25) holds.
Then it holds that

 (
2
1
1, r ∈ [0, 1)
b
lim lim lim E β − βAMP,T (X, Y )
=
(28)
p→∞
ǫ→0 T →+∞
p
0, r ∈ (1, ∞).
where the inner limits are taken as (n = np , p, σ 2 = σp2 ) scale to infinity with p → +∞, n/p → δǫ and
p/σ 2 → snrǫ .

3.1

Illustration of Theorems 4 and 6 via normalized potential function

To provide insight into the behavior described by Theorems 4 and 6, we consider the scaling δǫ = 2rHǫ / log(1+
snr) where the pair (snr, r) is considered fixed with respect to ǫ > 0. Note that in this scaling, δǫ = rδǫ,MMSE .
Define the normalized potential function


F (2Hǫ t)
Iǫ (2Hǫ t)
r
t log(1 + snr)
Feǫ,r (t) :=
.
(29)
=
+
φ
Hǫ
Hǫ
log(1 + snr)
r snr
Note that only the first term depends on ǫ. Under Assumption 1, using the I-MMSE relation we have
Iǫ (2Hǫ t)/Hǫ → 1 ∧ t as ǫ → 0. Thus for all t > 0,
lim Feǫ,r (t) = Fe0,r (t),

ǫ→0

where

Fe0,r (t) := 1 ∧ t +



t log(1 + snr)
r
.
φ
log(1 + snr)
r snr

(30)

(31)

For r ∈ (0, 1) ∪ (1, ∞), the function Fe0,r (t) has a unique global minimizer given by
t∗ =


r snr

 (1 + snr) log(1 + snr) ,



r snr
log(1 + snr)

r ∈ (0, 1)

(32)

r ∈ (1, ∞).

Importantly, using the elementary inequality x/(x + 1) ≤ log(1 + x) ≤ x for all x > −1, we deduce
r ∈ (0, 1)
r ∈ (1, ∞)

=⇒
=⇒

t∗ ∈ (0, 1)
t∗ ∈ (1, ∞).

This dichotomy together with Assumption 1 and equality (30) suggest the following all-or-nothing behavior;
as ǫ → 0, when r < 1 the MMSE converges to one and when r > 1 the MMSE converges to zero. This is the
same all-or-nothing behavior described and rigorously established in Theorem 4. Furthermore, the smallest
stationary point tAMP is given by

r snr
AMP

)
 (1 + snr) log(1 + snr) , r ∈ (0, r
AMP
t
=
(33)
r snr


,
r ∈ (rAMP , ∞),
log(1 + snr)
6

where rAMP := δǫ,AMP /δǫ,MMSE = (1 + 1/ snr) log(1 + snr). Similar to above, we deduce
r ∈ (0, rAMP )

r ∈ (rAMP , ∞)

=⇒
=⇒

tAMP ∈ (0, 1)

tAMP ∈ (1, ∞).

This dichotomy together with Assumption 1 and equality (30), suggest the following all-or-nothing behavior;
as ǫ → 0, when r < rAMP the MSE of the AMP converges to one and when r > rAMP the MSE of the
AMP converges to zero. This is the same all-or-nothing behavior described and rigorously established in
Theorem 6.
This behavior of the normalized potential function is illustrated graphically in Figure 2 where both Feǫ,r (t)
and Fe0,r (t) are plotted as a function of t for various r.

4

Application: Sparse binary regression

i.i.d.

We now present our main application of our two results to sparse binary regression, where βi ∼ Bern(ǫ). To
this end, we first consider the case where βi is i.i.d. drawn from the following two-point distribution:
πǫ = (1 − ǫ) δµ1 + ǫ δµ2 ,

(34)

p
p
where δx denotes a Dirac distribution with mass at x ∈ R, and µ1 = − ǫ/(1 − ǫ) and µ2 = (1 − ǫ)/ǫ are
chosen such that πǫ has zero mean and unit variance. The following Lemma holds for the family of MMSE
functions (Mǫ (s))ǫ>0 :
Lemma 8. The distribution πǫ in (34) has entropy Hǫ = −ǫ log ǫ − (1 − ǫ) log(1 − ǫ) and MMSE function



Mǫ (s) = E



1


q
,
s
s
1 − ǫ + ǫ exp 2ǫ(1−ǫ) + ǫ(1−ǫ) N

(35)

where N ∼ N (0, 1). Furthermore, the distribution πǫ satisfies the single-crossing condition [2] for all snr > 0
and

where Q(z) =

R∞
z

lim Hǫ /(ǫ log 1/ǫ) = 1
ǫ→0


s − 2ǫ log(1/ǫ)
√
lim sup Mǫ (s) − Q
= 0,
ǫ→0 s>0
2 sǫ

(36)
(37)

(2π)−1/2 exp(−t2 /2) dt.

An immediate implication of the result is that the family of distributions (πǫ )ǫ>0 satisfies Assumption 1
as well as the conditions of Corollaries 5 and 7. Hence, all-or-nothing phase transitions hold for the limiting
MMSE around δǫ,MMSE and for the MSE of the AMP around δǫ,AMP . Using that limǫ→0 Hǫ /(ǫ log 1/ǫ) = 1,
we can further simplify the phase transition points given in (5), (6) by observing


2ǫ log(1/ǫ)
=1
(38)
lim δǫ,MMSE /
ǫ→0
log(1 + snrǫ )
and


2(1 + snrǫ )ǫ log(1/ǫ)
lim δǫ,AMP /
ǫ→0
snrǫ

7



= 1.

(39)

normalized potential function Feǫ,r (t) with snr = 5 and ǫ = 10−16

2.5

2

1.5

1

0.5
global minimizer
smallest stationary point
0

0

1

2

3

4

5
t

6

7

8

9

10

normalized potential function Fe0,r (t) with snr = 5

2.5

2

1.5

1

0.5
global minimizer
smallest stationary point
0

0

1

2

3

4

5
t

6

7

8

9

10

Figure 2: Illustration of the normalized potential functions, Feǫ,r (t) for ǫ = 10−16 (top plot) and
Fe0,r (t)(bottom plot), where snr = 5 and r varies in (0, +∞). Black-colored curves correspond to r ∈ (0, 1),
magenta-colored curves correspond to r ∈ (1, rAMP ) and cyan-colored curves correspond to r ∈ (rAMP , ∞).
Note that the global minimizer of Fe0,r (t) transitions from being less than one to bigger than one exactly at
r = 1, while the smallest stationary point transitions from being less than one to bigger than one exactly at
r = rAMP . A similar approximate behavior takes place for Feǫ,r (t) with ǫ = 10−16 .

8

i.i.d.

Next, we extend the above results to the sparse binary regression problem of interest, where βi ∼ Bern(ǫ).
We denote by k = ǫp the (expected) number of non-zero coordinates of β. Define
β − E[β]
βe = p
.
ǫ(1 − ǫ)

i.i.d.
Then βei ∼ πǫ as given in (34). Moreover, define

Y − XE[β]
Ye = p
,
ǫ(1 − ǫ)

f= p W
W
.
ǫ(1 − ǫ)

f . Since W
fi i.i.d.
∼ N (0, σ
e2 ) with σ
e = σ/
Then it follows that Ye = X βe + W
snrǫ =

pǫ(1 − ǫ)
p
=
.
σ
e2
σ2

p
ǫ(1 − ǫ), it follows that

(40)

Hence, according to Corollary 5, (38), (40), we obtain that the limiting MMSE exhibits an all-or-nothing
behavior at
δǫ,MMSE = 2ǫ log(1/ǫ)/ log(1 + ǫ(1 − ǫ)p/σ 2 ),
which using k = ǫp as ǫ → 0 simplifies with negligible multiplicative error to
δǫ,MMSE = 2(k/p) log(p/k)/ log(1 + k/σ 2 ).
Note that this is the exact information-theoretic threshold for which an all-or-nothing phenomenon has been
proven to hold when lim supp log k/ log p < 0.5 in [4].
Similarly, according to Corollary 7, (39), and (40), the limiting MSE of the AMP exhibits an all-or-nothing
behavior at:


σ2
δǫ,AMP = 2 1 +
ǫ log(1/ǫ),
pǫ(1 − ǫ)

which using k = ǫp as ǫ → 0 simplifies with negligible multiplicative error to

δǫ,AMP = 2 k + σ 2 log(p/k)/p.

Note that this is the exact computational threshold for a number of computationally efficient methods in
the literature such as LASSO or Orthogonal Matching Pursuit (see [5, 6] for references). Our result suggests
that the threshold corresponds to a barrier also for AMP in a strong sense.

References
[1] Dongning Guo and S. Verdu, “Randomly spread cdma: asymptotics via statistical physics,” IEEE
Transactions on Information Theory, vol. 51, no. 6, pp. 1983–2010, June 2005.
[2] G. Reeves and H. D. Pfister, “The replica-symmetric prediction for random linear estimation with
Gaussian matrices is exact,” IEEE Trans. Inform. Theory, vol. 65, no. 4, pp. 2252–2283, Apr. 2019.
[3] J. Barbier, M. Dia, N. Macris, and F. Krzakala, “The mutual information in random linear estimation,”
in Proc. Annual Allerton Conf. on Commun., Control, and Comp., Monticello, IL, 2016.
[4] G. Reeves, J. Xu, and I. Zadik, “The all-or-nothing phenomenon in sparse linear regression,” in Conference On Learning Theory (COLT), 2019, [Online]. Available https://arxiv.org/abs/1903.05046.
[5] D. Gamarnik and I. Zadik, “High dimensional regression with binary coefficients. estimating squared
error and a phase transtition,” in COLT, 2017.
9

[6] ——, “Sparse high-dimensional linear regression. algorithmic barriers and a local search algorithm,”
2017, [Online]. Available https://arxiv.org/abs/1711.04952.
[7] Dongning Guo, S. Shamai, and S. Verdu, “Mutual information and mmse in gaussian channels,” in
International Symposium onInformation Theory, 2004. ISIT 2004. Proceedings., June 2004, pp. 349–
349.
[8] J. Barbier, F. Krzakala, N. Macris, L. Miolane, and L. Zdeborová, “Optimal errors and phase transitions
in high-dimensional generalized linear models,” Proceedings of the National Academy of Sciences, vol.
116, no. 12, pp. 5451–5460, 2019. [Online]. Available: https://www.pnas.org/content/116/12/5451
[9] D. L. Donoho, A. Maleki, and A. Montanari, “Message-passing algorithms for compressed sensing,”
Proceedings of the National Academy of Sciences, vol. 106, no. 45, pp. 18 914–18 919, Nov. 2009.
[10] M. Bayati and A. Montanari, “The dynamics of message passing on dense graphs, with applications to
compressed sensing,” IEEE Trans. Inform. Theory, vol. 57, no. 2, pp. 764–785, Feb. 2011.
[11] G. Reeves and M. Gastpar, “The sampling rate-distortion tradeoff for sparsity pattern recovery in
compressed sensing,” IEEE Trans. Inf. Theor., vol. 58, no. 5, pp. 3065–3092, May 2012. [Online].
Available: http://dx.doi.org/10.1109/TIT.2012.2184848

A

Properties of RS and AMP Formulas under finite entropy

This section describes some properties of the potential function F (s) defined in (11), such as its minimum
value F ∗ , the upper and lower minimizers (s∗ , s∗ ), and the smallest stationary point sAMP , as a function
of the problem parameters (δ, snr, π). Throughout, we make the additional assumption that π is a discrete
distribution with finite entropy H = H(π).
Lemma 9. The mutual information function I(s) of the single-letter channel under input distribution π
defined in (7) satisfies
ns o
ns o
(41)
min , H − L ≤ I(s) ≤ min , H
2
2
for all s ∈ [0, ∞), where

L := H − I(2H).

(42)

Proof. Define A(s) := s/2 − I(s). By the I-MMSE relation and the assumption of the unit variance, the
derivative A′ (s) = (1 − M (s))/2 is nonnegative, and thus 0 ≤ s ≤ 2H implies that A(0) ≤ A(s) ≤ A(2H).
Noting that A(0) = 0 and A(2H) = L yields 0 ≤ s/2 − I(s) ≤ L for all s ∈ [0, 2H]. Meanwhile, the mutual
information also satisfies the upper bound I(s) ≤ H for all s. Finally, because I(s) is non-decreasing, s ≥ 2H
implies that I(s) ≥ I(2H) = H − L. Combining these inequalities gives the stated result.

A.1

Minimizer of potential function

We now consider upper and lower bounds on the minimizers of the potential function F (s) defined in (11).
The basic idea behind our approach is to use the following simple relations for the minimizers of F (s):
min F (s) > min F (s)

=⇒

s∗ > t

(43)

min F (s) < min F (s)

=⇒

s∗ < t.

(44)

s∈(0,t]
s∈(0,∞)

s∈(0,∞)

s∈[t,∞)

Lemma 10. For any t ∈ [2H, ∞),
δ>

t + 2L
log(1 + snr)
10

=⇒

s∗ > t.

(45)

Proof. Observe that φ(x) is convex and thus φ(y) ≥ φ(x) + (y − x)φ′ (x) for all x, y ∈ (0, ∞). Noting that
φ′ (x) = 1 − 1/x and evaluating with y = s/(δ snr) and x = 1/(1 + snr) leads to
 s 
s
≥ log(1 + snr) − .
(46)
φ
δ snr
δ
Using this inequality to lower bound the potential function, we have


δ
s
min F (s) ≥ min I(s) + log(1 + snr) −
s∈[0,t)
s∈[0,t)
2
2
δ
t
≥ H − L + log(1 + snr) − > H,
2
2

(47)
(48)

where the second step follows from Lemma 9 and the assumption t ≥ 2H, and the last step follows from the
assumption on δ. Meanwhile, using the upper bound I(s) ≤ H, we have


δ  s 
= H.
(49)
min F (s) ≤ min H + φ
2 δ snr
s∈(0,∞)
s∈(0,∞)
Thus, we can conclude that the minimum is not attained in the interval (0, t].
Lemma 11. For any t ∈ (0, 2H],
δ<

t − 2L
log(1 + snr)

s∗ < t.

=⇒

(50)

Proof. Noting that φ(x) ≥ 0, we have
min F (s) ≥ min I(s) ≥ I(t) ≥

s∈[t,∞)

s∈[t,∞)

t
δ
− L > log(1 + snr),
2
2

where the third step follows from Lemma 9 for s = t and the assumption t ≤ 2H, and the last step follows
from the assumption on δ. Meanwhile, using the upper bound I(s) ≤ s/2, we have


δ
s δ  s 
= log(1 + snr).
+ φ
(51)
min F (s) ≤ min
2 δ snr
2
s∈(0,∞)
s∈(0,∞] 2

Thus, we can conclude that the minimum is not attained in the interval [t, ∞).

A.2

Smallest stationary point of the potential function

Lemma 12. For any t ∈ (0, ∞),



1
δ < sup s M (s) +
snr
s∈(0,t]


1
δ > sup s M (s) +
snr
s∈(0,t]

=⇒

sAMP < t

(52)

=⇒

sAMP > t.

(53)

Proof. By differentiation and the I-MMSE relation, one finds that every stationary point of F (s) satisfies
the fixed-point equation M (s) − δ/s + 1/ snr = 0. Rearranging and solving for δ, we see that s is a stationary
point if and only if δFP (s) = δ where


1
.
(54)
δFP (s) := s M (s) +
snr
The function δFP (s) is continuous with δFP (0) = 0. Therefore, if δ < sups∈(0,t] δFP (s) then the equation δFP (s) = δ has at least one solution on [0, t), and this implies that sAMP < t. Conversely, if δ >
sups∈(0,t] δFP (s) then there is no solution on [0, t) and this implies that sAMP > t.
11

B
B.1

Proofs of Main Results
Proof of Theorem 4

First we show that Assumption 1 implies that Lǫ /Hǫ → 0, where we recall that Lǫ = Hǫ − Iǫ (2Hǫ ). To see
why, observe that
1
Lǫ =
2

Z

0

2Hǫ

(1 − M (s)) ds = Hǫ

Z

0

1

(1 − M (2Hǫ t)) dt.

(55)

For any η ∈ (0, 1), the integral on the right can be upper bounded as
Z

0

1

(1 − M (2Hǫ t)) dt ≤

Z

1−η

0

(1 − M (2Hǫ t)) dt + η.

(56)

By Assumption 1, the first term on the right-hand side converges to zero in the small-ǫ limit. Noting that η
can be chosen arbitrarily small establishes that Lǫ /Hǫ → 0.
We are now ready to consider the case r ∈ (0, 1). Fix η such that r < 1 − η < 1 and let tǫ = (1 − η/2)2Hǫ.
For all ǫ sufficiently small, we have ηHǫ > 2Lǫ and thus tǫ − 2Lǫ ≥ (1 − η)2Hǫ > r2Hǫ . Under the assumed
scaling in (21), this means that δǫ ≤ (tǫ − 2Lǫ )/ log(1 + snrǫ ) for all ǫ small enough. By Lemma 11, this
implies that s∗ < (1 − η/2)2Hǫ and by Assumption 1, it follows that Mǫ (s∗ǫ ) → 1. The case r ∈ (1, ∞)
follows from a similar argument and is omitted.

B.2

Proof of Theorem 6

Consider the case r ∈ (0, 1). Fix η such that r < (1 − η)2 < 1 and let tǫ = (1 − η)2Hǫ . By (20), for all ǫ
sufficiently small, we have Mǫ (s) ≥ 1 − η for all s ∈ [0, tǫ ], and thus




1
1
≥ sup s 1 − η +
(57)
sup s Mǫ (s) +
snrǫ
snrǫ
s∈[0,tǫ ]
s∈[0,tǫ ]


1
= (1 − η)2Hǫ 1 − η +
(58)
snrǫ


1
.
(59)
> (1 − η)2 2Hǫ 1 +
snrǫ
The scaling (25) combined with the assumption r < (1 − η)2 means that, for all ǫ small enough,


1
.
δǫ < sup s Mǫ (s) +
snrǫ
s∈[0,tǫ ]

(60)

By Lemma 12, this implies that sAMP
< tǫ and by (20), this means that Mǫ (sAMP
) → 1. The case r ∈ (1, ∞)
ǫ
ǫ
follows from a similar argument and is omitted.

12

