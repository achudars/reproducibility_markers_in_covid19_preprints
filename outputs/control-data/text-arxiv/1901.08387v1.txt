Regret Minimisation in Multi-Armed Bandits Using Bounded Arm Memory

arXiv:1901.08387v1 [cs.LG] 24 Jan 2019

Arghya Roy Chaudhuri 1 Shivaram Kalyanakrishnan 1
for a finite time horizon, thereby minimising the expected
Abstract
regret measured with respect to the mean of the optimal
In this paper, we propose a constant word (RAM
arm.
model) algorithm for regret minimisation for
both finite and infinite Stochastic Multi-Armed
Bandit (MAB) instances. Most of the existing regret minimisation algorithms need to remember
the statistics of all the arms they encounter. This
may become a problem for the cases where the
number of available words of memory is limited.
Designing an efficient regret minimisation
algorithm that uses a constant number of words
has long been interesting to the community.
Some early attempts consider the number of
arms to be infinite, and require the reward
distribution of the arms to belong to some particular family. Recently, for finitely many-armed
bandits an explore-then-commit based algorithm (Liau et al., 2018) seems to escape such
assumption. However, due to the underlying
PAC-based elimination their method incurs a
high regret. We present a conceptually simple,
and efficient algorithm that needs to remember
statistics of at most M arms, and for any Karmed
pfinite bandit instance it enjoys a O(KM +
K 1.5 T log(T /M K)/M ) upper-bound on regret. We extend it to achieve sub-linear quantileregret
(Roy Chaudhuri & Kalyanakrishnan,
2018) and empirically verify the efficiency of
our algorithm via experiments.

1. Introduction
In this paper, we investigate the problem of regret minimisation in Multi-Armed Bandit (MAB) (Berry & Fristedt,
1985) using a bounded number of words. Each arm in a
bandit instance represents a slot-machine with a fixed (but
unknown) real-valued reward distribution associated with
it. At each time step, the experimenter is supposed to select and pull an arm, and observe the reward. The goal of
the experimenter is to maximise the expected total reward
1
Department of Computer Science and Engineering, Indian Institute of Technology Bombay, Mumbai 400076, India. Correspondence to: Arghya Roy Chaudhuri <arghya@cse.iitb.ac.in>,
Shivaram Kalyanakrishnan <shivaram@cse.iitb.ac.in>.

A range of real-world applications like drug
testing (Armitage, 1960; Colton, 1963), crowdsourcing (Tran-Thanh et al., 2014) etc. can be modelled
using multi-armed bandits, where the number of arms
is high. In such cases, due to budgetary constraints or
some other practical considerations, it might viable to
experiment only with a small number of arms instead
of the whole pool. The problem is of particular interest
because the experimenter gets to store statistics of a small
but fixed number of arms. Therefore, it adds another
layer of an exploration-exploitation dilemma for the task
of regret minimisation. This particular set-up has been
drawing attention since long ago (Cover, 1968); however,
only a few investigations have been made in this direction.
In this paper, we present a regret minimisation algorithm
that uses a bounded number of words, for both finite and
infinite-armed bandits. Unlike the existing algorithms, our
algorithm does not need any special assumption of reward
distribution of arms but bypass the explicit PAC-based exploration for the sake of efficiency. Below, we formalise
our problem followed by our specific contributions.

Background and Problem Setup. A bandit-instance
B = (A, D) consists of a set of arms A, and a set of subGaussian cumulative distribution functions (CDF) D. Each
arm a ∈ A, when pulled, generates a i.i.d. reward from the
corresponding CDF Da ∈ D, defined over [0, 1]. The exdef
pected reward of arm a ∈ A is given by µa = Er∼Da [r].
We also assume that the experimenter has no information
regarding D. The only way for her to gather knowledge
about D is via generated rewards by sampling the arms. We
def
define a set called history as Ht = {(ai , ri )}ti=1 , where,
ri ∈ [0, 1] is the reward produced at i-th step by pulling the
arm ai ∈ A.

def

Cumulative Regret Minimisation. Assuming µ∗ =
min{y ∈ [0, 1] : ∀a ∈ A, µ(a) ≤ y}, and the given
horizon of pulls as T , the conventional cumulative regret

Regret Minimisation in Multi-Armed Bandits Using Bounded Arm Memory

incurred by a algorithm is defined as

def

∗
∗
RT = T µ −

T
X

E[µat ],

(1)

t

wherein at is the arm pulled by the algorithm at time t. The
expectation is taken over random rewards and the possible
randomisation introduced by the algorithm.
We briefly restate the definition of “quantile regret” introduced by (Roy Chaudhuri & Kalyanakrishnan, 2018)
based on their previous contribution in a pure exploration setting (Roy Chaudhuri & Kalyanakrishnan, 2017).
A problem instance I = (B, PA ) consists of a bandit instance B, and a sampling distribution for choosing arm
from A. Letting ρ ∈ [0, 1], the (1 − ρ)-th quantile of PA is
defined as
µρ = inf{x ∈ [0, 1] : Pr {µa ≤ x} ≥ 1 − ρ}.
a∼PA

(2)

Then, for a given horizon of pulls as T , quantile regret with
respect to µρ is defined as
def

RT (ρ) = T µρ −

T
X

E[µat ],

(3)

t

wherein, at and E[·] bear the same interpretation.
RAM Model. It should be noted that given any bandit instance B = (A, D), as we are not considering any special
structure in A or D, putting a restriction on an algorithm
to use a bounded number of words of space, either restricts
the horizon of pulls, or restricts the algorithm to store statistics of only bounded number of arms simultaneously. In
this paper, we consider the latter and assume M to be that
number. We adopt the word RAM model (Aho et al., 1974;
Cormen et al., 2009), that considers a word as the unit of
space. This model facilitates to consider that each of the input values and variables can be stored in O(1) word space.
For finite bandit instances (|A| < ∞), we consider a word
to be consisted of O(log T ) bits. Therefore, our algorithm
needs space-complexity of O(M log T + log |A|) bits. For
the infinite bandit instances (|A| = ∞), for ρ ∈ [0, 1], if
the experimenter needs to analyse the performance with respect to µρ (the (1 − ρ)-th quantile), she must allow the
algorithm to use O(M log T + log(1/ρ)) bits.
We call this set of arm indices whose statistics are stored as
arm memory and its cardinality as arm memory size. Hence,
an algorithm with arm memory size M can store the statistics of at most M arms. Also, it should be noted that an
algorithm is allowed to pull an arm only if it is stored in the
memory. Hence, before pulling a new arm (which is not
currently in the arm memory), the algorithm should replace

an arm in its arm memory with this new arm. It is interesting to note that the algorithms that work with M = 1, can
only keep the stat of the arm it is currently pulling. Therefore, switching to a new arm costs such an algorithm to
lose all the experience gained by sampling the previous
arm. However, for a finite bandit instance, as the algorithms are allowed to remember all the arm indices, such
an algorithm can store the gained experience by storing a
bounded number of arm indices for possible further special
treatment. The scenario is widely different for infinite bandit instances, where an algorithm can pull a new arm only
if it is chosen by the given sampling distribution PA . In
such a scenario, once an algorithm discards an arm from the
arm memory, it can encounter that arm only if it is sampled
again in future by PA . Hence, the algorithm can not recall a
discarded arm. In the existing literature (Herschkorn et al.,
1996; Berry et al., 1997) on infinite bandit instances, such
algorithms are termed as non-recalling algorithms. However, for M > 1 (but bounded above), an algorithm enjoys
the freedom of ensuring a previously encountered good arm
to keep in the memory, irrespective of whether or not the
bandit instance is finite or not. Our findings show that this
is more beneficial than the non-recalling algorithms for infinite bandit instances.
Problem Definition. Given a positive integer M , below,
we define the problem of conventional regret minimisation
(CR-M) and extend the definition to quantile regret
minimisation (QR-M).
CR-M. An algorithm L is said to solve CR-M, if takes A,
and M as the input and for a sufficiently large budget T
(not necessarily known beforehand) it will achieve R∗T ∈
o(T ); using an arm memory size at most M . It is assumed
that for a finite bandit instance with |A| = K < ∞, the
algorithm is allowed to store O(M log(T ) + log K) bits of
information.
QR-M. Suppose we are given a problem instance I =
(B, PA ), and a positive integer M . Let, ρ0 ∈ (0, 1]. An
algorithm L is said to solve QR-M, if takes A, PA , and M
as the input, and for every ρ ∈ (ρ0 , 1], given a sufficiently
large budget T it will achieve RT (ρ) ∈ o(T ); using an arm
memory size at most M . It is assumed that the algorithm is
allowed to store O(M log(T ) + log(1/ρ0 )) bits of information.
In this paper we present algorithms solve CR-M on finite
bandit instances, and QR-M, with M ≥ 2. Following we
brief our contribution in this paper.
Contributions. We present algorithms for minimisation
of conventional regret and quantile regret, using a bounded
number of words of space. Following is the list of our specific contributions.

Regret Minimisation in Multi-Armed Bandits Using Bounded Arm Memory

1. In Section 4.1 we present an algorithm UCB-M,
∗
which solves
p CR-M, and achieves RT ∈ O(KM +
3/2
(K /M ) T log(T /M K)) over an unknown but finite horizon of T pulls. The existing upper bound on
regret is due to (Liau et al., 2018) and it involves problem specific quantities. Hence ours is the first problem independent finite time regret upper bound. Also,
in Section 4.2 we empirically compare UCB-M and
its variations with the existing algorithms those solve
CR-M.
2. In Section 5.1 we present a meta-algorithm QUCBM, that uses UCB-M as a subroutine to the algorithm QRM2 (Roy Chaudhuri & Kalyanakrishnan,
2018)
QR-M, and achieves RT (ρ) 
∈
 to solve
q
4.9
log M
1
1
T
0.205
0.81
o
+MT
+T
ρ log ρ
M 2 log M .
In Section 5.2 we experimentally demonstrate
that QUCB-M (in terms of conventional regret R∗T ) is more efficient than the algorithms
by (Herschkorn et al., 1996) and (Berry et al., 1997),
on problem instances with Bernoulli arms.

We briefly review the existing literature, before we present
the key intuitions in Section 3.

2. Related Work
Started by Robbins (1952) the predominant body of
literature in stochastic multi-armed bandit is dedicated
to the regret minimisation task on finite and infinite
bandit instances.
Later, a number of salient algorithms like UCB1 (Auer et al., 2002), Thompson Sampling (Chapelle & Li, 2011; Agrawal & Goyal, 2012),
M OSS (Audibert & Bubeck, 2009) has been shown to
achieve the order optimal cumulative regret on the finite
instances. When the number of arms is infinite, algorithms make special assumption is made on the reward
function (Agrawal, 1995; Kleinberg, 2005) or on the sampling distribution (Wang et al., 2008) to guarantee a sublinear regret. Despite a thorough study on the finite and
the infinite instances, the number of investigations in the
memory frugal algorithms is limited.
Finite memory hypothesis testing has been drawing the attention of researchers since long (Robbins, 1956; Isbell,
1959; Cover et al., 1976). However, in MAB setting Cover
(1968) first presented a finite memory algorithm for twoarmed Bernoulli instance, that achieves an average reward
which converges to the optimal proportion in the limit,
with probability 1. His approach consisted of a collection of interleaved test and trial blocks, where each test
block is divided into several sub-blocks and the switching
among these sub-blocks is governed by a finite state machine. However, he considered only two-armed Bernoulli

instances, and the approach guarantees only an asymptotic
convergence of the empirical average reward. Hence, this
setup is not very interesting, as our objective is to present a
finite-time analysis of regret for general bandit instances.
Herschkorn et al. (1996) presented the first non-recalling
algorithm for infinite bandit instances with Bernoulli arms,
that maximises the almost sure average reward over an infinite horizon. Berry et al. (1997) improved over them for
the problem instances where the sampling distribution PA
is uniform over the set expected rewards of the Bernoulli
arms. Towards relaxing the assumption of Bernoulli reward distribution Peköz (2003) showed that a peculiarity
that may arise if the reward distributions of the arms are
not stochastically-ordered. Specifically, for some function f : R+ 7→ R+ , he proposed two policies—PolicyA
and PolicyB parameterised by f (·), where the latter is a
non-stationary version of the former. Then he showed
that for some choice of f (·), there exist instances with a
bounded positive reward on which in the limit, exactly for
one of PolicyA and PolicyB, the average reward will converge to the supremum mean reward, while for the other,
it will converge to the infimum mean reward. Most recently, Liau et al. (2018) have presented an explore-thencommit strategy based algorithm UCBC ONSTANT S PACE
that incurs sublinear finite-time regret on any finite bandit
instance. However, their algorithm explicitly uses PACbased arm elimination strategy that leads to a high regret.
On the other hand, like the previous algorithms, their algorithm does not have the provision to take the advantage
availability of larger arm memory. Next, we describe the
key intuitions behind our approach.

3. Key Intuitions
One of our objectives is to solve CR-M for finite bandit instances (|A| < ∞). The problem is interesting
for M < |A|; otherwise, one can solve the problem
by using any existing regret minimisation algorithm like
UCB1 (Auer et al., 2002) etc. Intuitively, any algorithm
that solves CR-M for finite bandit instances, must ensure that the probability of pulling the optimal arm is increased by progressively increasing at least one of the two
probabilities—first, the probability of the optimal arm a∗ is
in arm memory; second, if a∗ is in arm memory, the probability that it will be pulled more often than the other arms
in arm memory. For any algorithm that achieves sub-linear
regret we can write, for any horizon T , R∗T ∈ o(T ) =⇒
∗
P
LtT ↑∞ RTT = LtT ↑∞ T1 Tt=1 Pr{at = a∗ } = 1. Now,
imposing the arm memory constraint, and letting Xt be
the current arm memory at t-th pull, we notice, {at =
a∗ } =⇒ {a∗ ∈ Xt }. Therefore,
Pr{at = a∗ } = EXt [Pr{at = a∗ |a∗ ∈ Xt } Pr{a∗ ∈ Xt }] . (4)

Regret Minimisation in Multi-Armed Bandits Using Bounded Arm Memory

Hence, the necessary and the sufficient condition for an algorithm that asymptotically solves CR-M is

LtT ↑∞

PT

t=1

∗ ∗
∗
EXt [Pr{at =a |a ∈Xt } Pr{a ∈Xt }]
T

= 1,

Given a bandit instance, algorithm of Liau et al. (2018) first
solves a pure exploration problem for a horizon T̄ (a function of the mean reward of the arms) to maximise the quantity Pr{a∗ ∈ Xt } in the R.H.S. of Equation (4). Once the
number of pulls crosses T̄ , it chooses the arm with the highest empirical reward in Xt as the contentious best arm, and
assigns the rest of the horizon to that arm. Therefore, for
t > T̄ , it switches to pure-exploitation mode, thus maximising the quantity Pr{at = a∗ |a∗ ∈ XT̄ }. On the contrary,
we adopt balanced exploration with an aim to simultaneously increase Pr{a∗ ∈ Xt } and Pr{at = a∗ |a∗ ∈ Xt }.
Therefore, our algorithm does not depend on such T̄ . It
should be noted that, for the sake of sufficient exploration,
an algorithm should not stick to the same arm memory for
too long, however, while selecting new arms (not in the current arm memory), it must judiciously choose the in memory arms to replace. This trade-off relies on the notion of
simple regret which we introduce next.
Simple Regret. Whereas the cumulative regret minimisation problem is based on the trade-off between exploration
and exploitation, there is a separate line of literature in pure
exploration setting. One of the popular problems in pure exploration setting is to minimise “simple regret”. If bt ∈ A
is the arm recommended by the algorithm after the t-th pull,
then the simple regret of the algorithm at t is defined as,
def

1. Execute allocation strategy that possibly depending
on Ht−1 selects an arm at .
2. Pull the arm at , and get a reward rt . Update Ht =
Ht−1 ∪ {(at , rt )}.

for |Xt | ≤ M , where 1 ≤ t ≤ T .

∗
∗
E[r ] = µ − E[µbt ].

t = 1, H0 = {∅}.
While (stopping condition is not met){

(5)

Relation of Cumulative Regret Minimisation with the
Minimisation of Simple Regret. Bubeck et al. (2009)
gave a general definition of a forecaster, depicted in Figure 1. Given a set of arms as A as input, at each step t,
possibly depending on Ht−1 , it selects an arm at by using
a strategy called “allocation strategy”. On pulling the arm
at it receives a reward rt , and executes a “recommendation
strategy” that takes Ht as the input and outputs an arm bt .
The forecaster continues to alternately execute allocation
strategy and recommendation strategy until some stopping
condition is met.
A careful look reveals that a forecaster that at each step, t,
recommends the arm, which is selected by allocation strategy in the same step (that is bt ≡ at in Figure 1), then the
cumulative regret (Equation 3) of that forecaster is identical to the sum of simple regret (Equation 5) over time steps
t. This tempts one to intuit that using an allocation strategy
which incurs low cumulative regret will help in designing

3. Execute recommendation strategy that possibly depending on Ht outputs an arm recommendation bt .
Update t = t + 1. }

Figure 1: A general forecaster.

a forecaster that achieves a small simple regret and viceversa. However, Bubeck et al. (2009) present a negative
result on this trade-off. Further they (Bubeck et al., 2009)
present a upper bound on E[r∗ ] for a number of forecasters (Bubeck et al., 2009, Table 1) one of which is defined
below:
UCB-MPA. A forecaster, which at each step uses
UCB1 (Auer et al., 2002) as the allocation strategy, and
uses the recommendation strategy that outputs the most
played arm (MPA), shall be called as UCB-MPA.
We quote their result (Bubeck et al., 2009, Theorem 3) in
Theorem 3.1 which serves as the cornerstone of analysis of
the algorithm UCB-M.
Theorem 3.1 (Distribution-free upper bound on Simple-Regret of UCB-MPA by Bubeck et al. (2009)). Given a Ksized set of arms A as input, if UCB-MPA runs for a horizon of T pulls such that T ≥ K(K + 2), then for some
constant Cq> 0, it achieves the expected simple regret
E[r∗ ] ≤ C

K log T
T

.

Although UCB1 was originally designed as a cumulative
regret minimisation algorithm, empirically it performs reasonably well as an exploration strategy to give us a good
balance between exploration and exploitation. We choose
UCB-MPA over other forecasters as it is easy to comprehend and leads a simpler derivation. For the rest of the
paper we shall use log and ln to denote base 2 and natural
logarithm respectively. Also, for any positive integer Z we
shall denote the set {1, 2, 3, · · · , Z} by [Z].

4. Algorithm for Finite Bandit Instances
We present the algorithm UCB-M and establish a problemindependent upper-bound on the cumulative regret. Then
we empirically compare UCB-M and its variations with
the algorithm by Liau et al. (2018). Algorithm 1 is based
on UCB-MPA. However, one can the replace the underlying call to UCB1 with any other allocation strategy like Thompson sampling (Agrawal & Goyal, 2012), or

Regret Minimisation in Multi-Armed Bandits Using Bounded Arm Memory

M OSS Audibert & Bubeck (2009), as we do in our experiments.
4.1. Algorithm and Regret-Analysis.
Algorithm 1 describes UCB-M that solves the problem
CR-M for finite bandit instances. We improve upon the
contribution of Liau et al. (2018) in three aspects—first,
UCB-M is empirically much more efficient even if we allow M = 2 (as opposed to M = 4 for theirs) as it does
not explicitly use pure exploration based elimination; second, it scales with the arm memory size; third, we present
a distribution-free upper bound on the incurred regret of
UCB-M for solving CR-M on finite bandit instances.
Given a finite set of arms A (|A| = K < ∞), and arm
memory size M (2 ≤ M < K) UCB-M approaches in
def
phases. It breaks each phase into h0 = ⌈(K − 1)/(M − 1)⌉
sub-phases. Inside any phase w, at each sub-phase j, it runs
UCB-MPA on an M -sized subset of arms S w,j (called
arm memory), and assigns the recommended arm to â, and
forwards it to the next sub-phase. On the subsequent subphase (that might belong to the next phase), it chooses
M − 1 new arms from A, along with the arm â forwarded
from the previous sub-phase, and repeat the previous steps.
It is to be noted that the horizon spent on each sub-phase
of a phase w is the same and is given by bw . Also, for
w ≥ 2, the total horizon spent in phase w is given by
h0 bw = 2h0 bw−1 . To satisfy the assumption in Theorem3 of Bubeck et al. (2009), at the first phase, for each of the
sub-phases, UCB-M chooses a horizon of b1 = M (M + 2)
pulls. For the rest of the analysis of UCB-M, we shall denote, the number of phases executed by UCB-M as x0 .
For M >= K, as the arm memory size is large enough, it is
effectively removing the memory constraint. In such unconstrained scenarios it is preferable run UCB1 (Auer et al.,
2002) on the whole instance, which will incur a lower regret. We adopt this into UCB-M, and hence, running UCBM with M ≥ K is identical to run original UCB1.
Theorem 4.1. Given a set of K arms A, with K ≥
3, an arm memory size M such that 2 ≤ M <
K, as input, then for a horizon of T pulls, with
T  > KM 2 (M + 2), UCB-M will incur R∗T =
p
O KM + (K 3/2 /M ) T log(T /KM) .

We note that for a given bandit instance with K arms, an
arm memory size M , and horizon T , the number of subphases is h0 = ⌈(K − 1)/(M − 1)⌉, and the total numdef
ber of phases is upper bounded by x0 = ⌈log(2T /M K)⌉
(Lemma A.2 in Appendix A). Now, we upper bound the
maximum regret incurred in a sub-phase inside a given
phase, and sum over all the sub-phases.
As UCB-M ensures inclusion of the optimal arm at least

Algorithm 1 UCB-M: For finite bandit instances.
Input: A: the set of K arms indexed by [K], M (≥ 2): Arm
memory size.
if M ≥ K then
Run UCB1 on A until the horizon runs out.
else
b1 = M (M + 2).{Initial horizon per sub-phase}
â = 1. {Initial arm recommendation}
w = 1.l {Counts
m number of phases}

K−1
.{The number of sub-phases in a phase}
h0 = M
−1
Randomly shuffle the arm indices.
while the horizon is not finished do
l=0
for j = 1, · · · , h0 ; if the horizon is not finished do
S w,j = {l + 1, · · · , min{l + 1 + (M − 1), K}}.
def
l = The highest arm index in S.
if â 6∈ S w,j then
S w,j = {â} ∪ S w,j \ {l}.
l = l − 1.
end if
{A LLOCATION S TRATEGY}
Run UCB1 on S w,j for horizon of bw pulls
or the remaining horizon; whichever is smaller.
{R ECOMMENDATION S TRATEGY}
def
â = The most played arm in S w,j .
end for
w = w + 1. {Increment phase count}
bw = 2 · bw−1 . {Increment horizon per sub-phase}
end while
end if

once in every phase, we note the following.
Corollary 4.2. Let us denote,
the sequence
def
of sub-phase-wise arm memory as S
=
{S 1,1 , S 1,2 , · · · , S 1,h0 , S 2,1 , S 2,2 , · · · , S 2,h0 , · · · , S x0 ,1 ,
S x0 ,2 , · · · , S x0 ,h0 }. Then, for d ≥ h0 , at least one of any
d consecutive elements of S contains a∗ .
In any given phase, we need to upper-bound the difference
of mean of the best in memory arm between two successive
sub-phase. Considering S as defined in Corollary 4.2, let
y,j
ay,j
∈ S be the arm recommended by the sub-phase
∗ ∈ S
j−1 to j. It is important to note that maxa∈S y,j µa ≥ µay,j
.
∗
Therefore, in the interest of finding a upper bound on the
= maxa∈S y,j µa as a
regret, it is safe to consider µay,j
∗
pessimistic estimate of the best mean in S y,j . In any given
def
−
sub-phase j ≤ h0 − 1 in phase y, we let E[ry ] = E[µay,j
∗
].
Now,
noticing
that
on
each
sub-phase
in
a
phase
y
µay,j+1
∗
UCB-M spends by pulls we upper bound E[ry ] as follows.
Corollary 4.3. Using Theorem 3.1, in phase y, at the end
of each sub-phase j, the expected simple regret
p with respect
y
(M log by )/by .
to µay,j
is
upper-bounded
as
[r
]
≤
C
E
∗
The upper bound is independent of j as the budget for each
sub-phase in a given phase remains the same.

Regret Minimisation in Multi-Armed Bandits Using Bounded Arm Memory

We notice, that the arm forwarded from each sub-phase to
the next one, not necessarily to be the optimal arm. Hence,
in the worst case, the expected difference between the mean
of the optimal arm, and the highest mean reward in the current arm memory grows linearly with the number of subphase in a given phase. We upper bound it as follows.
Lemma 4.4. Let, we are given a K-sized set of arms A,
and an arm memory size M . Also, let as defined in Corollary 4.3 at any phase y ≥ 2, in the sub-phase j, if µy,j
∗ is
the maximum of the mean of the arms in the arm-memory,
then
y
max E[µ∗ − µy,j
∗ ] ≤ 2h0 E[r ].
1≤j≤h0

The proof is presented in Appendix A. Next, we use
Lemma 4.4 to upper bound the cumulative regret (R∗T ).
Bifurcation of R∗T . For any given phase w, and a subdef
= max µa : a ∈ S w,j , and Rw,j be the
phase j, let µw,j
∗
incurred regret. Then,
∗
Rw,j
= bw µ∗ −

bw
X

E[µat ]

Lemma 4.7. For 2 ≤ M < K, and T > KM 2 (M +
Px0 Ph0
(2)
2), and for some constant C ′′ > 0, w=1
j=1 Rw,j ≤


p
C ′′ KM + T K log(T /M K) .
Proof of Theorem 4.1 Using Equation 6, and applying
Lemma 4.5 and Lemma 4.7 we prove the theorem.
Next, we present an empirical comparison of UCB-M
and some of its variations with the algorithm of Liau et al.
(2018).
4.2. Experiment
The use of UCB1 (Auer et al., 2002) as a subroutine in
Algorithm 1, can be replaced by any other allocation
strategies, which in effect will give rise to a different upper bound. In the interest of studying the empirical behaviour, we consider M OSS (Audibert & Bubeck, 2009)
and T HOMPSON Sampling (Agrawal & Goyal, 2013) in our
experiments, and rename UCB-M to TS-M and M OSS -M
respectively. However, everything else (in Algorithm 1) including the recommendation strategy, is kept unchanged.

i=1

= bw E[µ∗ − µw,j
∗ ]+

bw
X

(E[µw,j
∗ ] − E[µat ]).

t=1

Where the expectation is taken over all possible sources of
(1)
randomisation. Now, letting Rw,j = bw (µ∗ − µw,j
∗ ), and
P
(2)
bw
w,j
Rw,j = t=1 (E[µ∗ ] − E[µat ]), we can write,
RT∗ =

x0 X
h0
X

w=1 j=1

∗
Rw,j
=

x0 X
h0
X

(2)

(1)

(Rw,j + Rw,j ).

(6)

w=1 j=1

(1)

Now, using Lemma 4.4 we upper bound Rw,j as follows.
Lemma 4.5. For 2 ≤ M < K, and for T > KM 2 (M +
Px0 Ph0
(1)
′
2), and for some constant C ′ ,
w=1 j=1 Rw,j ≤ C

p
KM + (K 3/2 /M ) T log(T /M K) .
For the detailed proof we refer to Appendix A. We note,
Px0 Ph0
(2)
that w=1
j=1 Rw,j , can be upper-bounded using the
problem independent upper-bound on the cumulative regret
of UCB1 (Auer et al., 2002), as we restate below.

Lemma 4.6 (Distribution-Free Upper Bound on Cumulative Regret of UCB1 Auer et al. (2002)). Given a set of Karms as the input, for any horizon
T , the cumulative regret
√
incurred by UCB1 RT∗ ≤ √
12 T K log T + 6K. Further, if
T ≥ K/2, then RT∗ ≤ 18 T K log T .
Next,
using Lemma 4.6,
we upper bound
P x 0 P h0
(2)
,
with
the
proof
given
in Appendix A.
R
w=1
j=1 w,j

Bandit Instances. We run the experiments on three different instances. Let, K be the number of arms in the instance. Also, for convenience, let the arms indices be sorted
in descending order of their mean, with µ1 = µ∗ = 0.99.
As we randomly permute the arm indices in all our experiments, this assumption does not affect the results. We write
K
BL
to denote an instance in which the mean of the K arms
are linearly spaced between 0.99 (= µ∗ ) and 0.01. The
other two K-armed instances which are analogous to the
ones used by Jamieson et al. (2014). For α ∈ {0.3, 0.6},
they are defined as BαK , in which any sub-optimal arm
i > 1, has the mean µi = 0.01 + µ∗ − (µ∗ − 0.01)((i −
1)/(K − 1))α .
For K = 100, Figure 2 compares the cumulative regret incurred by algorithms UCB-M, TS-M, M OSS -M
for an arm memory size M = 2, with the algorithm of
Liau et al. (2018) (UCBC ONSTANT S PACE). A comparison of cumulative regret, and the number of pulls to the
individual arms in the instances with K = 10 is presented
in Figure 4 in Appendix B. It is important to note that despite using larger arm-memory of M = 4, which is twice
of the others, their algorithm incurs a significantly higher
regret.
Intuitively, Liau et al.’s (2018) algorithm first solves a pure
exploration problem to identify a near optimal arm, and
then commits the rest of the horizon to that arm. Consequently, it spends a prohibitively large number of pulls on
the sub-optimal arms leading to a high regret. In contrast,
we just make sure that at any instant, the expected difference between the mean of the optimal arm and the best arm

Regret Minimisation in Multi-Armed Bandits Using Bounded Arm Memory

in the current arm memory is not too large. Apparently, this
difference increases with the subsequent sub-phases. However, UCB-M ensures to choose the optimal arm in its arm
memory at least once in any given phase leading to a “reset” to this difference. On the other hand, this difference
progressively reduces due to doubling the budget in each
phase. This explains why UCB-M, TS-M, and M OSS -M
incur significantly lower regret.

Figure 3: Comparison of incurred regret (y axis) on the instance
100
BL
by different algorithms by varying arm memory size M (x
axis), and the horizon. Each bar represents incurred regret averaged over 100 iterations, and with one standard error. For details
about the instances and the algorithms we refer to Section 4.2.

5. Algorithm for Infinite Bandit Instances
Figure 2: Comparison of incurred cumulative regret in log scale
(y axis), by UCB-M, TS-M, M OSS -M, for M = 2, and the algorithm of Liau et al. (2018) (uses M = 4) after 106 pulls. Each bar
represents the average over 100 iterations, and with one standard
error on mean. For details about the instances and the algorithms
we refer to Section 4.2.

As UCB-M can take advantage of larger arm memory size,
next we shall compare the incurred regret by varying it. Recalling the algorithm UCB1 (Auer et al., 2002), if an arm
a has been pulled uta times till the time step t, and if µ̂ta
is its empirical average reward, then the upper
pconfidence
bound of that arm is given by ucbta = µ̂ta + η 2 log t/uta ,
with η = 1. It can be experimentally validated that tuning
η can lead to achieving a smaller regret as claimed by the
authors (Auer et al., 2002, Section 4). We present the regret incurred by UCB-M for η = 0.2, alongside the other
algorithms.
Intuition suggests that increasing arm memory should help
in achieving a low regret, as it increases the chance of
pulling the optimal arm more frequently. Also, the upper
bound given by Theorem 4.1 supports this intuition. However, in practice, we notice an interesting behaviour. On
the instance BL100 , we compare the cumulative regret incurred by UCB-M, TS-M, and M OSS -M in Figure 3 by
varying M . For a comparison on the other instances, the
reader is referred to Figure 5 in Appendix B. As expected,
UCB-M, TS-M and M OSS -M always incur a higher regret
than their unconstrained (M = K) counter parts. Also, for
UCB-M with η = 0.2, TS-M and M OSS -M increasing the
arm memory size M makes them achieve a lower regret.
However, the behaviour of UCB-M (η = 1) is significantly
different from the other two. If M < K, it incurs a relatively low regret for M = 2. Afterwards it increases with
M , followed by a slow decrease. We conclude that this peculiarity in its behaviour is due to the intrinsic looseness in
the calculation of upper confidence bound. Also, that is the
reason why UCB-M with η = 0.2, and the others not only
incur a lower regret but behave consistently.

In this section, we provide a bounded arm-memory algorithm and its upper bound on the incurred quantile-regret.
Also, on various problem instances we empirically compare its incurred conventional cumulative regret (R∗T ) with
the existing algorithms.
5.1. Algorithm and Quantile-Regret Analysis
We solve the problem of QR-M by modifying the algorithm QRM2 (Roy Chaudhuri & Kalyanakrishnan, 2018)
to make it use UCB-M as the sub-routine, and adjust the
arm exploration rate accordingly to minimise the upper
bound. We call it QUCB-M and describe in Algorithm 2.
Algorithm 2 QUCB-M
Input: A, PA , and arm memory size M
 1
Set α = 0.205, B = M 2 (M + 2) 1−α , and K0 = ∅
for r = 1, 2, 3, · · · do
tr = 2r B, nr = ⌈tα
r ⌉.
Form a set Kr by selecting additional nr −|Kr−1 | arms from
A using PA , and adding to Kr−1 .
Run UCB-M on Kr , for horizon of tr , with arm memory
size M
end for

Below, we present the upper bound on the quantile regret
incurred by QUCB-M.
Theorem 5.1 (Sub-linear quantile-regret of QUCB-M).
For ρ ∈ (0, 1) and for sufficiently large T , QUCB-M incurs
RT (ρ) ∈ o



1
ρ

log ρ1

4.89

+ M T 0.205 + T 0.81

q

log M
M2


T
.
log M

Proof. To
prove
the
theorem
we
follow
the steps of proof for Theorem 3.3 in
Roy Chaudhuri & Kalyanakrishnan (2018).
For any
fixed ρ ∈ (0, 1), we break the analysis for upper bound on
RT (ρ) in cases—first, the algorithm never encounters an
arm from T OP ρ ; second, it picks at least one arm from
T OP ρ .

Regret Minimisation in Multi-Armed Bandits Using Bounded Arm Memory

The key step in the analysis of the first part is showing
that there exists r∗ ≥ 1 such that for all r ≥ r∗ , the set
of arms Kr is sufficiently large to contain an arm from
T OP ρ with high probability. Defining T OP ρ is in Kr
def
as Er (ρ) = {Kr ∩ T OP ρ = ∅}, and following the steps
for the derivation of Equation (3) in the proof of Theorem
3.3 in Roy Chaudhuri & Kalyanakrishnan (2018) we arrive
at

Plog T
r=1

tr Pr{Er (ρ)} ∈ O



1
ρ

log ρ1

 α1


log e
1− α1+γ
+T
. (7)

The detailed derivation of Equation (7) is given in
Lemma C.2 in Appendix C.
In the second part, we upper-bound the incurred regret
for the case where QUCB-M encounters at least one
arm from the T OP ρ in Kr (the event ¬Er (ρ)). Using Theorem 4.1 and using the similar approach for
deriving Equation (4) in the proof of Theorem 3.3 in
Roy Chaudhuri & Kalyanakrishnan (2018) we arrive at
log T

X

r=r ∗

Table 1: Cumulative regret (/105 ) of QUCB-M, QTS-M,
QM OSS -M (with α = 0.347) and the strategies proposed by
Herschkorn et al. (1996) and Berry et al. (1997) after 106 pulls,
on instances I1 , I2 , I3 and I4 . Each result is the average of 20
runs, showing one standard error.
Algorithms
Non-stationary Policy
(Herschkorn
et al., 1996)
√
T -run
(Berry et al., 1997)
√
T ln T -learning
(Berry et al., 1997)
√
Non-recalling T -run
(Berry et al., 1997)
QUCB-M

QUCB-M (η = 0.2)

QTS-M

QM OSS -M

M

I1 : β(0.5, 2)
µ∗ = 1

I2 : β(1, 1)
µ∗ = 1

I3 : β(0.5, 2)
µ∗ = 0.6

I4 : β(1, 1)
µ∗ = 0.6

1

3.58 ±0.4

1.11 ±0.2

1.64 ± 0.2

0.79 ± 0.1

2

6.18±0.5

1.11±0.4

4.18±0.3

2.03±0.3

2

6.32±0.4

0.69±0.3

4.38±0.2

2.15±0.3

1

5.35 ±0.5

0.03 ±0.004

4.56 ± 0.001

2.55 ± 0.001

2

1.84±0.17

0.41±0.02

1.29±0.10

0.49±0.02

10

1.98±0.16

0.59±0.02

1.49±0.09

0.63±0.01

2

2.00±0.20

0.32±0.05

1.41±0.10

0.69±0.04

10

1.71±0.16

0.16±0.02

1.16±0.09

0.30±0.02

2

1.77±0.17

0.32±0.04

1.23±0.09

0.40±0.02

10

1.91±0.16

0.18±0.03

1.14±0.10

0.30±0.02

2

1.74±0.17

0.31±0.02

1.20±0.10

0.39±0.02

10

1.69±0.15

0.25±0.02

1.13±0.09

0.30±0.010



p
tr log tr ,
C nr M + (n3/2
r /M )



p
p
≤ C ′ M T α + ( log M /M ) T 1+3α log(T /M ) ,

(8)

for some constant C ′ . The intermediate steps to obtain (8)
are presented in Lemma C.3 in Appendix C. Combining
Equation (7), Equation (8) and substituting for tr∗ , the upper bound on RT (ρ) with respect to T gets minimised for
α = 1/(3 + 2 log e/(1 + γ)) ≈ 0.205, thus proving the
theorem.
It is to be noted that inside QUCB-M one can use the algorithm by Liau et al. (2018) instead of UCB-M. However,
as we have already shown in Section 4.2 that UCB-M is
empirically superior than their algorithm, we do not consider this variation in our our experiment.
5.2. Experiment
Although QUCB-M is designed with the aim to minimise quantile-regret, we use conventional cumulativeregret as the evaluation metric. Similar to UCB-M,
the algorithm QUCB-M can be altered to use TS-M or
M OSS -M as the subroutine instead, and we call them
QTS-M and QM OSS -M respectively. Algorithm 2 uses
the value of α that minimises the upper bound on regret in Theorem 5.1. However, for empirical efficiency,
we keep the α = 0.347 as used by the algorithm
QRM2 (Roy Chaudhuri & Kalyanakrishnan, 2018). We
compare incurred conventional regret by each of these algorithms against the algorithms by Herschkorn et al. (1996)
and Berry et al. (1997), and present it in Table 1.
We use the same four Bernoulli instances used by

Roy Chaudhuri & Kalyanakrishnan (2018)—instances I1
and I2 have µ∗ = 1, and the probability distributions on µ
induced by PA are given by β(0.5, 2), and β(1, 1) respectively. Similarly, instances I3 and I4 have µ∗ = 0.6, and
the probability distributions on µ induced by PA are given
by scaled β(0.5, 2), and β(1, 1) respectively. Each column
of the tables is labelled by the corresponding probability
density function of encountering the mean rewards. As Table 2 suggests, the existing algorithms incur a significantly
higher regret in most of the cases. We put the comparison
for α = 0.205 at Table 2 in Appendix D.
It is interesting to note that, like the finite instances, increasing arm memory leads to a lower regret. Specifically, the
scaled version of QUCB-M (using UCB-M with η = 0.2)
along with QTS-M and QM OSS -M show an improvement
with larger arm memory. However, with η = 1 in the underlying UCB-M, QUCB-M fails to take the advantage of
larger arm memory.

6. Conclusion
In this paper, we address the problem of regret minimisation using a bounded number of words of memory. This
problem becomes interesting where the number of arms is
too large to consider all of them simultaneously, for example, crowd-sourcing, drug testing etc. Some existing approaches (Herschkorn et al., 1996; Berry et al., 1997) considers only the infinite bandit instances consist of Bernoulli
arms. Recently, Liau et al. (2018) present an explore-thencommit based algorithm for finite bandit instances, which
escapes such assumptions, but very inefficient in practice.

Regret Minimisation in Multi-Armed Bandits Using Bounded Arm Memory

We provide a UCB1 (Auer et al., 2002) based algorithm
UCB-M for finite bandit instances, which is empirically
far more efficient and enjoys a sub-linear upper bound
on the cumulative regret, but uses a bounded number
of words of memory. Also, unlike all the existing algorithms, UCB-M offers the flexibility of varying the
arm memory size, facilitating the experimenter to use the
available memory resource. Further, we extend the existing algorithm QRM2 (Roy Chaudhuri & Kalyanakrishnan,
2017) for quantile-regret minimisation to QUCB-M to
achieve sub-linear quantile regret under the bounded arm
memory constraint. We empirically verify that QUCBM incurs a lower conventional cumulative regret on a
various infinite bandit instances than the existing algorithms (Herschkorn et al., 1996; Berry et al., 1997), which
needs O(1) memory.
We find that providing a lower bound on the cumulative regret under the bounded arm memory constraint is an interesting question, and we leave that for future investigation.

References
Agrawal, R. The continuum-armed bandit problem. SIAM
J. Control Optim., 33(6):1926–1951, 1995.
Agrawal, S. and Goyal, N. Analysis of Thompson sampling
for the multi-armed bandit problem. In Proc. of the 25th
Annual Conf. on Learning Theory, volume 23, pp. 39.1–
39.26, Edinburgh, Scotland, 2012. PMLR.
Agrawal, S. and Goyal, N. Further optimal regret bounds
for thompson sampling. In Proc. AISTATS 2013, volume 31, pp. 99–107. PMLR, 2013.
Aho, A. V., Hopcroft, J. E., and Ullman, J. D. The Design
and Analysis of Computer Algorithms. Addison-Wesley,
1974.
Armitage, P. Sequential Medical Trials. Blackwell Scientific Publications, 1960.
Audibert, J.-Y. and Bubeck, S. Minimax policies for adversarial and stochastic bandits. In Proc. COLT 2009, pp.
217–226, 2009.
Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2-3):235–256, 2002.
Berry, D. and Fristedt, B. Bandit Problems: Sequential
Allocation of Experiments. Chapman & Hall, 1985.
Berry, D. A., Chen, R. W., Zame, A., Heath, D. C., and
Shepp, L. A. Bandit problems with infinitely many arms.
The Annals of Statistics, 25(5):2103–2116, 1997.
Bubeck, S., Munos, R., and Stoltz, G. Pure exploration in
multi-armed bandits problems. In Algorithmic Learning
Theory, pp. 23–37. Springer Berlin Heidelberg, 2009.
Chapelle, O. and Li, L. An empirical evaluation of thompson sampling. In Advances in neural information processing systems, pp. 2249–2257, 2011.
Colton, T. A model for selecting one of two medical treatments. Journal of the American Statistical Association,
58(302):388–400, 1963.
Cormen, T. H., Leiserson, C. E., Rivest, R. L., and Stein,
C. Introduction to Algorithms, Third Edition. The MIT
Press, 2009.
Cover, T. M. A note on the two-armed bandit problem
with finite memory. Information and Control, 12(5):371
– 377, 1968.
Cover, T. M., Freedman, M. A., and Hellman, M. E. Optimal finite memory learning algorithms for the finite sample problem. Information and Control, 30(1):49 – 85,
1976.

Regret Minimisation in Multi-Armed Bandits Using Bounded Arm Memory

Herschkorn, S. J., Peköz, E., and Ross, S. M. Policies without memory for the infinite-armed Bernoulli bandit under the average-reward criterion. Prob. in the Engg. and
Info. Sc., 10(1):21–28, 1996.

Isbell, J. R. On a problem of robbins. Ann. Math. Stat., 30
(2):606–610, 06 1959.

Jamieson, K. G., Malloy, M., Nowak, R., and Bubeck, S.
lil’ UCB : An optimal exploration algorithm for multiarmed bandits. In Proc. COLT 2014, pp. 423–439, 2014.

Kleinberg, R. Nearly tight bounds for the continuum-armed
bandit problem. In Adv. NIPS 17, pp. 697–704. MIT
Press, 2005.

Liau, D., Price, E., Song, Z., and Yang, G. Stochastic multiarmed bandits in constant space. In Proc. AISTATS 2018,
volume 84, pp. 386–394. PMLR, 2018.

Peköz, E. A. Some memoryless bandit policies. Journal of
Applied Probability, 40(1):250–256, 2003.

Robbins, H. Some aspects of the sequential design of experiments. Bulletin of the AMS, 58(5):527–535, 1952.

Robbins, H. A sequential decision problem with a finite
memory. PNAS, 42(12):920–923, 1956.

Roy Chaudhuri, A. and Kalyanakrishnan, S. PAC identification of a bandit arm relative to a reward quantile. In
Proc. AAAI 2017, pp. 1977–1985. AAAI Press, 2017.

Roy Chaudhuri, A. and Kalyanakrishnan, S. Quantileregret minimisation in infinitely many-armed bandits. In
Proc. UAI 2018, 2018. To appear.

Tran-Thanh, L., Stein, S., Rogers, A., and Jennings,
N. R. Efficient crowdsourcing of unknown experts using bounded multi-armed bandits. Artif. Intl., 214:89 –
111, 2014.

Wang, Y., Audibert, J.-Y., and Munos, R. Algorithms for infinitely many-armed bandits. In Adv. NIPS 21, pp. 1729–
1736. Curran Associates Inc., 2008.

Regret Minimisation in Multi-Armed Bandits Using Bounded Arm Memory

A. Proofs from Section 4.1
Lemma A.1. For a given K-sized set of arms A, and an
arm memory size M < K, the number of sub-phases required to ensure that each arm in A has been chosen into
arm memory at least once is not more than h0 .
Proof. We notice that at the beginning of each sub-phase
there are exactly M − 1 arms except the arm â recommended from the previous step. Let, h be the maximum
number of sub-phases possible in a phase. We realise
that each phase w ends as soon as for every arm a ∈ A,
w,j
there exists a sub-phase j, such that
l S m ∋ a. Therefore,
y
K−1
w,j
h = min{y : A ⊆ ∪j=1 S } = M−1 = h0 .

Lemma A.2. For a given K-sized set of arms A, and an
arm memory size M < K, the number
l of phases
m UCB-M
def
2T
executes is upper bounded by x0 = log MK
.

Proof. Let x be the total number of phases executed by
UCB-M. It should be noted that the value of M , K, and T
might be such that the total horizon (T ) runs out before finishing the last phase. Now, for any given phase w (w ≥ 1),
the horizon spent on each sub-phase is the same, that is
bw = 2w−1 b1 . Therefore, we can write
T =

h0
x X
X

b w = h0 b 1

w=1 j=1

x
X

w=1

2w−1 = h0 b1 (2x − 1),


T
+1 ,
=⇒ x = log
h0 b 1



l K − 1 m
T
m + 1 , because, h0 =
≤ log  l
,
K−1
M −1
b
1
M−1


T
m
≤ log  l
+ 1 ,
K−1
M
(M
+
2)
M−1


[because, b1 = M (M + 2)],

!


T
T
+ 1 < log
+1 ,
≤ log K−1
KM
M−1 M (M + 2)


2T
≤ log
[because, T > 2M K],
KM
l
2T m
=⇒ x ≤ log
= x0 .
MK
Lemma 4.4. Let, we are given a K-sized set of arms A,
and an arm memory size M . Also, let as defined in Corollary 4.3 at any phase y ≥ 2, in the sub-phase j, if µy,j
∗ is
the maximum of the mean of the arms in the arm-memory,
then
y
max E[µ∗ − µy,j
∗ ] ≤ 2h0 E[r ].
1≤j≤h0

Proof. Letting, E[r∗y,j ] = E[µ∗ −µy,j
∗ ]. We break the proof
into two steps. Step 1 upper bounds E[r∗y,h0 ], which is an
upper bound on E[r∗y,j ], for all j ∈ [h0 ] ; while Step 2
upper bounds E[r∗y+1,j ]. Both the steps are based on Corollary 4.2, that ensures at least one of the h0 consecutive subphases (not necessarily from the same phase) must contain
the optimal arm a∗ in the arm-memory.
def

Step 1. Let, 1 ≤ k0 ≤ h0 − 1 be the first sub-phase
in phase y, to have a∗ in the arm-memory. Therefore,
def
k0 = min{i ∈ [h0 ] : a∗ ∈ S y,i }, and hence, by definition,
y,k +1
E[r∗ 0 ] = E[ry,k0 +1 ]. Therefore, for any subsequent
sub-phase j ∈ {k0 + 1, · · · , h0 } in phase y, E[r∗y,j ] =
Pj−1
y,v
y,j
y,k +1
E[µ∗ − µ∗ ] = E[µ∗ − µ∗ 0 ] + v=k0 +2 E[µ∗ −
y,v+1
µ∗
]. As there are h0 sub-phases in any phase, hence,
for all k0 + 1 ≤ j ≤ h0 , E[r∗y,j ] ≤ E[r∗y,h0 ] ≤ (h0 − k0 +
1) E[ry ] ≤ h0 E[ry ].

Step 2. Let, j0 be a sub-phase in phase y − 1, such that
a∗ ∈ S y−1,j0 . From Step 1, E[r∗y−1,h0 ] ≤ (h0 − j0 +
1) E[ry−1 ]. Now, considering sub-phase i in phase y, we
realise that if i ≥ j0 , then there exists a sub-phase w ∈
{1, · · · , i} such that a∗ ∈ S y,w . Now, for i ≤ j0 − 1,

E[r∗y,i ] ≤
≤ max

max

max

2≤j0 ≤[h0 ] 1≤i≤j0 −1

max

2≤j0 [h0 ] 1≤i≤j0 −1

y−1,h0

E[r∗

] + i · E[ry ],

≤ (h0 − j0 + 1) E[ry−1 ] + i · E[ry ],

≤ (h0 − 1) E[ry−1 ] + E[ry ] < 2h0 E[ry ]

[Because E[ry ] < E[ry−1 ] < 2 E[ry ]].

Together, Step 1 and Step 2 prove the lemma.

Lemma 4.5. For 2 ≤ M < K, and for T > KM 2 (M +
Px0 Ph0
(1)
′
2), and for some constant C ′ ,
w=1 j=1 Rw,j ≤ C

p
KM + (K 3/2 /M ) T log(T /M K) .

Regret Minimisation in Multi-Armed Bandits Using Bounded Arm Memory

Proof.
x0 X
h0
X

(1)

Rw,j =

w=1 j=1

h0
X

(1)

R1,j +

≤ h0 b 1 +
≤ h0 b 1 +
≤ h0 b 1 +

(1)

Rw,j

w=2 j=1

j=1

x0 X
h0
X

x0 X
h0
X

(1)
Rw,j

w=2 j=1

x0 X
h0
X

∗

bw E[µ −

µw,j
∗ ]
w

bw (2h0 E[r ]) [using Lemma 4.4]

≤ h0 b1 + 2C1 h20

x0
X

w=2
x0
X

w=2

bw

r

M log bw
[using Corollary 4.3]
bw

p
bw M log bw

x0
X
p
 1
≤ h0 b1 + 2C2 h20 M b1
2w−1 log 2w−1 b1 2
w=2

[because, bw = 2w−1 b1 ]

= h0 b1 + 2C2 h20

x0
X
p
1
(w − 1 + log b1 ) 2w−1 2
M b1
w=2

x0
X
p
1
≤ h0 b1 + C3 h20 M b1
(w − 1)2w−1 2
w=2


because, T > KM 2 (M + 2), and b1 = M (M + 2),

therefore, x0 − 1 ≥ log b1 = log M (M + 2)]
p
1
≤ h0 b1 + C4 h20 M b1 (x0 · 2x0 ) 2
&
'
K −1
≤ C5
M (M + 2)+
M −1

'2
&
 21

T
K−1 p 2
T

log
M (M + 2)
M −1
MK
MK

≤ C7
≤ C8

(2)

Rw,j =

≤ h0 b 1 +

[substituting for b1 , h0 and x0 ]
2 √
 21 !

T
K
T
K 2
3
M +
log
M
M
M
MK
MK
!
r
K 3/2
T
T log
KM +
M
MK
!
r
K 3/2
T
KM +
T log
,
M
MK


wherein, C1 , C2 , · · · , C8 are appropriate constants.
Lemma 4.7. For 2 ≤ M < K, and T > KM 2 (M +
P 0 Ph0
(2)
2), and for some constant C ′′ > 0, xw=1
j=1 Rw,j ≤


p
C ′′ KM + T K log(T /M K) .

h0
X

w=1 j=1
x0
X

h0
x0 X
X

(2)

Rw,j ,

w=2 j=1

C

w=1

&

(2)

R1,j +

j=1

h0
x0 X
X

= h0 b 1 + h0 C

w=2 j=1

≤ h0 b1 + 2C1 h20

≤ C6

h0
x0 X
X

w=1 j=1

w=2 j=1

h0
x0 X
X

Proof. We notice that at any sub-phase j of any phase w ≥
2, due to Lemma
4.6, there exists a constant C, such that
√
(2)
Rw,j ≤ C bw M log bw . Therefore,

p

bw M log bw [using Lemma 4.6],

p
2w−1 b1 M log (2w−1 b1 ),
[substituting for bw ]

'

K −1
M (M + 2)+
M −1
'
&
x0
X
K −1 p 2
C1
(w − 1)2w−1 +
M (M + 2)
M −1
w=1

2w−1 log(M (M + 2)) [substituting for h0 and b1 ],

K −1
≤ C2
M (M + 2)+
M −1
!
x0
X
p
K −1p 2
M (M + 2)
(w − 1)2w−1
M −1
w=1


because, T > KM 2 (M + 2)


K 2 K 3/2 √
M +
M
x0 2x0
≤ C3
M
M
!
r
√
T
T
,
log
= C3 KM + K M
MK
MK
!
r
T
≤ C4 KM + T K log
,
MK
≤

wherein, C1 , C2 , C3 , C4 are appropriate constants.

Regret Minimisation in Multi-Armed Bandits Using Bounded Arm Memory

B. Additional Experimental Results from
Section 4.2

(a)
100
Figure 6: Comparison of incurred regret on the instance B0.6
.
6
Each bar represents regret incurred after 10 pulls, averaged over
100 iterations, and with one standard error. For details about the
instances and the algorithms we refer to Section 4.2.

(a)
10 6
10 4
10 2
10 0
10 6
10 4
10 2
10 0
10 6
10 4
10 2
10 0

1

2

3

4

5

6

7

8

9

10

(b)
Figure 4: Comparison of incurred cumulative regret (4(a)) and
the number of pulls to the individual arms (4(b)) on the instances
10
10
10
BL
, B0.3
and B0.6
by algorithms UCB-M, TS-M, M OSS -M for
M = 2, and the algorithm of (Liau et al., 2018), after 106 pulls.
Each bar represents the average over 100 iterations, and with one
standard error. For details about the instances and the algorithms
we refer to Section 4.2.

(a)
100
Figure 5: Comparison of incurred regret on the instance B0.3
.
6
Each bar represents regret incurred after 10 pulls, averaged over
100 iterations, and with one standard error. For details about the
instances and the algorithms we refer to Section 4.2.

Regret Minimisation in Multi-Armed Bandits Using Bounded Arm Memory

C. Proofs from Section 5.1
In this appendix we provide the materials to complete the
proof of Theorem 5.1.

m

l
Lemma C.1. Let, r∗ = α1 log 1ρ log ρ1 − log B . Then,
for every phase r ≥l rm∗ , the lsize of Kr canm be lower
α log e
bounded as nr = tα
≥ (1+γ)ρ
· ln tr , wherein,
r
def

0.53 < γ = maxx

log log x
log x

< 0.531.

l

 α1 m

1
1
.
Proof. We notice, for every, r ≥ r∗ , tr ≥
ρ log ρ
∗
Then, for each r ≥ r , we can lower lbound
m the size of the
set Kr as follows. As, |Kr | = nr = tα
r is an integer, to
u

Therefore, Pr{Er (ρ)} = (1 − ρ)nr ≤ exp(−⌈(α/((1 +
e
γ))) · ln tlog
⌉) ≤ tr −α log e/(1+γ) .
r
Using Lemma C.1, below we present the detailed steps for
obtaining (7) in the proof of Theorem 5.1.
log(T /B)

X

1
log su∗
ρ

=

 α1

1
1
1
log
log
,
ρ
ρ
ρ


1
1
1
log + log log
,
=
αρ
ρ
ρ
!
log log ρ1
1
1
1+
log
=
,
αρ
ρ
log ρ1


1+γ
1
log log x
def
≤
log , as γ = max
x
αρ
ρ
log x


1
1+γ 1
,
log
=
α
ρ
ρ
1+γ α
=
s ∗.
α u
α
=⇒ sα
log su∗
u∗ ≥
(1 + γ)ρ
α log e
ln su∗ .
=⇒ sα
u∗ ≥
(1 + γ)ρ

log(T /B)

X

tr Pr{Er (ρ)} +

∗
rX
−1

tr Pr{Er (ρ)}

r=r ∗
log(T /B)

X

tr +

tr 1−

α log e
1+γ

r=r ∗

r=1

log(T /B)

≤ tr ∗ +

α log e
1+γ

X

tr 1−

X

(B2r )

r=r ∗
log(T /B)

= tr ∗ +

log e
1− α1+γ

r=r ∗

≤ tr ∗ + B
≤B·2
<2

log(T /B)−r ∗

log e
1− α1+γ

j=1

l

1
log
log( ρ

1
log
log( ρ

X

1
ρ

)

1
α

1
ρ

)

+1

1
α

−log B

+T

m



T
B2j

+T

log e
1− α1+γ

(9)

α log e
ln su∗
(1 + γ)ρ

Therefore, recalling that r is an integer, for all values of
r ≥ ⌈r∗ ⌉, the statement of the lemma follows.
Lemma C.2. The expected regret due to not encountering
any arm from 
the set T OP ρ is during running
of the algo

 α1
α log e
1
1
rithm, is in O
+ T 1− 1+γ .
ρ log ρ
Proof. We define an event that no arm from T OP ρ is in Kr
def
as Er (ρ) = {Kr ∩ T OP ρ = ∅}, and note Pr{Er (ρ)} =

log e
1− α1+γ

log e
1− α1+γ

logX
T −r ∗
j=0

∞  j(1−
X
1
j=0

As, sα
u grows with u faster than log su , therefore,
∀u ≥ u∗ , sα
u∗ ≥

∗
rX
−1

r=1

≤


tr Pr{Er (ρ)}

r=1

+

ease the calculation let us define su = 2 B, where u ∈ R ,
and therefore, su ∈ R+ does not need to be an integer.

 α1
def
Now, letting u∗ = log ρ1 log 1ρ , we get

=

(1 − ρ)nr . Now, for some α ∈ (0, 1) that shall be tuned
later, let r∗ = ⌈(1/α) log((1/ρ) log(1/ρ))⌉. Therefore, in
∗
the round r∗ , the number of pulls is given by tr∗ = 2r =
((1/ρ) log(1/ρ))1/α . Now, for r ≥ r∗ , the number of arms
log e
in Kr is given by nr = tα
⌉,
r ≥ ⌈(α/((1 + γ)ρ)) · ln tr
wherein, γ = maxx (log log x)/ log x (0.53 < γ < 0.531).

log e
 j(1− α1+γ
)
1
2

α log e
1+γ )

2



1
log e
1
log( ρ
log 1ρ ) α
1− α1+γ
=O 2
+T
!
1

log e
1 α
1
1− α1+γ
+T
log
=O
ρ
ρ
!
1

log e
1 α
1
1− α1+γ
.
+T
log
=O
ρ
ρ

Lemma C.3. For r∗ defined in Lemma C.1, given that for
all r ≥ r∗ , algorithm QUCB-M has encountered at least
one arm from T OP ρ , the incurred regretq
beyond the round


√
log
M
T
T 1+3α log M
r∗ is not more than C ′ M T α + M
;
for some constant C ′ .

Regret Minimisation in Multi-Armed Bandits Using Bounded Arm Memory

log(T /B)

r


tr
1
,
C nr M +
n3r tr log
M
nr M
r=r ∗
r


log(T /B)
X
tr
1
1+3α
log
,
C tα
M
+
t
r
r
M
nr M
r=r ∗
s
!

log(T /B)
X
1
B
αr α
,
=
C 2 B M+
2(1−α)r
2(1+3α)r B (1+3α) log
M
M
r=r ∗
!
p
log(T /B)
q
X
B (1+3α) log(B/M )
αr α
=
C 2 B M+
(1 − α)r2(1+3α)r ,
M
r=r ∗
!
p
log(T /B)
X
B (1+3α) log(B/M ) p (1+3α)r
αr α
≤
C1 2 B M +
r2
M
r=r ∗


s
p
α
1+3α

log(T /B)
(1+3α) log(B/M )
X
T
T
T
B
BαM +
log 
≤
C1  j
2 B
M
B2j
B
r=r ∗


s 
 α √
log(T /B)−r ∗
1+3α
X
T
T
T
log
M
C2 M
=
+
log  ,
2j
M
2j
B
j=0

j
log(T /B)−r ∗ 
X
1
α

+
≤ C3 M T
2α
j=0

s
r
√
 1+3α
log(T /B)−r ∗
X
log M (1+3α)/2
1
T
,
log
T
j
M
B
2
j=0
!
r
√
log
M
T
,
≤ C4 M T α +
T 1+3α log
M
B
!
r
√
 1
log M
T
α
1+3α
≤ C5 M T +
T
[because, B = M 2 (M + 2) 1−α ]
log
M
M
X

for some constants C1 , C2 , C3 , C4 and C5 .

Regret Minimisation in Multi-Armed Bandits Using Bounded Arm Memory

D. Additional Experimental Results from
Section 5.2
Table 2: Cumulative regret (/105 ) of QUCB-M, QTS-M,
QM OSS -M and the strategies proposed by (Herschkorn et al.,
1996) and (Berry et al., 1997) after 106 pulls, on instances I1 , I2 ,
I3 and I4 . Each result is the average of 20 runs, showing one
standard error.
Algorithms
Non-stationary Policy
(Herschkorn
et al., 1996)
√
T -run
(Berry et al., 1997)
√
T ln T -learning
(Berry et al., 1997)
√
Non-recalling T -run
(Berry et al., 1997)
QUCB-M

QUCB-M η = 0.2

QTS-M

QM OSS -M

M

I1 : β(0.5, 2)
µ∗ = 1

I2 : β(1, 1)
µ∗ = 1

I3 : β(0.5, 2)
µ∗ = 0.6

I4 : β(1, 1)
µ∗ = 0.6

1

3.58 ±0.4

1.11 ±0.2

1.64 ± 0.2

0.79 ± 0.1

2

6.18±0.5

1.11±0.4

4.18±0.3

2.03±0.3

2

6.32±0.4

0.69±0.3

4.38±0.2

2.15±0.3

1

5.35 ±0.5

0.03 ±0.004

4.56 ± 0.001

2.55 ± 0.001

2

3.69±0.34

0.74±0.11

2.27±0.21

0.51±0.07

10

4.26±0.37

0.91±0.19

2.65±0.22

0.63±0.11

2

3.67±0.35

0.72±0.12

2.21±0.21

0.55±0.08

10

4.15±0.36

0.79±0.19

2.51±0.22

0.54±0.11

2

3.14±0.39

0.62±0.07

1.97±0.19

0.44±0.07

10

3.88±0.35

0.67±0.13

2.49±0.23

0.45±0.06

2

3.64±0.34

0.70±0.11

2.21±0.21

0.46±0.07

10

4.16±0.36

0.80±0.19

2.53±0.22

0.52±0.11

For α = 0.205 the algorithms explore very small number
of arms, that causes incorporating a good arm very unlikely
leading to a high regret.

