Image Enhancement by Recurrently-trained Super-resolution Network
SAEM Park
Seoul National University, Seoul, Korea
LG Eletronics, Korea

Nojun Kwak
Seoul National University, Seoul, Korea
nojunk@snu.ac.kr

arXiv:1907.11341v1 [eess.IV] 26 Jul 2019

parksaem2@snu.ac.kr

Abstract
We introduce a new learning strategy for image enhancement by recurrently training the same simple superresolution (SR) network multiple times. After initially training an SR network by using pairs of a corrupted low resolution (LR) image and an original image, the proposed
method makes use of the trained SR network to generate
new high resolution (HR) images with a doubled resolution from the original uncorrupted images. Then, the new
HR images are downscaled to the original resolution, which
work as target images for the SR network in the next stage.
The newly generated HR images by the repeatedly trained
SR network show better image quality and this strategy of
training LR to mimic new HR can lead to a more efficient
SR network. Up to a certain point, by repeating this process
multiple times, better and better images are obtained. This
recurrent leaning strategy for SR can be a good solution for
downsizing convolution networks and making a more efficient SR network. To measure the enhanced image quality,
for the first time in this area of super-resolution and image
enhancement, we use VIQET [18] MOS score which reflects
human visual quality more accurately than the conventional
MSE measure.

1. Introduction
Nowadays, 2K (1920×1080) videos are widely used in
digital broadband systems. From the viewpoint of image
quality, there are two major sources of impaired quality in
the current video images. The first one is the intrinsic limitation of image capture systems such as lens optics, sensor
resolution, focusing performances and ISO noise. These defects make inherent limitations in image quality. Therefore
almost every videos cannot show ideal 2K image quality,
with maximum frequencies. The second source of corruptions is due from image processing such as compression,
resolution converting and noise reduction. Most components of a broadband system have a fixed resolution and
they require unavoidable needs of image scaling. Partic-

Figure 1. Proposed Concept. It would be better to learn through
super-resolution-processed images. The figures are HR0 (original), HR1 and HR3 obtained by our method. With the same
network, (c) shows better quality than (b).

ularly, in order to transmit sources with lower resolution
made in the past via a broadband system, video upscaling is
inevitable. Also most of contents providers use heavy compression and restoring the lost information is a difficult task
which requires enhanced techniques.
The early image enhancement techniques were mostly at
the level of unsharp-masking, which is just a simple amplification operation. With the advent of high-resolution
panels, super-resolution techniques have been developed to
improve the original resolution. In recent years, machine
learning technologies are widely used and deep learning
has achieved state-of-the-art super-resolution performance.
However, as the resolution of the image as well as the complexity of the algorithm increases, implementation becomes
harder and harder. Therefore, it is necessary to develop
cheaper and more effective algorithms for image enhancement and super-resolution.
In this paper, as shown in Figure 1, we introduce the
concept of iterative learning as an approach for image enhancement. In general, if the same algorithm is applied repeatedly, the result will be reinforced. Especially in deep
learning, we can approach this idea without additional computational cost at inference time. What we need is additional training with the new HR, made in the previous stage.
Through this recurrent learning, the image converges to a

better quality, and it is possible to obtain an enhancement
over the existing super-resolution with a smaller-sized network. By training a small network with 200K parameters
three repetitions, we could get images with better quality
compared to a model with over 1M parameters.
The contributions of this paper are as follows:
1. By showing an effective way of repeatedly training a
simple feed-forward network for super-resolution through
recurrent learning, we improve the efficiency of superresolution achieving similar results to those of complex
deep networks without much computational cost.
2. By shifting the evaluation criteria of the image quality from a simple arithmetic measure like Mean-SquaredError (MSE) to a user-centered measure of VIQET [18]
Mean-Opinion-Score (MOS), which can be computed in an
automated way without human involvement, we can focus
on additional subtle image quality improvement and set the
number of repetitions appropriately.

2. Related work
2.1. Image enhancement
Along with the development of computer vision and the
spread of digital broadcasting, image processing techniques
have been widely used in image production and reproduction. These techniques can be roughly divided into noise reduction and image enhancement. The former includes technologies such as de-noising and de-interlacing that remove
artifacts that should not be present in the original image. In
the latter, sharpness and contrast are used to amplify some
components in the original image for better image quality.
These algorithms typically perform a series of operations
that widen the min-max range of the original image, such
as the Laplacian filtering. The Laplacian filtering is inexpensive and effective, but it is not possible to perform more
complex processing such as restoring missing texture components or broken edges.
There are several image enhancement algorithms that resolve these weaknesses. The first one is improving the adaptiveness of the processing region such as Adaptive Unsharp
Masking [13], which selects the processing region wisely.
The second one is to use two or more weights in the function
to enable more complex processing, which includes Bilateral Filter [21] and NLM Filter[2]. There are huge amount
of related works, but with the advent of super-resolution, research on conventional image enhancement has diminished.
This is mainly because the improvement is due from the amplification of some components, and the original frequency
is maintained. Ultimately, it requires an approach such as
super-resolution in order to change the shape of the original.
We present a direction for developing new super-resolution
images for better quality.

2.2. Super-resolution
Before the boom of deep learning, as a better upscaler,
super-resolution algorithms were developed. These techniques have mainly been applied to a display system such
as a television. It is proposed as a solution to compensate for the mismatch between the input image size and
the panel resolution. There are two main categories of this
technology: 1) utilizing machine learning algorithms and 2)
weighted image blending using the patches from the original image. The former is similar to the super-resolution
approaches through the current deep learning, but with simple architectures consisting of only one layer. The latter
is a self-image generation method that retrieves the most
similar patches from the surrounding areas of images after upscaling [6]. Conventional super-resolution methods
are well documented in [19]. They can perform upscaling
while keeping the sharpness of an edge. However, there
are technical limitations such as the problem for a large
upscaling ratio or impossibility of restoring complex textures. These require huge computations, but it was difficult to learn enough with a human-designed shallow network. Recently, deep neural networks have been improved
to enable more powerful super-resolution processing. This
is why we use deep learning as a solution.

2.3. Super-resolution with deep learning
Recently, as a branch of deep learning research, ‘single image super-resolution’ has emerged. It makes downsized low-resolution (LR) images and trains convolution
networks so as to reproduce the original images from the LR
images. Compared to the revolutionarily simple SRCNN
[5], the architecture has become heavier and more complex. VDSR [7] introduced VGG and ResNet into the superresolution filed in 2015. ESPCN [14] and DRCN [8] present
various aspects of residual structure and the view of upscaling in 2016. SRGAN [10], SRResNET [9], DRRN [15],
EDSR [11], DenseSR [17], MDSR [11] and Memnet [16]
developed Resnet architecture and adjusted the GAN solution into super-resolution in 2017. In 2018, super-resolution
is now more widely used in terms of utilization. A good
example is [20]. There is a research utilizing a recursive
method for super-resolution [4]. Different from ours, it tries
to produce a face image from an 8×8 tiled mosaic through
repeated application of super-resolution, which is similar to
info-GAN [3] that produces a facial image.
Regardless of the super-resolution algorithm, the general
operation principle is similar. Basically, the single image
super-resolution is learned by back propagation so as to reduce the difference between the original and the generated
image through the SR network from an LR image.
There are some limitations in these works. First, in the
low-resolution images used in other SR papers, the improvement of the image quality by increasing the resolu-

Figure 2. Recurrent Training Strategy(RTS). LR is a low resolution image obtained by down-scaling the original image (HR0 ).
One stage is composed of the SR training phase (Phase A) and
the image enhancement phase (Phase B). By successive application of these two phases, we can obtain a better SR network and
enhanced images. Although it takes multiple times to learn, at inference time, it is possible to obtain images with quality equivalent
to the finally learned HRn level at once by feed-forwarding with
the learned weights.

tion is remarkable. However in a high-resolution image, the
improvement caused by an increase in resolution is not obvious. Second, the advances in super-resolution have led
to an increased performance sacrificing the computational
complexity. This high computational demand makes it hard
to implement super-resolution with hardware. Therefore,
it is necessary to develop a method that can achieve better performance with a more simple network. Finally, since
current super-resolution algorithms are learned only from
the original images, they target the original and we cannot
expect image quality beyond the original. In the following
section, we present a new approach to get images with better quality at a low cost through recurrent learning.

3. Proposed method
3.1. Recurrent Training Strategy (RTS)
The core of this paper is the recurrent training strategy of
an SR network which is achieved by reducing the size of the
output image of the SR network to its original size and retraining the SR network with this new target. The proposed
recurrent training strategy is briefly shown in Figure 2.
As can be seen in the figure, a stage of RTS consists of
two phases. In the first phase (Phase A), SR network is
trained. In this phase, the SR is learned so that the target image is obtained from the degraded LR image, and the weight
of the super-resolution network is updated. The LR image
is a low quality image which is corrupted through downscaling, adjusting compression and so on. In Stage 1, the

target is set to the original image, HR0 and the network is
tuned such that the LR is improved to the image quality of
the original HR0 level.
The second phase, Phase B, is the process of creating a
new target image using the network obtained in Phase A.
This is done by inputting the original image HR0 to the
trained network of Phase A and we can expect that the output image SR1 has higher quality than the original image
HR0 . By downscaling the output image SR1 , we can obtain a new image HR1 which acts as a target in the next
stage. Generally, super-resolution is accompanied by increasing the size of the image to increase the image resolution. So the additional down-scaling process is required
because the resolution must be the same as the original for
retraining the network.
To summarize, we have to go through two steps to get a
new HR image. The former is the process of obtaining the
super-resolution function SR(·), and the latter is the process of obtaining enhanced image by down-scaling the output image of the SR network back to the original size. Then,
we perform this process repeatedly on a stage basis, so that
the same super-resolution processing can be more effective.
The overall procedure of Stage n can be summarized as follows:
1
Phase A: SRn = arg min ||HRn−1 − SR(LR)||2
2
SR
(1)
Phase B: HRn = d(SRn (HR0 )).
Here, SR(·) and d(·) denote the super-resolution network
and the down-scaling operation, respectively. LR is an image which has lower resolution than the original HR0 .
By applying super-resolution to the original image, a
higher quality image is expected to be obtained. Even if
the obtained image is down-scaled to the original size, components from the additionally generated resolution may remain, this is the same as an UHD (Untra HD) channel that
looks better than the existing channels on a FHD (full HD)
TV. If HRn has a better image quality than HRn−1 , we
can assume that the super-resolution network can learn from
this better target, and after the learning, the resultant network will be better. Therefore, assuming that we go through
this additional process in a direction that gradually improves
the results, we will eventually reach the maximum improvement point of each image. And it will perform better than
the current super-resolution network trained to mimic the
original image.

3.2. Reason for enhanced quality
If y is the original image, U is the up-scaler, D is the
down-scaler, and R is the residual component, the super
resolution trained by x = D(y) can be expressed as follows:
SR(x) = U (x) + R(x),
(2)

Figure 3. Used network for recurrent learning. This network was developed exclusively for recursive learning systems. The biggest
difference from the existing SR network is that it was designed to produce the best result at the blue layer where the residual is added with
only 3 channels. The model is also very light and easy to control the number of layers.

SR(D(y)) = U (D(y)) + R(D(y)).

(3)

Here, we can suppose D(U (y)) is the same as y, and
U (D(y)) becomes a locally blurred version of y:
D(U (y)) = y,

U (D(y)) = ȳ

(4)

in x. By down-scaling R1 (y), the distribution will be more
contracted near zero and we can approximate D(R1 (y))
with a contracted version of R1 (x) as
D(R1 (y)) ≈ αR1 (x),

(0 ≤ α ≤ 1).

(11)

Finally, with (7), HR1 (y) can be approximated as
Combining (2) and (4), we get
R(x) = SR(x) − ȳ.

HR1 (y) ≈ y + αR1 (x) ≈ y + α(y − ȳ).
(5)

Because the goal of learning is to minimize the error ||y −
SR(x)||:


1
2
∗
SR = arg min
ky − SR(x)k ,
(6)
2
SR
if enough learning has been done, R(x) becomes similar to
y − ȳ:
R(x) ≈ y − ȳ.
(7)
In the first stage, after the super resolution is fully learned,
instead of D(y), we can input y to the SR network, then it
becomes
SR1 (y) = U (y) + R1 (y).
(8)
By down-scaling this, we get the target image for stage 2
which can be expressed as
HR1 (y) = D(SR1 (y)) = D(U (y) + R1 (y)).

(9)

Assuming a linear1 down-scaler, as D(U (y)) = y, it becomes
HR1 (y) = y + D(R1 (y)).
(10)
The distributions of zero-mean residual components of different sized images will be approximately equal, that is
p(z|z ∈ R1 (x)) = p(z|z ∈ R1 (y)), unless there are not
much high frequency components only existing in y and not
1 For a nonlinear down-scaler such as nearest neighbor or bicubic interpolation, we can take Taylor series expansion.

(12)

The above equation has high similarity with unsharp masking. In other words, HR1 (y) has a high probability that the
components with a large deviation from the mean are amplified. And it can be expected that the new target image
HR1 (y) can have improved sharpness compared to y.
When the super resolution network is trained with this
new target, it becomes
HR2 (y) ≈ y + αR2 (x) ≈ y + α(y − ȳ) + α2 (y − ȳ). (13)
This is obtained by replacing y in (7) with the new target
y + α(y − ȳ) in computing R2 (x). As stage goes on, it
becomes
n
X
HRn (y) ≈ y +
αi (y − ȳ).
(14)
i=1

Since α is smaller than 1, (Experimental results show that
the average α for stage 1 on DIV2K valid set is 0.779 with
the standard deviation of 0.098.) higher order α terms will
disappear and HRn can be expected to convergence to a
certain point ŷ not far from y. Using the SR network, the
actual difference y− ȳ is learned by a nonlinear function and
it makes hard to guarantee convergence, these non-linearity
has much better performance than the simple USM algorithm. As iteration goes on, the difference tend to converges
to ŷ − ȳ and the image quality of ŷ becomes better than that
of the original image y.

3.3. Configuration of super-resolution network
We set up a convolution network based on the residual
layer. The use of multiple layers of residual type has the

Table 1. Image difference ratio of validation set. In our model
with the proposed recurrent training strategy, the change of difference ratio from the previous stage is minimum at stage 4.

Stage N
HR1
HR2
HR3
HR4
HR5
HR6
HR7
HR8

Difference Ratio
form the Original(%)
0.95
1.35
1.47
1.48
1.51
1.55
1.62
1.76

Delta from (N-1)
0.95
0.40
0.12
0.01
0.03
0.04
0.07
0.14

advantage of automatically adapting image filters of various sizes. In order to make use of these advantages and to
easily control the number of parameters, we constructed the
network shown in Fig. 3.
This network is designed exclusively for recurrent learning, which uses a subsequent down-scaler after the operation of super-resolution. The biggest difference from the
regular super resolution network is that it produces the best
output of the same resolution with original, not in an upscaled domain. Learning proceeds in the same way as the
existing super resolution, but since the expected output is
synthesized in 3ch at the original resolution, optimum result is produced at the corresponding step.
It is composed of an input module, an output module and
three 64-channel residual networks using 3×3 filters The input module consists of 128×128 RGB (3ch) input and 64ch
output. After the input module, a series of residual layers, all of which have the same number of channels (64 ch)
with 3×3 convolutions, is applied. As shown in the figure,
the base model uses 3 layers. In the output module, the final residual layer does not make use of ReLU because the
difference between the input and the output will have both
a positive and negative value. Here, the output with blue
residual will be next stage’s HR, because the next channel
extension and pixel shuffle has no ReLU and it can be canceled by the down-scaler. The residual layer is based on
SRResNet [9] and VDSR [7]. The final residual without
ReLU is from VDSR, and the inner residual layer shares
the concept with SRResNet.
Finally, we need a down-scaler to convert the up-scaled
image to the same size as the original, using the Lanczos
scaler. Lanczos filter has better performance than linear and
bicubic filters, and it makes α greater in (11).

3.4. Setting the number of stages
In our proposal, the number of stages N can be set as
the point where the quality of HRN +1 is worse than that of

Figure 4. Change of image difference vs. stage. It can be seen
that the difference converges to almost zero in HR4 (Yellow Line).
But with repeated application of unsharp masking, we can observe
the delta increases linearly (Green Line).

HRN and the corresponding network SRN can be used as
the final super resolution network. The important thing here
is how to judge whether the output image gets better or not.
As a result of our experiment, we can consider a couple of
solutions. The easiest approach is to measure the quality of
the actual validation set to see if this score is increased. This
is intuitive, but it has the disadvantage that it is difficult to
use it because of the time and effort required for measurement. The second method is to confirm whether the change
||HRN +1 − HRN || becomes larger or smaller as the repetition progresses. As the output of each stage approaches
a certain ideal point, the difference will gradually decrease.
When the output of the stage deviates from the ideal point,
the divergence starts and the difference will increase. This
will be explained in more detail in the following experimental results.

4. Experimental result
4.1. Training setup
In our experiment, we used the DIV2K image dataset [1],
which contains 2K RGB images. This is to prove the performance of image enhancement on contents in the FHD level
which occupies the majority of current broadcasting contents. The DIV-2K dataset consists of 800 train sets and 100
validation sets. From the 800 DIV-2K training set, we made
our training database which was constructed by ripping 100
random patches with a 256×256 size. After making the
original images, HR0 , we adjusted the JPEG compression
and down-scaled the patches to the half size to create the
LR images. The learning was carried out from the 80,000
pairs made in this way.
In all of our experiments, we used MSE as a loss function and Adam as an optimizer. Batch size of 8 was used
with learning rate of 0.001. We trained the network for 20
epochs.

(Original)

(HR1)

(HR2)

(HR3)

(HR4)

Figure 5. Comparison of output images in each stage. For better comparison, each row shows a part (64 × 64 pixels) of an image in
DIV2K image set. As described in the introduction, the originals are naturally blurred in the capturing and the transmission process. The
stage number increases from left to right. As the stage increases, it can be observed that the texture is clearly seen and the result becomes
clearer. It is easily identifiable in a man’s beard, in the particle of the bricks, and in the clarity of the letters.

4.2. Convergence of recurrent learning
In order to confirm whether the proposed recurrent learning converges to the maximum improvement point or simply diverges as if the Laplacian filter is applied multiple
times, we observed the difference of output images between
consecutive stages. The image difference is an objective indicator that shows the average difference of all the pixels in
an image in terms of ratio. It can be similar to MSE, but
it is normalized to the image’s brightness and resolution. It
is a more interpretable measure in that it is more intuitive
telling the percentage of the image changed. The average
image difference of the 100 validation images in each stage

is shown in Table 1.
Figure 4 shows Table 1 to a graph. It shows that the
learning converges to a certain point. And it can be seen
that the change of difference starts to increase after a specific point, the Stage 4. In order to confirm that the recurrent
learning is different from the repetitive application of a simple filter, we repeatedly applied unsharp-masking (USM)
filter to the original image, which is shown by the green
line. The difference increases linearly and image difference
from Stage (N-1) does not converge ever. Also, from the
observation that the image difference is linearly increased
by repeated USM application, we can confirm that the image difference is properly measured.

Table 2. Result of VIQET image quality measurement. The final MOS score in this table is obtained based on the detailed score of 10
items excluding F LAT R EGION I NDEX which results in NaN for all the cases. Except for over and under exposed ratio, a high score lead
to a high MOS, which means a better quality.

Name
LR
HR0
HR1
HR2
HR3
HR4
HR5
HR6
HR7
HR8

MOS
Score
3.645
3.825
3.962
4.028
4.057
4.050
4.048
4.038
4.030
4.030

Resol
ution
2.8
2.8
2.8
2.8
2.8
2.8
2.8
2.8
2.8
2.8

Edge
Acutance
52.6
56.6
82.7
92.5
94.3
93.1
92.1
89.5
87.9
87.3

SN
Index
311.9
314.8
322.3
325.3
326.2
326.7
326.7
327.6
329.0
331.1

Texture
Acutance
96.3
108.7
150.7
167.8
171.8
171.4
170.5
168.1
166.9
166.1

Satu
ration
116.8
117.3
117.7
118.4
118.0
117.1
117.7
116.0
114.6
111.7

Color
Warmth
107.7
107.8
107.8
108.0
107.8
107.2
107.6
107.8
107.3
106.1

Illumi
nation
156.8
163.6
182.7
190.0
191.4
191.1
190.7
188.2
186.8
186.2

Dynamic
Range
101.0
101.0
101.3
101.2
101.2
101.3
101.3
101.1
101.1
101.2

Over
Expose
0.1
0.1
0.1
0.0
0.0
0.1
0.1
0.0
0.1
0.1

Under
Expose
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.4

the learned. This is not what we want, because we do not
have any reference. Fortunately, the Video Quality Experts
Group (VQEG) has developed a tool that measures the image quality and developed the Mean Opinion Score (MOS)
of an image as 0 (Poor) to 4.5 (Best). Using this tool, we
can evaluate the image quality improvement for each stage
objectively.
In the VQEG image quality Evaluation Tool (VIQET)
[18], the average MOS score is derived for each image
through a total of 11 measures: 1. R ESOLUTION , 2. M ULTI E DGE ACUTANCE , 3. N OISE S IGNATURE I NDEX , 4.
F LAT R EGION I NDEX , 5. M ULTI - SCALE T EXTURE ACUTANCE ,
6. S ATURATION , 7. C OLOR WARMTH , 8. I LLUMINATION , 9.
DYNAMIC R ANGE , 10. OVER E XPOSED RATIO , 11. U NDER
E XPOSED RATIO. To use VIQET, we must set a category of
an image among 4 categories: O UTDOOR DAY - L ANDSCAPE ,
I NDOOR - WALL H ANGING , I NDOOR - A RRANGEMENTS and
O UTDOOR N IGHT - L ANDMARK. Most of the DIV-2K images
SCALE

Figure 6. The MOS results for each stage. The final score, MOS
(Red line), peaked at HR3 . The increase in MOS is similar to
the convergence of the difference, yellow line in Fig.4. Also the
specific measures in other colors tend to be increasing until HR3
and then they saturate or decrease slightly.

4.3. The result images
Figure 5 shows some samples of the output images in
each stage. It shows how the resultant images for each
stages are being improved. All images are down-scaled
with the same resolution as the original, and we can visually confirm that the quality of the images is improved. The
improvement level at each stage is similar to the slope of the
graph in Fig. 4. As stage goes on, it can be observed that the
texture is clearly seen and the result becomes clearer. It can
be said that recurrent learning converges to a certain level of
improvement, since there is little difference between HR3
and HR4 in all the samples.

4.4. Verification using VIQET tool
In order to clearly demonstrate the effectiveness of image
enhancement, we looked for ways to more objectively measure the image quality. Many non-reference quality measure
models exists, but most of them need pre-learning the references and check how close the measured image is from

are bright images taken from the outside and we simply set
all the images to the OUTDOOR DAY category. The detailed
average scores of 100 DIV-2K validation images for each
stage are shown in Table 2.
The key point is that the overall score, MOS value, increases with the stage. The result proves that “The image quality can be increased by recurrent learning”. The
MOS results for each stage are shown in Fig. 6. Based on
the MOS, it can be confirmed that the image quality from
the original 3.825 was improved up to 4.057. The increase
is 0.232 points from the original, which is larger than the
difference between LR and the original, 0.180. It prove
that image enhancement through recurrent learning is efficient. There is a limitation in improving the image quality
based on the difference between HR0 and LR with onetime learning and we have resolved this by providing a new
target obtained through recurrently training the network. As
can be seen in Fig. 6, the MOS increase is largely due to the
improvement of four terms. The first one is the improve-

Table 3. MOS results for various networks. This table shows
that there is an additional improvement in the index after HR1.
It means the application of Recurrent Training Strategy (RTS) is
meaningful regardless of the network. And we can confirm the
proposed network is more suitable for recurrent learning. Images
are available in the supplementary material.
Network
Proposal
Residual1
Residual3
Residual6
Reference
SRCNN
VDSR
SRResNet
EDSR(64×16)

Param

HR0

HR1

HR2

HR3

41K
114K
225K

2.384
2.384
2.384

2.639
2.637
2.644

2.701
2.719
2.725

2.712
2.720
2.744

57K
666K
1.5M
1.7M

2.384
2.384
2.384
2.384

2.603
2.599
2.617
2.632

2.680
2.668
2.720
2.725

2.680
2.669
2.703
2.737

ment of multi-scale edge acutance value. It means the edge
got to be more sharp. The second is the Noise Signature
Index, which means that the distinction between image and
noise is more clear. The third one is multi-scale texture acutance, and it means increment of the level of activity and
detail in the scene. Lastly, illumination means the light has
become more enriched, also indicating that the image has
more detail.

4.5. RTS on various networks
To investigate the characteristics of the proposed RTS
more clearly and to check whether the proposed RTS is
still effective with other algorithms, we test RTS on several other networks as well as the proposed network. RTS
was applied to total six networks and the performances are
shown in Table 3. In the table, the above three networks
are designed based on our proposal shown in Fig. 3 by
changing the number of residual layers in the middle to 1,
3 and 6, respectively. The proposed architecture in Fig. 3
has the advantage of being able to easily compare the number of parameters in the network considering the number
of repetitions. For example, a comparison between learning once with doubled parameters and learning twice with
halved parameters is possible. In the three bottom rows,
we have chosen SRCNN, VSDR, SRResNet and ESDR to
check whether our recurrent training strategy is meaningful
even in a heavier super-resolution network. By this experiment, the characteristics of the proposed RTS on various
networks can be checked from the simplest network, SRCNN, to a latest heavy network, EDSR.
Unfortunately, the existing networks are so heavy that
the GPU memory overflows, making it impossible to keep
the original size of a DIV2K image intact. So we have
to divide each image into 4×2 and processed them independently. The MOS result is the averaged value of 800
pieces of divided 100 validation images. The segmentation
of the image caused a decrease in the content and resolution,

which resulted in a decrease in the MOS score. However,
since this segmentation is applied equally to all the six networks, the tendency is comparable and the experiment is
fair. Table 3 shows the resultant MOS scores as well as the
number of parameters in each network.
In Table 3, the scores of HR2 and HR3 are higher than
HR1 at all times. This suggests that recurrent learning may
lead to additional performance improvement in existing algorithms. In addition, it is shown that the improvement due
from the learning method is more meaningful than the improvement caused by using a different network. It means
that the quality of an image is more dependent on the number of repetitions in RTS than the size of the network, and
as a result, the proposed RTS can produce good results even
with light networks. Although the amount of increase in
MOS is different for each algorithm, the tendency of learning is similar. From this, we can say that applying RTS
makes it possible to use a less expensive network.

5. Conclusion
In this paper, we proposed a new image enhancement
method by a recurrently-trained super-resolution network.
The proposed recurrent learning consists of two phases:
Network Training phase to learn HR form LR, and Target
Update phase to apply the trained network to the original
to produce HR image for the next stage. We clarified the
characteristics of recurrent learning by analyzing the process and results. Using image difference, we can show the
results converge to a specific improvement point without divergence. Also, by using VIQET mean-opinion-score, it
has been numerically shown that the quality of an image
improves clearly as we repeat the learning procedure. It is
the first time to use these objective measures in the imageenhancement area, which is meaningful in that the MOS
score is useful not only in a theory [12] but also in practice.
The DIV2K image dataset used in this paper has an
equivalent image quality to that of an actual digital broadcasting video, which is different from the conventional
datasets with low quality used in the existing superresolution papers. Nonetheless, by RTS, the MOS score
increased more than the difference between the original
and its low-resolution version, which is impossible with the
conventional one-time learning. The increased MOS score
proves that image enhancement through recurrent learning
is efficient.
In order to clarify the effectiveness of the proposed RTS,
we experimented with various networks. The results show
that recurrent learning can make an additional improvement, regardless of the network used. And the results show
a way to obtain a cheap but effective network. We expect
that it will contribute to the related society by dramatically
reducing the cost of super-resolution and image enhancement.

References
[1] E. Agustsson and R. Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) Workshops, July 2017.
[2] A. Buades, B. Coll, and J. . Morel. A non-local algorithm for image denoising. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
(CVPR’05), volume 2, pages 60–65 vol. 2, June 2005.
[3] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever,
and P. Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets.
In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and
R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 2172–2180. Curran Associates, Inc.,
2016.
[4] R. Dahl, M. Norouzi, and J. Shlens. Pixel recursive super
resolution. In The IEEE International Conference on Computer Vision (ICCV), Oct 2017.
[5] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a
deep convolutional network for image super-resolution. In
D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, editors,
Computer Vision – ECCV 2014, pages 184–199, Cham,
2014. Springer International Publishing.
[6] D. Glasner, S. Bagon, and M. Irani. Super-resolution from a
single image. In 2009 IEEE 12th International Conference
on Computer Vision (ICCV), pages 349–356, Los Alamitos,
CA, USA, oct 2009. IEEE Computer Society.
[7] J. Kim, J. Kwon Lee, and K. Mu Lee. Accurate image superresolution using very deep convolutional networks. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
[8] J. Kim, J. Kwon Lee, and K. Mu Lee. Deeply-recursive
convolutional network for image super-resolution. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
[9] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. P. Aitken,
A. Tejani, J. Totz, Z. Wang, and W. Shi. Photo-realistic
single image super-resolution using a generative adversarial
network. CoRR, abs/1609.04802, 2016.
[10] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and
W. Shi. Photo-realistic single image super-resolution using
a generative adversarial network. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July
2017.
[11] B. Lim, S. Son, H. Kim, S. Nah, and K. Mu Lee. Enhanced
deep residual networks for single image super-resolution.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) Workshops, July 2017.
[12] M. Mrak, S. Grgic, and M. Grgic. Picture quality measures in
image compression systems. In The IEEE Region 8 EUROCON 2003. Computer as a Tool., volume 1, pages 233–236
vol.1, Sep. 2003.
[13] A. Polesel, G. Ramponi, and V. J. Mathews. Image enhancement via adaptive unsharp masking. IEEE Transactions on
Image Processing, 9(3):505–510, March 2000.

[14] W. Shi, J. Caballero, F. Huszar, J. Totz, A. P. Aitken,
R. Bishop, D. Rueckert, and Z. Wang. Real-time single image and video super-resolution using an efficient sub-pixel
convolutional neural network. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June
2016.
[15] Y. Tai, J. Yang, and X. Liu. Image super-resolution via
deep recursive residual network. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July
2017.
[16] Y. Tai, J. Yang, X. Liu, and C. Xu. Memnet: A persistent
memory network for image restoration. In The IEEE International Conference on Computer Vision (ICCV), Oct 2017.
[17] T. Tong, G. Li, X. Liu, and Q. Gao. Image super-resolution
using dense skip connections. In The IEEE International
Conference on Computer Vision (ICCV), Oct 2017.
[18] V. Q. E. G. (VQEG). Vqeg image quality evaluation tool
(viqet) version 2.3.117.87, 2016.
[19] J. Yang, J. Wright, T. S. Huang, and Y. Ma. Image superresolution via sparse representation. IEEE Transactions on
Image Processing, 19(11):2861–2873, Nov 2010.
[20] J. Yoo, S.-h. Lee, and N. Kwak. Image restoration by estimating frequency distribution of local patches. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
[21] B. Zhang and J. P. Allebach. Adaptive bilateral filter for
sharpness enhancement and noise removal. IEEE Transactions on Image Processing, 17(5):664–678, May 2008.

