An Integrated Autoencoder-Based Filter for Sparse Big Data
Wei Peng1, Baogui Xin1*
1

College of Economics and Management, Shandong University of Science and Technology, Qingdao
266590, China.
* Correspondence:
Corresponding Author
xin@tju.edu.cn
Keywords: Sparse big data, Integrated autoencoder (IAE), Data sparsity, Prediction model,
Filter
Abstract
We propose a novel filter for sparse big data, called an integrated autoencoder (IAE), which utilizes
auxiliary information to mitigate data sparsity. The proposed model achieves an appropriate balance
between prediction accuracy, convergence speed, and complexity. We implement experiments on a
GPS trajectory dataset, and the results demonstrate that the IAE is more accurate and robust than
some state-of-the-art methods.

1

Introduction

1.1

Motivation

Prediction is essential in many scientific fields, such as economics [1-7], engineering [8-12], and
medicine [13-16]. A valid prediction can facilitate sound decisions. But high sparsity of original data
can substantially decrease prediction performance. Existing prediction algorithms [17-21] are not good
at processing such data. We may leverage auxiliary information to mitigate the sparsity of data, but
these algorithms have difficulty representing complicated features when auxiliary data are unstructured
and heterogeneous. The deep learning-based scheme can extract complex nonlinear representations
from these data, and it usually shows better prediction performance.
Deep learning has led to remarkable achievements in fields such as computer vision [22-25], speech
recognition [26-30], and natural language processing [31, 32], which have attracted much attention in
theory and practice. Many deep learning frameworks have been developed, such as the convolutional
neural network (CNN) [24, 33-35], recurrent neural network (RNN) [36-39], long short-term memory
(LSTM) [1, 32, 40, 41], deep belief network (DBN) [6, 8, 42, 43], AE [9, 44-47] and so on. In some
circumstances, deep learning frameworks deal inefficiently with unstructured and heterogeneous data,
because data preprocessing increases the complexity of the structure.
The AE has low complexity and no data preprocessing requirement. AEs are widely applied in
prediction [2, 9, 11, 48], classification [49-52], and latent representation learning [44, 45, 53, 54].
Various requirements give rise to many variations of AEs. Vincent et al. [44, 45] proposed a denoising
AE (DAE), stack AEs (SAE), and stack denoising AEs (SDAE), which focus on extracting more
abstract representations for a standalone supervised deep learning architecture. Görgel and Simsek [12]

Filter for Sparse Big Data
developed deep stacked denoising sparse autoencoders (DSDSA) for face recognition and the sparsity
property of DSDSA substantially decreased the possibility of overfitting. Lv et al. [55] applied an SAE
to tourism demand prediction using search query data, which has a better performance than some other
state-of-the-art deep learning-based models. The SAE and SDAE are stacked by the AE and DAE and
obtain their optimal values through several optimization processes, which lead to low efficiency. In
brief, the above variations of the AE obtain higher accuracy by increasing complexity and decreasing
efficiency.
1.2

Contributions

Autoencoder (AE), stacked AE (SAE), denoising AE (DAE) and stacked DAE (SDAE) are efficient
prediction approaches with low complexity that has been widely applied in some scientific fields.
However, to obtain a well-trained SAE or SDAE stacked by k AEs or DAEs, we must optimize k  1
objective functions, which require a significant amount of computing resources. By integrating
autoencoders, we develop a novel filter only by optimizing a single objective function. In addition, the
integrated autoencoder (IAE) utilize auxiliary information to mitigate data sparsity to achieve an
appropriate balance among prediction accuracy, convergence speed, and complexity. Experiments
demonstrate that the proposed IAE performs superior than above-mentioned model in accuracy and
efficiency.
1.3

Framework

The remainder of this paper is organized as follows. Section 2 reviews some related prediction methods.
Section 3 describes the IAE in detail. Section 4 discusses experiments and results. Section 5
summarizes conclusions.
2

Related Work

2.1

Non-AE prediction methods

Machine learning-based prediction methods can effectively deal with nonlinear prediction problems.
They consist mainly of singular value decomposition (SVD) [56, 57], matrix factorization (MF) [18,
58], support vector machine (SVM) [59, 60], and Bayesian models [61, 62]. Machine learning-based
prediction can enhance prediction performance to some extent. However, it is hard to obtain a
substantial breakthrough.
Deep learning-based methods are typically composed of numerous neurons and parameters, which can
extract more complicated and nonlinear features from the input data. For example, Krizhevsky et al.
[24] proposed a deep CNN with 60 million parameters and 650,000 neurons to classify 1000 image
categories. Since the dataset they employed contained 1.2 million images, the deep CNN architecture
could prevent overfitting to some extent and performed better than previous deep learning-based
methods. However, it was too complicated to train quickly. Ting et al. [33] proposed a CNN-based
architecture (CNNI-BCC) for breast cancer classification. Compared with previous work, CNNI-BCC
is indeed more accurate. However, it contains 30 hidden layers and each layer contains lots of
parameters, which leads to low efficiency. Chan et al. [63] proposed a simpler unsupervised
convolutional neural network (PCANet) with fewer parameters, which could accelerate training, but
whose performance was significantly degraded when the images were not well prepared. Song et al.
[64] developed a content-based automatic tagging algorithm with five-layer RNNs which performed a
higher training efficiency than above-mentioned algorithms with numerous layers and parameters. But
the model need a preprocessing phase, which also takes too much time. LSTM, as an extension of
2

Filter for Sparse Big Data
RNN, is a more sophisticated but effective structure with a gating mechanism. Baek and Kim [1]
proposed a data-augmentation approach (ModAugNet) consisting of an overfitting prevention LSTM
and a prediction LSTM, which can avoid overfitting by overcoming the deficiency of limited data
availability. Mohamed et al. [65] replaced Gaussian mixture models by DBNs for acoustic modelling
and achieved a better performance. But the architecture contained lots of hidden neurons and
parameters in every layer, which could impact its convergence speed.
2.2

AE and its Variations

Though the above methods can significantly improve prediction performance, most are complicated,
which may cause low efficiency. The AE is a simple but effective deep learning approach that can
learn complex representations with high efficiency.
2.2.1 AE
An AE is an unsupervised representation learning framework with three parts: the input layer, hidden
layer, and output layer, which are defined as
H  f  x   f W  x  b 


(1)

O  g   H   g W   H  b

(2)

where x is the original input; H is the hidden representation; O is the output; f   and g   are
encoder and decoder activation functions, respectively; W and W  are weights; b and b are biases;
  W , b ; and    W , b . Figure 1 describes the architecture of an AE.

Figure 1: Architecture of an AE.

To obtain optimal parameters by minimizing the error between input and output layers, we write the
following loss function of the AE:
eAE = min x  g W   f W  x  b   b 

where 

2

2

(3)

denotes that we consider only the observed data.

The AE can learn latent representations by simply copying input, but the learned representations are
usually too specific to generalize well. In some scenarios, one must enhance the generalization ability
3

Filter for Sparse Big Data
by stacking several AEs, which can learn more representative features than a single AE. Also, for
different demands, one must propose some variations of the AE.
Thus some researchers have combined the AE with other deep learning models to both improve
efficiency and accuracy. AE-based methods perform better with sparse data, such as rating data in
recommender systems. Sedhain et al. [66] proposed an AE-based collaborative model (AutoRec),
which has advantages over existing neural network approaches in representing and computing. Lv et
al. [9] applied an SAE to traffic prediction, which is indispensable to better city planning.
2.2.2 Stacked AE (SAE)
An SAE stacks several typical AEs to attain stronger generalization ability. Its training process includes
pre-training and fine-tuning, as shown in Figure 2.

Figure 2: Training process of SAE.

According to Figure 2, assuming there is an SAE stacked by k AEs, the output of each AE in the pretraining stage is discarded, and each hidden layer is used as the input of the next AE until the k th AE
is well trained. After pre-training each AE, the original input is transformed to the top AE, layer by
layer. The hidden layer of each AE can be calculated by
 g (1) W1 f (1) W1  x  b1   b1  ,
i 1

Hi  
(i )
(i )
 g Wi  f Wi  H i 1  bi   bi , i  2,






,k

(4)

where H i represents the hidden layer of the i th AE; f (i ) and g (i ) respectively represent the encoder and
decoder activation functions of the i th AE; Wi and Wi  respectively represent the encoder and decoder
weight of the i th AE; and bi and bi respectively represent the encoder and decoder bias of the i th AE.
4

Filter for Sparse Big Data
The top hidden layer of the SAE can be taken as input to a standalone supervised learning approach.
For example, to finish classification tasks, one often adds a softmax classification layer connected to
the top hidden layer. In the fine-tuning stage, to obtain the fine-tuned parameters by minimizing the
reconstruction error, the loss function is defined by

eSAE = min  x  xˆ


where

k
eAE

2

  min  e   


k 
min eAE


k 1
AE


 


1
min eAE




(5)

denotes the reconstruction error of the k th AE.

Though the SAE has better generalization ability, its performance can be affected when the original
input is mixed with noise. In some circumstances, an AE needs a denoised input.
2.2.3 Denoising AE (DAE)
A DAE reconstructs a repaired (or “clean”) input from a partially destroyed (or “corrupted”) one. It is
achieved by first partially destroying the raw input x to obtain a corrupted version x via a stochastic
mapping x ~ q  x x  . And then x is mapped by an AE, which includes the encoding process
h  f  x   f Wx  b  and the decoding process x̂  g   y   g W h  b . Figure 3 illustrates the
mechanism of a DAE.
D

Figure 3: Mechanism of a DAE.

Similarly, parameters are optimized by the following loss function, which minimizes the reconstruction
error between the raw input x and output z :
eDAE = min x  g W   f W  x  b   b 

2

(6)

Indeed, it is natural to take into account stacking DAEs to obtain a more powerful AE-based model by
exploiting their respective advantages.
2.2.4 Stacking DAE (SDAE)
Stacking DAEs or typical AEs to build a deep network is the same as stacking RBMs in deep belief
networks. The training process of an SDAE also includes pre-training and fine-tuning. The structure
of an SDAE is shown in Figure 4.

5

Filter for Sparse Big Data

Figure 4: Training process of SDAE.

In the pre-training stage, the output of each DAE is dropped, and each hidden layer is used as the input
of the next DAE until the k th DAE is well trained. Similarly, to finish classification tasks, one often
adds a softmax classification layer as an output layer connected to the top hidden layer. In the finetuning stage, to obtain the fine-tuned parameters by minimizing the reconstruction error, the loss
function is defined by

eSDAE = min  x  xˆ


where

k 
eDAE

2


k 
min eDAE




 min  e   
k 1
DAE



1
min eDAE







(7)

denotes the reconstruction error of the k th DAE.

There are some previous works based on above-mentioned SAE and SDAE. Tong et al. [11] developed
a novel architecture which makes use of SDAEs and ensemble learning (SDAEsTSE) for software
defect prediction. And the SDAEsTSE achieves a good balance between accuracy and efficiency. Bai
et al. [67] proposed a DL-SSAE model which takes into account seasonality to predict PM 2.5
concentration. And the experimental results demonstrate that the DL-SSAE outperforms some stateof-the-art models whether or not to consider seasonality. To sum up, the AE is an efficient prediction
approach with low complexity that has been widely applied in some scientific fields.
2.2.5 Summary
There is no doubt that SAE and SDAE can improve prediction accuracy. However, to obtain a welltrained SAE or SDAE stacked by k AEs or DAEs, we must optimize k  1 objective functions, which
will consume a lot of computing resources. Therefore, it is urgent to develop a new variation of AE to
6

Filter for Sparse Big Data
overcome the shortcoming caused by multiply objective functions. In the next Section, we will develop
a novel filter only by optimizing a single objective function.
3

Integrated autoencoder (IAE)

In this section, we introduce a novel filter with a single objective function for sparse big data, called
IAE, which utilizes auxiliary information to mitigate data sparsity. The IAE model joins two AEs to
enhance both accuracy and efficiency. The structure of the IAE is depicted in Figure 5.

Figure 5: Structure of the proposed IAE model.

Assume that there is a matrix V  mn that must be predicted, and an auxiliary information matrix
A  mm that can be employed to enhance the prediction accuracy of V .
At the first stage, we feed A as the input of the first AE, which will perform representation learning
from A and output a reduction version A .



A  g (1) W1  f (1) W1  A  b1   b1



(8)

At the second stage, we concatenate V and A horizontally to generate a new matrix V , A which is
fed as the input of the second AE. Similarly, the second AE performs representation learning from
ˆ  . We take the first n columns of Vˆ , Aˆ  as the predicted
V , A  and derives a predicted matrix Vˆ , A




matrix Vˆ .
Also, we leverage the back-propagation algorithm to minimize the reconstruction error between V and
Vˆ at the second stage. To prevent overfitting, we add the regularization term to the following loss
function:







eJAE  min V , A  g (2) W2  f (2) W2  V , A  b2  b2



2




2

W

2
2 F

 W2

2
F



(9)

where  is a hyper parameter.
7

Filter for Sparse Big Data
and Vˆ , Aˆ  , while the
first AE does not perform the error minimization between A and A . What is more, there are only two
hidden layers in an IAE, and it needs no pre-processing. So, the IAE must consume less running time
than the SAE, DAE, or SDAE.
In the IAE, the second AE implements the error minimization between

4

Experiments

4.1

Experimental setup

V , A 

4.1.1 Datasets
We conducted experiments on the GPS trajectory dataset, which contains real-time traffic conditions
and POI features of Beijing. Our purpose is to take POI (point of interest) features as auxiliary
information to predict the traffic conditions vr ,t and sdvr ,t , which respectively denote the average travel
speed and standard deviation of the travel speed of all of the vehicles traversing road segment r in the
time slot t .
The GPS trajectory dataset also has been employed [68]; this is a context-aware matrix factorization
method to predict gas consumption and pollution emissions via the prediction of v and sdv . Moreover,
it can be widely used for urban computing scenarios [69].
To verify the performance of the IAE, we randomly divided the experimental dataset into two parts, a
training dataset Dtrain and testing dataset Dtest , accounting for  % and 1   % , respectively.
4.1.2 Evaluation Index
Root mean square error (RMSE) is a widely used evaluation index for recommender systems or
continuous type prediction. By calculating the RMSE between observed values and predicted values
on Dtest , we can test the accuracy of the IAE. A smaller RMSE value represents better performance. It
is defined as

RMSE 

 v

r ,t

( r , t )Dtest

 vˆr ,t



2

Dtest

where vˆr ,t represents the predicted value of vr ,t , and



(10)
represents the number of elements in a set.

The efficiency is measured by the average time At consumed in each epoch of the program:
At 

Tt
nepochs

(11)
where Tt denotes the total time consumed in the program, and nepochs denotes the number of epochs in
the program.
4.1.3 Experimental Implementation and Parameter Settings
We performed all experiments in Python on a PC with a 2.00 GHz CPU, 16 GB memory, and an
NVIDIA GTX 1080Ti-11G GPU.
8

Filter for Sparse Big Data
We adopted the Adam optimizer to minimize the reconstruction error and set some important
parameters, as shown in Table 1.
Table 1: Parameter settings.
Parameters

v

sdv

Hyper-parameter

 =0.01

 =0.01

Training ratio

  80

  80

Training epochs

n  100

n  100

AE

Number of neurons in hidden layer

1,000

1,000

SAE

Number of neurons in first hidden layer

1,000

1,000

Number of neurons in second hidden layer

3,000

3,000

DAE

Number of neurons in hidden layer

1,000

1,000

SDAE

Number of neurons in first hidden layer

1,000

1,000

Number of neurons in second hidden layer

3,000

3,000

Number of neurons in first hidden layer

1,000

1,000

Number of neurons in second hidden layer

3,000

3,000

Common parameters

IAE

4.2

Experimental Results

In this section, we set the training epochs of different AE-based algorithms to 100, and obtained the
corresponding results, as shown in Table 2.
Table 2: Performance Comparison of Algorithms.
Algorithm
AE

SAE

DAE

SDAE

IAE

RMSE and At

v

sdv

RMSE

3.968

1.374

At

4.18 s

3.92 s

RMSE

4.294

1.511

At

33.52 s

35.75 s

RMSE

4.054

1.444

At

4.14 s

4.77 s

RMSE

4.299

1.508

At

29.95 s

34.58 s

RMSE

2.079

0.455

At

23.83 s

21.44 s

Table 2 shows that the accuracy of the IAE exceeds that of other AE-based models. The efficiency of
the IAE is higher than that of the SAE and SDAE. It is inevitable that the speed of the AE and DAE is
higher than that of the IAE, because the AE and DAE consist of a single hidden layer while the SAE,
SDAE, and IAE consist of at least two hidden layers (in this paper, we consider only the SAE, SDAE,
and IAE with two hidden layers). Moreover, the SAE, DAE, and SDAE focus on extracting more
9

Filter for Sparse Big Data
abstract representations for a standalone supervised deep learning architecture. Therefore, it is
justifiable that the RMSEs of the SAE, DAE, and SDAE are all higher than that of the AE.
Figures 6 and 7 show the trend that the accuracy of different algorithms varies with the increase of
training epochs. From Figure 6, we note that the RMSEs of the IAE continue decreasing between about
epoch 28 and epoch 76, while those of the AE, DAE, SAE, and SDAE decrease at the first several
epochs but later fluctuate in a broad range. From Figure 7, we find that the RMSEs of the IAE continue
to decrease with increased training epochs, while those of the AE, DAE, SAE, and SDAE continue
fluctuating and do not show a significant downward trend. It is clear that the IAE is more accurate and
robust than the AE, DAE, SAE, and SDAE.

Figure 6: RMSE of v .

Figure 7: RMSE of

sdv .
10

Filter for Sparse Big Data
5

Conclusions

In this paper, we built a novel filter only with a single objective function, named IAE, which utilizes
auxiliary information to mitigate data sparsity. The IAE achieves an appropriate balance among
prediction accuracy, convergence speed, and complexity. We regard the IAE as a deep learning
structure that utilizes auxiliary information for prediction. Traffic prediction, as mentioned above, is
an example that demonstrates the IAE’s validity. The IAE can be used to efficiently predict big data
with auxiliary information in almost all fields [70, 71].
Conflict of Interest
The authors declare that the research was conducted in the absence of any commercial or financial
relationships that could be construed as a potential conflict of interest.
Author Contributions
BX: conceptualization. BX and WP: methodology. BX and WP: software. BX and WP: validation. BX:
formal analysis. WP: writing-original draft preparation. BX and WP: writing-review and editing. WP:
visualization. BX: supervision, project administration, and funding acquisition.
Funding
The work was supported by the National Social Science Foundation of China [No. 16FJY008]; the
National Natural Science Foundation of China [No. 11801060]; and the Natural Science Foundation
of Shandong Province [No. ZR2016FM26].
Acknowledgments
The authors would like to express sincere gratitude to the referees for their valuable suggestions and
comments.
Data Availability
The GPS trajectory dataset used to support the findings of this study are available from the
corresponding author upon request, or to directly download from the following website:
https://onedrive.live.com/?authkey=%21ADgmvTgfqs4hn4Q&id=CF159105855090C5%211438&ci
d=CF159105855090C5.
Reference
1.

Baek, Y. and H.Y. Kim, ModAugNet: A new forecasting framework for stock market index
value with an overfitting prevention LSTM module and a prediction LSTM module. Expert
Systems with Applications, 2018. 113: p. 457-480.

2.

Wang, K., et al., SDDRS: Stacked Discriminative Denoising Auto-Encoder based
Recommender System. Cognitive Systems Research, 2019. 55: p. 164-174.

3.

Tsantekidis, A., et al. Forecasting stock prices from the limit order book using convolutional
neural networks. in Business Informatics (CBI), 2017 IEEE 19th Conference on. 2017. IEEE.

11

Filter for Sparse Big Data
4.

Fischer, T. and C. Krauss, Deep learning with long short-term memory networks for financial
market predictions. European Journal of Operational Research, 2018. 270(2): p. 654-669.

5.

Chai, J., et al., A Decomposition–Integration Model with Dynamic Fuzzy Reconstruction for
Crude Oil Price Prediction and The Implications for Sustainable Development. Journal of
Cleaner Production, 2019.

6.

Shen, F., J. Chao, and J. Zhao, Forecasting exchange rate using deep belief networks and
conjugate gradient method. Neurocomputing, 2015. 167: p. 243-253.

7.

McGurk, Z., US Real Estate Inflation Prediction: Exchange Rates and Net Foreign Assets.
The Quarterly Review of Economics and Finance, 2019.

8.

Wang, K., et al., Deep belief network based k-means cluster approach for short-term wind
power forecasting. Energy, 2018. 165: p. 840-852.

9.

Lv, Y., et al., Traffic Flow Prediction With Big Data: A Deep Learning Approach. IEEE
Transactions on Intelligent Transportation Systems, 2015. 16(2): p. 865-873.

10.

Xu, F., W.t.P. Tse, and Y.L. Tse, Roller bearing fault diagnosis using stacked denoising
autoencoder in deep learning and Gath–Geva clustering algorithm without principal
component analysis and data label. Applied Soft Computing, 2018. 73: p. 898-913.

11.

Tong, H., B. Liu, and S. Wang, Software defect prediction using stacked denoising
autoencoders and two-stage ensemble learning. Information and Software Technology, 2018.
96: p. 94-111.

12.

Görgel, P. and A. Simsek, Face recognition via Deep Stacked Denoising Sparse
Autoencoders (DSDSA). Applied Mathematics and Computation, 2019. 355: p. 325-342.

13.

Alipanahi, B., et al., Predicting the sequence specificities of DNA-and RNA-binding proteins
by deep learning. Nature biotechnology, 2015. 33(8): p. 831.

14.

Abraham, B. and M.S. Nair, Computer-aided diagnosis of clinically significant prostate
cancer from MRI images using sparse autoencoder and random forest classifier.
Biocybernetics and Biomedical Engineering, 2018. 38(3): p. 733-744.

15.

Chen, M., et al., Disease prediction by machine learning over big data from healthcare
communities. IEEE Access, 2017. 5: p. 8869-8879.

16.

Deng, Y., et al., Towards automatic encoding of medical procedures using convolutional
neural networks and autoencoders. Artificial Intelligence in Medicine, 2019. 93: p. 29-42.

17.

Urso, A., et al., Data Mining: Prediction Methods, in Encyclopedia of Bioinformatics and
Computational Biology, S. Ranganathan, et al., Editors. 2019, Academic Press: Oxford. p.
413-430.

18.

Koren, Y., R. Bell, and C. Volinsky, Matrix factorization techniques for recommender
systems. Computer, 2009(8): p. 30-37.

19.

Hrasko, R., A.G.C. Pacheco, and R.A. Krohling, Time Series Prediction Using Restricted
Boltzmann Machines and Backpropagation. Procedia Computer Science, 2015. 55: p. 990999.

20.

Cong, Y., J. Wang, and X. Li, Traffic Flow Forecasting by a Least Squares Support Vector
Machine with a Fruit Fly Optimization Algorithm. Procedia Engineering, 2016. 137: p. 59-68.

12

Filter for Sparse Big Data
21.

Tang, J., et al., Traffic flow prediction based on combination of support vector machine and
data denoising schemes. Physica A: Statistical Mechanics and its Applications, 2019.

22.

Mathews, S.M., C. Kambhamettu, and K.E. Barner, A novel application of deep learning for
single-lead ECG classification. Computers in Biology and Medicine, 2018. 99: p. 53-62.

23.

Elmahmudi, A. and H. Ugail, Deep face recognition using imperfect facial data. Future
Generation Computer Systems, 2019. 99: p. 213-225.

24.

Krizhevsky, A., I. Sutskever, and G.E. Hinton. ImageNet Classification with Deep
Convolutional Neural Networks. in International Conference on Neural Information
Processing Systems. 2012.

25.

Beyaz, A., et al., Olive fly sting detection based on computer vision. Postharvest Biology and
Technology, 2019. 150: p. 129-136.

26.

Hinton, G., et al., Deep neural networks for acoustic modeling in speech recognition: The
shared views of four research groups. IEEE Signal processing magazine, 2012. 29(6): p. 8297.

27.

Fayek, H.M., M. Lech, and L. Cavedon, Evaluating deep learning architectures for Speech
Emotion Recognition. Neural Networks, 2017. 92: p. 60-68.

28.

Grozdić, Đ.T., S.T. Jovičić, and M. Subotić, Whispered speech recognition using deep
denoising autoencoder. Engineering Applications of Artificial Intelligence, 2017. 59: p. 1522.

29.

Tu, Y.-H., et al., An iterative mask estimation approach to deep learning based multi-channel
speech recognition. Speech Communication, 2019. 106: p. 31-43.

30.

Zhao, J., X. Mao, and L. Chen, Speech emotion recognition using deep 1D & 2D CNN LSTM
networks. Biomedical Signal Processing and Control, 2019. 47: p. 312-323.

31.

Araque, O., et al., Enhancing deep learning sentiment analysis with ensemble techniques in
social applications. Expert Systems with Applications, 2017. 77: p. 236-246.

32.

Wang, J., B. Peng, and X. Zhang, Using a stacked residual LSTM model for sentiment
intensity prediction. Neurocomputing, 2018. 322: p. 93-101.

33.

Ting, F.F., Y.J. Tan, and K.S. Sim, Convolutional neural network improvement for breast
cancer classification. Expert Systems with Applications, 2019. 120: p. 103-115.

34.

Abid, F., et al., Sentiment analysis through recurrent variants latterly on convolutional neural
network of Twitter. Future Generation Computer Systems, 2019. 95: p. 292-308.

35.

Steinbrener, J., K. Posch, and R. Leitner, Hyperspectral fruit and vegetable classification
using convolutional neural networks. Computers and Electronics in Agriculture, 2019. 162: p.
364-372.

36.

Liu, Z. and C.J. Sullivan, Prediction of weather induced background radiation fluctuation
with recurrent neural networks. Radiation Physics and Chemistry, 2019. 155: p. 275-280.

37.

Pan, Y., et al., Predicting bike sharing demand using recurrent neural networks. Procedia
Computer Science, 2019. 147: p. 562-566.

38.

Zagrebina, S.A., V.G. Mokhov, and V.I. Tsimbol, Electrical Energy Consumption Prediction
is based on the Recurrent Neural Network. Procedia Computer Science, 2019. 150: p. 340346.
13

Filter for Sparse Big Data
39.

Naghibi, Z., S.A. Sadrossadat, and S. Safari, Time-domain modeling of nonlinear circuits
using deep recurrent neural network technique. AEU - International Journal of Electronics
and Communications, 2019. 100: p. 66-74.

40.

Lei, J., C. Liu, and D. Jiang, Fault diagnosis of wind turbine based on Long Short-term
memory networks. Renewable Energy, 2019. 133: p. 422-432.

41.

Cen, Z. and J. Wang, Crude oil price prediction model with long short term memory deep
learning based on prior knowledge data transfer. Energy, 2019. 169: p. 160-171.

42.

Hassan, M.M., et al., Human emotion recognition using deep belief network architecture.
Information Fusion, 2019. 51: p. 10-18.

43.

Chen, G., et al., Predict effective drug combination by deep belief network and ontology
fingerprints. Journal of Biomedical Informatics, 2018. 85: p. 149-154.

44.

Vincent, P., et al. Extracting and composing robust features with denoising autoencoders. in
International Conference on Machine Learning. 2008.

45.

Vincent, P., et al., Stacked Denoising Autoencoders: Learning Useful Representations in a
Deep Network with a Local Denoising Criterion. Journal of Machine Learning Research,
2010. 11(12): p. 3371-3408.

46.

Jia, F., et al., A neural network constructed by deep learning technique and its application to
intelligent fault diagnosis of machines. Neurocomputing, 2018. 272: p. 619-628.

47.

Bao, W., J. Yue, and Y. Rao, A deep learning framework for financial time series using
stacked autoencoders and long-short term memory. PloS one, 2017. 12(7): p. e0180944.

48.

Mahdi, M. and V.M.I. Genc, Post-fault prediction of transient instabilities using stacked
sparse autoencoder. Electric Power Systems Research, 2018. 164: p. 243-252.

49.

Ng, W.W.Y., et al., Dual autoencoders features for imbalance classification problem. Pattern
Recognition, 2016. 60: p. 875-889.

50.

Kannadasan, K., D.R. Edla, and V. Kuppili, Type 2 diabetes data classification using stacked
autoencoders in deep neural networks. Clinical Epidemiology and Global Health, 2018.

51.

Fu, X., et al., Semi-supervised Aspect-level Sentiment Classification Model based on
Variational Autoencoder. Knowledge-Based Systems, 2019. 171: p. 81-92.

52.

Zhang, L., et al., PolSAR image classification based on multi-scale stacked sparse
autoencoder. Neurocomputing, 2019.

53.

Ohno, H., Linear guided autoencoder: Representation learning with linearity. Applied Soft
Computing, 2017. 55: p. 566-575.

54.

Zhuang, F., et al., Representation learning via Dual-Autoencoder for recommendation.
Neural Networks, 2017. 90: p. 83-89.

55.

Lv, S.-X., L. Peng, and L. Wang, Stacked autoencoder with echo-state regression for tourism
demand forecasting using search query data. Applied Soft Computing, 2018. 73: p. 119-133.

56.

Tănăsescu, A. and P.G. Popescu, A fast singular value decomposition algorithm of general ktridiagonal matrices. Journal of Computational Science, 2019. 31: p. 1-5.

57.

Busu, C. and M. Busu, Modelling the predictive power of the singular value decomposition
based-entropy. Empirical evidence from New York Stock Exchange. Physica A: Statistical
Mechanics and its Applications, 2019.
14

Filter for Sparse Big Data
58.

Belohlavek, R., J. Outrata, and M. Trnecka, Toward quality assessment of Boolean matrix
factorizations. Information Sciences, 2018. 459: p. 71-85.

59.

Xu, Z., et al., A regression-type support vector machine for k-class problem.
Neurocomputing, 2019. 340: p. 1-7.

60.

Karimi, F., et al., An enhanced support vector machine model for urban expansion prediction.
Computers, Environment and Urban Systems, 2019. 75: p. 61-75.

61.

Mei, L., et al., Dynamic risk assessment in healthcare based on Bayesian approach.
Reliability Engineering & System Safety, 2019.

62.

Frazier, D.T., et al., Approximate Bayesian forecasting. International Journal of Forecasting,
2019. 35(2): p. 521-539.

63.

Chan, T., et al., PCANet: A Simple Deep Learning Baseline for Image Classification? IEEE
Transactions on Image Processing, 2015. 24(12): p. 5017-5032.

64.

Song, G., et al., Music auto-tagging using deep Recurrent Neural Networks. Neurocomputing,
2018. 292: p. 104-110.

65.

Mohamed, A., G.E. Dahl, and G. Hinton, Acoustic Modeling Using Deep Belief Networks.
IEEE Transactions on Audio, Speech, and Language Processing, 2012. 20(1): p. 14-22.

66.

Sedhain, S., et al. AutoRec: Autoencoders Meet Collaborative Filtering. in International
Conference on World Wide Web. 2015.

67.

Bai, Y., et al., Hourly PM2.5 concentration forecast using stacked autoencoder model with
emphasis on seasonality. Journal of Cleaner Production, 2019. 224: p. 739-750.

68.

Shang, J., et al., Inferring gas consumption and pollution emission of vehicles throughout a
city. 2014: p. 1027-1036.

69.

Zheng, Y., et al., Urban computing: concepts, methodologies, and applications. ACM
Transactions on Intelligent Systems and Technology (TIST), 2014. 5(3): p. 38.

70.

Qi, L., et al., Time-aware distributed service recommendation with privacy-preservation.
Information Sciences, 2019. 480: p. 354-364.

71.

Shuang, W. and H. Ya-Ru, Deep Learning Network for Multiuser Detection in Satellite
Mobile Communication System. Computational Intelligence and Neuroscience, 2019. 2019.

15

