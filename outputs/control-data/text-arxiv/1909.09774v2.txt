LULC classification methodology based on simple Convolutional Neural Network
to map complex urban forms at finer scale: Evidence from Mumbai
Deepank Verma1
Centre for Urban Science and Engineering
Indian Institute of Technology Bombay, Mumbai, India
Email: deepank@iitb.ac.in
Arnab Jana
Centre for Urban Science and Engineering
Indian Institute of Technology Bombay, Mumbai, India
Email: arnab.jana@iitb.ac.in
Abstract. The satellite imagery classification task is fundamental to spatial knowledge discovery. Several image
classification methods are used to create standardized Land use and Land cover (LULC) maps, which facilitate
research on spatial and ecological processes and human activities. Local Climate Zones (LCZ) classification maps
are an example of standardized maps which have been widely used to demarcate the homogeneity in built and
natural character in the cities. The LCZ classification scheme is primarily focused on urban climate-related
research, in which 17 climate zones are mapped in a city area with the 100-150m spatial resolution. Each zone
exhibits physical properties related to urban form and functions essential for thermal behavior studies. Extending
this widely adopted approach to create LULC maps at finer resolution using the LCZ mapping scheme would
benefit the allied domains of urban planning, transportation, and water resources management. This study
proposes a novel solution to generate classification maps with a 10-band Sentinel-2B dataset and Convolutional
Neural Networks (CNN) at the 10m spatial resolution. The classification benefits from CNN’s property to preserve
local structures in the image datasets. The proposed CNN model outperforms traditional machine learning models
such as Artificial Neural Network, Random Forests, and Support Vector Machines. The overall accuracy and
kappa statistic of the CNN model trained on 14 urban and natural classes are 82 percent and 0.81, respectively.
The study also discusses the utility of the model for specialized remote sensing tasks such as change detection,
identification of slum settlements, and mapping pervious/ impervious layers in urban settlements with higher
accuracy.
Keywords: Land use Land Cover; Urban form; LCZ scheme; Machine Learning; Convolutional Neural Networks.

1. Introduction
The LULC classification aims to achieve uniform categorization of landforms at various scales
[1]. Such classification is an essential component in the creation of standardized maps which
help in decision- and plan-making processes. LULC maps have been used to estimate
agricultural production [2]–[4] , study urban change detection [5]–[7], climate [8], [9],
biodiversity [10], [11] and to map natural hazards [12], [13]. Earth observation (EO) datasets,
which have been made available by different space agencies, have fostered advancement in
image classification methods. Parametric techniques such as Maximum Likelihood (ML), and
non-parametric techniques such as Support Vector Machines (SVM), Random Forests (RF),
and Artificial Neural Networks (ANN) have been extensively experimented with per-pixel
image classification tasks [14]–[16]. Such widely adopted classification techniques have been
challenged by spatial-spectral classification methods such as Object-Based image analysis
1

Corresponding Author

1

(OBIA) which have shown significant improvement in classification accuracy [17]. However,
the OBIA-based research has been restricted to the Very High/High resolution (VHR/HR)
image datasets, which have limited availability to researchers. Among various EO datasets, the
Landsat dataset series (Landsat 1-8) have been widely used in classification tasks. Due to the
coarser spatial resolution of imagery products (Landsat 8: 15m (Pan) - 30m – 100m (Thermal
bands)), these datasets have been widely utilized to study mainly regional characteristics with
broad land cover classes such as built, croplands, water bodies, forests, and fallow lands [16].
In recent years, Landsat along with Sentinel products have been utilized to study intra-urban
characteristics with improved classification techniques [18]–[20].
One of the prerequisite components in any LULC classification study is the selection of a
classification system. The classification system is usually designed to cover the user’s
requirement, availability of reference samples and classification algorithms, and
reproducibility at various scales [21]. Some of the common land cover classification system
used by organizations such as United Nations Educational Scientific and Cultural Organization
(UNESCO) [22], Food and Agriculture Organization (FAO) [23], Federal Geographic Data
Committee (FGDC) [24], Commission of the European Communities (CEC) [25] and other
LULC classification systems such as Urban Atlas [26], National Remote Sensing Agency
(NRSA) [27] are utilized for large scale image classification for land monitoring and decision
making. Such schemes utilize different EO datasets for assessment of mainly vegetation cover
and broad land cover classifications. As most of the land cover schemes are applicable at lower
resolution satellite imagery, hence the urban features are represented as aggregated classes such
as a “built cover” or “continuous/discontinuous urban fabric”. Anderson [1] presented a
hierarchical LULC classification system at various levels of assessments which can be prepared
with the help of different EO datasets along with the Land use maps prepared through ground
surveys. Similar classification systems [28] have been in practice for urban development and
management. Some of the major classes used in such classifications are Residential,
Commercial, Recreation, Transportation, and Industrial which are further subdivided into
detailed sub-classes based on the arrangement of built structures, density, and functions.
However, the associated definitions and rulesets to define such classes lies with the governing
authority of the area in question, hence generalization of such classification schemes has not
been possible. Due to the lack of strict class boundaries and definitions with majority of
classification systems and the unavailability of a well-documented approach to generate LULC
maps, researchers have considered individual-requirement based classes to identify intra-urban
features. Studies included methods such as pattern recognition to identify spatial patterns [29],
delineation of impervious surfaces built types using morphological features [18], [30], [31],
and creation of land use maps using various EO datasets [19], [32]. The study of underlying
spatial form and function is an important area of research in urban planning, design, and
development. However, traditional approaches to prepare classification maps have been based
on inconsistent and limited observations due to a lack of a standardized approach and
classification system. This study utilizes the Local Climate Zone (LCZ) classification system
which consists of well-delineated intra-urban classes and large documentation explaining the
classification process for reproducibility and universal applicability.

2

The LCZ classification system has been widely used in urban climate studies to classify intraurban built and natural features and to facilitate recognition of urban morphology and built
arrangement. In short, LCZ is LULC for urban climate studies [33]. The LCZ classification
scheme consists of 17 classes, of which 10 classes define urban character and seven natural
character. LCZ classes categorize the packing of roughness features of built structures,
openness and vegetation to study permeability and urban geometry. The LCZ classification is
a method to create a global database of urban form and function in different cities for urban
climate studies [9]. The maps are utilized by scientists studying heat island effects in various
urban neighborhoods [34], [35]. The existing approaches to LCZ classification include (a)
imagery-based, in which the EO datasets along with the training samples created by local
experts are used to prepare maps with the help of various classification algorithms [36], [37],
and (b) GIS-based approaches, which include decision and rulemaking through GIS maps
comprising of geographical objects such as building footprints, heights, and roads to create
LCZ maps [38], [39].
The preparation of imagery-based classification maps is done through World Urban Database
and Access Portal Tools (WUDAPT) [40]. The method utilizes open source GIS plugins to
execute classification algorithms. It requires help from local experts to create training samples
of the selected city. Landsat data with RF classifier is used for image classification in open
source SAGA GIS software. In addition to the detailed step-by-step methodology [40] for the
creation of such maps, various studies have experimented with additional datasets and methods
which include transferability of training samples to other cities [41], the usage of ASTER data
along with Landsat data [42], and Sentinel-1 SAR data with multispectral Landsat data [43],
[44].
Traditional image classification tasks utilize the pixel-based classification methods, which do
not take into account the spatial information of neighboring pixels. LCZ classes comprise of
built and natural roughness features which, with context-aware pixel-based classification
techniques, may produce an improved representation of the physical characteristics of an area
[45]. Contextual classification approaches utilize neighboring pixels to solve the problem of
intraclass spectral variations [21]. However, the majority of commonly used classification
techniques, including LCZ, still utilize only spectral variables for image classification tasks
[46]. While contextual classification methods [47]–[49] have shown significant improvement
in accuracy in hyperspectral datasets, the studies showing replication of the methods in EO
datasets such as Landsat and Sentinel have been scarce. In recent years, Convolutional Neural
Networks (CNN) have shown significant advancements in feature detection and image
classification tasks. CNN models are useful in learning the representation of spatial and spectral
variations in the satellite dataset. While the majority of the practical use cases of CNN based
models are based on image and video analysis [50], some studies have modified and utilized
this concept in remote sensing domain [51]–[53]. Similar to OBIA, the studies have been
mostly focused on VHR/MR imagery types. LULC classification with the help of CNN has not
been tested with Medium Resolution satellite imagery. The only exception is hyperspectral
imagery where CNN models have outperformed traditional techniques in image classification
[54]–[56].

3

Figure 1: Flowchart showing methodology used in the study.

This study proposes a CNN based classifier for intra-urban built and natural areas classification
at 10m spatial resolution. It utilizes 10-band Sentinel 2B satellite imagery to create training and
testing datasets. This study further compares the classification results with other machine
learning algorithms such as Artificial Neural Network (ANN), Random Forests (RF) and
Support Vector Machines (SVM). These machine learning algorithms are modified to integrate
spatial information post-classification for suitable comparison. Altogether, this study proposes
a methodology to prepare the fine resolution classification map based on the LCZ classification
scheme (Fig. 1). The created approach takes into account the spatial-spectral information to
map complex urban features at higher details, which otherwise are difficult to classify with
traditional classification techniques.
2. Selection of Area and dataset
The city of Mumbai is chosen to demonstrate the results of the created methodology. We
collected the sentinel 2B dataset dated 15th March 2018 for this study. The Sentinel dataset
comprises 13 spectral bands with varying spatial resolution (10, 20 and 60m). The dataset does
not provide panchromatic band, which is commonly used to enhance the spatial resolution of
bands with coarser resolution. With the help of 4 fine (10m) resolution bands (2, 3, 4 and 8),
we pansharpened the remaining 20m resolution bands (5, 6, 7, 8A, 11 and 12). The process of
pansharpening was followed from various studies [57], [58]. The resulting sentinel imagery
consists of 10-bands each of 10m spatial resolution. Sentinel image dataset is represented in
16-bit data format, for which the normalization of data is done by dividing each Digital Number
(DN) of the pixel with the maximum DN value of the dataset.

4

Table 1: Built and land cover types considered in the study. Adapted from [33].

Built and
Land Cover
types

LCZ 2

LCZ 3

LCZ 4

Compact
mid-rise

Compact
low-rise

Open High
Rise

LCZ 5

Open midrise

LCZ 8

Large lowrise

LCZ 9

Sparsely
built

LCZ 10

Heavy
Industry

Built and
Land
Cover
types

Definitions

Dense
Trees

Heavily wooded
landscape of
deciduous and/or
evergreen trees.

Scattered
Trees

Lightly wooded
landscape of
deciduous and/or
evergreen trees.

Bush,
scrub

Open arrangement
of bushes, shrubs
and short woody
trees.

LCZ D

Low
Plants

Featureless
landscape of grass
and herbaceous
plants/crops.

LCZ E

Bare rock
or paved

Featureless
landscape of rock
or paved cover.

LCZ F

Bare soil
or sand

Featureless
landscape of soil
or sand cover.

Water

Large open water
bodies such as seas
and lakes, or small

Definitions
Dense mix of
buildings with 3-9
stories. Land cover
is mostly
impervious.
Dense mix of
buildings with 1-3
stories. Land cover
is hard-packed and
impervious.
Open arrangement
of tall buildings.
Combination of
arrangement with
impervious and
pervious land
cover.
Open arrangement
of buildings with
3-9 stories.
Abundance of
pervious land
cover.
Open arrangement
of buildings with
1-3 stories.
Impervious land
cover.
Sparse
arrangement of
small or medium
sized buildings in a
natural setting.
Pervious land
cover.
Low-rise and midrise industrial
structures (tower,

LCZ A

LCZ B

LCZ C

LCZ G

5

tanks, stacks).
Impervious land
cover.

bodies such as
rivers, reservoirs,
and lagoons.

By visual inspection of the city of Mumbai through high-resolution Google satellite imagery,
many observations can be made. The city predominantly comprises of compact low rise (LCZ
3), and open mid-rise (LCZ 5) built arrangements in the form of squatter settlements and
housing colonies respectively. High-rise buildings (LCZ 4), on the other hand, have scattered
presence throughout the city. Most of the high-rise buildings are part of small townships
surrounded by other natural and built classes making the distinction between such built
structures difficult. Compact mid-rise (LCZ 2) settlements are dominated by Victorian-era
built structures which are apparent in the southern part of the city. Mumbai city also consists
of a considerable percentage of land cover under sparsely built (LCZ 9) dominated by academic
and administrative institutions. LCZ 3 and Lightweight Low Rise (discussed in [33] as LCZ 7)
classes differ in construction materials, the proper delineation between the two cannot be made
through visual observation. Hence, these classes are collectively studied as LCZ 3. The
presence of the rest of the classes (discussed in [33] as LCZ 1 and LCZ 6) is scarce and hence
not considered in the study.
The natural classes in the city can be distinctly identified from the satellite map. The mangroves
in the coastal areas of the city represent dense trees class (LCZ A), whereas the city forests and
the national park in the city’s northwestern part are categorized as scattered trees (LCZ B).
Land cover comprising small shrubs and trees or plants between the built form and the parks
or gardens can be classified as bush and scrub (LCZ C). Impervious surfaces such as roads,
runways, and dockyards are identified as a paved class (LCZ E). The low plants (LCZ D)
category includes parks, stadiums, playgrounds which have green vegetation cover. Bare soil
or sand (LCZ F) class includes the sand in the beaches along the coastline, cleared forests or
vegetation for built purposes, and salt pans. A total of 14 prominent classes in the city of
Mumbai (Table. 1) are considered and samples are created with the help of the Google Earth
platform.
The proposed methodology utilizes a 10-band Sentinel dataset. Similar to the image-based
WUDAPT approach, training samples are collected with the help of Google Earth Pro software.
However, instead of the creation of polygons, point-based samples are created [59]. For better
generalization, all the areas of the city are considered for creating samples for each of the
classes (Fig. 2). In this study, approximately 3500 points belonging to 14 classes are created
which are further randomly split into the train, validation, and test sets in the proportion of
5:2:3. Patches of size 11x11x10 are extracted from point-based samples by creating a buffer of
5 pixels around each sample. Each patch is labeled using the central pixel of the patch.

Table 2: Point-based samples created for each class. Further divided into Train, Val and Test set in the ratio of
5:2:3.

Classes

Total Train Val Test

6

LCZ 2
LCZ 3
LCZ 4
LCZ 5
LCZ 8
LCZ 9
LCZ 10
LCZ A
LCZ B
LCZ C
LCZ D
LCZ E
LCZ F
LCZ G
Total

253
498
321
237
180
159
207
212
258
198
253
207
416
178

116
242
148
115
80
92
102
100
124
92
116
107
232
86

65
118
77
41
38
21
46
41
56
35
72
36
75
30

72
138
96
81
62
46
59
71
78
71
65
64
109
62

3577 1752 751 1074

7

Figure 2: Map showing locations of selected point-samples.

Point-based training samples creation is more effective than the creation of polygons for two
reasons. Firstly, in cities with spatial heterogeneity such as Mumbai, the majority of the
considered LCZ classes coexist or overlap in a random fashion. Delineating such areas by
polygon based training samples is difficult. For example, the scattered distribution of high rise
clusters (LCZ 4) is predominant in the city; if drawn as polygons, this would result in smaller
and unusable training samples. With the discussed approach, the buffer of fixed size is
automatically generated over the samples which encompass the spatial neighborhood of the
fixed size, thereby reducing the need for manually drawn polygons. Moreover, for further
evaluation, the size of the neighborhood can be modified to select the good fit between the
performance of the classifier and the neighborhood size. Secondly, CNN models expect a fixed
input size, which is not practical with hand-drawn polygons. The equal size buffer created from
the selected point samples ensure the consistency in the CNN model input.

8

3. Convolutional Neural Networks
Neural networks learn to detect patterns and representations from the datasets which are
utilized in various classification tasks [60]. Neural networks consist of three main layers which
comprise neurons (or nodes). The nodes in the input layer hold the data points. Each node is
connected to the hidden layer which is further connected to the output layer. The activation
functions are applied to hidden and output layers. Activation function introduces the nonlinearity in the model by firing selected nodes in the layer. Every connection between the nodes
has a weight. With every iteration, the loss is calculated with a loss function, and weights of
each connection are modified with the backpropagation algorithm. The model is run until the
loss is stabilized. The trained model is further used to predict the outcomes of the newer dataset.
CNNs are an extension to regular neural networks. The input to the CNN models are the
neurons arranged in the form of arrays of dimension N x M x R, where N, M, and R are the
length, breadth and the depth. Typically, RGB images, which have a depth of 3, are used as
inputs to the CNN model.
3.1. CNN structure design: Network Structure
The structure of CNN includes input layer, convolution layer, pooling layer, and fully
connected layers. The designed CNN architecture (Fig. 3) consists of an input layer with
dimensions of 11x11x10, which holds the individual image patches. A filter of size 3x3x10
is employed in the first convolution layer, which slides overall spatial locations and
calculates the dot product of the filter and the small chunk of the input data (size similar to
the filter), producing a 2-D activation map. 32 such activation maps are produced which
are bundled to create a volume of 9x9x32. Activation functions are used at each
convolutional layer to introduce the non-linearity in the model. RELU (Rectilinear Linear
Unit) is commonly used in neural networks due to its better performance on loss
convergence. RELU performs elementwise activation to the created volume of convolution
layer. The filter of size 3x3x32 is applied to the second convolutional layer. 64 activation
maps are created which produced the output volume of 7x7x64. Pooling is a downsampling
operation, which helps in reducing the number of parameters in the network. Pooling
operation is independently implemented for each activation layer. Max pooling with 3x3
filters and stride of 2 is applied to the output volume of the second convolution layer. The
resulting volume after pooling becomes 3x3x64. The volume is flattened to create 576
values, which are fully connected to the array of 128-D vector. The vector is further
connected to the 14-D vector which produces the predictions. The softmax function is
applied to the final layer which provides the probabilities of the existence of each class.
The CNN model is trained for 300 epochs.

9

Figure 3: CNN Network Structure Diagram.

3.2. Kernels
The Kernel size is an important hyper-parameter which is responsible for creating a group
of local regions in the provided input. The combinations of pixels in local regions provide
a hierarchical understanding of the object present in the input. The CNN learns to classify
shapes, colors and the distributions in an image. We compared the accuracy of the CNN
model with different sizes of kernels (1x1, 3x3, and 5x5). The model showed the best
performance when the size of the kernel is kept at 3x3.
3.3. Data augmentation
CNN, in general, require large datasets to provide satisfactory results. Data augmentation
is a technique to manufacture the new data from the existing dataset without sufficient loss
of representation of details. The random zoom, rotate, mirror, flip, contrast stretch, etc. are
often applied to the images to generate more dataset. However, in the case of satellite image
patches, most of the listed techniques would result in a loss of details. Therefore, we
introduce the rotation factor to the Train and Val set, where each patch was rotated 90
degrees three times and saved. It led to the increment in the dataset utilized by the CNN
classifier by the factor of 4 (2503 x 4 = 10012 samples).
3.4. Dropout rate
Dropout is a technique to reduce overfitting and improve generalization of the model [61].
The dropout rate is introduced in hidden layers as a probability factor at which the
connections between the neurons are randomly omitted. This prevents too much reliance
on the particular neurons. For example, for the dropout rate of 0.5, the model randomly
drops the connections with 50% probability. The model is forced to learn the features of
the input without relying on specific neurons, hence, it becomes better generalized to the
dataset. We experimented with different values of dropout rates and at different layers. In
this model, dropout rates of 0.5 and 0.25 are implemented at max pooling layer and the
dense layer respectively.
4. Other Machine Learning Classifiers
Widely used machine learning algorithms such as RF, SVM and ANN in LCZ classification
are further utilized to compare the per class accuracy of the CNN model. Random Forests (RF)
is an ensemble of random decision trees, in which class assignments are merged to increase

10

overall accuracy and stability in model predictions. Each decision tree will formulate a set of
rules, which are used by the model in performing predictions. Random Forests model randomly
subsets the input features and grows the decision tree from the calculated node. After the
creation of n trees, the class labels are determined by considering the most voted class. RF
classifiers are robust to noise, can handle high data dimensionality and insensitive to overfitting
[15], [62]. We experimented with the RF classifier by varying the number of trees, before
finally selecting 32 trees.
Given a labeled training dataset, Support Vector Machines (SVM) finds the best defining
boundaries (an optimal hyperplane) which increase class separability of the n-Dimensional
input features according to the class labels [14]. In a simpler two-dimensional dataset, the
hyperplane is the line dividing the two dataset clusters. SVMs are capable of better
generalization from relatively smaller datasets by performing complex feature transformations
using the set of mathematical operations (known as Kernels) [16]. The classifiers are trained
with the samples in patch based dataset (11x11x10 values), which are further subdivided into
121 independent features each with the size 1x10. The label of the particular patch is shared
among the independent features.
An ANN with 2 hidden layers, each containing 20 nodes, is created in which the input layer
holds the 10 feature values from each training sample. The output layer provides the predicted
output with the help of the softmax classifier. The dropout rate of 0.5 is implemented in each
hidden layer to prevent overfitting. The ANN model is trained for 250 epochs until the loss is
diminished. Keras [63] python library is used to design CNN and ANN architectures, while
Scikit-learn [64] is used to implement RF and SVM. The training, testing, and evaluation is
done on a system with a quad-core Xeon processor with NVIDIA K2000 GPU card. The
training time for each of the classifiers took less than 30 minutes.
4.1. Integrating spatio-contextual information with spectral-only classifiers
CNN, by design, incorporates spatial and spectral information for image classification. To
compare the spectral-only classifiers with CNN based classification and to improve the
performance of spectral-only classifiers, we incorporated spatial information in the image
classification process at post-classification [65]. Commonly used techniques to include
spatial context at pre-classification, during classification, and post-classification include
utilization of image textures [66], Mathematical Morphology [67], Object-Based Image
classification [68], Contextual support vector machines [49], Markov Random Fields [69],
and Majority Voting [48]. In this study, Majority Voting (MV) method is utilized, which is
considered accurate, simpler and faster than other spatial-spectral classification methods
[48].

11

Figure 4: Integration of spatial information to pixel-based classification.

Majority Voting (MV) utilizes a spatial neighborhood prepared with the help of various
segmentation algorithms such as Watershed [70], HSeg [71] and classification map as
prepared by the spectral classifier [47]. For each segmented neighborhood, all the pixels
are assigned to the most frequent class. This study performs a modified MV technique in
which the fixed-size sliding spatial neighborhood (kernel) of size 11x11 pixels is utilized
[45], which assigns the most frequent class to the central pixel (Fig. 4). For each pixel in
the pixel-based classification map, the final output gets refined with MV-based spatial
regulation. The accuracy statistics of all the three classifiers on the Test dataset is calculated
with the help of final classification maps.
5. Results
We present the experimental results of the classifiers’ performance on the Evaluation set. The
Train dataset is used to train the model while Val dataset is used to provide an unbiased
evaluation of the trained model while tuning hyperparameters to get the best performance of
the developed Neural Network model. The Test set is used to provide an unbiased evaluation
of the final prepared model. The Test set is not seen by the model during its training. It is used
only after the model is fully trained after hyper-parameters tuning. We made use of Train, Eval,
and Test set in case of CNN and ANN classifiers, in which hyperparameters such as kernels,
dropout rate, hidden layers, depth of layers, and activation functions are used. However, for
RF and SVM classifiers, we merged the Train and Eval set to create a larger training dataset
and assessed the performance of the classifier on the Test dataset (Table. 2).
We utilized Overall Accuracy and Kappa metrics to compare the per class performance. We
also calculated the F1-score metrics to compare the accuracy between classes. F1-score is a
harmonic mean of Precision and Recall. Precision is the proportion of positive detections of
the classifier which were actually correct, whereas Recall refers to the proportion of actual
positives which were detected correctly. Precision is also defined as TP/TP+FP (TP: True
Positives; FP: False Positives), while Recall as TP/TP+FN (FN: False Negatives). Precision
and Recall are alternatively called as Positive Predictive Value (PPV) and Sensitivity,

12

respectively. We created a confusion matrix to better understand the image classification results
(Fig. 5). The diagonal values in the matrix refer to the number of correctly identified pixels.
The ratio of a total number of correctly identified pixels to the total number of considered pixels
gives the classification’s overall accuracy. The overall accuracy metrics, however, is
influenced by unbalanced and prominent classes. Kappa index is therefore used to compare the
classifiers. Kappa index of Agreement has been widely used in assessing the classifier’s
performance in remote sensing domain. Kappa value provides the information on the classifier
as better or worse than random assignment of classes. The Kappa value of CNN classifier
essentially suggests that the classifier is 81 percent better than random assignment of classes.
Table 3: Class-wise comparison of results from different classifiers.

Classifiers
Classes
LCZ 2
LCZ 3
LCZ 4
LCZ 5
LCZ 8
LCZ 9
LCZ 10
LCZ A
LCZ B
LCZ C
LCZ D
LCZ E
LCZ F
LCZ G

CNN

SVM

0.87
0.92
0.8
0.61
0.57
0.69
0.77
0.98
0.85
0.72
0.94
0.81
0.86
0.98

ANN
RF
F1-score
0.8
0.91
0.79
0.99
0.4
0.53
0.58
0.28
0.09
0.66
0.65
0.62
0.74
0.76
0.99
0.99
0.91
0.94
0.68
0.76
0.9
0.98
0.28
0.33
0.84
0.94
0.89
0.94

O.A.
Kappa

0.82
0.81

0.72
0.69

0.49
0.44

0.74
0.72

0
0.57
0.21
0
0
0.12
0.03
0.98
0.84
0.22
0.74
0
0.62
0.74

As evident from Table 3, CNN classifier outperforms the pixel-based methods by a
considerable margin. CNN is followed by RF, ANN, and SVM; with an overall accuracy of 74,
72, and 49 percent respectively. CNN has shown superior classification capability where per
class F1-score is more than 0.70, except for the three classes. The score (0.57-0.92) of LCZ 3
stands out from the rest of the built classes in all classifiers, which shows the clear distinction
of the pixel values of compact low-rise built form among all the built classes. Similarly, LCZ
2 shows consistency in performance in at least three of the classifiers. LCZ A and LCZ G,
which represent dense trees and water respectively, are detected by all the classifiers with near
cent percent accuracy. As discussed in the earlier text, CNN benefits from spatial and spectral
variety, hence the built classes, which are studied in context with openness and vegetation
(LCZ 4, 5, 9 and 10), have been better represented by the CNN classifier. The LCZ 8 class

13

provides the least accurate predictions among the built classes, which is partially due to the
presence of built structures comprising of large roof-areas in warehouses, low-rise structures,
and homogeneity in pixel value distribution as in LCZ E and LCZ F classes. The RF classifier
shows an affinity to LCZ E, where a fraction of almost every class is misidentified as
impervious surfaces. LCZ 5 is interpreted as LCZ 4 in most cases due to the similarity in form
and texture pattern and the probable inconsistency in the interpretation of both the classes while
preparing the training samples. Among the natural classes, LCZ C is the most misclassified
class. The main reason can be attributed to the use of Google high-resolution imagery used to
visually select samples, while Sentinel imagery was used to create original training dataset. It
may have led to the inclusion of patches which are prominent in High res imagery but not
discernible to the classifier in mid Resolution Sentinel imagery.

Figure 5: Confusion matrix showing the performance of classifiers.

14

The Sentinel 2B satellite data for Mumbai after clipping the imagery to city extents consists of
4192 x 2192 x 10 pixels. A sliding window of 11x11 pixels with a stride of 1 pixel is passed
through the CNN trained classifier, the obtained result for every patch is represented by the
center pixel of the patch. For pixel-based classifiers such as ANN, RF, and SVM; each pixel
with 1x10 dimension is passed through the respective classifiers to obtain predictions. The
spatial regulation on the predicted map is applied with the sliding window MV approach (Fig.
4). The final classification map is prepared with the use of different Python modules such as
Rasterio [72] and Geospatial Data Abstraction Library (GDAL) [73]. The prediction map
generated from CNN classifier is shown in Fig. 6.

Figure 6: Classification map produced by CNN classifier (Additional maps in Fig. 7, 8, and 9).

15

The CNN based classifier generates the classification map at 10 m resolution. The performance
of the classifier is excellent in demarcating different human-made and natural textures at fine
spatial resolution. It is surprising that only a few points of training data can create a relatively
high-resolution classification map (Fig. 6) which includes complex urban classes. Figure. 7, 8,
and 9 show a zoomed version of the classification map showing each of the 14 classes. Highrise built structures (LCZ 4) do not form a continuous stretch, which is evident from the
classification map. These structures are present in the form of towers in the midst of mid-rise
colonies and structures. High rise buildings can be seen (in Red) as scattered in Fig. 7 A and 7
B in the midst of open and compact mid-rise built form (LCZ 2 and 5). The compact mid-rise
built (LCZ 2) character extends to form a contiguous stretch (Fig. 7 B). The classification
accuracy for Compact low-rise (LCZ 3), which is mostly represented by squatter settlements
is highest among other built classes. Scattered slum settlements even having different forms
and sizes are accurately demarcated in the city (Fig. 7 C). Figure 7 E shows the few occurrences
of the

16

Figure 7: Zoomed in areas showing Google imagery and corresponding CNN classification performed on
Sentinel 2B imagery (Part A).

17

Figure 8: Zoomed in areas showing Google imagery and corresponding CNN classification performed on
Sentinel 2B imagery (Part B).

18

Figure 9: Zoomed in areas showing Google imagery and corresponding CNN classification performed on
Sentinel 2B imagery (Part C).

LCZ 8 in the form of warehouses (in orange). Heavy industries (LCZ 10) can be visually
identified by pipelines, tankers, and machinery in Google imagery. However, the accuracy of
the classifier in detecting these features is comparatively lower. Figure 7 F shows LCZ 10 (in
magenta) along with the misclassified categories. The sparse built cover (LCZ 9) includes an
arrangement of built structures with large vegetation cover (Fig. 8 G). Natural classes such as
LCZ A (Fig. 8 H) and LCZ B (Fig. 8 I), LCZ C (Fig. 8 J), and LCZ D (Fig. 8 K) are easily
delineated by the CNN classifier. Coastal areas are highlighted (Fig. 9 M) which shows the
presence of bare sand and soil (LCZ F). Figure 8 L shows road networks (LCZ E) (in grey)
delineated by the classifier.
The CNN-based classifier produced excellent results in classifying 14 classes. However, some
noticeable errors in the classification map can be seen. Common urban features and structures
such as flyovers, bridges, and steep ridges are misinterpreted as high-rise (LCZ 4) class.
Further, some of the pixels belonging to the LCZ 4 are misclassified as water (LCZ G) due to
the inability of the classifier to detect the structure beneath the shadow cast by the tall buildings.
The pixels representing heavy industry (LCZ 10) class are present at various locations in the
city, while it exists only in the southern part of the city. It is due to the fact that industry-like
physical features are detected by the classifier in multiple locations such as metro/railway
stations, airport buildings, Warehouses, flyovers, and subways.

19

6. Discussions
6.1. Conversion of classification map as per WUDAPT protocol
LCZ mapping process follows WUDAPT protocol, which allows local experts from all
over the world to create consistent LCZ maps for different cities. For the universal
applicability of the procedure, easier data availability, along with computationally and
fiscally inexpensive software requirements have been given utmost focus. Several studies
have modified the protocol to include open EO datasets such as SAR [37], [43], ASTER
[42], and proprietary VHR datasets [74] to prepare LCZ maps to gain accuracy and better
class delineation. Further the inclusion of OBIA techniques [74] and rule-based analysis
from GIS datasets [38], [39] have also been used in preparation of such maps, which have
shown comparatively better classification results [36]. However, these additional
techniques and methods require advanced image analysis skills and software knowledge
which adds to the complexity for an inexperienced user and hence creates a lack of
reproducibility by experts in different cities [9].
The proposed methodology used in this study is different from WUDAPT protocol in that
it (a) takes into account 10 band Sentinel Imagery as opposed to Landsat imagery, (b)
considers point-based samples instead of polygons to overcome the difficulty in sample
creation in cities with large horizontal heterogeneity, and (c) utilizes native spatiocontextual classifier such as CNN and modified machine learning classifiers to prepare
classification maps. While openly available Sentinel dataset and selection of point-based
samples can be easily implemented as a part of LCZ maps preparation, the implementation
of Contextual classifiers such as CNN requires experience and training, which challenges
the original purpose of WUDAPT protocol. Studies such as [45] detail the need to integrate
spatial information as part of the WUDAPT classification process, especially in cities with
spatial heterogeneity. However, such methods are relatively difficult to replicate without
proper documentation regarding the implementation of such tools. More studies are
required to explore the spatial-spectral domain in LCZ classification.
According to [9], the spatial resolution of LCZ maps at 100-150m is optimal to fulfill the
original purpose of the LCZ maps preparation, i.e., to classify Urban Heat Island (UHI)
observation sites and to represent the climate zones at the local scale. The discussed
methodology generates the classification output as 10m resolution, which in LCZ
classification method is considered too high due to the creation of a large number of isolated
pixels. In this study, the problem of isolated pixels and patches in classification results at a
higher resolution is to a great extent solved with the help of spatio-contextual classifiers
such as CNN and modified machine learning image classification methods. Further, to
decrease the granularity and to create homogeneous zones, post-classification filters [36],
[75] can be applied in accordance to [9]. Following this approach, the classification map
can be effectively converted to a LCZ map for the city while also being relevant to various
other use cases.

20

6.2. Relevance of high-resolution LULC maps in urban studies.
Apart from urban climate-oriented studies, LULC mapping using LCZ scheme has
relevance to different domains and subjects of societal benefit. The high-resolution
classification maps can act as a remote sensing based proxy for micro-level zone
delineation which may assist in urban disaster risk management (DRM) and vulnerability
mapping [76]. The demographic characteristics and socioeconomic status of urban areas
are closely related to urban morphology [77], [78]. The areas represented as LCZ 3 (Fig. 7
C) mostly represent slum settlements, the location and growth of which can be efficiently
mapped and empirical connections can be drawn with indicators such as population density,
requirements of various services, and potential health risks. As the classification maps are
prepared on openly available Sentinel dataset, such assessments can be carried out at the
regional or national level to frame policies [77], [79], [80]. Availability of urban open and
green spaces have been extensively studied as a recreational potential and wellbeing with
the help of various remote sensing techniques [81]–[83]. The classification results obtained
from the proposed approach clearly demarcate the location of open playgrounds and green
areas (Fig. 7 A, C and Fig. 8 G), even when these classes are enclosed by other built or
natural classes. Micro-level assessments regarding the accessibility to open spaces and
calculation of natural and built character can greatly benefit from such classification maps.
In a similar way, image classification strategy discussed in the study may improve the
classification accuracy to detect impervious surfaces [84] and different natural classes,
which help in estimation of water runoff [85] and assessment of floods [86]. Overall, such
classification maps will hold great relevance to urban management authorities who devise
land use and site planning regulations [87] and monitor urban change detection and
haphazard development.
6.3. Limitations of the proposed methodology
This study discussed the application of CNN in detail. However, the preparation of the
CNN model to fit every use case is a laborious task. The hyperparameter tuning includes a
series of trial and error ranging from the size of the input to the selection of activation
functions. The size of the input patch in satellite imagery classification tasks is a tradeoff
between the introduction of neighborhood spatial heterogeneity and model accuracy.
Finding a sweet spot between the two factors is difficult. Further, CNN models require a
high computational cost. Hence the proper evaluation of the requirements and purpose is
necessary before deciding to use CNN based classification methods. Further studies are
required to evaluate the model performance using selected imagery bands and fusion of
other SAR and multispectral satellite datasets.
Integration of spatial information in machine learning classifiers can be executed with the
help of different methods; this study utilized one of the many such approaches. Similar to
the application of CNN, Majority Voting (MV) at neighborhood for each central pixel
creates a huge computational load. Such issues create major bottlenecks during the
implementation process. The evaluation of other spatial-spectral methods can be checked
to select the optimal approach. Further, the MV approach considers the fixed neighborhood

21

size of 11x11 pixels, which is kept equal to the spatial input to the CNN model. The
optimum neighborhood size can be determined with further reiterations.
7. Conclusions
In this study, we utilized state-of-the-art deep learning techniques such as CNN and open
satellite data sources to prepare a classification map as per the LCZ scheme. While highresolution datasets have been used to study the effectiveness of DL algorithms, studies
demonstrating the applicability of such techniques in mid-resolution satellite imagery are
scarce. Surprisingly, the performance of the created model is exceptional, given the inherent
complexity in urban texture mapping in a city like Mumbai and the relatively coarser resolution
of the Sentinel dataset. The process of creating such maps includes point-based training data
generation, which is achieved with the help of Google Earth Pro software. The creation of
training samples can be crowdsourced to achieve generalizability and more local expertise in
the selection of LCZ classes. This study provides a novel end to end approach to produce
classification maps with better accuracy and a relatively more straightforward method. The
approach differs from other imagery-based LCZ mapping methods as it considers the spatial
and spectral variations by comparing the native spatial-spectral classifier with spectral-only
ML classifiers modified to include spatial information.
Seasonal variations can be further studied to prepare a robust classification map. The presence
of natural classes, especially low plants, which indicate open fields with grass cover and bush
and scrubs vary with the seasons. Multitemporal classification maps may give insight into the
change in natural characters and thermal profiles in the city throughout the year [35], [75]. The
creation of an integrated GIS tool to accomplish the samples generation and CNN model
building may help in rapid prototyping and quick results. Such methods can be transformed
into larger studies including various cities and regions with relative ease. In recent years, much
research and development have been focused on establishing newer DL algorithms to solve a
variety of problems in different domains. Translating such efforts in remote sensing may help
in uncovering new frontiers. For example, studies have been conducted which measure poverty
and other social indicators through satellite imagery [88], [89]. Likewise, the delineation of
urban features with openly available satellite imagery may assist in rapidly changing
urbanscapes in developing and underdeveloped nations.

Acknowledgments
The authors would like to thank the Ministry of Human Resource Development (MHRD), India and
Industrial Research and Consultancy Centre (IRCC), IIT Bombay for funding this study under the grant
titled Frontier Areas of Science and Technology (FAST), Centre of Excellence in Urban Science and
Engineering (grant number 14MHRD005).

22

References
[1]

J. R. Anderson, E. E. Hardy, J. T. Roach, and R. E. Witmer, “A Land Use And Land Cover
Classification System For Use With Remote Sensor Data,” Geological Survey Professional
Paper, 1976. url: https://pubs.usgs.gov/pp/0964/report.pdf .

[2]

B. Zheng, S. W. Myint, P. S. Thenkabail, and R. M. Aggarwal, “A support vector machine to
identify irrigated crop types using time-series Landsat NDVI data,” Int. J. Appl. Earth Obs.
Geoinf., vol. 34, no. 1, pp. 103–112, 2015, doi: 10.1016/j.jag.2014.07.002.

[3]

C. Alcantara, T. Kuemmerle, A. V. Prishchepov, and V. C. Radeloff, “Mapping abandoned
agriculture with multi-temporal MODIS satellite data,” Remote Sens. Environ., vol. 124, pp.
334–347, 2012, doi: 10.1016/j.rse.2012.05.019.

[4]

Z. Pan et al., “Mapping crop phenology using NDVI time-series derived from HJ-1 A/B data,”
Int. J. Appl. Earth Obs. Geoinf., vol. 34, no. 1, pp. 188–197, 2015, doi:
10.1016/j.jag.2014.08.011.

[5]

J. Aguirre-Gutiérrez, A. C. Seijmonsbergen, and J. F. Duivenvoorden, “Optimizing land cover
classification accuracy for change detection, a combined pixel-based and object-based
approach in a mountainous area in Mexico,” Appl. Geogr., vol. 34, pp. 29–37, 2012, doi:
10.1016/j.apgeog.2011.10.010.

[6]

P. Xiao, X. Zhang, D. Wang, M. Yuan, X. Feng, and M. Kelly, “Change detection of built-up
land: A framework of combining pixel-based detection and object-based recognition,” ISPRS
J. Photogramm. Remote Sens., vol. 119, pp. 402–414, 2016, doi:
10.1016/j.isprsjprs.2016.07.003.

[7]

X. Zhan et al., “Detection of land cover changes using MODIS 250 m data,” Remote Sens.
Environ., vol. 83, no. 1–2, pp. 336 – 350, 2002, doi: 10.1016/S0034-4257(02)00081-0.

[8]

K. M. Keegan, M. R. Albert, J. R. McConnell, and I. Baker, “Climate change and forest fires
synergistically drive widespread melt events of the Greenland Ice Sheet,” Proc. Natl. Acad.
Sci., vol. 111, no. 22, pp. 7964–7967, 2014, doi: 10.1073/pnas.1405397111.

[9]

B. Bechtel et al., “Mapping Local Climate Zones for a Worldwide Database of the Form and
Function of Cities,” ISPRS Int. J. Geo-Information, vol. 4, no. 1, pp. 199–219, 2015, doi:
10.3390/ijgi4010199.

[10]

K. S. Schmidt, A. K. Skidmore, E. H. Kloosterman, H. van Oosten, L. Kumar, and J. A. M.
Janssen, “Mapping Coastal Vegetation Using an Expert System and Hyperspectral Imagery,”
Photogramm. Eng. Remote Sens., vol. 70, no. 6, pp. 703–715, 2004, doi:
10.14358/PERS.70.6.703.

[11]

G. P. Asner, E. N. Broadbent, P. J. C. Oliveira, M. Keller, D. E. Knapp, and J. N. M. Silva,
“Condition and fate of logged forests in the Brazilian Amazon,” Proc. Natl. Acad. Sci., vol.
103, no. 34, pp. 12947–12950, 2006, doi: 10.1073/pnas.0604093103.

[12]

R. Khatami and G. Mountrakis, “Implications of classification of methodological decisions in
flooding analysis from Hurricane Katrina,” Remote Sens., vol. 4, no. 12, pp. 3877–3891, 2012,
doi: 10.3390/rs4123877.

[13]

H. de Moel and J. C. J. H. Aerts, “Effect of uncertainty in land use, damage models and
inundation depth on flood damage estimates,” Nat. Hazards, vol. 58, no. 1, pp. 407–425, 2011,
doi: 10.1007/s11069-010-9675-6.

[14]

G. Mountrakis, J. Im, and C. Ogole, “Support vector machines in remote sensing: A review,”
ISPRS J. Photogramm. Remote Sens., vol. 66, no. 3, pp. 247–259, 2011, doi:

23

10.1016/j.isprsjprs.2010.11.001.
[15]

M. Belgiu and L. Drăgu, “Random forest in remote sensing: A review of applications and
future directions,” ISPRS J. Photogramm. Remote Sens., vol. 114, pp. 24–31, 2016, doi:
10.1016/j.isprsjprs.2016.01.011.

[16]

Y. Shao and R. S. Lunetta, “Comparison of support vector machine, neural network, and
CART algorithms for the land-cover classification using limited training data points,” ISPRS J.
Photogramm. Remote Sens., vol. 70, pp. 78–87, 2012, doi: 10.1016/j.isprsjprs.2012.04.001.

[17]

S. W. Myint, P. Gober, A. Brazel, S. Grossman-Clarke, and Q. Weng, “Per-pixel vs. objectbased classification of urban land cover extraction using high spatial resolution imagery,”
Remote Sens. Environ., vol. 115, no. 5, pp. 1145–1161, 2011, doi: 10.1016/j.rse.2010.12.017.

[18]

T. Van de Voorde, W. Jacquet, and F. Canters, “Mapping form and function in urban areas: An
approach based on urban metrics and continuous impervious surface data,” Landsc. Urban
Plan., vol. 102, no. 3, pp. 143–155, 2011, doi: 10.1016/j.landurbplan.2011.03.017.

[19]

T. Hu, J. Yang, X. Li, and P. Gong, “Mapping urban land use by using landsat images and
open social data,” Remote Sens., vol. 8, no. 2, 2016, doi: 10.3390/rs8020151.

[20]

J. Haas and Y. Ban, “Urban Land Cover and Ecosystem Service Changes based on Sentinel2A MSI and Landsat TM Data,” IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., vol. 11, no.
2, pp. 485–497, 2018, doi: 10.1109/JSTARS.2017.2786468.

[21]

D. Lu and Q. Weng, “A survey of image classification methods and techniques for improving
classification performance,” Int. J. Remote Sens., vol. 28, no. 5, pp. 823–870, 2007, doi:
10.1080/01431160600746456.

[22]

J. A. Carnahan, “International Classification and Mapping of Vegetation, Ecology and
Conservation Publication No.6, Unesco, Paris, 1973. 215 x 270 mm., 93 pages, text in English,
French and Spanish, folded classification sheet inside back cover. Paperback.,” Cartography,
vol. 9, no. 3, pp. 195–195, 1976, doi: 10.1080/00690805.1976.10437920.

[23]

A. D. Gregorio and L. J. M. Jansen, “Land Cover Classification System.” Food and
Agriculture Organization, Rome, 2005. url:
http://www.fao.org/docrep/008/y7220e/y7220e06.htm

[24]

FGDC, “NATIONAL VEGETATION CLASSIFICATION STANDARD,” Federal
Geographic Data Committee, 2008. url: https://www.fgdc.gov/standards/projects/FGDCstandards-projects/vegetation/NVCS_V2_FINAL_2008-02.pdf

[25]

EEA, “CORINE Land Cover,” European Environment Agency, 2000. url:
http://image2000.jrc.ec.europa.eu/reports/technical_guide.pdf

[26]

P. Prastacos, N. Chrysoulakis, and G. Kochilakis, “Urban Atlas, land use modelling and spatial
metric techniques,” ERSA Conf. Pap., 2011.url: http://wwwsre.wu.ac.at/ersa/ersaconfs/ersa11/e110830aFinal01406.pdf

[27]

NRSA, “National Land Use and Land Cover Mapping Using Multi-Temporal AWiFS Data,”
2006. url: http://bhuvan.nrsc.gov.in/gis/thematic/tools/document/LULC250/0405.pdf

[28]

MoUD, “Urban and Regional Development Plans Formulation and Implementation
Guidelines,” 2015. url: http://mohua.gov.in/upload/uploadfiles/files/URDPFI Guidelines Vol
I.pdf

[29]

M. J. Barnsley and S. L. Barr, “Distinguishing urban land-use categories in fine spatial
resolution land-cover data using a graph-based, structural pattern recognition system,”
Comput. Environ. Urban Syst., vol. 21, no. 3–4, pp. 209–225, 1997, doi: 10.1016/S01989715(97)10001-1.

24

[30]

M. Li, A. Stein, W. Bijker, and Q. Zhan, “Urban land use extraction from Very High
Resolution remote sensing imagery using a Bayesian network,” ISPRS J. Photogramm. Remote
Sens., vol. 122, no. 1970, pp. 192–205, Dec. 2016, doi: 10.1016/j.isprsjprs.2016.10.007.

[31]

T. Rashed, J. R. Weeks, D. Roberts, J. Rogan, and R. Powell, “Measuring the Physical
Composition of Urban Morphology Using Multiple Endmember Spectral Mixture Models,”
Photogramm. Eng. Remote Sens., vol. 69, no. 9, pp. 1011–1020, 2003, doi:
10.14358/PERS.69.9.1011.

[32]

M. J. Barnsley, L. Møller-Jensen, and S. L. Barr, “Inferring urban land use by spatial and
structural pattern recognition,” Remote Sens. urban Anal., vol. 9, pp. 115–144, 2003. url:
https://www.semanticscholar.org/paper/Inferring-Urban-Land-Use-by-Spatial-and-StructuralBarnsley-M%C3%B8ller-Jensen/f109a0823436f805346756c9d79ce547fd15143f

[33]

I. D. Stewart and T. R. Oke, “Local climate zones for urban temperature studies,” Bull. Am.
Meteorol. Soc., vol. 93, no. 12, pp. 1879–1900, 2012, doi: 10.1175/BAMS-D-11-00019.1.

[34]

J. Quanz, S. Ulrich, D. Fenner, A. Holtmann, and J. Eimermacher, “Micro-Scale Variability of
Air Temperature within a Local Climate Zone in Berlin, Germany, during Summer,” Climate,
vol. 6, no. 1, p. 5, 2018, doi: 10.3390/cli6010005.

[35]

J. Geletič, M. Lehnert, and P. Dobrovolný, “Land surface temperature differences within local
climate zones, Based on two central European cities,” Remote Sens., vol. 8, no. 10, pp. 1–18,
2016, doi: 10.3390/rs8100788.

[36]

T. Gál, B. Bechtel, and J. Unger, “Comparison of two different Local Climate Zone mapping
methods,” ICUC9-9th Int. Conf. Urban Clim. Toulouse, Fr. (20-24 July), no. Kottek 2006, pp.
1–6, 2015, doi: 10.1101/169862.

[37]

B. Bechtel and C. Daneke, “Classification of local climate zones based on multiple earth
observation data,” IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., vol. 5, no. 4, pp. 1191–
1202, 2012, doi: 10.1109/JSTARS.2012.2189873.

[38]

J. Geletič and M. Lehnert, “GIS-based delineation of local climate zones: The case of mediumsized Central European cities,” Morav. Geogr. Reports, vol. 24, no. 3, pp. 2–12, 2016, doi:
10.1515/mgr-2016-0012.

[39]

Y. Zheng et al., “GIS-based mapping of Local Climate Zone in the high-density city of Hong
Kong,” Urban Clim., vol. 24, pp. 419–448, 2018, doi: 10.1016/j.uclim.2017.05.008.

[40]

M. G. et al., “An Introduction to the WUDAPT project,” Proc. ICUC9. Meteo Fr., no.
February 2016, p. 6, 2015. url: http://www.wudapt.org/wpcontent/uploads/2015/05/Mills_etal_ICUC9.pdf

[41]

O. Danylo, L. See, B. Bechtel, D. Schepaschenko, and S. Fritz, “Contributing to WUDAPT: A
Local Climate Zone Classification of Two Cities in Ukraine,” IEEE J. Sel. Top. Appl. Earth
Obs. Remote Sens., vol. 9, no. 5, pp. 1841–1853, May 2016, doi:
10.1109/JSTARS.2016.2539977.

[42]

Y. Xu, C. Ren, M. Cai, N. Y. Y. Edward, and T. Wu, “Classification of Local Climate Zones
Using ASTER and Landsat Data for High-Density Cities,” IEEE J. Sel. Top. Appl. Earth Obs.
Remote Sens., vol. 10, no. 7, pp. 3397–3405, Jul. 2017, doi: 10.1109/JSTARS.2017.2683484.

[43]

B. Bechtel, L. See, G. Mills, and M. Foley, “Classification of Local Climate Zones Using SAR
and Multispectral Data in an Arid Environment,” IEEE J. Sel. Top. Appl. Earth Obs. Remote
Sens., vol. 9, no. 7, pp. 3097–3105, Jul. 2016, doi: 10.1109/JSTARS.2016.2531420.

[44]

J. Hu, P. Ghamisi, and X. Zhu, “Feature Extraction and Selection of Sentinel-1 Dual-Pol Data
for Global-Scale Local Climate Zone Classification,” ISPRS Int. J. Geo-Information, vol. 7,
no. 9, p. 379, 2018, doi: 10.3390/ijgi7090379.

25

[45]

M. L. Verdonck, A. Okujeni, S. van der Linden, M. Demuzere, R. De Wulf, and F. Van
Coillie, “Influence of neighbourhood information on ‘Local Climate Zone’ mapping in
heterogeneous cities,” Int. J. Appl. Earth Obs. Geoinf., vol. 62, no. March, pp. 102–113, 2017,
doi: 10.1016/j.jag.2017.05.017.

[46]

M. Li, S. Zang, B. Zhang, S. Li, and C. Wu, “A review of remote sensing image classification
techniques: The role of Spatio-contextual information,” Eur. J. Remote Sens., vol. 47, no. 1,
pp. 389–411, 2014, doi: 10.5721/EuJRS20144723.

[47]

Y. Tarabalka, J. A. Benediktsson, and J. Chanussot, “Spectral-spatial classification of
hyperspectral imagery based on partitional clustering techniques,” IEEE Trans. Geosci.
Remote Sens., vol. 47, no. 8, pp. 2973–2987, 2009, doi: 10.1109/TGRS.2009.2016214.

[48]

M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C. Tilton, “Advances in
spectral-spatial classification of hyperspectral images,” Proc. IEEE, vol. 101, no. 3, pp. 652–
675, 2013, doi: 10.1109/JPROC.2012.2197589.

[49]

M. Fauvel, J. Chanussot, J. A. Benediktsson, and J. R. Sveinsson, “Spectral and spatial
classification of hyperspectral data using SVMs and morphological profiles,” in 2007 IEEE
International Geoscience and Remote Sensing Symposium, 2007, pp. 4834–4837, doi:
10.1109/IGARSS.2007.4423943.

[50]

Y. Lecun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521, no. 7553, pp. 436–
444, 2015, doi: 10.1038/nature14539.

[51]

G. Scarpa, M. Gargiulo, A. Mazza, and R. Gaetano, “A CNN-based fusion method for feature
extraction from sentinel data,” Remote Sens., vol. 10, no. 2, pp. 1–20, 2018, doi:
10.3390/rs10020236.

[52]

A. Romero, C. Gatta, and G. Camps-Valls, “Unsupervised Deep Feature Extraction for
Remote Sensing Image Classification,” IEEE Trans. Geosci. Remote Sens., vol. 54, no. 3, pp.
1–14, Nov. 2015, doi: 10.1109/TGRS.2015.2478379.

[53]

M. Mahdianpari, B. Salehi, M. Rezaee, F. Mohammadimanesh, and Y. Zhang, “Very Deep
Convolutional Neural Networks for Complex Land Cover Mapping Using Multispectral
Remote Sensing Imagery,” Remote Sens., vol. 10, no. 7, p. 1119, 2018, doi:
10.3390/rs10071119.

[54]

Q. Gao, S. Lim, and X. Jia, “Hyperspectral image classification using convolutional neural
networks and multiple feature learning,” Remote Sens., vol. 10, no. 2, 2018, doi:
10.3390/rs10020299.

[55]

S. Yu, S. Jia, and C. Xu, “Convolutional neural networks for hyperspectral image
classification,” Neurocomputing, vol. 219, pp. 88–98, 2017, doi:
10.1016/j.neucom.2016.09.010.

[56]

W. Hu, Y. Huang, L. Wei, F. Zhang, and H. Li, “Deep convolutional neural networks for
hyperspectral image classification,” J. Sensors, vol. 2015, 2015, doi: 10.1155/2015/258619.

[57]

G. Kaplan, “Sentinel-2 Pan Sharpening—Comparative Analysis,” Proceedings, vol. 2, no. 7, p.
345, 2018, doi: 10.3390/ecrs-2-05158.

[58]

M. Gašparović and T. Jogun, “The effect of fusing Sentinel-2 bands on land-cover
classification,” Int. J. Remote Sens., vol. 39, no. 3, pp. 822–841, 2018, doi:
10.1080/01431161.2017.1392640.

[59]

D. Verma, A. Jana, and K. Ramamritham, “Transfer learning approach to map urban slums
using high and medium resolution satellite imagery,” Habitat Int., vol. 88, no. April, p.
101981, 2019, doi: 10.1016/j.habitatint.2019.04.008.

26

[60]

I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press, 2016.

[61]

G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, “Improving
neural networks by preventing co-adaptation of feature detectors,” ArXiv, Jul. 2012. url:
http://arxiv.org/abs/1207.0580.

[62]

V. F. Rodriguez-Galiano, M. Chica-Olmo, F. Abarca-Hernandez, P. M. Atkinson, and C.
Jeganathan, “Random Forest classification of Mediterranean land cover using multi-seasonal
imagery and multi-seasonal texture,” Remote Sens. Environ., vol. 121, pp. 93–107, 2012, doi:
10.1016/j.rse.2011.12.003.

[63]

F. Chollet and others, “Keras.” 2015.

[64]

F. Pedregosa et al., “Scikit-learn: Machine Learning in Python,” J. Mach. Learn. Res., vol. 12,
pp. 2825–2830, 2011. url:
http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf

[65]

L. Wang, C. Shi, C. Diao, W. Ji, and D. Yin, “A survey of methods incorporating spatial
information in image classification and spectral unmixing,” Int. J. Remote Sens., vol. 37, no.
16, pp. 3870–3910, 2016, doi: 10.1080/01431161.2016.1204032.

[66]

R. M. Haralick, K. Shanmugam, and I. Dinstein, “Textural Features for Image Classification,”
IEEE Trans. Syst. Man. Cybern., vol. SMC-3, no. 6, pp. 610–621, Nov. 1973, doi:
10.1109/TSMC.1973.4309314.

[67]

P. Soille and M. Pesaresi, “Advances in mathematical morphology applied to geoscience and
remote sensing,” IEEE Trans. Geosci. Remote Sens., vol. 40, no. 9, pp. 2042–2055, 2002, doi:
10.1109/TGRS.2002.804618.

[68]

G. J. Hay and G. Castilla, “Geographic Object-Based Image Analysis (GEOBIA): A new name
for a new discipline,” in Object-Based Image Analysis, vol. 133, no. 18, Berlin, Heidelberg:
Springer Berlin Heidelberg, 2016, pp. 75–89.

[69]

Y. Tarabalka, M. Fauvel, J. Chanussot, and J. A. Benediktsson, “SVM- and MRF-based
method for accurate classification of hyperspectral images,” IEEE Geosci. Remote Sens. Lett.,
vol. 7, no. 4, pp. 736–740, 2010, doi: 10.1109/LGRS.2010.2047711.

[70]

S. K. Mylonas, D. G. Stavrakoudis, J. B. Theocharis, and P. A. Mastorocostas, “Spectralspatial classification of remote sensing images using a region-based GeneSIS Segmentation
algorithm,” IEEE Int. Conf. Fuzzy Syst., pp. 1976–1984, 2014, doi: 10.1109/FUZZIEEE.2014.6891620.

[71]

A. J. Plaza and J. C. Tilton, “Automated selection of results in hierarchical segmentations of
remotely sensed hyperspectral images,” Int. Geosci. Remote Sens. Symp., vol. 7, pp. 4946–
4949, 2005, doi: 10.1109/IGARSS.2005.1526784.

[72]

S. Gillies, “Rasterio: geospatial raster I/O for Python programmers.” 2018.

[73]

GDAL Development Team, “GDAL - Geospatial Data Abstraction Library, Version 2.4.0.”
2018.

[74]

P. Gamba, G. Lisini, P. Liu, P. J. Du, and H. Lin, “Urban Climate Zone Detection and
Discrimination Using Object- Based Analysis of VHR Scenes,” Proc. 4th GEOBIA, pp. 70–74,
2012. url : http://mtc-m16c.sid.inpe.br/col/sid.inpe.br/mtc-m18/2012/05.18.17.35/doc/023.pdf

[75]

N. Yokoya, P. Ghamisi, and J. Xia, “Multimodal, multitemporal, and multisource global data
fusion for local climate zones classification based on ensemble learning,” in 2017 IEEE
International Geoscience and Remote Sensing Symposium (IGARSS), 2017, pp. 1197–1200,
doi: 10.1109/IGARSS.2017.8127172.

[76]

S. Ghaffarian, N. Kerle, and T. Filatova, “Remote Sensing-Based Proxies for Urban Disaster

27

Risk Management and Resilience: A Review,” Remote Sens., vol. 10, no. 11, p. 1760, 2018,
doi: 10.3390/rs10111760.
[77]

M. Yuan, Y. Song, and L. Guo, “Exploring determinants of urban form in China through an
empirical study among 115 cities,” Sustain., vol. 10, no. 10, pp. 1–14, 2018, doi:
10.3390/su10103648.

[78]

M. Kuffer, K. Pfeffer, and R. Sliuzas, “Slums from space-15 years of slum mapping using
remote sensing,” Remote Sens., vol. 8, no. 6, 2016, doi: 10.3390/rs8060455.

[79]

A. Krehl, S. Siedentop, H. Taubenböck, and M. Wurm, “A Comprehensive View on Urban
Spatial Structure: Urban Density Patterns of German City Regions,” ISPRS Int. J. GeoInformation, vol. 5, no. 6, p. 76, 2016, doi: 10.3390/ijgi5060076.

[80]

F. J. Goerlich Gisbert, I. Cantarino Martí, and E. Gielen, “Clustering cities through urban
metrics analysis,” J. Urban Des., vol. 22, no. 5, pp. 689–708, 2017, doi:
10.1080/13574809.2017.1305882.

[81]

M. Atasoy, “Monitoring the urban green spaces and landscape fragmentation using remote
sensing: a case study in Osmaniye, Turkey,” Environ. Monit. Assess., vol. 190, no. 12, p. 713,
2018, doi: 10.1007/s10661-018-7109-1.

[82]

K. Jupová et al., “Monitoring of green, open and sealed urban space,” 2017 Jt. Urban Remote
Sens. Event, JURSE 2017, pp. 1–4, 2017, doi: 10.1109/JURSE.2017.7924561.

[83]

P. Hofmann, J. Strobl, and A. Nazarkulova, “Mapping green spaces in Bishkek-how reliable
can spatial analysis be?,” Remote Sens., vol. 3, no. 6, pp. 1088–1103, 2011, doi:
10.3390/rs3061088.

[84]

Q. Weng, “Remote sensing of impervious surfaces in the urban areas: Requirements, methods,
and trends,” Remote Sens. Environ., vol. 117, pp. 34–49, 2012, doi: 10.1016/j.rse.2011.02.030.

[85]

J. Wang, Z. Wu, C. Wu, Z. Cao, W. Fan, and P. Tarolli, “Improving impervious surface
estimation: an integrated method of classification and regression trees (CART) and linear
spectral mixture analysis (LSMA) based on error analysis,” GIScience Remote Sens., vol. 55,
no. 4, pp. 583–603, 2018, doi: 10.1080/15481603.2017.1417690.

[86]

G. Sofia, G. Roder, G. Dalla Fontana, and P. Tarolli, “Flood dynamics in urbanised
landscapes: 100 years of climate and humans’ interaction,” Sci. Rep., vol. 7, no. December
2016, pp. 1–12, 2017, doi: 10.1038/srep40527.

[87]

N. G. R. Perera and R. Emmanuel, “A ‘Local Climate Zone’ based approach to urban planning
in Colombo, Sri Lanka,” Urban Clim., vol. 23, pp. 188–203, 2018, doi:
10.1016/j.uclim.2016.11.006.

[88]

P. K. Suraj, A. Gupta, M. Sharma, S. B. Paul, and S. Banerjee, “On monitoring development
using high resolution satellite images,” ArXiv, Dec. 2017. url : http://arxiv.org/abs/1712.02282

[89]

N. Jean, M. Burke, M. Xie, W. M. Davis, D. B. Lobell, and S. Ermon, “Combining satellite
imagery and machine learning to predict poverty,” Science (80-. )., vol. 353, no. 6301, pp.
790–794, Aug. 2016, doi: 10.1126/science.aaf7894.

28

