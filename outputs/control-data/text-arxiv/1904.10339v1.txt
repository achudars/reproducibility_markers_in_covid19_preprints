arXiv:1904.10339v1 [math.NA] 22 Apr 2019

Solution of the Linearly Structured Partial Polynomial
Inverse Eigenvalue Problem
Suman Rakshita , S. R. Kharea
a

Department of Mathematics, Indian Institute of Technology Kharagpur, Kharagpur
721302, India

Abstract
In this paper, linearly structured partial polynomial inverse eigenvalue problem is considered for the n×n matrix polynomial of arbitrary degree k. Given
a set of m eigenpairs (1 6 m 6 kn), this problem concerns with computing
the matrices Ai ∈ Rn×n for i = 0, 1, 2, . . . , (k − 1) ofP
specified linear structure
k−1 i
such that the matrix polynomial P (λ) = λk In + i=0
λ Ai has the given
eigenpairs as its eigenvalues and eigenvectors. Many practical applications
give rise to the linearly structured structured matrix polynomial. Therefore,
construction of the linearly structured matrix polynomial is the most important aspect of the polynomial inverse eigenvalue problem(PIEP). In this
paper, a necessary and sufficient condition for the existence of the solution
of this problem is derived. Additionally, we characterize the class of all solutions to this problem by giving the explicit expressions of solutions. The
results presented in this paper address some important open problems in the
area of PIEP raised in De Teran, Dopico and Van Dooren [SIAM Journal on
Matrix Analysis and Applications, 36(1) (2015), pp 302 − 328]. An attractive
feature of our solution approach is that it does not impose any restriction
on the number of eigendata for computing the solution of PIEP. The proposed method is validated with various numerical examples on a spring mass
problem.
Keywords: Matrix polynomial, linearly structured matrix, polynomial
inverse eigenvalue problem, polynomial eigenvalue problem.

Email addresses: sumanrakshit1991@gmail.com (Suman Rakshit),
srkhare@maths.iitkgp.ernet.in (S. R. Khare )

Preprint submitted to Elsevier

April 24, 2019

1. Introduction
Consider the higher order system of ordinary differential equations of the
form
dk v(t)
dk−1v(t)
dv(t)
Ak
+
A
+ · · · + A1
+ A0 v(t) = 0
(1)
k−1
k
k−1
dt
dt
dt
where Ai ∈ Rn×n for i = 0, 1, 2, . . . , k and Ak is a nonsingular matrix.
Assuming the solution of (1) is of the form v(t) = xeλt , using separation
of variables, (1) leads to the higher order polynomial eigenvalue problem
P (λ)x = 0

(2)

where P (λ) = λk Ak + λk−1Ak−1 + · · · + λA1 + A0 ∈ Rn×n [λ] is known as
matrix polynomial of degree k. The comprehensive theory and application
of the matrix polynomial is discussed in the classic reference [24].
A matrix polynomial P (λ) is regular when P (λ) is square and the scalar
polynomial det(P (λ)) has at least one nonzero coefficient. Otherwise, P (λ)
is said to be singular. We assume the matrix polynomial P (λ) is regular
throughout this paper. The roots of det(P (λ)) = 0 are the eigenvalues of
the matrix polynomial P (λ). The vectors y 6= 0 and z 6= 0 are corresponding left and right eigenvectors satisfying z H P (λ) = 0 and P (λ)y = 0 where
z H denotes the conjugate transpose of z. If the matrix Ak is nonsingular,
then the matrix polynomial P (λ) has kn finite eigenvalues and eigenvectors.
The kn eigenvalues of P (λ) are either real numbers or if not, are complex
conjugate pairs. The polynomial eigenvalue problem concerns with determining the eigenvalues and corresponding eigenvectors of the matrix polynomial
P (λ). This problem arises in many practical situations, for instance, vibration analysis of structural mechanical and acoustic system, electrical circuit
simulation, fluid mechanics, etc [13, 21]. This problem is well studied in
the literature and a lot of literature exists addressing the ways to solve the
polynomial problem (see [7, 13, 30, 46] and the references therein).
Mostly, matrix polynomial arising from practical applications are often inherently structured. For example, they are all symmetric [8], skew-symmetric
[19], they alternate between symmetric and skew-symmetric [33], symmetric
tridiagonal [2], etc. Also, pentadiagonal matrices occur in the discretization
of the fourth-order differential systems [22]. Generally, these matrices Ai for
i = 0, 1, 2, . . . , k are linearly structured matrices [27]. A matrix polynomial
P (λ) in which the coefficient matrices are linearly structured, is known as
linearly structured matrix polynomial. Since the matrix Ak is often diagonal
2

and positive definite in various applications, we assume, without loss of generality, that the leading coefficient Ak is an identity matrix. In this case, the
matrix polynomial is referred to as a monic matrix polynomial of degree k.
The polynomial inverse eigenvalue problem
addresses the conPk (PIEP)
i
struction of a matrix polynomial P (λ) =
λ
A
∈
Rn×n [λ] from the
i
i=0
given eigenvalues and associated eigenvectors. PIEP arises in many applications where parameters of a certain physical system are to be determined
from the knowledge of its dynamical behavior. It has applications in the mechanical vibrations, aerospace engineering, molecular spectroscopy, particle
physics, geophysical applications, numerical analysis, differential equations
etc (see for instance [3, 4, 10, 11, 36, 38]).
Generally, a small number of eigenvalues and eigenvectors of the associated eigenvalue problem are available from the computation or measurement. Unfortunately there is no analytical tool available to evaluate the
entire eigendata of a large physical system. It should be mentioned that
when the problem is large, as in the case with the most engineering applications, state of art computational methods are capable of computing a very
few eigenvalues and associated eigenvectors. Therefore, it might be more
sensible to solve the polynomial inverse eigenvalue problem when only a few
measured eigenvalues and associated eigenvectors are available.
P
The construction of the matrix polynomial P (λ) = ki=0 λi Ai ∈ Rn×n [λ]
using the partially described eigendata is known as the partial polynomial inverse eigenvalue problem(PPIEP). In view of practical applications, it might
be more realistic to solve PPIEP with these structure constraints on the coefficient matrices. This problem is termed as the structured partial polynomial inverse eigenvalue problem(LPPIEP). The structure constraint imposes
a great challenge for solving this problem.
The inverse eigenvalue problem (IEP) for linear and quadratic matrix
polynomial have been well studied in the literature since the 1970s (see [17]
the references therein). Some previous attempts at solving the inverse eigenvalue problem are listed in [1, 23, 25, 39, 40]. A large number of papers
have been published on the linear inverse eigenvalue problem [20, 41, 44].
An excellent review of this area can be found in the classic reference [10].
Special attention is paid to the quadratic inverse eigenvalue problem(QIEP)
(see[2, 8, 12, 15, 16, 26, 28, 29, 43, 48]). Most of the papers solve QIEP for
the symmetric structure (see[48, 8, 26]) and symmetric tridiagonal structure
(see[2, 43]). The quadratic inverse eigenvalue problem is considered in the
context of solving the finite element model updating problem [21, 34, 35, 42]
3

and eigenstructure assignment problem [14, 37].
Some earlier attempts at solving the higher order PIEP are listed in
[3, 5, 18, 32]. Also, IEP for the matrix polynomial of degree k is considered in the context of solutions of active vibration control (see[9, 31, 45, 47]).
Most significant contributions to the solution of the higher order PIEP have
been made in [5, 18]. In [5], higher order PIEP for the T -Alternating and
T -Palindromic matrix polynomials of degree k are considered. These results
are most phenomenal so far on the solution of higher order structured PIEP.
In [18], authors mention an important open problem in this area, namely, the
inverse eigenvalue problems for structured matrix polynomials such as symmetric, skew-symmetric matrix polynomials, etc. In this paper, we attempt
at addressing this open problem providing the solution of PIEP.
Throughout this paper, we shall adopt the following notations. A ⊗ B
denotes the Kronecker product of the two matrices A and B. Also, Vec(A)
denotes the vectorization of the matrix A. kAkF and kAk2 denote the Frobenius norm and 2-norm of the matrix A respectively. L denotes the real linear
subspaces of Rn×n representing the linearly structured matrices. A† is the
Moore Penrose pseudoinverse of A. In denotes the identity matrix of size
n × n. Also, ei is the ith row of Ik for 1 ≤ i ≤ k.
Problem Statement 1.1. LPPIEP: Given two positive integers k and n,
a set of partial eigenpairs (λj , φj )m
j=1 (where 1 ≤ m ≤ kn), construct a monic
Pk−1 i
k
matrix polynomial P (λ) = λ In + i=0
λ Ai ∈ Rn×n [λ] of degree k in such a
way that matrices Ai ∈ L are symmetric for i = 0, 1, 2, . . . , (k − 1) and P (λ)
has the specified set (λj , φj )m
j=1 as its eigenpairs.
Contributions
In this paper, we consider the linearly structured partial polynomial inverse eigenvalue problem for the monic matrix polynomial of arbitrary degree
k. The authors believe that this problem, in its full generality, has not been
addressed earlier in the literature. Our results solve some open problems in
the theory of polynomial inverse eigenvalue problem (see [18]).
In particular, key contributions made in this paper are listed below:
• The proposed method is capable to solve LPPIEP using a set of m (
1 ≤ m ≤ kn) eigenpairs without imposing any restrictions on it, unlike
some instances in the past where certain restrictions on m are imposed
(see [2, 8, 48]) for computing the solution of inverse eigenvalue problem
in the case of quadratic matrix polynomial.
4

• The proposed method is capable to solve LPPIEP for a monic matrix
polynomial of arbitrary degree k.
• We derive some necessary and sufficient conditions on the eigendata for
the existence of solution of this problem.
• We completely characterize the class of solutions of this problem and
present the explicit expression of the solution.
Real-Form Representations of Eigenvalues and Eigenvectors
We assume that the m eigenvalues of a matrix polynomial are given of
which t are complex conjugate pairs and remaining m − 2t are real. Also,
complex eigenvalues are αj ± iβj for j = 1, 2, . . . , t and real eigenvalues
are e2t+1 , e2t+2 , . . . , em . Eigenvectors corresponding to the complex eigenvalues are uj ± ivj and eigenvectors corresponding to the real eigenvalues are
φ2t+1 , φ2t+2 , . . . φm .
We relate this pair of complex eigenvalues with a matrix Ej ∈ R2×2 given
by


αj βj
Ej =
.
−βj αj
Thus given a set of m eigenvalues, we relate these numbers with a real blockdiagonal matrix E ∈ Rm×m of the following form
E = diag(E1 , E2 , E3 , . . . , Et , e2t+1 , . . . , em ).

(3)

Then E is the real-form matrix representation of these m eigenvalues in real
form. Similarly, for a set of m eigenvectors a real-form matrix representation
is given by


X = u1 v1 . . . ut vt φ2t+1 . . . φm ∈ Rn×m .
(4)

Thus the pair (X, E) is a real matrix eigenpair of the matrix polynomial of
degree k, then it satisfies
k
X
Ai XE i = 0.
(5)
i=0

This relation is known as eigenvalue eigenvector relation for the matrix polynomial of degree k.

5

1.1. Linearly structured matrices and its structure specifications
Linearly structured matrix is a linear combinations of sub structured
matrices. Let A ∈ L be a linearly structured matrix of the form
A=

r
X

Sℓ αℓ

(6)

ℓ=1

where α1 , α2 , . . . αr are the structure parameters, r is the dimension and {Sℓ ∈
Rn×n : ℓ = 1, 2, . . . r} is a standard basis of L. Here [ α1 α2 α3 α4 ··· αr−1 αr ]T
is the coordinate vector of A w.r.t the above standard basis.
Matrix A is the linear combinations of the sub structured matrices Sℓ for
ℓ = 1, 2, . . . r.
We give some examples of linearly structured matrices in the table given
below.

6

Table 1: Linearly structured matrices

Linearly structured matrix
Symmetric
Skew symmetric
Tridiagonal
Symmetric tridiagonal
Pentagonal
Hankel
Toeplitz

Dimension r
n(n+1)
2
n(n−1)
2

3n − 2
2n − 1
5n − 6
2n − 1
2n − 1

2. Solution of LPPIEP
In this section, we obtain the solution of LPPIEP from the eigenvalueeigenvector relation for monic matrix polynomial of degree k which is given
by
k−1
X
Ai XE i = −XE k
(7)
i=0

where X ∈ Rn×m and E ∈ Rm×m .
It is clear that (7) is a nonhomogenous linear system of nm equations.
Therefore, the solution of LPPIEP is obtained by computing the linearly
structured solution Ai of (7).
We now discuss an important concept of vectorization of a matrix which
will be used to derive the solution of LPPIEP.

Vectorization of a linearly structured matrix
Vectorization of a matrix A ∈ L, is denoted by Vec(A) and is defined as
2
a vector in Rn ×1 obtained by stacking the columns of the matrix A on top
of one another.
Define the vector Vec1 (A) as
Vec1 (A) = [ α1
We define the matrix P ∈ Rn

α2 α3 α4 ··· αr−1 αr ]T

2 ×r

.

as

P = [Vec(S1 ) Vec(S2 ) · · · Vec(Sr )]

7

(8)

where {Sℓ ∈ Rn×n : ℓ = 1, 2, . . . , r} is a standard basis of L such that Vec1 (A)
is the coordinate vector of A ∈ L w.r.t the above basis.
It is easy to see that Vec(A) and Vec1 (A) are related through the matrix
P as:
Vec(A) = P Vec1 (A)
(9)
Example 2.1. Consider the symmetric matrix(linearly structured) A ∈ R3×3
as


4 2 8
A =  2 7 9
8 9 5

Then Vec(A) ∈ R9×1 and Vec1 (A) ∈ R5×1 are given by


T
Vec(A) = 4 2 8 2 7 9 8 9 5

T
Vec1 (A) = 4 2 8 7 9 5

Let, {Sℓ ∈ Rn×n : ℓ = 1, 2, . . . , 5} be the standard basis of the space of all
symmetric matrices where
i
i
h
i
h
h
0 0 0
0 1 0
1 0 0
S1 = 0 0 0 , S2 = 1 0 0 , S3 = 0 1 0 ,
h0 0 0i
h0 0 0i
h0 0 0i
0 0 1
0 0 0
0 0 0
S4 = 0 0 0 , S5 = 0 0 1 , S6 = 0 0 0 .
1 0 0

0 1 0

The
P ∈ R9×6 is given by
 1 matrix
0 0 0 0 0

0 0 1

0 1 0 0 0 0

 00 01 00 10 00 00 

P =
 00 00 10 00 01 00 
0 0 0 1 0 0
0 0 0 0 1 0
0 0 0 0 0 1

For the symmetric matrix A, it is straightforward to verify that (9) holds.

Existence of a solution of LPPIEP
In this subsection, we derive a necessary and sufficient condition on the
eigendata for the existence of a solution of LPPIEP. Applying vectorization

8

operation on (7), we get,
!
k−1
X

Vec
Ai XE i = −Vec XE k
⇒
⇒

i=0
k−1
X

i=0
k−1
X
i=0



(XE i )T ⊗ I Vec (Ai ) = −Vec XE k



(XE i )T ⊗ I P Vec1 (Ai ) = −Vec XE k
. . . using (9)

⇒ [ ((XE k−1 )T ⊗I)P

((XE k−2 )T ⊗I)P ··· (X T ⊗I)P

⇒ Ux = b

 Vec1 (A

]

k−1 )
Vec1 (Ak−2 )

..
.

Vec1 (A0 )



 = −Vec(XE k )

(10)

where
U = [ ((XE k−1 )T ⊗In )P
 Vec1 (A ) 

((XE k−2 )T ⊗In )P ··· (X T ⊗In )P

] ∈ Rmn×kr ,

(11)

k−1

x=

Vec1 (Ak−2 )

..
.

Vec1 (A0 )

 ∈ Rkr×1 ,

b = Vec(−XE k ) ∈ Rmn×1 .

(12)
(13)

Above system of linear equations (10) has mn equations and kr unknowns.
We now state a necessary and sufficient condition for the existence of the
solution of a system of linear equations in the following theorem.
Theorem 2.2. [6] Let Ψζ = η be a system of linear equations where Ψ ∈
Rp×q and η ∈ Rp . Then Ψζ = η is consistent if and only if ΨΨ† η = η where
Ψ† is the generalized inverse of Ψ ∈ Rp×q . General solution of Ψζ = η is
given by
ζ = Ψ† η + (Iq − Ψ† Ψ)y
where y ∈ Rq×1 is an arbitrary vector. Moreover, Ψζ = η has a unique
solution if and only if Ψ† Ψ = Iq , ΨΨ† η = η and the unique solution is given
by
ζ = Ψ† η

9

First, we transform the eigenvalue eigenvector relation (7) to a system of
linear equations Ux = v. Therefore, determination of solution of LPPIEP
is equivalent to finding the solution of the system of linear equations in
(10). Thus, necessary and sufficient conditions for the existence of solution
of LPPIEP is same as the system of linear equation Ux = v. We now present
the main theorem to find a necessary and sufficient condition for the existence
of the solution of LPPIEP.
Theorem 2.3. Let an arbitrary matrix eigenpair (E, X) ∈ Rm×m × Rn×m
be given as in Equations (3) and (4). Then LPPIEP has a solution if and
only if UU † b = b where U and b are defined by (11) and (13). In that case
expression of Ai ∈ L for i = 0, 1, 2, . . . , (k − 1) are given by

Vec(Ai ) = P (ek−i ⊗ Ir ) U † b + (Ikr − U † U)y ,
(14)

where y ∈ Rkr×1 is an arbitrary vector. Moreover, LPPIEP has a unique
solution if and only if UU † b = b, U † U = Ikr . Explicit expressions of Ai ∈ L
are given below as

Vec(Ai ) = P (ek−i ⊗ Ir )U † b .
(15)

Proof. Computing the solution of LPPIEP is equivalent to solving the system
of linear equations Ux = b where U and b are defined by (11) and (13).
Necessary and sufficient condition for the existence of the solution of Ux = b
is UU † b = b and general solution is given by
x = U † b + (Ikr − U † U)y.

(16)

where y ∈ Rkr×1 is an arbitrary vector. Note that, x is of the form as in (12)
and Vec1 (Ak−1 ) can be obtained from x as follows.


Vec1 (Ak−1)





Vec1 (Ak−2)
Ir Θ Θ Θ . . . Θ x = Ir Θ Θ Θ . . . Θ 
 = Vec1 (Ak−1 )
..


.
Vec1 (A0 )
where Θ ∈ Rr×r be a zero matrix.

⇒ Vec1 (Ak−1 ) = (e1 ⊗ Ir )x
10

(17)

Similarly, Vec1 (Ai ) are given by

Vec1 (Ai ) = ek−i ⊗ Ir x for i = 0, 1, 2, 3, . . . , (k − 1)

(18)

Substituting the expression of x in (18), Vec1 (Ai ) can be obtained as in the
following as :

Vec1 (Ai ) = (ek−i ⊗ Ir ) U † b + (Ikr − U † U)y
(19)
General solution Ai is obtained from the vector Vec1 (Ai ) using the relation
(9) as
Vec(Ai ) = P Vec1 (Ai ).

(20)

Substituting the expressions of Vec1 (Ai ) in the above equations, we get

Vec(Ai ) = P (ek−i ⊗ Ir ) U † b + (Ikr − U † U)y .

Further, Ux = b has a unique solution if and only if UU † b = b and U † U = Ikr .
Explicitly, the unique solution x is given by x = U † b (see Theorem 2.2). If
Ux = b has a unique solution then LPPIEP has a unique solution Ai . In
that case, matrices Ai are given by uniquely as

Vec(Ai ) = P (ek−i ⊗ Ir )U † b .
(21)

Remark 2.4. We considered the standard ordered basis of L to represent
any linearly structured matrix and we construct the matrix P using this basis.
However, any other ordered basis can be chosen to construct the matrix P .
Result of Theorem (2.3) is also true if we choose any other ordered basis.
Construction of symmetric non-monic matrix polynomials
In Theorem 2.3, we construct the monic linearly structured matrix polynomial using partial eigendata. Now we generalize this result to find the
symmetric non-monic polynomials with positive definite leading coefficients
using similarity transformation.
Consider the matrix polynomial P (λ) = λk Ak + λk−1 Ak−1 + · · · + λA1 +
A0 ∈ Rn×n [λ] where Ai are symmetric and Ak is positive definite matrix.
1/2
Let, Ak be the positive definite square-root of Ak and modify the problem
11

1/2

by writing ξ = Ak x and observe that Eq. (2) reduces to the monic problem
as
−1/2

(λk Ak

−1/2

Ak Ak

−1/2

+ λk−1 Ak

−1/2

Ak−1 Ak

−1/2

+ · · · + Ak

−1/2

A0 Ak

1/2

)Ak x = 0

⇒ (λk I + λk−1Âk−1 + · · · + λÂ1 + Â0 )ξ = 0
−1/2

where Âi = Ak

−1/2

Ai Ak

are symmetric matrices.

3. Numerical Example
In this section, we give three numerical examples to illustrate the validity
of our proposed approach.
Example 3.1. Consider the mass-spring system having three degrees of freedom with the following target set of eigenvalues −1.3064 ± 0.5436i, −0.2582.
The eigenvalue and the eigenvector matrices are given by
h −0.0406 −0.4699 0.4231 i
X = −0.4504 −0.2542 0.3510
0.7128 −0.0438 −0.8353

E=

0.5436
0
−0.5436 −1.3064
0
0
0
−0.2582

h −1.3064

i

For this mass-spring system m = 3 and n = 3. Now we construct the
monic symmetric matrix polynomial P (λ) = λ2 I3 + λA1 + A0 of degree 2.
We take the standard basis of the space
of
i matricesh {Sℓ i∈
i all symmetric
h
h
0 0 1
0 1 0
1 0 0
n×n
R
: ℓ = 1, 2, . . . , 6} where S1 = 0 0 0 , S2 = 1 0 0 , S3 = 0 0 0 ,
1 0 0
0 0 0
i 000
i
h
i
h
h
0 0 0
0 0 0
0 0 0
S4 = 0 1 0 , S5 = 0 0 1 ,S6 = 0 0 0 .
0 0 1
0 1 0
0 0 0
Now, we construct symmetric matrices A0 and A1 from the above partial
eigendata. Here, UU † b = b and U † U 6= I10 where U ∈ R12×10 and b ∈ R12×1 .
Equation (10) has an infinite number of solutions. Therefore, LPPIEP has
an infinite number of solutions. Using Theorem 2.3, symmetric matrices A0
and A1 are given by


4.2248 −0.0174 2.4278
A0 = −0.0174 1.8133 0.2806
2.4278
0.2806 1.5618


2.3283 1.2405
2.7130
A1 = 1.2405 0.1189 −1.2603
2.7130 −1.2603 1.9321
12

Next, we study the effect of choosing different ordered basis of the space
of all symmetric matrices to the solution.
basish {Sℓ i∈
i
iUsing theh ordered
h
1 0 1
0 1 0
1 0 0
n×n
R
: ℓ = 1, 2, . . . , 6} where S1 = 0 0 0 , S2 = 1 0 0 , S3 = 0 0 0 ,
1 0 0
0 0 0
i 000
i
h
i
h
h
0 0 0
0 0 0
0 0 0
0
0
1
0
0
1
0
1
0
, we construct the matrices P and D.
,S6 =
, S5 =
S4 =
0 0 0

0 1 0

0 1 1

Here, UU † b = b and U † U 6= I10 where U ∈ R12×10 and b ∈ R12×1 . We also
get the same symmetric matrices A0 and A1 as above.
Therefore, if we take two different basis of the space of all symmetric
matrices for this example, we get same result.

Example 3.2. Consider the mass-spring system having four degrees of freedom with the following target set of eigenvalues 0.5950 + 9.5092i, 0.5950 −
9.5092i. The eigenvalue and the eigenvector matrices are given by


−0.2164 −0.6066
−0.5435 −0.0169

X =
−0.3518 0.2746 
−0.1845 0.2374


0.5950 9.5092
E=
−9.5092 0.5950

For this mass-spring system m = 2 and n = 4.
Now we construct the monic skew symmetric matrix polynomial P (λ) =
λ2 I4 + λA1 + A0 of degree 2.
We take the standard basis of the space of all skew
symmetric
matrices



0 1 0 0

0 0 0
{Sℓ ∈ Rn×n : ℓ = 1, 2, . . . , 6} where S1 = −1
0 0 0 0 , S2 =
0 0 0 0
 0 0 0 0  0 0 0 0 0 0 0 0 
 0 0 0 1
0 0 0 0
0 0 1 0
0 0 0 1
0 0 0 0
0 0 0 0 , S4 = 0 −1 0 0 , S5 = 0 0 0 0 , S6 = 0 0 0 1 .
−1 0 0 0

0 0 0 0

0
0
−1
0

0
0
0
0

1
0
0
0

0
0
0
0

, S3 =

0 0 −1 0

0 −1 0 0

Here, UU † b = b and U † U 6= I12 where U ∈ R8×12 and b ∈ R8×1 . Equation
(10) has an infinite number of solutions. Therefore, LPPIEP has an infinite
number of solutions. One of the solution x of Equation (10) is given by
x = [6.1761 5.1682 3.0933 2.9398 2.5033 0.6224 3.7036 3.0992 1.8550
1.7629 1.5011 0.3732].T
Using Theorem 2.3 matrices A0 and A1 are given by
 0

3.7036 3.0992 1.8550
−3.7036
0
1.7629 1.5011
A0 = −3.0992 −1.7629 0 0.3732
−1.8550 −1.5011 −0.3732

13

0

A1 =



0
6.1761 5.1682 3.0933
−6.1761
0
2.9398 2.5033
−5.1682 −2.9398
0
0.6224
−3.0933 −2.5033 −0.6224
0



Constructed matrices A0 and A1 are skew symmetric and they satisfy the
eigenvalue and eigenvector relation XE 2 + A1 XE + A0 X = 0 as kXE 2 +
A1 XE + A0 Xk2F = 8.0185 × 10−6 . Total computational time for running this
program in a system with 4Gb ram is 0.078 seconds. Therefore, we successfully reproduced the eigenvalues and eigenvectors from the constructed
monic skew symmetric quadratic matrix polynomial.
Next, we study the effect of choosing different ordered basis of the space
of all skew symmetric matrices to the solution.
n×n
basis

: ℓ = 1, 2, . . . ,6} where
 Using the ordered
 {Sℓ ∈ R
 S1 =
0
−1
2
0

1 −2
0 0
0 0
0 0
0
S6 = 00
0

0
0
0
0
0
0
0
0

0 0 1 0

0 0 0 1

0 0 0 0
0 0 0 0
, S2 = −1
0 0 0 , S3 =
0 0 0 0 , S4 =
−1
0 0 0
0
0
0
0

0 0
0 0
0 1 , matrices A0 and A1 are given by

0
0
0
0

0
0
−1
0

0
1
0
0

0
0
0
0

, S5 =

0
0
0
0

0
0
0
−1

0
0
0
0

0
1
0
0

−1 0

A0 =



0
−1.2396 6.4982 2.0008
1.2396
0
4.0440 3.6581
−6.4982 −4.0440
0
0.3732
−2.0008 −3.6581 −0.3732
0



A1 =



0
6.1815 5.1892 3.6862
−6.1815
0
2.7181 1.7956
−5.1892 −2.7181
0
1.3404
−3.6862 −1.7956 −1.3404
0



If we take two different basis of the space of all skew symmetric matrices for
this example, we get different skew symmetric matrices.
Example 3.3. Consider a 50×50 triplet (I50 , A1 , A0 ) where symmetric tridiagonal matrices A0 , A1 are generated using the MATLAB as
A1 = diag(a1 ) + diag(b1 , −1) + diag(b1 , 1)
A0 = diag(a2 ) + diag(b2 , −1) + diag(b2 , 1)
where a1 =[10 20 6 8 40 10 50 60 3 70 30 7 9 4 80 4.2 6.5 8.1 1.2 6.2 2.7 4.3
3.2 2.6 14 2.9 13 12.4 4.6 14.2 8 1.9 2.4 1.6 25 10.84 22.3 42.62 54.24 26.24 1
4 0.5 0.3 7 3 8 0.9 5 0.2],
b1 =[ 2.8 1.2 36 8 4 16 2 1.2 28 12 32 3.6 20 0.8 1.8 0.96 3.92 3.24 1.04 6 0.9
3 0.4 4 0.2 2 0.5 0.6 0.8 0.3 2 1 6 0.9 3 0.4 4 0.2 2 5 2 1 0.7 8 0.2 0.6 7 0.4 7],
a2 =[ 5.6 2.4 16 8 48 7.2 24 3.2 32 1.6 16 4 4.8 6.4 72 80168 328 432 200 17.6
26.4 23.2 17.6 96 19.2 84 75.2 35.6 85.6 52 12.4 15.6 11.2 168 85.04 175.8
337.72 433.44 207.44 0.4 4 0.2 2 0.5 0.6 0.8 9 10 21],
14

,

b2 =[3.2 3.6 16 20 8 4 2.8 32 0.8 2.4 28 1.6 28 2 76 96 112 136 204 4 0.2 2 0.5
0.6 0.7 0.3 2 1 6 8 16 4.8 6.4 32 8 40 48 2.4 56 24 5.6 7.2 3.2 64 3.36 5.2
6.48 0.96 4.96].
We compute all 100 eigenpairs of P (λ) = λ2 I50 + λA1 + A0 . Now, we consider the eigenvalues −1.5564±0.0232i, −2.5036, −2.1202 and corresponding
eigenvectors. We construct the matrix eigenpairs (E, X) ∈ R4×4 × R50×4 using the given four eigenvalues and corresponding eigenvectors. Here, n = 50,
k = 2 and m = 4. We construct the matrices U ∈ R200×198 , b ∈ R200×1 and
observe that UU † b = b and U † U = I198 . Therefore, LPPIEP has a unique
solution. We construct the symmetric tridiagonal matrices A0 ∈ R50×50 and
A1 ∈ R50×50 using Theorem 2.3 and they satisfy the eigenvalue and eigenvector relation XE 2 +A1 XE +A0 X = 0 as kXE 2 +A1 XE +A0 XkF = 7.1×10−8 .
Total computational time for running this program in a system with 4 GB
ram is 1.158 seconds. Therefore, we successfully reproduced the eigenvalues
and eigenvectors from the constructed monic symmetric tridiagonal quadratic
matrix polynomial.
Similarly for various cases of partial eigendata where m = 2, 6 and 10, we
construct the matrix eigenpairs (E, X) ∈ Rm×m × R50×m . We construct the
symmetric tridiagonal matrices A0 ∈ R50×50 and A1 ∈ R50×50 using Theorem
2.3. The numerical results are summarized in the following table.
Table 2: Summary of numerical results

n

m

Conditions Satisfied

50
50
50
50

2
4
6
10

UU † b = b,U † U 6= I198
UU † b = b, U † U = I198
UU † b = b, U † U = I198
UU † b = b, U † U = I198

kXE 2 + A1 XE
A0 XkF
2.5 × 10−11
7.1 × 10−8
3.6 × 10−8
5.74 × 10−6

+ Solution Time(s)
Infinite
Unique
Unique
Unique

1.12
1.15
1.22
1.29

s
s
s
s

4. Conclusions
In this paper, we have studied the linearly structured partial polynomial
inverse eigenvalue problem. A necessary and sufficient condition for the existence of solution to this problem is derived in this paper. Additionally, we
present an analytical expression of the solution. Further, we discuss the sensitivity of the solution when the eigendata is not exactly known. Thus, this
15

paper presents a complete theory on the structured solution of the inverse
eigenvalue problem for a monic matrix polynomial of arbitrary degree.
References
References
[1] Q. Al-Hassan. An inverse eigenvalue problem for general tridiagonal matrices. International Journal of Contemporary Mathematical Sciences,
4(13):625–634, 2009.
[2] Z. J. Bai. Symmetric tridiagonal inverse quadratic eigenvalue problems
with partial eigendata. Inverse Problems, 24(1):015005, 2007.
[3] V. Barcilon. On the solution of inverse eigenvalue problems of high
orders. Geophysical Journal International, 39(1):143–154, 1974.
[4] D. C. Barnes and R. Knobel. The inverse eigenvalue problem with finite
data for partial differential equations. SIAM Journal on Mathematical
Analysis, 26(3):616–632, 1995.
[5] L. Batzke and C. Mehl. On the inverse eigenvalue problem for Talternating and T-palindromic matrix polynomials. Linear Algebra and
its Applications, 452:172–191, 2014.
[6] A. Ben-Israel and T. NE. Greville. Generalized Inverses: Theory and
Applications, volume 15. Springer Science & Business Media, 2003.
[7] M. Berhanu. The polynomial eigenvalue problem. PhD thesis, University
of Manchester, 2005.
[8] Y. F. Cai, Y. C. Kuo, W. W. Lin, and S. F. Xu. Solutions to a
quadratic inverse eigenvalue problem. Linear Algebra and its Applications, 430(5):1590–1606, 2009.
[9] Y. F. Cai, J. Qian, and S. F. Xu. Robust partial pole assignment problem
for high order control systems. Automatica, 48(7):1462–1466, 2012.
[10] M. T. Chu. Inverse eigenvalue problems. SIAM review, 40(1):1–39, 1998.
[11] M. T. Chu. Inverse eigenvalue problems: theory and applications. A
series of lectures presented at IRMA, CRN, Bari, Italy, 2001.
16

[12] M. T. Chu, Y. C. Kuo, and W. W. Lin. On inverse quadratic eigenvalue
problems with partially prescribed eigenstructure. SIAM Journal on
Matrix Analysis and Applications, 25(4):995–1020, 2004.
[13] B. N. Datta. Numerical Linear Algebra and Applications. SIAM, 2010.
[14] B. N. Datta, S. Elhay, Y. Ram, and D. Sarkissian. Partial eigenstructure
assignment for the quadratic pencil. Journal of Sound and Vibration,
230(1):101–110, 2000.
[15] B. N. Datta and D. Sarkissian. Theory and computations of some inverse
eigenvalue problems for the quadratic pencil. Contemporary Mathematics, 280:221–240, 2001.
[16] B. N. Datta and V. Sokolov. A solution of the affine quadratic inverse
eigenvalue problem. Linear Algebra and its Applications, 434(7):1745–
1760, 2011.
[17] E. M. De Sa. Imbedding conditions for λ-matrices. Linear Algebra and
its Applications, 24:33–50, 1979.
[18] F. De Terán, F. M. Dopico, and P. Van Dooren. Matrix polynomials
with completely prescribed eigenstructure. SIAM Journal on Matrix
Analysis and Applications, 36(1):302–328, 2015.
[19] A. Dmytryshyn. Skew-Symmetric Matrix Pencils: Stratification Theory
and Tools. PhD thesis, Umeå universitet, 2014.
[20] S. Elhay and Y. M. Ram. An affine inverse eigenvalue problem. Inverse
Problems, 18(2):455, 2002.
[21] M. Friswell and J. Mottershead. Finite Element Model Updating in
Structural Dynamics, volume 38. Springer Science & Business Media,
1995.
[22] G. M. L. Gladwell. Inverse problems in vibration. Applied Mechanics
Reviews, 39(7):1013–1018, 1986.
[23] G. M. L. Gladwell, T. H. Jones, and N. B. Willms. A test matrix for
an inverse eigenvalue problem. Journal of Applied Mathematics, 2014,
2014.
17

[24] I. Gohberg, P Lancaster, and L. Rodman. Matrix Polynomials. Springer,
2005.
[25] O. H. Hald. Inverse eigenvalue problems for jacobi matrices. Linear
Algebra and Its Applications, 14(1):63–85, 1976.
[26] Y. C. Kuo, W.W. Lin, and S. F. Xu. Solutions of the partially described
inverse quadratic eigenvalue problem. SIAM Journal on Matrix Analysis
and Applications, 29(1):33–53, 2006.
[27] P. Lancaster. Lambda Matrices and Vibrating Systems. Courier Corporation, 2002.
[28] P. Lancaster. Inverse spectral problems for semisimple damped vibrating
systems. SIAM Journal on Matrix Analysis and Applications, 29(1):279–
301, 2007.
[29] P. Lancaster and I. Zaballa. On the inverse symmetric quadratic eigenvalue problem. SIAM Journal on Matrix Analysis and Applications,
35(1):254–278, 2014.
[30] D. S. Mackey, N. Mackey, C. Mehl, and V. Mehrmann. Structured polynomial eigenvalue problems: Good vibrations from good linearizations.
SIAM Journal on Matrix Analysis and Applications, 28(4):1029–1051,
2006.
[31] X. Mao and H. Dai. Minimum norm partial eigenvalue assignment of
high order linear system with no spill-over. Linear Algebra and its Applications, 438(5):2136–2154, 2013.
[32] J. R. McLaughlin. An inverse eigenvalue problem of order four. SIAM
Journal on Mathematical Analysis, 7(5):646–661, 1976.
[33] V. Mehrmann and D. Watkins. Polynomial eigenvalue problems with
hamiltonian structure. Electronic Transactions on Numerical Analysis,
13:106–118, 2002.
[34] J. Moreno, B. N. Datta, and M. Raydan. A symmetry preserving alternating projection method for matrix model updating. Mechanical
Systems and Signal Processing, 23(6):1784–1791, 2009.

18

[35] J. Mottershead and M. Friswell. Model updating in structural dynamics:
a survey. Journal of Sound and Vibration, 167(2):347–375, 1993.
[36] M. Müller. An inverse eigenvalue problem: Computing B-stable
Runge-Kutta methods having real poles. BIT Numerical Mathematics,
32(4):676–688, 1992.
[37] N. K. Nichols and J. Kautsky. Robust eigenstructure assignment in
quadratic matrix polynomials: Nonsingular case. SIAM Journal on Matrix Analysis and Applications, 23(1):77–102, 2001.
[38] R. L. Parker and K. A. Whaler. Numerical methods for establishing
solutions to the inverse problem of electromagnetic induction. Journal
of Geophysical Research: Solid Earth, 86(B10):9574–9584, 1981.
[39] B. Parlett, F. M. Dopico, and C. Ferreira. The inverse eigenvector problem for real tridiagonal matrices. SIAM Journal on Matrix Analysis and
Applications, 37(2):577–597, 2016.
[40] H. Pickmann, R. L. Soto, J. Egana, and M. Salas. An inverse eigenvalue
problem for symmetrical tridiagonal matrices. Computers & Mathematics with Applications, 54(5):699–708, 2007.
[41] S. Rakshit and S. R. Khare. Symmetric band structure preserving finite element model updating with no spillover. Mechanical Systems and
Signal Processing, 116:415–431, 2019.
[42] S. Rakshit, S. R. Khare, and B. N. Datta. Symmetric tridiagonal structure preserving finite element model updating problem for the quadratic
model. Mechanical Systems and Signal Processing, 107:278–290, 2018.
[43] Y. Ram and S. Elba. An inverse eigenvalue problem for the symmetric tridiagonal quadratic pencil with application to damped oscillatory
systems. SIAM Journal on Applied Mathematics, 56(1):232–244, 1996.
[44] Y. M. Ram. Inverse eigenvalue problem for a modified vibrating system.
SIAM Journal on Applied Mathematics, 53(6):1762–1775, 1993.
[45] M. A. Ramadan and E. A. El-Sayed. Partial eigenvalue assignment problem of high order control systems using orthogonality relations. Computers & Mathematics with Applications, 59(6):1918–1928, 2010.
19

[46] F. Tisseur and K. Meerbergen. The quadratic eigenvalue problem. SIAM
Review, 43(2):235–286, 2001.
[47] X. T. Wang and L. Zhang. Partial eigenvalue assignment of high
order systems with time delay. Linear Algebra and its Applications,
438(5):2174–2187, 2013.
[48] Y. Yuan and H. Dai. On a class of inverse quadratic eigenvalue problem.
Journal of Computational and Applied Mathematics, 235(8):2662–2669,
2011.

20

