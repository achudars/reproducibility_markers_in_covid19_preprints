Distributed Resource Allocation with Binary Decisions via
Newton-like Neural Network Dynamics

arXiv:1911.02111v2 [math.OC] 27 Jul 2020

Tor Anderson 1

Sonia Martı́nez 1

Abstract
This paper aims to solve a distributed resource allocation problem with binary local constraints. The problem is formulated
as a binary program with a cost function defined by the summation of agent costs plus a global mismatch/penalty term. We
propose a modification of the Hopfield Neural Network (HNN) dynamics in order to solve this problem while incorporating a
novel Newton-like weighting factor. This addition lends itself to fast avoidance of saddle points, which the gradient-like HNN is
susceptible to. Turning to a multi-agent setting, we reformulate the problem and develop a distributed implementation of the
Newton-like dynamics. We show that if a local solution to the distributed reformulation is obtained, it is also a local solution
to the centralized problem. A main contribution of this work is to show that the probability of converging to a saddle point
of an appropriately defined energy function in both the centralized and distributed settings is zero under light assumptions.
Finally, we enlarge our algorithm with an annealing technique which gradually learns a feasible binary solution. Simulation
results demonstrate that the proposed methods are competitive with centralized greedy and SDP relaxation approaches in
terms of solution quality, while the main advantage of our approach is a significant improvement in runtime over the SDP
relaxation method and the distributed quality of implementation.
Key words: second-order methods; dynamical systems; distributed optimization; neural networks; binary optimization.

1

Introduction

There has been an explosion of literature surrounding
the design of distributed algorithms for convex optimization problems and how these pertain to the operation of
future power grids. A common assumption of these algorithms is the property of convexity, which lends itself to
provably optimal solutions which are scalable and fast.
However, some settings give rise to nonconvex decision
sets. For example, in an optimal power dispatch setting,
devices available for providing load-side frequency regulation such as HVAC systems, household appliances,
and manufacturing systems are often limited to discrete
on/off operational modes. It is even preferable to charge
populations of electric vehicles in a discrete on/off manner due to nonlinear battery chemistries. The available
tools in optimization for these nonconvex settings are
less mature, and when considering a distributed setting
in which devices act as agents that collectively compute
a solution over a sparse communication graph, the avail1

Tor Anderson and Sonia Martı́nez are with the Department of Mechanical and Aerospace Engineering, University
of California, San Diego, CA, USA. Email:
{tka001, soniamd}@eng.ucsd.edu. This research was supported by the Advanced Research Projects Agency - Energy under the NODES program, Cooperative Agreement
de-ar0000695.

Preprint submitted to Automatica

able tools are significantly less developed. With this in
mind, we are motivated to develop a scalable, fast approach for these binary settings which is amenable to a
distributed implementation.
Quadratic programs with nonconvex binary constraints
are known to be NP-hard in general, see [7, 22]. In this
paper, we consider a problem which is quite applicable
to the economic dispatch problem in power networks,
see [16,17,19] for three recent examples in microgrid environments. However, none of these examples address
devices with binary constraint sets. The binary problem
is, however, desirable to approach in a distributed context [36, 37]. Greedy algorithms [9] have been proposed
for binary programs, such as the well-known Traveling
Salesman Problem (TSP), but it is well documented that
these methods can greatly suffer in performance [15]
except in cases where the cost function is submodular [26, 32]. A more modern approach to solving optimization problems with a binary feasibility set is to cast
them as a semidefinite program (SDP) with a nonlinear
rank constraint, see [4, 29, 34] for some classical references or [23,35] for more recent work on the topic. By relaxing the rank constraint, a convex problem is obtained
whose solution can be shown to be equal to the optimal
dual value of the original problem, see e.g. [28]. However, it is necessary in these approaches to either impose
a single centralized coordinator to compute the solution

29 July 2020

greedy methods in terms of scalability.

and broadcast it to the actuators or agents, or schedule
computations, which suffers from scalability issues, privacy concerns, and does not enjoy the simpler and more
robust implementation of a distributed architecture in a
large network.

2

Preliminaries

This section establishes notation 2 and background concepts to be used throughout the paper.

Neuro-dynamic programming is a different paradigm
for addressing nonconvex problems with computational
tractability, see [3] for a broad reference. A neuralnetwork based method for binary programs was first
developed by Hopfield in [18], which was originally proposed in order to address TSPs. We refer to this method
from here on as a Hopfield Neural Network (HNN).
This method provided a completely different avenue for
approaching binary optimizations, and followup works
are found in [2, 20, 24, 33]. These works formalize and
expand the framework in which the HNN method is
applicable. However, these algorithms essentially implement a gradient-descent on an applicable nonconvex
energy function, which is susceptible to being slowed
down by convergence to saddle-points. There are avenues for Newton-like algorithms in nonconvex environments to address this issue, which incorporate some
treatment of the negative Hessian eigenvalues in order
to maintain a monotonic descent of the cost function,
see e.g. [10, 11]. A recently developed method employs
a Positive-definite Truncated inverse (PT-inverse) operation on the Hessian of a nonconvex energy or cost
function in order to define a nonconvex Newton-descent
direction [31], although the technique does not presently
address binary settings. Perhaps more importantly, all
variants of existing HNN methods and the aforementioned works for nonconvex Newton-like algorithms are
framed for centralized environments in which each agent
knows global information about the state of all other
agents, which is not scalable.

We refer the reader to [6] as a Graph Theory supplement.
One can define a Laplacian matrix L associated with a
graph G as follows:


j ∈ N i,
−1,
P
Lij = − k6=i Lik , i = j,

0,
otherwise,

where N i is the set of neighbors of node i. An immediate
property is that 0 is an eigenvalue of L associated with
the eigenvector 1n . It is simple iff G is connected.
Next, we introduce the Positive-definite Truncated inverse (PT-inverse) and its relevance to nonconvex Newton methods.
Definition 1 ([31]) (PT-inverse). Let A ∈ Rn×n be
a symmetric matrix with an orthonormal basis of eigenvectors Q ∈ Rn×n and diagonal matrix of eigenvalues
Λ ∈ Rn×n . Consider a constant m > 0 and define |Λ|m ∈
Rn×n by:

|Λii |,
(|Λ|m )ii =
m,

|Λii | ≥ m,
otherwise.

The PT-inverse of A with parameter m is defined by
(|A|m )−1 = Q⊤ (|Λ|m )−1 Q ≻ 0.

The contributions of this paper are threefold. We
start by considering a binary programming problem formulated as a summation of local costs plus a
squared global term. By leveraging a specific choice
for the cost functions, we adapt the setting to an
HNN framework. Then, we propose a novel modification of the dynamics with a PT-inverse of the Hessian
of an appropriate energy function to define centralized Newton-like Neural Network (NNN-c). We
prove a rigorous convergence result to a local minimizer,
thus excluding saddle-points, with probability one,
given some mild assumptions on the algorithm parameters and initial condition. Thirdly, we reformulate the
problem so that it is solvable via a distributed algorithm
by means of an auxiliary variable. We show that local
solutions of the distributed reformulation are equivalent to local solutions of the centralized one, and we
define a corresponding energy function and distributed
algorithm for which we show convergence to a local minimizer with probability one. Simulations validate that
our method is superior to SDP relaxation approaches
in terms of runtime and scalability and outperforms

The PT-inverse operation flips the sign on the negative eigenvalues of A and truncates near-zero eigenvalues to a (small) positive value m before conducting the
2
The set of real numbers, real positive numbers, real ndimensional vectors, and real n-by-m matrices are written as
R, R+ , Rn , and Rn×m , respectively. We denote by xi the ith
element of x ∈ Rn and Aij the element in the ith row and j th
column of A ∈ Rn×m . For a square matrix A, we denote by
A† the Moore-Penrose pseudoinverse of A. We use the shorthand 1n = (1, . . . , 1)⊤ ∈ Rn and 0n = (0, . . . , 0)⊤ ∈ Rn .
Cartesian products of sets are denoted by a superscript, for
example, {0, 1}n = {0, 1} × · · · × {0, 1}. The gradient of a
function f : Rn → R with respect to x ∈ Rn at x is denoted
by ∇x f (x) ∈ Rn , and the Hessian matrix of f at x is written as ∇xx f (x) ∈ Rn×n . We denote elementwise operations
on vectors x, y ∈ Rn as (xi yi )i = (x1 y1 , . . . , xn yn )⊤ , (xi )2i =
(x21 , . . . , x2n )⊤ , (c/xi )i = (c/x1 , . . . , c/xn )⊤ , log(xi )i =
(log(x1 ), . . . , log(xn ))⊤ , and (exi )i = (ex1 , . . . , exn )⊤ . The
notation diag(x) indicates the diagonal matrix with entries
given by elements of x, and B(x, η) denotes the closed ball
of radius η centered at x.

2

points are infeasible. With this in mind, we enlarge the
cost model by adopting the following:

inverse. Effectively, this generates a positive definite matrix bounded away from zero to be inverted, circumventing near-singular cases. In terms of computational complexity, it is on the order of standard eigendecomposition
(or more generally, singular value decomposition), which
is roughly O(n3 ) [27]. However, we note in Section 5 that
the matrix to be PT-inverted is diagonal, which is O(n).

Assumption 1 (Quadratic Cost Functions). The
local cost functions fi take the form
fi (xi ) =

The PT-inverse is useful for nonconvex Newton approaches [31] in the following sense: first, recall that
the Newton descent direction of f at x is computed as
− (∇xx f (x))−1 ∇x f (x). For f strictly convex, it holds
that ∇xx f (x) ≻ 0 and the Newton direction is well defined and decreases the cost. For (non-strictly) convex
or nonconvex cases, ∇xx f (x) will be singular, indefinite,
or negative definite. A PT-inverse operation remedies
these cases and preserves the descent quality of the
method. Additionally, saddle points are a primary concern for first-order methods in nonconvex settings [10],
and the Newton flavor endowed by the PT-inverse effectively performs a change of coordinates on saddles
with “slow” unstable manifolds compared to the stable
manifolds. We discuss this further in Section 4.
3

ai
ai b2i
(xi − bi )2 −
+ di ,
2
2

with ai , bi , di ∈ R.
Note that, for any value ci = fi (1) − fi (0), there exists a family of coefficients ai , bi such that (ai /2)(1 −
bi )2 − (ai /2)b2i = ci . Further, the constant terms ensure
fi (0) = di and fi (1) = ci + di . The design of ai , bi will
be discussed in Section 4.
The problem we aim to solve can now be formulated as:
P1:

min

x∈{0,1}n

f (x) =

n
X
i

fi (xi ) +

2
γ ⊤
p x − Pr .
2

Here, Pr ∈ R is a given reference value to be matched
by the total output p⊤ x of the devices, with p ∈ Rn
having entries pi . This matching is enforced by means
of a penalty term with coefficient γ > 0 in P 1. In the
power systems setting, Pr can represent a real-power
quantity to be approximately matched by the collective
device-response. The coefficient γ and the signal Pr are
determined by an Independent System Operator (ISO)
and communicated to a Distributed Energy Resource
Provider (DERP) that solves P 1 to obtain a real-time
dispatch solution, see [1] for additional information.

Problem Statement and Dual Problem

Here, we formally state the nonconvex optimization
problem we wish to solve and formulate its dual for the
sake of deriving a lower bound to the optimal cost.
We aim to find an adequate solution to a resource allocation problem where the optimization variables take the
form of binary decisions over a population of n agents.
We note that the problem we consider is applicable to
generator dispatch and active device response in an economic dispatch power systems setting [1], but the remainder of the paper will frame it primarily as resource
allocation. Let each agent i ∈ {1, . . . , n} be endowed
with a decision variable xi and a cost ci ∈ R, a value
which indicates the incremental cost of operating in the
xi = 1 state versus the xi = 0 state. We do not impose a sign restriction on ci , but this may be a common
choice in the power systems setting where xi = 1 represents an “on” device state and xi = 0 represents “off.”
Additionally, each agent is endowed with a parameter
pi which represents some incremental consumption or
generation quantity when operating in the xi = 1 state
versus xi = 0 and also a passive cost di .

The primal P 1 has an associated dual D1 which takes
the form of a semidefinite program (SDP) whose optimal
value lower bounds the cost of P 1. This SDP is
D1 :

max

µ∈Rn ,∆∈R

subject to

∆,


1
Q(µ) ξ(µ)
2
  0.
ξ(µ)⊤ ζ − ∆

(1a)
(1b)

In D1, Q : Rn → Rn×n and ξ : Rn → Rn are real-affine
functions of µ and ζ is a constant.
These definitions are
γ ⊤
Q(µ) = diag(a/2 + µ) + pp , ξ(µ) = ((ai bi )i + µ +
2
Pn ai b2i γ 2
+ Pr . See [5] for more detail
γPr p), and ζ = i=1
2
2
on the derivation of D1.

We are afforded some design choice in the cost function
models for xi ∈
/ {0, 1}, and for each i ∈ {1, . . . , n}, so
we design abstracted cost functions fi : [0, 1] → R that
satisfy fi (0) = di and fi (1) = ci + di , ∀i. This design
choice is intrinsic to a cost model for any separable binary decision optimization context. In particular, the
value of fi (xi ) for any xi ∈
/ {0, 1} is only relevant to the
algorithm design, but need not have a physical interpretation or pertain to the optimization model since these

4

Centralized Newton-like Neural Network

In this section, we develop the Centralized
Newton-like Neural Network, or NNN-c, which
is well suited for solving P 1 in a centralized setting.
3

Q⊤ ΛQ at some x̃ near a saddle point, i.e. ∇x E(x̃) ≈ 0.
If many entries of Λ are small in magnitude and remain
small in the proximity of x̃, then the gradient is changing slowly along the “slow” manifolds associated with
the eigenspace of the small eigenvalues. This is precisely
what the PT-inverse is designed to combat: the weighting of the dynamics is increased along these manifolds by
a factor that is inversely proportional to the magnitude
of the eigenvalues. Additionally, negative eigenvalues of
the Hessian are flipped in sign, which causes attractive
manifolds around saddle points to become repellent.

To draw analogy with the classic Hopfield Neural Network approach we will briefly introduce an auxiliary variable ui whose relation to xi is given by the logistic function g for each i:
xi = g(ui ) =

1
,
1 + e−ui /T

ui = g −1 (xi ) = −T log



1
−1 ,
xi

ui ∈ R,

(2)

xi ∈ (0, 1) ,

with temperature parameter T > 0.

It is desirable for E to be concave on most of its domain so the trajectories are pushed towards the feasible
points of P 1; namely, the corners of the unit hypercube.
To examine this, the Hessian of E can be computed as
−1
2
T
1
H(x) = ddxf2 + τ1 diag( dg dx(x) ) = −W + diag( (xi −x
2 ) ).
i i
τ
Notice that the second term is positive definite on x ∈
(0, 1)n and promotes the convexity of E, particularly
for elements xi close to 0 or 1. For a fixed T, τ , choosing ai < −γkpk2 − 4T /τ, ∀i guarantees E(x) ≺ 0 at
x = (0.5) 1n . Generally speaking, choosing ai to be negative and large in magnitude lends itself to concavity of
E over a larger subset of its domain and to trajectories
converging closer to the set {0, 1}n . However, this comes
at the expense of not exploring a rich subset of the domain. At the end of this section, we develop a Deterministic Annealing (DA) approach inspired by [30] for the
online adjustment of T, τ to obtain an effective compromise between exploration of the state space and convergence to a feasible point of P 1.

Let x ∈ (0, 1)n , u ∈ Rn be vectors with entries given by
xi , ui . To establish our algorithm, it is appropriate to
first define an energy function related to P 1. Consider
E(x) = f (x) +

Z
1 X xi −1
g (ν)dν,
τ i 0

(3)

where τ > 0 is a time-constant and for z ∈ [0, 1],
Z

0

z

g

−1

(ν)dν =

(


T log(1 − z) − z log( z1 − 1) ,
0,

z ∈ (0, 1),
z ∈ {0, 1}.

The classic HNN implements dynamics of the form u̇ =
−∇x E(x), where the equivalent dynamics in x can be
computed as ẋ = −∇x E(x)dx/du. These dynamics can
be thought of to model the interactions between neurons
in a neural network or the interconnection of amplifiers
in an electronic circuit, where in both cases the physical
system tends toward low energy states, see [18, 33]. In
an optimization setting, low energy states draw analogy
to low cost solutions. We now describe our modification
to the classical HNN dynamics.

We now characterize the equilibria of (4) for x ∈ [0, 1]n .
It would appear that x with some components xi ∈
{0, 1} are candidate equilibria due to the xi − x2i factor vanishing. However, the dynamics are not well defined here due to the log term. Additionally, note that
th
limxi →δ e⊤
i H(x)ei = ∞, δ ∈ {0, 1}, ∀i, where ei is the i
T
canonical basis vector. Due to the τ (xi −x2 ) term domii
nating W in the expression for H when xi values are close
to {0, 1}, it follows that an eigenvalue of (|H(x)|m )−1 approaches zero as xi → 0 or 1 with corresponding eigenvector approaching vi = ei :

Recall that the domain of x is (0, 1)n and our elementwise
notation for log and division. We have the expressions
∇x E(x) = −W x−v−(T /τ ) log (1/xi − 1)i and dx/du =
(x − (x2i )i )/T , where W = − diag(a) − γpp⊤ ∈ Rn×n
and v = (ai bi )i + γPr p ∈ Rn are defined via f . From
this point forward, we work mostly in terms of x for
the sake of consistency. Consider modifying the classic
HNN dynamics with a PT-inverse (|H(x)|m )−1 ≻ 0 as
in [31], where H(x) = ∇xx E(x). The NNN-c dynamics
are then given by:

lim = vi⊤ (|H(x)|m )−1 vi =

xi →δ

dx
)∇x E(x)
du
(xi − x2i )i
= (|H(x)|m )−1 diag(
)
(4)
T


T
W x + v + log (1/xi − 1)i .
τ

T
(xi − x2i ) = 0,
τ

δ ∈ {0, 1}, ∀i.

Using this fact, and ignoring T, τ > 0, we can compute
the undetermined limits in the components of ẋ as xi →
δ ∈ {0, 1} by repeated applications of L’Hospital’s rule:

ẋ = −(|H(x)|m )−1 diag(

lim log

xi →δ





1
0, δ = 0+ ,
2 2
− 1 (xi − xi ) =
0, δ = 1− .
xi

(5)

Thus, components xi ∈ {0, 1} constitute candidate equilibria. We will, however, return to the first line of (5) in
the proof of Lemma 3 to show that they are unstable. As

These dynamics lend to the avoidance of saddle points of
E. To see this, consider the eigendecomposition H(x̃) =

4

for components of x in the interior of the hypercube, the
expression ẋ = 0 can not be solved for in closed form.
However, we provide the following Lemma which shows
that the set of equilibria is finite.

Lemma 3 (Forward Invariance of the Open Hypercube). The open hypercube (0, 1)n is a forwardinvariant set under the NNN-c dynamics (4).
Knowing that P 1 is generally NP-hard, it is unlikely that
a non-brute-force algorithm exists that can converge to
a global minimizer. For this reason, we aim to establish
asymptotic stability to a local minimizer of E. We first
establish some assumptions.

Lemma 2 (Finite Equilibria). Let X be the set of
equilibria of (4) satisfying ẋ = 0 on x ∈ [0, 1]n . The set
X is finite.
The proof can be found in the Appendix, and all proofs
for the remainder of the paper will be contained there.

Assumption 2 (Random Initial Condition). The
initial condition x(0) is chosen randomly according to a
distribution P that is nonzero on sets that have nonzero
volume in [0, 1]n .

To demonstrate the qualitative behavior of equilibria
in a simple case, consider a one-dimensional example
with a > −γp2 − 4T /τ and recall that, for x ∈ (0, 1),
the sign of −∇x E(x) is the same as ẋ. In Figure 1,
we observe that −∇x E(x) monotonically decreases in
x, and a globally stable equilibrium exists in the interior x ∈ (0, 1) near x = 0.5. On the other hand,
a < −γp2 − 4T /τ gives way to 3 isolated equilibria in
the interior (one locally unstable near x = 0.5 and two
locally stable near x ∈ {0, 1}). This behavior extends
in some sense to the higher-dimensional case. Therefore,
for a scheme in which T and τ are held fixed, we prescribe a < −γkpk2 − 4T /τ . We provide a Deterministic
Annealing (DA) approach inspired by [30] for the online adjustment of T, τ in the following subsection which
compromises with this strict design of a.

An appropriately unbiased initial condition for our algorithm is x(0) ≈ (0.5) 1n , which is adequately far from
the local minima located near corners of the unit cube.
So, we suggest choosing a uniformly random x(0) ∈
B((0.5) 1n , ǫ), where 0 < ǫ ≪ 1.
Assumption 3 (Choice of T, τ ). The constants
T, τ > 0 are each chosen randomly according to a distribution P̄ that is nonzero on sets that have nonzero
volume on R+ .
Similarly to x(0), we suggest choosing these constants
uniformly randomly in a ball around some nominal
T0 , τ0 , i.e. T ∈ B(T0 , ǫ), τ ∈ B(τ0 , ǫ), 0 < ǫ ≪ 1. The
T0 , τ0 themselves are design parameters stemming from
the neural network model, and we provide some intuition for selecting these in the simulation Section.
Now we state the main convergence result of NNN-c
in Theorem 4, which states that for a random choice of
T, τ , an initial condition chosen randomly from (0, 1)n
converges asymptotically to a local minimizer of E with
probability one.

0

0.5

1

0

0.5

1

0

0.5

1

Theorem 4 (Convergence of NNN-c). Given an
initial condition x(0) ∈ (0, 1)n , the trajectory x(t) under
NNN-c converges asymptotically to a critical point x⋆
of E. In addition, under Assumption 2, on the random
choice of initial conditions, and Assumption 3, on the
random choice of T, τ , the probability that x(0) is in the
set ∪ W s (x̂), where x̂ is a saddle-point or local maximum
x̂

of E, is zero.

0

0.5

1

0

0.5

1

0

0.5

We now define a Deterministic Annealing (DA) variant inspired by [30] to augment the NNN-c dynamics
and provide a method for gradually learning a justifiably good feasible point of P 1. In [30], the author justifies the deterministic online tuning of a temperature
parameter in the context of data clustering and shows
that this avoids poor local optima by more thoroughly
exploring the state space. Similarly, we aim to learn a
sufficiently good solution trajectory by allowing the dynamics to explore the interior of the unit hypercube in

1

Fig. 1. Illustration of −∇x E(x) (top) and ẋ (bottom) for
three instances of a. Case 1: a > −γkpk2 − 4T /τ , Case 2:
a = −γkpk2 − 4T /τ , Case 3: a < −γkpk2 − 4T /τ .

Finally, we establish a Lemma about the domain of the
trajectories of (4).

5

the early stages of the algorithm, and then to force the
trajectory outward to a feasible binary solution by gradually adjusting T or τ online.

to the Distributed Newton-like Neural Network,
or NNN-d, which we rigorously analyze for its convergence properties.

Consider either reducing the temperature T or increasing the time constant τ during the execution of NNN-c.
This reduces the terms in E which promote convexity,
particularly near the boundaries of the unit hypercube.
As T, τ are adjusted, for a ≺ −γkpk2, the domain of E
becomes gradually more concave away from the corners
of the unit hypercube. Thus, starting with T /τ sufficiently large, the early stages of the algorithm promote
exploration of the interior of the state space. As T /τ is
reduced, the equilibria of E are pushed closer to (and
eventually converge to) the feasible points of P 1. The
update policy we propose is described formally in Algorithm 1, and we further explore its performance in simulation.

It is clear from the PT-inverse operation and W being
nonsparse that NNN-c is indeed centralized. In this section, we design a distributed algorithm in which each
agent i must only know pj , j ∈ N i and the value of
an auxiliary variable yj , j ∈ N i ∪ N 2i , i.e. it must have
communication with its two-hop neighbor set. If twohop communications are not directly available, the algorithm can be implemented with two communication
rounds per algorithm step. We provide comments on a
one-hop algorithm in Remark 12.
Assumption 4 (Graph Properties and Connectivity). The graph G = (N , E) is undirected and connected;
that is, a path exists between any two pair of nodes and,
equivalently, its associated Laplacian matrix L = L⊤ has
rank n − 1.

Algorithm 1 Determinisitc Annealing
1: procedure Det-Anneal(β > 1, T0 , τ0 , td )
2:
Initialize x(0)
3:
T ← T0 , τ ← τ0
4:
while true do
5:
Implement NNN-c for td seconds
6:
τ ← βτ or T ← (1/β)T
7:
end while
8: end procedure

Now consider the n linear equations (pi xi )i + Ly =
(Pr /n) 1n . Notice that, by multiplying from the left by
⊤
⊤
⊤
1⊤
n and applying 1n L = 0n , we recover p x = Pr . Thus,
by augmenting the state with an additional variable
y ∈ Rn , we can impose a distributed penalty term. We
now formally state the distributed reformulation of P 1:
P2:

Note that Algorithm 1 leads to a hybrid dynamic system
with discrete jumps in an enlarged state φ = (x, T, τ ),
which can cast some doubt on basic existence and
uniqueness of solutions. We refer the reader to Propositions 2.10 and 2.11 of [12] to justify existence and
uniqueness of solutions in the case of td > 0 fixed.

x∈{0,1}

,y∈Rn

f˜(x, y) =

n
X
i

fi (xi ) +

γ ⊤
σ σ,
2

where the costs fi again satisfy fi (1) − fi (0) = ci and we
have defined σ = (pi xi )i + Ly − (Pr /n) 1n for notational
simplicity. Before proceeding, we provide some context
on the relationship between P 1 and P 2.

Corollary 5 (Convergence to Feasible Points).
Under Assumptions 2-3 and a ≺ −γkpk2, the NNN-c
dynamics augmented with Algorithm 1 converge asymptotically to feasible points of P 1.

Lemma 6 (Equivalence of P1 and P2). Let Assumption 4, on graph connectivity, hold, and let (x⋆ , y ⋆ )
be a solution to P 2. Then, x⋆ is a solution to P 1 and
f (x⋆ ) = f˜(x⋆ , y ⋆ ).

The result of the Corollary is quickly verified by inspecting the terms of H(x). The function E is smooth, strictly
concave near x = (0.5) 1n for small T /τ due to the design of ai , and becomes strictly convex as the elements of
x approach 0 or 1, corresponding to isolated local minima of E, due to the T /τ term dominating H(x). As the
quantity T /τ is reduced under Algorithm 1, these local
minima are shifted asymptotically closer to corners of
the unit hypercube, i.e. feasible points of P 1.
5

min
n

To define NNN-d, we augment the centralized NNN-c
with gradient-descent dynamics in y on a newly obtained
e of P 2. Define E
e as
energy function E
X Z xi
e y) = f˜(x, y) + 1
E(x,
g −1 (ν)dν.
τ i 0

(6)

In Section 4, we obtained a matrix W which was nonf , ṽ for E
e via f˜ as W
f = − diag(a +
sparse. Define W
2
γ(pi )i ), ṽ = (ai bi )i + γ diag(p) ((Pr /n) 1n −Ly) . Come with respect to only x as H(x)
e
pute the Hessian of E
=
2
e
f
e
∇xx E(x, y) = −W +(T /τ ) diag(1/x−(xi )i ). Since H(x)
e
is diagonal, the iith element of the PT-inverse of H(x)

Distributed Hopfield Neural Network

With the framework of the previous section we formulate a problem P 2 which is closely related to P 1, but for
which the global penalty term can be encoded by means
of an auxiliary decision variable. This formulation leads

6

x(0), and Assumption 3, on the random choice of T, τ ,
the probability that (x(0), y(0)) is in the set ∪ W s (x̂, ŷ),

can be computed locally by each agent i as:
−1
e
(|H(x)|
m )ii =

(

x̂,ŷ

e ii |−1 , |H(x)
e ii | ≥ m,
|H(x)
1/m,
o.w.

e is
where (x̂, ŷ) is a saddle-point or local maximum of E,
⋆ ⋆
e
zero. Lastly, all local minima (x , y ) of E are globally
e ⋆ , y) ≥ E(x
e ⋆ , y ⋆ ), ∀y ∈ Rn .
optimal in y: E(x

e ii = ai + γp2 + T /τ (xi − x2 )−1 . The NNN-d
where H(x)
i
i
dynamics, which are PT-Newton descent in x and grae are then stated as:
dient descent in y on E,

Lemma 11 (Forward Invariance of the Open Hypercube (Distributed)). The set (0, 1)n × Y is a
forward-invariant set under the NNN-d dynamics (7).

(xi − x2i )i
−1
e
ẋ = (|H(x)|
diag(
)
m)
T


f x + T log (1/xi − 1) + ṽ , (7)
W
i
τ
ẏ = −αγL ((pi xi )i + Ly) ,

Remark 12 (One-Hop Distributed Algorithm).
The proposed distributed algorithm requires two-hop
neighbor information, which may be intractable in
some settings. The source of the two-hop term stems
from the quadratic γ penalty term. However, it is possible to define a one-hop distributed algorithm via a
Lagrangian-relaxation route.

f , ṽ and the sparsity of L, ẋ
Due to the new matrices W
can be computed with one-hop information and ẏ with
two-hop information (note the L2 term); thus, (7) defines a distributed algorithm. Additionally, recalling the
discussion on parameter design, the problem data a and
b can now be locally designed.

Consider posing
p P 2 with the γ term
p instead as a linear
constraint: γ/2((pi xi )i + Ly) = γ/2(Pr /n) 1n . Applying Lagrangian relaxation to this problem introduces
a Lagrange multiplier on the linear terms, and from there
it would be appropriate to define a saddle-point-like algorithm along the lines of [8] in which gradient-ascent in
the dual variable is performed. This changes the nature
of the penalty from squared to linear, so the underlying optimization model is different in that sense, but it
follows that this approach could be implemented with
one-hop information.

Before proceeding, we establish a property of the domain
of y and some distributed extensions of Lemmas 2 and 3.
Lemma 7 (Domain of Auxiliary Variable). Given
an initial condition y(0) with 1⊤
n y(0) = κ, the trjaectory
y(t) is contained in the set
Y = {ω + (κ/n) 1n | 1⊤
n ω = 0}.

We note that, in some distributed contexts,
penalty
√
terms or constraints can be imposed via L which then
appears as L in the associated squared terms of the
dynamics (in place of L2 ). However, the linear
√ L also
appears in our algorithm, and substituting L would
not inherit the sparsity of the communication graph.
Therefore we leave the design of a fully one-hop mixed
first-order/second-order algorithm as an open problem.

(8)

Lemma 8 (Closed Form Auxiliary Solution). For
an arbitrary fixed x ∈ [0, 1]n , the unique minimizer y ⋆
e is given by
contained in Y of both f˜ and E
y ⋆ = −L† (pi x̃i )i +

κ
1n .
n

(9)

This is also the unique equilibrium of (7) in Y.

6

Lemma 9 (Finite Equilibria (Distributed)). Let
e ×Y
e be the set of equilibria of (7) satisfying (ẋ, ẏ) = 0
X
e ×Y
e is finite.
on (x, y) ∈ [0, 1]n × Y. The set X

Our simulation study is split in to two parts; the first
focuses on numerical comparisons related to runtime and
solution quality, and the second is a 2D visualization
of the trajectories of the Distributed Annealing (DA)
variants for both the centralized and distributed NNN
methods.

We now extend the results of Theorem 4 to the distributed case of solving P 2 via NNN-d. We have the
following theorem on the trajectories of (x(t), y(t)) under (7), which can be interpretted as establishing convergence to a local minimizer with probability one.

6.1

Simulations

Runtime and Solution Quality Comparison

In this section, we compare to a greedy method stated
as Algorithm 2 and a semidefinite programming (SDP)
relaxation method stated as Algorithm 3. In short, the
greedy method initializes the state as x = 0n and iteratively sets the element xi to one which decreases the
cost function the most. This is repeated until no element

Theorem 10 (Convergence of NNN-d). Given an
initial condition (x(0), y(0)) ∈ (0, 1)n × Rn , the trajectory (x(t), y(t)) under NNN-d converges asymptotically
e In addition, under Asto a critical point (x⋆ , y ⋆ ) of E.
sumption 2, on the random choice of initial condition
7

NNN-c), and the NNN-c and NNN-d methods we developed in Sections 4 and 5. The first obvious observation to make is that the runtime of brute force method
increases at a steep exponential rate with increasing n
and exceeds 120 seconds at n = 22, making it intractable
for even medium sized problems. Next, we note that
there are some spikes associated with the HNN method
around n = 25 to n = 40. These are reproducible,
and we suspect that this is due to the emergence of
saddle-points and increasing likelihood of encountering
these along the trajectory as n increases. This is a welldocumented problem observed in literature, see e.g. [10],
and we also confirm it empirically in this setting by observing that share of iterations for which the Hessian is
indefinite (as opposed to positive definite) tends to grow
as n increases. We also note that NNN-c scales relatively poorly, which can be attributed to a matrix eigendecomposition being performed at each discretized iteration of the continuous-time algorithm. For NNN-d, the
matrix being eigendecomposed is diagonal, which makes
it a trivial operation and allows NNN-d to scale well.
We note that the SDP method scales the worst amongst
the non brute-force methods. Unsurprisingly, the greedy
method remains the fastest at large scale, although recall that the motivation of developing our method is for
it to be distributed and that a greedy approach can not
be distributed due to the global penalty term.

remains for which the updated state has lower cost than
the current state. For the SDP method, a convex SDP
is obtained as the relaxation of P 1, see e.g. [34]. We use
the shorthand SDPrlx(•) to indicate this in the statement of Algorithm 3. This SDP is solved using CVX
software in MATLAB [13] and a lowest-cost partition is
computed to construct a feasible solution. For the sake
of convenience in stating both algorithms, we have defined f ′ : 2n → R to be the set function equivalent of f ,
i.e. the cost of P 1. That is, f ′ (S) = f (x), where i ∈ S
indicates xi = 1 and i ∈
/ S indicates xi = 0. Finally,
we additionally compare to a brute force method which
we have manually programmed as an exhaustive search
over the entire (finite) feasibility set.
Algorithm 2 Greedy Method
1: procedure Greedy(f ′ )
2:
S←∅
3:
done ← false
4:
while done = false do
5:
i⋆ ← argmin f ′ (S ∪{i})
i∈S
/

if f ′ (S ∪{i⋆ }) < f ′ (S) then
S ← S ∪{i⋆ }
else
done ← true
end if
end while

0, i ∈
/ S,
12:
xi ←
1, i ∈ S .
13:
return x
14: end procedure
6:
7:
8:
9:
10:
11:

As for algorithm performance as it pertains to the cost
of the obtained solution, we fix n = 50 and additionally
include DA variants of both NNN-c and NNN-d. We
also omit the brute force method due to intractability.
For the sake of comparison, we compute a performance
metric Q and provide it for each method in Table 1. The
metric Q is computed as follows: for each trial, sort the
methods by solution cost. Assign a value of 6 for the best
method, 5 for the second-best, and so on, down to the
seventh-best (worst) receiving zero. Add up these scores
for all 100 trials, and then normalize by a factor of 600
(the maximum possible score) to obtain Q. Note that Q
does not account for runtime in any way.

Algorithm 3 SDP Relaxation Method
1: procedure SDP(f ′ )
2:
P SDP ← SDPrlx(P 1)
3:
x⋆ ← argmin P SDP
x

4:
5:
6:
7:

S←∅
done ← false
while done = false do
i⋆ ← argmax xi

It should be unsurprising that the tried-and-true centralized greedy and SDP methods perform the best. However, we note that they were beaten by our methods in a
significant number of trials, which can be seen by noting
that a Q score for two methods which perform best or
second-best in all trials would sum to 1100/600 = 1.83,
while Q(greedy) + Q(SDP) = 1.75, or a cumulative prescaled score of 1050, indicating that our methods outperformed these methods in net 50 “placement spots”
over the 100 trials. In general, we find that the DA version of the NNN algorithms obtains better solutions than
the non-DA version, confirming the benefit of this approach. We also find that NNN-d generally outperforms
NNN-c. It’s possible that an initially “selfish” trajectory in x is beneficial, which would neglect the global
penalty until y adequately converges, although this is
speculative. Lastly, we note that the HNN method never

i∈S
/

if f ′ (S ∪{i⋆ }) < f ′ (S) then
S ← S ∪{i⋆ }
else
done ← true
end if
end while

0, i ∈
/ S,
14:
xi ←
1, i ∈ S .
15:
return x
16: end procedure
8:
9:
10:
11:
12:
13:

In Figure 2 we plot the runtime in MATLAB on a 3.5GHz
Intel Xeon E3-1245 processor over increasing problem
size n for each of six methods: a brute force search,
the aforementioned greedy and SDP methods, the HNN
first proposed in [18] (i.e. the gradient-like version of

8

Table 2
Problem data and parameter choices (where relevant) for
performance comparison. Problem data pi , ci is generated
randomly from given distributions for each of 100 trials.

Table 1
Comparison of performance metric Q for 100 randomized
trials with n = 50.
Method
Q

Data or parameter

Value

0.2891

n

50

0.5443

pi

NNN-c

0.2161

NNN-c-DA
NNN-d
NNN-d-DA

0.7005

ci

HNN

0

Pr

1500

Greedy

0.8411

γ

1

0.9089

T0

1

τ0

0.1

m

0.1

α

1

Learning steps

10

β

1.4

n

50

SDP

performs better than worst, which we attribute to the
steepest-descent nature of gradient algorithms which do
not use curveature information of the energy function.
It might be possible that the stopping criterion forces
HNN to terminate near saddle-points, although we do
not suspect this since we observe the Hessian is positivedefinite in the majority of termination instances.
As for parameter selection, we find that choosing m ≪ 1
is generally best, since m ≥ 1 would always produce a
PT-inverse Hessian with eigenvalues contained in (0, 1].
This effectively scales down ẋ in the eigenspace associated with Hessian eigenvalue magnitudes greater than 1,
but does not correspondingly scale up ẋ in the complementary eigenspace associated with small eigenvalues.
Additionally, choosing T /τ greater than 1 in the fixed
case tended to be effective. This may be related to selecting ai < −γkpk2 − 4T /τ to guarantee anti-stability
from (0.5) 1n , and would explain why a high T0 /τ0 that
decreases in the DA learning variant performs so well.
In general, for the DA learning variant, we recommend
choosing T0 , τ0 so that T0 /τ0 ≫ 1 and also β > 1 sufficiently large so that T /τ ≪ 1 by algorithm termination,
which gives rise to a robust exploration/exploitation
tradeoff. Finally, all α ≈ 1 seem to behave roughly the
same, with only α ≪ 1 and α ≫ 1 behaving poorly (the
former leading to slow convergence in y and “selfish”
behavior in x, and the latter being destabilizing in the
discretization of ẏ).
6.2

Note that, in each case, the trajectory approaches the optimal solution x⋆ = (1, 0)⊤ . However, it is worth noting
that a steep saddle point occurs around x = (0.75, 0.6)⊤.
Intuitively, this corresponds to a high risk of the trajectory veering away from the optimal solution had the DA
not been implemented. With the opportunity to gradually learn the curveature of the energy function, as
shown by stabilization to successive equilibria marked
by ×, each algorithm is given the opportunity to richly
explore the state space before stabilizing to the optimal
solution (1, 0)⊤ . Further studying the learning-rate T /τ
and a more complete analysis of Algorithm 1 and the
parameter β are subjects of future work.

Next, for the sake of understanding how the learning
rate T /τ affects the trajectories of the solutions, we have
provided Figure 3 which plots the 2-D trajectories of
NNN-c and NNN-d with T /τ being gradually reduced
over 15 learning steps. The contours of the energy function for the final step are also plotted. The problem data
and choice for a is:
p = (3, 1)⊤ ,

Pr = 2.8,

e ∼ U [2, 3]

Fig. 2. Runtime of each method for increasing problem sizes.

Learning Steps and 2-D Trajectories

c = (2, 1)⊤ ,

U[1, 50]
pei ,

γ = 4,

⊤

a := −(10, 10) .
9

problems which may include additional cost terms or
constraints and a deeper analysis of the Deterministic
Annealing variant as it pertains to the online adjustment of the learning-rate T /τ .
Appendix
Proofs of Lemmas, Theorems, and Propositions
Proof of Lemma 2: First consider only X ∩(0, 1)n .
Note that (|H(x)|m )−1 ≻ 0 (by construction) and
diag((xi − x2i )i /T ) ≻ 0 on x ∈ (0, 1)n , so we focus on
Wx +
(a)

T
log (1/xi − 1)i + v = 0n .
τ

(10)

Examining the above expression elementwise, it is nonconstant, continuous, and its derivative changes sign
only a finite number of times. Therefore, the total number of zeros on (0, 1)n must be finite.
Now consider the ith element of (10) for xj → 0 or 1
for all j in an arbitrary permutation of {1, . . . , n} \ {i}.
Since the number of these permutations is finite, and
each permutation still gives rise to a finite number of
solutions to (10) in the ith component, it follows that X
is finite.

Proof of Lemma 3: Consider again the terms of ẋ elementwise. There are two cases to consider for evaluating
xi : xi = ε and xi = 1 − ε for some 0 < ε ≪ 1 sufficiently
small such that the terms of (|H(x)|m )−1 are still dominated by (1/xi − x2i ) and the W x + v are still dominated
by the log term. Then, consider the expression

(b)
Fig. 3. Centralized NNN-c (a) and distributed NNN-d (b)
trajectories in 2D with 15 learning steps. Stable equilibrium
points between learning steps indicated by ×, contours of E
e in final step indicated by dashed lines.
and E

7

log (1/xi − 1) (xi − x2i )2 .

(11)

For xi = ε ≈ 0, (11) evaluates to a small positive value,
and for xi = 1 − ε ≈ 1, (11) evaluates to a small negative value. We have argued that these are the dominating terms regardless of values of the remaining components of x, and so we conclude that xi ∈ {0, 1} are componentwise anti-stable and that elements of x will never
approach 0 or 1. Thus, the open hypercube is forward
invariant.


Conclusion

This paper posed an optimal generator dispatch problem for settings in which the agents are generators
with binary controls. We first showed that the centralized problem is amenable to solution via a Centralized Newton-like Neural Network approach and
proved convergence to a local minimizer with probability one under light assumptions. Next, we developed an
approach to make the dynamics computable in a distributed setting in which agents exchange messages with
their two-hop neighbors in a communication graph. The
methods scale and perform well compared to standard
greedy and SDP-relaxation approaches, and the latter
method enjoys the qualities of a distributed algorithm,
unlike previous approaches. Future research directions
include application of the methods to a broader class of

Proof of Theorem 4: Let X be the set of all critical
points of E. We first establish that E decreases along the
trajectories of NNN-c and that x(t) converges asymptotically to X . Differentiating E in time, we obtain:

dE
= ẋ⊤ ∇x E(x) = ẋ⊤ −W x − v + g −1 (x)/τ
dt
T
(12)
= −ẋ⊤ diag(
)|H(x)|m ẋ < 0,
(xi − x2i )i
for ẋ 6= 0, x ∈ (0, 1)n .
10

Recall that x(t) ∈ (0, 1)n for all t ≥ 0 due to Lemma 3.
From (4) and the discussion that followed on equilibria,
ẋ = 0 implies ∇x E(x) = 0 due to (|H(x)|m )−1 ≻ 0 and
diag((xi − x2i )i /T ) ≻ 0 on x ∈ (0, 1)n . The domain of
E is the compact set [0, 1]n (per the definition of the integral terms), and E is continuous and bounded from
below on this domain, so at least one critical point exists. Combining this basic fact with (12) shows that the
NNN-c dynamics monotonically decrease E until reaching a critical point. More formally, applying the LaSalle
Invariance Principle [21] tells us that the trajectories
converge to the largest invariant set contained in the set
dE/dt = 0. This set is X , which is finite per Lemma 2. In
this case, the LaSalle Invariance Principle additionally
establishes that we converge to a single x⋆ ∈ X .

Now consider the set of critical points as an explicit
function of T, τ and write this set as X (T, τ ). Recalling Lemma 2, the set of x̂ that we are interested in
reduces to a finite set of critical points x⋆ ∈ X (T, τ ).
Thus, we can conclude that P̄(∪x⋆ ∈X (T,τ ) T (x⋆ )) ≤
P
⋆
x⋆ ∈X (T,τ ) P̄(T (x )) = 0.

There is an additional case which must be considered,
which is that h(x⋆ , T, τ ) 6= 0, but some eigenvalues of
DϕT,τ (x⋆ ) are purely imaginary and induce stable center manifolds, which could accommodate the case of a
globally stable set which is an n-dimensional manifold
(i.e. the “degenerate saddle” case). We consider the function h mostly out of convenience, but the argument can
be extended to a function h : (0, 1)n × R × R → Cn
which is a map to the roots of the characteristic equation of DϕT,τ (x). We are concerned that each element of
h(x, T, τ ) should have a nonzero real part almost everywhere. To extend the previous case to this, consider the
identification C ≡ R2 and compose h with the nonconstant real analytic function ζ(w, z) = w, for which the
zero set is w ≡ 0, corresponding to the imaginary axis in
our identification. From this, we obtain a nonconstant
real-analytic as before whose zero set is the imaginary
axis. Applying the argument in [25] in a similar way as
above, h(x, T, τ ) has nonzero real parts for almost all
(T, τ ) for each x. Therefore, the probability of a particular saddle point or local maximum x⋆ having a nonempty
stable center manifold is zero for arbitrary x(0) satisfying Assumption 2 and T, τ satisfying Assumption 3. 

The proof of the second statement of the theorem relies on an application of the Stable Manifold Theorem (see [14]) as well as Lemma 2. Let
ẋ = ϕT,τ (x) for a particular T, τ . We aim to show that
P[∪x̂ {W s (x̂) | x̂ is a saddle or local maximum}] = 0
under Assumptions 2-3. It is sufficient to show that,
for each critical point x⋆ such that ϕT,τ (x⋆ ) = 0, and
almost all T, τ , DϕT,τ (x⋆ ) is full rank and its eigenvalues have non-zero real parts. The reason for this
argument is the following: let x⋆ be a critical point with
DϕT,τ (x⋆ ) full rank and eigenvalues with non-zero real
parts. If the eigenvalues do not all have positive real
parts, then some have negative real parts, which indicates that x⋆ is a saddle or local maximum of E. These
negative real-part eigenvalues induce an unstable manifold of dimension nu ≥ 1. As such, the globally stable
set W s (x⋆ ) is a manifold with dimension n − nu < n,
and P [x(0) ∈ W s (x⋆ )] = 0 per Assumption 2.

Proof of Lemma 6: The equivalence stems from the
global term and the flexibility in the unconstrained y
variable. Notice

To argue this case, define h : (0, 1)n × R × R → R as

γ ⊤
γ
σ σ = σ ⊤ (In − 1n 1⊤
n /n)σ +
2
2
γ ⊤
= σ (In − 1n 1⊤
n /n)σ +
2

h(x, T, τ ) = det DϕT,τ (x).
We now leverage Assumption 3 and [25] to claim first
that P̄ [h(x⋆ , T, τ ) = 0] = 0 for each x⋆ ∈ X , i.e.
DϕT,τ (x⋆ ) is full rank for each x⋆ with probability one
w.r.t. P̄. We first address the points x for which the
function h is discontinuous. Define X̂ as the set of x
for which the truncation of the eigenvalues of H(x)
becomes active, i.e. the discontinuous points of h. Although we do not write it as such, note that H is implicitly a function of T, τ and that the eigenvalues of H
can be expressed as nonconstant real-analytic functions
of T, τ . Considering this fact and an arbitrary x, the set
of T, τ which give x ∈ X̂ has measure zero with respect
to R2 [25]. Thus, for particular T, τ , h is C ∞ almost
everywhere. Applying once more the argument in [25]
and Assumption 3 with the fact that h is a nonconstant
real analytic function of T, τ we have that
P̄ [T (x̂) , {(T, τ ) | h(x̂, T, τ ) = 0}] = 0,

γ
σ(1n 1⊤
n /n)σ
2
γ ⊤
(p x − Pr )2 .
2

We have recovered the original global term of P 1 in the
bottom line, so now we deal with the remaining term.
⊤
The matrix In −1n 1⊤
n /n  0 has image In −1n 1n /n =
span{1n }⊥ = image L, given that L is connected. Thus,
because y is unconstrained and does not enter the cost
anywhere else, we can compute the set of possible minimizers of f˜ in closed form with respect to any x as

y ⋆ ∈ {−L† (pi xi )i − (Pr /n) 1n + θ 1n | θ ∈ R}
= {−L† (pi xi )i + θ 1n | θ ∈ R}.

Moreover, substituting a y ⋆ gives σ ∈ span{1n }, and it
follows that the problem P 2 reduces precisely to P 1. 
Proof of Lemma 7: The proof is trivially seen by multiplying ẏ in (7) from the left by 1n and applying the
null space of L.


∀ x̂ ∈
/ X̂ .

11

e with probability one.
minimizer (x⋆ , y ⋆ ) of E

Proof of Lemma 8: The first term is computed by
e y ⋆ ) = 0n ) and
setting ∇y f˜(x, y ⋆ ) = 0n (resp. ∇y E(x,
⋆
solving for y . There is a hyperplane of possible solutions due to the rank deficiency of L, but we are looking for the unique solution in Y. The second term therefore follows from (8). The fact that this point is also
the unique equilibrium in Y follows from the fact that
e y ⋆ ).

ẏ = −α∇y E(x,

e ⋆ , y) ≥
The final part of the Theorem statement that E(x
e ⋆ , y ⋆ ), ∀y ∈ Rn can also be seen from the convexity
E(x
e in y and applying the first-order condition of conof E
vexity:

Proof of Theorem 10: The first part of the proof to
establish convergence to a critical point follows from a
similar argument to the proof of Theorem 4. Differentie with respect to time gives:
ating E

Proof of Lemma 11: The forward invariance of Y is already established per its definition and Lemma 7, but we
must establish that the trajectories y(t) remain bounded
in order to apply the argument in Lemma 3 to the proof
e with respect
of Theorem 10. Compute the Hessian of E
to y as:
e = γL2  0.
∇yy E
Due to the connectedness of L, the eigenspace associated
with the n − 1 strictly positive eigenvalues of γL2 is
e is strictly convex in y on this
parallel to Y. Therefore, E
e is bounded from below
subspace, and it follows that E
e
e
on Y. Due to dE/dt
≤ 0 (13) and the continuity of E
in y, it follows that y(t) is bounded for all t. Given this,
the argument from Lemma 3 applies to the trajectories
x(t), and the set (0, 1)n × Y is forward invariant under
NNN-d (7).


e ⋆ , y⋆)
e ⋆ , y) ≥ E(x
e ⋆ , y ⋆ ) + (y − y ⋆ )⊤ ∇y E(x
E(x

Proof of Lemma 9: The proof follows closely to the
proof of Lemma 2 with the variation that ṽ in the expression for ẋ is now a function of y. Given the result of
Lemma 8, we may directly substitute the unique y ⋆ (9)
for any x. Because y ⋆ is simply a linear expression in x,
e is finite folthe same argument as in Lemma 2 that X
lows.


e ⋆ , y ⋆ ) = 0n .
along with ∇y E(x

#
" #⊤ "
e y)
e
ẋ
∇x E(x,
dE
=
e y)
dt
∇y E(x,
ẏ
" #⊤ "
#
f x − ṽ + g −1 (x)/τ
ẋ
−W
=
ẏ
−α−1 ẏ

= −ẋ⊤ diag(T /(xi − x2i )i )|H̃(x)|m ẋ − α−1 ẏ ⊤ ẏ < 0,
ẋ 6= 0 or ẏ 6= 0, (x, y) ∈ (0, 1)n × Y.
(13)
e
Thus, E monotonically decreases along the trajectories
of NNN-d. Given (13), we call again on the forward
invariance property of the open hypercube for the distributed case via Lemma 11, which verifies that (x, y) ∈
(0, 1)n × Y at all times.



References

[1] CAISO business practice manual for market operation.
https://bpmcm.caiso.com/Pages/BPMDetails.aspx?BPM=Market%20Operations
2018. Version 57.
[2] S. Bauk and Z. Avramović. Hopfield network in solving
travelling salesman problem in navigation. In Seminar on
Neural Network Applications in Electrical Engineering, pages
207–2010, 2002.
[3] D. P. Bertsekas and J. Tsitsiklis.
Programming. Athena Scientific, 1996.

e is not radially
Due to the deficiency induced by L, E
n
unbounded in y over all of R , so we must be careful before applying the LaSalle Invariance Principle. Ine only on [0, 1]n × Y in consideration of
stead, define E
e is then obtained
Lemma 7. Radial unboundedness in E
given any y(0), and it follows that the trajectories cone
verge to largest invariant set contained in dE/dt
= 0
per the LaSalle Invariance Principle [21]. This is the fie per Lemma 9, and so it
nite set of critical points of E
additionally follows that we converge to a single critical
point (x⋆ , y ⋆ ).

Neuro-Dynamic

[4] S. Boyd and L. Vandenberghe. Semidefinite programming
relaxations of non-convex problems in control and
combinatorial optimization. In A. Paulraj, V. Roychowdhuri,
and C. Schaper, editors, Communications, Computation,
Control and Signal Processing: A Tribute to Thomas Kailath,
chapter 15, pages 279–288. Kluwer Academic Publishers,
1997.
[5] S. Boyd and L. Vandenberghe.
Cambridge University Press, 2004.

Convex Optimization.

[6] F. Bullo, J. Cortés, and S. Martı́nez. Distributed Control of
Robotic Networks. Applied Mathematics Series. Princeton
University Press, 2009.

e is convex in y, it follows that for any fixed
Because E
e with respect to y.
x there exist only local minima of E
In consideration of this, we need only apply the Stable
Manifold Theorem [14] to x. The argument for this develops similarly to the proof of Theorem 4, and we conclude that the trajectories of NNN-d converge to a local

[7] P. Chardaire and A. Sutter. A decomposition method
for quadratic zero-one programming. Management Science,
41(4):704–712, 1995.
[8] A. Cherukuri, E. Mallada, S. H. Low, and J. Cortés. The role
of convexity in saddle-point dynamics: Lyapunov function
and robustness. IEEE Transactions on Automatic Control,
63(8):2449–2464, 2018.

12

[9] T. Cormen, C. Leiserson, R. Rivest, and C. Stein.
Introduction to Algorithms. MIT Press, 3 edition, 2009.

[28] P. Parrilo and S. Lall. Semidefinite programming relaxations
and algebraic optimization in control. European Journal of
Control, 9(2-3):307–321, 2003.

[10] Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli,
and Y. Bengio.
Identifying and attacking the saddle
point problem in high-dimensional non-convex optimization.
International Conference on Neural Information Processing
Systems, pages 2933–2941, 2014.

[29] S. Poljak, F. Rendl, and H. Wolkowicz. A recipe for
semidefinite relaxation for (0,1)-quadratic programming.
Journal of Global Optimization, 7(1):51–73, 1995.
[30] K. Rose. Deterministic annealing for clustering, compression,
classification, regression, and related optimization problems.
Proceedings of IEEE, 86(11):2210–2239, 1998.

[11] P. Gill, W. Murray, and M. Wright. Practical optimization.
Academic Press, 1981.
[12] R. Goebel, R. G. Sanfelice, and A. Teel. Hybrid dynamical
systems. IEEE Control Systems Magazine, 29(2):28–93, 2009.

[31] S. Paternain, A. Mokhtari, and A. Ribeiro. A Newtonbased method for nonconvex optimization with fast evasion
of saddle points. SIAM Journal on Optimization, 29(1):343–
368, 2019.

[13] M. Grant and S. Boyd. CVX: Matlab software for disciplined
convex programming, version 2.1. http://cvxr.com/cvx,
Mar. 2014.

[32] M. Shamaiah, S. Banerjee, and H. Vikalo. Greedy sensor
selection: Leveraging submodularity. In IEEE Int. Conf. on
Decision and Control, pages 2572–2577, 2010.

[14] J. Guckenheimer and P. Holmes. Nonlinear Oscillations,
Dynamical Systems, and Bifurcations of Vector Fields.
Springer, 1983.

[33] K. Smith. Solving Combinatorial Optimization Problems
Using Neural Networks. PhD thesis, University of Melbourne,
March 1996.

[15] G. Gutin, A. Yeo, and A. Zverovich. Traveling salesman
should not be greedy: Domination analysis of greedy-type
heuristics for the TSP. Discrete Applied Mathematics, 117(13):81–86, 2002.

[34] L. Vandenberghe and S. Boyd. Semidefinite programming.
SIAM Review, 38(1):49–95, 1996.

[16] X. He, X. Fang, and J. Yu. Distributed energy management
strategy for reaching cost-driven optimal operation integrated
with wind forecasting in multimicrogrids system. IEEE
Transactions on Systems, Man, & Cybernetics. Part A:
Systems & Humans, 49(8):1643–1651, 2019.

[35] P. Wang, C. Shen, A. Hengel, and P. Torr. Large-scale
binary quadratic optimization using semidefinite relaxation
and applications. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 39(3):470–485, 2017.
[36] Z. Yang, A. Bose, H. Zhong, N. Zhang, Q. Xia, and
C. Kang. Optimal reactive power dispatch with accurately
modeled discrete control devices: A successive linear
approximation approach. IEEE Transactions on Power
Systems, 32(3):2435–2444, 2016.

[17] X. He, J. Yu, T. Huang, and C. Li.
Distributed
power management for dynamic economic dispatch in the
multimicrogrids environment. IEEE Transactions on Control
Systems Technology, 27(4):1651–1658, 2019.
[18] J. Hopfield and D. Tank. Neural computation of decisions
in optimization problems. Biological Cybernetics, 52(3):141–
152, 1985.

[37] P. Yi, Y. Hong, and L. Feng. Initialization-free distributed
algorithms for optimal resource allocation with feasibility
constraints and its application to economic dispatch of power
systems. Automatica, 74:259–269, 2016.

[19] B. Huang, L. Liu, H. Zhang, Y. Li, and Q. Sun. Distributed
optimal economic dispatch for microgrids considering
communication delays. IEEE Transactions on Systems, Man,
& Cybernetics. Part A: Systems & Humans, 49(8):1634–1642,
2019.
[20] B. Kamgar-Parsi and B. Kamgar-Parsi. Dynamical stability
and parameter selection in neural optimization.
In
International Joint Conference on Neural Networks, page
566571, 1992.
[21] H. Khalil. Nonlinear Systems. Prentice Hall, 2002.
[22] D. Li, X. Sun, S. Gu, J. Gao, and C. Liu. Polynomially
solvable cases of binary quadratic programs.
In
A. Chinchuluun, P. Pardalos, R. Enkhbat, and I. Tseveendorj,
editors, Optimization and Optimal Control, pages 199–225.
Springer, 2010.
[23] Z. Q. Luo, W. K. Ma, A. So, Y. Ye, and S. Zhang. Semidefinite
relaxation of quadratic optimization problems. IEEE Signal
Processing Magazine, 27(3):2034, 2010.
[24] J. Mandziuk. Solving the travelling salesman problem with
a Hopfield-type neural network. Demonstratio Mathematica,
29(1):219–231, 1996.
[25] B. Mityagin. The zero set of a real analytic function.
arXiv:1512.07276v1, 2015.
[26] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis
of approximations for maximizing submodular set functionsI. Mathematical Programming, 14(1):265–294, 1978.
[27] V. Pan and Z. Chen. The complexity of the matrix
eigenproblem. In ACM Symposium on Theory of Computing,
pages 507–516, 1999.

13

