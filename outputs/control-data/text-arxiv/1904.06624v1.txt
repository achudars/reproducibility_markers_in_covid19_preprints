Biphasic Learning of GANs for High-Resolution Image-to-Image Translation
Jie Cao1 , Huaibo Huang1 , Yi Li1 , Jingtuo Liu2 , Ran He1 , Zhenan Sun1
1
CRIPAC & NLPR, Institute of Automation, Chinese Academy of Sciences.
2
Baidu, Inc.
{jie.cao,huaibo.huang,yi.li}@cripac.ia.ac.cn, {rhe,znsun}@nlpr.ia.ac.cn

arXiv:1904.06624v1 [cs.CV] 14 Apr 2019

{liujingtuo}@baidu.com

Figure 1: Image-to-image translation results at 10242 resolution. Given source images (the first row), our algorithm automatically produces transformed images (the second row) with specified attributes (blond hair, adding makeup, and aging). Due
to the limitation of submission size, we have to downsample this figure.

Abstract

we present a novel training framework for GANs, namely
biphasic learning, to achieve image-to-image translation
in multiple visual domains at 10242 resolution. Our core
idea is to design an adjustable objective function that varies
across training phases. Within the biphasic learning framework, we propose a novel inherited adversarial loss to
achieve the enhancement of model capacity and stabilize

Despite that the performance of image-to-image translation has been significantly improved by recent progress
in generative models, current methods still suffer from severe degradation in training stability and sample quality
when applied to the high-resolution situation. In this work,
1

the training phase transition. Furthermore, we introduce a
perceptual-level consistency loss through mutual information estimation and maximization. To verify the superiority
of the proposed method, we apply it to a wide range of facerelated synthesis tasks and conduct experiments on multiple
large-scale datasets. Through comprehensive quantitative
analyses, we demonstrate that our method significantly outperforms existing methods.

1. Introduction
Image-to-image translation is a classical vision problem whose goal is to learn the mapping function among
multiple visual domains. Different from image generation from noise, the translation task requires a balanced approach that meets both the visual quality and overall consistency between the source and the transformed images. Recent progress in generative methods, including generative
adversarial networks (GANs) [8], variational autoencoders
(VAE) [20], and autoregressive models [28], have dramatically improved the quality of the transformed images. However, high-resolution image-to-image translation is far from
being solved. When dealing with images at 10242 , the sample quality and training stability of current methods decline
sharply, portending there is a long road to achieve true photorealism generation.
To push the frontiers of image-to-image translation to a
higher resolution, we design a novel training method for
GANs, namely biphasic learning, that improves the adversarial supervision manner. Our method has two distinct
characteristics: 1) our generator and discriminator start joint
adversarial training from dealing with images at a relatively
low resolution in the initial phase, and then they are enhanced to produce full-resolution images in the enhancing
phase; 2) the evaluation and optimization criterions of our
loss items update with the training phase transition. We regard the transition from the initial to the enhancing training phase as a knowledge transfer process [13]. The discriminator in the former stage plays the role of teacher net
and guides our network in the latter stage to inherit the
learned knowledge. In the enhancing phase, we adjust the
distillation objective to the adversarial learning condition.
Consequently, we propose the inherited adversarial loss to
guide the competing training process of the generator and
the discriminator. Furthermore, in the initial phase, we borrow several auxiliary discriminators employed in normalresolution translation tasks. In the enhancing phase, we
fix these discriminators, turning the adversarial training into
supervised training. Thanks to the effective adjustable supervision manner, our network is stable during the training phase transition and produces high-resolution samples
of good quality.
We also propose a novel mutual perceptual informa-

tion regularization to maintain the consistency between the
source and the transformed images. Note that we mainly
consider the translation tasks in the unconstrained situations
where the paired training data is not available. Despite that
existing methods, e.g., CycleGAN [43], have made great efforts, their sample quality in the high-resolution situation is
still very limited. They mainly measure image similarity in
the pixel domain, which leads to the neglect of fine-grained
texture information. In the high-resolution situation, the
over-smoothed results caused by the heavy reliance on the
pixel-level loss have significant flaws in visual realism. To
address this issue, we explore an alternative strategy and
maximize the mutual information between the inputs and
outputs. We employ a deep mutual information estimator
trained on external data and design a supervised learning
scheme to guide the estimator to measure image similarity
in the perceptual level. Benefiting from the effective training method, our estimator is robust and more powerful in
capturing the local texture information than existing unsupervised approaches [2, 1, 14]. Equipped with the mutual
perceptual information regularization, our network casts off
the dilemma of either sample quality or visual consistency.
We verify the effectiveness of our approach on CelebAHQ [17]. Furthermore, we also conduct comparison experiments on normal-resolution face datasets, including RaFD
[21] and Multi-PIE [10]. Experimental results demonstrate
that our approach not only outperforms current state-of-theart methods in quantitative measurements but also advances
the visual appearance and the resolution of image-to-image
translation.

2. Related work
High-resolution image generation is a broad and challenging study topic in computer vision and graphics. Particular study interest in this area has been raised along with
the release of high-resolution datasets [17, 18]. In the field
of image generation originating from noise, PGGAN [17]
achieves 10242 resolution for the first time, and then subsequent work [3] improves the sample variety and quality. Later on, a style-based generator [18] is represented
to achieve conditional high-resolution generation. Wang et
al. [36] make progress on high-resolution interactive image editing via semantic label maps. Yin et al. [38] focus on high-resolution facial attribute editing and propose
a scale-invariant approach based on geometric flow. Compared with existing work, the most significant difference is
that we aim at achieving the translation of any two visual
domains in a multi-domain translation task by training one
single generator.
The notion of Knowledge distillation (KD) is originally proposed by Hinton et al. [13]. The main purpose
is to transfer the learned knowledge from a large powerful
teacher network to a small compact student one. To nar-

Figure 2: An illustration of the proposed biphasic learning. The left and the right parts depict the network forward propagation
in POV (point-of-view) of the generator and the discriminator, respectively. In the initial phase (shown in the upper part), the
networks are trained in the classical adversarial training scheme. In the enhancing phase (shown in the lower part), importance
weighting and knowledge distillation schemes are integrated into the training process with the aid of the discriminator trained
in the former phase. The target domain label input is omitted for clarity.
row the performance gap between student and teacher networks, embedding matching [34], attention map matching
[40], Gramian matrix matching [37] methods are successively proposed. A new perspective of KD is brought by
the finding in Born-Again Network (BANs) [6]: Training a
student net with identical model structure can bring a significant outperformance than the teacher. Inspired by BANs,
we introduce KD methods to train a student net that has enhanced model capacity and deals with a higher resolution
than the teacher. In this work, we extend KD to the area
of adversarial image generation and propose the inherited
adversarial loss.

Representation learning methods aim at seeking informative representation with desired properties, including
compactness [9], disentanglement [7], independence [2],
etc. Mutual information (MI) is a reliable measure of statistical dependence that depicts the informativeness and diversity between random variables, but how to compute the
MI of high-dimensional data is an intractable problem. Recently, a very promising general-purpose solution [2] that
resorts to training a deep neural MI estimator is proposed.
Subsequent works [1, 14] make further improvements and
verify the effectiveness on an extensive range of applications. In this work, we explore a supervised training approach to replace the joint adversarial training for the estimator.

3. Approach
Given N different visual domains {Xi }N
i=1 , our goal is
to learn the mappings among these domains from training
samples. We assume that:
∀ i, j ∈ {1, 2, · · · , N }, i 6= j, ∃ fij : Xi 7→ Xj ,

(1)

where fij is a bijection. Furthermore, we assume that the
training data is unpaired. We make these assumptions based
on the conditions of a wide range of translation tasks, including facial attribute transfer [5], facial animation [32],
etc. In this paper, we consider that all the visual domains consist of human faces. We mainly study the situation where the visual domains consist of extremely highdimensional data, i.e., images at a resolution of 10242 in our
experiments.

3.1. Biphasic adversarial learning
We first describe the core idea of our biphasic learning,
which is illustrated in Fig. 2. We aim at employing a single conditional generator to learn the multi-domain mapping function. In the initial phase, we train a network G1 to
take the input image x at 2562 resolution and the target domain label c to achieve the low-resolution translation. In the
meantime, the corresponding discriminator, T , provides adversarial supervision for G1 by learning to discriminate the
real and the synthesized images. Without loss of generality,
we adopt the vanilla adversarial loss function of GAN:
LG
adv = Ex,c [− log(T (G1 (x, c))],

(2)

where the superscript on the loss item indicates that the loss
belongs to the generator or the discriminator. Correspondingly, the adversarial loss for T is formulated as:
LD
adv = Ex [− log(T (x))] +
Ex,c [− log(1 − T (G1 (x, c))].

(3)

In the initial phase, we employ the standard adversarial
learning scheme and build our backbone generation model.
We emphasize our major contribution is exploring an effective method to achieve the inheritance and enhancement of
model capacity in the staged training process, which is reflected in the enhancing phase. In this phase, our model
is trained for full-resolution image generation, and we replace the loss function with the proposed inherited adversarial loss. To adopt the increase of the spatial resolution,
we attach additional neural layers G2 to G1 , proposing the
enhanced generator G. Whereas we train a new discriminator D with similar architecture but more layers under the supervision of the former discriminator T , which is regarded
as the teacher net.
Recall that recent work [6] in Knowledge Distillation
(KD) [13] pointed out that benefiting from the dark knowledge, the student net can even achieve a better performance
than the teacher net. However, [6] assume that the training data distribution is static, i.e., the teacher and the student learn from the same data distribution that remains unchanged across the training process. This assumption does
not hold in our adversarial learning situation because D
draws fake data from G(x, c), which varies with the change
of its parameters. Actually, replacing the data label with
the prediction of T in the second term of Eq. 3 violates the
principle of adversarial training: if a sample is misclassified
by T , the label replacement will advocate D neglect this
mistake. Therefore, in Eq. 3, the guidance introduced from
KD is only feasible for the first term because D still draws
real data from a static data distribution. In summary, the inherited adversarial loss for our discriminator is formulated
as:
LD
adv = Ex,c [− log(1 − D(G(x, c))] +
Ex [−T (x) log(D(x)) − (1 − T (x)) log(1 − D(x))].

(4)

In Equation 4, the supervision for classifying the real
data is provided by the teacher net instead of the ground
truth label. Learning the dark knowledge distilled from the
initial phase, our network is less sensitive to the noise in the
training data.
However, we have not got a complete distillation method
for GANs yet. The power of T is not been fully utilized because Eq. 4 discards the knowledge about the fake data. To
address this issue, we design a sample importance weighting method for our G. Formally, the inherited adversarial
loss for the generator is:

LG
adv = Ex,c [−

ω(T (x))
· log(D(G(x, c))],
2
0

(5)

0

T (x)−min(T (x))
where ω(T (x)) = 1 + max(T
0 (x))−min(T 0 (x)) , and we de0
fine T (x) = 1 − T (x). The first constant item in our
weighting factor stands for the weight inherited from the
ordinary adversarial training; The second item ranges between 0 and 1 and attaches more importance to those samples that are not good at deceiving the teacher net. Equipped
with our importance weighting scheme, the generator combines the guidance from the data label and the prediction of
the teacher net.

3.2. Mutual perceptual information maximization
Although applying the proposed approaches above effectively guides our network to transfer large-scale images
with high quality, there is still one issue remained unsolved:
the consistency of these irrelevant attributes. For instance,
if we want to change the hair color of a given source face
image, on the one hand, the color should be changed and
the visual realism should be preserved; On the other hand,
we expect that the difference in the attributes except for the
hair color is imperceptible. However, calculating the adversarial loss alone does not consider the correspondence
between the input and the output images. Therefore, we advocate maximizing mutual information (MI) to maintain the
consistency between the input and the output.
The mutual information of high-dimensional data is notoriously difficult to compute, but recently brand new approaches [2, 1, 14] based on GANs have been explored.
They estimate MI by training a deep neural estimator jointly
with the generator and the discriminator. Different from
their methods, we employ a classifier pre-trained on external datasets in the supervised manner. Since we focus
on generating human faces in our experiments, we resort
to a face recognizer Tω trained on publicly available face
datasets as the estimator. Tω measures image similarity in
the perceptual level, so our proposed MI maximization is
referred to as mutual perceptual information maximization.
Concretely, given a pair of images, x1 and x2 , the similarity
is calculated as:
Tω (x1 , x2 ) = kφ(x1 ) − φ(x2 )k2 ,

(6)

where φ(·) denotes the extracted identity representation obtained by the second last fully connected layer within Tω
and k · k2 means the vector 2-norm. Then our mutual perceptual information loss is defined as:
b
LG
mp = −I(x, G(x, c); Tω ),

(7)

where Ib represents the mutual information estimation
funciton. Specifically, we adopt Noise-Contrastive Mutual

Infomation Estimation (infoNCE) [29]. Combining Eq. 6,
Eq. 7, and infoNCE, our mutual perceptual information is
measured as:
h
b
I(x,
G(x, c); Tω ) = Ex,c Tω (x, G(x, c)) −
h X
ii
eTω (x,G(x̃,c) ,
Ex̃,c log

(8)

x̃

where x̃ denotes a data batch resampled from the training
set. During the two training phases, we both calculate LG
mp
in the same manner, i.e., the form of LG
mp does not change
across our biphasic learning process.

3.3. Biphasic regularization
We also integrate several regularization items into our total loss function. These items have been proved to be beneficial for normal-resolution image generation tasks. In this
work, we adapt them to the biphasic form to cope with highresolution conditions.
We train an auxiliary network, Duv , to predict the UV
correspondence field (the UV field for short) of the input
face. The UV field is a powerful representation that depicts the geometric information of the human face. Compared with other geometric guidances for face generation,
e.g., landmarks and facial parsing maps, the UV field has
two significant advantages: 1) The input face is related to
an implicit facial texture map that indicates its identity, and
this relationship is pixel-wise; 2) 3D geometric prior is subtly integrated by the UV field. Following [4], we construct
the ground truth UV field via fitting a 3D Morphable Model
[30] through the Multi-Features Framework [33]. In the initial stage, Duv is trained to correctly predict the UV field of
the real faces, whereas G is trained to make Duv think the
transformed faces have the same UV field with its corresponding real faces. Concretely, we optimize:
uv

uv
LD
geom = Ex kD (x) − uvk1 ,

LG
geom

uv

= Ex,c kD (G1 (x, c)) − uvk1 ,

(9)
(10)

where k · k1 denotes calculating the mean of the elementwise absolute value summation of a matrix. In the enhancing phase, we fix the parameters of Duv , i.e., LG
geom is optimized with the guidance rather than the competition of
Duv . The reason is that normal-resolution images are already sufficient for estimating facial geometric information.
Therefore, keeping training Duv in the enhancing phase is
of no benefit. We find that applying our biphasic UV loss
is a simple yet effective manner to improve the experimental performance. Note that in our following experiments, if
we consider to alter those attributes that change the original
facial geometric information, e.g., changing facial expressions, the UV loss is excluded.

Furthermore, we introduce an auxiliary classifier, Da , to
predict the facial attribute. Similar with the UV loss, Da
and G are trained jointly in an adversarial manner in the
initial phase, and Da is transformed into a fixed network to
Da
supervise G in the enhancing phase. LG
attr and Lattr , i.e.,
the attribute loss for G and Da , are measured by the crossentropy loss function. The attribute loss guides our network
on the attribute translation in semantic level, ensuring that
the target attribute is imposed on the output.

3.4. Implementation details
The structure of our network is mainly adapted from existing work. We inherit the model structure in StarGAN [5]
to build our G1 and T in the initial phase. In the enhancing phase, the structure for D remains the same except for
some minor modifications to adapt the higher input resolution. Whereas for the generator, a fully convolutional backend network used in [22] is added to produce the enhanced
results. Detailed descriptions are given in Appendix B. We
denote our network as Biphasic Learning GAN (BiL-GAN)
in the following parts.
In our experiment, we replace the vanilla GAN objective function with LSGAN [25]. In summary, our total loss
functions are formulated as:
G
G
G
¨
LG = LG
adv + Lmp + Lgeom + Lattr ,
¨ + (LDuv ) + (LDa ),
LD = LD
adv

geom

attr

(11)
(12)

where the items in brackets do not appear in the enhancing training phase and the items with two dots on top are
calculated differently across the two phases. We set all the
weight factors of the loss items to 1.
We optimize the parameters of our model by Adam [19]
optimizer with β1 = 0.5, β1 = 0.999, and a learning rate
of 1e-4. For data augmentation, we flip the input images
horizontally with a probability of 0.5. The batch size is set
to 64 for all experiments. We train our BiL-GAN on 8 Tesla
V100 GPUs for approximately one week. The Appendix is
available here1 . The source code will be available soon.

4. Experiments
4.1. High-resolution image-to-image translation
To achieve image-to-image translation at 10242 resolution, we train our BiL-GAN on CelebA-HQ [17], which
is the most widely used face dataset for high-quality image synthesis at present. We randomly choose 300 images
from the total 30,000 ones for testing and use the rest part as
the training set. We consider image-to-image translation in
1 https://drive.google.com/file/d/
17t3YwB1OMYoIMaB-8XU55gxzRfM-p7qP/view?usp=
sharing

Figure 3: Facial attribute transfer results on CelebA-HQ. The first column shows the input image, and the rest four columns
show transformed results. Due to the limitation of submission size, we have to downsample this figure. Please zoom in for
better visualization.

Method

Aging

Makeup

Hair color

Ours > IcGAN [31]
Ours > CycleGAN [43]
Ours > StarGAN [5]

90.7%
86.3%
73.7%

89.6%
85.5%
71.9%

94.3%
89.2%
72.4%

Table 1: User study results across different attributes on
CelebA-HQ. The percentages of the cases where the user
prefers our results are listed above.

seven different domains: hair color (black, blond, brown),
makeup (with/without), and age (young/old).
Our BiL-GAN is able to alter the involved attributes to
any specified type, and we do not apply any post process,
e.g., warping the transformed facial region to the original
one to make the background identical [23]. Some visual
samples are shown in Fig. 1 for preview and the attribute
translation results are shown in Fig. 3. Please refer to Appendix A for more samples and transfer types. We argue
that the visual quality of the transformed images should be
judged by the following three points: (1) the overall realism, (2) the transfer quality of attribute(s), (3) the preservation of the original identity and the consistency of the
uninvolved attribute(s). We can observe that our method
can precisely change the target attribute and faithfully preserves the character of the original input, including expression, background, etc. Furthermore, we find most transformation results on hair color are very difficult for a human to
distinguish even with unlimited observation time.
We systematically compare our results with existing

methods in all aspects. Concretely, we compare with IcGAN [31], CycleGAN [43], and StarGAN [5]. It is notable that these methods are all designed for facial attribute
translation on CelebA [24] which only consists of normalresolution images. We have tried to train these methods
with 10242 resolution images directly, but they produced
unrealistic results with severe degradation (cf. Appendix
C). Thus we downsample the images in CelebA-HQ to a
resolution of 2562 and use them to retrain the models of
these approaches. However, the performance drop of existing methods is inevitable because of the reduction of training data (CelebA-HQ is only a subset of CelebA). In the following comparisons, we also downsample our results from
10242 to 2562 and calculate quantitative evaluation results.
We conduct a user study to evaluate the visual quality
of samples. For each face in our testing set, we produce 8
different transformed images, including 5 ones with single
altered attribute (hair color, makeup, or age), 3 ones with
two altered attributes. Hence, we get 2,400 transformed
images. Users are asked to compare two transformed images that are controlled by the target domain label but produced by two different methods, and then judge which one
is better. We record the percentage of the cases where the
user prefers our results. Thus, a higher percentage indicates
better performance of our method and 50% means that the
user thinks two methods are equal. Different attributes are
counted separately and the percentages are averaged over
all participated users. Synthesized images with two altered
attributes are counted by both attributes. The experimental
results of the user study are summarized in Table 1, which

Method

FID

MS-SSIM

CLAS Error

IcGAN [31]
CycleGAN [43]
StarGAN [5]
BiL-GAN (ours)

24.16
25.86
19.75
14.93

0.8072
0.8384
0.8509
0.8563

22.17%
14.86%
9.37%
5.49%

single phase learning
progressively learning
ordinary regularization
w/o mutual Info loss
w/ MINE loss
w/ cycle loss

21.38
17.94
16.36
19.15
17.92
18.56

0.8372
0.8511
0.8605
0.8541
0.8539
0.8420

11.70%
8.39%
6.93%
10.48%
7.34%
9.28%

Table 2: Comparisons of Fréchet inception distance (FID),
multi-scale structural similarity (MS-SSIM), and the attribute classification (CLAS) error on the CelebA-HQ
dataset. The upper part shows comparisons with existing
methods, and the lower part shows comparisons with our
proposed variations for ablation study.
demonstrates the superiority of our BiL-GAN.
We also calculate the Fréchet inception distance (FID)
[12], multi-scale structural similarity (MS-SSIM) [27], and
the attribute classification error to make objective comparisons. FID has been recently proposed to reveal the performance of general image generation tasks. In our experiment, we follow [26] and compute FID between the real
and the synthesized faces. Since lower FID score indicates
that the Wasserstein distance between two distributions is
smaller, the method with the lowest FID is considered to be
the one that produce the most realistic samples. MS-SSIM
values range between 0 and 1, and a higher value indicates
stronger perceptual similarity. We compute the MS-SSIM
between the transformed images and the inputs to evaluate
the ability to keep overall consistency. In the meantime, we
employ a pre-trained facial attribute classifier [24] to reflect
the effectiveness of transformation. The synthesized samples are supposed to be classified to the classes that accord
with their corresponding target domain labels. In summary,
we expect that a good method has low FID, MS-SSIM, and
classification error rate. We report these objective performance measurements in the upper part of Table 2. It can be
observed that our BiL-GAN outperforms other approaches
by a large margin. Our FID and classification error are
markedly lower than the second-best method, which clearly
indicates our improvements in visual realism and attribute
transfer.

4.2. Ablation study
In this section, we investigate the contributions of our
biphasic learning and mutual perceptual information regularization by ablation study. To this end, we design a series
of variations and report their performances in the lower part
of Table 2. Our analyses are listed below:

Figure 4: Facial animation results on RaFD. Given the input
(in the green box), our BiL-GAN is able to produce diverse
results (in the blue box) with specified facial expression.
The ground truth is also shown as a reference (in the red
box).

(1) We train a network totally without any biphasic
schemes (denoted as “single phase learning” in the table).
The network is simply trained by following the idea of classical adversarial training [8]. Through comparisons, we observe that single phase learning leads to much inferior performance. Besides, there are significant flaws in the produced samples. Our observation accords with the conclusions in [17, 36]: ordinary single stage adversarial training shows a limited capacity to generate high-resolution images.
(2) We train a network without both the inherited adversarial loss and biphasic regularization items (denoted as
“progressively learning”), and a network just without biphasic regularization items (denoted as “ordinary regularization”). We also add the resolution transition process [17]
for the former variation since it reflects the idea of PGGAN.
Through comparisons, we can see the superiority of the
proposed biphasic learning over the mainstream progressively growing method. Thanks to our supervision transition scheme and the inherited adversarial loss, BiL-GAN
has a considerably lower FID score. Besides, compared
with “ordinary regularization”, BiL-GAN further lowers the
FID score and the classification error at the cost of a slightly
higher MS-SSIM.

Method

±15◦

±30◦

±45◦

±60◦

±75◦

±90◦

DR-GAN [35]
FF-GAN [39]
TP-GAN [16]
CAPG-GAN [15]
PIM [41]
3D-PIM [42]
HF-PIM [4]

94.9
94.6
98.7
99.8
99.3
99.6
99.9

91.1
92.5
98.1
99.6
99.0
99.5
99.9

87.2
89.7
95.4
97.3
98.5
98.8
99.9

84.6
85.2
87.7
90.3
98.1
98.4
99.1

77.2
77.4
83.1
95.0
95.2
96.4

61.2
64.6
66.1
86.5
86.7
92.3

Light CNN [11]
BiL-GAN(Ours)

98.6
99.96

97.4
99.94

92.1
99.93

62.1
99.90

24.2
96.92

5.5
93.11

Table 3: Comparisons on rank-1 recognition rates (%)
across views under Multi-PIE Setting 2.

Figure 5: Face frontalization results on Multi-PIE. Given
the input across different poses (in the green box), our BiLGAN produces frontalized results (in the blue box). The
ground truth is also shown as a reference (in the red box).

(3) We train a network without our mutual perceptual
loss (denoted as “w/o mutual Info loss”), a network that
maximizes mutual information as described in MINE [1]
(denoted as “w/ MINE loss”), and a network that replaces
mutual information loss with the cycle consistency loss [43]
(denoted as “w/ cycle loss”). We design this group of variations to verify the effectiveness of the proposed mutual perceptual information loss. It can be observed that the performances of “w/o mutual Info loss” and “w/ cycle loss”
are inferior, and BiL-GAN also outperforms the variation
that employs adversarial mutual information loss, demonstrating that our pre-trained neural estimator provides more
effective supervision.

4.3. Image-to-image translation for specific tasks
In the discussions above, we mainly focus on addressing general semantical image-to-image translation problem.
Whereas the image-to-image translation can be divided into
many specific tasks. To demonstrate our methods can be
generalized to synthesis tasks in various conditions, we
test our biphasic learning framework on two specific facial attribute transfer tasks, i.e., facial animation and face
frontalization. Accordingly, we make three minor modifications on our BiL-GAN: we remove some up-sampling layers to adjust the spatial size of the output because following

experiments are conducted on currently available normalresolution datasets; we introduce pixel loss calculated with
the ground truth because paired data is available; we remove
the UV loss because the assumption that the facial geometric information should remain unchanged does not hold.
We conduct the facial animation experiment on RaFD
[21], which consists of 67 identities with 8 binary labels for
facial expressions in three gaze directions. We follow the
[5] to preprocess and split the dataset and produce 2562 results. Synthesized samples are shown in Fig. 4. We observe
that our results have appealing visual realism and faithfully
preserve the identity and gaze direction. Besides, another
interesting finding is that our results have the same facial
expression with the ground truth, but they are different in
the pixel domain.
Our face frontalization experiment is conducted on
Multi-PIE [10], which consists 13 poses within ±90◦ of 337
subjects. Those profiles with extreme poses (75◦ and 90◦ )
are very challenging for cross-view face synthesis. We train
BiL-GAN to frontalize the profiles across all poses at 1282
resolution and use the first 200 subjects for training and the
rest 137 for testing. Frontalization results are visualized in
Fig. 5. Face frontalization methods are mainly evaluated by
the performance of face recognition. To this end, existing
works [16, 39, 15] have explored the “generation via recognition” framework to calculate the recognition rate, making
a standard manner for comparisons among different methods. Therefore, we follow [4] and employ a pre-trained
Light CNN [11] to calculate our face recognition rate and
make comparisons with existing methods, as shown in Table 3. Please refer to Appendix B for more details about the
recognition rate calculation. We can see that our approach
further improves the recognition rate in extreme poses, indicating that our model not only produces results with high
visual quality but also preserves the identity information.

5. Conclusion
This paper proposes a novel training method for GANs,
namely biphasic learning, to improve the performance of

image-to-image translation. We apply our approach to a series of applications, including high-resolution and normalresolution, supervised and unsupervised, general and specified translation tasks. As a result, our model produces
visually appealing samples that outperform competing approaches in both objective and subjective evaluations. However, we discover that the quantity and quality of the training data still limit the performances to a great extent (cf.
Appendix D). Fortunately, recently proposed FFHQ dataset
[18] offers human face images with higher quality and
wider variation. In the future, we will explore further improvements with larger datasets.

References
[1] M. I. Belghazi, A. Baratin, S. Rajeswar, S. Ozair, Y. Bengio,
A. Courville, and R. D. Hjelm. MINE: mutual information
neural estimation. In ICML, 2018. 2, 3, 4, 8
[2] P. Brakel and Y. Bengio. Learning independent features with
adversarial nets for non-linear ICA. In ICML Workshop,
2017. 2, 3, 4
[3] A. Brock, J. Donahue, and K. Simonyan. Large scale GAN
training for high fidelity natural image synthesis. In ICLR,
2019. 2
[4] J. Cao, Y. Hu, H. Zhang, R. He, and Z. Sun. Learning a
high fidelity pose invariant model for high-resolution face
frontalization. In NeurIPS, 2018. 5, 8
[5] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo.
StarGAN: Unified generative adversarial networks for multidomain image-to-image translation. In CVPR, 2018. 3, 5, 6,
7, 8
[6] T. Furlanello, Z. C. Lipton, M. Tschannen, L. Itti, and
A. Anandkumar. Born again neural networks. In ICML,
2018. 3, 4
[7] A. Gonzalez-Garcia, J. van de Weijer, and Y. Bengio. Imageto-image translation for cross-domain disentanglement. In
NeurIPS, 2018. 3
[8] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NeurIPS, 2014. 2, 7
[9] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf, and
A. Smola. A kernel two-sample test. JMLR, 2012. 3
[10] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker.
Multi-PIE. IVC, 2010. 2, 8
[11] R. He, X. Wu, Z. Sun, and T. Tan. Learning invariant deep
representation for NIR-VIS face recognition. In AAAI, 2017.
8
[12] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler,
G. Klambauer, and S. Hochreiter. GANs trained by a two
time-scale update rule converge to a Nash equilibrium. In
NeurIPS, 2017. 7
[13] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
in a neural network. In NeurIPS Workshop, 2015. 2, 4
[14] R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Grewal, A. Trischler, and Y. Bengio. Learning deep representations by mutual information estimation and maximization.
In ICLR, 2019. 2, 3, 4
[15] Y. Hu, X. Wu, B. Yu, R. He, and Z. Sun. Pose-guided photorealistic face rotation. In CVPR, 2018. 8
[16] R. Huang, S. Zhang, T. Li, R. He, et al. Beyond face rotation: Global and local perception GAN for photorealistic and
identity preserving frontal view synthesis. In ICCV, 2017. 8
[17] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive
growing of GANs for improved quality, stability, and variation. In ICLR, 2018. 2, 5, 7
[18] T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial networks. arXiv
preprint arXiv:1812.04948, 2018. 2, 9
[19] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. In ICLR, 2015. 5

[20] D. P. Kingma and M. Welling. Auto-encoding variational
bayes. In ICLR, 2014. 2
[21] O. Langner, R. Dotsch, G. Bijlstra, D. H. Wigboldus, S. T.
Hawk, and A. Van Knippenberg. Presentation and validation
of the Radboud faces database. Cogn Emot, 2010. 2, 8
[22] C. Ledig, L. Theis, F. Huszár, J. Caballero, A. Cunningham,
A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al.
Photo-realistic single image super-resolution using a generative adversarial network. In CVPR, 2017. 5
[23] T. Li, R. Qian, C. Dong, S. Liu, Q. Yan, W. Zhu, and L. Lin.
Beautygan: Instance-level facial makeup transfer with deep
generative adversarial network. In ACM MM, 2018. 6
[24] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face
attributes in the wild. In ICCV, 2015. 6, 7
[25] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. Paul Smolley. Least squares generative adversarial networks. In ICCV,
2017. 5
[26] T. Miyato and M. Koyama. cGANs with projection discriminator. In ICLR, 2018. 7
[27] A. Odena, C. Olah, and J. Shlens. Conditional image synthesis with auxiliary classifier GANs. In ICML, 2017. 7
[28] A. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel
recurrent neural networks. In ICML, 2016. 2
[29] A. v. d. Oord, Y. Li, and O. Vinyals. Representation
learning with contrastive predictive coding. arXiv preprint
arXiv:1807.03748, 2018. 5
[30] P. Paysan, R. Knothe, B. Amberg, S. Romdhani, and T. Vetter. A 3D face model for pose and illumination invariant face
recognition. In AVSS, 2009. 5
[31] G. Perarnau, J. Van De Weijer, B. Raducanu, and J. M.
Álvarez. Invertible Conditional GANs for image editing. In
NeurIPS Workshop, 2016. 6, 7
[32] A. Pumarola, A. Agudo, A. M. Martinez, A. Sanfeliu, and
F. Moreno-Noguer. GANimation: Anatomically-aware facial animation from a single image. In ECCV, 2018. 3
[33] S. Romdhani and T. Vetter. Estimating 3D shape and texture using pixel intensity, edges, specular highlights, texture
constraints and a prior. In CVPR, 2005. 5
[34] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta,
and Y. Bengio. Fitnets: Hints for thin deep nets. In ICLR,
2015. 3
[35] L. Tran, X. Yin, and X. Liu. Disentangled representation
learning GAN for pose-invariant face recognition. In CVPR,
2017. 8
[36] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and
B. Catanzaro. High-resolution image synthesis and semantic
manipulation with conditional GANs. In CVPR, 2018. 2, 7
[37] J. Yim, D. Joo, J. Bae, and J. Kim. A gift from knowledge distillation: Fast optimization, network minimization
and transfer learning. In CVPR, 2017. 3
[38] W. Yin, Z. Liu, and C. C. Loy. Instance-level facial attributes
transfer with geometry-aware flow. In AAAI, 2019. 2
[39] X. Yin, X. Yu, K. Sohn, X. Liu, and M. Chandraker. Towards
large-pose face frontalization in the wild. In ICCV, 2017. 8
[40] S. Zagoruyko and N. Komodakis. Paying more attention to
attention: Improving the performance of convolutional neural networks via attention transfer. In ICLR, 2017. 3

[41] J. Zhao, Y. Cheng, Y. Xu, L. Xiong, J. Li, F. Zhao,
K. Jayashree, S. Pranata, S. Shen, J. Xing, and Others. Towards pose invariant face recognition in the wild. In CVPR,
2018. 8
[42] J. Zhao, L. Xiong, Y. Cheng, Y. Cheng, J. Li, L. Zhou, Y. Xu,
J. Karlekar, S. Pranata, S. Shen, et al. 3D-Aided deep poseinvariant face recognition. In IJCAI, 2018. 8
[43] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired imageto-image translation using cycle-consistent adversarial networks. In ICCV, 2017. 2, 6, 7, 8

