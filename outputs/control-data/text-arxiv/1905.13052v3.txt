1

Solving RED with Weighted Proximal Methods

arXiv:1905.13052v3 [eess.IV] 4 Mar 2020

Tao Hong, Irad Yavneh, and Michael Zibulevsky

Abstract‚ÄîREgularization by Denoising (RED) is an attractive
framework for solving inverse problems by incorporating stateof-the-art denoising algorithms as the priors. A drawback of
this approach is the high computational complexity of denoisers,
which dominate the computation time. In this paper, we apply a
general framework called weighted proximal methods (WPMs) to
solve RED efficiently. We first show that two recently introduced
RED solvers (using the fixed point and accelerated proximal
gradient methods) are particular cases of WPMs. Then we
show by numerical experiments that slightly more sophisticated
variants of WPM can lead to reduced run times for RED by
requiring a significantly smaller number of calls to the denoiser.
Index Terms‚ÄîInverse problem, denoising algorithms, RED,
weighted proximal methods, weighting.

I. I NTRODUCTION

seek ways to employ denoisers as priors for quite general
inverse problems. The authors in [13‚Äì15] ‚Äúmanually‚Äù adopted
priors used in existing denoisers for specific alternative inverse
problems. Following this path, several authors proposed a
general framework, called Plug-and-Play Priors (P3 ) [16, 17],
for using the abundance of high-performance image denoisers
as priors for other inverse problems. These authors formulate
inverse problems as an optimization task and employ an Alternating Direction Method of Multipliers (ADMM) algorithm
to tackle the corresponding minimization problem [18]. The
image denoising algorithm is incorporated in each step of
ADMM as an implicit prior.
Motivated by P3 , Romano et al. introduced REgularization
by Denoising (RED) [19], which defines an optimization
problem that includes the denoiser as an explicit prior. Given a
differentiable denoiser f (xx), RED employs the following prior,

T

HE goal of inverse problems is to recover an unknown
signal x ‚àà ‚ÑúN from an indirect measurement y ‚àà ‚ÑúM . The
measurement is commonly modelled as y = H (xx) + e , where
H (¬∑) denotes an abstract operator and e is often assumed to be
white Gaussian noise with mean zero and variance œÉ2 . In this
paper, we assume H (¬∑) to be a linear operator, H (xx) = H x ,
with H ‚àà ‚ÑúM√óN , and focus on natural images. Lacking any
prior knowledge about the signal x , we may reconstruct x via
the maximum likelihood (ML) minimization problem,

1
H x ‚àí y k22 .
(1)
x ‚àóML = arg min kH
x 2
However, it is well-known that this approach is not generally
useful. Even in the simple denoising problem, where H is the
identity matrix, ML results in x‚àóML = y, that is, we simply
recover the noisy image. Furthermore, quite often M < N,
resulting in infinitely many solutions, and even if this is not the
case, H may be highly ill-conditioned. For these reasons, the
prevalent approach is to assume that the signal x is sampled
from some prior distribution, and to employ the maximum
a posteriori probability (MAP) estimator, as formulated in
Section II. In our setting, MAP will result in adding to the
right-hand side of (1) a term Œ±R(xx), where Œ± is a parameter
and R is a regularization as discussed below. This approach
has been applied with a large variety of priors, such as `2 based regularization [1], wavelets [2], total variation [3], kernel
regularization [4], sparsity [5], and neural networks [6].
Naturally, the most widely studied problem in this framework is image denoising, e.g., [5‚Äì9]. Indeed, recent work
suggests that the performance of leading image denoisers is
close to a possible ceiling [10‚Äì12]. The availability of such
powerful denoising algorithms has motivated researchers to
T. Hong, I. Yavneh, and M. Zibulevsky are with the Department of
Computer Science, Technion-Israel Institute of Technology, Haifa, 32000
Israel. (Email: {hongtao,irad,mzib}@cs.technion.ac.il).

1
R(xx) = x T (xx ‚àí f (xx)) ,
2

(2)

where ¬∑T denotes transpose. Under two assumptions this leads
to a convex minimization problem, and standard gradient
based iterative methods are guaranteed to converge to a global
minimum. Further details are provided in Section II.
Using state-of-the-art denoisers to construct priors is appealing, as it enables us to exploit the vast progress in denoising
algorithms for addressing general inverse problems, and RED
is a good framework to achieve this goal due to its flexibility.
However, RED may be relatively expensive because at each
iteration we must apply the denoising algorithm to evaluate
the gradient, and the complexity of denoising algorithms is
generally high. Indeed, the numerical experiments in [19]
reveal this concern. In that paper the authors propose three
solvers for RED, namely, steepest descent (SD), the fixedpoint (FP) method and the ADMM scheme. Amongst these,
the FP method is the most efficient, but it still needs hundreds
of iterations to complete the recovery process.
Recently, [20] employed vector extrapolation to accelerate
the FP method for RED, whereas [21] applies an accelerated
proximal gradient (APG) algorithm1 . Both these approaches
are faster than FP for RED, but they still require dozens
of iterations. In this paper, we propose a general framework
called weighted proximal methods (WPMs) [24]. We show
that FP and APG are in fact two particular variants of WPMs,
and that by seeking a more effective weighting for WPMs we
obtain a faster algorithm.
The rest of this paper is organized as follows. We review
the RED framework and the FP and APG solvers in Section II.
The general WPM scheme is introduced in Section III, and the
choice of weighting is discussed. Numerical experiments on
1 APG

is also known as FISTA [22] or Nesterov‚Äôs acceleration [23].

2

image deblurring and super-resolution tasks are presented in
Section IV to demonstrate the efficiency of WPMs, followed
by conclusions in Section V.
II. RE GULARIZATION BY D ENOISING (RED)
The MAP recovery process is formulated as follows:
x ‚àóMAP

Algorithm 1 The APG Method [21]
Initialization: x 0 , z 0 = x 0 , and t0 = 1.
Iteration:
1: for k = 0, 1, . . . do
2:
Compute x x+1 by solving (7), with z k substituted for x k
as the input
q from the last iteration
1+ 1+4t 2

k
tk+1 =
2
‚àí1
(xxk+1 ‚àí x k )
4:
z k+1 ‚Üê x k+1 + ttkk+1
5: end for

3:

= arg maxx P(xx|yy)
(MAP)
= arg maxx P(yy|xx)P(xx)
(Bayes rule)
= arg minx ‚àí log{P(yy|xx)} ‚àí log P(xx).

Assuming a robust Gibbs-like distribution of x , we have
III. W EIGHTED P ROXIMAL M ETHODS

P(xx) = constant ¬∑ e‚àíŒ±R(xx) ,
where R denotes the so-called prior and Œ± > 0 is a scaling
parameter. Note that small R(xx) corresponds to highly probable
signals. If e is sampled from white Gaussian noise with mean
zero and variance œÉ2 , then we have

This leads to the following minimization problem [25],
1
H x ‚àí y k22 + Œ±R(xx).
kH
2œÉ2
Substituting the RED prior (2) into (3), we obtain
x

(3)

1
Œ±
H x ‚àí y k22 + x T (xx ‚àí f (xx)) . (4)
kH
2
x
2œÉ
2
In [19] two assumptions are made regarding the image
denoising algorithm used in RED:
x ‚àóMAP = arg min E(xx) ,

Assumption 1. For any scalar c arbitrarily close to 1, f (cxx) =
c f (xx).
Assumption 2. The spectral radius of the symmetric Jacobian
‚àáx f (xx) is upper bounded by one.
Given Assumption 1, we have
f (xx + Œµxx) ‚àí f (xx)
= f (xx).
Œµ‚Üí0
Œµ
Hence, the gradient of R(xx) is the residual of the denoiser,
‚àáx f (xx)xx = lim

‚àáx R(xx) = x ‚àí f (xx).

min F (xx) , g(xx) + h(xx),
x

(8)

where g and h are convex and differentiable. Denote the
proximal operator by


1
proxh,BB (xÃÇx) = arg min h(uu) + kuu ‚àí xÃÇxk2B ,
(9)
u
2

H x ‚àíyyk22
‚àí 12 kH
2œÉ
,

P(yy|xx) = constant ¬∑ e

x ‚àóMAP = arg min E(xx) ,

Consider the following composite problem and assume its
solution set is nonempty,

(5)

With (5), the gradient of E(xx) becomes
1 T
H x ‚àí y ) + Œ± (xx ‚àí f (xx)) .
H (H
(6)
œÉ2
Assumption 2 implies convexity of R(xx), and therefore of
E(xx) as well. Hence, any solution of ‚àáx E(xx) = 0 yields a
global minimum. This is a nonlinear problem, and we therefore
resort to iterative solvers. One such solver is the FP method
mentioned above, which lags the nonlinear term f (xx):
‚àáx E(xx) =

1 T
H x k+1 ‚àí y ) + Œ± (xxk+1 ‚àí f (xxk )) = 0 .
H (H
(7)
œÉ2
We note that (7) can efficiently be solved for x k+1 exactly
in the Fourier domain if H is block-circulant, or treated iteratively for a general H . The FP method can be accelerated using
the APG approach as described in the following algorithm.
Further discussion of APG can be found in [21].

where B is a symmetric positive definite matrix called
p the
weighting and k ¬∑ kB denotes the B -norm, kqqkB = q T B q .
With these, we describe the explicit form of WPMs for (8) in
Algorithm 2 [24, Chap. 10.7.5]. Note that by setting B = Œ≤II
with Œ≤ > 0, we recover the proximal gradient (PG) method.
Usually, PG is used for (8) when h is nonsmooth [26], whereas
here we use it even though h is differentiable. We do this for
computational efficiency, knowing that applying the denoiser
is the most expensive part of the solution process.
Algorithm 2 Weighed Proximal Methods (WPMs)
Initialization: x 0 .
Iteration:
1: for k = 0, 1, . . . do
2:
pick the step-size ak and the weighting
 Bk
x
3:
x k+1 ‚Üê proxak h,BBk x k ‚àí ak B ‚àí1
‚àá
g(x
)
x
k
k
4: end for
To apply Algorithm 2 to RED, we set g(xx) = Œ±R(xx) =
H x ‚àí y k22 . If h(xx) is convex,
(xx ‚àí f (xx)) and h(xx) = 2œÉ1 2 kH
solving (9) is equivalent to satisfying the first-order optimality
condition,
‚àáu h(uu) + B (uu ‚àí xÃÇx) = 0 .
(10)
Œ± T
2x

xk ), h(uu) ‚Üê ak h(uu) and u ‚Üê
Substituting xÃÇx ‚Üê x k ‚àí ak B ‚àí1
k ‚àáx g(x
x k+1 , at the kth iteration into (10) and rearranging, we obtain

a
a
k T
H
H
B
xk+1 = k2 H T y + Bk xk ‚àí ak Œ± (xxk ‚àí f (xxk )) .
+
k
œÉ2
œÉ
(11)
In this paper, we use the conjugate gradient (CG) method to
approximately solve (11) for x k+1 .
Next we discuss possible practical choices for the weighting
B k . Note first that if we set B k = Œ±II , where I is the identity
matrix, and select the step-size ak = 1, (11) is reduced to
(7) and we recover the FP method. Moreover, by using the
accelerated version of Algorithm 2 (cf. [24, Chap. 10.7.5]) we

3

get APG [21] . We now propose a more elaborate approach
of choosing some approximation to the Hessian of Œ±R(xx)
as the weighting. (Because of the abstract denoiser in R(xx),
the exact Hessian is not computable.) Specifically, we choose
the symmetric-rank-one (SR1) approximation to the Hessian
[27, Chap. 6.2], as is used in quasi-Newton methods. The
SR1 approximation is described in Algorithm 3. This choice
yields faster convergence in our experiments than either FP
or APG, as shown below. We henceforth use WPM to denote
Algorithm 2 with the weighting chosen by Algorithm 3.

gradients are Lipschitz continuous, WPM with SR1 estimation,
an appropriate step-size ak , and exact solution of (9), converges
linearly; see details in [29]. Because we depart from these
strict requirements for efficiency, we cannot claim provable
convergence in our implementation. However, in all our experiments WPM converged. Finally, we note that [21] challenges
the validity in practice of the underlying assumptions of RED
for most denoisers, concluding that (6) is not truly the gradient
of (4). Nevertheless, setting (6) to zero, as is the objective of
all the algorithms we discuss here, remains a most attractive
method for signal recovery.

Algorithm 3 SR1 updating
Initialization: k = 1, Œ≥ = 1.25, Œ¥ = 10‚àí8 , x k , x k‚àí1 , ‚àág(xxk ),
‚àág(xxk‚àí1 ).
1: if k = 1 then
2:
B k ‚Üê Œ±II
3: else
4:
Set s k ‚Üê x k ‚àí x k‚àí1 and m k ‚Üê ‚àág(xxk ) ‚àí ‚àág(xxk‚àí1 )
mk k22
km
5:
Calculate œÑ ‚Üê Œ≥ hss ,m
k mk i
6:
if œÑ < 0 then
7:
B k ‚Üê Œ±II
8:
else
9:
H 0 ‚Üê œÑII
mk ‚àí H 0 s k k2 then
mk ‚àí H 0 s k , s k i | ‚â§ Œ¥kssk k2 km
10:
if | hm
11:
uk ‚Üê 0
12:
else
H 0 sk
13:
u k ‚Üê ‚àö m k ‚àíH
mk ‚àíH
H 0 s k ,ssk i
hm

end if
B k ‚Üê H 0 + u k u Tk
end if
end if
18: Return: B k

14:
15:
16:
17:

Unlike the traditional SR1, we formulate each B k from the
initial H 0 rather the previous iterate B k‚àí1 [27]. Moreover, we
scale H 0 by Œ≥ > 1 as suggested in [28], which we found useful
in practice. In the practical implementation of Algorithm 3,
we efficiently represent B k as a matrix-vector multiplication
operator rather than as an explicit matrix.
In general, the step-size ak in Algorithm 2 needs to be
chosen by some line search process to guarantee monotonically decreasing objective values at each iteration. However,
because evaluating the objective value in RED requires calling
the denoiser, standard line search methods may dramatically
increase the complexity of the algorithm. To maintain a low
computational cost, we fix ak = 1 and reduce the step-size by
half only if the objective value exhibits a relative growth above
some threshold, i.e., E(xxk+1 ) ‚àí E(xxk ) > ŒµE(xxk+1 ), where we
use Œµ = 10‚àí2 in all our experiments. In practice, we found that
we never needed to reduce the step-size.
In this paper we only investigate the SR1 approximation to
the Hessian of Œ±R(xx). We acknowledge that a more accurate
Hessian estimate may prove to be even more cost-effective for
RED, but leave such investigation to future work. Because we
use an approximate Hessian for the weighting, our algorithm
is equivalent to a quasi-newton proximal method. It follows
that if both g(xx) and h(xx) are strongly convex and their

IV. N UMERICAL E XPERIMENTS
In this section we investigate the performance of solvers for
RED. Following [19], we perform our tests on image deblurring and super-resolution tasks and use the trainable nonlinear
reaction diffusion (TNRD) [6] method as the abstract denoiser.
We remark that one can adopt deep denoising techniques
instead of TNRD, since the differentiability requirement of the
denoiser is not mandatory in practice [21]. This may possibly
lead to improved results in practice, but we do not investigate
such options here. Also, since the authors in [19] already
show the superiority of RED for image deblurring and superresolution tasks compared with other popular algorithms, we
largely omit such comparisons in this paper and concentrate
on computational efficiency. Moreover, the experiments conducted in [20] demonstrated that the FP method converges
faster than LBFGS and Nesterov‚Äôs acceleration for RED.
Therefore, we only compare WPM to FP [19], FP-MPE [20],
and APG [21]. All of the experiments are carried out on a
laptop with Intel i7 ‚àí 6500U CPU @2.50GHz and 8GB RAM.
For image deblurring, the image is degraded by convolving
with a point spread function (PSF), 9 √ó 9 uniform blur or
a Gaussian blur with a standard derivation 1.6,
‚àö and then
adding Gaussian noise with mean zero and œÉ = 2. The recovered peak-signal-to-noise ratio (PSNR) versus the number
of denoiser evaluations (left column) and running time (right
column) when using RED for the ‚ÄúStarfish‚Äù image are shown
in Figure 2. We find that the performances of FP-MPE and
APG are similar, whereas WPM is more efficient than both,
requiring less denoiser evaluations and running time to achieve
a comparable PSNR. These results also indicate that indeed
the denoiser dominates the complexity of solving RED.
Next, we test the algorithms on image super-resolution.
A low resolution image is generated by blurring a highresolution image with a 7 √ó 7 Gaussian kernel with standard
derivation 1.6, and then downscaling by a factor of 3. To the
resulting image we add Gaussian noise with mean zero and
œÉ = 5, resulting in our deteriorated image. The PSNR of the
recovered fine-resolution image versus the number of denoiser
evaluations (left) and running time (right) for the ‚ÄúPlants‚Äù
image are presented in Figure 3. Again, we observe that WPM
requires less denoiser evaluations and running time to achieve
a comparable PSNR.
Examining the performance of the algorithms further, we
run them on eight additional images tested in [19]. For each
image, we run the FP method with 200 denoiser evaluations

4

Deblurring ‚Äì Uniform

Plants-Downscale

Plants-Downscale

(a) Original (b) Blurred

(c) 27.94

(d) 28.60

(e) 29.01

(f) 29.85

31.5

31

31

30.5

30.5

PSNR

PSNR

31.5

30

29.5

100

30

29.5
FP
FP-MPE
APG
WPM

29

Deblurring ‚Äì Gaussian

FP
FP-MPE
APG
WPM

101

102

29

103

100

101

Denoiser Evaluations

102

103

Seconds

Fig. 3. PSNR versus denoiser evaluations (left) and CPU time (right) for
super-resolution of the ‚ÄúPlants‚Äù image.
(g) Original (h) Blurred

(i) 30.13

(j) 30.91

(k) 31.23

(l) 31.63
TABLE I
D ENOISER EVALUATIONS REQUIRED TO ATTAIN A SIMILAR PSNR. T HE

Super-Resolution

FIRST AND SECOND ROWS PER EACH IMAGE REFER TO IMAGE
DEBLURRING AND THE THIRD ROW REFERS TO SUPER - RESOLUTION . T HE
MINIMAL NUMBER OF DENOISER EVALUATIONS IS MARKED IN BOLD .

(m) Original

(n) LR

(o) 24.56

(p) 25.13

(q) 25.38

(r) 26.20
Butterfly

Fig. 1. PSNR (dB) of the image recovered by, from left to right, FP, FP-MPE,
APG, and WPM, after 10 denoiser evaluations. LR stands for low-resolution.
Boats

and take the final PSNR as a benchmark. Then we examine
how many denoiser evaluations are needed for APG, FPMPE and WPM, to achieve a similar PSNR. The results are
listed in Table I. Evidently, with the exception of ‚ÄúBoats‚Äù
and ‚ÄúHouse‚Äù in the deblurring task, we observe that WPM
requires the smallest number of denoiser evaluations to achieve
a comparable PSNR, demonstrating its efficiency for solving
RED. Additionally, we present the recovered results of the
‚ÄúStarfish‚Äù and ‚ÄúLeaves‚Äù images from deblurring with uniform
and Gaussian blurs, respectively, and the ‚ÄúButterfly‚Äù image
from super-resolution in Figure 1 to visualize the effectiveness
of RED solved by WPM.

30

29

29

28

27

FP
FP-MPE
APG
WPM

28

26

25

25

100

101

102

103

100

101

Denoiser Evaluations

102

103

Seconds

(a) Deblurring with uniform Blur.
Starfish-GaussianBlur
32.5

32

32

31.5

31.5

31

31

FP
FP-MPE
APG
WPM

PSNR

PSNR

Starfish-GaussianBlur
32.5

30.5

30

29.5

29.5

10 1

10 2

Denoiser Evaluations

10 3

FP
FP-MPE
APG
WPM

30.5

30

10 0

Lena

Barbara

Peppers

Leaves

WPM
25
17
26
21
22
12
19
25
10
20
36
28
29
15
18
11
16
11
22
34
28
34
14
12

V. C ONCLUSION
FP
FP-MPE
APG
WPM

27

26

Parrot

APG
34
26
51
20
34
20
18
26
15
30
40
31
34
16
26
12
23
15
29
40
30
41
18
41

Starfish-UniformBlur

30

PSNR

PSNR

Starfish-UniformBlur

House

FP-MPE
54
54
80
24
60
36
24
62
18
39
52
49
48
47
37
14
48
17
42
41
38
50
36
60

100

101

102

103

Seconds

(b) Deblurring with Gaussian Blur.
Fig. 2. PSNR versus denoiser evaluations (left column) and CPU time (right
column) for deblurring the ‚ÄúStarfish‚Äù image.

In this paper, we propose a general framework for RED
called weighted proximal methods (WPMs). By setting B k =
Œ±II and ak = 1, we retrieve the FP and APG methods. However,
by choosing the weighting to be an approximation to the
Hessian of Œ±R(xx), we obtain a more efficient algorithm.
The experiments on image deblurring and super-resolution
tasks demonstrate that WPM with a simple and inexpensive
approximation to the Hessian can substantially reduce the
overall number of denoiser evaluations in the recovery process,
usually resulting in significant speedup. In future work we aim
to design better Hessian approximations in order to accelerate
the computation further.

5

R EFERENCES
[1] M. A. King, R. B. Schwinger, P. W. Doherty, and
B. C. Penney, ‚ÄúTwo-dimensional filtering of spect images
using the metz and wiener filters,‚Äù Journal of Nuclear
Medicine, vol. 25, no. 11, pp. 1234‚Äì1240, 1984.
[2] A. Chambolle, R. A. De Vore, N.-Y. Lee, and B. J. Lucier,
‚ÄúNonlinear wavelet image processing: variational problems, compression, and noise removal through wavelet
shrinkage,‚Äù IEEE Transactions on Image Processing,
vol. 7, no. 3, pp. 319‚Äì335, 1998.
[3] L. I. Rudin, S. Osher, and E. Fatemi, ‚ÄúNonlinear total
variation based noise removal algorithms,‚Äù Physica D:
nonlinear phenomena, vol. 60, no. 1-4, pp. 259‚Äì268,
1992.
[4] D. Zoran and Y. Weiss, ‚ÄúFrom learning models of natural
image patches to whole image restoration,‚Äù in Computer
Vision (ICCV), 2011 IEEE International Conference on.
IEEE, 2011, pp. 479‚Äì486.
[5] M. Elad and M. Aharon, ‚ÄúImage denoising via sparse
and redundant representations over learned dictionaries,‚Äù
IEEE Transactions on Image processing, vol. 15, no. 12,
pp. 3736‚Äì3745, 2006.
[6] Y. Chen and T. Pock, ‚ÄúTrainable nonlinear reaction diffusion: A flexible framework for fast and effective image
restoration,‚Äù IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 39, no. 6, pp. 1256‚Äì1272,
2017.
[7] A. Buades, B. Coll, and J.-M. Morel, ‚ÄúA non-local
algorithm for image denoising,‚Äù in Computer Vision
and Pattern Recognition, CVPR, IEEE Computer Society
Conference on, vol. 2, 2005, pp. 60‚Äì65.
[8] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, ‚ÄúImage denoising by sparse 3-d transform-domain collaborative filtering,‚Äù IEEE Transactions on Image Processing,
vol. 16, no. 8, pp. 2080‚Äì2095, 2007.
[9] W. Dong, L. Zhang, G. Shi, and X. Li, ‚ÄúNonlocally
centralized sparse representation for image restoration,‚Äù
IEEE Transactions on Image Processing, vol. 22, no. 4,
pp. 1620‚Äì1630, 2013.
[10] P. Chatterjee and P. Milanfar, ‚ÄúIs denoising dead?‚Äù IEEE
Transactions on Image Processing, vol. 19, no. 4, pp.
895‚Äì911, 2010.
[11] P. Milanfar, ‚ÄúA tour of modern image filtering: New
insights and methods, both practical and theoretical,‚Äù
IEEE Signal Processing Magazine, vol. 30, no. 1, pp.
106‚Äì128, 2013.
[12] A. Levin and B. Nadler, ‚ÄúNatural image denoising:
Optimality and inherent bounds,‚Äù in Computer Vision and
Pattern Recognition (CVPR), 2011 IEEE Conference on.
IEEE, 2011, pp. 2833‚Äì2840.
[13] M. Protter, M. Elad, H. Takeda, and P. Milanfar, ‚ÄúGeneralizing the nonlocal-means to super-resolution reconstruction,‚Äù IEEE Transactions on Image Processing,
vol. 18, no. 1, pp. 36‚Äì51, 2009.
[14] A. Danielyan, V. Katkovnik, and K. Egiazarian, ‚ÄúBm3d
frames and variational image deblurring,‚Äù IEEE Transactions on Image Processing, vol. 21, no. 4, pp. 1715‚Äì1728,

2012.
[15] C. A. Metzler, A. Maleki, and R. G. Baraniuk, ‚ÄúOptimal
recovery from compressive measurements via denoisingbased approximate message passing,‚Äù in Sampling Theory and Applications (SampTA), 2015 International Conference on. IEEE, 2015, pp. 508‚Äì512.
[16] S. V. Venkatakrishnan, C. A. Bouman, and B. Wohlberg,
‚ÄúPlug-and-play priors for model based reconstruction,‚Äù in
Global Conference on Signal and Information Processing
(GlobalSIP), 2013 IEEE. IEEE, 2013, pp. 945‚Äì948.
[17] S. Sreehari, S. V. Venkatakrishnan, B. Wohlberg, G. T.
Buzzard, L. F. Drummy, J. P. Simmons, and C. A.
Bouman, ‚ÄúPlug-and-play priors for bright field electron
tomography and sparse interpolation,‚Äù IEEE Transactions
on Computational Imaging, vol. 2, no. 4, pp. 408‚Äì423,
2016.
[18] S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein et al.,
‚ÄúDistributed optimization and statistical learning via the
alternating direction method of multipliers,‚Äù Foundations
and Trends R in Machine Learning, vol. 3, no. 1, pp.
1‚Äì122, 2011.
[19] Y. Romano, M. Elad, and P. Milanfar, ‚ÄúThe little engine
that could: Regularization by denoising (red),‚Äù SIAM
Journal on Imaging Sciences, vol. 10, no. 4, pp. 1804‚Äì
1844, 2017.
[20] T. Hong, Y. Romano, and M. Elad, ‚ÄúAcceleration of red
via vector extrapolation,‚Äù Journal of Visual Communication and Image Representation, p. 102575, 2019.
[21] E. T. Reehorst and P. Schniter, ‚ÄúRegularization by denoising: Clarifications and new interpretations,‚Äù IEEE
Transactions on Computational Imaging, vol. 5, no. 1,
pp. 52‚Äì67, 2019.
[22] A. Beck and M. Teboulle, ‚ÄúA fast iterative shrinkagethresholding algorithm for linear inverse problems,‚Äù
SIAM journal on imaging sciences, vol. 2, no. 1, pp.
183‚Äì202, 2009.
[23] Y. Nesterov, Lectures on Convex Optimization. Springer,
2018.
[24] A. Beck, First-Order Methods in Optimization. SIAM,
2017, vol. 25.
[25] A. Kirsch, An introduction to the mathematical theory of
inverse problems. Springer Science & Business Media,
2011, vol. 120.
[26] S. Roy and A. Borzƒ±ÃÄ, ‚ÄúA new optimization approach
to sparse reconstruction of log-conductivity in acoustoelectric tomography,‚Äù SIAM Journal on Imaging Sciences, vol. 11, no. 2, pp. 1759‚Äì1784, 2018.
[27] J. Nocedal and S. J. Wright, Numerical Optimization.
Springer, 2006.
[28] S. Becker and J. Fadili, ‚ÄúA quasi-newton proximal
splitting method,‚Äù in Advances in Neural Information
Processing Systems, 2012, pp. 2618‚Äì2626.
[29] S. Becker, J. Fadili, and P. Ochs, ‚ÄúOn quasi-newton
forward-backward splitting: Proximal calculus and convergence,‚Äù SIAM Journal on Optimization, vol. 29, no. 4,
pp. 2445‚Äì2481, 2019.

