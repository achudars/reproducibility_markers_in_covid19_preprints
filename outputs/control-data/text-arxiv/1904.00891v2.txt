Gaussian approximation for
empirical barycenters
Nazar Buzun
n.buzun@skoltech.ru∗

arXiv:1904.00891v2 [math.ST] 17 Jun 2019

Abstract
In this work we consider Wasserstein barycenters (average in Wasserstein distance) in Fourier
basis. We prove that random Fourier parameters of the barycenter converge to Gaussian
random vector by distribution. The convergence approximation has been done in finite√
sample condition with rate O(p/ n) depending on measures count (n) and the dimension of
parameters (p).
Keywords: GAR, Wasserstein distance, multivariate central limit theorem, statistical
learning, convex analysis.

1

Introduction

Monge-Kantorovich distance or Wasserstein distance is a distance between measures. It represents a transportation cost of measure µ1 into the other measure µ2 .
ˆ
Wp (µ1 , µ2 ) =

min

1/p
kx − yk dπ(x, y)
p

π∈Π[µ1 ,µ2 ]

´
where the condition π ∈ Π[µ1 , µ2 ] means that π(x, y) has two marginal distributions: y dπ(x, y) =
´
dµ1 (x) and x dπ(x, y) = dµ2 (y). We focus on regularized W1 distance with probabilistic space
{IRd , B(k · k2 ), L1 }
ˆ
f
W1 (µ1 , µ2 ) = min
kx − ykdπ(x, y) + Rε (π)
π∈Π[µ1 ,µ2 ]

where Rε (π) is a relatively small addition which improves differential properties of the distance.
Namely without Rε (π) we can only bound the first derivative, with it we can bound also the
second derivative. There is the notion of mean in Wasserstein distance, called barycenter µ
b. And
it is the main object in this paper.
µ
b = argmin
µ

n
X

f1 (µ, µi )
W

i=1

Barycenters are center-of-mass generalization. If we look at the barycenter of a set of uniform
measures it fits the common structure form of these measures. If the measures are sampled
from some distribution then their barycenter can be treated as an empirical approximation of the
distribution mean. A simple example is a circles set with means {mi } and radius’s {ri }.
ˆ 2π

1
2
W2 (m1 , r1 ), (m2 , r2 ) =
[(m2 − m1 ) − (r2 − r1 ) cos(a)]2 + [(r2 − r1 ) sin(a)]2 da
2π 0
= (m2 − m1 )2 + (r2 − r1 )2
∗

Skolkovo Institute of Science and Technology, Bolshoy Boulevard 30, bld. 1 Moscow, Russia 121205

1

Figure 1: Illustration for W2 distance computation between two circles (m1 , r1 ) and (m2 , r2 ).
P
P
Their W22 barycenter is also c circle with mean m = n1 ni=1 mi and radius r = n1 ni=1 ri . We refer
to papers [1], [7] for the overview of the barycenters and related study.
It is well known that the center-of-mass in l2 norm converges to a Gaussian random vector.
As for the barycenter it is also expected to have some Gaussian properties. For example the
measures are Gaussian themselves or one-dimensional or circles set then the Gaussian structure
of the barycenter is evident. In circles set case the mean and radius converges to some Gaussian
variables as a sum of independent observations according to Central Limit Theorem. In onedimensional case denoting distribution functions by Fi (x)
ˆ 1
|F1−1 (s) − F2−1 (s)|ds
W1 (µ1 , µ2 ) =
0

one get
n

1 X −1
Fb−1 (s) =
F (s)
n i=1 i
In the case with Gaussian measures with zero mean and variances {Si }
1/2

1/2

W22 (µ1 , µ2 ) = tr(S1 ) + tr(S2 ) − 2 tr((S2 S1 S2 )1/2 )
and for some non-random matrix S∗ [11]
n

1 X 1/2
(S Si S∗1/2 )1/2 + O(1/n)
Sb =
n i=1 ∗
In both last cases one have a mean of independent random variables that converges to Gaussian
random variable (or to Gaussian process in case of Fb−1 (s) by Donsker’s Theorem). In general case
it appears to be very difficult to reveal such convergence because the barycenter doesn’t have an
explicit equation and it is an infinite-dimensional object. In order to handle with this difficulty
we propose to construct a sum of independent variables using projection into Fourier basis and
some novel results from statistical learning theory. The perspective of Fourier Analysis provides a
suitable representation of the Wasserstein distance and it is already studied in the literature [10].
Denote a range of size p of the barycenter Fourier coefficients by


db
µ
(x)
θb = Fp
dx
The first our result is that for some non-random matrix D, non-random vector θ∗ and independent
random vectors {ξi }


n

 X
p
∗
ξi = O √
D θb − θ −
n
i=1
2

Further we show that for some Gaussian vector Z




∗



W1 D(θb − θ ), Z = O
and

p3/2
√
n






p
∗
b
P kD(θ − θ )k > x − P (kZk > x) = O √
n


Statistical Application: The last statement allows as to obtain the confidence region of parameter
θb and describe the distribution inside the region. The bootstrap procedure validity follows from
b using bootstrap it would be close by quantiles to
our proof as well. If one sample kD(θboot − θ)k
the random variable kD(θb − θ∗ )k. This is also relates to the construction of the confidence region.
The Structure of this paper is following. The main Theorems are in Section 2. In Sections 5,6
we compute derivatives of the Wasserstein distance using infimal convolution of support functions.
Section 3 deals with independent parametric models and describes how one can approximate
parameter deviations by a sum of independent random vectors {ξi }. In Section 4 we explore the
barycenters model and check the required assumptions from the 3-rd Section. The final part,
Gaussian approximation of the parameter θbp , is completed in Section 7, where we prove that {ξi }
is close to Z by distribution and by W1 .

2

The main result

Consider a set of random measures (random measure is a measure-valued random element)
with densities φ1 , . . . , φn . Let the barycenter measure µ
b has density φb and Fourier coefficients
∞
b ∈ IR .
θb = θ(φ)
n
X
f1 (φ, φi )
φb = argmin
W
φ

Let Fourier basis
function f

{ψk }∞
k=1

i=1

has a Gram function of the scalar product G(x), such that for some
ˆ
hf, ψk iG = f (x)ψk (x)G(x)dx
ˆ

and
θ(ϕ)[k] =

ϕ(x)ψk (x)dx

Denote Fourier coefficients of the other measures ∀i : θi = θ(ϕi ) ∈ IR∞ . Basing on Lemma 9 define
an independent parametric model with dataset (θ1 , . . . , θn ) and parameter θ.
L(θ) =

n
X

l(θ − θi )

i=1

where
T
f
l(θ − θi ) = max
T hη, θ − θi i − εη (K ◦ G)η = W1 (φ, φi )
η∈

and
that

T

Ex

Ex is a Sobolev ellipsoids intersection. Each ellipsoid Ex has matrix Kx = ∇T ψ∇ψ T (x) such

2
X

ηk ∇ψk (x) = η T Kx η
k∈Nd+

and
\


Ex = η : ∀x : η T Kx η ≤ 1

3

Define a positive matrix K ◦ G =

´

Kx G(x)dx

1/T 2


K ◦G= 0
 0
...

such that in case ψk (x) = eik

0
0
...0

..
.
0
. . . 0

. . . k 2 /T 2 . . . 0
...
...
...

T x/T

Define for this model MLE parameter value and reference parameter value:
θb = argmin L(θ)
θ

θ∗ = argmin IEL(θ)
θ

Define a local region around θ∗
Ω(r) = {θ : kD(θ − θ∗ )k ≤ r}
where D is the Fisher matrix of the model
D2 = −∇2 IEL(θ∗ )
Theorem 1. Let the random Fourier parameters of the dataset have a common density θ1 . . . θn ∼
q(θ) and it fulfills condition ∀θ ∈ Ω(r)
ˆ
CQ
kD−1 ∇q(θ)kdθ = √
n
b θ∗ ∈ IR∞ are Fourier coefficients of the MLE and reference barycenter defined above, then
Let θ,
with probability 1 − e−t
D(θb − θ∗ ) − D−1 ∇L(θ∗ ) ≤ ♦(r, t)
where ♦(r, t) is defined in Section 4 and has asymptotic
√
√


√
nO(rCQ + r pD + 2t)
1

+o √
♦(r, t) =
n
ελmin DK ◦ GD
and pD is an ellipsoid entropy (Section 3.3.) with matrix D
s
X log2 (λ2 (D))
i
pD =
2
λi (D)
i
and with probability 1 − e−t
−1

∗

r ≤ 4kD ∇L(θ )k ≤

√
√
8 n(1 + 2t)
1/2

λmin DK ◦ GD



Proof. Basing on Theorem 3 one have to prove Assumptions 1,2,3 from which follows
D(θb − θ∗ ) − D−1 ∇L(θ∗ ) ≤ {δ(r) + z(t)}r = ♦(r, t)
with probability 1 − e−t . The Assumptions 1,2,3 are proven in Section 4 where also is shown that
ˆ
rn
 kD−1 ∇q(θ)kdθ
δ(r) =
ελmin DK ◦ GD
4

tR
2t(v2 + 2RE) +
3
p
E = 12v 2pD + 24RpD

z(t) = E +

p

where
v2 =
and
R=

n

DK ◦ GD

ε2 λ2min

ελmin

1

DK ◦ GD

Setting v and R in previous equations gives an asymptotic
√
 
√
√
n(12 2pD + 2t)
1
 +O
z(t) =
n
ελmin DK ◦ GD
Lemma 6 gives bound
kD−1 ∇lk ≤

1
1/2
λmin

DK ◦ GD



From this bound Hoefding’s inequality [5] follows bound for kD−1 ∇L(θ∗ )k.
Define additional Fisher matrix corresponded to the projection into first p elements of the parameter θ (ref. for details in Section 3).
2
2
−2
2
D̆2 = Dp×p
− Dp×∞
D∞×∞
D∞×p

such that
2

D =



2
2
Dp×p
Dp×∞
2
2
D∞×p D∞×∞



and define the gradient of the projection into first p elements of the parameter θ.
˘ = ∇1...p − D2 D−2 ∇p...∞
∇
p×∞ ∞×∞
Theorem 2. Let θbp , θp∗ ∈ IRp are the first p Fourier coefficients of the MLE and reference barycen∗
˘
)]). Then with probability (1 − e−t ) W1 and
ters, and Z is a Gaussian vector N (0, Var[D̆−1 ∇L(θ
probability distances to Z are bounded as follows
W1 (D̆(θbp − θp∗ ), Z) ≤ µ3 O(log(n)) + ♦(r, t)
and ∀z ∈ IR+

|IP (kD(θb − θ∗ )k > z) − IP (kZk > z)| ≤ CA µ3 O(log2 n) + ♦(r, t)
where ♦(r, t) is defined in Theorem 1, CA = O(1/z) is anti-concentration constant defined in
Section 7 and
√
4 2p
µ3 ≤ 1/2

λmin DK ◦ GD
Proof. Bind Theorems 1 and 11. Form Theorem 3 follows that the bound in Theorem 1 also holds
for projection of the parameter θ:
∗
˘
kD̆(θbp − θp∗ ) − D̆−1 ∇L(θ
)k ≤ ♦(r, t)

So with probability 1 − e−t
W1 (D̆(θbp − θp∗ ), Z) = min IEkD̆(θbp − θp∗ ) − Zk
b
π(θ,Z)

5

∗
˘
≤ W1 (D̆−1 ∇L(θ
), Z) + ♦(r, t)

Furthermore from Theorem 11 follows
∗
˘
W1 (D̆−1 ∇L(θ
), Z) ≤

√



p
2µ3 1 + log(2 tr{Σ}µ2 ) − log(µ3 )

∗
˘
˘ ∗ − θi )
where Σ = Var[D̆−1 ∇L(θ
)] and setting Xi = D̆−1 ∇l(θ

µ3 =

n
X

IEkΣ

−1/2

(Xi −

Xi0 )kkΣ −1/2 Xi kkXi

−

Xi0 k

≤ 4 max kXi k

i=1

n
X

IEXiT Σ −1 Xi

i=1
n
X

(
IEXiT ΣXi = tr Σ −1

n
X

)
IEXi XiT

=p

i=1

i=1

˘ ∗ − θi )k ≤ kD−1 ∇l(θ∗ − θi )k ≤
max kXi k = kD̆−1 ∇l(θ

1
1/2
λmin

DK ◦ GD



Analogically one can make a consequence from Theorems 1 and 12. Let CA is the anti-concentration
constant of the distribution IP (kZk > x), then
|IP (kD̆(θbp − θp∗ )k > z) − IP (kZk > z)|
∗
˘
≤ |IP (kD̆−1 ∇L(θ
)k > z) − IP (kZk > z)| + CA ♦(r, t)

and
∗
˘
|IP (kD̆−1 ∇L(θ
)k > z) − IP (kZk > z)| ≤ CA µ3 O(log2 n)

As for the anti-concentration constant it can be estimated from Pinsker’s inequality applied in the
next equation


ˆ
1
∆
−xT Σ −1 x/2
IP (kZk ∈ [z, z + ∆]) =
e
dx = O
log(tr{Σ})
(2π det Σ)1/2 z<kxk<z+∆
z

3

Statistical learning theory

In this section we consider an infinite dimensional statistical model L(θ). Let parameter θ
consists of two parts (u, v), such that u = θ1...p ∈ IRp . Given a finite dataset we are going to
find MLE deviations basing on three assumptions listed below. Further we will specify these
assumptions for independent models and apply to barycenter model from the previous section.

3.1.

General approach

Let the Likelihood function L(θ) = L(θ, Y ) depends on parameters vector θ = (u, v) and a
fixed dataset Y of size n. Denote the parameter MLE and refernce values:
θb = argmax L(θ)
θ

θ∗ = argmax IEL(θ)
θ

We are going to study deviations of θb and u in the following sense. For some matrix D and random
vector ξ
√
1. kθb − θ∗ k is expected to be of order O(1/ n).
6

2. D(θb − θ∗ ) ≈ ξ
b − L(θ∗ ) ≈ kξk2 /2
3. L(θ)
Denote the stochastic part of the Likelihood
ζ(θ) = L(θ) − IEL(θ)
Involve the Fisher matrix
2

2

∗

D = −∇ IEL(θ ) =



2
Du2 Duv
2
Dvu
Dv2



2
= 0). One
It would be easier to deal with the model if matrix D2 has block-diagonal view (Duv
can make parameter replacement in order to satisfy to this condition. Define a new variable
ϑ = ϑ(u, v) such that
∇ϑ ∇Tu IEL(θ∗ ) = ∇ϑ ∇Tu IEL(θ∗ ) = 0

and
2
u,
ϑ = v + Dv−2 Dvu

or in other words the parameters transformation matrix is




I
0
I
0
−1
S=
, S =
2
2
Dv−2 Dvu
I
−Dv−2 Dvu
I
The gradient in the new coordinates (u, ϑ) may be obtained by rule ∇(u, ϑ) = (S −1 )T ∇(u, v). Use
˘ for the first part of it
notation ∇
˘ = ∇u (u, ϑ) = ∇u − D2 D−2 ∇v
∇
uv v
The Fisher matrix after parameters replacement changes by rule D2 (u, ϑ) = (S −1 )T D2 S −1 , so in
the new coordinates it has view
 2

D̆
0
2
2
∗
∗
D (u, ϑ) = −∇ IEL(u , ϑ ) =
0 Dϑ2
2
2
D̆2 = Du2 − Duv
Dv−2 Dvu

Define a local region around point θ∗
Ω(r) = {θ : kD(θ − θ∗ )k ≤ r}
Now we write down three conditions on the Likelihood derivatives essential for the deviations of
b The first and second conditions holds in the local region Ω(r). The third condition is required
θ.
to make expansion of local statements to the whole parameter space IR∞ . Further we will show
that from these conditions also follows deviation bounds of the parameter u
b or in other words
from deviations bound of θb follows bound of u
b.
Assumption 1: In the region Ω(r)
−D−1 {∇IEL(θ) − ∇IEL(θ∗ )} − D(θ − θ∗ ) ≤ δ(r)r
Assumption 2: In the region Ω(r) with probability 1 − e−t
sup

D−1 {∇ζ(θ) − ∇ζ(θ∗ )} ≤ z(t)r

θ∈Ω(r)

Assumption 3: The Likelihood function is convex (−∇2 L(v) ≥ 0) or the expectation of Likelihood function is upper-bounded by a strongly convex function (IEL(v ∗ ) − IEL(v) ≥ br2v ).
Use notation
{δ(r) + z(t)}r = ♦(r, t)
7

Theorem 3. [9] Let the Likelihood function is convex (−∇2 L(v) ≥ 0) and for rv (assigned further)
δ(r) + z(t) ≤ 1/2. Then under Assumptions 1, 2 with probability 1 − e−t
r ≤ 4kD−1 ∇L(θ∗ )k
kD(θb − θ∗ ) − D−1 ∇L(θ∗ )k ≤ ♦(r, t)
∗
˘
kD̆(b
u − u∗ ) − D̆−1 ∇L(θ
)k ≤ ♦(r, t)
b > L(θ∗ )) follows that the local region Ω(r) that includes
Proof. From (−∇2 L(θ) ≥ 0) and (L(θ)
θb should covers the next region
Ω(r) ⊃ {θ : L(θ) ≥ L(θ∗ )}
Estimate the minimum possible radius of Ω(r) that satisfy the previous condition.
1
1
0 ≥ L(θ∗ ) − L(θ) = −(θ − θ∗ )T ∇L(θ∗ ) − (θ − θ∗ )T ∇2 ζ(θ0 )(θ − θ∗ ) + kD(θ0 )(θ − θ∗ )k2
2
2
z(t) 2 1 − δ(r) 2
r +
r
2
2
r(1 − δ(r) − z(t)) ≤ 2kD−1 ∇L(θ∗ )k

{with probability (1 − e−t )} ≥ −kD−1 ∇L(θ∗ )kr −

r ≤ 4kD−1 ∇L(θ∗ )k
From Assumptions 1, 2 follows that
b − ∇L(θ∗ )}k ≤ ♦(r, t)
kD(θb − θ∗ ) + D−1 {∇L(θ)
kD(θb − θ∗ ) − D−1 ∇L(θ∗ )k ≤ ♦(r, t)
Not that for the coordinates transform S there is an invariant:
  −1




∗
˘
˘
u − u∗
D̆
0
D̆ 0
∇L(u,
ϑ) − ∇L(u
, ϑ∗ )
+
ϑ − ϑs
0 Dϑ−1
0 Dϑ
∇ϑ L(u, ϑ) + ∇ϑ L(u∗ , ϑ∗ )
= kD(θ − θ∗ ) + D−1 {∇L(θ) − ∇L(θ∗ )}k
Since



D̆−1
0


  2
u
D̆ 0
= θT S T [(S −1 )T D2 (S −1 )]Sθ = kDθk2
ϑ
0 Dϑ
  2
˘
0
∇
= ∇T S −1 [(S −1 )T D2 (S −1 )]−1 (S −1 )T ∇ = ∇T D−2 ∇
Dϑ−1
∇ϑ
 T  
˘
u
∇
= ∇T S −1 Sθ = ∇T θ
ϑ
∇ϑ

Subsequently basing on this invariant
∗
˘
kD̆(b
u − u∗ ) − D̆−1 ∇L(θ
)k

≤ kD(θb − θ∗ ) − D−1 ∇L(θ∗ )k ≤ ♦(r, t)

8

3.2.

Independent models

Consider independent models (models with independent observations) and obtain a simpler
variant of the Assumption 2 for this case. Involve three basic Lemmas for that.
Lemma 1 (Bernstein’s inequality [5]). Let X1 . . . Xn be independent real-valued random variables.
Assume that exist positive numbers v and R such that
2

v =

n
X

IEXi2

i=1

and for all integers q ≥ 3
n
X

IE [Xi ]q+ ≤

i=1

q! 2 q−2
v R
2

Then for all λ ∈ (1, 1/R)
log IEeλ

P

i (Xi −IEXi )

≤

v 2 λ2
2(1 − Rλ)

Lemma 2 (Dudley’s entropy integral [5]). Let Ω be a finite pseudometric space and let f (θ)
(θ ∈ Ω) be a collection of random variables such that for some constants a, v, R > 0, for all
θ1 , θ2 ∈ Ω and all 0 < λ < (Rd(θ1 , θ2 ))−1
v2 λ2 d2 (θ1 , θ2 )
log IE exp{λ(f (θ1 ) − f (θ2 ))} ≤ aλd(θ1 , θ2 ) +
2(1 − Rλd(θ1 , θ2 ))
Then for any θ0 ∈ Ω,
ˆ
IE[sup f (θ) − f (θ0 )] ≤ 3ar + 12v
θ

r/2

ˆ
p
log N (ε, Ω)dε + 12R

r/2

log N (ε, Ω)dε

0

0

where r = supθ∈Ω d(θ, θ0 ) and N (ε, Ω) is covering number.
Lemma 3 (Bousquet inequality [5]). Consider independent random variables X1 . . . Xn and let
F : X → R be countable set of functions that satisfy conditions IEf (Xi ) = 0 and kf k∞ ≤ R.
Define
n
X
Z = sup
f (Xi )
f ∈F i=1

Let v2 ≥ supf ∈F

Pn

i=1

IEf 2 (Xi ) then with probability 1 − e−t
Z < IEZ +

p
tR
2t(v2 + 2RIEZ) +
3

Apply three previous Lemmas in order to simplify Assumption 2 for independent models. Likelihood of an independent model is a sum of independent functions:
(L − IEL)(θ) = ζ(θ) =

n
X

ζi (θ)

i=1

Note that ζi depends from the implicit i-th element from the dataset, such that ζi (θ) = ζi (θ, Yi ).
Theorem 4. Let D2 be the Fisher matrix defined above and ∀θ ∈ Ω(r)
sup

n
X

IE(uT D−1 ∇2 ζi (θ)D−1 u)2 ≤ v2

kuk=1 i=1

9

and
kD−1 ∇2 ζi (θ)D−1 k ≤ R
Then Assumption 2 fulfills inside Ω(r) with probability 1 − e−t and


p
tR
2
z(t) ≤ E + 2t(v + 2RE) +
3
where

12v
E=
r

ˆ

∞

0

ˆ
p
12R ∞
2 log N (ε, Ω(r))dε +
2 log N (ε, Ω(r))dε
r 0

Proof. Set a random process for each i:
1
Xi (γ, θ) = γ T {∇ζi (θ) − ∇ζi (θ∗ )}
r
Such that
X

sup
kDγk≤r

Xi (γ, θ) = kD−1 {∇ζ(θ) − ∇ζ(θ∗ )}k

i

∗

∀ fixed (γ, θ) ∈ Ω(r, 0) × Ω(r, θ ) and kuk = 1:
sup IE
u

X
X1
(γ∇2 ζ(θ)T D−1 u)2
(∇θ Xi (γ, θ)T D−1 u)2 = sup IE
r
u
i
i
≤ sup IE
u

X

(uT D−1 ∇2 ζ(θ)T D−1 u)2 ≤ v2

i

Analogically
sup IE
u

X

(∇γ Xi (γ, θ)T D−1 u)2 ≤ v2

i

∀i ∈ 1, . . . , n :
kD−1 ∇Xi (γ, θ)k ≤ R
Apply Lemma 1 for the sum of random variables X(γ, θ) =

P

i

Xi (γ, θ) when (γ, θ) are fixed.

log IE exp λ (X(γ1 , θ1 ) − X(γ2 , θ2 ))


= log IE exp λ (γ1 − γ2 )T ∇γ X(γ, θ) + log IE exp λ (θ1 − θ2 )T ∇θ X(γ, θ)

≤ sup log IE exp λ kD(γ1 − γ2 )kuT D−1 ∇γ X(γ, θ)
u


+ sup log IE exp λ kD(θ1 − θ2 )kuT D−1 ∇θ X(γ, θ)
u

≤

v2 λ2 kD(γ2 − γ1 )k2
v2 λ2 kD(θ2 − θ1 )k2
+
2(1 − RλkD(γ2 − γ1 )k) 2(1 − RλkD(θ2 − θ1 )k)

≤

v2 λ2 d212
2(1 − Rλd12 )
d212 = kD(θ2 − θ1 )k2 + kD(γ2 − γ1 )k2

Denote
Υ = Ω(r) × Ω(r)
such that log N (ε, Υ ) = 2 log N (ε, Ω(r)). Then with Lemma 2 we obtain
ˆ ∞p
ˆ ∞
IE sup X(γ, θ) ≤ 12v
log N (ε, Υ )dε + 12R
log N (ε, Υ )dε
γ,θ

0

0

Applying Lemma 3 to the random variable Z = supγ,θ X(γ, θ) completes the proof.
10

3.3.

Covering numbers and entropy

Below one can read a short excerpt about an entropy of ball and ellipsoid. The general formula
for the covering number N of a convex set Ω in Rp with an arbitrary distance d(θ1 , θ2 ) is
 p
volume(Ω + (ε/2)B1 ) 2
N (ε, Ω) ≤
volume(B1 )
ε
where B1 is a unit ball.
Ball entropy: Let Ω = Br and d(θ1 , θ2 ) = kθ1 − θ2 k then

p
2
N (ε, B1 ) ≤ 1 +
ε
and since N (εr, Br ) = N (ε, B1 )
ˆ
ˆ ∞p
log N (ε, Br )dε = r

1

p
log N (ε, B1 )dε

0

0

√
≤r p

ˆ

1

p
√
log(3/ε)dε ≤ 1.42r p

0

ˆ

and

∞

log N (ε, Br )dε ≤ 2.1rp
0

Ellipsoid entropy: Let Ω = Er (D) and d(θ1 , θ2 ) = kD(θ1 − θ2 )k. The entropy in this case is rather
complicate in calculation. So we provide here only the the final statement from V. Spokoiny’s
lecture notes [9].
s
ˆ ∞p
X logα (λ2 (D))
√
i
log N (ε, Er (D))dε . r α − 1
2
λi (D)
0
i
ˆ

and

∞

log N (ε, Er (D))dε . r

X

0

4

i

1
λi (D)

Barycenters model

We are going to show that Assumptions 1,2,3 are fulfilled for the barycenters model defined
in Section 2. Also we need to estimate ♦(r, t). Remind that we deal with Likelihood function
L(θ) = L(θ, {θi }ni=1 ) where implicit random vectors {θi }ni=1 is a dataset of Fourier coefficients
corresponded to the random measures {µi }ni=1 .
Assumption 1:
kD−1 {∇2 IEL(θ) − ∇2 IEL(θ∗ )}D−1 k ≤ kD−1 {∇3 IEL(θ)D−1 }D−1 kr
Let q(θi ) be the distribution of each θi then
3

∇ IEi L(θ − θi ) =

n ˆ
X

3

∇ l(θ − θi )q(θi )dθi = −

i=1

kD−1 {∇3 IEL(θ)D−1 }D−1 k ≤

n ˆ
X

∇2 l(θ − θi ) × ∇q(θi )dθi

i=1

ˆ

kD−1 ∇2 L(θ − θx )D−1 kkD−1 ∇p(θx )kdθx

and from the consequence of Theorem 7 one gets
kD−1 ∇2 l(θ − θi )D−1 k ≤
11

ελmin

1

DK ◦ GD

−1

2

∗

2

−1

kD {∇ IEL(θ) − ∇ IEL(θ )}D k ≤

ελmin

Subsequently
δ(r) =

ελmin

rn

DK ◦ GD

rn

DK ◦ GD

ˆ
kD−1 ∇q(θ)kdθ

ˆ
kD−1 ∇q(θ)kdθ

Assumption 2: From Theorem 4 follows that if:
ε2 λ2min

n
 ≤ v2
DK ◦ GD

ελmin

1
 ≤R
DK ◦ GD

and

then
z(t) ≤ E +

p

2t(v2 + 2RE) +

tR
3

where
E = 12v

p
2pD + 24R pD

and pD is ellipsoid entropy with matrix D
pD =

s
X log2 (λ2 (D))
i
2
λi (D)

i

Assumption 3: Each model component l(θ − θi ) is convex since
l(λθ1 + (1 − λ)θ2 − θi ) = l(λ(θ1 − θi ) + (1 − λ)(θ2 − θi ))
= max hη, λ(θ1 − θi ) + (1 − λ)(θ2 − θi ))i
η∈W2,1

≤ max hη, λ(θ1 − θi )i + max hη, (1 − λ)(θ1 − θi )i
η∈W2,1

η∈W2,1

= λl(θ1 − θi ) + (1 − λ)l(θ2 − θi )
Note that l2 is also convex as a composition of convex functions and the complete model L is convex
(∇2 L > 0) as a positive aggregation of convex functions. Combination of these assumptions is
b
used in the proof of Theorem 1 which gives the deviation bound of parameter θ.

5

Support functions

Bounds for the first and second derivatives of the Likelihood of barycenters model involves
additional theory from Convex analysis.
Def (*). Legendre–Fenchel transform of a function f : X → IR or the convex conjugate function
calls
f ∗ (y) = sup(hx, yi − f (x))
x∈X

Def (s). Support function for a convex body E is
s(θ) = sup θT η
η∈E

Note that for indicator function δE (η) of a convex set E the conjugate function is support function
of E
δE∗ (θ) = s(θ)
12

Def (⊕). Let f1 , f2 : E → IR are convex functions. The infimal convolution of them is
(f1 ⊕ f2 )(x) =

inf (f1 (x1 ) + f2 (x2 ))

x1 +x2 =x

Lemma 4. [2] Let f1 , f2 : E → IR are convex lower-semi-continuous functions. Then
(f1 ⊕ f2 )∗ = f1∗ + f2∗
(f1 + f2 )∗ = f1∗ ⊕ f2∗
Lemma 5. The support function of intersection E = E1 ∩ E2 is infimal convolution of support
functions for E1 and E2
s(θ) = inf (s1 (θ1 ) + s2 (θ2 ))
θ1 +θ2 =θ

where
s1 (θ) = sup θT η,

s2 (θ) = sup θT η

η∈E1

η∈E2

Proof. Firstly
δE1 ∩E2 (η) = δE1 (η) + δE2 (η),
(δE1 + δE2 )∗ = δE∗ 1 ⊕ δE∗ 2
With additional property
intdom δE1 ∩ dom δE2 = intE1 ∩ E2 6= ∅
one have
(δE1 + δE2 )∗ = δE∗ 1 ⊕ δE∗ 2

Lemma 6. Let a support function s(θ) is differentiable, then its gradient lies on the border of
corresponded convex set E
∇s(θ) = ηb(θ) ∈ ∂E
where
ηb(θ) = argmax η T θ
η∈E

Proof. It follows from the convexity of E and linearity of optimization functional.
∂b
η (θ)
∂b
η (θ) T
=0⇒
θ=0
∂kθk
∂θ
∂b
η (θ) T
∇s(θ) =
θ + ηb(θ) = ηb(θ)
∂θ

13

Figure 2: Optimization related to support function.
Lemma 7. [2] Let f1 , f2 : E → IR are convex continuous functions. Then the subdifferential of
their infimal convolution can be computed by following formula
[
∂(f1 ⊕ f2 )(x) =
∂f (x1 ) ∩ ∂f (x2 )
x=x1 +x2

Consequence. If in addition f1 , f2 are differentiable, then their infimal convolution is differentiable and ∃x1 , x2 : x = x1 + x2
∇(f1 ⊕ f2 )(x) = ∇f1 (x1 ) = ∇f2 (x2 )

There are
Lemma 8. Let f1 , . . . , fm : E → IR are convex and two times differentiable functions.
P
t
=
1
following upper bound for the second derivative of the infimal convolution ∀t : m
i=1 i
∂∇T (f1 ⊕ . . . ⊕ fm )(x) 

m
X

t2i ∇2 f (xi )

i=1

where

Pm

i=1

xi = x.

Proof. Use notation f = f1 ⊕ . . . ⊕ fm . Let
f (y) =

X

fi (yi )

i

According to Lemma 7 if all the functions are differentiable then
X
∇f (y) =
ti ∇fi (yi )
i

From the definition ⊕ also follows that
f (y + z) ≤

X

fi (yi + ti z)

i

Make Tailor expansion for the left and right parts and account equality of the first derivatives.
X
z T ∂∇T f (y + θz)z ≤
t2i z T ∇2 fi (yi + θi z)z
i

Since direction z was chosen arbitrarily then dividing both parts of the previous equation by
kzk2 → 0 we come to inequality
X
∂∇T f (y) 
t2i ∇2 fi (yi )
i

14

Remark. One can find another provement of the similar Theorem in book [2] (Theorem 18.15).
Theorem 5. Let f1 , . . . , fm : E → IR are convex and two times differentiable functions. There
are following upper bounds for infimal convolution f = f1 ⊕ . . . ⊕ fm derivatives ∀γ and some
matrix A
f (xi )
γ T ∂∇T f (x)γ ≤ max γ T ∇2 fi (xi )γ
i
f (x)
and
γ T ∂∇T f 2 (x)γ ≤ 2(γ T ∇f (x))2 + 2 max γ T ∇2 fi (xi )γf (xi )
i

Proof. Choosing appropriate {ti } in Lemma 8 one get the required upper bounds. Set
f (xi )
ti = Pm
j=1 f (xj )
and since

m
X

f (xj ) = f (x)

j=1

then
X
i

t2i γ T ∇2 fi (yi )γ ≤ max ti γ T ∇2 fi (yi )γ = max γ T ∇2 fi (xi )γf (xi )
i

i

To prove the second formula apply this inequality in
∂∇T f 2 = 2∇f ∇T f + 2f ∂∇f

Consequence. Let s1 , . . . , sm : E ∗ → IR are support functions of the bounded convex smooth
sets E1 , . . . , Em . There is upper bound for the derivatives of support function s of intersection
E1 ∩ . . . ∩ Em , such that ∀i
γ T ∂∇T s(x)γ ≤

maxi γ T ∂ηi /∂xi γsi (xi )
s(x)

γ T ∂∇T s2 (x)γ ≤ 2(γ T ηi )2 + 2 max γ T ∂ηi /∂xi γsi (xi )
i

Proof. It follows from Theorem 5 and Lemma 6.

15

6

Wasserstein distance as a support function

Def (W-dual). Consider two random variables X and Y ∈ Rp with densities ϕX and ϕY . Define
Wasserstein distance in dual form between them as
W1 (ϕX , ϕY ) =

{IEf (X) − IEf (Y )}

max

∀x:k∇f (x)k≤1

where ∀x : k∇f (x)k ≤ 1 means that function f is 1-Lipshits. Note that if π(x, y) is a joint
distribution with marginals ϕX and ϕY then this definition is equivalent to the original definition
W1 (ϕX , ϕY ) = min IEkX − Y k
π

which follows from Kantorovich-Rubinstein duality [6]. Involve a normalized Fourier basis {ψk (x)}k∈Np
with scalar product Gram function G(x).
Def (W − dual − regularised). Consider two random variables X and Y ∈ Rp with densities ϕX
and ϕY . Define a penalized Wasserstein distance between them in dual form as


ˆ
2
f (ϕX , ϕY ) =
W
max
IEf (X) − IEf (Y ) − ε k∇f (x)k G(x)dx
∀x:k∇f (x)k≤1

The regulariser term in this definition allows to bound the second derivative of the distance which
will be shown below. Wasserstein distance is a support function (ref. Def(s)) in Fourier basis. In
which connection
X
ηk (f )ψk (x)
f (x) =
k

ˆ

where
ηk (f ) = hf, ψk iG =

f (x)ψk (x)G(x)dx

Now we can rewrite the expectation difference as
Ef (X) − Ef (Y ) = hf,

ϕX
ϕY
iG − hf,
iG = hη(f ), θ(ϕX )i − hη(f ), θ(ϕY )i
G
G
ˆ

where
θk (ϕ) =

ϕ(x)ψk (x)dx

Define positive symmetric matrices
 T

∇ ψ1 (x)
 ... 

 ∇ψ1 (x) . . . ∇ψk (x) . . . = (∇T ψ(x))(∇ψ T (x))
Kx = 
T
∇ ψk (x)
...
ˆ

and
K ◦G=

Kx G(x)dx

Each Kx is positive, since η T Kx η = k∇f (x)k2 . Condition ∀x : k∇f (x)k ≤ 1 is equivalent in
Fourier basis to


!2


\
X
T
η∈
Ex = η : ∀x :
ηk ∇ψk (x) = η Kx η ≤ 1


k

An important remark is that
\


Ex ⊂ η : η T (K ◦ G)η ≤ 1

Finally we have come to the Wasserstein distance in Fourier basis.
16

Lemma 9. Let random vectors X and Y have densities ϕX and ϕY with Fourier
T coefficients θX
and θY , then the Wasserstein distance is the support function of the convex set Ex defined above,
i.e.
W1 (ϕX , ϕY ) = max
T hη, θX − θY i
η∈

Ex

As for regularised case
T
f1 (ϕX , ϕY ) = max
W
T hη, θX − θY i − εη K ◦ Gη
η∈

Ex

Remind that barycenters Likelihood consists of independent components li (θ − θi ) with a random
vectors θi ∈ IR∞ and parameter θ ∈ IR∞ .
T
l(θ − θi ) = max
T hη, θ − θi i − εη K ◦ Gη
η∈

Ex

Note that by definition the dual function of l is
l∗ (η) = δT Ex (η) + εη T K ◦ Gη
Consequently from Lemma 4 follows that
T
∗
∗
l(θ − θi ) = δT
Ex (θ − θi ) ⊕ (εη K ◦ Gη) (θ − θi )

1
T
−1
= max
(1)
T hη, θ − θi i ⊕ (θ − θi ) (K ◦ G) (θ − θi )
η∈ Ex
ε

T
Apply Theorem 5 taking into account Ex ⊂ η : η T (K ◦ G)η ≤ 1 , one gets the following bounds
on the derivatives of the function l.
Theorem 6. The gradient upper bounds for functions l and l2 are
kD−1 ∇lk ≤

1
1/2
λmin

kD−1 ∇l2 (θ − θi )k ≤

DK ◦ GD



2k(K ◦ G)−1/2 (θ − θi )k

1/2
λmin DK ◦ GD

Proof. Denote
η ∗ (θ) = argmax
ηT θ
T
η∈

Ex

Use the equation (1). By the consequence of Lemma 7 and Lemma 6 ∃θ0 :
∇l(θ − θi ) = η ∗ (θ0 )
Since k(K ◦ G)1/2 η ∗ k ≤ 1
kD−1 ∇lk = kD−1 η ∗ k = kD−1 (K ◦ G)−1/2 (K ◦ G)1/2 η ∗ k ≤ kD−1 (K ◦ G)−1/2 k
and from ∇l2 = 2l∇l one gets
kD−1 ∇l2 (θ − θi )k ≤ 2l(θ − θi )kD−1 ∇lk ≤ 2k(K ◦ G)−1/2 (θ − θi )kkD−1 ∇lk

Theorem 7. The second derivative upper bounds for functions l and l2 are
kD−1 ∂∇T l(θ − θi )D−1 k ≤

1
minx λmin (DKx D)k(K ◦ G)−1/2 (θ − θi )k

1
ελmin (DK ◦ GD)
2
kD−1 ∂∇T l2 D−1 k ≤
minx λmin (DKx D)

kD−1 ∂∇T l(θ − θi )D−1 k ≤

17

Remark. Matrix Kx may be singular which makes the first bound non-informative. The second
bound comes from the regulariser εη T K ◦ Gη and has big coefficient (1/ε). It is a weak part of the
current theory and requires an improvement or an example which shows that this bound it tight.
Proof. Consider support function with one ellipsoid.
sx (θ) = max hη, θi = kKx−1/2 θk
η T Kx η≤1

Denote η ∗ (θ) = argmaxhη, θi, η T Kx η ≤ 1.
η ∗ (θ) =

Kx−1 θ
−1/2

kKx

θk

∂η ∗ (θ)
K −1 θT Kx−1 θ − Kx−1 θθT Kx−1
= x
∂θ
(θT Kx−1 θ)3/2
For some vector kγk = 1 and property kak2 kbk2 ≥ (aT b)2
γ T Kx−1 γθT Kx−1 θ − γ T Kx−1 θθT Kx−1 γ ≤ kKx−1 kθT Kx−1 θ
∂η ∗ (θ)
kKx−1 k
≤
∂θ
(θT Kx−1 θ)1/2
Apply Theorem 5
kD−1 ∂∇T l(θ − θi )D−1 k ≤ max D−1
x

maxx kD−1 Kx−1 D−1 k
∂ηx∗ (θx∗ ) −1 sx (θx∗ )
D
≤
∂θ
s(θ − θi )
k(K ◦ G)−1/2 (θ − θi )k

The second bound for this norm follows directly from Lemma 8 and equation (1). Now consider
the squared Wasserstein distance (l2 ) which has a better derivative bound. From Theorem 5 one
gets
∂η ∗ (θx∗ )
kKx−1/2 θx∗ kD−1
kD−1 ∂∇T l2 D−1 k ≤ 2 max D−1 η ∗ (θx∗ )η ∗ (θx∗ )T D−1 + D−1
x
∂θ
Note that

∂η ∗ (θ)
(Kx−1 θ)(Kx−1 θ)T
kKx−1/2 θk = Kx−1 −
−1/2
∂θ
kKx θk2
η ∗ (θ)η ∗ (θ)T +

∂η ∗ (θ)
kKx−1/2 θk = Kx−1
∂θ

Finally
kD−1 ∂∇T l2 D−1 k ≤ 2 max kD−1 Kx−1 D−1 k
x

Remark. Note that the Wasserstein distance also may be differentiated directly. In paper [8] one
may find the lemma about directional derivative. For directions h1 , h2 it holds
d0W (µ1 , µ2 )(h1 , h2 ) =

max
(u,v)∈Φ(µ1 ,µ2 )

−(hu, h1 i + hv, h2 i)

where
Φ = {(u, v) : hu, µ1 i + hv, µ2 i = dW (µ1 , µ2 ), ∀(x, y) : u(x) + v(y) ≤ kx − yk}

18

7

Gaussian approximation

Def (Hk ). The multivariate Hermite polynomial Hk is defined by
Hk (x) = (−1)|k| ex

T Σ −1 x/2

∂ |k|
T −1
e−x Σ x/2
k
k
p
1
∂ ...∂

Lemma 10. Consider a Gaussian vector Z ∼ N (0, Σ) and two functions h and fh such that
ˆ 1
fh (x) = −
IEh(Z(x, t))dt
0

√
√
h(Z(x, t)) = h( tx + 1 − tZ) − IEh(Z)
Then fh is a solution of the Stein equation
h(x) = (tr{∇2 Σ} − xT ∇)fh (x)
and
∂ |k|
fh (x) = −
∂ k1 . . . ∂ kp

ˆ

1

0

|k|

1 t 2 −1
IEHk (Z)h(Z(x, t))dt
−1
2 (1 − t) |k|
2

Consequence.
ˆ
2

2

∇ fh (x) − ∇ fh (y) = −
0

1

1
IEH2 (Z){h(Z(x, t)) − h(Z(y, t))}dt
2(1 − t)

where
H2 (Z) = (Σ −1 Z)(Σ −1 Z)T − Σ −1
= Σ −1/2 {(Σ −1/2 Z)(Σ −1/2 Z)T − I}Σ −1/2

X=

n
X

Xi

i=1
2

IEh(X) = IE tr{∇ Σ}fh (X) − IE

n
X

XiT ∇fh (X)

i=1

=

n
X


IEXiT ∇2 fh (X 0 + θ(Xi − Xi0 )) − ∇2 fh (X 0 ) (Xi − Xi0 )

i=1

=

n
X


IE(Σ −1/2 Xi )T Σ 1/2 ∇2 fh (X 0 + θ(Xi − Xi0 )) − ∇2 fh (X 0 ) Σ 1/2 (Σ −1/2 (Xi − Xi0 ))

i=1

For a unit vector kγk = 1 and conditional expectation IE−i = IE(·|Xi , Xi0 )

γ T IE−i Σ 1/2 ∇2 fh (X 0 + θ(Xi − Xi0 )) − ∇2 fh (X 0 ) Σ 1/2 γ
ˆ
=
0

1

1
IE−i {((Σ −1/2 Z)T γ)2 − 1}{h(Z(X 0 + θ(Xi − Xi0 ), t)) − h(Z(X 0 , t))}dt
2(1 − t)
ˆ 1−α
ˆ 1
t1/2
1
≤
Adt +
Bdt
1/2
2(1 − t)
0
1−α (1 − t)
√
A
≤ − log(α) + 2B α
2
19






2B
≤ A 1 + log
A
√
√
A = kXi − Xi0 k IE−i |((Σ −1/2 Z)T γ)2 − 1| k∇h( t(X 0 + θ1 (Xi − Xi0 ) + 1 − tZ)k
√
√
B = IE−i |((Σ −1/2 Z)T γ)2 − 1| kZk k∇h( t(X 0 + θ(Xi − Xi0 ) + θ2 1 − tZ)k
Lemma 11 (Multivariate Berry–Esseen TheoremPwith Wasserstein distance). Consider a sequence
of independent zero-mean random vectors X = ni=1 Xi in IRp with a covariance matrix
IEXX T = Σ
Then the Wasserstein distance between X and Gaussian vector Z ∈ N (0, Σ) has following upper
bound


p
√
dW (X, Z) ≤ 2µ3 1 + log(2 tr{Σ}µ2 ) − log(µ3 )
where
µ3 =

n
X

IEkΣ −1/2 (Xi − Xi0 )kkΣ −1/2 Xi kkXi − Xi0 k

i=1

µ2 =

n
X

IEkΣ −1/2 (Xi − Xi0 )kkΣ −1/2 Xi k

i=1

Remark. In i.i.d case with Σ = Ip

dW (X, Z) = O

p3/2 log(n)
√
n



These is the same theorem with a different provement in paper [3].
Lemma 12 (Multivariate
Theorem). Consider a sequence of independent zero-mean
Pn Berry–Esseen
p
random vectors X = i=1 Xi in IR with a covariance matrix
IEXX T = Σ
Let a function ϕ : IRp → IR+ be sub-additive:
ϕ(x + y) ≤ ϕ(x) + ϕ(y)
and with Gaussian vector Z ∈ N (0, Σ) fulfills the anti-concentration property, such that
IP (ϕ(Z) > x) − IP (ϕ(Z) > x + ∆) ≤ CA ∆
Then the measure difference between X and Gaussian vector Z has following upper bound ∀x
!
p


2
2IEϕ (Z)µ2
4µ2
|IP (ϕ(X) > x) − IP (ϕ(Z) > x)| ≤ 22CA µ3 log
log
C A µ3
20CA µ23
where
µ3 =

n
X

IEkΣ −1/2 (Xi − Xi0 )kkΣ −1/2 Xi kϕ(Xi − Xi0 )

i=1

µ2 =

n
X

IEkΣ −1/2 (Xi − Xi0 )kkΣ −1/2 Xi k

i=1

20

Proof. Define a smooth indicator function


t<x
0,
gx,∆ (t) = (t − x)/∆, t ∈ [x, x + ∆]


1,
t>x+∆
Set h = gx,∆ ◦ ϕ. Denote the required bound by δ:
|IP (ϕ(X) > x) − IP (ϕ(Z) > x)|
≤ max |IEgx,∆ ◦ ϕ(X) − IEgx,∆ ◦ ϕ(Z)| ≤ δ
∆

Note that from sub-additive property of the function ϕ follows


gx,∆ ϕ(X + dX) ≤ gx,∆ ϕ(X) + ϕ(dX)
and
0
gx,∆
(t) =

and

1
1I[x < t < x + ∆]
∆


1
IP (ϕ(Z) > x) − IP (ϕ(Z) > x + ∆) ≤ CA
∆
 2δ
1
2δ
0
IEgx,∆
(ϕ(Z(X, t))) ≤
≤ CA +
IP (ϕ(Z) > x) − IP (ϕ(Z) > x + ∆) +
∆
∆
∆
0
IEgx,∆
(ϕ(Z)) =

IE−i h(Z(X 0 + θ(Xi − Xi0 ), t)) − IE−i h(Z(X 0 , t))


≤ IE−i gx,∆ ϕ(Z(X 0 , t)) + ϕ(Xi − Xi0 ) − IE−i gx,∆ ϕ(Z(X 0 , t)

0
≤ IE−i gx,∆
ϕ(Z(X 0 , t)) + θϕ(Xi − Xi0 ) ϕ(Xi − Xi0 )


2δ
ϕ(Xi − Xi0 )
≤ CA +
∆
Analogically
0

0

IEh(Z(X , t)) − IEh(Z(X + θ(Xi −

Xi0 ), t))


≤

2δ
CA +
∆



ϕ(Xi − Xi0 )

Apply this inequality denoting ε2 = (Σ −1/2 Z)T γ)2 ∼ N 2 (0, 1)
IE−i {((Σ −1/2 Z)T γ)2 − 1}{h(Z(X 0 + θ(Xi − Xi0 ), t)) − h(Z(X 0 , t))}
≤ IE−i ε2 {gx,∆ [ϕ(Z(X 0 , t)) + ϕ(Xi − Xi0 )] − gx,∆ [ϕ(Z(X 0 , t))]}
+IE−i h(Z(X 0 , t)) − IE−i h(Z(X 0 + θ(Xi − Xi0 ), t))


2δ
≤ |τ − 1| CA +
ϕ(Xi − Xi0 ) + IE 1I[ε2 > τ ]ε2
∆
Lemma 13. Let a random variable ε has a tail bound ∀x ≥ x0
IP (ε > h(x)) ≤ e−x
Then for a function g : IR+ → IR+ with derivative g 0 : IR+ → IR+
ˆ ∞
−x0
IE 1I[ε > h(x0 )]g(ε) ≤ g(h(x0 ))e
+
e−x g 0 (h(x))h0 (x)dx
x0

21

In particular

ˆ
−x0

IE 1I[ε > h(x0 )]ε ≤ h(x0 )e

∞

e−x h0 (x)dx

+
ˆ

x0
∞

e−x h(x)r−1 h0 (x)dx

IE 1I[ε > h(x0 )]εr ≤ h(x0 )r e−x0 + r
x0

For ε ∼ N (0, 1) we have
IP (ε >

√

2x) ≤ e−x

and by means of the previous lemma we get
IE 1I[ε2 > τ ]ε2 = 2IE 1I[ε >

√ 2
τ ]ε ≤ 2(τ + 2)e−τ /2

IE−i {((Σ −1/2 Z)T γ)2 − 1}{h(Z(X 0 + θ(Xi − Xi0 ), t)) − h(Z(X 0 , t))}


2δ
≤ |τ − 1| CA +
ϕ(Xi − Xi0 ) + 2(τ + 2)e−τ /2
∆
We need also another upper bound for this expectation when t close to 1.
IE−i {((Σ −1/2 Z)T γ)2 − 1}h(Z(X 0 , t))
√
√
√
= IE−i {((Σ −1/2 Z)T γ)2 − 1}{h( tX 0 + 1 − tZ) − h( tX 0 )}
√
0
≤ IE−i |((Σ −1/2 Z)T γ)2 − 1| |gx,∆
| ϕ( 1 − tZ)
1p
≤
2IEϕ2 (Z)
∆
Set ∆ = δ/(2CA )
B=

2CA p
2IEϕ2 (Z) µ2
δ

Set τ = 2 log(4µ2 /(CA µ3 ))
A = 5|τ − 1|CA µ3 + 2(τ + 2)e−τ /2 µ2


4µ2
≤ 11CA µ3 log
C A µ3
√
A
log(α) + 2B α + CA ∆
2
≤ 2A (1 + log(2Bδ) − log(δ) − log(A))

δ≤−

≤ 2A (1 + log(2Bδ) − 2 log(A) + log log(2Bδ) − log log(A))
!
p


2IEϕ2 (Z)µ2
4µ2
≤ 22CA µ3 log
log
C A µ3
20CA µ23

Remark. In i.i.d case with Σ = Ip and φ(x) = O(kxk)

|IP (ϕ(X) > x) − IP (ϕ(Z) > x)| = O CA µ3 log2 (n)
Note that lemma 12 improves the classical Multivariate Berry–Esseen Theorem [4] for the case of
sub-additive functions φ(x) = O(kxk). Namely it answers the open question “Whether one can
remove or replace the factor p1/4 by a better one (eventually by 1)”.
22

Remark. In i.i.d case with Σ = Ip and φ(x) = kxk, which is rather common is statistical learning
√
theory, one have CA = O(1/ p) and
log2 (n)
|IP (kXk > x) − IP (kZk > x)| = O p √
n




***
The author thanks Prof. Roman Karasev, Prof. Vladimir Spokoiny and Prof. Dmitriy Dylov
for discussion and contribution to this paper.

References
[1] Martial Agueh and Guillaume Carlier. Barycenters in the Wasserstein space. SIAM Journal
on Mathematical Analysis, 43(2):904–924, 2011.
[2] Heinz H. Bauschke and Patrick L. Combettes. Convex Analysis and Monotone Operator
Theory in Hilbert Spaces. Springer Publishing Company, Incorporated, 1st edition, 2011.
[3] V. Bentkus. A new method for approximations in probability and operator theories. Lithuanian Mathematical Journal, 43(4):367–388, Oct 2003.
[4] V. Bentkus. On the dependence of the berry–esseen bound on dimension. Journal of Statistical
Planning and Inference, 2003.
[5] Massart P. Boucheron S., Lugosi G. Concentration inequalities: A nonasymptotic theory of
independence. Oxford University Press, 2013.
[6] D.A. Edwards. On the kantorovich–rubinstein theorem. Expositiones Mathematicae, 29(4):387
– 398, 2011.
[7] Nicolas Papadakis Jérémie Bigot, Elsa Cazelles. Penalized barycenters in the wasserstein
space. arXiv:1606.01025, 2016.
[8] Axel Munk Max Sommerfeld. Inference for empirical wasserstein distances on finite spaces.
arXiv:1610.03287v2, 2016.
[9] V. Spokoiny. Nonparametric estimation: parametric view. 2016.
[10] Stefan Steinerberger. Wasserstein distance, fourier series and applications. arXiv:1803.08011,
2018.
[11] Anja Sturm Thomas Rippl, Axel Munk. Limit laws of the empirical wasserstein distance:
Gaussian distributions. arXiv:1507.04090v2, 2015.

23

