Static and Dynamic Fusion for Multi-modal Cross-ethnicity
Face Anti-spoofing
Ajian Liu1∗ , Zichang Tan2∗, Xuan Li3 , Jun Wan2†, Sergio Escalera4 , Guodong Guo5 , Stan Z. Li1,2
1
MUST, Macau, China; 2 NLPR, CASIA, China; 3 BJTU, China; 4 CVC, UB, Spain; 5 Baidu Research, China

arXiv:1912.02340v2 [cs.CV] 16 Dec 2019

{jun.wan, szli}@ia.ac.cn, sergio@maia.ub.es,guoguodong01@baidu.com

Abstract
Regardless of the usage of deep learning and handcrafted methods, the dynamic information from videos and
the effect of cross-ethnicity are rarely considered in face
anti-spoofing. In this work, we propose a static-dynamic
fusion mechanism for multi-modal face anti-spoofing. Inspired by motion divergences between real and fake faces,
we incorporate the dynamic image calculated by rank pooling with static information into a conventional neural network (CNN) for each modality (i.e., RGB, Depth and infrared (IR)). Then, we develop a partially shared fusion
method to learn complementary information from multiple
modalities. Furthermore, in order to study the generalization capability of the proposal in terms of cross-ethnicity
attacks and unknown spoofs, we introduce the largest public cross-ethnicity Face Anti-spoofing (CASIA-SURF CeFA)
dataset, covering 3 ethnicities, 3 modalities, 1607 subjects,
and 2D plus 3D attack types. Experiments demonstrate that
the proposed method achieves state-of-the-art results on
CASIA-SURF CeFA, CASIA-SURF, OULU-NPU and SiW.

Figure 1. Samples of the CASIA-SURF CeFA dataset. It contains
1607 subjects, 3 different ethnicities (i.e., Africa, East Asia, and
Central Asia), with 4 attack types (i.e., print attack, replay attack,
3D print and silica gel attacks).

ages for face anti-spoofings [19, 26]. However, the analysis
of motion divergences between real and fake faces received
little attention. In order to improve robustness in real applications, some temporal-based methods [24, 26] have been
proposed, which require from a constrained human guided
interaction, such as movements of of eyes, lips, and head.
However, this does not provide with a natural user friendly
interaction. Different from those works, we capture the temporal/dynamic information by using dynamic image generated by rank pooling [14], which doesn’t need any human
guide interaction. Moreover, inspired by the motion divergences between real and fake faces, a static- and dynamicbased network (SD-Net) is further formulated by taking the
static and dynamic images as the input.

1. Introduction
In order to enhance security of face recognition systems,
the presentation attack detection (PAD) technique is a vital
stage prior to visual face recognition [4, 5, 20, 28]. Most
works in face anti-spoofing focus on still-images, including
RGB, Depth or IR). These methods can be divided into two
main categories: handcrafted methods [4, 10, 27] and deep
learning based methods [12, 19, 26]. Handcrafted methods
attempt to extract texture information or statistical features
(i.e., HOG [18, 36] and LBP [22, 10]) to distinguish between real and spoof faces. Deep learning based methods
automatically learn discriminative features from input im∗ Equal

Multi-modal face anti-spoofing have also absorbed an increasing number of researchers in recent two years. Some
fusion methods [41, 25] are published, which restrict the
interactions among different modalities since they are independent before the fusion. But it is difficult for different modalities to effectively utilize the modality relatedness from the beginning of the network to its end to
boost the overall performance. In this paper, we propose a
partially shared branch multi-modal network (PSMM-Net)

Contribution
Author, email: jun.wan@ia.ac.cn

† Corresponding

1

with allowing the exchanges and interactions among different modalities, aiming to capture correlated and complementary features.
Dataset
Replay-Attack [7]
CASIA-FASD [42]
3DMAD [11]
MSU-MFSD [35]
Replay-Mobile [9]
Msspoof [8]
OULU-NPU [6]
SiW [20]

Year # sub # num Attacks Mod. Dev.
2012 50
1200
Pr,Re
R
CR
2012 50
600 Pr,Cu,Re
R
CR
2014 17
255
M
R/D CR/K
2015 35
440
Pr,Re
R
P/L
2016 40
1030
Pr,Re
R
P
Pr
2016 21 4704i
R/I CR/CI
CR
2017 55
5940
Pr,Re
R
2018

165

4620

Pr,Re

R

CR

its high generalization capability.

2. Related Work
2.1. Methods

Eth.
*
*
*
*
*
*
*
AS/A/
U/I
E

Image-based Methods. Image-based methods take still
images as input, i.e., RGB, Depth or IR. Classical approaches based on handcrafted features, such as HOG [36],
LBP [22, 10], SIFT [27] or SURF [4] together with traditional classifiers, such as SVM or LDA, to perform binary anti-spoofing predictions. However, those methods
lack of good generalization capability when testing conditions vary, such as lighting and background. Owing to the
success of deep learning strategies over handcrafted alternatives in computer vision, some works [12, 19, 26] extended
feature vectors with features from CNN networks for face
anti-spoofing. Authors of [3, 20] presented a two-stream
network using RGB and Depth images as input. The work
of [21] proposes a deep tree network to model spoofs by hierarchically learning sub-groups of features. However, previous methods do not consider any kind of temporal information for face anti-spoofing.
Temporal-based Methods. In order to improve robustness in real applications, some temporal-based methods [24, 26] have been proposed, which require from a constrained human guided interaction, such as movements of
of eyes, lips, and head. However, those methods do not provide with a natural user friendly interaction. Even more importantly, these methods [24, 26] could become vulnerable
if someone presents a replay attack or a print photo attack
with cut eye/mouth regions. Given that the Photoplethysmography (rPPG) signals (i.e. heart plus signal) can be detected from real but not spoof, Liu et al. [20] proposed a
CNN-RNN model to estimate rPPG signals with sequencewise supervision and face depth with pixel-wise supervision. The estimated depth and rPPG are fused to distinguish real and fake faces. Feng et al. [12] distinguished between real and fake samples based on the the difference between image quality and optical flow information. Yang et
al. [37] proposed a spatio-temporal attention mechanism to
fuse global temporal and local spatial information. All previous methods rely on a single visual modality, and no work
considers the effect of cross-ethnicity for anti-spoofing.
Multi-modal Fusion Methods. Zhang et al. [41] proposed a fusion network with 3 streams using ResNet-18 as
the backbone, where each stream is used to extract low level
features from RGB, Depth and IR data, respectively. Then,
these features are concatenated and passed to the last two
residual blocks. Similar to [41], Aleksandr et al. [25] used
a fusion network with 3 streams. They used ResNet-34 as
the backbone and multi-scale feature fusion at all residual
blocks. Tao et al. [29] proposed a multi-stream CNN architecture called FaceBagNet, which uses patch-level im-

2019 1000 21000 Pr,Cu R/D/I
S
1500 18000 Pr, Re
CASIA-SURF CeFA
99
5346
M
R/D/I
S
A/E/C
2019
(Ours)
8
192
G
Total: 1607 subjects, 23538 videos
CASIA-SURF [41]

Table 1. Comparisons among existing face PAD databases. (i and
* indicates the dataset only contains imges and does not provide
specify ethnicities, respectively. Mod.: modalities, Dev.: devices,
Eth.: ethnicities, Pr: print attack, Re: replay attack, Cu: Cut, M:
3D print face mask, G: 3D silica gel face mask, R: RGB, D: Depth,
I: IR, CR: RGB Camera, CI: IR Camera, K: Kinect, P: Cellphone,
L: Laptop, S: Intel Realsense, AS: Asian, A: Africa, U: Caucasian,
I: Indian, E: East Asia, C: Central Asia.)

Furthermore, data plays a key role in face anti-spoofing
tasks. About existing face anti-spoofing datasets, such as
CASIA-FASD [42], Replay-Attack [7], OULU-NPU [6],
and SiW [20], the amount of sample is relatively small
and most of them just contain the RGB modality. The recently released CASIA-SURF [41] includes 1,000 subjects
and RGB, Depth and IR modalities. Although this provides
with a larger dataset in comparison to the existing alternatives, it suffers from limited attack types (2D print attack)
and single ethnicity (Chinese people). Overall, the effect of
cross-ethnicity for face anti-spoofing received little attention in previous works. Therefore, we introduce CASIASURF CeFA dataset, the largest dataset in terms of subjects
(see Table 1). In CASIA-SURF CeFA, attack types are diverse, including printing from cloth, video replay attack, 3D
print and silica gel attacks. More importantly, it is the first
public dataset designed for exploring the impact of crossethnicity in the study of face anti-spoofing. Some samples
of the CASIA-SURF CeFA dataset are shown in Fig. 1.
To sum up, the contributions of this paper are summarized as follows: (1) We propose the SD-Net to learn
both static and dynamic features for single modality . It is
the first work incorporating dynamic images for face antispoofing. (2) We propose the PSMM-Net to learn complementary information from multi-modal data in videos. (3)
We release the CASIA-SURF CeFA dataset, which includes
3 ethnicities, 1607 subjects and 4 diverse 2D/3D attack
types. (4) Extensive experiments of the proposed method
on CASIA-SURF CeFA and other 3 public datasets verify
2

into a feature vector. The learning process can be seen as a
convex optimization problem using the RankSVM [30] formulation in Eq.1. Let RGB (Depth or IR) video sequence
with K frames be represented as < I1 , I2 , ..., Ii , ..., IK >,
and Ii denotes the average of RGB (Depth or IR) features
over time up to i-frame. The process is formulated below.
X
1
argmin kdk2 + δ ×
ξij
2
d
i>j

(1)

T

s.t. d ·(Ii − Ij ) ≥ 1 − ξij , ξij ≥ 0
2
.
where ξij is the slack variable, and δ = K(K−1)
By optimizing Eq. 1, we map a sequence of K frames
to a single vector d. In this paper, rank pooling is directly
applied on the pixels of RGB (Depth or IR) frames and the
dynamic image d is of the same size as the input frames.
In our case, given input frame, we compute its dynamic image online with rank pooling using K consecutive frames.
Our selection of dynamic images for rank pooling in SDNet is further motivated by the fact that dynamic images
have proved its superiority to regular optical flow [32, 14].

Figure 2. SD-Net diagram, showing a single-modal staticdynamic-based network. We take the RGB modality and its corresponding dynamic image as an example. This architecture includes three branches: static (red arrow), dynamic (blue arrow)
and static-dynamic (green arrow). The static-dynamic branch
fuses the static and dynamic features of first res block outputs from
static and dynamic branches (best viewed in color).

ages as input and modality feature erasing (MFE) operation
to prevent overfitting and obtain more discriminative fused
features. All previous methods just consider as a key fusion component the concatenation of features from multiple
modalities. Unlike [41, 25, 29], we propose the PSMM-Net,
where three modality-specific networks and one shared network are connected by using a partially shared structure to
learn discriminative fused features for face anti-spoofing.

Single-modal SD-Net. As shown in Fig. 2, taking the RGB
modality as an example, we propose the SD-Net to learn
hybrid features from static and dynamic images. It contains
3 branches: static, dynamic and static-dynamic branches,
which learn complementary features. The network takes
ResNet-18 [16] as the backbone. For static and dynamic
branches, each of them consists of 5 blocks (i.e., conv, res1,
res2, res3, res4) and 1 Global Average Pooling (GAP) layer,
while in the static-dynamic branch, the conv and res1 blocks
are removed because it takes fused features of res1 blocks
from static and dynamic branches as input.
For convenience of terminology with the rest of the paper, we divide residual blocks of the network into a set
of modules {Mtκ }4t=1 according to feature level, where
κ ∈ {color, depth, ir} is an indicator of the modality and t
represents the feature level. Except for the first module M1κ ,
each module extracts static, dynamic and static-dynamic
features by using a residual block, denoted as Xts,κ , Xtd,κ
and Xtf,κ , respectively. The output features from each module are used as the input for the next module. The staticdynamic features X1f,κ of the first module are obtained by
directly summing X1s,κ and X1d,κ .
In order to ensure each branch learns independent features, each branch employs an independent loss function after the GAP layer [31]. In addition, a loss function based on
the summed features from all three branches is employed.
The binary cross-entropy loss is used as the loss function.
All branches are jointly and concurrently optimized to capture discriminative and complementary features for face
anti-spoofing in image sequences. The overall objective

2.2. Datasets
Table 1 lists existing face anti-spoofing datasets. One
can see that before 2019 the maximum number of available
subjects was 165 on the SiW dataset [20]. That was clearly
limiting the generalization ability of new approaches for
cross-dataset evaluation. Most of the datasets just contain
RGB data, such as Replay-Attack [7], CASIA-FASD [42],
SiW [20] and OULU-NPU [6]. Recently, the CASIASURF [41] has been released, including 1000 subjects with
three modalities, namely RGB, Depth and IR. Although this
relieved the problem of the amount of data, it is limited in
terms of attack types (only 2D print attack) and only includes 1 ethnicity (Chinese people). As shown in Table 1,
most datasets do not provide ethnicity information, except
SiW and CASIA-SURF. Although the SiW dataset provides
four ethnicities, it still does not consider the effect of crossethnicity for face anti-spoofing. This limitation also holds
for the CASIA-SURF dataset.

3. Proposed Method
3.1. SD-Net for Single-modal
Single-modal Dynamic Image Construction. Rank pooling [14, 33] defines a rank function that encodes a video
3

the first residual block is removed from the shared branch,
thus S1 equals to zero.
Backward Feeding. Shared features St are delivered
back to the SD-Nets of the different modalities. The static
features Xts,κ and dynamic features Xtd,κ add with St for
feature fusion. This can be denoted as:
e t = Xt + St ,
X
s,κ
s,κ

function of SD-Net for the κth modality is defined as:
(2)

L = Lwhole + Lcolor + Ldepth + Lir

where Lκs , Lκd , Lκf and Lκsdf are the losses for static branch,
dynamic branch, static-dynamic branch, and summed features from all three branches of the network, respectively.

(5)

4. CASIA-SURF CeFA dataset
This section describes the CASIA-SURF CeFA dataset.
The motivation of this dataset is to provide with an increased diversity of attack types compared to existing
datasets, as well as to explore the effect of cross-ethnicity
in face anti-spoofing, which has received little attention in
the literature. Furthermore, it contains three visual modalities, i.e.., RGB, Depth, and IR. Summarizing, the main purpose of CASIA-SURF CeFA is to provide with the largest
up to date face anti-spoofing dataset to allow for the evaluation of the generalization performance of new PAD methods in three main aspects: cross-ethnicity, cross-modality
and cross-attacks. In this section, we describe the CASIASURF CeFA dataset in detail, including acquisition details,
attack types, and proposed evaluation protocols.
Acquisition Details. We use the Intel Realsense to capture the RGB, Depth and IR videos simultaneously at 30
fps. The resolution is 1280 × 720 pixels for each video
frame and all modalities. Performers are asked to move
smoothly their head so as to have a maximum of around
300 deviation of head pose in relation to frontal view. Data
pre-processing is similar to the one performed in [41], expect that PRNet [13] is replaced by 3DFFA [44] for face region detection. Examples of original recorded images from
video sequences and processed face regions for different visual modalities are shown in Fig. 1.

3.2. PSMM-Net for Multi-modal Fusion
The architecture of the proposed PSMM-Net is shown
in Fig. 3. It consists of two main parts: a) the modalityspecific network, which contains three SD-Nets to learn features from RGB, Depth, IR modalities, respectively; b) and
a shared branch for all modalities, which aims to learn the
complementary features among different modalities.
For the shared branch, we adopt ResNet-18, removing
the first conv layer and res1 block. In order to capture
correlations and complementary semantics among different
modalities, information exchange and interaction among
SD-Nets and the shared branch are designed. This is done
in two different ways: a) forward feeding of fused SD-Net
features to the shared branch, and b) backward feeding from
shared branch modules output to SD-Net block inputs.
Forward Feeding. We fuse static and dynamic SD-Nets
features from all modality branches and fed them as input
to its corresponding shared block. The fused process at tth
feature level can be formulated as:
X
X
S̃t =
Xts,κ +
Xtd,κ + St t = 1, 2, 3
(3)
κ

(4)

where t ranges from 2 to 3.
e t and X
e t become the new
After feature fusion, X
s,κ
d,κ
static and dynamic features, which are then feed to the
next module Mt+1
κ . Note that the exchange and interaction
among SD-Nets and the shared branch are only performed
for static and dynamic features. This is done to avoid hybrid features among static and dynamic information to be
disturbed by multi-modal semantics.
Loss Optimization. There are two main kind of losses
employed to guide the training of PSMM-Net. The first corresponds to the losses of the three SD-Nets, i.e.. color, depth
and ir modalities, denoted as Lcolor , Ldepth and Lir , respectively. The second corresponds to the loss that guides
the entire network training, denoted as Lwhole , which bases
on the summed features from all SD-Nets and the shared
branch. The overall loss L of PSMM-Net is denoted as:

Figure 3. The PSMM-Net diagram. It consists of two main parts.
The first is the modality-specific network, which contains three
SD-Nets to learn features from RGB, Depth, IR modalities, respectivel. The second is a shared branch for all modalities, which
aims to learn the complementary features among different modalities.

Lκ = Lκs + Lκd + Lκf + Lκsdf

e t = Xt + St
X
d,κ
d,κ

κ

In the shared branch, S̃t denotes the input to the (t + 1)th
block, and St denotes the output of the tth block. Note that
4

Prot.

Subset
11
A
A
C&E

Ethnicity
12
13
C
E
C
E
A&E A&C

Subjects

Modalities

PAIs

# real videos

# fake videos

# all videos

1-200
201-300
301-500

R&D&I
R&D&I
R&D&I

600/600/600
300/300/300
1200/1200/1200

1800/1800/1800
900/900/900
6600/6600/6600

2400/2400/2400
1200/1200/1200
7800/7800/7800

R&D&I
R&D&I
R&D&I
31
32
33
R
D
I
R
D
I
D&I R&I R&D

Pr&Re
Pr&Re
Pr&Re
21 22
Pr
Re
Pr
Re
Pe
Pr

1800/1800
900/900
1800/1800

3600/1800
1800/900
4800/6600

5400/3600
2700/1800
6600/8400

Pr&Re
Pr&Re
Pr&Re

600/600/600
300/300/300
1200/1200/1200

1800/1800/1800
900/900/900
5600/5600/5600

2400/2400/2400
1200/1200/1200
6800/6800/6800

Re
Re
Pr

600/600/600
300/300/300
1200/1200/1200

600/600/600
300/300/300
5400/5400/5400

1200/1200/1200
600/600/600
6600/6600/6600

1

Train
Valid
Test

2

Train
Valid
Test

A&C&E
A&C&E
A&C&E

1-200
201-300
301-500

3

Train
Valid
Test

1-200
201-300
301-500

4

Train
Valid
Test

A&C&E
A&C&E
A&C&E
41
42
43
A
C
E
A
C
E
C&E A&E A&C

1-200
201-300
301-500

R
R
R

D
D
D

I
I
I

Table 2. Four evaluation protocols are defined for CASIA-SURF CeFA; 1) cross-ethnicity, 2) cross-PAI, 3) cross-modality and 4) crossethnicity&PAI, respectively. Note that 3D attacks subset of CASIA-SURF CeFA are included to the test set of every testing protocol (not
shown in the table). R: RGB, D: Depth, I: IR, A: Africa, C: Central Asia, E: East Asia, Pr: print attack, Re: replay attack; & indicates
merging; ∗ ∗ corresponds to the name of sub-protocols.

in two attacks styles and four lighting environments. Attacks include wearing a wig and glasses and wearing a wig
and no glasses. Lighting environments include indoor side
light, indoor front light, indoor backlit and indoor normal
light. In total, there are 196 videos (64 per modality).
Evaluation Protocols. We design four protocols for the
2D attacks subset, as shown in Table 2, totalling 11 subprotocols (1 1, 1 2, 1 3, 2 1, 2 2, 3 1, 3 2, 3 3, 4 1, 4 2,
and 4 3). We divide 500 subjects per ethnicity into three
subject-disjoint subsets (second and fourth columns in Table 2). Each protocol has three data subsets: training, validation and testing sets, which contain 200, 100, and 200
subjects, respectively.
• Protocol 1 (cross-ethnicity): Most of the public face
PAD datasets just contain a single ethnicity. Even though
there are few datasets [20, 41] containing multiple ethnicities, they lack of ethnicity labels or do not provide with a
protocol to perform cross-ethnicity evaluation. Therefore,
we design the first protocol to evaluate the generalization
of PAD methods for cross-ethnicity testing. One ethnicity
is used for training and validation, and the left two ethnicities are used for testing. Therefore, there are three different
evaluations (third column of Protocol 1 in Table 2.
• Protocol 2 (cross-PAI): Given the diversity and unpredictability of attack types from different presentation attack
instruments (PAI), it is necessary to evaluate the robustness
of face PAD algorithms to this kind of variations (sixth column of Protocol 2 in Table. 2).
• Protocol 3 (cross-modality): Given the release of affordable devices capturing complementary visual modalities (i.e., Intel Resense, Mircrosoft Kinect), recently the
multi-modal face anti-spoofing dataset was proposed [41].
However, there is no standard protocol to explore the generalization of face PAD methods when different train-test

Figure 4. Age and gender distributions of the CASIA-SURF CeFA.

Statistics. As shown in Table 1, CASIA-SURF CeFA
consists of 2D and 3D attack subsets. For the 2D attack
subset, it includes print and video-reply attacks, and three
ethnicites (African, East Asian and Central Asian) with 2
attacks (print face from cloth and video-replay). Each ethnicity has 500 subjects. Each subject has 1 real sample, 2
fake samples of print attack captured in indoor and outdoor,
and 1 fake sample of video-replay. In total, there are 18000
videos (6000 per modality). The age and gender statistics
for the 2D attack subset of CASIA-SURF CeFA is shown in
Fig. 4.
For the 3D attack subset, it has 3D print mask and silica gel face attacks. For 3D print mask, it has 99 subjects, each subject with 18 fake samples captured in three
attacks and six lighting environments. Attacks include only
mask, wearing a wig and glasses, and wearing a wig and
no glasses. Lighting conditions include outdoor sunshine,
outdoor shade, indoor side light, indoor front light, indoor
backlit and indoor regular light. In total, there are 5346
videos (1782 per modality). For silica gel face attacks, it
has 8 subjects, each subject has 8 fake samples captured
5

Prot. name

APCER(%) BPCER(%)

ACER(%)

11
0.5
0.8
0.6
12
4.8
4.0
4.4
Prot. 1
13
1.2
1.8
1.5
Avg±Std
2.2±2.3
2.2±1.6
2.2±2.0
21
0.1
0.7
0.4
Prot. 2
22
13.8
1.2
7.5
Avg±Std
7.0±9.7
1.0±0.4
4.0±5.0
31
8.9
0.9
4.9
32
22.6
4.6
13.6
Prot. 3
33
21.1
2.3
11.7
Avg±Std 17.5±7.5
2.6±1.9
10.1±4.6
41
33.3
15.8
24.5
42
78.2
8.3
43.2
Prot. 4
43
50.0
5.5
27.7
Avg±Std 53.8±22.7
9.9±5.3
31.8±10.0
Table 3. PSMM-Net evaluation on the four protocols of CASIASURF CeFA dataset, where A B represents sub-protocol B from
Protocol A, and Avg±Std indicates the mean and variance operation.

Figure 5. Comparison of network units for multi-modal fusion
strategies. From left to right: NHF, PSMM-NET-WoBF and
PSMM-Net. The fusion process for the tth feature level of each
strategy is shown at the bottom.

modalities are considered for evaluation. We define three
cross-modality evaluations, each of them having one modality for training and the two remaining ones for testing (fifth
column of Protocol 3 in Table. 2).
• Protocol 4 (cross-ethnicity & PAI): The most challenging protocol is designed via combining the condition of both
Protocol 1 and 2. As shown in Protocol 4 of Table. 2, the
testing subset introduces two unknown target variations simultaneously.
Like [6], the mean and variance of evaluate metrics for
these four protocols are calculated in our experiments. Detailed statistics for the different protocols are shown in Table 2. More information about CASIA-SURF CeFA can be
found in our supplementary material.

ing metrics based on respective official protocols: Attack
Presentation Classification Error Rate (APCER) [1], Bona
Fide Presentation Classification Error Rate (BPCER), Average Classification Error Rate (ACER), and Receiver Operating Characteristic (ROC) curve [41].

5.2. Implementation Details
The proposed PSMM-Net is implemented with Tensorflow [2] and run on a single NVIDIA TITAN X GPU. We
resize the cropped face region to 112×112, and use random
rotation within the range of [−1800 , 1800 ], flipping, cropping and color distortion for data augmentation. All models
are trained for 25 epochs via Adaptive Moment Estimation
(Adam) algorithm and initial learning rate of 0.1, which is
decreased after 15 and 20 epochs with a factor of 10. The
batch size of each CNN stream is 64, and the length of the
consecutive frames used to construct dynamic map is set to
7 by our experimental experience. In addition, all fusion
points in this work use element summation operations to
prevent dimension explosion.

5. Experiments
In this section, we conduct a series of experiments on
public available face anti-spoofing datasets to verify the effectiveness of our methodology and the benefits of the presented CASIA-SURF CeFA dataset. In the following, we
will introduce the employed datasets & metrics, implementation details, experimental setting, and results & analysis
sequentially.

5.1. Datasets & Metrics

5.3. Baseline Model Evaluation

We evaluate the performance of PSMM-Net on two
multi-modal (i.e., RGB, Depth and IR) datasets: CASIASURF CeFA and CASIA-SURF [41], while evaluate the
SD-Net on two single-modal (i.e., RGB) face anti-spoofing
benchmarks: OULU-NPU [6] and SiW [20]. They are the
mainstream datasets released in recent years with their own
characteristics in terms of the number of subject, modality
and ethnicity, attack types, acquisition device and PAIs et
al.. Therefore, experiments on these datasets can verify the
performance of our method more convincingly.
In order to perform a consistent evaluation with prior
works, we report the experimental results using the follow-

Before exploring the traits of our dataset, we first provide a benchmark for CASIA-SURF CeFA based on the
proposed method. From the Table 3, in which the results
of the four protocols are derived from all the respective
sub-protocols by calculating the mean and variance, we
can draw the following conclusions: (1) from the results of
the three sub-protocols in Protocol 1, the ACER scores are
0.6%, 4.4% and 1.5%, respectively, indicating that it is necessary to study the generalization of the face PAD method
for different ethnicity; (2) In the case of Protocol 2, when
print attack is used for training/validation and video-replay
6

Prot.1

RGB
Depth
IR
APCER(%) BPCER(%) ACER(%) APCER(%) BPCER(%) ACER(%) APCER(%) BPCER(%) ACER(%)

S-Net
28.1±3.6
6.4±4.6
17.2±3.6
5.6±3.0
9.8±4.2
7.7±3.5
11.4±2.1
8.2±1.2
9.8±1.7
D-Net
20.6±4.0
19.3±9.0
19.9±4.0
11.2±5.1
7.5±1.5
9.4±2.0
8.1±1.8
14.4±3.8
11.3±2.1
SD-Net 14.9±6.0
10.3±1.8
12.6±3.4
7.0±8.1
5.2±3.5
6.1±5.4
7.3±1.2
5.5±1.8
6.4±1.3
Table 4. Ablation experiments on three single-modal groups: RGB, Depth and IR. Each modality group contains three experiments: static
branch, dynamic branch and static-dynamic branch. Numbers in bold correspond to the best results per column.
Prot. name
Prot. 1
Prot. 2
Prot. 3∗
Prot. 4

APCER(%)

RGB
BPCER(%)

(ACER%)

APCER(%)

Depth
BPCER(%)

(ACER%)

APCER(%)

IR
BPCER(%)

(ACER%)

14.9±6.0
45.0±39.1
5.9
65.8±16.4

10.3±1.8
1.6±1.9
2.2
8.3±6.5

12.6±3.4
23.3±18.6
4.0
35.2±5.8

7.0±8.1
13.6±18.7
0.3
18.5±8.2

5.2±3.5
1.2±0.7
0.3
7.0±5.2

6.1±5.4
7.4±9.7
0.3
12.7±5.7

7.3±1.2
8.1±11.0
0.2
6.8±2.9

5.5±1.8
1.5±1.8
0.5
4.2±3.3

6.4±1.3
4.8±6.4
0.4
5.5±2.2

Table 5. Experimental results of the SD-Net based on single modality on four protocols (∗ indicates that the modal type of the testing subset
is consistent with the training subset).
PSMM-Net
APCER(%) BPCER(%) ACER(%)
RGB
14.9±6.0
10.3±1.8
12.6±3.4
RGB&Depth
2.3±2.9
9.2±5.9
5.7±3.5
RGB&Depth&IR
2.2±2.3
2.2±1.6
2.2±2.0
Table 6. Ablation experiments on the effect of multiple modalities. Numbers in bold correspond to the best result per column.

Protocol 1 (cross-ethnicity) of the CASIA-SURF CeFA
dataset.
Static and Dynamic Features. We evaluate S-Net
(Static branch of SD-Net), D-Net (Dynamic branch of SDNet) and SD-Net. Results for RGB, Depth and IR modalities are shown in Table 4. Compared with S-Net and D-Net,
SD-Net achieves superior performance. For RGB, Depth
and IR modalities, ACER of SD-Net is 12.6%, 6.1%, 6.4%,
versus 17.2%, 7.7%, 9.4% of S-Net (improved by 4.6%,
1.6%, 3.4%) and 19.9%, 9.4%, 11.3% of D-Net (improved
by 7.3%, 3.3%, 4.9%), respectively. Furthermore, Table 4
shows that the performance of Depth and IR modalities are
superior to the one of RGB. One reason is the variability in
lighting conditions included in CASIA-SURF CeFA.
In addition, we provide the results of single-modal experiments on the 4 protocols to facilitate comparison of face
PAD algorithms, shown in Table 5. It shows that when only
single modality is used, the performance of the depth or IR
modality is superior to that of the RGB modality.
Multiple Modalities. In order to show the effect of
analysing a different number of modalities, we evaluate
one modality (RGB), two modalities (RGB and Depth), and
three modalities (RGB, Depth and IR) on PSMM-Net. As
shown in Fig. 3, the PSMM-Net contains three SD-Nets and
one shared branch. When only RGB modality is considered,
we just use one SD-Net for evaluation. When two or three
modalities are considered, we use two or three SD-Nets and
one shared branch to train the PSMM-Net model, respectively. Results are shown in Table 6. The best results are
obtained when using all three modalities, which 2.2% of
APCER, 2.2% of BPCER and 2.2% of ACER. Compared
with the performance of using single RGB modality and two
modalities, the improvement in performance corresponds to
12.7% and 0.1% for APCER, 8.1% and 7.2% for BPCER,
and 10.4% and 3.5% for ACER, respectively.
Fusion Strategy. In order to evaluate the performance
of PSMM-Net, we compare it with other two variants:

Prot.1

Method
NHF
PSMM-WoBF
PSMM-Net

APCER(%)
25.3±12.2
12.7±0.4
2.2±2.3

BPCER(%)
4.4±3.1
3.2±2.3
2.2±1.6

ACER(%)
14.8±6.8
7.9±1.3
2.2±2.0

Table 7. Comparison of fusion strategies in Protocol 1 of CASIASURF CeFA. The number in black indicates best results.

and 3D mask are used for testing, the ACER score is 0.4%
(sub-protocol 2 1), while video-replay attack is used for
training/validation, and print attack and 3D attack are used
for testing, with an ACER score of 7.5% (sub-protocol 2 2).
The large gap between the results of the two sub-protocols is
mainly caused by different PAI (i.e.. different displays and
printers) create different artifacts. (3) Protocol 3 evaluates
cross-modality. The best result is achieved for sub-protocol
3 1, with ACER of 4.9%. The other two sub-protocols
achieve a similar low performance score. This means the
best performance is achieved when RGB data of 2D attack subset is used for training/validation while the other
two modalities of 2D and 3D attack subsets are used for
testing. (4) Protocol 4 is the most difficult evaluation scenario, which simultaneously considers cross-ethnicity and
cross-PAI. All sub-protocols achieve poor performance, being 24.5%, 43.2%, and 27.7% ACER scores for 4 1, 4 2,
and 4 3 achieve, respectively.

5.4. Ablation Analysis
In order to verify the effectiveness of the proposed
method, we perform a series of ablation experiments on
7

Method

@FPR=10−2

TPR (%)
@FPR=10−3

@FPR=10−4

APCER (%)

BPCER (%)

ACER (%)

NHF fusion [41]
Single-scale SE fusion [41]
Multi-scale SE fusion [40]
PSMM-Net
PSMM-Net(CASIA-SURF CeFA)

89.1
96.7
99.8
99.9
99.9

33.6
81.8
98.4
99.3
99.7

17.8
56.8
95.2
96.2
97.6

5.6
3.8
1.6
0.7
0.5

3.8
1.0
0.08
0.06
0.02

4.7
2.4
0.8
0.4
0.2

Table 8. Comparison of the proposed method with three fusion strategies. All models are trained on the CASIA-SURF training subset and
tested on the testing subset. Best results are bolded.
Prot.

Naive halfway fusion (NHF) and PSMM-Net without backward feeding mechanism (PSMM-Net-WoBF). As shown in
Fig. 5, NHF combines the modules of different modalities at
a later stage (i.e., after M1κ module) and PSMM-Net-WoBF
strategy removes the backward feeding from PSMM-Net.
The fusion comparison results are shown in Table 7, showing higher performance of the proposed PSMM-Net, with
ACER of 2.2%.

1

2

5.5. Methods Comparison
CASIA-SURF Dataset. Comparison results are show
in Table 8. The performance of the PSMM-Net is superior to the ones of the competing multi-modal fusion methods, including Halfway fusion [41], single-scale SE fusion [41], and multi-scale SE fusion [40]. When compared
with [41, 40], PSMM-Net improves the performance by
at least 0.9% for APCER, 0.02% for NPECE, and 0.4%
for ACER. When the PSMM-Net is pretrained on CASIASURF CeFA, it further improves performance. Concretely,
the performance of T P R@F P R = 10−4 is increased by
2.4% when pretraining with the proposed CASIA-SURF
CeFA dataset.
In 2019 a challenge on the CASIA-SURF dataset was
run at CVPR 1 . The results of the challenge were very
promising, where 3 winning teams VisionLab [25], ReadSense [29] and Feather [39] got TPR=99.87%@F P R =
10−4 , 99.81%@F P R = 10−4 and 99.14%@F P R =
10−4 , respectively. The 2 main reasons of these high performance are: 1) several external datasets were used. VisionLab [25] used four lare-scale datasets, namely CASIAWebFace [38], MSCeleb-1M [15], AFAD-lite [23] and
Asian dataset [43] for pretraining, while Feather [39] used
a large private dataset with a collection protocol similar
to CASIA-SURF. 2) Many network ensembles. VisonLab [25], ReadSense [29], and Feather [39] average the
outputs of 24, 12 and 10 networks to compute final results.
Thus in order to have a fair comparison we omit VisionLab [25], ReadSense [29] and Feather [39] from Table 8.
SiW Dataset. Results for this dataset are shown in Table 9. We compare the proposed SD-Net with other methods without pretraining. Taking the Protocol 1 of SiW as

3

Method

APCER (%) BPCER (%) ACER (%) Pretrain

FAS-BAS [20]
FAS-TD-SF [34]
STASN [37]
SD-Net
FAS-TD-SF
(CASIA-SURF) [41]
STASN (Data) [37]
SD-Net (CASIA-SURF CeFA)

3.58
1.27
0.14

3.58
0.83
1.34

3.58
1.05
1.00
0.74

1.27

0.33

0.80

0.21

0.50

0.30
0.35

FAS-BAS [20]
FAS-TD-SF [34]
STASN [37]
SD-Net
FAS-TD-SF
(CASIA-SURF) [41]
STASN (Data) [37]
SD-Net (CASIA-SURF CeFA)

0.57±0.69
0.33±0.27
0.25±0.32

0.57±0.69
0.29±0.39
0.29±0.34

0.57±0.69
0.31±0.28
0.28±0.05
0.27±0.28

0.08±0.17

0.25±0.22

0.17±0.16

0.09±0.17

0.21±0.25

0.15±0.05
0.15±0.11

FAS-BAS [20]
FAS-TD-SF [34]
STASN [37]
SD-Net
FAS-TD-SF
(CASIA-SURF) [41]
STASN (Data) [37]
SD-Net (CASIA-SURF CeFA)

8.31±3.81
7.70±3.88
3.74±2.15

8.31±3.81 8.31±3.81
7.76±4.09 7.73±3.99
12.10±1.50
7.85±1.42 5.80±0.36

6.27±4.36

6.43±4.42

6.35±4.39

2.70±1.56

7.10±1.56

5.85±0.85
4.90±0.00

No

Yes

No

Yes

No

Yes

Table 9. Comparisons on SiW. ’-’ indicates unprovided; ’()’ means
the method is used a pretrain model trained from a specific dataset.
Best results are bolded in the condition of with/without pretrain.
Prot.

Method

3

FAS-BAS [20]
FAS-Ds [17]
STASN [37]
SD-Net
STASN (Data) [37]
SD-Net (CASIA-SURF CeFA)

2.7±1.3
4.0±1.8
4.7±3.9
2.7±2.5
1.4±1.4
2.7±2.5

3.1±1.7
3.8±1.2
0.9±1.2
1.4±2.0
3.6±4.6
0.9±0.9

2.9±1.5
3.6±1.6
2.8±1.6
2.1±1.4
2.5±2.2
1.8±1.4

FAS-BAS [20]
FAS-Ds [17]
STASN [37]
SD-Net
STASN (Data) [37]
SD-Net (CASIA-SURF CeFA)

9.3±5.6
5.1±6.3
6.7±10.6
4.6±5.1
0.9±1.8
5.0±4.7

10.4±6.0
6.1±5.1
8.3±8.4
6.3±6.3
4.2±5.3
4.6±4.6

9.5±6.0
5.6±5.7
7.5±4.7
5.4±2.8
2.6±2.8
4.8±2.7

4

APCER (%) BPCER (%) ACER (%) Pretrain
No

Yes

No

Yes

Table 10. Results of Protocol 3 and 4 on OULU-NPU. ’()’ means
the method is used a pretrain model trained from a specific dataset.
Best results are bolded in the conditions of with/without pretrain.

an example, SD-Net achieves the best ACER of 0.74%,
an improvement of 0.26% with respect to the second best
score, 1.00% (ACER) from STASN [37]. In terms of
CASIA-SURF CeFA pretraining, our method is competitive to STASN (Data) [37] (0.35% versus 0.3% in term of
ACER), which used an large proviate dataset as pretrain.
For Protocol 2 and 3 of SiW, our methods has achieved the
best performance under three evaluation metrics.
OULU-NPU Dataset. We perform evaluation on the

1 https://sites.google.com/qq.com/
face-anti-spoofing/welcome/challengecvpr2019?
authuser=0

8

2 most challenging protocols of OULU-NPU. Protocol 3
studies the generalization across different acquisition devices and Protocol 4 considers all the conditions of previous
three protocols simultaneously. The experimental Results
are shown in Table 10. In the case of comparison without pretraining, SD-Net obtains the best results in both Protocol 3 and 4. The ACER of our SD-Net is 2.1% versus
2.8% of STASN [37]. When comparing the results using
pretraining, our method achieves the first and second position for Protocol 3 and 4, respectively. Based on the above
experiments, when without using pretraining, the proposed
method can get the state-of-the-art performance (ACER)
in all protocols on SiW and OULU-NPU. The proposed
method with pretraining on CASIA-SURF CeFA can also
get the best ACER scores on most of protocols. Those experimental results clearly demonstrate the effectiveness of
the proposed method and the collected CASIA-SURF CeFA
dataset.

[7]

[8]

[9]

[10]

[11]

[12]

6. Conclusion
[13]

In this paper, we have presented the CASIA-SURF CeFA
dataset for face anti-spoofing. This corresponds to the
largest public available dataset in terms of modalities, subjects, ethnicities and attacks. Moreover, we have proposed
a static and dynamic network (SD-Net) to learn both static
and dynamic features from single modality. Then, we have
proposed a partially shared multi-modal network (PSMMNet) to learn complementary information from multi-modal
data in videos. Extensive experiments on four popular
datasets show the high generalization capability of the proposed SD-Net and PSMM-Net, and the utility and challenges of the released CASIA-SURF CeFA dataset.

[14]

[15]

[16]

[17]

References

[18]

[1] ISO/IEC JTC 1/SC 37 Biometrics. information technology biometric presentation attack detection part 1: Framework. international organization for standardization. 2016.
https://www.iso.org/obp/ui/iso.
[2] Martn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,
Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, and Michael Isard. Tensorflow: A
system for large-scale machine learning.
[3] Yousef Atoum, Yaojie Liu, Amin Jourabloo, and Xiaoming
Liu. Face anti-spoofing using patch and depth-based cnns.
In IJCB, pages 319–328. IEEE, 2017.
[4] Zinelabidine Boulkenafet, Jukka Komulainen, and Abdenour
Hadid. Face spoofing detection using colour texture analysis.
TIFS, 2016.
[5] Zinelabidine Boulkenafet, Jukka Komulainen, and Abdenour
Hadid. Face antispoofing using speeded-up robust features
and fisher vector encoding. SPL, 2017.
[6] Zinelabinde Boulkenafet, Jukka Komulainen, Lei Li, Xiaoyi
Feng, and Abdenour Hadid. Oulu-npu: A mobile face pre-

[19]

[20]

[21]

[22]

[23]

[24]

9

sentation attack database with real-world variations. In FG,
2017.
I. Chingovska, A. Anjos, and S. Marcel. On the effectiveness
of local binary patterns in face anti-spoofing. In Biometrics
Special Interest Group, 2012.
Ivana Chingovska, Nesli Erdogmus, André Anjos, and
Sébastien Marcel. Face recognition systems under spoofing
attacks. In Face Recognition Across the Imaging Spectrum.
2016.
Artur Costa-Pazo, Sushil Bhattacharjee, Esteban VazquezFernandez, and Sebastien Marcel. The replay-mobile face
presentation-attack database. In BIOSIG, 2016.
Tiago de Freitas Pereira, André Anjos, José Mario De Martino, and Sébastien Marcel. Can face anti-spoofing countermeasures work in a real world scenario? In ICB, 2013.
Nesli Erdogmus and Sebastien Marcel. Spoofing in 2d face
recognition with 3d masks and anti-spoofing with kinect. In
BTAS, 2014.
Litong Feng, Lai-Man Po, Yuming Li, Xuyuan Xu, Fang
Yuan, Terence Chun-Ho Cheung, and Kwok-Wai Cheung.
Integration of image quality and motion cues for face antispoofing: A neural network approach. JVCIR, 2016.
Yao Feng, Fan Wu, Xiaohu Shao, Yanfeng Wang, and Xi
Zhou. Joint 3d face reconstruction and dense alignment with
position map regression network. In ECCV, 2018.
Basura Fernando, Efstratios Gavves, José Oramas, Amir
Ghodrati, and Tinne Tuytelaars. Rank pooling for action
recognition. TPAMI, 39(4):773–787, 2017.
Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and
Jianfeng Gao. Ms-celeb-1m: A dataset and benchmark
for large-scale face recognition. In ECCV, pages 87–102.
Springer, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR,
2016.
Amin Jourabloo, Yaojie Liu, and Xiaoming Liu. Face despoofing: Anti-spoofing via noise modeling. arXiv, 2018.
Jukka Komulainen, Abdenour Hadid, and Matti Pietikainen.
Context based face anti-spoofing. In BTAS, 2013.
Lei Li, Xiaoyi Feng, Zinelabidine Boulkenafet, Zhaoqiang
Xia, Mingming Li, and Abdenour Hadid. An original face
anti-spoofing approach using partial convolutional neural
network. In IPTA, 2016.
Yaojie Liu, Amin Jourabloo, and Xiaoming Liu. Learning
deep models for face anti-spoofing: Binary or auxiliary supervision. In CVPR, 2018.
Yaojie Liu, Joel Stehouwer, Amin Jourabloo, and Xiaoming
Liu. Deep tree learning for zero-shot face anti-spoofing. In
CVPR, pages 4680–4689, 2019.
Jukka Määttä, Abdenour Hadid, and Matti Pietikäinen. Face
spoofing detection from single images using micro-texture
analysis. In IJCB, pages 1–7. IEEE, 2011.
Zhenxing Niu, Mo Zhou, Le Wang, Xinbo Gao, and Gang
Hua. Ordinal regression with multiple output cnn for age
estimation. In CVPR, pages 4920–4928, 2016.
Gang Pan, Lin Sun, Zhaohui Wu, and Shihong Lao.
Eyeblink-based anti-spoofing in face recognition from a
generic webcamera. In ICCV, 2007.

[43] Jian Zhao, Yu Cheng, Yan Xu, Lin Xiong, Jianshu Li, Fang
Zhao, Karlekar Jayashree, Sugiri Pranata, Shengmei Shen,
Junliang Xing, et al. Towards pose invariant face recognition
in the wild. In CVPR, pages 2207–2216, 2018.
[44] Xiangyu Zhu, Xiaoming Liu, Zhen Lei, and Stan Z Li. Face
alignment in full pose range: A 3d total solution. TPAMI,
41(1):78–92, 2017.

[25] Aleksandr Parkin and Oleg Grinchuk. Recognizing multimodal face spoofing with face recognition networks. In
PRCVW, pages 0–0, 2019.
[26] Keyurkumar Patel, Hu Han, and Anil K Jain. Cross-database
face antispoofing with robust feature representation. In
CCBR, 2016.
[27] Keyurkumar Patel, Hu Han, and Anil K Jain. Secure face
unlock: Spoof detection on smartphones. TIFS, 2016.
[28] Rui Shao, Xiangyuan Lan, Jiawei Li, and Pong C Yuen.
Multi-adversarial discriminative deep domain generalization
for face presentation attack detection. In CVPR, pages
10023–10031, 2019.
[29] Tao Shen, Yuyu Huang, and Zhijun Tong. Facebagnet: Bagof-local-features model for multi-modal face anti-spoofing.
In PRCVW, pages 0–0, 2019.
[30] Alex J Smola and Bernhard Schölkopf. A tutorial on support
vector regression. Statistics and computing, 14(3):199–222,
2004.
[31] Zichang Tan, Yang Yang, Jun Wan, Guodong Guo, and Stan
Li. Deeply-learned hybrid representations for facial age estimation. pages 3548–3554, 08 2019.
[32] Jue Wang, Anoop Cherian, and Fatih Porikli. Ordered pooling of optical flow sequences for action recognition. In
WACV, pages 168–176. IEEE, 2017.
[33] Pichao Wang, Wanqing Li, Jun Wan, Philip Ogunbona, and
Xinwang Liu. Cooperative training of deep aggregation networks for rgb-d action recognition. In Thirty-Second AAAI
Conference on Artificial Intelligence, 2018.
[34] Zezheng Wang, Chenxu Zhao, Yunxiao Qin, Qiusheng Zhou,
and Zhen Lei. Exploiting temporal and depth information for
multi-frame face anti-spoofing. arXiv, 2018.
[35] Di Wen, Hu Han, and Anil K Jain. Face spoof detection with
image distortion analysis. TIFS, 2015.
[36] Jianwei Yang, Zhen Lei, Shengcai Liao, and Stan Z Li. Face
liveness detection with component dependent descriptor. In
ICB, 2013.
[37] Xiao Yang, Wenhan Luo, Linchao Bao, Yuan Gao, Dihong
Gong, Shibao Zheng, Zhifeng Li, and Wei Liu. Face antispoofing: Model matters, so does data. In CVPR, pages
3507–3516, 2019.
[38] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning
face representation from scratch. arXiv, 2014.
[39] Peng Zhang, Fuhao Zou, Zhiwen Wu, Nengli Dai, Skarpness Mark, Michael Fu, Juan Zhao, and Kai Li. Feathernets: Convolutional neural networks as light as feather for
face anti-spoofing. 2019.
[40] Shifeng Zhang, Ajian Liu, Jun Wan, Yanyan Liang, Guogong
Guo, Sergio Escalera, Hugo Jair Escalante, and Stan Z Li.
Casia-surf: A large-scale multi-modal benchmark for face
anti-spoofing. arXiv:1908.10654, 2019.
[41] Shifeng Zhang, Xiaobo Wang, Ajian Liu, Chenxu Zhao,
Jun Wan, Sergio Escalera, Hailin Shi, Zezheng Wang, and
Stan Z. Li. A dataset and benchmark for large-scale multimodal face anti-spoofing. In CVPR, 2019.
[42] Zhiwei Zhang, Junjie Yan, Sifei Liu, Zhen Lei, Dong Yi, and
Stan Z Li. A face antispoofing database with diverse attacks.
In ICB, 2012.

10

