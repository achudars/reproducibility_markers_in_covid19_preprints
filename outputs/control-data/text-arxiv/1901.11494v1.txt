Inducing Sparse Coding and And-Or Grammar from Generator Network
Xianglei Xing1,2� , Song-Chun Zhu2 , Ying Nian Wu2
1

2

College of Automation, Harbin Engineering University, Harbin 150001, China
Department of statistics, University of California, Los Angeles, California 90095
xingxl@hrbeu.edu.cn, sczhu@stat.ucla.edu, ywu@stat.ucla.edu

Abstract
We introduce an explainable generative model by applying
sparse operation on the feature maps of the generator network. Meaningful hierarchical representations are obtained
using the proposed generative model with sparse activations.
The convolutional kernels from the bottom layer to the top
layer of the generator network can learn primitives such as
edges and colors, object parts, and whole objects layer by layer. From the perspective of the generator network, we propose
a method for inducing both sparse coding and the AND-OR
grammar for images. Experiments show that our method is
capable of learning meaningful and explainable hierarchical
representations.

1

Introduction

The sparsity principle has played a fundamental role in highdimensional statistics, machine learning, and signal processing. In particular, sparse coding (Olshausen and Field 1996)
is an important principle for understanding the visual cortex. By imposing sparsity constraints on the coefﬁcients of
a linear generative model, (Olshausen and Field 1997) learn
Gabor-like wavelets that resemble the neurons in the primary visual cortex (V1) from natural image patches.
However, developing a top-down sparse coding model
that can generate (in addition to merely reconstruct) realistic natural image patterns has proven to be a difﬁcult task.
The model of (Olshausen and Field 1997) assumes that the
coefﬁcients of the linear model follow a sparse distribution.
However, it is difﬁcult to develop a realistic model for the sophisticate sparse patterns, which involves a selection process
that selects which coefﬁcients are active, in addition to generating the values of the active coefﬁcients. The commonly
used independent spike and slab model (Ishwaran and Rao
2005) can hardly generate realistic images. Moreover, even
if we do have such a selection model as a prior model for
the coefﬁcients, ﬁtting the model to each training example
involves a non-trivial inference process to identify the active
coefﬁcients and estimate their values. Because of the modeling and computing difﬁculties, we still do not have a realistic
c 2019, Association for the Advancement of Artiﬁcial
Copyright �
Intelligence (www.aaai.org) Workshop on Network Interpretability
for Deep Learning.All rights reserved.
�
This work has been done when author working as visiting
scholar in UCLA.

top-down generative model that has the linear sparse coding
model at the lowest layer or incorporates the sparse coding
principle across its layers.
Recently, the generator network (Goodfellow et al. 2014;
Higgins et al. 2016; Xing et al. 2018) has proven to be a surprisingly powerful top-down model for natural images (as
well as other types of signals). This model can also be considered a generalization of the factor analysis model. While
the sparse coding model generalizes the prior distribution
of the coefﬁcients from a Gaussian noise vector to a sparse
vector, the generator network retains the prior distribution of
the coefﬁcients as a Gaussian noise vector, but it generalizes
the linear mapping from the coefﬁcient vector to the signal
in factor analysis to a non-linear mapping parametrized by
a top-down convolutional neural network (or a so-called deconvolutional network).
However, unlike the sparse coding model, the generator
network is a dense model without sparsity. In this paper,
we try to fuse the sparse coding model and the generator
network, or more speciﬁcally, we try to induce the sparse
coding model from the generator network. To accomplish
this, we use a simple mechanism where at each layer of the
top-down generator network, we only select the top k coefﬁcients to be active and force all the other coefﬁcients to be
zeros. This leads to a top-down sparse coding generator network. The fusion leads to the following advantages. (1) The
model can still be learned and inferred efﬁciently as in the
original generator network. (2) The model can still generate
realistic images. (3) The model incorporates the sparse coding principle and learns meaningful basis functions. Thus the
proposed model naturally fuses the generator network and sparse coding model while maintaining their advantages.
Furthermore, the proposed model connects the sparsity
principle to the compositionality principle. The model is explainable since visual patterns are hierarchical compositions
of basis functions. In the language of AND-OR grammars
(Zhu and Mumford 2007), the dictionary of the basis functions can be considered a large OR node, where each basis is
a child node of this OR node and each generated image is itself an AND-OR structure, where AND means composition
of the constituent basis functions and OR means different
choices and conﬁgurations of the basis functions as well as
variations of their coefﬁcients.

2

Generative Model with Sparse Activations

A typical generator has a top-down structure as illustrated in Figure 1. The input latent vector Z ∈ Rd is fed
into a fully connected layer and through a Relu oper1
1
1
ation to produce featuremap1, or f m1 ∈ Rnw ×nh ×nc ,
with non-linear activations. Then, featuremap1 is sent through the ﬁrst deconvolutional layer with kernels ker1 ∈
1
1
1
2
Rnc ×h ×h ×nc and then through a Relu to produce fea2
2
2
turemap2, f m2 ∈ Rnw ×nh ×nc . The produced featuremap2
continues through deconvolutional layers with correspondi
i
i
i+1
ing kernels keroi ∈ oRnc ×h ×h ×nc after which the output
nw ×nh ×3
image Y ∈ R
is generated with the non-linear sigmoid or tanh function. To introduce an explainable generative model, we apply the sparse operation on each layer’s
feature map such that the activations within the feature map
are very sparse.

2.2

Inducing sparse coding

For the ith layer, the value of the K i sparse activations can be interpreted as the sparse coefﬁcients, sij , j ∈
{1, . . . , K i }. Suppose the corresponding K basis functions are denoted as Hji , j ∈ {1, . . . , K i }, Hji ∈
i+1

i+1

i+1

Rnw ×nh ×nc , the sparse activations in the (i + 1)
er can be obtained as
f mi+1
s

th

lay-

ReLU (T opK (f mi+1 , K i+1 ))

=

i

=

ReLU (T opK (

K
�

sij × Hji , K i+1 )) (1)

j=1

It is worth to note that both ReLU and the T opK operations are non-linear and can be seen as switches or masks
that partitioning the space. However, after we obtain the
parsing graph of a particular input latent vector Z, we can
remember these masks of the chosen elements of both the
T opK and ReLU operations. Then, these non-linear operations will be changed to linear operations. Therefore, we can
th
rewrite the sparse activations in the (i + 1) layer as
f mi+1
s

i+1
(f mi+1 � maskTi+1 ) � maskR

=

i

=

(

K
�

i+1
sij × Hji � maskTi+1 ) � maskR
(2)

j=1

Figure 1: An illustration of the generator with sparse activations.
The sparse operation on the ith layer’s feature map is deﬁned as the T opK (·, K) operation which selects and keeps
the K maximum elements within the tensor f mi and sets all
other elements to zero, f mis = T opK (f mi , K).

2.1

Inducing And-Or-Graphs

After the sparse operation, most of the elements of the feature map become zeros, and very sparse activations survive. These sparse activations are located at different spatial directions and channels. (e.g.) For a feature map of size
nw × nh × nc along the spatial directions, at most nw × nh
activations survive. At a particular (xi , yi ) of these nw × nh
locations, suppose ci activations survive along the channel
�nw ×nh
direction, then we have, K = i=1
ci , where K is the
number of overall sparse activations within the feature map.
At each location (xi , yi ), we can deﬁne an ‘AND’ node, since a full image must consist of contents (activations) at all
the spatial locations. For a particular spatial location (xi , yi ),
we name the surviving activations along the channel direction as the ‘OR’ nodes, since at each location, we have different choices to utilize the kernels from the ci channels to
construct the next layer’s feature map or the ﬁnal output image. Thus, these sparse activations form the And-Or Parsing
Graph. The sparse activations are determined by the input
latent vector and the learned parameters (weights and bias)
of the generator. A different latent vector will generate a different parsing graph.

where � denotes the dot product between two matrices.
maskT and maskR are the masks of the T opK and the
ReLU results on the parsing graph, and both of them have
i+1
i
the same size of ni+1
× ni+1
w × nh
c . The basis Hj can be
computed by setting the other sparse coefﬁcients to be zeros,
sik = 0, k �= j. Then we have
sij × Hji = f misj ⊗ keri +

1
biasi
Ki

(3)

where ⊗ denotes the deconvolutional operation, keri and
biasi denote the kernels and bias at the ith layer, and f misj
denotes the feature map at the ith layer with only one activation whose value is sij . In other words, the feature map of
each sparse activation at the ith layer, can be written as,
f misj

=

i
(f mi � maskTi,j ) � maskR

=

(

i−1
K
�

i
si−1
× Hji−1 � maskTi,j ) � maskR
(4)
j

j=1

where maskTi,j denotes the mask that only chooses the j th
largest elements from the top K i activations at layer i.
Moreover, the ﬁnal generated image Y can also be presented in the uniﬁed sparse coding framework as
i

Y = tanh(

K
�

sij Bji )

(5)

j=1

o

o

where Bji ∈ Rnw ×nh ×3 is the synthesis basis corresponding to the j th sparse activation in the ith layer, and can be

computed as
Bji

=

L

fm �

maskTL

1
biasL−1 (6)
K L−1
where L is number of the bottom layer, and f mL−1
can be
s
computed by Eq. (2) and (3) with recursion.
=

2.3

f mL−1
⊗ kerL−1 +
s

where C is the constant term, and is independent of Z and Y .
It can be shown that, given sufﬁcient transition steps, the Z
obtained from this procedure follows the joint posterior distribution. For each training example Yi , we run the Langevin
dynamics in Eq.(10) to get the corresponding posterior sample Z. The sample is then used for gradient computation in
Eq.(9). More precisely, the parameter θ is learned through
Monte Carlo approximation:

Learning and Inference

N
∂
1 � ∂
L(θ) ≈
log p(Yi , Zi ; θ, tk )
∂θ
N i=1 ∂θ

The generator model with sparse activations can be expressed as
Z ∼ N(0, Id ),
Y = g(Z; θ, tk ) + �, � ∼ N(0, σ 2 ID ).

(7)

g(Z; θ, tk ) is a top-down sparse ConvNet deﬁned by both
the parameters θ which includes the weight and bias parameters and tk which includes the number of each layer’s maximum surviving activations. The latent vector Z is
mapped to the signal Y by the sparse ConvNet g. To learn
this generator model, we introduce a learning and inference
algorithm, without designing and learning extra inference
networks. Speciﬁcally, the proposed model can be trained
by maximizing the log-likelihood on the training dataset
{Yi , i = 1, . . . , N },
L(θ)

=

=

N
1 �
log P (Yi ; θ, tk )
N i=1
�
N
1 �
log P (Yi , Z; θ, tk )dZ.
N i=1

=
=

(8)

(9)

In general, the expectation in (9) is analytically intractable,
and needs to be approximated by an MCMC method that
samples from the posterior P (Z|Y ; θ, tk ) ∝ p(Y, Z; θ, tk ),
such as the Langevin dynamic inference, which iterates
δ2 ∂
log P (Zτ , Y ; θ, tk ) + δEτ ,
(10)
2 ∂Z
where τ indexes the time step, δ is the step size, and Eτ
denotes the noise term,Eτ ∼ N(0, Id ). The log of the joint
density in Eq.(10) can be evaluated by
Zτ +1 = Zτ +

log p(Y, Z; θ, tk ) = log [p(Z)p(Y |Z; θ, tk )]
1
1
= − 2 �Y − g(Z; θ, tk )�2 − �Z�2 + C
2σ
2

2.4

N
1 � 1
∂
(Yi − g(Zi ; θ, tk )) g(Zi ; θ, tk ).
2
N i=1 σ
∂θ

(11)

(12)

Extension with Energy Based Model

It is well known that using squared Euclidean distance alone
to train generator networks often yields blurry reconstruction results, since the precise location information of details
may not be preserved, and the L2 loss in the image space
leads to averaging all likely locations. In order to improve
the generative performance, we employ a descriptor to describe the distribution and the context information to help
the generator produce better results. A descriptor model is
also a kind of energy-based model, and is in the form of exponential tilting of a reference distribution
P (Y ; φ) =

The uncertainty in inferring Z is taken into account by the
above observed-data log-likelihood. We can compute θ by
minimizing the Kullback-Leibler divergence KL(Pdata |Pθ )
from the data distribution Pdata to the model distribution Pθ .
The gradient of L(θ) is obtained according to the following equation which is related to the EM algorithm
∂
log P (Y ; θ, tk )
∂θ
�
1
∂
P (Y, Z; θ, tk )dZ
P (Y ; θ, tk ) ∂θ
�
�
∂
log P (Y, Z; θ, tk ) .
EP (Z|Y ;θ,tk )
∂θ

=

1
exp [f (Y ; φ)] q(Y ).
Z(φ)

(13)

q(Y ) is the reference distribution such as Gaussian white
noise. f (Y ; φ) is a bottom-up ConvNet which maps the image Y to the feature statistics.� f (Y ; φ) is also known as
the energy function. Z(φ) = exp [f (Y ; φ)] q(Y )dY =
Eq {exp[f (Y ; φ)]} is the normalizing constant. The descriptor can capture the distribution and context of the training
images by maximizing the log-likelihood function. To improve the generator’s performance, we feed Ỹi which is sampled from the descriptor into Eq. (12) to replace Yi . Since Ỹi
contains meaningful distribution and context information of
the training data learned from the energy-based model, it is
easier for the generator to learn from Ỹi than directly learn
from Yi . Learning from Yi directly is difﬁcult and the generated image may look blurry, since it requires per pixel reconstruction. The model can also be learned by VAE and GAN,
except that VAE requires an extra inference model and does
not have strong synthesis power, while GAN cannot do inference and has mode collapsing issue.

3

Experimental Results

To demonstrate the meaningful hierarchical representation
power of the proposed model, we show the hierarchical representation results of the generator. The generator network
contains one fully-connected layer and two convolutional
layers. We train the sparse generator on 5k images from
CelebA benchmark dataset.
Figure 2 shows the learned kernels of the sparse generator with two convolutional layers. When the visual ﬁelds

Figure 3: Analyzing generated face of sparse generator with
two convolutional layers using AND-OR Parsing Graph.
(a) Convolutional kernels in the second (bottom) layer.

(b) Hierarchical representation of the kernels in the ﬁrst layer.

Figure 2: Hierarchical representation of the kernels learned
by the sparse generator with two convolutional layers in the
(a) bottom layer (b) top layer.

Figure 4: Learned bottom kernels from generator with two
convolutional layers on the cars and brickwall images.

of the bottom layer is relatively large, the bottom convolutional kernels can learn different facial parts, such as eyes,
noses, mouths, chins and nasolabial folds, which are shown
in Figure 2 (a). As we can observe from Figure 2 (b), the top
layer’s kernels have learned some trends to combine the facial parts in the bottom layer’s kernels into a whole face. It is
worth to note that without the proposed sparse-K operation
on the activations (or feature maps), we cannot obtain these
meaningful results in both layer’s kernel. With the sparseK operation, the energies are forced to be collected into very
few activation, which makes the corresponding kernels’ content meaningful.
To understand the sparse generator’s working principle,
we further explain the images’ generating process as a hierarchical And-Or Parsing Graph as shown in Figure 3. Specifically, as we can observe from the left part of Figure 3, at
the bottom convolutional layer, the basis functions are the
convolutional kernels themselves. A generated image is partitioned by 4 × 4 = 16 ‘AND’ nodes, and each ‘AND’ node contains several ‘OR’ nodes along the channel direction.
The ‘AND’ node at each location (e.g. eyes, nose, or mouth)
of the face contains the ‘OR’ nodes consisting of the corresponding colors and shapes from the same category. For
different faces, the conﬁguration of the ‘OR’ nodes are different, although they share the same dictionaries of bases.
Due to these different conﬁgurations, we can generate different kinds of facial parts and different faces.
The generated face can also be described by the coefﬁcients of the sparse activations from any layer multiplied
by the corresponding basis functions according to Eq. (5).
Speciﬁcally, as we can observe from the right part of Figure
3, at the top layer, a face image is partitioned into 2 × 2 = 4
‘AND’ nodes, and each ‘AND’ node contains several ‘OR’
nodes. These ‘OR’ nodes (the basis functions at the top layer) reveal high-level semantics, and are the combinations of
the basis functions at the bottom later.
The proposed model can learn both other objects and tex-

ture patterns. Figure 4 shows some basis functions from the
bottom layer of the generator with two convolutional layers.

4

Conclusion

In this study, we introduce a sparse generative model which
naturally combines the generator network and sparse coding
model while maintaining their advantages. Our experiments
show that the model can learn meaningful dictionaries at different layers.

References
Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;
Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y.
2014. Generative adversarial nets. In Advances in neural
information processing systems, 2672–2680.
Higgins, I.; Matthey, L.; Pal, A.; Burgess, C.; Glorot, X.;
Botvinick, M.; Mohamed, S.; and Lerchner, A. 2016. betavae: Learning basic visual concepts with a constrained variational framework.
Ishwaran, H., and Rao, J. S. 2005. Spike and slab variable
selection: frequentist and bayesian strategies. The Annals of
Statistics 33(2):730–773.
Olshausen, B. A., and Field, D. J. 1996. Emergence of
simple-cell receptive ﬁeld properties by learning a sparse
code for natural images. Nature 381(6583):607.
Olshausen, B. A., and Field, D. J. 1997. Sparse coding
with an overcomplete basis set: A strategy employed by v1?
Vision research 37(23):3311–3325.
Xing, X.; Gao, R.; Han, T.; Zhu, S.-C.; and Wu, Y. N. 2018.
Deformable generator network: Unsupervised disentanglement of appearance and geometry. arXiv preprint arXiv:1806.06298.
Zhu, S.-C., and Mumford, D. 2007. A stochastic grammar
of images. Foundations and Trends in Computer Graphics
and Vision 2(4):259–362.

