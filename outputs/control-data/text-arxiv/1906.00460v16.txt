On The Radon–Nikodym Spectral Approach With Optimal
Clustering
Vladislav Gennadievich Malyshkin∗
Ioffe Institute, Politekhnicheskaya 26, St Petersburg, 194021, Russia
(Dated: May, 31, 2019)

arXiv:1906.00460v16 [cs.LG] 13 Jun 2021

$Id: RNSpectralMachineLearning.tex,v 1.619 2021/06/13 22:51:24 mal Exp $
Problems of interpolation, classification, and clustering are considered. In the tenets
of Radon–Nikodym approach f (x)ψ 2 / ψ 2 , where the ψ(x) is a linear function
on input attributes, all the answers are obtained from a generalized eigenproblem
f |ψ [i] = λ[i] ψ [i] . The solution to the interpolation problem is a regular Radon–
Nikodym derivative. The solution to the classification problem requires prior and
posterior probabilities that are obtained using the Lebesgue quadrature[1] technique.
Whereas in a Bayesian approach new observations change only outcome probabilities, in
the Radon–Nikodym approach not only outcome probabilities but also the probability
space ψ [i] change with new observations. This is a remarkable feature of the approach:
both the probabilities and the probability space are constructed from the data. The
Lebesgue quadrature technique can be also applied to the optimal clustering problem.
The problem is solved by constructing a Gaussian quadrature on the Lebesgue
measure. A distinguishing feature of the Radon–Nikodym approach is the knowledge
of the invariant group: all the answers are invariant relatively any non–degenerated
linear transform of input vector x components. A software product implementing the
algorithms of interpolation, classification, and optimal clustering is available from the
authors.

∗

malyshki@ton.ioffe.ru

2
I.

INTRODUCTION

In our previous work[1] the concept of Lebesgue Integral Quadrature was introduced
and subsequently applied to the problem of joint probability estimation[2]. In this paper a
different application of the Lebesgue Integral Quadrature is developed. Consider a problem
where attributes vector x of n components is mapped to a single outcome f (class label in
ML) for l = [1 . . . M ] observations:
weight ω (l)

(x0 , x1 , . . . , xk , . . . , xn−1 )(l) → f (l)

(1)

The data of this format is commonly available in practice. There is a number of problems of
interest, e.g.:
[m]

• For a continuous attribute f build optimal λf ; m = 0 . . . D − 1 discretization levels, a
discretization of continuous features problem.
• For a discrete f : construct a f –predictor for a given x input vector, statistical classification problem, that arise in ML, statistics, etc. For a continuous f : predict it’s value
for a given x.
• For a given x estimate the support of the measure in (1) problem, in the simplistic
formulation it is: find the number of observations that are “close enough” to a given
x. Find the Coverage(x). The Christoffel function is often used as a proxy for the
coverage[3–5], however a genuine Coverage(x) is a very important characteristics in
ML.
• Cluster the (1) dataset according to f separability (allocate D ≤ n linear combinations
P
[m]
[m]
ψG (x) = n−1
k=0 αk xk , m = 0 . . . D − 1, that optimally separate the f in terms of
hf ψ 2 i / hψ 2 i). For a given x construct the probability distribution of f to fall into the
found D clusters.
Currently used techniques typically construct a norm, loss function, penalty function, metric,
distance function, etc. on f , then perform an optimization minimizing the f –error according
to the norm chosen, a typical example is the backpropagation. The simplest approach of this
type is linear regression, L2 norm minimization:
[f (x) − fLS (x)]2 → min

(2)

3

fLS (x) =

n−1
X

(3)

β k xk

k=0

As we have shown in [6, 7] the major drawback of an approach of this type is a difficulty to
select a “good” norm, this is especially the case for non–Gaussian data with spikes[8, 9].

II.

RADON–NIKODYM SPECTRAL APPROACH

The Lebesgue integral quadrature[1] is an extension of Radon–Nikodym concept of constructing a classifier of hf ψ 2 i / hψ 2 i form, where the ψ(x) is a linear function on input
attributes, to build the support weight as a quadratic function on xk . It allows to approach
many ML problems in a completely new, norm–free way, this greatly increases practical
applicability. The main idea is to convert (1), a sample of M observations, to a set of n
eigenvalue/eigenvector pairs, subject to generalized eigenvalue problem:
E
f ψ [i] = λ[i] ψ [i]
n−1
X

[i]

hxj | f | xk i αk = λ[i]

n−1
X

k=0

(4)
[i]

hxj | xk i αk

(5)

k=0

ψ [i] (x) =

n−1
X

[i]

(6)

αk xk

k=0

Here and below the h·i is M observations sample averaging, for observations with equal
weights ω (l) = 1. This is a plain sum:
h1i =

M
X

(7a)

ω (l)

l=1

Fjk = hxj | f | xk i =
Gjk = hxj | xk i =

M
X

(l)

(l)

xj f (l) xk ω (l)

l=1
M
X (l) (l)
xj xk ω (l)
l=1

(7b)
(7c)

Here and below we assume that Gram matrix Gjk is a non–singular. In case of a degenerated
Gjk , e.g. in case of data redundancy in (1), for example a situation when two input attributes
are identical xk = xk+1 for all l, a regularization procedure is required. A regularization
algorithm is presented in the Appendix A. Below we consider the matrix Gjk to be positively
defined (a regularization is already applied).

4
Familiar L2 least squares minimization (2) regression answer to (3) is a linear system
solution:
n−1
X

fLS (x) =

(8)

xj G−1
jk hf xk i

j,k=0

The Radon–Nikodym answer[7] is:
n−1
P

fRN (x) =

−1
xj G−1
jk Fkl Glm xm

j,k,l,m=0

(9)

n−1
P

xj G−1
jk xk

j,k=0

1/K(x) =

n−1
X

(10)

xj G−1
jk xk

j,k=0

Here G−1
kj is Gram matrix inverse, the K(x) is a Christoffel–like function. In case xk = Qk (x),
where x is a continuous attribute and Qk (x) is a polynomial of the degree k, the Gjk and
Fjk matrices from (7) are the hQj | Qk i and hQj | f | Qk i matrices of Refs. [1, 7], and the
P
−1
Christoffel function is 1/K(x) = n−1
j,k=0 Qj (x)Gjk Qk (x). The (1) is a more general form, the
xk now can be of arbitrary origin, an important generalization of previously considered a
polynomial function of a continuous attribute.
The (5) solution is n pairs (λ[i] , ψ [i] (x)). For positively defined Gjk = hxj | xk i the solution
exists and is unique. For normalized ψ [i] we have:
δij = ψ

[i]

ψ

[j]

=

n−1
X

[j]

(11a)

[i]
αm
hxm | xk i αk

m,k=0
[i]

λ δij = ψ

[i]

f ψ

[j]

n−1
X

=

[j]

[i]
αm
hxm | f | xk i αk

(11b)

m,k=0

Familiar L2 least squares minimization (2) regression answer and Radon–Nikodym answers
can be written in ψ [i] basis. The (12), (13), and (14) are the (8), (9), and (10) written in the
ψ [i] basis:
fLS (x) =

n−1
X
i=0
n−1
P

fRN (x) =

λ[i] ψ [i] ψ [i] (x)

(12)


2
λ[i] ψ [i] (x)

i=0
n−1
P
i=0

(13)
2

[ψ [i] (x)]

5
n−1
X
 [i] 2
1/K(x) =
ψ (x)

(14)

i=0

The main result of [1] is a constriction of the Lebesgue integral quadrature:
(15a)

f [i] = λ[i]
2

(15b)

w[i]

(15c)

w[i] = ψ [i]
h1i =
n=

n−1
X

i=0
n−1
XD



ψ [i]

2 E

(15d)

i=0

The Gaussian quadrature groups sums by function argument; it can be viewed as a n–point
discrete measure, producing the Riemann integral. The Lebesgue quadrature groups sums
by function value; it can be viewed as a n–point discrete distribution with f [i] support
points (15a) and the weights w[i] (15b), producing the Lebesgue integral. Obtained discrete
distribution has the number of support points equals to the rank of hxj | xk i matrix, for
non-degenerated basis it is equal to the dimension n of vector x. The Lebesgue quadrature is
unique, hence the principal component spectral decomposition is also unique when written
in the Lebesgue quadrature basis. Substituting (12) to (2) obtain PCA variation expansion:
2

[f (x) − fLS (x)]

= f

2

−

n−1
X
i=0

f


[i] 2

[i]

w =

D

f −f

2 E

−

n−1
X

f [i] − f

2

w[i]

(16)

i=0

Here f = hf i/h1i. The difference between (16) and regular principal components is that the
basis ψ [i] (5) of the Lebesgue quadrature is unique. This removes the major limitation of the
principal components method: it’s dependence on the scale of x attributes. The (16) does not
require scaling and normalizing of input x, e.g. if xk attribute is a temperature in Fahrenheit,
when it is converted to Celsius or Kelvin — the (16) expansion will be identical. Due to (5)
invariance the variation expansion (16) will be the same for arbitrary non–degenerated linear
P
transform of x components: x0j = n−1
k=0 Tjk xk .
In the basis of the Lebesgue quadrature Radon–Nikodym derivative expression (13) is the
eigenvalues weighted with (22) weights. Such a solution is natural for interpolation type of
problem, however for a classification problem different weights should be used.

6
A.

Prior and Posterior Probabilities

Assume that in (13) for some x only a single eigenfunction ψ [i] (x) is non–zero, then (13)
gives the corresponding f [i] regardless the weigh w[i] . This is the proper approach to an
interpolation problem, where the f is known to be a deterministic function on x. When
considering f as a random variable, a more reasonable approach is to classify the outcomes
according to overall weight. Assume no information on x is available, what is the best answer
for estimation of outcomes probabilities of f ? The answer is given by the prior probabilities
(17a) that correspond to unconditional distribution of f according to (15b) weights.
Prior weight for f [i] : w[i]

(17a)

2
ψ [i] (x)
Posterior weight for f : w Proj (x) = w n−1
P [j]
2
[ψ (x)]


[i]

[i]

[i]

[i]

(17b)

j=0


2
The posterior distribution uses the same ψ [i] (x) probability as (13) adjusted to f [i] outcome
prior weight w[i] . The corresponding average
n−1
P

fRN W (x) =

n−1
P

λ[i] w[i] Proj[i] (x)

i=0
n−1
P

=
w[i] Proj[i] (x)

i=0


2
λ[i] w[i] ψ [i] (x)

i=0
n−1
P

(18)
w[i]

2
[ψ [i] (x)]

i=0

is similar to (13), but uses the posterior weights (17b). There are two distinctive cases of f
on x inference:
• If f is a deterministic function on x, such as in an interpolation problem, then the
probabilities of f outcomes are not important, the only important characteristic is:
how large is ψ [i] eigenvector at given x; the weight is the i–th eigenvector projection
(22). The best interpolation answer is then (13) fRN (x): the eigenvalues λ[i] weighted
with the projections Proj[i] (x) as the weights.
• If f (or some xk ) is a random variable, then inference answer depends on the distribution
of f . The classification answer should include not only what the outcome λ[i] corresponds
to a given x, but also how often the outcome λ[i] occurs; this is determined by the prior
weights w[i] . The best answer is then (18) fRN W (x): the eigenvalues λ[i] weighted with

7
the posterior weights w[i] Proj[i] (x). An important characteristic is
Coverage(x) =

n−1
X

w[i] Proj[i] (x)

(19)

i=0

that is equals to Lebesgue quadrature weights w[i] weighted with projections. For (15)
the probability space is n vectors ψ [i] with the probabilities w[i] . The coverage is a
characteristic of how often given x occurs in the observations (here we assume that
total sample space is projected to ψ [i] states). Entropy Sf of a random variable f
can be estimated from prior probabilities:
Sf = −

n−1
X
w[i]
i=0

h1i


ln

w[i]
h1i



(20)

It can be used as a measure of statistical dispersion of f . Similarly, conditional entropy
Sf |x can be obtained from prior and posterior probabilities (17):
!
n−1
X
w[i] Proj[i] (x)
w[i] Proj[i] (x)
ln
Sf |x = −
h1i
Coverage(x)
i=0

(21)

The fRN W can be interpreted as a Bayes style of answer. An observation x changes outcome
probabilities from (17a) to (17b). Despite all the similarity there is a very important difference
between Bayesian inference and Radon–Nikodym approach. In the Bayesian inference[10]
the probability space is fixed, new observations can adjust only the probabilities of pre–set
states. In the Radon–Nikodym approach, the probability space is the Lebesgue quadrature
(15) states ψ [i] , the solution to (4) eigenproblem. This problem is determined by two
matrices hxj | f | xk i and hxj | xk i, that depend on the observation sample themselves. The key
difference is that new observations coming to (1) change not only outcome probabilities, but
also the probability space ψ [i] . This is a remarkable feature of the approach: both the
probabilities and the probability space are constructed from the data. For probability space
of the Lebesgue quadrature (15) this flexibility allows us to solve the problem of optimal
clustering.

III.

OPTIMAL CLUSTERING

Considered in previous section two inference answers (13) and (18) use vector x of n
components as input attributes xk . In a typical ML setup the number of attributes can grow

8
quite substantially, and for a large enough n the problem of data overfitting is starting to
rise. This is especially the case for norm–minimization approaches such as (12), and is much
less so for Radon–Nikodym type of answer (13), where the answer is a linear superposition of
the observed f with positive weight ψ 2 (x) (the least squares answer is also a superposition
of the observed f , but the weight is not always positive). However, for large enough n the
overfitting problem also arises in fRN . The Lebesgue quadrature (15) builds n cluster centers,
for large enough n the (13) finds the closest cluster in terms of x to ψ [i] distance, this is the
projection Proj[i] (y) = ψy ψ [i]

2

to localized at x = y state ψy (x):

 [i] 2
ψ (x)
Proj[i] (x) = n−1
= ψx ψ [i]
P [j]
2
[ψ (x)]

2

(22)

j=0

1=

n−1
X

[i]

Proj (x) =

i=0

n−1
X

ψx ψ [i]

2

(23)

i=0

n−1
P

ψ [i] (y)ψ [i] (x)

n−1
P

yj G−1
jk xk

j,k=0

s
ψy (x) = i=0

n−1
P

=s
2

[ψ [i] (y)]

i=0

n−1
P

(24)
yj G−1
jk yk

j,k=0

and then uses corresponding f [i] as the result. Such a special cluster always exists for large
enough n, with n increase the Lebesgue quadrature (15) separates the x space on smaller
and smaller clusters in terms of (22) distance as the square of wavefunction projection.
In practical applications a hierarchy of dimensions is required. The number of sample
observations M is typically in a 1, 000 − 100, 000 range. The dimension n of the attributes
vector x is at least ten times lower than the M , n is typically 5 − 100. The number of clusters
D, required to identify the data is several times lower than the n, D is typically 2 − 10; the
D ≤ n ≤ M hierarchy must be always held.
The Lebesgue quadrature (15) gives us n cluster centers, the number of input attributes.
We need to construct D ≤ n clusters out of them, that provide “the best” classification for a
given D. Even the attributes selection problem (select D “best” attributes out of n available
xk ) is of combinatorial complexity[11], and can be solved only heuristically with a various
degree of success. The problem to construct D attributes out of n is even more complex.
The problem is typically reduced to some optimization problem, but the difficulty to chose a
norm and computational complexity makes it impractical.

9
In this paper an original approach is developed. The reason for our success is the very
specific form of the Lebesgue quadrature weights (15b) w[i] = ψ [i]

2

that allows us to

construct a D–point Gaussian quadrature in f – space, it provides the best D–dimensional
separation of f , and then to convert obtained solution to x space!
[m]

[m]

A Gaussian quadrature constructs a set of nodes fG and weights wG such that
hg(f )i ≈

D−1
X

[m]

[m]

g(fG )wG

(25)

m=0

is exact for g being a polynomial of a degree 2D − 1 or less. The Gaussian quadrature can
be considered as the optimal approximation of the distribution of f by a D–point discrete
measure. With the measure h·i in the form of M terms sample sum (7) no inference of f on
x can be obtained, we can only estimate the distribution of f (prior probabilities).
Now consider D–point Gaussian quadrature built on n point discrete measure of the
Lebesgue quadrature (15), D ≤ n. Introduce the measure h·iL
n−1
X

hg(f )iL =

g(f [i] )w[i]

(26)

i=0

(27)

h1iL = h1i

and build Gaussian quadrature (25) on the Lebesgue measure h·iL . Select some polynomials
Qk (f ), providing sufficient numerical stability, the result is invariant with respect to basis
choice, Qm (f ) = f m and Qm = Tm (f ) give identical results, but numerical stability can be
drastically different[12, 13]. Then construct two matrices Fst and Gst (in (28a) and (28b) the
f [i] and w[i] are (15a) and (15b)), solve generalized eigenvalue problem (28c), the D nodes
[m]

[m]

[m]

are fG = λG eigenvalues, the weights wG , m = 0 . . . D − 1, are:
Fst = hQs | f | Qt iL =
Gst = hQs | Qt iL =

n−1
X

i=0
n−1
X

Qs (f [i] )Qt (f [i] )f [i] w[i]

Qs (f [i] )Qt (f [i] )w[i]

(28a)
(28b)

i=0
[m]

F ψG
D−1
X

E

[m]

L
[m]

Fst αt

= λG

[m]

= λG

t=0

[m]

G ψG
D−1
X

E
L
[m]

(28c)

Gst αt

(28d)

αt Qt (f )

(28e)

t=0
[m]

ψG (f ) =

D−1
X
t=0

[m]

10
[m]

[m]

(28f)

f G = λG

1

[m]

wG = h

(28g)

i2
[m] [m]
ψG (λG )

h1iL = h1i =

D−1
X

[m]
wG

=

m=0

n−1
X

(28h)

w[i]

i=0

[m]

The eigenfunctions ψG (f ) are polynomials of D − 1 degree that are equal (within a constant)
to Lagrange interpolating polynomials L[m] (f )


[m]
1 if f = f [m]
ψG (f )
G
[m]
L (f ) = [m] [m] =
ψG (fG ) 
0 if f = f [s] ; s 6= m
G

(29)

Obtained D clusters in f –space are optimal in a sense they, as the Gaussian quadrature,
optimally approximate the distribution of f among all D–points discrete distributions. The
greatest advantage of this approach is that attributes selection problem of combinatorial
complexity is now reduced to the generalized eigenvalue problem (28d) of dimension D!
Obtained solution is more generic than typically used disjunctive conjunction or conjunctive
disjunction forms[11] because it is invariant with respect to arbitrary non–degenerated linear
transform of the input attribute components xk .
[m]

The eigenfunctions ψG (f ) (28d) are obtained in f –space. Because the measure h·iL (26)
2

[m]

was chosen with the Lebesgue quadratures weights w[i] = ψ [i] , the ψG (f ) (28e) can be
converted to x basis, m, s = 0 . . . D − 1:
[m]

ψG (x) =

n−1
X

[m]

(30)

ψG (f [i] ) ψ [i] ψ [i] (x)

i=0

δms =
[m]

λG δms
[m]

wG
[m]

D

[m]
ψG (x)

E

[s]
ψG (x)

(31)

D

E
[m]
[s]
= ψG (x) f ψG (x)
D
E2 D
E2
[m]
[m]
= ψG (x) = ψG (f )

(32)
(33)

L

[m]

The ψG (x) is a function on x, it is obtained from ψG (f ) basis conversion (30). This became
possible only because the Lebesgue quadratures weights w[i] = ψ [i]
[m]
ψG (f )

2

have been used to

[m]
construct the
in (28c). The ψG (x) satisfies the same orthogonality conditions (31)
[m]
and (32) for the measure h·i as the ψG (f ) for the measure h·iL . Lebesgue quadrature weight
[m]
[m]
for ψG (x) is the same as Gaussian quadrature weight for ψG (f ), Eq. (33).

11
The (30) is the solution to clustering problem. This solution optimally separates f –
space relatively D linear combinations of xk to construct1 the separation weights ψ 2 (x)
of hf ψ 2 i / hψ 2 i form. In the Appendix A a regularization procedure is described, and the
1 + dim S d linear combinations of xk were constructed to have a non–degenerated Gjk matrix.
No information on f have been used for that regularization. In contrast, the functions
(30) select D ≤ n linear combinations of xk , that optimally partition the f –space. The
partitioning is performed according to the distribution of f , the eigenvalue problem (28c) of
[m]

the dimension D has been solved to obtain the optimal clustering. Obtained ψG (x) (they
are linear combination of xk ) should be used as input attributes in the approach considered
in the Section II above, Eq. (13) is directly applicable, the sum now contains D terms, the
number of clusters2 . Familiar variation expansion (16) is also applicable, total variation
D−1
P  [m] 2 [m]
2
hf i −
λG
wG is the same when clustering to any D in the range 2 ≤ D ≤ n and is
m=0

equal to least square norm [f (x) − fLS (x)]2 calculated in original attributes basis x of the
dimension n, Eq. (2).

A.

Optimal Clustering For Unsupervised Learning

Obtained optimal clustering solution assumes that there is a scalar function f , which
can be put to (5) to obtain ψ [i] , then to construct the h·iL measure and to obtain optimal
clusters (30). For unsupervised learning a function f does not exist and the best what we
can do is to put the Christoffel function as f (x) = K(x):
n−1
X

[i]
hxj | K(x) | xk i αk

=

[i]
λK

n−1
X

k=0

[i]

hxj | xk i αk

(34)

k=0
[i]
ψK (x)

=

kρK k =

n−1
X
k=0
n−1
X

[i]

α k xk
E
D
[i]
[i]
[i]
ψK λK ψK

(35)
(36)

i=0
1

The (30) defines D clusters. If 1) D = n, 2) all Lebesgue quadrature nodes f [i] are distinct and 3) no weigh
[m]

2

[m]

w[i] is equal to zero, then λG = f [m] and ψG (x) = ψ [m] (x).
One can also consider a “hierarchical” clustering similar to “hidden layers” of the neural networks. The
simplest approach is to take n input xk and cluster them to D1 , then cluster obtained result to D2 , then to
D3 , etc., n ≤ D1 ≤ D2 ≤ D3 . . . . Another option is to initially group the xk attributes (e.g. by temporal
or spatial closeness), perform Section III optimal clustering for every group to some (possibly different for
[m]

different groups) D, then use obtained ψG (x) for all groups as input attributes for the “next layer”.

12

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
-1

-0.8

-0.6

-0.4

-0.2

0

0.2

0.4

0.6

0.8

1

√
FIG. 1. The Christoffel function K(x) for the measures dµ = dx (blue), dµ = dx/ 1 − x2 (green),
√
and dµ = dx 1 − x2 (olive) with n = 7 and n = 25 (thin). The 1/K(x) is a polynomial on x of the
degree 2n − 2. Christoffel function is determined by integration measure and the basis used. If one
√

chooses the harmonic basis: 1/ 2, sin(kπx), cos(kπx), x ∈ [−1 : 1], dµ = dx, k = 1, . . . , n − 1 then,
in contradistinction to the blue line of this chart for dµ = dx in a polynomial basis, the Christoffel
function is exactly the constant 1/(n − 0.5). Christoffel function study for non–polynomial bases may
be an important direction of further research. The first step in this direction is numerical experiments:
from polynomial bases (where an extra degree gives one more basis function) to harmonic basis
(where an extra degree gives two more basis functions), following a transition to “product” attributes
(47), where the number of basis functions growths with a degree as (50).

h1i =

n−1
X

[i]

(37)

λK

i=0
n−1
X

S=−

i=0

[i]

λK
ln
h1i

[i]

λK
h1i

!
(38)

The sum of all eigenvalues (37) is equal to total measure, see Theorem 4 of [1]. The (38)
is an entropy of the distribution of x(l) , it is similar to (20), but the weights are now
obtained only from x(l) . In Fig. 1 a demonstration of the Christoffel function in 1D case is

13
√
presented for the measures: dµ = dx and Chebyshev first and second kind dµ = dx/ 1 − x2
√
and dµ = dx 1 − x2 . One can see from the figure that K(x) for Chebyshev measure
√
dµ = dx/ 1 − x2 is close to a constant, this follows from the fact that all Gaussian quadrature
weights are the same for Chebyshev measure. The operator kρK k allows us to construct a
Chebyshev–like measure for a multi–dimensional basis:
kρT K k =

n−1
X

[i]
ψK

E

[i]
λT K

D

[i]

(39)

ψK

i=0
[i]

λT K =

h1i
n

(40)

The operator kρT K k has the same eigenvectors as the kρK k, but different eigenvalues; all the
eigenvalues are now the same (40), this is a generalization from 1D Chebyshev measure. For
a large enough n density matrix operator (39) has similar to Chebyshev measure properties.
Note that the entropy (38) is maximal for (40) distribution (all weights are equal). One may

also consider to put entropy density s(x) = −K(x) ln K(x)/ h1i to Eq. (34) instead of
n−1
P [i]
K(x) from Eq. (10) to obtain a “spectral decomposition of the entropy” as S =
λs . But it
i=0
[i]

would be less convenient than the entropy (38), where we construct a discrete distribution λK

and the entropy is then calculated in a usual way. For a large enough n these two approaches
produce similar results.
The technique of an operator’s eigenvalues adjustment was originally developed in [14]
and applied to hydroacoustic signals processing: first a covariation matrix is obtained and
diagonalized, second the eigenvalues (not the eigenvectors!) are adjusted for an effective
identification of weak hydroacoustic signals. The (39) is a transform of this type.
E
[i]
Before we go further let us take advantage of the basis ψK uniqueness to obtain a
familiar PCA variation expansion (16) but with the Christoffel function operator (36), the
average is defined as matrix Spur:

2 X
2
n−1 
h1i
h1i
[i]
Spur kρK k −
k1k =
λK −
n
n
i=0

(41)

The (41) is invariant with respect to an arbitrary non–degenerated linear transform of x
components, no scaling and normalizing is required, same as for (16). One can select a few
[i]

eigenvectors with a large λK − h1i /n difference to capture “most of variation”. However, our
goal is not to capture “most of variation” but to construct a basis of the dimension D ≤ n

14
that optimally separates the dataset. Note that when the kρT K k operator is used in (41) the
variation is minimal (zero).
We are interested not in variance expansion, but in coverage expansion. If we sort eigenvalues in (37) such as the sum
h1i =

n−1
X

[i]
λK

n−1 D
X

=

i=0

[i]

[i]

ψK ρK ψK

E

(42)

i=0

is a sum of continuously decreasing terms, by selecting a few eigenvectors we can create
a projected state, that covers a large portion of observations. This portion is minimal for
Chebyshev density matrix (39), where it is equal to the ratio of the number of taken/total
eigenvalues. As in the previous section we are going to obtain D ≤ n states that optimally
separate the kρK k by constructing a Gaussian quadrature of the dimension D. However, in
it’s original form there is an issue with the measure (26).
For f (x) = K(x) a different separation criteria is required. Consider the measure “all
eigenvalues are equal”, a typical one used in random matrix theory, it is actually the Chebyshev
density matrix (39).
hg(f )iE =

n−1
X

[i]

(43)

g(λK )

i=0

(44)

h1iE = n
[i]

The measure (43) takes all eigenvectors of (5) with equal weight, the nodes are λK , the weight
is 1 for every node. If we now construct the Gaussian quadrature (28) on the measure h·iE
instead of the h·iL , the quadrature nodes
D
[m]
λG

=

[m]
ψG

D

[m]
ψG

f

[m]
ψG

[m]
ψG

n−1
P

E
E

E
E

=

[i]
λK

i=0
n−1
Ph
i=0

h

i2

[m] [i]
ψG (λK )

m = 0...D − 1

i2

(45)

[m] [i]
ψG (λK )

have a meaning of a weight per original eigenvalue3 . Then m = 0 . . . D − 1 eigenfunctions
[m]

ψG (f ) of (28d) optimally cluster the weight per eigenvalue, a “density” like function required
for unsupervised learning. The measure (43) does not allow to convert obtained optimal
[m]

[m]

clustering solution ψG (f ), a pure state in f –space, to a pure state in x–space ψG (x),
3

If to use the Christoffel function average hg(f )iK =
n−1
P  [i] 2 h [m] [i] i2 .n−1
P [i] h [m] [i] i2
λK
ψG (λK )
λK ψG (λK )
i=0

i=0

Pn−1
i=0

[i]

[i]

λK g(λK ) the meaning of the nodes is unclear

15
[m]

however it can be converted to a density matrix state kΨG k, see Appendix C of [1]. While
h
i2
[m]
[m]
[m]
the ψG (x) does not exist for a mixed state, the p (x), an analogue of ψG (x) that enters
to the answers of Radon–Nikodym type, can always be obtained. For the measure (43) the
conversion is:
p[m] (x) =

n−1 h
X

i2
[i]
[m] [i]
ψK (x)ψG (λK )

m = 0...D − 1

(46)

i=0

for a general case see Appendix C of [1].
In this section a completely new look to unsupervised learning PCA expansion is presented.
Whereas a “regular” PCA expansion is attributes variation expansion, which is scale–dependent
and often does not have a clear domain problem meaning4 , the Christoffel function density
matrix expansion (42) is coverage expansion: every eigenvector covers some observations,
total sum of the eigenvalues is equal to total measure h1i, the answer is invariant relatively
any non–degenerated linear transform of input vector x components. In the simplistic form
[i]

one can select a few eigenvectors with a large λK ) (e.g. use --flag_replace_f_by_chris
toffel_function=true with the Appendix B software). In a more advanced form D ≤ n
optimal clusters can be obtained by constructing a Gaussian quadrature with the measure
(43) and then converting the result back to x–space with (46) projections.

IV.

SELECTION OF THE ANSWER: fRN VS. fRN W

For a given input attributes vector we now have two answers: interpolation fRN (13) and
classification fRN W (18). Both are the answers of Radon–Nikodym hf ψ 2 i / hψ 2 i form, that
can be reduced to weighted eigenvalues with Proj[i] and w[i] Proj[i] weights respectively. A
question arise which one to apply.
For a deterministic function f (x), the Proj[i] weights from (22) construct the state in
ψ [i] basis that is the most close to a given observation x. The fRN is a regular Radon–
Nikodym derivative of the measures f dµ and dµ, see Section II.C of [1]. This is a solution of
interpolatory type, see Appendix C below for a demonstration.
For a probabilistic f the w[i] Proj[i] weights, that include prior probability of f outcomes,
is a preferable form of outcome probabilities estimation, see Appendix B 2 below for a
4

There is a situtation[14] where the variation has a meaning of total energy E =
energy matrix Ejk is determined by antenna design.

Pn−1

j,k=0

xj Ejk xk , the

16
demonstration. The w[i] Proj[i] posterior weights typically produce a good classification even
without optimal clustering algorithm of Section III. For a given scalar f the solution to
supervised learning problem is obtained in the form of (outcome,weight) posterior distribution
(17b).
For unsupervised learning the function f does not exist, thus the eigenvalue problem (4)
cannot be formulated. However, we still want to obtain a unique basis that is constructed
from the data, for example to avoid PCA dependence on attributes scale. For unsupervised
learning the Christoffel function should be used as f (x) = K(x), then PCA expansion of
coverage can be obtained, this is an approach of Section III A to unsupervised learning.

V.

A FIRST ORDER LOGIC ANSWER TO THE CLASSIFICATION PROBLEM.
PRODUCT ATTRIBUTES.

Obtained solutions to interpolation (13) and classification (17b) problems are more general
than a propositional logic type of answer. A regular basis function expansion (3) is a local
function of arguments, thus it can be considered as a “propositional logic” type of answer.
Consider formulas including a quantor operator, e.g. for a binary xk and f in (1) expressions
like these:
if ∃xk = 1 then f = 1
if ∀xk = 0 then f = 1
Similar expressions can be written for continuous xk and f , the difference from the propositional logic is that these expressions include a quantor–like operator that is a function of
several xk attributes. The ψ 2 (x) expansion includes the products of xj xk , thus the Radon–
Nikodym representation can be viewed as a more general form than a propositional logic. The
most straightforward approach to obtain a “true” first order logic answer from a propositional
logic model is to add all possible Qk0 (x0 )Qk1 (x1 ) . . . Qkn−1 (xn−1 ) products to the list of input
attributes. For a large enough D (49) we obtain a model with properties that are very similar
to a first order logic model. The attributes xk are now polynomials of n variables with
multi–index k of degree D; they are constructed from initial attributes xk with regular index
k. Multi–index degree (49) is invariant relatively any linear transform of the attributes:
P
x0j = n−1
k=0 Tjk xk . Because in the Radon–Nikodym approach all the answers are invariant

17
relatively any non–degenerated linear transform of the basis, we can construct similar to the
first order logic knowledge representation with known invariant group! The situation was
different with the logical formulas of disjunctive conjunction or conjunctive disjunction, where
a basis transform change a formula index[11], and the invariant group is either completely
unknown or poorly understood; a typical solution in this situation is to introduce a “formula
complexity” concept to limit the formulas to be considered, a mutli–index constraint (49)
can be viewed as a complexity of the formulas allowed. The terms
k

n−1
xk = xk00 xk11 . . . xn−1

k = (k0 , k1 , . . . , kn−1 )
D=

n−1
X

kj

(47)
(48)
(49)

j=0
D
N (n, D) = Cn+D−1

(50)

are now identified by a multi–index k and added to (1) as attributes5 . We will call the set of
all possible (47) used as ML attributes in (1) — the “product” attributes. An individual (47)
is called “term”, see [17–19]. The number N (n, D) of “product” attributes is the number of
possible polynomial distinct terms with multi–index not higher than D, it is equal to (50). A
few values: N (n, 1) = n, N (n, 2) = (n + 1)n/2, N (7, 7) = 1716, N (8, 7) = 3432, etc. In a
typical ML setup such a transform to “product” attributes is not a good idea because of:
• A linear transform of input attributes produces a different solution, no gauge invariance.
• Attributes offset and normalizing difficulty.
• Data overfitting (curse of dimensionality), as we now have a much bigger number
of input attributes N (n, D). A second complexity criteria (the first one is maximal
multi–index (49)) of constructed attributes is typically introduced to limit the number
of input attributes. For example, a neural network topology can be considered as a
variant of a complexity criteria.
5

Note, that since the constant does always present in the original xk attributes (1) linear combinations,
the xj xk (and high order) products always include the xk (lower order products), what may produce a
degenerated basis. The degeneracy can be removed either manually or by applying any regularization
algorithm, such as the one from Appendix A. Unlike polynomials in a single variable, multidimensional
polynomials cannot, in general, be factored[15, 16].

18
The approach developed in this paper has these difficulties solved. The invariant group
is a non–degenerated linear transform Tjk of input attributes components, the xj xk and
Pn−1
j 0 ,k0 =0 Tjj 0 xj 0 Tkk0 xk0 attributes produce identical solutions; for the same reason the terms
(47) Qk0 (x0 )Qk1 (x1 ) . . . Qkn−1 (xn−1 ) are Qk invariant, e.g. Qk (x) = xk and Qk (x) = Tk (x)
produce identical solutions. The attributes offset and normalizing are not important since
(5) is invariant relatively any non–degenerated linear transform of x components. The
problem of data overfitting is not an issue since Section III optimal clustering solution (30)
allows to reduce N (n, D) input attributes to a given number D of their linear combinations
that optimally separate the f . The only cost to pay is that the Lebesgue quadrature now
requires a generalized eigenproblem of N (n, D) dimension to be solved, but this is purely a
computational complexity issue. Critically important, that we are now limited not by the data
overfitting, but by the computational complexity. Regardless the input attributes number
the optimal clustering solution (30) selects given number D  N (n, D) of input attributes
linear combinations that optimally separate f in terms of hf ψ 2 i / hψ 2 i.
In the Appendix C a simple example of usage of polynomial function of a single attribute
x as input attributes was demonstrated (C1). Similarly, a polynomial of several variables (47)
identified by the multi–index (48) can be used to construct input attributes6 . An increase
of attributes number from n to N (n, D) using “product” attributes (47) combined with
subsequent attributes number decrease to D by the clustering solution (30) is a path to ML
answers of the first order logic type: n original attributes (1) → N (n, D) “product” attributes
(47) → D cluster attributes (30).
6

See numerical implementation of multi–index recursive processing in com/polytechnik/utils/At
tributesProductsMultiIndexed.java. Due to invariant group of the Radon–Nikodym approach
the “product” attributes (47) can be calculated in any basis. For example these two solutions are
identical:
• Take original basis, perform basis regularization of Appendix A, obtain “product” attributes
(47) from Xk , then solve (5) of N (n, D) dimension. Obtain the Lebesgue quadrature (15).
• In the previous step, after Xk calculation, solve (5) of dimension n to find ψ [i] (x) (6), obtain
“product” attributes (47) from these ψ [i] (x), then solve (5) of N (n, D) dimension. Obtain (15).
See com/polytechnik/utils/TestDataReadObservationVectorXF.java:testAttributesProd
ucts() for unit test example. This result is also invariant to input attributes ordering method.
For highly degenerated input attributes a direct application of com/polytechnik/utils/Attri
butesProductsMultiIndexed.java algorithm to create N (n, D) “product attributes” and then
regularize them all at once may not be the best approach from computational stability point
of view. In this case it may be a better option to perform basis regularization incrementally,
simultaneously with product attributes construction: obtain original basis regularized attributes
B (1) , multiply them by itself (square), regularize the products to obtain the basis B (2) . Repeat the
procedure: on each step multiply the basis B (d−1) by B (1) and do a regularization of products to
obtain B (d) until the sought basis B (D) is obtained.

19
A.

Lenna Image Interpolation Example. Multi–index Constraints Comparison.

In [20] a two–dimensional image interpolation problem was considered with multi–index j
constraint
(x, y)(l) → f (l)

weight ω (l) = 1

(51)

j = (jx , jy )

(52)

0 ≤ jx ≤ nx − 1

(53)

0 ≤ jy ≤ ny − 1

(54)

basis : xjx y jy

dim(basis) = nx ny

(55)

of each multi–index component being in the [0 . . . n{x,y} − 1] range; total number of basis
functions is then nx ny (55). This is different from the constraint (49), where the sum of all
multi–index components is equal to D; total number of basis functions is then (59). Different
basis functions produce different interpolation, let us compare the interpolation in these two
bases. Transform dx × dy image pixel coordinates (x, y) (x = 0 . . . dx − 1; y = 0 . . . dy − 1)
and gray intensity f to the data of (1) form:
(x, y, 1)(l) → f (l)

weight ω (l) = 1

(56)

j = (jx , jy , jc )

(57)

D = jx + jy + jc

(58)

basis : xjx y jy = xjx y jy 1jc

dim(basis) = N (n, D)

(59)

Input attributes vector x is of dimension n = 3: pixel coordinates and const, this way the (47)
“product” attributes with the constraint (58) include all xjx y jy terms with lower than D degree
jx + jy ≤ D. Observation index l runs from 1 to the total number of pixels M = dx × dy .
Let us compare [20] nx = ny = 50; dim(basis) = nx ny = 2500 of basis (55) with n =
3; D = 69; dim(basis) = N (n, D) = 2485 of basis (59). The value of D = 69 is selected to
have approximately the same total number of basis functions. The bases are different: x67 y 2 ,
x66 y 2 , etc. are among “product” attributes (59), but they are not among the (55) where the
maximal degree for x and y is 49; similarly the x49 y 49 is in (55), but it is not in (59). As in
[20] we choose 512x512 Lenna grayscale image as a testbed. If you have scala installed run
scala com.polytechnik.algorithms.ExampleImageInterpolation \
file:dataexamples/lena512.bmp 50 50 chebyshev

20
to reproduce [20] results using (8) and (9) for least squares and Radon–Nikodym. Then run
(note: this code is unoptimized and slow):
java com/polytechnik/algorithms/ExampleImageInterpolation2 \
file:dataexamples/lena512.bmp 50 50 69
To obtain 4 files. The files lena512.bmp.LS.50.50.bs2500.png and lena512.bmp.RN.50.
50.bs2500.png are obtained as (12) and (13) using (55) basis with nx = ny = 50, the result
is identical to [20]. The files lena512.bmp.LS.D.69.bs2485.png and lena512.bmp.RN.D.6
9.bs2485.png are obtained from (12) and (13) using (59) basis with D = 69. The images
are presented in Fig. 2. It was shown [20] that the Radon–Nikodym interpolation produces
a sfumato type of picture because it averages with always positive weight ψ 2 (x); the (13)
preserves the bounds of f : if original gray intensity is [0 : 1] bounded then interpolated gray
intensity is [0 : 1] bounded as well; this is an important difference from positive polynomials
interpolation[21] where only a low bound (zero) is preserved. In contradistinction to Radon–
Nikodym the least squares interpolation strongly oscillates near image edges and may not
preserve the bounds of gray intensity f . In this section we compare not least squares vs.
Radon–Nikodym as we did in [20] but the bases: (55) vs. (59) as they have a different
multi–index constraint. We observe that:
• The bases produce similar results. Basis differences in LS are more pronounced, than
in RN; always positive weight makes the RN less sensitive to basis choice.
• In RN a small difference is observed near image edges. With (55) RN still has small
oscillations near edges, and with (59) RN has oscillations completely suppressed.
• The multi–index constraint (55) is not invariant relatively a linear transform of input
attributes, for example xnx −1 y ny −1 relatively x = x0 − y 0 , y = x0 + y 0 , but the (59) is
invariant.
This make us to conclude that the specific multi–index constraint is not very important, the
results are similar. Whereas in the interpolation problem an explosion of basis functions
number increases interpolation precision, in the classification problem an explosion of basis
functions number leads to data overfitting. The optimal clustering solution (30) reduces
the number of basis functions to a given D thus it solves the problem of data overfitting.

21

FIG. 2. Top: original image. Middle: least squares in (55) basis (left) and (59) basis (right). Bottom:
Radon–Nikodym in (55) basis (left) and (59) basis (right). The bases (55) and (59) are of 2500
elements (nx = ny = 50) and 2485 elements (n = 3, D = 69) respectively.

This reduction makes the multi–index constraint used for initial basis construction even less
important.

22
B.

On The Christoffel Function Conditional Optimization

All the solutions obtained in this paper have a distribution of f as the answer: the
distribution with posterior weights (17b), optimal clustering (28), etc. Recently, a promising
approach to interpolation problem has been developed [22]. In this subsection we consider
a modification of it to obtain, for a given x, not a single outcome of f , but a distribution.
Obtained weights can be considered as an alternative to the posterior weights (17b). A sketch
of [22] theory:
• Introduce a vector z = (x, f ) of the dimension n + 1.
• Construct “product” attributes (47) out of z components with the degree equals to D;
because a constant always presents in xk it is sufficient to consider the degree equals
to D, lower order terms are obtained automatically as in (59). There are N (n + 1, D)
“product” attributes obtained from n + 1 components of z.
• Construct Christoffel function (10) from obtained “product” attributes K(z) = K(x, f ).
Now the 1/K(z), for a given x, is a positive polynomial on f of the degree 2D.
• For a given x, the interpolation [22] of f is the value providing the minimum of the
polynomial 1/K(x, f ); the value of x is fixed:
−→ max

K(x, f )
x

f

(60)

As an extension of this approach consider Christoffel function average, Appendix B of [1],
but use the K(z) = K(x, f ) to calculate the moments of f :
m

D

E

m

hf iK(x,·) = f K(z)

x

=

M
X
l=1

m
f (l)
ω (l)
(l)
1/K(x, f )

(61)

When one uses x = x(l) as Christoffel function argument in the right hand side of (61), the
average is the Christoffel function average of Ref. [1] with the properties similar to regular
average (7); the Gaussian quadrature built from the moments obtained with the Christoffel
function average is similar to the one built from the regular moments hf m i, and to the one
built from (26) moments with g = f m . However, if to consider a fixed value of x, then the
solution becomes similar to the approach of Ref. [22], the K(x, f ) is now used as a proxy to
joint distribution ρ(x, f ). Because 1/K(x, f ) at fixed x is a positive polynomial on f of the

23
degree 2D, the moments hf m iK(x,·) do exist for at least m = 0 . . . 2D. A D–point Gaussian
quadrature can be built from them, exactly as (28), but with the measure h·iK(x,·) instead of
h·iL . The result is D nodes (28f) and weights (28g). The major difference from [22] is that
[i]

[i]

instead of single f we now obtained i = 0 . . . D − 1 (outcome,weight) pairs (fK(x,·) , wK(x,·) )
of the distribution of f conditional to a given x. The most close to [22] interpolation answer
[i]

[i]

is to find the fK(x,·) , corresponding to the maximal wK(x,·) . However, in ML the distribution
of outcomes, not a single “answer”, is of most interest. From the Gaussian quadrature built
on the h·iK(x,·) measure conditional distribution characteristics can be obtained:
• The h1iK(x,·) is an analogue of Coverage(x) from (19): how many observations are “close
enough” to a given x.
[i]

[i]

• The Gaussian quadrature nodes and weights (fK(x,·) , wK(x,·) ) are an analogue of the
[i]

posterior distribution (17b). However, in (61) approach both: the outcomes fK(x,·) and
[i]

the weights wK(x,·) depend on x. In (17b) approach the outcomes are always the same
f [i] and only posterior weights depend on x as w[i] Proj[i] (x). This distinction is similar
to [3] with x–dependent outcomes vs. [23] with x–independent outcomes.
• The approach (61) cannot provide an optimal clustering solution of (30) type. Ideologically, x–dependent outcomes make optimal clustering difficult. Technically, the
m = 0 . . . 2D moments hf m iK(x,·) cannot be reduced to a density matrix average of
Appendix C of [1] or to a simple pure state average (15b).

VI.

A SUPERVISED CLASSIFICATION PROBLEM WITH VECTOR–VALUED
CLASS LABEL

In the ML problem (1) the class label f is considered to be a scalar. A problem with
vector–valued class label f
(x0 , x1 , . . . , xk , . . . , xn−1 )(l) → (f0 , f1 , . . . , fj , . . . , fm−1 )(l)

weight ω (l)

(62)

where an attributes vector x of the dimension n is mapped to a class label vector f of
the dimension m is a much more interesting case. For a vector class label f , the most
straightforward approach is to build an individual model for every fj component. However,
constructed models are often completely different and obtained model set cannot be viewed

24
as a probability space. In addition, the invariant group of f (what transform of fj components
does not change the prediction) may become unknown and basis–dependent. The situation is
similar to the one of our previous works[3, 23], where the distribution regression problem
can be directly approached by the Radon–Nikodym technique, however the distribution to
distribution regression problem is a much more difficult case.
Whereas the Christoffel function maximization approach (60) of Ref. [22] is interesting
for a scalar f , it becomes extremely promising for a vector class label f . Consider a vector z
of the dimension n + m:
z = (x0 , x1 , . . . , xk , . . . , xn−1 , f0 , f1 , . . . , fj , . . . , fm−1 )(l)

weight ω (l)

(63)

The vector z mixes input attributes x with class label vector f . The N (n + m, D) “product”
attributes Zi can be obtained out of n+m z components as in (47). The “product” attributes Zi
with the constraint (49) are the ones with the simplest invariant group: the answer is invariant
Pn+m−1
relatively any non–degenerated linear transform of z components: zs0 =
Tss0 zs0 ;
s0 =0
s, s0 = 0 . . . n + m − 17 . The invariant group can be viewed as a gauge transformations and is
a critical insight into the ML model built.
From (63) z data construct N (n + m, D) “product” attributes Zi according to (49) (if
necessary perform regularization of the Appendix A), then, finally, construct the Christoffel
function K(z) according to (10). Classification problem is to find f –prediction for a given
x. When one puts xk , k = 0 . . . n − 1 part of vector z equal to a given x the K(x, f ), for a
fixed x, can be viewed as a a proxy to joint distribution ρ(x, f ). Find it’s maximum over the
vector f :
−→ max

K(x, f )
x

f

(64)

to obtain Ref. [22] solution. The solution (64) is exactly (60), but with a vector class label f !
For a fixed x and a degree D the 1/K(x, f )

x

is a polynomial on fj of the degree 2D,

there are total N (m, D) distinct terms. In applications it may be convenient to minimize the
polynomial 1/K(x, f )

x

instead of maximizing the Christoffel function (64), but these are

implementation details.
7

In practical applications, it is often convenient to consider different degree D for x and f , e.g. to consider
D > 1 only for x to obtain N (n, D) “product” attributes and, for the class label, consider D = 1. There are
will be m = N (m, 1) attributes fj , total N (n, D) + m attributes Zi . Below we consider only the case of
the constraint (49), providing N (n + m, D) attributes Zi . The transition to “product” attributes extends
the basis space, but the |ψi still form a linear space [24].

25
Critically important, that, for a given x, we now obtained a probability distribution of f
as K(x, f ) . When a specific value of f is required, it can be estimated from the distribution
as:

x

• Christoffel function maximum (64).
• The distribution of Christoffel function eigenvalues (34)
• The simplest one is to average f with K(x, f ) , same as (61) but with the vector f
x
D
E
m
instead of f : f K(z)
and similar generalizations.
x

The most remarkable feature is that the K(x, f )

x

approach is trivially applicable to a vector

class label f , and the constructed model has a known “gauge group”.

A.

A Vector–Valued Class Label: Selecting Solution Type

While the idea [22] to combine input attributes x with class label vector f into a single
vector z (63) with subsequent construction of “product” attributes Z (47) and finally to
obtain Gram matrix hZi Zj i and Christoffel function K(z) (10) is a very promising one, it
still has some limitations.
Consider a D = 1 example: let a datasample (62) has f0 = x0 for all l = 1 . . . M . Then
Gram matrix hzi zj i is degenerated. When attributes regularization is applied — it will remove
either f0 or x0 from z, thus the resulting K(z) depends on attributes regularization: a
x
D
E
polynomial 1/K(z) on f is different, thus f K(z)
produces the result depending on the
x

x

regularization. An ultimate example of this situation is: for k = 0 . . . n − 1, let fk = xk for
all l = 1 . . . M with n = m. In this case Gram matrix has two copies of exactly the same
attributes and what combination of them propagate to the final set of attributes depends on
regularization. For example if xk are selected and fk are dropped then K(z) is a constant
x
D
E
and f K(z)
is x–independent. Such a regularization–dependent answer cannot be a solid
x

foundation to ML classification problem, a regularization–independent solution is required.
Consider two Gram matrices hxk xk0 i and hfj fj 0 i with attributes possibly “producted” (47)
to Dx and Df . It’s “gauge transformation” is:
x0k =

n−1
X
k0 =0

Tkk0 xk0

(65a)

26

fj0

=

m−1
X

(65b)

Tjj 0 fj 0

j 0 =0

There are no x ⇔ z “cross” terms as when we were working with the combined z, this makes
the solution regularization–independent.
Consider the simplest practical solution. Let xk attributes being regularized and “producted”
(47) to a degree D. The f attributes are untransformed. The Radon–Nikodym interpolation
solution (9) is directly applicable:
n−1
P

fRN (x) =

−1
xl G−1
lj hxj | f | xk i Gki xi

l,j,k,i=0
n−1
P

(66)
xj G−1
jk xk

j,k=0

This “vector” type of solution to distribution to distribution regression problem (that was
obtained back in [23]) is just (9) applied to every component of f . As we discussed in Section
II and demonstrated in the Appendix B 2, such a solution, while being a good one to an
interpolation problem, leads to data overfitting when applied to a classification problem. We
need to use the posterior (17b) distribution weights to obtain an analogue of fRN W (x) (18),
but without generalized eigenvalue problem on f , as the f is now a vector. This is feasible
if we go from “regular” average to Christoffel function average of Section III A. All density
matrix averages posses the duality property[1]:
Spur kf |ρK k =

n−1
X

[i]
λK

D

[i]
ψK

[i]
ψK

f

E

i=0

[i]

λf

D

[i]

ψf

[i]

ρK ψf

E

(67)

i=0



[i]

[i]

Thus, for a vector f , where the pairs λf ; ψf
n−1
P

fRN W (x) =

=

n−1
X

i=0

E

[i]

do not exist, obtain in ψK

h
i2 D
E
[i]
[i]
[i]
[i]
λK ψK (x)
ψK f ψK
n−1
P
i=0

[i]
λK

h
i2
[i]
ψK (x)

E

basis:

(68)

This is the simplest practical solution8 to a classification problem with vector class label f . It
E
[i]
uses unsupervised learning basis ψK of generalized eigenvalue problem (34) to solve the
problem with a vector class label f . The solution (68) assumes every component of vector
8

One can also try the fRN (x) from (66) with hxj | K(x) | xk i and hxj | f (x)K(x) | xk i used instead of
Gjk = hxj | xk i and hxj | f | xk i.

27
E
[i]
f is diagonal in the basis ψK . This is not generally the case, but allows to build a single
classificator for a vector class label f instead of constructing an individual classificator for
every fj component. The option --flag_assume_f_is_diagonal_in_christoffel_funct
ion_basis=true of the provided software (see Appendix B below) builds such a classifier.
E
[i]
This “same ψK basis for all fj ” classifier typically has worse quality that the one built in
ψ [i] basis corresponding to an individual scalar class label fj
The approach of two Gram matrices hxk xk0 i, k, k 0 = 0 . . . n−1 and hfj fj 0 i, j, j 0 = 0 . . . m−1
without “mixed” terms hxk fj i in basis allows to obtain a “relative frequency” characteristic, a
density of state type of solution. Consider R, the ratio of two Christoffel functions:
(69)

K(f (x)) = R · K(x)
n−1
P

R=

αk hxk | K(f (x)) | xk0 i αk0

k,k0 =0
n−1
P

(70)
αk hxk | K(x) | xk0 i αk0

k,k0 =0

which is an estimator of Radon–Nikodym derivative[25]. The R is a dimensionless “relative
frequency”: how often a given realization of vector f corresponds to a given realization of
vector x in (62) sample. The K(x) and K(f ) are Christoffel functions calculated on x and
f portion of (62) data, possibly regularized and “producted”. The 1/K(x) and 1/K(f ) are
positive polynomials on xk and fj components respectively.
To obtain the distribution of R multiply left- and right- hand side of (69) by ψ 2 (x) and
integrate it over all l = 1 . . . M observations of (62) datasample, obtain (70). The calculation
of hxk | K(f (x)) | xk0 i matrix elements is no different from the one performed in (34): use (10)
expression, but now in f –space. A familiar generalized eigenvalue problem is then:
n−1
X

[i]
hxk | K(f (x)) | xk0 i αk0

=

n−1
X

[i]
λR

k0 =0

[i]

(71)

hxk | K(x) | xk0 i αk0

k0 =0
[i]
ψR (x)

=

n−1
X

[i]

(72)

α k xk

k=0

Obtained

[i]
λR

is a spectrum of “relative frequency”. In

[i]
ψR

E

[i]

state there are λR time more f

observations than x observations. The matrices hxk | K(f (x)) | xk0 i and hxk | K(x) | xk0 i are
n × n matrices calculated from a training datasample. The knowledge is accumulated in their
spectrum. When evaluating a testing dataset the simplest usage of (70) is this: for a given

28
x, how often/seldom we see an f ? The answer is (70) with localized αk =

Pn−1

k0 =0

G−1
kk0 xk0 or,

when written in (72) basis
n−1
P

R(x) =

h
i2
[i]
[i]
λR ψR (x)

i=0
n−1
Ph
i=0

(73)

i2

[i]
ψR (x)

While the (68) is f –value predictor, the R is “relative frequency” estimator, an important
characteristic when considering a vector–to–vector type of mapping.

B.

A Vector–Valued Class Label: Error Estimation

The vector–value estimators (66) and (68) are an estimation of f by averaging class label
f (l) = (f0 , f1 , . . . , fj , . . . , fm−1 )(l) from (63) with a x– dependent positive weight Wx (x(l) ):
M
P

f (x) =

h1iWx =

Wx (x(l) )f (l)

l=1
M
P

(74)
Wx

(x(l) )

l=1
M
X

(75)

Wx (x(l) )

l=1

What is the best way to estimate an error of a solution of this type? A “traditional” approach
D
2 E
would be to consider a standard deviation type of answer f − f
, a variation of f
components relatively their average value. This solution can be obtained from Gram matrix
in f –space (with some complications because of vector class label f ):
Gjk = hfj fk iWx =

M
X

(l) (l)

Wx (x(l) )fj fk

j, k = 0 . . . m − 1

(76)

l=1

As we discussed in [8] and then earlier in this paper all standard deviation error estimators
cannot be applied to non–Gaussian data, thus they have a limited applicability domain. A
much better estimator can be constructed from the Christoffel function. Consider Christoffel
Pm−1
function in f –space KWx (f ), obtained from Gram matrix (76) as 1/KWx (f ) = j,k=0
fj G−1
jk fk ,
exactly as we did in (10) in x–space9 . Consider the best possible situation when (74) has no
9

To calculate Christoffel function properly there always should be a constant present in the
(f0 , f1 , . . . , fj , . . . , fm−1 ) basis space, if it does not have one – add an attribute fm = 1 to the basis.
If Gjk is degenerated the vector (f0 , f1 , . . . , fj , . . . , fm−1 ) should be regularized according to Appendix A
with the replacement xj → fj . Described there regularization algorithms always add a constant to the
basis if it does not have one.

29
variation, i.e. the averaging gives exact values. The support of this measure is then a single
point f from (74) (compare with a Gaussian quadrature in case when a single node has a
dominantly large weight). When a prediction is not perfect we have a variation of f (l) around
average. Exactly as we did above, instead of considering a variation in f –space, consider the
support of a measure, a “Lebesgue” style approach. The total measure is h1iWx , the support
of f –localized state is KWx (f ), their difference gives error estimation:
(77)

Error = h1iWx − KWx (f )
Errorrel =

Error
KWx (f )
=1−
h1iWx
h1iWx

(78)

Error estimator (77) has a dimension of weight (number of observations). It has the meaning
of the difference between total measure and the measure of f –localized state. It is gauge
invariant relatively (65).
Even when a predictor (in a form of x– dependent positive weight Wx (x)) does not exist
we can still obtain an information of how well a vector in f -space can be recovered from
x-space. In scalar case f = f the simplistic solution to the problem is the aforementioned L2
norm (2): if standard deviation is zero then f can be completely recovered from the value of
x. However, this solution, besides depending on the scale of f , is problematically to generalize
to a vector f .
We can construct a solution to vector f from three matrices: hfj 0 fk0 i (the (76) with
Wx = 1), hxj xk i, and hxj fk0 i. The first two are Gram matrices in f - and x- space respectively:
j 0, k0 = 0 . . . m − 1

(79)

Gxjk = hxj xk i

j, k = 0 . . . n − 1

(80)

Gxf
jk0 = hxj fk0 i

j = 0 . . . n − 1; k 0 = 0 . . . m − 1

(81)

Gfj 0 k0 = hfj 0 fk0 i

In scalar f case we have m = 2 or greater:
f = (1, f )
f = (1, f, f 2 )
f = (1, f, f 2 , f 3 )

f0 = 1; f1 = f ; m = 2

(82)

f0 = 1; f1 = f ; f2 = f 2 ; m = 3
f0 = 1; f1 = f ; f1 = f 2 ; f3 = f 3 ; m = 4

a constant should always present in the basis (both in f and x). A criterion of how well f can
be recovered from x is to compare the matrices hfj 0 fk0 i and hfj 0 (x)fk0 (x)i; the fj 0 is exact

30
value and the fj 0 (x) is obtained from (8) projection of f on x-space:
(83)

f (x) = Proj(f →x) f
fj 0 (x) =

n−1
X

−1
xj Gx;
hfj 0 xk i
jk

(84)

j,k=0

hfj 0 (x)fk0 (x)i =

n−1
X

−1
hfj 0 xj i Gx;
jk

hfk0 xk i =

j,k=0

n−1
X

x; −1 xf
Gxf
jj 0 Gjk Gkk0

(85)

j,k=0

−1
Here Gx;
is an inverse of Gxjk from (80). The non–negative m × m symmetric matrices10 :
jk

hfj 0 (x)fk0 (x)i (Eq. (85)) and hfj 0 fk0 i (Eq. (79)) coincide if f is a subspace of x; both represent
the f -space: the former is projected on x, the later is calculated directly.
Solve generalized eigenproblem with these two matrices in left- and right- hand side
respectively, exactly as in (5):
m−1
X

[i]
hfj 0 (x)fk0 (x)i αk0

=λ

[i]

k0 =0
m−1
n−1
XX

m−1
X

[i]

hfj 0 | fk0 i αk0

(86)

k0 =0
x; −1 xf [i]
Gxf
jj 0 Gjk Gkk0 αk0

=λ

[i]

m−1
X

[i]

Gfj 0 k0 αk0

k0 =0

k0 =0 j,k=0

If f -space is a subspace of x-space then all i = 0 . . . m − 1 eigenvalues λ[i] are equal to 1 and
their sum is equal to matrix hfj 0 | fk0 i rank m. Otherwise the difference represents an error:
how big is the remaining error after projecting f -space on x-space:
Errorrank = m −

m−1
X
i=0

[i]

λ =m−

m−1
X

hfj (x)fk (x)i Gfkj; −1

(87)

j,k=0

This error is gauge–invariant relatively (65), it is dimensionless and represents how well f -space
can be projected on x-space. It can be viewed as a gauge–invariant “squared multi–dimensional
correlation” between f (x(l) ) and f (l) , l = 1 . . . M . If n = m = 2 we have: x = (1, x); f = (1, f )
then (86) has the maximal eigenvalue λ[1] = 1 because a constant presents in both bases, and
minimal eigenvalue is equal to regular correlation between x and f squared: λ[0] = ρ2 (x, f ).
The (87) can also be calculated directly using matrix Spur, without solving a generalized
eigenvalue problem. It is a “rank–difference” error estimator what makes it not always
convenient in practical ML applications. The most convenient error estimator in ML is of
“coverage” type: how many observations are correctly classified (or misclassified). This error
10

If the matrix hfj 0 fk0 i is not positive — apply Appendix A regularization first.

31
can be obtained using (84) projection and Christoffel function technique we applied in Section
VI C below to the Low-Rank Representation(LRR) problem. The solution is straightforward:
• Construct a ψg (f ) state, localized at f = g, it is exactly (24) with a replace x → f ;
y → g; G → Gf , see Eq. (E5).
• In every g = f (l) point we have ψf2(l) = 1, exactly as in full basis expansion (110).
• If one, instead of ψg (f ), take it’s projection (84) to x-space — the value (88) can be
lower than 1, similarly to (111). Then sum it over all l = 1 . . . M sample observations
to obtain the number of covered points. The Error is then:
n−1
P

h
i2 
(f →x)
$(g) = Proj
ψg
=

m−1
P

j,k=0 s0 ,j 0 ,k0 ,t0 =0

x; −1 xf
xf
gs0 Gfs0;j−1
Gkk0 Gfk;0 t−1
0 Gjj 0 Gjk
0 gt0
m−1
P
j 0 ,k0 =0

Error = h1i −

M
X

(88)
gj 0 Gfj 0;k−1
0 gk 0
(89)

$(f (l) )ω (l)

l=1

The (89) is an analogue of (77) with no predictor available, this is a characteristics of the
data, not of a predictor, the sum of basis projection successes $(f (l) ) in every observation
point l with the weight ω (l) . This expression can be generalized with an operator U in x-space
converting ψx(l) (x) to some other function in x-space |ψ(x)i = |U|ψx(l) (x)i and only then
projecting the result to actual realization ψf (l) (f ) in f -space:
Error = h1i −

M
X

|hψf (l) | U | ψx(l) i|2 ω (l)

(90)

l=1

This error is the number of misclassified observations for specific predictor kUk, it is always
greater than the error (89). The (89) corresponds to |U|ψx(l) i (a single vector in x-space)
being replaced by direct projection to a full orthogonal basis ψ [i] in x-space, similar to
(111) and (F1):
$(g) =

n−1
X

ψg ψ [i]

2

1 ≥ $(g)

(91)

i=0

The $(g) determines how well a localized in f -space state ψg (f ) can be projected to x-space
basis. This criterion is then tested for all l = 1 . . . M observation points, For the reason
of testing the entire sample of M points, not just n basis functions, the Error (89) is an

32
estimation of the best possible predictor performance, thus it is useful as a bound (G3) for a
predictor of (90) form.
The Error can be spectrally expanded. Introduce
fj K

(f )

fk =

(l) (l)

M
X

fj fk
m−1
P

l=1

j 0 ,k0 =0

ω (l)

(92)

j, k = 0 . . . m − 1

(l)
(l)
fj 0 Gfj 0;k−1
0 fk 0

Which is exactly Christoffel function matrix (34), but in f -space. Then (89) can be expressed
as matrix spur (94):
(f →x)
Kjk

=

m−1
X

f ; −1
xf
Gxf
ft0 K (f ) fs0 Gfs0;j−1
0 Gjj 0
kk0 Gk0 t0

j, k = 0 . . . n − 1

(93)

k0 ,t0 ,s0 ,j 0 =0

Error = h1i −

n−1
X

(f →x)

Kjk

−1
Gx;
= h1i − SpurK (f →x) Gx; −1
kj

(94)

j,k=0
(f →x)

From which immediately follows, that if we solve generalized eigenproblem with Kjk

and

Gxjk = hxj xk i matrices in left- and right- hand side respectively, the Error can be spectrally
expanded:
n−1
X

(f →x) [i]
Kjk αk

k=0

=λ

[i]

n−1
X

[i]

hxj xk i αk

k=0
n−1
X

Error = h1i −

λ[i]

(95)
(96)

i=0

The (96) is a spectral decomposition of (89), it has at most m non–zero eigenvalues (the rank
of (93) is m or lower, we also assume m ≤ n). If f belongs to a subspace of x then the sum
of these m eigenvalues in (96) is equal to h1i. The eigenvectors corresponding to a few (m or
lower) maximal eigenvalues of (95) is the solution to vector class label classification problem
target basis (not the problem itself).
Consider a simple demonstrative solution. Let us project ψg (f ) to ψfLS (x) (f ) to obtain a
joint probability estimator: what is the probability11 of outcome g given input vector y if
fLS (x) model is assumed.
n−1 m−1
X
X
1
−1 xf
f ; −1
yj Gx;
ψfLS (y) (f ) =
jk Gkj 0 Gj 0 k0 fk0
Norm(y) j,k=0 j 0 ,k0 =0
11

(97)

The coverage of the predictor (99) at y can be estimated from the value of 1/Norm2 (y), similar to using
Christoffel function K(y) for estimation of the support of the measure of localized at x = y state.

33

2

Norm (y) =

n−1
X

m−1
X

x; −1
f ; −1 xf
−1 xf
yt
yj Gx;
jk Gkj 0 Gj 0 k0 Gsk0 Gst

(98)

j,k,s,t=0 j 0 ,k0 =0

"
Prob(g|y) = ψfLS (y) (f ) ψg (f )

2

=

n−1
P

m−1
P

j,k=0 j 0 ,k0 =0

#2
−1 xf
f ; −1
yj Gx;
jk Gkj 0 Gj 0 k0 gk0

2

Norm (y)

(99)

m−1
P
j 0 ,k0 =0

] = h1i −
Error

M
X

gj 0 Gfj 0;k−1
0 gk 0
(100)

ω (l) Prob(f (l) |x(l) )

l=1

This solution has a form of conditional probability (99) which can be used to introduce
] Whereas the “maximal coverage” estimator (89)
a predictor-specific error estimator Error.
estimates data recoverability without constructing a predictor, the estimator (100) estimates
specific simple prediction of least squares type; usual least squares property holds: it is zero
if f is a subspace of x. This estimator can be spectrally decomposed only at some given x,
this makes it’s properties (64) related. Introduce bj 0 (y):
n−1
X
1
−1 xf
bj 0 (y) =
yj Gx;
jk Gkj 0
Norm(y) j,k=0
m−1
X

[i]
bj 0 (y)bk0 (y)αk0

k0 =0

[i]

=λ

m−1
X

(101)

[i]

(102)

Gfj 0 k0 αk0

k0 =0

Then (102) has a single non–zero eigenvalue λ[m−1] =

f ; −1
j,k=0 bj Gjk bk

Pm−1

= 1, which is the

maximal value of (99). While vector–to–vector prediction models are not implemented in
the provided software yet, a reference unit test for (99) and (100) is available therein; it
can be run with random data. The calculations require only matrix algebra: the (99) is a
ratio of a quadratic form squared and a product of two quadratic forms. Hence, as with any
Radon–Nikodym type of solution, it tends to a constant (not to infinity like e.g. least squares)
when y → ∞ or g → ∞. See SolutionVectorXVectorF.java:evaluateAt(final double
[]X) for simple examples. The (99) estimates conditional probability, not the value of most
probable outcome. A familiar least squares (84) estimation of f given x can be obtained from:
fLS (x) = Norm(x)b(x)

f

LS j 0

(x) =

n−1
X

−1 xf
xj Gx;
jk Gkj 0

(103)

j,k=0

Prob(fLS (x)|x) = 1

(104)

The (99) is just a simple example of conditional probability estimator, a demonstration, that
even with least squares naı̈ve form (103) there exists a big improvement when we consider a

34
conditional probability estimation instead of typically considered value estimation. A general
form a “unitary” type of conditional probability estimator is discussed below in Appendix E,
All considered estimators are gauge–invariant relatively 65). The idea is straightforward:
consider localized at f = g state ψg (f ) (the (24) in f -space), project it to some x-dependent
vector space (in the simplistic case it is just (84) direct projection, in most general case – a
unitary transformation (E7) following a projection (E2)), then sum it over the entire sample
as in (89), (100), (112), or (E8) below to obtain the number of covered observations.
This approach can be deployed to estimate, as the number of misclassified observations,
other vector–to–vector predictor systems that result in the value f (x), not in conditional
probability Prob(f |x): for example a distribution–to–distribution regression model, a neural
network with vector output, etc. Take a projection12 of the state localized in realized outcome
ψf (l) (f ) to the state localized in predicted outcome ψf (x(l) ) (f ), obtain an expression similar to
(99) weighted over the entire sample:

Error = h1i −

M
X

ω (l) ψf (l) ψf (x(l) )

2

(105)

l=1

"
hψf | ψg i2 =

m−1
P

#2
fj Gfjk; −1 gk

j,k=0
m−1
P
j,k=0

fj Gfjk; −1 fk

m−1
P

(106)
gj Gfjk; −1 gk

j,k=0

This error estimator is outlier–stable, it has the meaning of the number of misclassified
observations. In can be applied to any predictor of f (x) output type; when least squares
prediction fLS (x(l) ) is put to (105) obtain (100). These are not bounded by (89) as they are
not of (90) form.
Another interesting option to consider is to put f ≡ x, then spectral decomposition (96)
corresponds to “coverage expansion” (42) above and to LRR solution (114) below with D = n.
Let us demonstrate an application of this technique to the Low-Rank Representation problem.

12

Note: this is a different concept from a typical consideration of how close are predicted and realized
outcomes. For an estimation of this type — one can test how much the (86) eigenvalues are lower than 1.
The Errorrank from (87) is an aggregated estimator of this type.

35
C.

A Christoffel Function Solution to Low-Rank Representation

For an unlabeled data (no class label f available) consider the problem of clustering to
build a Low-Rank Representation (LRR). Consider a data (1) without f :
weight ω (l)

(x0 , x1 , . . . , xk , . . . , xn−1 )(l)

(107)

the problem is to cluster x of a dimension n on a subspace of D < n dimension. A solution[26]
(l)

is to introduce a n × M matrix xk of the rank n (we assume the problem is already
(l)

regularized), and to represent it by n × M matrix Xk of lower rank D < n and an “error”
(l)

matrix Ek :
(l)

(l)

(l)

(108)

xk = X k + E k

(l)

The problem is then to find a low-rank representation Xk from the given observation matrix
(l)

(l)

xk , that allows to recover the given matrix with a small enough error Ek . The [26] authors
consider the following minimization problem:
h
i
e
min rank(X ) + λkEk
F
X ,E

(109)

e > 0 is a parameter and kEkF is a norm, such as the squared Frobenius norm. The
where λ
main issue with (109) minimization, besides computational difficulities, is that the solution
is not gauge invariant relatively (65a).
The (77) type of error estimator allows us to construct a gauge invariant solution. Consider
(24) state ψy (x) localized at x = y. As a regular wavefunction, when expanded in any full
basis ψ [i] obtain:
1=

n−1
X

ψy ψ [i]

2

(110)

i=0

When, instead of a full basis ψ [i] of the dimension n, a basis of lower dimension D < n is
[i]

used, this can be for example ψG (x) of the dimension D < n from (30) or any other lower
dimension basis φ[i] orthogonal as δij = φ[i] φ[j] , the sum of squared projections can be
lower than 1:
1≥

D−1
X
i=0

ψy φ[i]

2

(111)

36
The (111) was obtained back in [6] as Eq. (20) therein, where we summed it over the
entire sample. Similarly, let us sum (111) with the weights ω (l) over all y ∈ x(l) , l = 1 . . . M
observations. If all (111) terms are equal to 1 then the total measure h1i is obtained. Otherwise
the difference is an estimation: how well the space φ[i] of the dimension D < n allows to
recover the full space xk of the dimension n. The error is:
Error = h1i −

M
X

ω

(l)

D−1
X

l=1
(l)
Xk

=

D−1
X

ψx(l) φ[i]

2

(112)

i=0

(113)

xk φ[i] φ[i] (x(l) )

i=0

Unsupervised clustering solution is a D–dimensional φ[i] (x) basis minimizing the (112)
error. The solution to (112) minimization problem can be readily obtained from hψy | φi2 =
E
[i]
K(y)φ2 (y) and ψK definition in (35):
Error = h1i −

D−1
X

[i]

(114)

λK

i=0

This is (112) written in a subset of

[i]
ψK

E

basis. For D = n this is previously obtained coverage

expansion (42). The Christoffel function clustering solution φ[i] is then: the D ≤ n vectors
E
[i]
[i]
ψK out of n corresponding to D largest λK . It can be converted to x basis as (113). The
(l)

(113) is a low-rank representation of the data: the matrix Xk of rank D represents the
(l)

original data matrix xk of rank n. In contradistinction to (109) solution, the solution (114)
[i]

is gauge invariant relatively (65a) and unique if there is no λK degeneracy. This property
enables a new range of availabilities that are not practical (or even not possible) for other
clustering methods. The two most remarkable features — a possibility to use the “product
attributes” (47) and the fact that the “coverage expansion” solution (114) is obtained from
the expansion (36) of the Christoffel function, that is small for a seldom observed x. This is
important when input data (107) is a union of subspaces. If x ∈ S1 and y ∈ S2 the union
S1 ∪ S2 does not form a vector space (ax + by ∈ S1 ∪ S2 iff S1 ⊆ S2 or S2 ⊆ S1 ). The
Christoffel function is small for the vectors not in S1 ∪ S2 , thus it serves as an indicator
function of a vector from subspaces direct sum S1 ⊕ S2 to belong to subspaces union S1 ∪ S2 .
The option --flag_replace_f_by_christoffel_function=true of Appendix B software
[i]

(l)

makes the program to construct and output the ψK (x(l) ) matrix from read xi input matrix
of the dimensions: i = 0 . . . n − 1; l = 1 . . . M . Set option --flag_print_verbosity=3 to

37
D
E
[i]
[i]
(l)
print all xk ψK coefficients and ψK (x(l) ) values to obtain Xk . The error (114) depends
E
E
[i]
[i]
[i]
on how many ψK are included in (113) as φ , the error is zero if all ψK are included.

D.

An application of LRR representation solution to a dynamic system
identification problem.

For an application of LRR solution to a dynamic system identification consider a linear
stochastic dynamic system:
(l+1)

xj

(l)

(l)

n−1
X
− xj
dxj
(l)
(l)
≈
=
Mjk xk + j
τ
dt
k=0

(115)

Here we assume that the dataset (107) is l–ordered (e.g. l is time and all ω (l) = 1). The
(115) left–hand side is a discrete analogue of time–derivative, the (l) is a noise with some
distribution (not necessary Gaussian). The problem: to determine the matrix Mjk for a given
(l)

observation set xk , k = 0 . . . n − 1; l = 1 . . . M .
This problem has a trivial “projection” solution, similar to (84) projection with a replace
fk → dxk /dt:
Mjk =

n−1 
X
dxj
i=0

dt



xi G−1
ik

(116)

corresponding to a direct projection of dxj /dt vectors to x-space; it has zero error when
(l) = 0. This solution is formally applicable even when x and dx/dt spaces are of different
dimension, e.g. dxj /dt, j = 0 . . . n − 1, are original attributes derivatives, and xk are product
attributes (47) with a multi–index k; there are N (n, D) product attributes (50). Then
the matrix Mjk is of the dimension n × N (n, D) and the matrix G−1
ik is of the dimension
N (n, D) × N (n, D) The selection of a space to project is the key element of any approach, a
direct use of the full x-space (even more so for product attributes space) typically produces
poor results.
The x is a phase space of the dynamic system (115), for a mechanical system it is coordinates
and momentums x = (q, p). Dynamic system equation determines the evolution of a point in
the phase space. The biggest practical problem with a dynamic system identification is that
the phase space can be of a very large dimension. We need a low–dimensional subset that
captures most of the dynamic features.

38
In case of a stationary dynamic system (115) our solution is straightforward: apply Section
(l)

VI C LRR solution to the phase space matrix xk , k = 0 . . . n − 1; l = 1 . . . M : Construct the
K(x), perform (34) coverage expansion in x–space, then select D ≤ n maximal eigenvalues
(according to (114) error condition), new basis functions φ[i] , i = 0 . . . D − 1 are corresponding
to them eigenvectors (35). Then study the system dynamics in φ[i] basis of dimension D ≤ n:
D−1
dφ[i] X f [k]
=
Mik φ + i
dt
k=0
[i]

φ =

n−1
X

i, k = 0 . . . D − 1

[i]

(117)
(118)

α j xj

j=0

Instead of the original problem to identify the matrix M of the dimension n the problem
f of the dimension D ≤ n.
became to identify the matrix M
The (117) is a “projected” dynamic equation. One can use (113) to obtain the dynamics
in original variables xj and dxj /dt. The LRR solution of Section VI C constructs the φ[i]
basis of the dimension D, this basis is the optimal one to recover the dynamics of (115) in
the form (117) among all D-dimensional bases.

E.

Localized states |ψy i dynamics.

A dynamic equation of (115) form is written in x-space directly. It is equivalent to a
recurrent relation:
(l+1)
xj

=

n−1
X

(l)

(l)

Mjk xk + j

(119)

k=0

with Mjk = δjk + τ Mjk being evolution matrix and a renormalized noise. This equation
determines the dynamics of a point in the original phase space x of the system. Existing
dynamics study techniques typically use a variant of Kalman filter[27] approach, which is a
linear quadratic estimation (LQE). The central concept of these approaches is covariance
matrix, a “glorified standard deviation” concept. The technique developed in this paper is
P
based on using a wavefunction ψ(x) = n−1
k=0 αk xk and obtaining the results by averaging
with the ψ 2 (x) weight. For this reason, instead of considering the dynamic of the point itself,
we are going to consider the dynamics of a wavefunction localized at some point of the phase
space: not the dynamics of x(l) but of a state ψx(l) (x), localized at x = x(l) ; it is the state
ψy (x) from (24) with y = x(l) .

39
The transition x(l) → x(l+1) corresponds to localized wavefunction transition |ψx(l) i →
|ψx(l+1) i:
(120)

ψx(l+1) (x) = Uψx(l) (x) + 
|ψx(l+1) i = |U|ψx(l) i + kk

Here the kUk is a unitary operator (to preserve normalizing) converting ψy (x) from (24)
from y = x(l) to y = x(l+1) ; in the simplest stationary case it can be considered l–independent,
and kk is a noise operator. The (120) is written in two types of notation; it can be projected
to any orthogonal basis ψ [i] (for example (6) with any f , Christoffel basis (35), regularized
basis Xi from the Appendix A, etc.) to be written in the matrix form:
(l)
si = ψx(l) ψ [i] = s

ψ [i] (x(l) )

n−1
P

(121)
2

|ψ [j] (x(l) )|

j=0
(l+1)

sj

=

n−1
X

(l)

(122)

Ujk sk + 

k=0

The (122) is the dynamic equation for the projections ψx(l) ψ [i] .
(l)

A dynamic system identification problem, for a given observation set xk , k = 0 . . . n−1; l =
1 . . . M , instead of determining an evolution matrix Mjk of the dimension n×n that transforms
x(l) to x(l+1) now became: to determine a unitary operator Ujk of the dimension n × n that
transforms ψx(l) to ψx(l+1) . If one apply (116) solution to (122) this will be incorrect13 : the
(116) is a equation for a point in phase space, it minimizes predicted/observed differences. It
corresponds to minimizing the L2 norm error in (119):
"
#2
M
n−1
X
X
(l+1)
(l)
xj
−
Mjk xk
ω (l) −−→ min
l=1

k=0

to obtain a linear system solution from

j = 0...n − 1

Mjk

M
P

(l+1) (l) (l)
xk ω

xj

(123)

determining linear system right part

l=1

and Gram matrix (7c) determining linear systems matrix.
The (122) is a equation for the wavefunction, e.g. if, for a few selected l, make a transform
(l)
si
13

(l)

→ −si , i = 0 . . . n − 1, the result should be identical; similarly Ujk and −Ujk should

It is also incorrect to consider time evolution operator as an “average” of observed state transitions:
e = PM |ψx(l+1) i hψx(l) | with subsequent “unitarization” procedure (e.g. SVD followed by setting
kUk
l=1
Σjk = δjk we deployed in Eq. (D9) for numerical optimization) because identical dynamics must be
obtained under transform ψx(l) → exp(iϕ(l) )ψx(l) with arbitrary phases ϕ(l) , l = 1 . . . M ; this invariance is
satisfied only in (125).

40
provide identical dynamics. Were we study a quantum system time evolution operator can
be readily obtained as Hamiltonian related:


t
U = exp −i H
~



(124)

ψ (t) = U|ψ (t=0)
Now, however, we are trying to construct the operator U from the data. The functional14
M
X

2

(125)

hψx(l+1) | U | ψx(l) i ω (l) −→ max
U

l=1

determines how well ψx(l+1) is reconstructed from ψx(l) by a unitary operator U when system
dynamics takes the form of a sequence of unitary transformations (120). If there is a perfect
recovery for all l – then total coverage h1i is obtained, the difference is an error. The problem
is: to find a unitary transformation U maximizing (125). In (121) basis the (125) is:
Sjk;j 0 k0 =

M
X

(l+1) (l) (l+1) ∗ (l) ∗
s k sj 0
sk 0

(126)

ω (l) sj

l=1
n−1
X

U

j,k,j 0 ,k0 =0
n−1
X

(127)

Ujk Sjk;j 0 k0 Uj∗0 k0 −→ max

(128)

∗
Ujk0 Ukk
0 = δjk

k0 =0

(129)

Sjk;j 0 k0 = Sj∗0 k0 ;jk

The optimization problem (127) is considered for a matrix Ujk satisfying unitarity constraint
(128); the Sjk;j 0 k0 is a Hermitian tensor (129) obtained from the data sample, in an orthogonal
basis it takes the form (126); for Sjk;j 0 k0 = δjj 0 δkk0 Eq. (127) becomes (130). A complex unitary
matrix Ujk of dimension n is determined by n2 real parameters (a complex Hermitian matrix
of full rank is determined by n2 real parameters, a unitary matrix is obtained from it’s complex
P
∗
exponent, similar to (124)). Were the constraint (128) be of scalar type n−1
j,k,k0 =0 Ujk0 Ukk0 = n
or, even better
n−1
X

(130)

∗
Ujk Ujk
=n

j,k=0
14

2

In (125) the | · | denote absolute value, not an operator.

Here

∗

hψx(l+1) | U | ψx(l) i

=

hψx(l+1) | U | ψx(l) i hψx(l+1) | U | ψx(l) i is [0 . . . 1] bounded value having the meaning of conditional probability
and determining how well the ψx(l+1) is recovered from ψx(l) using (120).

41
which is the sum of all (128) diagonal components, then Eq. (127) can be considered as a
quadratic form with a vector of n2 dimension obtained from matrix elements of operator Ujk
row by row; the (130) is a regular Euclidean scalar product for this vector. A solution with
the constraint (130) instead of (128) can be obtained as a regular eigenproblem solution,
however it does not produce a matrix Ujk that is exactly unitary, nevertheless it may be a
good starting point for a numerical method.
For exact unitary constraint optimization problem (127) can be approached using Lagrange
multipliers technique, where it takes the form similar to an eigenvalue problem: SU = λU
but S is now a Hermitian tensor, “eigenvector” U is a unitary matrix, and “eigenvalues” λ is
a Hermitian matrix (D10); functional (125) extremal value is equal to λ spur.
While a complete mathematical structure of this problem requires a separate study, it’s
portion required for a dynamic system identification: find a unitary matrix Ujk maximizing
(127), can be readily solved numerically, see Appendix D below.
In this section a new approach to dynamic system identification is developed. Instead of
considering a trajectory in phase space we convert a sequence of phase space observations
x(l) to a sequence of probability states ψx(l) (x) (wavefunctions) localized at x(l) . Then system
dynamics is considered as a sequence of unitary transformations of the wavefunction. The
approach allows to write the dynamics of these probability states, quality criterion (127)
estimates the number of correctly predicted outcomes. The probability of the next outcome
x(l+1) being equal y given currently observed outcome equal x(l) is:
P (x(l+1) = y)

2
x(l)

= hψy | U | ψx(l) i

(131)

The approach can be readily generalized to density matrix states, the aforementioned x(l) ⇒
ψx(l) (x) correspondence becomes x(l) ⇒ kρ(l) k = |ψx(l) i hψx(l) |, then use a density matrix of
general form. In this section we solved the problem of determining evolution matrix Ujk from
a “sequence of wavefunctions” ψx(l) (x) that are obtained from a sequence of observation points
in phase space x(l) . The key element for this success is the (125) form of quality criteria.
One may ask a question: given a sequence of quantum mechanical wavefunctions, can this
approach identify a quantum system? The answer is yes if only time–evolution operator (124)
is required. If the Hamiltonian, not just time evolution operator, is required then the formal
answer is yes, but practically this requires taking a logarithm of a unitary matrix, what is a
complex problem required a separate consideration[28].

42
Another important topic to discuss is allowed symmetry of Ujk matrix. Whereas in quantum
systems Ujk can be only unitary, in data analysis it can possibly be of a non–unitary form.
We see a “non–unitary dynamics” as an important direction of further research, see Appendix
E below.

VII.

CONCLUSION

In this work the support weight of Radon–Nikodym form ψ 2 (x), with ψ(x) function to be
a linear function on xk components was considered and applied to interpolation, classification,
and optimal clustering problems. The most remarkable feature of the Radon–Nikodym
approach is that input attributes xk are used not for constructing the f , but for constructing
a probability density (support weight) ψ 2 (x), which is then used for evaluation of the value
f = hf (x)ψ 2 i / hψ 2 i or conditional probability. This way we can avoid using a norm in
f –space, what greatly increases practical applicability of the approach.
A distinguishing feature of the developed approach is the knowledge of the predictor’s
invariant group. Given (1) dataset, what x basis transform does not change the solution?
Typically in ML (neural networks, decision tree, SVM, etc.) the invariance is either completely
unknown or poorly understood. The invariance is known for linear regression (and a few
other linear models), but linear regression has an unsatisfactory knowledge representation.
Developed in this paper Radon–Nikodym approach has 1) known invariant group (non–
degenerated linear transform of x components) and 2) advanced knowledge representation in
the form of matrix spectrum; even an answer of the first order logic type becomes feasible.
The knowledge is extracted by applying projection operators, thus completely avoiding using
a norm in the solution to interpolation (13), classification (18), and optimal clustering (30)
problems.
The developed approach, while being mostly completed for the case of a scalar class
label f , has a number of unsolved problems in case of a vector class label f . As the most
intriguing one we see the question: whether the optimal clustering solution of Section III can
be generalized to vector–valued class label approach of Section VI: the solutions (66) and
(68) have no basis dimension reduction feature, and the conditional probability solution (99)
currently always sets clusters number to be equal to the dimension of vector class label. For
our first try with unitary operator dynamics see Appendix G below.

43
Appendix A: Regularization Example

An input vector x = (x0 , x1 , . . . , xk , . . . , xn−1 )(l) from (1) may have redundant data, often
highly redundant. An example of a redundant data is the situation when two attribute
components are equal e.g. xk = xk+1 for all l. In this case the Gjk = hxj | xk i matrix becomes
degenerated and the generalized eigenvalue problem (5) cannot be solved directly, thus a
regularization is required. A regularization process consists in selection of such xk linear
combinations that remove the redundancy, mathematically the problem is equivalent to
finding the rank of a symmetric matrix.
All the theory of this paper is invariant with respect to any non–degenerated linear
e with equal to zero
transform of x components. For this reason we may consider the vector x
average, as this transform improves the numerical stability of hxj | xk i calculation. Obtain
he
xj | x
ek i matrix (it is plain covariance matrix):
e = (x0 − x0 , x1 − x1 , . . . , xk − xk , . . . , xn−1 − xn−1 )
x
xk =

hxk i
h1i

(A1)
(A2)
(A3)

ejk = he
G
xj | x
ek i
s
ekk
G
σk =
h1i

(A4)

For each k = 0 . . . n − 1 consider standard deviation σk of xk , select the set S of indexes k,
that have standard deviation greater that a given ε, determined by computer’s numerical
ejk with the indexes in the set obtained: j, k ∈ S. The
precision. Then construct the matrix G
ejk is obtained by removing xk components that are equal to a constant, but it
new matrix G
still can be degenerated.
We need to regularize the problem by removing the redundancy. The criteria is like a
condition number in a linear system problem, but because we deploy generalized eigenproblem
anyway, we can do it straightforward. Consider generalized eigenproblem (A7) with the right
ejk .
hand side matrix equals to diagonal components of G
(A5)

j, k ∈S
ed = δjk G
ekk
G
jk
X
X
ejk α[i] = λ[i]
ed α[i]
G
G
jk k
k
k∈S

k∈S

(A6)
(A7)

44
S d :a set of i, such that:λ[i] > ε
X [S d ]
XS d =
αk (xk − xk )

(A8)
(A9)

k∈S

ed has only positive
By construction of the S set the right hand side diagonal matrix G
jk
terms, that are not small, hence the (A7) has a unique solution. The eigenvalues λ[i] of the
problem (A7) have a meaning of a “normalized standard deviation”. Select (A8) set: the
indexes i, such that the λ[i] is greater than a given ε, determined by computer’s numerical
precision. Obtained S d set determines regularized basis (A9). The matrix hXi | Xm i with
i, m ∈ S d is non–degenerated. After the constant component X = 1 is added to the basis
(A9) the X = (. . . Xi . . . , 1) can be used in (1) instead of the x = (. . . xk . . . ). This algorithm
is implemented in com/polytechnik/utils/DataReadObservationVectorXF.java:getDa
taRegularized_EV().
Alternatively to (A8), a regularization can be performed without solving the eigenproblem
(A7), using an approach similar to Gaussian elimination with pivoting in a linear system
problem. This algorithm is implemented in com/polytechnik/utils/DataReadObservati
onVectorXF.java:getDataRegularized_LIN(). Which regularization method to be used
depends on the parameter --regularization_method= supplied to com/polytechnik/uti
ls/RN.java driver, see Appendix B below.
A singular value decomposition is often used as a regularization method. However, for a
symmetric matrix considered in this appendix, without pseudoinverse required, a regularization method based on symmetric eigenproblem (A7) provides the same result with lower
computational complexity. Moreover, even a “Gaussian elimination with pivoting” type of
regularization provides the result of about the same quality.
Regardless the regularization details, for a given input data in the basis xk , different
regularization methods produce the same number of X components, formed vector space
is the same regardless the regularization used; the dimension of it is the rank of hxj | xk i
matrix. Important, that because the developed theory is “gauge invariant” relatively (65), all
inference results are identical regardless regularization method used, see com/polytechnik
/utils/TestDataReadObservationVectorXF.java:testRegularizations() unit test for
a demonstration. It is important to stress that:
• No any information on f have been used in the regularization of Gjk = hxj | xk i.

45
• All “standard deviation“ type of thresholds were compared with a given ε, determined
by the computer’s numerical precision. No “standard deviation“ is used in solving the
inference problem itself.
The result of this appendix is a new basis X = (. . . Xi . . . , 1) of 1 + dim S d elements ((A9) and
const, the rank of hxj | xk i), that can be used in (1). This basis provides a non–degenerated
Gram matrix hXi | Xm i (7c).

Appendix B: RN Software Usage Description

The provided software is written in java. The source code files of interest are com/polytec
hnik/utils/{RN,RadonNikodymSpectralModel,DataReadObservationVectorXF,Attrib
utesProductsMultiIndexed}.java. The class DataReadObservationVectorXF reads input
data (1) from a comma–separated file and stores the observations. The methods getDataRe
gularized_EV() or getDataRegularized_LIN() perform Appendix A data regularization
and return an object of DataRegularized type that contains the matrices hXj | Xk i and
hXj | f | Xk i in the regularized basis X. The method getRadonNikodymSpectralModel() of
this object creates Radon–Nikodym spectral model of Section II, it returns an object of Rado
nNikodymSpectralModel class. The method getRNatXoriginal(double [] xorig) of this
object evaluates an observation at a xorig in the original basis (1) and returns an object
of RadonNikodymSpectralModel.RNPointEvaluation type; this object has the methods ge
tRN(), getRNW(), and getPsikAtX() that, for a xorig given, calculate the (13), (18), and
ψ [i] (xorig) components. An object of RadonNikodymSpectralModel type has a method red
uceBasisSize(int D) that performs optimal clustering of Section III and returns RadonNi
kodymSpectralModel object with the basis chosen as the optimal dimension D clusterization
of f . The documentation produced by javadoc is bundled with the provided software.
The com/polytechnik/utils/RN.java is a driver to be called from a command line. The
driver’s arguments are:
• --data_file_to_build_model_from= The input file name to read (1) data and build
a Radon–Nikodym model from it. The file is comma–separated, if the first line starts
with the |# — it is considered to be the column names, otherwise the column names are
created from their indexes. Empty lines and the lines starting with the | are considered
comments. All non–comment lines must have identical number of columns.

46
• --data_file_evaluation= The input files (multiple options with multiple files possible) to evaluate the model built. The same format.
• --data_cols= The description of the input files data columns. The format is --data_c
ols=numcols:xstart,xend:f:w:label, where numcols is the total number of columns
in the input file, xstart,xend are the columns to be used for xk , e.g. the columns (
xstart,xstart+1,...,xend-1,xend) are used as the (x0 , x1 , . . . , xk , . . . , xn−1 ) in (1)
input. The f and w are the columns for class label f and weight ω, if weight column
index w is set to negative then all weights ω are set to 1. The label is column index of
observation identification string (uniquely identifies a data row in the input data file,
a typical identification is: row number 12345, x × y image pixel id 132x15, customer
id johnsmith1990, etc.), it is copied without modification (or set to ?? if label is
negative) from input data file to the first column of output file. All column identifiers
are integers, base 0 column index. For example input file dataexamples/runge_fun
ction.csv of Appendix C has 9 columns, the xk are in the first 7 columns, then f
and ω columns follow, the x1 is used as observation string label of input file row. This
corresponds to --data_cols=9:0,6:7:8:1
• --clusters_number= The value of D. If presents Section III optimal clustering is
performed with this D and the output is of this dimension. Otherwise all n input
components are used to construct the ψ [i] (x) from (6) and the dimension of the output
is the rank of hxj | xk i matrix.
• --regularization_method= Data regularization method to be used, possible values:
NONE, EV (default), and LIN, see Appendix A for algorithms description.
• --max_multiindex= The value of D. If presents then N (n, D) “product” attributes
k

n−1
X0k0 X1k1 . . . Xn−1
are constructed (47) in regularized basis (using recursive algorithm)

with the multi–index k lower or equal than the D, these “product” attributes are then
used instead of n original attributes xk , see Section V above. For a large enough D
the problem may become numerically unstable. For N (n, D) ≥ 500 used eigenvalue
routines may be very slow15 . The option is intended to be deployed together with --cl
usters_number= with the goal to obtain a model of a “first order logic” type.
15

For eigenproblem routines one can use JNI interface com/polytechnik/lapack/Eigenvalues_JNI_lapac
ke.java to LAPACK instead of java code, see com/polytechnik/utils/EVSolver.java for selection.

47
• --flag_print_verbosity= By default is 2. Set --flag_print_verbosity=1 to suppress the output of ψ [i] (x(l) ) values or set --flag_print_verbosity=3 to output the
P
(l)
[i]
projections xk ψ [i] in expansion xk = n−1
ψ [i] (x(l) ). Usefult for obtaining
i=0 xk ψ
(l)

LRR Xk matrix (113) from printed ψ [i] (x(l) ) values.
• --flag_replace_f_by_christoffel_function= By default is false. If set to true
then, after regularization of the Appendix A, the Christoffel function (10) is calculated
for every observation and used instead of f ; datafile read values of f are discarded.
Useful for unsupervised learning. While mathematical result does not depend on f , the
specific basis used may affect numerical stability because of initial regularization; in
this situation a good heuristic is to use observation number as the f , this removes class
label degeneracy and makes the basis more stable.
• --flag_assume_f_is_diagonal_in_christoffel_function_basis= By default is
E
[i]
false. If set to true then f is considered to be diagonal in ψK basis (35). Sampled
D
E
[j]
[k]
matrix hxj | f | xk i is converted to ψK f ψK , all off–diagonal elements are removed,
E
[i]
then the matrix diagonal in ψK basis is converted back to xi basis. This can be viewed
ED
n−1
P [i] E D [i]
[i]
[i]
as [14] type of transform: kf k ≈
ψK
ψK f ψK
ψK . This is an experimental
i=0

option to vector class label classification problem of Section VI A.
• --output_files_prefix= If set all output files are prefixed by this string. A typical
usage is to save output to some directory, such as --output_files_prefix=/tmp/.
The program reads the data, builds Radon–Nikodym model from --data_file_to_
build_model_from= then evaluates it on itself and on all --data_file_evaluation=
files. The output file has the same filename with the .RN.csv extension appended. In the
comments section it prints data statistics (filename, observations number, and the Lebesgue
quadrature (15)). Column data description is presented in the column header. Every output
row corresponds to an input file row. An output row has a number of columns. The first
column is observation string label, then n + 2 columns follow: observation original input
attributes xk , observation class label f , and observation weight ω. Calculated data is put
into additional columns of the same row. The columns are: f_RN (13), f_LS (8), Christoffe
l (14), f_RNW (18) Coverage (19), and, unless --flag_print_verbosity=1, the ψ [i] (x(l) ) (6)
D components. Here the D is either the rank of hxj | xk i matrix, or the parameter --cluste

48
rs_number= if specified. For all output files the following relations are held for the columns:
D−1
P

f_RN(l) =


2
f [i] ψ [i] (x(l) )

i=0
D−1
P

(B1)
2
[ψ [i] (x(l) )]

i=0

Christoffel(l) =

1
D−1
P
i=0
D−1
P

f_RNW(l) =

(B2)

2
[ψ [i] (x(l) )]


2
f [i] w[i] ψ [i] (x(l) )

i=0
D−1
P

(B3)
w[i]

2
[ψ [i] (x(l) )]

i=0
D−1
P


2
w[i] ψ [i] (x(l) )

Coverage(l) =

i=0
D−1
P

(B4)
2
[ψ [i] (x(l) )]

i=0

For the file the model is built from (learning data) a few additional relations are held
(i, m = 0 . . . D − 1):
"
w

[m]

=

f [m] δim =
δim =

M
X

#2
ψ

[m]

(l)

(x )ω

(B5)

(l)

l=1
M
X
l=1
M
X

ψ [i] (x(l) )ψ [m] (x(l) )f (l) ω (l)

(B6)

ψ [i] (x(l) )ψ [m] (x(l) )ω (l)

(B7)

l=1
[m]

Obtained D states ψ [m] (x) (for D < rank of hxj | xk i these are the ψG (x) from (30), w[m] =
[m]

[m]

wG from (33), and f [m] = λG ) provide the optimal clustering of class label f among all
D–point discrete measures.

1.

Software Installation And Testing

• Install java 11 or later.
• Download the source code code_polynomials_quadratures.zip from [29].
• Decompress and recompile the program. Run a selftest.

49
unzip code_polynomials_quadratures.zip
javac -g com/polytechnik/*/*java
java com/polytechnik/utils/TestDataReadObservationVectorXF
• Run the program with bundled deterministic data file (Runge function (C2)).
java com/polytechnik/utils/RN --data_cols=9:0,6:7:8:1 \
--data_file_to_build_model_from=dataexamples/runge_function.csv \
--data_file_evaluation=dataexamples/runge_function.csv
Here, for usage demonstration, we evaluate the model twice. The file runge_function.
csv.RN.csv will be created (the same file is written twice, because the built model
is then test–evaluated on the same input dataexamples/runge_function.csv). See
Appendix C below for interpolation results obtained from the output.
• Run the program with the constructed ψ [i] (x(l) ) (6) as input. They are in the columns
with the index 15 to 21 of the file runge_function.csv.RN.csv (22 columns total).
java com/polytechnik/utils/RN --data_cols=22:15,21:8:9:0 \
--data_file_to_build_model_from=runge_function.csv.RN.csv
The file runge_function.csv.RN.csv.RN.csv will be created. Because the input xk
are now selected as ψ [k] (x), with this input, the Radon–Nikodym approach of Section
II produce exactly the input xk as the result ψ [k] (x), possibly with ±1 factor. There
are 7 nodes/weights of the Lebesgue quadrature (15) for input data file dataexamples
/runge_function.csv:
f [0] = 0.042293402383175485 w[0] = 0.2453611587632685
f [1] = 0.043621284685679745 w[1] = 0
f [2] = 0.06535351052058812

w[2] = 0.5222926033815862

f [3] = 0.07864169617926474

w[3] = 0

f [4] = 0.16469273913045052

w[4] = 0.6710343400073819

f [5] = 0.28493524789476266

w[5] = 0

f [6] = 0.7025238747369117

w[6] = 0.5613118978475747

(B8)

50
Some of the Lebesgue weights are 0. This may happen with (15b) definition. The
weights sum is equal to total measure, for (C3) it is equal to 2.
• The dimension of the Lebesgue quadrature is n, it is the number of input attributes xk .
When we start to increase the n, the Lebesgue quadrature starts to partition the x
space on smaller and smaller elements. The (13) type of answer will eventually start
to exhibit data overfitting effect. Radon–Nikodym is much less prone to it than a
direct expansion of f in xk , a (3) type of answers, but for a large enough n even the
hf ψ 2 i / hψ 2 i type of answer is starting to overfit the data. We need to select D ≤ n
linear combinations of xk that optimally separate the f . Optimal clustering is described
in Section III. Run the program

java com/polytechnik/utils/RN --data_cols=9:0,6:7:8:1 \
--data_file_to_build_model_from=dataexamples/runge_function.csv \
--clusters_number=4
Running with --clusters_number equals to 5, 6, or 7 may fail to construct a Gaussian
quadrature (28c) as the number of the measure (26) support points should be greater
or equal than the dimension of Gaussian quadrature built on this measure. For --c
lusters_number=4 the obtained quadrature gives exactly the (B8) nodes with zero
weights removed: the optimal approximation of the measure with four support points
by a four points discrete measure is the measure itself.
f [0] = 0.04229340238319568 w[0] = 0.24536115876382128
f [1] = 0.065353510520606
f

[2]

= 0.1646927391304516

f [3] = 0.7025238747369116

w[1] = 0.5222926033810373
w

[2]

(B9)

= 0.6710343400073585

w[3] = 0.5613118978475746

A more interesting case is to set --clusters_number=3

java com/polytechnik/utils/RN --data_cols=9:0,6:7:8:1 \
--data_file_to_build_model_from=dataexamples/runge_function.csv \
--clusters_number=3

51
f [0] = 0.0553329558917533

w[0] = 0.737454390130916
(B10)

f [1] = 0.16285402990411255 w[1] = 0.701183615381193
f [2] = 0.7025131758981266

w[2] = 0.5613619944877021

The (B10) is the optimal approximation of the measure (B8) with 4 support points by a
3–point discrete distribution, this is a typical application of Gaussian quadrature. The
n–point Gaussian quadrature requires 0 . . . 2n − 1 distribution moments for calculation,
the measure must have at least n support points. The distribution moments of f can
be obtained using a different method, for example using the sample sum (7) directly. A
remarkable feature of the Lebesgue integral measure (26) is that obtained eigenvectors
[m]

(28e) can be converted from f to x space. The conversion formula is (30). The ψG (x),
m = 0 . . . D − 1 create the weights, that optimally separate f in terms of hf ψ 2 i / hψ 2 i
separation. This is a typical setup of the technique we developed:
– For a large number n of input attributes create the Lebesgue integral quadrature
(15).
– Select the number of clusters D ≤ n. Using Lebesgue measure (26) build Gaussian
quadrature (28) in f space. It provides the optimal clustering of the dimension D.
[m]

– Convert obtained results from f to x space using (30), obtain the ψG (x) classifiers.
– One can also entertain a first order logic –like model using the attributes of Section
V.
[m]

• The three function ψG (x), corresponding to (B10) nodes, are presented in Fig. 3. The
[m]

Proj[i] (x) (this is squared and normalized ψG (x) as (22)). One can clearly see that
[m]

the states ψG (x) are localized exactly near the f [m] nodes (B10). This technique is a
much more powerful one, than, say, support–vector machine linear separation. In the
h
i2
[m]
Radon–Nikodym approach the separation weights are the ψG (x) that are obtained
without an introduction of a norm with subsequent minimization the difference between
the result and a prediction with respect to the norm. The separation by the functions
[m]

ψG (x) is optimal among all D– dimensional separations of [ψ(x)]2 type. The cost is
that the solution is now two–step[3]. On the first step the Lebesgue quadrature is built
and the measure (26) is obtained. On the second step the Gaussian quadrature (28) is

52

1
0.9
0.8
0.7
0.6
0.5
0.4
f(x)
Proj (x)
Proj[1](x)
Proj[2](x)

0.3

[0]

0.2
0.1
0
-1

-0.8

-0.6

-0.4

-0.2

0

0.2

0.4

0.6

0.8

1

FIG. 3. Runge function (C2) data (C1) clustered to D = 3. Corresponds to (B10) data. The
[m]

projections (22) to ψG (x), m = 0 . . . D − 1 are presented.

h
i2
[m]
built on this measure; the result is then converted to x space (30). The ψG (x) are
the optimal separation weights.

2.

Nominal Attributes Example

In ML applications the attributes (1) can be nominal. They may be of orderable (low,
medium, high) or unorderable (apple, orange, tomato) type. A nominal attribute taking two
values can be converted to {0, 1} binary attribute. Orderable attributes (low, medium, high)
can be converted to {1, 2, 3}, or, say, {1, 2, 10} this depends on the problem. For unorderable
attributes the conversion is more difficult, however in some situations it is straightforward: a
“country” attribute taking the value: “country name from a list of eight countries”, can be
converted to three binary attributes.
The f , predicted by a ML system, is called class label. It is often a binary attribute. This
leads to the nodes (15a) of the Lebesgue quadrature to be grouped near two values of the class
label. We have tested a number of datasets from UC Irvine Machine Learning Repository,
Weka datasets, and other sources. For direct comparison with the existing software such

53
as C5.0 or Weka 3: Machine Learning Software in Java a care should be taken of nominal
attributes conversion and class label representation. We are going to discuss the details in a
separate publication, here we present only qualitative aspects of Radon–Nikodym approach
application to ML problem with the binary class label. Take breast-cancer-wisconsin database,
the breast-cancer-wisconsin.data dataset[30] is of 699 records, we removed 16 records
with unknown (“?”) attributes and split the dataset as 500:183 for training:testing. Obtained
files are
wc breast-cancer-wisconsin_S.names \
breast-cancer-wisconsin_S.data \
breast-cancer-wisconsin_S.test
139

938

500

500 14266 breast-cancer-wisconsin_S.data

183

183

822

6234 breast-cancer-wisconsin_S.names

5182 breast-cancer-wisconsin_S.test

1621 25682 total

The data has nominal class label 2:Benign, 4:Malignant. C5.0, when run on this dataset
produces a very good classifier:
c5.0

-f mldata/breast-cancer-wisconsin_S

Evaluation on training data (500 cases):
(a)

(b)

<-classified as

----

----

293

10

(a): class 2

3

194

(b): class 4

Evaluation on test data (183 cases):
(a)

(b)

<-classified as

----

----

139

2

(a): class 2

4

38

(b): class 4

Now let us run the RN program to obtain the Lebesgue quadrature
java com/polytechnik/utils/RN --data_cols=11:1,9:10:-1:0 \
--data_file_to_build_model_from=mldata/breast-cancer-wisconsin_S.data \

54
--data_file_evaluation=mldata/breast-cancer-wisconsin_S.test
The number of the nodes is 10, it is equal to the number of input attributes xk .
f [0] = 2.090917684500027

w[0] = 308.30166232236996

f [1] = 3.198032991602546

w[1] = 5.307371268658678

f [2] = 3.344418191526764

w[2] = 0.0189894231470068

f [3] = 3.5619620739712725 w[3] = 0.3341989402039986
f [4] = 3.6221628167395497 w[4] = 0.2549558854552573
f [5] = 3.7509806530824346 w[5] = 1.2339290581894928

(B11)

f [6] = 3.7939096228600513 w[6] = 5.146789024450902
f [7] = 3.8081118648848045 w[7] = 0.16082536035874645
f [8] = 3.8799894340830727 w[8] = 50.25004460556501
f [9] = 3.9574710127612613 w[9] = 128.99123411160124
Then we calculate a joint distribution of realization/prediction for fRN and fRN W . The
continuous to nominal conversion for fRN and fRN W was performed by comparing predicted
value with the average. Evaluation without clustering on training data (B12) (500 cases),
and on test data (B13) (183 cases) is:
Distribution(fRN ) :

183 120

Distribution(fRN W ) :

0 197
Distribution(fRN ) :

91 50
0 42

294 9
13 184

Distribution(fRN W ) :

140 1
0 42

(B12)

(B13)

We see that fRN that equally treats the states with low and high prior probability often gives
spurious misclassifications. In the same time the fRN W that uses the projections adjusted to
prior probability gives a superior prediction.
When we cluster to D = 2:
java com/polytechnik/utils/RN --data_cols=11:1,9:10:-1:0 \
--data_file_to_build_model_from=mldata/breast-cancer-wisconsin_S.data \
--data_file_evaluation=mldata/breast-cancer-wisconsin_S.test \
--clusters_number=2

55
f [0] = 2.09463398432689

w[0] = 310.52326905818705

f [1] = 3.924320437715293 w[1] = 189.47673094181317

(B14)

The evaluation with D = 2 clustering on training data (B15) (500 cases) and on test data
(B16) (183 cases) gives joint distribution of realization/prediction for fRN and fRN W :
Distribution(fRN ) :

292 11

Distribution(fRN W ) :

7 190
Distribution(fRN ) :

295 8
13 184

141 0

Distribution(fRN W ) :

0 42

141 0
1 41

(B15)

(B16)

Now, after the states with low prior probabilities (17a) are removed, both fRN and fRN W
exhibit a good classification. For D = 3, however, we still get a type of (B12) and (B13)
behavior of spurious misclassifications by fRN and no such behavior in fRN W .
This makes us to conclude that the fRN W answer is the superior answer for predicting a
probabilistic f . The posterior distribution (17b) is Radon–Nikodym alternative to Bayes.

Appendix C: RN Program Application With A Different Definition Of The
Probability

Besides a typical ML classification problem the RN Program can be used for a number of
different tasks, e.g. it can be applied to an interpolation problem. The reason is simple: as an
input Radon–Nikodym only needs (7) matrices Fjk and Gjk , which are calculated from (1)
sample, that is a file of M rows and n + 2 columns (n for xk and two for f and the weight ω).
In the Appendix B the probabilities (7) were obtained as an ensemble average, calculated
from the data, this is typical for a ML classification problem.
Input file can be constructed in a way that calculated averages represent a probability
of different kind, such as time average probability. Consider function interpolation problem,
R
the h·i now has a meaning of time–average hgi = g(x)ω(x)dx, see Section II of [13]. A one–
dimensional interpolation problem[7] can be reduced to (1) data by converting a two–columns
sequence x(l) → f (l) , l = 1 . . . M to:
(1, x, x2 , . . . , xn−1 )(l) → f (l)

weight ω (l)

(C1)

Because the result is invariant relatively any non–degenerated basis components linear
transform any polynomials (e.g. Pm (x), Tm (x), etc.) can be used instead of the xm in (C1).

56

1

f(x)
fRN(x)
fRNW(x)
K(x)
Coverage(x)

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
-1

-0.8

-0.6

-0.4

-0.2

0

0.2

0.4

0.6

0.8

1

FIG. 4. Runge function (C2) interpolation result for n = 7. The input data (1) was prepared (C1)
in a way the classification problem solver from Appendix B to reproduce interpolation results of the
Appendix D of [13]. The fRN W (x) (18) (olive), Christoffel function (blue) (14), and the Coverage(x)
R1
(sky) (19) for the measure hgi = −1 g(x)dx (C3) are also calculated.

For example: to reproduce Runge function d = 1 interpolation problem
f (x) =

1
1 + 25x2

dµ = dx

(C2)
(C3)

x ∈ [−1 : 1]
for n = 7, the result of the Appendix D of [13], take x sequence with a small step
about dx = 10−4 , it will be about M = 1 + 2/dx total points x ∈ [−1, −1 + dx, −1 +
2dx, . . . , 1 − 2dx, 1 − dx, 1] and create a comma–separated file of M rows and n + 2 columns:
1, x, x2 , . . . , xn−1 , f (x), ω. First n columns are the x from (C1), then f (x) from (C2) follows,
and the last column is the observation weight ω = dx for all points except the dx/2 for
the edges. This file dataexamples/runge_function.csv is bundled with provided software.
Run the program
java com/polytechnik/utils/RN --data_cols=9:0,6:7:8:1 \

57
--data_file_to_build_model_from=dataexamples/runge_function.csv
The output file runge_function.csv.RN.csv has a few more columns, four of them are: the
fRN from (13), the Christoffel function (14), the fRN W from (18), and the Coverage(x) (19).
The result is presented in Fig. 4. With the data prepared as (C1) the Christoffel–like function
(14) is the regular Christoffel function for the measure (C3). The fRN W (x) is also presented in
Fig. 4. The fRN W (x), same as the fRN (x), is a weighted superposition (18) of (4) eigenvalues,
but the weights are the posterior weights (17b), that are the product of prior weights by
the ψ [i] projections: w[i] Proj[i] . For Runge function in n = 7 case only four prior weights
(B8) are non–zero, thus in Fig. 4 the fRN W (x) is a superposition of four eigenvalues. As we
discussed above in Section II A, the fRN (x) should be used for a deterministic functions, and
the fRN W (x) is a solution to classification problem for a probabilistic f ; it uses the posterior
weights (17b). Same result can be also obtained using multi–index multiplications of Section
V, take a single x attribute and multiply it by itself 6 times. The quadrature will be identical.
java com/polytechnik/utils/RN --data_cols=9:0,1:7:8:1 \
--max_multiindex=6 \
--data_file_to_build_model_from=dataexamples/runge_function.csv
Radon–Nikodym interpolation [20] of an image (d = 2 problem), can be performed in
a similar way. Create a file of M = dx × dy rows and n = nx × ny + 2 columns. Each
row corresponds to a single pixel. The last two columns are: pixel gray intensity and the
weight (equals to 1). The first n = nx × ny columns are a function of pixel coordinate
l
l
(xl ∈ 0 . . . dx − 1, yl ∈ 0 . . . dy − 1) as Tjx (2 dxx−1
− 1)Tjy (2 dyy−1
− 1), jx = 0 . . . nx − 1,

jy = 0 . . . ny − 1. The Tm (x) is Chebyshev polynomial T0 = 1; T1 = x; . . . , they are chosen for
numerical stability. In [20] the multi–index j = (jx , jy ) has (53) and (54) constraints. After
running the RN Program interpolated fRN and Christoffel function columns are added to
output file, the fRN (xl , yl ) provides required interpolation. While the Gaussian quadrature
cannot be obtained for d ≥ 2, the Christoffel function (10) can be easily calculated not only
in d ≥ 2 case, but also for an arbitrary x space with a measure h·i.
The input file can be also constructed for x vector to represent a random variable. For
example a distribution regression problem where a “bag” of observations is mapped to a
single outcome f can be approached[23] by using the moments of the distribution of a single
“observations bag” as an input x. For every “bag”, calculate it’s distribution moments (one

58
can use any choice of polynomials), then put these moments as x (now the xk components
are the moments of the distribution of a bag’s instance), and use the f as the outcome.
Similarly, temporal dependencies can be converted to (1) type of data. Assume f has
a f (x(t)) form. Then each xk (t) can be converted to the moments hQs (xk )it , s = 0 . . . nt ,
relatively some time–averaging h·it measure, such as in the Section II of [13]. Then the
n × nt input attributes hQs (xk )it , k = 0 . . . n = 1; s = 0 . . . nt − 1, are “mixed” moments: time
averaged h·it first and then ensemble averaged in (7). They can be used in (1) data input.
Note, that “combined” averaging in (7) as hQs (xj (t)) | Qs0 (xk (t))it produces different result
than “mixed” one: hQs (xj (t))it hQs0 (xk (t))it . Numerical experiments show that hQs (xk )it
attributes typically show a better result than using (xk (t), xk (t − δ), xk (t − 2δ), . . . ) as a
“vectorish” xk . With temporal (and spatial) attributes the dimension of (1) input can grow
very fast. In such a situation Section III optimal clustering is of critical importance: this way
we can select only a few combinations of input attributes, that optimally separate the f .
The strength of the Radon–Nikodym approach is that it requires only two matrices (7)
as an input, and the average h·i, used to calculate the Fjk and Gjk , can be chosen with a
different definition of the probability. The input file (--data_file_to_build_model_fro
m= parameter) can be prepared in a form to represent any probability space in any basis
of any dimension. One row corresponds to a single realization, all rows correspond to the
entire sample. After input datafile is prepared for the chosen probability space — the features
[m]

introduced in this paper fRN (x), K(x), fRN W (x), Coverage(x), along with ψG (x) clusters
(30) are calculated by the provided software.

Appendix D: A Numerical Solution to a Quadratic Form Maximization Problem in
Unitary Matrix Space

Consider a constrained optimization problem (127)
F=

n−1
X

Ujk Sjk;j 0 k0 Uj∗0 k0 −→ max
U

j,k,j 0 ,k0 =0
n−1
X

∗
Ujk0 Ukk
0 = δjk

(D1)
(D2)

k0 =0

This is a problem of optimization of a scalar function (quadratic form with a Hermitian tensor
Sjk;j 0 k0 from (129)) on the unitary group U (n). It is equivalent to a problem of maximizing a

59
quadratic form with a Hermitian matrix given multiple constraints (D2) of quadratic form
as well. When contributing subspace is known explicitly – the constraint may be of more
general form (G6); a slight algorithm modification is then required. A regular eigenvalue
problem has a single quadratic form constraint, the problem in question has multiple. We
have already approached a problem with an extra quadratic form constraint in the Appendix
F of [9], the problem in question is of this type. Consider a “simplified constraint” (130)
n−1
X

(D3)

∗
Ujk Ujk
=n

j,k=0

as a “partial” constraint for which optimization problem (D1) can be readily converted to
an eigenvalue problem to be directly solved. The idea is then to adjust obtained solution to
satisfy full unitary constraints and calculate new values for Lagrange multipliers. Performing
several iterations the process will converge to (D1) optimization problem solution with the
required constraints (D2).
Consider Lagrange multipliers λjk to optimize (D1) with the constraints (D2)
"
#
n−1
n−1
n−1
X
X
X
∗
Ujk Sjk;j 0 k0 Uj∗0 k0 +
λjk δjk −
Ujk0 Ukk
−→ max
0
j,k,j 0 ,k0 =0

k0 =0

j,k=0

U

(D4)

and variate it over all Ujk components. There are 2n2 real number coefficients defining
Ujk = ajk + ibjk , only n2 of them are independent for a unitary matrix. One more coefficient
is dropped as a common phase, so (D1) optimization with the constraints (D2) is equivalent
to an unconstrained optimization problem over n2 − 1 independent real parameters.
∗
It is typically more convenient to variate (D4) over Ujk and Ujk
rather than over ajk and

bjk , then take care of the constraints by adjusting Lagrange multipliers λjk . The variations
0=

n−1
X

U

j 0 k0

S

j 0 k0 ;pq

−

j 0 ,k0 =0

0=

n−1
X

n−1
X

λj 0 p Uj 0 q

(D5a)

λpj 0 Uj∗0 q

(D5b)

j 0 =0

Spq;j 0 k0 Uj∗0 k0

−

j 0 ,k0 =0

n−1
X
j 0 =0

are consistent only when λjk is a Hermitian matrix
λjk = λ∗kj

(D6)

From (D5) also immediately follows: the functional (D1) extremal value is equal to the spur

60
of λjk :
F

(extr)

=

n−1
X

λjj

(D7)

j=0

An algorithm finding extremal (D1) is a generalization of the one from the Appendix F of
[9] to multiple constraints:
1. Take initial λjk and solve (D4) optimization with simplified constraint (D3). Solution
method – an eigenvalue problem of n2 dimension in a vector space formed by writing
all Ujk matrix elements in a vector, row by row. The result is: F and Ujk matrix
reconstructed back from the eigenvector corresponding to maximal eigenvalue, row by
row.
2. Obtained from this solution matrix Ujk may not be unitary as the constraint (D3) is a
subset of the full one (D2). Expand Ujk in SVD
Ujk =

n−1
X

Ujj 0 Σj 0 k0 Vk†0 k

(D8)

j 0 ,k0 =0

Uejk =

n−1
X

Ujj 0 Vj†0 k

(D9)

j 0 =0

and adjust all SVD numbers to 1: Σjk = δjk , obtained Uejk is a unitary matrix, it is the
next iteration of the solution. This matrix (D9) satisfies exact constraint (D2), but the
value of F is now increased. The Uejk becomes a new Ujk at this iteration.
∗
3. Put this new Ujk to (D5a), then multiply it by Ujq
and sum over q = 0 . . . n − 1. As
Pn−1
∗
the Ujk is unitary λjk = p,q=0 λjp Upq Ukq
obtain new values for Lagrange multipliers
ejk and take it’s Hermitian part:
λ

ejk =
λ

n−1
X

∗
Uj 0 k0 Sj 0 ,k0 ;kq Ujq

(D10)

j 0 ,k0 ,q=0

λjk =

i
1 he
e∗
λjk + λ
kj
2

(D11)

ejk
This λjk is the next iteration of Lagrange multipliers. As iterations proceed – the λ
should converge to a Hermitian matrix by itself, without (D11) required.
4. Put this new λjk to (D4) and repeat iterational process until converged. On the first
iteration take initial values for Lagrange multipliers as λjk = 0.

61
Appendix E: Non–Unitary Dynamics

In the previous section an approach to numerical solution of optimization problem (D1)
with unitary constraint (D2) has been developed. Whereas for quantum systems time evolution
operator Ujk can be only unitary, in data analysis it can possibly be of a non–unitary form.
The difference arises because in data analysis wavefunction is directly “observable” with the
goal to construct a “time evolution operator” (120).
The first non–unitary matrix of this type to consider is the (D3), having the sum of squared
elements equal to n. With this matrix the problem can be easily solved. It does not preserve the
normalizing, but gives more weight to correctly matched predictions. Regardless interpretation
difficulties the dynamics with a matrix constrained the sum of squared elements being equal
n is the first one to try for the reasons of computational simplicity (no iterational process
required) and mathematical interpretation simplicity (eigenvalue problem equivalence).
Another matrix of interest is a subspace-projection matrix. This type of constraint typically
makes Lagrange multipliers λjk calculation problematic, however some results can be obtained
analytically, what makes a subspace-projection matrix the first one to try for an analytic
study.
In the considered above approach to dynamics the x(l) and x(l+1) were belong to the same
phase space. It is of great interest to consider a situation where |ψx(l) i and |ψx(l+1) i belong to
different vector spaces, e.g. to use |ψf (l) i instead of |ψx(l+1) i. In this case in (120) operator U
is transforming |ψx i to a different vector space |ψf i; this is not a true “dynamics” (l is the
same), but such a transform can be applied to a traditional ML classification problem.
While a study of a general non–unitary x → f homomorphism producing the most
general form of non–unitary dynamics is definitely out of scope of this work, let us consider
a simple composition of a unitary transformation U: x → x followed by projection of x
on f , a “projective” form of non–unitary dynamics16 . Let us apply it to a vector–to–vector
classification problem of Section VI B. Assume we have a problem with vector–valued class
16

Similar composition of a unitary transformation f → f followed by transform projection on x can be
constructed in exactly the same way; it looks, however, much less attractive. For isomorphic f -space and
x-space (e.g. considered in Section VI E above) the projection retains the full basis, thus f on x and x on f
inferences produce evolution operators U in (120) different only in time inverse. A promising direction for
future research may be to consider two unitary transformation: U x acting x → x and U f acting f → f
then do transforms projection, see Appendix F below.

62
label (62)
weight ω (l) ; l = 1 . . . M

x(l) → f (l)

(E1)

The choice of knowledge representation is the most important feature of a ML approach. For
example it can be a linear regression (84), a ratio of two quadratic forms (66) or (68), neural
network weights, etc. An important result of this appendix is to consider not x → f mapping,
but instead to construct localized wavefunctions (24) in x- and f - space: ψy (x) and ψg (f )
to study ψy (x) mapping with a unitary operator U in x-space following a projection of the
transform |U|ψy i on f -space outcome ψg (f ):
Prob(g|y) = |hψg | U | ψy i|2
F=

M
X

2

|hψf (l) | U | ψx(l) i| ω

1 ≥ $(g) ≥ Prob(g|y)
(l)

l=1

=

M
X

(E2)
(E3)

Prob(f (l) |x(l) )ω (l)

l=1

(E4)

Error = h1i − F

Conditional probability (E2) is bounded by the value $(g) of full basis expansion (91), a
situation without predictor available, this is the problem we considered in Section VI B above.
Because x- and f - space are different – a projection of a wavefunction from one to another
gives 1 ≥ $(g) ≥ |hψg | U | ψy i|2 in (E2). This non–unitarity, however, does not create any
practical difficulties as we separated a “unitary dynamics” in x-space and a “non–unitary
projection” to f -space. The (E4) error estimator has the meaning of misclassified observations
number, it is bounded by considered above simple projective estimator (89); it is zero if f is
a subspace of x (in (E10) below consider Ψ as a direct sum of Φ and the space orthogonal to
Φ, then in (E12) numerator cancels denominator).
Given the expressions (24) for ψy (x) and for ψg (f ):
m−1
P

gj Gfjk; −1 fk

j,k=0

ψg (f ) = s

m−1
P

(E5)
gj Gfjk; −1 gk

j,k=0

here Gfjk; −1 is an inverse of Gfjk from (79), we can write conditional probability (E2) as:

Prob(g|y) =

n−1
P

m−1
P

j,k,p=0

j 0 ,k0 =0

n−1
P
j,k=0

2
−1
f ; −1
xf
yj Gx;
jk ukp Gpj 0 Gj 0 k0 gk0

−1
yj Gx;
jk yk

m−1
P
j 0 ,k0 =0

(E6)
gj 0 Gfj 0;k−1
0 gk 0

63

|U|xk i =

n−1
X

(E7)

ukp xp

p=0

The expression is very similar to (99), the difference is that instead of Gxf
kj 0 we now have x
Pn−1
transformed by a unitary operator U as p=0 ukp Gxf
pj 0 . This is the key difference: instead of
“direct projection” we now have a unitary transformation and then a projection. In
M
X

F=

Prob(f (l) |x(l) )ω (l) =

l=1

n−1
X

(E8)

ujk Sjk;pq u∗pq

j,k,p,q=0

a Hermitian tensor Sjk;pq is readily obtained from (E6) and (E8) with simple algebra. Thus
we reduced x → f classification problem to a dynamic problem of finding a unitary matrix
maximizing (E8), i.e. the problem considered in Section D! This is the most general solution to
a vector class label classification problem, it finds a unitary transformation U (E7), producing
the maximal coverage in (E8).
Note, that unitary operator U coefficients ukp are defined in (E7) in a general, non–
orthogonal basis xk , a one with a real symmetric Gram matrix Gxjk = hxj xk i. This makes
unitarity constraint more verbose:
Gxpq

=

n−1
X

(E9)

upj Gxjk u∗qk

j,k=0

It is convenient to select orthogonal bases Ψ[i] (x), i = 0 . . . n − 1 and Φ[j] (f ), j = 0 . . . m − 1
for input data, we already did this in Eq. (121) above:
[i]

Ψ (x) =

n−1
X

x
Bik
xk

i = 0...n − 1

(E10)

k=0
(l)
si

[i]

= ψx(l) Ψ

Ψ[i] (x(l) )

=s

n−1
P

1=
2

n−1
X

(l)

2

si

i=0

|Ψ[j] (x(l) )|

j=0
[p]

δpq = Ψ

[q]

Ψ

=

n−1
X

x
x
Bpj
Gxjk Bqk

p, q = 0 . . . n − 1

j,k=0

Φ[i] (f ) =

m−1
X

f
Bik
fk

i = 0...m − 1

k=0
(l)
di = ψf (l) Φ[i] = s

Φ[i] (f (l) )
m−1
P
j=0

1=
2

|Φ[j] (f (l) )|

m−1
X
i=0

(l)

di

2

(E11)

64

δpq = Φ

[p]

Φ

[q]

m−1
X

=

f
f
Gfjk Bqk
Bpj

p, q = 0 . . . m − 1

j,k=0

As the solution is gauge–invariant relatively (65) we can use any basis. An orthogonal
basis choice is also beneficial for computational complexity: it takes O(n) instead of O(n2 )
P
x; −1
x
to calculate a quadratic form n−1
j,k=0 yj Gjk yk in a basis in which Gjk is diagonal. The
Prob(Φ|Ψ) also takes much simpler form:

n−1
P m−1
P

Prob(f |x) = Prob(Φ|Ψ) =

2
[j]

Ψ

[i]
Ujk GΨΦ
ki Φ

j,k=0 i=0
n−1
P

2
|Ψ[j] |

m−1
P

j=0
[k] [i]
GΨΦ
=
ki = Ψ Φ

n−1 m−1
X
X

(E12)
2
|Φ[i] |

i=0

(E13)

x
f
Bkj
Gxf
jj 0 Bij 0

j=0 j 0 =0

Sjk;pq =

M
X
l=1

ω (l)

m−1
X

(l)

(l)

(E14)

(l) (l) ΨΦ
sj GΨΦ
kr dr sp Gqt dt

r,t=0

(l+1)

The (E14) corresponds to (126) when put formally sk

=

Pm−1
j=0

(l)

GΨΦ
kj dj and swap tensor

"

indexes (inverse time): S jk;pq = Skj;qp . A unitary operator U now has a matrix Ujk with regular
unitarity constraint (D2). As the result is basis–independent it is practically convenient to use
(l)

(l)

input data xk and fj to calculate the matrices (79) and (80), then build from them the bases
(E10) and (E11), with possible regularization of the Appendix A, then finally use Ψ[k] (x(l) )
and Φ[j] (f (l) ) as they were input data sample. In new bases the problem with Hermitian
tensor (E14) can be directly approached by (D1) optimization with unitary constraint (D2).
Obtained solution is independent on bases Ψ[k] and Φ[j] specific choice (gauge–invariant).
Solution dimension n can be reduced to m using clustering approach (G5) of Appendix G
below; in this case the contributing subspace is known explicitly thus the constraint may be
of a more general “partial unitarity” form (G6).
What is the main application of the approach of this appendix? Most often – it is a
“replacement” of a regression in a problem of recovering some hidden x → f relation. Both
theories take (E1) data as input and have zero error if f is a subspace of x. The differences
can be summarized in the table:

65
Regression
The Result

Optimization

“Dynamic” theory

Function value f (x) (84); diverges Conditional probability Prob(f |x)
at x → ∞

(E12); does not diverge at x → ∞

L2 norm (2) in f -space

The number of correctly classified observations (E3)

Mathematical

Linear system solution

Conditional optimization (D1) with
unitary constraint (D2)

problem

Outliers and fat Very sensitive; a single “several Not sensitive; a single outlier may
tail sensitivity

orders off” outlier completely in- invalidate only a single observation
validates the solution

point

Symmetry

Broken: observable is linear on x; Preserved: ψ is linear on x, but the

ψ → −ψ

ψ is also linear on x.

probability (E2) behaves as ψ 2 , invariant with: ψx → −ψx ; ψf → −ψf

Physical world

A model

Most of dynamic equations in na-

relation

ture are equivalent to a sequence
of unitary transformations (Newton,
Maxwell, Schrödinger equations)

Appendix F: A Projective Non–Unitary Dynamics

Considered in Section E projective dynamics consists in a unitary transformation of x
following a projection of the transform on f . The problem can be further generalized. Consider
input data (E1) as vector spaces x and f (it is convenient to convert them to Ψ and Φ of
Eqs. (E10) and (E11)). The Ψ and Φ are regular vector spaces of the dimensions n and
m with a scalar product determined by positively defined (otherwise apply Appendix A
regularization) matrices (79) and (80) calculated from the data sample (E1). In addition we
have a “cross–product” hΨ | Φi (E13) determined by the matrix Gxf
jk0 (81) calculated from the
same data sample. These bases may not be full with respect to each other:

1≥

m−1
X
j=0

Ψ[i] Φ[j]

2

i = 0...n − 1

(F1a)

66

1≥

n−1
X

Ψ[j] Φ[i]

2

i = 0...m − 1

(F1b)

j=0

In Section VI A we considered an approach of various Ψ ↔ Φ projections. In Appendix E we
considered a composition of a unitary transformation U Ψ Ψ → Ψ following a projection of
the transform on Φ. In this appendix we consider the most general case, a composition of:
1. A Ψ → Ψ unitary transformation U Ψ , the transform is U Ψ |Ψ .
2. A Φ → Φ unitary transformation U Φ , the transform is U Φ |Φ .
3. Projection of these two transforms on each other: Φ U Φ U Ψ Ψ using (E13) “scalar
product”.
The number of “covered” observations is then:
Prob(f |x) = Prob(Φ|Ψ) =
F=

M
X

Φ UΦ UΨ Ψ

Φf (l) U Φ U Ψ Ψx(l)

2

2

ω (l) =

l=1

(F2)
M
X

Prob(f (l) |x(l) )ω (l)

(F3)

l=1

These expressions are different from (E2) and (E3) in a second unitary transformation kU Φ k.
Ψ
Φ
The problem is then: Maximize (F3) over Ujk
and Ujk
given two unitary constraints:

δjk =
δjk =

n−1
X
i=0
m−1
X

Ψ∗
UjiΨ Uki

j, k = 0 . . . n − 1

(F4a)

Φ∗
UjiΦ Uki

j, k = 0 . . . m − 1

(F4b)

i=0

The optimization (F3) with the constraints (F4) can be approached by Appendix D type
Ψ Φ
of algorithm, however, as (F3) is a quadratic form over matrix elements products Ujk
Uqp (a

“two–particle” system wavefunction basis is a product of individual particles wavefunction),
this makes the problem of dimensions product, thus makes it impractical. We expect that
Ψ
Φ
a heuristic algorithm, such as alternately optimize (F3) over Ujk
and Uqp
, can be a better

fit. For isomorphic f -space and x-space (n = m and all coefficients in (F1) are equal to 1)
the dynamics is unitary and the problem itself becomes degenerated: It then depends on
a single operator kUk = kU Ψ |U Φ k what is equivalent to the problem already considered
in Section VI E. This makes us to conclude that considered in Section E composition: a
unitary transformation of Ψ following a projection of the transform on Φ is the most practical
approach to traditional ML classification problem x → f .

67
Appendix G: On Clustering of a Dynamic System Phase Space

In Appendix E a “projective” solution to dynamic system identification problem has been
developed. The solution has the form of a unitary operator kUk in x-space. Conditional
probability given possible input/output is determined by (E2) projection of x vector transform
to a vector in f -space (localized state). The dimension of x-space and f -space can be quite
different. The n is typically of hundreds, often thousands, for a system with internal state
(memory), see Appendix H below, it may reach millions. The m is the dimension of f , the
number of values of interest, it is always below a few dozen. From this relation naturally
arises the problem of clustering: to construct a low dimension D < n subspace of phase space
x that captures most of the information about f . For a problem with vector class label only
the case D = m is easy.
Consider some orthogonal basis ψ [i] in x-space and expand x(l) -localized states ψx(l) (x)
in this basis:
|ψx(l) i =

n−1
X

ψx(l) ψ [i]

(G1)

ψ [i]

i=0

then substitute to (E3), obtain the number of covered observations:
F=

=

M
X
l=1
M
X

|hψf (l) | U | ψx(l) i|2 ω (l)
ω

(l)

n−1
X

ψ [i] ψx(l)

ψ [i] U † ψf (l)

ψf (l) U ψ [j]

ψx(l) ψ [j]

(G2)

i,j=0

l=1

Were we operate in terms of simple “projective paradigm” of Section VI B this would
correspond to (94) error with (96) spectral expansion. Now, however, the problem is that
sought basis ψ [i] enters (G2) coverage four times, thus a direct eigenvalues expansion is no
longer possible. As the conditional probablities are bounded (E2) with direct projection to
entire x-space by probabilities (91), obtain F upper bound:
F

DP

=

M
X

$(f (l) )ω (l)

F ≤ F DP

(G3)

l=1

The spectral expansion (96) has at most m eigenvectors (95) contributing to coverage
expansion with |ψf (l) i, for (G2) this means that only these |φi contribute to coverage:
ψ [i] ∈ |U|φi

(G4)

68
where ψ [i] is (95) eigenvectors having non–zero eigenvalue, there are at most m out of total
n. From this follows that only vector space φ[i] contribute:
φ[i] = U † |ψ [i]

(G5)

where i takes m out of n values such that λ[i] > 0 in (95). The φ[i] is the only x-subspace
contributing to total coverage (G2).
Appendix D solution to maximization (G2) (which is a quality criteria) finds unitary
matrix kUk in x-space of dimension n. However, as quality criteria operates in f -space of
dimension m, the transform (G5) allows to build x-subspace of the dimension D = m as the
only contributing to quality criteria.
For a system with known contributing subspace numerical optimization algorithm of
Appendix D can be optimized by modifying unitarity constraint (D2) to
δjk = ψ [j] U −1† U −1 ψ [k]

j, k : λ[j] > 0; λ[k] > 0; total m values

(G6)

where λ[i] and ψ [i] are (95) problem eigenvalues/eigenvectors; this constraint means scalar
product invariance only for ψ [i] -subspace of x, not for full x-space as for unitary operators;
the condition U −1 = U † may no longer hold true: only (G6) and δjk = φ[j] U † U φ[k] are
satisfied for j, k from (G6). The solution with partial unitarity constraint (G6) instead of the
full one (D2) produces the same coverage (G2) as all non–unitary contributions are canceled
in the objective function.

Appendix H: The Dynamics of a System with Internal State

The data (E1) x(l) → f (l) is the form most frequently studied in ML, where observations
corresponding to different l are considered as independent observations. Same data studied
in signal processing typically considered as l–ordered (e.g. l is time), where the problem of
timeserie prediction corresponds to f (l) = x(l+1) . Such an embedding of timeserie data to (E1)
implicitly selects a time–scale. Real system have some internal state z (memory); the output
now depends not only on the input signals x, but also on the internal state z:

x(l) , z(l) → f (l)
weight ω (l) ; l = 1 . . . M

(H1)

This produces a omnifarious dynamics, much richer compared to systems without internal
state. An example of a system with memory is a finite-state machine. From practical point
of view it is convenient to classify them as the systems with:

69
• Completely observable internal state.
• Partially observable internal state.
The same system (e.g. a vending machine) can be completely observable to a support team
(have a full access to vending machine memory) and partially observable to a customer (can
only see whether it is empty and not working). In this appendix we will be only considering
the systems with completely observable internal state.
Consider a very simple finite-state machine: synchronous positive-edge-triggered D flip-flop
(D trigger); it’s circuit has a positive feedback loop what creates a bistable system. CD4013
chip is a typical example of this device.
D

C

Q

Q

(H2)

It operates as following: on every 0 → 1 transition on C (on the rising edge

of the clock)

input D is recorded and becomes immediately available on Q, the Q is it’s inverse. Any
changes on D has no effect on the state unless there is a rising edge on C:
C

D

Q

0

0

1

1

X

unchanged

(H3)

0
1

This device can be used as a 1-bit memory register, pulses counter, frequency divider by 2
(connect D with Q to inverse the state on every

on C), etc.

Consider a simple problem of the dimensions n = 2, m = 1. Take edge–triggered D flip-flop,
let x0 = D, x1 = C, and output f = Q. Also assume (to avoid timing considerations) that on
(l)

(l)

every tick l the x1 takes the value slightly after x0 was set. The output Q at l now depends
not only on current input x(l) but also on the previous state (and hence, previous inputs).
Now assume that all the input x(l) are completely random. For every new l–th input x(l)

70
coming (completely random) the system undergo transition:

f

(l)

=



x(l)
0

if x1


f (l−1)

otherwise

(l−1)

(l)

= 0 and x1 = 1

(H4)

It is clear that this D-trigger cannot be predicted by n = 2, m = 1 system corresponding to
D, C, Q trigger terminals “connected” to x0 , x1 and f . A system with (H4) transition rules
has a long–range dynamics17 .
A typical result of interest for the study of such a system is: given a long sequence of
random x(l) as input be able to tell: there is a D-trigger inside. It is clear that an approach
typical for signal processing: take a finite number of previous inputs x(l−1) , x(l−2) , x(l−3) , . . . ,
the length is determined by e.g. autocorrelation length of the signal, is poorly applicable to a
system with internal memory.
For a system with completely observable internal state the problem can be directly
approached by using f and some previous x (like in signal processing) as system memory: put


(l−1)
z(l) = f (l−1) , x1
in (H1), making a system of the dimensions n = 4, m = 1. Given this
input almost any ML technique can build an accurate predictor for D-trigger. The problem,
however, is that to apply obtained rules an information about system current internal state
is required and this information is typically not available. The approach of Appendix (E)
separates the system dynamics (in a form of unitary operator kUk obtained from (E3)
optimization) and calculation of conditional probability (E2) for a given input/output. When
applied to this problem only the first step is straightforward: construct a unitary operator of

dimension 4 in (H1) space that can be selected as a subspace of x(l) , f (l−1) , x(l−1) , x(l−2) , . . .
the transform then to be projected to f (l) ; the Error from (E4) will be 0. However, the second
step: it’s application to a prediction of future value of f is problematic as the “system current
state” is typically available only for training data. Nevertheless, obtained unitary operator

17

A more straightforward example of a system with long–range dynamics is the aforementioned frequency
divider by 2 (connect D with Q) and use x = C, f = Q; this single input system switches the state to
(l)

(l−1)

the inverted f (l+1) = f (l) for every x0 = 1 such that x0
determined by the initial state and the number of

= 0; this system has the state completely

transition on C input.

71
precisely identifies (H3) system dynamics and tells us exactly: there is a D-trigger inside!

[1] V. G. Malyshkin, On Lebesgue Integral Quadrature, ArXiv e-prints (2018), arXiv:1807.06007
[math.NA].
[2] V. G. Malyshkin, On Numerical Estimation of Joint Probability Distribution from Lebesgue
Integral Quadratures, ArXiv e-prints (2018), arXiv:1807.08197 [math.NA].
[3] V. G. Malyshkin, Multiple–Instance Learning: Christoffel Function Approach to Distribution
Regression Problem, ArXiv e-prints (2015), arXiv:1511.07085 [cs.LG].
[4] J.-B. Lasserre and E. Pauwels, The empirical Christoffel function with applications in data
analysis, Advances in Computational Mathematics , 1 (2019).
[5] B. Beckermann, M. Putinar, E. B. Saff, and N. Stylianopoulos, Perturbations of Christoffel–
Darboux Kernels: Detection of Outliers, Foundations of Computational Mathematics , 1 (2020).
[6] V. G. Malyshkin, Norm-Free Radon-Nikodym Approach to Machine Learning, ArXiv e-prints
(2015), http://arxiv.org/abs/1512.03219, arXiv:1512.03219 [cs.LG].
[7] A. V. Bobyl, A. G. Zabrodskii, M. E. Kompan, V. G. Malyshkin, O. V. Novikova, E. E.
Terukova, and D. V. Agafonov, Generalized Radon–Nikodym Spectral Approach. Application
to Relaxation Dynamics Study., ArXiv e-prints 10.2139/ssrn.3229466 (2016), arXiv:1611.07386
[math.NA].
[8] A. V. Bobyl, V. V. Davydov, A. G. Zabrodskii, N. R. Kostik, V. G. Malyshkin, O. V. Novikova,
D. M. Urishov, and E. A. Yusupova, The Spectral approach to timeserie bursts analysis
(Спектральный подход к анализу всплесков временной последовательности), ISSN 01315226.Теоретический и научно-практический журнал. ИАЭП. , 77 (2018).
[9] V. G. Malyshkin, Market Dynamics: On Directional Information Derived From (Time, Execution
Price, Shares Traded) Transaction Sequences., ArXiv e-prints (2019), arXiv:1903.11530 [qfin.TR].
[10] F. Mosteller and D. L. Wallace, Applied Bayesian and classical inference: the case of the
Federalist papers (Springer Science & Business Media, 1984).
[11] V. G. Malyshkin, R. Bakhramov, and A. E. Gorodetsky, A Massive Local Rules Search Approach
to the Classification Problem, arXiv:cs/0609007 (2001), cs/0609007.
[12] B. Beckermann, On the numerical condition of polynomial bases: estimates for the condi-

72
tion number of Vandermonde, Krylov and Hankel matrices, Ph.D. thesis, Habilitationsschrift,
Universität Hannover (1996).
[13] V. G. Malyshkin and R. Bakhramov, Mathematical Foundations of Realtime Equity Trading.
Liquidity Deficit and Market Dynamics. Automated Trading Machines., ArXiv e-prints (2015),
http://arxiv.org/abs/1510.05510, arXiv:1510.05510 [q-fin.CP].
[14] G. S. Malyshkin, The comparative efficiency of classical and fast projection algorithms in the resolution of weak hydroacoustic signals (Сравнительная эффективность классических и быстрых
проекционных алгоритмов при разрешении слабых гидроакустических сигналов), Acoustical
Physics 63, 216 (2017), doi:10.1134/S1063771017020099 (eng) ; doi:10.7868/S0320791917020095
(рус).
[15] M. H. Hayes and J. H. McClellan, Reducible polynomials in more than one variable, Proceedings
of the IEEE 70, 197 (1982).
[16] M. Nieto-Vesperinas, F. J. Fuentes, R. Navarro, and M. Perez-Ilzarbe, A FORTRAN routine to
estimate a function of two variables from its autocorrelation, Computer physics communications
78, 211 (1993).
[17] T. Becker and V. Weispfenning, Gröbner bases: Computational Approach to Commutative
Algebra, Vol. 141 (Springer, 1993) ISBN:978-0387979717.
[18] V. V. Nalimov and N. A. Chernova, Statistical Methods for Design of Extremal Experiments,
Tech. Rep. AD0673747 (FOREIGN TECHNOLOGY DIV WRIGHT-PATTERSON AFB OHIO,
1968).
[19] V. V. Nalimov, Theory of Experiment (Теория эксперимента) (Nauka, USSR, 1971).
[20] V. G. Malyshkin, Radon–Nikodym approximation in application to image reconstruction.,
ArXiv e-prints (2015), http://arxiv.org/abs/1511.01887, arXiv:1511.01887 [cs.CV].
[21] J.-B. Lasserre, Moments, positive polynomials and their applications, Vol. 1 (World Scientific,
2009).
[22] S. Marx, E. Pauwels, T. Weisser, D. Henrion, and J.-B. Lasserre, Tractable semi-algebraic
approximation using Christoffel-Darboux kernel, arXiv preprint arXiv:1904.01833 (2019).
[23] V. G. Malyshkin, Multiple-Instance Learning: Radon-Nikodym Approach to Distribution
Regression Problem, ArXiv e-prints (2015), arXiv:1511.09058 [cs.LG].
[24] A. Bourass, B. Ferrahi, B. M. Schreiber, and M. V. Velasco, A Random multivalued uniform
boundedness principle, Set-Valued Analysis 13, 105 (2005).

73
[25] B. Simon, Szegő’s Theorem and Its Descendants (Princeton University Press, 2011).
[26] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma, Robust recovery of subspace structures by
low-rank representation, IEEE transactions on pattern analysis and machine intelligence 35,
171 (2012).
[27] R. E. Kalman, A New Approach to Linear Filtering and Prediction Problems, Journal of Basic
Engineering 82, 35 (1960).
[28] T. A. Loring, Computing a logarithm of a unitary matrix with general spectrum, Numerical
Linear Algebra with Applications 21, 744 (2014).
[29] V. G. Malyshkin, (2014), the code for polynomials calculation, http://www.ioffe.ru/LNEPS/
malyshkin/code.html.
[30] O. L. Mangasarian and W. H. Wolberg, Cancer diagnosis via linear programming, Tech. Rep.
(University of Wisconsin-Madison Department of Computer Sciences, 1990).

