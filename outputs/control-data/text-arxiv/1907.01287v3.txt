Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood
Estimation for Stochastic Multi-Armed Bandits

arXiv:1907.01287v3 [cs.LG] 23 Oct 2020

Xi Liu * 1 Ping-Chun Hsieh * 2 Yu-Heng Hung 2 Anirban Bhattacharya 3 P. R. Kumar 1

Abstract
Inspired by the Reward-Biased Maximum Likelihood Estimate method of adaptive control,
we propose RBMLE – a novel family of
learning algorithms for stochastic multi-armed
bandits (SMABs).
For a broad range of
SMABs including both the parametric Exponential Family as well as the non-parametric
sub-Gaussian/Exponential family, we show that
RBMLE yields an index policy. To choose
the bias-growth rate α(t) in RBMLE, we reveal the nontrivial interplay between α(t) and
the regret bound that generally applies in both
the Exponential Family as well as the subGaussian/Exponential family bandits. To quantify the finite-time performance, we prove that
RBMLE attains order-optimality by adaptively
estimating the unknown constants in the expression of α(t) for Gaussian and sub-Gaussian bandits. Extensive experiments demonstrate that
the proposed RBMLE achieves empirical regret
performance competitive with the state-of-the-art
methods, while being more computationally efficient and scalable in comparison to the bestperforming ones among them.

lihood estimate (MLE) of the unknown system parameters and then applies the action optimal for that estimate.
Specifically, consider an MDP with state space S, action
space U, and controlled transition probabilities p(i, j, u; η)
denoting the probability of transition to a next state s(t +
1) = j when the current state s(t) = i and action u(t) = u
is applied at time t, indexed by a parameter η in a set Ξ.
The true parameter is η 0 ∈ Ξ, but is unknown. A reward
r(i, j, u) is obtained when the system transitions from i to j
under u. Consider the goal
maximizing the long-term avPof
T −1
erage reward limT →∞ T1 t=0 r(s(t), s(t + 1), u(t)). Let
J(φ, η) denote the long-term average reward accrued by
the stationary control law φ : S → U which chooses the
action u(t) = φ(s(t)). Let Jopt (η) := maxφ J(φ, η) =
J(φη , η) be the optimal long-term reward, and φη an optimal control law,Q
if the true parameter is η. Denote by
t−1
ηbt ∈ argmaxη∈Ξ τ =0 p(s(τ ), s(τ + 1), u(τ ); η) a MLE
of the true parameter η 0 . Under the CE approach, the action taken at time t is u(t) = φηbt (s(t)). This approach was
shown to be sub-optimal by Borkar & Varaiya (1979) since
it suffers from the “closed-loop identifiability problem”:
the parameter estimates ηbt converge to a η ∗ for which it
can only be guaranteed that:
p(i, j, φη∗ (i); η ∗ ) = p(i, j, φη∗ (i); η 0 ) for all i, j,

(1)

and, in general, φη∗ need not be optimal for η 0 .

1. Introduction
Controlling an unknown system to maximize long-term average reward is a well studied adaptive control problem
(Kumar, 1985). For unknown Markov Decision Processes
(MDPs), Mandl (1974) proposed the certainty equivalent
(CE) method that at each stage makes a maximum like*

Equal contribution 1 Department of Electrical and Computer
Engineering, Texas A&M University, College Station, Texas,
USA 2 Department of Computer Science, National Chiao Tung
University, Hsinchu, Taiwan 3 Department of Statistics, Texas
A&M University, College Station, Texas, USA. Correspondence to: Ping-Chun Hsieh <pinghsieh@nctu.edu.tw>, Xi Liu
<xiliu.tamu@gmail.com>.
Proceedings of the 37 th International Conference on Machine
Learning, Online, PMLR 119, 2020. Copyright 2020 by the author(s).

To solve this fundamental problem, Kumar & Becker
(1982) noticed that due to (1), J(φη∗ , η ∗ ) = J(φη∗ , η 0 ).
Since J(φη∗ , η 0 ) ≤ Jopt (η 0 ), but J(φη∗ , η ∗ ) = Jopt (η ∗ )
due to φ∗η being optimal for η ∗ , it follows that Jopt (η ∗ ) ≤
Jopt (η 0 ), i.e., the parameter estimates are biased in favor of
parameters with smaller optimal reward. To undo this bias,
with f denoting any strictly monotone increasing fucntion,
they suggested employing the RBMLE estimate:
ηbtRBMLE = argmax
η∈Ξ

α(t)

f (Jopt (η))

t−1
Y

p(s(τ ), s(τ + 1), u(τ ), η),

τ =0

(2)

in the CE scheme, with u(t) = φηbtRBMLE (s(t)). In (2), α(t) :
[1, ∞) → R+ is allowed to be any function that satisfies

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

limt→∞ α(t) = ∞ and limt→∞ α(t)/t = 0. This method
was shown to yield the optimal long-term average reward in
a variety of settings (Kumar & Lin, 1982; Kumar, 1983b;a;
Borkar, 1990; 1991; Stettner, 1993; Duncan et al., 1994;
Campi & Kumar, 1998; Prandini & Campi, 2000). For the
case of Bernoulli bandits, it was shown in Becker & Kumar
(1981) that the RBMLE approach provides an index policy
where each arm has a simple index, and the policy is to just
play the arm with the largest index.
The structure of (2) has a few critical properties that
contribute the success of RBMLE. First, the bias term
Jopt (η)α(t) multiplying the likelihood encourages active
exploration by favoring ηs with potentially higher maximal
long-term rewards. Second, the effect of the bias term gradually diminishes as α(t) grows like o(t), which makes the
exploitation dominate the estimation at later stages.
Several critical questions need to be answered to tailor the
RBMLE to the stochastic multi-armed bandits (SMABs).
1. The average reward optimality proved in prior RBMLE
studies is a gross measure implying only that regret (defined below) is o(T ), while in SMABs a much stronger
O(log(T )) finite-time order-optimal regret is desired. What
is the regret bound of the RBMLE algorithms in SMABs?
2. The above options for α(t) are very broad, and not all
of them lead to order-optimality of regret. How should one
choose the function α(t) to attain order-optimality?
3. What are the advantages of RBMLE algorithms compared to the existing methods? Recall that the Upper Confidence Bounds (UCB) approach pioneered by
(Lai & Robbins, 1985), and streamlined by (Auer et al.,
2002), is conductive to establish the regret bound, but
suffers from much higher empirical regret than its counterparts, while the Information Directed Sampling (IDS)
(Russo & Van Roy, 2014; 2018a) approach appears to
achieve the smallest empirical regret in various bandits, but
suffers from high computational complexity with resulting
poor scalability due to the calculation or estimation of high
dimensional integrals.

“minimum gap,” the difference between the means of the
best and second best arms, and an upper bound on the maximal mean reward are known, simple closed-form indices
as well as O(log(T )) order-optimal regret are achieved for
reward distributions for both parametric Exponential families as well as sub-Gaussian/Exponential non-parametric
families.
3. When the two bounds are unknown, the proposed
RBMLE algorithms still attain order-optimality in the
Gaussian and sub-Gaussian cases by adaptively estimating
them in the index expressions on the fly.
4. We evaluate the empirical performance of RBMLE algorithms in extensive experiments. They demonstrate competitive performance in regret as well as scalability against
the current best policies.

2. Problem Setup
Consider an N -armed bandit, where each arm i is characterized by its reward distribution Di with mean θi ∈ Θ, where
Θ denotes the set of possible values for the mean rewards.
Without loss of generality, let θ1 > θ2 ≥ · · · ≥ θN ≥ 0.
For each arm i, let ∆i := θ1 − θi be the gap between its
mean reward and that of the optimal arm, and ∆ := ∆2
the “minimum gap.” Let θ denote the vector (θ1 , · · · , θN ).
At each time t = 1, · · · , T , the decision maker chooses
an arm πt ∈ [N ] := {1, · · · , N } and obtains the corresponding reward Xt , which is independently drawn from
the distribution Dπt . Let Ni (t) and Si (t) be the total
number of trials of arm i and the total reward collected
from pulls of arm i up to time t, respectively. Define
pi (t) := Si (t)/Ni (t) as the empirical mean reward up to
t. Denote by Ht = (π1 , X1 , π2 , X2 , · · · , πt , Xt ) the history of all the choices of the decision maker and the reward observations up to time t. Let L(Ht ; {Di }) denote
the likelihood of the history Ht under the reward distributions {Di }. The objective is to minimize the regret defined
PT
as R(T ) := T θ1 − E[ t=1 Xt ], where the expectation is
taken with respect to the randomness of the rewards and the
employed policy.

The major contributions of this paper are:
1. We show that RBMLE yields an “index policy” for
Exponential Family SMABs, and explicitly determine the
indices. (An index policy is one where each arm has
an index that depends only on its own past performance
history, and one simply plays the policy with the highest index). We also propose novel RBMLE learning algorithms for SMABs from sub-Gaussian/Exponential nonparametric families.
2. We reveal the general interplay between the choice of
α(t) and the regret bound. When a lower bound on the

3. The RBMLE Policy for Exponential
Families and its Indexability
Let the probability density function of the reward obtained
from arm i be a one-parameter Exponential Family distribution:

p(x; ηi ) = A(x) exp ηi x − F (ηi ) ,
(3)

where ηi ∈ N is the canonical parameter, N is the parameter space, A(·) is a real-valued function, and F (·) is a realvalued twice-differentiable function. Then (see (Jordan,

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

2010)), θi = Ḟ (ηi ) (“dot” denoting derivative) is the mean
of the reward distribution, and its variance is F̈ (ηi ). Also,
(i) F (·) is strictly convex, and hence (ii) the function Ḟ (η)
is strictly increasing, hence invertible. A critical property
that will be used to derive the RBMLE is that ηi = Ḟ −1 (θi )
is strictly monotone increasing in the mean reward θi .
We consider the case where the reward distribution of each
arm i has the density function p(·; ηi ), with F (·) and A(·)
being identical across all the arms 1 ≤ i ≤ N . Let
η := (η1 , η2 , . . . , ηN ) denote the parameter vector that collectively describes the set of all arms. Based on (3), if Xτ
is the reward obtained at time τ by pulling arm πτ , then the
likelihood of Ht at time t under the parameter vector η is
L(Ht ; η) =

t
Y

τ =1


A(Xτ ) exp ηπτ Xτ − F (ηπτ ) .

(4)
α(t)

Now let us consider the reward bias term f (Jopt (η))
in (2). The optimal reward obtainable from η is the maximum of the mean rewards of the arms, i.e., Jopt (η) =
max1≤i≤N θi , where θi = Ḟ (ηi ) is the mean reward from
arm i. By choosing the strictly monotone increasing function f (·) = exp(Ḟ −1 (·)), the reward-bias term reduces to

(5)
f (Jopt (η))α(t) := max exp(ηi α(t)) .
i∈[N ]

Multiplying the reward-bias term and the likelihood term,
the RBMLE estimator is

btRBMLE := argmax L(Ht ; η) max exp(ηi α(t)) . (6)
η
i∈[N ]

η:ηj ∈N ,∀j

The corresponding arm chosen by RBMLE at time t is
 RBMLE
.
(7)
πtRBMLE = argmax ηbt,i
i∈[N ]

By combining (7) with (6), we have the following index
strategy equivalent to (7):
o
n

max
L(Ht ; η) exp(ηi α(t)) .
πtRBMLE = argmax
i∈[N ]

η:ηj ∈N ,∀j

(8)

The proof of the above result is provided in Appendix A.
3.1. The RBMLE Indices for Exponential Families
An index policy is one where each arm i has an index Ii (t)
that is only a function of the history of that arm i up to time
t, and the policy is to simply play the arm with the largest
index Ii (t) at time t (Whittle, 1980). For the RBMLE policy as written in (8) it is not quite obvious that it is an index
policy since the term L(Ht ; η) exp(ηi α(t)) for arm i depends on the combined history Ht of all arms, including
arms other than i. It was recognized in (Becker & Kumar,

1981) that RBMLE yields an index policy for the case of
Bernoulli bandits. The following proposition shows that
RBMLE is indeed an index policy for the Exponential Family described above, i.e., RBMLE is “indexable,” and explicitly identifies what the index of an arm is.1
Proposition 1 The arm selected at time t by the RBMLE
algorithm for Exponential Family rewards is
πtRBMLE = argmax I(pi (t), Ni (t), α(t)), where

(9)

i∈[N ]

 −1 
α(t)  
(10)
I(ν, n, α(t)) = nν + α(t) Ḟ
ν+
n Θ


α(t)  
+ nF Ḟ −1 (ν) ,
− nν Ḟ −1 (ν) − nF Ḟ −1 ν +
Θ
n
(11)
with [·]Θ denoting the clipped value within the set Θ.
Remark 1 The clipping ensures that the input of Ḟ −1 is
within [0, 1], since, for example, under Bernoulli reward
distributions, pi (t) + α(t)/Ni (t) could be larger than 1.
The indices for three commonly-studied distributions are
provided below.
Corollary 1 For Bernoulli distributions, with F (η) =
−1
eη
θ
log(1 + eη ), Ḟ (η) = 1+e
),
(θ) = log( 1−θ
η , Ḟ
−1
1
F (Ḟ (θ)) = log( 1−θ ), and with p̃i (t) := min{pi (t) +
α(t)/Ni (t), 1}, the RBMLE index is
I(pi (t), Ni (t), α(t)) =
(12)

Ni (t) p̃i (t) log p̃i (t) + (1 − p̃i (t)) log(1 − p̃i (t)) (13)
− pi (t) log(pi (t)) − (1 − pi (t)) log(1 − pi (t)) .

(14)

Corollary 2 For Gaussian reward distributions with the
same variance σ 2 across arms, F (ηi ) = σ 2 ηi2 /2, Ḟ (ηi ) =
σ 2 ηi , Ḟ −1 (θi ) = θi /σ 2 , and F (Ḟ −1 (θi )) = θi2 /2σ 2 , for
each arm i, the RBMLE index is
I(pi (t), Ni (t), α(t)) = pi (t) +

α(t)
.
2Ni (t)

(15)

Corollary 3 For Exponential distributions, the index is
I(pi (t), Ni (t), α(t)) = Ni (t) log


Ni (t)pi (t)
.
Ni (t)pi (t) + α(t)
(16)

Remark 2 The RBMLE indices derived for parametric distributions can also be applied to non-parametric distributions. As shown in Propositions 4 and 5, they still achieve
O(log(T )) regret.
Table 1 compares the RBMLE indices with other policies.
That these new indices have performance competitive with
state-of-the-art (Section 5), is of potential interest.
1
The proofs of all Lemmas, Corollaries and Propositions are
provided in the Appendices.

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

Table 1: Comparison of indices produced by RBMLE with
other approaches. Below, H(p) is the binary entropy, V t (i)
is the upper bound on the variance, and the other quantities
are defined in Sections 2 and 3.
Algorithm
BMLE
UCB
UCB-Tuned
MOSS

Index

(Bernoulli) Ni (t) H(pi (t)) − H(p̃i (t)
(Gaussian) pi (t) + α(t)/(2Ni (t))

Ni (t)pi (t)
(Exponential) Ni (t) log Ni (t)p
(t)+α(t)
i
p
pi (t) + q2 log t/Ni (t)
min{ 41 , V t (i)} log(t)/Ni (t)}
q
T
), 0)/Ni (t)
pi (t) + max(log( Ni (t)·N
pi (t) +

Remark 3 Lemmas 2 and 3 show how RBMLE naturally engages in the exploration vs. exploitation tradeoff. Lemma 2 shows that RBMLE indeed tends to avoid
an arm with a smaller empirical mean reward after sufficient exploration, as quantified in terms of α(t) by n2 >
K ∗ (µ1 , µ2 )α(t). On the other hand, Lemma 3 suggests
that RBMLE is designed to continue exploration even if the
empirical mean reward is initially fairly low (which is reflected by the fact that there is no restriction on the ordering
between µ1 and µ2 in Lemma 3), when there has been insufficient exploration, as quantified by n1 ≤ K ∗ (µ0 , µ1 )α(t).
These properties emerge naturally out of the reward biasing.

4. Regret Analysis of the RBMLE Algorithm
3.2. Properties of the RBMLE Indices
We introduce several useful properties of the index
I(ν, n, α(t)) in (10)-(11) to better understand the behavior
of the derived RBMLE indices and prepare for regret analysis in subsequent sections. To begin with, we discuss the
dependence of I(ν, n, α(t)) on ν and n.
Lemma 1 (i) For a fixed ν ∈ Θ and α(t) > 0, I(ν, n, α(t))
is strictly decreasing with n, for all n > 0.
(ii) For a fixed n > 0 and α(t) > 0, I(ν, n, α(t)) is strictly
increasing with ν, for all ν ∈ Θ.

We now analyze the finite-time performance of the proposed RBMLE algorithm. The KL divergence KL(η ′ || η ′′ )
between two distributions can be expressed as
KL(η ′ || η ′′ ) = F (η ′′ ) − [F (η ′ ) + Ḟ (η ′ )(η ′′ − η ′ )]. (20)
Define D(θ′ , θ′′ ) : Θ × Θ → R+ by
D(θ′ , θ′′ ) := KL(Ḟ −1 (θ′ ) || Ḟ −1 (θ′′ )).

(21)

4.1. Interplay of Bias-Growth Rate α(t) and Regret

Since the RBMLE index is I(pi (t), Ni (t), α(t)), Lemma
1.(ii) suggests that the index of an arm increases with its
empirical mean reward pi (t).

We determine the regret bounds for several classes of distributions, both parametric as well as non-parametric.

To prepare for the following lemmas, we first define a function ξ(k; ν) : R++ → R as

4.1.1. L OWER -B OUNDED E XPONENTIAL FAMILY

h
i
1  −1
1
−1
ξ(k; ν) =k ν + Ḟ (ν + ) − ν Ḟ (ν)
(17)
k
k

i
h 
1
−1
−1
− F (Ḟ (ν)) . (18)
− k F Ḟ
ν+
k

It is easy to verify that I(ν, kα(t), α(t)) = α(t)ξ(k; ν). By
Lemma 1.(i), we know ξ(k; ν) is strictly decreasing with k.
Moreover, define a function K ∗ (θ′ , θ′′ ) as
K ∗ (θ′ , θ′′ ) = inf{k : Ḟ

−1

(θ′ ) > ξ(k; θ′′ )}.

(19)

Lemma 2 Given any pair of real numbers µ1 , µ2 ∈ Θ with
µ1 > µ2 , for any real numbers n1 , n2 that satisfy n1 > 0
and n2 > K ∗ (µ1 , µ2 )α(t) (with K ∗ (µ1 , µ2 ) being finite),
we have I(µ1 , n1 , α(t)) > I(µ2 , n2 , α(t)).
Lemma 3 Given any real numbers µ0 , µ1 , µ2 ∈ Θ with
µ0 > µ1 and µ0 > µ2 , for any real numbers n1 , n2 that
satisfy n1 ≤ K ∗ (µ0 , µ1 )α(t) and n2 > K ∗ (µ0 , µ2 )α(t),
we have I(µ1 , n1 , α(t)) > I(µ2 , n2 , α(t)).

We consider the regret performance of RBMLE for Exponential Families with a known lower bound θ on the mean.
E.g., θ = 0 for Bernoulli distributions. Such a collection
includes commonly-studied Exponential Families that are
defined on the positive half real line, such as Exponential,
Binomial, Poisson, and Gamma (with a fixed shape parameter).
Proposition 2 For any Exponential Family with a lower
bound θ on the mean, for any ε ∈ (0, 1), the regret of
RBMLE using (10)-(11) with α(t) = Cα log t and Cα ≥
ε∆
∗
4/(D(θ1 − ε∆
2 , θ1 )K (θ1 − 2 , θ)) satisfies
R(T ) ≤

N
X

a=2

Cα K ∗ (θ1 −



∆a max

4
D(θa +

ε∆a
2 , θa )

,

(22)

ε∆a
ε∆a
π2 
. (23)
, θa +
) log T + 1 +
2
2
3

4.1.2. G AUSSIAN D ISTRIBUTIONS
Proposition 3 For Gaussian reward distributions with
variance bounded by σ 2 for all arms, the regret of RBMLE

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

using (15) with α(t) = Cα log t and Cα ≥
R(T ) ≤

N
X

∆a

a=2

256σ2
∆

 2
2π 2 
.
Cα log T +
∆a
3

satisfies
(24)

4.1.3. B EYOND PARAMETRIC D ISTRIBUTIONS
RBMLE indices derived for Exponential Families can be
readily applied to other non-parametric distributions. Moreover, the regret proofs in Propositions 2-3 can be readily extended if the non-parametric rewards also satisfy
proper concentration inequalities. We consider two classes
of reward distributions, namely sub-Gaussian and subExponential (Wainwright, 2019):
Definition 1 A random variable X with mean µ = E[X]
is σ-sub-Gaussian if there exists σ > 0 such that
E[eλ(X−µ) ] ≤ e

σ 2 λ2
2

, ∀λ ∈ R.

(25)

Definition 2 A random variable X with mean µ = E[X]
is (ρ, κ)-sub-Exponential if there exist ρ, κ ≥ 0 such that

1
, ∀|λ| < .
(26)
E[e
]≤e
κ
Proposition 4 For any σ-sub-Gaussian reward distributions, RBMLE using (15) with α(t) = Cα log t and Cα ≥
 2

P
256σ2
2π 2
yields R(T ) ≤ N
a=2 ∆a ∆a Cα log T + 3 .
∆
λ(X−µ)

ρ2 λ2
2

Proof The proof of Proposition 3 still holds for Proposition 4 without any change as Hoeffding’s inequality directly works for sub-Gaussian distributions.
Proposition 5 For any (ρ, κ)-sub-Exponential reward distribution defined on the positive half line with a lower
bound θ on the mean, RBMLE using (10)-(11) with α(t) =
Cα log t and Cα ≥ 16(κε∆ + 2ρ2 )/((ε∆)2 K ∗ (θ1 −
ε∆
2 , θ)) achieves a regret bound
R(T ) ≤

N
X

 16(κε∆ + 2ρ2 )

π2
, (27)
+ max
∆a 1 +
3
(ε∆a )2
a=2


ε∆a
ε∆a
, θa +
) log T .
(28)
2
2
Remark 4 We highlight that the Propositions 2-5 aim to
provide the relationship between the upper bound on regret
and the gap ∆. We find that to establish the O(log(T ))
regret bound, the pre-constant Cα has to be large enough.
One of our major technical contributions is to quantify the
non-trivial relationship between Cα and ∆ as well as the
dependency between the bound and ∆. A similar dependency also exists in other algorithms such as UCB, KLUCB, and IDS, etc, due to the sharp characterization of
the pre-constant by (Lai & Robbins, 1985). Moreover, we
consider adaptive estimation of the gap as illustrated in
Algorithms 1-3. In practice, we show that such adaptive
scheme is sufficient to achieve excellent performance (see
Section 5).
Cα K ∗ (θ1 −

4.2. RBMLE with Adaptive Estimation of Cα
In this section, we provide the pseudo code of the experiments in Section 5. As discussed above, RBMLE achieves
logarithmic regret by estimating Cα in α(t) = Cα log t,
where the estimation of Cα involves the minimum gap ∆
and the largest mean θ1 . We consider the following adaptive scheme that gradually learns ∆ and θ1 .
Algorithm 1 Adaptive Scheme with Estimation of Cα in
Gaussian Bandits
1: Input: N , σ, and β(t)
2: for t = 1, 2, · · · do
3:
for i = 1 to N do p
4:
Ui (t) = pi (t) + 2σ 2 (N + 2) log t/Ni (t) // upper confidence bound
p of the empirical mean
5:
Li (t) = pi (t) − 2σ 2 (N + 2) log t/Ni (t) //
lower confidence bound of the empirical mean
6:
end for
n
o
ˆ t = maxi max 0, Li (t) − maxj6=i Uj (t)
7:
∆
8:

Calculate Ĉα (t) =

256σ2
ˆt
∆

9:
α(t) = min{Ĉα (t), β(t)} log t
10: end for

• Estimate ∆ and θ1 : Note that ∆ can be expressed
as max1≤i≤N {θi − maxj6=i θj }. For each arm i, construct Ui (t) and Li (t) as the upper and lower confidence
bounds of pi (t) based on proper concentration inequalˆ
ities. Then,
 construct an estimator of ∆ as ∆t :=
max1≤i≤N max 0, Li (t) − maxj6=i Uj (t) . Meanwhile, we use Umax (t) := max1≤i≤N Ui (t) as an estimate of θ1 . Based on the confidence bounds, we know
ˆ t ≤ ∆ and Umax (t) ≥ θ1 , with high probability.
∆
• Construct the bias using estimators: We construct
bα (t), β(t)} log t, where C
bα (t) estimates
α(t) = min{C
ˆ
Cα (t) by replacing ∆ with ∆t and θ1 with Umax (t), with
β(t) a non-negative strictly increasing
√ function satisfying
limt→∞ β(t) = ∞ (e.g. β(t) = log t in the experibα (t) gradments in Section 5). With high probability, C
ually approaches the target value Cα (t) from above as
time evolves. On the other hand, β(t) guarantees smooth
bα (t).
exploration initially and will ultimately exceed C
4.2.1. ( SUB ) G AUSSIAN D ISTRIBUTIONS

To further illustrate the overall procedure, we first use the
Cα in Propositions 3 and 4 as an example, with the pseudo
code provided in Algorithm 1. The main idea is to learn the
appropriate Cα considered in the regret analysis by gradually increasing Cα until it is sufficiently large. This is
accomplished by setting α(t) = min{Ĉα (t), β(t)} log t

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

(Line 9 in Algorithm 1) where Ĉα (t) serves as an overestimate of the minimum required Cα based on the estimaˆ t is a conserˆ t for ∆ (Lines 3-8 in Algorithm 1). ∆
tors ∆
ˆ
vative estimate of ∆ in the sense that ∆t ≤ ∆, conditioned
on the high probability events θi ∈ [Li (t), Ui (t)], for all
i. Here the confidence bounds Li (t) and Ui (t) are constructed with the help of Hoeffding’s inequality. For small
ˆ t is very close to zero and hence
t, it is expected that ∆
Ĉα (t) is large. Therefore, initially β(t) serves to gradually increase Cα and guarantees enough exploration after
β(t) exceeds the minimum required Cα . Given sufficient
ˆ t gets accurate
exploration enabled by β(t), the estimate ∆
ˆ
(i.e. ∆t ≈ ∆), and subsequently Ĉα (t) is clamped at some
value slightly larger than the minimum required Cα .
Next, we quantify the regret performance of RBMLE in
Algorithm 1 in Proposition 6 as follows.
Proposition 6 For any σ-sub-Gaussian reward distributions, the regret of RBMLE given by Algorithm 1 satisfies

Algorithm 2 Adaptive Scheme with Estimation of Cα in
Bernoulli Bandits
1: Input: N , ε ∈ (0, 12 ), and β(t)
2: for t = 1, 2, · · · do
3:
for i = 1 to N do


p
4:
Ui (t) = min pi (t) + (N + 2) log t/Ni (t), 1
5:

6:
7:
8:
9:
10:
11:

n 1024σ 2(N + 2)
o N π 2 i 12:
R(T ) ≤
∆a max
+
,
log
T,
T
0
∆2
3
a=2
(29)
256σ2 (N +2)
13:
where T0 := min{t ∈ N : β(t) ≥
} < ∞.
∆
N
X

h

4.2.2. B ERNOULLI D ISTRIBUTIONS
As above, Algorithm 2 shows the pseudo code for estimating the Cα of α(t) in Bernoulli bandits. Similar to the
ˆ t and Umax (t)
Gaussian case, Cα is estimated based on ∆
with the help of Hoeffding’s inequality. In addition, as the
calculation of Ĉα (t) involves the subroutine of searching
ˆt
for the value K ∗ (Umax (t) − ε∆
2 , 0), we can accelerate the
adaptive scheme by first checking if it is possible to have
Ĉα (t) ≥ β(t). Equivalently, this can be done by quickly
ˆt
−1
N +2
(Umax (t)− ε∆
verifying whether ξ( 2(ε∆
ˆ t )2 β(t) , 0) < Ḟ
2 )
(Line 9 in Algorithm 2).
4.2.3. E XPONENTIAL D ISTRIBUTIONS
Algorithm 3 demonstrates the pseudo code for selecting
α(t) in exponential bandits. Compared to the Bernoulli
case, the main difference in the exponential case lies in the
construction of the confidence bounds (Lines 4-5 in Algorithm 3), which leverage the sub-exponential tail bounds
instead of Hoeffding’s inequality.

5. Simulation Experiments
To evaluate the performance of the proposed RBMLE algorithms, we conduct a comprehensive empirical comparison
with other state-of-the-art methods vis-a-vis three aspects:
effectiveness (cumulative regret), efficiency (computation
time per decision vs. cumulative regret), and scalability (in

// upper confidence bound of the empirical mean


p
Li (t) = max pi (t) − (N + 2) log t/Ni (t), 0
// lower confidence bound of the empirical mean

end for
Umax (t) = max
n i=1,··· ,N Ui (t)
o
ˆ
∆t = maxi max 0, Li (t) − maxj6=i Uj (t)

ˆt
−1
N +2
if ξ 2(ε∆
(Umax (t) − ε∆
ˆ t )2 β(t) , 0 < Ḟ
2 ) then
α(t) = β(t) log t // In this case, we know
Ĉα (t) > β(t)
else
N +2
by
Find Ĉα (t) =
ˆ
ε∆
t
ˆ 2 ∗
2(ε∆t ) K (Umax (t)−

2

,0)

solving the minimization problem of (19) for
ˆt
K ∗ (Umax (t) − ε∆
2 , 0).
α(t) = min{Ĉα (t), β(t)} log t
14:
end if
15: end for

number of arms). We paid particular attention to the fairness of comparisons and reproducibility of results. To ensure sample-path sameness for all methods, we considered
each method over a pre-prepared dataset containing the context of each arm and the outcomes of pulling each arm over
all rounds. Hence, the outcome of pulling an arm is obtained by querying the pre-prepared data instead of calling
the random generator and changing its state. A few benchmarks such as Thompson Sampling (TS) and Variancebased Information Directed Sampling (VIDS) that rely on
outcomes of random sampling in each round of decisionmaking are separately evaluated with the same prepared
data and with the same seed. To ensure reproducibility of
experimental results, we set up the seeds for the random
number generators at the beginning of each experiment.
The
benchmark
methods
compared
include
UCB (Auer et al., 2002),
UCB-Tuned (UCBT)
(Auer et al., 2002), KLUCB (Cappé et al., 2013),
MOSS (Audibert & Bubeck, 2009), Bayes-UCB (BUCB)
(Kaufmann et al., 2012a), GPUCB (Srinivas et al., 2012),
GPUCB-Tuned (GPUCBT) (Srinivas et al., 2012), TS
(Agrawal & Goyal, 2012), Knowledge Gradient (KG)
(Ryzhov et al., 2012), KG* (Ryzhov et al., 2010), KGMin (Kamiński, 2015) KGMN (Kamiński, 2015), IDS
(Russo & Van Roy, 2018b), and VIDS, (Russo & Van Roy,

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

700
UCB
MOSS
BUCB
KG
KGMin
KGMN

2500

1000

750

500

250

0
0

2

4

6

Time Horizon

8

10
10

2000

1500

RBMLE
VIDS
KLUCB & UCB
GPUCB
GPUCBT
TS
BUCB
KG
KG*

1000

500

0
0

RBMLE
VIDS
KLUCB
MOSS
TS
BUCB
UCB
KG

600

Averaged Cumulative Regret

1250

RBMLE
IDS
VIDS
KLUCB
UCBT
TS

Averaged Cumulative Regret

Averaged Cumulative Regret

1500

500
400
300
200
100

2

4

4

6

8

Time Horizon

(a)

10
10

(b)

4

0
0

2

4

6

Time Horizon

8

10
104

(c)

(θi )10
i=1

Figure 1: Averaged cumulative regret: (a) Bernoulli bandits with
= (0.66, 0.67, 0.68, 0.69, 0.7, 0.61, 0.62, 0.63,
0.64, 0.65) & ∆ = 0.01; (b) Gaussian bandits with (θi )10
=
(0.41,
0.52,
0.66, 0.43, 0.58, 0.65, 0.48, 0.67, 0.59, 0.63) &
i=1
∆ = 0.01; (c) Exponential bandits with (θi )10
=
(0.31,
0.1,
0.2,
0.32,
0.33,
0.29, 0.2, 0.3, 0.15, 0.08) & ∆ = 0.01.
i=1
Algorithm 3 Adaptive Scheme with Estimation α(t) in Exponential Bandits
1: Input: N , ε ∈ (0, 21 ), and β(t)
2: for t = 1, 2, · · · do
3:
for i = 1 to N do
4:
Ui (t)
=
√
κ(N +2) log t+

5:

// upper confidence bound
Li (t)
=
√ 2
2
κ(N +2) log t+

6:
7:
8:
9:
10:
11:
12:

pi (t)
κ2 (N +2)2 (log t)2 +2ρ2 (N +2) log t
Ni (t)

max pi (t)

(log t)2 +2ρ2 (N +2) log t

κ (N +2)
Ni (t)

+

,0

−


// lower confidence bound
end for
Umax (t) = max
n i=1,··· ,N Ui (t)
o
ˆ
∆t = maxi max 0, Li (t) − maxj6=i Uj (t)

ˆ t +2ρ2 )
ˆt
∆
−1
if ξ 16(κε
(Umax (t) − ε∆
ˆ t )2 β(t) , 0 < Ḟ
2 ) then
(ε∆
α(t) = β(t) log t // In this case, we know
Ĉα (t) > β(t)
else
ˆ t +2ρ2 )
16(κε∆
Find Ĉα (t)
=
ˆ
ε∆
t
ˆ 2 ∗
(ε∆t ) K (Umax (t)−

2

by solving the minimization problem
ˆt
α(t)
(19) for K ∗ (Umax (t) − ε∆
2 , 0).
min{Ĉα (t), β(t)} log t
13:
end if
14: end for

,0)

of
=

2018b). A detailed review of these methods is presented
in Section 6. The values of their hyper-parameters are
as follows. In searching for a solution of KLUCB and
Ĉα (t) in RBMLE, the maximum number of iterations is

set to be 100. Following the suggestion in the original
papers, we take c = 0 in KLUCB and BUCB. We take
δ = 10−5 in GPUCB. We tune the parameter c in GPUCBT
for each experiment and choose c = 0.9 that achieves
the best performance. In the comparison with IDS and
VIDS, we uniformly sampled 100 points over the interval
[0, 1] for Bernoulli and Exponential Bandits and sampled
1000 points for Gaussian bandits (the q in Algorithm 4
in (Russo & Van Roy, 2018b)) and take M = 104 in
sampling (Algorithm 3 in (Russo & Van Roy, 2018b)).
The conjugate priors for Bayesian-family methods are
Beta distribution B(1, 1) for Bernoulli bandits, N (0, 1)
for Gaussian bandits with σ = 1, and Gamma distribution
Γ(1, 1) for Exponential bandits with ρ = 10 and κ = 10.
The average is taken over 100 trials. The time horizon in
the experiments for effectiveness and efficiency is 105 , and
for scalability is 104 .
Effectiveness. Figures 1, 3–4 and Tables 2-10 (some are in
Appendix M.1) illustrate the effectiveness of RBMLE with
respect to the cumulative regret as well as quantiles. Note
that in the (b) sub-figures of these figures, since KLUCB
shares the same closed-form index as UCB in Gaussian bandits, their curves coincide. We observe that for all three
types of bandits, Bernoulli, Gaussian and Exponential,
RBMLE achieves competitive performance, often slightly
better than the best performing benchmark method. IDS or
VIDS are often the closest competitors to RBMLE. However, the computational complexity of RBMLE is much
lower compared to IDS and VIDS, which need to compute high dimensional integrals or estimate them through
sampling. One other advantage of RBMLE over some
benchmark methods is that it is “time horizon agnostic”,
i.e., the computation of RBMLE index does not need the

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

knowledge of time horizon T . In contrast, BUCB, MOSS,
GPUCBT, and KG-family algorithms (KG, KG*) need to
know T . It is worth mentioning that in Bernoulli bandits,
KG, KGMin, and KGMN perform poorly as they explore
insufficiently. This is not surprising as several papers have
pointed out the limitations of KG-family methods when observations are discrete (Kamiński, 2015).
Efficiency Figures 2 and 5–6 (some are in Appendix M.2)
present the efficiency of RBMLE in terms of averaged computation time per decision vs. averaged final cumulative
regret. The former averaged the total time spent in all trials over the number of trials and number of rounds. The
latter averaged the total cumulative regret in all trials over
the number of trials. RBMLE is seen to provide competitive performance compared to other benchmark methods.
It achieves slightly better regret, and does so with orders
of magnitude less computation time, than IDS, VIDS and
KLUCB. This is largely because IDS and VIDS need to
estimate several integrals in each round, and KLUCB often relies on Newton’s method or bisection search to find
the index for an arm, except in Gaussian bandtis, where
KLUCB has a simple closed-form solution. It is also observed that the computation time of RBMLE is larger than
some benchmark methods such as UCB, GPUCB, and KG,
which enjoy a simple closed-form index. However, their regret performance is far worse than RBMLE’s. In the comparison of efficiency, the closest competitors to RBMLE
are TS, MOSS, UCBT, and GPUCBT. Compared to them,
RBMLE still enjoys salient advantages in different aspects.
Compared to TS, RBMLE follows the frequentist formulation and thus its performance does not deteriorate when a
bad prior is chosen. Compared to MOSS, RBMLE does not
rely on the knowledge of T to compute its index. Compared
to UCBT as well as GPUCBT, RBMLE has a order-optimal
regret bound, as proved in the earlier sections.
Scalability Tables 11-13 show the scalability of RBMLE as
the number of arms is increased. This is illustrated through
comparing different methods’ averaged computation time
per decision averaged computation time per decision under
varying numbers of arms. We observe that RBMLE scales
well for various reward distributions as the number of arms
increases, often demonstrating performance comparable to
the most scalable ones among the benchmark methods. The
averaged computation time per decision stays at a few 10−4
seconds even when the number of arms reaches 70. In contrast, it can be as high as thousands of 10−4 seconds for IDS
and VIDS, and it is often tens of times higher for KLUCB.

6. Related Work
The RBMLE approach proposed in (Kumar & Becker,
1982) has been examined in a variety of settings, including MDPs and Linear-Quadratic-Gaussian systems,

in (Kumar & Lin, 1982; Kumar, 1983b;a; Borkar, 1990;
1991; Stettner, 1993; Duncan et al., 1994; Campi & Kumar,
1998; Prandini & Campi, 2000). The simple index for the
case of Bernoulli bandits was derived in (Becker & Kumar,
1981).
However, prior studies have been limited to focusing on
long-term average reward optimality, which corresponds to
a loose o(T ) bound on regret. This paper aims to tailor
RBMLE to more general SMAB problems, and to prove its
finite-time regret performance.
Learning algorithms for SMAB problems have been extensively studied. Most prior studies can be categorized into
frequentist approaches or Bayesian approaches. In the frequentist settings, the family of UCB algorithms, including
UCB (Auer et al., 2002), UCBT (Auer et al., 2002), and
MOSS (Audibert & Bubeck, 2009; Degenne & Perchet,
2016), are among the most popular ones given their simplicity in implementation and the established regret bounds.
An upper confidence bound can be directly derived from
concentration inequalities or constructed with the help
of other information measures, such as the Kullback–
Leibler divergence used by KLUCB (Filippi et al., 2010;
Garivier & Cappé, 2011; Cappé et al., 2013). The concept of upper confidence bound has later been extended to various types of models, such as contextual linear bandits (Chu et al., 2011; Abbasi-Yadkori et al.,
2011; Rusmevichientong & Tsitsiklis, 2010), Gaussian
process bandit optimization (GPUCB and GPUCBT)
(Srinivas et al., 2012), and model-based reinforcement
learning (Jaksch et al., 2010). The above list is by no
means exhaustive but is mainly meant to illustrate the
wide applicability of the UCB approach in different settings. While being a simple and generic index-type algorithm, UCB-based methods sometimes suffer from much
higher regret than their counterparts (Russo & Van Roy,
2014; Chapelle & Li, 2011). Different from the UCB solutions, the proposed RBMLE algorithm addresses the exploration and exploitation tradeoff by directly operating
with the likelihood function to navigate the exploration, and
therefore it makes better use of the information of the parametric distributions.
On the other hand, the Bayesian approach studies
the setting where the unknown reward parameters are
drawn from an underlying prior distribution. As one
of the most popular Bayesian bandit algorithms, TS
(Scott, 2010; Chapelle & Li, 2011; Agrawal & Goyal,
2012; Korda et al., 2013; Kaufmann et al., 2012b) follows
the principle of probability matching by continuously updating the posterior distribution based on a prior. In addition to strong theoretical guarantees (Agrawal & Goyal,
2012; Kaufmann et al., 2012b), TS has been reported to
achieve superior empirical performance to its counterparts

10

10

-2

RBMLE
IDS
VIDS
KLUCB
UCBT
TS
UCB
MOSS
BUCB
KG
KGMin
KGMN

-3

-4

10-5

0

500

1000

1500

2000

2500

10

10

-2

RBMLE
VIDS
KLUCB & UCB
GPUCB
GPUCBT
TS
BUCB
KG
KG*

-3

10-4

10-5
500

1000

1500

2000

2500

3000

Averaged Computation Time Per Decision (secs)

10

Averaged Computation Time Per Decision (secs)

Averaged Computation Time Per Decision (secs)

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

10

-2

10

-3

10

-4

RBMLE
VIDS
KLUCB
TS
UCB
MOSS
BUCB
KG

0

500

1000

Averaged Final Cumulative Regret

Averaged Final Cumulative Regret

Averaged Final Cumulative Regret

(a)

(b)

(c)

1500

Figure 2: Averaged computation time per decision vs. averaged final cumulative regret: (a) Figure 1(a); (b) Figure 1(b);
(c) Figure 1(c).
(Chapelle & Li, 2011; Scott, 2010). While being a powerful bandit algorithm, TS can be sensitive to the choice of the
prior (Korda et al., 2013; Liu & Li, 2016). Another popular Bayesian algorithm is BUCB (Kaufmann et al., 2012a),
which combines the Bayesian interpretation of bandit problems and the simple closed-form expression of UCB-type
algorithms. In contrast, RBMLE does not rely on a prior
and hence completely obviates the potential issues arising from an inappropriate prior choice. Another line of
investigation takes advantage of the Bayesian update in
information-related measures. KG (Ryzhov et al., 2012)
and its variant KG* (Ryzhov et al., 2010), KGMin, and
KGMN (Kamiński, 2015; Ryzhov et al., 2012; 2010) proceed by making a greedy one-step look-ahead measurement for exploration, as suggested by their names. While
KG has been shown to empirically perform well for Gaussian distributions (Ryzhov et al., 2010; Wang et al., 2016),
its performance is not readily quantifiable, and it does not
always converge to optimality (Russo & Van Roy, 2014).
Another competitive solution is IDS and its variant,VIDS,
proposed by Russo & Van Roy (2018b). Different from
the KG algorithm, IDS blends in the concept of information gain by looking at the ratio between the square of expected immediate regret and the expected reduction in the
entropy of the target. Moreover, it has been reported in
(Russo & Van Roy, 2014; 2018a) that IDS achieves stateof-the-art results in various bandit models. However, IDS
and VIDS suffer from high computational complexity and
poor scalability due to the excessive sampling required for
estimating high dimensional integrals. Compared to these
regret-competitive solutions, the proposed RBMLE algorithms can achieve comparable performance both theoretically and empirically, but at the same time it retains computational efficiency.

7. Concluding Remarks
The RBMLE method, developed in the general study of
adaptive control four decades ago, provides a scheme for
optimal control of general Markovian systems. It exploits
the observation that the MLE has a one-sided bias favoring parameters with smaller optimal rewards, and so delicately steers the scheme to an optimal estimate by biasing the MLE in the reverse way. Just like the later Upper
Confidence Bound policy, it also can be regarded as exemplifying the philosophy of being “optimistic in the face of
uncertainty,” but does it in a very different way. The resulting indices (Table 1) are very different. Over the four
decades since its introduction, the RBMLE method has not
been further analyzed or empirically evaluated for its regret
performance.
This paper takes an initial step in this direction. It shows
how indices can be derived naturally for the Exponential
Family of reward distributions, and how these indices can
even be applied to other non-parametric distributions. It
studies the interplay between the choice of the growth rate
of the reward-bias and the resulting regret. It exposes the
important role played by the knowledge of the minimum
gap in the choice of the reward-bias growth rate. When
this minimum gap is not known, it shows how it can be
adaptively estimated. It empirically shows that RBMLE
attains excellent regret performance compared with other
state-of-art methods, while requiring low computation time
per decision compared to other methods with comparable
regret performance, and scales well as the number of arms
is increased.
Being a general purpose approach for optimal decision
making under certainty, RBMLE holds potential for a number of such problems, including contextual bandits, adversarial bandits, Bayesian optimization and more beyond.

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

Acknowledgments
We sincerely thank all anonymous reviewers and meta reviewers for their insightful suggestions during rebuttal!
This work is partially supported by the Ministry of Science and Technology of Taiwan under Contract No. MOST
108-2636-E-009-014. This material is also based upon
work partially supported by the U.S. Army Research Office under Contract No. W911NF-18-10331, U.S. ONR under Contract No. N00014-18-1-2048, and NSF Science &
Technology Center Grant CCF-0939370.

References
Abbasi-Yadkori, Y., Pál, D., and Szepesvári, C. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems, pp. 2312–2320, 2011.
Agrawal, S. and Goyal, N. Analysis of Thompson sampling for
the multi-armed bandit problem. In Conference on Learning
Theory (COLT), pp. 39–1, 2012.

Duncan, T. E., Pasik-Duncan, B., and Stettner, L. Almost selfoptimizing strategies for the adaptive control of diffusion processes. Journal of optimization theory and applications, 81(3):
479–507, 1994.
Filippi, S., Cappé, O., and Garivier, A. Optimism in reinforcement learning and Kullback-Leibler divergence. In 2010 48th
Annual Allerton Conference on Communication, Control, and
Computing (Allerton), pp. 115–122, 2010.
Garivier, A. and Cappé, O. The KL-UCB algorithm for bounded
stochastic bandits and beyond. In Proceedings of the 24th annual Conference On Learning Theory (COLT), pp. 359–376,
2011.
Jaksch, T., Ortner, R., and Auer, P. Near-optimal regret bounds
for reinforcement learning. Journal of Machine Learning Research, 11(Apr):1563–1600, 2010.

Jordan,
M.
Chapter
8:
The
Exponential
family:
Basics,
2010.
URL
https://people.eecs.berkeley.edu/˜jordan/courses/260-sp

Audibert, J.-Y. and Bubeck, S. Minimax policies for adversarial
and stochastic bandits. In COLT, pp. 217–226, 2009.

Kamiński, B. Refined knowledge-gradient policy for learning
probabilities. Operations Research Letters, 43(2):143–147,
2015.

Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time analysis
of the multiarmed bandit problem. Machine learning, 47(2-3):
235–256, 2002.

Kaufmann, E., Cappé, O., and Garivier, A. On Bayesian upper
confidence bounds for bandit problems. In Artificial intelligence and statistics (AISTATS), pp. 592–600, 2012a.

Becker, A. and Kumar, P. R. Optimal strategies for the Narmed bandit problem. Technical report, Mathematics Research Report No. 81-1, Department of Mathematics, University of Maryland Baltimore County, Jan 1981.

Kaufmann, E., Korda, N., and Munos, R. Thompson sampling:
an asymptotically optimal finite-time analysis. In Proceedings
of the 23rd international conference on Algorithmic Learning
Theory, pp. 199–213. Springer-Verlag, 2012b.

Borkar, V. and Varaiya, P. Adaptive control of Markov chains, I
Finite parameter set. IEEE Transactions on Automatic Control,
24(6):953–957, 1979.
Borkar, V. S. The Kumar-Becker-Lin scheme revisited. Journal of
Optimization Theory and Applications, 66(2):289–309, 1990.
Borkar, V. S. Self-tuning control of diffusions without the identifiability condition. Journal of optimization theory and applications, 68(1):117–138, 1991.
Campi, M. C. and Kumar, P. R. Adaptive linear quadratic Gaussian control: the cost-biased approach revisited. SIAM Journal
on Control and Optimization, 36(6):1890–1907, 1998.
Cappé, O., Garivier, A., Maillard, O.-A., Munos, R., Stoltz, G.,
et al. Kullback-leibler upper confidence bounds for optimal sequential allocation. The Annals of Statistics, 41(3):1516–1541,
2013.
Chapelle, O. and Li, L. An empirical evaluation of Thompson
sampling. In Advances in neural information processing systems, pp. 2249–2257, 2011.

Korda, N., Kaufmann, E., and Munos, R. Thompson sampling
for 1-dimensional exponential family bandits. In Advances in
Neural Information Processing Systems, pp. 1448–1456, 2013.
Kumar, P. R. Optimal adaptive control of linear-quadraticGaussian systems. SIAM Journal on Control and Optimization,
21(2):163–178, 1983a.
Kumar, P. R. Simultaneous identification and adaptive control of
unknown systems over finite parameter sets. IEEE Transactions on Automatic Control, 28(1):68–76, 1983b.
Kumar, P. R. A survey of some results in stochastic adaptive control. SIAM Journal on Control and Optimization, 23(3):329–
380, 1985.
Kumar, P. R. and Becker, A. A new family of optimal adaptive
controllers for Markov chains. IEEE Transactions on Automatic Control, 27(1):137–146, 1982.
Kumar, P. R. and Lin, W. Optimal adaptive controllers for unknown Markov chains. IEEE Transactions on Automatic Control, 27(4):765–774, 1982.

Chu, W., Li, L., Reyzin, L., and Schapire, R. Contextual bandits
with linear payoff functions. In Proceedings of the Fourteenth
International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 208–214, 2011.

Lai, T. L. and Robbins, H. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, 6(1):4–22,
1985.

Degenne, R. and Perchet, V. Anytime optimal algorithms in
stochastic multi-armed bandits. In International Conference
on Machine Learning, pp. 1587–1595, 2016.

Liu, C.-Y. and Li, L. On the prior sensitivity of Thompson sampling. In International Conference on Algorithmic Learning
Theory (ALT), pp. 321–336, 2016.

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits
Mandl, P. Estimation and control in markov chains. Advances in
Applied Probability, pp. 40–60, 1974.
Prandini, M. and Campi, M. C. Adaptive LQG control of inputoutput systems—A cost-biased approach. SIAM Journal on
Control and Optimization, 39(5):1499–1519, 2000.
Rusmevichientong, P. and Tsitsiklis, J. N. Linearly parameterized
bandits. Mathematics of Operations Research, 35(2):395–411,
2010.
Russo, D. and Van Roy, B. Learning to optimize via informationdirected sampling. In Advances in Neural Information Processing Systems, pp. 1583–1591, 2014.
Russo, D. and Van Roy, B. Learning to optimize via informationdirected sampling. Operations Research, 66(1):230–252,
2018a.
Russo, D. and Van Roy, B. Learning to optimize via informationdirected sampling. Operations Research, 66(1):230–252,
2018b.
Ryzhov, I. O., Frazier, P. I., and Powell, W. B. On the robustness of a one-period look-ahead policy in multi-armed bandit
problems. Procedia Computer Science, 1(1):1635–1644, 2010.
Ryzhov, I. O., Powell, W. B., and Frazier, P. I. The knowledge gradient algorithm for a general class of online learning problems.
Operations Research, 60(1):180–195, 2012.
Scott, S. L. A modern Bayesian look at the multi-armed bandit. Applied Stochastic Models in Business and Industry, 26(6):
639–658, 2010.
Srinivas, N., Krause, A., Kakade, S. M., and Seeger, M. W.
Information-theoretic regret bounds for Gaussian process optimization in the bandit setting. IEEE Transactions on Information Theory, 58(5):3250–3265, 2012.
Stettner, Ł. On nearly self-optimizing strategies for a discretetime uniformly ergodic adaptive model. Applied Mathematics
and Optimization, 27(2):161–177, 1993.
Wainwright, M. J. High-dimensional statistics: A non-asymptotic
viewpoint, volume 48. Cambridge University Press, 2019.
Wang, Y., Wang, C., and Powell, W. The knowledge gradient for
sequential decision making with stochastic binary feedbacks.
In International Conference on Machine Learning (ICML), pp.
1138–1147, 2016.
Whittle, P. Multi-armed bandits and the Gittins index. Journal of
the Royal Statistical Society: Series B (Methodological), 42(2):
143–149, 1980.

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

Appendix
A. Proof of the Index Strategy in (8)
RBMLE
RBMLE
Recall that ηbtRBMLE = (b
ηt,1
, · · · , ηbt,N
) is the reward-biased MLE for η and from (6) that ηbtRBMLE is a maximizer of
the following problem:

max
L(Ht ; η) max exp(ηi α(t)) .
(30)
η:ηj ∈N ,∀j

i∈[N ]

Define an index set and a parameter set as

RBMLE
It := argmax{b
ηt,i
}
i∈[N ]

Ht,i := argmax

η:ηj ∈N ,∀j



(31)

L(Ht ; η) exp(ηi α(t))

(32)

Note that at each time t, RBMLE would select an arm from the index set It , as shown in (7). For each arm i, consider an
(i)
(i)
(i)
estimator η̄t = (η̄t,1 , · · · , η̄t,N ) ∈ Ht,i . Accordingly, we further define an index set
(i)

(i)

It′ := argmax{L(Ht ; η̄t ) exp(η̄t,i α(t))}.

(33)

i∈[N ]

btRBMLE) does not depend on i, we know
Next, we show that the two index sets are identical, i.e. It = It′ . Since L(Ht ; η

RBMLE
RBMLE
btRBMLE ) exp(b
argmax{b
ηt,i
} = argmax L(Ht ; η
ηt,i
α(t)) .
(34)
i∈[N ]

i∈[N ]

Moreover, we have

o
n
RBMLE
RBMLE
btRBMLE ) exp(b
α(t)) = L(Ht ; ηbtRBMLE ) · max exp(b
ηt,i
α(t))
ηt,i
max L(Ht ; η
i∈[N ]
i∈[N ]
n
o
L(Ht ; η) · max exp(ηi α(t))
= max
η:ηj ∈N ,∀j
i∈[N ]
o
n

max L(Ht ; η) · exp(ηi α(t))
= max
η:ηj ∈N ,∀j i∈[N ]
o
n

max
L(Ht ; η) · exp(ηi α(t))
= max
i∈[N ]

η:ηj ∈N ,∀j


(i)
(i)
= max L(Ht ; η̄t ) exp(η̄t,i α(t)) ,
i∈[N ]

(35)
(36)
(37)
(38)
(39)

btRBMLE , (37)-(38)
where (35) follows from the fact that L(Ht ; ηbtRBMLE ) does not depend on i, (36) holds by the definition of η
follow from that interchanging the order of the two max operations does not change the optimal value and the optimizers,
(i)
and (39) follows from the definitions of Ht,i and η̄t . By (32), (33), and (35)-(39), we conclude that It = It′ , and hence
(8) indeed holds.

B. Proof of Proposition 1
Recall from (8) that
πtRBMLE = argmax
i∈[N ]

n

max

η:ηj ∈N ,∀j

o

L(Ht ; η) exp(ηi α(t))

(40)

By plugging L(Ht ; η) into (40) using the density function of the Exponential Families and taking the logarithm of (40),
πtBMLE = argmax

i∈{1,··· ,N }



max

η:ηj ∈N ,∀j

t
nX

τ =1

|

o

ηπτ Xτ − F (ηπτ ) + ηi α(t)
.
{z

=:ℓi (Ht ;η)

}

(41)

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

Note that the inner maximization problem for ℓi (Ht ; η) over η is convex since F (·) is a convex function. Recall that Ni (t)
and Si (t) denote the total number of trials of arm i and the total reward collected from pulling arm i up to time t, as defined
in Section 2. By taking the partial derivatives of ℓi (Ht ; η) with respect to each ηi , we know that ℓi (Ht ; η) is maximized


S (t)
and Ḟ (ηj ) = Njj (t) , for j 6= i, where [·]Θ denotes the clipped value within the set Θ. For
when Ḟ (ηi ) = Si (t)+α(t)
Ni (t)
Θ
each i = 1, · · · , N , we then define
Si (t) 
,
(42)
ηi∗ := Ḟ −1
Ni (t)
h S (t) + α(t) i 
i
.
(43)
ηi∗∗ := Ḟ −1
Ni (t)
Θ
By substituting {ηi∗ } and {ηi∗∗ } into (41), we have
n
o
πtBMLE = argmax ℓi Ht ; ηi∗∗ , {ηj∗∗ }j6=i
i∈{1,··· ,N }

= argmax

i∈{1,··· ,N }

= argmax
i∈{1,··· ,N }

(44)

n
o
ℓi Ht ; ηi∗∗ , {ηj∗ }j6=i ) − ℓi (Ht ; {ηj∗ }j=1,··· ,N
h

i h
i
 ∗∗
∗∗
∗
∗
(Si (t) + α(t)) ηi − Ni (t)F (ηi ) − Si (t)ηi − Ni (t)F (ηi ) .

By substituting Ni (t)pi (t) for Si (t) in (46), we then arrive at the index as
h
i h
i

I(pi (t), Ni (t), α(t)) = (Ni (t)pi (t) + α(t)) ηi∗∗ − Ni (t)F (ηi∗∗ ) − Ni (t)pi (t)ηi∗ − Ni (t)F (ηi∗ ) .

(45)
(46)

(47)


C. Proof of Corollary 1
Recall from (47) that for the Exponential Family rewards, the BMLE index is
h
i h
i

I(pi (t), Ni (t), α(t)) = (Ni (t)pi (t) + α(t)) ηi∗∗ − Ni (t)F (ηi∗∗ ) − Ni (t)pi (t)ηi∗ − Ni (t)F (ηi∗ ) .

(48)

η

−1
e
1
θ
For the Bernoulli case, we know F (η) = log(1 + eη ), Ḟ (η) = 1+e
), and F (Ḟ −1 (θ)) = log( 1−θ
).
(θ) = log( 1−θ
η , Ḟ
−1
Since Θ = [0, 1] for Bernoulli rewards, we need to analyze the following two cases when substituting the above Ḟ (θ)
and F (Ḟ −1 (θ)) into (48):

• Case 1: α(t) < Ni (t)(1 − pi (t)) (or equivalently p̃i (t) < 1)
We have

I(pi (t), Ni (t), α(t))


= Ni (t)pi (t) + α(t) log




Ni (t)pi (t) + α(t)
Ni (t)
− Ni (t) log
Ni (t) − (Ni (t)pi (t) + α(t))
Ni (t) − (Ni (t)pi (t) + α(t))




Ni (t)
Ni (t)pi (t)
+ Ni (t) log
− Ni (t)pi (t) log
Ni (t) − Ni (t)pi (t)
Ni (t) − Ni (t)pi (t)



α(t) 
α(t) 
α(t) 
α(t) 
log pi (t) +
) log 1 − (pi (t) +
)
+ 1 − (pi (t) +
=Ni (t) pi (t) +
Ni (t)
Ni (t)
Ni (t)
Ni (t)

− pi (t) log(pi (t)) − (1 − pi (t)) log(1 − pi (t)) ,

(49)
(50)
(51)
(52)
(53)

where (52)-(53) are obtained by reorganizing the terms in (50)-(51).
• Case 2: α(t) ≥ Ni (t)(1 − pi (t)) (or equivalently p̃i (t) = 1)
In this case, the index would be the same as the case where pi (t) + α(t)/Ni (t) = 1. Therefore, we simply have
n
o
I(pi (t), Ni (t), α(t)) = Ni (t) − pi (t) log(pi (t)) − (1 − pi (t)) log(1 − pi (t)) .

(54)


Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

D. Proof of Corollary 2
Recall from (47) that for the Exponential Family rewards, the BMLE index is
h
i h
i

I(pi (t), Ni (t), α(t)) = (Ni (t)pi (t) + α(t)) ηi∗∗ − Ni (t)F (ηi∗∗ ) − Ni (t)pi (t)ηi∗ − Ni (t)F (ηi∗ ) ,
Si (t) 
∗∗
Ni (t) and ηi
σ 2 ηi2 /2, Ḟ (ηi ) =

where ηi∗ = Ḟ −1

= Ḟ −1

have F (ηi ) =
BMLE index becomes

σ 2 ηi , Ḟ

Si (t)+α(t) 
.
Ni (t)

−1

(55)

For Gaussian rewards with the same variance σ 2 for all arms, we

(θi ) = θi /σ 2 , and F (Ḟ −1 (θi )) = θi2 /2σ 2 , for each arm i. Therefore, the

I(pi (t), Ni (t), α(t))

(56)
2

σ
Si (t) + α(t)
(Si (t) + α(t)) − Ni (t)
σ 2 Ni (t)
2
2
2Si (t)α(t) + α(t)
.
=
2σ 2 Ni (t)

=

Si (t) + α(t) 2
Si (t)
σ 2  Si (t) 2
−
S
(t)
+
N
(t)
i
i
σ 2 Ni (t)
σ 2 Ni (t)
2 σ 2 Ni (t)

(57)
(58)

Equivalently, for the Gaussian rewards, the selected arm at each time t is
n
α(t) o
πtBMLE = argmax pi (t) +
.
2Ni (t)
i∈{1,··· ,N }

(59)


E. Proof of Corollary 3
Recall from (47) that for the Exponential Family distributions, the BMLE index is
h
i h
i

I(pi (t), Ni (t), α(t)) = (Ni (t)pi (t) + α(t)) ηi∗∗ − Ni (t)F (ηi∗∗ ) − Ni (t)pi (t)ηi∗ − Ni (t)F (ηi∗ ) ,

Si (t) 
∗∗
= Ḟ −1 Si (t)+α(t)
. For the exponential distribution, we have F (ηi )
Ni (t) and ηi
Ni (t)
−1
−1
−1
−1
(θi ) = θi , and F (Ḟ (θi )) = log θi , for each arm i. Therefore, the BMLE index becomes
ηi , Ḟ

where ηi∗ = Ḟ −1
Ḟ (ηi ) =

(60)

= log( −1
ηi ),

I(pi (t), Ni (t), α(t))

(61)



 N (t)p (t) + α(t) 
Ni (t)
i
i
=(Ni (t)pi (t) + α(t)) · −
− Ni (t) log
Ni (t)pi (t) + α(t)
Ni (t)

1 
− Ni (t)pi (t) −
+ Ni (t) log pi (t)
pi (t)

Ni (t)pi (t)
=Ni (t) log
.
Ni (t)pi (t) + α(t)

(62)
(63)
(64)


F. Proof of Lemma 1
(i) Recall that



α(t) 
α(t) 
+ nF Ḟ −1 (ν) .
− nν Ḟ −1 (ν) − nF Ḟ −1 ν +
I(ν, n, α(t)) = nν + α(t) Ḟ −1 ν +
n
n
By taking the partial derivative of I(ν, n, α(t)) with respect to n, we have

 ∂ Ḟ −1 ν + α(t)
α(t) 
∂I
−1
−1
n
+ nν + α(t)
= ν Ḟ
ν+
− ν Ḟ (ν)
∂n
n
∂n

−1



ν + α(t)
α(t)  ∂ Ḟ
α(t) 
−1
−1
n
− nḞ Ḟ
ν+
− F Ḟ
+ F Ḟ −1 (ν)
ν+
n
n
∂n
i h 
h
i
α(t) 
α(t) 
−1
−1
−1
−1
− F Ḟ (ν) .
− Ḟ (ν) − F Ḟ
= ν · Ḟ
ν+
ν+
n
n

(65)
(66)
(67)

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

Since Ḟ (·) is strictly increasing for the Exponential Families, we know Ḟ −1 (·) is also strictly increasing and Ḟ −1 (ν +
α(t)/n) > Ḟ −1 (ν). Moreover, by the strict convexity of F (·), we have



  −1
α(t) 
α(t) 
−1
−1
−1
−1
(68)
ν+
− Ḟ (ν) · Ḟ Ḟ (ν) .
− F Ḟ (ν) > Ḟ
ν+
F Ḟ
n
n
| {z }
=ν

Therefore, by (65)-(68), we conclude that

∂I
∂n

< 0 and hence I(ν, n, α(t)) is strictly decreasing with n.

(ii) Recall that

 −1

α(t) 
α(t) 
−1
−1
−1
I(ν, n, α(t)) = nν + α(t) Ḟ
ν+
+ nF Ḟ (ν) .
− nν Ḟ (ν) − nF Ḟ
ν+
n
n

By taking the partial derivative of I(ν, n, α(t)) with respect to ν, we have
 
∂ Ḟ −1 ν + α(t)
∂I
α(t) 
∂ Ḟ −1 (ν) 
−1
−1
n
+ (nν + α(t))
= nḞ
ν+
− nḞ (ν) + nν
∂ν
n
∂ν
∂ν
α(t) 
−1
−1




ν+ n
∂ Ḟ (ν)
α(t)  ∂ Ḟ
−1
−1
− n Ḟ Ḟ
ν+
+ n Ḟ Ḟ (ν)
∂ν
∂ν
{z
}
|
{z n }
|

(69)
(70)

=ν

≤ν+α(t)/n

i
h
α(t) 
−1
−1
− Ḟ (ν)
≥ n · Ḟ
ν+
n
> 0,

(71)
(72)

where the last inequality follows from the fact that Ḟ −1 (·) is strictly increasing for the Exponential Families. Therefore,
we can conclude that I(ν, n, α(t)) is strictly increasing with ν, for all α(t) > 0 and for all n > 0.

G. Proof of Lemma 2
Recall that we define
i
h
i
h 
1  −1
1
1 
−1
−1
−1
ξ(k; ν) =k ν + Ḟ (ν + ) − ν Ḟ (ν) − k F Ḟ
− F (Ḟ (ν)) ,
ν+
k
k
k
K ∗ (θ′ , θ′′ ) = inf{k : Ḟ −1 (θ′ ) > ξ(k; θ′′ )}.

(73)
(74)

Moreover, we have I(µ1 , kα(t), α(t)) = α(t)ξ(k; µ1 ). By Lemma 1.(i), we know that I(µ1 , kα(t), α(t)) decreases with
k, for all k > 0. Let z = k1 . Under any fixed µ1 ∈ Θ and α(t) > 0, we also know that


 


µ1 + z Ḟ −1 (µ1 + z) − µ1 Ḟ −1 (µ1 ) − F Ḟ −1 µ1 + z − F (Ḟ −1 (µ1 ))
lim ξ(k; µ1 ) = lim
(75)
k→∞
z↓0
z
 ∂ Ḟ −1 (µ1 + z)
∂ Ḟ −1 (µ1 + z)
−1
−1
= lim Ḟ (µ1 + z) + (µ1 + z)
− Ḟ Ḟ (µ1 + z)
(76)
z↓0
∂z
∂z
=Ḟ −1 (µ1 ),

(77)

where (75) is obtained by replacing 1/k with z, and (76) follows from L’Hôpital’s rule. Therefore, we have
lim I(µ1 , kα(t), α(t)) = α(t) · Ḟ

k→∞

−1

(µ1 ).

(78)

By Lemma 1.(i) and (78), we know
I(µ1 , kα(t), α(t)) ≥ α(t)Ḟ

−1

(µ1 ), for all k > 0.

(79)

For any n2 > K ∗ (µ1 , µ2 )α(t), we have
I(µ1 , n1 , α(t)) ≥ α(t)Ḟ (µ1 )
≥ I(µ2 , K ∗ (µ1 , µ2 )α(t), α(t))
−1

> I(µ2 , n2 , α(t)),

(80)
(81)
(82)

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

where (80) follows from (79), (81) holds from the definition of K ∗ (·, ·), and (82) holds due to Lemma 1.(i). Finally, we
show that K ∗ (µ1 , µ2 ) is finite given that µ1 > µ2 . We consider the limit of ξ(k; µ2 ) when k approaches zero and again let
z = k1 :


 


µ2 + z Ḟ −1 (µ2 + z) − ν Ḟ −1 (µ2 ) − F Ḟ −1 µ2 + z − F (Ḟ −1 (µ2 ))
lim ξ(k; µ2 ) = lim
(83)
z→∞
k↓0
z
 ∂ Ḟ −1 (µ2 + z)
∂ Ḟ −1 (µ2 + z)
−1
−1
− Ḟ Ḟ (µ2 + z)
(84)
= lim Ḟ (µ2 + z) + (µ2 + z)
z→∞
∂z
∂z
{z
}|
|
{z
} |
{z
}
≥0

≥ lim Ḟ

−1

z→∞

≥Ḟ

−1

≤µ2 +z

≥0

(µ2 + z)

(85)

(µ1 ),

(86)

where (84) follows from L’Hôpital’s rule and (86) holds due to the fact that Ḟ −1 is increasing. By (83)-(86) and since
ξ(k; µ2 ) is continuous and strictly decreasing with k, we know there must exist a finite k ′ ≥ 0 such that Ḟ −1 (µ1 ) =
ξ(k ′ ; µ2 ). This implies that K ∗ (µ1 , µ2 ) is finite given that µ1 > µ2 .


H. Proof of Lemma 3
Similar to the proof of Lemma 2, we leverage the function K ∗ (·, ·) as defined in (74). By (74), we know that for any
k > K ∗ (µ0 , µ2 ), we have ξ(k; µ2 ) < Ḟ −1 (µ0 ). Therefore, if n2 > K ∗ (µ0 , µ2 )α(t),
I(µ2 , n2 , α(t)) < I(µ2 , K ∗ (µ0 , µ2 ), α(t))

(87)

∗

= α(t)ξ(K (µ0 , µ2 ); µ2 )

(88)

= α(t)Ḟ

(µ0 ).

(89)

−1

Similarly, for any k ≤ K ∗ (µ0 , µ1 ), we have ξ(k; µ1 ) ≥ Ḟ −1 (µ0 ). Then, if n1 ≤ K ∗ (µ0 , µ1 )α(t), we know
I(µ1 , n1 , α(t)) ≥ I(µ1 , K ∗ (µ0 , µ1 ), α(t))
= α(t)ξ(K ∗ (µ0 , µ1 ); µ1 )
= α(t)Ḟ

−1

(90)
(91)

(µ0 ).

(92)

Hence, by (87)-(92), we conclude that I(µ1 , n1 , α(t)) > I(µ2 , n2 , α(t)), for all n1 ≤ K ∗ (µ0 , µ1 )α(t) and n2 >
K ∗ (µ0 , µ2 )α(t).


I. Proof of Proposition 2
Proof Sketch: Our target is to quantify the expected number of trials of each sub-optimal arm a up to time T . The
regret bound proof starts with a similar demonstration as for UCB1 (Auer et al., 2002) by studying the probability of the
event {I(p1 (t), N1 (t), α(t)) ≤ I(pa (t), Na (t), α(t))}, using the Chernoff bound for Exponential Families. However, it is
significantly different from the original proof as the dependency between the level of exploration and the bias term α(t) is
technically more complex, compared to the straightforward confidence interval used by the conventional UCB-type policies.
Specifically, the main challenge lies in characterizing the behavior of the RBMLE index for both regimes where N1 (t) is
small compared to α(t), as well as when it is large compared to α(t). Such a challenge is handled by considering three
cases separately: (i) Consider N1 (t) > D(θ1 −4ε ∆,θ1 ) log t and apply Lemma 2; (ii) Consider N1 (t) ≤ D(θ1 −4ε ∆,θ1 ) log t
2

and N1 (t) ≤ K ∗ (θ1 − 2ε ∆, θ)α(t) and apply Lemma 3; (iii) Use Lemma 3 to show that {N1 (t) ≤
{N1 (t) > K ∗ (θ1 − 2ε ∆, θ)α(t)} cannot occur simultaneously.

2

4
D(θ1 − 2ε ∆,θ1 )

log t} and

To begin with, for each arm i, we define pi,n to be the empirical average reward collected in the first n pulls of arm i.
For any Exponential Family reward distribution, the empirical mean of each arm i satisfies the following concentration
inequalities (Korda et al., 2013): For any δ > 0,
P(pi,n − θi ≥ δ) ≤ exp(−nD(θi + δ, θi )),

P(θi − pi,n ≥ δ) ≤ exp(−nD(θi − δ, θi )).

(93)
(94)

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

Next, for each arm i, we define the following confidence intervals for each pair of n, t ∈ N:
n
1o
δi+ (n, t) := inf δ : exp(−nD(θi + δ, θi )) ≤ 4 ,
t
n
1o
−
δi (n, t) := inf δ : exp(−nD(θi − δ, θi )) ≤ 4 .
t

Accordingly, for each arm i and for each pair of n, t ∈ N, we define the following events:
n
o
+
G+
(n,
t)
=
p
−
θ
≤
δ
(n,
t)
,
i,n
i
i
i
n
o
−
G−
i (n, t) = θi − pi,n ≤ δi (n, t) .

(95)
(96)

(97)
(98)

By the concentration inequality considered in Section 2, we have

+

−nD(θi +δi
P(G+
i (n, t) ) ≤ e

(n,t),θi )

−nD(θi −δi
P(G−
i (n, t) ) ≤ e

(n,t),θi )

c

c

−

1
,
t4
1
≤ 4.
t

≤

(99)
(100)

Consider the bias term α(t) = Cα log t with Cα ≥ 4/(D(θ1 − 2ε ∆, θ1 ) · K ∗ (θ1 − 2ε ∆, θ)) and ε ∈ (0, 1). Recall that
we assume arm 1 is the unique optimal arm. Our target is to quantify the total number of trials of each sub-optimal arm.
Define
o
n
ε
ε
4
, Cα K ∗ (θ1 − ∆a , θa + ∆a ) log T + 1.
(101)
Qa (T ) := max
ε
D(θa + 2 ∆a , θa )
2
2

We start by characterizing E[Na (T )] for each a = 2, · · · , N :
E[Na (T )]

(102)

≤ Qa (T ) + E
= Qa (T ) +



T
X

t=Qa (T )+1
T
X

t=Qa (T )+1

≤ Qa (T ) +
≤ Qa (T ) +
≤ Qa (T ) +

+

T
X

T
X





P I pa (t), Na (t), α(t) ≥ I p1 (t), N1 (t), α(t) , Na (t) ≥ Qa (T )
P

t=Qa (T )+1
T
X



I I(pa (t), Na (t), α(t) ≥ I(p1 (t), N1 (t), α(t), Na (t) ≥ Qa (T )



Qa (T )≤na ≤t

t
X

t
X

t=Qa (T )+1 n1 =1 na =Qa (T )
T
X

t
X

t
X

t=Qa (T )+1 n1 =1 na =Qa (T )
t
X

t
X

t=Qa (T )+1 n1 =1 na =Qa (T )

≤ Qa (T ) +

max

π2
+
3

T
X



I pa,na , na , α(t) ≥ min I p1,n1 , n1 , α(t)
1≤n1 ≤t




P I pa,na , na , α(t) ≥ I p1,n1 , n1 , α(t)





c
+
c
P G−
(n
,
t)
+
P
G
(n
,
t)
1
a
a
1
|
{z
} |
{z
}
≤ t14

t
X

t=Qa (T )+1 n1 =1 na =Qa (T )

(104)

(105)

(106)

(107)

≤ t14





+
(n
,
t),
G
(n
,
t)
P I pa,na , na , α(t) ≥ I p1,n1 , n1 , α(t) , G−
1
a
a
1
t
X

(103)

(108)





+
(n
,
t),
G
(n
,
t)
, (109)
P I pa,na , na , α(t) ≥ I p1,n1 , n1 , α(t) , G−
1
a
a
1

PT
where the last equation follows from the fact that t=Qa (T )+1 ( t12 ) ≤ π 2 /6 and (103) can be obtained by taking the
expectation on both sides of the first inequality of (6) in (Auer et al., 2002) and using the fact that arm i is chosen implies
that i’s index is larger than the optimal arm’s. Next, to provide an upper bound for (109), we need to consider the following
three cases separately. As suggested by (109), we can focus on the case where na ≥ Qa (T ).

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

• Case 1: n1 >
Since n1 >

4
D(θ1 − 2ε ∆,θ1 )

4
D(θ1 − ε2 ∆,θ1 )

4
D(θa + 2ε ∆a ,θa )

log t

log t, we have p1,n1 ≥ θ1 − 2ε ∆ on the event G−
1 (n1 , t). Similarly, as na ≥ Qa (T ) >

log t, we have pa,na ≤ θa + 2ε ∆a on the event G+
a (na , t). Therefore, we know
p1,n1 − pa,na > (1 − ε)∆.

(110)

Then, we have
ε
I(p1,n1 , n1 , α(t)) > I(θ1 − ∆, n1 , α(t))
2
ε
ε
ε
≥ I(θa − ∆, K ∗ (θ1 − ∆, θa + ∆)α(t), α(t))
2
2
2
ε
ε
ε
≥ I(θa − ∆a , K ∗ (θ1 − ∆, θa + ∆)α(t), α(t))
2
2
2
ε
ε
≥ I(pa,na , K ∗ (θ1 − ∆, θa + ∆)α(t), α(t))
2
2
≥ I(pa,na , Qa (T ), α(t))
≥ I(pa,na , na , α(t)),

(111)
(112)
(113)
(114)
(115)
(116)

where (111) and (113)-(114) hold by Lemma 1.(i) (i.e., K ∗ (θ, θ′ ) is strictly decreasing with respect to θ and strictly
increasing with respect to θ′ ), (112) holds by Lemma
 2, and (115)-(116) follow from Lemma 1.(i). Hence, in Case 1, we
always have I p1,n1 , n1 , α(t) > I pa,na , na , α(t) .

• Case 2: n1 ≤

4
D(θ1 − 2ε ∆,θ1 )

log t and n1 ≤ K ∗ (θ1 − 2ε ∆, θ)α(t)

Similar to Case 1, since na ≥ Qa (T ) > D(θa ,θa4+ ε ∆a ) log t, we have pa,na ≤ θa + 2ε ∆a on the event G+
a (na , t).
2
ε
ε
ε
∗
∗
Moreover, as n1 ≤ K (θ1 − 2 ∆, θ)α(t) and na ≥ Qa (T ) > K (θ1 − 2 ∆, θa + 2 ∆)α(t), by Lemma 3 we know
ε
(117)
I(θ, n1 , α(t)) > I(θa + ∆, na , α(t)).
2
Therefore, we obtain that


I p1,n1 , n1 , α(t) > I θ, n1 , α(t)
(118)
ε
(119)
> I(θa + ∆, na , α(t))
2

(120)
> I pa,na , na , α(t) ,

where (118) and (120) follow from Lemma
1.(ii), and (119) is a direct result of (117). Hence, in Case 2, we still have

I p1,n1 , n1 , α(t) > I pa,na , na , α(t) .

• Case 3: n1 ≤

4
D(θ1 − 2ε ∆,θ1 )

log t and n1 > K ∗ (θ1 − 2ε ∆, θ)α(t)

Recall that α(t) = Cα log t with Cα ≥ 4/(D(θ1 − 2ε ∆, θ1 ) · K ∗ (θ1 − 2ε ∆, θ)). Therefore, the two events {n1 ≤
ε
4
∗
D(θ1 − ε ∆,θ1 ) log t} and {n1 > K (θ1 − 2 ∆, θ)α(t)} cannot happen at the same time.
2

To sum up, in all the above three cases, we have




+
P I pa,na , na , α(t) ≥ I p1,n1 , n1 , α(t) , G−
1 (n1 , t), Ga (na , t) = 0.
By (109) and (121), we conclude that E[Na (T )] ≤ Qa (T ) +

π2
3 ,

(121)

for every a 6= 1.

Finally, the total regret can be upper bounded as
R(T ) ≤
=

N
X

a=2
N
X

a=2

∆a · E[Na (T )]
∆a



(122)


o
ε
π2
ε
.
, Cα K (θ1 − ∆a , θa + ∆a ) log T + 1 +
max
D(θa + 2ε ∆a , θa )
2
2
3
n

4

∗

(123)


Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

J. Proof of Proposition 3
Proof Sketch: We extend the proof procedure of Proposition 2 for Gaussian rewards, with the help of Hoeffding’s inequality. We then prove an additional lemma, which shows that conditioned on the “good” events, the RBMLE index of the
2
optimal arm (i.e., arm 1) is always larger than that of a sub-optimal arm a if Na (t) ≥ ∆2a α(t) and α(t) ≥ 256σ
∆a , regardless
of N1 (t).
We extend the proof of Proposition 2 to the case of Gaussian rewards. To begin with, we define the confidence intervals
and the “good” events. Recall that for each arm i, we define pi,n to be the empirical average reward collected in the first n
pulls of arm i. For each arm i, for each pair of n, t ∈ N, we define
n

1o
δi (n, t) := inf δ : max exp(−nD(θi + δ, θi )), exp(−nD(θi − δ, θi )) ≤ 4 .
(124)
t
Accordingly, for each arm i and for each pair of n, t ∈ N, we define the following events:
n
o
Gi (n, t) = |pi,n − θi | ≤ δi (n, t) ,

(125)

For the Gaussian rewards, we can leverage Hoeffding’s inequality for sub-Gaussian distributions as follows:
Lemma J.1 Under σ-sub-Gaussian rewards for all arms, for any n ∈ N, we have
P(|pi,n − θi | ≥ δ) ≤ 2 exp(−

n 2
δ ).
2σ 2

(126)

Proof of Lemma J.1: This is a direct result of Proposition 2.5 in (Wainwright, 2019).
′

′′

Based on Lemma J.1, we focus on the case D(θ , θ ) =
notation, we use γ∗ to denote the constant 8σ 2 .

1
′
2σ2 (|θ


p
2
− θ |) and δi (n, t) = (8σ log t)/n. For ease of
′′

2

Before providing the regret analysis, we first introduce the following useful lemma.
Lemma J.2 Suppose γ > 0 and µ1 , µ2 ∈ R with µ1 > µ2 . Given α(t) = c log t with c ≥ µ132γ
−µ2 , for any n2 ≥
p
p
and any n1 > 0, we have I(µ1 − (γ log t)/n1 , n1 , α(t)) > I(µ2 + (γ log t)/n2 , n2 , α(t)).

2
µ1 −µ2 α(t)

Proof of Lemma J.2: We start by considering n2 ≥ M α(t), for some M > 0. Then, note that
r
r


γ log t
γ log t α(t)
, n1 , α(t) = µ1 −
+
,
(127)
I µ1 −
n1
n1
2n1
r
r


γ log t
γ log t α(t)
I µ2 +
, n2 , α(t) = µ2 +
+
.
(128)
n2
n2
2n2
p
p
For ease of notation, we use x1 and x2 to denote (γ log t)/n1 and (γ log t)/n2 , respectively. Then, we know
r
r




c 2
γ log t
γ log t
, n1 , α(t) − I µ2 +
, n2 , α(t) ≥ (µ1 − µ2 ) − (x1 + x2 ) +
(x − x22 )
(129)
I µ1 −
n1
n1
2γ 1
r
c 2
1
γ
+
x1 −
,
(130)
≥ (µ1 − µ2 ) − x1 −
cM
2γ
2M
p γ
c 2
1
+ 2γ
x1 − 2M
. The quadratic polynomial
where (130) follows from n2 ≥ M α(t). Define w(x1 ) := (µ1 − µ2 )− x1 − cM
w(x1 ) remains positive for all x1 ∈ R if the discriminant of w(x1 ), denoted by Disc(w(x1 )), is negative. Indeed, we have
r
c
1
γ
Disc(w(x1 )) = 1 − 4 ·
· (−
−
+ (µ1 − µ2 )) ≤ −39,
(131)
2γ
cM
2M
where the last inequality follows from c ≥

32γ
µ1 −µ2

and M =

2
µ1 −µ2 .


∗

∗
2
Now, we are ready to prove Proposition 3: Consider the bias term α(t) = Cα log t with Cα ≥ 32γ
∆ , where γ = 8σ .
Recall that we assume arm 1 is the unique optimal arm. Our target is to quantify the total number of trials of each suboptimal arm. Next, we characterize the expected total number of trials of each sub-optimal arm, i.e., E[Na (T )]. We define

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

Q∗a (T ) =

2
∆a Cα

E[Na (T )] ≤
≤ Q∗a (T ) +
≤ Q∗a (T ) +

+

T
X

log T . By using a similar argument to (102)-(109), we have

Q∗a (T )

T
X

+

t=Q∗
a (T )+1
T
X

t
X

t
X

n =1 na =Q∗
t=Q∗
a (T )+1 1
a (T )
T
X

t
X

t
X

n =1 na =Q∗
t=Q∗
a (T )+1 1
a (T )
t
X

t
X

n =1 na =Q∗
t=Q∗
a (T )+1 1
a (T )

≤ Q∗a (T ) +





P I pa (t), Na (t), α(t) ≥ I p1 (t), N1 (t), α(t) , Na (t) ≥ Q∗a (T )

2π 2
+
3

T
X




P I pa,na , na , α(t) ≥ I p1,n1 , n1 , α(t)





P G1 (n1 , t)c + P Ga (na , t)c
|
{z
} |
{z
}
≤ t24

t
X

n =1 na =Q∗
t=Q∗
a (T )+1 1
a (T )

(133)

(134)

≤ t24





P I pa,na , na , α(t) ≥ I p1,n1 , n1 , α(t) , G1 (n1 , t), Ga (na , t)
t
X

(132)

(135)





P I pa,na , na , α(t) ≥ I p1,n1 , n1 , α(t) , G1 (n1 , t), Ga (na , t) . (136)

Conditioned on the events Gi (n1 , t) and Ga (na , t), we obtain that
p

I p1,n1 , n1 , α(t) ≥ I(θ1 − (γ∗ log t)/n1 , n1 , α(t))
p
> I(θa + (γ∗ log t)/na , na , α(t))

≥ I pa,na , na , α(t) ,

(137)
(138)
(139)

where (137) and (139) follow from Lemma 1.(i), and (138) follows from Lemma J.2. Hence, for n1 > 0 and na ≥ Q∗a (T ),




(140)
P I pa,na , na , α(t) ≥ I p1,n1 , n1 , α(t) , G1 (n1 , t), Ga (na , t) = 0.
By (136) and (140), we know E[Na (T )] ≤ Q∗a (T ) +
as
R(T ) ≤

N
X

a=2

2π 2
3 ,

∆a

for every a 6= 1. Hence, the total regret can be upper bounded

 2
2π 2 
.
Cα log T +
∆a
3

(141)


K. Proof of Proposition 5
The proof of Proposition 2 can be easily extended to Proposition 5 by replacing the Chernoff bound with the subExponential tail bound. For sub-exponential reward distributions, we consider the sub-exponential tail bound as follows:
Lemma K.1 Under (ρ, κ)-sub-exponential rewards for all arms, for any n ∈ N, we have


n2 δ 2
.
(142)
P(pi,n − θi ≥ δ) ≤ exp −
2(nκδ + ρ2 )
Similar to the proof of Proposition 2, we consider the bias term α(t) = Cα log t, but with Cα ≥
(ε∆)2
ε∆
16(κε∆ + 2ρ2 )/((ε∆)2 K ∗ (θ1 − ε∆
2 , θ)). Note that here we simply replace D(θ1 − 2 , θ1 ) with 4(κε∆+2ρ2 ) by comparing (142) with (93). Similarly, we define
n 16(κε∆ + 2ρ2 )
o
ε
ε
∗
Q̃a (T ) := max
,
C
K
(θ
−
∆
,
θ
+
∆
)
log T + 1.
(143)
α
1
a a
a
(ε∆a )2
2
2
Note that the proof of Proposition 2 relies only on Lemmas 1-3, and these lemmas are tied to the distributions for deriving
the RBMLE index, not to the underlying true reward distributions. Therefore, it is easy to verify that the same proof
procedure still holds here by replacing Qa (T ) with Q̃a (T ).


Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

L. Proof of Proposition 6
Proof Sketch: An O(log T ) regret bound can be obtained by considering the extensions as follows:
• By extending Lemma J.2, we show that for any two arms i and j, there exist constants M1 > 0 and M2 > 0 such that
Ii > Ij for any ni ≤ M1 log T and nj ≥ M2 log T .

• We then extend (132)-(133) by using the fact that arm a is chosen implies that its index is larger than all the other arm’s.

• Extend (134)-(136) by considering the good events across all arms (instead of just arm a and the optimal arm).
ˆ t is between
• Finally, we extend (137)-(139) by using Lemma J.2 and the fact that under the good events, the estimated ∆
∆/2 and ∆.
In Algorithm 1, since gradually learning the minimal gap ∆ involves all the arms (see Line 7 of Algorithm 1), we need to
extend Lemma J.2 to remove the assumption µ1 > µ2 . The extension is conducted in Lemma L.1 below.
Lemma L.1 Let γ be a positive constant. For any two arms i and j with µi , µj ∈ R and δ := µj − µi , given α(t) = c log t
p
+2)
∆
, then for any ni ≤ 81 M max(δ,∆)
log(t) and nj ≥ M log t, we have I(µi − (γ log t)/ni , ni , α(t)) >
with c ≥ 32γ(N
∆
p
+2)
.
I(µj + (γ log t)/nj , nj , α(t)), where M = 32γ(N
∆2
p
Proof of Lemma L.1: For ease of notation, we use Ii and Ij to denote I(µi − (γ log t)/ni , ni , α(t)) and I(µj +
p
(γ log t)/nj , nj , α(t)), respectively, within this proof. Then, we have
r
γ log t α(t)
Ii = µi −
+
,
(144)
n
2ni
s i
γ log t α(t)
+
.
(145)
Ij = µj +
nj
2nj
p
Using x to denote (γ log t)/ni , we have
r
r
c 2
c  γ 2
γ
(146)
+
x −
Ii − Ij ≥ (µi − µj ) − x −
M
2γ
2γ
M
r
c
c 2
γ
= −δ −
−
+
x − x,
(147)
M
2M
2γ

∆
log(t), we have
where (146) follows from nj ≥ M log(t). Since ni ≤ 18 M max(δ,∆)
s
s
r
γ log t
8γ log t
∆ · max(δ, ∆)
x=
≥
.
=
32γ(N +2)
∆
ni
4(N + 2)
2
∆

(148)

max(δ,∆)

By (148) and the fact that the cx2 /(2γ) − x has its minimum at x = γ/c ≤ ∆/(32(N + 2)), we can construct a lower
bound for cx2 /(2γ) − x in (147):
r
c 2
8γ max(δ, ∆)
c 8γ max(δ, ∆)
x −x≥
−
.
(149)
2γ
2γ M
∆
M
∆
Then we can obtain a lower bound of Ii − Ij :
r
r
γ
8γ max(δ, ∆)
c
c
max(δ, ∆)
Ii − Ij ≥ −δ −
−
+
·8
−
·
(150)
M
2M
2M
∆
M
∆
r
r γ
8γ max(δ, ∆)   c
c
max(δ, ∆) 
−
(151)
+
·
−
·8
= −δ −
M
M
∆
2M
2M
∆
r
√
c
max(δ, ∆)
γ max(δ, ∆)
·
+
·7·
(152)
≥ −δ − ( 8 + 1)
M
∆
2M
∆
s
√
∆ max(δ, ∆) 7
+ max(δ, ∆),
(153)
≥ −δ − ( 8 + 1)
32(N + 2)
2
> 0,

(154)

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

where (152)-(153) follow from that c ≥

32γ(N +2)
∆

and M =

32γ(N +2)
.
∆2



+2)
Proof of Proposition 6: First, we set γ = 8σ 2 . Recall that T0 := min{t ∈ N : β(t) ≥ 32γ(N
} < ∞. Within this proof,
∆
p
we take δi (n, t) = (2σ 2 (N + 2) log t)/n and define the good events as Gi (n, t) := {|pi,n − θi | ≤ δi (n, t)}. By Lemma
+2)
log T, T0 }. Denote by
J.1, we know P(Gi (ni , t)c ) ≤ 2/tN +2 , for all t and i. We also define Q∗a (T ) := max{4 · 32γ(N
∆2
E[Na (T )] the expected total number of trials of arm a. Then for each a, we can construct an upper bound of E[Na (T )] by:

E[Na (T )] ≤ Q∗a (T ) +
≤ Q∗a (T ) +
≤ Q∗a (T ) +

+

T
X

T
X

t=Q∗
a (T )+1





P I pa (t), Na (t), α(t) ≥ I pi (t), Ni (t), α(t) , ∀i 6= a, Na (t) ≥ Q∗a (T )

T
X

t
X

t
X




P I pa,na , na , α(t) ≥ I pi,ni , ni , α(t), ∀i 6= a

T
X

t
X

t
X

N
X

n =1 na =Q∗
t=Q∗
a (T )+1 i
a (T )
i6=a

n =1 na =Q∗
t=Q∗
a (T )
a (T )+1 i
i6=a
t
X

t
X

n =1 na =Q∗
t=Q∗
a (T )
a (T )+1 i
i6=a

|

i

P Gi (ni , t)c
{z

≤2N/tN +2



(155)

(156)

(157)

}





P I pa,na , na , α(t) ≥ I pi,ni , n, α(t) , ∀i 6= a, G1 (n1 , t), G2 (n2 , t), · · · , GN (nN , t)
(158)

≤ Q∗a (T ) +
+

T
X

2

Nπ
3
t
X

(159)
t
X

n =1 na =Q∗
t=Q∗
a (T )+1 1
a (T )





P I pa,na , na , α(t) ≥ I pi,ni , ni , α(t) , ∀i 6= a, G1 (n1 , t), G2 (n2 , t), · · · , GN (nN , t) .

(160)

+2)
∆
• Case 1: ni ≥ 18 32γ(N
∆2
max(θa −θi ,∆) log t for all i 6= a. Since ni is large enough, it is easy to check that the following
inequality holds under the good events for all i 6= a:
v
u
u
2σ 2 log t
max(θa − θi , ∆)
max(θa − θi , ∆)
√
≤
≤
|pi,ni − θi | ≤ t 32γ(N +2)
.
(161)
∆
1
8
4 N +2
8
∆2
max(θa −θi ) log t

∆
Similarly, we have |pa,na − θa | ≤ 8√2√
. Moreover, by checking Li (t) and Ui (t) under the good events, we also
N +2
∆
bα (t) ≤ 32γ(N +2) and hence α(t) ≤ 32γ(N +2) log t. Therefore by Lemma J.2,
b t ≥ . This also implies that C
know ∆
2
∆/2
∆/2

we know that I p1,n1 , n1 , α(t) ≥ I pa,na , na , α(t)).

• Case 2: There exists some i 6= a such that ni <

I pi,ni , ni , α(t) > I pa,na , na , α(t)).

∆
1 32γ(N +2)
8
∆2
max(θa −θi ,∆)

log t. By Lemma L.1, this implies that

Therefore, we have for every a 6= 1,

N π2
E[Na (T )] ≤ Q∗a (T ) +
3
n 128γ(N + 2)
o N π2
= max
log
T,
T
.
+
0
∆2
3
Hence, the total regret can be upper bounded as
R(T ) ≤

o N π2 i
h
n 128γ(N + 2)
log
T,
T
+
.
∆a max
0
∆2
3
a=2
N
X

(162)
(163)

(164)


Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

M. Additional Empirical Results
In this subsection, we present additional empirical results for more examples to demonstrate the effectiveness, efficiency
and scalability of the proposed RBMLE algorithm.
M.1. Effectiveness
Figures 3-4 illustrate the effectiveness of RBMLE with respect to the cumulative regret, under a different set of parameters,
for the three types of bandits. Tables 2-10 provide detailed statistics, including the mean as well as the standard deviation
and quantiles of the final regrets, with the row-wise smallest values highlighted in boldface. From the Tables, we observe
that RBMLE tends to have the smallest value of regret at medium to high quantiles, and comparable to the smallest values at
other lower quantiles among those that have comparable mean values (e.g., IDS, VIDS, KLUCB). Along with the presented
statistics of standard deviation, they suggest that RBMLE’s performance enjoys comparable robustness as those baselines
that achieve similar mean regret.
M.2. Efficiency
Figures 5-6 present the efficiency of RBMLE in terms of averaged computation time per decision (ACTPD) vs. averaged
final cumulative regret. The computation times are measured on a Linux server with (i) an Intel Xeon E7 v4 server operating
at a maximal clock rate of 3.60 GHz, and (ii) a total of 528 GB memory. While there are 64 cores in the server, we force
the program to run on just one core for a fair comparison.
M.3. Scalability
Tables 11-13 show the computation time per decision of different policies under varying numbers of arms.

1500

UCB
MOSS
BUCB
KG
KGMin
KGMN

2500

500

250

0
0

2

4

6

Time Horizon

(a)

8

10
10

4

2000

RBMLE
VIDS
KLUCB & UCB
GPUCB
GPUCBT

TS
BUCB
KG
KG*

Averaged Cumulative Regret

750

RBMLE
IDS
VIDS
KLUCB
UCBT
TS

Averaged Cumulative Regret

Averaged Cumulative Regret

1000

1500

1000

500

0
0

2

4

6

8

10
104

Time Horizon

(b)

1000

RBMLE
VIDS
KLUCB
MOSS
TS
BUCB
UCB
KG

500

0
0

2

4

6

Time Horizon

8

10
10

4

(c)

(θi )10
i=1

Figure 3: Averaged cumulative regret: (a) Bernoulli bandits with
= (0.655, 0.6, 0.665, 0.67, 0.675, 0.68, 0.685,
0.69, 0.695, 0.7) & ∆ = 0.005; (b) Gaussian bandits with (θi )10
i=1 = (0.5, 0.75, 0.4, 0.6, 0.55, 0.76, 0.68, 0.41, 0.52, 0.67)
& ∆ = 0.01; (c) Exponential bandits with (θi )10
i=1 = (0.46, 0.45, 0.5, 0.48, 0.51, 0.4, 0.43, 0.42, 0.45, 0.44) & ∆ = 0.01.

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

2500
UCB
MOSS
BUCB
KG
KGMin
KGMN

750

500

250

2000

1500

800

RBMLE
VIDS
KLUCB & UCB
GPUCB
GPUCBT
TS
BUCB
KG
KG*

RBMLE
VIDS
KLUCB
MOSS
TS
BUCB
UCB
KG

700

Averaged Cumulative Regret

1000

RBMLE
IDS
VIDS
KLUCB
UCBT
TS

Averaged Cumulative Regret

Averaged Cumulative Regret

1250

1000

500

600
500
400
300
200
100

0
0

2

4

6

Time Horizon

8

10
10

0
0

2

4

6

8

Time Horizon

4

(a)

10
4
10

0
0

2

4

6

8

Time Horizon

(b)

10
104

(c)

(θi )10
i=1

Figure 4: Averaged cumulative regret: (a) Bernoulli bandits with
= (0.755, 0.76, 0.765, 0.77, 0.775, 0.78, 0.785,
0.79, 0.795, 0.8) & ∆ = 0.005; (b) Gaussian bandits with (θi )10
=
(0.65,
0.35, 0.66, 0.4, 0.65, 0.64, 0.55, 0.4, 0.57, 0.54)
i=1
& ∆ = 0.01; (c) Exponential bandits with (θi )10
=
(0.25,
0.28,
0.27,
0.3,
0.29, 0.22, 0.21, 0.24, 0.23, 0.26) & ∆ = 0.01.
i=1
Table 2: Statistics of the final cumulative regret in Figure 1(a). The best in each row is highlighted.
Algorithm
Mean Regret
Std. Dev.
Quantile .10
Quantile .25
Quantile .50
Quantile .75
Quantile .90
Quantile .95

RBMLE
263.5
233.5
142.4
161.4
190.6
237.8
430.0
993.1

IDS
406.3
466.7
74.7
113.5
184.3
461.4
1138.3
1247.9

VIDS
449.6
618.2
54.2
90.0
134.5
1043.2
1116.4
1247.9

KLUCB
730.4
109.3
584.4
661.0
717.7
804.0
860.9
926.3

UCBT
474.7
176.3
309.9
362.8
448.7
543.5
655.0
759.7

TS
426.9
149.3
283.2
326.6
404.0
489.2
595.1
647.6

UCB
1809.5
113.0
1647.3
1750.4
1815.5
1874.3
1963.1
1996.6

MOSS
464.5
93.1
355.8
394.2
458.9
520.8
570.8
615.6

BUCB
580.9
105.8
452.9
519.2
563.77
638.0
713.8
766.0

KG
2379.5
2163.2
3.7
1000.9
2001.4
3999.8
5013.8
6992.2

KGMin
2384.2
355.4
1899.1
2103.2
2411.6
2620.3
2809.4
2911.4

KGMN
1814.3
344.0
1344.8
1555.3
1844.2
2057.7
2254.1
2326.5

Table 3: Statistics of the final cumulative regret in Figure 3(a). The best in each row is highlighted.
Algorithm
Mean Regret
Std. Dev.
Quantile .10
Quantile .25
Quantile .50
Quantile .75
Quantile .90
Quantile .95

RBMLE
361.5
247.6
133.0
165.1
223.5
608.4
661.2
722.9

IDS
371.3
285.9
116.3
164.4
262.8
568.9
681.7
835.1

VIDS
416.7
342.8
77.2
147.0
248.3
593.1
1003.2
1060.9

KLUCB
831.9
131.9
650.3
732.4
823.1
930.3
1020.6
1062.0

UCBT
616.9
130.7
440.6
532.3
598.0
693.4
779.5
857.9

TS
505.8
156.3
334.4
385.0
477.9
575.3
698.3
793.4

UCB
1437.9
78.5
1335.8
1388.3
1436.7
1495.6
1540.4
1561.2

MOSS
582.9
169.9
411.6
461.1
532.7
654.8
816.1
943.2

BUCB
712.5
120.3
564.3
641.2
715.1
782.6
865.3
906.4

KG
1637.9
1592.7
2.3
501.5
1002.6
2996.5
3499.3
4497.6

KGMin
1309.0
214.1
1013.3
1184.1
1338.8
1447.2
1538.3
1620.9

KGMN
991.1
198.4
741.9
876.2
987.8
1119.8
1228.4
1343.0

Table 4: Statistics of the final cumulative regret in Figure 4(a). The best in each row is highlighted.
Algorithm
Mean Regret
Std. Dev.
Quantile .10
Quantile .25
Quantile .50
Quantile .75
Quantile .90
Quantile .95

RBMLE
313.2
228.1
142.2
169.4
203.7
369.7
680.2
720.2

IDS
355.7
386.5
95.1
133.2
179.0
543.2
695.5
891.4

VIDS
425.5
474.7
70.2
102.5
173.9
589.7
1067.4
1739.8

KLUCB
740.5
126.9
581.4
651.6
725.9
806.5
886.3
999.6

UCBT
669.5
120.3
506.7
573.5
680.5
751.9
833.6
867.1

TS
493.0
171.9
328.9
374.0
463.3
534.0
726.2
774.1

UCB
1445.6
69.1
1347.8
1396.6
1446.4
1497.7
1541.1
1554.6

MOSS
572.1
132.7
433.6
463.5
543.0
648.6
760.8
830.9

BUCB
638.9
127.8
485.6
542.7
623.3
724.3
813.5
867.4

KG
2131.2
1336.5
451.3
1001.1
2001.2
3000.2
3999.8
4498.3

KGMin
1301.0
209.8
1024.6
1174.2
1322.2
1442.1
1563.4
1590.7

KGMN
757.2
178.3
531.2
628.7
754.0
897.2
963.8
1039.2

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

Table 5: Statistics of the final cumulative regret in Figure 1(b). The best in each row is highlighted.
Algorithm
Mean Regret
Std. Dev.
Quantile .10
Quantile .25
Quantile .50
Quantile .75
Quantile .90
Quantile .95

RBMLE
730.6
827.4
135.3
160.2
263.1
1140.8
2107.9
2157.6

VIDS
775.0
678.7
233.9
336.0
544.1
1137.7
1516.5
1862.0

KLUCB&UCB
1412.2
219.2
1147.2
1272.1
1395.9
1545.9
1674.6
1724.6

GPUCB
2640.3
227.0
2382.8
2500.0
2600.4
2787.1
2916.1
3024.4

GPUCBT
848.5
314.2
529.3
608.0
814.7
1001.1
1228.6
1578.7

TS
932.7
282.1
657.8
706.6
876.0
1125.3
1304.8
1472.7

BUCB
1222.3
231.4
960.8
1036.5
1205.9
1390.6
1512.9
1565.5

KG
1684.3
2056.8
20.4
59.9
1035.8
2028.0
4028.8
7818.3

KG*
1046.0
238.9
788.0
891.6
1000.6
1171.1
1314.1
1413.7

Table 6: Statistics of the final cumulative regret in Figure 3(b). The best in each row is highlighted.
Algorithm
Mean Regret
Std. Dev.
Quantile .10
Quantile .25
Quantile .50
Quantile .75
Quantile .90
Quantile .95

RBMLE
531.1
469.5
145.5
167.4
207.7
1131.8
1188.1
1204.2

VIDS
638.5
1117.0
143.7
206.6
314.1
889.0
1183.3
1248.6

KLUCB&UCB
1102.7
196.9
859.7
937.4
1093.2
1232.0
1346.8
1439.0

GPUCB
2464.2
210.8
2200.1
2320.7
2466.4
2605.0
2726.0
2804.9

GPUCBT
607.7
234.1
361.4
444.3
544.8
714.6
926.2
1041.8

TS
684.3
250.1
411.1
501.7
623.1
792.2
1058.9
1209.2

BUCB
923.6
178.7
724.5
792.9
927.2
1042.0
1174.1
1193.5

KG
1995.0
3541.8
21.1
30.2
1014.4
1044.3
8121.5
9023.5

KG*
760.2
163.8
568.4
664.5
752.5
851.4
930.0
959.5

Table 7: Statistics of the final cumulative regret in Figure 4(b). The best in each row is highlighted.
Algorithm
Mean Regret
Std. Dev.
Quantile .10
Quantile .25
Quantile .50
Quantile .75
Quantile .90
Quantile .95

RBMLE
652.0
581.8
127.3
155.7
265.4
1116.2
1202.8
2021.8

VIDS
694.7
776.1
193.6
322.9
471.9
861.0
1236.1
1467.5

KLUCB&UCB
1302.0
164.5
1100.0
1173.4
1295.7
1428.3
1492.8
1549.4

GPUCB
2281.0
169.5
2062.5
2156.6
2262.7
2397.7
2506.3
2545.1

GPUCBT
856.5
255.8
561.1
665.7
814.3
1007.8
1164.6
1334.9

TS
903.4
268.2
574.8
715.8
849.3
1085.6
1283.0
1394.5

BUCB
1149.5
201.0
897.0
1000.4
1130.5
1294.0
1404.6
1511.5

KG
1233.6
1659.2
24.5
72.0
1021.1
1987.0
2028.1
2055.5

KG*
1001.7
234.8
747.2
827.9
944.4
1128.1
1346.7
1467.2

Table 8: Statistics of the final cumulative regret in Figure 1(c). The best in each row is highlighted.
Algorithm
Mean Regret
Std. Dev.
Quantile .10
Quantile .25
Quantile .50
Quantile .75
Quantile .90
Quantile .95

RBMLE
179.6
119.4
128.7
139.7
155.2
173.4
195.4
291.7

VIDS
243.3
463.1
37.6
47.9
70.5
103.7
1039.9
1074.1

KLUCB
322.7
63.9
239.4
271.3
331.7
367.2
407.0
423.2

TS
208.6
61.3
132.8
157.7
202.3
243.4
303.1
320.1

UCB
1504.6
66.1
1430.9
1452.0
1505.4
1550.6
1586.5
1617.6

MOSS BUCB
379.9
288.2
44.5
71.9
329.4
196.7
345.8
238.3
380.1
275.1
405.9
330.6
435.0
377.3
457.8
405.3

KG
961.6
1063.3
26.5
37.2
387.2
2450.7
2509.9
2522.7

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

Table 9: Statistics of the final cumulative regret in Figure 3(c). The best in each row is highlighted.
Algorithm
Mean Regret
Std. Dev.
Quantile .10
Quantile .25
Quantile .50
Quantile .75
Quantile .90
Quantile .95

RBMLE
294.6
301.3
139.8
148.7
176.9
237.4
919.0
1183.3

VIDS
322.4
352.5
93.3
116.4
166.1
273.8
1064.9
1112.1

KLUCB
710.6
118.0
565.4
609.8
695.1
784.9
875.6
916.6

TS
436.7
168.7
288.1
335.9
411.0
468.3
610.0
682.5

UCB
1805.6
126.6
1653.3
1713.9
1789.0
1898.1
1970.0
2035.3

MOSS
453.5
147.8
342.8
374.8
419.6
483.9
578.0
644.9

BUCB
600.8
126.3
464.1
792.9
592.2
662.3
739.0
789.5

KG
1000.0
1637.9
34.9
30.2
77.4
1050.0
4920.6
5042.0

Table 10: Statistics of the final cumulative regret in Figure 4(c). The best in each row is highlighted.

10

-2

RBMLE
IDS
VIDS
KLUCB
UCBT
TS
UCB
MOSS
BUCB
KG
KGMin
KGMN

-3

10-4

10

-5

0

500

1000

1500

VIDS
215.9
425.2
43.5
55.9
70.5
94.1
1037.1
1064.6

KLUCB
339.1
53.6
264.9
301.4
335.7
373.1
408.4
433.2

TS
221.3
60.3
159.6
176.9
211.3
248.6
296.9
319.3

UCB
1815.8
69.2
1729.3
1776.9
1818.0
1863.7
1897.8
1934.1

10-2

10

10

RBMLE
VIDS
KLUCB & UCB
GPUCB
GPUCBT
TS
BUCB
KG
KG*

-3

-4

10-5

500

1000

1500

2000

MOSS
462.0
53.3
402.8
428.6
456.7
480.2
532.3
563.1

Averaged Computation Time Per Decision (secs)

10

RBMLE
195.2
140.2
140.9
153.1
166.2
188.0
225.8
291.7

Averaged Computation Time Per Decision (secs)

Averaged Computation Time Per Decision (secs)

Algorithm
Mean Regret
Std. Dev.
Quantile .10
Quantile .25
Quantile .50
Quantile .75
Quantile .90
Quantile .95

10

BUCB
298.8
45.9
247.8
263.5
297.7
326.5
365.8
383.3

KG
1460.3
2035.8
26.7
33.7
58.3
3249.7
4955.2
4966.9

-2

RBMLE
VIDS
KLUCB
TS
UCB
MOSS
BUCB
KG

10-3

10-4

0

500

1000

1500

Averaged Final Cumulative Regret

Averaged Final Cumulative Regret

Averaged Final Cumulative Regret

(a)

(b)

(c)

2000

Figure 5: Averaged computation time per decision vs. averaged final cumulative regret: (a) Figure 3(a); (b) Figure 3(b);
(a) Figure 3(c).
Table 11: Average computation time per decision for Bernoulli bandits, under different numbers of arms. All numbers are
averaged over 100 trials with T = 104 and in 10−4 seconds. The best in each row is highlighted.
# Arms (Statistics)
10 (Mean)
30 (Mean)
50 (Mean)
70 (Mean)
10 (Std. Err.)
30 (Std. Err.)
50 (Std. Err.)
70 (Std. Err.)

RBMLE
1.36
3.61
4.58
7.56
0.236
1.30
2.04
2.70

IDS
175
1260
3630
6660
54.8
458
972
1330

VIDS
123
788
1930
3590
33.1
232
536
883

KLUCB
12.8
49.7
80.3
113
1.53
17.3
29.4
36.6

UCBT
1.53
4.96
7.85
10.3
0.586
1.52
2.59
3.63

TS
0.225
0.628
0.628
0.628
0.0380
0.106
0.106
0.106

UCB
0.712
2.19
3.42
4.49
0.268
0.646
1.11
1.53

MOSS
0.895
2.83
4.40
5.87
0.333
0.844
1.40
2.00

BUCB
0.855
2.58
4.11
5.43
0.351
0.714
1.25
1.76

KG
28.7
97.6
159
209
10.9
29.2
49.5
69.3

KGMin
0.649
1.89
2.95
3.97
0.284
0.557
0.931
1.34

KGMN
0.453
1.36
2.14
2.86
0.172
0.408
0.678
0.962

-2

RBMLE
IDS
VIDS
KLUCB
UCBT
TS
UCB
MOSS
BUCB
KG
KGMin
KGMN

10-3

10

10

-4

-5

0

500

1000

1500

2000

10-2
RBMLE
VIDS
KLUCB & UCB
GPUCB
GPUCBT
TS
BUCB
KG
KG*

10-3

10-4

10-5
500

1000

1500

2000

2500

Averaged Computation Time Per Decision (secs)

10

Averaged Computation Time Per Decision (secs)

Averaged Computation Time Per Decision (secs)

Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits

10-2

RBMLE
VIDS
KLUCB
TS
UCB
MOSS
BUCB
KG

10-3

10-4

0

500

1000

1500

Averaged Final Cumulative Regret

Averaged Final Cumulative Regret

Averaged Final Cumulative Regret

(a)

(b)

(c)

2000

Figure 6: Averaged computation time per decision vs. averaged final cumulative regret: (a) Figure 4(a); (b) Figure 4(b);
(a) Figure 4(c).

Table 12: Average computation time per decision for Gaussian bandits, under different numbers of arms. All numbers are
averaged over 100 trials with T = 104 and in 10−4 seconds. The best in each row is highlighted.
# Arms (Statistics)
10 (Mean)
30 (Mean)
50 (Mean)
70 (Mean)
10 (Std. Err.)
30 (Std. Err.)
50 (Std. Err.)
70 (Std. Err.)

RBMLE
0.617
1.07
1.49
1.95
0.284
0.484
0.686
0.871

VIDS
135
1410
3580
6610
53.9
409
866
1290

KLUCB&UCB
0.341
1.12
1.22
1.67
0.417
1.28
2.14
2.95

GPUCB
0.346
1.10
1.79
2.24
0.136
0.370
0.563
0.755

GPUCBT
0.318
1.08
1.76
2.22
0.160
0.370
0.563
0.773

TS
0.451
1.33
2.44
3.16
0.0425
0.321
0.562
0.774

BUCB
17.9
75.2
121
162
6.98
26.2
42.1
58.5

KG
25.1
103
168
226
9.37
35
56.1
77.6

KG*
10.9
21.2
33.9
45.9
2.77
5.61
9.77
15.7

Table 13: Average computation time per decision for Exponential bandits, under different numbers of arms. All numbers
are averaged over 100 trials with T = 104 and in 10−4 seconds. The best in each row is highlighted.
# Arms (Statistics)
10 (Mean)
30 (Mean)
50 (Mean)
70 (Mean)
10 (Std. Err.)
30 (Std. Err.)
50 (Std. Err.)
70 (Std. Err.)

RBMLE
1.01
1.93
2.97
3.79
0.435
0.890
1.24
1.56

VIDS
133
1160
3170
6430
13.6
187
447
788

KLUCB
7.26
22.8
36.5
53.7
0.884
2.79
5.47
6.92

TS
1.38
3.97
6.64
9.30
0.316
0.777
1.20
1.96

UCB
0.420
1.20
1.92
2.67
0.0980
0.263
0.397
0.531

MOSS
0.548
1.61
2.53
3.59
0.112
0.340
0.498
0.688

BUCB
14.9
42.6
75.5
102
1.55
5.02
10.2
12.3

KG
0.519
1.36
2.23
3.06
0.101
0.265
0.456
0.605

