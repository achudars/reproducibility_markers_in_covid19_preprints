Model and Reinforcement Learning for Markov Games with Risk
Preferences
Wenjie Huang,1,2 Pham Viet Hai,3 William B. Haskell

arXiv:1901.04882v2 [cs.GT] 21 Nov 2019

1
2
3
4

4

Shenzhen Research Institute of Big Data (SRIBD)

Institute for Data and Decision Analysis, The Chinese University of Hong Kong, Shenzhen

Department of Computer Science, School of Computing, National University of Singapore (NUS)

Supply Chain and Operations Management Area, Krannert School of Management, Purdue University
wenjiehuang@cuhk.edu.cn, dcspvh@nus.edu.sg, whaskell@purdue.edu

November 22, 2019
Abstract
We motivate and propose a new model for non-cooperative Markov game which considers the interactions of risk-aware players. This model characterizes the time-consistent dynamic risk from both
stochastic state transitions (inherent to the game) and randomized mixed strategies (due to all other
players). An appropriate risk-aware equilibrium concept is proposed and the existence of such equilibria
is demonstrated in stationary strategies by an application of Kakutanis fixed point theorem. We further
propose a simulation-based Q-learning type algorithm for risk-aware equilibrium computation. This algorithm works with a special form of minimax risk measures which can naturally be written as saddle-point
stochastic optimization problems, and covers many widely investigated risk measures. Finally, the almost
sure convergence of this simulation-based algorithm to an equilibrium is demonstrated under some mild
conditions. Our numerical experiments on a two player queuing game validate the properties of our model
and algorithm, and demonstrate their worth and applicability in real life competitive decision-making.
Keywords: Markov games; time-consistent risk preferences; fixed point theorem; Q-learning

1

Introduction

Markov games (a.k.a stochastic games) generalize Markov decision processes (MDPs) to the multi-player
setting. In the classical case, each player seeks to minimize his expected costs. In a corresponding equilibrium,
no player can decrease his expected costs by changing his strategy. We often want to compute equilibria to
predict the outcome of the game and understand the behavior of the players.
In this paper, we directly account for the risk preferences of the players in a Markov game. Informally,
risk aversion is at least weakly preferring a gamble with smaller variance when payoffs are the same. Riskaverse players give more attention to low probability but high cost events compared to risk-neutral players.
Models for the risk preferences of a single agent are well established [2, 45] for the static problems and [44, 48]
for the dynamic case. We extend these ideas to general sum Markov games and extend the framework of
Markov risk measures [44, 48] to the multi-agent setting. Our model specifically addresses the risk from
the stochastic state transitions as well as the risk from the randomized strategies of the other players.
The traditional multilinear formulation approach [1, 30] for computing equilibria in robust games fails in
our settings, because our model has an intrinsic bilinear term due to the product of probabilities (the state
transitions and mixed strategies) which leads to computational intractability. Thus, it is necessary to develop
an alternative algorithm to compute equilibria.
Risk Preferences Expected utility theory [14, 51, 52] is a highly developed framework for modeling risk
preferences. Yet, some experiments [35] show that real human behavior may violate the independence axiom
1

of expected utility theory. Risk measures (as developed in [2, 45]) do not require the independence axiom
and have favorable properties for optimization.
In the dynamic setting, [44, 48] develop the class of Markov (a.k.a. dynamic/nested/iterated) risk measures and establish their connection to time-consistency. This class of risk measures is notable for its recursive
formulation, which leads to dynamic programming equations. Practical computational schemes for solving
large-scale risk-aware MDPs have been proposed, for instance, Q-learning type algorithms [25, 26, 27] and
simulation-based fitted value iteration [55].
Risk-sensitive/Robust Games Risk-sensitive games have already been considered in [3, 5, 19, 28, 32].
Risk-sensitivity refers to the specific certainty equivalent (1/θ) ln (E [exp (θ X)]) where θ > 0 is the risk
sensitivity parameter. [3, 19] focus on zero-sum risk-sensitive games under continuous time setting.
Robust games study ambiguity about costs and/or state transition probabilities of the game. [1] develop
the robust equilibrium concept where each player optimizes against the worst-case expected cost over the
range of model ambiguity. This paradigm is extended to Markov games in [30], and the existence of robust
Markov perfect equilibria is demonstrated. [1, 30] formulate robust Markov perfect equilibria as multilinear
systems.
Games with risk preferences are not artificial; rather, they emerge organically from many real problems. Traffic equilibrium problems with risk-averse agents are analyzed in [6] with non-cooperative game
theory. The preferences of risk-aware adversaries are modeled in Stackelberg security games in [43], and a
computational scheme for robust defender strategies is presented.
Contributions of This Work

We make three main contributions in this paper:

1. We develop a model for risk-aware Markov games where agents have time-consistent risk preferences.
This model specifically addresses both sources of risk in a Markov game: (i) the risk from the stochastic
state transitions and (ii) the risk from the randomized strategies of the other players.
2. We propose a notion of ‘risk-aware’ Markov perfect equilibria for this game. We show that there exist
risk-aware equilibria in stationary strategies.
3. We create a practical simulation-based Q-learning type algorithm for computing risk-aware Markov
perfect equilibria, and we show that it converges to an equilibrium almost surely. This algorithm is
model-free and so does not require any knowledge of the true model, and thus can search for equilibria
purely by observations.

2

Risk-aware Markov Games

In this section, we develop risk-aware Markov games. Our game consists of the following ingredients: finite set
of players I; finite set of states S; finite set of actions Ai for each player i ∈ I; strategy profiles A := ×i∈I Ai ;
state-action pairs K := S × A; transition kernel P (·|s, a) ∈ P(S) (here P(S) denotes the distribution over
S) for all (s, a) ∈ K, and cost functions ci : S × A → R for all players i ∈ I.
Each round t ≥ 0 of the game has four steps: (i) first, all players observe the current state st ∈ S;
(ii) second, each player i ∈ I chooses ait ∈ Ai (all moves are simultaneous and independent, and the
corresponding strategy profile is at = ait i∈I ); (iii) third, each player i ∈ I realizes cost ci (st , at ); and (iv)
lastly, the state transitions to st+1 according to P (· | st , at ).
We next characterize the players’ strategies. In this work, we focus on ‘stationary strategies’. Stationary
strategies prescribe a player the same probabilities over his actions each time the player visits a certain state,
no matter what route he follows to reach that state. Stationary strategies are more prevalent than normal
strategies (which rely on the entire history), due to their mathematical tractability [53, 17, 30]. Furthermore,
the memoryless property of stationary strategies conforms to real human behavior [53].
We introduce some additional notations to characterize stationary strategies
x. Let P(Ai ) denote the

distribution over Ai . For each player i ∈ I and state s ∈ S, xis ∈ P Ai is the mixed strategy over actions
i
i
i
where xis ai denotes the probability of choosing ai at
 state s. We definei the strategy x := (xs )s∈S ∈ X :=
i
i
×s∈S P A of player i, the multi-strategy x := x i∈I ∈ X := ×i∈I X of all players, the complementary
2



strategy x−i := (xj )j6=i ∈ X −i := ×j6=i X j , and the multi-strategy xs = xis i∈I ∈ Xs := ×i∈I P Ai for all
players in state s ∈ S. We sometimes write a multi-strategy as x = (ui , x−i ) to emphasize player i’s strategy
ui .
There are two sources of stochasticity in the cost sequence: the stochastic state transitions characterized
by the transition kernel P (·|s, a), and the randomized mixed strategies of players characterized by x−i . In
this work, we consider the risk from both sources of stochasticity. We begin by constructing the framework
for evaluating the risk of sequences of random variables. A dynamic risk measure is a sequence of conditional
risk measures each mapping a future stream of random costs into a risk assessment at the current stage,
following the definition of risk maps from [48], and satisfying the stationary and time-consistency property
of [44, Definition 3] and [47, Definition 1]. We assume each conditional risk measure satisfies three axioms:
normalization, convexity, and positive homogeneity , which were originally introduced for static risk measures
in the pioneering paper [2]. Here “convexity” characterizes the risk-averse behavior of players. From [47,
Definition 1], a risk-aware optimal policy is time-consistent if, the risk of the sub-sequence of random outcome
from any future stage is optimized by the resolved policy. In the Appendix, we give explicit definitions of
the above three axioms of risk measures, stationary and time-consistency risk preferences, and derivation of
recursive evaluation of dynamic risk.
From [44, Theorem 4] and [47, Proposition 4], time-consistency allows for a recursive (iterative) evaluation
of risk. The infinite-horizon discounted risk for player i under multi-strategy x will be:
Jsi0 (xi , x−i ) :=ρi (ci (s0 , a0 ) + γ ρi (ci (s1 , a1 )

+ γ ρi ci (s2 , a2 ) + · · · )),

(1)

where ρi is a one-step conditional risk measure that maps random cost from the next stage to current stage,
with respect to the joint distribution of randomized mixed strategies and transition kernel. In Eq. (16), each
ci (st , at ), t ≥ 1 is governed by the joint distribution of randomized mixed strategies and transition kernel
×i∈I xist (ait )P (st |st−1 , at−1 ),
which is defined for fixed (st−1 , at−1 ) and for all st and ait . The initial cost ci (s0 , a0 ) is only governed by
the random mixed strategies distribution ×i∈I xis0 (ai0 ).
The corresponding best response function for player i is:
min Jsi0 (xi , x−i ).

xi ∈X i

(2)

P∞ t i

x
Suppose we replace all ρi with expectation E in Eq. (16) which leads to Exs
t=0 γ c (st , at ) , where Es
denotes expectation with respect to multi-strategies x, then Problem (2) will become risk-neutral. Thus our
formulation recovers the risk-neutral game as a special case.
Denote the ingredients of game Jsi (xi , x−i ) s∈S, i∈I as {I, S, A, P, c, ρ}. In line with the classical
definition of Markov perfect equilibrium in [17], we now define risk-aware Markov perfect equilibrium.
Definition 1. (Risk-aware Markov perfect equilibrium) A multi-strategy x ∈ X is a risk-aware Markov
perfect equilibrium for {I, S, A, P, c, ρ} if
Jsi (xi , x−i ) ≤ Jsi (ui , x−i ), ∀s ∈ S, ui ∈ X i , i ∈ I.

(3)

In Definition 1, each player i ∈ I implements a (risk-aware) stationary best response given the stationary
complementary strategy x−i . It also states that x is an equilibrium if and only if no player can reduce his
discounted risk by unilaterally changing his strategy.
Existence of Stationary Equilibria We prove the existence of stationary equilibira in this section. Let
v i denote player i’s value function, which is an estimate of the discounted risk starting from the next state S 0 .
i
i
For each player i,
 the value of the stationary strategy x ∈ X in state s ∈ S is defined to be v (s) := Js (x),
i
i
and v := v (s) s∈S is the entire value function for player i. The space of value functions for all players is
V := ×i∈I R|S| , equipped with the supremum norm kvk∞ := maxs∈S, i∈I |v i (s) |. Eq. (16) states that each
player must evaluate the stage-wise risk of random variables on A × S, formulated as
ci (s, A) + γ v i (S 0 ) ,
3

(4)

where A is the random strategy profile chosen from A according to xs , and S 0 is the random next state
visited (which first depends on x through the random choice of strategy profile a, and then depends on the
transition kernel P (· | s, a) after a ∈ A is realized).
Recall that in state s ∈ S, 
the probability that a = (ai )i∈I ∈ A is chosen and then the system transitions
i
i
to state k ∈ S is ×i∈I xs a P (k | s, a). The probability distribution of the strategy profile a ∈ A and
next state visited k ∈ S is given by the matrix

Ps uis , x−i
s




(5)
:= uis ai ×j6=i xjs aj P (k | s, a) (a, k)∈A×S ,

where we explicitly denote the dependence
on the multi-strategy xs = uis , x−i
in state s. For simplicity,
s

when
it
is
not
necessary
to
indicate
the
dependence
on (u, x).
we often write Ps instead of Ps uis , x−i
s
Let Csi v i := ci (s, A) + γ v i (S 0 ) be the random cost-to-go for player i at state s. Based on the Fencheli
Moreau representation of risk [18, 45, 20], the convex risk of random cost-to-go denoted by ψsi (uis , x−i
s , v )
can be computed as the worst-case expected cost-to-go

i
i
i
i
0
ψsi (uis , x−i
s , v ) :=ρ c (s , A) + γ v (S )


= sup
hµ, Csi v i i − bis (µ) ,
µ∈Mis (Ps )


where Mis (Ps ) s∈S, i∈I ⊂ P(A × S) is the risk envelope of ρi that depends on the distribution Ps , and
 i
bs s∈S, i∈I : P (A × S) → R are convex functions satisfying inf µ∈P(A×S) bis (µ) = 0 for all i ∈ I and s ∈ S.

i
To connect to risk-neutral games, we can just choose all Mis (Ps ) to be singletons {Ps uis , x−i
s } and bs (µ) = 0
i
for all µ ∈ Ms (Ps ), i ∈ I, and s ∈ S.


We next introduce further assumptions on ρi , Mis (Ps ) s∈S, i∈I , and bis s∈S, i∈I , that will lead to the
existence of stationary equilibria.
Assumption 1. (i) All ρi are law invariant, ρi (X) = ρi (Y ) for all X =D Y , where =D denotes equality in
distribution.

(ii) Mis (Ps ) s∈S, i∈I ⊂ P (A × S) is a collection of set-valued mappings where Mis (Ps ) are closed and
polyhedral convex for all Ps . Explicitly, there exists M ≥ 1 linear constraints and [M ] := {1, 2, ..., M }. Then
Mis (Ps ) is defined as:


Ais, m µ + fm (Ps ) ≥ his, m , m ∈ [M ], 

(6)
µ ∈ R|A||S| :
eT µ = 1,


µ ≥ 0,
where Ais, m are matrices, fm are linear functions in Ps and his, m are constants.

(iii) All bis s∈S, i∈I are convex and Lipschitz continuous.
Formulation (6) explains how Mis (Ps ) depends on Ps . In addition, if fm depends linearly on Ps , then
fm also depends linearly on uis and x−i
by definition of Ps in Eq. (5). In computational terms, this
s
assumption is close to [30] which assumes polyhedral uncertainty sets for the transition probabilities in its
robust Markov game model. This assumption also corresponds to the one in [16] about representation of
agent risk preferences.
Example 1. Conditional value-at-risk (CVaR) is a widely investigated coherent risk measure that computes
the conditional expectation of random losses exceeding a threshold with probability α.
CVaR can be constructed from system (6) when we choose M = 1, Ais,m = −e, fm (Ps ) = Ps /(1 − αi ),
and his,m = 0 with m = 1.
The best response function v∗ corresponding to a risk-aware Markov perfect equilibrium, for all s ∈ S, i ∈

4

I, satisfies
v∗i (s) =
=

min

Jsi (ui , x−i )

min

i
ψsi (uis , x−i
s , v∗ ),

uis ∈P(Ai )
uis ∈P(Ai )

Jsi (ui , x−i ),
xis ∈ arg min
i
i
u ∈X

(7)
(8)


and v∗i may not be unique. In the mapping Csi v i on A×S, the players control the distribution on P (A × S)
through their mixed strategies. Eqs. (7) - (8) together simply restate Eq. (3). However, Eqs. (7) - (8) give
a computational recipe that can be encoded into an operator on multi-strategies. We define this operator Φ
on X :
n
i
Φ(x) := q̃ ∈ X : q̃si ∈ arg i min i ψsi (uis , x−i
s , v∗ ),
us ∈P(A )
o
i
v∗i (s) = i min i ψsi (uis , x−i
(9)
s , v∗ ), ∀s ∈ S, i ∈ I .
us ∈P(A )

This operator returns the set of strategies for every player that are best responses to all other players’
strategies.
The following Theorem 1 briefly describes the existence of stationary strategies with detailed proof in
the Appendix.
Theorem 1. Suppose Assumption 1 holds, then the game {I, S, A, P, c, ρ} has an equilibrium in stationary
strategies.
Proof. (Proof sketch) Our proof of existence of risk-aware Markov perfect equilibrium draws from [17, 30].
The main idea is to show that Φ is a nonempty, closed, and convex subset of X , and that Φ is upper
semicontinuous. Then, we apply Kakutani’s fixed point theorem to show that this correspondence Φ has a
fixed point which coincides with a risk-aware Markov perfect equilibrium.

3

A Q-Learning Algorithm

We propose a simulation-based and asynchronous algorithm for computing equilibria of the risk-aware game
{I, S, A, P, c, ρ}, called
 Risk-aware Nash Q-learning (RaNashQL). This algorithm does not require a model
for the cost functions ci i∈I or the transition kernel P , nor does not it require prior knowledge on S. The
algorithm has an outer-inner loop structure, where the risk estimation is performed in the inner loop and
the equilibrium estimation is performed in the outer loop.
In each iteration of RaQL, a collection of Q-values for each player for all strategy profiles, is generated.
The one-shot game formed by the collection of Q-values is called a stage game. We will later formulate stage
game explicitly. The outer-inner loop structure follows [25, 27, 26] where multiple “stochastic approximation
instances” for both risk estimation and Q-value updates are “pasted” together. We show that the Nash
equilibria mapping for stage games is non-expansive, and both the risk estimation error and equilibrium
estimation error are bounded by the gap between the estimated Q-value and the Q-value under the equilibrium. These two conditions allow us to prove the convergence of the algorithm using the theory of stochastic
approximation, as shown in [15].

For this section, we assume that our risk measures ρi have a special form as stochastic saddle-point
problems to facilitate computation. Define a probability space (Ω, F, P ) and the space of essentially bounded
random variables L = L∞ (Ω, F, P ).
Assumption 2. (Stochastic saddle-point problem) For all i ∈ I,


ρi (X) = mini maxi EP Gi (X, y, z) , ∀X ∈ L,

(10)

y∈Y z∈Z

where: (i) Y i ⊂ Rd1 and Z i ⊂ Rd2 are compact and convex with diameters DY and DZ , respectively. (ii) Gi
is Lipschitz continuous on L × Y i × Z i with constant KG > 1. (iii) G is convex in y ∈ Y i and concave in
z ∈ Z i . (iv) The subgradients of G on y and z are Borel measurable and uniformly bounded for all X ∈ L.
5

In [26, Theorem 3.2], conditions on Gi are given to ensure that the corresponding minimax structure
(10) is a convex risk measure. Some examples of the functions Gi are shown in the Appendix such that the
corresponding risk-aware Markov perfect equilibria exist. For instance, CVaR can be written as:


1
E [max {X − η, 0}] ,
(11)
CVaRαi (X) := min η +
η∈R
1 − αi
where αi ∈ [0, 1) is the risk tolerance for player i.
Risk-aware Nash Q-learning Algorithm RaNashQL is updated based on future equilibrium costs
(which depend on all players). In contrast, single-agent Q-learning updates are only based on the player’s
own costs. Thus, to predict equilibrium losses, every player must maintain and update a model for all other
player’s costs and their risk assessments, which follows the settings in [23].
For all (s, a) ∈ S × A, i ∈ I,

Qi∗ (s, a) := mini maxi EP (· | s, a)
y∈Y z∈Z

Gi ci (s , A) + γ v∗i (S), y, z ,
(12)
denotes the Q-values corresponding to a stationary equilibrium and its best response function v∗ . In the
case of multiple equilibria, different Nash strategy profiles may have different equilibrium Q-values, so the
pair (v∗i , Qi∗ ) may not be unique.
In a multi-agent Q-learning algorithm, the agents play a sequence of stage games where the payoffs are
the current Q-values. In each state s ∈ S, the corresponding stage game is the collection (Qi (s))i∈I , where
Qi (s) := {Qi (s, a) : a ∈ A} is the array of Q-values for player i for all strategy profiles. Let xs be a Nash
equilibrium of the stage game (Qi (s))i∈I , then the corresponding Nash Q-value for all i ∈ I is denoted:
X

N ashi (Qj (s))j∈I :=
×j∈I xjs aj Qi (s, a) ,
a∈A

which gives each player’s corresponding expected cost in state s ∈ S (with respect to the Q-values) under
xs .
RaNashQL builds upon the algorithm in [23] for the risk-aware case. Figure 1 illustrates how players
interact with others and update their equilibrium estimation through RaQL. Each player chooses an action

Figure 1: Illustration of RaQL

based on a Nash equilibrium of their current Q-values, observed cost, other players’ actions, and then the
new state in each iteration. The Q-values follow a stochastic approximation-type update as in standard
Q-learning.
The steps of RaNashQL are summarized in Algorithm 1, which contains N and T number of iterations
for outer and inner loops, respectively. In Step 4, we use the stochastic approximation for saddle-point
6

Algorithm 1 Risk-aware Nash Q-learning
(Step 0) Initialize: Let n = 1, and t = 1, get the initial state s1 . Let the learning agent be indexed by i.
For all s ∈ S and ai ∈ Ai , i ∈ I, let Qin,t (s, a) = 0.
For n = 1, ..., N do
(Step 1) Choose ain based on the exploration policy π. Observe the actions and costs for all players, then
observe a new state;
For t = 1, ..., T do
(Step 2) Compute the Nash Q-value; Compute the risk-aware cost-to-go for all players;
(Step 3) Update each Qin,t , i ∈ I using stochastic approximation;
(Step 4) Stochastic approximation of risk measure by SASP;
end for
end for
Return Approximated Q-value QiN,T , i ∈ I.
Game 1
Up
Down

Left
0, 1
7, 10

Right
10, 7
11, 8

Game 2
Up
Down

Left
5, 5
4, 10

Right
10, 4
8, 8

Game 3
Up
Down

Left
0, 1
7, 10

Right
10, 9
8, 8

Table 1: Examples of I 0 -mixed point

problems (SASP) algorithm, [40, Algorithm 2.1]. Classical stochastic approximation may result in extremely
slow convergence for degenerate objectives (i.e. when the objective has a singular Hessian). However, the
SASP algorithm with a properly chosen parameter preserves a “reasonable” (close to O(n−1/2 )) convergence
rate, even when the objective is non-smooth and/or degenerate. Thus, SASP is a robust choice for solving
problem (10). The extended formulations from Steps (0)-(4) in Algorithm 1 are given in the Appendix.
Almost Sure Convergence Let {Qn,T }i∈I be the Q-value estimations at iteration n and T (the end of
each inner loop after the risk estimation has been done) from Algorithm 1. We would like to demonstrate
the almost sure convergence of Qin,T to the risk-aware equilibrium Q-values Qi∗ for all players. [23] introduce
two conditions on the Nash equilibria of all the stage games that lead to almost sure convergence, a global
optimal point when every player receives his lowest cost at this point, and a saddle point when each agent
would receive a lower cost when at least one of the other players deviates. We found a special type of Nash
equilibria that we call an I 0 -mixed point, which builds on [23], and plays a major role in our convergence
analysis.
Definition 2. Let (C i )i∈I denote the expected cost
 of all players as a function of the multi-strategy x ∈ X .
A multi-strategy x ∈ X is a I 0 -mixed point of C i i∈I if: (i) it is a Nash equilibrium and (ii) there exists an


index of players I 0 ⊆ I such that: C i (x) ≤ C i (x0 ) , ∀x0 ∈ X , i ∈ I 0 , and C i xi , x−i ≤ C i xi , u−i , ∀u−i ∈
X −i , i ∈ I\I 0 .
Our definition of ‘I 0 -mixed point’ combines both notions of global optimal point and saddle point. From
Definition 2, a subset of players I 0 ⊆ I minimizes their expected costs at x. The rest of the players I\I 0
each would receive a lower expected cost when at least one of the other players deviates. An example of an
I 0 -mixed point in a one shot game follows.
Example 2. Player 1 has choices Up and Down, and Player 2 has choices Left and Right. Player 1’s loss
is the first entry in each cell, and Player 2’s are the second. The first game has a unique Nash equilibrium
(Up, Left), which is a global optimal point. The second game also has a unique Nash equilibrium (Down,
Right), which is a saddle-point. The third game has two Nash equilibrium: a global optimum (Up, Left),
and a mixed point (Down, Right). In equilibrium (Down, Right), Player 1 receives a lower cost if Player 2
deviates, while Player 2 receives a higher cost if Player 1 deviates.
We now introduce the following additional assumptions for our analysis of RaNashQL.

7

Assumption 3. One of the following holds for all stage games (Qin,T (s))i∈I for all n and s ∈ S in Algorithm
1.
(i) Every (Qin,T (s))i∈I for all n and s ∈ S has a global optimal point.
(ii) Every (Qin,T (s))i∈I for all n and s ∈ S has a saddle point.
(iii) For any two stage games Q, Q̃ ∈ (Qin,T (s))i∈I for all n and s ∈ S, we suppose Q1 has a I1 -mixed
point x and Q2 has a I2 -mixed point x̃. Then: For i ∈ I1 ∪ (I\I2 ), then Qi (x) ≥ Q̃i (x̃); For i ∈ I2 ∪ (I\I1 ),
then Qi (x) ≤ Q̃i (x̃).
Compared with [23, Assumption 3], Assumption 3(iii) enables wider application of RaNashQL. In particular, even the indices I1 and I2 of all the stage games may differ across iterations. Next we list further
standard assumptions on exploration in RaNashQL and its asynchronous updates.
Assumption 4. (i) The exploration policy π is ε−greedy, meaning with probability ε ∈ (0, 1), action ai is
chosen uniformly from Ai , and with probability 1 − ε, action ai is drawn from Ai according to xis which is
the equilibrium of the stage game {Qi (s)}i∈I ; (ii) a single state-action pair is updated when it is observed in
each iteration.
By the Extended Borel-Cantelli Lemma [11], the algorithm satisfying Assumption 4(i) will visit every
state-action pair infinitely often with probability one.
Theorem
2. Suppose Assumptions 3 and 4 hold. For any T ≥ 1, Algorithm 1 generates sequences
 i
Qn,T n≥1 such that Qin, T → Qi∗ almost surely as n → ∞ for all i ∈ I.
Proof. (Proof sketch) (i) Show that all I 0 -mixed points of a stage game have equal value, and the property
also holds for global optimal points and saddle points. Consequently, from [23], the mapping from Q-values
to Nash equilibrium (of the stage games) is non-expansive.
(ii) Show that the Hausdorff distance between the subdifferentials of the estimated risk on Y i and Z i
(corresponding to Eq. (10)), is bounded by a function of kQin−1,T − Qi∗ k2 .
(iii) Show that the duality gaps of all the saddle point estimation problems are bounded by a function of
kQin−1,T − Qi∗ k2 .
(iv) If the conditions in (i)-(iii) hold, then Qin, T from RaNashQL are a well-behaved stochastic approximation sequence [15, Definition 7] that converges to Qi∗ with probability one.
The full proof Theorem 2 is presented in the Appendix.
[26, Theorem 4.7] shows that the single-agent version of RaNashQL has complexity


√
1/β
+ (ln( S A/))1/(1−β) ,
Ω S A ln(S A/δ)/2

(13)

with probability 1 − δ, where S and A denote the cardinality of state and actions spaces and β ∈ (0, 1] is the
learning rate. In the multi-agent case, our conjecture is to replace A with |A| in the term (13) to get a rough
estimate of the time complexity of RaNashQL. However, the explicit complexity bound is difficult to derive
and remains for future research. In RaNashQL, there are multiple Q-values being updated in each iteration
for each state, and their relationships are complex (they are linked by the solutions of a stage game, since
each stage game may yield multiple Nash equilibria).
In the Appendix, we also discuss (i) methods for computing Nash equilibria of stage games involving two
or more players; (ii) a rule for choosing a unique Nash equilibrium of stage games from multiple choices; (iii)
the storage space requirement of RaNashQL.

4

A Queuing Control Application

We apply our techniques to the single server exponential queuing system from [30]. In this packet switched
network, it is service provider’s (denoted as “SP” latter in the tables) benefit to increase the amount of
packets processed in the system. However, such an increase may result in an increase in packets’ waiting
times in the buffer (called latency), and routers (denoted as “R” latter in the tables) are used to reduce

8

Player
SP
R

Method
Neutral
CVaR
Neutral
CVaR

Mean
−22.22
−77.78
37.48
83.68

Variance
1.4736e − 06
407.84
7.32
491.20

5%-CVaR
−22.22
−69.34
37.94
86.03

Method

Mean

5%-CVaR

10%-CVaR

Neutral
CVaR

15.26
5.9

15.72
16.69

15.96
19.28

10%-CVaR
−22.22
−68.26
38.18
87.54

Table 2: Simulation (Constructing CVaR with α1 = α2 = 0.1)

packets’ waiting times. Thus, the game arises because the service provider and router choose their service
rates to achieve competing objectives.
The state space S represents the maximum number (30 in these experiments) of packets allowed in the
system. We assume that the time until the admission of a new packet and the next service completion
are both exponentially distributed. Therefore, the number of packets in the system can be modeled as a
birth and death process with fixed state transition probabilities. In the Appendix, we provide the explicit
formulation of cost functions, state transition probabilities, as well as other parameter settings. We suppose
that each player has the same two available actions (service rates) in every state. CVaR is the risk measure
for both players in all the experiments. The players risk preferences are obtained by setting αi for i = 1, 2,
and we allow α1 6= α2 .
Experiment I (RaNashQL vs. Nash Q-learning) We compare RaNashQL with Nash Q-learning in
[23] in terms of their convergence rates. Given any precision  > 0, we record the iteration count n until
the convergence criterion kQin, T − Qi∗ k2 ≤  is satisfied. Figure 2 (top) reveals that RaNashQL is more
computationally expensive than Nash Q-learning. Table 2 shows the discounted cost under equilibrium by
simulation (1000 samples). The first table reveals that incorporating risk will help the service provider reduce
its mean cost, while increase the mean cost of the router. The second table shows that incorporating risk
will help to reduce the overall cost to the entire system with only a slightly higher variance.
The first part of Table 3 shows that the mean cost of service provider (−44.31) is lower than that under
the risk-neutral Markov perfect equilibrium (−22.22), and the mean cost of router (59.64) is lower than
that under the risk-aware Markov perfect equilibrium (37.48). This result shows that incorporating risk
preference can help decision makers reach a new equilibrium that further reduces his mean cost compared to
cases where both players are either risk-neutral or risk-aware. Similar phenomena can also be shown in the
second part of Table 3. In the final part of Table 3, we construct a new two-player one-shot game where the
risk preferences (risk-neutral and risk-aware) are the actions and the expected value from simulation will be
outcome of the game. We find that a equilibrium is attained for this game when the router is risk-neutral and
the service provider is risk-aware. This one-shot game demonstrates that the router should be risk-neutral
when service provider is risk-aware, in order to reduce his expected cost.
In the Appendix, we further explain the reason for the increase in variance in risk-aware games in Table
2 which is counter-intuitive.
Experiment II (RaNashQL vs. Multilinear System) In this experiment, we consider a special case
where the risk only comes from state transitions (this setting is basically a risk-aware interpretation of [30]).
In this case, we can compute the risk-aware Markov equilibrium “exactly” using a multilinear system and
interior point algorithm as detailed in the Appendix. We evaluate performance in terms of the relative error
r

2
P
j
i
i
s∈S N ash (Qn, T (s))j∈I − v∗ (s)
pP
, n ≤ N,
i
2
s∈S v∗ (s)

9

Player

Method

Mean

Variance

5%-CVaR

10%-CVaR

SP
R

CVaR
Neutral

−44.31
59.64

266.06
316.71

−43.38
61.18

−42.70
62.77

Player

Method

Mean

Variance

5%-CVaR

10%-CVaR

SP
R

CVaR
Neutral

−54.76
70.56

26.05
31.03

−54.71
71.56

−54.67
71.81

Service Provider

Risk-neutral
Risk-aware

Router
Risk-neutral
Risk-aware
(−22.22, 37.48) (−54.76, 70.56)
(−44.44, 59.64) (−77.78, 83.68)

Table 3: Simulation ( Constructing CVaR with α1 = 0.95, α2 = 0.1 for the first table, and α1 = 0.1, α2 = 0.95 for
the second)

where v∗i is the value function corresponding to the equilibrium solved by multilinear system. The Appendix
confirms that the service provider’s strategy produced by RaNashQL converges almost surely to the one
produced by multilinear system. From the Appendix, interior point algorithm finds a local optimum with
10471.975 seconds, and RaNashQL has relative error lower than 25% with 5122.657 seconds. Thus, our
approach possesses superior computational performance compared to an interior point algorithm for solving
multilinear systems.
Experiment III (Computational Complexity Conjecture) In this experiment, we explore the conjecture on the computational complexity of RaNashQL. Given a fixed , we could compute the complexity
conjecture through formulation (13). Figure 2 (bottom) shows that the relative errors of service provider
and router under computed complexity conjecture are bounded by . Thus we derive a potential heuristic
for the computational complexity of solving a general sum game given the size of the game. In other words,
each practitioner can estimate the upper bound of total complexity in computing the − equilibrium through
this conjecture.

5

Conclusion

In this paper, we propose a model and simulation-based algorithm for non-cooperative Markov games with
time-consistent risk-aware players. This work has made the following contributions: (i) The model characterizes the risk from both the stochastic state transitions and the randomized strategies of the other players.
(ii) We define risk-aware Markov perfect equilibrium and prove its existence in stationary strategies. (iii)
We show that our algorithm converges to risk-aware Markov perfect equilibrium almost surely. (iv) From a
queuing control numerical example, we find that risk-aware Markov games will reach new equilibria other
than risk-neutral ones (this is the equilibrium shifting phenomenon). Moreover, the variance is increased for
risk-aware Markov games, which is contrary to the variance reduction property of risk-aware optimization
for single agents. The sum of expected cost over all players is reduced in risk-aware Markov game, compared
to risk-neutral ones. In future research, we seek to improve the scalability of our framework for large-scale
Markov games.

Acknowledgements
This work is supported by SRIBD International Postdoctoral Fellowship and the NUS Young Investigator
Award “Practical Considerations for Large-Scale Competitive Decision Making”.

10

Figure 2: Computational Complexity

References
[1] Michele Aghassi and Dimitris Bertsimas. Robust game theory. Mathematical Programming, 107(12):231–273, 2006.
[2] Philippe Artzner, Freddy Delbaen, Jean-Marc Eber, and David Heath. Coherent measures of risk. Math.
Finance, 9(3):203–228, 1999.
[3] Arnab Basu and Mrinal K Ghosh. Nonzero-sum risk-sensitive stochastic games on a countable state
space. Mathematics of Operations Research, 43(2):516–532, 2017.
[4] Robert G. Batson. Combinatorial behavior of extreme points of perturbed polyhedra. 127:130–139,
1987.
[5] Nicole Bäuerle and Ulrich Rieder. Zero-sum risk-sensitive stochastic games. Stochastic Processes and
their Applications, 127(2):622–642, 2017.
[6] Michael GH Bell and Chris Cassir. Risk-averse user equilibrium traffic assignment: an application of
game theory. Transportation Research Part B: Methodological, 36(8):671–681, 2002.
[7] Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming. Athena Scientific Belmont,
MA, 1996.
[8] Dimitris Bertsimas and David B. Brown. Constructing uncertainty sets for robust linear optimization.
Operations Research, 57(6):1483–1495, 2009.
[9] Vivek S Borkar et al. Stochastic approximation. Cambridge Books, 2008.
[10] Vivek S Borkar and Sean P Meyn. The ode method for convergence of stochastic approximation and
reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447–469, 2000.

11

[11] Leo Breiman. Probability, volume 7 of classics in applied mathematics. Society for Industrial and
Applied Mathematics (SIAM), Philadelphia, PA, 1992.
[12] Artur Czumaj, Michail Fasoulakis, and Marcin Jurdziński. Multi-player approximate nash equilibria. In
Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems, pages 1511–1513.
International Foundation for Autonomous Agents and Multiagent Systems, 2017.
[13] Ruchira S. Datta. Using computer algebra to find nash equilibria. In Proceedings of the 2003 International Symposium on Symbolic and Algebraic Computation, ISSAC ’03, pages 74–79, New York, NY,
USA, 2003. ACM.
[14] Dirk Engelmann and Jakub Steiner. The effects of risk preferences in mixed-strategy equilibria of 2× 2
games. Games and Economic Behavior, 60(2):381–388, 2007.
[15] Eyal Even-Dar and Yishay Mansour. Learning rates for q-learning. The Journal of Machine Learning
Research, 5:1–25, 2004.
[16] Michael Ferris and Andy Philpott. Dynamic risked equilibrium, 2018.
[17] Arlington M Fink. Equilibrium in a stochastic n-person game. Journal of science of the hiroshima
university, series ai (mathematics), 28(1):89–93, 1964.
[18] Hans Föllmer and Alexander Schied. Convex measures of risk and trading constraints. Finance and
stochastics, 6(4):429–447, 2002.
[19] Mrinal K Ghosh, K Suresh Kumar, and Chandan Pal. Zero-sum risk-sensitive stochastic games for
continuous time markov chains. Stochastic Analysis and Applications, 34(5):835–851, 2016.
[20] Vincent Guigues, Volker Krätschmer, and Alexander Shapiro. Statistical inference and hypotheses
testing of risk averse stochastic programs. arXiv preprint arXiv:1603.07384, 2016.
[21] Sébastien Hémon, Michel de Rougemont, and Miklos Santha. Approximate nash equilibria for multiplayer games. In International Symposium on Algorithmic Game Theory, pages 267–278. Springer,
2008.
[22] P. Jean-Jacques Herings and Ronald J. A. P. Peeters. Stationary equilibria in stochastic games: structure, selection, and computation. Journal of Economic Theory, 118:32–60, 2004.
[23] Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. Journal of
machine learning research, 4(Nov):1039–1069, 2003.
[24] Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. Journal of
machine learning research, 4(Nov):1039–1069, 2003.
[25] W. Huang and W. B. Haskell. Risk-aware q-learning for Markov decision processes. In Proc. IEEE 56th
Annual Conf. Decision and Control (CDC), pages 4928–4933, December 2017.
[26] Wenjie Huang and William B Haskell. Stochastic approximation for risk-aware markov decision processes. arXiv preprint arXiv:1805.04238, 2018.
[27] Daniel R Jiang and Warren B Powell. Risk-averse approximate dynamic programming with quantilebased risk measures. Mathematics of Operations Research, 43(2):554–579, 2017.
[28] Victor Richmond R Jose and Jun Zhuang. Incorporating risk preferences in stochastic noncooperative
games. IISE Transactions, 50(1):1–13, 2018.
[29] Shizuo Kakutani et al. A generalization of brouwer’s fixed point theorem. Duke mathematical journal,
8(3):457–459, 1941.
[30] Erim Kardes, Fernando Ordonez, and Randolph W. Hall. Discounted robust stochastic games and an
application to queueing control. Operations Research, 59(2):365–382, 2011.
12

[31] Erim Kardeş, Fernando Ordóñez, and Randolph W Hall. Discounted robust stochastic games and an
application to queueing control. Operations research, 59(2):365–382, 2011.
[32] M. B. Klompstra. Nash equilibria in risk-sensitive dynamic games. IEEE Transactions on Automatic
Control, 45(7):1397–1401, July 2000.
[33] Harold J. Kushner and G.George Yin. Stochastic Approximation and Recursive Algorithms and Applications. Springer, 2003.
[34] Carlton E Lemke and Joseph T Howson, Jr. Equilibrium points of bimatrix games. Journal of the
Society for Industrial and Applied Mathematics, 12(2):413–423, 1964.
[35] Jonathan Levin. Choice under uncertainty. Lecture Notes, 2006.
[36] Adam B Levy, Ren6 A Poliquin, and R Tyrrell Rockafellar. Stability of locally optimal solutions. SIAM
Journal on Optimization, 10(2):580–604, 2000.
[37] Michael L Littman, Nishkam Ravi, Arjun Talwar, and Martin Zinkevich. An efficient optimalequilibrium algorithm for two-player game trees. arXiv preprint arXiv:1206.6855, 2012.
[38] Richard D McKelvey and Andrew McLennan. Computation of equilibria in finite games. Handbook of
computational economics, 1:87–142, 1996.
[39] Karthik Natarajan, Dessislava Pachamanova, and Melvyn Sim. Constructing risk measures from uncertainty sets. Oper. Res., 57:1129–1141, September 2009.
[40] Arkadi Nemirovski and Reuven Rubinstein. An efficient stochastic approximation algorithm for stochastic saddle point problems. Modeling Uncertainty, pages 156–184, 2005.
[41] Jean-Paul Penot. On the convergence of subdifferentials of convex functions. Nonlinear Analysis:
Theory, Methods & Applications, 21(2):87–101, 1993.
[42] Krzysztof Postek, Dick Den Hertog, and Bertrand Melenberg. Computationally tractable counterparts
of distributionally robust constraints on risk measures. 2015.
[43] Yundi Qian, William B Haskell, and Milind Tambe. Robust strategy against unknown risk-averse attackers in security games. In Proceedings of the 2015 International Conference on Autonomous Agents and
Multiagent Systems, pages 1341–1349. International Foundation for Autonomous Agents and Multiagent
Systems, 2015.
[44] Andrzej Ruszczyński. Risk-averse dynamic programming for markov decision processes. Mathematical
programming, 125(2):235–261, 2010.
[45] Andrzej Ruszczynski and Alexander Shapiro. Optimization of convex risk functions. Mathematics of
operations research, 31(3):433–452, 2006.
[46] Andrzej Ruszczyński and Alexander Shapiro. Optimization of convex risk functions. Mathematics of
operations research, 31(3):433–452, 2006.
[47] Alexander Shapiro and Alois Pichler. Time and dynamic consistency of risk averse stochastic programs.
optimization-online.org, 2016.
[48] Yun Shen, Wilhelm Stannat, and Klaus Obermayer. Risk-sensitive markov control processes. SIAM
Journal on Control and Optimization, 51(5):3652–3672, 2013.
[49] Csaba Szepesvári and Michael L Littman. A unified analysis of value-function-based reinforcementlearning algorithms. Neural computation, 11(8):2017–2060, 1999.
[50] Yasushi Terazono and Ayumu Matani. Continuity of optimal solution functions and their conditions on
objective functions. SIAM Journal on Optimization, 25(4):2050–2060, 2015.

13

[51] PJ Thomas. Measuring risk-aversion: The challenge. Measurement, 79:285–301, 2016.
[52] John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton
University Press, 1944.
[53] OJ Vrieze. Stochastic games and stationary strategies. In Stochastic Games and Applications, pages
37–50. Springer, 2003.
[54] David W Walkup and Roger J-B Wets. A lipschitzian characterization of convex polyhedra. Proceedings
of the American Mathematical Society, pages 167–173, 1969.
[55] Pengqian Yu, William B Haskell, and Huan Xu. Approximate value iteration for risk-aware markov
decision processes. IEEE Transactions on Automatic Control, 2018.

14

A
A.1

Appendix
Dynamic Risk Measures

In this section, we describe the risk measures in our risk-aware Markov games. In our model, each player
i faces a sequence of costs Xt = ci (st , at ) for all t ≥ 0. There are two sources of stochasticity in this
cost sequence: (i) stochastic state transitions characterized by the transition kernel P (· | s, a); and (ii) the
randomized mixed strategies of other players characterized by x−i . The key question is: how should player
i account for both sources of stochasticity and evaluate the risk of the tail subsequence Xt , Xt+1 , . . . from
the perspective of time t?
We begin by formalizing some details about the risk of finite cost sequences Xt, T := (Xt , Xt+1 , . . . , XT )
before we consider the risk of the infinite cost sequence X0 , X1 , . . . actually faced by the players. For a
reference distribution P on (Ω, F), and we define Lt := L∞ (Ω, F t , P ) and Lt, T := Lt × Lt+1 × · · · × LT for
all 0 ≤ t ≤ T < ∞.
Definition 3. (i) A mapping ρt, T : Lt, T → Lt , is called a conditional risk measure if: ρt, T (Zt, T ) ≤
ρt, T (Xt, T ) for all Zt, T , Xt, T ∈ Lt, T such that Zt, T ≤ Xt, T .
(ii) A dynamic risk measure is a sequence of conditional risk measures {ρt, T }Tt=0 .
Given a dynamic risk measure {ρt, T }Tt=0 , we may define a larger family of risk measures ρt, τ for 0 ≤ t ≤
τ ≤ T via the convention ρt, τ (Xt , . . . , Xτ ) = ρt, τ (Xt , . . . , Xτ , 0, . . . , 0).
We now make our key assumptions about player risk preferences.
Assumption 5. The dynamic risk measure {ρt, T }Tt=0 satisfies the following conditions:
(i) (Normalization) ρt, T (0, 0, ..., 0) = 0.
(ii) (Conditional translation invariance) For any Xt, T ∈ Lt, T ,
ρt, T (Xt , Xt+1 , ..., XT ) = Xt + ρt, T (0, Xt+1 , ..., XT ).
(iii) (Convexity) For any Xt, T , Yt, T ∈ Lt, T and 0 ≤ λ ≤ 1, ρt, T (λ Xt, T + (1 − λ)Yt, T ) ≤ λ ρt, T (Xt, T ) +
(1 − λ)ρt, T (Yt, T ).
(iv) (Positive homogeneity) For any Xt, T ∈ Lt, T and α ≥ 0, ρt, T (α Xt, T ) = α ρt, T (Xt, T ).
(v) (Time-consistency) For any Xt, T , Yt, T ∈ Lt, T and 0 ≤ τ ≤ θ ≤ T , the conditions Xk = Yk for
k = τ, ..., θ − 1 and ρθ, T (Xθ , ...., XT ) ≤ ρθ, T (Yθ , ..., YT ) imply ρτ, T (Xτ , ...., XT ) ≤ ρτ, T (Yτ , ..., YT ).
Many of these properties (monotonicity, convexity, positive homogeneity, and translation invariance)
were originally introduced for static risk measures in the pioneering paper [2]. They have since been heavily
justified in other works including [46, 8, 39].
The next theorem gives a recursive formulation for dynamic risk measures satisfying Assumption 5. This
representation is the foundation of [44] and subsequent works on time-consistent risk measures. For this
result, we define a mapping ρt : Lt+1 → Lt , where t ≥ 0, to be a one-step (conditional) risk measure if
ρt (Xt+1 ) = ρt, t+1 (0, Xt+1 ).
Theorem 3. [44, Theorem 1] Suppose Assumption 5 holds, then
ρt, T (Xt , Xt+1 , ..., XT , . . .) = Xt + ρt (Xt+1 + ρt+1 (Xt+2 + · · · + ρT (XT ) + · · ·)),

(14)

for all 0 ≤ t ≤ T , where ρt , . . . , ρT are one-step conditional risk measures.
Now we may consider the risk of an infinite cost sequence. Based on [44], the discounted measure of risk
ργt, T : Lt, T → R is defined via
ργt, T (Xt , Xt+1 , . . . , XT ) := ρt, T (γ t Xt , γ t+1 Xt+1 , . . . , γ T XT ).
Define Lt, ∞ := Lt × Lt+1 × · · · for t ≥ 0 and ργ : L0, ∞ → R via
ργ (X0 , X1 , . . .) := lim ργ0, T (X0 , X1 , . . . , XT ).
T →∞

To provide our final representation result, we introduce the additional assumption that player risk preferences
are stationary (they only depend on the sequence of costs ahead, and are independent of the current time).
15

Assumption 6. (Stationary risk preferences) For all T ≥ 1 and s ≥ 0,
ργ0, T (X0 , X1 , . . . , XT ) = ργs, T +s (X0 , X1 , . . . , XT ).
When Assumptions 5 and 6 are satisfied, the corresponding dynamic risk measure is given by the recursion:
ργ (X0 , X1 , ..., XT , . . .) = X0 + ρ1 (γX1 + ρ2 (γ 2 X2 + · · · + ρT (γ T XT ) + · · ·)),

(15)

where ρ1 , ρ2 , . . . are all one-step risk measures. Based on representation (15), we may define the risk-aware
objective for player i to be:
Jsi0 (xi , x−i ) = ρi (ci (s0 , a0 ) + γ ρi (ci (s1 , a1 ) + γ ρi (ci (s2 , a2 ) + · · ·))).

(16)

where ρi is a one-step conditional risk measure that maps random cost from the next stage to current stage,
with respect to the joint distribution of randomized mixed strategies and transition kernels. In formulation
(16), each ci (st , at ), t ≥ 1 is governed by the joint distribution of randomized mixed strategies and transition
kernel
×i∈I xist (ait )P (st |st−1 , at−1 ),
which is defined for fixed (st−1 , at−1 ) and for all st and ait . The distribution of ci is only governed by
×i∈I xis0 (ai0 ).

A.2

Proof of Theorem 1

Fundamental Inequalities
We make heavy use of the following fundamental inequalities and algebraic identity.
Fact 1. Let X be a nonempty set and f1 , f2 : X → R be functions on X .
(i) | minx∈X f1 (x) − minx∈X f2 (x) | ≤ maxx∈X |f1 (x) − f2 (x) |.
(ii) | maxx∈X f1 (x) − maxx∈X f2 (x) | ≤ maxx∈X |f1 (x) − f2 (x) |.
Proof. For part (i), we compute
min f1 (x) = min (f1 (x) − f2 (x) + f2 (x))

x∈X

x∈X

≤ min (f2 (x) + |f1 (x) − f2 (x) |)
x∈X

≤ min f2 (x) + max |f1 (x) − f2 (x) |
x∈X

x∈X

which gives
min f1 (x) − min f2 (x) ≤ max |f1 (x) − f2 (x) |.

x∈X

x∈X

x∈X

By a symmetric argument, we have
min f2 (x) − min f1 (x) ≤ max |f1 (x) − f2 (x) |,

x∈X

x∈X

x∈X

from which the desired conclusion follows. The proof for part (ii) is similar.
Define dH (A, B) to be the Hausdorff distance between nonempty subsets A and B of Rd with respect to
the Euclidean norm k · k2 , explicitly,


dH (A, B) := max sup inf ka − bk2 , sup inf ka − bk2 .
a∈A b∈B

b∈B a∈A

Fact 2. Let (X , k · k) be a normed space, f : X → R be an Lf -Lipschitz function, and X1 , X2 ⊂ X . Then
| min f (x) − min f (x) | ≤ Lf dH (X1 , X2 ).
x∈X1

x∈X2

16

Proof. Let x∗1 ∈ X1 be an optimal solution of minx∈X1 f (x). There is an x2 ∈ X2 by definition of dH such
that kx∗1 − x2 k ≤ dH (X1 , X2 ), and so
min f (x) ≤ f (x2 ) ≤ f (x∗1 ) + Lf dH (X1 , X2 ) = min f (x1 ) + Lf dH (X1 , X2 ),

x∈X2

x∈X1

where the second inequality follows by Lf -Lipschitz continuity of f . The other direction follows by symmetric
reasoning.
Fact 3. Define
δ0 () :=

min{, 1}
,
(2|I| − 1)|A|

then Πi∈I xis (ai ) − Πi∈I ysi (ai ) ≤  holds for any x, y ∈ X .
Proof. We make use of the following algebraic identity
Πi∈I xis (ai ) − Πi∈I ysi (ai )



X
Y
Y



(xis (ai ) − ysi (ai ) 
ysi (ai )
=

{Ω⊂I, Ω6=∅}
i∈Ω
i∈
/Ω
X

Y

{Ω⊂I, Ω6=∅}

i∈Ω

≤

X

≤

(δ0 ())

(xis (ai ) − ysi (ai )
|Ω|

{Ω⊂I, Ω6=∅}

X

≤

δ0 ()

{Ω⊂I, Ω6=∅}

=.

Existence of Stationary Equilibria
This section develops the machinery for our proof of Theorem 1, which is based on Kakutani’s fixed point
theorem.
Theorem 4. [29] (Kakutani’s fixed point theorem) If X is a closed, bounded, and convex set in Euclidean
space, and Φ is an upper semicontinuous correspondence mapping from X into the family of closed, convex
subsets of X , then there exists an element x ∈ X such that x ∈ Φ(x).
For a multi-strategy x ∈ X , we define the operator Tx : V → V via

[Tx (v)]is := i min i {ρi ci (s , A) + γ v i (S 0 ) : (A, S 0 ) ∼ Ps (uis , x−i
s )}, ∀i ∈ I, s ∈ S.
us ∈P(A )

For simplification, we define, for any v ∈ V, i ∈ I and s ∈ S,

i
i
i
i
0
0
i
−i
ψsi (uis , x−i
s , v ) := ρ c (s , A) + γ v (S ) , (A, S ) ∼ Ps (us , xs ).
Our proof of Theorem 1 has three main steps:
1. (Step 1) Show that Tx is a contraction.
i
2. (Step 2) Show that the cost-to-go function ψsi (uis , x−i
s , v ) is continuous.

3. (Step 3) Verify that the assumptions of Kakutani’s fixed point theorem are met for Φ.
17

Step 1: Show that Tx is a contraction
We establish that the operator Tx is a contraction, and subsequently that given any stationary strategy x,
there is a corresponding unique value function v i for all players.
Proposition 1. For each x ∈ X , Tx is a contraction with constant γ ∈ (0, 1).
i

Proof. Let v, w ∈ V. For i ∈ I and s ∈ S, let gsi attain the minimum in the definition of [Tx (v)]s ,


i
[Tx (v)]is = i min i ψsi uis , x−i
=
sup
hµ, Csi (v i )i − αsi (µ) .
s , v
us ∈P(A )

µ∈Mis (Ps (gsi , x−i
s ))

i

Similarly, let zsi attain the minimum in [Tx (w)]s ,
[Tx (w)]is =

sup
µ∈Mis (Ps (zsi , x−i
s ))


hµ, Csi (wi )i − αsi (µ) .

It follows that
[Tx (v)]is − [Tx (w)]is

=
sup
hµ, Csi (v i )i − αsi (µ) −
µ∈Mis (Ps (gsi , x−i
s ))

≤

sup
µ∈Mis (Ps (zsi , x−i
s ))

≤γ


hµ, Csi (v i )i − αsi (µ) −
X

sup



sup
µ∈Mis (Ps (zsi , x−i
s ))

sup
µ∈Mis (Ps (zsi , x−i
s ))

hµ, Csi (wi )i − αsi (µ)


hµ, Csi (wi )i − αsi (µ)

µ (a, s0 ) |v i (s0 ) − wi (s0 ) |

µ∈Mis (Ps (zsi , x−i
s )) (a, s0 )∈K

≤ γkv − wk∞ ,
where the first inequality holds by choice of gsi . The argument to upper bound [Tx (w)]is −[Tx (v)]is is symmetric.
Since (V, k · k∞ ) is a complete metric space and Tx is a contraction mapping on V by Proposition 1, Tx
has a unique fixed point by the Banach fixed point theorem.
Proposition 2. For any stationary strategy x ∈ X , there exists a unique value function v ∈ V such that
∀s ∈ S, i ∈ I,
v i (s) =

min

uis ∈P(Ai )

ρi (ci (s, A) + γ v i (S 0 )) =

min

uis ∈P(Ai )

i
ψsi (uis , x−i
s , v ).

Step 2: Show that ψsi is continuous

i
We want to establish continuity of ψsi xis , x−i
in all its arguments for all i ∈ I and s ∈ S. Firstly, we
s , v
know that the function (µ, v i ) → hµ, Csi (v i )i − αsi (µ) is Lipschitz continuous based on Assumption 1. We
use L to denote the Lipschitz constant. Our argument is based on the following chain of inequalities:
i
i i
−i
i
|ψsi (xis , x−i
s , v ) − ψs (ys , ys , w )|
i
i
i
−i
i
i
i
−i
i
i i
−i
i
≤ |ψsi (xis , x−i
s , v ) − ψs (xs , xs , w )| + |ψs (xs , xs , w ) − ψs (ys , ys , w )|


≤|
sup
hµ, Csi (v i )i − αsi (µ) −
sup
hµ, Csi (wi )i − αsi (µ) |
µ∈Mis (Ps (xs ))

+|

sup
µ∈Mis (Ps (xs ))

µ∈Mis (Ps (xs ))


hµ, Csi (wi )i − αsi (µ) −

sup
µ∈Mis (Ps (ys ))


hµ, Csi (wi )i − αsi (µ) |


≤ γ kv i − wi k∞ + L dH Mis (Ps (xs )), Mis (Ps (ys )) ,

where we use Fact 2 to obtain the last inequality. Let Ex Mis (Ps (xs )) denote the set of extreme points of
(the bounded polyhedron) Mis (Ps (xs )), then we also have
18

i
i i
−i
i
|ψsi (xis , x−i
s , v ) − ψs (ys , ys , w )|


≤γ kv i − wi k∞ + L dH Mis (Ps (xs )), Mis (Ps (ys )) ,
≤γ kv i − wi k∞ + L dH (Ex(Mis (Ps (xs ))), Ex(Mis (Ps (ys )))),

(17)

where the last inequality is from [54, Theorem 1].
We define the following metrics for stationary strategies and value functions:
dXs (xs , ys ) :=

max |xis (a) − ysi (a) |, ∀s ∈ S,

i∈I, a∈A

dV i (v i , wi ) := max |v i (s) − wi (s) |,
s∈S


a metric on Xs × V i is then given by dXs ×V i (xs , v i ), (ys , wi ) := dXs (xs , ys ) + dV i (v i , wi ).
In the next lemma, we show that the extreme points of Mis (Ps (xs )) and Mis (Ps (ys )) are “close” when
the stationary strategies xs and ys are “close”.
Lemma 1. [4, Theorem 3.1] Choose  > 0 and B > 0. For xs , ys ∈ Xs , suppose
dXs (xs , ys ) ≤ δ1 () :=

B

min{1, }
p
,
|S| |A|(2|I| − 1)|A|

then


dH Ex Mis (Ps (xs )) , Ex Mis (Ps (ys )) ≤ .
Proof. By definition, we have

[Ps (xs )] (a, k) − [Ps (ys )] (a, k) = Πi∈I xis (ai ) − Πi∈I ysi (ai ) P (k | s, a).
Now, suppose
dXs (xs , ys ) ≤ δ0 () :=

min{, 1}
.
(2|I| − 1)|A|

Using the Fact 3, we have
kPs (xs ) − Ps (ys )k2
p
≤ |S| |A| Πi∈I xis (ai ) − Πi∈I ysi (ai )
p
≤ |S| |A| Πi∈I xis (ai ) − Πi∈I ysi (ai )
p
= |S| |A| .

max P (k | s, a)

a∈A, k∈S

By [4, Theorem 3.1], it follows that there exists a constant B > 0 such that
p


dH Ex Mis (Ps (xs )) , Ex Mis (Ps (ys )) ≤ BkPs − Ps0 k2 ≤ |S| |A| B ,
and the desired conclusion follows.
As a consequence, we have the following lemma.
Lemma 2. There exist constants B, C > 0 such that, for any  > 0, if
(
)

min{1, }

i
i
p
dXs ×V i (xs , v ), (ys , w ) ≤ δ2 () := max
,
,
2L B |S| |A|(2|I| − 1)|A| 2γ
then
i
i i
−i
i
|ψsi (xis , x−i
s , v ) − ψs (ys , ys , w )| ≤ .

19

Proof. By inequality (17), we have
i
i i
−i
i
|ψsi (xis , x−i
s , v ) − ψs (ys , ys , w )|

≤ γ kv i − wi k∞ + L dH Mis (Ps (xs )), Mis (Ps (ys ))



≤ γ kv i − wi k∞


+ L dH Ex Mis (Ps (xs )) , Ex Mis (Ps (ys ))
≤.

As a consequence of these lemmas, we have the following proposition.
i
Proposition 3. For all i ∈ I and s ∈ S, the function ψsi (uis , x−i
s , v ) is continuous in all of its variables.

Step 3: Apply Kakutani’s fixed point theorem
In the following two technical results, we establish upper semicontinuity of Φ.
Definition 4. A correspondence Φ : X → 2X is upper semicontinuous if y n ∈ Φ(xn ), limn→∞ xn = x, and
limn→∞ y n = y implies y ∈ Φ(x).
The proof of Lemma 3 below follows directly from [17] and our earlier Lemma 2. The proof of Lemma 4
follows directly from Lemma 3 and [17]. Lemma 3 is then used to prove Lemma 4, and Lemma 4 is used to
establish upper semicontinuity.
We define the operator Dsi as follows:
i
Dsi (x−i
s , v ) :=

min

uis ∈P(Ai )

i
ψsi (uis , x−i
s , v ),

it returns the optimal risk-to-go for any i ∈ I and s ∈ S as a function of the complementary strategy x−i
s
and the value function v i .
i
−i
Lemma 3. The operator Dsi (x−i
s , v ) is continuous in xs . Furthermore, the collection of functions

{Dsi (·, v i ) : kv i k∞ < ∞},
is equicontinuous.
Proof. Let
i
Dsi (x−i
s , v )=

Dsi (ys−i , v i ) =

min

i
i
i
−i
i
ψsi (uis , x−i
s , v ) = ψs (ws , xs , v ),

min

ψsi (uis , ys−i , v i ) = ψsi (zsi , ys−i , v i ).

uis ∈P(Ai )
uis ∈P(Ai )

Then
i
Dsi (ys−i , v i ) − Dsi (xs−i , v i ) ≤ ψsi (wsi , ys−i , v i ) − ψsi (zsi , x−i
s , v ),

and
i
i −i
i
i i
−i
i
i i
−i
i
Dsi (x−i
s , v ) − Ds (ys , v ) ≤ ψs (zs , xs , v ) − ψs (zs , ys , v ).

If v i is bounded, then the right-hand side of the above inequalities can be made arbitrarily small through
−i
control of x−i
s and ys via Proposition 3.

20

Let us define a mapping from stationary strategies to value functions via
τ i (x−i ) := {v i = (v i (s))s∈S : , v i (s) =

min

uis ∈P(Ai )

i
ψsi (uis , x−i
s , v ), s ∈ S}, ∀i ∈ I.

Each τ i (x−i ) returns the value function for player i corresponding to a best response to the complementary
strategy x−i . Denote the sth element of τ i (x−i ) by τsi (x−i ), let (x−i )n be a sequence of mixed strategies of all
players satisfying limn→∞ (x−i )n = x−i , and let the corresponding value functions for player i be τ i ( x−i n ).
The proof of the next Lemma 4 follows directly from Proposition 3 as shown in [17].
Lemma 4. If (x−i )n → x−i and τsi ((x−i )n ) → v i (s) as n → ∞, then τsi (x−i ) = v i (s).
The proof of our main result Theorem 1 is encapsulated in the following three lemmas.
Lemma 5. For all x ∈ X , the set Φ (x) is nonempty and is a subset of X .
i
Proof. By Proposition 3, ψsi (uis , x−i
s , v ) is continuous in all of its arguments. By the Weierstrass theorem,
the minimum of this function on the compact set P(Ai ) exists and is attained. Thus, the equality:

v i (s) =

min

uis ∈P(Ai )

i
ψsi (uis , x−i
s , v ),

can be established and therefore Φ(x) 6= ∅. By definition, Φ(x) ⊆ X for all x ∈ X .
The following intermediate result plays a key role in proving that Φ(x) is convex for all x ∈ X .
Lemma 6. Suppose that (g i )i∈I , (z i )i∈I ∈ Φ(x), then given x−i ∈ X −i we have






i
i
−i
i
i
−i
Mis Ps ((1 − λ) z i + λ gsi , x−i
s ) ⊆ Ms Ps (zs , xs ) ∪ Ms Ps (gs , xs ) ,
for any λ ∈ [0, 1].
Proof. From Assumption 1(ii), we know that Mis (Ps ) is a polyhedron characterized by formulation (6).
Suppose that fm , m = 1, ...., M are linear in us , x−i
s , and transition kernel Ps . It follows that for any
λ ∈ [0, 1] and µ ∈ Mis Ps ((1 − λ) z i + λ gsi , x−i
)
satisfies
for m = 1, ..., M ,
s
i
Ais, m µ + fm ((1 − λ) z i + λ gsi , x−i
s , Ps ) ≥ hs, m .

Since fm , for m = 1, ...., M are linear (and thus quasiconvex) functions, for any λ ∈ [0, 1] and µ ∈
Mis (Ps ((1 − λ) z i + λ gsi , x−i
s )) for m = 1, ..., M , the inequalities

i
−i
Ais, m µ + max fm (zsi , x−i
s , Ps ), fm (gs , xs , Ps )
i
≥ Ais, m µ + fm ((1 − λ) z i + λ gsi , x−i
s , Ps ) ≥ hs, m ,





i
i
−i
also hold. Thus,
for anyλ ∈ [0, 1] and µ ∈ Mis Ps ((1 − λ) z i + λ gsi , x−i
s ) , at least one of µ ∈ Ms Ps (zs , xs )

or µ ∈ Mis Ps (gsi , x−i
s ) must hold.
We now establish the convexity of Φ(x).
Lemma 7. The set Φ(x) is convex.
Proof. Suppose that (g i )i∈I , (z i )i∈I ∈ Φ(x). For all uis ∈ P(Ai ) we have
i
i i
−i
i
i
i
−i
i
v i (s) = ψsi (gsi , x−i
s , v ) = ψs (zs , xs , v ) ≤ ψs (us , xs , v ).

The above inequality holds based on the definition of operator Φ(x) via formulation (9), which returns the
best responses to all other players’ strategies. Hence, for any λ ∈ [0, 1] we have
i
i i
−i
i
i
i
−i
i
v i (s) = λ ψsi (zsi , x−i
s , v ) + (1 − λ)ψs (gs , xs , v ) ≤ ψs (us , xs , v ).

21

Then, we have
i
i i
−i
i
λ ψsi (gsi , x−i
s , v ) + (1 − λ)ψs (zs , xs , v )

=λ
max
hµ, Csi (v i )i − αsi (µ)
µ∈Mis (Ps (zsi , x−i
s ))

+ (1 − λ)



max

µ∈Mis (Ps (gsi , x−i
s ))

hµ, Csi (v i )i − αsi (µ) ,

based on the Fenchel-Moreau representation. Furthermore, we have

λ
max −i hµ, Csi (v i )i − αsi (µ)
µ∈Mis (Ps (zsi , xs ))

+ (1 − λ)
=

max

µ∈Mis (Ps (gsi , x−i
s ))


hµ, Csi (v i )i − αsi (µ)

max

−i
i
i
µ∈Mis (Ps (zsi , x−i
s ))∪Ms (Ps (gs , xs ))


hµ, Csi (v i )i − αsi (µ) ,

i
i i
−i
i
since ψsi (gsi , x−i
s , v ) = ψs (zs , xs , v ) holds in our setting. Thus,

λ
max −i hµ, Csi (v i )i − αsi (µ)
µ∈Mis (Ps (zsi , xs ))

+ (1 − λ)
≥

max

µ∈Mis (Ps (gsi , x−i
s ))


hµ, Csi (v i )i − αsi (µ)


max

µ∈Mis (Ps ((1−λ) z i +λ gsi , x−i
s ))

hµ, Csi (v i )i − αsi (µ) ,

by Lemma 6. Consequently,
i
i
ψsi (uis , x−i
s , v ) ≥ v (s)
i
i i
−i
i
= λ ψsi (zsi , x−i
s , v ) + (1 − λ)ψs (gs , xs , v )
i
i
≥ ψsi (λzsi + (1 − λ)gsi , x−i
s , v ) ≥ v (s),

and hence λ(z i )i∈I + (1 − λ)(g i )i∈I ∈ Φ(x).
The next lemma completes our proof.
Lemma 8. Φ is an upper semicontinuous correspondence on X .
Proof. Suppose xn → x, yn → y, and yn ∈ Φ(xn ). Taking a subsequence, we can suppose τsi ((x−i )n ) → v i (s).
Using the triangle inequality, for any s ∈ S and i ∈ I we have
i
i
ψsi (ysi , x−i
s , v ) − v (s)
i
i i
−i
i
−i
i i
−i
i
−i
i
≤ ψsi (ysi , x−i
s , v ) − ψs (ys , xs , τs ((x )n )) + ψs (ys , xs , τs ((x )n )) − v (s)
i
i i
−i
i
−i
i
−i
i
= ψsi (ysi , x−i
s , v ) − ψs (ys , xs , τs ((x )n )) + τs ((x )n ) − v (s) → 0,
i
as n → ∞. The above equality holds by definition of a best response and therefore, v i (s) = ψsi (ysi , x−i
s , v ).
i
−i
i
By Lemma 4, we also have τs ((x )n ) = v (s). Thus, we have established
i
v i (s) =ψsi (ysi , x−i
s , v )

=τsi ((x−i )n ) =

min

uis ∈P(Ai )

i
ψsi (uis , x−i
s , v ),

and so y ∈ Φ(x), completing the proof that Φ is an upper semicontinuous correspondence. The fact that
Φ(x) is a closed set for any x ∈ X follows from the definition of upper semicontinuity.

22

A.3

Examples of Saddle-Point Risk Measures

For our Q-learning algorithm, we specifically focus on risk measures that can be estimated by solving a
stochastic saddle-point problem such as as Problem (11). The following result, based on [26, Theorem 3.2],
gives special conditions on Gi for the corresponding risk function ρi in Problem (11).
Theorem 5. Suppose there is a collection of functions {hz }z∈Z i such that: (i) hz is P -square summable
for every y ∈ Y i , z ∈ Z i ; (ii) y → hz (X − y) is convex; (iii) z → hz (X − y) is concave; and (iv)
Gi : L × Y i × Z i → R is given by Gi (X, y, z) = y + hz (X − y), then the saddle-point risk measure
(Problem (11)) is a convex risk measure.
We now give some examples of functions {hz }z∈Z i satisfying the conditions of Theorem 5 such that a
corresponding risk-aware Markov perfect equilibrium exists.
Example 3. The distance between any probability distribution and a reference distribution may be measured
by a φ-divergence function, several examples of φ-divergence functions are shown in Table 4. We can,
in principle, approximate convex φ-divergence functions with piecewise linear convex functions of the form
φ̂(µ) = maxj∈J {hdj , µi + gj }. Using this form of φ̂, we may define a corresponding set of probability
distributions:
Mis (Ps ) = {µ : µ = Ps ◦ ξs , Bs µ = e, µ ≥ 0, Bs Ps ◦ (dj ◦ ξs + gj ) ≤ αi · e, ∀j ∈ J },

(18)

for constants αi ∈ (0, 1) for all i ∈ I. Based on [42, Lemma 1], the risk measure corresponding to (18) has
the form


 
X −η
i
∗
,
(19)
ρ(X) = inf
η + b α + b EPs φ̂
b≥0, η∈R
b
where φ̂∗ is the convex conjugate of φ̂.
(i) Let φz denote a family of φ-divergence functions parameterized by z ∈ Z i that is concave in Z i , and
let φ̂z and φ∗z denote the corresponding piecewise linear approximation and its convex conjugate, respectively.
Then, we may define
Mz := {µ : µ = Ps ◦ ξs , Bs µ = e, µ ≥ 0,
Bs Ps ◦ φˆz (ξs ) ≤ αi · e},
and the risk measure corresponding to ∪z∈Z i Mz is
(

 )
 
X −η
∗
.
ρ(X) = inf maxi η + b α + b EPs φz
b≥0, η∈R z∈Z
b
i

Suppose we choose hz (from Theorem 5) to be
hz (X − η)
= φ∗z
b



X −η
b


,

for any η ∈ R and b > 0. Assume X has bounded support [ηmin , ηmax ], then formulation (20) becomes
ρ(X) =

min

max {η + EPs [hz (X − η)]} ,

η∈[ηmin , ηmax ] z∈Z i

which conforms to the saddle-point structure in Problem (11).
(ii) To recover CVaR, we let αi ∈ (0, 1) for all i ∈ I and choose the φ-divergence function
(
e
0
0 ≤ x ≤ 1−α
i,
φ(x) =
∞ otherwise,

23

(20)

Name
Kullback-Leibler
Burg entropy
χ2 distance

φ(t) t ≥ 0
t log t − t + 1
− log t + t − 1
1
2
t (t − 1)

Modified χ2 distance
Hellinger distance

(t − 1)2
√
( t − 1)2

χ- divergence

|t − 1|θ

Variation distance
Cressie-Read

φ∗ (s)
exp(s) − 1
− log(1
√ − s), s < 1
(2 − 2 1 − s, s < 1
−1
s < −2
2
s + s /4
s ≥ −2
s
,
s
<
1
1−s
 θ/(θ−1)
s + (θ + 1) |s|
θ

|t − 1|
t 6= 0, 1

1−θ+θt−tθ
θ(1−θ) ,

1
θ (1

max {−1, s} , s ≤ 1
− s(1 − θ))θ/(1−θ) − θ1 , s <

1
1−θ

Table 4: Examples of φ-divergence functions and their convex conjugate functions

and we take
n
Mis (Ps ) = µ : µ = Ps ◦ ξs , Bs µ = e, µ ≥ 0, 0 ≤ µ ≤

Ps o
.
1 − αi

If we take the convex conjugate of this φ-divergence function and substitute it into Eqs. (19), we obtain
ρ(X) =

min
η∈[ηmin , ηmax ]

{η + (1 − αi )−1 EPs [max {X − η, 0}]},

corresponding to hz (X − η) = −(1 − αi )−1 max {X − η, 0} for all z ∈ Z i .

A.4

RaNashQL Implementation Details

We give further details on each step of Algorithm 1 as follows. We will shortly require the definition
ΠY×Z [(y, z)] := arg

min

(y 0 , z 0 )∈Y×Z

k(y, z) − (y 0 , z 0 )k2 ,

i.e., the Euclidean projection onto Y × Z.
• Step 0: Initialization:
– Step 0a: Initialize all Q-values Qi1,1 (s, a) for all (s, a) ∈ K and i ∈ I;

i
i
– Step 0b: Initialize y0,t
(s, a), z0,t
(s, a) for all t ≤ T , (s, a) ∈ K, and i ∈ I.
• Step 1: For all (s, a) ∈ K and i ∈ I, set


i
i
i
i
yn,1
(s, a), zn,1
(s, a) = yn−1,T
(s, a), zn−1,T
(s, a) ,
and Qin,1 (s, a) = Qin−1,T (s, a). All agents observe the current state snt :
– Step 1a: Generate an action ain from policy π (which gives some positive probability to all
actions);

– Step 1b: Observe actions an = (ain )i∈I , costs ci (snt , an ) i∈I , and next state snt+1 ∼ P (· | snt , an ).


i
• Step 2: Compute Nash Q-values vn−1
(snt+1 ) = N ashi Qjn−1,T (snt+1 )
for all i ∈ I.Compute
j∈I

i
q̂n,t
(snt ,

n

i

i

a ) = G (c

(snt ,

an ) + γ

i
vn−1
(snt+1 ),

i
yn,t
(snt ,

i
an ), zn,t
(snt , an )),

and

i
i
yn,t
(snt , an ), zn,t
(snt , an ) =

t
X
1
t − τ∗ (t) + 1


i
i
yn,τ
(snt , an ), zn.τ
(snt , an ) ,

τ =τ∗ (t)

i
for all i ∈ I. This step observes a new state and computes the estimated Q-value q̂n,t
;

24

(21)

• Step 3: For all (s, a) ∈ K, and i ∈ I, compute

i
Qin,t (s, a) = 1 − θnβ (s, a) Qin−1,T (s, a) + θβn (s, a) q̂n,t
(snt , an ).

(22)

This update is the same as in standard Q-learning w.r.t. the outer loop.
• Step 4: Update

i
i
yn,t+1
(snt , an ), zn,t+1
(snt , an )
 i

i
=ΠY i ×Z i yn,t
(snt , an ), zn,t
(snt , an )

i
i
i
− λt,α ψ ci (snt , an ) + γ vn−1
(snt+1 ), yn,t
(snt , an ), zn,t
(snt , an ) ,

(23)

for all i ∈ I, and

i
i
i
ψ i vn−1
(snt+1 ), yn,t
(snt , an ), zn,t
(snt , an )

HY Giy (ci (snt , an )+
i
n
i
i
 γ vn−1
(snt , an ))
(st+1 ), yn,t
(snt , an ), zn,t


=

−HZ Giz (ci (snt , an )+
i
i
n
i
(snt , an ))
(snt , an ), zn,t
γ vn−1 (st+1 ), yn,t




.



(24)

This is the risk estimation step, it updates the current iterate of the risk corresponding to each selected
state-action pair.

A.5

Assumptions for RaNashQL

We now list the necessary definitions and assumptions for our algorithm, most of which are standard in the
stochastic approximation literature. We first define a probability space (Ω, G, P ) where
G = σ {(snt , an ), n ≤ N, t ≤ T } ,
is the σ-algebra for the history state-action pairs up to iteration T and N , and the filtration is
 
Gtn = σ (siτ , aiτ ), i < n, τ ≤ T ∪ {(snτ , anτ ), τ ≤ t} ,
n
for 1 ≤ t ≤ T − 1
for t ≤ T and n ≤ N , with Gt0 = {∅, Ω} for all t ≤ T . This filtration is nested, Gtn ⊆ Gt+1
n+1
n
and GT ⊆ G0 . The following assumption reflects our exploration requirement.

Assumption 7. (ε-greedy policy) There is an
the policy π satisfies,
for any n ≤ N, t ≤ T ,
 ε > 0 such that

n
and all (s, a) ∈ K, P (snt , an ) = (s, a) | Gt−1
≥ ε and P (sn1 , an ) = (s, a) | GTn−1 ≥ ε.
In particular, let xs ∈ Xs be a Nash equilibrium of the stage game (Qi (s))i∈I . Then, with probability
ε ∈ (0, 1), the action ai is chosen randomly from Ai , and with probability 1 − ε, the action ai is drawn from
Ai according to xis . Assumption 7 guarantees, by the Extended Borel-Cantelli Lemma in [11], that we will
visit every state-action pair infinitely often with probability one.
The next assumption contains our requirements on the step-sizes for the Q-value update.
Assumption
8. For all P
(s, a) ∈ K and for all n ≤ N, t ≤ T , the step-sizes for the Q-value update satisfy:
P∞ n
∞
n
2
θ
(s,
a)
=
∞
and
β
n=1
n=1 θβ (s, a) < ∞ for all t ≤ T and (s, a) ∈ K a.s. Let #(s, a, n) denote one
plus the number of times, until the beginning of iteration n, that the state-action pair (s, a) has been visited,
and let N s,a denote the set of outer iterations where action a was performed in state s. The step-sizes
1
s,a
θβn (s, a) satisfy θβn (s, a) = [#(s,a,n)]
and θβn (s, a) = 0 otherwise.
β if n ∈ N
Assumption 8 reflects the asynchronous nature of the Q-learning algorithm as stated in [15], only a single
state-action pair is updated when it is observed in each iteration, which can be implemented when there is
no initial knowledge on the state space.

25

A.6

Proof of Theorem 2

In this section, we develop the proof of almost sure convergence of RaNashQL (Theorem 2). This proof uses
techniques from the stochastic approximation literature [33, 10, 9, 25, 26], and is based on the following
steps:
1. Show that the stage game Nash equilibrium operator is non-expansive.
2. Bound the saddle-point estimation error in terms of the error between estimated Q-value to the optimal
Q-value.
3. Apply the classical stochastic approximation convergence theorem.
Preliminaries
We first present preliminary definitions and properties that will be used in the proof of Theorem 2. For any
i
i
(s, a) ∈ K, we define (yn,∗
(s, a), zn,∗
(s, a)) to be a saddle point of


i
(y(s, a), z(s, a)) → EPs (a, s0 ) Gi ci (s, a) + γ vn−1
(s0 ), y(s, a), z(s, a) ,
for each (s, a) ∈ K, where
i
vn−1
(s0 ) = N ashi Qj∗ (s0 )


j∈J

.

Similarly, we define (y∗i (s, a), z∗i (s, a)) to be a saddle point of
(y(s, a), z(s, a))


→EPs (a, s0 ) Gi ci (s, a) + γ v∗i (s0 ), y(s, a), z(s, a) ,
for each (s, a) ∈ K, where

v∗i (s0 ) = N ashi Qj∗ (s0 ) j∈J .
Let
i
i
i
i
i
i
i
Sn,t
:= {(∂Giy (ci + γ vn−1
, yn,t
, zn,t
), ∂Giz (ci + γ vn−1
, yn,t
, zn,t
))},

and
i

i
i
i
i
S n,t := {(∂Giy (ci + γ v∗ , yn,t
, zn,t
), ∂Giz (ci + γ v∗ , yn,t
, zn,t
))},
i
i
i
be the subdifferentials of Gi with respect to vn−1
and v∗i , given (yn,t
, zn,t
). The following Lemma 9 bounds
i

i
, S n,t ) in terms of k · k2 . This result follows from the convergence of subdifferentials of convex
dH (Sn,t
functions, the Lipschitz continuity of Gi , and non-expansiveness of the Nash equilibrium mapping for stage
games.
(1)

(2)

Lemma 9. [41, Theorem 4.1] Under the Lipschitz continuity of function Gi , there exist constants Kψ , Kψ >
0, such that
q
i
(1)
(2)
i
kQin−1,T − Qi∗ k2 .
dH (Sn,t
, S n,t ) ≤ Kψ kQin−1,T − Qi∗ k2 + Kψ
(25)
We conclude this preliminary subsection by showing that all mixed points of a stage game have equal
value.
Lemma 10. Let x and y be I 0 −mixed points of (C i )i∈I , then C i (x) = C i (y) for all i ∈ I.
Proof. Suppose x is a I1 -mixed point and y is a I2 -mixed point. For i ∈ I1 ∪ (I/I2 ), we have C i (x) ≤ C i (y)
and
i
C i (x) ≥ C i (ysi , x−i
s ) ≥ C (y).
The only consistent solution is C i (x) = C i (y). Similarly, for i ∈ I2 ∪ (I/I1 ), we have the similar argument.
For i ∈ I1 ∩ I2 and i ∈ (I/I1 ) ∩ (I/I2 ), we have C i (x) = C i (y) by the definitions of global optimal point
and saddle point.
26

Step 1: Show that the stage game Nash equilibrium operator is non-expansive
The following Lemma 11 shows that the Nash equilibrium mapping is non-expansive. We use the following
norm
kQi (s) − Q̃i (s)k∞ := max |Qi (s, a) − Q̃i (s, a)|
a∈A

for Q-values in all state s ∈ S.
Lemma 11. For Q and Q̃, let x and x̃ be Nash equilibria for Q and Q̃, respectively. Then, for all s ∈ S,
h
i


| Qi (s) (x) − Q̃i (s) (x̃)| ≤ kQi (s) − Q̃i (s)k∞ .
h
i


Proof. Let v i (s) = Qi (s) (x) and ṽ i (s) = Q̃i (s) (x̃). By Assumption 3, there are three possible cases to
consider:
h
i


Case 1: Both x and x̃ are global optimal points. If Qi (s) (x) ≥ Q̃i (s) (x̃), we have
v i (s) − ṽ i (s)
h
i


= Qi (s) (x) − Q̃i (s) (x̃)
h
i


≤ Qi (s) (x̃) − Q̃i (s) (x̃)
!


X Y
=
x̃i (ai )
Qi (s, a) − Q̃i (s, a)
a∈A

i∈I

X

Y

a∈A

i∈I

!
≤

i

i

x̃ (a ) kQi (s) − Q̃i (s)k∞

i

= kQ (s) − Q̃i (s)k∞ .
i
h


If Qi (s) (x) ≤ Q̃i (s) (x̃), the proof follows similarly.
h
i


Case 2: Both x and x̃ are saddle points. If Qi (s) (x) ≥ Q̃i (s) (x̃), we have
v i (s) − ṽ i (s)
i
h


= Qi (s) (x) − Q̃i (s) (x̃)
i
h


≤ Qi (s) (x) − Q̃i (s) (xi , x̃−i )
h
i


≤ Qi (s) (xi , x̃−i ) − Q̃i (s) (xi , x̃−i )

(26)
(27)

≤ kQi (s) − Q̃i (s)k∞ ,
where the first inequality (26) is by definition of Nash equilibrium, and inequality (27) is from Assumption
3 (ii)
Case 3: Both
x iand x̃ are I1 - and I2 -mixed points, respectively. Then, for i ∈ I1 ∪ (I/I2 ), and
h
 i 
Q (s) (x) ≥ Q̃i (s) (x̃), and by the arguments from Cases 1 and 2, we know that v i (s) − ṽ i (s) ≤
h
i


kQi (s) − Q̃i (s)k∞ . Alternatively, for i ∈ I2 ∪ (I/I1 ) and Qi (s) (x) ≤ Q̃i (s) (x̃), we have ṽ i (s) − v i (s) ≤
kQi (s) − Q̃i (s)k∞ .
Step 2: Bound the saddle-point estimation error in terms of the Q-value error
i
i
i
i
In this step we will bound k(yn,t
, zn,t
) − (yn,∗
, zn,∗
)k22 by a function of kQin−1,T − Qi∗ k22 and then we will
i
i
bound k(y∗i , z∗i ) − (yn,∗
, zn,∗
)k2 by a function of kQin−1,T − Qi∗ k2 . Our intent is to establish a relationship
between the risk estimation error (which depends on the saddle points of the risk measure) and the error
between estimated Q-value and the optimal one.

27

Lemma 12. [26, Lemma 5.3] Suppose Assumptions 7 and 8 hold, then there exists a constant 0 < κ <
(1)
1/C Kψ such that
i
i
i
i
2
k(yn,
t , zn, t ) − (yn,∗ , zn,∗ )k2 ≤

C(τ∗ (t))−α
(1)

κ(1 − C(τ∗ (t))−α Kψ κ)

kQin−1,T − Qi∗ k22 ,

(28)

for all t ≤ T and n ≤ N .
Proof. As a consequence of Eq. (23) in Step 4 of Algorithm 1, we have
i
i
i
i
2
k(yn,
t+1 , zn, t+1 ) − (yn,∗ , zn,∗ )k2
2

=

Y

i
(yn,
t,

Y i ×Z i
i
i
≤ k(yn,
t , zn, t )
i
i
≤ k(yn,
t , zn, t )

−
−

i
zn,
t)

−

i
λt,α ψ(vn−1
,

i
yn,
t,

i
zn,
t)



−

Y

i
(yn,∗
,

i
zn,∗
)

Y i ×Z i
i
i i
i
i
i
2
zn,∗ ) − λt,α ψ (c + γ vn−1 , yn,
t , zn, t )k2
i
zn,∗
)k22 + (HY2 + HZ2 )L2 C 2 t−2α
>
i
i
i
i
i
(yn,∗
, zn,∗
) Ct−α ψ i (ci + γ vn−1
, yn,
t , zn, t ),

2

i
(yn,∗
,
i
(yn,∗ ,

i
i
− 2 (yn,
t , zn, t ) −

(29)

where the first inequality holds by non-expansiveness of the projection operator and the second inequality
holds since the subgradients of Gi are bounded based on Assumption 2. Based on Lemma 9, we have
i
i
i
i i
i
i
i
kψ i (ci + γ vn−1
, yn,
t , zn, t ) − ψ (c + γ v∗ , yn, t , zn, t )k2
q
i
(1)
(2)
i
≤ dH (Sn,t
, S n,t ) ≤ Kψ kQin−1,T − Qi∗ k2 + Kψ
kQin−1,T − Qi∗ k2 .

> −α i i
i
i
i
i
i
i
i
Ct · ψ (c + γ vn−1
, yn,
Sum the terms (yn,
t , zn, t ) from τ∗ (t) to t, then divide by
t , zn, t ) − (yn,∗ , zn,∗ )
1
t−τ∗ (t)+1 to obtain:
t
X
1
t − τ∗ (t) + 1

i
i
i
i
(yn,τ
, zn,τ
) − (yn,∗
, zn,∗
)

>

i
i
i
Cτ −α ψ i (ci + γ vn−1
, yn,
t , zn, t )



τ =τ∗ (t)

>

i
i
i
i
i
i
i
i i
i
i
i
≤ (yn,
C(τ∗ (t))−α ψ i (ci + γ vn−1
, yn,
t , zn, t ) − (yn,∗ , zn,∗ )
t , zn, t ) − ψ (c + γ v∗ , yn, t , zn, t )
i
i
i
i
−α
i
i
i
i i
i
i
i
≤ k(yn,
kψ i (ci + γ vn−1
, yn,
t , zn, t ) − (yn,∗ , zn,∗ )k2 C(τ∗ (t))
t , zn, t ) − ψ (c + γ v∗ , yn, t , zn, t )k2
q


(1)
(2)
i
i
i
i
i
i
i
ik
−
Q
kQ
≤ C(τ∗ (t))−α k(yn,
,
z
)
−
(y
,
z
)k
K
kQ
−
Q
k
+
K
∗ 2 ,
t
n, t
n,∗
n,∗ 2
n−1,T
∗ 2
n−1,T
ψ
ψ
where the first inequality is by convexity of Gi in y and concavity of Gi in z. Using the standard inequality
2ab ≤ a2 κ + b2 /κ for all κ > 0, we see that
>

i
i
i
i
i
i
i
i i
i
i
i
− 2 (yn,
C(τ∗ (t))−α ψ i (ci + γ vn−1
, yn,
t , zn, t ) − (yn,∗ , zn,∗ )
t , zn, t ) − ψ (c + γ v∗ , yn, t , zn, t )
(1)

i
i
i
i
2
≥ − C(τ∗ (t))−α Kψ k(yn,
t , zn, t ) − (yn,∗ , zn,∗ )k2 κ

− C(τ∗ (t))−α kQin−1,T − Qi∗ k22 /κ
(2)

i
i
i
i
− C(τ∗ (t))−α Kψ k(yn,
t , zn, t ) − (yn,∗ , zn,∗ )k2

q

kQin−1,T − Qi∗ k2 .

By summing the right hand side of inequality (29) from τ∗ (t) to t, dividing by

28

(30)
1
t−τ∗ (t)+1 ,

and combining with

inequality (30) we obtain
t
X
1
i
i
i
i
(k(yn,τ
, zn,τ
) − (yn,∗
, zn,∗
)k22 + (HY2 + HZ2 )L2 C 2 τ −2α )
t − τ∗ (t) + 1
τ =τ∗ (t)

>
i
i
i
i
i
, yn,
− (yn,∗
, zn,∗
) C(τ∗ (t))−α × ψ i (ci + γ vn−1
t , zn, t )
>
i
i
i
i
i
i
i
2
i
i,n,t
i
i
, yn,
≥ k(yn,
) − (yn,∗
, zn,∗
) C(τ∗ (t))−α × ψ i (ci + γ vn−1
t , zn, t )
t , zn, t ) − (yn,∗ , zn,∗ )k2 − 2 (yn, t , z
−2

i
(yn,
t,

i
zn,
t)

(1)

i
i
i
i
2
−α
≥ (1 − C(τ∗ (t))−α Kψ κ)k(yn,
kQin−1,T − Qi∗ k22 /κ
t , zn, t ) − (yn,∗ , zn,∗ )k2 − C(τ∗ (t))
q
(2)
i
i
i
i
i
i
− C(τ∗ (t))−α Kψ k(yn,
t , zn, t ) − (yn,∗ , zn,∗ )k2 kQn−1,T − Q∗ k2 .

(31)

(1)

We further claim that we can choose κ satisfying 0 < κ < 1/C Kψ such that
(1)

i
i
i
i
2
−α
(1 − C(τ∗ (t))−α Kψ κ)k(yn,
kQin−1,T − Qi∗ k22 /κ ≤ 0,
t , zn, t ) − (yn,∗ , zn,∗ )k2 − C(τ∗ (t))

(32)

since the right hand side of inequality (32) will go to infinity when κ approaches zero, while the left hand
side is bounded by a constant. Then we achieve the desired result.
i
i
).
, zn,∗
The following lemma bounds the difference between (y∗i , z∗i ) and (yn,∗

Lemma 13. [26, Lemma 5.5] Under the Lipschitz continuity of Gi and Lemma 11, there exists KS > 0
such that for all n ≤ N we have
i
i
k(y∗i , z∗i ) − (yn,∗
, zn,∗
)k2 ≤ KS KG kQin−1,T − Qi∗ k2 .

(33)

Proof. It can be shown that
i
i
k(y∗i , z∗i ) − (yn,∗
, zn,∗
)k2
 i i

i
(s0 ), y∗i (s, a), z∗i (s, a)
≤ KS Es0 ∼P (·|s,a) G c + γ vn−1


i
i
i
(s0 ), yn,∗
(s, a), zn,∗
(s, a) 2
− Es0 ∼P (·|s,a) Gi ci + γ vn−1


i
(s0 ), y(s, a), z(s, a)
≤ KS maxi mini Es0 ∼P (·|s,a) Gi ci + γ vn−1
z∈Z

y∈Y



− mini Es0 ∼P (·|s,a) Gi ci + γ v∗i (s0 ), y(s, a), z(s, a)
y∈Y
2
 i i

i
0
0 ∼P (·|s,a) G
E
c
+
γ
v
(s
),
y(s,
a),
z(s,
a)
≤ KS max
s
n−1
y∈Y i , z∈Z i


− Es0 ∼P (·|s,a) Gi ci + γ v∗i (s0 ), y(s, a), z(s, a) 2
i
≤ KS KG kvn,t−1
(s) − v∗i (s)k2 ≤ KS KG kQin−1,T − Qi∗ k2 ,

where the first inequality follows from [50, Theorem 3.1] and [36, Proposition 3.1] (results on the stability of
optimal solutions of stochastic optimization problems), the second and fourth inequalities are due to Lemma
11, and the third inequality is by Lipschitz continuity of Gi .
Step 3: Apply the classical stochastic approximation convergence theorem
This step completes the proof of Theorem 2 by applying the stochastic approximation convergence theorem
(as in [9]). We first introduce a functional operator H i : V × Y × Z → V for each player i ∈ I, defined for
all (s, a) ∈ K,
 i


H (v, y, z) (s, a) := Gi ci (s, a) + γ v(s0 ), y(s, a), z(s, a) ,
(34)
where s0 ∼ P (· | s, a).

29

Eq. (13) can then be written as, ∀(s, a) ∈ K,


Qi∗ (s, a) = Es0 ∼P (·|s,a) H i (v∗i , y∗i , z∗i ) (s, a).
Next, for all (s, a) ∈ K, we define two stochastic processes:




i
i
i
i
i
i
in,t (s, a) := H i (vn−1
, yn,∗
, zn,∗
) (s, a) − H i (vn−1
, yn,t
, zn,t
) (s, a),




i
i
i
i
ξn,t
(s, a) := H i (v∗i , y∗i , z∗i ) (s, a) − H i (vn−1
, yn,∗
, zn,∗
) (s, a),

(35)

(36)
(37)

for t ≤ T and n ≤ N . The process in,t represents the risk estimation error (e.g. the duality gap in the
i
corresponding stochastic saddle-point problem) and the process ξn,t
represents the Q-value approximation
i
i
error of Qn,T with respect to Q∗ . In this new notation, we may write Step 3 in Algorithm 1 as
Qin,t (s, a) − Qin−1,T (s, a)
i
= − θnk [Qin−1,T (s, a) − Qi∗ (s, a) + ξn,t
(s, a) + in,t (s, a) + Qi∗ (s, a) − H i (v∗i , y∗i , z∗i )(s, a)],

(38)

for all (s, a) ∈ K, t ≤ T , and n ≤ N . Based on Eq. (35), we see that for all (s, a) ∈ K,




n−1
E Qi∗ (s, a) − H i (v∗i , y∗i , z∗i ) (s, a) | Gt+1
= 0.
By Lemma 12, we know that
kin,t k22 ≤

2
γ 2 C KG
(1)

κ(1 − C Kψ κ)

kQin−1,T − Qi∗ k22 ,

(39)

by setting t = 1 in inequality (28). In particular, inequality (39) shows that the conditional expectation
n−1
w.r.t. Gt+1
of the risk estimation error is bounded by kQin−1,T − Qi∗ k22 . In addition, by Lipschitz continuity
i
of G , we have
i
2 i
2
i
i
kξn,t
k22 ≤ γ 2 KG
|vn−1,T (s) − v∗i (s)| + γ 2 KG
|(y∗i (s, a), z∗i (s, a)) − (yn,∗
(s, a), zn,∗
(s, a))|
2
i
i
≤ γ 2 KG
[kQin−1,T − Qi∗ k2 + k(y∗i , z∗i ) − (yn,∗
, zn,∗
)k2 ]
2
≤ γ 2 KG
(1 + KG KS )kQin−1,T − Qi∗ k2 .

(40)

An iterative stochastic algorithm is of the form:
Xt+1 (s) = (1 − αt (s)) Xt (s) + αt (s) ((Bt Xt )(s) + wt (s)) , ∀s ∈ S,
where wt is bounded zero-mean noise, αt is the step size, and each Bt belongs to a family B of pseudocontraction mappings (see [7] for details).
Definition 5. [15, Definition 7] An iterative stochastic algorithm is well-behaved if:
P∞
P∞
1. The step sizes {αt (s)} satisfy: (i) t=0 αt (s) = ∞, (ii) t=0 αt2 (s) < ∞, and (iii) αt (s) ∈ (0, 1) for
all s ∈ S.
2. There exists B < ∞ such that |wt (s)| ≤ B for all s ∈ S and t ≥ 0.
3. For each Bt , there exists γ ∈ [0, 1) and X ∗ such that kBt X − X ∗ k ≤ γkX − X ∗ k for all X.
We define additional operators Uin,t : R|S| |A| → R|S| |A| on the Q-values for i ∈ I,
 
i
i
Uin,t Qi := H i (v i , yn,t
, zn,t
) − H i (v∗i , y∗i , z∗i ),
(41)

where v i is the value function in a Nash equilibrium of the stage game Qi (s) i∈I . We can then formulate
the process (38) as


Qin,t − Qi∗ = (1 − θkn )(Qin−1,T − Qi∗ ) + θkn (Uin,t Qin−1,T − Qi∗ + Qi∗ − H i (v∗i , y∗i , z∗i )).
(42)
Noting that


 
i
i
i
i i
i
i
kUin,t Qin−1,T − Uin,t Qi∗ k2 = kH i (vn−1
, yn,
t , zn, t ) − H (v∗ , yn, t , zn, t )k2 ,
where H i is defined in (34). By leveraging the non-expansiveness of the stage game equilibrium mapping in
Lemma 11 and Lipschitz continuity of G, it follows that the operators Uin,t is a pseudo-contraction.
30



 
Theorem 6. For all Qin−1, T , we have kUin,t Qin−1,T − Uin,t Qi∗ k2 ≤ γKG kQin−1,T − Qi∗ k2 .
In addition, from Assumption 8, we know that the update rule (42) satisfies Condition 1 of Definition 5.
Furthermore, based on Eq. (35), we know that update
 rule (42)
 satisfies Condition 2 of Definition 5. For the
following Lemma 14, we bound the l2 -norm of Uin,t Qin−1,T in terms of the estimation error. Such results
conform to Condition 3 in [15, Definition 7] or Condition 3 in Definition 5.
Lemma 14. Let

v
(1)
u
u
κ(1 − C Kψ κ)
1
,
min t
γ<
 C + (1 + KG KS )κ(1 − C K (1) κ)
KG
ψ

1




.



There exists γ 0 ∈ [0, 1) such that
v
u
u
γ =γt

2
C KG

0

(1)

κ(1 − C Kψ κ)

2 (1 + K K ),
+ KG
G S

and


kUin,t Qin−1,T k2 ≤ γ 0 kQin−1,T − Qi∗ k2 .
Proof. We have


kUin,t Qin−1,T k22
i
i
i i
i
i
= H i (v i , yn,
t , zn, t ) − H (v∗ , y∗ , z∗ )

2
2

i
= kin,t + ξn,t
k22
i
≤ kin,t k22 + kξn,t
k22

≤γ

2

!

2
C KG
(1)

κ(1 − C Kψ κ)

+

2
KG
(1

+ KG KS ) kQin−1,T − Qi∗ k22 ,

where the first equality holds by definition of Uin,t in Eq. (41), the second equality holds by Eq. (36) and
Eq. (37), and the last inequality follows from inequalities (39) and (40). Given the relationship between γ
and γ 0 , we get the desired result.
Given the update rule (38), Theorem 6, Lemma 14, and unbiasedness of


n−1
E Qi∗ − H i (v∗i , y∗i , z∗i )|Gt+1
= 0.
We may now apply the stochastic approximation convergence theorem [49, Corollary 5] or [15, Theorem 8].
We conclude that Qin,T (s, a) → Qi∗ (s, a) almost surely as n → ∞, for all i ∈ I and (s, a) ∈ K.

A.7

Practical Implementation of RaNashQL

There are several methods for computing Nash equilibria of stage games. The Lemke-Howson algorithm for
two player (bimatrix) games is proposed in [34]. This algorithm is efficient in practice, yet, in the worst case
the number of pivot operations may be exponential in the number of the game’s pure strategies. Recently,
[37] gives an algorithm for two player games that achieves polynomial-time complexity. Polynomial-time
approximation methods, such as [12, 21, 38], have been proposed for general sum games with more than two
players.
Implementation of RaNashQL is complicated by the fact that there might be multiple Nash equilibria
for a stage game. In RaNashQL, we choose a unique Nash equilibrium either based on its expected loss, or
based on the order it is ranked in a list of solutions. Such an order is determined by the action sequence,
which has little to do with the equilibrium conditions. For a two-player game, we calculate Nash equilibria
using the Lemke-Howson method (see [34]), which can generate equilibrium in a certain order.
We briefly discuss the storage requirement of RaNashQL. RaNashQL needs to maintain |I| Q-values and
|I| × |S| risk estimates (in terms of computing solutions of the corresponding saddle-point problems). In
31

each iteration, RaNashQL
updates all Qi (s, a) for all (s, a) ∈ S × A and i ∈ I. Additionally, it updates

i
i
y (s, a), z (s, a) for all i ∈ I through SASP. The total number of entries in each array Qi is |S| × |A|.
Since RaNashQL has to maintain the Q-values for every player, the total space requirement is |I| × |S| × |A|.
The storage requirement for the risk estimation is similar. Therefore, the storage requirement of RaNashQL
in terms of space is linear in the number of states, polynomial in the number of actions, and exponential in
the number of players.
The algorithm’s running time is dominated by computation of Nash equilibrium for the Q-function
updates. In general, the complexity of equilibrium computation in matrix games is unknown. As mentioned
in the previous section, some commonly used algorithms for two player games have exponential worst-case
bounds, and approximation methods are typically used for n-player games (see [38]).

A.8

Experiment Settings

We apply our techniques to the single server exponential queuing system from [30]. In this packet switched
network, packets (blocks of data) are routed between servers over links shared with other traffic. The service
rate of each server can be set to different levels and is controlled by a service provider (Player 1). Packets
are routed by a programmable physical device, called a router (Player 2). A router dynamically controls
the flow of arriving packets into a finite buffer at each server. The rates chosen by the service provider and
router depend on the number of packets in the system. In fact, it is to the benefit of a service provider to
increase the amount of packets processed in the system. However, such an increase may result in an increase
in packets’ waiting times in the buffer (called latency), and routers are used to reduce packets’ waiting times.
Thus, the game theoretic nature of the problem arises because the service provider and router the have such
competing objectives.
The state space is S = {0, 1, ..., S}, where S < ∞ is the maximum number of packets allowed in the
system. Only one packet can be in service at each time, while the remaining packets wait for service in the
buffer. The router admits one packet into the system at each time. Every time a state is visited, the service
provider and the router simultaneously choose a service rate µ > 0 and an admission rate λ > 0. Suppose
there are s packets in the system and the players choose the action tuple (µ, λ), then the router incurs a
holding cost h(s) and a cost θ(µ, λ) associated with having packets served at rate µ when it admits packets
at rate λ. If there are no packets in the system, θ(µ, λ) can be interpreted as the setup cost of the server.
These payoffs are modeled as being paid to the service provider, since the players’ objectives are in conflict.
The service provider, in turn, pays the router β(µ, λ) which represents the reward to the router for choosing
the rate λ. It can also be interpreted as the setup cost of the router. The cost functions for strategy profile
a = (µ, λ) are then:
c1 (s, a) := β(a) − θ(a),
and
c2 (s, a) := h(s) + θ(a) − β(a).
We assume that the time until the admission of a new packet and the next service completion are both
exponentially distributed with means 1/λ and 1/µ, respectively. We can therefore model the number of
packets in the system as a birth and death process with state transition probabilities:

µ/(λ + µ), 1 < s < S, k = s − 1,



λ/(λ + µ), 0 < s < S − 1, k = s + 1,
P (k | s, a) :=

1,
s = 0, k = 1,



1,
s = S, k = S − 1.
We choose the following parameters for our example:
• S = 30.
• Each player has the same two available actions in every state:
– router: first action (denoted λ) is to admit one packet into the system every 10s; second action
(denoted λ) is to admit one packet every 25s.
32

– service provider: first action (denoted µ) is to serve one packet every 11s; second action (denoted
µ) is to serve a packet every 20s.
• Holding costs are exponential h(s) = a bαs for s ≥ 1 with a = 1.2 and b = e, and α = 0.2 and
h (0) = 0. We set costs: θ(µ, λ) = θ(µ, λ) = 110, θ(µ, λ) = θ(µ, λ) = 90, β(µ, λ) = 60, β(µ, λ) = 30,
β(µ, λ) = 20, and β(µ, λ) = 70.
In this setting, the router pays the service provider more when the service rate is higher. Also, the router
receives higher reward when both players choose higher rates or lower rates. The router receives lower reward
when the admission and service rates do not match.
We conduct three experiments, where all risk-aware players’ use CVaR. The CVaR for player i is


1
E
[max
{X
−
η,
0}]
,
CVaRαi (X) := min η +
η∈R
1 − αi
where αi ∈ [0, 1) is the risk tolerance for player i.
When implementing RaNashQL, we use the Lemke-Howson method to compute the Nash equilibria of
stage games, and we update the Q-values based on the first Nash equilibrium generated from the method.
We run our experiments in Matlab R2015a on a computer with an Intel Core i7 2.30GHz processor, 8GM
RAM, running the 64-bit Windows 8 operating system.

A.9

Multilinear Systems

In this section, we only focus on stochasticity from state transitions and derive a multilinear system formulation for risk-aware Markov perfect equilibria. This is to facilitate comparison with the robust Markov
perfect equilibria in[30]. All ρi are taken to be CVaR. Corresponding to CVaR, we define
Ps
≥ 0,
1 − αi
o
e> µ = 1, µ ≥ 0 .

n
Mis (Ps ) ≡ µ : − e> µ +

(43)

The set (43) conforms to Assumption 1(ii). Suppose Assumption 1 holds, then x∗ ∈ X is a risk-aware Markov
perfect equilibrium if and only if (x, v) ∈ X × V is a solution of:
xis ∈ argminuis ∈P(Ai )

max

hµ, Csi (v i )i,

(44)

µ∈Mis (Ps )

for all s ∈ S, i ∈ I. Since we only consider the risk from the stochastic state transitions, Problem (44) is
equivalent to the system, for all s ∈ S, i ∈ I,
"
#
X
X

xis ∈ argminuis ∈P(Ai ) max
uis (ai ) Πj6=i xjs (aj )
ci (s, a) + γ
P (k | s, a)v i (k) .
(45)
i
µ∈Ms (Ps )

a∈A

k∈S

Formulation (45) can be further rewritten as
(
xis

∈argminuis ∈P(Ai ) qsi :
qsi ≥

max
i

µ∈Ms (Ps )

X

uis (ai ) Πj6=i xjs (aj )
a∈A

"

#)
i

c (s, a) + γ

X

i

P (k | s, a)v (k)

.

k∈S

We introduce the following notation for our multilinear system formulation:

33

(46)

i |I|−1

i
|A
1. Let Esi (x−i
s , C )∈R

|

×|Ai |

denote the matrix whose rows are given by the vector


Y
 xjs (aj )ci (s, (a−i , ai ))
.
j6=i

(47)

ai ∈Ai

2. Define the vector zsi ∈ R|S| |A| as


Y

zsi :=  uis (ai ) Πj6=i xjs (aj ) v i (k)
j6=i

.

(48)

aj ∈Aj , ai ∈Ai , k∈S

i

i
|S| |A|×|A |
3. Let Ysi (x−i
be the matrix such that
s , v )∈R
i i
i
Ysi (x−i
s , v )us = zs .

(49)

4. Let tis := [ti (k | s, a)]a∈A, k∈S be probability distributions that satisfy
X

ti (k | s, a) = 1, ∀s ∈ S, a ∈ A.
k∈S

5. Finally, let




X
T i (x) := 
a∈A

Y


xis (ai )ti (k | s, a)

i∈I

,

s, k∈S

and denote the sth row of T i (x) by [tis (x)]> .
The next theorem uses the strategy of [31] to give a multilinear system formulation for the equilibrium
conditions (44) (which correspond to a risk-aware Markov perfect equilibrium).
Theorem
7. A stationary strategy x is a CVaR risk-aware Markov perfect equilibrium point with value
 i
v i∈I if and only if for all i ∈ I and s ∈ S, there exist mis , nis ∈ R|A| , tis ∈ R|S| |A| such that for any
a ∈ A, and for h ∈ A, (v i , xs , mis , nis , tis ) satisfies
 i > i
i i
v i (s) = e> Esi (x−i
v
s , C )xs + γ ts (x)
 i −i i >
i
>
v (s) ≤ eh Es (xs , C ) e
i −i
i i
+ γe>
h Ys (xs , v )ts ,
Ps > i
v i (s) ≥ [−
] ns
1 − αi
i i
+ e> mis + e> Esi (x−i
s , C )us ,
i i
> i
γYsi (x−i
s , v )us ≤ − e ns ,
Ps
−e> tis ≤ −
,
1 − αi
e> tis = 1,

e> xis = 1,
xis ≥ 0,
nis ≤ 0,
where eh is the hth unit column vector of dimension |A|, e> is a row vector of all ones of appropriate
i
i −i
i
dimension, Esi (x−i
s , C ) is obtained from formulation (47) and Ys (xs , v ) is the matrix given by Eq. (49).
34

Proof. (Proof sketch) (Step 1) First we reformulate the inner maximization (primal problem) in Problem
(45) as

i i
max γ µ, v i + e> Esi (x−i
s , C )us ,
µ≥0

then take the dual of the first maximization term

max γ µ, v i : − e> µ +
µ≥0


Ps
>
≥ 0, e µ = 1, µ ≥ 0 .
1 − αi

The dual problem is
(
−

min

nis ≥0, mis



>
nis

γYsi (x−i
s ,

Ps
1 − αi
)uis

+

Ps
1 − αi



v

i



− mis :

>
nis

)
e+

mis e

≤0 ,

which can be rewritten as
(
nis

min
i

ns ≤0, mis

>



γYsi (x−i
s ,

v

i

)uis

+ mis :

−

>
nis

)
e−

mis e

≤0 .

The primal problem has a non-empty bounded feasible region (by assumption) so it attains its optimal value.
Strong duality then holds, and so the dual problem also attains its optimal value and the optimal values are
equal.
(Step 2) We combine the two minimization objectives to rewrite Problem (44), use formulation (46), and
then derive a single linear programming problem as
min

uis ,qsi ,mis ,nis

qsi

(50)

s.t. qsi ≥ nis

>



Ps
1 − αi



+ mis

i i
+ γYsi (x−i
s , v )us ,
>
i i
nis e − mis e ≥ γYsi (x−i
s , v )us ,

e

T

uis

uis

(51)
(52)

= 1,

≥ 0,

nis

(53)
≤ 0.

(54)

(Step 3) For the final step, we take the dual of Problem (50)-(54). And the resulting dual problem is
max v i (s)

v i (s), tis

Ps
,
1 − αi
e> tis = 1,
 i −i i >
i −i
i i
v i (s) ≤ e>
1 + γe>
h Es (xs , C )
h Ys (xs , v )ts ,

s.t. e> tis ≥

h = 1, ..., A.

The desired multilinear system follows by strong duality since the primal feasible region is non-empty and
bounded.
Remark 1. Nonlinear optimization methods have been used to solve multilinear systems that arise from
equilibrium computation of one-shot games with up to four players and four actions per player in less than
five minutes (see [1, 13]). Homotopy methods have been used to solve multilinear systems that arise from
35

complete information stochastic games for two players, two actions per player, and five states in less than
one minute, see [22]. We adopt the methodology proposed in [1, Section 5.2.2]. We first cast the multilinear
constraints into an appropriate penalty function, and then solve the resulting unconstrained minimization
problem with an interior point algorithm. This procedure will converge a local minimum, but not necessarily
to a global minimum.

A.10

Additional Experiments

In the section, we provide supplementary materials for Experiment I and II.
Experiment I: We compare RaNashQL for risk-aware Markov game with Nash Q-learning in [24] for
risk-neutral Markov game, in terms of their convergence rates. Given any precision  > 0, we record the
iteration count n until the convergence criterion kQin, T − Qi∗ k2 ≤  is satisfied. Here we choose T = 10 and
we choose N = 1 × 105 for RaNashQL and N = 1 × 106 for Nash Q-learning, such that both methods have
the same total number of iterations. When  is extremely small e.g.,  = 0.001, the total number of iterations
for RaNashQL and Nash Q-learning for the two players are respectively: 983443 (Nash Q-learning, Service
provider), 936761 (Nash Q-learning, Router), 999991 (RaNashQL, Service provider and Router), which are
relatively equal. Moreover, Figure 1 shows that the total number of iterations for Nash Q-learning decrease
dramatically as the increase of precision , which reveals that RaNashQL is more computationally expensive
than Nash Q-learning in terms of achieving the same convergence criterion.

Figure 3: Comparison between NashQL and RaNashQL

Figure 2 presents the Markov perfect equilibrium for the risk-neutral and risk-aware cases. It shows the
equilibrium shifting when considering the risk-awareness of players. It also shows that the both risk-neutral
and risk-aware Markov perfect equilibrium are sensitive to the perturbations in the service rates, and riskaware strategies for both players highly fluctuate with the change of state (number of packet in the queuing
system). We also study how the risk tolerance level αi (See Table 2) affects the risk-aware Markov perfect
equilibrium, which also shows the risk-aware Markov perfect equilibrium fluctuates with the change of the
risk tolerance level of CVaR.

36

Figure 4: Comparison of Risk-Neutral and Risk-aware Markov Perfect Equilibrium

Scenario
Scenario
Scenario
Scenario

1
2
3
4

Service Provider (α1 )
0.1
0.2
0.1
0.2

Router (α2 )
0.1
0.2
0.2
0.1

Table 5: Risk Tolerance Level αi

Next, we evaluate the discounted cost under risk-neutral and risk-aware Markov perfect equilibrium in
simulation (1000 complete runs of the algorithm to compute the entire risk-aware Markov perfect equilibria).
The risk tolerance levels are selected as α1 = α2 = 0.1, for the risk-aware (CVaR) method in Table 3 here.
Table 3 shows that considering risk awareness will significantly increase the variance of the discounted cost,
which is contrary to expectation. The possible reason is the higher fluctuation of risk-aware strategies with
the change of state (number of packet in the queuing system) than risk-neutral strategies.
Experiment II: In this experiment, we consider a special case where the risk only comes from the
stochasticity from state transitions (this setting is basically a risk-aware interpretation of [31] where the
ambiguity is over the transition kernel). In this special case, we can compute risk-aware Markov equilibrium
using a multilinear system as detailed in Section A.9. We evaluate performance in terms of the relative error
r

2
P
i (Qj
i (s)
N
ash
(s))
−
v
j∈I
∗
n, T
s∈S
pP
, n ≤ N.
i
2
s∈S v∗ (s)
37

Player
Service Provider
Router

Method
Risk-neutral
Risk-aware (CVaR)
Risk-neutral
Risk-aware (CVaR)

Mean
−22.22
−77.78
37.48
83.68

Variance
1.4736e − 06
407.84
7.32
491.20

5%-CVaR
−22.22
−69.34
37.94
86.03

10%-CVaR
−22.22
−68.26
38.18
87.54

Table 6: Simulation for Risk-neutral Strategies and Risk-aware Strategies (α1 = α2 = 0.1)

Figure 5: Almost Sure Convergence of RaNashQL

In this experiment, we take the risk measure as 10%-CVaR. The multilinear system is solved by an interior
point algorithm within 5×107 maximum function evaluation and 1×105 maximum iterations, and it converges
to a local optimal solution in 10471.975 seconds. For RaNashQL, we choose T = 10 and N = 2 × 106 , and
the total implementation time for RaNashQL is 10245.314 seconds. The following Figure 3 validates the
almost sure convergence of RaNashQL to the service provider’s strategy. For the router, the relative error is
large (around 190%). One possible reason is that RaNashQL converges to different equilibria compared to
the one obtained by the multilinear system solver. We see that RaNashQL possesses superior computational
performance than interior point algorithm for this task, since the relative error of service provider is within
25% in 1 × 106 iterations, and the implementation time will be 5122.657 seconds.

38

