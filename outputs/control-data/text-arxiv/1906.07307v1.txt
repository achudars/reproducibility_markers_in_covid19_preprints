Towards Transfer Learning for End-to-End Speech Synthesis
from Deep Pre-Trained Language Models
Wei Fang, Yu-An Chung, James Glass
MIT Computer Science and Artificial Intelligence Laboratory
Cambridge, MA 02139, USA
{weifang,andyyuan,glass}@mit.edu

arXiv:1906.07307v1 [cs.CL] 17 Jun 2019

Abstract
Modern text-to-speech (TTS) systems are able to generate audio
that sounds almost as natural as human speech. However, the
bar of developing high-quality TTS systems remains high since
a sizable set of studio-quality <text, audio> pairs is usually required. Compared to commercial data used to develop stateof-the-art systems, publicly available data are usually worse
in terms of both quality and size. Audio generated by TTS
systems trained on publicly available data tends to not only
sound less natural, but also exhibits more background noise.
In this work, we aim to lower TTS systems’ reliance on highquality data by providing them the textual knowledge extracted
by deep pre-trained language models during training. In particular, we investigate the use of BERT to assist the training of
Tacotron-2, a state of the art TTS consisting of an encoder and
an attention-based decoder. BERT representations learned from
large amounts of unlabeled text data are shown to contain very
rich semantic and syntactic information about the input text, and
have potential to be leveraged by a TTS system to compensate
the lack of high-quality data. We incorporate BERT as a parallel branch to the Tacotron-2 encoder with its own attention
head. For an input text, it is simultaneously passed into BERT
and the Tacotron-2 encoder. The representations extracted by
the two branches are concatenated and then fed to the decoder.
As a preliminary study, although we have not found incorporating BERT into Tacotron-2 generates more natural or cleaner
speech at a human-perceivable level, we observe improvements
in other aspects such as the model is being significantly better
at knowing when to stop decoding such that there is much less
babbling at the end of the synthesized audio and faster convergence during training.
Index Terms: speech synthesis, text-to-speech, transfer learning, pre-trained language models, semi-supervised learning

1. Introduction
End-to-end, deep learning-based approaches are causing a
paradigm shift in the field of text-to-speech (TTS) [1, 2, 3, 4, 5].
Unlike traditional parametric TTS systems [6, 7], which typically pipeline a text front-end, a duration model, an acoustic
feature prediction model, and a vocoder, end-to-end neural TTS
approaches integrate all these components into a single network
and thus only require paired text and audio for training. This
removes the need of extensive domain expertise for designing
each component individually, offering a much simplified voice
building pipeline. More importantly, they have been shown to
be capable of generating speech that sounds almost as natural
as humans [4, 8].
TTS systems that produce speech close to human quality,
however, are usually trained on data collected by individual in-

stitutions that are not publicly accessible. Due to their strictlycontrolled conditions during the collection process, it is a common belief that these internal data have higher quality than the
publicly available ones. Such a belief is supported by the fact
that when training modern TTS models such as Tacotron-2 [4]
on publicly available dataset like LJ Speech [9], the synthesized
speech sounds less natural and exhibits more noise in the audio background than that produced by Tacotron-2 trained on
Google’s internal data. Therefore, it is still unclear whether we
can achieve state-of-the-art TTS performance using only publicly available data.
Recently, deep pre-trained language models (LMs) [10, 11,
12, 13] have shown to be capable of extracting textual representations that contain very rich syntactic and semantic information about the text sequences. By transferring the textual
knowledge contained in these deep pre-trained LMs (e.g., by replacing the original text input with the extracted text features),
a simple downstream model is able to achieve state-of-the-art
performance on a wide range of natural language processing
tasks such as natural language inference, sentiment analysis,
and question answering, to name a few. These deep LMs are
first trained on large amounts of unlabeled text data using selfsupervised objectives, and then fine-tuned with task-specific
losses along with models for the downstream tasks.
In this work, we aim to leverage the textual knowledge contained in these deep pre-trained LMs to lower end-to-end TTS
systems’ reliance on high-quality data. Particularly, we investigate the use of BERT [13] for assisting the training of Tacotron2 [4]. The backbone of Tacotron-2 is a sequence-to-sequence
network [14] with attention [15] that consists of an encoder and
a decoder. The goal of the encoder is to transform the input
text into robust sequential representations of text, which are
then consumed by the attention-based decoder for predicting
the spectral features. We enrich the textual information to be
consumed by the decoder by feeding the linguistic features extracted by BERT from the same input text to the decoder as well
along with the original encoder representations.
Existing work such as [16, 17] also attempts to make use
of the textual knowledge learned from large text corpora to
improve TTS. However, they either focus on improving the
data efficiency of TTS training, i.e., to minimize the amount
of paired text and audio for training, in a small-data regime,
or the word embedding modules they use to provide textual
knowledge have some limitations by nature, e.g., the word embeddings are trained based on very shallow language modeling
tasks and are hence considered less powerful than the one we
use in this work.

Waveform
BERT Encoder
WaveGlow Vocoder
Transformer layers

WordPiece Embedding

Location-sensitive
Attention

Mel Spectrogram

Stop

Original Tacotron2
Encoder

Decoder
Post-Net

Bi-directional LSTM
Convolution Layers

Location-sensitive
Attention

Character Embedding

Autoregressive LSTM

Pre-Net

Input text

Figure 1: Overall architecture of the proposed model. The bottom block depicts the original Tacotron-2, which is used as our base TTS
system. Tacotron-2 consists of an encoder and an attention-based decoder. For an input text, the goal of the encoder is to transform
it into robust textual representations that are then consumed by the attention-based decoder for predicting the spectral features. The
upper part of the figure illustrates how we incorporate BERT into the TTS system. Specifically, BERT is fed with the same input text and
extracts another sequence of textual representations of it. For each decoding time step, the decoder attends back to the two sequences
of textual representations produced by the two branches (i.e., the Tacotron-2 encoder and BERT) with separate attention heads. The
two generated attention context vectors are concatenated and fed to the decoder as the new input for decoding.

2. Proposed Approach
In this section, we start with introducing BERT [13] and
Tacotron-2 [4]. We then present the proposed approach for incorporating BERT representations into the training of Tacotron2. The proposed approach is illustrated in Figure 1.
2.1. Tacotron-2
Tacotron-2 follows the sequence-to-sequence (seq2seq) with attention framework and functions as a spectral feature (e.g., mel
spectrogram) prediction network. The predicted spectral features are then inverted by a separately trained vocoder into
time-domain waveforms. In this work, we modify the seq2seq
component by incorporating BERT as a parallel branch to the
Tacotron-2 encoder, and use WaveGlow [18] as the vocoder to
synthesize the audio waveforms.
The seq2seq component consists of two main building
blocks: an encoder and an attention-based decoder. At a high
level, the encoder takes a text sequence as input and transforms
it into a sequence of textual representations. For each decoding
time step, the decoder conditions on these textual representations and predicts a mel spectrogram frame.
Specifically, the encoder takes a character sequence as input, passes it through a stack of convolutional layers followed
by a single-layer bidirectional LSTM, and the hidden states
of the recurrent network are used as the encoded representations. These representations will then be consumed by the
decoder. The decoder is an autoregressive LSTM with the
location-sensitive attention [15] mechanism that summarizes
the encoded representations at each decoding time step. The
input to the decoder at each time step, which is the prediction

from the previous step, is first passed through a pre-net before
being fed into the LSTM. Lastly, the output of the LSTM is processed by a convolutional post-net to predict the final spectrogram. The output of the LSTM is also fed into another network
with a sigmoid activation to determine when to stop decoding.
During training, the mean squared error is calculated at the
spectrogram prediction output, while the stop-token prediction
is trained with binary cross-entropy loss. Additionally, teacherforcing is used to train the recurrent decoder. During inference,
the ground truth targets are unknown. The decoder generates
the mel spectrogram in an autoregressive fashion as opposed to
teacher-forcing. Generation is completed when the stop token
output exceeds a threshold of 0.5.
2.2. BERT
Our goal is to leverage rich textual knowledge contained in deep
pre-trained LMs to assist TTS training. To do so, we use BERT
to transform the input text sequence into textual representations
that are in parallel to those extracted by the Tacotron-2 encoder,
and provide both of them to the Tacotron-2 decoder. BERT is a
Transformer-based [19] model trained on large amounts of text
in an unsupervised manner. Below we briefly summarize BERT.
Input Representations. Unlike Tacotron-2 that takes character sequence as input, the input to BERT is a sequence of
subword units that are usually referred to as WordPieces [20],
where the tokenization is determined by a Byte-Pair Encoding
process [21]. Since a Transformer is position-agnostic, it needs
positional information encoded in their input, thus it uses learnable positional embeddings for WordPiece tokens with length
up to 512. Additionally, a special [CLS] token is added at the

beginning of each sequence so that the final hidden state corresponding to this token represents the aggregate vector of the
sequence.
The Transformer Model. The backbone of BERT is a multilayer bidirectional Transformer encoder, which consists of multiple blocks of multi-head self-attention stacked together. In this
work we use the BERTBASE configuration that contains 12
attention blocks each with 12 attention heads and hidden size
of 768.
Pre-Training. The BERT model is pre-trained using two unsupervised objectives: masked language modeling, for which
the model has to predict a randomly masked out token, and next
sentence prediction, where two sentences are packed as input to
the encoder and the aggregate vector is used to predict whether
they are adjacent sentences.
2.3. Using BERT in Tacotron-2 Framework
As mentioned previously, the Tacotron-2 encoder aims to extract robust sequential representations of the input text, the decoder then decodes the spectrogram by conditioning on these
textual representations. By drawing analogy to the traditional
parametric TTS systems, the Tacotron-2 encoder can be viewed
as the linguistic front-end and the decoder is similar to the statistical acoustic prediction model.
From this view, we propose to inject the textual information contained in the BERT representations to the Tacotron-2
decoder, so that it has access to the textual features from both
the Tacotron-2 encoder and BERT to make a spectral prediction. In practice, we feed the input text, which is first tokenized
into WordPiece sequence, into BERT, and the representations
from the last attention block is exposed to the decoder. The
decoder then attends back to both the Tacotron-2 encoder representations and BERT representations using separate locationsensitive attention heads in order to produce the corresponding
attention context vectors. The two vectors are then concatenated
before feeding into the autoregressive recurrent network.
There are other ways for incorporating external textual representations into the text front-end of a TTS system, for example by concatenating the representations with the character embedding sequence [16]. However, there usually exists a
mismatch between the time resolutions of the two sequences
and the existing solutions are not as intuitive as simply letting the decoder attend to the representations extracted by both
branches.
Note that although we use Tacotron-2 as the base TTS system in this work, our framework can be easily extended to other
TTS systems as well.

Table 1: Performance of our model versus the baseline
Tacotron-2 model on several evaluation metrics.
Model
Tacotron-2
Ours

3.1. Dataset and Settings
We use LJ Speech [9], a public domain speech dataset consisting of 13100 audio clips of a single speaker reading from nonfiction books. The audio utterances vary in length from 1 to 10
seconds, with a total length of about 24 hours of speech. They
are encoded with 16-bit PCM with a sample rate of 22050Hz.
The dataset is split into train, validation, and test sets of 12500,
100, and 500 clips respectively.
We use the implementation of Tacotron-2 by Nvidia1 and
keep the default hyperparameters to train the baseline model.
1 https://github.com/NVIDIA/tacotron2

GPE
0.722
0.720

FFE
0.740
0.735

For our model we use the same configuration for the Tacotron-2
component. The parameterization of the additional locationsensitive attention layer for attending to the BERT representations is also kept the same. The entire model is trained endto-end, thus the losses are also back-propagated into the BERT
encoder to fine-tune the textual representations. The Adam optimizer is used with a learning rate of 0.001 to learn the parameters.
3.2. Evaluation Metrics
To measure performance, we use several metrics used in previous works that correlate with voice quality and prosody. To
compare the generated audio to the reference audio, these metrics are only calculated up to the length of the shorter audio
signal. The pitch and voicing metrics are computed using the
YIN [22] pitch tracking algorithm.
Mean Cepstral Distortion (MCDK ) [23]:
v
T −1 uX
K
1 Xu
t (ct,k − ĉt,k )2 ,
MCDK =
T t=0
k=1

where ct,k and ĉt,k are the k-th mel frequency cepstral coefficient (MFCC) of the t-th frame from the reference and generated audio, respectively. The overall energy ct,0 is discarded in
this measure. We follow previous work and set K = 13.
Gross Pitch Error (GPE) [24]:
P
p̂t | > 0.2pt ]1[vt ]1[v̂t ]
t 1[|pt −
P
GPE =
,
t 1[vt ]1[v̂t ]
where pt and p̂t are the pitch signals from the reference and generated audio, vt and v̂t are the voicing decisions from the reference and generated audio, and 1 denotes the indicator function.
GPE measures the percentage of voiced frames that deviate by
more than 20% in the pitch signal of the generated audio compared to the reference.
F0 Frame Error (FFE) [25]:
FFE =

3. Experiments

MCD13
21.88
25.21

1 X
1[|pt − p̂t | > 0.2pt ]1[vt ]1[v̂t ] + 1[vt 6= v̂t ].
T t

Following the definitions of GPE, FFE measures the percentage of frames that either have a 20% pitch error or a differing
voicing decision between the generated and reference audio.
3.3. Results and Observations
Table 1 shows the performance of our model versus the baseline
Tacotron-2 model without the BERT encoder. All metrics calculate some form of error of the generated audio against the reference audio, thus the lower the better. On the MCD13 metric,
our model performs a little worse than the baseline model, but
on the GPE and FFE metrics our model performs slightly better.
Overall, there is not much difference in terms of these metrics

0.9
40

FFE

MCD13

Tacotron-2
Ours

Tacotron-2
Ours

50

30

0.8

20
0

20
40
Training steps (k)

0

10 20 30 40
Training steps (k)

50

Figure 2: Plots for M CD13 and FFE metrics as training steps increases.

(a) Encoder Attention

(b) BERT Attention

Figure 3: Attention alignments for encoder and BERT encoder on a test phrase.

at the end of training between both models. Additionally, we
do not find a difference in naturalness for audios synthesized by
the two models at the end.
However, if we visualize the performance metrics against
the number of training steps, as shown in Figure 2, we see that
our model converges faster compared to the baseline Tacotron2. This suggests that the addition of the learned representations
of BERT may have helped our model learn quicker early on during training. Even though the metrics converge to similar values at the end of training, our model still benefits from the pretrained knowledge from BERT. We also discover a difference
early on in training in terms of the quality of the audio generated
by the two models. The quality of the audio generated by our
model is generally better than the baseline Tacotron-2, but the
discrepancy disappears as training progresses. These results are
to some extent similar to those reported in previous work [16],
where semi-supervised learning with external text representations and pre-training improves synthesis performance in lowresource settings but not in large-data regimes.
From the learning curves, we can also see that the MCD13
metric converges much quicker than the FFE metric and oscillates within a small range, and we do not observe much correlation of the MCD13 metric with audio quality. We even find
that models at early training stages that achieve relatively low
MCD13 synthesize gibberish audio. Compared to MCD13 , we
find that FFE offers a much better indication of audio quality.
Additionally, we observe a clear improvement in predicting
when to stop generation with our model. The baseline Tacotron2 often has trouble ending its decoding process, generating gibberish noise at the end. Our model, on the other hand, almost
never exhibits this behavior.
We visualize the attention alignments of the decoder against

both attention layers of our model in Figure 3. From Figure 3a,
we see that the attention is a almost diagonal, meaning that the
decoder mostly focuses on the correct characters as the audio sequence is generated. This pattern is also observed in the original
Tacotron [1], as well as the baseline Tacotron-2 model. From
what we observe, this behavior is strongly correlated with synthesized audio quality, but unfortunately there isn’t a straightforward method to quantify this behavior. On the other hand,
in our model the attention layer that attends to the BERT representations, shown in Figure 3b, only has a rough diagonal pattern at the beginning of decoding. Its attention is more spread
across different time steps compared to the encoder attention
alignments. We can also see from the figures that the values of
the attention are much lower for the BERT attention. Since the
encoder attention patterns are similar, we hypothesize that the
model still learns to map the text sequence to acoustic feature
sequence mainly by the textual representations learned from the
encoder. The BERT representations serve as additional information that the decoder uses to improve its prediction.

4. Discussion and Future Work
In this work, we propose to exploit the textual representations
from pre-trained deep LMs for improving end-to-end neural
speech synthesis. As a preliminary study, we have not found incorporating BERT into the Tacotron-2 framework significantly
improves the quality of the synthesized audio; however, we do
find that our approach improves the Tacotron-2 model in other
aspects such as faster convergence during training and the final
model is significantly better at knowing when to stop decoding
such that the synthesized audio has less babbling in the end.
This is only a preliminary work, and there is still a lot left

to be studied. For instance, instead of utilizing only knowledge from unlabeled text data, we can also make use of large
amounts of unlabeled speech corpora [26] to assist the decoder
in learning the acoustic representations and alignments. A potential method could be to initialize Tacotron decoder with a
pre-trained auto-regressive predictive coding model [27].

[20] Y. Wu, M. Schuster, Z. Chen, Q. Le, M. Norouzi, W. Macherey,
M. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah,
M. Johnson, X. Liu, . Kaiser, S. Gouws, Y. Kato, T. Kudo,
H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young,
J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado,
M. Hughes, and J. Dean, “Google’s neural machine translation
system: Bridging the gap between human and machine translation,” arXiv preprint arXiv:1609.08144, 2016.

5. References

[21] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of rare words with subword units,” in ACL, 2016.

[1] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. Weiss, N. Jaitly,
Z. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. Le, Y. Agiomyrgiannakis, R. Clark, and R. Saurous, “Tacotron: Towards end-to-end
speech synthesis,” in Interspeech, 2017.
[2] S. Ö. Arik, M. Chrzanowski, A. Coates, G. Diamos, A. Gibiansky,
Y. Kang, X. Li, J. Miller, A. Ng, J. Raiman, S. Sengupta, and
M. Shoeybi, “Deep voice: Real-time neural text-to-speech,” in
ICML, 2017.
[3] J. Sotelo, S. Mehri, K. Kumar, J. F. Santos, K. Kastner,
A. Courville, and Y. Bengio, “Char2wav: End-to-end speech synthesis,” in ICLR Workshop, 2017.
[4] J. Shen, R. Pang, R. Weiss, M. Schuster, N. Jaitly, Z. Yang,
Z. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan, R. Saurous,
Y. Agiomyrgiannakis, and Y. Wu, “Natural TTS synthesis by conditioning wavenet on mel spectrogram predictions,” in ICASSP,
2018.

[22] A. De Cheveigné and H. Kawahara, “YIN, a fundamental frequency estimator for speech and music,” The Journal of the
Acoustical Society of America, vol. 111, no. 4, pp. 1917–1930,
2002.
[23] R. Kubichek, “Mel-cepstral distance measure for objective speech
quality assessment,” in PacRim, 1993.
[24] T. Nakatani, S. Amano, T. Irino, K. Ishizuka, and T. Kondo, “A
method for fundamental frequency estimation and voicing decision: Application to infant utterances recorded in real acoustical
environments,” Speech Communication, vol. 50, no. 3, pp. 203–
214, 2008.
[25] W. Chu and A. Alwan, “Reducing f0 frame error of f0 tracking
algorithms under noisy conditions with an unvoiced/voiced classification frontend,” in ICASSP, 2009.

[5] W. Ping, K. Peng, and J. Chen, “Clarinet: Parallel wave generation
in end-to-end text-to-speech,” in ICLR, 2019.

[26] W.-N. Hsu, Y. Zhang, R. Weiss, Y.-A. Chung, Y. Wang, Y. Wu,
and J. Glass, “Disentangling correlated speaker and noise for
speech synthesis via data augmentation and adversarial factorization,” in ICASSP, 2019.

[6] H. Zen, K. Tokuda, and A. Black, “Statistical parametric speech
synthesis,” Speech Communication, vol. 51, no. 11, pp. 1039–
1064, 2009.

[27] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised
autoregressive model for speech representation learning,” arXiv
preprint arXiv:1904.03240, 2019.

[7] P. Taylor, Text-to-speech synthesis. Cambridge University Press,
2009.
[8] N. Li, S. Liu, Y. Liu, S. Zhao, M. Liu, and M. Zhou, “Neural
speech synthesis with transformer network,” in AAAI, 2019.
[9] K. Ito, “The LJ speech
LJ-Speech-Dataset/, 2017.

dataset,”

https://keithito.com/

[10] M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,
and L. Zettlemoyer, “Deep contextualized word representations,”
in NAACL-HLT, 2018.
[11] J. Howard and S. Ruder, “Universal language model fine-tuning
for text classification,” in ACL, 2018.
[12] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language understanding by generative pre-training,” OpenAI, Tech. Rep., 2018.
[13] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pretraining of deep bidirectional transformers for language understanding,” in NAACL-HLT, 2019.
[14] I. Sutskever, O. Vinyals, and Q. Le, “Sequence to sequence learning with neural networks,” in NIPS, 2014.
[15] J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio,
“Attention-based models for speech recognition,” in NIPS, 2015.
[16] Y.-A. Chung, Y. Wang, W.-N. Hsu, Y. Zhang, and R. Skerry-Ryan,
“Semi-supervised training for improving data efficiency in end-toend speech synthesis,” in ICASSP, 2019.
[17] H. Ming, L. He, H. Guo, and F. Soong, “Feature reinforcement
with word embedding and parsing information in neural TTS,”
arXiv preprint arXiv:1901.00707, 2019.
[18] R. Prenger, R. Valle, and B. Catanzaro, “Waveglow: A flow-based
generative network for speech synthesis,” in ICASSP, 2019.
[19] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you
need,” in NIPS, 2017.

