A Grid Based Approach to Analysing Spatial
Weighting Matrix Specification
Charles Rahal‡
‡

Leverhulme Centre for Demographic Science, University of Oxford, OX1 3UQ

E-mail: charles.rahal@sociology.ox.ac.uk
8th October, 2017
Abstract:

We outline a grid-based approach to provide further evidence against the misconception that the results of spatial econometric models are sensitive to the exact specification of
the exogenously set weighting matrix (otherwise known as the ‘biggest myth in spatial econometrics’). Our application estimates three large sets of specifications using an original dataset which
contains information on the Prime Central London housing market. We show that while posterior
model probabilities may indicate a strong preference for an extremely small number of models,
and while the spatial autocorrelation parameter varies substantially, median direct effects remain
stable across the entire permissible spatial weighting matrix space. We argue that spatial econometric models should be estimated across this entire space, as opposed to the current convention
of merely estimating a cursory number of points for robustness.
Keywords:

Spatial Econometrics, Real Estate Economics, Model Selection, Data Visualisation

1. INTRODUCTION
The most frequently asked question when discussing spatial econometric models regards the sensitivity of the results to the specification of the spatial weighting matrix (W ). A recent introduction
to a special issue of Papers in Regional Science summarizes this succinctly, stating: ‘Critics of spatial econometrics almost always in our experience home in on the arbitrary nature of the weights’
(Arbia and Fingleton, 2008, p.316). This misconception that spatial regression models are sensitive to W is pervasive and may have arisen from previous research incorrectly interpreting model
coefficients as if they were partial derivatives, or from the use of misspecified models (even in cases
where the spatial weighting matrix will not theoretically change coefficient point estimates). The
perceived issue of sensitivity is highlighted directly within a housing context in an early influential article which claims: ‘There does not appear to be any consensus regarding which scheme
represents the best realization of the correlation structure appearing in the housing market. This
is problematic because all of the results are conditional on the researcher’s a priori specification
of the spatial structure’ (Dubin, 1998, p.88). LeSage and Pace (2014) convincingly conclude that
‘the notion that estimates and inferences are sensitive to use of a particular weight matrix as perhaps the biggest myth about spatial regression models’ [p.5]. Amongst others, they attack a well
cited study (Bell and Bockstael (2000)) which claims that their results are more sensitive to the
specification of the spatial weight matrix than to estimation technique. However, for a correctly
specified Spatial Error Model (SEM – the type used in that paper), changes in W only change the
measures of dispersion: not the point estimates. More generally, methods for specifying W have
been discussed extensively, dating back to as early as Kooijman (1976) who proposed choices based
on maximizing Moran’s coefficient. Much work has since followed aiming to reduce the degree of
arbitrariness in the choice of W , and LeSage and Parent (2007) develop a Bayesian model averaging procedure (using posterior model probabilities as weights) to incorporate uncertainty. Rather

2

than picking (or ‘boosting’, as per Kostov (2010)) a certain specification, we argue for visualising
the ‘permissible set’ of W .
Spatial econometric modeling with real estate sales data provides a particularly interesting
challenge due to the need to incorporate the uni-directional nature of time (as opposed to the multidirectional nature of spatial autocorrelation). Within this branch of literature, spatial econometric
models have been frequently deployed and often report one or two variants of a spatial model based
upon competing weighting matrix specifications. Examples include Can (1990), who compares two
weighting specifications (‘inverse distance’ and ‘inverse distance squared’), with an additional two
which are limited to distance cut-offs, and Pace et al. (1998) who claim distance based approaches
‘subsume two basic paradigms of real estate work’ [p.8]. Wilhelmsson (2002) considers four different
variants, again within a housing context. Can and Megbolugbe (1997) identify and utilize only
(recent) ‘comparable sales’: properties within an arbitrarily pre-specified distance which sold within
an arbitrarily determined ‘a priori’ time period. A number of related papers explicitly incorporate
the temporal distance between sales into the weighting matrix itself (Thanos et al. (2016); Dubé
and Legros (2014, 2013)), and this is the approach which we follow within this paper for two of
our three sets of models.
The purpose of this paper is twofold. The first objective is to outline a grid based approach
which can provide visualisations of sets of model outcomes. This allows a significantly more robust
analysis contingent on a large set of inputs which we term the ‘permissible set’ of specifications,
made possible by recent improvements in computer processing power. The second is to provide
further evidence against the myth that small changes in the specification of W can dramatically
effect modeling outcomes in an application which utilizes a new dataset for ‘Prime Central London’
(PCL) real estate. Section 2 introduces the dataset and Section 3 discusses the grid based approach.
We visualise and analyse the outputs in Section 4 before concluding in Section 5.
2. DATASET: PRIME CENTRAL LONDON HOUSING MARKET
We use information on a large number of real estate sales in PCL exclusively from postcode
areas SW1, SW3, SW7, W1, W2, W8, W11 and W14. The data is collected from a subscription
based archive with contributions from individual realtors which is then manually parsed into a
tabular database. It contains a large number of structural characteristics in order to avoid any
potential misspecification of the independent variables: a problem which frequently plagues hedonic
regressions of this sort. It contains the type of property (terraced, apartment, etc.), number of
bedrooms, bathrooms, shower rooms, reception rooms, the condition and whether the property
has a garage, (communal) garden, patio or a terrace. Importantly, it contains the ‘gross floor area’
and the date of sale and a postcode which we subsequently geocode into latitude and longitude
using the Googlemaps API. We clean the dataset in accordance with the existing hedonic literature
and trim it to contain only sales between 2000-2015, dropping observations in the lower and upper
2.5 percentiles of the sale price. This leaves 4,731 observations.1

1 The supplementary material (available at github.com/crahal) also contains an example of our approach applied
to the infamous Boston housing dataset of Harrison and Rubinfeld (1978), further developing Section 3.3 of LeSage
and Pace (2014).

3
Figure 1:

Weighting Matrix Sparsity

α,δ
α,δ
(a) Sparsity of WA
and WB
Specifications

(b) Sparsity of WCα,δ Specifications

3. METHODOLOGY
As the simplest exposition of our grid based approach, we utilize the popular Spatial Autoregression
(SAR) model which takes the form:
y = θιn + λW y + Xβ + ε

(3.1)

with ε ∼ N (0, σ 2 I n ). In this deliberately simple setup, y is an N × 1 vector of sales prices and X is
an N × k matrix of observations on the explanatory variables (as per Section 2). β is a k × 1 vector
of coefficients associated with X, and ιn an N × 1 vector of constants. The N disturbances (ε) are
distributed normally with constant variance (σ 2 ) and zero covariance across observations, and λ
represents a scalar spatial dependence parametre (typically in the interval [0,1]). While we focus
on PCL house prices within a SAR model, this approach applies equally to any spatial econometric
approach where W is exogenously specified. We augment X with quarterly dummies in order to
account for the upward trend in the PCL market in our sample period.
We examine three sets of matrices which expand the ideology of LeSage and Pace (2014), with
the first two incorporating a temporal dimension. We first construct distance (D) and time (T)
matrices:




0
d1,2 . . . d1,j
0
t1,2 . . . t1,j




0
. . . d2,j 
0
. . . t2,j 
 d1,2
 −t1,2



D= .
T = .
(3.2)
.. 
.. 
..
..

.
.
.
.
 .
 .
...
...
. 
. 
d1,j

d2,j

...

0

−t1,j

−t2,j

...

0

where di,j is the Haversine distance (in kilometers) between i and j and ti,j = SaleMonthi SaleMonthj is the temporal distance where SaleMonth refers to a categorical, numerical ordering of
the months in which properties were sold over time. By design, D is symmetric, and T is symmetric
with signs reversed. The first two sets of matrices (W α,δ
and W α,δ
A
B ) exploit the ‘Hadamard’

4
α,δ
product style of weighting matrix as per Thanos et al. (2016). For W α,δ
A , elements wi,j,A are
defined as:

α,δ
wi,j,A
=


1


α ×

 ti,j
1×



0,

if 0 < ti,j < t̄ and di,j < d¯ and i 6= j

1
dα
i,j

if 0 = ti,j and di,j < d¯ and i 6= j

1
,
dα
i,j

(3.3)

otherwise

and the traditional decay parameter (discussed below) is denoted α. Critically, δ denotes a
T
S
function of δm
and δm
, defined as the fraction of the maximum distance along either the time
T
S
or space dimension: for t̄ and d¯ where t̄ = δm
× maxi∈{1,...,N },j∈{1,...,N } {ti,j } and d¯ = δm
×
α,δ
2,3
maxi∈{1,...,N },j∈{1,...,N } {di,j }.
The second matrix, denoted as W B , applies inverse distance
weights (as opposed to inverse distance and time as per W α,δ
A ) and represents a more traditional spatial weighting specification, merely restricted to observations which have already occurred through the natural passage of time. What is of critical importance to re-emphasize at this
α,δ
point is that if one observation happens before another (ti,j < 0), then the element wi,j,A
and
α,δ
wi,j,B is set to zero, highlighting the uni-directional time dimension under examination:

α,δ
wi,j,B
=




if 0 ≤ ti,j < t̄ and di,j < d¯ and i 6= j

1
dα
i,j

0,

(3.4)

otherwise

In both specifications, observations at period ti,j = 0 might be thought of as ‘under offer’ or
‘subject to completion’ within a real-estate context. For our first two specification sets, we estimate
across a grid of size Ω×Ω where each location on the grid contains a structure of estimation results
for a specific weighting matrix specification related to a specific cut-off (d¯ and t̄ as defined above).
S
In our application, our grid contains estimates based on a range of cut-offs contingent on δm
and
m×(δ
−δ
)
U
L
T
) for m = {1, ..., Ω}. In the empirical application which follows, we set
δm across (δL +
Ω
the lower and upper bounds respectively as δL = 0.1 and δU = 0.5 and our grid to contain 400
S
T
points (Ω=20). That is, for the first two methods, we vary the space (δm
) and time (δm
) cut-offs
simulatenously twenty times each, moving linearly between each minimum (δL ) and maximum
(δU ) cuttoff to create 400 variations in total. The third of the three specification sets (to align with
S
papers such as LeSage and Pace (2014) and Kostov (2010)) can be expressed as W α,δ
– where
C
S

S
we vary δm
evenly across m = {1, ..., 400} to create 400 versions of W α,δ
over increasing inverse
C
¯ This represents a more conventional spatial (as opposed to spatio-temporal)
distance cut-offs (d).
weighting matrix where each element is constructed as:

α,δ S
wi,j,C

=




1
dα
i,j

0,

if di,j < d¯ and i 6= j

(3.5)

otherwise

and allows a bi-directional temporal mechanism with no cut-off based on the date of sale.
2 In our PCL example, these maximum elements for D and T correspond to 9.904 km and 191 months respectively
(sixteen years of data across twelve months per year).
3 Conversely, the α subscript represents the matrix elements determined by a specified decay function.

5
Figure 2:

Evaluating Model Fit
S

(a) Log Likelihoods (W α,δ
A )

(b) Log Likelihoods (W α,δ
B )

(c) Log Likelihoods (W α,δ
)
C

(d) Posterior Probabilities (W α,δ
A )

(e) Posterior Probabilities (W α,δ
B )

(f) Posterior Probabilities (W α,δ
C )

Within our application to PCL data, these three specification sets (visualised as a ‘grid’ of results
across estimations within a SAR model as per Equation 3.1) result in sparsity (the number of
non-zero elements in the weighting matrices) structures as shown in Figure 1.4 The inclusion of
the temporal dimension in the first two specification sets significantly increases the number of
zero elements when compared to the spatial specification, and goes some way to addressing the
problem of over-connectivity bias. For all three specification sets discussed above, we then re-run
the analysis fixing t̄ = d¯ = 0.5, but now instead varying α linearly between 0.41 and 4.4 (inclusive)
in increments of 0.01, whereas in the first set of variations across space and time (i.e. ‘δ space’), α
is fixed to 1. These variations represent what might be thought of as the ‘permissible space’ of all
specifications.
4. ANALYSIS
We aim to provide further empirical evidence on whether small changes – either through ‘boosting’ based algorithms or accidental mis-specification – provide substantial differences in effects
estimates. We first plot the histograms and kernel density estimates of the distributions of individually maximized log-likelihood values from each weight matrix within each of our three specification
sets (Figures 2a-2c) for variations across space and time cut-offs, where each observation represents
a single one of the 400 hundred models in each specification set. We then plot a direct (Bayesian)
test for model comparison based on (LeSage and Pace, 2009, p. 175-178) in Figures 2e-2f, where

4 Note that schemes W α,δ and W α,δ necessarily result in the same sparsity structures but with different repreA
B
sentative weights.

6
Figure 3:

Estimation Results Across δ Space
S

(a) λ̂ Estimates (W α,δ
A )

(b) λ̂ Estimates (W α,δ
B )

(c) λ̂ Estimates (W α,δ
)
C

(d) Direct: GFA (W α,δ
A )

(e) Direct: GFA (W α,δ
B )

(f) Direct: GFA (W α,δ
)
C

(g) Direct: Detached (W α,δ
A )

(h) Direct: Detached (W α,δ
B )

)
(i) Direct: Detached (W α,δ
C

S

S

each element on the grid represents a single combination of space (δ S ) and time (δ T ) cut-off.5 We
observe substantial variation in log likelihoods and virtually no posterior probability support for all
but a small number of models.6 Given such strong support for such a small number of models, we
might expect that estimates and inferences might be sensitive to an incorrect choice of weighting
matrix specification.
We observe a relatively large variation in estimates for the spatial dependence parameter (λ̂) in
response to changes in the spatial weighting matrix specification (Figures 3a to 3c). This ranges
α,δ
α,δ S
from 0.0397 to 0.4740 for W α,δ
.
A , 0.0326 to 0.7890 for W B , and 0.4740 to 0.6690 for W C
However, as shown in LeSage and Pace (2014), it is the effects estimates which are relevant, not
5 Each distance iteration is equivalent to approximately 208.5m and each time iteration equal to 4.02 months for
α,δ
α,δ
W α,δ
A and W B . For W C , each distance iteration is approximately 10m.
6 An unreported figure which compares all 1200 models directly shows that one model (from set W α,δ ) obtains a
B
posterior model probability of 0.82 alone.

7
Figure 4:

Estimation Results Across α space

(a) Posterior Probabilities

(b) λ̂ Estimates

(c) Direct Effects: GFA

(d) Direct Effects: Detached

the coefficient estimates: the β̂ and λ̂ adjust in response to changes in the weighting specification
in an effort to produce consistent effects estimates. In contrast to the λ̂ estimates in Figures 3d
to 3i, the direct effects estimates (shown here for the logarithm of Gross Floor Area (GFA) and
the ‘detached’ dummy) across all 1200 specifications are remarkably similar.7 Where the results
in Kostov (2010) are used to suggest the need to ‘fine-tune’ weighting matrix specifications, our
results (consistent across direct and indirect effects estimates for other variables) show a resilience
and robustness across large sets of spatial weighting specifications in our grid of time and space cutoff points. Figure 5 shows the variation across decay parameter with all three of our specification
sets now overlaid on top of one another. We again observe the same characteristics in the results:
that while only a small number of models have essentially non-zero posterior model probabilities
(Figure 4a), with λ̂ varying substantially (Figure 4b), there is relatively little variation in the direct
effects estimates (Figures 4c-4d) other than for a small number of models in the W α,δ
B scheme.
5. CONCLUSION
We provide further evidence against the ‘biggest myth in spatial econometrics’ by estimating large
sets of different weighting matrix specifications in a new application to modeling PCL house prices.
We provide a methodology for visualising large sets of specifications simultaneously, showing that
despite the models showing a distinct preference (through log likelihoods and posterior probabilities), direct effects remain largely unchanged. We encourage future researchers to estimate a grid
of specifications in order to consider the distribution of parameters across a permissible spatial
7 We use the median of the effects estimates since the effects can have a non-symmetric distribution.

8

weighting matrix specification space as opposed to estimating a small set of robustness checks in
their applications.
ACKNOWLEDGMENTS
Thanks are due to Anindya Banerjee, Alexandros Rigos, Stephen Hall, William Pouliot, Marco
Barassi, John Fender, Rob Elliot and seminar participants at the University of Birmingham. We
gratefully acknowledge James Wyatt for providing access to the dataset.
REFERENCES
Arbia, G. and B. Fingleton (2008). New spatial econometric techniques and applications in regional
science. Papers in Regional Science 87 (3), 311–317.
Bell, K. P. and N. E. Bockstael (2000). Applying the generalized-moments estimation approach
to spatial problems involving microlevel data. The Review of Economics and Statistics 82 (1),
72–82.
Can, A. (1990). The measurement of neighborhood dynamics in urban house prices. Economic
Geography 66 (3), 254–272.
Can, A. and I. Megbolugbe (1997). Spatial dependence and house price index construction. The
Journal of Real Estate Finance and Economics 14 (1), 203–222.
Dubé, J. and D. Legros (2013). Dealing with spatial data pooled over time in statistical models.
Letters in Spatial and Resource Sciences 6 (1), 1–18.
Dubé, J. and D. Legros (2014). Spatial econometrics and the hedonic pricing model: what about
the temporal dimension? Journal of Property Research 31 (4), 333–359.
Dubin, R. A. (1998). Spatial autocorrelation: A primer. Journal of Housing Economics 7 (4), 304
– 327.
Harrison, D. and D. L. Rubinfeld (1978). Hedonic housing prices and the demand for clean air.
Journal of Environmental Economics and Management 5 (1), 81–102.
Kooijman, S. A. L. M. (1976). Some Remarks on the Statistical Analysis of Grids Especially with
Respect to Ecology, pp. 113–132. Boston, MA: Springer US.
Kostov, P. (2010). Model boosting for spatial weighting matrix selection in spatial lag models.
Environment and Planning B: Planning and Design 37 (3), 533–549.
LeSage, J. and R. K. Pace (2009). Introduction to spatial econometrics. Taylor and Francis Boca
Raton.
LeSage, J. and R. K. Pace (2014). The biggest myth in spatial econometrics. Econometrics 2 (4),
1–33.
LeSage, J. P. and O. Parent (2007). Bayesian model averaging for spatial econometric models.
Geographical Analysis 39 (3), 241–267.
Pace, R. K., R. Barry, and C. Sirmans (1998). Spatial statistics and real estate. The Journal of
Real Estate Finance and Economics 17 (1), 5–13.
Pace, R. K. and O. Gilley (1997). Using the spatial configuration of the data to improve estimation.
The Journal of Real Estate Finance and Economics 14 (3), 333–340.
Thanos, S., J. Dubé, and D. Legros (2016). Putting time into space: the temporal coherence of
spatial applications in the housing market. Regional Science and Urban Economics 58, 78 – 88.
Wilhelmsson, M. (2002). Spatial models in real estate economics. Housing, Theory and Society 19 (2), 92–101.

9

SUPPLEMENTARY: A GRID BASED APPROACH TO THE BOSTON DATASET

Figure 5:

A Grid Based Approach to the Harrison and Rubinfeld (1978) Boston Dataset

(a) Posterior Model Probabilities

(c) Log Likelihoods

(b) Direct Effects: Crime)

(d) λ̂ Estimates

We emphasize the viability of this approach by estimating a simple set of models across a grid
comprised of nearest neighbor and alpha variation using a popular, open-source dataset. This
can be seen as a development of Section 3.3 of LeSage and Pace (2014) and Kostov (2010), both
based on the Pace and Gilley (1997) amendments which augmented the infamous Harrison and
Rubinfeld (1978) data with spatial co-ordinates. We follow the ten-variable specification of LeSage
and Pace (2014), and transform the variables accordingly. We then follow their weighting scheme
1
Wi,j = d(i,j)
α , where d(i, j)m denotes the distance between the m nearest neighboring observations
m
between j and i and α is the decay parameter. We vary the grid between 1-20 nearest neighbors
and across 20 alpha intervals between 0.4-4 to show that despite a strong posterior preference for
a small number of models, a high variance in the log-likelihoods and the λ parameter, the direct
effects (Crime) stay relatively constant between -0.0119 and -0.0079.

