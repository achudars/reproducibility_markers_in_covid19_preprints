Enhancing and accelerating social science via
automation: Challenges and opportunities
Tal Yarkoni1* , Dean Eckles2 , James A. J. Heathers3 , Margaret C. Levenstein4 , Paul E.
Smaldino5 , and Julia I. Lane6
1

Department of Psychology, University of Texas at Austin
Sloan School of Management, Massachusetts Institute of Technology
3
Bouvé College of Health Sciences, Northeastern University
4
Inter-university Consortium for Political and Social Research, University of Michigan
5
Department of Cognitive and Information Sciences, University of California, Merced
6
Wagner School of Public Policy, New York University
*
Corresponding author: tyarkoni@gmail.com
2

Automation plays an increasingly important role in science, but the social sciences have been comparatively slow to take
advantage of emerging technologies and methods. In this review, we argue that greater investment in automation would
be one of the most effective and cost-effective ways to boost the reliability, validity, and utility of social science research.
We identify five core areas ripe for potentially transformative investment, including (1) machine-readable standards, (2)
data access platforms, (3) search and discoverability, (4) claim validation, and (5) insight generation. In each case, we review
limitations associated with current practices, identify concrete opportunities for improvement via automation, and discuss
near-term barriers to progress. We conclude with a discussion of practical and ethical considerations researchers will need
to keep in mind when working to enhance and accelerate social science progress via automation.

Keywords: social science, automation, standardization, discoverability, data sharing, data access, validation
ing nature of work—and require robust social science
to understand the sources and consequences and inform policy-making. Yet the lack of reproducibility
and replicability evident in many fields (Camerer et
al., 2018; Dafoe, 2014; J. Ioannidis, 2008; J. P. A.
Ioannidis, 2005; Munafò et al., 2017; Open Science
Collaboration, 2015) is even more acute in the study
of human behavior?both because of the difficulty of
sharing confidential data and because of the lack of
scientific infrastructure. The central argument we
advance in this paper is that advances in technology—
and particularly, in automation—can now change the
way in which social science is done. Social scientists
have eagerly adopted new technologies in virtually
every area of research—from literature searches to
data storage to statistical analysis to dissemination
of results. We identify five core investments that can
transform our science:

Introduction
The social sciences are at a crossroads. The enormous growth of the scientific enterprise, coupled with
rapid technological progress, has created opportunities to conduct research at a scale that would have
been almost unimaginable a generation or two ago.
The rise of cheap computing, connected mobile devices, and social networks with global reach allows
researchers to rapidly acquire massive, rich datasets;
to routinely fit statistical models that would once
have seemed intractably complex; and to probe the
way that people think, feel, behave, and interact with
one another in ever more naturalistic, fine-grained
ways. Yet much of the core infrastructure is manual
and ad-hoc in nature, threatening the legitimacy and
utility of social science research.

We can and must do better. The great challenges
of our time are human in nature—terrorism, climate
1) Standardization: The data, results, and other
change, misuse of natural resources, and the chang- scientific outputs generated by social scientists are
1

represented in a bewildering array of formats and this likely to become increasingly salient as the scope of
lack of consistency limits their utility. Automation automation within the social sciences grows over time.
could be applied to develop standardized, machinereadable representations of social science objects.

Challenge 1: Standardization

2) Data access: Researchers who wish to share
Arguably the most pressing short-term barrier to
or use existing datasets routinely face a series of lelarge-scale application of automation in the social
gal, ethical, computational, and logistical barriers.
sciences is a lack of standards for representing data,
Automated tools, protocols, and infrastructure could
results, and other scientific outputs. Almost every aureduce these barriers,
tomated technology designed to improve social science
3) Search and discoverability: The vast majority research will at some point need to perform operations
of social science data and outputs cannot be easily on machine-readable scientific objects. For example,
discovered by other researchers even when nominally searching for datasets requires standardized query
deposited in the public domain. A new generation of languages and structured metadata; developing algoautomated search tools could help researchers discover rithms for automated detection of statistical errors
how data are being used, in what research fields, with requires standardized representations of statistical
what methods, with what code and with what findings. models and/or tests; and automatically synthesizing
And automation can be used to reward researchers the results of disparate findings meta-analytically is
who validate the results and contribute additional greatly facilitated by the availability of formal ontoloinformation about use, fields, methods, code, and gies or controlled vocabularies.
findings.
Historically, the social sciences have not placed
much emphasis on standardization. Many bespoke
data formats have been constructed that vary both
within and between fields; many fields have standardized on similar, but not always identical, default analysis strategies (e.g., ANOVAs in experimental psychology versus regression in economics). Each journal
structures papers in different ways, including differences in citation formats. Electronic copies of papers
are stored in different formats; even PDFs contain an
ad hoc mixture of genuine text and images of text. Imposing some order on the chaos of bespoke standards
and formats that currently pervades social science
would enable a wide range of automated applications
that have enormous potential to improve the quality
and reliability of social science research.

4) Claim validation: The social sciences study
complex phenomena, and the rate of inferential errors
made by researchers is correspondingly high—and
often costly. Automation could be applied to develop
error-detection systems that automatically identify,
and potentially even correct, errors that are timeconsuming or difficult for researchers to detect.
5) Insight generation: Our individual abilities
to generate new social science theories, predictions,
and study designs are subject to human cognitive
biases and limitations. Automated systems could help
generate and test important new insights.
These five challenges are neither independent of
one another nor mutually exhaustive; moreover, for
the sake of brevity, we largely avoid discussion of
technologies and practices that have already been
widely adopted by social scientists (e.g., online survey
or crowdsourcing platforms like SurveyMonkey and
Mechanical Turk). Collectively, however, they span
many of the most pressing problems and opportunities
facing the social sciences today. We discuss each core
challenge in turn, then conclude with a discussion of
several practical and ethical considerations that are

Developing machine-readable standards for the social sciences cannot hope to be a singular effort; different kinds of research objects demand fundamentally
different kinds of machine-readable representations.
To convey the scope of the problem and highlight the
enormous benefits conferred by potential solutions, we
review several areas that are ripe for increased standardization efforts. We deliberately focus on domaingeneral problems that apply throughout the social
2

sciences, recognizing of course that individual fields and manipulate data from a BIDS dataset; and to mawill face many other standardization challenges not jor online data sharing platforms like OpenNeuro.org
addressed here.
that will soon allow users to execute sophisticated
fully-reproducible, cloud-based analyses on uploaded
BIDS datasets (K. Gorgolewski, Esteban, Schaefer,
Data representation
Wandell, & Poldrack, 2017). Developing and promotPerhaps the most common standardization-related
ing corresponding field-wide standards in the social
challenge social scientists regularly face—often withsciences is likely to have similar benefits.
out realizing it—is data representation and organization. Many researchers fail to represent data in a
consistent way even from study to study, to say noth- Controlled vocabularies and ontologies
ing of the enormous variation found between labs or
In fields where many of the entities have been
fields. Most social scientists also rely heavily on proclearly
defined (e.g., molecules in chemistry, genomic
prietary data formats (e.g., Excel, SPSS, SAS) that
sequences
in genomics, etc.), machine-readable ontolocan impose serious barriers to data access. Converging
gies
are
often
relatively straightforward to develop.
on field-wide or even social science-wide conventions
Such
ontologies
delineate which terms or concepts are
for basic organization of data—including directory
valid
for
a
domain,
what those terms mean, and how
structure and file naming, file formats for metadata
they
relate
to
one
another;
they can greatly facilitate
representation, etc.—would have immense benefits. A
the
development
of
rich,
interoperable
tooling ecosysnumber of these benefits are illustrated by successtems.
In
the
social
sciences,
by
contrast,
there is
ful social science data-sharing repositories like the
often
no
clear
fact
of
the
matter
about
what
concepts
Inter-university Consortium for Political and Social
Research (ICPSR), which curates data to standards or variables mean, or how they should be used (e.g.,
it has promoted (in terms of file formats, metadata what does political polarization mean in the United
requirements, codebooks and documentation, etc., States, and is it reasonable to operationalize it in
(Inter-university Consortium for Political and Social terms of voting patterns in congressional elections?).
Research, 2009). Curating data to meet ICPSR stan- The absence of controlled ontologies impedes theory
dards requires significant effort. The large payoff is testing, model building, and data discovery and reuse.
that it is then much easier for other researchers to
Although some measure of linguistic ambiguity is
access, understand, and make use of one’s dataset
likely to be unavoidable in the social sciences given
(Levenstein & Lyle, 2018). Automating the process of
the complex subject matter, even modest ontologies
data curation, especially if automated curation were
that minimally control the vocabulary researchers use
built into the research process, could significantly
would have important benefits (Poldrack & Yarkoni,
reduce costs to researchers and repositories while in2016). For example, in psychology, codifying the
creasing data discovery and re-use.
knowledge that the concepts of Working Memory and
Beyond data sharing, the widespread adoption of a Executive Control are closely related would enable
data standard can have many other salutary effects. A semantic search engines to return datasets tagged
good example can be found in neuroimaging research, with either of the two labels even when a query string
where the recently-introduced Brain Imaging Data includes only one. More aspirationally, having a forStructure (BIDS) standard (K. J. Gorgolewski et al., mal ontology for country-level economic and political
2016) has attracted rapid adoption from tool develop- indicators might one day enable researchers to exeers and end users alike. BIDS has given rise to a thriv- cute sophisticated analytical queries like “estimate
ing ecosystem of self-contained “BIDS-Apps” that can the 10-year impact on GDP of switching from an aube readily applied to any BIDS-compliant dataset (K. tocratic or colonial system of government to a demoJ. Gorgolewski et al., 2017); to utility toolboxes like cratic one, for all African countries, for starting years
PyBIDS that make it much easier to retrieve, query, between 1945 and 1995.” Some degree of control over
3

terminology is an essential prerequisite for a huge pro- an existing effort along these lines is the SHARE iniportion of automation applications, including many tiative (share-research.org), which provides a unified
we discuss in later sections.
interface to humanities and social science resource
outputs aggregated from over 100 disparate sources.
Controlled vocabularies and ontologies already exist in some areas of social science—typically in fairly
Outlook
narrow domains (e.g., Library of Congress subject
Standardization is a prerequisite for almost every
headings in library science, or part-of-speech tagging
in linguistics; the National Library of Medicine’s Com- potential application of automation to social science
mon Data Elements cover some social and behavioral research. Consequently, standardization initiatives ofdomains). A number of efforts create standards that fer perhaps the most attractive target for short-term
span the full range of the social sciences; for example, resource investment if one’s goal is to catalyze developthe Data Documentation Initiative (DDI) provides a ment of a rich ecosystem of interoperable, automated
machine-actionable generic specification for describing social science applications. It helps that the barrier
surveys and other observational data, and is actively to entry is relatively low: the relative lack of formal
used by many statistical agencies and academic data standards in the social sciences means that even small
repositories (Vardigan, Heus, & Thomas, 2008). How- groups of motivated, skilled researchers currently have
ever, DDI was developed for observational data; there the potential to introduce new standards that could
is a need for extensions or alternative specifications ultimately end up shaping the trajectory of broad
that target or encompass other kinds of data. An open research domains. Leadership is critical here, as the
challenge for social scientists and informaticians is to proliferation of different standards could reinforce silos
develop abstractions that hide much of the complexity and undermine the transdisciplinary research critical
in specific sub-domains of social science in favor of to transformative social science.
standardized entities common to multiple sub-fields
Naturally, there are also a number of practical huror even entire disciplines.
dles. First, because the problem space is massively

heterogeneous, solutions will have to be correspondingly varied; this is particularly clear in the case of
controlled vocabularies, where the scope for social
science-wide standards is probably fairly limited (e.g.,
to basic statistical methods and key variable classes).
Second, when developing new standards, there is often
a strong tension between simplicity and comprehensiveness: while it is easier to learn and implement a
simple standard than a complex one, simple standards
necessary leave out many important use cases. Third,
diversity and innovation must be carefully balanced
against consistency and commensurability: while some
measure of competition between standards is desirable,
researchers should attempt to extend and improve on
existing standards before introducing new ones.

Harmonization across standards and fields

Our discussion so far may make it sound as though
the intended end goal for each of the above areas is a
single standard that researchers within a given field
agree to abide by. However, complete consensus is
neither likely nor desirable; there will undoubtedly
be competing technical solutions to each of the problems identified above. Rather than encouraging a
winner-take-all outcome, an efficient way to ensure
commensurability while encouraging innovation is to
develop meta-standards that support common access
to different standards. In software development, the
need to interoperate with multiple tools that have
different interfaces is often solved by introducing a
higher-level abstraction that exposes a uniform userChallenge 2: Access to data
facing interface while internally querying multiple
services as needed. The same kind of approach will
A second set of challenges routinely faced by relikely be necessary to reconcile disparate standards searchers in all areas of social science concerns acand resources in the social sciences. One example of cess to data on human subjects—or, rather, the lack
4

thereof. The increasing use of administrative records
for social science research is hampered by “the current
legal environment for data collection, protection, and
sharing lacks consistency, leading to confusion and
inefficiency among departments, external researchers,
and other members of the evidence-building community” and “formal data access agreements (e.g., Memoranda of Understanding or MOUs) between two or
more agencies can take years to develop” (Commission
on Evidence-Based Policymaking, 2017). Many Institutional Review Boards (IRBs) are focused primarily
on evaluation of high-risk clinical trials and invasive
biomedical studies, and lack expertise in lower-risk
social science research—often leading to unnecessary
restrictions on data access and data sharing (National
Research Council et al., 2014).

compliance requirements (GDPR, HIPAA, FERPA,
FedRAMP, Title 13, Title 26, SOX, GLBA, etc.). On
the technical side, complex and costly computational
infrastructure must be built and maintained. All of
this is compounded further when research questions involve sensitive datasets from multiple providers, each
subject to their own constraints. Few organizations
with research capacity are presently able to tackle
these legal, technical, and logistical barriers.
The main problems organizations face can be summarized as ensuring that the five “safes” are addressed:
safe people, working on safe projects, in safe settings,
safe (de-identified) data and safe (disclosure proofed)
outputs are released[1] . The core challenge is thus
to develop automated systems that facilitate much
wider access to data in the face of legal, ethical, logistical, and technical barriers, while simultaneously
respecting those barriers when they are set up for
socially important reasons. We consider a number
of representative applications here, including infrastructure for data dissemination and access control,
platforms for automated, adaptive training, and tools
for automated data extraction.

As a result, the rapid rate of novel data generation in social science is undermined by low rates
of data re-use (Wallis, Rolando, & Borgman, 2013).
For example, nearly half of articles published in the
flagship economics journal—the American Economic
Review—currently receive an exemption from the journal’s mandatory data sharing policy (Christensen &
Miguel, 2018), making data re-analysis effectively impossible. Although massive investments have enabled
the sharing and reuse of open data, the hundreds
of millions of dollars statistical agencies, universities,
research organizations, and philanthropic foundations
have spent to make better use of sensitive data have
had limited impact on social science research. The reasons are well understood. As noted above, there are
fundamental structural problems: the startup costs to
access and use confidential data are daunting, and the
rewards to individuals are too low. In prosaic terms,
the data plumbing needs to be installed before the evidence house is built, and an investment in plumbing
has been lacking.

Data infrastructure
The nature of research using data on human subjects has fundamentally changed as the surge of new
types of data have become available, but infrastructure for researchers in fields dealing with those data
has not. New types of data management and programming capabilities are needed, as are new ways of
summarizing, describing, and analyzing massive and
unstructured data sets (Einav & Levin, 2014; Lane,
2016). Such advances revolutionized much of science
ranging from life sciences to astrophysics. This did not
happen by chance, but as a result of careful deliberate
investment in infrastructure and agreement on data
sharing and management principles—the Bermuda
Accord in the case of the Human Genome Project
(Collins, Morgan, & Patrinos, 2003) and the Sloan
Digital Sky Survey in the case of astrophysics (York,
Adelman, & Anderson, 2000). Similar major investment should be made in social science disciplines if
research is to advance on core human issues like child

Given constraints on resources, organizations wanting to promote better access to data will want to
focus those resources on data sources with the widest
and most valuable research utility. Unfortunately,
organizations often don’t know, and can’t report on,
who is accessing what data, in what settings, for what
purposes and for what results. On the legal side,
organizations face a range of security, auditing and
5

health and learning, the results of investment in research, and the health and well-being of cities. That
investment should include a focus on both technical
and workforce capacity.

the private sector—who need to understand how to use
data science tools as part of their regular employment.
It will be important to train these students to use
new models and tools while grounding the approach
in fundamental statistical concepts like population
frames, sampling, and valid inference. This will require combining foundational social science research
principles with current analytic and computing skills,
ideally grounded in the study of real world social and
economic problems.

The data revolution has not only produced massive
new types of data on human subjects, but has also
changed the way datasets are typically collected and
disseminated (Jarmin & O’Hara, 2016; Lane, 2016),
as well as the skills needed to access and use them.
Researchers using data on human subjects need to collect, manage and use diverse data in new ways. There
are many technical challenges that can be automated
to reduce costs and promote transparent data stewardship. Automation could speed up and standardize the
way in which disparate datasets can be ingested, their
provenance determined and metadata documented.
Automation could ensure that researchers can easily
query datasets to know what data are available and
how they can be used. Automation could similarly
ensure that the workflows associated with data management and linkage can be traced and replicated.
Automated process could track access and use so that
data stewards could be sure that the bureaucratic and
legal issues are addressed. Finally, automated rather
than ad-hoc procedures could be instituted to ensure
that the data are securely housed, and privacy and
confidentiality protected.

The last few years have seen a growing response
to this need. Many universities have degree and/or
certification programs dedicated to better training in
computational methods in the social sciences.[2] Moreover, there are increasing numbers of short courses
on computational methods for both social scientists
and industry professionals, including Data Carpentry,
Software Carpentry, and the Graduate Workshop in
Computational Social Science at the Santa Fe Institute. Such programs are highly promising, and in
our view represent the future of training for quantitative social science, but the methods they promote are
still underrepresented in most social science training
programs.
Delivering training programs at scale requires automation as well. While online classes have been
developed to use open data, there are only limited
in-person training sessions for confidential micro data
on human subjects (integrated online and in-person
training with confidential micro data are described in
Weinberg et al., 2017). Levenstein et al (2018) document the lack of consensus within the data community
about what such training should include.

Training platforms
Training can also be delivered in a less ad-hoc manner. Training is critical. While the new types of data
have enormous appeal in their potential to describe
economic and social activity, the data science and numerical computing skills needed to take advantage of
such data are still taught predominantly in computer
science or statistics departments. As a result, while
there are many courses in machine learning, database
management, or search optimization, the content of
those curricula are typically not focused on how to
use these skills to contribute to the scientific analysis
of social problems. In addition, most such programs
are targeted at traditional graduate students, and
are part of a continuous course of study. There is
a great need for graduate training for all kinds of
students—including those in government agencies and

Automated data extraction
The digitization of most economic transactions
has fundamentally changed the kind of information
available on individuals, households, and businesses.
Passively-generated records of day-to-day transactions
and interpersonal interactions offer the opportunity
to improve the measurement of key social and economic indicators while drastically reducing the effort
required to acquire such data. In addition, declining response rates and increasing costs of household
6

and business surveys provide additional incentives
to explore new source data for economic and social
measurement. The current use of commercial organizations’ data for social science research is often ad
hoc; a framework is needed to move from these ad
hoc arrangements to a systematic, institutionalized
approach. Successful automation of the extraction
of economic and social content from transaction and
other non-designed data has the potential to transform social science research in multiple arenas by
creating much more frequent and timely data—“nowcasting” (Antenucci, Cafarella, & Levenstein, 2013;
Giannone, Reichlin, & Small, 2008)—of social and
economic phenomena that are now only observable
with long lags.

basis for building a national framework for data access
using modern automated technologies.

Challenge 3: Search & discoverability
Good scholarship involves effective use of existing
data, results, and ideas; it requires the ability to
dredge the vast sea of publicly available research objects for the small subset that one deems both relevant
and reliable enough to build upon in one’s own work.
Unfortunately, this is often quite challenging. Scientific research is overwhelmingly reported in journal
articles and conference proceedings that have access
restrictions and insufficient metadata for reliable semantic search; consequently, researchers who fail to
use exactly the right search terms can easily miss
out on entire relevant literatures. Scholarly search
engines like Google Scholar regularly miscategorize
journal article and book chapters—and don’t indicate
what data were used in the articles and chapters, or
where it can be found. Papers reporting results that
have failed to replicate or even undergone retraction
continue to be cited for years (Bar-Ilan & Halevi,
2018; Gewin, 2014; Greenberg, 2009; Korpela, 2010),
largely because published replications are not readily
linked to the original research and to one another. Formal machine-readable connections between research
findings are practically nonexistent. And even when
researchers manage to overcome such hurdles and successfully identify relevant potential resources, they
then typically still have to determine whether or not
they should trust those resources—a task that often
demands a breadth and depth of expertise that few
individual researchers possess.

Automating extraction of this social and economic
content requires collaboration between data and social
scientists, particularly given the enormity of the crosssectional data (millions or billions of transactions at
any point in time), and as contrasted with the sparsity
of the time series data relative to social time (e.g.,
time series of Uber trips are very dense over the span
of time that Uber has existed, but very short relative
to the business cycle or the pace of technological
change in automobiles). This requires models that
integrate data analysis with domain specific knowledge
to leverage the available data for robust social and
economic measurement. It also requires building
collaborative tools that facilitate these synergies.

Outlook

There is a general recognition that it is important
to build infrastructures to improve access to social
Vannevar Bush foreshadowed the issue more than
science data and promote collaborative analysis. In60
years ago:
deed, Card et al (2010) call for competition among
federal agencies to provide secure access and reward
performance. A new opportunity has arisen with the
“There is a growing mountain of research.
call for a National Secure Data Service (Commission
But there is increased evidence that we are
on Evidence-Based Policymaking, 2017), and the first
being bogged down today as specialization
steps have been implemented with the passage of the
extends. The investigator is staggered by
Foundations for Evidence-Based Policymaking Act of
the findings and conclusions of thousands of
2018. Collaboration between statistical agencies and
other workers—conclusions which he cannot
the broader research community is necessary to reap
find time to grasp, much less to remember,
the potential benefits of these steps and create the
as they appear. … Mendel’s concept of the
7

laws of genetics was lost to the world for a
generation because his publication did not
reach the few who were capable of grasping
and extending it; and this sort of catastrophe
is undoubtedly being repeated all about us,
as truly significant attainments become lost
in the mass of the inconsequential”(Bush &
Others, 1945).

Jupyter; jupyter.org) that facilitate integrated presentation of data, analysis code, results, figures, and
text has made it considerably easier for authors to
construct, share, and interact with reproducible analysis workflows (Kluyver et al., 2016; Shen, 2014).
Containerization technologies like Docker take reproducibility even further by ensuring that all software
and data dependencies can be bundled into a single
self-encapsulated container that can (in principle) be
executed on most major platforms (Boettiger, 2015).
Building communally-adopted standards directly into
containerized applications could, in principle, give rise
to entire ecosystems of containerized, interoperable
applications that annotate and curate social science resources with minimal effort on the part of researchers
(cf. efforts like Galaxy and BIDS in other fields; Afgan
et al., 2018; K. J. Gorgolewski et al., 2017).

This challenge is particularly true for empirical research. Faced with a never-ending stream of new findings and datasets generated using different code and
analytical techniques, researchers cannot readily determine who has worked in an area before, what methods were used, what was produced, and where those
products can be found. Resolving such uncertainties
consumes an enormous amount of time and energy for
many social scientists. Automated tools and services
could greatly facilitate the process—often by passively capitalizing on the accumulated labor of one’s
extended research community. We break the problem
up into 3 separate sub-challenges focused, respectively,
on (1) fully-reproducible, standards-compliant analysis workflows that generate comprehensive metadata
at the same time as the primary scientific objects; (2)
machine learning tools for large-scale, efficient annotation and retrieval of existing research objects; and
(3) collaborative filtering tools that can help evaluate,
curate and synthesize the enormous emergent research
graph.

An admitted limitation of the above vision is that,
at present, many social scientists lack the technical
skills needed to take full advantage of existing (mostly
programmatically-driven) technologies. While we believe the ideal long-term solution to this problem is for
social science graduate programs to greatly increase
emphasis on scientific computing skills, a complementary and perhaps more realistic near-term approach
is to focus on designing platforms that lower barriers
to usage by hiding the full complexity of a modern
analysis workflow behind simple command-line tools
and intuitive graphical interfaces. For example, developing free web-based analogs of widely-used statistical packages like SPSS and SAS would enable passive annotation and curation of datasets and results
with little or no additional effort on the part of users
(though access to the generated resources would, of
course, remain under users’ control). An incipient but
illustrative example can be found in the Coleridge Initiative (coleridgeinitiative.org), which seeks to create
a highly contextual, engaging computational platform
for researchers working with sensitive datasets. The
long-term goal is to build user interfaces that present
rich context to users about the datasets as they work
in secure environments, while also incentivizing users
to participate in the creations of additional rich context. These interfaces act to gather metadata that
provide information about who else has used the data,

Reproducible workflows
Common standards of the kind described in Challenge 1 are a minimal prerequisite for constructing
massive, machine-readable research graphs that formally link datasets, papers, analyses, and ideas, but
they are by no means sufficient. Even if a consensus existed about what standards should be used to
annotate research objects, we would still need tools
and platforms that can facilitate their actual application to the research literature. Part of this problem
is arguably already capably addressed by existing
technologies that dramatically improve scientific reproducibility and provenance tracking. For example,
the rapid uptake of interactive notebooks (e.g. Project
8

for what purpose, and how others users have accessed
and analyzed data in their research work.

scholarship. The need to encourage prosocial participation in a community structure is a widespread feature of collective action problems (Nyborg et al., 2016;
Ostrom, 1990; Poteete, Janssen, & Ostrom, 2010). In
many domains, people have tackled this challenge by
building collaborative online platforms that encourage users to both generate and curate an enormous
amount of high-quality content. Examples range from
domain-specific expert communities like Stack Overflow and TripAdvisor—where answering other users’
questions is the explicit purpose of the platform—to
general infrastructure platforms like GitHub that have
other primary goals (e.g., to support open-source software development), but also provide excellent tools
for discovering, filtering, and using platform contents.

Annotation and knowledge representation
While generating comprehensive metadata at the
same time as the corresponding primary research objects is an ideal worth striving for, it is unrealistic
to expect researchers to rapidly converge on a single
set of best practices—and such an outcome would
still do nothing for the vast sea of research objects
created in the days before Jupyter notebooks and
Docker containers. A prospective, forward-looking approach should therefore be complemented with tools
designed to retroactively annotate the existing literature. Rather than approaching this as a social
science-specific problem, we suggest that social scientists look to, and adapt methods from, machine
learning and computational linguistics—fields that
are rapidly generating a wide range of models suitable for use in annotation and search applications. In
particular, recent advances in embedding techniques
(Mikolov, Chen, Corrado, & Dean, 2013; Zhang et
al., 2016) have led to significant improvements in semantic representation of complex texts—including
scholarly papers—using low-dimensional spaces that
allow efficient and accurate information retrieval (Kusner, Sun, Kolkin, & Weinberger, 2015; Ye, Shen, Ma,
Bunescu, & Liu, 2016). A number of promising applications have already emerged in biomedical fields (e.g.,
Habibi, Weber, Neves, Wiegandt, & Leser, 2017; Kim,
Fiorini, Wilbur, & Lu, 2017), and there is similar potential in the social sciences. We envision the future
development of large-scale, continuously-learning research engines that would automatically encode every
research object in a common, domain-specific latent
semantic space—a development that would greatly
facilitate a wide range of applications, including automated categorization and annotation, semantic search,
and personalized recommendations.

Online platforms that effectively solve the information curation challenge have a number of common
features: they typically provide clear visual feedback—
and a sense that one is contributing to a communal
enterprise—following even minor user actions, and
they reward increased user engagement with new abilities, community feedback, and/or greater visibility.
For example, in data.world, user onboarding happens
immediately from sign-up, and super-users are highly
visible from the homepage. On StackOverflow, users
gain a range of editorial powers (e.g., the ability to
edit others’ answers) as their user score increases. And
on GitHub, the authors of highly-starred repositories
gain in reputation and exposure, often leading to new
career opportunities.

Arguably the most general common feature of such
systems is that they work hard to align individual
and communal incentives—i.e., to ensure that what’s
good for an individual user is also good for the entire
community. When such an alignment is absent—as
is arguably often the case in social science research—
individuals may continue to behave in ways that harm
the collective endeavor (Liu, Yang, Adamic, & Chen,
2014; Smaldino & McElreath, 2016). Minimizing
the transaction costs to participation and ensuring
that such participation is appropriately rewarded is
Collaborative filtering
a critical part of the solution. Adapting successful
The problem of extracting relevant high-quality in- modes of online collaboration to the needs of social
formation from a vast sea of background noise is not scientists could help improve the quality and efficiency
limited to the social sciences, or even to academic of multiple aspects of the typical research workflow,
9

ranging from literature and dataset discovery to post- (3) automatic creation of checklists for researchers to
publication peer review (Kriegeskorte, Walther, & verify their in-progress work.
Deca, 2012; Yarkoni, 2012).
While all of the above quality assurance steps are
possible today, they currently require additional huOutlook
man labor that is time-intensive and is itself errorprone. As an example, consider Young’s work assessThe outlook for applications that boost search and ing the validity of parametric modeling assumptions
discoverability via automation is extremely positive in economics papers, which exposed many results for
in our view—in large part because many of the appli- which non-parametric techniques that make fewer ascations discussed here are domain-general, and tools sumptions would have led to substantially different
and platforms introduced in other fields are likely inferences (Young, 2015). While this type of effort is
to be adapted relatively easily to the needs of so- extremely valuable, it requires a direct time commitcial scientists. Given the rapid technical progress ment from a small pool of experts whose time is scarce
researchers are making in fields like machine learning, relative to the amount of work that could be reviewed.
we think the primary focus of social science researchers The development of a new generation of automated
and funding agencies intent on improving search and tools would have a potentially transformative effect
discoverability should be on adapting and adopting on our ability to rapidly detect and correct scientific
existing techniques rather than on de novo meth- errors in nascent and previously published work.
ods development. Collaboration between social and
data scientists will have a large payoff as each learns
Enumeration of statistical assumptions
from the other. Challenges include bridging disciMany papers have investigated the importance of
plinary differences in terminology and publication
norms. These can be addressed by a critical mass of statistical assumptions by manually re-analyzing large
forward-looking institutions and/or funding agencies bodies of existing work. King et al. (1998) described
who commit dedicated and substantial resources to the importance of assumptions about the safety of
ignoring missing values and re-analyzed several pasupport these collaborative objectives.
pers to demonstrate the practical impact of this often
implicit assumption. We believe that this assumption
is now amenable to automatic evaluation using tools
Challenge 4: Claim validation
commonplace in software engineering. For example, it
Human-caused methodological error is pervasive in is common to “lint” computer programs to check for
social science, but it is labor-intensive to detect errors. known patterns of undesirable programming practices
A fertile and largely unexplored application ground by having one computer program read other computer
for increased automation is the automatic detection programs and catch errors that can be detected based
and/or correction of methodological errors. Automa- on obvious textual features (e.g. the use of functions
tion of correctness checking is a very new line of work that have been deprecated or are known to be ineffiwithin social science, but the recent success of the cient). Even more sophisticated analyses are possible
Statcheck framework (Epskamp & Nuijten, 2014)—a using tools based on static program analysis—a field of
tool for detection of basic statistical errors that has research that investigates the properties of programs
been automatically applied to thousands of published that can be learned without executing programs (e.g.,
articles (Baker, 2016)—suggests substantial growth Chess & McGraw, 2004; Zheng et al., 2006). NASA
potential. Three kinds of automation that we believe has used static analysis tools to catch errors in the
would be especially fruitful are: (1) automatic enumer- Mars Rover and other mission-critical systems (Brat
ation of statistical assumptions that are implicit in & Klemm, 2003); Boeing uses static analysis to verify
analyses; ; (2) automatic sensitivity analysis to assess the safety of airplane designs. Such approaches could
the range of assumptions that support a result; and be applied to catch simple concerns like the frequency
10

of ignoring missing data or the exclusion of intercepts
in regression models. The output of such systems
could be a standardized representation of statistical
assumptions.

potentially fragile results (e.g., in cases where data
are publicly available, by automatically evaluating a
large range of alternative model specifications that
include additional dataset columns as covariates).

Robustness/sensitivity analysis

Reporting checklist verification

Empirical research frequently involves “robustness
checks” where researchers evaluate how sensitive their
results are to alternative analysis specifications (Steegen, Tuerlinckx, Gelman, & Vanpaemel, 2016). Less
common are more formal sensitivity analyses that
consider, e.g., the consequences of unobserved confounding for causal inference. The near-complete
lack of the latter type of analysis raises serious concerns about the credibility of the published social
science literature. To quote economist Charles Manski, “If people only report robustness checks … that
show that their stuff works and they don’t push it to
the breakdown point, then that’s deception basically”
(Tamer, 2018). Automated robustness checks and sensitivity analyses could potentially play a large role in
increasing the credibility of—or detecting problems
with—reported findings.

In health care settings, the mandatory use of checklists in has been shown to substantially improve a
variety of clinical outcomes (Hales & Pronovost, 2006;
Haynes et al., 2009). There is an analogous movement in science to promote scientific reproducibility
and reliability by encouraging or requiring authors to
complete reporting checklists when submitting articles for publication (LeBel et al., 2013; Nichols et al.,
2017). Unfortunately, enforcement of such checklists
currently requires human attention, and hence is often
lax. Automating the process of verifying that authors
have provided required information—and potentially
even validating the information itself—could help prevent many errors of omission at the front end of the
publication process. For example, we can envision automated, domain-general manuscript-checking tools
that (probabilistically) detect whether or not authors
have included required items like sample size justification, formal model specification, preregistration
document, or links to external data. Building on some
of the other assessment applications discussed above,
more elaborate versions of such platforms could perform cursory quality assessment over these items—for
example, flagging for human attention cases where
reported sample sizes deviate substantially from other
works in the same area, where the numbers in a preregistration document appear to conflict with those
detected in the manuscript body, and so on.

As one example, recent work by Schuemie and colleagues has examined the degree of latent confounding
of effects in epidemiology by estimating distributions
of estimated effects for known nulls (Schuemie, Ryan,
DuMouchel, Suchard, & Madigan, 2014). This allows
the automatic construction of empirically calibrated
tests and confidence intervals for new analyses. Another promising broad direction lies in automated
construction of “multiverse analyses” (Steegen et al.,
2016)—that is, analyses that construct distributions
of alternative results by systematically varying data
preprocessing choices, selected variables, estimation
parameters, and so on. Early efforts in this direction
used millions of distinct specifications of statistical
models (Sala-i-Martin, 1997), but such approaches
have not yet been widely adopted by authors. Development of general-purpose, easy-to-use tools for
robustness analysis would likely improve matters. In
cases where authors remain hesitant to spontaneously
report robustness analyses, automated robustness
checks could still be used during the review process,
or by post-publication review platforms, to detect

Outlook
As the success of StatCheck demonstrates, even relatively simple automated validation methods that can
be implemented given present standards and technologies can have a large impact on the quality of social
science research. Accordingly, one natural target for
near-term investment in this area is development of
centralized, automated versions of existing quality assessment tools—e.g., various QRP and bias detection
methods (Brown & Heathers, 2016; e.g., Gerber &

11

Malhotra, 2008; Heathers, Anaya, van der Zee, &
Brown, 2018; Simonsohn, Nelson, & Simmons, 2014),
reporting checklists, etc.—that are presently deployed
manually and sporadically. More sophisticated kinds
of validation—e.g., the ability to automatically identify errors in formal mathematical models—will likely
improve progressively in the coming decades as computer science, statistics and social science make further advances. Many of the tools we envision here will
also have as prerequisite the kinds of standardized
representations of research objects that we discussed
in Challenge 1.

automation-related advancement of social science lie
in the area of scientific discovery and insight generation. While it is not hard to convince most social
scientists that automation could one day play a major
role in standardizing data, improving search, or detecting errors, the notion that machines might one day
exhibit something akin to scientific insight or creativity often elicits considerable skepticism. To be sure,
the prospects of such advances are more distant than
those discussed elsewhere in this paper. Nevertheless,
we believe there are several areas of research where
automated technologies could plausibly be deployed
within the next decade to produce important new
Going forward, we perceive two main risks in this
scientific insights. Examples include (1) automated
area. First, automated validation tools may fail to
signal discovery; (2) automated meta-analysis; and
attract widespread adoption if users perceive that the
(3) reasoning systems or inference engines.
required effort investment outweighs the prospective
benefits. Aside from reiterating the importance of
wrapping sophisticated programmatic tools in sim- Signal discovery and hypothesis generation
ple, intuitive user interfaces, we believe that the most
The social sciences are increasingly awash in higheffective means to ensure widespread adoption of automated validation methods may be to integrate such dimensional, unstructured data. Unlike most purposemethods into centralized platforms that researchers built, lower-dimensional data (e.g., a simple randomalready routinely interact with—for example, by hav- ized lab experiment, a short survey), it’s often unclear
ing grant proposal and journal submission systems how to extract information relevant to basic and apautomatically run a suite of automated QC analysis plied sciences from such sources. Even many purposebuilt data sets are increasingly quite high-dimensional
checks on every uploaded document.
(e.g., neuroimaging data or suveys linked to genetic
Second, as we have already seen in the case of and other biological data). Other data sources (e.g.,
StatCheck, there may be opposition to automated flag- large collections of images of urban environments)
ging of errors that are perceived (rightly or wrongly) have clear relevance to social science questions, but
to be invalid (Baker, 2016). In software engineering, scalable analysis requires automation to be practical
for example, linting tools often impose subjective style — and such automation via computer vision methods
rules that stir controversy and require social coordi- may uncover important signals not readily accessible
nation and proper incentives to resolve. We do not by humans (Naik et al., 2017).
pretend that every automated validation tool will produce error-free results that satisfy every researcher;
Automated explorations of such high-dimensional
regular calibration and iteration will undoubtedly be and unstructured data for social science presents some
required to ensure that such tools continue to perform unique challenges that are absent or less important
at a satisfactory level. The initial contribution of such in other empirical fields; these challenges have often
tools may be to provide a limited number of cases been neglected by work in, e.g., data mining, even
suitable for manual analysis before full automation is when applied to behavioral data. Social science is
possible.
generally concerned, like other sciences, with learning
about causal relationships, but these are frequently
confounded by highly heterogeneous units selecting
Challenge 5: Automated insight
into different exposures and behaviors based on their
Perhaps the most tantalizing opportunities for expectations (e.g., workers selecting into job training,
12

social media users selecting into exposure to misinformation). Thus, automated generation of insights
should aim to (a) surface to researchers empirical
regularities that are useful for causal inference while
adjusting for confounding and (b) help researchers
identify sources of plausibly exogenous (i.e., as-goodas-random) variation in variables of interest.

Automated meta-analysis
One area where there is considerable potential for
automation to generate new insights without requiring
major advances in technology is automated synthesis
of large amounts of data via meta- or mega-analysis.
Examples of such approaches are common in the
biomedical sciences: in genetics, researchers have automatically conducted genome-wide association studies
(GWAS) of thousands of phenotypes (McInnes et al.,
2018); in neuroimaging, large-scale automated metaanalyses of functional MRI data have been conducted
for hundreds of concepts (Yarkoni, Poldrack, Nichols,
Van Essen, & Wager, 2011). Although the social sciences deal with less well-defined entities, there are
many domains where even relatively simple automated
meta-analysis tools could potentially provide valuable
benefits. Limited forms of automated meta-analysis
should be almost immediately feasible in virtually
any domain where researchers have converged on, and
widely adopted, a common ontology.
To illustrate, consider the question of whether
and how Neuroticism—a personality trait defined
by the tendency towards frequent and intense negative emotion—influences physical health (Lahey,
2009; Smith, 2006). There are literally hundreds
of publicly accessible datasets containing at least one
Neuroticism-related measure and at least one measured health outcome. Given controlled vocabularies
stipulating what variables can be treated as proxies
of Neuroticism and health, respectively, it would be
relatively straightforward to write meta-analytic algorithms that automatically analyze all such datasets
and produce a variety of meta-analytic estimates (e.g.,
automatically conducting separate analyses for selfreported versus objectively-measured health outcomes;
for different sexes and populations; etc.). While such
estimates would undoubtedly be of considerably lower

quality than comparable manual analyses, they would
be effortlessly scalable, and could provide researchers
with powerful new tools for large-scale exploration
and hypothesis generation.

Reasoning systems
At present, truly creative scientific inference remains the sole province of human beings. However,
there are reasons to believe that machine intelligence
may, in limited domains, begin to complement human
reasoning system in the coming decades. In mathematical and computational fields, where theories deal
with more precisely defined entities and much of inference is deductive in nature, there are many examples
of automated systems solving non-trivial problems—
most commonly under the guise of automated theorem
proving (Loveland, 2016). However, isolated examples
of automated systems inductively discovering new insights with little or no human intervention can be
found in the biological sciences too (e.g., R. D. King
et al., 2009).
Broadly speaking, we envision two general ways
in which automated scientific reasoning technologies
could be introduced into the social sciences. The
first resembles existing approaches in other fields, in
that it relies largely on the application of explicit
rule-based inference systems (sometimes called “inference engines”) to codified, formal representations
of scientific entities. In the social science context,
an autonomous reasoning system of this sort might
proceed by (i) detecting novel regularities in available data; (ii) encoding these regularities as formal
hypotheses; (iii) generating quantitative predictions
and identifying suitable additional datasets to test
them in; and (iv) carrying out those tests and drawing appropriate, statistically-grounded conclusions.
As a speculative example, consider the NeuroticismHealth relationship described in the previous section.
Given a sufficiently comprehensive formal representation of the personality and health domains, once
an automated system establishes that Neuroticism
is correlated with poorer health, it could potentially
go on to generate and test specific causal models relating the two. For example, upon recognizing that
NEUROTICISM is associated with greater STRESS,

13

that STRESS triggers COPING STRATEGIES, and Practical and ethical considerations
that some COPING STRATEGIES like SMOKING,
DRUG USE, or COMPULSIVE EATING are associOur review has focused predominantly on technical
ated with poor HEALTH, the system might formulate challenges and opportunities associated with efforts
a set of causal graphical models that can be explicitly to automate social science research workflows. But
tested and compared using available datasets.
as we have already observed above, the introduction
The second route to automated inference eschews and widespread of automation to the social sciences
explicit inference rules and instead takes inspiration will undoubtedly also bring into focus a range of pracfrom numerous machine learning applications that tical and ethical considerations. We focus on two
have successfully solved important real world problems in particular here: first, the importance of adoptionusing large-scale neural network models. Such mod- centric thinking, and second, the need for continual
els are typically trained to maximize performance on calibration and human oversight.

some well-defined predictive criterion; however, once
acceptable performance is achieved, the focus then
often shifts to generating low-dimensional interpretations of the learned representations, providing human
experts with potentially valuable insights (Montavon,
Samek, & Müller, 2018; Olah et al., 2018). In this
vein, we could imagine an extremely complex “blackbox” neural network that can non-trivially predict
future economic performance from current economic
and political indicators. Even if the internal dynamics of the model were too complex for humans to
truly understand, probing the system in the right way
might reveal simpler approximations that yield novel
scientific hypotheses and help guide policy decisions.

Outlook
While limited forms of automated insight generation are probably already feasible in some domains
of social science, most of the applications discussed
here depend on technological and standardization advances that may take decades to realize and widely
adopt. Rather than attempting to directly develop
reasoning systems that display something akin to scientific creativity, we think such applications may be
most useful as aspirational long-term objectives that
can help constrain and guide short-term efforts of the
kinds discussed in the previous sections. Viewing automated insight generation as a long-term aspirational
goal should also hopefully provide the motivation and
justification for researchers to carefully consider a
number of important pragmatic and ethical considerations that the realization of such technologies might
introduce—a topic we turn to next.

Practical considerations:
training

Adoption and

We have already highlighted many of the practical
considerations for our proposals. Here we discuss two
in more detail: adoption and training. The movie
Field of Dreams famously popularized the aphorism
that if you build it, they will come. Unfortunately, in
the world of scientific tool development, they (usually)
do not come just because you build it. Open-source
software repositories and methods-oriented journals
are littered with the abandoned husks of “state-ofthe-art” tools that once objectively performed well
on some seemingly important metric, but were never
widely adopted by the community. Addressing the
proliferation of scientific tools in the face of limited
user attention is a critical problem that cuts across
all of the challenges we discuss above. Unfortunately,
it’s also one that funders and researchers alike tend
to overlook—arguably because many of the tasks involved, however important, can seem peripheral to
the core scientific enterprise.
To address this, scientific tool developers should
take user experience seriously. Most scientists are
unlikely to learn a new programming language just
to use a new tool, so it’s important for tool developers to lower barriers to access whenever possible—for
example, by wrapping one’s robust but hard-to-use
software library in intuitive graphical interfaces. Relatedly, it’s difficult to exaggerate how important good
documentation is for tool adoption. Ideally, a complex
new tool or standard should be accompanied not only

14

by a technical reference, but also by interactive tuto- social scientists’ needs will likely require continual
rials, worked examples, and user guides that target evaluation and calibration in order to maintain high
potential users at all levels of expertise.
performance and minimize undesirable consequences.
The fact that a technology works well in one conTools and platforms should also be designed in a
text does not guarantee successful generalization to
way that provides immediate benefits to new users
other contexts; for example, a statistical validation
(Roure & Goble, 2009). Many a state-of-the-art tool
tool that operates precisely as expected in one social
has floundered because its creators were thinking priscience domain may fail catastrophically if uncritimarily about its long-term benefits, once users have
cally applied to another domain where one or more of
conquered a potentially steep learning curve and inits assumptions are routinely violated. The probabilvested considerable energy transitioning to a new
ity of generalization failures may further increase in
system. Appeals to altruism or long-term gains are
cases where a technology relies heavily on “blackbox”
much less compelling to most people than concrete
components—as in recent high-profile examples of
demonstrations of short-term benefits. For example,
machine learning algorithms producing decisions that
it’s probably a mistake to try to pitch a new ontology
bias against certain groups (Corbett-Davies, Pierson,
to researchers by observing that if everyone were to
Feller, Goel, & Huq, 2017; Johndrow & Lum, 2017).
use this ontology, things would be great. Better to
Consequently, it is imperative that researchers who are
release the ontology alongside one or two easy-to-use
working to publicly deploy new automated technolosoftware packages or web applications that provide imgies also simultaneously develop rules and guidelines
mediate benefits to users if they take the small step of
for continued quality assurance—including specific opannotating their data—for example, by automatically
erating requirements and concrete contingency plans
conducting basic quality control checks, identifying
for addressing potential failures.
related public datasets, and so on.
At the same time, it is important to remember that
human analysts are also prone to error and bias, and
that the actual causes of human-issued decisions also
often defy simple explanation (i.e., a good deal of human behavior is also arguably a product of blackbox
reasoning). Automated technologies need not be perfect to merit widespread deployment; they need only
have more favorable cost/benefit profiles than corresponding manual approaches. Of course, this is only
true in the case of well-defined problems—the kind
at which computers presently excel. Automated tools
are not particularly good at defining the problems
in the first place, and we must not undervalue the
importance of humans for their ability to find those
problems. This applies both to the creative nature
of hypothesis formation and to the societal impact
of automation whenever research has technological
or policy implications. The latter often requires an
Ethical considerations: Oversight and origi- on-the-ground understanding of how people interact
with their physical and social environments. For exnality
ample, when automated systems used by police to
Automation does not imply independence from hu- allocate patrols are calibrated using existing arrest
man oversight. Most automated tools that address rates, the results often reinforce racial biases due to

Another consideration concerns the challenge of
training computationally adept social scientists of
the future. A common retort to calls for additional
training in the social sciences is “What would you
take away to make room?” We should not underplay the value of deep domain knowledge, lest we
simply add computational scientists to the butt of
the joke about the naive physicist entering a new
discipline (https://xkcd.com/793/). Social science is
increasingly an interdisciplinary endeavor, and such
interdisciplinarity will require not merely more specialization, but also more integration between experts
with complementary domain knowledge. Institutional
inertia is legendary, so re-engineering training programs to produce not just skilled social scientists but
functional teams of social scientists will be an important consideration for the universities of the future.

15

the inclusion of petty crimes such as loitering (O’Neil,
Antenucci, D., Cafarella, M. J., & Levenstein, M.
2017). The price of automation is eternal vigilance. C. (2013). Ringtail: Feature Selection for Easier Nowcasting.Proceedings of the VLDB Endowment,6(12),
1358–1361.

Conclusion

Automation plays a central and rapidly growing
role in our daily lives; it has influenced virtually every
domain of human activity, and science is no exception. But the impacts have been uneven, with the
social sciences presently lagging far behind the natural and biomedical sciences in the use of automation
to solve practical and theoretical challenges. While
the entities studied by the social sciences may be
more difficult to operationalize and formalize than
those found in many other sciences, we have argued
that social scientists could nevertheless be harnessing automation to a far greater extent. Our review
highlights a number of important challenges in social
science that increased automation could help mitigate
or even solve —including some that appear relatively
tractable in the near term. Greater investment in
automation would be an effective and extremely costeffective way to boost the reliability, validity, and
utility of a good deal of social science research.

Baker, M. (2016). Stat-checking software stirs up
psychology.Nature,540(7631), 151–152.
Bar-Ilan, J., & Halevi, G. (2018). Temporal characteristics of retracted articles.Scientometrics,116(3),
1771–1783.
Boettiger, C. (2015). An introduction to Docker
for reproducible research.ACM SIGOPS Operating
Systems Review,49(1), 71–79.
Brat, G., & Klemm, R. (2003). Static analysis of
the mars exploration rover flight software.Proceedings
of the First International Space Mission Challenges
for Information Technology, 321–326.
Brown, N. J. L., & Heathers, J. A. J. (2016). The
GRIM Test.Social Psychological and Personality Science,8(4), 363–369.
Bush, V., & Others. (1945). As we may think.The
Atlantic Monthly,176(1), 101–108.

Camerer, C. F., Dreber, A., Holzmeister, F., Ho,
T.-H., Huber, J., Johannesson, M., … Wu, H. (2018).
Evaluating the replicability of social science experThis paper emerged from discussions at a 2-day iments in Nature and Science between 2010 and
DARPA-sponsored workshop held at the Center for 2015.Nature Human Behaviour,2(9), 637–644.
Open Science (COS) in September 2018. Work was
Card, D., Chetty, R., Feldstein, M. S., &
partially supported by NIH award R01MH109682 to
Saez,
E. (2010).Expanding Access to AdminisTY, and by awards to JIL from the Overdeck Family
trative
Data for Research in the United States.
Foundation, Eric and Wendy Schmidt by recommendahttps://doi.org/10.2139/ssrn.1888586
tion of the Schmidt Futures program, and the Alfred
P. Sloan Foundation. The authors are grateful to John
Chess, B., & McGraw, G. (2004). Static analysis
Myles White for valuable discussion and feedback.
for security.IEEE Security Privacy,2(6), 76–79.

Acknowledgments

Christensen, G., & Miguel, E. (2018). Transparency,
Reproducibility, and the Credibility of Economics
Research.Journal of Economic Literature,56(3), 920–
Afgan, E., Baker, D., Batut, B., van den Beek, M., 980.
Bouvier, D., Cech, M., … Blankenberg, D. (2018). The
Galaxy platform for accessible, reproducible and colCollins, F. S., Morgan, M., & Patrinos, A. (2003).
laborative biomedical analyses: 2018 update.Nucleic The Human Genome Project: lessons from large-scale
Acids Research,46(W1), W537–W544.
biology.Science,300(5617), 286–290.

References

16

Commission on Evidence-Based Policymaking.
Gorgolewski, K. J., Alfaro-Almagro, F., Auer, T.,
(2017).CEP Final Report:
The Promise of Bellec, P., Capotă, M., Chakravarty, M. M., … PolEvidence-Based Policymaking.
Retrieved from drack, R. A. (2017). BIDS apps: Improving ease of
https://www.cep.gov/cep-final-report.html
use, accessibility, and reproducibility of neuroimaging data analysis methods.PLoS Computational BiolCorbett-Davies, S., Pierson, E., Feller, A., Goel, S., ogy,13(3), e1005209.
& Huq, A. (2017). Algorithmic Decision Making and
the Cost of Fairness. InProceedings of the 23rd ACM
Gorgolewski, K. J., Auer, T., Calhoun, V. D., CradSIGKDD International Conference on Knowledge Dis- dock, R. C., Das, S., Duff, E. P., … Poldrack, R. A.
covery and Data Mining (pp. 797–806). New York, (2016). The brain imaging data structure, a format
NY, USA: ACM.
for organizing and describing outputs of neuroimaging
experiments.Scientific Data,3, 160044.
Dafoe, A. (2014).
Science deserves better:
the imperative to share complete replication
Greenberg, S. A. (2009). How citation distortions
files.PS, Political Science & Politics. Retrieved create unfounded authority: analysis of a citation
from https://www.cambridge.org/core/journals/ps- network.BMJ,339, b2680.
political-science-and-politics/article/science-deservesHabibi, M., Weber, L., Neves, M., Wiegandt, D.
better-the-imperative-to-share-complete-replicationL.,
& Leser, U. (2017). Deep learning with word
files/C19AE087642810DD9C0C83BF8D0908A9
embeddings improves biomedical named entity recogEinav, L., & Levin, J. (2014). Economics in the nition.Bioinformatics,33(14), i37–i48.
age of big data.Science,346(6210), 1243089.
Hales, B. M., & Pronovost, P. J. (2006). The
Epskamp, S., & Nuijten, M. B. (2014). checklist—a tool for error management and perforstatcheck: Extract statistics from articles and mance improvement.Journal of Critical Care,21(3),
recompute p values (R package version 1.0. 0.). 231–235.
Retrieved from https://www.narcis.nl/publicaHaynes, A. B., Weiser, T. G., Berry, W. R., Liption/RecordID/oai:tilburguniversity.edu:publicasitz,
S. R., Breizat, A.-H. S., Dellinger, E. P., … Safe
tions%2F8ec1db3f-18a7-4c12-816b-8f95c194da14
Surgery Saves Lives Study Group. (2009). A surgical
Gerber, A., & Malhotra, N. (2008). Do Statisti- safety checklist to reduce morbidity and mortality
cal Reporting Standards Affect What Is Published? in a global population.The New England Journal of
Publication Bias in Two Leading Political Science Medicine,360(5), 491–499.
Journals.Quarterly Journal of Political Science,3(3),
Heathers, J. A., Anaya, J., van der Zee, T.,
313–326.
& Brown, N. J. L. (2018).Recovering data
Gewin, V. (2014). Retractions: A clean slate.Na- from summary statistics:
Sample Parameture,507(7492), 389–391.
ter Reconstruction via Iterative TEchniques
(SPRITE) (No.
e26968v1).
PeerJ Preprints.
Giannone, D., Reichlin, L., & Small, D. (2008). https://doi.org/10.7287/peerj.preprints.26968v1
Nowcasting: The real-time informational content
of macroeconomic data.Journal of Monetary EcoInter-university Consortium for Political and Social
nomics,55(4), 665–676.
Research. (2009).Guide to Social Science Data Preparation and Archiving: Best Practice Throughout the
Gorgolewski, K., Esteban, O., Schaefer, G., Wan- Life Cycle. Inter-University Consortium for Political
dell, B., & Poldrack, R. (2017). OpenNeuro—a free & Social Research.
online platform for sharing and analysis of neuroimaging data.Organization for Human Brain Mapping.
Ioannidis, J. (2008). Why most discovered true
Vancouver, Canada, 1677.
associations are inflated.Epidemiology,19(5), 640–648.
17

Ioannidis, J. P. A. (2005). Why most published
Kusner, M., Sun, Y., Kolkin, N., & Weinberger,
research findings are false.PLoS Medicine,2(8), e124. K. (2015). From Word Embeddings To Document
Distances. InInternational Conference on Machine
Jarmin, R. S., & O’Hara, A. B. (2016). Big data and Learning (pp. 957–966).
the transformation of public policy analysis.Journal
Lahey, B. B. (2009). Public health significance of
of Policy Analysis and Management: [the Journal
neuroticism.The
American Psychologist,64(4), 241–
of the Association for Public Policy Analysis and
256.
Management],35(3), 715–721.
Lane, J. (2016). Big data for public policy: The
Johndrow, J. E., & Lum, K. (2017).An algorithm for
quadruple
helix.Journal of Policy Analysis and Manremoving sensitive information: application to raceagement:
[the
Journal of the Association for Public
independent recidivism prediction.arXiv [stat.AP]. RePolicy
Analysis
and Management],35(3), 708–715.
trieved from http://arxiv.org/abs/1703.04957
LeBel, E. P., Borsboom, D., Giner-Sorolla, R., HasKim, S., Fiorini, N., Wilbur, W. J., & Lu, Z. (2017).
selman, F., Peters, K. R., Ratliff, K. A., & Smith, C.
Bridging the gap: Incorporating a semantic similarity
T. (2013). PsychDisclosure.org : Grassroots Support
measure for effectively mapping PubMed queries to
for Reforming Reporting Standards in Psychology.Perdocuments.Journal of Biomedical Informatics,75, 122–
spectives on Psychological Science: A Journal of the
127.
Association for Psychological Science,8(4), 424–432.
King, G., Honaker, J., Joseph, A., & Scheve,
Levenstein, M. C., & Lyle, J. A. (2018). Data:
K. (1998). List-wise deletion is evil: what to do Sharing Is Caring.Advances in Methods and Practices
about missing data in political science. InAn- in Psychological Science,1(1), 95–103.
nual Meeting of the American Political Science
Levenstein, M. C., Tyler, A. R. B., & Davidson
Association, Boston. Retrieved from https://pdfs.semanticscholar.org/813e/3040f8a0988bfd3b385f49cb- Bleckman, J. (2018).The Researcher Passport: Improving Data Access and Confidentiality Protection:
babb802afa0c.pdf
ICPSR?s Strategy for a Community-normed System
King, R. D., Rowland, J., Oliver, S. G., Young, of Digital Identities of Access. ICPSR White PaM., Aubrey, W., Byrne, E., … Clare, A. (2009). The per Series No. 1. Ann Arbor, MI: University of
automation of science.Science,324(5923), 85–89.
Michigan Inter-university Consortium for Political
and Social Research.. Retrieved from https://deepKluyver, T., Ragan-Kelley, B., Pérez, F., Granger,
blue.lib.umich.edu/handle/2027.42/143808
B., Bussonnier, M., Frederic, J., … and the Jupyter
Liu, T. X., Yang, J., Adamic, L. A., & Chen,
Development Team. (2016). Jupyter Notebooks—
a publishing format for reproducible computational Y. (2014). Crowdsourcing with All-Pay Auctions:
workflows. In F. &. S. Loizides (Ed.),Positioning and A Field Experiment on Taskcn.Management SciPower in Academic Publishing: Players, Agents and ence,60(8), 2020–2037.
Agendas (pp. 87–90). IOS Press.
Loveland, D. W. (2016).Automated Theorem ProvKorpela, K. M. (2010). How long does it take for ing: A Logical Basis. Elsevier.
the scientific literature to purge itself of fraudulent maMcInnes, G., Tanigawa, Y., DeBoever, C., Lavertu,
terial?: the Breuning case revisited.Current Medical A., Olivieri, J. E., Aguirre, M., & Rivas, M.
Research and Opinion,26(4), 843–847.
(2018).Global Biobank Engine: enabling genotypephenotype browsing for biobank summary statisKriegeskorte, N., Walther, A., & Deca, D. (2012).
tics.bioRxiv. https://doi.org/10.1101/304188
An emerging consensus for open evaluation: 18 visions for the future of scientific publishing.Frontiers
Mikolov, T., Chen, K., Corrado, G., & Dean,
in Computational Neuroscience,6, 94.
J. (2013).Efficient Estimation of Word Representa18

tions in Vector Space.arXiv [cs.CL]. Retrieved from Search for Mental Structure.Annual Review of Psyhttp://arxiv.org/abs/1301.3781
chology,67, 587–612.
Poteete, A. R., Janssen, M. A., & Ostrom, E.
Montavon, G., Samek, W., & Müller, K.-R. (2018).
Methods for interpreting and understanding deep neu- (2010).Working Together: Collective Action, the Commons, and Multiple Methods in Practice. Princeton
ral networks.Digital Signal Processing,73, 1–15.
University Press.
Munafò, M. R., Nosek, B. A., Bishop, D. V. M.,
Roure, D. D., & Goble, C. (2009). Software Design
Button, K. S., Chambers, C. D., Percie du Sert, N.,
… Ioannidis, J. P. A. (2017). A manifesto for repro- for Empowering Scientists.IEEE Software,26(1), 88–
95.
ducible science.Nature Human Behaviour,1, 0021.
National Research Council, Division of Behavioral
and Social Sciences and Education, Committee on
Population, Committee on National Statistics, Board
on Behavioral, Cognitive, and Sensory Sciences, &
Committee on Revisions to the Common Rule for
the Protection of Human Subjects in Research in
the Behavioral and Social Sciences. (2014).Proposed
Revisions to the Common Rule for the Protection of
Human Subjects in the Behavioral and Social Sciences.
National Academies Press.

Sala-i-Martin, X. X. (1997). I just ran two million
regressions.The American Economic Review,87(2),
178.
Schuemie, M. J., Ryan, P. B., DuMouchel, W.,
Suchard, M. A., & Madigan, D. (2014). Interpreting
observational studies: why empirical calibration is
needed to correct p-values.Statistics in Medicine,33(2),
209–218.
Shen, H. (2014). Interactive notebooks: Sharing
the code.Nature,515(7525), 151–152.

Nichols, T. E., Das, S., Eickhoff, S. B., Evans, A. C.,
Simonsohn, U., Nelson, L. D., & Simmons, J. P.
Glatard, T., Hanke, M., … Yeo, B. T. T. (2017). Best
(2014).
P-curve: a key to the file-drawer.Journal of
practices in data analysis and sharing in neuroimaging
Experimental
Psychology. General,143(2), 534–547.
using MRI.Nature Neuroscience,20(3), 299–303.
Smaldino, P. E., & McElreath, R. (2016). The
Nyborg, K., Anderies, J. M., Dannenberg, A., Linnatural
selection of bad science.Royal Society Open
dahl, T., Schill, C., Schlüter, M., … de Zeeuw, A.
Science,3(9),
160384.
(2016). Social norms as solutions.Science,354(6308),
42–43.

Smith, T. W. (2006). Personality as Risk and Resilience
in Physical Health.Current Directions in PsyOlah, C., Satyanarayan, A., Johnson, I., Carter, S.,
chological
Science,15(5), 227–231.
Schubert, L., Ye, K., & Mordvintsev, A. (2018). The
building blocks of interpretability.Distill,3(3), e10.
Steegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel,
W. (2016). Increasing Transparency Through
O’Neil, C. (2017).Weapons of Math Destruction:
a
Multiverse
Analysis.Perspectives on Psychological
How Big Data Increases Inequality and Threatens
Science:
A
Journal
of the Association for PsychologiDemocracy. Crown.
cal Science,11(5), 702–712.
Open Science Collaboration. (2015). EstimatTamer, E. (2018). THE ET INTERVIEW: PROing the reproducibility of psychological science.SciFESSOR CHARLES MANSKI.Econometric Theory,
ence,349(6251), aac4716.
1–62.
Ostrom, E. (1990).Governing the commons: the
Vardigan, M., Heus, P., & Thomas, W. (2008).
evolution of institutions for collective action. CamData documentation initiative: Toward a standard for
bridge, Cambridge University Press.
the social sciences.International Journal of Digital CuPoldrack, R. A., & Yarkoni, T. (2016). From Brain ration,3(1). Retrieved from http://www.ijdc.net/inMaps to Cognitive Ontologies: Informatics and the dex.php/ijdc/article/view/66
19

Wallis, J. C., Rolando, E., & Borgman, C. L. (2013). Value of Static Analysis for Fault Detection in SoftIf we share data, will anyone use them? Data shar- ware.IEEE Transactions on Software Engineering, (4),
ing and reuse in the long tail of science and technol- 240–253.
ogy.PloS One,8(7), e67332.
Weinberg, D., Abowd, J. M., Belli, R. F., Cressie,
N., Folch, D. C., Holan, S. H., … Others. (2017).
Effects of a Government-Academic Partnership: Has
[1] https://blog.ons.gov.uk/2017/01/27/the-fivethe NSF-Census Bureau Research Network Helped safes-data-privacy-at-ons/
Secure the Future of the Federal Statistical System?
[2] Some examples include the Department of ComRetrieved from https://ecommons.cornell.edu/hanputational
Social Science at George Mason University,
dle/1813/52650
the Data Science Initiative at UC Davis, the MasYarkoni, T. (2012). Designing next-generation plat- ter’s program in Computational Social Science at the
forms for evaluating scientific output: what scientists University of Chicago, the Master’s program in Apcan learn from the social web.Frontiers in Computa- plied Social Data Science at the London School of
tional Neuroscience,6, 72.
Economics, and the certification program in CompuYarkoni, T., Poldrack, R. A., Nichols, T. E., Van tational Social Science at Stanford University.
Essen, D. C., & Wager, T. D. (2011). Large-scale automated synthesis of human functional neuroimaging
data.Nature Methods,8(8), 665–670.

Ye, X., Shen, H., Ma, X., Bunescu, R., & Liu, C.
(2016). From word embeddings to document similarities for improved information retrieval in software
engineering. InProceedings of the 38th International
Conference on Software Engineering (pp. 404–415).
ACM.
York, D. G., Adelman, J., & Anderson, J.
E., Jr.
(2000).
The sloan digital sky survey: Technical summary.The Astronomical Journal. Retrieved from http://iopscience.iop.org/article/10.1086/301513/meta
Young, A. (2015). Channeling fisher: Randomization tests and the statistical insignificance of seemingly
significant experimental results.E, 0: 0–0. Retrieved
from
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.737.5037&rep=rep1&type=pdf
Zhang, Y., Rahman, M. M., Braylan, A.,
Dang, B., Chang, H.-L., Kim, H., … Lease,
M. (2016).Neural Information Retrieval: A Literature Review.arXiv [cs.IR]. Retrieved from
http://arxiv.org/abs/1611.06792
Zheng, J., Williams, L., Nagappan, N., Snipes, W.,
Hudepohl, J. P., & Vouk, M. A. (2006). On the
20

