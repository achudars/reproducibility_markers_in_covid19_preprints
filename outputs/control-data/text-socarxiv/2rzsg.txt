Under consideration for publication in Euro. Jnl of Applied Mathematics

1

Pull out all the stops:
Textual analysis via punctuation sequences
ALEXANDRA N. M. D A R M O N 1 , MARYA B A Z Z I 1,2,3 , SAM D. H O W I S O N 1 ,
and MASON A. P O R T E R 1,4
1

Oxford Centre for Industrial and Applied Mathematics, Mathematical Institute, University of
Oxford, Oxford OX2 6GG, United Kingdom
2 The Alan Turing Institute, London NW1 2DB, United Kingdom
3 Warwick Mathematics Institute, University of Warwick, Coventry CV4 7AL, United Kingdom
4 Department of Mathematics, University of California, Los Angeles, Los Angeles, California 90095,
USA

(Received 31 December 2018)

"I’m tired of wasting letters when punctuation will do, period."
— Steve Martin, Twitter, 2011
Whether enjoying the lucid prose of a favorite author or slogging through some other
writer’s cumbersome, heavy-set prattle (full of parentheses, em-dashes, compound adjectives, and Oxford commas), readers will notice stylistic signatures not only in word choice
and grammar, but also in punctuation itself. Indeed, visual sequences of punctuation from
different authors produce marvelously different (and visually striking) sequences. Punctuation is a largely overlooked stylistic feature in “stylometry”, the quantitative analysis
of written text. In this paper, we examine punctuation sequences in a corpus of literary
documents and ask the following questions: Are the properties of such sequences a distinctive feature of different authors? Is it possible to distinguish literary genres based on
their punctuation sequences? Do the punctuation styles of authors evolve over time? Are
we on to something interesting in trying to do stylometry without words, or are we full of
sound and fury (signifying nothing)?

Key Words: Stylometry, computational linguistics, natural language processing, digital humanities, computational methods, mathematical modeling, Markov processes, categorical time
series

1 Introduction
"Yesterday Mr. Hall wrote that the printer’s proof-reader was improving my
punctuation for me, & I telegraphed orders to have him shot without giving
him time to pray."
— Mark Twain, Letter to W. Howells, 1989
(,,).;.,””,:(.,,);,,?;,,??

2

A. N. M. Darmon et al.

(a) Punctuation
sequence: Sharing
Her Crime, Agnes
May Fleming

(c) Punctuation
sequence King Lear,
William Shakespeare

(d) Heat map: King
Lear, William
Shakespeare

(b) Heat map:
Sharing Her Crime,
Agnes May Fleming

(e) Punctuation
(f) Heat map: The
sequence The History History of Mr. Polly,
of Mr. Polly, Herbert Herbert George Wells
George Wells

Figure 1. (a,c,e) Excerpts of ordered punctuation sequences and (b,d,e) corresponding
heat maps for books by three different authors: (a,b) Agnes May Fleming; (c,d) William
Shakespeare; and (e,f) Herbert George Wells. Each depicted punctuation sequence consists of 3000 successive punctuation marks starting from the midpoint of the full punctuation sequence of the corresponding document. The colour bar gives the mapping between
punctuation marks and colors.
The sequence of punctuation marks above is what remains of this opening paragraph
of our paper (but, to avoid recursion, without the sequence itself) after we remove all of
the words. It is perhaps hard to credit that such a minimal sequence encodes any useful
information at all; yet it does. In this paper, we investigate the information content of
“de-worded” documents, asking questions like the following: Do authors have identifiable
punctuation styles (see fig. 1, which was inspired by the visualizations from [3, 4]); and,
if so, can we use them to attribute texts to authors? Do different genres of text differ in
their punctuation styles; and, if so, how? How has punctuation usage evolved over the
last few centuries?
Computational linguistics is a wide research area, which (broadly speaking) focuses
on the development of computational tools to process and analyze natural language.
Stylometry, a part of computational linguistics — as well as cultural analytics, within
the broader context of digital humanities — encompasses quantitative analysis of written

Pull out all the stops

3

text, with the goal of characterizing authorship or other characteristics [19,35,44]. Some of
the earliest attempts at quantifying the writing style of a document include Mendenhall’s
work on William Shakespeare’s plays in 1887 [33] and Mosteller et al.’s work on the The
Federalist Papers in 1964 [34]. The latter is often regarded as the foundation of computerassisted stylometry (in contrast with methods based on human expertise) [35, 44]. Uses
of stylometry include (1) authorship attribution, recognition, or detection (which aims
to determine the probability that a document was written by a given author); (2) authorship verification (which aims to determine whether a set of documents were written
by the same author); (3) authorship profiling (which aims to determine certain demographics, such as gender, or other characteristics without directly identifying an author);
(4) stylochronometry (which is the study and detection of changes in authorial style over
time); and (5) adversarial stylometry (which aims to evade authorship attribution via
alteration of style).
There has been extensive work on author recognition using a wide variety of stylometric features, such as “lexical features” (e.g., number of words and mean sentence
length), “syntactic features” (e.g., frequency of different punctuation marks), “semantic
features” (e.g., synonyms), and “structural features” (e.g., paragraph length and number
of words per paragraph). Two common stylometric features for author recognition are
“n-grams” (e.g., in the form of n contiguous words or characters) and “function words”
(e.g., pronouns, prepositions, and auxiliary verbs).
In this paper, in contrast with prior work, we focus on punctuation rather than on
words or characters such as letters. We explore several stylometric tasks through the lens
of punctuation, illustrating their distinctive role in text. Punctuation refers to the various systems of dots and other marks that accompany letters as part of a writing system.
Punctuation is distinct from diacritic marks, which are typically modifications of individual letters (e.g., ç, ö, and ő) and logographs, which are symbolic representations of lexical
items (e.g., # and &). Other common symbols, such as the slash to indicate alternation
(e.g., and/or) and the asterisk “ * ”, are intermediate between these categories, but they
are not considered to be true punctuation marks [27]. Common punctuation marks are
the period (i.e., full stop) “ . ”; the comma “ , ”; the colon “ : ”; the semicolon “ ; ”; the
left and right parentheses, “ ( ” and “ ) ”; the question mark “ ? ”; the exclamation point
(which is also called the exclamation mark) “ ! ”; the hyphen “ - ”; the en dash “ – ”; the
em dash “ — ”; the opening and closing single quotation marks (i.e., inverted commas),
“ ‘ ” and “ ’ ”; the opening and closing double quotation marks (which are also known
as inverted commas), “ “ ”, and “ ” ”; the apostrophe “ ’ ”; and the ellipsis “ ... ”.
The aforementioned punctuation set (with minor variations) is used today in practically all alphabetic writing systems and across most alphabetic languages [27]. In this
sense, punctuation is a supra-linguistic representational system. However, punctuation
varies significantly across individuals, and there is no consensus on how it should be
used [13,29,36,38,46]; authors, editors, and typesetters can sometimes get into emphatic
disagreements about it.1 Accordingly, as a representational system, punctuation is not
standardized; and it may never achieve standardization [27].
For our study, we use Project Gutenberg [17] to obtain a large corpus of documents,
1

Not that any of us would ever descend to this.

4

A. N. M. Darmon et al.

and we extract an ordered set of punctuation marks from each document in the corpus
(see section 2). Although Project Gutenberg is a popular database for the statistical
analysis of language, most previous studies that have used it have considered only a
small number of manually selected documents [14]. Broadly, our goal is to investigate the
following question: Do punctuation marks encode stylistic information about an author,
a genre, or a time period? Different writers have different writing styles (e.g., long versus
short sentences, frequent versus sparse dialogue, and so on), and a given writer’s style
can also evolve over time or differ across different types of works. It is plausible that
an author’s use of punctuation is — consciously or unconsciously — at least partly
indicative of an idiosyncratic style, and we seek to explore the extent to which this is the
case. Although there is a wealth of work that focuses on quantitative analysis of writing
styles, punctuation marks and their (conscious or unconscious) stylistic footprints have
largely been overlooked. Analysis of punctuation is also pertinent to “prosody,” the study
of the tune and rhythm of speech2 and how these features contribute to meaning [18].
To our knowledge, very few researchers have explored author recognition using only
stylometric features that are punctuation-focused [7, 16]. Additionally, the few existing
works that include a punctuation-focused analysis used a very small author corpus (40
authors in [16] and 5 authors in [7]) and focused on the frequency with which different
punctuation marks occur (ignoring, e.g., the order in which they occur). In the present
paper, we investigate author recognition using features that account for both the frequency and the order of punctuation marks in a corpus of 651authors and 14947documents (see section 3). We then explore genre recognition [9,23,40,41] from a punctuation
perspective and stylochronometry [6, 12, 21, 22, 37, 45, 48] in section 4 and section 5, respectively. There are not many studies of stylochronometry, and existing ones tend to
be rather specific in nature (e.g., focused on particular authors, such as Shakespeare [48]
and band members from the Beatles [22], or on particular time frames) [35, 45]. Literary
genre recognition (e.g., fiction, philosophy, etc.) has also received limited attention, and
we are not aware of even a single study that has attempted genre recognition solely using
solely punctuation. We wish to examine (1) whether punctuation is at all indicative of
the style of an author, genre, or time period; and, if so, (2) the strength of stylistic signatures when one ignores words. In other words, how much can one learn from punctuation
alone?
In this paper, we seek to offer quantitative insight into punctuation, an often overlooked
stylistic feature of literature. Importantly, we do not seek to try to identify the best set of
features for a given stylometric task, nor do we seek to conduct a thorough comparison of
different methods for a given stylometric task. Instead, our goal is to give punctuation, an
unsung hero of style, some overdue credit through an initial study of punctuation-focused
stylometry. To do this, we focus on a small number of punctuation-related stylometric
features and use this set of features to investigate questions in author recognition, genre
recognition, and stylochronometry. Our analysis can also be performed across different
languages that use the same set of punctuations (e.g., across different translations), as
2

An amusing illustration is the contrast between the Oxford comma, the Walken comma,
and the Shatner comma. For one example of this meme, see https://www.scoopnest.com/user/
JournalistsLike/529351917986934784.

Pull out all the stops

5

our features are punctuation-based and thus (predominantly) language-independent. We
offer a novel perspective on stylometry that we hope others will carry forward in their
own punctuational pursuits, which include many exciting future directions.
Our paper proceeds as follows. We describe our data set, punctuation-based features,
and classification techniques in section 2. We compare the use of punctuation across
authors in section 3, across genres in section 4, and over time in section 5. We conclude
and offer directions for future work in section 6

2 Data and methodology
"This sentence has five words. Here are five more words. Five-word sentences
are fine. But several together become monotonous. Listen to what is happening. The writing is getting boring. The sound of it drones. It’s like
a stuck record. The ear demands some variety. Now listen. I vary the sentence length, and I create music. Music. The writing sings. It has a pleasant
rhythm, a lilt, a harmony. I use short sentences. And I use sentences of
medium length. And sometimes, when I am certain the reader is rested, I will
engage him with a sentence of considerable length, a sentence that burns with
energy and builds with all the impetus of a crescendo, the roll of the drums,
the crash of the cymbals — sounds that say listen to this, it is important."
— Gary Provost, 100 Ways to Improve Your Writing, 1985.
2.1 Data set
We use the API functionality of Project Gutenberg [17] to obtain our document corpus
and the natural-language-processing (NLP) library spaCy [20] to extract an ordered
punctuation sequence from each document. Using data from Project Gutenberg requires
various filtering and cleaning steps before it is possible to meaningfully perform statistical
analysis [14]. We describe our steps below.
We retain only documents that are written in English (a document’s language is specified in metadata). We remove the author labels “Various”, “Anonymous”, and “Unknown”. To try and mitigate, in an automated way, the issue of a document appearing
more than once in our corpus (e.g., “Tales and Novels of J. de La Fontaine – Complete”,
“The Third Part of King Henry the Sixth”, “Henry VI, Part 3”, “The Complete Works
of William Shakespeare”, and “The History of Don Quixote, Volume 1, Complete”), we
ensure that any given title appears only once, and we remove all documents with the
word “complete” in the title.3 (Note that the word “anthology” does not appear in any
titles in our final corpus.) We also adjust some instances where a punctuation mark or
a space appears incorrectly in the Project Gutenberg raw data (specifically, instances in
which a double quotation appears as uni-code or the spacing between words and punc3
It is still possible for a document to appear more than once in our corpus (e.g., “The Third
Part of King Henry the Sixth” and “Henry VI, Part 3”). However, from the perspective of
author recognition (see section 3), it is interesting to ask whether punctuation in a subset of a
document is representative of punctuation in the whole. We manually remove such duplicates
when investigating specific authors over time (see section 5).

6

A. N. M. Darmon et al.

Figure 2. Histogram of the number of documents per author in our corpus.

tuation marks is missing), and we remove any documents in which double quotations do
not appear. (Although the latter may constitute legitimate documents, we remove them
to err on the side of caution.) Among the remaining documents, we retain only authors
who have written at least ten documents in our corpus. For each of these documents,
we remove headers using the python function “strip headers,” which is available in
Gutenberg’s Python package. This yields a data set with 651 authors and 14947 documents. We show the distribution of documents per author in fig. 2. The documents in our
corpus have various metadata, such as author birth year, author death year, document
“bookshelf” (with at most one unique bookshelf per document), document subject (with
multiple subjects possible per document), document language, and document rights. In
some our computational experiments, we use the following metadata: author birth year,
author death year, and document “bookshelf” (which we term document “genre”, as that
is what it appears to represent). The authors of [14] pointed out recently that “bookshelf”
may be better suited than “subject” to practical purposes such as text classification, because the former constitute broader categories and provide a unique assignment of labels
to documents.
For each document, we extract an ordered sequence of the following ten punctuation
marks: the period “ . ”; the comma “ , ”; the colon “ : ”; the semicolon “ ; ”; the left
parenthesis “ ( ”; the right parenthesis “ ) ”; the question mark “ ? ”; the exclamation
mark “ ! ”; double quotation marks, “ “ ” and “ ” ” (which are not differentiated in our
document format); single quotation marks, “ ‘ ” and “ ’ ” (which also are not differentiated in our document format), which we amalgamate with double quotation marks;
and the ellipsis “ ... ”. To promote a language-independent approach to punctuation
(e.g., apostrophes in French can arise as required parts of words), we do not include
apostrophes in our analysis. We also do not include hyphens, en dashes, or em dashes, as
we cannot differentiate between them in the document formats and we find the choices
among these marks in different documents — standard rules of language be damned —
to be unreliable upon a visual inspection of some documents in our corpus.

Pull out all the stops
Table 1. Summary of the punctuation-sequence features that we study.

7

Feature

Description

Formula

f1

Punctuation-mark frequency

(2.1)

f2

Conditional frequency of successive punctuation marks

(2.2)

f3

Frequency of successive punctuation marks

(2.3)

f4

Sentence-length frequency

(2.4)

f5

Frequency of number of words between successive punctuation marks

(2.5)

f6

Mean number of words between successive occurrences
of the elements in ordered pairs of punctuation marks

(2.7)

2.2 Features
Using standard terminology from the machine-learning literature, we use the term “feature” to refer to any quantitative characteristic of a document or set of documents. We
compute six features vectors for each document k in our corpus to quantify the frequency
with which punctuation marks occur, the order in which they occur, and the number of
words that tend to occur between them. Specifically, we compute the following:
(1) f 1,k , the frequency vector for punctuation marks in a given document k;
(2) f 2,k , an empirical approximation of the conditional probability of the successive
occurrence of elements in an ordered pair of punctuation marks in document k;
(3) f 3,k , an empirical approximation of the joint probability of the successive occurrence
of elements in an ordered pair of punctuation marks in document k;
(4) f 4,k , the frequency vector for sentence lengths in a given document k, where we
consider the end of a sentence to be marked by a period, exclamation mark, question
mark, or ellipsis;
(5) f 5,k , the frequency vector for the number of words between successive punctuation
marks in a given document k; and
(6) f 6,k , the mean number of words between successive occurrences of the elements in
an ordered pair of punctuation marks in document k.
We summarize these features in table 1 (where we suppress the superscript k for ease of
writing) and define each of these six features below.
Let Θ = {θ1 , . . . , θ10 } denote the (unordered) set of ten punctuation marks (see section 2.1). Let n denote the total number of documents in our corpus; and let Dk =
{θ1k , . . . , θnk k }, with k ∈ {1, . . . , n}, denote the ordered set of nk punctuation marks in
document k. As an example, consider the following quote by Ursula K. Le Guin (from
an essay in her 2004 collection, The Wave in the Mind ):
I don’t have a gun and I don’t have even one wife and my sentences tend

8

A. N. M. Darmon et al.

(a) f 1 King Lear, W.
Shakespeare

(b) f 1 Hamlet, W.
Shakespeare

(c) f 1 The History of (d) f 1 The Wheels of
Mr. Polly, H. G.
Chance, H. G. Wells
Wells

(e) f 5 Hamlet, W.
Shakespeare

(f) f 5 Hamlet, W.
Shakespeare

(g) f 5 The History of (h) f 5 The Wheels of
Mr. Polly, H. G.
Chance, H. G. Wells
Wells

Figure 3. (a,b,c,d) Histograms of frequency of punctuation marks (f 1 ) and (e,f,g,h) histograms of the number of words that occur between successive punctuation marks (f 5 )
for two documents by William Shakespeare and two documents by Herbert George Wells.
to go on and on and on, with all this syntax in them. Ernest Hemingway would
have died rather than have syntax. Or semicolons. I use a whole lot of halfassed semicolons; there was one of them just now; that was a semicolon after
"semicolons," and another one after "now."
The set Dk for this quote is {, , . , . , . , ; , ; , “ , , , ” , “ , . , ”}, with nk = 12.
From Dk , we can calculate f 1,k , f 2,k , and f 3,k .
We determine each entry of f 1,k from the number of times that the associated punctuation mark appears in a document, relative to the total number of punctuation marks
in a document:
|{θlk ∈ Dk | θlk = θi }|
fi1,k =
.
(2.1)
nk
The feature f 1,k induces a discrete probability distribution on the set of punctuation
P|Θ|
marks for each document in our corpus (i.e., i=1 fi1 = 1 for all k) and is independent
of the order of the punctuation marks. For the Le Guin quote, f 1 is given by


! " ( ) , . : ; ? ...
1
f =
,
0 13 0 0 16 13 0 16 0 0
where the second row indicates the elements of the vector and the first row indicates the
corresponding punctuation marks. (Recall from section 2.1 that we amalgamate opening
and closing double and single quotation marks into a single punctuation mark, so that
entry refers to the appearance of either of those two marks.) An alternative is to consider
the frequency of punctuation marks relative to the number of characters or words in a
document [16].
To compute f 2,k and f 3,k , we consider a categorical Markov chain on the ordered set of

Pull out all the stops

9

punctuation marks and associate each punctuation mark with a state of the Markov chain.
We first need two types of transition matrices. We calculate the matrix P k ∈ [0, 1]|Θ|×|Θ|
from the number of times that elements in an ordered pair of punctuation marks occur
successively in a document, relative to the number of times that the first punctuation
mark in this pair occurs in the document:
Pijk =

k
|{θlk ∈ Dk |θlk = θi and θl+1
= θj }|
,
k
k
|{θl ∈ Dk | θl = θi }|

with

X

Pijk = 1 .

(2.2)

j

e k ∈ [0, 1]|Θ|×|Θ| from the number of times that elements in an
We calculate the matrix P
ordered pair of successive punctuation marks occurs in a document, relative to the total
number of punctuation marks in the document:
Peijk =

k
|{θlk ∈ Dk |θlk = θi and θl+1
= θj }|
,
nk

with

X

Peijk = 1 ;

(2.3)

i,j

and we note that Peijk = Pijk fi1,k .
The transition matrix P k is an estimate of the conditional probability of observing
punctuation mark θj after punctuation mark θi in document k, and the transition matrix
e k is an estimate of the joint probability of observing punctuation marks θi and θj in
P
succession in document k. The relationship Peijk = Pijk fi1,k ensures that rare (respectively,
frequent) events are given less (respectively, more) weight in Pe than in P . For example, if
an author seldom uses the ellipsis “...” in a document, the few ways in which it was used
(which, arguably, are not representative of authorial style) are assigned high probabilities
in P but low probabilities in Pe . For the Le Guin quote, P and Pe are




! " ( ) , . : ; ? ...
! " ( ) ,
. : ; ? ...








0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 








0 31 0 0 13 13 0 0 0 0 
0 19 0 0 91 19 0 0 0 0 




0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 




0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 
e




P =
1
1
 , P = 0 1 0 0 0 1 0 0 0 0  ,

0 2 0 0 0 2 0 0 0 0 

12
12
0 1 0 0 0 1 0 1 0 0 
0 1 0 0 0 1 0 1 0 0 




6
12
0 04 0 0 0 02 0 04 0 0 
0 12
0 0 0 0 0 0 0 0 0








1
1
0 0 0 0 0 12
0 0
0 12 0 0 0 0 0 12 0 0 
0 12




0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0
where the first row of each matrix indicates the corresponding punctuation mark. Observe
that Pe56 < Pe66 , even though these entries are equal in P , because two successive periods
occur more frequently than a period followed by a comma in Le Guin’s quote.
We obtain f 2 and f 3 by “flattening” (i.e., concatenating the columns of) the matrices
k
P k and Pe , respectively. The feature f 3 induces a joint probability distribution on the
space of ordered punctuation pairs. In contrast to f 1,k , the features f 2 and f 3 depend on
the order in which punctuation marks occur in a document. As we will see in section 3,
the feature f 3 is very effective at distinguishing different authors. We account for order

10

A. N. M. Darmon et al.

with a one-step lag in f 2,k and f 3,k (i.e., each state depends only on the previous state).
One can generalize these features to account for memory or “long-range correlations” [30]
(e.g., the probability of closing a parenthesis increases after it has been opened).
The features f 4,k , f 5,k , and f 6,k account for the number of words that occur between punctuation marks. Let Dkw = {w0k , w1k , . . . , wnk k −1 } denote the number of words
that occur between successive punctuation marks in Dk , with w0k equal to the number
of words before the first punctuation mark. Therefore, w1k is the number of words between punctuation marks θ1k and θ2k , and so on. The set Dkw for Le Guin’s comment is
{25, 6, 9, 2, 9, 7, 5, 1, 0, 4, 1, 0}, where we count “don’t” as two words and we also count
“half-assed” as two words. The minimum number of words that can occur between successive punctuation marks is 0, and we cap the maximum number of words that can
occur between successive punctuation marks at ns = 40 and the number of words in a
sentence at nS = 200. (These caps are exceeded fewer than 0.05% of the time in our
corpus.)
The entries of the feature f 4,k ∈ [0, 1]nS ×1 , which quantifies the frequency of sentence
lengths, are
fi4,k =

|{wlk ∈ Dkw | wlk = i and θl , θl+1 ∈ {. , ... , ! , ?} }|
.
nk

(2.4)

In the Le Guin quote, there are four sentences, with lengths 31, 9, 2, and 27 (in sequential
order). The feature f 4,k , an nS × 1 vector with nS = 200, thus has the value 1/4 in the
9th , 2nd , 27th , and 31st positions and the value 0 in all other entries. One can also consider
other measures of sentence length (e.g., the number of characters, instead of the number
of words) [47].
The entries of the feature f 5,k ∈ [0, 1]ns ×1 , which quantifies the frequency of the
number of words between successive punctuation marks, are
fi5,k =

|{wlk ∈ Dkw | wlk = i}|
.
nk

(2.5)

In the Le Guin quote, recall that Dkw = {25, 6, 9, 2, 9, 7, 5, 1, 0, 4, 1, 0} (which includes
9 unique integers), so the ns × 1 vector (with ns = 40, as mentioned above) f 5 has 9
nonzero entries. For example, f 51 = 1/12 (because 0 occurs twice out of nk = 12 total
punctuation marks) and f 54 = 0 (because 3 never occurs out of nk = 12 possible times).
The features f 4,k and f 5,k induce discrete probability distributions on the number
of words in sentences and the number of words between successive punctuation marks,
respectively. The expectation of the feature f 5,k quantifies the “rate of punctuation” and
is equal to the total number of words, relative to the total number of punctuation marks:
ns
ns
X
 5,k  X
1
|Dkw |
5,k
×
i × {wlk ∈ Dkw | wlk = i} =
.
E f
=
i × fi =
nk i=0
nk
i=0

(2.6)

The feature f 5,k tracks word-count frequency between successive punctuation marks,
without distinguishing between different punctuation marks.
With f 6,k , we compute the mean number of words between successive occurrences of
the elements in ordered pairs of punctuation marks using a matrix W k ∈ [0, ns ]|Θ|×|Θ|

Pull out all the stops

11

with entries
Wijk = h{wlk ∈ Dkw | θl = θi and θl+1 = θj }i ,
where h · i denotes the sample mean of a

! " ( )


0 0 0 0


0 4 0 0

0 0 0 0

0 0 0 0
W =
0 0 0 0

0 0 0 0

0 0 0 0


0 5 0 0

0 0 0 0
0 0 0 0

(2.7)

set. The matrix for the Le Guin excerpt is

,
.
: ; ? ...


0 0 0 0 0 0


1 1 0 0 0 0

0 0 0 0 0 0

0 0 0 0 0 0
.
0 0 0 0 0 0

0 5.5 0 9 0 0 

0 0 0 0 0 0


0 0 0 7 0 0

0 0 0 0 0 0
0 0 0 0 0 0

We obtain f 6,k by flattening the matrix W k by concatenating its columns. In variants
of this feature, one need not require that punctuation-mark occurrences are successive,
and one can subsequently compute the number of words or even the number of (other)
punctuation marks between the elements of an ordered pair of punctuation marks.
In the rest of our paper, we focus on the six features f 1,k , . . . , f 6,k . Where appropriate,
we suppress the superscript k (which indexes the document for which we compute a
feature) from f i,k for ease of writing. We show example histograms of f 1 (punctuation
frequency) and f 5 (mean number of words between successive punctuation marks) for
some documents by the same authors in fig. 3.

2.3 Kullback–Leibler divergence
To quantify the similarity between two discrete distributions (e.g., between the features f 1 , f 3 , f 4 , and f 5 from different documents), we use Kullback–Leibler (KL) divergence [25], an information-theoretic measure related to Shannon entropy and ideas
from maximum-likelihood theory. KL divergence and variants of it have been used in
prior research on author recognition [2, 35, 50]. One can also consider other similarity
measures, such as chi-squared distance [35] and Jensen–Shannon divergence [1, 15, 31].
Consider a random variable X with discrete finite support x ∈ X , and let p ∈ [0, 1]|X |×1
and q ∈ [0, 1]|X |×1 be two probability distributions for X which we assume are absolutely
continuous with respect to each other. Broadly speaking, KL divergence quantifies how
close a probability distribution p = {pi } is to a candidate distribution q = {qi }, where pi
(respectively, qi ) denotes the probability that X takes the value i when it is distributed
according to p (respectively, q) [10]. The KL divergence between the probability distributions p and q is defined as
dKL (p, q) =

|X |
X
i=1


pi log

pi
qi


(2.8)

12

A. N. M. Darmon et al.

and satisfies four important properties:
(1) dKL (p, q) > 0 ;
(2) dKL (p, q) = 0 if and only if pi = qi for all i ;
(3) dKL (. , .) is asymmetric in its arguments; and
(4) dKL (p, q) = H(p, q) − H(p) ,
P
where H(p) = i pi log pi denotes the Shannon entropy of p and H(p, q) denotes the
Shannon entropy of the joint distribution of p and q [28, 42]. Entropy quantifies the
“unevenness” of a probability distribution. It represents the mean information that is
required to specify an outcome of a random variable given its probability distribution. It
achieves its minimum value 0 for a constant random variable (e.g., p1 = 1 and pi = 0 for
i 6= 1) and its maximum value log(|X |) for a uniform distribution. In some sense, dKL (p, q)
measures the “unevenness” of the joint distribution of p and q relative to the distribution
of p. One can also derive KL divergence from likelihood theory. In particular, one can
show that, as the number of samples from the discrete random variable X tends to infinity,
KL divergence measures the mean likelihood of observing data with the distribution p if
the distribution q actually generated the data [11, 43].
To adjust for cases in which p and q are not absolutely continuous with respect to each
other (e.g., one document has one or more ellipses, but another does not, resulting in unequal supports), we remove any frequency component that corresponds to a punctuation
mark that is not in the common support and then distribute the weight of the removed
frequency uniformly across the other frequencies. For example, suppose that p1 6= 0 but
q1 = 0. We then define pe such that pe = {pi /(1 − p1 ) , i 6= 1} and compute dKL (e
p, q).

2.4 Classification models
We describe the two classification approaches that we use for author recognition (see section 3.2) and genre recognition (see section 4.2). Much of the existing classification work
on author recognition uses machine-learning classifiers (e.g., support vector machines
or neural networks) or similarity-based classification techniques (e.g., using KL divergence) [35,44]. We use neural networks and similarity-based classification with KL divergence for both author and genre classification. Following standard practice, we split the
n documents in our data set into into a training set and a testing set. Broadly speaking, a
training set calibrates a classification model (e.g., to “feed” a neural network and adjust
its parameters), and one then uses a testing set to evaluate the accuracy of a calibrated
model. We ensure that all authors or genres (i.e., all “classes”) that appear in the testing
set also appear in the training corpus; this is known as “closed-set attribution” and is
common practice in author recognition [35, 44]. We also ensure that each class appears
in the two sets in equal proportion. In particular, we follow standard practice and place
80% of the documents of each class in the training set and the remaining 20% of the
documents of each class in the testing set.

Pull out all the stops

13

2.4.1 Similarity-based classification
We label our p classes by c1 , c2 , . . . , cp (recall that these can correspond to authors or
genres), and we denote the set of training documents for class cj by Dj . For each class cj ,
we define a class-level feature f l,cj , with l ∈ {1, . . . , 6} and j ∈ {1, . . . , p}, by averaging
the features across the training documents in that class. That is, the ith entry of f l,cj is
1 X l,k
l,c
fi j =
(2.9)
fi ,
|Dj |
k∈Dj

where l ∈ {1, . . . , 6} and we use the features f 1,k , f 2,k , . . . , f 6,k from section 2.2. This
yields a set φk = {f 1,k , . . . , f 6,k } of features for each document and a set φcj =
{f 1,cj , . . . , f 6,cj } of features for each class.
To determine which class is “most similar” to a document k in our testing set, we solve
the following minimization problem:
argminj∈{1,...,p} d(φk , φcj ) ,

(2.10)

for some choice of similarity measure d( . , . ). In our numerical experiments of section 3,
we use the KL-divergence similarity measure dKL to define d( . , . ) as
d(φk , φcj ) = argminl∈L dKL (f l,cj , f l,k ) ,

(2.11)

where we restrict the set of features to those that induce discrete probability distributions
and consider each feature individually (i.e., L = {1}, L = {3}, L = {4}, or L = {5}).
2.4.2 Neural networks
We use feedforward neural networks with the standard backpropagation algorithm as
a machine-learning classifier [24]. A neural network uses the features of a training set
to automatically infer rules for recognizing the classes of a testing set by adjusting the
weights of each neuron using a stochastic gradient-descent-based learning algorithm. In
contrast with neural networks for classical NLP classification, where it is standard to use
word embeddings and employ convolutional or recurrent neural networks [26] to ensure
that input vectors have equal lengths, we have already defined our features such that
they have equal length. It thus suffices for us to use feedforward neural networks. The
input vector that corresponds to each document is a concatenation of the six features
(or a subset thereof) in section 2.2, and the output is a probability vector, which one
can interpret as the likelihood that a given document belongs to a given class. We assign
each document in our testing set to the class with highest probability.
2.5 Model evaluation
We report two quantities for each test of our classification model (where each test corresponds to a different random sample of the training and testing sets): (1) the accuracy on
the testing set, which we measure as the ratio of correctly assigned documents relative to
the total number of documents in the testing set; and (2) a baseline accuracy, which we
measure by assigning each document in the testing set to each class with a probability
that is proportional to the class’s size in the training set.

14

A. N. M. Darmon et al.

(a) Sharing Her
Crime, M. A.
Fleming

(b) The Actress’
Daughter, M. A.
Fleming

(e) The History of
Mr. Polly, H. G.
Wells

(c) King Lear, W.
Shakespeare

(d) Hamlet, W.
Shakespeare

(f) The Wheels of
Chance, H. G. Wells

Figure 4. Ordered sequences of successive punctuation marks that we extract from documents by (a,b) May Agnes Fleming, (c,d) William Shakespeare, and (e,f) Herbert George
Wells. We map each punctuation mark to a distinct colour. We cap the length of the
punctuation sequence at 3000 entries, which start at the midpoint of the punctuation
sequence of the corresponding document.
3 Case study: Author analysis
"It is almost always a greater pleasure to come across a semicolon than a
period. The period tells you that that is that; if you didn’t get all the
meaning you wanted or expected, anyway you got all the writer intended to
parcel out and now you have to move along. But with a semicolon there you get
a pleasant little feeling of expectancy; there is more to come; to read on;
it will get clearer."
— Thomas Lewis, Notes on Punctuation, 1979
3.1 Consistency
We explore punctuation sequences of a few authors to gain some insight into whether
certain authors have more distinguishable punctuation styles than others. In fig. 4, we
show (augmenting fig. 1) raw sequences of punctuation marks for two books by each of the
following three authors: May Agnes Fleming, William Shakespeare, and Herbert George
(H. G.) Wells. We observe for this document sample that, visually, one can correctly

Pull out all the stops

15

(a) Sharing Her Crime
and The Actress’
Daughter

(b) The History of Mr.
Polly and The Wheels of
Chance

(c) King Lear and
Hamlet

(d) The Actress’
Daughter and King Lear

(e) Hamlet and The
History of Mr.Polly

(f) The Wheels of
Chance and Sharing Her
Crime

Figure 5. Scatter plots of frequency vectors of punctuation marks to compare books
from the same author: (a) Sharing Her Crime and The Actress’ Daughter by May Agnes
Fleming, (c) King Lear and Hamlet by William Shakespeare, and (e) The History of Mr.
Polly and The Wheels of Chance by H. G. Wells. Scatter plots of frequency vectors of
punctuation marks to compare books from different authors: (b) The Actress’ Daughter
and King Lear, (d) Hamlet and The History of Mr. Polly, and (f) The Wheels of Chance
and Sharing Her Crime. We represent each punctuation mark by a coloured marker,
with coordinates given by the punctuation frequencies in a vector associated to each
document. The grey line represents the identity function. More similar frequency vectors
correspond to dots that are closer to the grey line.

guess which documents were written by the same author based only on the sequences of
punctuation marks. This striking possibility was illustrated previously in A. J. Calhoun’s
blog entry [4] (which motivated our research). From fig. 4, we see that Wells appears to
use noticeably more quotation marks than the other two authors. We also observe that
Shakespeare appears to use more periods than Wells. These observations are consistent
with the histograms in fig. 3 (where we also observe that Shakespeare appears to use more
exclamation marks and question marks than Wells), which we compute from the entire
documents, so our observations from the samples in fig. 4 appear to hold throughout
those documents.

16

A. N. M. Darmon et al.
20

20

40

40

60

60

20

20

40

40
60

80

80

60

100

100

80

80

120

120
100

100

120

120

140

140

160

160

180

180

200

200
50

100

150

200

1

(a) f : 10 authors

140

140

50

100

150

200

3

(b) f : 10 authors

20

40

60

80

100

120

140

20

4

(c) f : 10 authors

100

100

100

200

200

200

200

300

300

300

300

400

400

400

400

500

500

500

500

600

600

600

600

700

700

700

700

800

800

800

900

900

900

200

1

300

400

500

600

700

800

900

(e) f : 50 authors

100

200

3

300

400

500

600

700

800

900

(f) f : 50 authors

60

80

100

120

140

(d) f : 10 authors

100

100

40

5

800
900
100

200

4

300

400

500

600

700

800

900

(g) f : 50 authors

100

200

300

400

500

600

700

800

900

5

(h) f : 50 authors

Figure 6. Heat maps showing KL divergence between the features (a,e) f 1 , (b,f) f 3 , (c,g)
f 4 , and (d,h) f 5 for different sets of authors. We show the ten most-consistent (see the
main text for our notion of “consistency”) authors for each feature in the top row and
the fifty most-consistent authors for each feature in the bottom row. The diagonal blocks
enclosed in black indicate documents by the same author. Authors can differ across panels, because author consistency can differ across features. The colours scale (nonlinearly)
from dark blue (corresponding to a KL divergence of 0) to dark red (corresponding to
the maximum value of KL divergence in the underlying matrix). For ease of exposition,
we suppress colour bars (they span the interval [0, 3.35]), given that the purpose of this
figure is to illustrate the presence and/or absence of high-level structure. The pronounced
dark-red stripes seem to correspond to documents that consist primarily of tables (e.g.,
the second author in panel (c)).
In fig. 5, we plot examples of the punctuation frequency (i.e., f 1 ) of one document
versus that of another written by the same author (top row) and by a different author
(bottom row). We base these plots on the “rank order” plots in [49], who used them
to illustrate the top-ranking words in various texts. In our plots, any punctuation mark
(which we represent by a coloured marker) that has the same frequency in both documents lies on the grey diagonal line. Any marker above (respectively, below) the grey
line signifies that it is used more (respectively, less) frequently by the author on the
vertical axis (respectively, horizontal axis). In these examples, we see for documents by
the same author that the markers tend to be closer to the grey line than for documents
by different authors. In fig. 5(d), for example, we observe that Fleming used more quotation marks and commas in The Actress’ Daughter than Shakespeare did in King Lear,
whereas Shakespeare used more periods in King Lear than Fleming did in The Actress’
Daughter. One can make similar observations about panels (e) and (f) of fig. 5. These
observations are consistent with those of fig. 3 and fig. 4.
Our illustrations in fig. 4 and fig. 5 use a very small number of documents by only a
few authors. To try and quantify the “consistency” of an author across all documents by
that author in our corpus, we use KL divergence.

Pull out all the stops

(a) feature f 1

(b) feature f 3

(c) feature f 4

17

(d) feature f 5

Figure 7. Author consistency plot. In each panel, we show author consistency (3.1) for (a)
f 1 , (b) f 3 , (c) f 4 , and (d) f 5 using a solid black curve. We plot the standard deviation
of KL divergence across pairs of documents for each author in grey. The dotted blue line
indicates a consistency baseline, which we obtain by choosing, uniformly at random, 1000
ordered pairs of documents by distinct authors and computing the mean KL divergence
between the features of these document pairs.
In fig. 6, we show heat maps of KL divergence between discrete probability distributions
induced by the feature vectors f 1 , f 3 , f 4 , and f 5 . We define the “consistency” of an
author relative to a feature as the mean KL divergence for that feature computed across
all pairs of documents by that author. That is,
X
0
2
Cf i (a) =
dKL (f i,k , f i,k ) ,
(3.1)
|Da − 1||Da | 0
k,k ∈Da

where a denotes an author in our corpus and Da is the set of documents by author a. For
each feature in fig. 6, we show the 10 (respectively, 50) most-consistent authors in the
top row (respectively, bottom row). Diagonal blocks with black outlines correspond to
documents by the same author. Although there appears to be greater similarity within
diagonal blocks than between them for several of the authors, the heat maps are difficult
to interpret.
In fig. 7, we show author consistency in our entire corpus for the feature vectors f 1 ,
3
f , f 4 , and f 5 . In each panel, we show a baseline (in blue), which we obtain by choosing,
uniformly at random, 1000 ordered pairs of documents by distinct authors and computing
the mean KL divergence between the features of these document pairs. (One pair is a
single element of an off-diagonal block of a matrix like fig. 6.)
We order each panel from the least-consistent author to the most-consistent author.
Authors can differ across panels, because consistency (3.1) is a feature-dependent quantity. We observe in all panels of fig. 7 that most authors are more consistent on average
that the baseline. (In other words, the black curve lies below the blue horizontal line for
most authors.) We observe that differences between authors relative to the baseline is
most pronounced for the feature f 3 (see table 1).

3.2 Author recognition
We use the classification techniques from section 2.4 to perform author recognition. We
show our results using KL divergence (see section 2.4.1) in table 2 and using neural
networks (see section 2.4.2) in table 3. In each table, we specify the number of authors

18

A. N. M. Darmon et al.

(“No. authors”), the number of documents in the training set (“Training size”), the
number of documents in the testing set (“Testing size”), the accuracy of the test using
various sets of features, and the baseline accuracy (as defined in section 2.5). Each row
in a table corresponds to an experiment on a set of distinct authors, which we choose
uniformly at random. (This is equal to the entire corpus when the number of authors is
651.) For a given number of authors, we use the same sample across both tables to allow
a fair comparison.
We show classification results using KL divergence in table 2 using each individual
frequency feature vector as input. As we consider more authors, the accuracy on the
testing set tends to decrease significantly. The issue of developing a method that scales
well as one increases the number of authors is an open problem in author recognition
even when using words from text [35], and we are exploring stylistic signatures from
punctuation only, a much smaller set of information. Remarkably, we are able to achieve
an accuracy of 66% on a sample of 50 authors using only the feature f 3 . This is consistent
with the plots in fig. 7, where f 3 gives the best improvement from the baseline.
We show classification results using a one-layer neural network with 2000 neurons
in table 3 using various sets of input vectors (which, contrary to when one uses KL
divergence, need not be feature vectors that induce probability distributions). We also
observe in table 3 that accuracy on the testing set tends to decrease significantly as one
increases the number of authors. Overall, however, the neural network outperforms our
KL divergence-based classification. (Recall that, once we specify the number of authors,
we use the same document sample with both classifiers to allow a fair comparison.) We
achieve an accuracy of 62% when using only f 3 and an accuracy of 72% when using
all feature vectors on a sample of 651 authors (i.e., on the entire corpus). Interestingly,
in some of our experiments, using the features {f 1 , f 3 , f 4 , f 5 } gives slightly better
accuracy than using all features.
Based on preliminary experiments, our accuracy results in table 2 and table 3 seem to
be robust to (1) different author samples of the same size and (2) different training and
testing samples for a given author sample. However, the heterogeneity in accuracy across
different author samples of the same size is more pronounced than the heterogeneity
that we observe from different training and testing samples for a given author sample, as
different author samples can sometimes yield significantly different training and testing
set sizes (see fig. 2). Such heterogeneity across different author samples decreases as one
increases the number of authors.
To our knowledge, most attempts thus far at author recognition of literary documents
have used data sets that are of significantly smaller scale than our corpus [14, 35]. One
recent example of author analysis from a corpus extracted from Project Gutenberg is the
one in Qian et al. [39]. Their corpus consists of 50 authors (with their choices of authors
based on a popularity criterion) and 900 single-paragraph excerpts for each author. (They
extracted their excerpts from several books by a corresponding author.) Using word-based
features and machine-learning classifiers, they achieved an accuracy of 89.2% using 90%
of their data for training and 10% of it for testing.

Pull out all the stops
19
Table 2. Results of our author-recognition experiments using a KL divergence-based classification (see section 2.4.1) for author samples of various sizes and using the individual
features f 1 , f 3 , f 4 , and f 5 as input.
No. authors Training size Testing size

10
50
100
200
400

216
834
2006
3549
7439

55
209
502
888
1860

Accuracy on the testing set
f1
0.69
0.54
0.37
0.30
0.27

f3
0.74
0.66
0.49
0.47
0.41

f4
0.52
0.30
0.25
0.16
0.15

f5
0.63
0.31
0.23
0.20
0.16

baseline
0.21
0.029
0.019
0.0079
0.0047

Table 3. Results of our author-recognition experiments using a one-layer, 2000-neuron
neural network (see section 2.4.2) for author samples of various sizes and using different
features or sets of features as input: f 1 , f 3 , f 4 , f 5 , {f 1 , f 3 , f 4 , f 5 }, and {f 1 , f 2 , f 3 ,
f 4 , f 5 , f 6 } (denoted “all”).
No. authors Training size Testing size

10
50
100
200
400
600
651

216
834
2006
3549
7439
11102
11957

55
209
502
888
1860
2776
2990

Accuracy on testing set
f1
0.89
0.65
0.55
0.46
0.39
0.37
0.36

f3
0.93
0.81
0.79
0.71
0.70
0.70
0.62

f4
0.64
0.44
0.37
0.23
0.23
0.21
0.20

f 5 {f 1 ,
0.80
0.49
0.39
0.32
0.27
0.25
0.23

f 3, f 4, f 5}
0.89
0.81
0.79
0.71
0.71
0.61
0.67

all
0.87
0.82
0.80
0.75
0.73
0.74
0.72

baseline
0.21
0.029
0.019
0.0079
0.0047
0.0029
0.0024

4 Case study: Genre analysis
"Cut out all those exclamation marks. An exclamation mark is like laughing
at your own jokes."
— Attributed to F. Scott Fitzgerald, as conveyed by Sheilah
Graham and Gerold Frank in Beloved Infidel: The Education of a Woman, 1958
"‘Multiple exclamation marks,’ he went on, shaking his head, ‘are a sure sign
of a diseased mind.’"
— Terry Pratchett, Eric, 1990
We now use genres as our classes. Among the 121 genre (“bookshelf”) labels available
in Gutenberg4 , we keep those that include at least 10 documents. Among the remaining
genres, we select 32 relatively unspecialized genre labels. We show the resulting list of
genres in appendix A. This yields a data set with 2413 documents.
4
Every document in our corpus has at most one genre, but most documents are not assigned
a genre.

20

A. N. M. Darmon et al.

(a) feature f 1

(b) feature f 3

(c) feature f 4

(d) feature f 5

Figure 8. Genre consistency plot. In each panel, we show genre consistency (i.e., replace
authors by genre in equation (3.1)) for (a) f 1 , (b) f 3 , (c) f 4 , and (d) f 5 as a solid
black curve. We show the standard deviation of KL divergence across pairs of documents
for each genre in grey. The dotted blue line indicates a consistency baseline, which we
obtain by choosing, uniformly at random, 1000 ordered pairs of documents corresponding
to distinct genres and computing the mean KL divergence between the features of these
document pairs.
Table 4. Results of our genre-recognition experiments using a one-layer, 2000-neuron
neural network (see section 2.4.2) using different features or sets of features as input: f 1 ,
f 3 , f 4 , f 5 , {f 1 , f 3 , f 4 , f 5 }, and {f 1 , f 2 , f 3 , f 4 , f 5 , f 6 } (denoted “all”).
No. genres Training size Testing size

32

1930

483

Accuracy on testing set
f 1 f 3 f 4 f 5 {f 1 , f 3 , f 4 , f 5 } all baseline
0.56 0.65 0.37 0.40
0.61
0.64 0.094

4.1 Consistency
In fig. 8, we show consistency plots (of the same type as in fig. 7), but we now use
genres (instead of authors) as our classes. We observe that the KL-divergence consistency
relative to the baseline is less pronounced for genres that it was for authors. Nevertheless,
most genres are more consistent than the baseline, and the frequency feature vector
f 3 once again appears to be the most helpful of our features for evaluating a genre’s
punctuation style.

4.2 Genre recognition
We perform genre recognition using neural networks and show our results in table 4.
We are less successful at genre detection than we were at author detection. This is
consistent with our genre consistency plots (see fig. 8) indicating less differentiation from
the baseline than in our author consistency plots (see fig. 7). Our highest accuracy for
genre recognition is 65%, and we achieve it when using only the feature f 3 as input.
These observations are robust to different samples of the training and testing sets.

Pull out all the stops

21

5 Case study: Temporal analysis
"Whatever it is that you know, or that you don’t know, tell me about it. We
can exchange tirades. The comma is my favorite piece of punctuation and I’ve
got all night."
— Rasmenia Massoud, Human Detritus, 2011
"Who gives a @!#?@! about an Oxford comma?
I’ve seen those English dramas too
They’re cruel"
— Vampire Weekend, Oxford Comma, 2008
We perform experiments to get preliminary insight into how punctuation has changed
over time. In our corpus, we have access to the birth year and death year of 614 and 615
authors, respectively, of the 651 total authors. We have both the birth and death years
for 607 authors. In fig. 9, we show the distribution of the number of documents by author
birth year, death year, and “middle year”. (See the caption of fig. 9 for a definition of
middle year.) We restrict our analysis to authors with a middle year between 1500 and
2012. Of the authors that have either a birth year or a death year, 616 of them have a
middle year between 1500 and 2012. We show the evolution of punctuation marks over
time for these 616 authors in fig. 10 and fig. 11, and we examine specific over time
in fig. 12. Based on our experiments, it appears from fig. 10 that the use of quotation
marks and periods has increased over time (at least in our corpus), but that the use of
commas has decreased over time. Less noticeably, the use of semicolons has also decreased
over time. In fig. 12, we observe that the punctuation rate (as measured by formula (2.6))
has also decreased over time in our corpus. Because of our relatively small number of
documents per author and the uneven distribution of documents in time, our experiments
in fig. 12 give only preliminary insights into the temporal evolution of punctuation, which
merits a thorough analysis with a much larger (and more appropriately sampled) corpus.
Nevertheless, this case study illustrates the potential for studying the temporal evolution
of punctuation styles of authors, genres, and literature (and other text) more generally.

22

A. N. M. Darmon et al.

(a) Birth year and death year

(b) Birth year, death year, and
middle year

Figure 9. Distribution of author dates over time in our corpus. The bars represent the
number of documents by author birth year (blue) and death year (grey) split into bins,
where each bin represents a ten-year period. (We start at 1500.) For ease of visualization,
we only show documents for authors born in 1500 or later. Only six of our authors for
whom we have birth years were born before 1500. We determine the “middle year” of an
author by taking the mean of the birth year and the death year if they are both available.
If we know only the birth year, we assume that the middle year of an author is 30 years
after the birth year; if we know only the death year, we assume that the middle year is
30 years prior to the death year.

Pull out all the stops

23

(a) Punctuation marks over time

(b) “ " ”, “ . ”, “ , ”

(c) “ ! ”, “ ; ”

(d) “ ( ”, “ ) ”, “ : ”, “ ? ”,
“ ... ”

Figure 10. Mean frequency of punctuation marks versus the middle years of authors. We
bin middle years into ten-year periods, starting at 1700. In (a), we show the temporal
evolution of all punctuation marks. For clarity, we also separately plot (b) the three
punctuation marks with the largest frequencies in the final year of our data set, (c) the
next two most-frequent punctuation marks, and (d) the remaining punctuation marks.

24

A. N. M. Darmon et al.

Figure 11. Temporal evolution
h
i of mean number of words between two consecutive punctuation marks (i.e., E f 5,k from formula (2.6)) versus author middle years, which we
bin into ten-year periods starting at 1700. This reflects how the punctuation rate in our
corpus has changed over time.

(a) H. G. Wells over time

(b) A. M. Fleming over time

(c) C. Dickens over time

h
i
Figure 12. Mean frequency of punctuation marks (as encapsulated by E f 5,k from
formula (2.6)) versus publication date for works by (a) Herbert George Wells, (b) Agnes
May Fleming, and (c) Charles Dickens.

Pull out all the stops

25

6 Conclusions
"La punteggiatura è come l’elettroencefalogramma di un cervello che sogna —
non dà le immagini ma rivela il ritmo del flusso sottostante."
— Andrea Moro, Il Segreto di Pietramala, 2018

We have explored whether punctuation is a sufficiently rich stylistic feature to distinguish between authors and genres, and we have also examined how it has evolved
over time. Using a large corpus of documents from Project Gutenberg, we observed that
simple punctuation-based quantitative features (which account for both frequency and
order) can distinguish accurately between the styles of different authors. These features
can also help distinguish between genres, although less successfully than for authors. In
preliminary explorations, we also observed temporal changes in punctuation style across
time, but it is necessary to conduct more thorough analyses of temporal usage patterns.
To assess whether our observations extend beyond our Gutenberg corpus, it is necessary
to conduct further experiments (e.g., on a larger corpus, across different e-book sources,
and so on). For example, it is desirable to repeat our analysis using the “Text data” level
of granularity in the recently introduced (while we were in the final stages of writing this
article) Standardized Project Gutenberg Corpus [14].
Our framework allows the exploration of numerous other fascinating ideas. For example, we expect it to be fruitful to examine higher-order categorical Markov chains when
accounting for punctuation order. Additionally, we look forward to extensions of our work
that explore other features, such as the number of words between elements in ordered
pairs of punctuation marks (even when they are not successive) and different ways of
measuring punctuation frequency [16] and sentence length [47]), and try to quantify how
large a sample of a document is necessary to correctly identify its features of punctuation
style. If it is sufficiently small, it may even be possible to identify punctuation style from
collections of short text (such as tweets from politicians with limited coherence). It is
also likely to be useful to exploit more sophisticated machine-learning classifiers that can
take raw punctuation sequences (rather than features produced from them) as input and
exploit “long-range correlations” [30] between punctuation marks.
Building on our analysis, it will be interesting to investigate other aspects of stylometry — such as author pacing or the influence on an author of gender, culture, other
demographics, local history, or other aspects of humanity — and to compare the results
of punctuation-based stylometry with existing (word-based) approaches in NLP on the
same tasks. Further investigations of a punctuation-based approach to stylometry also
provide an opportunity to apply other methods for analyzing categorical time series (e.g.,
an extension of rough-path signatures to categorical time series [8, 32]).
On a more general front, relevant stylometric applications include analysis of stylistic
differences in punctuation between politicians from different political parties [5] and in
comparisons between different editions of the same book. It would also be interesting
to explore the effects of an editor’s or journal’s style on documents by a given author,
as well as the effects of a translator’s style on documents. We envisage that the latter
application is particularly well-suited to punctuation-based stylometry, as punctuation

26

A. N. M. Darmon et al.

marks depend far less than words on the specific choice of language. We also expect there
to be commercial applications (e.g., using online data sources) of time-series analysis of
symbols without the use of words.
Acknowledgements
The original inspiration for this project was Adam Calhoun’s blog entry [4] and its
striking visualizations of punctuation sequences. We thank Terry Lyons, Ursula Martin,
Stephen Pulman, Massimo Stella, Adam Tsakalidis, and Bo Wang for helpful comments.
Other attendees at SDH’s 60th birthday workshop (see https://www.maths.ox.ac.uk/
groups/mathematical-finance/sam-howisons-60th-birthday-workshop-2018) also
made helpful comments. MAP and SDH thank their students for putting up with many
long discussions about punctuation, when they perhaps should have been discussing other
elements of their scholarship. (It was inevitable that we would eventually write an article
like this.) MAP thanks SDH for his collaboration and friendship, and he wishes him
a very happy birthday, filled with British spelling, the word “which” (and occasionally
“that”), and minimal commas (and parenthetical remarks).
Appendix A Genre list
"Mr Speaker, I said the honourable Member was a liar it is true and I am sorry
for it. The honourable Member may place the punctuation where he pleases."
— Attributed to Richard Brinsley Sheridan (1751–1816), responding to a rebuke from the Chair for calling a fellow Member of Parliament a liar.

In table A 1, we list the genres that we use in our analysis of genres.

Pull out all the stops
Table A 1. The genres that we use in our analysis of genres.
Science Fiction
Children’s Book Series
Historical Fiction
Children’s Fiction
Bestsellers, American
1895–1923
Children’s Literature
US Civil War
Humor
Best Books Ever Listings
Western
Christianity
Fantasy
World War I
Classical Antiquity
Children’s Instructional Books
World War II
Horror

27

Philosophy
Adventure
Christmas
Children’s Picture Books
Harvard Classics
Movie Books
School Stories
One Act Plays
Crime Fiction
Children’s History
Poetry
Crime Nonfiction
Animal
Travel
Biology
Precursors of Science Fiction
Art
Detective Fiction

References
[1] E. G. Altmann, L. Dias, and M. Gerlach, Generalized entropies and the similarity of
texts, Journal of Statistical Mechanics Theory and Experiment, 1 (2017), p. 014002.
[2] R. Arun, V. Suresh, and C. E. V. Madhavan, Stopword graphs and authorship attribution in text corpora, in Proceedings of the IEEE International Conference on Semantic
Computing, 2009, pp. 192–196.
[3] A. J. Calhoun, Punctuation code, 2016. Available at https://github.com/adamjcalhoun/
punctuation.
[4]
, Punctuation in novels, 2016. Available at https://medium.com/~@neuroecology/
punctuation-in-novels-8f316d542ec4#.brev0b3w1.
[5]
,
What
does
punctuation
tell
us
about
Republicans
and
Democrats?,
2016.
Available
at
https://medium.com/@neuroecology/
what-does-punctuation-tell-us-about-republicans-and-democrats-bd46b9f98220.
[6] F. Can and J. M. Patton, Change of writing style with time, Computers and the Humanites, 38 (2004), pp. 61–82.
[7] C. E. Chaski, Empirical evaluation of language-based author identification techniques,
Forensic Linguistics, 8 (2001), pp. 1–65.
[8] I. Chevyreva and A. Kormilitzin, A primer on the signature method in machine learning, arXiv:1603.03788, (2016).
[9] H. Chiang, Y. Ge, and C. Wu, Classification of book genres by cover and title, 2015.
Class report, Computer Science 229, Stanford University, available at http://cs229.
stanford.edu/proj2015/127_report.pdf.
[10] T. M. Cover and J. A. Thomas, Elements of Information Theory, Wiley, New York City,
NY, USA, 1991.
[11] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classification, Wiley and Sons, New
York City, NY, USA, 2001.
[12] R. S. Forsyth, Stylochronometry with substrings, or: A poet young and old, Literary and
Linguistic Computing, 14 (1999), pp. 467–477.
[13] H. W. Fowler and F. G. Fowler, The King’s English, Oxford University Press, 1906.
[14] M. Gerlach and F. Font-Clos, A standardized Project Gutenberg corpus for statistical
analysis of natural language and quantitative linguistics, arXiv:1812.08092, (2018).

28

A. N. M. Darmon et al.

[15] M. Gerlach, F. Font-Clos, and E. G. Altmann, Similarity of symbol frequency distributions with heavy tails, Physical Review X, 6 (2016), p. 021009.
[16] J. Grieve, Quantitative authorship attribution: An evaluation of techniques, Literary and
Linguistic Computing, 22 (2007), pp. 251–270.
[17] M. S. Hart, Project Gutenberg, 1971. Available at https://www.gutenberg.org.
[18] C. O. Hartman, Verse: An Introduction to Prosody, Wiley Blackwell, Hoboken, NJ, USA,
2015.
[19] D. I. Holmes, The evolution of stylometry in humanities scholarship, Literary and Linguistic Computing, 50 (1998), pp. 111–117.
[20] M. Honnibal, spaCy, 2017. Available at https://spacy.io.
[21] J. M. Hughes, N. J. Foti, D. C. Krakauer, and D. N. Rockmore, Quantitative
patterns of stylistic influence in the evolution of literature, Proceedings of the National
Academy of Sciences of the United States of America, 109 (2012), pp. 7682–7686.
[22] M. P. Jackson, Pause patterns in Shakespeare’s verse: Canon and chronology, Literary
and Linguistic Computing, 17 (2002), pp. 37–46.
[23] B. Kessler, G. Nunberg, and H. Schutze, Automatic detection of text genre, in Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics
and Eighth Conference of the European Chapter of the Association for Computational
Linguistics, 1996.
[24] B. Kjell, Authorship attribution of text samples using neural networks and bayesian classifiers, in Proceedings of IEEE International Conference on Systems, Man and Cybernetics,
vol. 2, Oct 1994, pp. 1660–1664.
[25] S. Kullback and R. A. Leibler, On information and sufficiency, The Annals of Mathematical Statistics, 22 (1951), pp. 79–86.
[26] S. Lai, L. Xu, K. Liu, and J. Zhao, Recurrent convolutional neural networks for text
classification, in Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI’15, AAAI Press, 2015, pp. 2267–2273.
[27] J. Lawler, Punctuation, The International Encyclopedia of Language and Linguistics,
(2006). Available at https://doi.org/10.1016/B0-08-044854-2/04573-9.
[28] A. Lesne, Mathematical structures in computer science, Shannon entropy: A rigorous notion at the crossroads between probability, information theory, dynamical systems and
statistical physics, 24 (2014), p. e240311.
[29] T. Lewis, ‘Notes on Punctuation’, in The Medusa and the Snail: More Notes of a Biology
Watcher, Viking Press, New York City, NY, USA, 1979.
[30] H. W. Lin and M. Tegmark, Critical behavior from deep dynamics: A hidden dimension
in natural language, arXiv:1606.06737, (2016).
[31] J. Lin, Divergence measures based on the Shannon entropy, IEEE Transactions on Information Theory, 37 (1991), pp. 145–151.
[32] T. Lyons, Rough paths, signatures and the modelling of functions on streams, Proceedings
of the International Congress of Mathematicians 2014, Korea, (2014). Available at http:
//www.icm2014.org/download/Proceedings_Volume_IV.pdf.
[33] T. C. Mendenhall, The characteristic curves of composition, Science, 9 (1887), pp. 237–
249.
[34] F. Mosteller and D. L. Wallace, Inference and Disputed Authorship: The Federalist,
Addison-Wesley, Reading, MA, USA, 1964.
[35] T. Neal, K. Sundararajan, A. Fatima, and D. Woodard, Surveying stylometry techniques and applications, ACM Computing Surveys, 50 (2017), p. 86.
[36] G. Nunberg, The Linguistics of Punctuation, Center for the Study of Language and Information, Stanford, CA, USA, 1990.
[37] M. B. Parkes, ed., Pause and Effect: An Introduction to the History of Punctuation in
the West, University of California Press, Berkeley, CA, USA, 1992.
[38] G. Pullum and R. Huddleston, The Cambridge Grammar of the English Language,
Cambridge University Press, The Other Place, UK, 2001.

Pull out all the stops

29

[39] C. Qian, T. He, and R. Zhang, Deep learning based authorship identification, (2017).
Class report, Computer Science 224, Stanford University, available at https://web.
stanford.edu/class/cs224n/reports/2760185.pdf.
[40] M. Santini, A shallow approach to syntactic feature extraction for genre classification,
in Proceedings of the 7th Annual Colloquium for the UK Special Interest Group for
Computational Linguistics, 2004.
[41]
, State-of-the-art on automatic genre identification, Information Technology Research
Institute (ITRI) Technical Report Series 04-03, University of Brighton, UK, (2004). Available at http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.5.7680.
[42] C. E. Shannon, A mathematical theory of communication, The Bell System Technical
Journal, (1948), pp. 379–423, 623–656.
[43] J. Shlens, Notes on Kullback–Leibler divergence and likelihood theory, arXiv:1404.2000,
(2014).
[44] E. Stamatatos, A survey of modern authorship attribution methods, Journal of the American Society for Information Science and Technology, 60 (2009), pp. 538–556.
[45] C. Stamou, Stylochronometry: Stylistic development, sequence of composition, and relative
dating, Literary and Linguistic Computing, 23 (2008), pp. 181–199.
[46] L. Truss, Eats, Shoots and Leaves: The Zero Tolerance Approach to Punctuation, Profile
Books, London, UK, 2004.
[47] D. S. Vieira, S. Picoli, and R. S. Mendes, Robustness of sentence length measures in
written texts, Physica A, 506 (2018), pp. 749–754.
[48] C. Whissell, Traditional and emotional stylometric analysis of the songs of Beatles Paul
McCartney and John Lennon, Computers and the Humanities, 30 (1996), pp. 257–265.
[49] A. C.-C. Yang, C.-K. Peng, H.-W. Yien, and A. Goldberger, Information categorization approach to literary authorship disputes, Physica A, 329 (2003), pp. 473–483.
[50] Y. Zhao, J. Zobel, and P. Vines, Using relative entropy for authorship attribution, in
Proceedings of the Third Asia Conference on Information Retrieval Technology, AIRS’06,
Berlin, Heidelberg, 2006, Springer-Verlag, pp. 92–105.

