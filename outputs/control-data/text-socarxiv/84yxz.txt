1

Foresight in a Game of Leadership

2

Logan Perry1 and Sergey Gavrilets1,2,*

3

1 Department

4
5
6
7

8

9

10

11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38

of Mathematics, Center for the Dynamics of Social Complexity, University of Tennessee, Knoxville, TN
37996 USA
2 Department of Ecology and Evolutionary Biology, National Institute for Mathematical and Biological Synthesis,
University of Tennessee, Knoxville, TN 37996 USA
* Corresponding author: gavrila@utk.edu

ABSTRACT
Leadership can be effective in promoting cooperation within a group, but as the saying goes “heavy is the head that wears the
crown.” A lot of debate still surrounds exactly what motivates individuals to expend the effort necessary to lead their groupmates.
Evolutionary game theoretic models represent individual’s thought processes by strategy update protocols. The most common
of these are random mutation, individual learning, selective imitation, and myopic optimization. Recently we introduced a
new strategy update protocol - foresight - which takes into account future payoffs, and how groupmates respond to one’s own
strategies. Here we apply our approach to a new 2x2 game, where one player, a leader, ensures via inspection and punishment
that the other player, a subordinate, produces collective good. We compare the levels of inspection and production predicted by
Nash Equilibrium, Quantal Response Equilibrium, level-k cognition, fictitious play, reinforcement learning, selective imitation,
and foresight. We show that only foresight and selective imitation are effective at promoting contribution by the subordinate and
inspection and punishment by the leader. The role of selective imitation in cultural and social evolution is well appreciated. In
line with our prior findings, foresight is a viable alternative route to cooperation.

Introduction
The web of social and economic ties that weaves us together has never been thicker. Every day we make decisions based on our
expectations of how others will act1 . While these waters are difficult to traverse, we are not without aid as there exists a plethora
of mores to help us navigate this interpersonal maelstrom. The mores that coordinate political and economic relationships are
referred to as institutions2–4 . Whether it is a teacher corralling their students’ behavior or the United Nations placing sanctions
on an entire nation, institutions serve as an effective tool for resolving conflict and shaping behavior. The role of institutions in
modern society cannot be understated, with some claiming they are the determining factor of whether nations succeed or fail3, 5 .
However, their origins are much older than modern society as they existed even in hunter-gather groups6 .
Here, we are concerned with the institution of leadership in small-scale societies7, 8 . A leader is defined as an individual
who has non-random differential influence over group behavior9 . Leaders can take on several different roles within a group,
e.g. role-models10 , managers11 , punishers12 or volunteers13 . Leader-follower relationships are likely to emerge in groups of
conspecifics that benefit from acting in a unified manner14 . Examples of actions that necessitate high degrees of coordination
include migration, hunting, deterring predation, resolving internal conflicts, and competing with neighboring groups14 .
Joint actions often lead to the collective action problem15 when all individuals can benefit from an action but no one is
willing to bear its cost. The collective action problem (CAP) is present in many animal and human groups15–19 . Several
solutions to the CAP exist. They include kin selection, direct and indirect reciprocity, punishment20 , selective incentives and
institutional design15, 21 , the presence of within-group heterogeneity17, 22–24 and influential individuals25 .
Here we are focused on the solution by way of punishment and the institution of leadership. In particular, we are concerned
with the question of why leaders would choose to enforce cooperation since abstaining from doing it could be less costly - the
so called second-order free-rider effect. We investigate this by developing a modified inspection game26 .
Inspection games are typically concerned with modeling an inspector that seeks to verify the adherence to some pre-arranged
contract and an inspectee that may be tempted to violate said contract. In the past, inspection games have been used to better
understand relationships between law enforcement and criminals27–30 , employers and their employees31 , and countries dealing
with nuclear armament32 . Here we modify the standard inspection game26 to account for interactions between a leader and
a subordinate. We do this by altering basic assumptions so that the subordinate has no incentive to produce a good, while
the leader has a vested interest in seeing the good produced. Our motivation for doing this is to mirror a standard CAP with
punishment in a two person game. In particular we are interested in our model having properties that parallel the first- and
second-order free-rider effects. The simplicity of our proposed model allows us the opportunity to obtain analytical results,
which can in turn be used to provide a baseline understanding of more complicated models.

Parameter
b
c
d
h
k
θ
λ
ω

Interpretation
Benefit produced by a subordinate
Cost of contributing to a subordinate
Punishment for shirking
Cost of inspecting to a leader
Cost of punishing to a leader
Taxation rate
Precision parameter
Foresight parameter

Table 1. Parameters in the leadership game.

39
40
41
42
43
44
45
46
47
48
49
50
51
52
53

To accurately portray the dynamics of the leader-subordinate relationship we must consider their thought processes. While
both individuals are interested in bettering their own positions, the leader has sway over the subordinate and must make
decisions based on this influence. This means that the leader must anticipate how their subordinate will react and hence must
take into consideration how their subordinate makes decisions. This consideration of other’s cognitive processes is referred
to as a theory of mind33, 34 . A theory of mind is thought to be key to promoting cooperation withing groups34 , has been
linked to the size of individuals’ social networks35 , their propensity for social cooperation36 , and the extent to which they are
agreeable37 . Standard approaches of myopic optimization, adjustment through error and mutation, reinforced learning, and
selective imitation fail to account for this theory. Here we continue to develop the strategy update protocol of foresight38 , which,
we argue, incorporates some aspects of the theory of mind and in doing so solves the second-order free-rider effect.
Foresight works by altering agents’ utility functions to consider not only how a strategy will do in this round, but how it will
affect the next round as well38 . Fundamental to foresight is the fact that agents consider how their actions will shape the future
behaviors of others. Thus it accounts both for human’s theory of mind and the “shadow of the future”39 . The major focus of our
earlier work was the impact of foresight on a population facing a collective action problem in the presence of peer punishment.
We showed that in heterogeneous groups possessing foresight a division of labor will develop where the strong will specialize
in enforcing the contributions of the weak. We now seek to further develop our theory of foresight and obtain analytical results.

63

Our approach here is broken into several phases. We begin by developing a two-player game based on the classical
inspection game26 to investigate the relationship between a leader and their subordinate, which we dub the leadership game.
As in our original paper38 , we assume heterogeneity, but now take it to be systemic. This is done by pre-designating a leader
who serves as an enforcer over a subordinate. We show that the leader lacks motivation to enforce the contribution of their
subordinate despite their vested interests in seeing goods produced. Our investigation of the leadership game is structured as
follows. First, we consider some typical approaches to analyzing the leadership game, e.g. Nash equilibria, Quantal Response
Equilibria (QRE) models, level-k models, and fictitious play models. We follow this up by investigating the notion of foresight,
which we previously introduced38 . Our results show that the ability of foresight successfully motivates leaders to punish
subordinates, and in turn motivates subordinates to produce. Finally, we compare and contrast foresight with two different
learning protocols: Cross’s reinforcment learning model40 and selective imitation41 .

64

Results

65

Leadership Game

54
55
56
57
58
59
60
61
62

66
67
68
69
70
71
72
73
74
75
76

We consider a simple 2 × 2 game played between a leader and a subordinate, which is based on the inspection game26 described
in the Supplementary Information (SI). The subordinate is tasked with producing a good or benefit at a personal cost to
themselves, while the leader has a vested interest in seeing the good is produced. Since we are interested in drawing parallels
with collective action problems we make assumptions in such a way that the subordinate has no incentive to see the good be
produced unless they are facing punishment.
The subordinate can either produce the good (x = 1) or shirk on the production of the good (x = 0). In the case the
subordinate produces the good, they pay a cost of c to produce a good of value b. Any benefit produced by the subordinate is
split with the leader in a θ : 1 − θ ratio. Here 0 ≤ θ ≤ 1 can be thought of as a taxation rate. The strategies available to the
leader are to enforce production via inspection (y = 1) or to not inspect (y = 0). Inspection costs the leader h, but in the event
that a leader inspects a non-producing subordinate they inflict a punishment of d at a cost of k. We assume that all parameters
are positive (see Table 1). Table 2 describes the corresponding payoff matrix.
2/14

Subordinate

Produce
Shirk

Leader
Inspect
(1 − θ )b − c, θ b − h
−d, −h − k

Don’t Inspect
(1 − θ )b − c, θ b
0, 0

Table 2. Payoff matrix for the basic leadership game.

77

78
79
80
81
82
83
84
85
86
87
88

89
90

The payoff functions for the leader and the subordinate are then
πS (x, y) = [(1 − θ )b − c]x − dy(1 − x),

(1a)

πL (x, y) = bθ x − [h + k(1 − x)]y.

(1b)

We will make three assumptions. First, given the subordinate contributes (i.e., x = 1), the benefit to the leader exceeds its
cost of inspection, i.e., θ b > h. Second, given our aforementioned interest in CAP, we assume that without punishment (i.e.,
if y = 0) the subordinate is not motivated to contribute, i.e., (1 − θ )b − c < 0. Third, facing the threat of punishment (i.e., if
y = 1), the subordinate however is motivated to contribute, i.e., (1 − θ )b − c > −d.
Pure strategies. Next we derive best response functions for the subordinate, BRS (y), given the leader’s action y and for the
leader, BRL (x), given the subordinate’s action x. If the leader inspects (y = 1), the subordinate prefers to produce (x = 1). If,
however, the leader doesn’t inspects, then the subordinate’s best option is to do nothing. Therefore, BRS (y) = y. This implies
that the subordinate can be motivated to produce the good. On the other hand, the leader’s best response is not to inspect no
matter what the subordinate does: BRL (x) = 0. Therefore, the only Nash equilibrium is (x∗ , y∗ ) = (0, 0).
Mixed strategies. Suppose that the subordinate chooses to produce the good with probability p while the leader opts to
inspect with probability q. Then the expected payoffs are
ES (p, q) = p[(1 − θ )b − c] − (1 − p)qd,

(2a)

EL (p, q) = pθ b − q[h + (1 − p)k].

(2b)

From the above we can see that the subordinates best response depends upon the leaders strategy. In particular, there is a critical
inspection rate of the leader,
qc =

91
92
93
94
95
96
97
98

99
100
101
102
103
104
105
106

107

(3)

(0 ≤ qc ≤ 1), such that if q < qc , the subordinate is best off always doing nothing: BRS (q) = 0. If q > qc , the subordinate is
best off always producing the good: BRS (q) = 1. If q = qc , the subordinate will receive the same payoff no matter what they do.
The case of the leader is much simpler, as the leader’s best response is always to do nothing: BRL (p) ≡ 0. Hence, the only
Nash equilibrium is at (p∗ , q∗ ) = (0, 0) when the subordinate does not contribute and the leader does not inspect.
Implications for evolutionary dynamics. Assume that individuals are bounded rational and attempt to increase their payoffs
by evaluating some “candidate” strategies, which they generate mentally, and choosing one with probabilities proportional to
estimated payoffs. (In the terminology of Ref.42 , this is a direct strategy revision protocol.) The results above imply that the
game will converge to the Nash equilibrium (0, 0) of nothing being done.
Quantal Response Equilibrium (QRE)
Next, we generalize our results for the case when agents make errors in evaluating payoffs. We do this by relaxing the
assumption that individuals are best responders and replace it with the assumption that individuals are better responders. In this
new paradigm, all strategies are played with non-zero probabilities, but the rate at which they are played is proportional to their
payoff. This approach leads to investigate what is known as the Quantal Response Equilibrium43 of our model.
Let p be the probability the subordinate contributes, and q be the probability the leader inspects. Let ES,0 = πS (1, q)
and ES,1 = πS (0, q) be the expected payoff of not contributing and contributing to the subordinate, respectively. Define
EL,0 = πL (p, 0) and EL,1 = πL (p, 1) for the leader similarly. In the QRE approach, p and q are specified as
p=

108

c − (1 − θ )b
,
d

1
1
, q=
,
1 + exp[λ (ES,0 − ES,1 )]
1 + exp[λ (EL,0 − EL,1 )]

(4)

where λ is the precision parameter (e.g., with λ = 0, the players’ decision are random: p = q = 0.5, while as λ → ∞, both
player use myopic best response). Note that we assume both players have the same precision. The QRE solutions for p and q
3/14

109

satisfy the equalities43
ln(p(1 − p))
ln(q(1 − q))
= ES,0 − ES,1 ,
= EL,0 − EL,1 .
λ
λ

110
111
112
113
114
115

116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135

136
137
138
139
140
141
142
143
144
145

146
147
148
149
150
151
152
153
154

(5)

We solve the above equations numerically. Figure S1 in the SI shows the impact that the precision parameter λ has on the QRE
values (p∗ , q∗ ). For λ = 0 play is perfectly random so that p∗ = q∗ = 12 , as we would expect. As we increase λ , play converges
to the single Nash equilibrium at (0, 0), again as we would expect. So unless error is very high (i.e., λ is small), there will be
not much inspection or contribution.
Before introducing foresight, we show next that just incorporating a theory of mind in our model is not enough to overcome
the free-rider-like effects. We consider two different models attempting to capture some aspects of this theory.
Level-k Approach
A common way for capturing a theory of mind in models of decision-making is by supposing the agents utilize level-k rationality,
which is a hierarchical way of thinking based on iterative logic44 . In it the most basic model of cognition is level-0 rationality,
which makes agents to just play the strategies available to them at random. It is important to note, that players are never
assumed to be level-0, but rather it is the simplest model of others a player may have. A level-1 player will assume that all
other individuals are level-0 and select a strategy which best responds to their predicted actions. Likewise, a level-2 player will
assume all others are level-1 and select a strategy accordingly. In this way we can iteratively define a level-k player who will
assume all others are level-k − 1 and choose a strategy which best responds to their predicted action.
We begin by supposing both agents are level-1, which means they assume the other is level-0. Generalizing slightly, we
suppose that a level-0 subordinate is assumed to contribute with probability p0 , while a level-0 leader is assumed to contribute
with probability q0 (e.g. p0 = q0 = 0.50). Now the expected payoffs are given by ES (p, q0 ) and EL (p0 , q) (See Equations (2a)
and (2b)). This implies the best response for a subordinate is to not contribute provided the expected cost of being punished is
less than the net cost of producing, i.e. q0 d < c − (1 − θ )b. Likewise, it is best to contribute provided the expected cost of being
punished is greater than the net cost of producing. Meanwhile the leader’s expected payoff EL (p0 , q) is always maximized by
setting q = 0 implying it is always best for the leader to not inspect.
Now a level-2 subordinate will assume that the leader is level-1, and thus anticipates that the leader will never inspect. This
in turn means that a level-2 subordinate will never contribute. A level-2 leader on the other hand will expect a subordinate to
contribute sometimes, but will always be better off doing nothing. So a level-2 leader will always opt to not inspect. Finally, for
levels 3 onward we have by similar logic that neither player will do anything. Hence, level-k modeling is unable to overcome
the free-rider-like problem in our model.
Fictitious Play
In the model of fictitious play45 , every player assumes their opponents are playing strategies drawn from a certain stationary
distribution which the player attempts to estimate via observation. Each player then chooses their action (i.e., a value of x or y)
in an attempt to maximize their payoff given a prediction or assessment of their opponent’s strategy.
The leader assumes that the subordinate uses a mixed strategy contributing with a certain probability. Let p̃(t) be the
leader’s estimation of this probability at time t. The subordinate assumes that the leader uses a mixed strategy inspecting with
a certain probability. Let q̃(t) be the subordinate’s estimation of this probability at time t. We take p̃(0), and q̃(0) to be the
initial beliefs. Let x(t) and y(t) be the action taken, i.e. 0 or 1, by the subordinate and leader, respectively, in round t. Now we
define a system of recurrence equations describing how the leader and subordinate adjust their believes based on observations
of previous actions
p̃(t + 1) =(1 − `) p̃(t) + `x(t),

(6a)

q̃(t + 1) =(1 − `)q̃(t) + `y(t).

(6b)

Here, ` is a parameter which scales the impact of the most recent action on the agent’s estimation. In general, ` can depend on t.
1
For example, ` = 1+t
corresponds to the original approach45 . The case of ` = 1 corresponds to best response.
Fictitious play itself is then defined as any rule the agent uses to choose a response from the set of best responses to his or her
estimation of the opponent’s strategy. For our case, the natural choice of the rule is given by the best response functions BRS (q)
and BRL (p) established above. Since our game is dominance solvable via iteration (see the SI for details), we know from Ref.46
that it will converge to an equilibrium asymptotically. As it has only one Nash equilibrium at (0, 0), it will converge to it. This
makes sense as a rational leader would never choose to inspect as not inspecting offers a higher payoff in all circumstances, and
a rational subordinate would quickly learn this is the case and thus choose to not contribute. Hence, fictitious play modeling is
unable to overcome the free-rider-like problem in our model.
4/14

155
156
157
158
159
160
161
162
163
164
165
166

Foresight
We have shown that under myopic optimization, level-k modeling, or fictitious play leaders will fail to enforce and subordinates
will fail to contribute. One method to overcoming this is to introduce foresight38 . If we assume that the leader is willing to
suffer a cost this round in order to make a gain in future rounds, then they could be motivated to inspect the subordinate. More
specifically, we introduce the foresight parameter ω ∈ (0, 1), which measures the weight placed on next round’s forecasted
payoffs versus this round’s anticipated payoff. This averaging of payoff now with payoff later can be compared with the typical
practice of discounting future payoffs. Where foresight is particularly novel is how we account for the leader forecasting
future payoffs. These forecasted payoffs depend upon the leader’s model of their subordinate. This consideration of how their
subordinate reasons is where our leader’s theory of mind is on display. We assume that the leader’s model of their subordinate’s
behavior is based on a best response and focus on the effect foresight has on a leader’s strategy selection.
We will assume that only the leaders use foresight. (The SI shows that allowing for the subordinate to use foresight does
not change our conclusions.) Consider a weighted sum of the leader’s payoffs for this and the next rounds:
(1 − ω) × (θ bx − [h + k(1 − x)]y) + ω × (θ bx̃ − [h + k(1 − x̃)]ỹ) ,

167
168
169
170
171

where x̃ and ỹ are the subordinate and leader’s efforts in the next round. The leader expects that their action this round y will
affect the subordinate’s action x̃ in the next round. If the subordinate uses best response, as we will assume, x̃ = y. At the same
time, y has no effect on the benefit to be produced by the subordinate this round, θ bx, or the cost of the inspection in the next
round, [h + k(1 − x̃)]y0 . Therefore the can define the leader’s utility function as a weighted sum of the costs of inspection and
punishment this round and the benefit next round
uL (x, y) = (1 − ω) × (−[h + k(1 − x)]y) + ω × (θ by) .

172
173
174
175
176
177
178
179
180

181
182
183

184
185
186
187
188
189
190
191
192
193

(7)

For the subordinate who uses best response, the utility function is equal to the expected payoff given by equation (1a). Table 3
defines the utilities of different actions in this model.
Pure strategies. We can see that the state (0, 0) is still a Nash equilibrium but only if (1 − ω)(h + k) > ωθ b, that is, if the
h+k
foresight parameter is small enough: ω < ω ∗ ≡ θ b+h+k
. However there is a new possibility: if ωθ b > (1 − ω)h, or, equivalently,
h
∗∗
if the foresight parameter is large enough: ω > ω ≡ θ b+h
, there is another Nash equilibrium (1, 1). For intermediate values
∗∗
∗
of ω, i.e. if ω < ω < ω , these two equilibria coexist. The leader does the best at the (1, 1) equilibrium where the payoffs are
uS = (1 − θ )b − c and uL = θ b − h. The subordinate does the best at the (0, 0) equilibrium where the payoffs are 0 and 0.
Mixed strategies. Assume that the two players make efforts with probabilities p and q, respectively, and consider formally
the corresponding expected utilities:
US (p, q) = [(1 − θ )b − c]p − dq(1 − p),

(8a)

UL (p, q) = (1 − ω)(−(h + k(1 − p))q) + ωθ bq.

(8b)

Computing the derivative

∂US (p,q)
,
∂p

one finds that the subordinate’s utility uS increases with p for q > qc and decrease otherwise,

where qc is defined by equation (3). Similarly, computing the derivative
and decreases otherwise. The critical value


1
ωθ b
pc =
h+k−
.
k
1−ω

∂UL (p,q)
,
∂q

the leader’s utility increases with q if p > pc

(9)

Note that pc < 0 and thus the condition p > pc is always satisfied if ωθ b > (1 − ω)(h + k), i.e., if the expected future benefit is
larger than the current cost of inspection and punishment (or, equivalently, if ω > ω ∗∗ ). There is a mixed Nash equilibrium
(p∗ = pc , q∗ = qc ), but this equilibrium is unstable: if one player deviates from it, the other player will be motivated to change
their strategy as well.
Implications for evolutionary dynamics. These results imply that in corresponding evolutionary models utilizing direct
strategy revision protocols42 , depending on parameters and initial conditions, the system can go to either (0, 0) or (1, 1) state.
To illustrate these possibilities, assume that the subordinate always plays the best response to the leader’s previous action,
i.e. x̃ = y. Let the leader use a mixed strategy q. There are four possibilities for a combination of x and y entering the leader’s
utility equation (7): (0, 0), (0, 1), (1, 0) and (1, 1) with probabilities (1 − q)2 , q(1 − q), q(1 − q) and q2 , respectively. [Note that
x is equal to y in the previous time step.] Therefore the expected utility to strategy q
UL (q) = (1 − q)2 uL (0, 0) + q(1 − q)uL (0, 1) + q(1 − q)uL (1, 0) + q2 uL (1, 1) = k(1 − ω)q(q − q∗ ).

194

where q∗ = pc . That is, function UL (q) is quadratic with a maximum at q = 1.
5/14

Subordinate

Produce
Shirk

Leader
Inspect
(1 − θ )b − c, −(1 − ω)h + ωθ b
−d, −(1 − ω)(h + k) + ωθ b

Don’t Inspect
(1 − θ )b − c, 0
0, 0

Table 3. Utility matrix for the leadership game with foresight.

195
196
197
198
199
200

201
202

203
204
205
206
207

208
209
210
211

If candidate strategies evaluated by the leader deviate only slightly from their current strategy, the dynamics will proceed in
the direction of the gradient of UL (q). That is, if q∗ < 0, or equivalently θ b > (1 − ω)(k + h), q will evolve to 1 for any initial
condition. If θ b < (1 − ω)(k + h), q evolves to 1 if it exceeds q∗ /2 initially. These conclusions are not affected qualitatively if
the leader make errors in predicting the subordinate’s behavior (see the SI). If candidate strategies can deviate from the current
strategy substantially, reaching the state q = 1 can happen quickly and for any initial condition. The evolution towards the state
where both players always make maximum efforts is the new dynamical feature made possible by the leader’s foresight.
Learning
Finally we compare foresight with two models of learning: reinforcement learning40 and payoff-biased selective imitation41 .
Reinforcement Learning

In reinforcement learning, agents form opinions of strategies based on the payoffs received when those strategies are implemented. Following Ref.40 , let uX (x, y) be the utility to player X ∈ L,C when the subordinate plays x and the leader plays y. Let
p(t) be the probability the subordinate contributes and q(t) be the probability the leader inspects at time step t. Then in Cross’
learning process40 version of our model the probabilities p and q change according to stochastic equations
p(t + 1) =xuS (x, y) + (1 − uS (x, y))p(t),

(10a)

q(t + 1) =yuL (x, y) + (1 − uL (x, y))q(t).

(10b)

What this means is that, after players observe how their current action (i.e. x or y) did, they update their state (i.e., p or q) by
taking a weighted average between their old state and the state that puts all the weight on the current action (either 0 or 1),
where utility UX (x, y) serves as the weight. This approach requires40 that all utilities are scaled to be between 0 and 1. We can
achieve this, e.g. be defining them as
uS (x, y) =

212
213

exp{λ πL S(x, y)}
exp{λ πS (x, y)}
, uL (x, y) =
,
∑x exp(λ πS (x, y))
∑y exp(λ πL (x, y))

where λ is a parameter. By this construction, utility increases with the payoff, and all utilities fall between 0 and 1.
In the continuous time limit, stochastic system (10) can be approximated40, 47 by deterministic differential equations:
d p(t)
=p(t)(US (x = 1) − ŪS ),
dt
dq(t)
=q(t)(UR (y = 1) − ŪL ).
dt

214

(11a)
(11b)

where
US (x = 1) = (1 − q)uS (1, 0) + quS (1, 1),
UR (y = 1) = (1 − p)uL (0, 1) + puL (1, 1)

215

are the expected utilities of strategies x = 1 and y = 1, and
ŪS = pq uS (1, 1) + p(1 − q) uS (1, 0) + (1 − p)q uS (0, 1) + (1 − p)(1 − q) uS (0, 0),
ŪL = pq uL (1, 1) + p(1 − q) uL (1, 0) + (1 − p)q uL (0, 1) + (1 − p)(1 − q) uL (0, 0)

216
217
218
219
220

are the expected utilities of subordinates and leaders, respectively.
We analyzed both the stochastic and deterministic versions of this model. Stochastic numerical simulations of equations (10)
show that the system always converges to equilibrium (0, 0) (see the SI). This conclusion is supported by linear stability analysis
of equilibria of equations (11): the only stable equilibrium is (0, 0) (see the SI). We conclude that reinforcement learning is
unable to overcome the free-rider-like problem in our model.
6/14

221
222
223
224
225
226
227

Selective Imitation

Here we assume that individuals compare their payoff with that of a peer and choose to either copy the selected individual (if
their payoff is higher than the focal individual’s) or keep their own strategy41 .
Consider a population of pairs each consisting of a subordinate and a leader. Let the i-th pair’s actions at time t be denoted
by (xi (t), xi (t)) where all variable retain their usual interpretations. For each individual i, randomly select a peer j (i.e. leader
for leader and subordinate for subordinate) and have the focal individual observe that peer’s payoff and action. Then the
probability that at time t subordinate i continues playing their current action given they observed subordinate j is
pt (i → i| j) =

228
229
230
231
232
233
234
235
236
237
238

exp{λ πS (xi (t), yi (t))}
,
exp{λ πS (xi (t), yi (t))} + exp{λ πS (x j (t), y j (t))}

which of course means the probability i switches to mimic j is pt (i → j| j) = 1 − pt (i → i| j). A similar equation describes
changes in the leader’s probability qi given they observe leader j0 . As above, λ measures precision in payoff comparisons.
Assume that leaders and subordinates update their strategies at the same rate. In numerical simulations (not shown) the
leaders and subordinates opt to do nothing. However if subordinates always play the best response to their leader’s previous
action (i.e., if x = y prev ), the system can evolve to a state with nonzero efforts. (See the SI for numerical illustrations.)
Pure strategies. Consider the case of pure strategies: inspect and not inspect. Given our assumptions about parameters and
best response in the subordinates, the former strategy always has a higher payoff than the latter. Therefore the frequency of
leaders who inspect will always increase (subject to stochastic errors). The larger precision parameter λ , the faster it happens.
Mixed strategies. If leaders use mixed strategies, then using an approach similar to the one we applied to analysing foresight,
there are four possible combinations of x and y in the equation for the leader’s payoff πL (x, y): (0, 0), (0, 1), (1, 0) and (1, 1)
with probabilities (1 − q)2 , q(1 − q), q(1 − q) and q2 , respectively. Therefore the expected payoff to the leader’s strategy q is
ΠL (q) = (1 − q)2 πL (0, 0) + q(1 − q)πL (0, 1) + q(1 − q)πL (1, 0) + q2 πL (1, 1) = kq(q − q∗∗ ).

245

where q∗∗ = 1 − θ b−h
k . This is a quadratic maximized at q = 1. If variation in q in the population is small, q will evolve in the
direction of the gradient of ΠL (q). That is, if the cost of punishment is small (i.e., k < θ b − h), q∗∗ is negative, and ΠL (q) is
always increasing with q ∈ [0, 1]. Thus, q is expected to evolve by selective imitation to q = 1 for any initial value. If the cost
of punishment is large (i.e., k > θ b − h), then 0 ≤ q∗∗ ≤ 1. So q will increase to one for initial q > q∗∗ /2, but will decrease to
zero for initial q < q∗∗ /2. If variation in q in the population is large, the population will always evolve towards increasing q.
That is, with the best response in subordinates and selective imitation in leaders the dynamics are similar to those under
foresight. In both cases, the system can evolve to state (1, 1).

246

Discussion

239
240
241
242
243
244

247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268

Here we have studied the impact of foresight on leader-subordinate dynamics in some simple models. Our aim in doing
this was to shed light on what can motivate individuals to enforce contribution to production of a collective good. Typically,
such enforcement comes with an inherent cost that discourages group members from being coercive as they seek to avoid
the cost. This is known as the the second-order free-rider problem. Earlier work highlighted several mechanisms such as
meta-punishment12 , conformism48 , signaling49 , and group-selection50, 51 as potential routes to overcoming the second-order
free-rider problem. We have shown here as well as in Ref.38 that, in addition to these mechanisms, foresight is an effective
way of motivating enforcement of cooperation. “Foresight” refers to a novel strategy update protocol, which stresses two key
components38 . First, that individuals care about their future payoffs. Second, that individuals consider how others will respond
to their present actions in future interactions. Both of these are fairly intuitive assumptions that make few requirements of
agent’s cognitive abilities. By developing foresight we sought to incorporate the deterrence theory52 into our model, which is
the notion that punishment is used to modify the future behavior of the target.
We approached this problem by altering the payoffs and assumptions of the inspector game26 . In particular, we were
concerned with modeling the interaction between a single leader interested in enforcing production and a single commoner
tasked with producing some good. In this way we were able to incorporate characteristics of the general collective action
problem (namely the first- and second-order free-rider effects) into a simple 2x2 game. While our earlier work38 has relied
exclusively on numerical simulations, the simplicity of our models here has allowed us to get some analytical results.
Our models can be interpreted as describing a simple case of institutionalized punishment. There are both similarities but
also differences with earlier evolutionary studies of social institutions. In our paper, the evolving part of the institution of
leadership was the level of monitoring which translated into punishment levels in a way similar to that in Ref.53, 54 . In Ref.11, 55
it was the tax imposed by the leaders while in Ref.56 it was the proportion of public goods invested into the group’s growth
rate. In Ref.11, 55, 56 players inherited their strategies from parents (subject to rare random mutation). In Ref.53, 54 players used
payoff-biased imitation. In contrast, we have considered and compared a number of different strategy revision protocols.
7/14

309

We started by considering several different ways of simulating human behaviour, namely Nash equilibria, Quantal Response
Equilibria, level-k cognition and fictitious play. Our results show that each of these methods were vulnerable to the second-order
free-rider problem. That is, in these basic models while the subordinate could be motivated to produced the good, the leader was
not inclined to enforce production and as a result nothing got done. We proceeded by analyzing the effect foresight had on the
best response functions and the Nash equilibrium. Upon introducing foresight, we saw that the leader now viewed punishment
as an utility increasing action and thus (provided sufficient emphasis on future payoffs) willingly enforced production of
their subordinate. Foresight in the subordinate only served to lessen the magnitude of the their payoff. This difference in
impact is due the fact that the subordinate’s action do not directly influence the leader on the same scale as the leader’s action
directly influence the subordinate. Our main results are that the introduction of foresight produced new Nash equilibria at
which leaders led and subordinates followed. These new equilibria were found to be dependent upon the emphasis placed on
future payoffs measured by parameter ω. Additionally, we found that in the repeated leadership game that foresight could
effectively overcome the second-order free-rider effect. Even when error was introduced into the leader’s predictions, they were
motivated to inspect provided certain conditions were met. Our final task was to compare foresight with two other strategy
update protocols: reinforcement learning and selective imitation. Our results show that reinforcement learning is not able to
overcome the second-order free-rider effect. In contrast, selective imitation is able to accomplish that.
Earlier Ref.53, 54 studied similar models of institutionalized punishment with multiple subordinates per leader. They showed
that selective imitation can lead to the evolution of punishment if leaders update their strategy at a much slower rate than
subordinates. This happens because a low update rate prevents the leaders from abandoning a costly punishment strategy before
subordinates have learned to contribute to avoid punishment. In contrast, in our model of selective imitation subordinates do not
have to learn from others via incremental improvements to adapt but rather they use the best response to the current strategy of
the leader. This introduces a new Nash equilibrium which can be then discovered by some leaders via random innovation and
then spread across the whole system by imitation. In a similar way, foresight in leaders results in the appearance of a new Nash
equilibrium discoverable by leaders via, say, a process of mental scenario building by considering several candidate strategies
and comparing their expected utilities (i.e., by using a direct strategy revision protocol sensu Ref.42 ).
Our approach is related to models of level-k cognition44, 57 . Specifically, best response utilized by subordinates can be
viewed as a level-1 strategy to level-0 players who do not change their strategies while foresight in leaders is related to level-2
reasoning. Typically level-k model assume that level-0 players choose their strategies uniformly randomly. Were we to make
this assumption, neither inspection nor production would happen in our model. Thus, our work shows that the exact assumptions
placed on the level-0 players strongly impacts the overall dynamics of the game.
Overall, our work highlights the importance of strategy revision protocols in evolutionary dynamics42 . While the free-rider
problems exists regardless of the strategy revision protocol employed, the assumptions made on how people think can impact
how effective groups are at overcoming these problems. Our protocol of foresight is a new way to consider how people think,
which can be used in conjuncture with existing strategy revision protocols.
There are several different questions of interest that must be answered by future work. First and foremost is the question of
whether foresight would evolve in a population where it is initial absent. In our current and prior paper, we have taken for
granted that foresight is present and sought only to show how it could be an effective route to overcoming the second-order
free-rider problem. Having proved its efficacy we should now turn our attention to whether or not its emergence is a reasonable
assumption. Secondly, here we considered the leadership game for only two players. A reasonable extension would be to
assume multiple agents acting in the role of subordinates (and potential in the role of leaders as well). Thirdly, our results
indicate that foresight can affect the basic dynamics of a game (in that it alters the Nash equilibria). It would be a worthwhile
endeavour to investigate the impact foresight has on a wider range of classical games.

310

References

269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308

311

1. Schick, F. Democracy and interdependent preferences. Theory and Decision (1972).

312

2. Schelling, T. The Strategy of Conflict (Harvard University Press, 1960).

314

3. North, D. Institutions, Institutional Change and Economic Performance (Political Economy of Institutions and Decisions)
(Cambridge University Press, 1990).

315

4. Hurwicz, L. Institutions as families of game forms. The Jpn. Econ. Rev. 47, 113–132 (1996).

313

316
317
318
319
320

5. Acemoglu, D. & Robinson. Why Nations Fail: The Origins of Power, Prosperity and Poverty (Crown Publishers, New
York, 2011).
6. Kaplan, H., Gurven, M., Hill, K. & Hurtado, A. The natural history of human food sharing and cooperation: A review and
a new multi-individual approach to the negotiation of norms. In Gintis, H., Bowles, S., Boyd, R. & Fehr, E. (eds.) Moral
sentiments and Material Iterests: The Foundations of Cooperation in Economic Life, 75–113 (MIT Press, 2005).
8/14

321
322
323
324
325
326
327
328
329
330
331
332

7. Glowacki, L. & von Rueden, C. Leadership solves collective action problems in small-scale societies. Philos. Transactions
Royal Soc. Lond. B 370, 20150010 (2015).
8. Garfield, Z. H., Hubbard, R. L. & Hagen, E. H. Evolutionary models of leadership: Tests and synthesis. Hum. Nat. 30,
23–58 (2019).
9. Smith, J. E. et al. Leadership in mammalian societies: emergence, distribution, power, and payoff. Trends Ecol. & Evol.
31, 54–66 (2016).
10. Zimmerman, M. & Eguiluz, V. Cooperation, social networks, and the emergence of leadership in a prisoner’s dilemma
with adaptive local interactions. Phys. Rev. Lett. 72, 056118 (2005).
11. Hooper, P. L., Kaplan, H. S. & Boone, J. L. A theory of leadership in human cooperative groups. J. Theor. Biol. 265,
633–646 (2010).
12. Boyd, R. & Richerson, P. J. Punishment allows the evolution of cooperation (or anything else) in sizable groups. Ethol.
Sociobiol. 13, 171–195 (1992).

334

13. Archetti, M. & Scheuring, I. Coexistence of cooperation and defection in public goods games. Evolution 65, 1140–1148
(2011).

335

14. van Vugt, M. Evolutionary origins of leaders and followership. Pers. Soc. Psychol. Rev. 10, 354–371 (2006).

333

336
337
338
339
340
341
342
343
344
345

15. Olson, M. The logic of collective action: Public Goods And The Theory of Groups (Harvard University Press, Cambride,
MA, 1965).
16. Bowles, S. & Gintis, H. A cooperative species: Human reciprocity and its evolution (Princeton University Press, Princeton,
NJ, 2011).
17. Gavrilets, S. & Fortunato, L. A solution to the collective action problem in between-group conflict with within-group
inequality. Nat. Commun. 5, article 3526 (2014).
18. Henrich, J. P. Foundations of human sociality: Economic experiments and ethnographic evidence from fifteen small-scale
societies (Oxford University Press on Demand, Oxford, United Kingdom, 2004).
19. Willems, E. P. & van Schaik, C. P. Collective action and the intensity of between-group competition in nonhuman primates.
Behav. Ecol. 26, 625–631 (2015).

347

20. McElreath, R. & Boyd, R. Mathematical models of social evolution. A guide for the perplexed (Chicago University Press,
Chicago, 2007).

348

21. Ostrom, E. Collective action and the evolution of social norms. J. Econ. Perspectives 14, 137–158 (2000).

349

22. Andreoni, J. Privately provided public goods in a large economy: the limits of altruism. J. Public Econ. 35, 57–73 (1988).

350

23. Gavrilets, S. Collective action problem in heterogeneous groups. Proc. Royal Soc. Lond. B 370, 20150016 (2015b).

346

351
352

24. McGinty, M. & Milam, G. Public goods provision by asymmetric agents: experimental evidence. Soc. Choice Welf. 40,
1159–1177 (2013).

354

25. Gilby, I. C. et al. “Impact hunters” catalyse cooperative hunting in two wild chimpanzee communities. Philos. Transactions
Royal Soc. B 370, article 20150005 (2015).

355

26. Fudenberg, D. & Tirole, J. Game Theory (The MIT Press, Cambride, MS, 1992).

356

27. Tsebelis, G. The abuse of probability in political analysis: The Robinson Crusoe fallacy. The Am. 83, 77–91 (1989).

357

28. Tsebelis, G. Are sanctions effective? J. Confl. Resolut. 34, 3–28 (1990).

358

29. Tsebelis, G. Penalty has no impact on crime. Ration. Soc. 2, 255–286 (1990).

359

30. Holler, M. J. Fighting pollution when decisions are strategic. Public Choice 76, 347–356 (1993).

353

360
361

31. Borch, K. Insuring and auditing the auditor. In Deistler, M., Furst, E. & Schwodiauer, G. (eds.) Games, Economic
Dynamics, and Time Series Anlaysis (Physica, Heidelberg, 1982).

363

32. Avenhaus, R., Caty, M., Kilgour, D. M., von Stengel, B. & Zamir, S. Inspection games in arms control. Eur. J. Oper. Res.
90, 383–394 (1996).

364

33. Premack, D. & Woodruff, G. Does the chimpanzee have a theory of mind? Behav. Brain Sci. 1, 515–526 (1978).

362

365
366

34. Tomasello, M., Carpenter, M., Call, J., Behne, T. & Moll, H. Understanding and sharing intentions: The origins of cultural
cognition. Behav. Brain Sci. 28, 675–691 (2005).
9/14

367
368
369
370
371
372

35. Stiller, J. & Dunbar, R. Perspective-taking and memory capacity predict social network size. Soc. Networks 29, 93–104
(2007).
36. Paal, T. & Bereczkei, T. Adult theory of mind, cooperation, Machiavellianism: The effect of mindreading on social
realtions. Pers. Individ. Differ. 43, 541–551 (2007).
37. Nettle, D. & Liddle, B. Agreeableness is related to social-cognitive, but not social-perceptual. Eur. J. Pers. 22, 323–335
(2008).

374

38. Perry, L., Shrestha, M. D., Vose, M. D. & Gavrilets, S. Collective action problem in heterogeneous groups with punishment
and foresight. Journal of Statistical Physics 172, 293–312 (2018).

375

39. Axelrod, R. The Evolution of Cooperation (Basic Books, New York, NY, 1984).

376

40. Borgers, T. & Sarin, R. Learning through reinforcement and replicator dynamics. Journal of Economic Theory (1997).

373

378

41. Hofbauer, J. & Sigmund, K. Evolutionary games and population dynamics (Cambridge University Press, Cambridge,
1998).

379

42. Sandholm, W. H. Population games and evolutionary dynamics (MIT press, Cambridge, MA, 2010).

377

380
381

43. Goeree, J. K., Holt, C. A. & Smith, A. M. An experimental examination of the volunteer’s dilemma. Games Econ. Behav.
102, 303–315 (2017).

383

44. Stahl, D. O. & Wilson, P. W. On players models of other players: theory and experimental evidence. Games Econ. Behav.
10, 218–254 (1995).

384

45. Fudenberg, D. & Levin, D. The Theory of Learning in Games (MIT Press, 1998).

382

386

46. Nachbar, J. “Evolutionary” selection dynamics in games: Convergence and limit properties. Int. J. Game Theory 19, 59–89
(1990).

387

47. Taylor, M. Anarchy and cooperation (Wiley, New York, NY, 1976).

385

389

48. Henrich, J. & Boyd, R. Why people punish defectors: weak conformist transmission can stabilize costly enforcement of
norms in cooperative dilemmas. J. Theor. Biol. 208, 79–89 (2001).

390

49. Gintis, H., Smith, E. A. & Bowles, S. Costly signaling and cooperation. J. Theor. Biol. 213, 103–119 (2001).

388

391
392

50. Boyd, R., Gintis, H., Bowles, S. & Richerson, P. J. The evolution of altruistic punishment. Proc. Natl. Acad. Sci. United
States Am. 100, 3531–3535 (2003).

394

51. Sober, E. & Wilson, D. S. Unto Others: The Evolution and Psychology of Unselfish Behavior (Harvard University Press,
Cambridge, MC, 1999).

395

52. Axelrod, R. An evolutionary approach to norms. American Political Science Review 80, 1095–1111 (1986).

396

53. Isakov, A. & Rand, D. The evolution of coercive institutional punishment. Dyn. Games Appl. 2, 97–109 (2012).

393

397
398
399
400

54. Roithmayr, D., Isakov, A. & Rand, D. Should law keep pace with society? Relative update rates determine the co-evolution
of institutional punishment and citizen contributions to public goods. Games 6, 124–149 (2015).
55. Powers, S. & Lehmann, L. An evolutionary model explaining the neolithic transition from egalitarianism to leadership and
despotism. Proc. Royal Soc. Lond. 281, 20141349 (2014).

402

56. Powers, S. & Lehmann, L. The co-evolution of social institutions, demography, and large-scale human cooperation. Ecol.
letters 16, 1356–1364 (2013).

403

57. Nagel, R. Unraveling in guessing games: an experimental study. The Am. Econ. Rev. 85, 1313–1326 (1995).

404

58. Hastings, A. et al. Transient phenomena in ecology. Science 361, eaat6412 (2018).

405

Acknowledgements

401

407

We thank A. Bentley, M. Berry, C. Strickland, and Y. Chen for comments and suggestions. Supported by the U. S. Army Research
Office grants W911NF-14-1-0637 and W911NF-18-1-0138 and the Office of Naval Research grant W911NF-17-1-0150.

408

Autor contributions statement

409

L.P and S.G. conceived the original idea, performed the analysis and simulations, and wrote the paper.

410

Competing Interest Statement

411

The author’s have no competing interests.

406

10/14

412

413
414

Supplementary Information for

“Foresight in a Game of Leadership”
Logan Perry and Sergey Gavrilets

415
416

Here we provide additional information on the following topics discussed in the main paper:

417

• Classic Inspection Game

418

• Quantal Response Equilibrium (QRE) solution

419

• Foresight for subordinate

420

• Phase plane analysis of the foresight model

421

• Errors in foresight

422

• Reinforcement learning dynamics

423

• Selective imitation dynamics

424
425
426
427
428
429
430
431

Classic Inspection Game
In the classic Inspection Game, there is a worker who works for a principal26 . The worker can choose to shirk or work, where
working costs the worker g and produces output of value v for the principal. The principal can either inspect or not inspect. The
act of inspecting costs the principal h but is allows the principal to determine if the worker shirked or not. Now the principal
is set to pay the worker a wage of w unless they have evidence that the worker shirked. Additional assumptions are that
w > g > h > 0. This yields the game as described
in Table S1. This game has no pure-strategy equilibrium, but does have a
g
mixed Nash equilibrium at (x, y) = w−h
,
,
where
x is the probability that the worker shirks, and y is the probability that the
w w
principal inspects.

Worker

Works
Shirks

Principal
Inspect
w − g, v − w − h
0, −h

Don’t Inspect
w − g, v − w
w, −w

Table S1. Payoff matrix for the classic Inspection Game26 .

432
433
434
435

436
437
438
439

440

Quantum Response equilibrium (QRE) solution
Figure S1 shows the impact of the precision parameter λ on the QRE values (p∗ , q∗ ) as found numerically by solving equation ()
of the main text. For λ = 0, play is perfectly random so that p∗ = q∗ = 21 , as we would expect. As we increase λ , play converges
to the single Nash equilibrium at (0, 0), again as we would expect.
Foresight for subordinate
Assume that the subordinates cares about the future payoff and predicts that the leader will use the best response. Because
the leader’s best response is always 0, the subordinate predicts that there will be no monitoring and punishment and thus will
choose not to contribute.
Phase plane analysis in the foresight model
Let p and q be the probabilities of making an effort (i.e., the mixed strategies) for the subordinate and leader. Their expected
utilities are
US = ((1 − θ )b − c)p − dq(1 − p)

(S1a)

UL = (1 − ω)(−h + k(1 − p))q) + ωθ bq

(S1b)

This corresponding derivatives are given by
∂ pUS = ((1 − θ )b − c) + dq

(S2a)

∂qUL = (1 − ω)(−(h + k(1 − p))) + ωθ b.

(S2b)
11/14

0.5

=0

0.45

0.4

0.35

q

0.3

=1

0.25

0.2

=

0.15

0.1

=4
=2

0.05

0
0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

p

Figure S1. Effect of the precision parameter λ on the Quantal Response Equilibrium values p∗ and q∗ . Parameters:
b = 1, c = 2, d = 2, h = 0.1, k = 1, and θ = 0.5.

441
442
443


ωθ b
As discussed in the main text, ∂qUL < 0 whenever p < q∗∗ = 1 + 1k h − 1−ω
, and ∂qUL > 0 whenever p > q∗∗ , and ∂ p uS < 0
whenever q < qc =
in Figure S2.

c−(1−θ )b
d

and ∂ p uS > 0 whenever q > qc . This yields the expected flow on the phase plane (p, q) as outlined

q

1

qc

0
0

q**

1

p

Figure S2. Flow associated with equations (S1a. Parameters: b = 1, c = 2, d = 3, k = 1, h = .25, θ = 0.5, and ω = 0.6. Red
dots mark Nash equilibria two of which are stable (0, 0 and (1, 1)) and one unstable at (q∗∗ , qv ).

444
445
446
447
448

Errors in foresight
Here we incorporate errors into the leader’s predictions of the actions of the subordinate. We can interpret this error in one of
two ways. First, it can be that the leader makes mistakes when attempting to decide what the subordinate will do. Second, it
can be that the leader anticipates that the subordinate will make mistakes and takes this into account. Either way, we define the
probability p(y) the leader will predict the subordinate contributes given the leader inspects with probability q as

p(y) =

1
1
=
.
1 + exp[λ (πS (0, y) − πS (1, y))] 1 + exp[λ (−dy − b(1 − θ ) + c]
12/14

449

That is, we allow for logit errors with precision parameter λ as above.
Using the same approach as in the main text, we write the leader’s utility as
UL (y prev , y) = (1 − ω) × (−[h + k(1 − p(y prev ))]y) + ω × (θ bp(y))
where y prev and y are two subsequent values of the leader’s y. Then, for a leader using strategy q, the expected utility is
U(q) = (1 − q)2UL (0, 0) + q(1 − q)UL (0, 1) + q(1 − q)UL (1, 0) + q2UL (1, 1).

450
451
452
453

From the above equation we can find the critical value q∗ such that for initial values of q above q∗ , q is expected to evolve to
1. Figure S3 illustrate the dependence of q∗ on λ and ω. This Figure shows that increases ω increases decreases q∗ and thus
increases the range of conditions leading to production and inspection. As λ → ∞, the predicted value of q∗ converges to that
one found in the main text.

Figure S3. The critical value q∗ in the model with errors in the leader’s foresight for different values of precision parameter λ .
The three curves correspond to 0 = 0.75, 0.5 and 0.25 (from top to bottom). Other parameters:
b = 1, c = 2, d = 2, h = 0.25, k = 1, and θ = 0.5.

454
455
456
457
458
459
460
461
462

463
464
465
466
467
468
469
470

Reinforcement learning dynamics
The system of differential equations (11) in the main text has four equilibria: (0, 0), (1, 0), (0, 1) and (1, 1). Standard stability
analysis shows that the only locally stable equilibrium is (0, 0). For example, the eigenvalues of the Jacobian matrix at this state
are uS (1, 0) − uS (0, 0) and uL (0, 1) − uL (0, 0), which are always negative by our assumptions. The corresponding eigenvalues
of state (1, 1) are uS (0, 1) − uS (1, 1) and uL (1, 0) − uL (1, 1), which are always positive by our assumptions.
Numerical simulations show that the system may spend very long time in a neighborhood of each of the other three
equilibria, that is, the dynamics are characterized by long transients58 . For example, Figure S4 shows the stochastic trajectories
in the reinforcement learning model for a range of random initial conditions. With small precision λ , fluctuations in p and q
persist for a long time. Even with large λ some subordinate-leader pairs can stay close to state (1, 1) for a long time.
Selective Imitation
Consider first the case where both leaders and subordinates update their actions x and y via selective imitation. Figure S5 shows
the trajectories associated with a range of random initial conditions drawn from the uniform distribution over [0.1, 0.9]. We see
that regardless of the initial condition, the system eventually settles into the equilibrium of (0, 0).
Next consider the case where leaders update via selective imitation, but subordinates update via myopic optimization.
Figure S6 shows the trajectories associated with a range of random initial conditions drawn from the uniform distribution over
[0.1, 0.9]. With small precision λ the system may vocationally end up at (0, 0) state. [This is analogous to fixing a deleterious
allele by random genetic drift in population genetics.] With increasing precision λ , the system always evolves to (1, 1) state.

13/14

(a) λ = 1

(b) λ = 5

(c) λ = 10

Figure S4. Time-series in the case of reinforcement learning for different values of λ . The graphs show 100 independent runs
with initial conditions drawn from the uniform distribution over [.1, .9]. The thin lines represent the individual runs while the
thick lines represent the mean over all runs with the red corresponding the the leader and the blue to the subordinate.
Parameters: b = 1, c = 2, d = 2, k = .5, h = .25, and θ = 0.5.

1
0.9
0.8
0.7

p, q

0.6
0.5
0.4
0.3
0.2
0.1
0
10

20

30

40

50

60

70

80

90

100

t

1

1

0.9

0.9

0.8

0.8

0.8

0.7

0.7

0.7

0.6

0.6

0.6

0.5

p, q

1
0.9

p, q

p, q

Figure S5. Time-series in the case of selective imitation in both players. The thin lines represent the individual runs while the
thick lines represent the mean over all runs with the red corresponding the the leader and the blue to the subordinate. Note that
all trajectories tend toward the equilibrium (0,0). Parameters: b = 1, c = 2, d = 2, k = .5, h = .25, θ = 0.5, and λ = 1.

0.5

0.5

0.4

0.4

0.4

0.3

0.3

0.3

0.2

0.2

0.2

0.1

0.1

0

0.1

0
20

40

60

80

100

120

t

(a) λ = 1

140

160

180

200

0
20

40

60

80

100

120

t

(b) λ = 5

140

160

180

200

20

40

60

80

100

120

140

160

180

200

t

(c) λ = 10

Figure S6. Time-series in the case of myopic optimization in the subordinate and selective imitation in the leader for different
values of λ . The results summarize 100 independent runs with initial conditions drawn from the uniform distribution over
[0.1, 0.9]. The thin lines represent the average over all 1000 groups while the thick lines represent the mean over all runs with
the red corresponding the the leader and the blue to the subordinate. Parameters: b = 1, c = 2, d = 2, k = .5, h = .25, and
θ = 0.5.

14/14

