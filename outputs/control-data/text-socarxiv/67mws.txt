Economic experiments and inference
Norbert Hirschauer (a), Sven Grüner (b), Oliver Mußhoff (c), and Claudia Becker (d)

Abstract: Replication crisis and debates about p-values have raised doubts about what we can statistically infer from research findings, both in experimental and observational studies. The goal of this
paper is to provide an adequate differentiation of experimental studies that enables researchers to better understand which inferences can and – perhaps more important – cannot be made from particular
designs.
Keywords: Economic experiments, ceteris paribus, confounders, control, inference, randomization,
random sampling, superpopulation
JEL: B41, C18, C90

(a)

Martin Luther University Halle-Wittenberg, Institute of Agricultural and Nutritional Sciences, Chair
of Agribusiness Management, Karl-Freiherr-von-Fritsch-Str. 4, 06120 Halle (Saale), Germany.
Corresponding author and presenter (norbert.hirschauer@landw.uni-halle.de)

(b)

Martin Luther University Halle-Wittenberg, Institute of Agricultural and Nutritional Sciences, Chair
of Agribusiness Management, Karl-Freiherr-von-Fritsch-Str. 4, 06120 Halle (Saale), Germany
(sven.gruener@landw.uni-halle.de)

(c)

Georg August University Göttingen, Department for Agricultural Economics and Rural Development, Farm Management, Platz der Göttinger Sieben 5, 37073 Göttingen, Germany
(oliver.musshoff@agr.uni-goettingen.de)

(d)

Martin Luther University Halle-Wittenberg, Institute of Business Studies, Chair of Statistics, Große
Steinstraße 73, 06099 Halle (Saale), Germany.
(claudia.becker@wiwi.uni-halle.de)

1

Economic experiments and inference

Abstract: Replication crisis and debates about p-values have raised doubts about what we can statistically infer from research findings, both in experimental and observational studies. The goal of this
paper is to provide an adequate differentiation of experimental designs that enables researchers to better understand which inferences can and – perhaps more important –cannot be made from particular
designs.
Keywords: Economic experiments, ceteris paribus, confounders, control, inference, randomization,
random sampling, superpopulation
JEL: B41, C18, C90

1

Introduction

Starting with CHAMBERLIN (1948), SAUERMANN and SELTEN (1959), HOGGATT (1959), SIEGEL and
FOURAKER (1960), and SMITH (1962), economists have increasingly adopted experimental designs in
empirical research over the last decades. Their motivation to do so was to obtain – compared to observational studies – more reliable information about the causalities that govern the behavior of economic
agents. Unfortunately, it seems that in the process of adopting the experimental method, no clear-cut
and harmonized definition has emerged of what constitutes an economic experiment. Some scholars
seem to use randomization as the defining quality of an experiment and equate “experiments” with
“randomized controlled trials” (ATHEY and IMBEN 2017). Others include non-randomized designs into
the definition as long as behavioral data are generated through a treatment manipulation (HARRISON
and LIST (2004). One might speculate that economists tend to conceptually stretch the term “experiment” because the seemingly attractive label suggests that they have finally adopted “trustworthy”
research methods that are comparable to those in the natural sciences. Whatever the reason, confusion
regarding the often fundamentally different types of research designs that are labeled as experiments
entails the risk of inferential errors.
In general, the inferences that can be made from controlled experiments based on the ceteris paribus
approach where “everything else but the item under investigation is held constant” (SAMUELSON and
NORDHAUS 1985: 8) are different from those that can be made from observational studies. The former
rely on the research design to ex ante ensure ceteris paribus conditions that facilitate the identification
of the causal effects of a treatment. Observational studies, in contrast, rely on an ex post control of
confounders through statistical modeling that, despite attempts to move from correlation to causation,
does not provide a way of ascertaining causal relationships which is as reliable as a strong ex ante
research design (ATHEY and IMBEN 2017). But even within experimental approaches, different designs
facilitate different inferences.
In this paper, we address the question of scientific and statistical induction and, more particularly, the
role of the p-value for making inferences beyond the confines of a particular experimental study. What
we aim at is an adequate differentiation of experimental designs that enables researchers to better understand which inferences can and – perhaps more important –cannot be made from particular designs.

2

Experiments aimed at identifying causal treatment effects

A study is generally considered to be experimental when a researcher deliberately introduces a welldefined treatment (also known as intervention) into a controlled environment. Identifying the effects of

2

the treatment on the units (subjects) under study requires a comparison; often a no-treatment group of
subjects is compared to a with-treatment group. Two different designs are used to ensure control and
thus ceteris paribus conditions: (i) Randomized controlled trials rely on a between-subject design and
randomization to generate equivalence between compared groups; i.e. we randomly assign subjects to
treatments to ensure that known and unknown confounders are balanced across treatment groups (statistical independence). (ii) Non-randomized controlled trials, in contrast, rely on a within-subject design and before-and-after comparisons; i.e. we try to hold everything but the treatment constant over
time1 and compare the before-and-after-treatment outcomes for each subject of the experimental population.2
Table 1 sketches out experimental designs based on treatment comparisons. The label “experiment” is
first of all used for studies that, instead of using survey data or pre-existing observational data, are
based on a deliberate intervention and a design-based control over confounders. The persuasiveness of
causal claims depends on the credibility of the alleged control. Comparing randomized treatment
groups is generally held to be a more convincing device to identify causal relationships than beforeand-after treatment comparisons (CHARNESS et al. 2012).3 This is due to the fact that randomization
balances known and unknown confounders across treatment groups and thus ensures statistical independence.4 In contrast, efforts to hold everything else but the treatment constant over time in beforeand-after comparisons are limited by the researcher’s capacity to identify and fix confounders.
Table 1: Design-based control for confounding influences in experimental treatment comparisons
Randomized-treatment-group comparison (between-subject design)

Before-and-after-treatment comparison (within-subject design)

Intervention by researcher
(experimental study)

Randomized controlled experiments/trials

Non-randomized controlled experiments/trials

No intervention by researcher
(observational study)

“Natural experiments”

-

Due to the particular credibility of randomization as a means to establish control over confounders, the
use of the term “experiment” – accompanied by the label “natural” – has even been extended to observational settings (see gray print in Table 1) where, instead of a treatment manipulation by a researcher,
the socio-economic or natural environment has randomly assigned “treatments” among some set of
units. Regarding this terminology, DUNNING (2013: 16) notes “that the label ‘natural experiment’ is
perhaps unfortunate. […], the social and political forces that give rise to as-if random assignment of
interventions are not generally ‘natural’ in any ordinary sense of that term. […], natural experiments

1

Trying to hold “everything” but the treatment constant over time can be difficult. A particular threat to causal
inference arises when subjects’ properties change through treatment exposure. In other words, sequentially exposing subjects to multiple treatments may cause order effects that violate the ceteris paribus condition
(CHARNESS et al. 2012).
2
The term “non-randomized controlled trial” is also used for between-subject designs when the technique of
assigning subjects to treatments, e.g. alternate assignment, is not truly random but claimed to be as good as a
random.
3
CZIBOR et al. (2019) emphasize that within-subject designs also have advantages. Besides the fact that they can
more effectively make use of small experimental populations, they facilitate the identification of higher moments
of the distribution. Whereas between-subject designs are limited to estimating average treatment effects, withinsubject designs enable researchers to look at quantiles and assess heterogeneous treatment effects among subjects.
4
DUFLO et al. (2007) note that randomization only achieves that confounders are balanced across treatments “in
expectation;” i.e. balance is only ensured if we have a “sufficiently” large experimental population to start with
(law of large numbers). In the case of a very small experimental population, an observed difference between two
randomized treatment groups could easily be contaminated by unbalanced confounders.
3

are observational studies, not true experiments, again, because they lack an experimental manipulation. In sum, natural experiments are neither natural nor experiments.” 5

3

Inferences in experimental treatment comparisons

Sharing the essential quality of providing an ex-ante, design-based control over confounders, randomized-treatment-group comparisons and before-and-after-treatment comparisons facilitate causal inferences.6 The meaning of statistical inference and the p-value, however, is different in the two cases. In
randomized-treatment-group comparisons, the p-value associated with the treatment difference is
based on the approximation of the “randomization distribution” (cf. RAMSEY and SCHAFER 2013), i.e.
the distribution of the difference between group averages and the standard error used in a twoindependent-sample t-test (ANOVA). Regardless of the recruiting procedure used to obtain the study
population, this p-value targets the following question: when there is no treatment-group difference,
how likely is it that we would find a difference as large as (or larger than) the one observed when we
repeatedly assigned the experimental subjects at random to the treatments under investigation (VOGT
et al. 2014: 242). In randomized controlled experiments, the evaluation of internal validity and causal
inference can be aided by statistical inference based on the p-value, which represents a continuous
measure of the strength of evidence against the null hypothesis of there being no treatment effect in
the experimental study population.7 While scientific inferences beyond the confines of the experimental population are often desired, it is important to recognize that randomization-based inference is
no help for generalizing from experimental subjects to a wider population from which they have been
recruited. Using statistical inference to help make such generalizations would require that the experimental subjects are a random sample of a defined parent population; if they are not, extending inference from the experimental subjects to any other group must be based on scientific reasoning beyond
statistical measures such as p-values. This implies accounting for contextual factors and the entirety of
available knowledge including external evidence for the phenomenon under study.
Assuming a defined parent population from which experimental subjects are recruited through random
sampling, the question arises of how to link randomization-based inference, which is concerned with
internal validity and causality, to sampling-based inference, which is concerned with external validity
in terms of generalizing towards the parent population. The standard error of the randomization distribution would directly reflect the idea of frequently re-randomizing a given study population of, let’s
say, n =100 subjects in hypothetical experimental replications. The standard error in a twoindependent-sample t-test, in contrast, presumes that we repeatedly draw random samples of n = 100
from a population before carrying out the randomized experiment. While being motivated by sampling
from populations, two-sample t-tests are also applied to randomized-treatment-group comparisons.
The sampling-based standard error serves as (upwardly-biased) approximation of the standard error of
the randomization distribution (ATHEY and IMBEN 2017). If one accepts the approximation as sufficiently accurate, the resulting p-value can be used as an aid for assessing both the internal and the ex5

Using deliberate treatment manipulation as the defining quality of an experiment, HARRISON and LIST (2004)
speak of “doing natural field experiments” and use this label to tag a field experiment with subjects from the
relevant population and a covert manipulation of subjects’ real-life environment.
6
For the sake of simplicity, we limit our discussion in the following to binary treatments.
7
Even though the p-value is the cornerstone of the statistical methodology that is currently in dominant use, it
must be recognized that it is but a summary statistic of the data at hand from which inductive inferences do not
flow automatically; and “testing” against a point null hypothesis may often not be particularly useful for assessing how much knowledge one gained from a specific set of data. However, discussing the misinterpretations
and the per se limited inferential content of the p-value is not within the scope of this paper. For a discussion of
these issues in connection with the current debate on the reproducibility crisis see, for example, HIRSCHAUER et
al. (2016, 2018), MCSHANE et al. (2017), TRAFIMOV et al. (2018), WASSERSTEIN and LAZAR (2016), and ZILIAK
and McCloskey (2008).
4

ternal validity of an experiment. However, if the experimental population is not a random sample, the
interpretation must be strictly limited to the re-randomization of the given study population.
Contrary to randomization, a p-value associated with the treatment difference in before-and-aftertreatment comparisons is conceptually per se based on random sampling and the sampling distribution, i.e. the distribution of the average individual before-and-after difference and thus the standard
error in a paired t-test. This is just another label for a one-sample t-test on the variable “individual
before-and-after differences.” Statistical inference based on the one-sample p-value implies that we
concern ourselves with the question of what we can learn about the population mean from a random
sample. In other words, we are asking the following question: assuming there is no difference in the
population, how likely is it that we would find an average before-and-after difference as large as (or
larger than) the one observed if we carried out very many statistical replications and subjected repeatedly drawn random samples to the same treatment procedure. Therefore, our p-value is a continuous
measure of the strength of evidence against the null of there being no treatment effect in the parent
population. While being an inferential tool to help make generalizations from the sample of experimental subjects to a wider population (external validity), it must be recognized that assessing causality
in before-and-after comparisons is not within the scope of sampling-based inference. Instead, causality
claims, which hinge on the credibility of the ceteris paribus assumption, must be based on transparent
experimental protocols. What the p-value in a one-sample t-test does is to inform us about the random
sampling error, irrespective of whether our experimental procedure was successful in holding everything but the treatment constant over time or not. The only important assumption is that the procedure
that leads to the observation of individual before-and-after differences presumably remains unchanged
over all statistical replications.
Being a probabilistic concept based on a chance model (i.e. a hypothetical replication of a chance mechanism), p-values are not applicable if there is no random process of data generation (either randomization
or random sampling). When there is no randomization, maintaining the p-value’s probabilistic foundation poses serious conceptual challenges when there is no random sampling either because we already
have the data of the whole population (DENTON 1988: 166f.). An example is an experimental withinsubject design when the experimental population is clearly not a random sample, or when we do not
want to generalize beyond the confines of the study population to start with. In such cases, the sample
of experimental subjects constitutes the finite population to which we are limited. Due to the lack of a
chance mechanism that could hypothetically be replicated, there is no role for the frequentist p-value
and statistical significance testing.8 If p-values are nonetheless calculated, the frequentist statistician
would have to imagine an infinite “unseen parent population” (or “superpopulation”), i.e. an underlying stochastic mechanism that is hypothesized to have generated the observations in the population.
DENTON (1988) critically notes that this rhetorical device, which is also known as “great urn of nature,” does not evoke wild enthusiasm from everybody. “However, some notion of an underlying [random] process – as distinct from merely a record of empirical observations – has to be accepted for the
testing of hypotheses in econometrics to make any sense” (DENTON 1988: 167). We would add that

8

The fact that there is no room for statistical inference when we already have an entire population is formally
reflected in the finite population correction factor (KNAUB 2008). Rather than assuming that a sample was drawn
from an infinite population – or at least that a small sample of size n was drawn from a very large population of
size N – the finite population correction factor [(N-n)/(N-1)]0.5 accounts for the fact that, besides absolute sample
size, sampling error decreases when the sample size becomes large relative to the whole population. The correction reduces the standard error and is commonly used when sample share is more than 5% of the population.
Having the entire population corresponds to a correction factor of zero and thus a corrected standard error of
zero.
5

researchers who resort to the p-value in such circumstances should explicitly explain why and how
they base their inferential reasoning on the notion of an unseen “superpopulation.”

4

Experiments without treatment comparisons, and their inferences

In experimental treatment-group comparisons, the term “control” means first of all generating ceteris
paribus conditions (ex-ante control over confounders) with the objective of identifying causal treatment effects. This ex-ante control comes in two forms: in randomized-treatment-group comparisons,
control over confounders is achieved without exercising control over the environment, i.e. there is no
need to know, let alone fix and maintain all variables that could affect behaviors besides the treatment.
In before-and-after-treatment comparisons, in contrast, control over confounders requires that we exercise control over the environment and fix and maintain all relevant factors that could influence subjects’ behaviors besides the treatment under investigation.
Often, economic experiments do not settle for identifying causal treatment effects among experimental
subjects in more or less artificial experimental environments. Instead, experimenters want to learn
what governs the behaviors of certain social groups in relevant real-world situation and, eventually,
which policy interventions work in practice. This requires moving beyond causal inference within the
narrow confines of an experiment and tackling the question of external validity. In this exercise, statistical inference is only the first step, which is limited, as one should not forget, to the attempt of generalizing from the (experimental behavior of the) sample to the (experimental behavior of the) population. Therefore, experimental designs must additionally ensure “control over subjects’ preferences”
(SMITH 1982) in that subjects’ choices in the experiment should reveal their true preferences. That is,
in the attempt to achieve external validity, the term “control” covers a methodological aspect (i.e.
measurement validity) that goes beyond the ceteris paribus issue connected to causal inferences. A
large randomized experiment, for example, will effectively control for confounders and generate statistical independence of treatments. However, if otherwise ill-designed, we might be reduced to reliably observing the causal effects of the experimental treatment without learning much about subjects’
true preferences.
Control over subjects’ preferences is crucial for the external validity of economic experiments including randomized ones. However, this aspect of “control” is often more salient in economic experiments
that study only one treatment and do not aim for causal inferences through ceteris paribus treatment
comparisons. While still relying on an experimenter’s intervention, such experiments might be better
understood as devices aimed at measuring latent preferences (e.g. individual risk or social preferences)
in a way that allows us to generalize to relevant real-world settings, which usually differ to a lesser or
greater extent from the experimental environments. Prominent examples are experimental games such
as prisoner’s dilemmas, trust games, or public goods games that are implemented to find out, for instance, whether the choices made by individuals are in line with conventional rational choice predictions.9 Rather than addressing the statistical independence of different treatments and average treatment effects, “control” is now concerned with an intrinsic quality of the treatment itself. For example,
one might deliberate how large the real payments (incentives) that are linked to subjects’ abstract earnings in a dictator “game” would have to be to achieve a valid measurement in that these incentives
make subjects reveal their true prosocial preferences. Another example is the attempt to avoid “experimenter demand effects” that often threaten external validity because subjects are usually aware of
participating in an experiment and often inclined to please experimenters (DE QUIDT et al. 2018).

9

Of course, all these games could also be used within a randomized design. Simply imagine an experiment in
which subjects are randomly assigned to two dictator experiments with differing initial endowments.
6

5

Field experiments and quasi-experiments, and their inferences

Control over environmental factors decreases from lab experiments to field experiments, irrespective
of whether they are based on treatment comparisons or not. Table 2 highlights that any taxonomic
proposal that takes account of the diminishing control over the environment from the lab to the field is
open to debate – at least for non-randomized experiments. Attaching the label “experiment” to studies
that rely on proper randomization to control for confounders is likely to cause little controversy even
when they are carried out in the field where it is difficult to know, let alone fix all relevant factors
besides the treatment. In non-randomized designs, in contrast, the classification is likely to become
controversial at some point; i.e. an arguable minimum level of control over the relevant environment
would seem to be a prerequisite for calling a non-randomized approach an experiment. Irrespective of
the label, inferences must take account of the specific research design: we know that causal inferences
cannot be supported by the p-value when an experiment is not based on randomization. We also know
that random sampling from a defined parent population is the prerequisite for statistical inference towards that population; and when causal inferences are based on doubtful claims of control over confounders, one should consider alternative experimental designs (e.g. randomized instead of nonrandomized designs) or even a regression-based statistical control of observable confounders;10 and
when the control of subjects’ preferences is in question, one should avoid overhasty conclusions and
check the robustness of results in replication studies with more valid experimental designs – preferably
in field experiments with subjects from the relevant population and a manipulation of subjects’ reallife environments (HARRISON and LIST 2004).
Table 2: When is a study of behavior induced by an intervention called an experiment?

High Control over the
environment (lab)

Low control over the
environment (field)

Randomized-treatmentgroup comparisons

Before-and-aftertreatment comparisons

Designs without built-in
treatment comparisons

Experiment

Experiment

Experiment

Experiment

No experiment

No experiment

Often, non-randomized designs focus on the behavioral outcomes induced by an intervention in one
social group as opposed to another. Such designs are examples of “quasi-experiments” (CAMPBELL
and STANLEY 1966) in which the ceteris paribus condition is in question. For illustration, imagine a
dictator “game” in which a mixed-sex group of experimental subjects are used as first players who can

10

There is no need to resort to regression when proper randomization ensures ex ante that confounders are statistically independent of treatments. In some cases, switching to an ex-post control of confounders in a statistical
model may be appropriate, however. If so, it may be useful to realize how, in the simplest case without confounders, a treatment-group comparison relates to a linear model where we regress the response to a binary
treatment dummy and a constant. Generally speaking, the sampling distributions of estimated regression coefficients that link predictors to response are the distributions of the point estimates derived from a hypothetically repeated random sampling of the response variable at the fixed values of the predictors (RAMSEY and
SCHAFER 2013: 184). Using a dummy regression (and a p-value based on the sampling distribution) instead of
comparing two group averages (and a p-value based on the randomization distribution) can therefore be questioned on the grounds that it implies switching to a chance model that is at odds with the actually applied chance
mechanism. There are specific constellations (equal variance in both groups or, alternatively, robust regression
standard errors accounting for heteroscedasticity) that lead to identical standard errors. Group comparison and
dummy regression only coincide as long as the former is based on the sampling-based approximation of the
standard error of the randomization distribution. If the group comparison were based on the standard error of the
randomization distribution, we would obtain a lower standard error compared to which the standard error in the
regression would be upwardly biased (ATHEY and IMBEN 2017).
7

decide which share of their initial endowment they give to a second player (one person acts as second
player for the whole group). Additionally, assume that the experimental subjects are a pure convenience sample but not a random sample of a definable wider population. What kind of statistical inferences are possible? Neither one of the two chance mechanisms – random sampling or randomization –
applies. Consequently, there is no role for the p-value: (i) statistical inference towards a wider population beyond our experimental subjects is not possible because we are limited to a non-random sample.
(ii) statistical inference regarding causal relationships is not possible because there was no random
assignment of subjects to treatments. Instead, one treatment was used to obtain a behavioral measurement in two predefined social groups. We should therefore simply describe, without reference to a pvalue, the observed difference and the experimental conditions – or carry out a regression analysis to
control for confounders if necessary; for example, the male subjects may be more or less wealthy than
the female subjects which could be another explanation for the differences between the two groups.
Due to engrained disciplinary habits, researchers might be tempted to implement “statistical significance testing” routines in our dictator game example even though there is no chance model upon
which to base statistical inference. While there is no random process, implementing a two-sample ttest might be the spontaneous reflex to find out whether there is a “statistically significant” difference
between the two sexes. One should recognize, however, that doing so would require that some notion
of a random mechanism is accepted. In our case, this would require imagining a fictitious randomization distribution that would have resulted if the great urn of nature had randomly assigned money
amounts to sexes (“treatments”). Our question would be whether the money amounts transferred to the
second player differed more between the sexes than what would be expected in the case of such a random assignment. We must realize, however, that there was no random assignment of subjects (with all
their potentially confounding characteristics) to treatments, i.e. the sexes might not be independent of
covariates. Therefore, the p-value based on a two-sample t-test for a difference in mean does not address the question of whether the difference in the average transferred money amount is caused by the
subjects’ being male or female. That could be the case, but the difference could also be due to other
reasons such as female subjects being less or more wealthy than male subjects. As stated above, it
would therefore make sense to control for known confounders in a regression analysis ex post – again,
without reference to a p-value as long as the experimental subjects have not been recruited through
random sampling.

6

Conclusion

Over the last decades, the experimental method has become quite popular among economists. However, no clear-cut definition has as yet emerged of what constitutes an economic experiment. Usages
range from a broad perspective of “trying something out” to a narrow view of “applying randomization” to identify causal effects. Our paper has shown that an adequate differentiation of experimental
designs advances the understanding of what we can infer from different types of experimental studies.
Several points should be kept in mind: first, a random process of data generation – either random assignment or random sampling – is required for frequentist tools such as p-values to make any sense,
however little it may be. Second, the informational content of frequentist tools such as p-values are
different in randomization-based inference as opposed to sampling-based inference. Randomizationbased inference is concerned with internal validity and causality, whereas sampling-based inference is
concerned with external validity in terms of generalizing from a sample to its parent population. This
must be taken account of when p-values are used as an inferential aid in the interpretation of findings
from randomized (between-subject) designs as opposed to non-randomized (within-subject) designs.
Third, while being conceptually different, the sampling-based standard error used in a two-sample ttest can be used as an approximation in randomization-based inference. If one accepts the approxima8

tion, and if experimental subjects are recruited through random sampling, the resulting p-value can be
used as an aid both for assessing internal validity and for generalizing to the parent population. However, if the study population is not randomly recruited, statistical inferences must be limited to assess
the causalities within the given study population. Forth, in the context of economic experiments, there
are two essentially different meanings of the term “control” that must not be confused. In experiments
aimed at identifying causal treatment effects, control means first of all ensuring ceteris paribus conditions (statistical independence of treatments). Besides the ceteris paribus issue, the term “control” is
concerned with external validity beyond the sample-population relationship. The composite expression
“control over preferences” is used to indicate experimental designs in which a valid measurement is
achieved in that subjects can be believed to reveal their true preferences. This design quality, which is
crucial for making valid inferences, is part of scientific reasoning but cannot be aided by statistical
inferential tools.
Acknowledgment
The authors would like to thank the Deutsche Forschungsgemeinschaft (DFG, German Research
Foundation) for funding.

References
Athey, S., Imben, G.W. (2017): The Econometrics of Randomized Experiments. In: Banerjee, A.V., Duflo, E.
(eds.): Handbook of Field Experiments. Volume 1. Amsterdam: Elsevier: 74–140.
Campbell, D.T., Stanley, J.C. (1966): Experimental and Quasi-experimental Designs for Research. Chicago:
Rand McNally.
Chamberlin, E.H. (1948): An experimental imperfect market. Journal of Political Economy 56(2): 95–108.
Charness, G., Gneezy, U., Kuhn, M.A. (2012): Experimental methods: Between-subject and within-subject design. Journal of Economic Behavior & Organization 81(1): 1–8.
Czibor, E., Jimenez-Gomez, D., List, J.A. (2019): The Dozen Things Experimental Economists Should Do
(More of). NBER Working Paper No. 25451.
Denton, F.T. (1988): The significance of significance: Rhetorical aspects of statistical hypothesis testing in economics. In: Klamer, A., McCloskey, D.N., Solow, R.M. (eds.): The consequences of economic rhetoric.
Cambridge: Cambridge University Press: 163–193.
de Quidt, J., Haushofer, J., Roth, C. (2018): Measuring and Bounding Experimenter Demand. American Economic Review 108(11): 3266–3302.
Duflo, E., Glennerster, R., Kremer, M. (2007): Using randomization in development economics research: A
toolkit. In: Schultz, T. Strauss, J. (eds.): Handbook of Development Economics, Volume 4. Amsterdam:
Elsevier: 3895–3962.
Dunning, T. (2012): Natural Experiments in the Social Sciences: A Design-based Approach. Cambridge: Cambridge University Press.
Harrison, G.W., List, J.A. (2004): Field Experiments. Journal of Economic Literature 42(4): 1009–1055.
Hirschauer, N., Grüner, S., Mußhoff, O., Becker, C. (2018): Pitfalls of significance testing and p-value variability: An econometrics perspective. Statistics Surveys 12: 136–172.
Hirschauer, N., Mußhoff, O., Grüner, S., Frey, U., Theesfeld, I., Wagner, P. (2016): Inferential misconceptions and
replication crisis. Journal of Epidemiology, Biostatistics, and Public Health 13(4): e12066-1–e12066-16.
Hoggatt, A.C. (1959): An experimental business game. Behavioral Science 4: 192–203.
Knaub, J. (2008): Finite Population Correction (fcp) Factor. In: Lavrakas, P. (ed.): Encyclopedia of Survey Research Methods. THousand Oaks: Sage Publications: 284–286.
McShane, B., Gal, D., Gelman, A., Robert, C., Tackett, J.L. (2017): Abandon Statistical Significance.
http://www.stat.columbia.edu/~gelman/research/unpublished/abandon.pdf
Ramsey, F.L., Schafer, D.W. (2013): The statistical sleuth: a course in the methods of data analysis. Belmont:
Brooks/Cole.
Samuelson, P.A., Nordhaus, W.D. (1985): Economics. 12th ed. New York: McGraw-Hill.
9

Sauermann, H., Selten, R. (1959): Ein Oligopolexperiment. Zeitschrift für die gesamte Staatswissenschaft 115: 427–471.
Siegel, S., Fouraker, L.E. (1960): Bargaining and group decision making. New York: McGraw-Hill.
Smith, V.L. (1962): An Experimental Study of Market Behavior. Journal of Political Economy 70:111–137.
Smith, V.L. (1982): Microeconomic systems as an experimental science. The American Economic Review
72(5): 923–955.
Trafimov et al. (2018): Manipulating the Alpha Level Cannot Cure Significance Testing. Frontiers in Psychology. https://doi.org/10.3389/fpsyg.2018.00699
Vogt, W.P., Vogt, E.R., Gardner, D.C., Haeffele, L.M. (2014): Selecting the right analyses for your data: quant itative, qualitative, and mixed methods. New York: The Guilford Publishing.
Wasserstein, R.L., Lazar N.A. (2016): The ASA’s statement on p-values: context, process, and purpose, The
American Statistician 70(2): 129–133.
Ziliak, S.T., McCloskey, D.N. (2008): The Cult of Statistical Significance. How the Standard Error Costs Us
Jobs, Justice, and Lives. Ann Arbor: The University of Michigan Press.

10

