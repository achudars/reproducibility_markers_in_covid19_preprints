Optimal Climate Policy When Damages are
Unknown
Ivan Rudik∗
Integrated assessment models (IAMs) are economists’ primary tool for analyzing the optimal carbon tax. Damage functions, which link temperature
to economic impacts, have come under fire because of their assumptions that
may be incorrect in significant, but a priori unknowable ways. Here I develop recursive IAM frameworks to model uncertainty, learning, and concern
for misspecification about damages. I decompose the carbon tax into channels capturing state uncertainty, insurance motives, and precautionary saving.
Damage learning improves ex ante welfare by $750 billion. If damage functions are misspecified and omit the potential for catastrophic damages, robust
control may be beneficial ex post.
JEL: H23, Q54, Q58
Keywords: climate, integrated assessment, damages, deep uncertainty, robust control, learning, social cost of carbon, carbon tax

Integrated assessment models (IAMs) are economists’ primary tool for analyzing the optimal carbon tax. IAMs are macroeconomic models linked to
a climate module by the “damage function,” which translates rising temperature into economic losses.1 The world’s true underlying damage function
∗

Charles H. Dyson School of Applied Economics and Management, Cornell University,
Ithaca, NY, USA, 14853-6201. irudik@cornell.edu. I am immensely grateful for advice
and support from Derek Lemoine. I also benefited from comments and discussions with
Lint Barrage, Price Fishback, Alex Hollingsworth, Dave Kelly, Stan Reynolds, and Quinn
Weninger as well as participants at many seminars. Funding from The University of Arizona
GPSC Research Grant is gratefully acknowledged.
1
This is specific to the benchmark DICE model. Other IAMs may have explicit impacts
of CO2 . Despite varying levels of detail across IAMs, damage functional forms are largely
uncertain and some economists have claimed they are implemented in IAMs in an ad hoc
fashion (e.g. Pindyck, 2013).

1

is complex and characterized by deep uncertainties stemming from a lack of
knowledge of how warming will affect natural and economic systems. In order
to develop tractable models in the face of these unknowns, integrated assessment modelers have historically pinned down the damage function with strong
structural and parametric assumptions. Alongside the discount rate, the damage function is one of the most contentious feature of IAMs. In fact, some
economists have suggested abandoning quantitative integrated assessment because of the alleged arbitrariness of the damage assumptions underpinning it
(Pindyck, 2013, 2017).
Rather than abandoning the quantitative integrated assessment agenda
that has proved useful for analysis and policymaking, I integrate uncertainty
and even skepticism about damage functions into an IAM. I advance methodology in two ways. First, I incorporate parametric uncertainty about the parameters of the damage function and Bayesian learning about a parameter that
maps into the temperature elasticity of damages. Second, I implement robust
control, a technique to account for concerns from economists and scientists
about structural damage function uncertainty, namely that it is misspecified
in unknown ways. Using these methodological advances, I aim to ascertain the
policy and welfare implications of: (1) accounting for parametric uncertainty
over the damage function, (2) including endogenous damage learning over a
parameter that has been a central focus of the climate economics literature,
(3) allowing the policymaker to distrust the structure of her damage model via
the use of robust control, and (4) interacting learning with the policymaker’s
distrust of her own model.
Similar to approaches in the saving literature (e.g., Gollier, 2004), I decompose the optimal carbon tax into different channels representing the policy
implications of uncertainty and learning, and I present a novel channel representing concerns about model misspecification. The other uncertainty channels are analogous to precautionary saving and insurance motives and may
increase or decrease the optimal carbon tax depending on both the current
state of the world and whether the policymaker updates her damage beliefs.
Without learning, the total effect of these uncertainty channels initially de2

creases the carbon tax by a small amount, however over time it increases the
carbon tax by up to 1 percent. With learning, the total effect always increases
the carbon tax, by up to 5 percent. Insurance against model misspecification
increases the optimal carbon tax by up to 5 percent when not learning, and
slightly decreases the optimal carbon tax when learning.
Allowing for updating the estimate of the temperature elasticity of damages
leads to ex ante present value welfare gains of about $750 billion. Using robust
control to guard against model misspecification increases the carbon tax over
the next century, but if the model is correctly specified, these policy changes
lead to welfare losses of $150 billion. Since the purpose of robust control is to
guard against unknown misspecifications I also compute ex post welfare gains
when the damage function in the model is misspecified and the true damage
function has an extra catastrophic term similar to Weitzman (2012) and Dietz
and Stern (2015). Here I find that robust control can generate ex post present
value welfare gains if catastrophe sets in at sufficiently low temperatures or ex
post welfare is calculated over a sufficiently long time horizon.
In this paper I develop an IAM by combining an annual version of the
DICE model’s economic system, a generalized version of the DICE damage
function, and a reduced-form climate module that takes advantage of recent
findings in climate science. Damage functions have been a central focus of
analysis by researchers and have garnered a range of criticisms that may lead
a policymaker to distrust IAMs.2 Broadly, economists have voiced their concerns about the calibration and parameterization of damage functions. Some
researchers have suggested that the data used to calibrate damage functions
are missing sectoral damage estimates such as diminished ecosystem services
or increased healthcare costs (Howard, 2014), while others claim that the studies historically used in the calibration of damage functions underestimate the
actual impact of warming (Hanemann, 2008; Howard and Sterner, 2017). The
sharpest critiques focus on the functional form, saying that it is “...completely
2

See Diaz and Moore (2017) for a review of damage function critiques, one of which
is that there has not been sufficient attention paid towards parametric uncertainty and
stochasticity. Both are accounted for here.

3

ad hoc, with no theoretical or empirical foundation...” (Pindyck, 2013), and
that there is “...no rationale, whether empirical or theoretical, for adopting [the
DICE model’s] quadratic form...although the practice is endemic in IAMs...”
(Stanton, Ackerman and Kartha, 2009). Scientists and economists have said
the damage function is far removed from evidence in economics and natural
sciences (Ackerman and Stanton, 2012; Pindyck, 2013).
Damages functions in DICE and other IAMs often take a power functional
form
Damages = d1 [Temperature]d2 ,
where temperature is defined as the temperature increase over pre-industrial
levels, damages are the percent loss of GDP, and where d2 , the temperature
elasticity of damages, is assumed to be equal to 2 in DICE. The remaining
parameter d1 is calibrated by fitting the quadratic function to a set of the
existing damage estimates. Some kind of aggregate damage functional form
assumption is necessary for several reasons. First, optimizing climate policy requires us to be able to quantify the benefits of abating emissions, the
avoided damages. Second, there has historically been a lack of theory and
data to tell us what this damage functional form should be (Pindyck, 2013).
Last, the ideal approach to capturing damages would be to develop a detailed
competitive economy with a disaggregated set of sector- and location-specific
damage functions, and with substitution and trade responses to climate damages. This is highly burdensome computationally, particularly if a modeler
were to account for damage uncertainties.
Uncertainty and criticism about these key damage function parameters
have lead to a proliferation of research aimed at understanding the sensitivity of policy prescriptions to parametric assumptions and the implications of
parametric uncertainty. Recent work analyzing the sensitivity of optimal policy to both the functional form and calibration of the damage function has
demonstrated that changing the damage function parameters may have nontrivial policy impacts (Stanton, Ackerman and Kartha, 2009; Kopp et al.,
2012; Weitzman, 2012), and that several alternative polynomial functions can
fit real world data just as well as the quadratic form (Stern, 2006; Hsiang
4

et al., 2017). Because of the limitations of existing damage functions, some
economists have developed alternative polynomial damage functions that better fit stylized facts about how we expect damages functions to act at high
temperatures (Weitzman, 2010, 2012; Pindyck, 2012; Dietz and Stern, 2015),
or explored alternative polynomial forms or coefficients as a sensitivity analysis
(e.g. Dietz and Stern, 2015; Dietz and Venmans, 2019; Hsiang et al., 2017).3
Addressing these concerns about the damage function is critical since IAMs
have become a key component of determining environmental regulation. For
instance, a suite of IAMs has been used by the U.S. government to price greenhouse gas emissions in cost-benefit analyses of federal policies (Greenstone,
Kopits and Wolverton, 2013).
The weaknesses of current damage functions and uncertainty about the
economic consequences of additional warming indicate that damage uncertainty should be explicitly captured in modeling. Damage uncertainty has
been included only recently in IAMs (Crost and Traeger, 2013, 2014), because
the dynamic stochastic IAM literature is relatively new. The literature has
heavily focused on policy and learning when the sensitivity of the climate
to CO2 is uncertain (Kelly and Kolstad, 1999; Leach, 2007; Kelly and Tan,
2015; Fitzpatrick and Kelly, 2017; Hwang, Reynès and Tol, 2017; Lemoine
and Rudik, 2017), even though there is evidence that damage uncertainty has
more significant policy implications (Lemoine and McJeon, 2013). Dynamic
stochastic IAMs have also recently been used to explore tipping points or irreversibilities (Lemoine and Traeger, 2014; Cai et al., 2015; Lontzek et al., 2015;
Lemoine and Traeger, 2016a,b; Cai, Judd and Lontzek, 2018), and to examine
the implications of geoengineering (Heutel, Moreno-Cruz and Shayegh, 2016,
2018).4
3

Some of these papers use an exponential damage function however the exponential
function maps into an infinite monomial series.
4
The dynamic stochastic IAM literature expands on previous work that approximates
uncertainty using Monte Carlo methods (Hope, 2006; Stern, 2006; Nordhaus, 2008; Ackerman, Stanton and Bueno, 2010; Kopp et al., 2012), however Monte Carlo analyses do
not correctly capture decisionmaking under uncertainty (Lemoine and Rudik, 2017). There
has been recent calls for improved treatment of risk, uncertainty, and ambiguity in pricing
carbon (Stoerk, Wagner and Ward, 2018).

5

Our current level of uncertainty in estimates of climate damages is not permanent. Uncertainty can be resolved through research, and in recent years,
there has been an explosion of effort aimed at estimating sectoral climate
damages.5 Moreover, there have also been recent surveys and meta-analyses
aimed at updating estimates of aggregate global damage functions (Howard
and Sylvan, 2016; Howard and Sterner, 2017). The updating of damage functions, and anticipation of future damage function updates by a forward-looking
policymaker, has yet to be incorporated into an IAM.
Robust control and other techniques to account for model uncertainty
have been recently applied environmental and integrated assessment contexts
(Roseta-Palma and Xepapadeas, 2004; Athanassoglou and Xepapadeas, 2012;
Anderson et al., 2014; Li, Narajabad and Temzelides, 2016; Berger, Emmerling
and Tavoni, 2016).6 I build upon these strands of existing work by realistically capturing parametric uncertainty, endogenous learning and estimation of
damages, and structural misspecification concerns in a recursive IAM where a
policymaker trades off near-term costs of emissions reductions with the future
benefits of lower temperature via reduced damages.
This paper makes several contributions to the broader discussion about the
use of damage functions. First, if the standard damage functions in IAMs are
misspecified and the true damage function follows one of the recently proposed
catastrophic functional forms,7 protecting against model misspecification via
robust control has limited welfare effects over the next 200 years relative to
simply using a carbon tax optimized for a non-catastrophic damage function.
This suggests that if real world climate policy eventually follows IAM policy
prescriptions, guarding against these specific catastrophic misspecifications by
5
See Hsiang (2016), Hsiang, Oliva and Walker (2019) and National Academy of Sciences
(2017) for recent reviews of this literature. Hsiang et al. (2017) aggregates sectoral damage
estimates in the United States and estimates a US-specific damage function. They also
provide a framework for easily updating this damage function in response to new research.
6
Robust control has also been applied to a policymaker learning about the dynamics of
inflation and unemployment (Cogley et al., 2008).
7
These catastrophic forms are informed by scientific arguments. For example, Weitzman
(2012) and Dietz and Stern (2015) reference heat stress (Sherwood and Huber, 2010) and
tipping points (Kriegler et al., 2009).

6

using an even more aggressive carbon tax may not be of first-order importance.8 Second, when the true damage function has one of these catastrophic
forms, updating a misspecified damage function can increase or decrease welfare by trillions of dollars depending on the true damage functional form and
the time horizon for evaluating ex post welfare. This indicates that the realized social benefits of incorporating new damage estimates into IAMs will
be highly dependent upon the damage functional form assumptions of these
IAMs.
The paper is organized as follows. Section I gives more background on
the construction of the damage function, and provides an overview of the
dynamic stochastic IAM and all four frameworks. Section II describes how to
decompose the optimal carbon tax under uncertainty into different channels.
Section III reports results on how the different frameworks affect optimal policy
and welfare. Section IV concludes. The appendix fully describes the IAM in
this paper, details the computational methodology, provides a numerical error
analysis, and provides a full derivation of the optimal carbon tax and its
decomposition.

I

The Damage Function and the Integrated
Assessment Modeling Framework

I begin by giving a brief overview of the DICE damage function, which is the
foundation for the damage function in this paper. Next, I outline the IAM
as a whole and then describe the four frameworks I use to investigate damage
uncertainty, learning, and misspecification concerns through the use of robust
control.
8
If climate sensitivity was uncertain as in Kelly and Kolstad (1999), Kelly and Tan
(2015), or Lemoine and Rudik (2017) then a particularly sensitive climate may result in
extreme warming and catastrophic damages. In this case there may be large ex post benefits
from robust control. Here I do not explore uncertainty about the climate sensitivity but
interactions between these two uncertainties is important and should be studied in future
work.

7

I.A

The DICE Damage Function

s
In the benchmark DICE model, time t + 1 damages, D(Tt+1
), multiplicatively
g
reduce the level of time t + 1 output, Yt+1 , as a function of the time t + 1
s
.9 The damage function specific to the DICE model
surface temperature, Tt+1
is given by
s
s
D(Tt+1
) = d1 [Tt+1
]d2 ,

where d2 is assumed to be 2 and d1 is calibrated to estimates of damages from
specific levels of warming over preindustrial levels.
The most recent version of the DICE damage function is directly calibrated
to a set of monetized damage estimates contained in Tol (2009), with an upward adjustment of 25 percent to account for impacts that are more difficult
to estimate such as human conflict (Nordhaus and Sztorc, 2013). Even with
this change in the damage calibration procedure, it does not include damage
estimates from a burgeoning strand of literature that has estimated climate
damages in a variety of sectors.10

I.B

Four Frameworks for Damage Uncertainty and The
Dynamic Programming DICE Model

In light of critiques about the damage function, I develop four different frameworks for investigating the policy and welfare implications of damage uncertainty. The uncertainty framework has the policymaker applying distributions
over d1 and d2 and treating these parameters as uncertain. The learning framework allows the policymaker to learn and refine her beliefs about d2 over time.
The robust control framework has the policymaker being uncertain about both
9

Although not investigated here, there are models that incorporate damages directly on
capital (Kopp et al., 2012; Dietz and Stern, 2015) or utility (Sterner and Persson, 2008;
Barrage, Forthcoming). Sterner and Persson (2008) explicitly include non-market environmental services in the utility function with limited substitutability, which can be degraded
by warming. There is also empirical evidence that climate damages affect the growth rate
of output, not just the level (Dell, Jones and Olken, 2012), and this may have substantial
policy impacts (Moore and Diaz, 2015).
10
See Hsiang (2016) for references to several recent examples.

8

Emissions

Cumulative
Carbon System

Economy
Abate

Invest

Damages

Policymaker

d2 beliefs

Consume

Utility

Figure 1: A schematic of this paper’s IAM with learning. Capital, labor and
technology are combined to produce output. Emissions from factor production
flow into the cumulative carbon framework representation of the joint carbonclimate system. Greater cumulative emissions raise surface temperature which
damages output. A learning policymaker uses her observations of temperature
and realized damages to refine her beliefs. Her beliefs influence her policy
choices.
parameters, but the framework also applies robust control to capture concerns
that the damage function in the model is misspecified. Finally, the robust control and learning (RC+L) framework has the policymaker combine updating
her beliefs about d2 with the use of robust control.
The IAM I use to study damage uncertainty is a finite-horizon, annual
timestep Ramsey-Cass-Koopmans growth model coupled to a cumulative carbon framework representation of the joint carbon-climate system.11 The state
space is composed of capital; the cumulative emissions of CO2 ; a state to capture the realized amount of damages; and states for the location and scale
parameters governing the distribution of the policymaker’s beliefs about d2 .
Figure 1 displays a schematic of the model. The full model description can
be found in the online appendix. The model begins in 2005 and ends in 2605
11
This representation of the climate system has recently been gaining traction in climate
economics (Anderson et al., 2014; Berger and Marinacci, 2017; Brock and Xepapadeas, 2017;
Dietz and Venmans, 2019).

9

with a given terminal value function.12 Each period, the economy begins with
an existing level of capital, labor and technology. These are combined in a
Cobb-Douglas production function to generate output.
The production process generates CO2 emissions, and the cumulative amount
of CO2 emitted is stored as a state variable. IAMs like DICE often use a complex multi-state carbon-climate systems to model climate dynamics. However
recent advances in climate science have indicated that surface temperature
s
is proportional to the cumulative amount of CO2 emissions Et+1 , both in
Tt+1
the historical data and over different global climate model simulations spanning multiple climate scenarios and timeframes (Matthews et al., 2009; IPCC,
2013; Knutti, Rugenstein and Hegerl, 2017).13 Exploiting this reduced-form
relationship, surface temperature is then given by
s
Tt+1
= ζEt+1 ,

where ζ is called the transient climate response to emissions. Higher surface
temperature causes more damage and reduces output through the damage
function.
The remaining output after damages can be used in three ways: investment to increase the future stock of capital, abatement to reduce industrial
CO2 emissions caused by factor production, or consumption to increase flow
utility. The policymaker’s objective is to maximize her discounted expected
stream of utility from consumption. When deciding on the optimal way to
divide output between abatement, consumption, and investment, the policymaker uses her current beliefs about future damages. These beliefs are given
by probability distributions over the damage function parameters d1 and d2 ,
and the probability distribution over a multiplicative damage shock ωt+1 that
inhibits her learning about d2 . If the policymaker learns over time, she uses
12

The same terminal value function is used across all four frameworks since it is sufficiently
far in the future to have a negligible impact on policy. For example, given the growthadjusted discount factor in the model, $1 at the terminal time 2605 is discounted to far less
than $0.01 in 2105.
13
The online appendix contains more details on why this relationship arises.

10

Beginning
of period

Observe
realized
level of
damages

Update d2
distribution

Select
optimal
policy

Transition
to next
period’s
state

Figure 2: The timeline for each period t. The policymaker begins by observing
the realization of damages which was unknown prior to the beginning of the
period due to uncertainty about the damage function parameters and damage
shock. She then uses the damage observation to update her beliefs about d2 .
Finally, she optimizes her policy conditional on the current state of the world
which includes capital, CO2 , the amount of damages, and the parameters
governing her updated beliefs.
observations of surface temperature (or equivalently cumulative emissions) and
damages in a given period to update her beliefs about d2 .
The timing of the model is shown in Figure 2. The period begins and then
the policymaker observes the state of the world. Damages are unknown prior
to the beginning of the period because the policymaker does not know the true
value of the damage parameters and because of objective stochasticity in the
realization of the damage shock. After the policymaker observes the current
state, she uses Bayes’ Law to update her distribution over d2 . Next, with
her new beliefs, she selects levels of abatement, consumption, and investment
that maximize her expected welfare. Finally, the world transitions to the next
period.
The policymaker’s problem can be represented by a Bellman equation

Vt (St ) = max U (ct ) + βt Ed2 ,d1 ,ωt+1 [Vt+1 (St+1 )]
ct ,αt

(1)

subject to:
St+1 = f (St , ct , αt )
where St is the time t state vector, αt is abatement, ct is consumption, U (ct )
is her flow utility function, Vt (St ) is the policymaker’s time t value function, f
is the set of state transition equations, βt is a growth-adjusted discount factor,
and E is the expectation operator. Expectations are always taken using the
11

time t information set. In each period, the policymaker maximizes the sum
of her current flow utility and her discounted expected continuation value
where she takes expectations over the two damage function parameters and
the damage shock using her time t beliefs. The appendix contains the full
representation of the policymaker’s problem.

I.C

The Damage Function Calibration and Learning

In all four frameworks the policymaker takes both parameters of the damage
function as uncertain. At time t the policymaker assigns the damage coefficient a lognormal prior distribution, d1 ∼ logN (µc , σc2 ) and she assigns the
damage exponent a normal distribution, d2 ∼ N (µet , Σet ). In the frameworks
with learning, the policymaker updates her distribution over d2 each period.
There are no time subscripts on the d1 prior parameters since the policymaker
is not learning about d1 . I select these distributional families since they admit
Markovian updating rules which makes modeling learning tractable.14 The
policymaker’s immediate learning of the value of d2 is hindered by a sequence
of independent and identically distributed lognormal random damage shocks.
These shocks capture random variation in how warming affects factor production through, for example, random variation in the frequency and magnitude
of droughts or cyclones. The shock ωt+1 enters the damage function multiplicatively
s
s
D(Tt+1
, ωt+1 ) = d1 Tt+1

d2

ωt+1 .

(2)

Here I focus on time t + 1 damages so the timing matches the carbon tax
g
derivation in Section II. Yt+1
is the gross output from factor production prior
n
to any damages, and Yt+1 is net output after damages. Damages reduce the
14

Tractable learning has played a role in distributional family selection in the prior literature (e.g. Kelly and Kolstad, 1999; Kelly and Tan, 2015; Lemoine and Rudik, 2017).

12

level of gross output in the following fashion (Nordhaus, 2008)
n
Yt+1

=
1 + d1

g
Yt+1
.
d2
s
Tt+1
ωt+1

Rearranging the equation and taking the natural logarithm of both sides yields
an equation such that observed variables are on the left hand side, and all
unobserved variables are on the right hand side

log


g

Yt+1
s
− 1 = log(d1 ) + d2 log Tt+1
+ log(ωt+1 ).
n
Yt+1

(3)

When the policymaker begins time period t+1, she observes the stochastically
n
, and she infers gross output before
evolving level of output net of damages, Yt+1
g
damages, Yt+1 , using the production function, the level of technology, and the
capital and labor stocks.15 This comprises all the variables on the left hand
side. On the other side of the equation, log(d1 ), d2 , and log(ωt+1 ) are normally

s
distributed random variables that are not directly observed, while log Tt+1
is observed by the policymaker.
 g

Yt+1
Define Qt+1 ≡ log Y n − 1 . Since the random variables on the right
t+1
hand side of equation (3) are linearly separable, we have that




2
s
s
Qt+1 ∼ N µc + µet+1 log Tt+1
+ µω , σc2 + Σet log Tt+1
+ σω2

(4)

where µω and σω2 are the location and scale parameters of the damage shock
ωt+1 .16 With each observation of the random variable Qt+1 equal to some realized value qt+1 at time t + 1, the learning policymaker updates the parameters
15

This is an approximation to the real world learning process. In reality we observe net
output, but we may not know the global production function nor stock of capital that we
required to back out gross output. However, researchers have been estimating damages
over time in different sectors using observations of temperature fluctuations and measures
of production. See Hsiang (2016) for examples of recent research.
16
Since d1 is not being learned and the parameters of its distribution are constant, d1 ωt+1
could be combined into a single random variable d1 ωt+1 ≡ ω̃t+1 ∼ logN (µc + µω , σc2 + σω2 ).

13

of her prior over d2 according to Bayes’ Law
µet+1

 e
s
(σω2 + σc2 ) µet + log Tt+1
Σt [qt+1 − (µc + µω )]
=

2 e
s
(σω2 + σc2 ) + log Tt+1
Σt
Σet (σω2 + σc2 )
Σet+1 =
2 e .

s
Σt
(σω2 + σc2 ) + log Tt+1

(5)
(6)

µet+1 , the location parameter and mean of the policymaker’s beliefs about d2 , is
a weighted average of its previous value, and the realization of qt+1 −(µc + µω ),
a noisy signal of the true value of d2 . In expectation it is equal to µet . Σet+1 , is
the scale parameter and variance of the policymaker’s beliefs about d2 .
I estimate the distributions over d1 , d2 , and ωt+1 using data on temperature
and damages from the recent meta-analysis by Howard and Sterner (2017).17
Specifically I estimate the logarithm of the damage function definition in equation (2)
log(Damagesi ) = log(d1 ) + d2 log(Temperaturei ) + εi
using ordinary least squares where i indicates a separate estimate within the
meta-analysis.18 The estimate on the log temperature coefficient dˆ2 is distributed N (µe0 , Σe0 ) where µe2005 and Σe2005 are the point estimate and squared
standard error, and the subscript 2005 indicates these will be the values for
the policymaker’s initial beliefs in 2005. dˆ2 has the conventional regression
interpretation as the estimate of the temperature elasticity of damages. Simiˆ 1 ) ∼ N (µc , σ 2 ) yields the estimated distribution of log(d1 ), or if this
larly, log(d
c
term is exponentiated we have that d1 ∼ logN (µc , σc2 ). Finally the estimated
variance for the disturbance term εi yields the variance of log(ωt+1 ).
Table 1 shows the estimates for the location and scale parameters of the
three distributions. The location (µc ) and scale (σc2 ) parameters for d1 imply
an expectation of 0.00556 and a standard deviation of 0.00379. The expected
coefficient is approximately double that of the standard DICE calibration.
17

The dataset includes estimates of severe and catastrophic damages (Weitzman, 2010;
Burke, Hsiang and Miguel, 2015). The temperature data are in terms of anomalies and the
damage data are in percentage terms.
18
Taking the logarithm allows me to estimate the parameters with a linear regression.

14

Table 1: Estimates for the location and scale parameters of the lognormally
distributed damage coefficient, normally distributed damage exponent, and
lognormally distributed damage shock.
Coefficient
Parameter
Estimated Value

Exponent

σc2
0.23

µc
-5.31

µe2005
1.88

Shock

Σe2005
0.203

µω
-0.59

200
150

σω2
1.18

1°C

0.75

50

0.50

Density

Density

Density

150
100

100

0.25
50
2°C
3°C

0

0

0.00
0.000

0.005

0.010

0.015

Damage Coefficient

0.020

0

1

2

Damage Exponent

3

4

0.00

0.01

0.02

0.03

0.04

0.05

Damages

Figure 3: The probability density function of the coefficient’s distribution
(left), the probability density function of the exponent’s prior distribution in
2005 (middle), the probability density function of damages given 2005 beliefs
at 1◦ C, 2◦ C, and 3◦ C of warming (right).
The mean estimate for d2 is approximately equal the the selected value of 2
in DICE. The damage shock ωt+1 has mean of 1 and a standard deviation of
1.49.
The year 2005 distribution for d1 is shown in the left panel of Figure 3, and
the year 2005 distribution for d2 is shown in the center panel. The right panel
of Figure 3 displays the distribution of damages at 1◦ C, 2◦ C, and 3◦ C using the
year 2005 information set. The 1◦ C distribution is highly peaked at damages of
less than 1 percent of output, and the right tail decays rapidly. The 2◦ C and
3◦ C distributions peak at slightly higher damages, but are characterized by
right tails that decay more slowly and assign greater weight to higher damage
outcomes.

15

I.D

Robust Control

Ideally, skepticism about damage functions should be built into an IAM by
modeling a policymaker who does not completely trust that the damage function in her model is a precise representation of reality. In this case, the policymaker would believe that her damage model is only an approximation to
the real world, and she would recognize that the optimal policy she obtains
from her approximating model will almost surely not be optimal if taken and
applied in real world policy. Instead of striving to develop an optimal policy
using a model that is likely incorrect, the policymaker can instead try to find
policy rules that are robust to unknown, and in the short-term unlearnable,
errors in her model.19
To capture concerns about damage function misspecifications, I model a
policymaker who uses robust control techniques in order to find policies that
perform well even when the damage function inside her IAM may be incorrect
(Hansen and Sargent, 2007, 2008). The policymaker begins with her approximating model in equation (2) and the associated parameter distributions. She
then optimizes her policy, but she does not optimize policy conditional on only
her approximating model. She instead optimizes over a set potential models
that are close to her approximating model in terms of Kullback-Leibler divergence. The policymaker believes the true model likely resides in this set, but
because the models in this set are all relatively similar to her own, they are
difficult to statistically distinguish from one another over the time frame of her
policymaking. During policy optimization, alternative damage models more
similar to her approximating model are given greater weight, while those that
are more different get less weight. By taking this approach the policymaker
is recognizing that her approximating damage function is not exactly correct,
19

Implicitly the four frameworks are taking different approaches to estimating an unknown
data generating process for damages. In the uncertainty framework a monomial damage
function is estimated once before policymaking begins. The learning framework allows for
periodic updates to the model, the robust control framework considers the possibility of
other approximations to the damage function, and the framework combining robust control
and learning allows for the possibility of other damage functions with continual re-estimation
of the exponent on the approximating damage function.

16

and she wishes to guard against unknown errors.20
Omitting the transition equations, the policymaker’s objective under robust control can be formulated as a min-max problem (Hansen and Sargent,
2008)




Vt (St ) = min max u(ct ) + βt
qt+1

ct ,αt

Z


θDt+1 (pt+1 ||qt+1 ) + Vt+1 (St+1 ) dqt+1

,
(7)

where θ ∈ (0, ∞] is called the penalty parameter and will be described below,
Dt+1 (pt+1 ||qt+1 ) is the Kullback-Leibler divergence function


Z
Dt+1 (pt+1 ||qt+1 ) =

log

dpt+1
dqt+1


dpt+1 ,

and pt+1 is her approximating model’s probability measure for taking expectations. qt+1 is a distorted probability measure that she acts as though is the
true model when she selects her controls, however pt+1 still governs the actual
state transitions. In other words, qt+1 only affects the policymaker’s decision
rule, not the actual state transitions.
Hansen and Sargent (2008) provide an intuitive interpretation of the robust
control formulation in equation (7). qt+1 is the optimal selection of a probability measure by an evil agent whose objective is to minimize the payoff of the
policymaker. The evil agent selects a new qt+1 at each time t. The evil agent’s
selection of qt+1 is penalized proportionally to the Kullback-Leibler divergence
of its chosen distorted probability measure qt+1 relative to the approximating
model’s probability measure pt+1 , where the coefficient of proportionality of
the penalty is θ, the penalty parameter. Large values of θ significantly penalize
the evil agent for selecting probability measures qt+1 that are much different
than the approximating model pt+1 , while small values of θ let the evil agent
select large distortions without much cost.
The distortions by the evil agent are what induce robust decision rules
20

This is in contrast to maxmin expected utility which maximizes welfare subject to the
worst-case model in some set (Gilboa and Schmeidler, 1989).

17

on behalf of the policymaker. Since the evil agent aims to reduce the policymaker’s welfare, the distortions it selects skew the policymaker’s perceived
expected continuation value downward. This induces her to select policies
that are designed for lower welfare worlds, i.e. worlds where the damage function is more severe. By implicitly changing the policymaker’s beliefs about the
damage function to be more pessimistic, it induces her to uptake more precautionary action. How strongly she guards against misspecifications is inversely
related to the size of θ since it controls the cost of the evil agent selecting a
more distorted model qt+1 .
The policymaker maximizes her objective while accounting for potential
distortions to her approximating model’s probability measure, weighted by
their Kullback-Leibler divergence. Equation (7) is equivalent to replacing the
continuation value in the policymaker’s Bellman equation in equation (1) with
a risk sensitivity operator T 1 (Hansen and Sargent, 2007) that is tractable to
use in dynamic programming settings21


1



T (βt Vt+1 (St+1 )|θ) = −θlog Ed1 ,ωt+1



βt Vt+1 (St+1 )
exp −
θ


,

where the expectation operator is over d1 and ωt+1 . This results in a new
Bellman equation



Vt (St ) = max U (ct ) + Ed2 T 1 (βt Vt+1 (St+1 )|θ)
ct ,αt

(8)

subject to:
St+1 = f (St , ct , αt )
where the expectation acting outside the risk sensitivity operator is over d2 .22
21

The distortions induced by the risk sensitivity operator are equivalent to the policymaker
facing state-dependent shocks to the transition distributions, so that the distortions can be
temporally linked and persist over time. This is what allows the distortions to represent
real misspecifications to the policymaker’s model.
22
Since the risk sensitivity operator is acting directly on the time-invariant distributions of
d1 and ωt+1 , learning about d2 will not result in complete dissipation of the effects of robust
control, although changing beliefs about d2 may affect how the risk sensitivity operator
alters the policymaker’s decisions.

18

2

Undistorted EV

1

Value

0

Distorted EV: N(1,1)

−1

Distorted EV: N(1,8)

−2

−3

−4

Distorted EV: N(1,24)

0

1

2

3

4

5

Penalty Parameter ( θ )

Figure 4: Three examples of the risk sensitivity operator distorting distributions. The plot displays the distorted expected value of three normal distributions as a function of the the penalty parameter. All three distributions
have mean 1, but have variances equal to 1 (top), 8 (middle), or 24 (bottom).
The undistorted expected value is the dashed line and is independent of the
penalty parameter.
All expectations are taken using the time t information set.
Figure 4 shows three simple cases to illustrate how the risk sensitivity
operator distorts the policymaker’s expected continuation value. Consider a
case where the continuation value, Vt+1 , has a normal distribution with mean
equal to 1, and variance equal to 1, 8, or 24. For simplicity, let the discount
factor βt = 1 and assume d2 is known so we can ignore the expectation over
d2 outside the risk sensitivity operator.
When we apply the risk sensitivity operator to Vt+1 , the exponential term
inside the expectation over d1 and ωt+1 has a lognormal distribution which
allows us to obtain a closed form solution for the expected continuation value:
2
1 − 2σ θ where σ 2 is the variance.23 Figure 4 plots these distorted continuation
values as a function of the penalty parameter and illustrates two key points.
First, a smaller θ allows the evil agent to select transition probability measures
that are increasingly distorted: for any given distribution, a smaller θ results
23

The general takeaways will hold for other types of transition distributions but the normal
distribution delivers a closed form solution.

19

in a lower expected value relative to the undistorted expected value. Second, a
higher variance baseline continuation value, or equivalently a higher variance
baseline transition measure, results in a more distorted expected value for
some given θ. A higher variance baseline continuation value means that the
policymaker is putting more weight on potential extreme outcomes. This frees
up the evil agent to select larger distortions since this additional weight on
extreme values in the baseline model will tend to reduce Kullback-Leibler
divergence.
Figure 4 shows that as θ → ∞, the distorted expected value approaches its
undistorted value and we approach a subjective expected utility setting. As θ
decreases, the distorted expected continuation value declines. Smaller values
of θ make her expected future look worse and induces her to select policies that
better guard against misspecifications that would reduce her future welfare.
There typically exists some point θ̄, where the problem “breaks down” for
θ < θ̄.24 I calibrate θ by selecting close to the breakdown point that would
induce high levels of concern for model misspecification.25 For both frameworks
I set θ = 3.9. In Section III.B I show how varying the concern for model
misspecification alters policy.

II

The Policy Effects of Uncertainty and Concern for Model Misspecification

The time t optimal carbon tax is the shadow cost of time t emissions, et , and
is brought into dollar terms using the optimized time t marginal utility of
consumption. Time t emissions affect the following time t + 1 states: cumula24

The problem breaks down because when θ < θ̄, the exponential term in
T (β Vt+1 (St+1 )|θ) is large (e.g. it is much larger than 10100 ) so that it is equivalent
to numerical infinity. The problem is then no longer well-defined.
25
Hansen and Sargent (2008) demonstrate how to calibrate θ using detection error probabilities in a linear control setting. Athanassoglou and Xepapadeas (2012) formally derive a
closed form solution for the worst case misspecification exploiting a linear-quadratic model.
Since the DICE model is highly non-linear I take an approach similar to Gonzalez (2007)
where I explore how policy changes under different values of the penalty parameter in Section III.B.
1

20

tive emissions
(and thus temperature),
the fraction of output remaining after


damages Lt+1 = 1+d1 [T s 1 ]d2 ωt+1 , and the location and scale parameters.26
t+1
From here on I call Lt+1 fractional net output.27 Without loss of generality,
consider the optimal carbon tax for a policymaker using robust control. The
shadow cost of emissions is the negative partial derivative of the right hand
side of equation (8) with respect to time t emissions, et
h


i 
−βt Vt+1
−∂Vt+1 ∂St+1
exp
E
d
,ω
1 t+1
θ
∂St+1 ∂et
βt
,
h

i
Ed2 
Taxt = 0
u (ct )
Ed1 ,ωt+1 exp −βt θVt+1


where the expectations are separated because of the risk sensitivity

 operator
−βt Vt+1
,
T1 . The application of robust control introduces a new term, exp
θ
which when normalized by its expectation, corresponds to the twisting factor
of the worst-case transition distortion induced by the risk sensitivity operator
(Hansen and Sargent, 2007).
Now I will focus on only the key steps and terms in the derivation in order to
convey the intuition for the different carbon tax channels as cleanly as possible.
The full carbon tax derivation can be found in the online appendix. Passing
the expectations through and exploiting covariance identities, the carbon tax
equation can be rearranged to recover a conventional carbon tax expression
(first three lines) and an additively separable adjustment for robust control
(fourth line)
(


−∂Vt+1 ∂Et+1
Ed2 ,d1 ,ωt+1
(9)
∂Et+1
∂et






−∂Vt+1
∂Lt+1
−∂Vt+1 ∂Lt+1
+ Ed2 ,d1 ,ωt+1
Ed2 ,d1 ,ωt+1
+ covd2 ,d1 ,ωt+1
,
∂L
∂et
∂Lt+1
∂et
 t+1



e
e
−∂Vt+1 ∂µt+1
−∂Vt+1 ∂Σt+1
+ covd2 ,d1 ,ωt+1
,
+ Ed2 ,d1 ,ωt+1
e
∂µt+1
∂et
∂Σet+1
∂et

βt
Taxt = 0
u (ct )



26
In expectation, the effect of time t emissions on the time t + 1 location parameter is
zero but we will see that a covariance term arises that may not be zero.
27
I use fractional net output as a state instead of damages to obtain a bounded domain,
Lt+1 ∈ [0, 1].

21



"

+ Ed2 covd1 ,ωt+1 

−∂Vt+1 ∂St+1
,
∂St+1 ∂et E

exp
h

d1 ,ωt+1



−βt Vt+1
θ

exp





−βt Vt+1
θ

 #)
.

i 

Next, I introduce two state vectors. The first is the expected time t + 1
state vector from the perspective of the time t information set

υ := kt+1 , Et+1 , Ed2 ,d1 ,ωt+1 [Lt+1 ], µet , Σet+1 ,
which recognizes that Ed2 ,d1 ,ωt+1 [µet+1 ] = µet given time t beliefs, and the other
states without expectations transition deterministically.28 The second is the
certainty state
(
ce :=

)

kt+1 , Et+1 , Lt+1
d1 =exp(µc + 21 σc2 ),

2)
ωt+1 =exp(µω + 12 σω

, µet , 0 ,

which is the time t + 1 state that would be reached if all random variables
were set equal to their means and the prior variance was set to zero.
Last, I decompose the tax into channels that map closely into the consumptionsaving literature with a two step procedure. In the first step of the decomposition I perform a second-order Taylor expansion of the expectations over the
value function partial derivative terms on the first three lines of equation (9)
around the expected state υ.29 In the second step I add and subtract two sets
of identical terms evaluated at the certainty state ce to the Taylor expansion.
This results in an expression that I will group into seven different channels.30
To economize on space I omit both the leading βt /u0 (ct ) term in equation (9) and the Taylor expansion terms for the value function partial deriva28 e
µt+1
29

has a normal distribution and Lt+1 does not have a named distribution.
I require the value function to be three times differentiable in a number of the arguments.
This is not necessarily guaranteed by having the utility function be three times differentiable
and proving that my analytic value function is three times differentiable is not trivial. Here
I am using high order polynomial approximations to the value function that are greater than
third order so my approximating value function is trivially three times differentiable.
30
Lemoine and Rudik (2017) similarly demonstrate how to use Taylor expansions of the
value function to gain insight into how uncertainty over the climate’s sensitivity to emissions
drives optimal policy.

22

tives with respect to the scale parameter Σet+1 since they will be the same as
the other states. The derivation of the full Taylor expansions are in the online
appendix. For the subsequent analysis, recognize that ∂Et /∂et = 1, and that
fractional net output is decreasing in emissions (∂Lt+1 /∂et < 0).
Channel 1: Certainty tax
−∂Vt+1
∂Et+1

+
ce

−∂Vt+1
∂Lt+1

∂Lt+1
ce ∂et

ce

The first channel is the certainty tax. The certainty tax arises from adding
−∂Vt+1
∂Et+1

+
ce

−∂Vt+1
∂Lt+1

∂Lt+1
∂et
ce

to the full Taylor expansion around the expected
ce

state υ. The certainty tax is the shadow cost of an additional unit of emissions
when all uncertain terms are perfectly known and fixed at their time t means.
This is the tax that would be set by a policymaker that happened to be at the
current time t state St and states transitioned deterministically (Lemoine and
Rudik, 2017).
Channel 2: State uncertainty adjustment
−∂Vt+1
∂Et+1

ζ

−∂Vt+1
+
∂Lt+1


Ed2 ,d1 ,ωt+1
ζ

∂Lt+1
∂et

!


−

−∂Vt+1
∂Et+1

−∂Vt+1
+
∂Lt+1
ce

ce

∂Lt+1
∂et

The second channel is the state uncertainty adjustment. This channel alters
the certainty tax so that it correctly accounts for how uncertainty affects the
expected time t + 1 state. It is the difference in the tax evaluated at the
expected state and the tax evaluated at the certainty state; the second term
arises from subtracting the certainty tax terms from the full Taylor expansion.
The rest of the channels account for how uncertainty affects the expected
shadow cost of emissions because of curvature in the value function partial
derivatives.

23


ce

Channel 3: Precautionary abatement
1 −∂ 3 Vt+1
2 ∂Et+1 L2t+1

ζ

1 −∂ 3 Vt+1
vard2 ,d1 ,ωt+1 (Lt+1 ) +
2 ∂L3t+1

3

+

−∂ Vt+1
∂Et+1 ∂Lt+1 ∂µet+1

ζ




∂Lt+1
Ed2 ,d1 ,ωt+1
vard2 ,d1 ,ωt+1 (Lt+1 )
∂et
ζ


−∂ 3 Vt+1
∂Lt+1
e
Ed ,d ,ω
covd2 ,d1 ,ωt+1 (Lt+1 , µt+1 ) +
covd2 ,d1 ,ωt+1 (Lt+1 , µet
∂L2t+1 ∂µet+1 ζ 2 1 t+1 ∂et

The third channel is the precautionary abatement motive. The numerical results indicate that this channel tends to increase the carbon tax when not
learning about d2 and decrease the carbon tax when learning about d2 . The
third derivative of utility corresponds to how uncertainty about future consumption affects the policymaker’s contemporaneous saving decision (Leland,
1968; Dreze and Modigliani, 1972; Kimball, 1990). If uncertainty about future
consumption increases contemporaneous saving, the agent is said to exhibit
prudence. In a climate-economy setting, abatement is a form of environmental saving: increasing current abatement means that the policymaker forgoes
a sure consumption payoff now for increased consumption later due to lower
future damages. The second term on the first line captures this most clearly.
When not learning, the third derivative of her continuation value with respect
to fractional net output is positive so that
1 −∂ 3 Vt+1
2 ∂L3t+1


Ed2 ,d1 ,ωt+1
ζ


∂Lt+1
vard2 ,d1 ,ωt+1 (Lt+1 ) > 0,
∂et

and the policymaker increases abatement and her optimal carbon tax. The
opposite is true when learning. The precautionary motive also scales in size
with the variance of future output.
The first term on the first line captures similar precautionary abatement
motives in the face of uncertain output. If this term increases abatement then
the agent would be called cross-prudent (Gollier, 2010). The policymaker is
cross-prudent if she would prefer to have a mean-zero risk attached to fractional
net output when cumulative emissions are lower rather than when they are
higher.
The second line captures precautionary abatement motives because of cross24

prudence, and these channels generally increase the optimal carbon tax. Here,
the policymaker abates more because of co-variability between fractional net
output and her beliefs about the damage exponent. In the numerical results
the covariance terms are generally negative since smaller than expected Lt+1 is
the signal that the policymaker would need to receive to revise her expectations
about d2 upward. This covariance matters because when future beliefs about
damages are uncertain, then future output and future consumption appear to
be even more variable. Whatever information the policymaker receives about
output in the future feeds back onto her payoffs through her expectations.
Channel 4: Signal smoothing
1 −∂ 3 Vt+1
2 ∂Et+1 ∂µet+1 2

vard2 ,d1 ,ωt+1 (µet+1 )
ζ



∂ Lt+1
1 −∂ 3 Vt+1
Ed ,d ,ω
vard2 ,d1 ,ωt+1 (µet+1 )
+
2 ∂Lt+1 ∂µet+1 2 ζ 2 1 t+1 ∂et

The fourth channel is the signal smoothing motive which tends to increase
the optimal carbon tax in the numerical results. This channel captures one
effect of learning on the optimal tax. The second derivative of the value
function with respect to the location parameter, −∂ 2 Vt+1 /∂µet+1 2 , captures
how well the policymaker can use new information to smooth welfare over
possible values of d2 . Since a higher location parameter for the policymaker’s
beliefs strictly increases her expectation of d2 , and since −∂ 2 Vt+1 /∂µet+1 2 > 0 in
the simulations, the marginal welfare cost of a higher d2 belief is increasing and
convex. If there is not much curvature in her beliefs and ∂ 2 Vt+1 /∂µet+1 2 is small,
then she is able to smooth welfare effectively, but the larger ∂ 2 Vt+1 /∂µet+1 2 is in
magnitude, the less she can smooth welfare, and the more costly a bad signal
of d2 becomes.
The triple derivatives then indicate how a marginal increase in cumulative emissions or fractional net output affects the policymaker’s ability to
smooth welfare in response to new information. Indeed, additional emissions
decrease her ability to smooth welfare (−∂ 3 Vt+1 /∂Et+1 ∂µet+1 2 > 0), and having a greater fraction of her gross output would increase her ability to smooth
welfare (−∂ 3 Vt+1 /∂Lt+1 ∂µet+1 2 < 0). Additional emissions results in greater
25

damages and less output to be able to use towards abatement, while additional fractional net output has the opposite effect. In these cases, additional
variability in future beliefs magnifies this effect and increases the optimal tax.
Channel 5: Output insurance

covd2 ,d1 ,ωt+1

−∂Vt+1 ∂Lt+1
,
∂Lt+1
∂et



The fifth channel is the output insurance channel. In simulations, this channel
decreases the optimal carbon tax over the first 90 years when not learning
before eventually increasing the carbon tax. It always increases the optimal
carbon tax in the simulations with learning. Output insurance increases the
optimal carbon tax if and only if the covariance is positive. The first term
in the covariance captures the marginal welfare cost of reducing fractional net
output, and the second term in the covariance captures the marginal reduction
in fractional net output from emissions. The policymaker cares about the covariance of returns to emissions reductions with marginal utility. This channel
increases the optimal carbon tax if emissions reductions are most effective in
preserving output when output is most valuable to welfare.
Channel 6: Learning insurance

covd2 ,d1 ,ωt+1

−∂Vt+1 ∂µet+1
,
∂µet+1
∂et



The sixth channel is the learning insurance channel. This channel generally increases the optimal carbon tax in the simulations. Learning insurance motives
are similar to output insurance motives in that the policymaker cares about
the covariance between the effect of emissions reductions on expectations and
marginal welfare. The covariance term is generally positive so this channel
reduces the optimal level of emissions.

26

Channel 7: Misspecification insurance



−βt Vt+1
exp
θ
−∂Vt+1 ∂St+1
h

i 
,
Ed2 covd1 ,ωt+1 
−βt Vt+1
∂St+1 ∂et E
d1 ,ωt+1 exp
θ




(10)

The final channel is the misspecification insurance channel. The magnitude
and direction of the misspecification insurance channel depends on how the
marginal welfare cost of emissions covaries with the worst-case distortion to
the transition density. The misspecification insurance channel increases the
carbon tax if and only if the covariance is positive. This channel increases
the optimal carbon tax when not learning and slightly decreases the optimal
carbon tax when learning. A decrease in the optimal carbon tax is consistent
with previous findings that concerns about model misspecification increase
motives for policy experimentation (Cogley et al., 2008).

III

Results

First, I show the mean optimal carbon tax trajectories and the corresponding climate outcomes over the next century for each of the four frameworks.
Next, I vary the robust control penalty parameter to examine how the policymaker’s concern for model misspecification affects policy. Then, I decompose
the carbon taxes into each channel outlined in Section II to determine how
uncertainty matters for policy. Finally, I analyze welfare outcomes under the
different frameworks. In each simulation run, I randomly sample a vector of
annual damage shocks {ω2005 , ..., ω2105 } from the damage shock distribution.
In cases where I take ex ante expectations of the optimal carbon tax, I also
randomly sample a {d1 , d2 } pair from their year 2005 prior distributions in
each simulation.

27

200

Uncertainty
Learning
Robust Control
RC + Learning

250

Optimal Carbon Tax ($/tCO2)

Optimal Carbon Tax ($/tCO2)

250

150

100

50

200

150

100

0

50

0
2005

2030

2055

2080

2105

2005

2030

Year

(a) Ex ante Expected Tax

2080

2105

(b) d1 = 0.01, d2 = 1.88

250

250

200

200

Optimal Carbon Tax ($/tCO2)

Optimal Carbon Tax ($/tCO2)

2055

Year

150

100

50

0

150

100

50

0
2005

2030

2055

2080

2105

2005

Year

2030

2055

2080

2105

Year

(c) d1 = 0.0056, d2 = 3.0

(d) d1 = 0.01, d2 = 3.0

Figure 5: The ex ante expected optimal carbon tax over 50,000 simulations for each framework (top left), and the expected optimal carbon tax
under specific damage functions over 5,000 simulations for each framework
(other three panels). For the top left panel, each of the 50,000 simulations
randomly samples a different set of damage parameters and damage shocks
{d1 , d2 , ω2005 , ..., ω2105 }. For the other three panels, all simulations use the
same damage parameters listed below the panel, and a random sample of
damage shocks {ω2005 , ..., ω2105 }.

28

III.A

Optimal Carbon Tax Trajectories

Figure 5 displays the first century’s optimal carbon tax trajectories in $/tCO2 .31
The top left panel displays the ex ante expected carbon tax trajectories averaged over 50,000 simulations. Each simulation randomly draws a pair of damage function parameters and a sequence of annual damage shocks {d1 , d2 , ω2005 , ..., ω2105 }
from their distributions defined by the estimated parameters in Table 1. The
remaining three panels display results averaged over 5,000 simulations when d1
and d2 are fixed to a single value across all 5,000 simulations, but each simulation still randomly draws a sequence of annual damage shocks {ω2005 , ..., ω2105 }.
The top right panel sets the true coefficient to 0.01 and the true exponent equal
to its expectation, the bottom left panel sets the true coefficient to its expectation and the true exponent to 3.0, and the bottom right panel sets the true
coefficient to 0.01 and the true exponent to 3.0.
The top left panel plots the ex ante expected carbon tax trajectories, i.e.
the tax trajectories the policymaker expects to set over the first 100 years given
her 2005 information set. Under the uncertainty framework, the policymaker
begins her optimal tax in 2005 at $13.68/tCO2 and expects to ramp it up to
$86.76/tCO2 in 2105. When the policymaker is able to learn d2 over time, she
sets an initial carbon tax 2 percent lower at $13.34/tCO2 , which on average
rises to $81.44/tCO2 at the end of the century. Applying robust control to
guard against potential model misspecification tends to increase the tax when
not updating beliefs. In 2005 the robust control tax begins slightly higher
than the uncertainty framework at $14.52/tCO2 . Over the first century, the
robust control tax is expected to increase faster than the uncertainty tax and
reaches $90.99/tCO2 at the end of the century. Applying robust control on
top of learning increases the initial carbon tax with learning by $0.15/tCO2 ,
and by 2105 the ex ante expected carbon tax with both robust control and
learning is $1.41/tCO2 higher than just learning alone.
The top right panel shows the mean carbon tax trajectories when the damage coefficient is about double the policymaker’s expectation and the damage
31

Some papers report carbon taxes in terms of dollars per ton of carbon. The unit
conversion is 11 tons of CO2 to three tons of carbon.

29

exponent is equal to her expectation. Initial carbon taxes start at the same
level as in the top left panel since the policymaker is making decisions with the
same information set. When facing a higher damage coefficient, the carbon
taxes under the uncertainty and robust control frameworks ramp up at a slower
rate, reaching $82.89/tCO2 for the uncertainty framework and $86.95/tCO2
for the robust control framework. The large damage coefficient results in more
damage and less output to allocate toward abatement so the policymaker sets
a lower carbon tax. The greater level of damages acts like a negative income
effect. The two frameworks with learning have carbon tax trajectories that
initially decrease. Temperature starts below 1◦ C and the damage function has
a monomial form, so when a learning policymaker observes higher damages
in the early years she attributes it to a smaller damage exponent and revises
her expectation about d2 downward to approximately 1.6. When temperature crosses the 1◦ C threshold, she starts revising her beliefs about d2 upward
and begins quickly increasing her carbon tax which reaches $171.34/tCO2
and $173.37/tCO2 in 2105 under the learning and robust control and learning
frameworks.32
The bottom left panel shows the mean carbon tax trajectories when the
damage coefficient is equal to the policymaker’s expectation and the damage
exponent is about double her expectation. Similar to the top right panel, the
non-learning frameworks’ carbon taxes are lower than ex ante expectations
because the policymaker has less output to allocate toward abatement under a
more severe damage function. Optimal carbon taxes reach only $81.42/tCO2
and $85.44/tCO2 in 2105 for the uncertainty and robust control frameworks.
Over the first few decades, carbon taxes under the learning and robust control and learning frameworks are similar to ex ante expected levels plotted in
the top left panel. Damage functions with different exponents yield similar
levels of damages at low temperatures so its difficult to statistically distinguish them. In this setting, the policymaker’s mean belief under the learning
and RC+L frameworks only increases to about 1.9 by 2035 when temperature
reaches about 1.1◦ C. In the second half of the century the policymaker begins
32

The policymaker expects d2 ≈ 2.8 at the end of the first century.

30

Table 2: The ex ante expected optimal carbon tax in 2055 and 2105, CO2
and temperature in 2105, and the 5th and 95th percentiles (in parentheses)
for each framework over 50,000 simulations. Each of the 50,000 simulations
randomly samples a different set of damage parameters and damage shocks
{d1 , d2 , ω2005 , ..., ω2105 }.
Framework

2055 Tax ($/tCO2 )
2105 Tax ($/tCO2 )
2105 CO2 (parts per million)
2105 Temperature (◦ C)

Uncertainty

Learning

RC

RC+L

37
(36,37)
87
(82,90)
613
(612,613)
2.08
(2.08,2.08)

35
(25,46)
81
(30,138)
620
(575,666)
2.10
(1.95,2.26)

39
(38,39)
91
(86,94)
611
(610,611)
2.07
(2.07,2.07)

36
(25,47)
83
(30,140)
618
(573,665)
2.10
(1.94,2.25)

learning more rapidly and quickly increases her carbon tax to $173.62/tCO2
and $175.41/tCO2 in 2105 for the learning and RC+L frameworks.
The bottom right panel plots mean carbon tax trajectories when the damage coefficient and damage exponent are both higher than the policymaker’s ex
ante expectation. This is the most severe damage function so the non-learning
frameworks face their smallest output budgets and set carbon taxes of only
$74.04/tCO2 and $77.77/tCO2 in 2105. High damages generate high learning
carbon taxes. In 2105 they reach $232.58/tCO2 for the learning framework
and $234.47/tCO2 for the RC+L framework. The learning policymakers attribute the high observed amounts of damages entirely to a large exponent,
so that expectations about d2 overshoot its true value and reach about 3.6 in
2105.
Table 2 displays the mean, 5th and 95th percentile outcomes for the 2055
carbon tax, the 2105 carbon tax, and 2105 atmospheric CO2 concentrations
and and temperature. Learning results in significant variability in realized
carbon taxes since the policymaker adapts her policy to the noisy information
she receives about each simulation’s specific d2 . After 50 years the learning
31

carbon tax is about $2/tCO2 less than the uncertainty tax on average, but in
some cases it may be nearly 30 percent lower or 30 percent higher. After 100
years the learning carbon tax is about 7 percent smaller than the uncertainty
carbon tax on average, but it still displays substantial variability depending on
the realized values of the damage function parameters and the damage shocks.
After 50 years the mean robust control carbon tax is $2/tCO2 higher than
the mean uncertainty carbon tax, but after 100 years the difference grows to
$4/tCO2 .
When learning, the policymaker allows CO2 to be 7 parts per million (ppm)
higher in 2105, which results in greater warming compared to the uncertainty
framework. Conversely, the more aggressive carbon tax in the robust control
framework tends to keep CO2 lower than the uncertainty framework by 2
ppm in 2105. There is also significant variability in climatic outcomes when
learning. The 90 percent confidence interval for 2105 CO2 is about 90 ppm
wide. This is approximately the same as the real world change in atmospheric
CO2 concentrations from 1960–2018.

III.B

The Effect of Concern for Model Misspecification

Figure 6 shows how changing the level of the policymaker’s concern about
model misspecification affects the initial optimal carbon tax. Smaller values
of the penalty parameter indicate greater misspecification concern. The left
panel of Figure 6 displays the optimal carbon tax as a function of the penalty
parameter for the robust control framework. When the penalty parameter is
sufficiently large, the year 2005 optimal carbon tax is effectively equal to the
optimal carbon tax of the uncertainty framework. As the penalty parameter
declines and we move to the left on the plot, the optimal carbon tax increases,
and on this plot, peaks at $14.52/tCO2 when the penalty parameter is 3.9.
Decreasing it further results in the problem breaking down so the model no
longer solves.
The right panel of Figure 6 displays the initial optimal carbon tax for the
robust control and learning framework. Again, a sufficiently large penalty

32

13.50

●

●

14.25

14.00

●

13.75

●

●
●

●

●

●

Year 2005 Optimal Carbon Tax ($/tCO2)

Year 2005 Optimal Carbon Tax ($/tCO2)

14.50

13.45
●

●

13.40

●

●
●

13.35

Breakdown

●

●

Breakdown

13.50

13.30
10

100

1,000

10

Penalty Parameter

100

1,000

Penalty Parameter

(a) Robust Control Framework

(b) RC+L Framework

Figure 6: The year 2005 optimal carbon taxes as a function of θ for the robust
control framework (left) and the RC+L framework (right). The dashed line
indicates the approximate location of θ̄ where the problem breaks down. Note
that each panel has a different y-axis scale and both x axes are on a log10
scale.
parameter leads the optimal carbon tax to be effectively equal to the learning framework without robust control. Decreasing the penalty parameter to
3.9, right before the problem breaks down, increases the 2005 carbon tax to
$13.49/tCO2 .

III.C

Decomposing the Optimal Carbon Tax

Figure 7 plots the six channels that capture uncertainty’s effect on the optimal carbon taxes plotted in the top left panel of Figure 5.33 Note that all
panels have different scales to better display differences between the different
frameworks. The certainty tax is in the appendix. Panel (a) plots the state
uncertainty adjustment, the strongest of the channels. For the uncertainty
framework and robust control framework, the state uncertainty adjustment
33
The total effect of these channels is to increase the optimal carbon tax above the certainty tax by up to 1 percent for the uncertainty framework, 5 percent for the robust control
framework, 5 percent for the learning framework, and 7 percent for the RC+L framework.

33

Uncertainty
Learning
Robust Control
RC + Learning

0.15

Contribution to Optimal Carbon Tax ($/tCO2)

Contribution to Optimal Carbon Tax ($/tCO2)

4

3

2

1

0

0.10

0.05

0.00

−0.05
2005

2030

2055

2080

2105

2005

2030

Year

(a) State Uncertainty Adjustment

2080

2105

(b) Precautionary Abatement
0.100

0.05

Contribution to Optimal Carbon Tax ($/tCO2)

Contribution to Optimal Carbon Tax ($/tCO2)

2055

Year

0.04

0.03

0.02

0.01

0.075

0.050

0.025

0.000

0.00
2005

2030

2055

2080

−0.025

2105

2005

2030

Year

0.02

0.01

0.00
2030

2055

2080

2105

(d) Output Insurance
Contribution to Optimal Carbon Tax ($/tCO2)

Contribution to Optimal Carbon Tax ($/tCO2)

(c) Signal Smoothing

2005

2055

Year

2080

Year

(e) Learning Insurance

2105

2

1

0
2005

2030

2055

2080

2105

Year

(f) Misspecification Insurance

Figure 7: The ex ante expectations of the six carbon tax channels related
to uncertainty. The expectations are taken over 50,000 simulations, where
each simulation randomly samples a different set of damage parameters and
damage shocks {d1 , d2 , ω2005 , ..., ω2105 }.
34 Note that each panel has a different
y-axis scale.

begins slightly below zero but becomes positive at 2030 and then grows in
magnitude over time to about $1/tCO2 by 2105. The change in sign is driven
by the curvature of fractional net output in the three random variables. At
lower levels of warming, fractional net output is convex in d1 and ωt+1 , and
concave in d2 . The convexity in d1 and ωt+1 dominates early on and makes
fractional net output larger at the expected state versus the certainty state.
This results in a negative state uncertainty adjustment because on the margin,
the shadow cost of emissions is smaller if output is higher. As temperature
rises, the concavity in d2 begins to dominate, making the uncertainty adjustment positive. For the two frameworks with learning, the state uncertainty
adjustment is always positive, larger in magnitude than without learning, and
monotonically increases in size from $0.33/tCO2 and $0.42/tCO2 in 2005 to
$2.71/tCO2 and $3.86/tCO2 in 2105. Learning makes future beliefs variable
and thus future states are more variable. This tends to increase the optimal
carbon tax.
Panel (b) plots the precautionary abatement motive. Initially, precautionary abatement is effectively zero since variability in damages is negligible when
temperature is low. Damage variability increases as temperature rises, leading
to more precautionary abatement for the non-learning frameworks because the
third derivative of their value functions with respect to fractional net output
is positive, and less precautionary abatement for the frameworks with learning
because the third derivative of their value functions with respect to fractional
net output is negative. In the learning frameworks, increased damage variability is partially offset by the policymaker resolving uncertainty so this channel’s
magnitude is dampened.
Panel (c) plots the signal smoothing channel, which is specific to the frameworks with learning. This channel increases the optimal carbon tax because
emissions hinder the policymaker’s ability to smooth welfare. The signal
smoothing channel starts below $0.01/tCO2 in 2005 and declines as her future beliefs become less variable. As temperature rises, emissions increasingly
reduce the policymaker’s ability to smooth welfare to new information. This
begins dominating the decrease in belief variability around 2030 and the signal
35

smoothing channel increases thereafter to $0.05/tCO2 in 2105.
Panel (d) displays the effect of output insurance. This channel eventually
increases the optimal carbon tax for all frameworks. This channel is near zero
at first since surface temperature is low and there is not much variability in
the marginal effect of emissions on fractional net output. For the non-learning
frameworks this channel decreases initially because the covariance between the
effect of emissions on fractional net output with the marginal welfare cost of
less fractional net output is negative, but it eventually becomes positive and
reaches $0.01/tCO2 . For the learning frameworks the size of the channel grows
at an increasing rate to approximately $0.09/tCO2 .
Panel (e) displays the effect of learning insurance. This channel tends to
have the smallest impact out of all six channels. It generally increases the
optimal carbon tax because additional emissions tend to increase future mean
beliefs about d2 (∂µet+1 /∂et > 0) when welfare is most sensitive to increases
in the mean belief about d2 (−∂Vt+1 /∂µet+1 > 0). This channel is near zero at
first since temperature is low and there is not much variability in the marginal
effect of emissions on fractional net output. The size of the channel grows at
an increasing rate to approximately $0.03/tCO2 at 2085 before declining.
Panel (f) plots the misspecification insurance channel. The channel increases the carbon tax when not learning and decreases the carbon tax when
learning. In both cases, this channel begins small as there is less variability
early on when temperature is low, but the size of misspecification insurance
grows over time for both frameworks using robust control. This is because the
marginal benefit of emissions reductions and the distortion to the transition
density become larger and increasingly correlated. The size of misspecification insurance reaches over $2/tCO2 without learning, and about -$0.02/tCO2
when learning. A negative effect of robust control on a learning carbon tax is
consistent with findings of increased experimentation motives in other settings
(Cogley et al., 2008). A learning policymaker can accelerate the learning process by allowing faster warming. Robust control amplifies this experimentation
motive.34
34

The scale parameter is strictly decreasing in temperature so allowing temperature to

36

III.D

Welfare Implications of Learning and Robust Control

Next I investigate the relative welfare performance of the frameworks. First,
I quantify ex ante welfare differences assuming that the policymaker has correctly specified her model so that the true damage function parameters are
drawn from the distributions in Section I.C. Results for the learning framework gives us the expected benefits of updating the distribution over d2 , while
the results for the robust control framework gives us the costs of designing policy to guard against misspecifications when we have actually specified damages
correctly.35
Second, I quantify ex post welfare when the policymaker’s approximating
damage model may be misspecified and the true damage function is instead
one of three catastrophic damage functions. The first two are the damage
functions of Weitzman (2012) and Dietz and Stern (2015) where damages
follow close to DICE at low levels of warming, but rise rapidly to 50 percent
of output at 6◦ C and 4◦ C respectively.36 The third is a more extreme version
of these two damage functions where damages hit 50 percent of output at only
3◦ C. Damages are subject to the same sequence of damage shocks with the
parameters estimated in Section I.C. Since I am exploring outcomes when
models are misspecified in the second part of the welfare analysis, I must
simulate the model and calculate the welfare outcomes over a finite horizon.
The frameworks’ value functions only yield the true ex ante expected welfare
when the model is specified correctly.
Table 3 displays ex ante welfare results for each of the frameworks relative
to the uncertainty framework when the policymaker has correctly specified
her damage model. The first line displays the lump sum present value benefit
of using one of the frameworks over the uncertainty framework, the second
rise quicker will reduce the variance of beliefs.
35
Recall that the robust control framework does not distort the actual state transitions,
only the policymaker’s beliefs about them.
36
Except for very high levels of warming, the DICE damage function implies less damage
than a damage function with parameters given by the distributional means associated with
Table 1.

37

Table 3: Measures of ex ante welfare of the learning, robust control, and robust
control and learning frameworks relative to the uncertainty framework.
Framework
2005 Lump Sum (billions of $)
2005 Per Capita ($)
BGE Gain (percent)

Learning

Robust Control

Robust Control + Learning

758
116
0.039

-156
-24
-0.008

615
94
0.032

line displays the present value benefit in per capita terms, and the final line
shows this in terms of balanced growth equivalent (BGE) consumption gain
(Mirrlees and Stern, 1972; Lemoine and Traeger, 2014; Jensen and Traeger,
2016). The BGE is the difference in growth rates between two counterfactual
consumption trajectories that grow at a constant rate and also yield the same
welfare as the uncertainty framework and the comparison framework.
When damages are specified correctly, learning is worth $758 billion, a one
time payment in 2005 of $116 per person, or a permanent gain in consumption
of 0.039 percent. These welfare gains come about because the policymaker
uses observations of damages to better match her policy to the actual damage
function she faces. When the policymaker uses robust control but the damage
function is correctly specified, she incurs losses of $156 billion or equivalently
$24 per capita. Robust control induces the policymaker to use too high of
a carbon tax which results in less consumption and lower welfare. Below
I explore outcomes where the damage function is misspecified since that is
the motivating factor for using robust control. The RC+L framework yields
a present value benefit of $615 billion, 20 percent less than without robust
control.
Figure 8 displays the present value ex post welfare gain for each of the
frameworks relative to the uncertainty framework for four different time horizons. I compute ex post welfare for each combination of framework, damage
function, and time horizon by first performing 10,000 simulations, each with a
randomly drawn vector of annual damage shocks. I then average the sum of the
present value of flow utilities across the 10,000 simulations. Finally I translate
the expected present value from utils into dollars using the initial marginal
38

utility of consumption from the uncertainty framework’s optimal trajectory.
The horizontal axis on each plot in Figure 8 shows the four different time
horizons over which ex post welfare is evaluated: 50, 100, 150, and 200 years.
The vertical axis shows the present value expected welfare gain relative to
the uncertainty framework. The uncertainty framework is omitted since it
is the baseline framework. An equivalent level of welfare as the uncertainty
framework is denoted by the dashed line. Plots above the dashed line indicate
gains relative to the uncertainty framework and plots below the dashed line
indicate losses. Each plot corresponds to one of the three catastrophic damage
functions. All three catastrophic damage functions have the following form
s
s
s
D(Tt+1
, ωt+1 ) = (d1 [Tt+1
]d2 + d3 [Tt+1
]d4 )ωt+1 .

The damage functions have the following common parameter values: d1 =
.00284, d2 = 2, d4 = 6.754. For the Weitzman damage function d3 = 5.07 ×
10−6 , for the Dietz and Stern damage function d3 = 8.19 × 10−5 and for the
extreme damage function d3 = 5.85 × 10−4 . ωt+1 is the damage shock drawn
from the same distribution as before.
The left panel displays outcomes when the policymaker’s damage model is
misspecified and the true damage function is the one developed in Weitzman
(2012). Learning (with or without robust control) delivers ex post welfare gains
over time horizons of 50–150 years. Although the policymaker has misspecified her model by omitting the extra catastrophic term, and she mis-learns by
using the information from new damage observations to update d2 instead of
changing the structure of her damage function, learning a misspecified damage
function improves ex post welfare for a set of time horizons. Until temperature
rises substantially more than the initial level, the Weitzman damage function
results in relatively low damages. The policymaker believes this is because she
faces a low true value of d2 and revises her expectation down to about 1.28 in
2105 when temperature is only 2.22◦ C. Despite the damage function misspecification, this helps her push policy in the right direction for the near-term low
damage world and she only sets a carbon tax of $32/tCO2 in 2105. Welfare

39

Learn

200

●

●

●

RC+L

●

RC

−400

0

PV Welfare Gain (billion $)

PV Welfare Gain (billion $)

PV Welfare Gain (billion $)

0

40000
●

●

RC+L
400

RC

Learn

●

−200

−400

30000

20000

RC+L
●

●
●

−800
50

100

150

Horizon Length (years)

(a) Weitzman

200

RC
0

−600
50

100

150

Horizon Length (years)

(b) Dietz and Stern

Learn

10000

200

●

●

50

100

150

200

Horizon Length (years)

(c) Extreme

Figure 8: The present value welfare gain of the learning, robust control, and
robust control and learning frameworks relative to the uncertainty framework
for three different catastrophic damage functions. The left panel is for the
Weitzman (2012) damage function where damages reach 50 percent of GDP
at 6◦ C, the center panel is for the Dietz and Stern (2015) damage function
where damages reach 50 percent of GDP at 4◦ C and the right panel is for an
extreme damage function where damages reach 50 percent of GDP at 3◦ C. Ex
post welfare gains are averaged over 10,000 simulations, each with a random
sample of damage shocks {ω2005 , ..., ω2105 }.
gains are over $400 billion for 100 and 150 year time horizons. Over time,
this low carbon tax results in accelerated warming and eventually, and unexpectedly to the policymaker because of her misspecification, massive damages.
Damages are nearly 4 times the levels of the uncertainty framework in the last
50 years of the simulation, and consumption under the learning framework is
lower than under the uncertainty framework by tens of trillions of dollars per
year. The accumulation of damage more than offsets the early welfare gains
and generates total losses of over $750 billion in present value terms by 2205.
The robust control framework results in ex post welfare losses of up to $60
billion even out to 200 years in the future. This is because when a non-learning
policymaker is optimizing carbon taxes under her misspecified damage function, temperature never crosses 3◦ C, so the catastrophic part of the Weitzman
(2012) damage function does not kick in. Over time horizons up to 200 years,
the robust control policymaker implements too stringent of a carbon tax relative to the uncertainty framework.
40

The middle panel shows welfare outcomes when the true damage function
is the one developed in Dietz and Stern (2015). Here learning results gains
of about $100 billion relative to the uncertainty framework over the first 100
years, large losses at 150 years, and small gains at 200 years. Similar to the left
panel, the policymaker believes early on that she is not in a high damage world
because temperature does not reach the point where the catastrophic part of
the damage function kicks in. Near the end of the first century, she believes
d2 to be as low as 1.72. Correspondingly, she sets a moderate carbon tax
of $43/tCO2 in 2075 as opposed to a tax of $54/tCO2 under the uncertainty
framework, and she reallocates output toward consumption which increases
present value ex post welfare at a 100 year horizon. In the second century,
temperature crosses 2◦ C and gets closer to the catastrophic part of the damage
function. The policymaker begins revising her beliefs upward and by 2155 she
expects d2 to be 2.30. This rapid shift in her beliefs leads to a reallocation of
output from consumption toward abatement. The learning carbon tax triples
over a 50 year time span, from $79/tCO2 in 2105 to $255/tCO2 in 2155,
while under the uncertainty framework the carbon tax only about doubles over
this timespan, from $87/tCO2 to $159/tCO2 . This aggressive climate action
results in short term welfare losses between 2105 and 2155, but it improves ex
post welfare relative to the uncertainty framework by 2205 through reduced
warming and damages. Adding robust control on top of learning increases
welfare by about $100 billion in 2205 relative to learning alone because of
more aggressive climate action.
When using robust control but not learning, positive net benefits can be
achieved relative to the uncertainty framework after 150 years. Expected ex
post welfare gains are $180 billion at 200 years in the future. Over the first
century, temperature is below 2◦ C and the Dietz and Stern (2015) damage
function closely follows the actual DICE damage function. Over this timeframe guarding against misspecification results in a carbon tax that’s too high
for a non-catastrophic damage function and slight welfare losses. After 150
years, temperature crosses 2.5◦ C for both the robust control and uncertainty
frameworks, and damages start growing quicker as temperature begins reach41

ing catastrophic levels. By the end of the second century, the avoided damages
from the robust control framework’s more aggressive carbon tax preserves an
additional half a percentage point of output per year.
The right panel shows welfare outcomes under the extreme damage function
where damages reach 50 percent of output at only 3◦ C. Here learning delivers
large ex post welfare gains after the first century. The catastrophic part of the
damage function kicks in rapidly and the learning policymaker quickly adapts
her carbon tax in the right direction despite her misspecified damage function.
At the end of the first century, the learning and RC+L policymakers set carbon
taxes over twice as high as under the uncertainty framework and have nearly
achieved full abatement of carbon emissions. By the end of the second century
the learning and RC+L carbon taxes are 8 times as large. Learning achieves
welfare gains of up to $35 trillion dollars after 200 years. Using robust control
alone also begins delivering positive welfare gains at around 100 years. At this
point the robust control tax is only about 5 percent larger than the uncertainty
tax. After 200 years, robust control welfare gains are $3 trillion and the robust
control tax is now 15 percent larger. Learning welfare gains are an order of
magnitude larger than robust control welfare gains.

IV

Conclusions

Most economists believe that damage functions in IAMs are misspecified. Yet,
analyses of optimal climate policy have not investigated the effects of endogenously improving the damage function over time, or for adapting policy to take
account of the widely noted concerns about damage function misspecification.
I fill this gap in the literature by comparing the performance of four policy
frameworks with different degrees of damage assumptions. The uncertainty
framework accounts for uncertainty over all parameters of the damage function. The learning framework acknowledges that economists do learn over time
and allows the policymaker to endogenously update her distribution over the
uncertain temperature elasticity of damages. The robust control framework
borrows techniques from the macroeconomic literature to incorporate concerns
42

that the damage function is misspecified in unknown ways. The robust control policymaker constructs policies that guard against model misspecification.
Last, the robust control and learning framework combines concern for model
misspecification with updating of beliefs over the temperature elasticity of
damages. For each of the frameworks, I demonstrate how misspecification
concerns, and uncertainty about damages and future beliefs feed into the optimal carbon tax.
I find that uncertainty over the damage parameters generally increases the
optimal carbon tax through the state uncertainty adjustment. A decomposition of the carbon tax into six uncertainty-related channels reveals that there
are conflicting effects of uncertainty on the optimal carbon tax due to precaution and insurance motives. Depending on whether the policymaker learns
about the damage function, motives to insure against model misspecification
may increase or decrease the carbon tax, but the total effect of robust control
is to increase the carbon tax.
Learning about the temperature elasticity of damages can deliver ex ante
welfare gains worth hundreds of billions of dollars, indicating that current research aimed at updating damage functions has the potential to be extremely
valuable. Ex ante welfare losses from using robust control when the model
has been correctly specified are $156 billion. In cases where the model’s damage function is misspecified and the true damage function instead allows for
catastrophe at temperatures of 3◦ C or higher, robust control and (incorrect)
learning can both achieve higher welfare than accounting for parametric damage uncertainty alone, but with several caveats. Learning and robust control
both perform well when the damage function has been highly misspecified,
such as in worst-case scenarios where massive damages occur at temperatures
we are expected to reach well before the end of the century along a business
as usual trajectory. However if damages are expected to be low and noncatastrophic until 4◦ C or higher, this may not be true. Learning about a
misspecified damage function can backfire and reduce welfare by erroneously
ruling out that there is catastrophe further down the line.
This paper’s results make several contributions to the broader discussions
43

about the specification of damage functions and the updating of damage functions.37 The first is to raise the question about whether a focus on protecting
against Weitzman (2012) and Dietz and Stern (2015) style catastrophe is of
first-order importance since the effects of a common method to guard against
misspecification are small relative to that of learning. One omitted factor in
my analysis is that uncertainty about climate sensitivity should make these
catastrophes more relevant, however I leave adding climate sensitivity uncertainty into this framework to future work.38 The second contribution is to
emphasize the high stakes of current research on estimating damages. The
welfare effects of updating beliefs are large, and their magnitude and sign depend on the form of the true damage function. The damage functional form
onto which updated estimates are placed will have a major influence on the
real world effectiveness of IAM policy prescriptions.
The sensitivity of welfare to damage functional forms suggests that damage
functions must simultaneously be more flexible and also better grounded in
science in order to improve the utility of IAMs for policymaking. Continually
updating quadratic or even general monomial damage functions will not adequately capture non-linearities. However, in order to estimate future damages
under temperatures never before seen in human history, we cannot simply rely
on flexible polynomial forms and statistical updating. Future temperatures
will be far outside our historical sample and extrapolating without any structure on the damage function will give us poor estimates of future damages.
More scientific guidance is needed to tell modelers what damage observations
today might mean for the structure of the real world damage function at
never before seen temperatures. The dynamic stochastic IAM literature has
begun making inroads in using scientific knowledge to better incorporate tipping points into models (e.g. Lemoine and Traeger, 2014, 2016b; Cai, Judd
and Lontzek, 2018), but there is much work left to be done to address other
37

Beyond the greater damage function debate, this paper also points out the relevance of
inter-generational equity. The costs of a more robust climate policy will be borne by current
generations but the benefits will not be felt until potentially hundreds of years in the future.
38
Future work may also focus on learning about both parameters of the damage function
or modeling learning in a way that is more agnostic about the damage functional form.

44

areas of the climate-economy system.

45

References
Ackerman, F, and Elizabeth A. Stanton. 2012. “Climate Risks and Carbon Prices: Revising the Social Cost of Carbon.” Economics : The OpenAccess, Open-Assessment E-Journal, 6(10).
Ackerman, Frank, Elizabeth A. Stanton, and Ramón Bueno. 2010.
“Fat Tails, Exponents, Extreme Uncertainty: Simulating Catastrophe in
DICE.” Ecological Economics, 69(8): 1657–1665.
Anderson, Evan W., William A. Brock, Lars Peter Hansen, and
Alan H. Sanstad. 2014. “Robust Analytical and Computational Explorations of Coupled Economic-Climate Models with Carbon-Climate Response.”
Athanassoglou, Stergios, and Anastasios Xepapadeas. 2012. “Pollution
Control with Uncertain Stock Dynamics: When, and How, to be Precautious.” Journal of Environmental Economics and Management, 63(3): 304–
320.
Barrage, Lint. Forthcoming. “Optimal Dynamic Carbon Taxes in a ClimateEconomy Model with Distortionary Fiscal Policy.” The Review of Economic
Studies.
Berger, Loic, and Massimo Marinacci. 2017. “Model Uncertainty in Climate Change Economics.” , (Cmcc): 1–33.
Berger, Loı̈c, Johannes Emmerling, and Massimo Tavoni. 2016. “Managing catastrophic climate risks under model uncertainty aversion.” Management Science, 63(3): 749–765.
Brock, William, and Anastasios Xepapadeas. 2017. “Climate change
policy under polar amplification.” European Economic Review, 99: 93–112.
Burke, Marshall, Solomon M Hsiang, and Edward Miguel. 2015. “Climate and Conflict.” Annual Review of Economics, 7(1): 577–617.
46

Cai, Yongyang, Kenneth L. Judd, and Thomas S. Lontzek. 2018.
“The Social Cost of Carbon with Economic and Climate Risks.” Journal of
Political Economy.
Cai, Yongyang, Kenneth L Judd, Timothy M Lenton, Thomas S
Lontzek, and Daiju Narita. 2015. “Environmental Tipping Points Significantly Affect the Cost-Benefit Assessment of Climate Policies.” Proceedings
of the National Academy of Sciences, 112(15): 4606–4611.
Cogley, Timothy, Riccardo Colacito, Lars Peter Hansen, and
Thomas J. Sargent. 2008. “Robustness and U.S. Monetary Policy Experimentation.” Journal of Money, Credit and Banking, 40(8): 1599–1623.
Crost, Benjamin, and Christian P. Traeger. 2013. “Optimal Climate
Policy: Uncertainty Versus Monte Carlo.” Economics Letters, 120(3): 552–
558.
Crost, Benjamin, and Christian P. Traeger. 2014. “Optimal CO2 Mitigation Under Damage Risk Valuation.” Nature Climate Change, 4(7): 631–636.
Dell, Melissa, Benjamin F Jones, and Benjamin A Olken. 2012. “Temperature Shocks and Economic Growth: Evidence from the Last Half Century.” American Economic Journal: Macroeconomics, 4(3): 66–95.
Diaz, Delavane B., and Frances C. Moore. 2017. “Quantifying the Economic Risks of Climate Change.” Nature Climate Change, 7(11): 774–782.
Dietz, Simon, and Frank Venmans. 2019. “Cumulative carbon emissions
and economic policy: in search of general principles.” Journal of Environmental Economics and Management, 96: 108–129.
Dietz, Simon, and Nicholas Stern. 2015. “Endogenous Growth, Convexity
of Damage and Climate Risk: How Nordhaus’ Framework Supports Deep
Cuts in Carbon Emissions.” Economic Journal, 125(583): 574–620.
Dreze, Jacques H., and Franco Modigliani. 1972. “Consumption Decisions Under Uncertainty.” Journal of Economic Theory, 5(3): 308–335.
47

Fitzpatrick, Luke G, and David L Kelly. 2017. “Probabilistic Stabilization Targets.” Journal of the Association of Environmental and Resource
Economists, 4(2): 611–657.
Gilboa, I, and D Schmeidler. 1989. “Maxmin Expected Utility with NonUnique Prior.” Journal of Mathematical Economics, 18: 141–153.
Gollier, Christian. 2004. The Economics of Risk and Time. MIT press.
Gollier, Christian. 2010. “Ecological Discounting.” Journal of Economic
Theory, 145(2): 812–829.
Gonzalez, Fidel. 2007. “Precautionary Principle and Robustness for a Stock
Pollutant with Multiplicative Risk.” Environmental and Resource Economics, 41(1): 25–46.
Greenstone, M., E. Kopits, and A. Wolverton. 2013. “Developing a
Social Cost of Carbon for US Regulatory Analysis: A Methodology and
Interpretation.” Review of Environmental Economics and Policy, 7(1): 23–
46.
Hanemann, WM. 2008. “What is the Economic Cost of Climate Change?”
CUDARE Working Papers.
Hansen, Lars Peter, and Thomas J. Sargent. 2007. “Recursive Robust
Estimation and Control Without Commitment.” Journal of Economic Theory, 136(1): 1–27.
Hansen, Lars Peter, and Thomas J. Sargent. 2008. Robustness. Princeton, N.J.:Princeton University Press.
Heutel, Garth, Juan Moreno-Cruz, and Soheil Shayegh. 2016. “Climate Tipping Points and Solar Geoengineering.” Journal of Economic Behavior & Organization, 132: 19–45.
Heutel, Garth, Juan Moreno-Cruz, and Soheil Shayegh. 2018. “Solar
Geoengineering, Uncertainty, and the Price of Carbon.” Journal of Environmental Economics and Management, 87: 24–41.
48

Hope, Chris. 2006. “The Marginal Impact of CO2 from PAGE2002: An
Integrated Assessment Model Incorporating the IPCC’s Five Reasons for
Concern.” Integrated Assessment, 6: 19–56.
Howard, Peter. 2014. “Omitted Damages: What’s Missing from the Social
Cost of Carbon.”
Howard, Peter H, and Derek Sylvan. 2016. “The Wisdom of the Economic
Crowd: Calibrating Integrated Assessment Models Using Consensus.”
Howard, Peter H, and Thomas Sterner. 2017. “Few and not so far between: a meta-analysis of climate damage estimates.” Environmental and
Resource Economics, 68(1): 197–225. doi:10.1007/s10640-017-0166-z.
Hsiang, Solomon M. 2016. Annual Review of Resource Economics, 8: 43–75.
Hsiang, Solomon, Michael Delgado, Shashank Mohan, D J Rasmussen, Robert Muirwood, Paul Wilson, Michael Oppenheimer,
Kate Larsen, and Trevor Houser. 2017. “Estimating Economic Damage
from Climate Change in the United States.” Science, 1369(June): 1362–1369.
Hsiang, Solomon, Paulina Oliva, and Reed Walker. 2019. “The distribution of environmental damages.” Review of Environmental Economics
and Policy, 13(1): 83–103.
Hwang, In Chang, Frédéric Reynès, and Richard SJ Tol. 2017. “The
effect of learning on climate policy under fat-tailed risk.” Resource and Energy Economics, 48: 1–18.
IPCC. 2013. Climate Change 2013: The Physical Science Basis: Working
Group I Contribution to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change. Cambridge University Press.
Jensen, Svenn, and CP Traeger. 2016. “Pricing Climate Risk.”
Kelly, David L., and Charles D. Kolstad. 1999. “Bayesian Learning, Growth, and Pollution.” Journal of Economic Dynamics and Control,
23: 491–518.
49

Kelly, David L, and Zhuo Tan. 2015. “Learning and Climate Feedbacks:
Optimal Climate Insurance and Fat Tails.” Journal of Environmental Economics and Management, 72: 98–122.
Kimball, Miles S. 1990. “Precautionary Saving in the Small and in the
Large.” Econometrica, 58(1): 53–73.
Knutti, Reto, Maria A. A. Rugenstein, and Gabriele C. Hegerl. 2017.
“Beyond equilibrium climate sensitivity.” Nature Geoscience, 10(10): 727.
Kopp, Robert E, Alexander Golub, Nathaniel O Keohane, and
Chikara Onda. 2012. “The Influence of the Specification of Climate
Change Damages on the Social Cost of Carbon.” Economics: The OpenAccess, Open-Assessment E-Journal, 6.
Kriegler, Elmar, Jim W Hall, Hermann Held, Richard Dawson, and
Hans Joachim Schellnhuber. 2009. “Imprecise probability assessment of
tipping points in the climate system.” Proceedings of the national Academy
of Sciences, 106(13): 5041–5046.
Leach, Andrew. 2007. “The Climate Change Learning Curve.” Journal of
Economic Dynamics and Control, 31(5): 1728–1752.
Leland, Hayne E. 1968. “Saving and Uncertainty: The Precautionary Demand For Saving.” The Quarterly Journal of Economics, 82(3): 465–473.
Lemoine, Derek, and Christian Traeger. 2014. “Watch Your Step: Optimal Policy in a Tipping Climate.” American Economic Journal: Economic
Policy, 6(1): 137–166.
Lemoine, Derek, and Christian Traeger. 2016a. “Ambiguous Tipping
Points.” Journal of Economic Behavior & Organization.
Lemoine, Derek, and Christian Traeger. 2016b. “Economics of Tipping
the Climate Dominoes.” Nature Climate Change, 6(5): 514–519.

50

Lemoine, Derek, and Haewon C McJeon. 2013. “Trapped Between Two
Tails: Trading Off Scientific Uncertainties via Climate Targets.” Environmental Research Letters, 8(3).
Lemoine, Derek, and Ivan Rudik. 2017. “Managing Climate Change Under Uncertainty: Recursive Integrated Assessment at an Inflection Point.”
Annual Review of Resource Economics, 9.
Li, Xin, Borghan Narajabad, and Ted Temzelides. 2016. “Robust Dynamic Optimal Taxation and Environmental Externalities.”
Lontzek, Thomas S., Yongyang Cai, Kenneth L. Judd, and Timothy M. Lenton. 2015. “Stochastic Integrated Assessment of Climate Tipping Points Indicates the Need for Strict Climate Policy.” Nature Climate
Change, 5(5): 441–444.
Matthews, H Damon, Nathan P Gillett, Peter a Stott, and Kirsten
Zickfeld. 2009. “The proportionality of global warming to cumulative carbon emissions.” Nature, 459(7248): 829–32.
Mirrlees, James A, and Nicholas H Stern. 1972. “Fairly good plans.”
Journal of Economic Theory, 4(2): 268–288.
Moore, Frances C., and Delavane B. Diaz. 2015. “Temperature Impacts
on Economic Growth Warrant Stringent Mitigation Policy.” Nature Climate
Change, , (January): 1–5.
National Academy of Sciences. 2017. Valuing Climate Damages: Updating
Estimation of the Social Cost of Carbon Dioxide.
Nordhaus, W, and Paul Sztorc. 2013. “DICE 2013R: Introduction and
User’s Manual.”
Nordhaus, William D. 2008. A Question of Balance: Weighing the Options
on Global Warming Policies. New Haven:Yale University Press.

51

Pindyck, Robert S. 2012. “Uncertain Outcomes and Climate Change Policy.” Journal of Environmental Economics and Management, 63(3): 289–
303.
Pindyck, Robert S. 2013. “Climate Change Policy: What Do the Models
Tell Us?” Journal of Economic Literature, 51(3): 860–872.
Pindyck, Robert S. 2017. “The use and misuse of models for climate policy.”
Review of Environmental Economics and Policy, 11(1): 100–114.
Roseta-Palma, C, and A Xepapadeas. 2004. “Robust Control in Water
Management.” Journal of Risk and Uncertainty, 29(1): 21–34.
Sherwood, Steven C, and Matthew Huber. 2010. “An adaptability limit
to climate change due to heat stress.” Proceedings of the National Academy
of Sciences, 107(21): 9552–9555.
Stanton, Elizabeth A., Frank Ackerman, and Sivan Kartha. 2009. “Inside the Integrated Assessment Models: Four Issues in Climate Economics.”
Climate and Development, 1(2): 166.
Sterner, T., and U. M. Persson. 2008. “An Even Sterner Review: Introducing Relative Prices into the Discounting Debate.” Review of Environmental Economics and Policy, 2(1): 61–76.
Stern, Nicholas. 2006. “The Stern Review on the Economic Effects of Climate Change.”
Stoerk, Thomas, Gernot Wagner, and Robert ET Ward. 2018. “Policy BriefRecommendations for Improving the Treatment of Risk and Uncertainty in Economic Estimates of Climate Impacts in the Sixth Intergovernmental Panel on Climate Change Assessment Report.” Review of Environmental Economics and Policy, 12(2): 371–376.
Tol, Richard S. J. 2009. “The Economic Effects of Climate Change.” The
Journal of Economic Perspectives, 23(2): 29–51.
52

Weitzman, Martin. 2010. “Some Dynamic Economic Consequences of the
Climate Sensitivity Inference Dilemma.” Handbook of Environmental Accounting.
Weitzman, Martin L. 2012. “GHG Targets as Insurance Against Catastrophic Climate Damages.” Journal of Public Economic Theory, 14(2): 221–
244.

53

Online Appendix: Optimal Climate Policy when
Damages are Unknown - Ivan Rudik
The online appendix gives full details on the dynamic stochastic climate-economy
model, describes how I use a linear climate model consistent with recent developments
in climate science, describes my computational algorithms, provides an error analysis
of the results, shows the full derivation of the carbon tax, and plots the certainty tax.

A

The Full Stochastic Climate-Economy Model

The model is a Ramsey-Cass-Koopmans growth model coupled to a climate system.
The model is governed by a representative policymaker whose objective is to maximize
her expected discounted welfare. Each period lasts one year and the model begins in
2005.
In each period t, the policymaker has an endowment of capital Kt , labor Lt , and
technology At . To improve numerical accuracy I express capital in effective labor
terms, kt = AKt Lt t (Traeger, 2014). Capital, labor and technology are combined in a
Cobb-Douglas production function to produce gross output
Ytg = ktκt .
Warming of the Earth’s surface causes damage to output, resulting in net output
after damages
Ytn = Ytg Lt ,
where Lt is the fraction of output remaining after damages as defined in the main
text (fractional net output)
Lt =

1
1 + d1 [Tts ]d2 wt

.

The policymaker has three ways to use her remaining output after damages. First,
she can use it for consumption Ct , also expressed in effective labor terms ct = ACt Lt t to

I

increase flow utility
c1−η
U (ct ) = t
,
(1 − η)

η 6= 1.

Second, she can use it to abate some fraction αt ∈ [0, 1] of emissions from factor
production with a cost given by
C(αt ) = Ψt αta2 .
The residual output is left for investment into increasing the future capital stock,
which depreciates at an annual rate of δk . Net emissions after abatement et is
et = σt (1 − αt )Ytg + Bt .
Bt is emissions from exogenous land use change, and σt is the emissions intensity of
output. Following the recent climate science literature (e.g. Matthews et al., 2009;
IPCC, 2014; Knutti et al., 2017), I use a 1 state climate system that is a function
of cumulative CO2 emissions. Contemporaneous surface temperature (relative to
preindustrial) is proportional to cumulative CO2 emissions since preindustrial times
Tts = ζ

t
X

es ,

s=1880

where ζ is the transient climate response to emissions (TCRE). Cumulative emissions
has a Markov transition
Et = Et−1 + et
so that I can write temperature as
Tts = ζEt .
The model’s exogenously evolving processes are
Lt = L0 + (L∞ − L0 ) (1 − exp(−δL t))
−1

L∞
exp(δL t) − 1
gL,t = δL
L∞ − L0


gA,0
At = A0 exp (1 − exp(−δA t))
δA
II

(Labor population)
(Labor growth rate)
(Production technology)

gA,t = gA,0 exp(−δA t)

(Production technology growth rate)

βt = exp (−ρ + (1 − η)gA,t + gL,t )


gσ,0
(1 − exp(−δσ t))
σt = σ0 exp
δσ


a0 σ t
1 − exp(gΨ t)
Ψt =
1−
a2
a1
Bt = B0 gBt

(Growth adjusted discount factor)
(Gross emissions per unit of output)
(Abatement cost coefficient)
(Non-industrial CO2 emissions)

Table A1 reports the values of the model parameters. The calibration of the
distribution over d1 , d2 , and ωt+1 are described in the main text.

III

Table A1: The parameters of the model.
Parameter
A0
gA,0
δA
L0
L∞
δL
σ0
gσ,0
δσ
a0
a1
a2
gΨ
B0
gB
κ
δk
E0
ρ
η
ζ
k0
µc
2
σ
 c


σ2
exp µc + 2c



exp 2µc + σc2 exp σc2 − 1
µ0
Σ0
µω
2
 σω

σ2
exp µω + 2ω



exp 2µω + σω2 exp σω2 − 1
µcω
2
σcω

Value

Description

0.006
0.013
0.001
6514
8600
0.035
0.13
-0.0073
0.003
1.17
2
2.8
-0.005
1.1
-0.01
0.3
0.1
454.90
0.015
2
0.0016
137/(A0 L0 )
-5.38
0.38

Initial production technology
Initial growth rate of production technology
Change in growth rate of production technology
Year 2005 population (millions)
Asymptotic population (millions)
Rate of approach to asymptotic population level
Initial emission intensity of output (Gigatons of carbon per unit output
Initial growth rate of decarbonization
Change in growth rate of emissions intensity
Cost of backstop technology in 2005 ($1000 per ton of carbon)
Initial backstop technology cost / Final backstop technology cost
Abatement cost function exponent
Growth rate of backstop technology cost
Initial non-industrial CO2 emissions (Gigatons of carbon)
Growth rate of non-industrial emissions
Capital elasticity in production
Capital depreciation rate
Cumulative emissions in initial model year (Gigatons of carbon in 2005
Pure rate of time preference
1/EIS, and RRA
TCRE (◦ C per 1,000 gigatons of carbon)
Year 2005 effective capital
d1 location parameter
d1 scale parameter

0.00556

d1 mean

0.003792
1.88
0.20
-0.59
1.18

d1 variance
Year 2005 d2 mean
Year 2005 d2 variance
ωt location parameter
ωt scale parameter

1

ωt mean

1.222
-5.97
1.56

ωt variance
Joint d1 , ωt location parameter
Joint d1 , ωt scale parameter

IV

Without loss of generality, the robust control policymaker’s problem is then
Vt (kt , Et ,Lt , µt , Σt ) =





 
βt Vt+1 (kt+1 , Et+1 , Lt+1 , µt+1 , Σt+1 )
max u(ct ) + Ed2 −θlog Ed1 ,ωt+1 exp −
ct ,αt
θ
subject to transitions:
kt+1 = exp (−(gL,t + gA,t )) [(1 − δk )kt − ct + (1 − Ψt αta2 )Ytg Lt ] ,
Et+1 = Et + σt (1 − αt )Ytg + Bt ,
1
,
Lt+1 =
1 + d1 [ζEt+1 ]d2 ωt+1
µt+1 =
Σt+1


i
h 
1
(σw2 + σc2 ) µt + log (ζEt+1 ) Σt log Lt+1
− 1 − (µc + µw )

(σw2 + σc2 ) + [log (ζEt+1 )]2 Σt
Σt (σw2 + σc2 )
=
.
(σw2 + σc2 ) + [log (ζEt+1 )]2 Σt

,

Finally I constrain abatement to be less than 100 percent and I impose the resource
constraint
αt ≤ 1,
ct + Ψt αta2 Ytg Lt ≤ Ytg Lt .
If the policymaker does not learn, then µt+1 = µt and Σt+1 = Σt . Note that the
transition for µt+1 is stochastic and depends on the realization of ωt+1 and the true
values of d1 and d2 .

A.1

Climate System

To make the model tractable to solve numerically, I take advantage of recent findings in climate science that find warming is proportional to cumulative emissions of
carbon (Matthews et al., 2009; IPCC, 2014; Knutti et al., 2017). This relationship
comes about because a pulse of carbon into the atmosphere increases equilibrium
temperature quickly by a constant amount (Matthews and Caldeira, 2008). This
has lead scientists to conclude that contemporaneous temperature is almost entirely
determined by the level of cumulative past emissions (Allen et al., 2009; Matthews
et al., 2009). Past cumulative emissions is translated into temperature by a factor of
V

proportionality called the transient climate response to emissions (TCRE).
Define the TCRE as (Williams et al., 2016; Dietz and Venmans, 2019)
ζ≡

∆T
∆T ∆M
=
,
∆E
∆M ∆E

where T is temperature, M is atmospheric carbon, and E is cumulative emissions.
∆T
Let ∆ denote changes since preindustrial levels. ∆M
gives the change in warming
∆M
from additional carbon in the atmosphere and ∆E gives the change in atmospheric
carbon concentrations given a change in emissions. Each of these terms is individually
non-linear but the product of this ends up being a simple proportional relationship for
two key reasons: (1) warming from a pulse of emissions realizes quickly, and then is
constant over time and (2) the marginal effect of emissions on temperature is constant
and independent of the state of the climate.
∆T
How does this come about?1 First, ∆M
is increasing and concave. Over time,
greater CO2 concentrations lead to more warming as additional heat gets trapped.
The relationship is concave because both the relationship between CO2 concentrations and temperature is approximately logarithmic, and the oceans’ significant heat
capacity leads to thermal inertia: a lag between changes in carbon concentrations and
, is increasing and convex. Additional
equilibrium warming. The second term, ∆M
∆E
emissions initially go into the atmosphere and increase atmospheric CO2 , while some
of these emissions eventually get absorbed in land and ocean carbon sinks. The convex relationship arises because the strength of these sinks is decreasing in cumulative
emissions. If there has been greater past cumulative emissions, a larger fraction of
the next emission will remain in the atmosphere instead of being absorbed by one of
∆T
the sinks. The curvature in ∆M
and ∆M
effectively cancel out so that ζ is constant
∆E
and positive.
Using the TCRE climate model allows me to reduce the climate system to 1
state as opposed to other options such as the 5 state DICE climate system. This
makes using standard tensor product approximation methods tractable and updates
the climate system to match the most recent developments in climate science. I
use a TCRE value of ζ = 0.0016 (◦ C/1,000 gigatons of carbon). This value and
the proportional translation of cumulative emissions to temperature follow closely to
parallel work on climate model uncertainty (Berger and Marinacci, 2017) and recent
1

See Figure 1 of Millar et al. (2016) for plots of these relationships.

VI

●

●

●
●

DICE Optimal

●
●

3

●

DICE 1%

●
●

Temperature

●
●
●

2

TCRE Optimal

●●●●●●●
●●●●●●
●●●●
●●●●
●●●
●●●
●●●
●
●
●●●
●●●
●●●
●●
●
●●
●●
●●
●●
●●
●
●
●●
●●
●●
●●
●
●●
●●
●●
●●
● ●●●●●●
●
●●
●●
●●
●
●●
●●
●●●

●

●

TCRE 1%

1

●

0
2015

2030

2045

2060

2075

2090

2105

Year

Figure A1: The temperature trajectories for DICE-2016 along the DICE-optimal
emissions path (gray circles), DICE-2016 along an emissions path that grows cumulative emissions by 1% per year (gray triangles), the TCRE-based model used in this
paper along the DICE-optimal emissions path (black circles) and the TCRE-based
model used in the paper along an emissions path that grows cumulative emissions by
1% per year (black triangles).
climate economics papers (e.g. Dietz and Venmans, 2019; van der Ploeg, 2018). This
value for the TCRE is in the middle of the most likely range estimated by the IPCC
(IPCC, 2014; Knutti et al., 2017). ζ has been found to be constant up to various levels
of cumulative emissions, including up to 2,000 GtC (Matthews et al., 2009), or 3,000
GtC (Leduc et al., 2015), and the TCRE has also been found to be approximately
constant across all RCP scenarios (MacDougall and Friedlingstein, 2015).
Figure A1 plots temperature trajectories comparing the DICE climate system to
the TCRE system. The DICE temperature trajectories are shown in gray and the
TCRE trajectories are shown in black. Circle markers denote emissions trajectories
that correspond to the optimal emissions trajectory from DICE-2016 while triangle
markers correspond to an emissions trajectory that grows cumulative emissions by
1% per year. Both trajectories result in cumulative emissions of 1,300-1,400 GtC by
2105. The TCRE system predicts approximately 2.1◦ C of warming along these two
trajectories which is well within the range of predicted temperatures of global climate
models at this cumulative emissions level (IPCC, 2013). The DICE model predicts

VII

about 1◦ C more warming than the TCRE model and tends to be on the high end of
temperature predictions from the suite of global climate models used in IPCC (2013).

A.2

Model Solution Method and Error Analysis

General approximation scheme The model is solved using value function iteration on a finite horizon. The collocation grid and polynomial interpolant are built
using standard tensor product methods. The grid is constructed from a tensor product of one dimensional vectors of zeros of Chebyshev polynomials. For the learning
frameworks, the grid has 2,100 grid points for 2005–2505 and 1,575 grid points for
2506–2605.2 For the non-learning frameworks, the grid has 729 grid points.3 The
polynomial interpolant is constructed from a tensor product of Chebyshev polynomials. Below I test the sensitivity of my results to the number of collocation grid points,
quadrature points, and the bounds for the state space.
Terminal value function The terminal year is 2605. The terminal continuation
value function corresponding to 2606 has the policymaker not learning while holding
her initial beliefs about {d2 , d1 , ωt+1 }, and has all exogenous processes held constant at
their 2606 levels. Changing the terminal value function to one where the policymaker
does not expect damage stochasticity and believes the damage function to be exactly
equal to that in the conventional DICE model does not significantly alter the results
since the continuation value 600 years in the future is effectively discounted to zero.
Expectations Expectations over future states are taken using Gauss-Hermite quadrature with 11 unique points over the d2 distribution, and 11 unique points over the joint
{d1 , ωt+1 } distribution for a total of 121 quadrature points. Note that the support of
the location parameter for a lognormal distribution is the entire real line. Depending
on the draws of the random variables, it can take on any value in (−∞, ∞). Below
I test the sensitivity of my results to the number of quadrature points to determine
whether evaluating the random variables at additional points further into the tail of
the distribution significantly affects my results.
2
These correspond to 5 unique grid points for cumulative emissions, 3 for the location parameter,
7 for the scale parameter, 5 for fractional net output, and either 3 or 4 for effective capital.
3
This corresponds to 9 on each state.

VIII

Table A2: Upper and lower bounds for each state. In the case of learning and RC+L
frameworks the bounds are only for the first iteration of the adaptive grid algorithm.
k
State Upper Bound: Uncertainty and RC
State Lower Bound: Uncertainty and RC
Initial State Upper Bound: Learning and RC+L
Initial State Lower Bound: Learning and RC+L

E

4.30 3500.0
0.65 454.9
6.0 5000.0
0.0 454.9

µ

Σ

–
–
–
–
3.13 .452
0.63 0.5

L
1.0
0.3
1.0
0.4

Adaptive grid I use a time-varying set of bounds to approximate the learning and
RC+L value functions.4 Earlier years have tighter bounds since the set of possible
realized states is smaller, while later years have wider bounds.5 This allows me to
use significantly fewer collocation points than on a standard hyperrectangle grid. I
generate this adaptive grid using the following algorithm:
1. Solve the model on the time-invariant set of bounds given by Table A2. Use
3 unique grid points for effective capital, 5 for cumulative emissions, 3 for the
location parameter, 7 for the scale parameter, and 5 for fractional net output.
2. Perform 1,000 simulations that are 301 years in length.
3. For each state
(a) Recover the maximum and minimum simulated values in each year over
the 1,000 simulations.
(b) These time series will be noisy in later years even with a very large number
of simulations, so fit a spline to the time series of maxima and another
spline to the time series of minima.6
(c) Set the new time t upper/lower bounds to a be weighted average of the
fitted maxima/minima splines evaluated at time t, and the previous time
4

This scheme is similar to that used in a number of recent stochastic implementations of DICE
with even more states (Cai et al., 2015; Lontzek et al., 2015; Cai et al., 2016, Forthcoming). Cai
(2018) provides more details on why this approach is particularly efficient for climate-economy
models related to DICE and how it can allow researchers to solve extremely complicated models in
minutes.
5
For example, cumulative emissions can only grow so much in the first five years and the trajectory
is non-decreasing since there are no negative emissions technologies.
6
Smoothing out a time series of maxima or minima takes many more simulation runs than
smoothing out the mean. The process can be greatly accelerated by fitting a spline.

IX

t bounds, where weights on the bounds given by the splines are shown in
Table A3.7
4. Solve the model on the new time-varying set of bounds obtained in Step 3. Use
3 unique grid points for effective capital, 5 for cumulative emissions, 3 for the
location parameter, 7 for the scale parameter, and 5 for fractional net output.
5. Repeat steps 2–4 16 times.
6. Repeat steps 2–4 4 times but use 5 grid points for effective capital starting at
year 2505.
7. Set the bounds in 2005 to effectively be an epsilon ball around the initial state
since it is the only state reached in that year.
8. Solve the model on this final set of adapted bounds using 4 unique grid points
for effective capital, 5 for cumulative emissions, 3 for the location parameter, 7
for the scale parameter, 5 for fractional net output.
I do not adapt the bounds for the scale parameter (i.e. the 0.0 weights in Table A3)
because the misspecified damage function simulations tend to result in it jumping
outside the bounds if its bounds are adapted, while the other states stay within the
bounds even when the damage function is misspecified. I do not adapt the upper
bound for fractional net output because the maximum simulated values are generally
very close to 1.0, the initial upper bound. The scale parameter bounds are not adapted
except for the initial period. The algorithm allows for tight bounds on the remaining
states compared to the non-learning frameworks’ hyperrectangle grid.
Figure A2 depicts the bounds and 1,000 simulated state trajectories. The final
set of adapted grid bounds are the thick black lines, the simulated state trajectories
are the thin gray lines, and the expected state trajectories are the dotted black lines.
For the states where the bounds were adapted, the bounds are tight around the set
of states actually reached in simulations. For about 20 total simulation years each,
fractional net output and the location parameter are no longer within the state bounds
where the value function approximant is reliable. Although this introduces error, 20
years outside the bounds is a negligible fraction of the 101,000 total years displayed in
7

This is similar to damping procedures used in other iterative algorithms like infinite-horizon
value function iteration.

X

4.0

1600

1.0

Cumulative Emissions

Effective Capital

3.0

2.5

Fractional Net Output

0.9
3.5

1200

800

0.8

0.7

0.6

2.0

400
2005

2030

2055

2080

2105

0.5
2005

2030

Year

2055

2080

2105

2005

2030

Year

(a) Effective Capital

2055

2080

2105

Year

(b) Cumulative Emissions

(c) Fractional Net Output

0.20

Scale Parameter

Location Parameter

3

2

0.15

0.10

1
0.05

0

0.00
2005

2030

2055

2080

2105

2005

Year

2030

2055

2080

2105

Year

(d) Location Parameter

(e) Scale Parameter

Figure A2: The bounds for each state for the learning framework and the robust
control and learning framework (thick black line). The bounds for each state for the
learning framework and the robust control and learning framework in the secondto-last iteration of the adaptive grid algorithm (thick dashed gray line). Simulated
state trajectories from the first 1,000 simulations (thin gray lines). Expected state
trajectory (dotted black line).
the simulations (101 year simulation × 1,000 simulations). The figure also shows the
bounds from the second-to-last iteration of the adaptive grid algorithm as a thick gray
dashed line. These are almost identical to the actual state bounds in black, except for
the capital lower bound. The average relative difference in the capital lower bounds
between the final two algorithm iterations is less than 1%.
The non-learning frameworks do not use the adaptive grid. This is for two reasons.
First, the non-learning frameworks have low dimensionality and can be solved on a
standard hyperrectangle grid. Second, without learning, the states often drift outside
adapted approximation bounds when facing a misspecified damage function. If the
policymaker’s beliefs about the damage function are correct, the adaptive grid can
XI

Table A3: Weights on fitted spline bounds obtained from Step 3c in the adaptive grid
algorithm. Weights on the previous iteration’s bounds are 1 minus the weights in the
table.
k

E

µ

Σ

L

Upper Bound Weight 0.3 0.4 0.5 0.0 0.0
Lower Bound Weight 0.1 0.3 0.5 0.0 0.5
easily be used for a non-learning policymaker. The state bounds for the non-learning
frameworks are contained in Table A2.
Error analysis I investigate the accuracy of the model by calculating the relative
differences in simulated policy trajectories when (i) increasing the number of unique
collocation points by 1 by over each dimension (e.g. capital goes from 4 → 5, cumulative emissions goes from 5 → 6, etc), (ii) increasing the number of quadrature points
by 2 over the d2 distribution and the joint {d1 , ωt+1 } distribution, (iii) increasing the
collocation upper bound by 10% and decreasing the collocation lower bound by 10%
for the non-adapted grids,8 and (iv) only doing the first 10 adaptive grid iterations
instead of the full 20. For the frameworks with learning, (i)-(iii) use the same adaptive
grid bounds as the baseline results. Tables A4 displays the maximum and average
relative differences of the simulated trajectories’ carbon taxes and consumption levels
over the first 100 years.
Average differences in taxes and consumption for all frameworks are on the order
of 10−3 or smaller, while the maximum error is larger but still relatively small.9

B

Carbon Tax Derivation

The conventional definition of the time t carbon tax used in the dynamic stochastic
integrated assessment literature is that it is the shadow cost of time t emissions. It
is converted into dollar terms using time t marginal utility evaluated at the optimal
consumption level. Without loss of generality I derive the carbon tax using the value
8

For fractional net output I reduce the lower bound by 20% since increasing the upper bound
beyond 1.0 corresponds to generating more than 100% of possible output without climate change.
9
Changing the number of collocation points on capital by 2 instead of 1 generates similarly sized
errors of about 10−3 for averages and 10−2 for maximums.

XII

Table A4: Average and maximum differences in carbon taxes and consumption from
increasing the number of collocation grid points or quadrature points, expanding the
size of the approximation domain for the non-learning frameworks, and performing
10 fewer iterations of the adaptive grid algorithm for the frameworks with learning. The average and maximum carbon tax trajectory differences are taken from
50,000 simulations over 2005–2105 where each simulation has a random draw of
{d1 , d2 , ω2005 , ..., ω2105 }.
Parameters Changed

Relative Differences

Uncertainty

Learning

RC

RC+L

# Collocation Points:
Add 1 unique point
to each state

Tax: Max
Tax: Avg
Consumption: Max
Consumption: Avg

1.1%
0.2%
0.2%
0.02%

1.4%
0.5%
0.08%
9 × 10−3 %

1.3%
0.2%
0.2%
0.02%

0.7%
0.4%
0.06%
8 × 10−3 %

# Quadrature Points:
Add 2 unique points
to both distributions:
{d2 } and {d1 , ωt+1 }

Tax: Max
Tax: Avg
Consumption: Max
Consumption: Avg

0.06%
0.02%
0.03%
7 × 10−3 %

0.6%
0.3%
0.03%
8 × 10−3 %

0.05%
0.02%
0.03%
7 × 10−3 %

0.5%
0.2%
0.03%
8 × 10−3 %

Domain Bounds:
Increase UB and
decrease LB by 10%

Tax: Max
Tax: Avg
Consumption: Max
Consumption: Avg

0.8%
0.3%
0.2%
0.09%

–
–
–
–

0.9%
0.3%
0.2%
0.09%

–
–
–
–

Tax: Max
Tax: Avg
Consumption: Max
Consumption: Avg

–
–
–
–

0.8%
0.3%
0.2%
0.06%

–
–
–
–

0.8%
0.3%
0.2%
0.06%

# Adaptive Iterations:
Decrease from 20 to 10

XIII

function for the robust control and learning framework. Recall the right hand side of
the Bellman equation is






−βt Vt+1 (St+1 )
Vt (St ) = max U (ct ) + Ed2 −θlog Ed1 ,ωt+1 exp
ct ,αt
θ
where St+1 ≡ {kt+1 , Et+1 , Lt+1 , µt+1 , Σt+1 } and expectations are taken using the time
t information set. To economize on space I omit the value function arguments for
now. The shadow cost of emissions is the negative partial derivative of the right hand
side of the Bellman with respect to time t emissions et , which yields


i
h

∂Vt+1 ∂St+1

 Ed1 ,ωt+1 − βθt exp −βtθVt+1 ∂S
t+1 ∂et
h

i
.
θEd2


Ed1 ,ωt+1 exp −βtθVt+1
The θ terms cancel. Expand the expectation term over d1 , ωt+1 in the numerator of
the fraction using the covariance identity to obtain
i
h
i


h




−∂Vt+1 ∂St+1
−βt Vt+1
t+1 ∂St+1
 Ed1 ,ωt+1 exp −βtθVt+1 Ed1 ,ωt+1 −∂V

,
cov
exp
d1 ,ωt+1
∂St+1 ∂et
θ
∂St+1 ∂et
h

h

i
i
βt Ed2
.
+


Ed1 ,ωt+1 exp −βtθVt+1
Ed1 ,ωt+1 exp −βtθVt+1
We can cancel the expectations over the exponentials in the first term, and bring
the denominator of the second term inside of the covariance operator such that it
multiplies the first argument. The last step to obtain the optimal carbon tax is to
divide by u0 (ct ) to translate into dollar terms
(







exp
h



−βt Vt+1
θ



−∂Vt+1 ∂St+1
−∂Vt+1 ∂St+1
βt
i

E
+
cov
,
E
d
d
,ω
d
,ω
1
t+1
1
t+1
2
u0 (ct )
∂St+1 ∂et
∂St+1 ∂et
Ed1 ,ωt+1 exp −βtθVt+1
(1)
The first term is the standard carbon tax expression without robust control. The
second term is the additively separable adjustment for robust control.
Now focus on the first term (the standard carbon tax expression term) and ignore
the leading u0β(ct t ) for clarity. Expanding out the state vector, and recognizing that

XIV

)
.

time t + 1 capital is not a function of time t emissions, we have that

Ed2 ,d1 ,ωt+1






−∂Vt+1 ∂St+1
−∂Vt+1 ∂Et+1
−∂Vt+1 ∂Lt
=Ed2 ,d1 ,ωt+1
+ Ed2 ,d1 ,ωt+1
∂St+1 ∂et
∂Et+1 ∂et
∂Lt+1 ∂et




−∂Vt+1 ∂Σt+1
−∂Vt+1 ∂µt+1
+ Ed2 ,d1 ,ωt+1
.
+Ed2 ,d1 ,ωt+1
∂µt+1 ∂et
∂Σt+1 ∂et

Next pass the expectations through. Recognize ∂L∂et+1
and ∂µ∂et+1
are random variables
t
t
so we can apply the covariance
identity
to the expectations over those terms. And
h
i
∂µt+1
finally note that Ed2 ,d1 ,ωt+1 ∂et = 0 for a Bayesian, and ∂E∂et+1
= 1 by definition.
t
Then we arrive at the final standard carbon tax expression before performing Taylor
expansions

Ed2 ,d1 ,ωt+1




−∂Vt+1 ∂St+1
−∂Vt+1
=Ed2 ,d1 ,ωt+1
(2)
∂St+1 ∂et
∂Et+1






−∂Vt+1
∂Lt
−∂Vt+1 ∂Lt+1
+Ed2 ,d1 ,ωt+1
Ed2 ,d1 ,ωt+1
+ covd2 ,d1 ,ωt+1
,
∂Lt+1
∂et
∂Lt+1
∂et




−∂Vt+1 ∂Σt+1
−∂Vt+1 ∂µt+1
,
.
+ Ed2 ,d1 ,ωt+1
+covd2 ,d1 ,ωt+1
∂µt+1
∂et
∂Σt+1
∂et

Recall that for a simple bivariate case, a second-order Taylor expansion of Ex,y [f (x, y)]
about (c, d) is

Ex,y [f (x, y)] ≈Ex,y f (c, d)
+ fx (x, y)(x − c) + fy (c, d)(y − d)
1
+ fxy (c, d)(x − c)2 + fxy (c, d)(x − c)(y − d)
2


2
+ fyx (c, d)(y − d)(x − c) + fyy (c, d)(y − d)
where the term on the first line and on right hand side of the equality is the zerothorder term, the second line contains the first-order terms, and the third and fourth
lines contain the second-order terms. Now perform a second-order Taylor expansion
of the expectations of the value function partial derivatives around

υ := kt+1 , Et+1 , Ed2 ,d1 ,ωt+1 [Lt+1 ], Ed2 ,d1 ,ωt+1 [µt+1 ], Σt+1

XV

where Ed2 ,d1 ,ωt+1 [µt+1 ] = µt .
h
i
t+1
evaluated at
Here I show the second-order Taylor expansion for Ed2 ,d1 ,ωt+1 −∂V
∂Et+1
υ. It consists of 1 zeroth-order Taylor term, five first-order Taylor terms, and twenty
five second-order Taylor terms

Ed2 ,d1 ,ωt+1

"

−∂Vt+1
−∂Vt+1
≈Ed2 ,d1 ,ωt+1
∂Et+1
∂Et+1

(Zeroth-order)
υ

−∂ 2 Vt+1
(kt+1 − Ed2 ,d1 ,ωt+1 [kt+1 ])
(First-order)
∂Et+1 ∂kt+1 υ
−∂ 2 Vt+1
(Et+1 − Ed2 ,d1 ,ωt+1 [Et+1 ])
+
2
∂Et+1
υ
−∂ 2 Vt+1
(Lt+1 − Ed2 ,d1 ,ωt+1 [Lt+1 ])
+
∂Et+1 ∂Lt+1 υ
−∂ 2 Vt+1
+
(µt+1 − Ed2 ,d1 ,ωt+1 [µt+1 ])
∂Et+1 ∂µt+1 υ
−∂ 2 Vt+1
+
(Σt+1 − Ed2 ,d1 ,ωt+1 [Σt+1 ])
∂Et+1 ∂Σt+1 υ
1 −∂ 3 Vt+1
(kt+1 − Ed2 ,d1 ,ωt+1 [kt+1 ])2
(Second-order)
+
2
2 ∂Et+1 ∂kt+1
υ
1
−∂ 3 Vt+1
(kt+1 − Ed2 ,d1 ,ωt+1 [kt+1 ])(Et+1 − Ed2 ,d1 ,ωt+1 [Et+1 ])
+
2 ∂Et+1 ∂kt+1 ∂Et+1 υ
1
−∂ 3 Vt+1
+
(kt+1 − Ed2 ,d1 ,ωt+1 [kt+1 ])(Lt+1 − Ed2 ,d1 ,ωt+1 [Lt+1 ])
2 ∂Et+1 ∂kt+1 ∂Lt+1 υ
−∂ 3 Vt+1
1
(kt+1 − Ed2 ,d1 ,ωt+1 [kt+1 ])(µt+1 − Ed2 ,d1 ,ωt+1 [µt+1 ])
+
2 ∂Et+1 ∂kt+1 ∂µt+1 υ
−∂ 3 Vt+1
1
(kt+1 − Ed2 ,d1 ,ωt+1 [kt+1 ])(Σt+1 − Ed2 ,d1 ,ωt+1 [Σt+1 ])
+
2 ∂Et+1 ∂kt+1 ∂Σt+1 υ
1 −∂ 3 Vt+1
+
(Et+1 − Ed2 ,d1 ,ωt+1 [Et+1 ])(kt+1 − Ed2 ,d1 ,ωt+1 [kt+1 ])
2
2 ∂Et+1
∂kt+1 υ
1 −∂ 3 Vt+1
(Et+1 − Ed2 ,d1 ,ωt+1 [Et+1 ])2
+
3
2 ∂Et+1
υ
3
1 −∂ Vt+1
+
(Et+1 − Ed2 ,d1 ,ωt+1 [Et+1 ])(Lt+1 − Ed2 ,d1 ,ωt+1 [Lt+1 ])
2
2 ∂Et+1
∂Lt+1 υ
1 −∂ 3 Vt+1
+
(Et+1 − Ed2 ,d1 ,ωt+1 [Et+1 ])(µt+1 − Ed2 ,d1 ,ωt+1 [µt+1 ])
2
2 ∂Et+1
∂µt+1 υ
+

XVI

1 −∂ 3 Vt+1
(Et+1 − Ed2 ,d1 ,ωt+1 [Et+1 ])(Σt+1 − Ed2 ,d1 ,ωt+1 [Σt+1 ])
2
∂Σt+1 υ
2 ∂Et+1
−∂ 3 Vt+1
1
+
(Lt+1 − Ed2 ,d1 ,ωt+1 [Lt+1 ])(kt+1 − Ed2 ,d1 ,ωt+1 [kt+1 ])
2 ∂Et+1 ∂Lt+1 ∂kt+1 υ
1
−∂ 3 Vt+1
+
(Lt+1 − Ed2 ,d1 ,ωt+1 [Lt+1 ])(Et+1 − Ed2 ,d1 ,ωt+1 [Et+1 ])
2 ∂Et+1 ∂Lt+1 ∂Et+1 υ
1 −∂ 3 Vt+1
(Lt+1 − Ed2 ,d1 ,ωt+1 [Lt+1 ])2
+
2
2 ∂Et+1 ∂Lt+1 υ
1
−∂ 3 Vt+1
+
(Lt+1 − Ed2 ,d1 ,ωt+1 [Lt+1 ])(µt+1 − Ed2 ,d1 ,ωt+1 [µt+1 ])
2 ∂Et+1 ∂Lt+1 ∂µt+1 υ
−∂ 3 Vt+1
1
(Lt+1 − Ed2 ,d1 ,ωt+1 [Lt+1 ])(Σt+1 − Ed2 ,d1 ,ωt+1 [Σt+1 ])
+
2 ∂Et+1 ∂Lt+1 ∂Σt+1 υ
−∂ 3 Vt+1
1
(µt+1 − Ed2 ,d1 ,ωt+1 [µt+1 ])(kt+1 − Ed2 ,d1 ,ωt+1 [kt+1 ])
+
2 ∂Et+1 ∂µt+1 ∂kt+1 υ
1
−∂ 3 Vt+1
+
(µt+1 − Ed2 ,d1 ,ωt+1 [µt+1 ])(Et+1 − Ed2 ,d1 ,ωt+1 [Et+1 ])
2 ∂Et+1 ∂µt+1 ∂Et+1 υ
1
−∂ 3 Vt+1
(µt+1 − Ed2 ,d1 ,ωt+1 [µt+1 ])(Lt+1 − Ed2 ,d1 ,ωt+1 [Lt+1 ])
+
2 ∂Et+1 ∂µt+1 ∂Lt+1 υ
1 −∂ 3 Vt+1
(µt+1 − Ed2 ,d1 ,ωt+1 [µt+1 ])2
+
2
2 ∂Et+1 ∂µt+1 υ
1
−∂ 3 Vt+1
+
(µt+1 − Ed2 ,d1 ,ωt+1 [µt+1 ])(Σt+1 − Ed2 ,d1 ,ωt+1 [Σt+1 ])
2 ∂Et+1 ∂µt+1 ∂Σt+1 υ
1
−∂ 3 Vt+1
(Σt+1 − Ed2 ,d1 ,ωt+1 [Σt+1 ])(kt+1 − Ed2 ,d1 ,ωt+1 [kt+1 ])
+
2 ∂Et+1 ∂Σt+1 ∂kt+1 υ
1
−∂ 3 Vt+1
(Σt+1 − Ed2 ,d1 ,ωt+1 [Σt+1 ])(Et+1 − Ed2 ,d1 ,ωt+1 [Et+1 ])
+
2 ∂Et+1 ∂Σt+1 ∂Et+1 υ
1
−∂ 3 Vt+1
+
(Σt+1 − Ed2 ,d1 ,ωt+1 [Σt+1 ])(Lt+1 − Ed2 ,d1 ,ωt+1 [Lt+1 ])
2 ∂Et+1 ∂Σt+1 ∂Lt+1 υ
1
−∂ 3 Vt+1
+
(Σt+1 − Ed2 ,d1 ,ωt+1 [Σt+1 ])(µt+1 − Ed2 ,d1 ,ωt+1 [µt+1 ])
2 ∂Et+1 ∂Σt+1 ∂µt+1 υ
#
3
1 −∂ Vt+1
+
(Σt+1 − Ed2 ,d1 ,ωt+1 [Σt+1 ])2 .
2
2 ∂Et+1 ∂Σt+1 υ
+

(3)
Note that the expectation is taken over all terms in the expansion. The term on the
right hand side of the equality on the first line is the zeroth-order term, the value
XVII

function partial evaluated at υ, which is the state vector about which I do the Taylor
expansion. This term is not random since it is evaluated at a specific state, so the
expectation passes through.
The next five lines are the first-order Taylor terms. Here, another partial derivative
of the value function is taken with respect to each state, then it is multiplied by the
difference between the state and that state’s value at υ, which is just the expectation
of the state. The value function terms here have two partial derivatives, and are
evaluated at υ. They also are not random so the expectation passes through those
terms onto the terms capturing the difference between the state and its value at υ.
The expectation of the difference between a variable and its expectation is zero so all
first-order Taylor terms of the expansion are zero.
The second-order Taylor terms take another partial derivative, so these are the
terms that include all of the triple value function partial derivatives. The triple
partials are multiplied by the product of two differences between a state and that
state’s value at υ. The state in each difference corresponds to the state with respect to
which the second and third partial derivatives are taken. As in zeroth-order and firstorder Taylor terms, the triple partial derivatives are not random and the expectation
passes through all of them onto the two difference terms. In the cases where at
least one of the differences in the second-order Taylor term consists of a deterministic
state (kt+1 , Et+1 , Σt+1 ), the expectation of the product of the two differences is again
zero and the Taylor terms are zero. The remaining case is when both states in
the product of differences are uncertain (Lt+1 , µt+1 ). After passing the expectation
through the triple partial derivative terms, the difference terms are variances (e.g.
Ed2 ,d1 ,ωt+1 [(µt+1 − Ed2 ,d1 ,ωt+1 [µt+1 ])2 ] = vard2 ,d1 ,ωt+1 (µt+1 ) on the seventh to last line)
or covariances (e.g. Ed2 ,d1 ,ωt+1 [(µt+1 − Ed2 ,d1 ,ωt+1 [µt+1 ])(Lt+1 − Ed2 ,d1 ,ωt+1 [Lt+1 ])] =
covd2 ,d1 ,ωt+1 (µt+1 , Lt+1 ) on the eighth to last line). Similar logic follows for Taylor
expansions for the other value function partials in equation (2).
Now define
)
(
ce :=

kt+1 , Et+1 , Lt+1

, µt , 0 .
2)
d1 =exp(µc + 21 σc2 ), ωt+1 =exp(µω + 12 σω

ce is a certainty state. Terms evaluated at ce are the terms that would arise if the
policymaker happened to arrive at the same state at time t but states transitioned
deterministically (with d1 , d2 , ωt fixed to their expectations). I will add and subXVIII

tract value function partial derivatives evaluated at ce, effectively adding 0, to obtain
the certainty tax and the state uncertainty adjustment. After omitting the Taylor
terms that are zero, the expectation of the cumulative emissions partial derivative is
approximately

Ed2 ,d1 ,ωt+1




−∂Vt+1
−∂Vt+1
−∂Vt+1
−∂Vt+1
≈
+
−
∂Et+1
∂Et+1 ce
∂Et+1 υ
∂Et+1 ce
1 −∂ 3 Vt+1
−∂ 3 Vt+1
+
var
(L
)
+
covd2 ,d1 ,ωt+1 (Lt+1 , µt+1 )
d
,d
,ω
t+1
2
1
t+1
2 ∂Et+1 ∂L2t+1 υ
∂Et+1 ∂Lt+1 ∂µt+1 υ
1 −∂ 3 Vt+1
vard2 ,d1 ,ωt+1 (µt+1 )
+
2 ∂Et+1 ∂µ2t+1 υ

The expectation of the fractional net output partial derivative is approximately

Ed2 ,d1 ,ωt+1




−∂Vt+1
−∂Vt+1
−∂Vt+1
−∂Vt+1
+
−
≈
∂Lt+1
∂Lt+1 ce
∂Lt+1 υ
∂Lt+1 ce
3
1 −∂ Vt+1
−∂ 3 Vt+1
var
covd2 ,d1 ,ωt+1 (Lt+1 , µt+1 )
+
(L
)
+
d2 ,d1 ,ωt+1
t+1
2 ∂L3t+1 υ
∂L2t+1 ∂µt+1 υ
1 −∂ 3 Vt+1
vard2 ,d1 ,ωt+1 (µt+1 )
+
2 ∂Lt+1 ∂µ2t+1 υ

The expectation of the scale parameter partial derivative is approximately

Ed2 ,d1 ,ωt+1




−∂Vt+1
−∂Vt+1
−∂Vt+1
−∂Vt+1
+
−
≈
∂Σt+1
∂Σt+1 ce
∂Σt+1 υ
∂Σt+1 ce
3
1 −∂ Vt+1
−∂ 3 Vt+1
var
covd2 ,d1 ,ωt+1 (Lt+1 , µt+1 )
+
(L
)
+
d
,d
,ω
t+1
2 1 t+1
2 ∂Σt+1 ∂L2t+1 υ
∂Σt+1 ∂Lt+1 ∂µt+1 υ
1 −∂ 3 Vt+1
+
vard2 ,d1 ,ωt+1 (µt+1 )
2 ∂Σt+1 ∂µ2t+1 υ

Substituting the Taylor expansion terms into equation (2) and then substituting equation (2) into equation (1) yields the full carbon tax expression.

C

Certainty Tax

Figure A3 displays the certainty tax channel for each framework. The certainty tax
is the remaining difference between the carbon tax in Figure 5 and the uncertainty
channels in Figure 7 in the main text.
XIX

Contribution to Optimal Carbon Tax ($/tCO2)

75

Uncertainty
Learning
Robust Control
RC + Learning

50

25

0
2005

2030

2055

2080

2105

Year

Figure A3: The mean certainty tax channel over 50,000 simulations for each framework. Each simulation samples a different set of {d1 , d2 , ω2005 , ..., ω2105 }.

References
Allen, Myles R, David J Frame, Chris Huntingford, Chris D Jones, Jason A Lowe,
Malte Meinshausen, and Nicolai Meinshausen (2009) “Warming caused by cumulative carbon emissions towards the trillionth tonne,” Nature, Vol. 458, No. 7242,
pp. 1163–1166.
Berger, Loic and Massimo Marinacci (2017) “Model Uncertainty in Climate Change
Economics,” No. Cmcc, pp. 1–33.
Cai, Yongyang (2018) “Computational Methods in Environmental and Resource Economics,” Annual Review of Resource Economics, Vol. 11.
Cai, Yongyang, Kenneth L Judd, Timothy M Lenton, Thomas S Lontzek, and Daiju
Narita (2015) “Environmental tipping points significantly affect the cost- benefit
assessment of climate policies,” Proceedings of the National Academy of Sciences,
p. 201503890.
Cai, Yongyang, Kenneth L. Judd, and Thomas S. Lontzek (Forthcoming) “The Social
Cost of Carbon with Economic and Climate Risks,” Journal of Political Economy.
Cai, Yongyang, Timothy M Lenton, and Thomas S Lontzek (2016) “Risk of multiple
interacting tipping points should encourage rapid CO 2 emission reduction,” Nature
Climate Change, Vol. 6, No. 5, p. 520.

XX

Dietz, Simon and Frank Venmans (2019) “Cumulative carbon emissions and economic
policy: in search of general principles,” Journal of Environmental Economics and
Management, Vol. 96, pp. 108–129.
IPCC (2013) “Climate Change 2013: The Physical Science Basis. Contribution of
Working Group I to the Fifth Assessment Report of the Intergovernmental Panel
on Climate Change,” Cambridge University Press, Cambridge, United Kingdom
and New York, NY, USA.
(2014) Climate Change 2013: The Physical Science Basis: Working Group
I Contribution to the Fifth Assessment Report of the Intergovernmental Panel on
Climate Change: Cambridge University Press.
Knutti, Reto, Maria A. A. Rugenstein, and Gabriele C. Hegerl (2017) “Beyond equilibrium climate sensitivity,” Nature Geoscience, Vol. 10, No. 10, p. 727.
Leduc, Martin, H Damon Matthews, and Ramón de Elı́a (2015) “Quantifying the
limits of a linear temperature response to cumulative CO2 emissions,” Journal of
Climate, Vol. 28, No. 24, pp. 9955–9968.
Lontzek, Thomas S., Yongyang Cai, Kenneth L. Judd, and Timothy M. Lenton (2015)
“Stochastic integrated assessment of climate tipping points indicates the need for
strict climate policy,” Nature Climate Change, Vol. 5, No. 5, pp. 441–444.
MacDougall, Andrew H and Pierre Friedlingstein (2015) “The Origin and Limits of the
Near Proportionality Between Climate Warming and Cumulative CO2 Emissions,”
Journal of Climate, Vol. 28, No. 10, pp. 4217–4230.
Matthews, H Damon and Ken Caldeira (2008) “Stabilizing Climate Requires NearZero Emissions,” Geophysical Research Letters, Vol. 35, No. 4.
Matthews, H Damon, Nathan P Gillett, Peter a Stott, and Kirsten Zickfeld (2009)
“The Proportionality of Global Warming to Cumulative Carbon Emissions,” Nature, Vol. 459, No. 7248, pp. 829–32.
Millar, Richard, Myles Allen, Joeri Rogelj, and Pierre Friedlingstein (2016) “The Cumulative Carbon Budget and its Implications,” Oxford Review of Economic Policy,
Vol. 32, No. 2, pp. 323–342.
van der Ploeg, Frederick (2018) “The Safe Carbon Budget,” Climatic Change, Vol.
147, No. 1, pp. 47–59.
Traeger, Christian (2014) “A 4-Stated DICE: Quantitatively Addressing Uncertainty
Effects in Climate Change,” Environmental and Resource Economics, Vol. 59, No.
1, pp. 1–37.

XXI

Williams, Richard G, Philip Goodwin, Vassil M Roussenov, and Laurent Bopp (2016)
“A framework to understand the transient climate response to emissions,” Environmental Research Letters, Vol. 11, No. 1, p. 015003.

XXII

