Congressional Representation:
Accountability from the Constituent’s Perspective
Stephen Ansolabehere

Shiro Kuriwaki∗

August 28, 2020

Forthcoming, American Journal of Political Science

Abstract
The premise that constituents hold representatives accountable for their legislative decisions undergirds political theories of democracy and legal theories of statutory interpretation. But studies of this at the individual level are rare, examine only a handful
of issues, and arrive at mixed results. We provide an extensive assessment of issue
accountability at the individual level. We trace the congressional rollcall votes on 44
bills across seven Congresses (2006–2018), and link them to constituent’s perceptions
of their representative’s votes and their evaluation of their representative. Correlational, instrumental variables, and experimental approaches all show that constituents
hold representatives accountable. A one-standard deviation increase in a constituent’s
perceived issue agreement with their representative can improve net approval by 35
percentage points. Congressional districts, however, are heterogeneous. Consequently,
the effect of issue agreement on vote is much smaller at the district-level, resolving an
apparent discrepancy between micro and macro studies.

Keywords: Electoral Accountability, Representation, Congress, Rollcall Voting.
Replication Materials: The data, code, and any additional materials required to
replicate all analyses in this article are available on the American Journal of Political
Science Dataverse within the Harvard Dataverse Network, at: https://doi.org/10.7910/
DVN/QOVWMM (pending review by AJPS).
∗

Stephen Ansolabehere (sda@gov.harvard.edu; http://scholar.harvard.edu/sansolabehere) is Frank G.
Thomson Professor of Government and Shiro Kuriwaki (kuriwaki@g.harvard.edu; https://www.shirokuriwaki.
com) is Ph.D. Candidate, both in the Department of Government at Harvard University, 1737 Cambridge St,
Cambridge, MA 02138. We thank Mathias Tromborg and participants at EPSA 2018, Chris Tausanovitch
and participants at SPSA 2019, as well as David Broockman, Christopher Cochran, Sebastian Thieme,
Soichiro Yamauchi, and workshop participants at the University of California Santa Barbra, University of
Texas Austin, Arizona State University, Princeton University, and Harvard University for their comments
and suggestions. We acknowledge partial support from NSF Award Number SES-1756447 and the Harvard
FAS Dean’s Competitive Fund. Authors listed alphabetically.

Representative democracy rests on a simple idea. Constituents elect politicians to be their
agents in passing laws and setting public policy. If an individual constituent disagrees with
the actions or decisions of a representative, the constituent may choose someone else at the
next election. A majority of the electorate can elect another legislator or party to represent
them. Modern political science has taken this notion of accountability as the cornerstone for
theorizing about representation and for studying law-making in representative democracies,
especially within the American Congress (Mayhew 1974). Constitutional theory and even
Supreme Court doctrine treats electoral accountability as the wellspring of legislative and
executive authority in the U.S. (Eskridge 1987).
As important as the classical theory of accountability is, it is ultimately just a theory. The
empirical foundations for this idea are, as Stephanopoulos (2018) recently surmised, underdeveloped. One significant line of research has established a connection at the district-level
between the legislator’s congruence with their district and election results (Canes-Wrone et
al. 2002). But as Carson et al. (2010) suggest, without constituent-level data, these aggregate
estimates might also reflect party loyalty, ideology, presidential approval or other factors. A
second line of research has explored the individual foundations for electoral accountability.
In their path-setting article, Miller and Stokes (1963) concluded that most constituents lack
the knowledge to hold their representatives accountable and, as a result, that there is low
congruence between constituents and legislators.
The conclusion that Miller and Stokes reached has spawned a very different view of
congressional politics in the U.S. than that embraced in the classical theory of representation.
If voters cannot exert electoral accountability, representation breaks down. That gives elites,
such as interest groups, policy ideologues, or wealthy donors, an opening to capture the
political process (Bartels 2008; Bawn et al. 2012). Bawn et al. (2012) challenge the notion
that “to win elections politicians must do what voters want”. They argue “voters do not pay
so much attention to politics,” and those “limitations of most voters to hold their legislators
accountable” creates the conditions for extreme partisanship in Congress (p.589-590). Has

1

the constituents’ side of the accountability mechanism actually broken down?
Scholarship in the past two decades has repeatedly debated the questions raised by Miller
and Stokes. Clinton (2006) and Bafumi and Herron (2010) used key votes in Congress as
better measures of ideological congruence, and Ansolabehere and Jones (2010) and Guisinger
(2009) provided evidence that individual constituents reward representatives with whom they
perceive to be in agreement on specific rollcall votes. Since those initial studies, the debate
over whether partisan constituents can hold their representatives accountable on issues has
intensified. Several studies have argued that partisanship distorts people’s perceptions and
swamps issue voting (Lenz 2012; Broockman and Butler 2017), but others reach opposite
conclusions (Bullock 2011; Fowler 2020).
Here we offer an extensive empirical assessment of congressional accountability, tracing
representative’s roll call votes to constituent’s perceptions about those specific votes, and
finally from those pictures in people’s heads to the electoral evaluations that they make of
their representatives. We study the entire 12-year span of the Cooperative Congressional
Election Study (CCES), which covers over 67 roll call votes and asks constituent’s perception
on 44 of them. Dramatic swings in political control of the U.S. government from 2006 to 2018
mean that our study captures nearly every political constellation: unified Republican control,
unified Democratic control, divided control with a Republican President and a Democratic
Congress, and divided control with a Democratic President and a Republican Congress. The
key votes we track reflect the wide-ranging policy agenda during this time, including war,
health care, trade, banking, wages and labor discrimination, the budget and taxation, welfare
programs, immigration, crime, guns, education, abortion, agriculture and gay rights.
This study contributes to four foundational questions on electoral accountability. We
significantly extend past findings on these questions with new data and designs, and we
reconcile seemingly contradictory claims in the literature.
First, what is the relationship between legislators’ congruence with their constituents
and constituents’ evaluations of their legislators? Consistent with past work we find that

2

an individual constituent’s agreement between representatives lead to positive evaluations
of that representative. In contrast to the work that focuses on ideological agreement, we
also measure agreement in terms of the specific votes Members of Congress cast. We find
that agreement on these key votes affects evaluations even after controlling for agreement
in terms of party affiliation and latent ideology, suggesting that each key vote matters. We
call this relationship the reduced form because it is simply the relationship between what
legislators do and what voters do, and does not establish the mechanism operating in voters’
minds.
Second, are constituent’s perception of their representative’s actions in Congress accurate? This is the critical first step in the accountability process. Do people have, to use
Walter Lippmann’s expression, a picture in their heads about how their representatives
voted, and is that picture close to reality? The answer is largely yes. Most people have a
belief about how their representatives voted, and among those who provided an answer, a
majority have the correct belief about their representative’s votes. To the extent that there
is slippage, it takes two forms: uncertainty and misperception. About two in five people do
not readily express a belief about how their representatives voted on the average bill. Also,
co-partisans tend to perceive more issue agreement than actual agreement, but these biases
appear to be second-order effects compared with the main effect of correct issue perception.
Third, do constituents in fact support representatives because they think they agree on
key legislation that Congress has voted on, independently from party? Or, is the correlation between issue agreement and evaluation actually partisan projection (Lenz 2012)? We
start with difference-in-means estimates suggesting that perceived issue agreement has independent effects on approval and vote choice. We then replicate the instrumental variable
identification strategy of Ansolabehere and Jones (2010), albeit over a much wider span of
time and issues. Finally, we conduct two survey experiments and conduct sensitivity analyses to address the concerns that the instrumental variable conditions may not hold. These
correlational, instrumental variable, and experimental approaches all show that perceived

3

issue agreement on key legislation does translate into electoral support, and the effects are
substantial and operate independent of party.
Fourth, why are the individual-level effects of issue accountability so much larger than
the district-level effects, as noted by Tausanovitch and Warshaw (2018)? Studies using
survey data at the individual-level typically report a 10 to 20 percentage point effect of
congruence on an individual’s vote choice (Jessee 2009; Ansolabehere and Jones 2010; Shor
and Rogowski 2018). Yet Canes-Wrone et al. (2002) find that House members gain only
1 to 3 percentage points of their voteshare from moderating towards the party’s median
rather than from voting at the extreme of their party (see also Fiorina 1974; Erikson et al.
2002; Bonica and Cox 2018). We show that the tension results from the aggregation of
individual voters to the district level. Congressional districts are sufficiently heterogeneous
that a Member of Congress voting on the side of the majority of her constituents would still
disappoint a sizable minority. As a result, even when many constituents care about and are
knowledgeable about salient issues and when a representative votes with the majority in a
district, the aggregate congruence can come out quite low. This does not mean that issues
are unimportant, because voting against the district majority would be even more costly.
In what follows, we uncover a picture of the electorate that, while not hyper-informed
and hyper-rational, is one in which constituents are sufficiently attentive that the majority
can and does hold their representatives accountable for the decisions that they make on
important pieces of legislation.

Models and Methods
We start by outlining our model of constituent accountability, setting up our identification
strategy, and describing how we combine CCES data, experiments, and roll call votes on the
House floor to identify these mechanisms.

4

Figure 1 – Accountability from the Constituent’s Perspective

Actual Issue Agreement (ZI )

Perceived Issue Agreement (AI )
Evaluation (Y)

Actual Party Agreement (ZP )

Perceived Party Agreement (AP )

Note: Arrows show possible causal effects, and bold arrows show issue accountability. This
is a Directed Acyclic Graph (DAG) for our estimation strategy. The random variables we
use to denote each concept are shown in parentheses. Control variables and unobserved
confounders are not shown for clarity but are addressed in the main text.

A Model of Reality, Perception, and Evaluation
Figure 1 depicts the causal process of issue accountability. We trace the bolded arrows from
left to right: In the initial stage, representatives belong to a party, they cast a vote on an bill,
and constituent’s have a preference for that bill as well. A representative and her constituent
are in actual issue agreement when they have the same preferences.
The subtlety in testing theories of accountability is that constituents can only act on
what they know (Gilens 2001). Therefore we distinguish between two sorts of agreement in
Figure 1: actual and perceived. For example, a pro-Affordable Care Act (ACA) constituent
might believe that his representative also voted for the ACA (perceived issue agreement),
either correctly (they are also in actual issue agreement) or incorrectly (the representative in
fact had voted against the ACA). Unlike most studies of accountability, our study measures
these perceptions directly.
The purpose of Figure 1 is to distill our operationalization of accountability and estimation strategy. It does not exhaustively display alternative causal pathways, including the
possibility of projection in which voters perceive to be in agreement because they approve
of the representative (Lenz 2012), or the possibility that constituents infer party loyalty
(Carson et al. 2010) or ideological extremity (Nyhan et al. 2012) from roll call votes. These
mechanisms may be occurring simultaneously with issue accountability, perhaps with some
voters but not others. Our contribution is to estimate one particular quantity — issue ac5

countability as envisioned by the classical theory of representation — by controlling away
such alternative explanations from our estimates. We focus on the dyadic relationship between a constituent and his representative and assume that, if a constituent can and does
hold his representative accountable, he does so regardless of whether the legislator is pivotal.
This model of constituent perception is far from novel: It mirrors Figure 1 of “Constituency Influence in Congress” (Miller and Stokes 1963), which conceptualized how representatives made decisions based on their perceptions of their constituents’ preferences. In
fact, one goal of the present study is to provide a constituent’s perspective of the sort of
accountability studied in Miller and Stokes.

Estimation Strategy
We focus on estimating three components of issue accountability depicted in Figure 1 from
observed data and survey experiments. Most studies of accountability estimate the effect
of actual agreement on evaluations. We estimate this quantity through the reduced form
equation, indexing constituents by i ∈ {1, ..., n}:

Reduced Form: Yi = ρ0 + ρi ZIi + ρp ZPi + X0i ρx + ε1i

(1)

where Yi , following Figure 1, is constituent i’s evaluation of his representative, ZIi is their
actual agreement on issues, ZPi is their actual agreement on party affiliation, and Xi denotes
a set of control variables we discuss later. A positive value of ρi , interpreted causally,
represents “Out of Step, Out of Office” (Canes-Wrone et al. 2002): when a representative
takes a vote that is not in agreement with her constituent, the constituent reacts by lowering
their propensity to vote to re-elect that incumbent. We refer to equation (1) as the reduced
form anticipating our instrumental variable estimation strategy.
Although important, the reduced form does not describe how the constituent came to that
evaluation. The first stage therefore asks whether the actual agreement implied by legislators’

6

roll call votes shapes constituents’ perceived agreement on those votes. Estimating this
relationship from data corresponds to a linear regression:

First Stage: AIi = α0 + αi ZIi + αp ZPi + X0i αx + ε2i

(2)

where AIi refers to i’s perceived agreement with their representative on the issues. A positive
value of αi , again interpreted causally, indicates that reality shapes perception: controlling
for actual party agreement and other possible confounders [ZPi , Xi ], constituents form on
the whole correct perceptions about their representative’s votes in Congress.
The first stage then leads to the central question of issue accountability: to what extent do
constituents act upon those perceptions, as measured by their evaluations (e.g., job approval
or their propensity to re-elect her)?

Second Stage: Yi = β0 + βI AIi + βp APi + X0i βx + ε3i

(3)

One threat to inference that is new in interpreting the β coefficients causally is projection,
which we can formalize as the endogeneity of perceived agreement. For example, a respondent
might have underlying trust in the representative, which both leads to higher job approval
and also leads him to the belief that the representative probably agrees with him on key
issues too. To remove such potential confounding we implement an instrumental variables
strategy, instrumenting perception with actual agreement with equation (2) as suggested
by Figure 1. If our instrumental variables specification is appropriate, a two stage least
squares regression combining equations (2) and (3) will estimate the magnitude of issue
accountability unconfounded by projection.
For the instrumental variables approach to produce unbiased estimates, three conditions
must be met. First, the first stage outlined in equation (2) must be strong. We show in our
results that this condition is easily met in our data. Second, the effect of the instrument on
the outcome must flow exclusively through the variable being instrumented. Although this
7

is an untestable assumption, we point out that it is a natural one in our setting given that
constituents can only form evaluations based on what they perceive.
The third condition, exogeneity of the instrument conditional on controls, is the most
difficult of the three to meet in our setting. The same sort of exogeneity condition is required
for interpreting the reduced form and the first stage coefficients causally as well.
To achieve conditional exogeneity, we control for five types of well-known potential confounders in all our regressions. (i) Representative fixed effects accounts for any time-invariant
and issue-invariant characteristics of each Member of Congress, such as party affiliation or
personality that may induce spurious correlation across districts. (ii) Theories of partisan
heuristics predict that many constituents infer positions only from associations with party labels, so we treat actual party agreement (ZP ) as a control. Next, a member’s voting patterns
is surely correlated with her latent ideology, so we control for (iii) ideological agreement with
the incumbent to account for the representative’s residual voting pattern after accounting
for the key votes of interest. For similar reasons we control for the (iv) ideological distance
from the challenger in general election years, as proxied by constituents’ placements of those
candidates. Finally, we include (v) socio-demographic characteristics of the respondent that
may shape evaluations and perceptions, including age, gender, race, education, and income.
In a “perfect” experiment, representative’s actual rollcall votes would be randomized,
exogeneity would be satisfied by design, and then analysts would only need to compare
constituents’ approval of a representative who voted ‘yea’ with approval among otherwise
similar constituents of a representative who voted ‘nay’. To approximate that impossible
experiment and anticipating potential violations of conditional exogeneity, we conducted
two survey experiments that randomize information about representative’s votes. We hasten
to note that any randomized control trial faces a limitation when testing theories of electoral
accountability. Almost all field experiments assign constituents to hear about representative’s
real positions (e.g. Broockman and Butler 2017), and therefore induce variation in perceived
agreement (AI ) but not in actual agreement (ZI ). The inferential strengths of observational

8

and experimental approaches complement each other’s weaknesses.
The final way we address potential violations to the exogeneity assumption is by applying
sensitivity analyses. Recent statistical developments in this area provide a benchmark of how
large the unobserved confounding must be for our main conclusions to reduce to null (Cinelli
and Hazlett 2020). Put together, our experiments and sensitivity analyses indicate that
constituents do react to information consistent with classical theories of accountability, and
that any unobserved confounding in our observational analyses would have to be larger than
the effect of copartisanship on the same outcome to flip our main findings.

Data and Measurement
To operationalize our measures we rely on the CCES from 2006 to 2018, covering the 109th
Congress under the presidency of George W. Bush through the 115th Congress under the
first two years of President Trump. A measure of constituent opinion on key votes is available
for all CCES respondents, but measures of constituent’s perceptions of those votes is only
included in one or two team modules a year (Table A1).1 We therefore primarily use these
respondents in the team module, and append information from the common content.
Each year’s CCES polls important issues from Congress’ agenda each year, identified by
the Congressional Quarterly or the Washington Post Key Vote. Table A2 lists all of the
issues on the CCES on which there were corresponding bills in Congress, and how the House
and Senate dealt with that bill. In particular, we analyze 44 floor votes in the U.S. House
of Representatives2 for which perception questions were asked. In the first three Congresses
(2006 - 2010), both the House and the Senate usually took up key legislation. But during
1

Each team module and the Common Content are separately weighted to be representative
of the national adult population.

2

Throughout this paper we focus on representation in the U.S. House. The U.S. Senate
requires an even more complex analysis, owing to the multiple representatives per district
and the possible effects of state size on representation. We leave that for future work.

9

2013 and 2015, Republican congressional leaders put few substantive policies for a vote on the
House floor, fighting the President to a stalemate over the budget resulting in a government
shutdown which clogged the legislative agenda. Once the Republican party gained unified
control in 2017, they passed more significant bills.
Representative behavior and constituent opinion are difficult to compare on the same
scale. The CCES addresses this measurement challenge by presenting issues as a key vote
that Congress is considering or is anticipated to consider,3 and describing the issue in concrete
terms. These questions have been widely used in other work on representation (Bafumi and
Herron 2010; Tausanovitch and Warshaw 2018; Ahler and Broockman 2018).4
After each congressional term we find the roll call vote corresponding to each of the
questions, if a floor vote was held, and link the respondent’s U.S. House representative’s vote
to that response for our measure of actual agreement. Our measure of perceived agreement
is built from the interaction between a constituent’s perception of their representative and
their own preferences. An example of a short5 perception question comes from 2017:
“This year Congress considered several bills to repeal or change the Affordable
Care Act. For each of the following bills we would like to know how you think
your member of Congress voted and whether you support or oppose the bill.
3

The CCES is fielded in the fall of each year. The legislative calendar is such that for almost
all questions, the rollcall vote has already occurred before respondents answer the CCES.

4

The benefits of this issue-by-issue approach are summarized well in Lax et al. (2019). Hill
and Huber (2019) show that providing contextual information such as the party leader’s
positions moves respondent’s reported preference towards those positions. We use the
responses to our questions as a measure of preferences before such cues are made explicit.

5

Other bills were described more concretely, for example the Dodd-Frank Act was described
as “Protects consumers against abusive lending, regulates high risk investments known as
derivatives, and allow government to shut down failing financial institutions.” All question
wordings are available on the CCES Dataverse.
10

“A Bill to repeal the Affordable Care Act, known as Obamacare.
“Do you support or oppose this bill?
 Support
 Oppose
“Do you think [Representative]6 voted for or against a bill to Repeal Obamacare?7
 For
 Against
 Not Sure
We then represent respondent i’s Perceived Issue Agreement on issue j ∈ {1, ..., m} as
AIij and assign it a value of 1 if respondent i’s preference agrees with his perception of his
representative’s roll call vote on issue j (i.e., for-for or against-against). If the respondent’s
preference is in disagreement with his belief (i.e., for-against, or against-for), then AIij =
−1. And AIij = 0 if the respondent either does not have a belief or does not express a
preference on the issue. We then compute respondent i’s Perceived Issue Agreement with
P
his representative as the average across issues, i.e. AIi = m1 m
j=1 AIij .
We code the rest of the variables similarly, from −1 to 1 to facilitate comparison. Perceived Party Agreement (APi ) is the party equivalent of Perceived Issue Agreement: It is 1
if respondent i identifies himself as the same party as he perceives the representative to be
(i.e., Republican-Republican, or Democrat-Democrat). It is −1 if he thinks the representa6

The name of the representative is filled in with the respondent’s U.S. House incumbent
representative, without displaying their party affiliation.

7

Several dozen unrelated questions are typically placed in between the first question (asked
in the common content) and the second question (asked in the team modules) to minimize
demand effects.

11

tive is of the opposite party, and 0 if either the respondent is not sure of the party of his
representative or he identifies as an Independent.
Actual Issue Agreement (ZIi ) is the counterpart to perceived issue agreement, with the
respondent’s belief about the vote replaced by the representative’s actual roll call vote. In
other words, ZIij 6= AIij indicates respondent i’s perception of his representative’s vote on
issue j is incorrect. Actual Party Agreement (ZPi ) is the counterpart to perceived party
agreement, with the respondents’ perceptions of their representative’s party replaced with
their representative’s actual party affiliation.
We measure evaluations of the representative Yi by approval and vote. Approval of the
current representative ranges from “Strongly Disapprove” to “Strongly Approve”, rescaled
from -1 to +1 with equal intervals. The CCES measures Vote Choice in general election
years by asking who respondents intend to vote for, presenting candidates’ name and party.
The variable is 1 if the respondent intends to vote for the incumbent, −1 if voting for the
challenger, and 0 if he does not plan to vote or is unsure. If an incumbent is not running for
reelection, the observation is dropped from this analysis.
The control variable Actual Ideological Agreement is measured as the proximity between
the representative’s DW-NOMINATE score and the voter’s ideological self-placement. This
is an admittedly coarse measure because we do not jointly scale constituents and candidates.
But Broockman (2016) highlights the challenges of scaling public opinion. And more importantly, our main task is to test models of accountability, in which voters evaluate incumbent
legislators ex-post (Fearon 1999) — not to explore whether candidate choice is a function of
spatial distance. Further details on operationalization are left to Appendix A.
Table 1 presents summary statistics for each of the variables. To substantively interpret
these values, note that the mean of a variable that is coded {1, 0, −1} is the difference
between the percent of the sample coded 1 and the percent coded −1. For example, the
mean value of vote choice is the percent of the sample who would vote for the representative
minus the percent who would vote for the challenger. In other words, on average incumbents

12

Table 1 – Descriptive Statistics
(a) Predictor Variables and their Instruments

Perceived Issue Agreement
Perceived Party Agreement
Actual Issue Agreement
Actual Party Agreement

Mean

Standard
Deviation

Observations

0.090
0.114
0.101
0.119

0.590
0.714
0.662
0.788

51,115
49,195
51,172
47,664

(b) Outcome Variables

Job Approval
Vote Choice (for incumbent)

Mean

Standard
Deviation

Observations

0.066
0.230

0.711
0.844

45,600
25,984

Note: All variables range from -1 to 1.

enjoy a 23 point vote margin from their constituents. Panel (a) shows that representatives
have a 10 point net agreement on all four measures. In other words, House representatives
are 10 percentage points more likely to vote on the same side of their constituent, in terms
of roll call vote and party, in perception as well as in reality.

Effects of Actual Roll Call Votes on Evaluations
Our model of accountability (Figure 1) is a three-component process, tracing the votes representatives make to constituent’s perceptions and evaluations. We examine the reduced form
effect of actual agreement on downstream evaluations first, because the finding is a familiar
one to the literature and it sets the stage for the two remaining psychological mechanisms.
Actual Issue Agreement appears to have a strong, consistent effect on people’s evaluations
of their representatives. Table 2 presents estimates from the regression in equation (1).
These regressions include the controls previously discussed — representative fixed effects,
ideological agreement, ideological distance from the challenger, and demographic variables.
Standard errors are clustered at the representative level to account for correlated errors. The
coefficient on Actual Issue Agreement in predicting approval is 0.23. That means a person
13

Table 2 – Actual Agreement and Evaluations (Reduced Form)
Outcome: Approval
All Years

Bush 2nd
(2006-2008)

Obama 1st
(2009-2012)

Obama 2nd
(2013-2016)

Trump
(2017-2018)

Actual Issue Agreement

0.23
(0.006)

0.33
(0.01)

0.29
(0.008)

0.06
(0.01)

0.23
(0.03)

Actual Party Agreement

0.22
(0.006)

0.11
(0.01)

0.23
(0.007)

0.23
(0.01)

0.22
(0.02)

Average of Outcome
R-squared
Clusters
Observations

0.07
0.39
847
42,559

0.04
0.36
482
10,010

0.08
0.47
529
23,675

0.05
0.32
498
6,286

0.06
0.46
434
2,588

Outcome: Vote Choice
All Even
Years

Bush 2nd
(2006, 2008)

Obama 1st
(2010, 2012)

Obama 2nd
(2014, 2016)

Trump
(2018)

Actual Issue Agreement

0.20
(0.010)

0.28
(0.04)

0.24
(0.01)

0.08
(0.02)

0.30
(0.04)

Actual Party Agreement

0.35
(0.01)

0.35
(0.03)

0.30
(0.01)

0.49
(0.02)

0.45
(0.04)

Average of Outcome
R-squared
Clusters
Observations

0.23
0.48
786
24,051

0.24
0.63
411
1,801

0.24
0.48
484
16,946

0.21
0.53
445
3,749

0.19
0.67
368
1,555

Note: Estimate of the reduced form models in equation (1). Standard errors clustered by
representative in parenthesis, and control variables not shown.

whose own preferences on issues are in complete agreement with their representatives’ roll
call votes on those issues is 11 percentage points more likely to approve of the representative
than another constituent of the same representative, of the same party, and of the same
ideology, but agrees with only half of the issues. The coefficient on Actual Issue Agreement
in predicting vote choice is 0.20. That means the incumbent’s share of the votes is 10 points
higher among constituents who are in complete agreement on issues than, again, another
constituent with the same observable characteristics but is in agreement with only half of
the issues. These are substantively large effect sizes, given the strict match on covariates we
are enforcing with the controls in the regression.
Actual Party Agreement is also, unsurprisingly, associated with strong evaluations. The
14

effect of Actual Party Agreement is 0.22 on approval and 0.35 on vote choice. Party and issues
have comparable effects on approval, but party agreement controlling for issue agreement
has a larger effect on vote choice.
The threat to inference remaining after Table 2 is the lingering suspicion of omitted
variable bias. We take the sensitivity analysis approach by Cinelli and Hazlett (2020) to
address this risk, asking how strong an unobservable omitted variable would have to be
to render the coefficient on Actual Issue Agreement null. In Appendix E, we find that an
unobserved confounder would have to be more than twice as strong as co-partisanship to
explain away our results in the approval regression, and about 1.5 times as strong in the
vote choice regression. Given the predominance of partisanship in vote choice, it is hard to
imagine such a variable that is not already in our list of controls.
The conclusions we can draw from these findings are similar to aggregate studies which
measure agreement in terms of rollcall scores and presidential vote, party loyalty, and
individual-level studies that measure ideological distance. Where our results part from these
studies is that we suggest that specific votes on key issues may move evaluations, even holding party or ideological congruence constant. But this reduced form effect is incomplete.
It is unclear from this quantity alone how or if constituents perceive issue agreement and
whether they act on it, which we turn to next.

Reality and Perception
The first requirement for accountability is accurate perception of actual agreement. Issuevoters who are nevertheless misinformed about how their representative stands on those
issues leads to what Bartels (2008) called “unenlightened self-interest” (p.150). Survey researchers have long documented that citizens appear to have thin factual knowledge about
Congress (Delli Carpini and Keeter 1997; Fowler and Margolis 2014). Other scholars argue that the electorate reasons, even with incomplete or partial information, to draw fairly
accurate inferences about politics, such as the positions their representatives take (Lupia
15

and McCubbins 1998). Our measure of perceived and actual agreement are well-suited to
adjudicate these claims.

Correct Perceptions
Twice as many respondents in the CCES data hold correct beliefs about how their representatives voted as hold incorrect beliefs. For the average issue, 43 percent of voters perceive
correctly, 42 percent are not sure, and 19 percent have an incorrect perception. 73 percent
can name the correct party affiliation, 21 percent are not sure, and 6 percent are incorrect.
Table B1 in the Appendix presents these numbers for each issue and for party.
Further, constituents who are more educated, express higher interest in the news, and are
higher-income are significantly more likely to have correct perceptions. And constituents of
extremist representatives are also more likely to have correct perceptions of how their member voted compared to constituents with similar individual demographics but in a district
represented by a more moderate representative (also see Dancey and Sheagley 2016). These
patterns are borne out by a Heckman selection model which estimates first the likelihood of
a constituent to make a guess, and second the likelihood that the guess is correct conditional
on making a response (Appendix C). All together, the factors that shape correct perceptions for issues fall squarely into theories of communication that find the receptivity of the
respondent (in this case, the constituent) and the strength of the signaler (in this case, the
representative) to be important determinants of perceptions.
One possible concern with our measurement of perception is that respondents might have
looked up the answers while taking the online survey. In Appendix C we provide evidence
showing this is unlikely. The CCES tracks how many seconds each respondent spent on
each page. Respondents take about as long answering the perception question as they do
answering other questions of the same length and format, and respondents who take longer
to answer are actually less likely to provide correct answers.

16

Figure 2 – Does Perceived Agreement Reflect Actual Agreement?
All Repsondents

By Perceived Party Agreement
1.0

0.50

0.5

0.0

−0.5

−0.42

Perceived Issue Agreement

Perceived Issue Agreement

1.0

−1.0

Co−Partisans
0.5

0.0

−0.06

0.59
0.41
0.20

Independents

−0.33
−0.5

−0.60 Opposite Party

−1.0
−1.0

−0.5

0.0

0.5

1.0

−1.0

Actual Issue Agreement

−0.5

0.0

0.5

1.0

Actual Issue Agreement

Note: Lines show OLS best fit lines and numbers show predicted value at endpoints. The
45 degree line indicates perfect correspondence.

Actual Agreement and Perceived Agreement
Modeling the relationship between perceived and actual issue agreement illustrates the coexistence of partisan bias and accurate learning more clearly. The left panel of Figure 2 shows
that constituents who in fact disagrees with their representative on all the issues asked in
the CCES also perceive to be disagreement: an average of -0.42 on a -1 to 1 scale. Those
who agree with their member on all bills perceive an agreement of 0.50. The resulting slope
of 0.46 reflects how perception does track reality on average, but not perfectly (which would
result in a slope of 1). The attenuation is explained by both incorrect perceptions and “not
sure” responses.
This relationship between reality and perception might be spurious, however, driven by
the composition of partisan loyalists who are oblivious to actual issues. The right panel
controls for the perceived party agreement and lends support to both stories: perception
is both biased in terms of party, but it is also responsive to actual agreement. Among
perceived co-partisans, those who are in-truth in complete disagreement (−1) on the issues
perceive an agreement of −0.06 on average. If they were perfect perceivers, the score would
17

Table 3 – Actual Agreement and Perceived Agreement (First Stage)
Outcome:
Perceived
Issue
Agreement

Outcome:
Perceived
Party
Agreement

Actual Issue Agreement

0.34
(0.006)

0.05
(0.004)

Actual Party Agreement

0.10
(0.004)

0.63
(0.006)

Actual Ideological Agreement

0.30
(0.008)

0.14
(0.007)

Average of Outcome
Std. Dev. of Outcome
R-squared
F-test for Weak Instruments
Clusters
Observations

0.09
0.59
0.36
2,116
848
46,574

0.12
0.71
0.60
6,388
848
46,585

Clustered Standard Errors by Representative.
Note: Each column is a OLS regression. Controls, representative fixed effects, and year
fixed effects not shown. Clustered standard errors by representative.

be at −1. Similarly, perceived opposite-partisans who are in-truth in complete agreement
with the member on the specific issues have a net perceived agreement of only 0.20. The
slopes of all three groups are attenuated towards zero but are still positive and significantly
distinguishable from a flat line. It does not appear that the correlation is completely driven
by partisan biases.
The first stage of equation (2) in Table 3 further confirm that it is the facts of the roll
call vote, rather than party heuristics, that predominantly shape the perception of votes.
The coefficient on Actual Issue Agreement in predicting Perceived Issue Agreement is 0.34,
meaning that if a constituent supports a bill and is represented by a member who voted
for that bill, he is 34 percentage points more likely to believe they are in agreement with
the legislator on that bill (compared to if the representative had voted against the bill).
Sensitivity analyses in Appendix E show how unlikely it is that this relationship is confounded
by unobserved variables. Even if there were an omitted confounder that is as strong as Actual
Party Agreement, the coefficient estimate would only drop to 0.30.

18

Party does appear to serve as a heuristic in shaping beliefs about representative’s legislative decisions. However, the coefficient sizes suggest that it is of secondary importance:
the effect of Actual Issue Agreement is three times larger than the effect of Actual Party
Agreement on how voters perceive issue agreement. And a parallel pattern emerges with constituent’s perceptions of party agreement. In the second column of Table 3, the coefficient
on Actual Party Agreement in predicting Perceived Party Agreement is 0.63.
There is a symmetry, then, between the two regressions in Table 3: constituents learn
about issues more from issues than from party, and learn about party more from party than
from issues. In Appendix A we provide estimates by issue and find some variation over time
which, in Appendix D, we in part attribute to the Congressional agenda.
On the whole, the public’s perceptions are rooted in the reality of the decisions representatives make.8 There is evidence of uncertainty and copartisan misperception, but Table 3
shows these to be second-order. The typical person’s understanding of how their representative voted on key legislation is, on balance, a fairly accurate reflection of their legislator’s
actual behavior. The question we turn to next is how much constituents use that information
to hold legislators accountable.

Perception and Evaluation
The reduced form indicates that there are downstream consequences to a representative’s
votes, and the first stage indicates that constituent’s perceptions about those positions are
noisy and biased, but on average track actual positions. This sets the stage for the third and
final component of electoral accountability in Figure 1: how constituents translate perceived
agreement to the evaluations of their representative. We use three approaches to isolate this
causal quantity.
8

In the context of our instrumental variables strategy, this means that our instruments are
strong. The F -statistic is over 2,000 instrumenting for perceived issue agreement.

19

Figure 3 – Differences in Evaluation by Perceived Agreement
(a)

(b)

Approval, by Agreement

Vote Choice, by Agreement

Cells: mean of values −1, −0.5, 0, +0.5, +1

Cells: mean of values −1, 0, +1

Perceived Party Agreement
Don't Know or
Disagree PID = Indep. Agree

−0.67 −0.41

0.22

Don't Know or
Disagree PID = Indep. Agree

Total

−0.43

Bottom Tercile

(n = 6,541) (n = 6,505) (n = 2,157) (n = 15,377)

Middle Tercile

−0.26

0.04

0.44

Perceived Issue Agreement

Perceived Issue Agreement

Bottom Tercile

Perceived Party Agreement

0.12

(n = 1,828) (n = 7,042) (n = 3,767) (n = 12,917)

Top Tercile

−0.06

0.35

0.64

0.47

(n = 1,234) (n = 6,519) (n = 9,113) (n = 17,070)

Total

−0.51 −0.00

0.53

0.07

0.62

0.75

0.42

0.60

−0.22

(n = 3,818) (n = 4,054) (n = 1,317) (n = 9,324)

Middle Tercile

−0.27

0.20

0.73

0.29

(n = 995) (n = 3,543) (n = 2,054) (n = 6,780)

Top Tercile

−0.12

0.48

0.82

0.62

(n = 668) (n = 3,807) (n = 5,062) (n = 9,691)

Total

(n = 9,626) (n = 20,165)(n = 15,094)(n = 45,600)
Difference
(Top − Bottom)

−0.58 −0.14

Total

−0.47

0.17

0.77

0.23

(n = 5,500) (n = 11,476) (n = 8,457) (n = 25,984)
Difference
(Top − Bottom)

0.91

0.47

0.62

0.22

0.85

Note: Cells are averages of (a) approval and (b) vote for the representative.

Difference in Conditional Means
We first sketch out the relationship with conditional difference in means. Figure 3 displays
weighted averages of approval and vote choice by subsets of perceived agreement. Recall
that our outcome variables range from −1 to 1, so the average of the vote choice variable
is equivalent to the electoral margin of the incumbent. The average of approval is similarly
interpreted as the net approval, percent approval minus percent disapproval. Consider first
the differences in row averages in panel (a). Constituents who perceive low levels of issue
agreement with their representatives, displayed in the top row, express a net approval rating
of -0.43, but those who see themselves in agreement with their representatives’ roll call votes
have a net approval of 0.47.
These differences due to issue agreement are not explained away by partisan agreement.
The row below the solid line of Figure 3 shows the difference in outcomes between the top and
bottom terciles of issue agreement, within each level of party agreement. Take incumbent
vote choice in panel (b). Among people who believe they are the same party as the incumbent
20

(the third column), the difference between high and low issue agreement is 22 points in vote
margin. Among people who believe they are the opposite party as their representatives, the
same difference is 47 points in vote margin. And among independents and those who did
not know the party of their representatives, the improvement is 62 points.
We hold constant more characteristics of the representative, the constituent, the issue
at stake, and the congressional district by estimating equation (3). These estimates in Appendix B show that a one-unit increase in perceived issue agreement on a scale of -1 to +1
is associated with an increase in the respondent’s net job approval of 19 percentage points,
holding constant other correlates of issue agreement such as perceived party agreement and
perceived ideological agreement. For vote choice, an improvement in perceived issue agreement from the middle of the scale to complete issue agreement is associated with an increase
in the respondent’s likelihood of voting for that incumbent by about 11 percentage points.
These indicate that accountability does exist, smaller than the simple difference-in-means
suggested by Figure 3 but on the same order of magnitude.

Instrumental Variable Estimates
The difference-in-means approach may overstate the causal effect of perceived issue and
party agreement on evaluations if there are unobserved confounders that are correlated with
perceived agreement and correlated with evaluations. To correct for these statistical biases,
we implemented our instrumental variable (IV) estimator shown in Figure 1 and equations
(2) and (3).
We see substantively large direct effects of issues on vote choice in every Congress studied,
with some variation over time. Table 4 summarizes our key results (See also Appendix B
for issue-specific estimates). We start with the first column that uses the data from all
years. The IV coefficient on Perceived Issue Agreement predicting approval is 0.64 and the
coefficient on predicting vote choice is 0.58. Because the standard deviation of Perceived Issue
Agreement is about 0.60 (Table 1), this indicates that a one-standard deviation improvement

21

Table 4 – Perceived Agreement and Evaluations (Instrumental Variables)

Outcome: Approval
All Years

Bush 2nd
(2006-2008)

Obama 1st
(2009-2012)

Obama 2nd
(2013-2016)

Trump
(2017-2018)

Perceived Issue Agreement

0.64
(0.02)

0.70
(0.03)

0.73
(0.02)

0.36
(0.08)

0.60
(0.06)

Perceived Party Agreement

0.22
(0.009)

0.08
(0.02)

0.23
(0.01)

0.34
(0.02)

0.28
(0.03)

Average of Outcome
Clusters
Observations

0.07
847
42,417

0.04
482
9,999

0.08
529
23,625

0.05
498
6,205

0.06
434
2,588

Outcome: Vote Choice
All Even
Years

Bush 2nd
(2006, 2008)

Obama 1st
(2010, 2012)

Obama 2nd
(2014, 2016)

Trump
(2018)

Perceived Issue Agreement

0.58
(0.03)

0.97
(0.1)

0.60
(0.04)

0.48
(0.2)

0.78
(0.1)

Perceived Party Agreement

0.43
(0.02)

0.42
(0.05)

0.33
(0.02)

0.87
(0.06)

0.69
(0.07)

Average of Outcome
Clusters
Observations

0.23
786
23,949

0.24
411
1,799

0.24
484
16,915

0.22
445
3,680

0.19
368
1,555

Note: Each column is an instrumental variables regression. Controls, representative fixed
effects, and year fixed effects not shown. Clustered standard errors by representative.

in a constituent’s Perceived Issue Agreement improves net approval or the vote margin of
the incumbent by about 35 percentage points.
We further explore the possibility that the degree of issue voting varies across types of
people, types of issues, and the context of specific roll call votes. Details of those analyses
are in Appendix D. First, the estimates may vary with the salience of the issue to the public.
All of these issues were salient in the sense of being key votes in Congress that made it to a
floor vote. Nonetheless, some of the issues, especially health care, were routinely at the top
of the legislative agenda for both parties. We divided the issues as highly salient and less
salient, and found that the estimates were quite similar.
Second, we examined the heterogeneity of effects due to strategic roll call voting. It may
22

be the case that voters reward and punish legislators more sharply when the legislators’
votes are pivotal to the passage of legislation (Snyder and Groseclose 2000). We found no
evidence that voters responded more to their legislator being pivotal on close votes than to
other contexts. Nor did we find evidence that abstention insulates legislators by creating
ambiguity (Arnold 1990).
Finally, we examined whether the effect of issues only exists among high interest voters. We divided the sample by level of political interest (following Bartels 1996) and found
no consistent differences in issue voting: The coefficients on perceived agreement on issues
were similar for high, medium, and low levels of interest. Higher interest voters were, however, more ideological and less partisan than low interest voters. This pattern suggests an
important way in which issue voting is distinct from ideological and partisan voting.
The interpretation of the IV estimate also deserves more nuance. There are two ways to
interpret the IV coefficient — one from an omitted variable perspective and the other as a
local average treatment effect. In the former, the IV coefficient represents the average effect
of the treatment variable (in this case, perceived issue agreement) after controlling away
attenuation biases due to measurement error and unobserved confounding. In other words,
the IV estimate is an improved version of the OLS estimate.
If the effect among respondents that change their perceived agreement in response to
actual agreement is different than the effect among other groups, however, the IV coefficient
identifies the average treatment effect among the former group, also known as compliers.9
On the one hand, this means that the IV estimates are less generalizable to an average
effect. Fortunately, the nature of our instrument means that the compliers are a theoretically
important group in their own right. These are constituents who, by definition, respond to
changes in reality. Our large IV estimates interpreted as a local average treatment effect
9

These do not include constituents who, upon an increase in actual agreement decrease their
perceived agreement, or vice versa. Such individuals would be defiers in the IV context
and must be assumed away.

23

therefore suggests that this perceptive subset of the electorate enforces a strong degree of
accountability.
We do not deny the existence of projection. For example, one could imagine that prior
approval (independent of actual issue agreement) affects perceived agreement — the reverse
of our causal claim. If this is correlated with current approval, it would induce a correlation
between perceived agreement and current approval. Still, what our IV results show is that
another causal pathway, perhaps together with some partisan projection, exists: one in
which actual agreement flows through perceived agreement such that constituents hold their
representatives accountable.

Experimental Evidence of the Causal Connection
Our third approach to measuring the effect of perceived issue agreement on evaluation addresses the concern that the IV estimates may suffer from a violation in the exogeneity condition. We conducted two randomized experiments, one during the Democratic Congress under President Obama and another during the Republican Congress under President Trump.
Two of our CCES modules contained experiments that selectively provided respondents with
information about their representatives.
The 2009 study (n = 5,700) provided correct information to randomly chosen subsets of
respondents and no information to others. One type of information regarded roll call votes.
Respondents were told how their House representatives actually voted on two randomly
chosen votes. The possible votes were the State Children’s Health Insurance Program, the
American Reinvestment and Recovery Act, the American Clean Energy and Security Act,
and the Patient Protection and Affordable Care Act. An additional type of information was
party: half of the sample was randomly chosen to be told the correct party affiliation of their
representative and half were told no party information.
The 2018 study (n = 2,000) provided respondents with randomly determined information
about four roll call votes. The study randomly assigned a Yes or No vote to the representative

24

on four votes separately, regardless of whether that information was correct or incorrect. The
bills were the 2018 Bipartisan Budget Act, the Tax Cuts and Jobs Act of 2017, the Mobilizing
Against Sanctuary Cities Act, and the American Health Care Act which partially repealed
the ACA. There was no party treatment in this study. To limit the risks associated with
deception, all participants were debriefed shortly after and informed that the information
they were provided was randomly chosen and was not a reflection of how their member of
Congress actually voted on those issues.
To make estimates comparable with our observational analyses, we coded the issue treatment variables as 1 if the respondent had a preference that in fact agreed with the experimentally provided information about the representative’s roll call vote, −1 if the respondent
in fact disagreed with the provided information, and 0 if no information on that issue was
provided. We coded the party treatment similarly, with 1 indicating treatment providing
co-partisan information and −1 indicating treatment providing out-partisan information.
We then computed the sum of the agreement measures for the different roll call votes divided by the number of treated roll call votes. Because our treatment variable includes
non-randomized preferences, we control for pre-treatment Perceived Issue Agreement, pretreatment Perceived Party Agreement, the baseline measure of approval, and demographic
variables when estimating treatment effects.
These experiments are meant to confirm the causal inferences from the main IV estimation
strategy. Experiments have the advantage that, by design, whether the respondent received
the information in the treatments is independent of any other factor. Experiments, of course,
have their limitations. For example, we do not change how legislators actually voted, but
only offer information to respondents. These are messages that respondents may accept or
reject. The 2018 experiment may also be limited in external validity because it presents
off-equilibrium signals, counterfactual votes that representatives chose not to cast. Such
counterfactuals strength inferences about causality, but may weaken effects.
Both the issue and party treatments moved approval by 7 to 10 percentage points. The

25

Table 5 – Experimental Effects of Issue Agreement
(a) 2009 Study
All

Subsets
No Prior

Some Wrong

All Correct

Vote information treatment (in agreement)

0.10
(0.01)

0.08
(0.02)

0.11
(0.02)

0.01
(0.03)

Party information treatment (in agreement)

0.07
(0.01)

0.12
(0.02)

0.05
(0.02)

0.03
(0.02)

Average Outcome in Control
Proportion High News Interest
R-squared
Observations

0.04
0.57
0.56
4,863

0.00
0.27
0.25
1,409

0.08
0.59
0.49
1,626

0.02
0.79
0.76
1,828

(b) 2018 Study
All

Subsets
No Prior

Some Wrong

All Correct

Vote information treatment (in agreement)

0.08
(0.02)

0.08
(0.06)

0.08
(0.03)

0.11
(0.07)

Average Outcome in Control
Proportion High News Interest
R-squared
Observations

0.03
0.51
0.62
1,947

0.01
0.18
0.35
284

0.10
0.52
0.59
1,348

-0.23
0.73
0.79
315

Note: Each column is an OLS regression where the outcome is approval. Pre-treatment
controls not shown. Robust standard errors in parentheses.

first column in each panel of Table 5 presents the treatment effects for all respondents.
The coefficients on the issue treatments are 0.10 in 2009 and 0.08 in 2018, and both are
statistically distinguishable from 0 (p < 0.01). The coefficient on the party information
treatment is of similar magnitude.
The effect of additional information should depend on voter’s prior beliefs. We therefore
divided the sample into subgroups of prior levels of completeness and correctness of beliefs.
One subset did not have any belief about how their representatives voted on any votes; the
second had incomplete and incorrect prior beliefs on some votes; and a third had correct
prior beliefs on all votes. In the 2009 experiment, the information provided is correct so
only the last group would not have received new information. In the 2018 experiment the
information treatment is orthogonal to prior beliefs so all groups are equally treated.
26

Our results are consistent with Bayesian updating. Respondents who had correct prior
beliefs exhibited no statistically significant increase in approval in response to confirmatory
information in the 2009 experiment. We also took the subset of 2018 respondents who had
correct prior beliefs and estimated separate effects among those assigned correct information
and those assigned incorrect information. The effect was concentrated among the latter
(Appendix B).
Three implications of these experiments deserve emphasis. First, the experiments reaffirm
the findings of issue accountability from the observational and instrumental variables. As
Bullock (2011) found with a similar design to our 2009 experiment, people use information
about roll call votes, when it is available, to evaluate their legislators. Second, people value
the roll call vote information and party labels about equally in updating their evaluations
of their representatives. Third, the subgroup comparisons confirm that our findings reflect
real beliefs instead of random guessing.

Reconciling Individual versus Aggregate Effects
Our estimates indicate that the effect of a one standard deviation increase in perceived issue
agreement is approximately 35 percentage points on an individual’s likelihood of voting to
re-elect the incumbent. That is in line with existing estimates from surveys, but much larger
than estimates using aggregate election data. Canes-Wrone et al. (2002) estimate that a one
standard deviation change in the rollcall score of the legislator to change their vote share by
1 to 3 percentage points or less (see also Bonica and Cox 2018). Tausanovitch and Warshaw
(2018) reasonably ask why the individual and aggregate estimates in this literature differ.
The answer lies in aggregation. It is well known that analyses of correlations among aggregates suffer from the ecological fallacy, and use of proxy variables, such as use of presidential
vote to measure constituents’ preferences, introduces measurement error. Even setting aside
these measurement problems, there are two first-order consequences of aggregation.
First, aggregation cancels out individual-level effects of opposing signs. If 100 percent of
27

Figure 4 – Consequences of Aggregation for Representation
Aggregated across Key Votes
(Voter Level)

Individual Vote
Mean = 0.11,
Std. Dev. = 0.96

0.4

Aggregated across Constituents
(District Level)
0.003

Mean = 0.10,
Std. Dev. = 0.66

0.05

Mean = 0.11,
Std. Dev. = 0.2

Proportion

0.04
0.002

0.3
0.03
0.2
0.02
0.1

0.0

0.001

0.01

−1.0

−0.5

0.0

0.5

1.0

0.00

−1.0

−0.5

0.0

0.5

1.0

0.000

−1.0

−0.5

0.0

0.5

1.0

Actual Issue Agreement at Different Levels of Aggregation

Note: Panels show the distribution of Actual Issue Agreement at the individual-level (left),
the constituent-level (center) and the district-level (right).

constituents in a district support a bill, then, our estimates (Table 2) would suggest that a
legislator can expect to see her voteshare increase by 20 percentage points if she votes for
the bill instead of voting against it. But constituencies are never completely for or against
a bill. On the typical CCES issue, a congressional district’s constituents are split 60-40.
In that case, the representative will increase her standing among the 60 percent of people
who support the bill by 20 percentage points, but will simultaneously lose 40 percent of
her constituents by the same magnitude. The net gain is only 4 percentage points in vote
margin. The average actual issue agreement in our data at the individual-level is 0.10 (Table
1), which translates into 55 percent in agreement and 45 percent in disagreement. Therefore
even assuming that the effect of a rollcall vote on an individual constituent’s vote choice is
20 points, its contribution to vote share is only 2 points.
The scale of comparison is also smaller at the aggregate level. Typically, studies report
the effect of a one standard deviation unit change in agreement on vote or approval. But the
variation in the mean of a variable is necessarily smaller than the variation in the variable
itself. Figure 4 illustrates this using the CCES, showing the distributions of Actual Issue
Agreement measured at three levels. The standard deviation of Actual Issue Agreement

28

at the individual level is 0.66, while the standard deviation of its district-level counterpart
is only 0.21. The two distributions have the same mean but that similarity masks stark
differences in scale. Hence, the effect of a one standard deviation change in issue agreement
at the individual level is 13 percentage points (i.e., 0.66 × 0.20), but the effect of a onestandard deviation change in issue agreement at the district level is only 4 percentage points
in vote margin (i.e., 0.21 × 0.20), or a 2 percentage point change in vote share.
The aggregate effects of issue congruence implied by the individual level estimates are
on the same order as those estimated by researchers using aggregate data, even setting aside
potential aggregation and measurement biases with those analyses. Put another way, small
aggregate differences can still reflect strong issue voting at the individual-level.

Conclusion
This study has sought to advance the longstanding debate on electoral accountability by
bringing extensive data on constituent knowledge and issue voting, combining multiple estimation strategies, and providing explanations to reconcile seemingly inconsistent findings.
The classical theory of representation posits that constituents pay attention to and care
about the policy decisions their representatives make. V.O. Key (1966), examining party
switching between 1936 to 1960, argued that voters are “moved by concern about central and
relevant questions of public policy” (p.8). Many others have openly challenged the tenets
of the classical theory. Warren Miller and Donald Stokes (1963), examining the 1958 National Election Study, reached the conclusion that “given the limited information the average
voter carries to the polls, the public might be thought incompetent to perform any task of
appraisal” (p.53).
Twelve years of data across various political contexts demonstrate that the American
electorate approximates the classical ideal in two essential respects. First, while the public is
somewhat biased towards copartisan representatives, on the whole it sees Congress correctly.
Second, constituents hold their representatives accountable for their votes on key legislative
29

decisions. The typical constituent expresses considerably higher support for their congressional representatives when she or he sees that the representative has voted the way the
constituent would have. Over twenty years ago Lupia and McCubbins (1998) asked whether
“citizens can know what they need to know.” Our findings on electoral accountability for
key legislative decisions answer that question in the affirmative.
In the present political context, these findings are particularly striking. Against a background of party polarization in Congress, one might expect that the electorate has abandoned
their own preferences on issues and, instead, blindly taken sides with one party. The evidence mustered here shows that voters can punish representatives with whom they disagree
on legislative decisions, even if the representative is a copartisan. The effects of issues are
approximately as large as the effects of party on constituents’ evaluations. This contrasts
starkly with theories that begin with the claim that most voters are largely ignorant about
legislative decisions and thus conclude that constituents must rely on elites and party labels
for representation (Bawn et al. 2012). We are not arguing that elite capture does not occur.
Rather, we suggest that theories of representational failure cannot rely on the premise that
individual voters are unable to hold legislators accountable on issues.
Our findings also help reconcile two observations. On the one hand, individual constituents respond strongly to their legislators’ roll call votes. But on the other hand, aggregate voteshares are only modestly correlated with legislators’ roll call voting records. This
is a result of aggregation. Many legislative districts are fairly evenly split on key legislation.
A legislator may vote with the majority of her district and get the support of 55 percent of
her constituents, but lose the support of the remaining 45 percent. Those with whom the
legislator sides care deeply about the issue, as do those opposed to the legislator’s vote. But,
in the aggregate the net effect is modest because much of the support and opposition for the
bill cancels out. Aggregate correlations should not be taken as measures of the true degree
to which individuals care about or vote on the issues. By the same token, in extremely competitive districts, representatives have a difficult time satisfying the majority of the voters

30

back home.
In the end, were Miller and Stokes wrong? No, they simply did not have a powerful
enough microscope. Advances in survey methodology and technology have made it possible
to measure with greater accuracy and statistical power how individual voters see and respond
to their representatives’ policy decisions. The portrait that emerges is not an inattentive
and uncaring electorate; nor is it a hyper-attentive, hyper-rational electorate. Rather, the
electorate on the whole is sufficiently attentive and sufficiently motivated by public policy
to exert electoral control, albeit imperfectly, as envisioned by the classical theory of representation.

31

References
Ahler, Douglas and David E. Broockman (2018). “The Delegate Paradox: Why Polarized
Politicians Can Represent Citizens Best”. Journal of Politics 80(4), 1117–1133.
Ansolabehere, Stephen and Philip Edward Jones (2010). “Constituents’ Responses to Congressional Roll-Call Voting”. American Journal of Political Science 54(3), 583–597.
Arnold, R. Douglas (1990). The Logic of Congressional Action. Yale University Press.
Bafumi, Joseph and Michael C. Herron (2010). “Leapfrog Representation and Extremism: A
Study of American Voters and Their Members in Congress”. American Political Science
Review 104(3), 519–542.
Bartels, Larry M. (1996). “Uninformed Votes: Information Effects in Presidential Elections”.
American Journal of Political Science 37(2), 1051–1068.
Bartels, Larry M. (2008). Unequal Democracy: The Political Economy of the New Gilded
Age. Princeton University Press.
Bawn, Kathleen et al. (2012). “A Theory of Political Parties: Groups, Policy Demands and
Nominations in American Politics”. Perspectives on Politics 10(3), 571–597.
Bonica, Adam and Gary W. Cox (2018). “Ideological Extremists in the U.S. Congress: Out
of Step but Still in Office”. Quarterly Journal of Political Science 13, 207–236.
Broockman, David E. (2016). “Approaches to Studying Policy Representation”. Legislative
Studies Quarterly 41(1), 181–215.
Broockman, David E. and Daniel M. Butler (2017). “The Causal Effects of Elite PositionTaking on Voter Attitudes: Field Experiments with Elite Communication”. American
Journal of Political Science 61(1), 208–221.
Bullock, John G. (2011). “Elite Influence on Public Opinion in an Informed Electorate”.
American Political Science Review 105(3), 496–515.

32

Canes-Wrone, Brandice, David W. Brady, and John F. Cogan (2002). “Out of Step, out of
Office: Electoral Accountability and House Members’ Voting”. American Political Science
Review 96(1), 127–140.
Carson, Jamie L. et al. (2010). “The Electoral Costs of Party Loyalty in Congress”. American
Journal of Political Science 54(3), 598–616.
Cinelli, Carlos and Chad Hazlett (2020). “Making Sense of Sensitivity: Extending Omitted
Variable Bias”. Journal of the Royal Statistical Society. Series B 82(1), 39–67.
Clinton, Joshua D. (2006). “Representation in Congress: Constituents and Roll Calls in the
106th House”. Journal of Politics 68(2), 397–409.
Dancey, Logan and Geoffrey Sheagley (2016). “Inferences Made Easy: Partisan Voting in
Congress, Voter Awareness, and Senator Approval”. American Politics Research 44(5),
844–874.
Delli Carpini, Michael X. and Scott Keeter (1997). What Americans Know about Politics
and Why it Matters. Yale University Press.
Erikson, Robert S., Michael B. Mackuen, and James A. Stimson (2002). The Macro Polity.
Cambridge University Press.
Eskridge, William N. (1987). “Dynamic Statutory Interpretation”. University of Pennsylvania
Law Review 135, 1479–1555.
Fearon, James D. (1999). “Electoral Accountability and the Control of Politicians: Selecting Good Types versus Sanctioning Poor Performance”. Democracy, Accountability, and
Representation. Ed. by Przeworski, Adam, Susan Stokes, and Bernard Manin. Cambridge
University Press, 55–97.
Fiorina, Morris P. (1974). Representatives, Roll calls, and Constituencies. Lexington Books.
Fowler, Anthony (2020). “Partisan Intoxication or Policy Voting?” Quarterly Journal of Political Science 15, 141–179.
Fowler, Anthony and Michele Margolis (2014). “The Political Consequences of Uninformed
Voters”. Electoral Studies 34, 100–110.

33

Gilens, Martin (2001). “Political Ignorance and Collective Policy Preferences”. American
Political Science Review 95(2), 379–396.
Guisinger, Alexandra (2009). “Determining Trade Policy: Do Voters Hold Politicians Accountable?” International Organization 63(3), 533–557.
Hill, Seth J. and Gregory A. Huber (2019). “On The Meaning of Survey Reports of Roll Call
Votes Not Cast in a Legislature”. American Journal of Political Science 63(3), 611–625.
Jessee, Stephen A. (2009). “Spatial Voting in the 2004 Presidential Election”. American
Political Science Review 103(1), 59–81.
Key, V. O. (1966). The Responsible Electorate: Rationality in Presidential Voting: 1936-1960.
Harvard University Press.
Lax, Jeffrey R., Justin H. Phillips, and Adam Zelizer (2019). “The Party or the Purse:
Unequal Representation in the US Senate”. American Political Science Review 113(4),
917–940.
Lenz, Gabriel S. (2012). Follow the Leader? How Voters Respond to Politicians’ Policies and
Performance. Princeton University Press.
Lupia, Arthur and Matthew D. McCubbins (1998). The Democratic Dilemma: Can Citizens
Learn What They Need to Know? Cambridge University Press.
Mayhew, David (1974). The Electoral Connection. Yale University Press.
Miller, Warren E. and Donald E. Stokes (1963). “Constituency Influence in Congress”. American Political Science Review 57(1), 45–56.
Nyhan, Brendan et al. (2012). “One Vote Out of Step? The Effects of Salient Roll Call Votes
in the 2010 Election”. American Politics Research 40(5), 844–879.
Shor, Boris and Jon C. Rogowski (2018). “Ideology and the US Congressional Vote”. Political
Science Research and Methods 6(2), 323–341.
Snyder, James M. and Tim Groseclose (2000). “Estimating Party Influence in Congressional
Roll-call Voting.” American Journal of Political Science 44(2), 193.

34

Stephanopoulos, Nicholas O (2018). “Accountability Claims in Constitutional Law”. Northwestern University Law Review 112(5), 989–1068.
Tausanovitch, Chris and Christopher Warshaw (2018). “Does the Spatial Proximity Between
Legislators and Voters Affect Voting Decisions in U. S. House Elections?” Political Behavior 40(1), 223–245.

35

Online Appendix (Supporting Information)

Appendix Table of Contents
A Data Sources and Variable Construction
Table A1: Data Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Table A2: CCES Issue Questions and the Congressional Agenda . . . . . . . .
Table A3: Roll Call Votes and Passage Rates . . . . . . . . . . . . . . . . . .

1
1
2
3

B Additional Analyses
Table B1: Uncertainty and Correctness of Beliefs . . . . . .
Table B2: All Main Regressions with Coefficients on Control
Table B3: Second Stage Estimates Using OLS . . . . . . . .
Table B4: Interaction between Issues and Party Agreement
Table B5: Subgroup Analysis of 2018 Experiment . . . . .
Figure B1: Estimates by Issue . . . . . . . . . . . . . . . . .

.
.
.
.
.
.

4
5
6
7
8
8
9

. . . . . .
Variables
. . . . . .
. . . . . .
. . . . . .
. . . . . .

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

C Predictors of Correct Perceptions
C.1 Sociodemographic Predictors . . . . . . . . . . . . . . . . . . . . .
Figure C1: Signal Strength and Receptivity Predict Correct Perceptions
C.2 Time Spent on Perception Questions . . . . . . . . . . . . . . . .
Table C1: Descriptive Statistics of Time Spent on Answering Questions
Table C2: Time Spent on Perception Questions and Correct Answers .

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

10
10
11
11
12
13

D Heterogeneity Analyses
D.1 Heterogeneity by Political Interest . . . . . . . . . . .
Table D1: Issue and Party Accountability by News Interest .
D.2 Heterogeneity by Salience . . . . . . . . . . . . . . . . .
D.3 Heterogeneity by Close Votes, Marginal Districts . .
Table D2: Effects in Close Votes and Marginal Districts . . .
D.4 Heterogeneity by Abstentions in Roll Call Votes . .
Table D3: Heterogeneity between Taken and Abstained Votes

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

14
14
15
15
16
17
18
18

. . .

19
20

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

E Sensitivity Analysis
Figure E1: Sensitivity of Treatment Effects to Unobserved Confounding

2

.
.
.
.
.
.
.

A

Data Sources and Variable Construction

CCES The datasets we use for each year are listed in Table A1. Each year’s CCES is
composed of several team modules each with samples of 1,000 to 2,000, and share 50 or so
standard questions called the Common Content.
Table A1 – Data Sources

1
2
3
4
5
6
7
8
9
10
11
12

Year

Dataset

CDs

Observations

Questions

2006
2007
2008
2009
2010
2012
2013
2014
2015
2016
2017
2018

CCES 2006 Module
CCES 2007 Common Content
CCES 2008 Module
CCES 2009 Module
CCES 2010 Module
CCES 2012 Wave of 2010-2014 Panel
CCES 2013 Module
CCES 2014 Module
CCES 2015 Module
CCES 2016 Module
CCES 2017 Module
CCES 2018 Module

291
437
426
437
434
436
412
412
387
436
390
433

1,013
10,000
3,000
6,000
3,000
19,500
1,500
1,500
1,000
3,000
1,000
2,000

4
3
4
5
5
3
2
1
1
7
4
5

Note: Each year’s survey sample is drawn from a part of the year’s CCES. The last column
indicates how many roll call perception questions were used in that module.

Key Votes The issues covered in the CCES, along with their rollcall vote outcomes, are
summarized in Table A2. The votes used for agreement in this paper are bolded. Table A3
shows the result of the rollcall vote of those votes and how close each one was.
Additional Details on the Operationalization of Control Variables
Ideological Agreement Respondent’s ideology is taken from their placement on a 7-point
scale ranging from “Very Liberal” to “Very Conservative,” standardized to range from −1
to 1. NOMINATE scores in a given Congress lie between around −1 (Democrats) and
1 (Republicans). The absolute difference between the two measures are then flipped so
positive values indicate less distance, and ranges from −1 to 1.
Ideological Distance from Challenger Distance is coded similar to agreement but on the
reverse scale, so that 1 indicates the highest distance. Perception of the challenger’s ideology suffers from a substantial amount of missingness in some years, because they were not
uniformly asked. To preserve the sample size, we control for an indicator of missingness and
impute values of mean.
Appendix Page 1

Table A2 – CCES Issue Questions and the Congressional Agenda
Note: Questions grouped by congressional session at the time of the survey. Each rows
reports the question number. Questions begin with CC for Common Content, followed by
the year, further followed by the official question number. Final passage in a chamber is
indicated by “ Y” (passage), “ N” (vote taken but failed), and “ (nv)” (chamber did not hold a
floor vote). Congresses are labeled by the President, Speaker, and Senate Majority Leader
at the time. Bold font indicates questions in which a perception question was asked in the
module used in this paper.
109th Congress (Bush, Hastert, Frist)
CC06_3060
CC06_3063
CC06_3066
CC06_3069
CC06_3072
CC06_3075
CC06_3078

Ban Partial Birth Abortion
Fund Stem Cell 2005
Withdraw Iraq 2006
Immigration Reform
Raise Minimum Wage 2006
Cut Capital Gains Tax
CAFTA

Hou. Sen.
Y

Y

Y

Y

(nv)

N

(nv)

N

Y

Y

Y

Y

Y

Y

110th Congress (Bush, Pelosi, Reid)
CC07_34, CC08_316E
CC07_38, CC08_316D
CC06_V2072, CC08_316B
CC07_46, CC08_316A
CC08_316C
CC08_316F
CC08_316G
CC08_316H
CC08_316I

111th Congress (Obama, Pelosi, Reid)
CC09_59A
CC09_59B
CC09_59C, CC10_332A
CC09_59D, CC10_332B
CC09_59E, CC10_332C
CC09_59F, CC10_332D
CC09_59G
CC09_59H
CC10_332E
CC10_332F
CC10_332G
CC10_332H
CC10_332I
CC10_332J

Ledbetter Fair Pay
Hate Crime Prevention
ARRA
SCHIP 2009
ACESA
PPACA
Sotomayor
House Bill ACA
Kagan
Dodd Frank
End DADT
FISA
Fund Stem Cell 2007
TARP

Hou. Sen.
Y

Y

Y

Y

Y

Y

Y

Y

Y

(nv)

Y

Y
Y

Y

(nv)
Y

Y

Y

Y

Y

Y

Y

Y

Y

Y

Y

112th Congress (Obama, Boehner, Reid)
CC11_340a
CC11_341A
CC11_341B
CC11_341C
CC11_341D, CC12_332I
CC11_341E, CC12_332J
CC11_341F
CC11_341G
CC11_341H
CC12_332A
CC12_332B
CC12_332C
CC12_332D
CC12_332E
CC12_332F
CC12_332G
CC12_332H

113th Congress (Obama, Boehner, Reid)
CC13_320A, CC14_320A
CC13_320B, CC14_320B
CC13_320C, CC14_320C
CC13_320D, CC14_320D
CC13_320E, CC14_320E
CC13_332A, CC14_323_3
CC13_332B, CC14_325_2
CC13_332C
CC13_332D
CC13_332E
CC13_332F
CC13_332G, CC14_331_2
CC13_332H
CC14_325_1
CC14_325_3
CC14_325_4
CC14_325_5
CC14_331_1
CC14_331_3
CC14_331_4
CC14_331_5

Gun Background Check
Gun No Disclosure
Ban Hi Capacity Gun
Ban Assault Rifle
Concealed Gun Permit
Ban 20 week Abortion
Simpson Bowles
Repeal ACA 2013
Keystone 2012
Sales Tax Online
Violence against Women
Freedom Act 2013
Student Success
Ryan Budget
Extend Tax Cut
Tax Relief 2014
Raise Debt Ceiling 2014
Agriculture
End Nomination Filibuster
Relig. Exempt. ACA
South Korea FTA

115th Congress (Trump, Ryan, McConnell)
CC17_340A
CC17_340B
CC17_340C
CC17_340D
CC17_340E
CC17_340F
CC17_340G
CC17_340H
CC17_340I

Repeal ACA 2017
Gorsuch
AHCA
CHOICE
Kate’s Law
Iran N.K. Russia Sanctions
No Sanctuary
DeVos
Continue Funding

SCHIP
FISA
Raise Minimum Wage 2007
Withdraw Iraq 2007
Fund Stem Cell 2007
Ban Gay Marriage
Foreclosure Assistance
NAFTA
TARP

Raise Debt Ceiling
ARRA
SCHIP 2009
ACESA
PPACA
End DADT
FISA
Fund Stem Cell 2007
TARP
Ryan Budget
Simpson Bowles
Middle Class Tax Cut
Tax Relief 2012
Relig. Exempt. ACA
South Korea FTA
Repeal ACA 2012
Keystone 2012

Hou. Sen.

114th Congress (Obama, Boehner / Ryan, McConnell)
CC15_327A, CC16_351I
CC15_327B

(nv)

N

(nv)

Y

(nv)

N

(nv)

N

(nv)

N

Y

(nv)

N

(nv)

Y

(nv)

(nv)

N

(nv)

Y

Y

Y

CC15_327C, CC16_351G
CC15_327D, CC16_351B
CC15_327E
CC15_327F1
CC15_327F2,
CC16_351C
CC15_327G, CC16_351D
CC15_327H
CC16_351A
CC16_351E

(nv)

(nv)

Y

(nv)

Y

N

CC16_351F
CC16_351H

(nv)

Y

CC16_351K

Y

Y

Y

Y

Y

Y
Y

(nv)

N

Y

Y

Hou. Sen.
(nv)

N
Y

Y

(nv)

Y

(nv)

Y

(nv)

Y

Y

Y

(nv)

(nv)

Y

Y

Y

Repeal ACA 2015
Keystone 2014
Iran Sanction
TPA
Cuba Open
Renew Patriot Act
Freedom Act 2015
TAA
Violence against Women
Garland
Education To States
Highway Funding
Medicare Reform
Raise Minimum Wage

Hou. Sen.
Y

Y

Y

Y

Y

Y

Y

N

Y

Y

N

N

Y

Y

Y

Y

Y

Y

Hou. Sen.
Y

Y

Y

Y

Y

Y

Y

(nv)

Y

Y

Y

Y

Y

Y

Y

Y

Y

Y

Y

N

N

(nv)

(nv)

Y

Y

Y

(nv)

Y

Y

Y

Y

(nv)

(nv)

N

Hou. Sen.
Y
Y
(nv)
Y
N
(nv)

(nv)
N
(nv)
Y
(nv)
(nv)

Y

Y

Y

Y

Y

Y
(nv)

Y

Y

Y

Y

Y
(nv)

Y
(nv)

Table A3 – Roll Call Votes and Passage Rates
Roll Call
Issue

Closeness

Date

Yeas

Nays

Miss

Pass By

Ban Partial Birth Abortion
Fund Stem Cell 2005
CAFTA
Cut Capital Gains Tax
Withdraw Iraq 2008

2003-06-04

282
238
217
244
171

139
194
215
185
255

13
2
2
4
7

66
22
1
28
-45

R
R
R
R
D

+23
+29
+29
+30
+31

Withdraw Iraq 2007
Foreclosure Assistance
SCHIP 2007
FISA
Ledbetter Fair Pay

2007-07-12

223
241
265
293
250

201
172
159
129
177

8
20
9
13
6

7
25
49
77
34

D
D
D
D
D

+30
+29
+31
+37
+77

SCHIP 2009
ARRA
ACESA
PPACA
Dodd Frank

2009-02-04

290
246
219
219
237

135
183
212
212
192

8
4
3
0
4

74
30
3
3
21

D
D
D
D
D

+77
+77
+78
+75
+77

End DADT
Simpson Bowles
Ryan Budget
Repeal ACA 2012
Violence Against Women

2010-12-15

250
38
228
244
286

175
382
191
185
138

9
11
12
2
7

34
-178
12
28
70

D
R
R
R
R

+76
+51
+51
+49
+31

Ban 20wk Abortion
Cut Food Stamps
Repeal ACA 2015
Medicare Reform
Freedom Act 2015

2013-06-18

229
216
239
392
338

196
208
186
37
88

9
11
8
4
6

13
0
23
176
122

R
R
R
R
R

+32
+33
+57
+57
+56

TAA
TPA
Education To States
Highway Funding
AHCA

2015-06-12

126
218
359
359
217

302
208
64
65
213

6
8
10
9
1

-90
2
143
143
1

R
R
R
R
R

+58
+58
+57
+57
+45

CHOICE
Withold Sanctuary Funding
Kates Law
Tax Cut Jobs Act
Budget Bipartisan

2017-06-08

2018-02-09

233
228
257
224
240

186
195
167
201
186

11
10
9
7
5

17
12
41
8
24

R
R
R
R
R

+44
+47
+47
+46
+45

Immigration Ryan

2018-06-27

121

301

6

-95

R +42

2005-05-24
2005-07-28
2006-05-10
2007-05-10

2007-08-04
2007-09-25
2008-06-20
2009-01-27

2009-02-13
2009-06-26
2010-03-21
2010-06-30

2012-03-28
2012-03-29
2012-07-11
2013-02-28

2013-07-11
2015-02-03
2015-03-26
2015-05-13

2015-06-18
2015-12-02
2015-12-03
2017-05-04

2017-06-29
2017-06-29
2017-12-20

Party

Close
X
X

X

X
X
X

X

X
X
X

X

X
X
X
X

Note: The “Pass By” column presents the number of Yea votes over the majority threshold.
Negative values indicates the bill failed. The party column shows the majority party
advantage by seats. Close votes are those between 45 to 55 percent support. Snyder and
Groseclose (2000) define a “very close vote” as a vote in a 40 to 60 percent.

Appendix Page 3

B

Additional Analyses

First Stage Table B1 lists out the proportion of correct responses for each issue asked.
Figure B1 panel (a) shows the first stage coefficients separated out by issue.
Second Stage Table B2 shows key coefficient estimates without hiding the estimates for
control variables. Table B3 displays OLS estimates for the second stage, overall and by
Congress. Figure B1 panel (b) shows the OLS and IV estimates separately by issue. Table
B4 models an interaction term as a third endogenous variable to the model. The substantive
finding is the same: the new interaction term is negative, but is an order of magnitude
smaller than the main effect.
Experiment Table B5 displays regression analysis in the n = 144 respondents in the 2018
experiment of Table 5 who had correct priors. We ask if whether the issue treatment being
correct or incorrect had differential effects on approval by including interactions with the
treatment and the proportion of correct (or incorrect) information. This is a small subgroup,
of course, so we are cautious in drawing strong conclusions from this finding.

Appendix Page 4

Table B1 – Uncertainty and Correctness of Beliefs
Perceived Vote (proportion)
Issue

Congress

CCES

Correct

Not
Sure

Incorrect Correct
(twoway)

n

Ban Partial Birth Abortion

108

2006

0.40

0.46

0.13

0.75

507

CAFTA
Cut Capital Gains Tax
Fund Stem Cell 2005

109
109
109

2006
2006
2006

0.33
0.51
0.48

0.47
0.37
0.33

0.20
0.12
0.19

0.63
0.81
0.72

503
504
506

Withdraw Iraq 2007
FISA
SCHIP 2007
Foreclosure Assistance
Withdraw Iraq 2008

110
110
110
110
110

2007
2007, 2008
2007, 2008
2008
2008

0.34
0.26
0.33
0.26
0.32

0.61
0.66
0.61
0.57
0.56

0.05
0.09
0.06
0.17
0.12

0.86
0.75
0.85
0.60
0.73

9,890
11,895
11,903
2,040
2,035

Ledbetter Fair Pay
SCHIP 2009
ACESA
ARRA
PPACA
Dodd Frank
End DADT

111
111
111
111
111
111
111

2009
2009
2009, 2010
2009, 2010
2009, 2010
2010
2010

0.35
0.39
0.39
0.46
0.51
0.37
0.25

0.58
0.51
0.50
0.46
0.42
0.57
0.66

0.07
0.10
0.11
0.08
0.07
0.06
0.08

0.82
0.80
0.79
0.85
0.87
0.85
0.76

5,124
5,120
8,015
8,038
8,021
2,863
2,860

Repeal ACA 2012
Ryan Budget
Simpson Bowles

112
112
112

2012
2012
2012

0.69
0.58
0.70

0.30
0.38
0.27

0.70
0.61
0.72

18,816
18,816
18,816

Ban 20 week Abortion
Violence Against Women
Cut Food Stamps

113
113
113

2013
2013
2014

0.52
0.55
0.38

0.25
0.20
0.32

0.23
0.24
0.30

0.69
0.69
0.56

1,497
1,493
1,488

TPA
Education To States
Freedom Act 2015
Highway Funding
Medicare Reform
Repeal ACA 2015
TAA

114
114
114
114
114
114
114

2015, 2016
2016
2016
2016
2016
2016
2016

0.29
0.41
0.45
0.47
0.41
0.57
0.34

0.40
0.30
0.22
0.31
0.25
0.20
0.20

0.31
0.29
0.33
0.22
0.34
0.24
0.46

0.48
0.58
0.58
0.68
0.55
0.71
0.42

2,468
1,445
1,466
2,941
1,472
2,960
1,464

CHOICE
Kate’s Law
AHCA
Withold Sanctuary Funding
Budget Bipartisan
Immigration Ryan
Tax Cut Jobs Act

115
115
115
115
115
115
115

2017
2017
2017, 2018
2017, 2018
2018
2018
2018

0.40
0.42
0.51
0.54
0.31
0.38
0.52

0.49
0.41
0.34
0.33
0.42
0.41
0.33

0.11
0.16
0.14
0.13
0.27
0.21
0.14

0.79
0.72
0.78
0.81
0.54
0.64
0.79

984
988
2,951
2,950
1,960
1,949
1,955

all years
all years

0.43
0.46
0.73

0.42
0.42
0.21

0.19
0.13
0.06

0.71
0.75
0.92

168,703
163,666

Average of Questions
Average of Observations
Party Affiliation

Note: Three columns show weighted proportion of a respondent’s correct answers, don’t
knows, and wrong answers. “two-way” correct is the proportion of correct perceptions
among correct and incorrect responses. Issues are sorted by year.

Appendix Page 5

Table B2 – All Main Regressions with Coefficients on Control Variables
Outcome: Approval
(1)
OLS

(2)
IV

Perceived Issue Agreement

0.39
(0.006)

Perceived Party Agreement

0.26
(0.006)

(3)
RF

Outcome: Vote Choice
(4)
OLS

(5)
IV

0.66
(0.02)

0.22
(0.01)

0.59
(0.03)

0.22
(0.009)

0.29
(0.01)

0.44
(0.02)

(6)
RF

Actual Issue Agreement

0.23
(0.006)

0.20
(0.010)

Actual Party Agreement

0.21
(0.006)

0.35
(0.01)

Actual Ideological Agreement

0.39
(0.010)

0.24
(0.01)

0.48
(0.01)

0.64
(0.02)

0.27
(0.02)

0.54
(0.02)

Perceived ideological distance from challenger

0.15
(0.009)

0.10
(0.009)

0.20
(0.010)

0.41
(0.02)

0.27
(0.02)

0.39
(0.02)

Ideology (-1 to 1)

0.03
(0.007)

0.04
(0.006)

0.02
(0.009)

0.06
(0.02)

0.06
(0.01)

0.04
(0.01)

Ideological Moderate

-0.04
(0.007)

-0.02
(0.007)

-0.04
(0.007)

-0.01
(0.01)

0.03
(0.01)

0.02
(0.01)

Respondent identifies as Independent

-0.05
(0.006)

-0.04
(0.006)

-0.07
(0.007)

0.02
(0.01)

0.06
(0.01)

0.03
(0.010)

Perceived party of Rep is correct

0.00
(0.006)

0.00
(0.006)

-0.01
(0.006)

0.02
(0.01)

0.01
(0.01)

0.03
(0.009)

Average of Outcome
Std. Dev. of Outcome
R-squared
Clusters
Observations

0.07
0.71
0.47
849
42,516

0.07
0.71
0.41
847
42,417

0.07
0.71
0.39
847
42,559

0.23
0.84
0.46
787
24,032

0.23
0.85
0.36
786
23,949

0.23
0.84
0.48
786
24,051

Clustered Standard Errors by Representative.
Note: Reproduces key regression tables but displays estimates for control variables. All
models include fixed effects for representative (not shown) and indicators for the
missingness of ideological distance, which is itself set to 0 if missing. OLS: Ordinary Least
Squares, IV: Instrumental Variables, RF: Reduced form models.

Appendix Page 6

Table B3 – Second Stage Estimates Using OLS
Outcome: Approval
All Years

109th
(2006)

110th
(2007-08)

111th
(2009-10)

112th
(2011-12)

113th
(2013-14)

114th
(2015-16)

115th
(2017-18)

Perceived Issue Agreement

0.38
(0.006)

0.23
(0.1)

0.51
(0.01)

0.63
(0.01)

0.36
(0.009)

0.12
(0.02)

0.21
(0.02)

0.46
(0.03)

Perceived Party Agreement

0.26
(0.006)

0.34
(0.1)

0.18
(0.01)

0.14
(0.01)

0.29
(0.008)

0.31
(0.02)

0.32
(0.02)

0.31
(0.02)

Average of Outcome
Std. Dev. of Outcome
R-squared
Clusters
Observations

0.07
0.71
0.47
849
42,516

0.11
0.70
0.81
271
448

0.04
0.73
0.45
441
9,555

0.05
0.68
0.52
438
7,211

0.10
0.74
0.55
398
16,477

0.05
0.70
0.47
430
2,327

0.05
0.61
0.39
434
3,908

0.06
0.69
0.56
435
2,590

Outcome: Vote Choice
All Even
Years

109th
(2006)

110th
(2008)

111th
(2010)

112th
(2012)

113th
(2014)

114th
(2016)

115th
(2018)

Perceived Issue Agreement

0.22
(0.01)

0.16
(0.2)

0.57
(0.05)

0.52
(0.04)

0.20
(0.01)

-0.00
(0.03)

0.30
(0.03)

0.38
(0.05)

Perceived Party Agreement

0.29
(0.01)

0.44
(0.2)

0.27
(0.04)

0.26
(0.03)

0.26
(0.01)

0.47
(0.04)

0.35
(0.03)

0.34
(0.04)

Average of Outcome
Std. Dev. of Outcome
R-squared
Clusters
Observations

0.23
0.84
0.46
787
24,032

0.22
0.81
0.84
246
397

0.24
0.83
0.64
332
1,405

0.19
0.85
0.70
389
2,163

0.25
0.86
0.44
391
14,806

0.23
0.78
0.63
360
1,168

0.21
0.80
0.50
394
2,538

0.19
0.85
0.63
368
1,555

Note: Each column is an OLS (instead of IV) regression, with control variables not shown. Clustered
standard errors by Representative in parentheses.

Appendix Page 7

Table B4 – Interaction between Issues and Party Agreement
Outcome: Approval

Outcome: Vote Choice

(1)
OLS

(2)
IV

(3)
OLS

(4)
IV

Perceived Issue Agreement (0 - 1)

0.43
(0.01)

0.73
(0.03)

0.29
(0.02)

0.57
(0.07)

Perceived Party Agreement (0 - 1)

0.30
(0.01)

0.31
(0.03)

0.36
(0.02)

0.42
(0.06)

Interaction

-0.07
(0.02)

-0.17
(0.06)

-0.12
(0.03)

0.03
(0.11)

Average of Outcome
Clusters
Observations

0.53
839
42,254

0.53
837
42,156

0.62
778
23,817

0.62
777
23,735

Note: Here both predictors and outcome variables are re-scaled to range from 0 to 1, so
that the interaction of two disagreements do not get a positive number.

Table B5 – Subgroup Analysis of 2018 Experiment
Approval (-1 to 1)
Correct vote info treatment (in agreement)

-0.18
(0.12)

Incorrect vote info treatment (in agreement)

0.35
(0.13)

Average Outcome in Control
R-squared
Observations

-0.16
0.76
143

Note: See Table 5 for description on layout and the control variables not shown.

Appendix Page 8

Immigration Ryan (2018)

Immigration Ryan (2018)
Budget Bipartisan (2018)
Tax Cut Jobs Act (2018)

Withold Sanctuary Funding (2018)
AHCA (2018)
Kate's Law (2017)

Withold Sanctuary Funding (2017)
CHOICE (2017)
AHCA (2017)
Highway Funding (2016)
Education To States (2016)
TPA (2016)
TAA (2016)
Freedom Act 2015 (2016)
Medicare Reform (2016)
Repeal ACA 2015 (2016)
TPA (2015)
Cut Food Stamps (2014)
Ban 20 week Abortion (2013)

Violence Against Women (2013)
Repeal ACA 2012 (2012)
Ryan Budget (2012)
Simpson Bowles (2012)
End DADT (2010)
Dodd Frank (2010)
PPACA (2010)
ACESA (2010)
ARRA (2010)
PPACA (2009)
ACESA (2009)
ARRA (2009)
SCHIP 2009 (2009)
Ledbetter Fair Pay (2009)
FISA (2008)
SCHIP 2007 (2008)
Foreclosure Assistance (2008)
Withdraw Iraq 2008 (2008)
FISA (2007)
SCHIP 2007 (2007)
Withdraw Iraq 2007 (2007)
Cut Capital Gains Tax (2006)
CAFTA (2006)
Fund Stem Cell 2005 (2006)

Note: Model follows equations 2 and 3 in the main text but with modeling each issue
separately. Intervals are 95 percent confidence intervals.

Budget Bipartisan (2018)
Tax Cut Jobs Act (2018)
Withold Sanctuary Funding (2018)
AHCA (2018)
Kate's Law (2017)
Withold Sanctuary Funding (2017)
CHOICE (2017)
AHCA (2017)
Highway Funding (2016)
Education To States (2016)
TPA (2016)
TAA (2016)
Freedom Act 2015 (2016)
Medicare Reform (2016)
Repeal ACA 2015 (2016)
TPA (2015)
Cut Food Stamps (2014)
Ban 20 week Abortion (2013)
Violence Against Women (2013)
Repeal ACA 2012 (2012)
Ryan Budget (2012)
Simpson Bowles (2012)
End DADT (2010)
Dodd Frank (2010)
PPACA (2010)
ACESA (2010)
ARRA (2010)
PPACA (2009)
ACESA (2009)
ARRA (2009)
SCHIP 2009 (2009)
Ledbetter Fair Pay (2009)
FISA (2008)
SCHIP 2007 (2008)
Foreclosure Assistance (2008)
Withdraw Iraq 2008 (2008)
FISA (2007)
SCHIP 2007 (2007)
Withdraw Iraq 2007 (2007)
Cut Capital Gains Tax (2006)
CAFTA (2006)
Fund Stem Cell 2005 (2006)

Appendix Page 9

OLS

Instrumental Variables

Model

−0.75

−0.75

Ban Partial Birth Abortion (2006)

Issue (ordered by date of vote)

−0.50
−0.50

−1.00

−1.00

−0.25
−0.25

1.00
115th Congress
(Unified Republican)
111th Congress
(Unified Democrat)
1.00

First Stage of Party Agreement
First Stage of Issues Agreement

Model

−0.75
−0.75

−1.00
−1.00

−0.50

0.75

0.75

0.50

0.50

0.25

0.25

0.00

0.00

First Stage Effect
(Outcome: Perceived Issue Agreement)

−0.50

1.00
115th Congress
(Unified Republican)
111th Congress
(Unified Democrat)
1.00

−0.25
−0.25

Ban Partial Birth Abortion (2006)

0.75

0.75

0.50

0.50

0.25

0.25

0.00

0.00

Estimated Effect on Approval (−1 to 1)

Figure B1 – Estimates by Issue

(a) First Stage Estimates

Issue (ordered by date of vote)

(b) Second Stage Estimates

C

Predictors of Correct Perceptions

This section further examines the first stage estimates. We first describe the predictors of
correct predictions. Separately, we examine data on timings to ask if respondents are looking
up the answers to perception questions.
C.1

Sociodemographic Predictors

We model perception formation using a two-step Heckman selection model, which corrects
for potential censoring biases due to respondents who do not hazard a guess. Formally, let
Di be an indicator for respondent i providing a yes or no response to the perceived rollcall
question, and let Ci be a continuous variable that indicates the degree of correct perceptions
the respondent has. Ci can only be observed for those with Di = 1. If we let Ci∗ be the
potentially unobserved value for all respondents, then the Heckman correction will estimate
the population relationship E(Ci∗ | X) = Xβ +  from the observed data and a selection
model predicting Di = 1. In our setup, the first step differentiates between people who
express some belief on some roll call vote and people who said “Don’t Know” on every roll
call vote. The second step is whether people who express a belief hold correct ones.
Communication theory suggests three sets of predictor variables: signals, receptivity,
and filters. First, the nature of the signaler likely matters. Extremism sends clearer signals
than does moderation. We include the absolute value of the representatives’ NOMINATE
score within party to measure extremity versus moderation. We also include their years
serving in Congress, because legislators might garner more understanding through their
tenure. Second, receptivity to information should lead to both having a perception and
reaching a correct one. We proxy this by demographic measures ranging from education to
news interest. Finally, partisan and ideological information biases might filter perceptions.
We include indicators of party identification, left-right ideological orientation, self-identified
Independents, self-identified moderates, and actual party agreement.
Figure C1 plots estimated marginal effects from each stage of the Heckman model. The
first panel shows the average marginal effects from a logit model predicting Di . The second
panel displays the effect of the same independent variables in explaining Ci , conditional on
making some guess. Standard errors are clustered at the representative level.
By far the most important factors explaining perception formation are the ideological
orientation of the representative (the nature of the signaler) and the individual’s attentiveness
and resources (receptivity to information), such as news interest and income. The relative
contribution of these factors are similar between the first and second stage.

Appendix Page 10

Figure C1 – Signal Strength and Receptivity Predict Correct Perceptions
Outcome: Having a Belief
(Selection Model)

Outcome: Correct Belief
(Outcome Model)

Follow the News (0 to 1)
Education (0 to 1)
Income (0 to 1)
Age (in decades)
Married
Woman
Democratic Rep's absolute NOMINATE
Republican Rep's absolute NOMINATE
Tenure of representative (in decades)
Respondent is Democrat
Respondent is Republican
Ideological Moderate
Ideology (−1 to 1)
Actual Party Agreement
0.0

0.2

0.4

0.6

0.0

0.2

0.4

0.6

Marginal Effect Estimate
Note: Bars show 95 and 99 percent confidence intervals. NOMINATE scores are flipped so
positive values of the predictor indicate more extreme voting patterns (within party).

C.2

Time Spent on Perception Questions

As an indirect test of whether respondents are looking up the correct answers to questions
about their representative’s roll call votes, we investigate the time a respondent takes to
answer a perception question. YouGov, which runs the CCES, collects information on how
many seconds a respondent took at the page-level.
One page of a perception question typically contains a one-sentence description of the
bill, and three questions for each respondent’s House Representative and two Senators, with
a follow-up for people who responded “Not Sure.” We designed the module anticipating
that reading the question and the bill description will take 10-15 seconds, and answering the
questions will take 5 seconds each.
Table C1 provides summary statistics of the range of seconds respondents took to answer
the perception questions. We take the 2016 CCES module where seven issue perception
questions were asked in succession. Each row of the table in panel (a) shows these issues in
the order they appeared. We see that the median respondent takes 10-25 seconds to complete
a page. As the 90th percentile shows, only 10 percent of respondents take around 50 seconds
to complete a issue question. We compare this with timings of non-political questions in the
same module, shown in panel (b). These questions asked about respondent’s preferences on
food and diet, and are a useful comparison because there would be no incentive to look up
the “correct” answer for such non-political questions. We see that the range of timings are

Appendix Page 11

roughly similar, suggesting little cheating in the perception questions.
Table C1 – Descriptive Statistics of Time Spent on Answering Questions
(a) Percentiles of Seconds Spent on Issue Perception Questions

TPP Trade Deal (82 words)
Education Reform (68 words)
Highway Funding (58 words)
Medicare Reform (66 words)
Freedom Act (64 words)
TAA Assistance (63 words)
Repeal Obamacare (50 words)

10th
(secs.)

25th
(secs.)

50th
(secs.)

75th
(secs.)

90th
(secs.)

13
10
6
6
7
6
5

17
15
9
9
10
9
8

25
21
12
14
17
13
11

37
32
19
22
29
20
16

56
52
28
37
46
31
23

(b) Percentiles of Seconds Spent on Non-Political Questions

Magazine Readership (18 words)
Canned Tomatoes (63 words)
Salad Dressing (60 words)

10th
(secs.)

25th
(secs.)

50th
(secs.)

75th
(secs.)

90th
(secs.)

6
9
7

8
15
11

11
21
16

16
29
23

22
40
32

Note: Sample taken from one of the CCES modules in 2016, n = 1,500. Perceived vote
questions ask the perceptions for three offices, while we use only the responses for the US
House.

But the distribution of time spent cannot ascertain whether people who take more time
than others are looking up the answers online, genuinely debating the answer, or are simply
distracted by their daily lives. If they are indeed taking time to look up the answers, longer
times should correlate with more correct answers. We test this possibility by estimating the
linear probability model of the form:
Cij = γ0 + γ1 sij + Issue FEj + Respondent FEi + εij
where Cij , following the previous section, is 1 if respondent i answers correctly for issue
question j, and 0 otherwise. Our coefficient of interest is γ1 on the variable sij , or the
seconds it took respondent i to answer question j. We can also include issue fixed effects
(FE) to account for the variation in length and difficulty of each of the seven questions, as
well as respondent fixed effects to account for the within-respondent tendency to take time.
Estimates suggest that to the extent that respondents take more time to answer a question, they are less certain and less correct in their answers — the opposite of what we would
expect if they had been using that extra time to look up the correct answers. Table C2
columns (1) - (3) shows the estimates of γ1 across specifications. The coefficients are substantially small and, if anything, negative. One extra second spent on answering a question
Appendix Page 12

is associated with a 0.2 - 0.3 percentage points decrease in the probability of getting that
answer correct. In columns (4) - (6), we replace the outcome with an indicator for being
Not Sure on the same question. An extra second spent on a question is associated with a
0.3 - 0.4 percentage point increase in being unsure. We do not necessarily interpret these
coefficients causally: a more plausible interpretation is that the types of questions that take
any given respondent more time are genuinely those where the respondent is not sure, does
not look up the answers, and therefore less likely to be correct.
Table C2 – Time Spent on Perception Questions and Correct Answers
Outcome: Correct Perception
(1)
Seconds Spent on Page
Respondent-Issue Pairs
Issue Fixed Effects
Respondent Fixed Effects

(2)

(3)

-0.0030 -0.0020 -0.0024
(0.0003) (0.0003) (0.0004)
9,996

9,996
X

9,996
X
X

Outcome: Not Sure
(4)

(5)

(6)

0.0039
0.0033
0.0037
(0.0004) (0.0004) (0.0004)
9,996

9,996
X

9,996
X
X

Note: Each column is a regression, with clustered standard errors by respondent in
parentheses. Sample taken from one of the CCES modules in 2016, n = 1,500, where each
respondent answered perception questions on seven bills. Observations of less than 2
seconds or more than 200 seconds are excluded as outliers.

Appendix Page 13

D

Heterogeneity Analyses

Here we examine if our key IV estimates change systematically by four types of important
covariates: voter’s political interest (Section D.1), issue salience (Section D.2), pivotality of
the representative’s vote (Section D.3), and member’s abstention (Section D.4).
D.1

Heterogeneity by Political Interest

Our findings may not be generalizable to the public if the CCES sample is disproportionately
knowledgeable about politics. In this appendix section, we find that it is unlikely that the
choice of our survey sample overestimates the size of accountability we report.
Online surveys like the CCES tends to be attract more political knowledgeable respondents compared to other modes, but not by much. First, while becoming a panelist is opt-in,
answering the CCES is not, because the survey firm uses a matched sampling frame and
draws panelists to form a representative sample in terms of demographics and political interest. A 2014 study2 addressed this question by comparing surveys of the same questions on
three different modes: mail, phone, and a CCES module. The online sample was about 10
percentage points more likely to correctly know the unemployment rate, and party control
of the House. Among internet users, the difference in these political knowledge questions
was about 5 percentage points but difficult to distinguish from zero.
The question of generalizability hinges not only on whether the CCES sample is more
knowledgeable than the general population, but also to the extent to which political knowledge affects our outcome of interest. Published analysis of other outcome variables of the
CCES do not indicate that differences in the demographic composition is consequential.3 We
approximate tests like this with the outcome in our main findings below.
Table D1 shows our key coefficients estimated by instrumental variables, but running a
separate regression for each group of news interest: we classify “Not Sure”, “From Time to
Time”, and “Hardly at all” as Low interest, “Some of the Time” as Some, and “Most of the
Time” as High.4 If our findings were driven by high-interest constituents, we would only find
effects among the last group.
We find that although the effects of accountability are sometimes higher among high
2

3

4

Ansolabehere, Stephen, and Brian F. Schaffner (2014). “Does Survey Mode Still Matter? Findings from a
2010 Multi-mode Comparison.” Political Analysis, 22(3), 285–303.
Ansolabehere and Schaffner (2014) cited above demonstrates this. For an example using vote outcomes,
see Ansolabehere, Stephen, and Douglas Rivers (2013). “Cooperative Survey Research.” Annual Review
of Political Science, 16(1), 307–329.
The prompt was “Some people seem to follow what’s going on in government and public affairs most of
the time, whether there’s an election going on or not. Others aren’t that interested. Would you say you
follow what’s going on in government and public affairs ...”

Appendix Page 14

interest constituents, the pattern is not consistent and constituents with low news interest
still exhibit sizable effect sizes. The estimated effect of (instrumented) perceived issue agreement on vote is nearly 0.60 among high interest voters and 0.40 among low interest voters.
Yet constituents with some news interest have the largest coefficient. When the outcome
is approval, the pattern is if anything reversed, where low interest voters exhibit higher effects. Perceived party agreement (instrumented) exerts the largest effect among low interest
constituents, and the effect sizes decreases by news interest. In contrast, actual ideological
agreement has the largest effect among high interest constituents.
Table D1 – Issue and Party Accountability by News Interest
Outcome: Approval

Outcome: Vote Choice

(1)
Low

(2)
Some

(3)
High

(4)
Low

(5)
Some

(6)
High

Perceived Issue Agreement

0.72
(0.19)

0.59
(0.06)

0.64
(0.02)

0.50
(0.29)

0.78
(0.14)

0.58
(0.03)

Perceived Party Agreement

0.30
(0.05)

0.27
(0.02)

0.19
(0.01)

0.96
(0.09)

0.61
(0.05)

0.29
(0.02)

Actual Ideological Agreement

0.03
(0.04)

0.15
(0.03)

0.32
(0.01)

0.08
(0.06)

0.16
(0.05)

0.39
(0.03)

Average of Outcome
Clusters
Observations

0.06
750
4,885

0.09
804
9,136

0.06
821
28,135

0.16
633
3,305

0.23
689
5,071

0.25
750
15,359

Note: Each column is a regression and standard errors clustered by representative are in
parentheses. News interest is a 5-point ordinal question asked 2007 onwards. Estimates are
IV regressions detailed in equation 3 where perceived issue and party agreement
instrumented by actual issue and party agreement.

D.2

Heterogeneity by Salience

Another source of obvious heterogeneity suggested by the literature is issue salience. Salience
can mean at least four different things. We consider each in turn, though we leave more
detailed analysis for future work.
One definition of salience is informational (or cognitive, as opposed to value-based) and
internal to the public (as opposed to defined by the media or politicians). This internal
information salience is measured by our first stage estimates in Figure B1 panel (a). We
see that in President Obama’s second term, roll call votes have small or even negligible first
stages, and there are not many roll call votes that made it to a floor vote to begin with.
A weak first stage therefore has a substantive interpretation. It means that the degree to
which the issue and polices are understood by the voter is low.
Appendix Page 15

A weak first stage statistically leads to a noisy and unreliable instrumental variables
estimate, as confirmed in the outsized confidence intervals of Figure B1 panel (b). This
estimate on Perceived Issue Agreement in equation (3) can be interpreted as a measure of
internal value salience. If voters cared a lot about the issue, they would be more likely to
change their approval of their legislator because of it. Figure B1 suggests that issues with
low internal value salience were concentrated in President Obama’s second term.
We can also measure salience externally, defined as the degree to which the media or
politicians talk about the issue. We examined Pew’s measure of what the public perceived
as the “Most Important Priority” (MIP) in each year. We take the top 5 issues in each
year and code our roll call vote “salient” (as an external matter) if it squarely falls into that
category.5
This distinction, which we can call salience measured by external information salience,
turns out not to matter on average. The t-test between the issue-specific IV coefficients for
year-specific MIP issues gives a difference of t = 1.32 (p = 0.20). We also consider health
care bills, as health care is arguably the defining issue of the decade — an issue where parties
took highly contrasting positions, affected everyone, and was the focus of many Congresses.
However, the t-test of coefficients with healthcare vs. non-healthcare issues is also not
significant, t = 0.39 (p = 0.70).
Yet another definition of salience is whatever the Congress and party leaders find important enough to put on the agenda. By this definition, all our issues are salient because
they were chosen from a list of key votes in each year. This is a measure of external value
salience.
D.3

Heterogeneity by Close Votes, Marginal Districts

Constituents may hold individual legislators accountable only when their rollcall vote was
pivotal, although in the main text we assume otherwise for simplicity. Here we examine
whether results by the closeness of the vote and the marginality of the member. We define
a marginal member as a member within ±20 members from the median all in terms of
NOMINATE at the time of each vote, i.e. the 45-55 percentile. And we call a vote as close
if its final voteshare is also 45-55 percent. In other words, we use the ex ante marginality
of the member and the ex post closeness of the final rollcall vote. This operationalization
can be scrutinized further — in some sense, no single member is pivotal in a rollcall that is
5

One problem with this classification is that Pew or Gallup’s categories are likely too broad to capture
salience of the types of particular issues we measure. For example, the bill on withholding funding to
sanctuary cities (2018) has one of the highest first stage effects in our data as well as a high IV estimate,
but most news organizations did not ask about this in their MIP measure. Dodd-Frank (2010) is another
that does not easily fall into one of the provided categories.

Appendix Page 16

Table D2 – Effects in Close Votes and Marginal Districts
Outcome: Approval
(1)
All

(2)
Lopsided Vote
Safe MC

(3)
Lopsided Vote
Marginal MC

(4)
Close Vote
Safe MC

(5)
Close Vote
Marginal MC

Perceived Issue Agreement

0.35
(0.008)

0.33
(0.009)

0.31
(0.03)

0.40
(0.01)

0.38
(0.03)

Perceived Party Agreement

0.46
(0.008)

0.47
(0.008)

0.48
(0.03)

0.44
(0.009)

0.42
(0.03)

Average of Outcome
Clusters
Observations

0.07
847
143,391

0.07
835
77,416

0.07
387
8,453

0.06
831
50,636

0.06
354
5,592

Outcome: Approval
(1)
All

(2)
Lopsided Vote
Safe MC

(3)
Lopsided Vote
Marginal MC

(4)
Close Vote
Safe MC

(5)
Close Vote
Marginal MC

Perceived Issue Agreement

0.30
(0.01)

0.26
(0.02)

0.23
(0.05)

0.38
(0.02)

0.38
(0.07)

Perceived Party Agreement

0.71
(0.01)

0.71
(0.02)

0.74
(0.05)

0.70
(0.02)

0.72
(0.06)

Average of Outcome
Clusters
Observations

0.23
788
79,377

0.24
770
43,087

0.23
324
4,545

0.22
752
27,969

0.24
300
3,105

Note: Clustered standard errors by representative. Columns (2) - (5) are subsets of the
overall group shown in column (1).

decided by more than one vote — but this follows conventions in past empirical literature.
To allow the results to change by issue, we take a different form of the data where each
observation is a person-issue combination. We then run the IV regression for each of the
2 by 2 combination. Table D2 shows the results. Coefficients do not appear to change by
the marginality of the member, but they are perhaps somewhat larger in close votes than
in lopsided vote. The first result is somewhat difficult to interpret. We may be finding no
difference either because the constituent does not know or care about the MC’s pivotality
in the vote, or it may be that the member in the median 10 percent of the NOMINATE
distribution may actually not have been pivotal in some votes. The second comparison
suggests that constituents perhaps exert more control in close votes. However, it may be
that close votes are inherently controversial and lopsided ones are, naturally, consensual for
the constituents.

Appendix Page 17

D.4

Heterogeneity by Abstentions in Roll Call Votes

Given our results, one might suspect that members have an incentive to obscure their position
when they want to vote on clearly unpopular issues. The final heterogeneity analysis we
conduct is whether missed or abstained votes lead to different constituent reactions. Here
we again use person-issue level data to account for the fact that the representative will take
different actions on different votes in the same year. Then we estimate separate regressions
for abstained and taken votes. The overall rate of abstention can be seen in Table A3.
In our IV regressions, we use the actual votes as an instrument. But for abstained
votes, the actual issue agreement is 0, so there is no variation in that instrument to use for
estimating a coefficient. Therefore, we can only use the OLS here. The coefficient estimates
may be biased but can be compared with the full regressions in Table B3.
Table D3 shows the results for Approval and OLS. The coefficients on issues are similar
and so there are no noticeable differences. We again add the caveat that these are only OLS
estimates.
Table D3 – Heterogeneity between Taken and Abstained Votes
Outcome: Approval

Outcome: Vote Choice

(1)
Taken Votes

(2)
Abstention

(3)
Taken Votes

(4)
Abstention

Perceived Issue Agreement

0.21
(0.003)

0.20
(0.022)

0.14
(0.005)

0.10
(0.039)

Perceived Party Agreement

0.43
(0.006)

0.43
(0.031)

0.56
(0.010)

0.47
(0.094)

Average of Outcome
Clusters
Observations

0.07
847
141,082

0.03
154
2,309

0.23
789
79,897

0.29
105
1,094

Note: OLS estimates similar to Table B3 but with subgroups. Clustered standard errors in
parentheses.

Appendix Page 18

E

Sensitivity Analysis

Most of our observational analyses rest on selection-on-observable assumptions for coefficient
estimates to be unbiased. Therefore we perform a series of sensitivity analyses that ask
whether our results are sensitive to unobserved and unmeasured confounders and the omitted
variable bias they induce. It is well-known that variables that are both correlated with the
outcome and correlated with the treatment variable can induce omitted variable bias. Cinelli
and Hazlett (2020) outline a method that parameterizes the magnitude of the bias by the
partial R2 of the two relationships and generates benchmarks of the size of the bias. Their
method extends classic sensitivity analyses but in a more readily interpretable way, because
the partial R2 is invariant to the unit of measurement and more easily interpretable.
Figure E1 shows the result of their method for different regressions. All regressions
used the variable for Actual Issue Agreement as the main treatment variable of interest.
Contour lines show the value of the coefficient on that variable in different settings for
various degrees of confounding of an unobserved Actual Issue Agreement (the horizontal
axis) or the regression outcome (the vertical axis). The top two graphs show the reduced
form regression on the outcome (equation 1), using approval (left) and vote choice (right).
The bottom graph show the first stage regression on issues (equation 2), where the outcome
is Perceived Issue Agreement. We benchmark the possible level of unobserved confounding
by an observed variable we know to be strongly of the outcome – Actual Party Agreement
(denoted “Copartisan” in the graphs). Each red point shows the estimated effect of Actual
Issue Agreement on the outcome if the unobserved confounder was as strong (1×) or twice
as (2×) strong than Actual Party Agreement, or co-partisanship. The triangles show the
reported value from the regular regressions where there are implicitly assumed to be no
unobserved confounders.
We see in the Figure that the positive contribution of issue agreement is largely robust to
our main outcome variables. The fact that each “1 × Copartisan” point lies on a contour line
that is positive indicates that even if the true relationship between actual issue agreement
were confounded by an unobserved that is as strong as the contribution of co-partisanship,
the coefficient would still be positive. The points “2 × Copartisan” shows the same pattern,
with the reduced form on vote choice being an exception. However, it is difficult to think
of any confounder not already in the regression that is twice as strong copartisanship in
shaping a respondent’s voting behavior in the U.S. House.

Appendix Page 19

Figure E1 – Sensitivity of Treatment Effects to Unobserved Confounding

0.2

0.0

0.1

0.2

0.3

−0

0.3
0.2

1x Copartisan
(0.097)

−0

0.1

.1

−0.

05

0

0.4

0.05
0.1

Unadjusted
(0.24)

0.15

0.0

Partial R2 of confounder(s) with the treatment

0.1

0.2

0.3

Partial R2 of confounder(s) with the treatment

0.3

.1
−0

0.2

5

.0
−0
0

0.1

5x Copartisan
0.0
(0.1)
5

0.0

Partial R2 of confounder(s) with the outcome

.15

−0

0.4

First Stage Regression

0.1

0.15

1x Copartisan
Unadjusted (0.302)
(0.34)
0.3

0.0

0.1

0.2

.2

−0

.15

0.0

Partial R2 of confounder(s) with the outcome

0.3
0.2
0.1
0.0

Partial R2 of confounder(s) with the outcome

0.1
0.15

5

Unadjusted
(0.24)

.2

0.05

1.5x Copartisan
(0.018)

−0

5

0

.3

.1

−0
.05

−0

.2
−0

−0

2x Copartisan
(−0.065)

.35
−0

.25

−0

−0
.1

2x Copartisan
(0.075)
1.5x Copartisan
(0.121)
1x Copartisan
(0.164)

0.4

Outcome Regression (Vote Choice)

0.4

Outcome Regression (Approval)

0.3

0.2
0.25

0.4

Partial R2 of confounder(s) with the treatment

Note: Each plot shows the sensitivity of our main treatment variable — issue agreement —
for some of the main regressions in this paper. Each plot shows the degree of confounding
between the unobserved confounder(s) and our treatment variable in the x-axis and the
degree of confounding between the unobserved confounder(s) and our outcome variable in
the y-axis; the values of the contour indicate the coefficient on our treatment variable in
given the degree of confounding. See text for details.

Appendix Page 20

0.2

0.4

