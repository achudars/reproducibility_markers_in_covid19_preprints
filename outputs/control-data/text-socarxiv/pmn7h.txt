Partisan Gaps in Political Information and
Information-Seeking Behavior: Motivated Reasoning or
Cheerleading?
Erik Peterson
Texas A&M University
Department of Political Science
4348 TAMU
College Station, TX 77843
erik.peterson@tamu.edu

Shanto Iyengar
Stanford University
Department of Political Science
Encina Hall West, Suite 100
Stanford, CA 94305
siyengar@stanford.edu ⇤

Forthcoming at the American Journal of Political Science

Abstract

Do partisan disagreements over politically-relevant facts, and preferences for the
information sources from which to obtain them, represent genuine di↵erences
of opinion or insincere cheerleading? The answer to this question is crucial for
understanding the scope of partisan polarization. We test between these alternatives with experiments that o↵er incentives for correct survey responses and
allow respondents to search for information before answering each question. We
find that partisan cheerleading inflates divides in factual information, but only
modestly. Incentives have no impact on partisan divides in information search;
these divides are no di↵erent from those that occur outside the survey context
when we examine web browsing data from the same respondents. Overall, our
findings support the motivated reasoning interpretation of misinformation; partisans seek out information with congenial slant and sincerely adopt inaccurate
beliefs that cast their party in a favorable light.

⇤

We acknowledge with gratitude financial support provided by the Hewlett Foundation
and a Facebook Integrity Foundational Research Project award. We also thank John Bullock,
Jamie Druckman, Jennifer Jerit and Yusaku Horiuchi for their comments.

Over the past three decades, heightened polarization has made partisanship increasingly
influential in political life. Rank and file partisans now hold such harsh views of their opponents (Iyengar et al., 2012; Mason, 2018) that they avoid social interactions with out-party
members (Huber and Malhotra, 2017; Iyengar et al., 2018). Partisan cues from polarized
elites powerfully influence the candidate evaluations and policy views of their supporters
(Levendusky, 2010; Druckman et al., 2013).
At the frontier of these developments is the intrusion of partisanship into the public’s
factual beliefs (for reviews, see Jerit and Zhao, ND; Bullock and Lenz, 2019; Flynn et al.,
2017; Shapiro and Block-Elkon, 2008). A number of studies find that partisans disagree
about politically-relevant facts on topics ranging from the performance of the economy (e.g.,
Bartels, 2002; Scha↵ner and Roche, 2017; Bisgaard and Slothuus, 2018) to the details of
health care policy (e.g., Berinsky, 2017; Nyhan, 2010). These divides raise the possibility that
misinformation undermines voters’ ability to hold elected officials accountable (Kuklinski et
al., 2000; Shapiro and Block-Elkon, 2008; Hochshild and Einstein, 2015).
We assess two competing interpretations of polarized responses to information questions.
In one, partisan divides reflect genuine di↵erences in factual beliefs. In this motivated reasoning account, polarization has strengthened partisans’ willingness to ignore or counter-argue
with credible facts that cast aspersions on their party or party leaders and, conversely, to
uncritically accept false information that reflects favorably on their side (Gaines et al., 2007;
Shapiro and Block-Elkon, 2008; Kraft et al., 2015).
A contrasting interpretation is that these divides reflect insincere partisan cheerleading
in which partisans knowingly distort their responses to survey questions to signal support for
their side (Prior et al., 2015; Bullock et al., 2015; Scha↵ner and Luks, 2018). In this view,
partisans are in fact well informed, but prefer to act misinformed in surveys. In essence,
the cheerleading account implies that the partisan divides over factual evidence measured in
surveys are largely illusory.
1

We use survey experiments that incentivize respondents to correctly answer factual knowledge questions to implement new tests for distinguishing between these competing accounts
(for previous applications of this approach see Bullock et al., 2015; Prior et al., 2015). To
understand the nature of partisan divides over politically-relevant facts, and preferences
for the information sources from which to obtain them, we introduce three changes to the
incentive-based experimental design used in earlier work.
First, while previous studies incentivize partisans to correctly answer questions about
past political controversies (Bullock and Lenz, 2019; 339), we assess their responsiveness to
incentives on highly salient controversies ongoing at the time we fielded our surveys. Second,
we broaden the range of outcome variables to include partisan divides in information search.
Before answering each information question, respondents selected one news report to read
from a set of two partisan news sources, two non-partisan news sources, and an expert source.
Information seeking behavior is no less subject to a cheerleading interpretation since, in the
context of the typical unincentivized survey, partisans may support their party by expressing
an affinity for co-partisan news sources they do not ordinarily use (see e.g., Prior, 2013;
114-115). Third, we validate our survey results by using behavioral web browsing data to
compare the information search preferences of respondents in the survey to their real-world
news consumption outside it.
Overall, our findings support motivated reasoning rather than the cheerleading interpretation of partisan divides in information and information seeking behavior. First, while
incentives reduce partisan divides in information, substantial partisan gaps – on the order
of 60 to 70 percent of the initial unincentivized divide – persist even when incentives are
present (see also Berinsky, 2018). Second, the provision of incentives has no bearing on
information selection. Partisans seek out information from co-partisan media sources to the
same extent whether incentives are available or not. Moreover, the divides in information
search we observe in the survey context mirror partisan divides in online news consumption
2

registered by the same individuals outside the survey setting.
We next review previous studies of political misinformation and consider the debate over
the appropriate interpretation of partisan divides over political facts. We then introduce and
implement a research design that combines survey experiments regarding political misinformation with behavioral web-browsing data to consider partisan divides in information and
information-seeking behavior.

From “Dark Areas of Ignorance” to “Alternative Facts”
Political scientists’ interest in political knowledge dates back at least to the mid-twentieth
century. Following the Second World War, surveys probing the depth of the public’s political
information revealed “dark areas of ignorance” (Kreisberg, 1949). Most Americans were
unable to name prominent political figures or recognize current debates in the foreign or
domestic policy arenas (Almond, 1950). These findings revived earlier debates over the
implications of information deficits for representative democracy (Lippman, 1922).
This classic work on political knowledge, which reflected the assumptions of democratic
theory, primarily focused on the distinction between the informed and the uninformed (Delli
Carpini and Keeter, 1996). Only later did researchers consider the possibility that some respondents were misinformed (Kuklinski et al., 2000) and that incorrect answers to knowledge
questions reflected partisan bias rather than simple ignorance.
In an influential paper, Bartels (2002) identified systematic partisan biases in answers to
factual questions dating back to the 1980s. Despite significant improvements in inflation and
unemployment over the eight years of the Reagan Presidency, Democrats o↵ered generally
pessimistic views of national economic conditions. Bartels concluded partisans’ perceptions
of the economy, rather than reflecting real-world conditions, showed “substantial partisan
bias” (p. 136; see also Scha↵ner and Roche, 2016; Bisgaard, 2015).

3

There are also partisan di↵erences in consumer confidence (Mian et al., 2017; Enns et
al. 2012) and awareness of policy initiatives pursued by incumbents (Gaines et al., 2007).
These discrepancies are greater for topics on which the news media provides more extensive
coverage, suggesting partisans selectively interpret media coverage (Jerit and Barabas, 2012;
Scha↵ner and Roche, 2017; Kim and Kim, 2019). Note, however, that despite significant partisan gaps in economic perceptions and expectations, indicators of economic behavior (e.g.
purchasing) showed no partisan di↵erences (McGrath, 2017). This disconnect between attitudes and behavior is suggestive of the current controversies (described below) that surround
explanations for misinformed survey responses.
Recent studies make clear that misinformation extends beyond questions of economic
performance and policy to false claims about Barack Obama’s religion and country of birth,
conspiracy theories that the government had advance knowledge of the September 11th
terrorist attacks, and beliefs about climate change (see e.g., Hochshild and Einstein, 2015;
Flynn et al. 2017; Bullock and Lenz, 2019). Moreover, inaccurate beliefs are not limited
to some fringe element of the electorate. On survey items we later examine in this study,
partisan di↵erences approach 50 percentage points on some matters related to misstatements
made by Donald Trump, including widely publicized claims about illegal voting in the 2016
election and the alleged wiretapping of Trump Tower by the Obama Administration.
Before we proceed it is important to note that, even in contemporary politics, partisan
divisions over political information are not always present. Graham (ND) and Roush and
Sood (2019) conduct systematic assessments of political information and identify many topics
with modest or non-existent partisan divides. At the same time, our preceding review
establishes some issues on which Republicans and Democrats endorse di↵erent sets of facts.
In what follows, we focus on these areas of intense partisan disagreement as they underlie
concerns about the implications of political misinformation, represent an important setting
for understanding the sincerity of the misinformation measured in surveys and have not been
4

used in previous studies that provide monetary incentives for correctly answering political
information questions.

Motivated Reasoning and Misinformation
What explains these partisan divides over facts? One common interpretation is that they
stem from motivated reasoning. Psychologists have uncovered several information-processing
heuristics including confirmatory bias and motivated skepticism (Lord et al., 1979; Kunda,
1990) that allow individuals to protect their identity from dissonant information. These
biases in information processing are supplemented by other mechanisms that shape the
information people encounter in the first place, in particular theories of cognitive consistency
(Festinger, 1957) in which selective exposure limits the public’s encounters to information
at odds with their beliefs.
This motivated reasoning account is well-suited to explaining the adoption of partycongenial misinformation in contemporary politics (Taber and Lodge, 2013; Gaines et al.,
2007; Kraft et al., 2015; Miller et al. 2016; Flynn et al., 2017). Rising elite polarization
and growing antipathy towards the other political party have strengthened partisans’ motivation to ignore credible facts that cast aspersions on their party or party leaders and,
conversely, to accept false information that reflects favorably on their side. These tendencies
are complemented by a high-choice information environment that creates an opportunity for
partisans to screen out uncongenial information (Stroud, 2011; Iyengar and Hahn, 2009) and
the availability of misinformation from political elites (e.g., Bisgaard and Slothuus, 2018).
This is particularly relevant following the election of Donald Trump, a politician willing to
make misleading and false claims on a regular basis. The current information environment
thus creates multiple opportunities and incentives for partisans to accept misinformation.
Importantly for what follows, from this perspective the divides in political information

5

that result from motivated reasoning are real and troubling. Individuals accept misinformation and go on to incorporate it into their future political decisions (see e.g., Thorson, 2016;
Swire et al., 2017; Flynn and Krupnikov, 2019).
Before proceeding, we note that motivated reasoning is not the only mechanism through
which the genuine adoption of factually inaccurate beliefs may occur. For instance, di↵erential beliefs regarding source credibility and evidence quality may produce similar partisan
divides through a process that does not depend on biased information processing (Druckman
and McGrath, 2019). Our design does not let us distinguish motivated reasoning from these
other processes. Such examinations remain an important topic for future work.1

Misinformation as Partisan Cheerleading
In contrast to the preceding account of motivated reasoning, in which partisan divides over
facts are genuine, a recent alternative perspective suggests these divides are illusory. Under
this account, typically referred to as partisan cheerleading (see e.g., Bullock and Lenz, 2019),
partisans know the correct answer to knowledge questions, but prefer to o↵er an incorrect,
party-congenial survey response to support their side (Prior et al., 2015; Bullock et al., 2015;
Scha↵ner and Luks, 2018).
These contrasting interpretations of existing evidence have led scholars to develop research designs that can distinguish motivated reasoning from partisan cheerleading. As
described below, one widely used approach is to provide financial incentives that reward respondents for accurate answers to information questions. To the extent partisan di↵erences
1

Our use of “motivated reasoning” to describe the process producing sincerely held in-

accurate beliefs follows the vast misinformation literature in which motivated reasoning is
identified as “the most useful framework for understanding misperceptions about politics”
(Flynn, Nyhan and Reifler, 2017, 131; see also Miller et al., 2016).

6

in information shrink or vanish when researchers incentivize respondents to provide accurate
answers, it strengthens the interpretation that incorrect responses represent “cheerleading”
rather than genuine misinformation.
In the first attempt to distinguish between these alternative accounts of misinformation,
Lupia and Prior (2008) o↵ered respondents one dollar per correct answer before asking a battery of 14 factual questions. Unfortunately, the questions generally lacked partisan valence,
focusing instead on candidate issue positions and other matters relating to the 2004 election.
Respondents incentivized to give accurate answers scored higher on information, however,
the study did not examine the e↵ects of incentives on partisan di↵erences in information.
A second study, conducted in 2008 (Bullock et al., 2015), provided more persuasive
evidence on the “corrective” impact of financial incentives. The partisan di↵erence in information, which was substantial among respondents o↵ered no incentive, fell by half among
respondents informed that their chances of winning a 200 dollar gift certificate would increase with correct answers. In a follow-up study, the researchers o↵ered respondents varying
amounts of money (ranging from 10 cents to a dollar) for correct responses; as they expected,
the partisan gap in information fell most dramatically in the highest payo↵ condition. Finally, another 2008 study (Prior et al., 2015) asked respondents about economic conditions
under the Bush Administration and paid one dollar per each correct response. The partisan
di↵erence in correct responses fell substantially in the incentivized group, leading the authors
to conclude that partisan bias in perceptions of economic conditions represents “an artifact
of partisan consistency pressures” (p. 510).
Contrary to these studies, recent work on the acceptance of rumors (Berinsky, 2018)
reaches the conclusion that incentives and related interventions designed to increase the
accuracy of survey responses have little impact on strongly valenced partisan beliefs. In this
study, incentives – operationalized using the time respondents would be required to spend on
the survey rather than a direct monetary payment – did little to weaken Republicans’ belief
7

that former President Obama is a Muslim, or Democrats’ willingness to state that members
of the Bush Administration knew about the 9/11 attacks in advance. Across multiple studies
that deployed three di↵erent techniques for boosting response accuracy, partisan di↵erences
in rumor acceptance did not change. Berinsky’s conclusion? “Expressive responses to rumor
questions from across the political spectrum are relatively rare. More generally, these results
suggest that – contrary to the concerns raised by some scholars such as Bullock et al. (2015)
– survey responses serve as a window into the underlying beliefs and true preferences of the
mass public” (p. 213).
In summary, there is agreement that on many political information questions Democrats
and Republicans gravitate to survey responses that cast their party in a favorable light.
There is disagreement, however, over the meaning of these divides. Some scholars discount
the evidence of considerable misinformation as an artifact, citing impression management
objectives that motivate partisans to answer incorrectly. Others dispute this account, believing that misinformation is a genuine phenomenon rooted in polarization and the availability
of biased information.

Research Design
We introduce a modified version of the experimental design previously used to identify the
contribution of cheerleading to partisan divides on political misinformation. There are three
stages to our modified incentive design. First, following earlier studies, we randomly assign survey respondents to either receive or not receive incentives for correctly answering
political knowledge questions. In both the surveys we consider, these incentives were $0.50
per question for a five-item information battery. Respondents assigned to receive incentives
were informed of this before answering the information questions. In a point we later revisit
when discussing the implications of our findings, this amount falls in between the incentive

8

levels used in previous studies. It is less than those in study 1 ($1.00) and study 2 ($2.00)
of Prior et al. (2015), but greater than the incentives in study 1 of Bullock et al. (2015),
which provided a lottery entry worth approximately $0.17, and some conditions of study 2
of Bullock et al. (2015), which varied incentive levels from $0.10 to $1.00.
The second stage of this design departs from earlier studies. Before answering each
question, respondents selected a news article on the topic to read. They saw a menu of five
information sources, each accompanied by an article headline, and selected one to view.2
They were then taken to a brief report (approximately two paragraphs in length) drawn
from a real story on the topic available from that source.
While the specific sources varied across each survey item, the choice set remained consistent. In each case, respondents could choose to read information from one left-leaning source
(i.e., MSNBC or Huffington Post), one right-leaning source (i.e., Fox News or Breitbart), one
source with specialized expertise (e.g., excerpts from a Bureau of Labor Statistics press release in the case of a question about the unemployment rate), or from a pair of mainstream
news sources (i.e., CNN, New York Times).
Finally, in the third stage, after reading the report they selected, respondents answered
the political information question and also indicated how confident they were (ranging from
“not at all sure” to “extremely sure”) of their answer.
This design accomplishes two goals. First, the random assignment of incentives provides
insight into the extent to which divides in factual beliefs are inflated by partisans knowingly
providing incorrect, but party-congenial, answers to the questions. We assess this inflation
by comparing the magnitude of the partisan divide across the treatment conditions. This
element of the design allows us to extend prior studies of partisan cheerleading to a set of
new hyper-partisan information items centered around controversies that were ongoing as we
fielded our studies (we discuss the content of the information questions in the next section).
2

Appendix A2 and A3 show how the news selection tasks appeared to respondents.
9

Second, the information search component of our design allows us to move beyond previous work and consider whether the availability of incentives changes individuals’ preferences
for information sources. Of particular interest is the impact of incentives on partisan differences in news choice. We can observe whether partisan cheerleading applies not only to
factual beliefs, but also to partisan selective exposure. Some past work suggests this possibility could occur if, in surveys, partisans feel pressure to express an affinity towards news
sources associated with their side, but which they typically avoid when looking for accurate
information in the real-world (e.g., Prior, 2013). Accordingly, this outcome yields important
insight into the reputation of media outlets. If these reputations are weakly held and partisans knowingly trade o↵ unbiased for biased news coverage in the unincentivized conditions,
they should alter their search preferences in favor of unbiased sources when provided with
incentives for correct answers.
The final piece of our research design is the ability to observe the news consumption of
some survey respondents outside the survey context. While we discuss this feature of the
study in more detail later, some survey respondents had previously installed an application
that tracked their web browsing behavior. We can therefore compare the news choices they
made in the survey with the choices that characterize their everyday web browsing behavior.
This provides a rare opportunity to understand which, if any, of the experimental conditions
in our survey corresponds with the naturally-occurring incentives for information acquisition
that respondents face in the real world. While we refer to this real-world browsing data, at
times, as a measure of information search, we note here these real-world media consumption
choices reflect a mix of information acquisition, desire for congenial information and interest
in the non-political material available from these media sources (see e.g., Stroud, 2011).

10

Data
We collected data using this modified incentive design in two surveys administered by
YouGov. These surveys use the firm’s standard procedure of drawing a sample from its
panel that is matched to the voting-age population on key demographic characteristics (e.g.,
Vavreck and Iyengar, 2011). The first, conducted in September 2018, had 1,000 respondents.
The second, conducted in March 2019, had 1,700 respondents.3

Because of their similar

formats, we pool the surveys together for analysis, although in Appendix B we note the
results are no di↵erent when we consider the surveys separately.
This matched sampling approach is also used in other academic surveys (e.g., the Cooperative Congressional Election Study). We note here that the unweighted sample used in our
primary analysis is older and more highly educated than a nationally-representative sample
would be (see Table A1 for demographic information). At the same time, there is little
evidence this contributes to our findings, particularly as we see similar results in robustness
checks that incorporate survey weights to correct for these deviations between our realized
sample and nationally-representative targets (See Appendix B).
Our information items focused on several topics covered in prior studies of political misinformation (e.g., national economic conditions and climate change) as well as new questions
bearing on inaccurate and controversial statements associated with President Trump. Table
1 displays the information items in the two surveys.

3

While we do not incorporate this feature of the data into our current analysis, there was

a panel structure to these surveys. 954 individuals completed both surveys.
11

Table 1: Misinformation Items
Topic

Item
The vast majority (over 90%) of climate
Climate Change
scientists believe that global warming is
(Survey 1)
an established fact and that it is most
likely caused by man-made emissions
Michael Cohen, Donald Trump’s personal
Cohen Plea
lawyer, pleaded guilty to fraud and illegal
(Survey 2)
campaign finance charges in August 2018
Firearm Sales
40% of firearm sales in the US occur
(Survey 2)
without a background check
Illegal immigrants commit violent crime
Immigrant Crime
at a significantly higher rate than legal
(Survey 1)
American citizens
Obama Wiretap
Former President Obama ordered
(Survey 1 & 2)
wire taps on Donald Trump’s phones
Voter Fraud
Millions of illegal votes were cast
(Survey 1 & 2)
in the 2016 presidential election.
Compared to January 2017, has the level
Unemployment
of unemployment in the country increased,
(Survey 1 & 2)
stayed the same, or decreased?

Incorrect Response

Party Valence

False

Republican

False

Republican

True

Democratic

True

Republican

True

Republican

True

Republican

Increased OR
Remained the same

Democratic

As Table 1 shows, we include items with di↵erent partisan valence, which we define as
the directional motivation to provide party-congenial, inaccurate answers. Depending on the
item, these pressures can apply to either Democrats or Republicans. The items regarding unemployment under the Trump administration and firearm sales push Democrats toward the
incorrect answer. Since most of the remaining items address inaccurate statements by President Trump (e.g., his unsubstantiated claim of widespread voter fraud in the 2016 election),
the pressure to treat these statements as accurate is concentrated among Republicans.

Hypotheses
Motivated reasoning and cheerleading are two alternative explanations for partisan di↵erences in beliefs about current issues and events. Proponents of cheerleading argue that
polarization creates short-term psychic rewards for partisans to o↵er knowingly incorrect
12

responses to knowledge questions, even as it does not induce genuine belief in these partycongenial falsehoods. In contrast, discussions of motivated reasoning posit that party polarization is so intense that partisans accept claims that cast their party in a favorable light,
even when the claims are at odds with the facts.
Our design permits us to compare these competing hypotheses using three tests. First,
we assess the impact of financial incentives on partisan divides in answers to the political
knowledge questions. If cheerleading is the predominant explanation for these di↵erences,
providing incentives should markedly weaken the partisan divide, as partisans would be
expected to revert to their “true” state of knowledge when responding. On the other hand,
if the motivated reasoning hypothesis holds, then misinformation is genuine and incentives
should have minimal e↵ects on partisan divides across these items.
Second, we apply the same logic to assess the e↵ect of financial incentives on respondents’
information-seeking behavior, and specifically, whether the presence of incentives encourages
the use of unbiased or non-partisan news providers (versus co-partisan news sources). Again,
if cheerleading is at work, we would expect reliance on partisan sources to be weakened among
incentivized respondents, while the motivated reasoning account predicts no di↵erences in
information preferences across the experimental conditions.
Finally, we were able to directly compare news preferences exhibited in both the incentivized and non-incentivized conditions with naturally occurring news consumption. This
was possible because we recruited a subset of our experimental participants from an online
panel whose survey responses could be linked to their daily web browsing behavior. Here,
the cheerleading account predicts greater consistency of news preferences between the incentivized condition and everyday browsing behavior, while the non-incentivized condition
should deviate from both the incentivized condition and everyday behavior (precisely because
in the non-incentivized condition cheerleading is operative). On the other hand, the motivated reasoning paradigm leads us to expect the incentivized and non-incentivized conditions
13

will be similar to each other each other and to naturally occurring browsing behavior.

Results: Partisan Divides in Information
We begin by documenting partisan divides in information among participants in the unincentivized condition of our surveys. For each item, Table 2 displays the share of Democrats
providing the correct answer, the share of Republicans providing the correct answer and the
divide between these groups, operationalized as the absolute value of the di↵erence between
the share answering correctly in each group. The items are sorted from those with the most
pronounced gaps between parties to those with the smallest gaps.
Table 2: Partisan Information Divides in Unincentivized Conditions
1
2
3
4
5
6
7
8
9
10

Topic
Immigrant Crime (Survey 1)
Voter Fraud (Survey 2)
Obama Wiretap (Survey 2)
Voter Fraud (Survey 1)
Obama Wiretap (Survey 1)
Firearm Sales (Survey 2)
Climate Change (Survey 1)
Cohen Plea (Survey 2)
Unemployment (Survey 1)
Unemployment (Survey 2)

Share Correct (Dem)
0.97
0.86
0.98
0.92
0.98
0.22
0.96
0.93
0.74
0.72

Share Correct (Rep)
0.45
0.38
0.53
0.49
0.58
0.56
0.63
0.80
0.85
0.80

Partisan Divide
0.52
0.48
0.45
0.43
0.40
0.34
0.33
0.13
0.11
0.08

Table 2 reveals substantial partisan divides across the items we consider. Notably, the
smallest divides occur on the misinformation items that overlap with those included in earlier
studies that find partisan cheerleading to explain a substantial proportion of the partisan
divide over political information (i.e., the items regarding unemployment). This speaks to
the particularly divisive nature of the new items we examine, which derive from controversies
that were ongoing as the surveys were fielded.
To consider how the availability of incentives a↵ected these substantial partisan divides,
we stack each respondent’s answers to the political information questions to conduct a pooled
14

analysis of these di↵erent items. We follow previous studies (e.g., Bullock et al., 2015) and
standardize the direction of these outcome measures. Specifically, we code responses so
that the information items are oriented in a similar manner, with the answer that is partycongenial for Democrats always receiving the higher value. So, for the items where the
misinformed answer is party-congenial for Democrats, incorrect answers are coded as one
and correct answers are coded as zero. This is reversed on items where the misinformed
answer is party-congenial for Republicans. Here incorrect answers are coded as zero and
correct answers are coded as one. This allows us to readily interpret partisan di↵erences
that emerge across these di↵erent items.
Respondents were not provided with a “Don’t know” option across these items, meaning
they had to provide some form of response. In this respect our study is similar to the first
study in Bullock et al. (2015) and Prior et al. (2015), but departs from the conditions in
the second study of Bullock et al. (2015) in which partisan divides over fact decline when
a “Don’t know” option is made available. However, in later sections we also incorporate a
measure of response certainty that allows us to isolate firmly-held inaccurate beliefs.
We use the following specification, where i indexes respondents and k indexes the different questions, to examine partisan di↵erences in political information in the baseline and
incentive treatment conditions:

Informationik =
+

0

+

1 Democrati

3 Democrati

+

2 Incentive

Treatmenti

⇥ Incentive Treatmenti + ↵k + ✏ik

Democrat is a binary variable with a value of one for Democrats and zero for Republicans.
We include those who “lean” toward one of the parties with the party they are close to.
Because they do not face directional pressures in the same way as partisans, we exclude

15

“pure” independents from the analysis.4

The coefficient on this variable ( 1 ) indicates

the magnitude of the partisan informational divide in the baseline, unincentivized condition.
The interaction between partisanship and the incentive treatment ( 3 ) captures the impact
of the incentive treatment on this divide. We also include fixed e↵ects for each survey item
(↵k ) and cluster standard errors by respondent to account for the multiple observations per
respondent and because we assigned treatments at the respondent level.
Table 3 displays the results of this analysis. The first column considers all partisan
respondents. There is a substantial partisan divide (.32) over political information (95%
Confidence Interval [0.31, 0.33]). Since the information items have a dichotomous metric,
the observed divide amounts to roughly one-third the range of the outcome variable.
Moving to the other coefficient of interest, the interaction between partisanship and the
incentive treatment is negative and statistically significant (-0.10, 95% Confidence Interval
[-.12, -.08]). Incentives reduce the partisan divide to two-thirds of the magnitude in the
unincentivized conditions. Consistent with earlier work, this amounts to some evidence of
cheerleading (e.g., Bullock et al., 2015, Prior et al., 2015).
The next three columns examine this pattern across three subsets of the data: strong
partisans (Column 2), information items for which Democrats were motivated to answer incorrectly (Column 3), and information items where the incorrect answer was party-congenial
for Republicans (Column 4). There is some minor variation in the initial magnitude of these
partisan divides across these di↵erent specifications, but the e↵ect of incentives remains
similar and statistically significant e↵ect across all three.
While the partisan gap in information closes due to incentives, it is by no means eliminated. Large partisan divisions remain in the incentivized condition. This means that
cheerleading alone is an insufficient explanation for the partisan information divide. In our
primary specification from Column 1 that includes all items and all partisan respondents, the
4

See Appendix B.2 for the results among this group
16

Table 3: E↵ect of Incentives on Partisan Information Divide
All Partisans Strong Partisans Dem-Valenced Rep-Valenced
0.64⇤
0.59⇤
0.56⇤
0.59⇤
(0.02)
(0.02)
(0.02)
(0.02)
Democrat
0.32⇤
0.38⇤
0.19⇤
0.38⇤
(0.01)
(0.02)
(0.02)
(0.02)
Incentive Treatment
0.07⇤
0.09⇤
0.00
0.11⇤
(0.02)
(0.02)
(0.02)
(0.02)
Democrat ⇥ Incentive Treatment
0.10⇤
0.12⇤
0.08⇤
0.11⇤
(0.02)
(0.02)
(0.03)
(0.02)
N
11761
6197
3844
7917
(Intercept)

Robust standard errors, clustered by respondent, in parentheses
Models include item fixed e↵ects
⇤
indicates significance at p < 0.05

partisan divide is .22 (95% Confidence Interval [0.19, 0.24]) among incentivized respondents.
Across the other specifications the pattern is similar, roughly 60 to 70 percent of the initial
partisan di↵erences persists even when incentives are available.
The persistence of partisan divides in factual beliefs among the groups provided with
incentives is noticeably higher than in previous work. In the closest comparison, Bullock et
al. (2015, 521) find that only 40% to 45% of the initial partisan gaps remain when incentives
are made available.5

This speaks to the more persistent attachment to party-congenial

misinformation when survey items address highly polarizing topics, as in our studies. While
the treatment e↵ects of incentives across these studies are of similar magnitude to those
observed in Bullock et al. (2015), the initial partisan gap on these items is much larger than
previously obtained in incentive-based designs.
We also note that, when isolating the information items about unemployment in Appendix Table B.1, we observe a similar pattern of findings to previous work that considers
this topic. Our findings diverge from previous studies due to the inclusion of new, more
5

Other studies of misinformation are more difficult to compare as they either focus on

correct responses rather than partisan divides (e.g., Prior et al., 2015), or, by design, are
only conducted among members of one party (e.g., Berinsky, 2018).
17

divisive information questions.6

Incorporating Respondent Certainty About Misinformation
The preceding analysis may capture a pattern in which partisans choose the incorrect answer
when it is party-congenial, but are not particularly confident in their response. For this
reason, several studies note the need to consider the certainty with which respondents hold
their beliefs (e.g., Pasek et al., 2015; Graham, ND).
To incorporate confidence, we asked respondents how certain they were of their answer on
a five-point scale after each information question. We now reassess the partisan divide and
the e↵ects of incentives only among those respondents who were certain of their responses to
the information questions (i.e., state they are “very” or “extremely” sure of their answer, the
top two categories of the five-point certainty scale) and then estimate the e↵ects of incentives
on partisan divides in political information.7
These results are displayed in Table 4, using the same model specification estimated
among all partisan respondents (Column 1) and di↵erent subsets of the data (Columns 2-4).
While the initial magnitude of these partisan divides is smaller across the various specifications in this more stringent coding, the pattern is largely consistent with our earlier analysis.
While the availability of incentives reduces partisan gaps on the information items, large
divides remain even in the presence of incentives. In the pooled analysis that incorporates
6

In Appendix B.2 we find no evidence these baseline divides in our study are due to the

sample composition and show large divides exist on similar items asked in other surveys.
7
If incentives a↵ect how certain respondents are of their responses, this specification risks
inducing post-treatment bias in our estimates (see e.g., Montgomery et al., 2018). However,
we find no direct e↵ect of incentives on response certainty and, in Appendix B.9, observe
similar findings when we estimate an alternative specification that incorporates the certainty
with which misinformation is held, but is not subject to these concerns.
18

Table 4: E↵ect of Incentives on Partisan Information Divide (Certain Respondents)
All Partisans Strong Partisans Dem-Valenced Rep-Valenced
0.59⇤
0.52⇤
0.57⇤
0.50⇤
(0.02)
(0.03)
(0.03)
(0.02)
Democrat
0.38⇤
0.46⇤
0.18⇤
0.48⇤
(0.02)
(0.02)
(0.02)
(0.02)
Incentive Treatment
0.09⇤
0.10⇤
0.02
0.15⇤
(0.02)
(0.03)
(0.02)
(0.03)
Democrat ⇥ Incentive Treatment
0.11⇤
0.13⇤
0.08⇤
0.15⇤
(0.02)
(0.03)
(0.03)
(0.03)
N
8047
4477
2239
5808
(Intercept)

Robust standard errors, clustered by respondent, in parentheses
Models include item fixed e↵ects
⇤
indicates significance at p < 0.05

all items and partisan respondents, the initial gap between partisans shrinks to roughly twothirds its initial size when respondents receive incentives. This remaining gap of .27 (95%
Confidence Interval [0.24, 0.30]) is both substantial and statistically significant.8
In this sense our findings hold even when we account for the possibility that many individuals do not hold their beliefs with a high degree of certainty and are aware they may
be wrong (Graham, ND). There are substantial partisan divides over questions of fact that
remain in the presence of incentives when we limit the focus to those who are confident in
their answers to these questions.

Partisan Divides in News Choice
We turn next to the impact of incentives on news choice. We group the news sources
available to respondents into four categories: a co-partisan news source (e.g., Fox News for
Republicans), an out-partisan news source (e.g., Huffington Post for Republicans), the two
8

Appendix Table B12 also shows partisan divides remain among low-certainty respondents

even when incentives are made available.

19

mainstream (i.e., non-partisan) news sources and the domain-specific expert source.9
In the control condition lacking incentives, the partisan divide in news choice is glaring.
Across the news selections made in the control condition of the surveys (i.e., those where
respondents were not assigned incentives for correct answers to the information questions),
29% of selections went to a co-partisan source, 26% went to the expert source, 38% to one
of the two mainstream news sources and only 7% to an out-party source. This 4:1 ratio in
the selection of co-partisan sources, relative to out-partisan sources, is far from a complete
partisan echo chamber, but indicates that partisans prefer in-party to opposing sources by
a wide margin. This is consistent with recent observational studies of online web browsing
behavior which find that, with respect to political news, partisans di↵er in their typical
media diet even as they do not completely retreat into echo chambers (see e.g., Peterson et
al., ND, Flaxman et al., 2016, Guess, 2019).10
How did the availability of incentives change these baseline patterns of news consumption?
The partisan cheerleading hypothesis leads us to expect a decline in reliance on congenial
sources in the incentivized condition. Conversely, the motivated reasoning hypothesis predicts that partisans’ preferences for biased information will be una↵ected by incentives. We
distinguish between these accounts by estimating the e↵ects of incentives across the di↵erent news choice outcomes using the following specification, in which we regress an indicator
variable for news selection on the incentive treatment and a fixed e↵ect for each information
item:

News Choiceik =
9

0

+

1 Incentive

Treatmenti +

2 Democrati

+ ↵k + ✏ik

We observe similar results when using a finer distinction between the di↵erent news

sources. See Appendix B.
10
As we later document in greater detail, these divides are also comparable to those that
emerge in real-world media choices outside the survey setting.

20

Table 5 displays the results of these regressions. The first column shows the e↵ects of
the incentive treatment on the probability that a respondent selected a co-partisan source
(i.e., the outcome is an indicator variable for whether a co-partisan source was selected or
not). The second column does this for the expert source. The third column uses a binary
indicator for selecting one of the two mainstream news sources as an outcome. Finally, the
fourth column does so for the out-partisan source.
Table 5: E↵ect of Incentives on Information Source Selection
Co-Partisan News Expert Mainstream News Out-Partisan News
(Intercept)
0.41⇤
0.37⇤
0.16⇤
0.06⇤
(0.02)
(0.02)
(0.02)
(0.01)
Incentive Treatment
0.00
0.00
0.01
0.00
(0.01)
(0.01)
(0.01)
(0.01)
N
11821
11821
11821
11821
Robust standard errors, clustered by respondent, in parentheses
Models include item fixed e↵ects and condition on respondent partisanship

A similar pattern holds across all these outcomes. Incentives have no e↵ect whatsoever
on news choice. For instance, in Column 1 the point estimate for the e↵ect of incentives on
the probability of selecting a co-partisan news source is negative, but substantively small
and indistinguishable from zero (-0.003, 95% Confidence Interval [-0.03, 0.02]). This pattern
of no sizeable or statistically significant e↵ects of incentives on information source selection
continues when we consider the probability of selecting an expert source (Column 2), one
of the two mainstream news sources that was available (Column 3) or an out-partisan news
source (Column 4).11
In contrast to the evidence of modest partisan cheerleading for the information items, we
observe no change in news choices when incentives are made available. The continued use
of co-partisan information sources in the incentivized condition suggests that the partisans
who use them do not believe the information provided by party-congenial sources is any
11

In Appendix B we show there are similar results when we examine each party separately.
21

less accurate than information from non-partisan or expert sources. As predicted by the
motivated reasoning hypothesis, partisans’ news preferences are immune to the e↵ects of
financial incentives.

Comparison With Real-World Media Choice
A fundamental limitation of survey experimental designs is the generalizability of behavior
observed in the experimental context (Barabas and Jerit, 2010). For our results, this raises
several concerns. First, participants’ news preferences, irrespective of the condition to which
they were assigned, may have been influenced by “demand” e↵ects – subtle cues conveyed by
the study description or instructions – which may influence party-congenial news selection
in both the treatment and control groups (but see Mummolo and Peterson, 2019). Second,
to the extent partisans know the correct answer to knowledge questions when they enter
the survey – and whether or not they ultimately plan to answer these questions in a sincere
manner – they may not need to acquire new information over the course of the survey even
when incentivized. This means both the unincentivized and incentivized conditions of our
surveys could exaggerate partisans’ tendency to search for information from party-congenial
sources relative to their real-world tendency to do so.
Fortunately, we can draw on the unique nature of our dataset to address concerns that
the in-survey news selections made over the course of the experiment do not mirror everyday
news consumption. 855 of the respondents in the survey were members of the YouGov Pulse
Panel. They had all consented to install a toolbar on their primary desktop web browser
that tracked the websites they visited in the months prior to these surveys.12 They received
YouGov points for keeping the toolbar active, but had the option to turn it o↵ if they wished.
12

For recruitment, YouGov employs a consent screen using language taken from the Market

Research Society’s code of conduct. Panelists are informed that their web browsing behavior
will remain anonymous and that the data will only be used for research purposes.
22

In general, measures of web traffic collected from this panel resemble those from sources (see
e.g., Guess, 2019; Peterson et al. ND for other applications in political science).
For this group of respondents, we can compare the news choices they made within the
experiment with their normal web browsing behavior. We emphasize here that our measure
only captures web browsing conducted from desktop computers. This misses other forms
of media use (e.g., mobile web browsing and cable television consumption). We note our
evidence cannot speak to the correspondence between survey-based and behavioral measures
of media exposure in these other settings, which represent a setting for future work. In this
case we proceed using this measure of desktop-based online news exposure as it provides an
important supplement to our previous analysis. This data o↵ers the chance to see whether
the news choices respondents make within the survey context align with their behavior in
an important form of news consumption outside the survey.
We compare the partisan slant of a respondent’s real-world media diet to the partisan
slant of the news selections they made in our surveys. We characterize the partisan slant of
media choices in the experiment as a trichotomy, with right-leaning sources scored as one,
the mainstream and expert sources as zero, and left-leaning sources as negative one.13 We
average this measure across the multiple news choices respondents made over the course of
the survey. We then compare this indicator of preference for partisan news with the slant that
respondents displayed in their everyday browsing behavior. We characterize the partisanship
of their web browsing behavior outside the experimental setting using the “alignment scores”
13

We recognize that, in public perceptions of their perceived slant and the partisanship of

their audiences, the mainstream outlets we consider (i.e., CNN and the New York Times)
appear similar to the left-leaning news sources (i.e., Huffington Post and MSNBC). In Appendix C.2 we observe similar results using two alternative measures of the partisanship of
an individual’s survey-based media choices.

23

for a set of 452 news websites presented in Bakshy et al. (2015).14

The alignment scores

reflect the partisanship of news domains based on the ideology of individuals who shared their
content on Facebook. Domains shared more frequently by conservatives receive alignment
scores closer to 1, while those o↵ering content shared more frequently by liberals receive
scores closer to -1.
Figure 1 displays the comparison of a respondent’s real-world media slant (the X axis)
to the slant of their news selections in the experimental context (the Y axis). Individual
observations are shown in gray. We use a loess smoother to plot the relationship between
these variables. The dashed line corresponds to a slope of one, providing a baseline of exact
correspondence between the two measures of slant.
Figure 1 shows that news-seeking behavior within the experiment di↵ers only slightly
from news consumption in the real world. The correspondence is particularly strong for those
with everyday media diets that range from moderate to conservative. For respondents at the
other end of the ideological spectrum the correspondence is weaker; those with very liberal
real-world media preferences were less apt to rely on the left-leaning sources available in the
experiment. Overall, there is a substantial correlation (.58) between the two measures of
partisan slant. Relative to other comparisons of attitudes or behaviors observed within survey
settings and real-world behavior (see, for instance, Wicker, 1969; Bertrand and Mullainathan,
2001), this represents an impressive degree of correspondence.
Having validated our measure of news preferences (partisan slant), we turn to the question
of whether incentives strengthen or weaken the partisan divide in news preferences. For this
14

The original data contain five hundred websites, but for our study we drop non-news

sources included in the original data (e.g., Amazon, Wikipedia) and websites no longer in
operation over the time window we consider. As in Peterson et al. ND, we also restrict our
analysis of several high-traffic websites that contain both news and non-news content (i.e.,
Yahoo, MSN and AOL) to traffic to the news portions of those websites.
24

●
●

●●

●
●

0.5

●

●●
●
●●
● ●●
●●●●
●
● ●
●
●●
●●●
●
●● ●
● ● ●●●●
● ●●●
● ●● ●
●
●●●●
●●
●●● ● ●●
●●
●●●
●
●● ●●●●
● ● ●
● ● ●●●
● ●●● ●●●●●●●

● ●●
●●
●
●
● ● ● ●●●●
●● ●
●●
●●●
●
●
● ● ●●
●
●
●
●
●●●●
●● ●
●
●●●● ● ● ● ●
●
●●
●
●
●●
●
●●
●
●
●●● ●● ●

●

●

●● ●
●●
● ●● ● ● ● ●
●
●
●●●
● ●●● ●●●
●
●
●
●●
●
●●
●
●●
●●●
●
● ●
●●
●●●
●
●
●
●●● ●
●●●●● ●
●
●
●
●
●
●
●●●
●●●
●
●
●●●
●
●
●●
●
●
● ●●●●●
● ●●●●
● ●
●
●●
●● ●
●

●

●●
●
●

● ●●● ● ●
●
●●
● ●
●
●● ● ●
●

● ●●
●●
●●●● ● ●
●
●●●● ●
● ●●
●
●
●●●● ●
● ●
●
●●●●
● ●●●
● ●
● ●
●●
●●
● ●●
● ●● ●●●●●●●
●●
●
●●
●
●
●
● ●●● ●
●●
●
●●●
●●
●
●
●●
●
●● ●
●
●●
●●●

●

● ●●
●
● ● ●●●
●●
●●

●●
● ●● ●
● ●● ●
●
● ●
●

0.0

●
●●● ● ●
●●
●
●●
●●
●
● ●●
●
●●●●
●●
●
●●
●
●
●
●●
● ●
●●●
●●
●●
●
●
●●
●●●
●
●
●● ●
●
● ●●
● ●●● ●●
●
●
●
●
●●●
●
●●●●
●
●
●●●●
●●
●
●● ●● ●●
●
●
● ● ● ●● ●
●
●
●●
●
●
●●●●●●
●
●●●
●● ●
●●
●
●●
●●●
●●
●●
●● ●● ●
●
●●●●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
● ●●●●●●
●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●● ●●
●
●●●●●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
● ●
●
●
●●
●
●● ● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
● ●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
● ●
●
●
●
●
●●
●●
●
●
●●
●
●
●
●
●
●
●
●●
●●
● ●● ● ●
●● ●
●
●
●●
●●
●
●
●●●●
●
●
●●
● ●●
●
●●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●
●
●
● ●
●●● ●●●
●
●
●
●
●
●
● ● ●●
●
● ●●●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●●
●●
●
●
●
●
●●
●
●
●
● ●● ●●
●
●
●
●
●●
●●
●●
●●●● ● ● ●
●
●
●●
●●●
●●●
●●
●●

−0.5

●

●

●

●

●

●

−1.0

●●
●
●●
●●●● ●

●
●

●

●
●

●●
●

●●

● ●● ●
●●●● ●
●
● ●●
●
●
●●
●
●● ●
●
●
●
●● ●
●●
● ● ●●● ●●
●●
●●●
●●
●●
●
●● ●
●
●●
●
●
●●
●
●
●●
●●
●
●●●●●●
●
●
●
●● ●
● ●●

−1.0

Partisanship of Media Diet (Survey)

1.0

Figure 1: Partisan Slant of Survey and Real-World News Choice

●

●●
●● ● ● ● ● ● ●●
●
●
●● ●●
● ● ●● ●
● ● ●●
●
●
●
●
● ●●
●
●● ●
● ●● ●
●
●

●
●
● ●● ●
●

−0.5

●
●
●●
●●●

●

●●
●

0.0

0.5

1.0

Partisanship of Media Diet (Behavior)

analysis, we calculate the di↵erence between the average media choice for Democrats and
the average media choice for Republicans (scaled in terms of the standard deviation of each
outcome) for the three relevant settings – the unincentivized survey, the incentivized survey
condition, and everyday web browsing behavior. The cheerleading prediction is that the
partisan divide in media choice should shrink under incentives and outside of the survey; the
motivated reasoning prediction is that the partisan divide will remain stable across all three
settings. We present the results in Table 6.

25

Table 6: Partisan Divides in Media Choice By Setting
Setting
Partisan Divide
Survey + No Incentives 1.24 SD
Survey + Incentives
1.30 SD
Real-World Media Diet 1.36 SD

As shown in Table 6, partisan di↵erences in news preferences are of similar magnitude
across the di↵erent settings. These divides prove durable to the presence of incentives and
when examined outside the survey environment. Altogether, we find little evidence that
partisans exaggerate their affinity for co-partisan news sources inside surveys relative to the
manner in which they typically consume political news outside that context.

Discussion and Conclusion
The rise of partisan divides over factual evidence in surveys is readily apparent. But does
this trend pose a genuine threat to the democratic process? To understand the scope of
the problem – and whether potential remedies involve improving measures of beliefs about
politics to reduce cheerleading or e↵orts to correct genuinely held misinformation (see e.g.,
Jerit and Zhao, ND; Porter and Wood, 2019) – we assessed the relative importance of
motivated reasoning and cheerleading as explanations for misinformation.

Implications for Political Misinformation
We first discuss the implications of the substantial partisan divides over political information.
Consistent with the motivated reasoning perspective, our studies indicate that partisans
are genuinely committed to the inaccurate beliefs they report in surveys. The threat to
democratic functioning posed by polarization is real. The availability of incentives reduced,
but did not eliminate, the partisan divide in information. Approximately two-thirds of the
26

initial partisan divide in the unincentivized conditions persisted when we o↵ered respondents
the opportunity to earn rewards for correct responses. We observed this pattern for both
Democrats and Republicans, depending on the partisan valence of the topic.
One challenge to the validity of this motivated reasoning interpretation is that we cannot
address the possibility that, for some partisans, the “psychic” rewards for cheerleading exceed
the monetary rewards available in our studies. This would mean the incentive conditions
in our design still contain some degree of insincere cheerleading. Future research will need
to assess the elasticity of survey respondents’ responsiveness to even larger incentives than
those we employ here (for some evidence that larger amounts exert stronger remedial e↵ects,
see Bullock et al., 2015).
Relatedly, there is a need to calibrate incentive levels in these designs to match those
individuals face in the real world. Prior studies o↵er mixed evidence on this point. On the
one hand, much political decision-making occurs in a context that more closely resembles an
unincentivized survey than one in which clear material benefits for accuracy are available. A
long tradition in political science emphasizes that the material benefits of political participation that accrue to individuals are trivial (Downs, 1957) and, absent an express admonition
to avoid bias, previous studies find that information processing appears to resemble motivated reasoning (Bolsen et al., 2014; Prior et al., 2015). On the other hand, the material
incentives for accuracy available in our study are likely weaker than those facing voters who
perceive substantial societal stakes in an election outcome (Edlin et al., 2007) or feel social
pressure to accurately process information (Klar, 2014).
Given the ambiguity over the scale of incentives o↵ered in our study versus those that
apply naturalistically, we are unable to make claims about the realism of our manipulations.
Instead, our primary contribution is to show that partisan divides over a new class of political
information items – those directly related to ongoing political controversies – prove more
resistant to incentives than the types of information questions previously considered.
27

Implications for Political Information Search
Relative to the findings on misinformation, our evidence concerning information search provides more one-sided support for motivated reasoning. Partisans consulted congenial sources
to the same extent whether or not they were incentivized to answer correctly. Moreover, the
partisan divides in news choice present in the survey were no di↵erent from the divides in
the same respondents’ everyday news browsing behavior. Here we are better able to address
concerns about how the survey environment compares to real-world contexts. The ready
translation of partisan divides in the survey into web browsing behavior outside it suggests
that, in terms of their ability to motivate information search, incentives for accuracy are no
more pronounced in real-world news use than in the unincentivized survey conditions.

Methodological Implications
Finally, while our primary focus has been directed at the motivated reasoning versus cheerleading debate, this study also yields two methodological lessons. First, the partisan slant
of individuals’ news preferences, as elicited through a set of information-seeking tasks, corresponds to real-world news consumption. While concerns over inaccurate measures of news
exposure are endemic in survey research, particularly when it comes to respondents’ ability
to recall accurately the amount of news they encounter from various platforms (e.g. Guess,
2015, Prior, 2009), we find that eliciting the partisanship of a respondent’s news preferences
through a series of information-seeking tasks matches well with the preferences revealed in
their everyday behavior.
Second, our modified incentive design also sheds light on the psychological processes
underlying the remedial e↵ects of incentives in surveys. Bullock and Lenz (2019) point
out that the impact of incentives on accurate survey responses might occur either because
respondents expend greater e↵ort when asked an information question or due to increased

28

honesty (i.e., incentives discourage individuals who know the correct answer from responding
incorrectly). Our study speaks to this distinction by showing that at the information-search
stage, the availability of incentives does not alter respondent interest in information from
expert sources. On the assumption that comprehension of information provided by expert
sources requires greater e↵ort, this lack of movement in information search is consistent with
the interpretation that the mechanism underlying the e↵ects of incentives is not cognitive
e↵ort, but honesty, as posited in previous studies of partisan cheerleading.
In closing, we note this study raises important questions for future research. We have
interpreted the magnitude of the partisan information divide and the stability of partisans’
preferences for congenial information as evidence of motivated reasoning stemming from polarization -– the interpretation commonly given in the misinformation literature. However,
partisan motives may come into play at di↵erent stages of the belief formation process, beginning with information reception and following through to information acceptance (see,
for instance, the classic description of these stages in McGuire, 1985). Partisans’ continued preference for co-partisan news providers, for instance, suggests that the expression of
misinformed beliefs may have less to do with selective interpretation of neutral or unbiased
information, and more to do with exposure to one-sided or biased information in the first
place. Mapping the e↵ects of partisan motivated reasoning through the stages of belief
formation represents a promising agenda for misinformation research.

29

References
Almond, Gabriel. 1950. The American People and Foreign Policy. Harcourt.
Barabas, Jason and Jennifer Jerit. 2010. “Are Survey Experiments Externally Valid?”
American Political Science Review 104(2): 226-242.
Bakshy, Eytan, Solomon Messing and Lada Adamic. 2015. “Exposure to Ideologically
Diverse News and Opinion on Facebook.” Science 348(6239): 1130-1132.
Bartels, Larry M. 2002. “Beyond the Running Tally: Partisan Bias in Political Perceptions.”
Political Behavior 24(2), 117-150.
Berinsky, Adam. 2018. “Telling the Truth about Believing the Lies? Evidence for the
Limited Prevalence of Expressive Survey Responding.” Journal of Politics 81(1): 20112024.
Berinsky, Adam. 2017. “Rumors and Health Care Reform: Experiments in Political Misinformation.” British Journal of Political Science 47(2): 241-262.
Bertrand, Marianne and Sendhil Mullainathan. 2001. “Do People Mean What They Say?
Implications for Subjective Survey Data.” American Economic Review 91(2): 67-72.
Bisgaard, Martin. 2015. “Bias Will Find a Way: Economic Perceptions, Attributions of
Blame, and Partisan-Motivated Reasoning During Crisis.” Journal of Politics 77(3): 849860.
Bisgaard, Martin and Rune Slothuus. 2018. “Partisan Elites as Culprits? How Party Cues
Shape Perceptual Gaps.” American Journal of Political Science 62(2): 456-469.
Bolsen, Toby, James N. Druckman and Fay Lomax Cook. 2014. “The Influence of Partisan
Motivated Reasoning on Public Opinion.” Political Behavior 36(2): 235-262.
Bullock, John and Gabriel Lenz. 2019. “Partisan Bias in Surveys.” Annual Review of
Political Science 22: 325-342.
Bullock John, Alan Gerber, Seth Hill and Gregory Huber. 2015. “Partisan Bias in Factual

30

Beliefs about Politics.” Quarterly Journal of Political Science 10, 519-578.
Delli Carpini, Michael and Scott Keeter. 1996. What Americans Know about Politics and
Why It Matters. Yale University Press.
Druckman, James N. and Mary C. McGrath. 2019. “The Evidence for Motivated Reasoning
in Climate Change Preference Formation.” Nature Climate Change 9:111-119.
Druckman, James N., Erik Peterson and Rune Slothuus. 2013. “How Elite Partisan Polarization A↵ects Public Opinion Formation.” American Political Science Review 107(1),
57-79.
Edlin, Aaron, Andrew Gelman and Noah Kaplan. 2007. “Voting as a Rational Choice:
Why and How People Vote to Improve the Well-Being of Others.” Rationality and Society
19(3): 293-314.
Enns, Peter K., Paul M. Kellstedt and Gregory E. McAvoy. 2012. “The Political Consequences of Partisanship in Economic Perceptions.” Public Opinion Quarterly 76(2),
287-310.
Festinger, Leon. 1957. A Theory of Cognitive Dissonance. Stanford University Press.
Flaxman, Seth, Sharad Goel and Justin M. Rao. 2016. “Filter Bubbles, Echo Chambers
and Online News Consumption.” Public Opinion Quarterly 80(S1): 298-320.
Flynn, D.J. and Yanna Krupnikov. 2019. “Misinformation and the Justification of Socially
Undesirable Preferences.” Journal of Experimental Political Science 6(1): 5-16.
Flynn, D.J., Brendan Nyhan and Jason Reifler. 2017. “The Nature and Origins of Misperceptions.” Political Psychology 38(S1): 127-150.
Gaines, Brian J., James H. Kuklinski, Paul J. Quirk, Buddy Peyton and Jay Verkuilen.
2007. “Same Facts, Di↵erent Interpretations: Partisan Motivation and Opinion on Iraq.”
Journal of Politics 69(4), 957-974.
Garrett, R. Kelly, Brian E. Weeks and Rachel L. Neo. 2016. “Driving a Wedge Between
Evidence and Beliefs: How Online Ideology News Exposure Promotes Misperceptions.”
31

Journal of Computer-Mediated Communication 21(5): 331-348.
Graham, Matthew H. ND. “Self-Awareness of Political Knowledge.” Political Behavior Forthcoming.
Guess, Andrew. 2019. “(Almost) Everything in Moderation: New Evidence on Americans’
Online Media Diets.” Working Paper. https://www.dropbox.com/s/4qbn29l0pkpe2oa/
Guess_OMD.pdf?dl=0 Last Accessed: April 13, 2020.
Guess, Andrew. 2015. “Measure for Measure: An Experimental Test of Online Political
Media Exposure.” Political Analysis 23: 59-75.
Hochschild, Jennifer and Katherine Einstein. 2015. “Do Facts Matter? Information and
Misinformation in American Politics.” Political Science Quarterly 130(4): 585-624.
Huber, Gregory A. and Neil Malhotra. 2017. “Political Homophily in Social Relationships:
Evidence from Online Dating Behavior.” Journal of Politics 79(1): 269-283.
Iyengar, Shanto and Kyu S. Hahn. 2009. “Red Media, Blue Media: Evidence of Ideological
Selectivity in Media Use.” Journal of Communication 59(1), 19-39.
Iyengar, Shanto, Gaurav Sood and Yphtach Lelkes. 2012. “A↵ect, Not Ideology: A Social
Identity Perspective on Polarization.” Public Opinion Quarterly 76(3): 405-431.
Iyengar, Shanto, Tobias Konitzer and Kent Tedin. 2018. “The Home as a Political Fortress:
Family Agreement in an Era of Polarization.” Journal of Politics 80(4): 1326-1338.
Jerit, Jennifer and Yangzi Zhao. ND. “Political Misinformation.” Annual Review of Political
Science Forthcoming.
Jerit, Jennifer, and Jason Barabas. 2012. “Partisan Perceptual Bias and the Information
Environment.” Journal of Politics 74(3): 672-684.
Kim, Jin Woo and Eunji Kim. 2019. “Identifying the E↵ect of Political Rumor Di↵usion
Using Variations in Survey Timing.” Quarterly Journal of Political Science 14(3): 293311.
Klar, Samara. 2014. “Partisanship in a Social Setting.” American Journal of Political
32

Science 58(3): 687-704.
Kraft, Patrick W., Milton Lodge and Charles S. Taber. 2015. “Why People ‘Don’t trust the
evidence’: Motivated Reasoning and Scientific Beliefs.” The ANNALS of the American
Academy of Political and Social Science 658(1): 121-133.
Kreisberg, Martin. 1949. “Dark Areas of Ignorance.” in Public Opinion and Foreign Policy
(Ed. L. Markel). pp 49-64. Harper.
Kunda, Ziva. 1990. “The Case for Motivated Reasoning.” Psychological Bulletin 108(3),
480-498.
Kuklinski, James H., Paul J. Quirk, Jennifer Jerit, David Schwieder, and Robert F. Rich.
2000. “Misinformation and the Currency of Democratic Citizenship.” Journal of Politics
62(3): 790-816.
Levendusky, Matthew. 2010. “Clearer Cues, More Consistent Voters: A Benefit of Elite
Polarization.” Political Behavior 32(1): 111-131.
Lippmann, Walter. 1922. Public Opinion. Harcourt, Brace & Co.
Lord, Charles G., Lee Ross and Mark R. Lepper. 1979. “Biased Assimilation and Attitude Polarization: The E↵ects of Prior Theories on Subsequently Considered Evidence.”
Journal of Personality and Social Psychology 37(11): 2098-2109.
Lodge, Milton and Charles S. Taber. 2013. The Rationalizing Voter. Cambridge University
Press.
Mason, Lilliana. 2018. Uncivil Agreement: How Politics Became Our Identity. University
of Chicago Press.
McGrath, Mary C. 2017. “Economic Behavior and the Partisan Perceptual Screen.” Quarterly Journal of Political Science 11(4): 363-383.
McGuire, William J. 1985. “Attitudes and Attitude Change.” in The Handbook of Social
Psychology (Eds. G. LIndzey and E. Aronson) Random House. pp 233-346.
Mian, Atif, Amir Sufi and Nasim Khoshkhou. 2017. “Partisan Bias, Econoimc Expec33

tations, and Household Spending.” Working Paper. https://faculty.chicagobooth.
edu/amir.sufi/research/papers/miansufikhoshkhou_partisanbias.pdf Last Accessed:
April 13, 2020.
Miller, Joanne M., Kyle L. Saunders and Christina E. Farhart. 2016. “Conspiracy Endorsement as Motivated Reasoning: The Moderating Roles of Political Knowledge and Trust.”
American Journal of Political Science 60(4): 824-844.
Montgomery, Jacob M., Brendan Nyhan and Michelle Torres. 2018. “How Conditioning
on Posttreatment Variables Can Ruin Your Experiment and What To Do About It.”
American Journal of Political Science 62(3): 760-775.
Mummolo, Jonathan and Erik Peterson. 2019. “Demand E↵ects in Survey Experiments:
An Empirical Assessment.” American Political Science Review 113(2): 517-529.
Nyhan, Brendan. 2010. “Why the ‘death panel’ myth won’t die.” The Forum 8(1).
Pasek, Josh, Tobias H. Stark, Jon A. Krosnick and Trevor Tompson. 2015. “What Motivates
a Conspiracy Theory? Birther Beliefs, Partisanship, Liberal-Conservative Ideology, and
anti-Black Attitudes.” Electoral Studies 40: 482-489.
Peterson, Erik, Sharad Goel and Shanto Iyengar. ND. “Partisan Selective Exposure in Online
News Consumption: Evidence from the 2016 Presidential Campaign.” Political Science
Research & Methods Forthcoming.
Porter, Ethan and Thomas J. Wood. 2019. False Alarm: The Truth about Political Mistruths
in the Trump Era. Cambridge University Press.
Prior, Markus. 2013. “Media and Political Polarization.” Annual Review of Political Science
16: 101-127.
Prior, Markus. 2009. “Improving Media E↵ects Research through Better Measurement of
News Exposure” Journal of Politics 71(3): 893-908.
Prior, Markus and Arthur Lupia. 2008. “Money, Time, and Political Knowledge: Distinguishing Quick Recall and Political Learning Skills.” American Journal of Political
34

Science 52(1), 169–183.
Prior, Markus, Gaurav Sood, and Kabir Khanna. 2015. “You Cannot be Serious: The
Impact of Accuracy Incentives on Partisan Bias in Reports of Economic Perceptions.”
Quarterly Journal of Political Science 10(4): 489-518.
Roush, Carolyn and Gaurav Sood. 2019. “A Gap in Our Understanding? Reconsidering
the Evidence for Partisan Knowledge Gaps.” Working Paper. https://www.gsood.com/
research/papers/partisan_gap.pdf Last Accessed: April 13, 2020.
Scha↵ner, Brian F. and Samantha Luks. 2018. “Misinformation or Expressive Responding?
What an inauguration crowd can tell us about the source of political misinformation in
surveys.” Public Opinion Quarterly 82(1): 135-147.
Scha↵ner, Brian F. and Cameron Roche. 2017. “Misinformation and Motivated Reasoning:
Responses to Economic News in a Politicized Environment.” Public Opinion Quarterly
81(1): 86-110.
Shapiro, Robert and and Yaeli Bloch-Elkon. 2008. “Do The Facts Speak For Themselves?
Partisan Disagreement as a Challenge to Democratic Competence.” Critical Review 20(12): 115-139.
Stroud, Natalie Jomini. 2011. Niche News. Oxford University Press.
Swire, Briony, Adam J. Berinsky, Stephan Lewandowsky and Ullrich Ecker. 2017. “Processing Political Misinformation: Comprehending the Trump Phenomenon.” Royal Society
Open Science 4(3): 160802.
Thorson, Emily. 2016. “Belief Echoes: The Persistent E↵ects of Corrected Misinformation.”
Political Communication 33(3): 460-480.
Vavreck, Lynn and Shanto Iyengar. 2011. “The Future of Political Communication Research:
Online Panels and Experimentation.” in Oxford Handbook of American Public Opinion
and Media (Eds. George C. Edwards III, Robert Shapiro and Lawrence Jacobs.)
Wicker, Allan W. 1969. “Attitudes Versus Actions: The Relationship of Verbal and Overt
35

Behavioral Responses to Attitude Objects.” Journal of Social Issues 25(4): 41-78.

36

Supporting Information for:
“Partisan Gaps in Political Information and
Information-Seeking Behavior: Motivated
Reasoning or Cheerleading?”

Contents
A Appendix A: Survey Information

1

A.1 Survey Sample Descriptives . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

A.2 Information Sources by Topic . . . . . . . . . . . . . . . . . . . . . . . . . .

2

A.3 Format of News Choice Tasks . . . . . . . . . . . . . . . . . . . . . . . . . .

3

B Appendix B: Supplemental Experimental Analyses

4

B.1 Partisan Di↵erences by Item and Incentive Condition . . . . . . . . . . . . .

4

B.2 Partisan Di↵erences in Unincentivized Conditions . . . . . . . . . . . . . . .

6

B.3 E↵ects of Incentives Using Sampling Weights . . . . . . . . . . . . . . . . . .

7

B.4 Responsiveness of Independents to Incentives . . . . . . . . . . . . . . . . . .

8

B.5 E↵ects of Incentives on News Choice . . . . . . . . . . . . . . . . . . . . . .

9

B.6 Distribution of Response Certainty by Item

. . . . . . . . . . . . . . . . . .

11

B.7 Incorporating Response Certainty . . . . . . . . . . . . . . . . . . . . . . . .

12

B.8 E↵ect of Incentives Among Low-Certainty Respondents . . . . . . . . . . . .

12

B.9 Isolating Confidently Held Misinformation . . . . . . . . . . . . . . . . . . .

13

B.10 Incorporating Response Direction and Certainty . . . . . . . . . . . . . . . .

14

C Appendix C: Web Tracking Data

16

C.1 Web Traffic Sample Descriptives . . . . . . . . . . . . . . . . . . . . . . . . .

16

C.2 Alternative Coding of Survey-Based News Choice . . . . . . . . . . . . . . .

17

C.3 Explaining Survey-Based News Choice . . . . . . . . . . . . . . . . . . . . .

20

A
A.1

Appendix A: Survey Information
Survey Sample Descriptives

Respondents were recruited using YouGov’s standard approach of matching a non-representative
online sample to a nationally representative target. This has been utilized in a variety of
public opinion studies, including the Cooperative Congressional Election Study.

The table below displays the unweighted demographic composition of the pooled sample
(unweighted, as in the main text), survey 1 sample (weighted) and the survey 2 sample
(weighted). Because there was a panel structure to the survey data, in which 954 respondents were re-interviewed and appeared in both, the total number of unique respondents is
less than the sum of respondents across the two survey waves. As we present the pooled,
unweighted results in the main text we do not incorporate them in this analysis either, although we note our results remain similar when we reanalyze the experiments in each survey
individually using the sampling weights provided by YouGov (See Appendix B)

1

Table A1: Sample Demographics
Study
Black
Hispanic
White
Other Race
College Plus
Female
Age
Income ($)
Democrat
Republican
Independent
Sample Size

CCES
Pooled
Survey 1
(Weighted) (Unweighted) (Weighted)
0.12
0.07
0.11
0.07
0.07
0.09
0.73
0.79
0.73
0.07
0.08
0.07
0.26
0.39
0.38
0.52
0.53
0.54
47.1
54.38
49.84
64581.65
71430.70
69622.90
0.46
0.49
0.51
0.36
0.37
0.35
0.18
0.14
0.14
64600
1852
1017

Survey 2
(Weighted)
0.12
0.15
0.65
0.08
0.30
0.52
47.45
65729.42
0.49
0.37
0.14
1754

As a point of reference for the survey demographics, we also display the same information
for the 2016 CCES. The unweighted sample composition analyzed in our study is generally
more educated and more white than the CCES sample.

A.2

Information Sources by Topic

The table below contains the information sources that were available prior to each information question. Subjects saw the source name and headline for each articles as they made
a selection of which article to read. For information items included in both the first and
second surveys, the same sources/articles were available in each.
Table A2: News Sources by Topic
Climate Change
Cohen Plea
Firearm Sales
Immigrant Crime
Obama Wiretap
Voter Fraud
Unemployment

Left Partisan
Hu↵Po
Hu↵Po
Hu↵Po
Hu↵Po
Hu↵Po
Hu↵Po
MSNBC

Mainstream 1
CNN
CNN
CNN
CNN
CNN
CNN
CNN

Mainstream 2
NY Times
NY Times
NY Times
NY Times
NY Times
NY Times
NY Times

2

Expert
National Academy of Science
New York Law Journal
American Academy of Pediatrics
National Academy of Science
Department of Justice
White House Press Office
Bureau of Labor Statistics

Right Partisan
Breitbart
Breitbart
Fox
Breitbart
Fox
Fox
Fox

A.3

Format of News Choice Tasks

When making each news selection, respondents saw a list of news sources and a headline
for each article. The figure below provides an example of the articles available for the voter
fraud news selection task.
Figure A1: News Choice Task on Voter Fraud Topic

These articles were all based on content that had originally appeared from the source it was
attributed to in the survey. This was done to ensure each article could be realistically viewed
as coming from that outlet and to ensure the choice set in the study reflected the real-world
array of news coverage available on that topic from media outlets with di↵ering ideological
reputations. The articles that respondents encountered in the survey were edited to ensure
they were of similar length (roughly 250-300 words) for each information source.

3

B
B.1

Appendix B: Supplemental Experimental Analyses
Partisan Di↵erences by Item and Incentive Condition

Table B1 displays the share correctly answering each item among Democrats and Republicans by treatment condition (Unincentivized or Incentivized). This table also displays the
baseline di↵erence between Democrats and Republicans in the unincentivized conditions, the
between-party di↵erence in the incentivized condition and the di↵erence in party di↵erences
between the two conditions.

We note that when we isolate the unemployment items, our findings resemble earlier studies.
Here modest initial between-party di↵erences are substantially reduced by the availability of
incentives. In contrast, our findings depart from previous work when focusing on the new
items introduced into our surveys, where substantial divides remain even when incentives are
available. This points to these new survey items, rather than di↵erences in how respondents
in these surveys respond to incentives, as the factor that distinguishes the present study
from earlier research using the incentive design.

4

Table B1: Partisan Di↵erences by Item and Incentive Condition
Topic

5

1
2
3
4
5
6
7
8
9
10

Climate Change (Survey 1)
Cohen Plea (Survey 2)
Firearm Sales (Survey 2)
Immigrant Crime (Survey 1)
Obama Wiretap (Survey 1)
Obama Wiretap (Survey 2)
Unemployment (Survey 1)
Unemployment (Survey 2)
Voter Fraud (Survey 1)
Voter Fraud (Survey 2)

Dem Correct
(No Incentive)
0.96
0.93
0.22
0.97
0.98
0.98
0.74
0.72
0.92
0.86

Rep Correct
(No Incentive)
0.63
0.80
0.56
0.45
0.58
0.53
0.85
0.80
0.49
0.38

Baseline
Di↵erence
0.33
0.13
-0.34
0.52
0.40
0.45
-0.11
-0.08
0.43
0.48

Dem Correct Rep Correct
(Incentive)
(Incentive)
0.95
0.69
0.89
0.80
0.24
0.49
0.96
0.59
0.98
0.75
0.97
0.73
0.84
0.88
0.84
0.85
0.94
0.55
0.88
0.53

Incentive
Di↵erence
0.26
0.09
-0.25
0.37
0.23
0.24
-0.04
-0.01
0.39
0.35

Di↵erence
in Di↵erence
0.07
0.04
-0.09
0.15
0.17
0.21
-0.07
-0.07
0.04
0.13

B.2

Partisan Di↵erences in Unincentivized Conditions

Our finding that partisan gaps remain even when incentives for correct answers are available
depends, in part, on the substantial divides in the unincentivized conditions. Here we assess
whether these baseline gaps are inflated by unrepresentative aspects of our sample.

In Table B2 we consider the magnitude of the partisan divides in the unincentivized conditions of our survey when we incorporate the sampling weights provided by YouGov. For
survey 1 we use post-stratification weights for the control group of the study (we received
overall weights as well as separate weights for those in the incentived and unincentivized
conditions from YouGov). For survey 2 we use the weights designed for the entire sample,
but only apply them to the control group.
Table B2: Partisan Di↵erences in Unincentivized Survey Conditions
1
2
3
4
5
6
7
8
9
10

Topic
Immigrant Crime (Survey 1)
Voter Fraud (Survey 2)
Obama Wiretap (Survey 2)
Voter Fraud (Survey 1)
Obama Wiretap (Survey 1)
Firearm Sales (Survey 2)
Climate Change (Survey 1)
Cohen Plea (Survey 2)
Unemployment (Survey 1)
Unemployment (Survey 2)

Unweighted Weighted
0.52
0.51
0.48
0.39
0.45
0.44
0.43
0.42
0.40
0.40
0.34
0.34
0.33
0.29
0.13
0.12
0.11
0.16
0.08
0.12

In both surveys there are limited di↵erences in the partisan divide in the unincentivized
conditions when incorporating these weights. This suggests demographics do not inflate the
baseline levels of partisan division in our experiments.

Substantial partisan gaps also exist when similar items are asked in other surveys. The 2018
ANES pilot asked respondents “Does illegal immigration increase, decrease or have no e↵ect
6

on the crime rate?” 89% of Republicans indicated they believed illegal immigration increased
crime, compared to only 30% of Democrats. The ANES pilot also asked respondents whether
the world’s temperature had increased over the past 100 years. 93% of Democrats indicated
this “has probably been happening.” Only 56% of Republicans did. Thus, the partisan gaps
revealed in the unincentivized conditions of our surveys are not outliers.

B.3

E↵ects of Incentives Using Sampling Weights

Table B3 examines the e↵ects of incentives on partisan divides in political information in
each survey individually, rather than the pooled analysis discussed in the main text. For
this analysis, we also incorporate sampling weights provided by YouGov that correct for any
imbalances between our realized sample and the initial demographic targets.

This analysis shows a largely similar pattern of findings across the two surveys as we find in
our pooled analysis in the main text. The initial partisan divide over these items is slightly
smaller in the second survey relative to the first. In both cases, the magnitude of the e↵ect
of incentives on reducing the initial partisan divide over misinformation is similar.

7

Table B3: E↵ect of Incentives on Partisan Divides over Factual Evidence (By Survey)
Survey 1 Survey 2
(Intercept)
0.64⇤
0.70⇤
(0.03)
(0.02)
Democrat
0.34⇤
0.28⇤
(0.03)
(0.02)
Incentive Treatment
0.09⇤
0.07⇤
(0.03)
(0.02)
⇤
Democrat ⇥ Incentive Treatment
0.12
0.12⇤
(0.04)
(0.03)
N
4245
7516
Robust standard errors, clustered by respondent, in parentheses
Models include item fixed e↵ects and incorporate sampling weights
⇤
indicates significance at p < 0.05

B.4

Responsiveness of Independents to Incentives

Our analysis in the main text does not consider “pure” independents who do not lean toward
either political party. This is because we lack expectations about the directional factors that
will influence them to respond with misinformation on any given item. Table B4 displays
the share of independents o↵ering correct answers to each item. The share of independents
answering correctly falls in between those answering correctly among Democrats and Republicans. This pattern is expected, as independents lack the directional pressures, either
to engage in motivated reasoning or cheerlead for their party, that face partisan respondents
across the set of items focused on here.

8

Table B4: Correct Answers Among Political Independents
1
2
3
4
5
6
7
8
9
10

Topic
Climate Change (Survey 1)
Cohen Plea (Survey 2)
Firearm Sales (Survey 2)
Immigrant Crime (Survey 1)
Obama Wiretap (Survey 1)
Obama Wiretap (Survey 2)
Unemployment (Survey 1)
Unemployment (Survey 2)
Voter Fraud (Survey 1)
Voter Fraud (Survey 2)

Share Correct (Independents)
0.81
0.87
0.36
0.72
0.78
0.78
0.79
0.76
0.63
0.62

Table B5 shows the e↵ect of incentives on correct answers to the information questions among
Democrats, Independents and Republicans. When the outcome variable is operationalized
in this way, the point estimate for this group falls in between those for Democrats and
Republicans, although it does not reach statistical significance.
Table B5: E↵ect of Incentives on Correct Answers
Democrats Independents Republicans
(Intercept)
0.95⇤
0.79⇤
0.62⇤
(0.01)
(0.04)
(0.03)
Incentive Treatment
0.02⇤
0.04
0.07⇤
(0.01)
(0.02)
(0.01)
N
6767
1917
4994
Robust standard errors in parentheses
Models include item fixed e↵ects
⇤
indicates significance at p < 0.05

B.5

E↵ects of Incentives on News Choice

We also consider the e↵ects of incentives on news choice among several di↵erent sub-groups
of the sample. Table B6 examines the e↵ects of incentives on the outcomes we consider in
the main text, but only among Democrats.

9

Table B6: E↵ect of Incentives on Information Source Selection (Democrats Only)
Co-Partisan News Expert Mainstream News Out-Partisan News
0.13⇤
0.46⇤
0.36⇤
0.05⇤
(0.02)
(0.02)
(0.02)
(0.01)
Incentive Treatment
0.02
0.00
0.02
0.00
(0.01)
(0.02)
(0.02)
(0.01)
N
6807
6807
6807
6807
(Intercept)

Robust standard errors, clustered by Respondent, in parentheses

Table B7 replicates this analysis for Republicans, again finding no substantively significant
e↵ect of incentives on news choice.
Table B7: E↵ect of Incentives on Information Source Selection (Republicans Only)
Co-Partisan News Expert Mainstream News Out-Partisan News
0.36⇤
0.29⇤
0.28⇤
0.07⇤
(0.03)
(0.03)
(0.03)
(0.01)
Incentive Treatment
0.02
0.00
0.01
0.00
(0.02)
(0.02)
(0.02)
(0.01)
N
5014
5014
5014
5014
(Intercept)

Robust standard errors, clustered by Respondent, in parentheses

Table B8 examines the e↵ects, limiting the data to the first study and employing the sampling
weights provided by YouGov. It is important to note that in this subset of the data and
approach to analysis we observe a decline in co-partisan news choice selection and an increase
in the probability of selecting neutral news sources. This appears to be, in part, because
including the sampling weights increases the amount of co-partisan source selection in the
control group. We note this result does not hold in any of the other pooled or subset analyses
we consider (or if we estimate the e↵ects of these features in Study 1 without including
sampling weights) and that, even granting a reduction in partisan selective exposure due to
incentives, we still observe elevated levels of selectivity even in the conditions where incentives
are available, a similar pattern of results to what we see with the information items.

10

Table B8: E↵ect of Incentives on Information Source Selection (Survey 1 Only)
Co-Partisan News Expert Mainstream News Out-Partisan News
0.41⇤
0.33⇤
0.18⇤
0.08⇤
(0.03)
(0.03)
(0.03)
(0.02)
Incentive Treatment
0.06⇤
0.06⇤
0.00
0.00
(0.03)
(0.03)
(0.03)
(0.01)
N
4336
4336
4336
4336
(Intercept)

Robust standard errors, clustered by Respondent, in parentheses

Table B9 examines the e↵ects of incentives, this time using only the second study and
incorporating sampling weights. Here, as with the pooled analysis, we do not observe a
significant or substantively large e↵ect of incentives on news selection.
Table B9: E↵ect of Incentives on Information Source Selection (Survey 2 Only)
Co-Partisan News Expert Mainstream News Out-Partisan News
0.38⇤
0.17⇤
0.37⇤
0.09⇤
(0.02)
(0.02)
(0.03)
(0.01)
Incentive Treatment
0.02
0.02
0.01
0.01
(0.02)
(0.02)
(0.02)
(0.01)
N
7485
7485
7485
7485
(Intercept)

Robust standard errors, clustered by Respondent, in parentheses

B.6

Distribution of Response Certainty by Item

We incorporate measures of response certainty into some of our analyses. Table B8 displays
the share of respondents who are “very” or “extremely” certain of their answer across the
di↵erent items. In general, respondents expressed a relatively high degree of certainty in
their answers to the information questions. We suspect this is due to the highly partisan
nature of these questions.

11

Table B10: Response Certainty by Item
1
2
3
4
5
6
7
8
9
10

B.7

Topic
Immigrant Crime (Survey 1)
Climate Change (Survey 1)
Voter Fraud (Survey 1)
Unemployment (Survey 1)
Obama Wiretap (Survey 1)
Obama Wiretap (Survey 2)
Unemployment (Survey 1)
Unemployment (Survey 2)
Cohen Plea (Survey 2)
Firearm Sales (Survey 2)

Share Certain
0.81
0.78
0.77
0.75
0.73
0.71
0.68
0.67
0.62
0.38

Incorporating Response Certainty

Our analysis that subsets to only those respondents who are “certain” of their answer in
the main text is potentially subject to post-treatment bias if the incentive treatments a↵ect
both the direction and certainty of responses to the political information questions. Below
we consider the e↵ects of incentives on response certainty and do not observe a detectable
e↵ects of incentives on this measure.
Table B11: E↵ect of Incentives on Response Certainty
All Partisans
(Intercept)
0.70⇤
(0.02)
Incentive Treatment
0.02
(0.01)
N
11761
Robust standard errors in parentheses
⇤
indicates significance at p < 0.05

B.8

E↵ect of Incentives Among Low-Certainty Respondents

To complement the analysis in the main text, here we consider the e↵ects of incentives among
respondents that expressed a low level of certainty in their response to the information ques12

tions. This is a companion table to the one that features high-certainty respondents in the
main text (Table 4).

Table B12: E↵ect of Incentives on Partisan Information Divide (Uncertain Respondents)
All Partisans Strong Partisans Dem-Valenced Rep-Valenced
0.72⇤
0.71⇤
0.56⇤
0.71⇤
(0.03)
(0.05)
(0.03)
(0.04)
Democrat
0.16⇤
0.19⇤
0.16⇤
0.15⇤
(0.02)
(0.03)
(0.03)
(0.03)
Incentive Treatment
0.06⇤
0.07⇤
0.03
0.08⇤
(0.02)
(0.03)
(0.04)
(0.03)
Democrat ⇥ Incentive Treatment
0.09⇤
0.10⇤
0.07
0.10⇤
(0.03)
(0.04)
(0.05)
(0.04)
N
3707
1715
1604
2103
(Intercept)

Robust standard errors in parentheses
⇤
indicates significance at p < 0.05

In the conditions in which incentives are available, a partisan divide of 0.07 (95% Confidence
Interval [.02, 11]) remains. This is roughly 45% of the initial partisan gap among this group
of low-certainty respondents.

B.9

Isolating Confidently Held Misinformation

Below we estimate a di↵erent model specification below to address the question of how
large the partisan divides are among those who hold misinformed views in a relatively certain manner. In this specification we reassess the partisan divide using the same approach
to constructing our outcome measure as in the main text (i.e., the congenial answer for
Democrats always received a higher value), but with a more stringent classification of the
“misinformed.” For this analysis we define misinformation as the combination of an incorrect
response and a high degree of certainty in their response (i.e., partisans who state they are
“very” or “extremely” sure of their answer, the top two categories on the five-point certainty
scale). If respondents did not meet this category, they were categorized as correct on an
issue. This modified outcome measure considers the divide between those who are strongly
13

misinformed and the all other respondents.

Table B13: E↵ect of Incentives on Partisan Information Divide (Isolating Confident and
Misinformed Responses)
All Partisans Strong Partisans Dem-Valenced Rep-Valenced
0.76⇤
0.72⇤
0.21⇤
0.72⇤
(0.02)
(0.02)
(0.01)
(0.02)
Democrat
0.19⇤
0.24⇤
0.08⇤
0.25⇤
(0.01)
(0.02)
(0.02)
(0.02)
Incentive Treatment
0.04⇤
0.05⇤
0.00
0.07⇤
(0.01)
(0.02)
(0.02)
(0.02)
Democrat ⇥ Incentive Treatment
0.06⇤
0.07⇤
0.05⇤
0.07⇤
(0.01)
(0.02)
(0.02)
(0.02)
N
11761
6197
3844
7917
(Intercept)

Robust standard errors, clustered by respondent, in parentheses
Models include item fixed e↵ects
⇤
indicates significance at p < 0.05

These results are displayed in Table B13, using the same model specification estimated among
all partisan respondents (Column 1) and di↵erent subsets of the data (Columns 2-4). While
the initial magnitude of these partisan divides is smaller across the various specifications in
this more stringent coding, the pattern is largely consistent with our earlier analysis. While
the availability of incentives reduces partisan gaps on the information items, the divides
remain in the presence of incentives. In the pooled analysis that incorporates all items and
partisan respondents, the initial gap between partisans shrinks to roughly two-thirds its
initial size when respondents receive incentives. This remaining gap of .13 (95% Confidence
Interval [0.11, 0.15]) is both substantial and statistically significant.

B.10

Incorporating Response Direction and Certainty

We also consider these divides using a more granular outcome that incorporates both the
direction and certainty with which an individual holds their beliefs. More specifically, we
create a 10-pt scale by coding responses from 1 (a confidently-held incorrect response) to 10
14

(a confidently-held correct response) with the intermediate categories representing more or
less confident responses. We then estimate the same set of models included in the main text
with this alternative outcome.
Table B14: E↵ect of Incentives on Partisan Information Divide (10-pt DV Incorporating
Certainty)
All Partisans Strong Partisans Dem-Valenced Rep-Valenced
6.41⇤
6.06⇤
5.71⇤
5.87⇤
(0.12)
(0.16)
(0.12)
(0.14)
Partisanship
2.78⇤
3.36⇤
1.33⇤
3.50⇤
(0.09)
(0.12)
(0.14)
(0.11)
Incentive Treatment
0.56⇤
0.65⇤
0.04
0.86⇤
(0.11)
(0.16)
(0.15)
(0.14)
Partisanship ⇥ Incentive Treatment
0.75⇤
0.89⇤
0.54⇤
0.87⇤
(0.12)
(0.17)
(0.19)
(0.16)
N
11754
6192
3843
7911
(Intercept)

Robust standard errors, clustered by respondent, in parentheses
Models include item fixed e↵ects
⇤
indicates significance at p < 0.05

Using this alternative outcome, we observe a similar pattern to the findings reported in the
main text. The initial partisan gap in the unincentivized survey conditions is reduced by
27% of its initial size when incentives are made available. A partisan gap of 2.03 points (95%
Confidence Interval [1.86, 2.20]) on this 10-pt outcome measure remains in the conditions
where incentives are made available.

15

C

Appendix C: Web Tracking Data

C.1

Web Traffic Sample Descriptives

We observe web tracking data for 46% of our pooled sample across the two surveys. These
survey respondents had all previously agreed to install an online toolbar, from Wakoopa, on
their primary desktop web browser. When active, this toolbar tracked the individual urls
they encountered while browsing the internet. Respondents were able to turn the toolbar on
and o↵ as they wished, but received incentives for keeping it active. We use counts of the
number of times they visited 452 news websites that were assigned an alignment score by
Bakshy et al. (2015) and average the visit-weighted alignment score of all of a respondent’s
web traffic in the months preceding the survey (the window of time we observe their web
traffic varies by participant based on how long they remained active in using the Wakoopa
toolbar) to characterize their news choices outside of the survey context.

Table C1 compares the demographics of our overall sample to only those respondents where
we observe web traffic. Because we focus on a within-subject comparison of news choices
inside the surveys with web browsing behavior outside the surveys, we do not weight these
results.

16

Table C1: Comparing Pooled Survey Sample and Web Traffic Sample
Study
Black
Hispanic
White
Other Race
College Plus
Female
Age
Income ($)
Democrat
Republican
Independent
Sample Size

Pooled
Web Traffic
(Unweighted) (Unweighted)
0.07
0.06
0.07
0.05
0.79
0.82
0.08
0.07
0.39
0.43
0.53
0.50
54.38
58.53
71430.70
71211.93
0.49
0.51
0.37
0.37
0.14
0.13
1852
855

This comparison shows that those respondents for which we observe web traffic are largely
similar to the demographic compositon of our overall pooled sample across the surveys.

C.2

Alternative Coding of Survey-Based News Choice

In the comparison between survey-based and real-world news choice presented in the main
text, we consider visits to CNN and the New York Times to be neutral, from the perspective
of partisans, by giving them a similar code to the expert sources. This assumption can be
contested in contemporary politics where the partisanship of the audience that these websites receive is not dissimilar from more clearly partisan sources of information such as the
Huffington Post.

For this reason, we now consider two alternative approaches to characterizing the partisanship of a respondent’s news choices in the surveys. Under Alternative 1, we consider visits
to the left-leaning partisan source OR the mainstream news sources to be the same (-1),
code visits to the expert source as neutral (0), and visits to the right-leaning source the

17

same as in the analysis in the main text (1). The key distinction between this and the
findings in the main text is that CNN and the New York Times are now characterized as
left-leaning sources. Under Alternative 2, we rely on the same alignment scores from Bakshy
et al. (2015) that we used to characterize online news domains to measure the partisanship
of survey-based news choices. For example, this means that individuals selecting Fox News
receive a score of 0.78 for that choice, those selecting MSNBC receive a score of -0.81 and
those selecting CNN receive a score of -0.27. Alignment scores for the expert sources are
unavailable in this data, so we code them as 0.

When we examine the relationship between media choice in the survey and behavior outside
it, the correspondence remains similar using these alternative outcome measures (r = 0.60
for Alternative 1 and r = 0.63 for Alternative 2).

Figure C1 displays the relationship between news choice online and in the survey using these
two alternatives. As in the analysis presented in the main text, and even with this change in
how media choice in the surveys is coded, there is a relatively high correspondence between
the partisanship of an individual’s news selections in the two settings.

18

Figure C1: Partisan Slant of Survey and Real-World News Choice (Alternative Outcomes)

0.0

●

● ●

●

●
● ●
●● ● ● ●
●●●
● ●
●
●
●
●●●●
●
●●
●●
●
●●● ● ●●
● ●●●
●
●●
● ●●● ●●● ●
●● ● ●

−1.0
−1.0

●
●

●
●●
●
●
●●
●●●
●●
● ●●
●
●
●
●
●
●
●●
●
●● ●● ●●
●● ●
●●●
●●
●
●
●●
●●
●●●
●●●●●●
●●
●
●●
●● ●
●●●●●●●
●●●
●●●
●●
●
●●
●● ●
● ● ●
●
●●●●
●●
●● ●
●●
●● ●
●
●
●●●
●●
●●
●
● ●
●
●●●● ●
●
● ● ●●●

●●
●
●
●
●
● ●●●
●
●
● ●● ●
●●● ●
●
●
●
●●●● ●
●
●
●
●●
●
●
●
●●
● ●●
●●
●●
●
●●●
●●
● ●
●● ●●●●
●
●● ●●●●
●●●
●
●●●●●
●
●
●
●
●●
●●
●
●
●●
●
●●
●●
● ●●
●
●
●
●●●
●
●●
●●
●●●●● ●
●
●
●●●
●
●
●
●
●
●
●
●●●●
●
●
●

● ●●● ●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●●●●
●
●●●●●●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●●●
●
● ●●
●
●
●
●
●
●●
●
●
●
●
●
●●●
●
●
●
●●
●
●
●
●
●
●
●
●
●●●●●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●●
●●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●●
●●●
●

−0.5

●●
●●

●●
●

1.0
●

● ● ●
●
● ●● ● ● ●
● ●
●
● ● ●
● ●

●

●

●
●

●
●

●

0.5

●●

●
●●●● ●
●●
●●● ● ●●●
●
●
●
●●
●●
●
●●
●
●
●●
●●●
●● ●● ●
●●●●
●● ●
●
●
●
●
● ●● ●
●
●
●●
●
●●● ● ● ●●●
●●● ●●●●
● ●●●
●
●●●
●
●●●

●●
● ●

● ●
●
● ●
● ●●
●
●●
●●
●●●●●
●
● ●●●● ●
●●
●●●●●
●●
●●●●●
●
●
●
●
● ● ● ●●
●
●
●
●
●
●
●●●
● ●
●● ● ●
●
●●
●● ● ● ●
●
●
●●
●●
●●●
●●● ●
●
●
●
●
●
●
●
●
●● ● ●
●
●
●
●
●
●●● ●● ●●
● ●●
●
● ●●●
● ●
● ●
●
●
●
●●
●●●●
●●
●
●
●
●
●
●
●
●
●
●● ●●●
●
●
●●●
● ●● ●
●
●
●
● ●●
●●● ●● ●
●● ●
● ●●● ●
●
● ● ●
● ● ●
●
●
● ●●
●
● ●●
●● ●
●● ●●●●● ●
● ●●
●
●
●●●
● ●● ●
●●● ●
●
●
● ● ●●
● ●
●
●●●●● ● ● ● ●● ●
●
●
●
●
●
●
● ● ●
●●
●
●
●
● ●●●●●
●
●
●●●
●●●
●●●
●
●●●
●
●● ● ●
● ●●●
●
●
● ●
●
● ● ● ● ●● ●
● ●●
●● ●
● ●
● ●●●
●●
●● ●
●
●
●● ●
●
●● ●●
●
● ● ●●
●
●●● ● ● ● ●
● ● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●●
●●
● ●●
● ●
●● ● ●
●●
●
●●
●● ● ●
● ●● ● ● ●
●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
● ●●●
●●●
●
●
●●
●●● ●
●●●
●
●
●
●
●●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●● ●●
●●
●●
●
●●
●●
●●
●●●●
●
●
● ●● ●●●●●●
●●
●
●●
●●
●
●● ●
●
●●
●●
●●
●●●
●
●
● ●
●
●●
●
●
●
●●●
●
●●
●
●
●●
●●
●●
●●●●●
●●
● ●
●
●
● ● ● ● ●●
●
●●
●
●●
●
●
●
●
●
● ●●
●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
● ●● ●
●●●
●●
●
●● ●
●●●
●
●●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
● ●● ●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ●
●
●● ●●● ●●
●● ●● ●●
●
●● ●
●●
●
●
●
●
●●●
●
●
●
●●
●●●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
● ●●
●
●
●
●
●●●● ● ●
●
●
●
●
●
●● ●●●
● ● ● ●●●●●●●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●
●●
●
●
●
●
●●
●●
●
●●
●
●
●
●●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
● ●● ●●
●●
●●
●●
●
●
●
●
●
●
●●
●● ●
●
●
●●●
●
●
●
●
●●
● ●
●
●
●
●
●
●
●●●●
●●
●●
●
●
●
●●●
●
●
●
●●
●
●●●
●
●
●
●●
●
●
●●
●
●
●●
●
●
●
● ●●●
●
●●●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●●
●
●●●
●
●
●
●●● ●● ●
●●●●●
●
●●
●
●
●●
●
●●
●
●
●
●
●●
●●
●
●
●
●
●
● ●
● ●
●● ●
●●●
●
●
●
●
●●
●●●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●●● ●●●
●
●
●
●
●●●●
●●●
●
● ●●● ●
●
●●●●
●●●
●●
●
●●
●●●●●
●
●●●
●●
●
●
●
●●
●●
● ●●●●
●
●●
●
●
●
●
●
● ● ● ●●
●
●●●
●
●● ●
●
●
●●
● ●
●
● ●
●
●●●
●
●●
●●
●●
●●
●
●
●●●
●
●
●●
●●
●● ● ●●●
●●●●
●
●●● ●● ● ●
● ●●
● ●●
●
●
●●
●
●●
●
●●
●●●
●●●
●
●
●
●
●
●●
●
●
●
●●
●
●
●●
●
●●
●
●
●● ● ●●●●
●
●●●●
●●
●
●
●
● ●●
● ●
●●
●●●
●●
●
●
● ●
●●
●●●
●
●
●●●● ●●
●
●
●●
●● ●
●● ●●
●
● ●
●
●●
●●
● ●●
●

●

●

●●

0.0

●
●

● ●
● ● ●
●
●
●●
●● ●
● ●
●

●

●●●●●
●
●
●●●● ●● ●● ●
●
●●●●
●●
●
●
● ●● ●
●
●●●
● ●●
●●
●
●
●● ●
●
●
●●●●
●
●●
●
●
●●
●
●
●●
●
●
●
●●
●
●●
●
●●
●
●
●
●●●
●
●●
●
●●
●●●
●
●
●●● ●
●
●
●●
●
●
●●
●
●●
●
●●
●
●
●
●
●
●●
●
●
●●
●●
●●
●
●
● ●●●
●
●
●●
●●
●●●●
● ●
●●
●●
●
●
●
●

●
●

●
●●
●●●●
●
●●● ●
● ●●
● ●●
●

●● ● ● ●● ● ●●
●●
●● ●
●●● ● ● ● ● ●
●
●●
●●
● ●●●●
●
●●
●
●
●● ● ●
●●
● ●
●
●●
●
●
●
●
●● ●
●●
●●
●
●● ●●●
●● ● ●
●
●
● ●
● ●
●
● ●●●

●● ●
●
●
●●
●●
●●
● ●
●●●
●●
●
●●
●
●
●
●
●●●● ●
●
●
●
●
●
●
●●
●
●
●● ●●
●●
●
●●
●
●
●●
●●
●
●
●●
●●●● ●●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●●
●
●
●
●
●●
●
●
●●●
●
● ●● ●
●●
●
●
●
●
●
●
●
●
●
●●●
●
●●
●
●
●●
●
●
●●
●
●●●
●
●●●
●
●
●●●
●
●
●
●
●
●●
●●
●
●●●
●
●
●
●●
●

●●●

●● ●
●●
● ●
●
●
●
●●●

●
●
●●
●
●●
●
●
●
●
●● ●
●
●
● ●
●
●●●
●●
●●
●●●
●
●
●
●●●
●●
●●
● ●
●●
●
●
●●●
●
●
●●
●
● ●●
●● ●
●
●
●●
●
●● ●
●●●● ●
●
●● ● ●●
● ●
●

●
●
●●
●●
●●
●
●●
●●●
●●
●
●
●●
●
●
●
●●
●
●
●
●
● ● ● ●●
●
●● ●
●
●
●
●
●●●
●
●
●
●●●
●●●
●
●
●
●●
● ●
●●
●
●
●●●
●
●●
●●
●
● ●●
● ● ●●
●
●
●
●
●
●
●
●●
●●
●●●●
●
●
●
●●
●●
●
●
●
●
●
●
● ●●
●
●●
●
●●● ●
●●●
●
● ●●
● ●
●
●
●
●
●
●
●
●
● ●
●●
●
●
●●●
●●
●
●
●
●●●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ●●
● ● ●●
●●
●
● ●
● ●
●

●
●
●●

0.5

●

●●
●
●
●●
●
●
●● ●●
●
●● ●
● ●●
●●●
●
●
● ●
● ● ●
●● ●
●
●●
●

0.0

0.5

●
●●
● ●

−0.5

Partisanship of Media Diet (Survey)

● ●
●●
●
● ●

−1.0

●
●

Alternative 2

●●
●● ●
●
●● ●● ●
●●
●
● ●
●●
● ●
●
● ●●●
●
●●●
●
●
● ● ●●●●
●
●●●
●
● ● ●●
●
●●
●● ● ●
●
●●
●
● ●●
●
●●●
●●●●●
● ●●●●
●●●
●●
●●
●●
●
●
● ● ● ● ●●● ●

−0.5

●
●

Partisanship of Media Diet (Survey)

1.0

Alternative 1

1.0

−1.0

Partisanship of Media Diet (Behavior)

−0.5

0.0

0.5

1.0

Partisanship of Media Diet (Behavior)

Finally, we consider the partisan divide in news choice across these di↵erent settings using
the two alternative codings of survey-based news choice.
Table C2: Partisan Divides in Media Choice By Setting (Alternative DV Codings)
Setting
Survey + No Incentives
Survey + Incentives
Real-World Media Diet

Partisan Divide (Alt 1)
1.35 SD
1.36 SD
1.36 SD

Partisan Divide (Alt 2)
1.37 SD
1.39 SD
1.36 SD

As Table C2 indicates, the survey-based divides in news choice using these alternative approaches are similar in magnitude to those in the main text. Altogether, this analysis shows
the results in the main text are not an artifact of placing sources with ambiguous or contested
reputations into the neutral category.

19

C.3

Explaining Survey-Based News Choice

Below we supplement the plots relating the web-browsing behavior by regressing the partisanship of an individual’s survey-based media choices on the partisanship of their web-browsing
behavior and the partisanship they reported in the survey. For this we separate out “pure”
independents on the 7-pt scale and include leaners with the party they lean towards. The
available sample here (840) is slightly smaller than the entire web traffic sample because 15
respondents in the web traffic sample did not visit any of the political websites on our list,
meaning we are unable to categorize the partisan slant of their real-world political media
exposure.
Table C3: Explaining Partisanship of Survey-Based News Choice
(Intercept)
Partisanship of Online News Use
Independent
Republican
N

In-Text DV Alternative 1 Alternative 2
0.01
0.45⇤
0.17⇤
(0.02)
(0.03)
(0.02)
0.31⇤
0.47⇤
0.35⇤
(0.04)
(0.06)
(0.04)
0.16⇤
0.29⇤
0.19⇤
(0.04)
(0.05)
(0.03)
⇤
⇤
0.37
0.61
0.39⇤
(0.03)
(0.04)
(0.03)
840
840
840

Reference category for partisanship is “Democrat”
Standard errors in parentheses
⇤
indicates significance at p < 0.05

Here the partisanship of an individual’s online news use remains a strong predictor of the
partisanship of the choices they make inside the survey.

20

