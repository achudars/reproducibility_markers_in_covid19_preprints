Using Computer Algebra Systems together with lavaan
to generate symbolic model-implied covariance matrices
in SEM
Henrik Kenneth Andersen
25 September 2019
This article is meant as (brief, introductory) guide to using computer algebra systems to work out analytical
model-implied covariance matrices for structural equation models. To my knowledge there is no pre-packaged
software available for this task. The article is meant primarily for those with experience with structural
equation modeling as well as lavaan and ideally R in general.
Any questions or comments can be sent to henrik.andersen@soziologie.tu-chemnitz.de.

Introduction
A careful inspection of the model-implied covariance matrix should be considered an essential step in any
SEM-based regression model. Software default settings, cluttered or inaccurate path diagrams as well
as an over reliance on modification indices often hide important aspects of SEM-based regression models.
In complex models, researchers may not even be aware of the underlying algebra that is responsible for the
reported parameter estimates.
This brief article demonstrates the use of Computer Algebra Systems (CAS) such as Ryacas (Andersen
et al. 2019), an R interface with the software Yacas (“Yet Another Computer Algebra System”), along
with the popular Structural Equation Modeling (SEM) package lavaan (Rosseel 2012) to generate symbolic
representations of model-implied covariance matrices.
The model-implied covariance matrix can be derived using traditional methods such as the tracing rule, or
with covariance algebra. However, these methods are a) tedious, b) prone to errors, c) must be done by
hand and entered into the software of choice later. The method outlined here involves entering the model
matrices into Ryacas and letting the software come up with the entire symbolic covariance matrix in one
go. It saves time, is less error prone and can reveal details that may not be obvious (especially with more
complex models or ones with latent variables, e.g. fixed-effects regression in SEM).
The article is outlined as follows: first I simulate a simple mediation model with manifest indicators to be
used as the dataset in the later demonstration. Then I outline the model matrices used in popular SEM
software like lavaan and Mplus and derive the equation for the model-implied covariance matrix. I then
run the model as outlined in the simulation in lavaan and show how to double-check the model matrices.
Before demonstrating the CAS-based approach, I give an example of generating the model-implied covariance
between just two model variables algebraically. After doing so, I then introduce the CAS-based approach.
This involves introducing the model elements as symbols into Ryacas and entering in the model matrices.
Then I generate the entire symbolic model-implied covariance matrix and show that the procedure does in
fact give the proper results. Lastly, I show just one example of how a close inspection of the model-implied
covariance matrix can help researchers gain a better understanding of their SEM models. This applies
regardless of the method of generating the model-implied covariance matrix. The CAS-based approach can
be seen as another available tool towards this goal.

Set-up
The only packages needed are lavaan and Ryacas.

1

install.packages( "lavaan")
install.packages( "Ryacas")
library( "lavaan")
library( "Ryacas")
Let us simulate a simple model using tools available in base R (R Core Team 2017) to use as our dataset.
That way, the ‘true’ parameter values are known so that it will be possible to later double-check the work.
x1
x2
z1
y1
df

<<<<<-

rnorm( 10000, 0, 1)
0.1*x1 + rnorm( 10000, 0, 1)
0.3*x1 + 0.15*x2 + rnorm( 10000, 0, 1)
0.2*z1 + 0.05*x1 + 0.0*x2 + rnorm( 10000, 0, 1)
data.frame( x1, x2, z1, y1)

I chose a sample size of n = 10, 000 in order to minimize but not eliminate sampling error, and each of the
exogenous (independent) variables in the model (x1 , δ2 , υ1 and ε1 ) are N ∼ (0, 1).
The empirical covariance matrix can be viewed with
cov( df)
##
##
##
##
##

x1
x2
z1
y1

x1
0.98880443
0.09639345
0.30023840
0.10500371

x2
0.09639345
0.99990831
0.18509987
0.03891528

z1
0.3002384
0.1850999
1.1147841
0.2414665

y1
0.10500371
0.03891528
0.24146647
1.07471688

We can write equations for each of the model variables

x1 = δ 1
x2 = ϕ1 x1 + δ 2
z1 = γ1 x1 + γ2 x2 + υ1
y1 = β1 x1 + β2 z1 + ε1
where, from the code above, we know ϕ1 = 0.1, γ1 = 0.3, γ2 = 0.15, β1 = 0.05, β2 = 0.2. Notice we have
also written an equation for the only observed exogenous variable, x1 = δ1 , which basically states that the
entire variance is due to influences not contained in the model. This will be important to note below when
we specify the model matrices. The path diagram of our model is shown in Figure 1.

Model matrices
The SEM-matrix notation I use most closely mimics the one used in popular SEM software like lavaan and
Mplus. It has been outlined by (Bollen 1989; Graff 1979) The basic model is

η = Bη + ζ
y = Λη.
If m stands for the number of independent latent variables, n is the number of dependent latent variables, p is
the number of independent observed (manifest) variables and q is the number of dependent observed variables,
then the (m + n + p + q × 1) vector η holds the observed and latent variables, ζ is also (m + n + p + q × 1)

2

x1
β1
γ1
ϕ1

z1
γ2

y1

β2

υ1

x2

ε1

δ2
Figure 1: Path diagram
and holds any latent variables and the errors of the manifest variables and y is (p + q × 1) and holds only the
observed variables. The matrix Λ is (p + q × m + n + p + q) and has only ones and zeros as elements and
selects the observed variables from η. The Ψ matrix (introduced below) is (m + n + p + q × m + n + p + q) and
specifies the (co-)variances of the variables in ζ. Finally, the B matrix is also (m + n + p + q × m + n + p + q)
and holds the model coefficients.
For this example model, we have: η 0 = (x1 , x2 , z1 , y1 ), ζ 0 = (δ1 δ2 , υ1 , ε1 ) and y 0 = (x1 , x2 , z1 , y1 ). Because
there are no latent variables in this example, η is identical to y and the matrix Λ is an identity matrix
x1
x1

Λ=

x2
z1
y1



1.0
 0

 0
0

x2

0
1.0
0
0

z1

y1

0
0
0
0 
,
1.0 0 
0 1.0

and B will be

x1

B=

x2
z1
y1

x1

x2

z1

y1

0
 ϕ1

 γ1
β1

0
0
γ2
0

0
0
0
β2


0
0 
.
0 
0



The Ψ matrix is important as it specifies the covariances of the independent variables (including the errors),
essentially laying out all the model assumptions. Before showing the Ψ matrix we must first derive the the
Σ(θ) model-implied covariance matrix of the observed variables analytically. To do so, we first put η in
reduced form

η = Bη + ζ
= (I − B)−1 ζ
and find the expected value of yy 0

3

yy 0 = (Λη)(Λη)0
= (Λ(I − B)−1 ζ)(Λ(I − B)−1 ζ)0
= (Λ(I − B)−1 ζ)(ζ 0 (I − B)−10 Λ0 )
= Λ(I − B)−1 ζζ 0 (I − B)−10 Λ0
E[yy 0 ] = Λ(I − B)−1 E[ζζ 0 ](I − B)−10 Λ0
Σ(θ) = Λ(I − B)−1 Ψ(I − B)−10 Λ0 .
It should be clear from this that the model is specified primarily according to the assumptions we place
on B and Ψ.1 In B we are deciding how the variables are related to one another and in Ψ we are (often
implicitly) stating our further model assumptions. In this example, we are assuming that the model errors
are uncorrelated with the exogenous variable x1 and with each other (see the path diagram in Figure 1).
Therefore, Ψ has the variances of the exogenous variables on the diagonal and zeros everywhere else

Ψ=

x1
σδ21

x1



x2

 0

 0
0

z1
y1

x2

z1

0
σδ22
0
0

0
0
συ21
0

y1


0
0 
,
0 
σε21

(keeping in mind that x1 is exogenous, so x1 = δ1 ).
Running the model in lavaan
Having now reached our model specification, we can compare the matrices outlined above to the ones that
lavaan used to come to the estimates. First though, the model is run to check the estimates. I use labels,
e.g. x2 ~ p1*x1, to help remind me later that the coefficient for the regression of x2 on x1 was called “phi
one”.
m1 <- '
x1 ~ 1*x1 # Simply to preserve order of variables as they appear in the output
x2 ~ p1*x1
z1 ~ g1*x1 + g2*x2
y1 ~ b1*x1 + b2*z1
'
summary( m1.fit <- sem( m1, data = df))
## lavaan 0.6-3 ended normally after 16 iterations
##
##
Optimization method
NLMINB
##
Number of free parameters
9
##
##
Number of observations
10000
##
##
Estimator
ML
##
Model Fit Test Statistic
0.106
##
Degrees of freedom
1
##
P-value (Chi-square)
0.745
##
1 The

only choices we make regarding the vectors η, ζ and y are which variables to include in the model in the first place.

4

## Parameter Estimates:
##
##
Information
##
Information saturated (h1) model
##
Standard Errors
##
## Regressions:
##
Estimate Std.Err
##
x1 ~
##
x1
1.000
##
x2 ~
##
x1
(p1)
0.097
0.010
##
z1 ~
##
x1
(g1)
0.288
0.010
##
x2
(g2)
0.157
0.010
##
y1 ~
##
x1
(b1)
0.044
0.011
##
z1
(b2)
0.205
0.010
##
## Variances:
##
Estimate Std.Err
##
.x1
0.989
0.014
##
.x2
0.990
0.014
##
.z1
0.999
0.014
##
.y1
1.021
0.014

Expected
Structured
Standard
z-value

P(>|z|)

9.740

0.000

28.546
15.665

0.000
0.000

4.152
20.504

0.000
0.000

z-value
70.711
70.711
70.711
70.711

P(>|z|)
0.000
0.000
0.000
0.000

Because of the large sample size, the estimated parameters are very close to the ones we used to generate
the data. This should come as no surprise because we simply copied the data-generating process into our
code and let the software come up with the estimates. Now, we can inspect the model matrices using the
inspect() function in lavaan, where 'est' returns a list of the model matrices
inspect( m1.fit, 'est')
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##

$lambda
x1 x2 z1 y1
x1 1 0 0 0
x2 0 1 0 0
z1 0 0 1 0
y1 0 0 0 1
$theta
x1 x2 z1 y1
x1 0
x2 0 0
z1 0 0 0
y1 0 0 0 0
$psi
x1
x1 0.989
x2 0.000
z1 0.000
y1 0.000

x2

z1

y1

0.990
0.000 0.999
0.000 0.000 1.021

5

##
##
##
##
##
##

$beta
x1
x2
z1
y1

x1
1.000
0.097
0.288
0.044

x2
0.000
0.000
0.157
0.000

z1 y1
0.000 0
0.000 0
0.000 0
0.205 0

Here we see the matrices used to estimate the model are, in fact, the exact same ones we outlined above. The
only difference is that here the estimated coefficients have been substituted in already.

Model-implied covariance matrix
Typical approaches
If we are interested in looking at how the parameter estimates were found, we need to look closer at the
model-implied covariances.
It is of course possible to use the tracing rule (Kenny 1979) to come up with the underlying model-implied
covariances but this is inefficient, tedious and it must be done by hand. Another option is to find the algebraic
equation per covariance which is straightforward, but also tedious. For example, if we want to know how the
β1
effect x1 −→ y1 was estimated, then we need to look at Cov(y1 , x1 )2

Cov(y1 , x1 ) = E[y1 x1 ]
= E[(β1 x1 + β2 z1 + ε1 )(x1 )]
= E[β1 x21 + β2 z1 x1 + ε1 x1 ]
= β1 V ar(x1 ) + β2 E[z1 x1 ]
= β1 V ar(x1 ) + β2 E[(γ1 x1 + γ2 x2 + υ1 )(x1 )]
= β1 V ar(x1 ) + β2 E[γ1 x21 + γ2 x2 x1 + υ1 x1 ]
= β1 V ar(x1 ) + β2 γ1 V ar(x1 ) + β2 γ2 E[x2 x1 ]
= β1 V ar(x1 ) + β2 γ1 V ar(x1 ) + β2 γ2 E[(ϕ1 x1 + δ2 )(x1 )]
= β1 V ar(x1 ) + β2 γ1 V ar(x1 ) + β2 γ2 E[ϕ1 x21 + δ2 x1 ]
= β1 V ar(x1 ) + β2 γ1 V ar(x1 ) + β2 γ2 ϕ1 V ar(x1 )
β̂1 =

Cov(y1 , x1 ) − β2 γ1 V ar(x1 ) − β2 γ2 ϕ1 V ar(x1 )
V ar(x1 )

which we can double-check by substituting in the model estimates (the option 'list' gives the parameter
table in the form of a matrix that can easily be accessed and worked with)
# The square brackets are used to access elements of the matrix
# E.g. the parameter estimate for beta1 is found in row 5, column 14
b1 <- inspect( m1.fit, 'list')[ 5, 14] # beta1
b2 <- inspect( m1.fit, 'list')[ 6, 14] # beta2
g1 <- inspect( m1.fit, 'list')[ 3, 14] # gamma1
g2 <- inspect( m1.fit, 'list')[ 4, 14] # gamma2
p1 <- inspect( m1.fit, 'list')[ 2, 14] # phi1
# Cov(y1,x1)
- b2*g1*Var(x1)
- b2*g2*p1*Var(x1)
/Var(x1)
( cov( df)[ 4, 1] - b2*g1*cov( df)[ 1, 1] - b2*g2*p1*cov( df)[ 1, 1])/cov( df)[ 1, 1]
## [1] 0.04402362
2 Here

the covariance algebra is simplified because the variables are all centered with mean zero.

6

# Compare with the lavaan estimate of beta1
inspect( m1.fit, 'list')[ 5, 14]
## [1] 0.04402362
The equation for β̂1 tells us that it is the result of the covariance of y1 and x1 minus the weighted (by β̂2 )
covariance of z1 and x1 , divided by the variance of x1 .
However, as I stated earlier, this procedure is tedious if one wants to inspect the model-implied covariances
of each of the elements in the covariance matrix. And the more complex the model in terms of the number of
variables and the number of relationships between the variables, the more tedious the procedure becomes.
Ryacas model-implied covariance matrix
The more efficient option I will present converts the model matrices specified above into the entire symbolic
model-implied covariance matrix. This method is efficient because it uses only the matrices Λ, B, Ψ and an
(m + n + p + q)2 identity matrix. The matrices will get larger the more complex the model, but they will not
become more numerous.
The first step is to introduce all the model variables (including the errors) and coefficients as symbols using
the Sym( 'variable') function in Ryacas.
# Symbols for variables (including errors)
x1 <- Sym( 'x1')
x2 <- Sym( 'x2')
z1 <- Sym( 'z1')
y1 <- Sym( 'y1')
d1 <- Sym( 'd1') # Delta1
d2 <- Sym( 'd2') # Delta2
u1 <- Sym( 'u1') # Upsilon1
e1 <- Sym( 'e1') # Epsilon1
# Symbols for coefficients
p1 <- Sym( 'p1') # Phi1
g1 <- Sym( 'g1') # Gamma1
g2 <- Sym( 'g2') # Gamma2
b1 <- Sym( 'b1') # Beta1
b2 <- Sym( 'b2') # Beta2
Second, one specifies the model matrices as they were shown above. In Ryacas, vectors are inputted as row
vectors using vector := {a, b, c} while for matrices, one uses double curly braces: matrix := {{a, b,
c}, {d, e, f}}. Matrices are entered row-wise, so element d, for example, is found in the second row, first
column. Besides defining the symbolic variables, to use Ryacas one must enclose the code in yacas( "").
# Lambda
yacas( "L := {{1,
{0,
{0,
{0,

0,
1,
0,
0,

0,
0,
1,
0,

0},
0},
0},
1}}")

# Psi
yacas( "P := {{x1^2, 0, 0, 0},
{0, d2^2, 0, 0},
{0, 0, u1^2, 0},
{0, 0, 0, e1^2}}")

7

# Beta
yacas( "B := {{0,
{p1,
{g1,
{b1,

0,
0,
g2,
0,

0,
0,
0,
b2,

0},
0},
0},
0}}")

# Identity matrix # "I" is reserved, must use "Id" or something else
yacas( "Id := {{1, 0, 0, 0},
{0, 1, 0, 0},
{0, 0, 1, 0},
{0, 0, 0, 1}}")
After inputting the model matrices, the equation for Σ(θ) can be derived straightforwardly. Note that matrix
multiplication works with Dot( matrix1, matrix2), and that the functions in Ryacas are case sensitive.
The transpose and inverse of a matrix works with Transpose( matrix) and Inverse( matrix), respectively.
Remember, the equation for the model-implied covariance matrix was Λ(I − B)−1 Ψ(I − B)−10 Λ0
# First make the (I-B)^(-1) matrix
yacas( "IdmB := Id - B")
yacas( "iIdmB := Inverse( IdmB)") # iIdmB stands for "inverse of Identity minus B"
# Then
yacas(
yacas(
yacas(
yacas(

use matrix multiplication to arrive at model-implied covariance matrix
"LB := Dot( L, iIdmB)")
"LBP := Dot( LB, P)")
"LBPB := Dot( LBP, Transpose( iIdmB))")
"LBPBL := Dot( LBPB, Transpose( L))")

Doing so results in the symbolic model-implied matrix.
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##

Yacas matrix:
[,1]
[1,] x1^2
[2,] p1 * x1^2
[3,] (g1 + g2 * p1) * x1^2
[4,] (b1 + b2 * (g1 + g2 * p1)) * x1^2
[,2]
[1,] x1^2 * p1
[2,] x1^2 * p1^2 + d2^2
[3,] (g1 + g2 * p1) * x1^2 * p1 + g2 * d2^2
[4,] (b1 + b2 * (g1 + g2 * p1)) * x1^2 * p1 + b2 * g2 * d2^2
[,3]
[1,] x1^2 * (g1 + g2 * p1)
[2,] p1 * x1^2 * (g1 + g2 * p1) + d2^2 * g2
[3,] x1^2 * (g1 + g2 * p1)^2 + d2^2 * g2^2 + u1^2
[4,] (b1 + b2 * (g1 + g2 * p1)) * x1^2 * (g1 + g2 * p1) + b2 * g2 * d2^2 * g2 + b2 * u1^2
[,4]
[1,] x1^2 * (b1 + b2 * (g1 + g2 * p1))
[2,] p1 * x1^2 * (b1 + b2 * (g1 + g2 * p1)) + d2^2 * (b2 * g2)
[3,] (g1 + g2 * p1) * x1^2 * (b1 + b2 * (g1 + g2 * p1)) + g2 * d2^2 * (b2 * g2) + u1^2 * b2
[4,] x1^2 * (b1 + b2 * (g1 + g2 * p1))^2 + d2^2 * (b2 * g2)^2 + u1^2 * b2^2 + e1^2

Again, we can double check and see if the proper symbolic model-implied covariance matrix resulted by

8

x1
β1
γ1
z1
γ2

β2

υ1

x2

y1

ε1

Figure 2: Alternative Model Path diagram
looking, for example, at Cov(y1 , x1 ) in the fourth row, first column of the results
#
#
#
#
#
#

Remember to save parameter estimates of the lavaan model
as objects. You may have to clear the symbolic variables
from memory before you can use the names again as normal
R objects or use other names, i.e.
b1 <- Sym( 'b1') # Symbolic variable for Ryacas
beta1 <- inspect( m1.fit, 'list')[ 5, 14] # Normal R object

# Cov(y1,x1) =
(b1 + b2 * (g1 + g2 * p1)) * var( df$x1)
## [1] 0.1050037
# Compare with the empirical covariance matrix df
cov( df$x1,df$y1)
## [1] 0.1050037

Using the analytical model-implied covariance matrix to gain a better understanding of SEM models
Regardless of whether one uses the tracing rule, writes out the covariances algebraically or generates the entire
symbolic matrix with the method above, it is important to carefully inspect the model-implied covariance
matrix. Doing so can reveal things that are often hidden even in otherwise accurate path diagrams.
For example, suppose we believed the variables x1 and x2 in the first model were unrelated. Our path diagram
would need to be updated to reflect this, see Figure 2.
The question now is, which of the following two scenarios is true?

z1 = γ1 x1 + υ1a
z1 = γ2 x2 + υ1b
or

z1 = γ1 x1 + γ2 x2 + υ1
In other words, are the estimated coefficients γ1 and γ2 both total bivariate effects or are they partial
coefficients? Could it be that the software is actually basically running two separate bivariate equations for
9

z1 ? After all, the covariance between x1 and x2 is shown as zero in the model matrix Ψ.3 The answer may
be clear to some, but the path diagram does not help us in any way, nor do the typical maximum-likelihood
based explanations one often hears along the lines of ‘then the optimal estimates are found in an iterative
process’.
One way to approach this problem is to inspect the model-implied covariance matrix and check. We can
ϕ1
run a second lavaan model, this time without the effect x1 −→ x2 and again generate the model-implied
covariance matrix.
m2 <- '
x1 ~ 1*x1 # Simply to preserve order of variables in output
x2 ~ 1*x2 # Here as well
# x2 ~ p1*x1
z1 ~ g1*x1 + g2*x2
y1 ~ b1*x1 + b2*z1
'
summary( m2.fit <- sem( m2, data = df))
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##

lavaan 0.6-3 ended normally after 12 iterations
Optimization method
Number of free parameters

NLMINB
8

Number of observations

10000

Estimator
Model Fit Test Statistic
Degrees of freedom
P-value (Chi-square)

ML
94.528
2
0.000

Parameter Estimates:
Information
Information saturated (h1) model
Standard Errors
Regressions:
x1 ~
x1
x2 ~
x2
z1 ~
x1
x2
y1 ~
x1
z1
Variances:
.x1

Estimate

Expected
Structured
Standard

Std.Err

z-value

P(>|z|)

1.000
1.000
(g1)
(g2)

0.288
0.157

0.010
0.010

28.681
15.739

0.000
0.000

(b1)
(b2)

0.044
0.205

0.011
0.010

4.169
20.507

0.000
0.000

Estimate
0.989

Std.Err
0.014

z-value
70.711

P(>|z|)
0.000

3 This example is motivated by a playful argument between myself and thesis advisor concerning whether to explicitly specify
covariances between exogenous predictors.

10

##
##
##

.x2
.z1
.y1

1.000
0.999
1.021

0.014
0.014
0.014

70.711
70.711
70.711

0.000
0.000
0.000

The model matrices can be viewed again using
inspect( m2.fit, 'est')
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##

$lambda
x1 x2 z1 y1
x1 1 0 0 0
x2 0 1 0 0
z1 0 0 1 0
y1 0 0 0 1
$theta
x1 x2 z1 y1
x1 0
x2 0 0
z1 0 0 0
y1 0 0 0 0
$psi
x1
x1 0.989
x2 0.000
z1 0.000
y1 0.000
$beta
x1
x2
z1
y1

x1
1.000
0.000
0.288
0.044

x2

z1

y1

1.000
0.000 0.999
0.000 0.000 1.021
x2
0.000
1.000
0.157
0.000

z1 y1
0.000 0
0.000 0
0.000 0
0.205 0

notice the change to the B matrix. Now modify the Ryacas matrix accordingly and generate the symbolic
model-implied covariance matrix. All the other matrices stay the same, so as long as they are still in memory
one does not need to enter them again.
# Beta
yacas( "B := {{0,
{0,
{g1,
{b1,

0,
0,
g2,
0,

0,
0,
0,
b2,

0},
0},
0},
0}}")

# Multiplication
yacas( "IdmB := Id - B")
yacas( "iIdmB := Inverse( IdmB)")
yacas( "LB := Dot( L, iIdmB)")
yacas( "LBP := Dot( LB, P)")
yacas( "LBPB := Dot( LBP, Transpose( iIdmB))")

11

yacas( "LBPBL := Dot( LBPB, Transpose( L))")
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##

Yacas matrix:
[,1]
[,2]
[1,] x1^2
0
[2,] 0
d2^2
[3,] g1 * x1^2
g2 * d2^2
[4,] (b1 + b2 * g1) * x1^2 b2 * g2 * d2^2
[,3]
[1,] x1^2 * g1
[2,] d2^2 * g2
[3,] x1^2 * g1^2 + d2^2 * g2^2 + u1^2
[4,] (b1 + b2 * g1) * x1^2 * g1 + b2 * g2 * d2^2 * g2 + b2 * u1^2
[,4]
[1,] x1^2 * (b1 + b2 * g1)
[2,] d2^2 * (b2 * g2)
[3,] g1 * x1^2 * (b1 + b2 * g1) + g2 * d2^2 * (b2 * g2) + u1^2 * b2
[4,] x1^2 * (b1 + b2 * g1)^2 + d2^2 * (b2 * g2)^2 + u1^2 * b2^2 + e1^2

The resulting matrix seems to support the first scenario, that both regression coefficients are uncontrolled
bivariate estimates. However, if we substitute in our estimates and solve for γ1 (as an example, we could do
the same for γ2 ), we see that something is not right.
# z1*x1 = g1 * x1^2
# g1
= z1*x1/x1^2
cov( df$z1, df$x1)/var( df$x1)
## [1] 0.3036378
# Compare with the estimated coefficient
inspect( m2.fit, 'list')[ 3, 14]
## [1] 0.2883011
The analytical equation for the coefficient does not equal the estimate from the lavaan model. To check
the alternative scenario, that the estimated coefficients are partial coefficients, we can change Ψ to reflect a
correlation between x1 and x2
# Psi
yacas( "P := {{x1^2,
{x2*x1,
{0,
{0,

x1*x2,
d2^2,
0,
0,

0,
0,
u1^2,
0,

0},
0},
0},
e1^2}}")

and then re-run the matrix multiplication and see what changes
# Multiplication
yacas( "IdmB := Id - B")
yacas( "iIdmB := Inverse( IdmB)")
yacas(
yacas(
yacas(
yacas(

"LB := Dot( L, iIdmB)")
"LBP := Dot( LB, P)")
"LBPB := Dot( LBP, Transpose( iIdmB))")
"LBPBL := Dot( LBPB, Transpose( L))")

12

##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##

Yacas matrix:
[,1]
[1,] x1^2
[2,] x2 * x1
[3,] g1 * x1^2 + g2 * (x2 * x1)
[4,] (b1 + b2 * g1) * x1^2 + b2 * g2 * (x2 * x1)
[,2]
[1,] x1 * x2
[2,] d2^2
[3,] g1 * (x1 * x2) + g2 * d2^2
[4,] (b1 + b2 * g1) * (x1 * x2) + b2 * g2 * d2^2
[,3]
[1,] x1^2 * g1 + x1 * x2 * g2
[2,] x2 * x1 * g1 + d2^2 * g2
[3,] (g1 * x1^2 + g2 * (x2 * x1)) * g1 + (g1 * (x1
[4,] ((b1 + b2 * g1) * x1^2 + b2 * g2 * (x2 * x1))
[,4]
[1,] x1^2 * (b1 + b2 * g1) + x1 * x2 * (b2 * g2)
[2,] x2 * x1 * (b1 + b2 * g1) + d2^2 * (b2 * g2)
[3,] (g1 * x1^2 + g2 * (x2 * x1)) * (b1 + b2 * g1)
[4,] ((b1 + b2 * g1) * x1^2 + b2 * g2 * (x2 * x1))

* x2) + g2 * d2^2) * g2 + u1^2
* g1 + ((b1 + b2 * g1) * (x1 * x2) + b2 * g2 * d2^2) * g2 + b2 * u1^2

+ (g1 * (x1 * x2) + g2 * d2^2) * (b2 * g2) + u1^2 * b2
* (b1 + b2 * g1) + ((b1 + b2 * g1) * (x1 * x2) + b2 * g2 * d2^2) * (b2 * g2) + u1^2 * b2^2 +

(The equation for y12 gets cut off but that doesn’t really matter.)
Now we can solve the alternative equation for γ1 to show that the estimated coefficients are, in fact, partial
coefficients.
# z1 * x1 = g1 * x1^2 + g2 * (x2 * x1)
# g1
= ( z1*x1 - g2 * (x2 * x1))/x1^2
(cov( df$z1, df$x1) - inspect( m2.fit, 'list')[ 4, 14]*cov( df$x2, df$x1))/var( df$x1)
## [1] 0.2883011
# Compare with estimated gamma1
inspect( m2.fit, 'list')[ 3, 14]
## [1] 0.2883011
From this, we can see that the default behaviour in lavaan is to implicitly account for covariances between
exogenous variables with common ‘children’ to use the language of Judea Pearl (2009). This was not apparent
from the path diagram above, nor was it reflected in the lavaan model — even the model matrices showed a
0 in Ψ for the correlation between x1 and x2 .4 However, by taking a closer look at the algebraically derived
equation for the parameter of interest and double checking, we were able to conclude that the estimates are in
fact partial effects and we learned something about the default behaviour of the software. Upon reflection, it
should be clear that this is the only way the model can work but it was not trivial to find conclusive evidence.

Conclusion
Regardless of the method; tracing, algebraic or using the CAS Ryacas method outlined here, knowing how
to generate and inspect the model-implied covariance matrix can be a useful tool. By running the model,
specifying the matrices in Ryacas in exactly the same way and deriving the matrix algebra model-implied
covariance matrix, we can be sure that no mistakes were made along the way. The tedious algebraic derivation
shown above proves that there are many opportunities to make a mistake.
On the other hand, it should be noted that this approaches does not always make sense. For example, the
equation for the variance of y1 in the last output is probably too long to be of much practical use (although
it is accurate!). For a more intutitive understanding of what goes into the variance of that variable, it is
likely better to work out the algebra by hand. Then one can decide for themselves when it makes sense to
stop expanding. In the algebraic equation for Cov(y1 , x1 ) above for example, one could have stopped at line
4 and simply plugged in Cov(z1 , x1 ) from the empirical covariance matrix instead of expanding even further.
But this is something each researcher must decide for themselves on a case by case basis.
4 It

should, however, have been apparent from the fact that the estimates in both of the lavaan models were identical.

13

The goal of this article was to outline an approach that can compliment the tracing rule and covariance
algebra by hand. The benefits are that the CAS approach is efficient, less prone to careless mistakes and that
the results are available digitally for further use in R — results derived by hand need to be entered in to the
software manually. In any case, a close inspection of the underlying algebra of the model-implied covariance
matrix should be seen as a necessary step in any SEM regression model. It is often difficult to accurately
reflect the SEM model in path diagrams alone and there are even situtations where software default settings
can be misleading as we have seen from the simple example above.

References
Andersen, Mikkel, Rob Goedman, Gabor Grothendieck, Søren Højsgaard, Grzegorz Mazur, Ayal Pinkus,
and Nemanja Trifunovic. 2019. Ryacas: R Interface to the “Yacas” Computer Algebra System. https:
//cran.r-project.org/web/packages/Ryacas/index.html.
Bollen, Kenneth. 1989. Structural Equations with Latent Variables. New York, Chichester: Wiley.
Graff, J. 1979. “Verallgemeinertes LISREL-Modell.” Mannheim, Germany.
Kenny, David. 1979. Correlation and Causality. New York: John Wiley & Sons.
Pearl, Judea. 2009. Causality: Models, Reasoning, and Inference. Second Edition. Cambridge, New York,
Melbourne: Cambridge University Press.
R Core Team. 2017. R: A Language and Environment for Statistical Computing. Vienna, Austria: R
Foundation for Statistical Computing. https://www.R-project.org/.
Rosseel, Yves. 2012. “lavaan: An R Package for Structural Equation Modeling.” Journal of Statistical
Software 48 (2): 1–36. http://www.jstatsoft.org/v48/i02/.

14

