These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

The production of research elites. Research performance assessment in the
United Kingdom
Julian Hamann
Forum Internationale Wissenschaft, University of Bonn

1. Introduction
Performance assessment has always been a vital part of academia. Knowledge is perceived as sound for
as long as it can withstand the critical evaluation of peers. For a long time, there was no direct link between
research performance assessment and the distribution of resources. The governing bodies in higher education distributed funds more or less equally and left the performance assessment to a decentralized community of peers. However, in the 1980s, there was a focus in higher education governance on performance-oriented research output rather than financial input, increasingly stimulating research with centralized instruments and orienting assessment partially on non-academic standards like societal impact
(Braun and Merrien, 1999; Paradeise et al., 2009). New instruments of systematic research performance
assessment were installed at the interface between academia, the state, and the market. The assessment
instruments now guide a selective allocation of scarce resources to what are supposed to be the best
performers, thus rewarding “research elites” (Slaughter and Leslie, 1999; Münch, 2014).
Drawing on data that I have examined elsewhere to address related questions (Hamann, 2016), the
current contribution asks whether the production of research elites produces unintended stratification
effects. The case study is the Research Assessment Exercise (RAE)/Research Excellence Framework (REF),
an instrument of research performance assessment in the United Kingdom (UK). After discussing consequences of research performance assessments, particular attention is drawn to whether stratification is
actually a consequence intended by the RAE/REF. Building on data from the three most recent assessments
(RAE 2001, RAE 2008, REF 2014), the unequal distribution of symbolic, social, and economic resources in
the discipline of history is examined in a field and capital theoretical framework. This distribution is correlated to RAE/REF rank groups. The closing discussion interprets the stratification of the field and its consecration by RAE/REF rank groups. The contribution concludes that the elite (re-)produced by research performance assessments in the UK is not (solely) based on “excellence,” but on previous allocations of resources.

1

These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

2. Research performance assessment in the UK
The British system pioneered the development outlined in the previous section. It institutionalized performance assessments very early on, and since then has developed one of the most advanced assessment
programs in Europe. When close government regulation was withdrawn under New Public Management,
British universities and departments were forced to compete for financial resources, researchers, and students (Deem et al., 2008; Brown and Carasso, 2013). Traditional bureaucratic systems of delivery were
superseded by competitive quasi-markets, which were supposed to be the more efficient form of organization, although in contrast to conventional markets their providers are not necessarily for-profit. Ever
since, the decisive assessments for the selective allocation of public research funds are delivered roughly
every five years by Research Assessment Exercises (RAE) and, since 2014, by the Research Excellence
Framework (REF). The assessments are conducted by the funding councils of England (HEFCE), Scotland
(SHEFC), Wales (HEFCW), and Northern Ireland (DENI).
The UK funding councils organize a centralized peer review system that evaluates research output. The
output is submitted, mainly in the form of publications, by research staff who have been selected for assessment by their respective departments. The actual assessment is conducted by subject-specific panels
that are appointed by the funding councils. Composed of 15 to 30 experts from within a relevant academic
field, these panels grade the quality of research across dozens of fields and more than 150 institutions.
The assessment panels are charged with “identifying excellence in the rich diversity of research” they cover
(REF, 2012). In doing so, the panels employ a grade system that appears to be rather simplistic compared
to the complexity of research in even a single academic field (Figure 2.1).

Figure 2.1: Grades of research quality, according to RAE 2008 and REF 2014
Rating

Description

4*

Quality that is world-leading in terms of originality, significance and rigour

3*

Quality that is internationally excellent in terms of originality, significance and rigour but which
nonetheless falls short of the highest standards of excellence

2*

Quality that is recognised internationally in terms of originality, significance and rigour

1*

Quality that is recognised nationally in terms of originality, significance and rigour

Unclassified

Quality that falls below the standard of nationally recognised work. Or work which does not meet
the published definition of research for the purposes of this assessment.

Source: (RAE, 2008a; REF, 2011)

2

These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

Their high degree of simplification – or, in other terms, their ability to reduce complexity – may contribute
to the efficacy and potency of the assessments (see also Werron, 2014; Hamann, 2017). More crucial,
however, is that the RAE/REF produce distinctions that are not merely symbolic and thus blurry enough to
be contested, reinterpreted, or ignored. The assessments are so powerful because the symbolic distinctions they make are linked to the (re-)production of material classes (cf. Maeße, 2016). Since the RAE/REF
results inform the allocation of funds by the UK funding councils, the entire basic research funding of institutions and, ultimately, research fields is at stake. The most recent REF 2014, for example, informed the
distribution of £1.6 billion annually for the subsequent six years. From a sum of almost £10 billion, the
departments rewarded with the best grade (4*) receive 80 percent, those with the second best grade (3*)
receive 20 percent. The departments below those grades receive no public basic research funding at all
(HEFCE, 2015).
While the main aim of the RAE/REF has always been to assess the research quality of departments and
thereby inform the distribution of public funds, the assessments have evolved significantly since they
started in 1986. In the space of 30 years, the RAE/REF have become increasingly sophisticated, advancing
the criteria, their calculation, and compilation (Bence and Oppenheim, 2005). The RAE 2001, for example,
was characterized above all by grade inflation, which led to a more concentrated funding policy. The main
change in the RAE 2008 was the introduction of research profiles for each department based on the proportion of publications that met respective quality standards. In the RAE 2001 and 2008, the submissions
included data on staff, research output, and the research environment, for example, research income and
doctoral degrees awarded. The most important new feature of the REF 2014 is that this data has been
expanded to include information that is intended to document the societal impact of research. Thus the
former two pillars of research assessment, research “output quality” and “research environment,” have
been complemented by “impact” (weighted with 65, 15, and 20 percent respectively) (REF, 2011).
Regardless of their development over time, the underlying principle behind the assessments is straightforward: “Institutions conducting the best research receive a larger proportion of the available grant so
that the infrastructure for the top level of research in the UK is protected and developed.” (RAE, 2001e).
The following section will consider the effects of a policy that sets out to “protect” the “best research”.

3. Consequences of research performance assessments
The literature on performance assessments in general and the RAE/REF in particular suggests that they
have several effects. One finding is that instruments of higher education governance may influence the

3

These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

development of disciplines by dictating the criteria used in peer review (Hicks, 2012). In the RAE/REF, assessments are conducted by experts from the research fields, but in order to ensure fairness across disciplinary fields, the experts must adhere to common criteria that are defined by the funding councils (Tapper
and Salter, 2003). Although defined by the funding councils, the criteria are grounded in expert advice –
which can have a reinforcing effect in itself (cf. Martin and Whitley, 2010). Disclosing such wide-ranging
assessment criteria has standardizing effects on research. Overall, researchers’ choices of publication topics (Talib, 2001) and publication patterns (Moed, 2008) seem to have changed under the influence of the
RAE/REF. In economics, heterodox approaches have fallen victim to orthodox assessment panels, criteria
geared toward mainstream journals, and a general orientation toward “excellence” (Lee et al., 2013;
Maeße, 2016). In life sciences, academics have shifted their research practices in order to cooperate with
an intrusive policy regime (Morris and Rip, 2006). In law departments, academic work is increasingly concentrating on placing articles in a small number of highly ranked journals (Campbell et al., 1999).
Studies have also revealed that status assignments create new layers of dependency and authority, for
example between funding institutions and universities (Salter and Tapper, 2002), between university management and the departments they intervene in (Henkel, 1999), between panel members and the colleagues whose research they are judging (Sharp and Coleman, 2005), and between research active personnel and the colleagues who are required to take over their teaching duties (Salter and Tapper, 2002).
The literature has further identified increased stratification as an effect of research performance assessment. On the general level of universities, the RAE/REF is seen as a mechanism of status allocation and
resource concentration (Henkel, 1999) that manifests, for example, a bias against new universities and in
favor of traditional universities (Tapper and Salter, 2003). In chemistry, research grants and highly cited
scientists are concentrated in just a few institutions (Münch and Schäfer, 2014). Economics, a rather stratified discipline even before performance assessments were applied, is even more dominated by a select
group of prestigious elite departments that receive the major share of funding (Lee et al., 2013; Maeße,
2015). In the humanities in general, the RAE/REF is assumed to have led to a concentration of research
activities in certain institutions (Kehm and Leišytė, 2010; Hamann, 2016). In all cases, the accumulation of
capital can be interpreted as a result of the intensified struggle for resources in an increasingly stratified
system consolidated by the RAE/REF.
This study contributes a longitudinal perspective to the literature. It reveals that the RAE/REF (re-)produce and consecrate a disciplinary center-periphery structure that is not oriented toward research “excellence” (alone) but follows previous allocations of resources. This argument will be developed taking the
discipline of history as a case study. The currencies that are valued most highly on the newly established

4

These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

quasi-markets – journal articles and research with societal impact (Moed, 2008; Martin, 2011) – have less
weight in a discipline which still perceives research and teaching as a unit, traditionally focuses on monographs and edited volumes rather than journal articles, and attaches more weight to basic rather than
applied research (Kehm and Leišytė, 2010; Zuccala et al., 2014). A longitudinal examination of the production of elites in the discipline of history thus makes it possible to see the effects a highly developed system
of performance assessment has on a discipline with very different research and publication cultures. The
empirical evidence revealed for history may not be as distinct in disciplines that link teaching and research
less closely, have a proclivity for incremental research within an established paradigm, and are used to
quantify performance measures in the form of impact indices.

4. The (dys)functionality of stratification
With present research already indicating effects of stratification, this contribution must address one aspect in particular: it is the declared aim of the RAE/REF not only to “identify excellence,” but also to protect
and develop a “research elite” by granting the corresponding institutions the largest proportion of funding
(RAE, 2001e; REF, 2012). It is thus a vital part of the mission behind the assessment to indeed create a
stratified academic landscape. The question is therefore to what extent the effects of stratification discussed here might actually be intentional.
The position of the RAE/REF on stratification can best be described in terms of traditional structural
functionalist perspectives. Here, the hierarchical structuration of social entities appears to be functional
for the establishment of social order (Davis and Moore, 1944). Positions at the top of social hierarchies are
occupied by a performance elite that is evaluated according to unambiguous, universal, meritocratic criteria, and rewarded appropriately. This is exactly how the RAE/REF is supposed to operate. Authors that
have applied a functionalist outlook on science (Cole and Cole, 1973; Merton, 1973b) have emphasized
that the functionality of stratification relies on at least two conditions: the performance assessments that
justify stratification should not be influenced by the assessment procedures themselves (reliability); and
the assessment indicators should be able to actually measure what is supposed to be measured (validity),
that is, they should indeed be able to identify “research performance” in the complex reality of the academic world. Regardless of whether the hierarchical structuration of academia – or any other social order
for that matter – is actually considered desirable, the corresponding performance assessment instruments
must be reliable and valid.
Neither seems to be the case for the RAE/REF, however. An entire body of literature concerns itself
with the reliability of performance assessments. Studies highlight that what the RAE/REF measures is not

5

These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

research performance (alone), but the ability of researchers and departments to adapt to the metrics of
the assessments (Talib, 2001; Hare, 2003). In addition to what is referred to as the reactivity of rankings
(Espeland and Sauder, 2007), scholars have also questioned whether performance indicators exist that
permit valid measurement of research quality at all, let alone capture the broad range of academic performance beyond it (Laudel, 2005; Blockmans et al., 2014). Valid research performance indicators are especially difficult to imagine for disciplines in the social sciences and humanities, where highly differentiated publication practices, schools and communities are even more prominent and research is thus standardized to an even lesser degree (Archambault et al., 2006; Angermüller, 2010).
Given these serious problems regarding the validity and reliability of research performance assessments, the current contribution does not intend to promote a better approach to identifying “excellence”
or demonstrate how research performance should actually be measured. Picking up on studies that illustrate the general complexity and situational embeddedness of notions such as “research quality” (Lamont,
2009; Hirschauer, 2010), any notion of “good,” let alone “excellent,” research must be far too vague and
slippery to develop and operationalize a definition in assessment instruments. What can be examined,
however, is the assessment instruments themselves and their effects on research infrastructures.
In light of the significant doubts regarding a structural functionalist interpretation of stratification in
academia, it seems worthwhile to consider alternative approaches. The current contribution will draw on
a Bourdieusian (1988) heuristic of field and capital theory. Equipped with this heuristic, the analysis in this
paper is sensitized for the construction of status hierarchies alongside the structurally unequal distribution
of resources. Attempts to identify research elites then (re-)produce a relatively stable center-periphery
structure in the academic landscape, rewarding social, symbolic, and economic capital rather than “excellence” alone (Burris, 2004; Weakliem et al., 2012; Münch and Schäfer, 2014). The analysis will pursue this
perspective for the RAE/REF in the discipline of history.

5. Data
The investigation is based on data for history in the UK according to the three most recent assessments
(RAE 2001, RAE 2008, REF 2014). In order to reveal longitudinal stratification effects, four rank groups, two
at the top and two at the bottom of the RAE/REF status hierarchies, are constructed for exploratory analytical purposes. The ranks are based on the overall ratings in the case of the REF 2014, and on grade point
averages of the quality profiles in the case of the RAE 2008 and REF 2014. The “top 6” and “top 14” represent research elites attributed with a strong research output, the “bottom 6” and “bottom 14” represent

6

These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

departments attributed with a lower research output (see Table 6.1, Section 6).1 These rank groups are
necessarily artificial. As in any other ranking, they overemphasize gradual differences between key figures.
However, the purpose of the rank groups is not to highlight actual differences in research performance
but to serve as a proxy for the rank differences produced by the RAE/REF. In this way, the rank groups can
be used as a starting point for an exploratory longitudinal analysis. The current study relates these rank
groups to three different types of data, all of which are listed in the reports of the RAE/REF.
The contribution first investigates the composition of assessment panels (see Table 6.2, Section 6.1).
Members can be nominated by professional associations, for example, by the British Society of Sports
History, and by stakeholders from business and society, for example, by the British Museum of Natural
History. The four UK funding bodies assemble the panels on the basis of these nominations (RAE, 2001c;
RAE, 2008b). The panels for history consist exclusively of British historians.2 Membership of an assessment
panel indicates symbolic capital, defined as academic authority that makes it possible to consecrate research by determining legitimate problem definitions and problem solutions (Bourdieu, 1988). Second, the
contribution analyzes research staff in terms of full time equivalent (FTE) research positions at history
departments (see Figures 6.1 and 6.2, Section 6.2). These are the staff nominated by departments to submit their publications for assessment. Research staff serve as a proxy for social capital, which is defined as
the aggregate of resources that are linked to more or less institutionalized membership of a group
(Bourdieu, 1986). In this sense, the number of research staff indicates the resources available in a department for research proposals, reviews, or academic networks, for example. Third, external research funding
indicates the allocation of economic resources to departments (see Figure 6.3, Section 6.3). External grants
can include, for example, funding from the research councils,3 public and private funding, and funding
from the European Union.

6. Research elites and the structurally unequal distribution of resources
Results from the three most recent RAE/REF show that, apart from isolated cases, there seems to be considerable consistency among the departments that are attributed with the strongest research output and

1

These categories are rank groups from the RAE/REF rankings. They are not the author’s invention, and thus make
no claim to sociological validity.
2
This is not only in contrast to a number of other panels that also include non-academic members. The decidedly
national composition of the history panel also contrasts with the RAE/REF’s emphasis of with international quality
standards (see Figure 2.1).
3
The UK dual support system combines the allocation of public funds by the funding councils (oriented on RAE/ REF
results) and funding of specific projects by the research councils.

7

These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

those regarded as having the lowest research output (see Table 6.1). From 2001 to 2014, three departments make it into the top 14 in all three assessments, and 10 more departments are included in the top
14 two out of three times. There is a similar stability at the bottom of the status hierarchy: while only one
department is in the bottom 14 in all three assessments, nine more are in the bottom group two out of
three times. Only one department moves from the bottom 14 into the top group, and not a single department is relegated from the top into the bottom rank group.

Table 6.1: Rank groups based on results for history in RAE 2001, RAE 2008, and REF 2014
RAE 2001
RAE 2008
REF 2014
Birkbeck
Imperial College
Birmingham
Cambridge
Essex
York
Durham
Kent
Sheffield
East Anglia
Liverpool
Southampton
King‘s College
Oxford
Hertfordshire
SOAS
Warwick
King’s College
Oxford Brookes
Cambridge
Warwick
LSE
UCL
Oxford
Birmingham
Birkbeck
Exeter
Essex
Southampton
Cambridge
Exeter
Hertfordshire
Manchester
Hertfordshire
LSE
Leeds
Huddersfield
Sheffield
St Andrews
Hull
Aberdeen
UCL
St Martin‘s
Goldsmiths
Chichester
St Mary‘s
Sheffield Hallam
Newman
Westminster
Leeds TAS
Chester
Worcester
Canterbury CC
Westminster
York
Chichester
Central Lancashire
Glamorgan
Cumbria
Liverpool Hope
Bath Spa
Westminster
Leeds Trinity
Bolton
Gloucestershire
Greenwich
Chester
Liverpool JMU
St Mary’s
Edge Hill
Edge Hill
Bath Spa
Liverpool Hope
Northumbria
Sunderland
Middlesex
Newman College
Anglia Ruskin
Staffordshire
Wales, Newport
Gloucestershire
Thames Valley
Worcester
Bishop Grosseteste
Sum of departments
95
83
83
Sources: (RAE, 2001a; RAE, 2001b; RAE, 2008b; RAE, 2008c; REF, 2014a; REF, 2014b), author’s presentation and calculation

“Bottom 6”

“Bottom 14”

“Top 14”

“Top 6”

Groups

8

These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

A classical functionalist perspective might explain the high stability of status allocation with a stable allocation of “excellent” research among the elite positions of the hierarchy. The most relevant contributions
to the state of research are valued highly and thus rewarded, while insignificant contributions are less
visible and ultimately dispensable. In contrast to this – inevitably simplistic – sketch of functionalist perspectives, an analysis informed by Bourdieu’s field and capital theory can explain the stable hierarchy with
structurally unequal opportunities of capital accumulation that are consecrated by the respective rank
groups. In the following, the unequal distribution of resources will be examined for symbolic capital in
terms of panel membership (6.1), for social capital in terms of research active staff (6.2), and for economic
capital in terms of external research grants (6.3). The analysis will examine how this distribution correlates
to RAE/REF rank groups and is thus consecrated and (re-)produced.

6.1 Symbolic capital in terms of panel membership
The RAE/REF rank groups can be related to the composition of assessment panels. This gives insight, first,
into the link between the performance of departments in the assessment and their simultaneous representation on the panels of the same assessment; and second, into the recruitment of panel members according to their rank group in the respective previous assessments (Table 6.2).

9

These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

Table 6.2: History panel member affiliation and whether they were in the top 14 in the respective previous assessment (PreT14) and in the respective current assessment (CurT14)*
Panel members’ affiliation
RAE 2001
East Anglia
Cambridge
Edinburgh
York
Nottingham
SOAS
Liverpool
Hull
Hull
Reading
Durham
King’s College
Manchester
Glasgow
Teesside
Oxford
Leeds

Panel members’ affiliation PreT14
REF 2014
x
x
x
East Anglia
x
x
x
x
Cambridge
x
x
Edinburgh
x
York
Nottingham
x
x
x
St. Andrews
x
St. Andrews
x
x
Cardiff
x
x
Newcastle
x
x
Southampton
x
x
London U
x
x
King’s College
x
Manchester
x
Glasgow
Glasgow
x
x
Oxford
x
x
Oxford
x
Keele
Exeter
Hertfordshire
x
UCL
x
Birmingham
*
If a department is named twice, it had two colleagues in the respective panel.
Sources: (RAE, 1996; RAE, 2001b; RAE, 2008b; REF, 2014a), author’s presentation and calculation
PreT14

CurT14

Panel members’ affiliation
RAE 2008
East Anglia
Cambridge
Edinburgh
York
Nottingham
SOAS
Liverpool
Cardiff
Newcastle
Southampton
London U
Sheffield H.
Belfast
Sheffield
Aberystwyth
Oxford
Dundee

PreT14

CurT14

CurT14

x
x
x
x

x
x
x

x
x
x
x
x
x

Table 6.2 illustrates that 16 departments were represented on the history panel of the RAE 2001, 17 on
that of the RAE 2008, and 19 on that of the REF 2014. Without exception, panel members come from
departments that received grades higher than the average grade of all history departments in the respective assessment. In combination with the rank groups from Table 6.1, the data illustrates that twelve,
three, and five departments respectively were ranked among the top 14 in the previous assessments
(PreT14). These are cases in which panel members were recruited from elite departments. Six, four, and
eleven departments respectively were placed in the top 14 in the same assessment (CurT14). These are
cases in which the departments of the panel members became elite. Hence, in the period investigated,
there is a close relationship between the status elite of the field and the allocation of symbolic capital. The
converse pattern can be found for the periphery: in all three assessments, not a single panel member was
recruited from a department that was ranked in the bottom 14 in the previous assessment, and overall

10

These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

only two departments fell into the bottom 14 despite being represented on the panel of the same assessment. In other words, in the period investigated, no department from the periphery is granted symbolic
capital through panel membership, and cases in which departments with symbolic capital nevertheless fall
into the periphery are extremely rare.

6.2 Social capital in terms of research active staff
The RAE/REF rank groups from Table 6.1 can further be related to the distribution of research staff in the
history field. Figure 6.1 illustrates for each assessment the shares that respective rank groups have in the
overall number of history research positions.

Figure 6.1: Rank groups and their share of FTE research positions in history

% of FTE research positions in the field

40

36%

35

30%
30
25

21%

20
15

12%

12%

10%

10
5%
5

5%
2%

4%

2%

2%

0
RAE 2001

RAE 2008
Top 6

Top 14

Bottom 14

REF 2014
Bottom 6

Sources: (RAE, 2001a; RAE, 2008c; REF, 2014b), author’s presentation and calculation

On average, the top 14 departments from 2001 to 2014 account for 30 percent of all FTE research positions
in the field, while only 5 percent of the researchers are located in the bottom 14. This stratification becomes even more apparent for the top 6, with 11 percent on average, and the bottom 6, with 2 percent
of all research positions. The unequal distribution of research staff is exacerbated over time.
Building on this, the data depicted in Figure 6.2 illustrates how endowment with research staff changes
after departments have been assigned to status groups. In other words, it demonstrates whether there is

11

These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

a correlation between the classification of departments into respective status groups and their research
staff.

Figure 6.2: Rank groups and development of FTE research positions over time*

Development of FTE research positions in %

60

55%

50
40
30

24%

21%
17%

20

17%

15%

9%

10
2% 1%
0
-10

Overall
development

Top 6

-20

Top 14

Bottom 6

Bottom 14

-16%
2001 rank groups by 2008

2008 rank groups by 2014

*Imperial College London, Wales Newport and Cumbria have not been included in the 2014 assessment; their number of FTE positions has been taken from the 2008 assessment.
Sources: (RAE, 2001a; RAE, 2008c; REF, 2014b), author’s presentation and calculation

Figure 6.2 shows that the elite departments of the RAE 2001 increased their research staff considerably
by 2008: the top 6 of 2001 had 21 percent more researchers at their disposal by 2008 and the top 14 had
24 percent more by the same year. Both growth rates are much higher than the 2 percent total average
increase in research staff. The same pattern is revealed for the development of the elite departments of
the RAE 2008: the top 6 of 2008 had 17 percent more research staff by 2014 and the top 14 had 9 percent
more. Again, both growth rates are much higher than the total average increase (1 percent) in research
staff in this period. The movement of research staff to the elite departments suggests that very good assessments contribute to an improved endowment with research staff. The development of research staff
for the departments in the periphery, however, indicates that staff movement is not a zero sum game.
Less successful assessments can also lead to an increase in research staff. This is illustrated for the bottom
14 of 2001 and 2008, which increase the research staff at their disposal by 17 and 15 percent respectively.
The bottom 6 of 2008 have 55 percent more researchers by 2014, while the bottom 6 of 2001 have 16
percent fewer by 2008.

12

These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

The flow of research staff subsequent to status allocations as displayed in Figure 6.2 should be interpreted in light of the markedly different absolute staff numbers from Figure 6.1. For example, while a 55
percent increase in research staff for the bottom 6 of 2008 corresponds to an absolute growth of 16 FTE
research positions, the 17 percent increase in research staff for the top 6 departments in the same period
is equal to absolute growth of 36 FTE research positions. Even though the differences in relative staff increases (55 percent and 17 percent) may indicate the contrary, the gap between both rank groups still
grows in favor of the top rank group.
In any case, the movement of research staff as depicted in Figure 6.2 shows that very good assessments
are followed by an improved endowment with researchers, while less successful assessments do not by
default imply an exodus of research staff. Unsuccessful performance can indeed lead to reduced funds and
therefore cuts in research positions, although another reason for reduced staff endowment may equally
be that departments are more selective about whose work is submitted to the next assessment. Nevertheless, low status assignments can also lead to heavy investment in research staff and a subsequent rise
in research positions (cf. Elton, 2000 on strategic staffing in the wake of RAE/REF results). Nonetheless, a
concentration of social capital in the center of the field is evident.

6.3 Economic capital in terms of external research grants
Relating rank groups to the external funding they attract reveals a close connection between research
grants and status attributions (Figure 6.3). This is to be expected, because departments submit their funding information to the RAE/REF for assessment, and since incoming funds are performance indicators, they
directly influence the assessments. However, this means that the RAE/REF merely ennoble an established
research elite that is already successfully attracting third-party funding, while the economic periphery can
also expect worse overall grades in terms of research output. The RAE/REF thus attribute research performance to those departments that have attracted the most external funding in total and per research position.

13

These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

Figure 6.3: Rank groups and external research funding (in £), in total and per research position

External research funding in £ x 10000

Total

5000
4000
3000
2000
1000
0
Top 6

Top 14
RAE 2001

RAE 2008

Bottom 6

Bottom 14

REF 2014

External research funding in £ x 10000

Per research position
10
8
6
4
2
0
Top 6

Top 14
RAE 2001

Bottom 6
RAE 2008

Bottom 14

REF 2014

Sources: (RAE, 2001d; RAE, 2008d; REF, 2014b), author’s presentation and calculation; cf. footnote 4 for an elaboration on the extraordinary rise in funds in the top 14 of 2008.

Figure 6.3 demonstrates the unequal distribution of external research funding across RAE/REF rank groups.
It is illustrated for the absolute amount of external funds as well as for their distribution per FTE research
position. Elite departments tend to have the highest absolute amount of external funds as well as the
highest per capita amount. In a longitudinal perspective, the data reveals that the gap between the rank

14

These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

groups by external funds was much smaller in 2001 and has been growing since then. For example, the
top 6 departments of 2001 attract five times more external grants than the bottom 6, the top 6 of 2008
over 150 times more than the bottom 6, and the top 6 of 2014 still 18 times more than the bottom 6.4 This
aggravation applies to absolute funding and almost to the same extent to per capita funding.
The distribution of external research funds per capita, displayed in the lower half of Figure 6.3, requires
particular attention. Since top rank groups acquire more funds per capita than lower rank groups, the data
might suggest at first sight that elite departments do indeed perform better and thus confirm the
RAE/REF’s meritocratic ideology. If this was indeed the case, expensive and time-consuming peer review
assessments could be dispensed with and simply those departments rewarded that attract the most external funds. However, the underlying assumption that external funds – an input variable – are a reliable
indicator for research performance – an output variable – is questionable at least (cf. Johnes, 1996 on
input and output performance indicators). Some departments might attract little funding but use the funds
very well in order to produce “good” research; other departments might attract a lot of funding but use
the funds inefficiently – and thus “underperform”. Hence assuming that a concentration of external funding in the top ranked departments is a meritocratic distribution of economic resources confuses input with
output indicators.
Crucially, the meritocratic explanation of a concentration of external funding also ignores the marginal
utility a higher number of research staff has for attracting research grants. Personnel resources can be
expected to contribute to the accumulation of economic resources. More research staff means that a department has more time for grant proposals, not least because the research staff have an overall lower
teaching load. More research staff brings with it a larger network of colleagues, which in turn is advantageous for collaborations and reviews of proposals. Lastly, more research staff also leads to a higher visibility of departments in the field, again beneficial in a competitive research environment. In sum, research
staff has a marginal utility for external research funds (cf. Münch, 2007 for similar effects in the German
case). With the highly stratified distribution of research staff in mind (cf. Section 6.2), larger departments
can be expected to have better chances with research proposals due to the marginal utility of their research staff. This interpretation would explain the concentration of economic resources in the top rank

4

The extraordinary gulf between the rank groups from 2001 to 2008 (and between the top 6 and the top 14) is caused
by the exceptional financial position of the history department at UCL. The UCL Centre for the History of Medicine
received two major grants from the Wellcome Trust in the period in question. This significantly increased the funds
of the top 14 rank group in 2008 RAE (2008d) RAE 2008 submissions, UOA 62 History. [online] Available at:
http://www.rae.ac.uk/submissions/submissions.aspx?id=62&type=uoa [Accessed: 08.08.2015]. Subtracting the UCL
out of the top 14 rank group of 2008 “normalizes” the gap between 2001 and 2008, and puts the top 14 in a region
similar to the top 6.

15

These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

groups not in terms of meritocracy but as a self-reinforcing process that encourages the emergence of
monopoly structures.

7. Discussion: the consecration and (re-)production of research elites
This paper has examined stratification effects in relation to RAE/REF rank groups. First, data on symbolic
capital in terms of panel members reveals that a substantial amount of authority over evaluation criteria
is placed not only in the hands of very few researchers but in the hands of largely the same very few
researchers throughout three assessments. Elite departments have good chances of being represented on
a panel and, in turn, departments represented on a panel have good chances of becoming – or remaining
– elite departments. Simultaneously, being represented on a panel is a very good safeguard against being
pushed out into the periphery, while being in the periphery drastically reduces a department’s chances of
being represented on a panel. These relationships reveal a persistent concentration of symbolic capital
among the elite departments in the history field.
Second, data on social capital in terms of research staff suggest that departments might follow different
strategies in response to status allocations, namely, acquiring more or fewer research staff, or being more
or less selective when nominating staff for the assessment. Low scores in the assessment can lead to development strategies in which departments invest in research staff in order to perform better in the following assessment. However, low scores can also lead to a reduction in the research staff of a department,
or even its closure. What holds true either way is an unequal distribution of FTE research positions between elite departments and the periphery. The center-periphery structure in terms of social capital is
aggravated over time.
Third, data on economic capital in terms of external grants illustrates that the elite departments were
very successfully increasing their rate of external grants over time, while the periphery could not keep
pace. The widening gap between the status groups is hardly surprising, as it is based on a self-fulfilling
prophecy: successful acquisition of funds is already included as a performance indicator in the assessments
and thus directly influences status allocation. Departments performing well in the assessments are further
endowed with economic capital, and, completing the cycle, they can also be assumed to have advantages
for future external funding. Hence the RAE/REF reproduce an economic center-periphery structure and
consecrate the unequal distribution of external funds.
The many ways in which research elites are consecrated and (re-)produced have been revealed for the
discipline of history. Current research suggests that the findings also apply to other disciplinary fields (cf.
Campbell et al., 1999 for law studies; McNay, 2003 for education; Kehm and Leišytė, 2010 for medieval

16

These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

history; Lee et al., 2013 for economics). Still, further research must address a variety of disciplinary cultures
and how they relate to the effects of the assessments in different ways. In order to do this, a reasonable
starting point seems to be to distinguish social sciences and humanities disciplines from other disciplines
that have a culture of refereeing and linking quality to a hierarchy of journals, established rather uniform
paradigms guiding incremental research, and a weaker link between teaching and research (Martin and
Whitley, 2010). Only then can a more systematic comparative perspective with other disciplines be developed.
The empirical evidence at hand could be explained by a functionalist perspective. In this view, scarce
resources are allocated to the departments with the best research. Following this logic, departments that
attract high amounts of funding, employ high numbers of research staff, and are represented on assessment panels are indeed part of a meritocratic research elite that needs to be protected under higher education governance. It is therefore only logical that the center-periphery structure revealed here is further
developed and promoted by the assessments and the funding that accompanies them.
However, it is not by mistake that Merton (1968), himself an advocate of the functionalist perspective,
warned that a capitalization of research achievements would lead to the Matthew effect, according to
which the probability of gaining reputation or resources increases exponentially with every previous gain
in reputation or resources until these gains reach a point at which their marginal utility begins to diminish.
In this light, the evidence that departments already well endowed with economic resources receive further
funds, that the gulf between research staff endowment of elite and peripheral departments is widening,
and that there is an almost circular nomination of panel members and the top rank groups of their departments can be explained by the previous distribution of these resources. Research performance (alone)
does not seem to be the cause of this stratification. Another mechanism that, like the Matthew effect,
intertwines the material and symbolic dimension of stratification provides further important insights. According to Weber’s (1978) theory of social status, prestige hierarchies are reproduced when an elite adopts
a distinctive style of life, and when there is no social intercourse between status groups. This mechanism
of social closure explains persistent status differences between, on one side, peripheral departments that
have few resources at their disposal and (have to) concentrate on teaching and, on the other, elite departments that exemplify a privileged academic lifestyle with sufficient grants and research staff, deciding on
panels over the quality standards for the entire field. They enjoy the benefits of ample resources, including
network effects in terms of social capital, marginal utility effects in terms of research equipment, and
magnitude effects in terms of visibility in the field (Burris, 2004; Münch, 2008).

17

These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

The empirical evidence supports the claim that the RAE/REF not only consecrates reinforces the serious
stratification in the field by producing rankings that reward those at the center of the field with an “elite”
label. The assessments also (re-)produce research elites themselves because the distribution of basic research funding based on them follows previous allocations of resources. The production of research elites
through stratification is inherently antithetical to the distribution of rewards on the basis of universalistic
criteria of value or merit, as envisioned by functionalistic perspectives (Merton, 1973a). As soon as the
Matthew effect and the effect of social closure set in, any competitive distribution of funding privileges
those who enter the competition with ample resources at their disposal. The present contribution cannot
put forward a concise definition of research merit or “excellence,” and thus it cannot refute that the elites
produced by the RAE/REF are indeed research elites. However, it shows that the RAE/REF results, rather
than rewarding research elites alone, certainly reward resource elites, whose status is not necessarily
linked to actual research achievements.

References
Angermüller, J. (2010). Beyond Excellence – An Essay on the Social Organization of the Social Sciences and
Humanities. Sociologica, 2010(3), pp. 1-16.
Archambault, É., Vignola Gagné, É., Côté, G., Larivière, V. and Gingras, Y. (2006). Benchmarking scientific output in
the social sciences and humanities: The limits of existing databases. Scientometrics, 68(3), pp. 329-342.
Bence, V. and Oppenheim, C. (2005). The Evolution of the UK's Research Assessment Exercise: Publications,
Performance and Perceptions. Journal of Educational Administration and History, 37(2), pp. 137-155.
Blockmans, W., Engwall, L. and Weaire, D. (eds.) (2014) Bibliometrics: Use and Abuse in the Review of Research
Performance London: Portland Press.
Bourdieu, P. (1986). The Forms of Capital. In: Richardson, J. G., ed., Handbook of Theory and Research for the
Sociology of Education, New York: Greenwood Press, pp. 241-258.
Bourdieu, P. (1988). Homo Academicus. Cambridge: Polity Press.
Braun, D. and Merrien, F.-X. (eds.) (1999) Towards a New Model of Governance for Universitites? A Comparative
View. London: Jessica Kingsley.
Brown, R. and Carasso, H. (2013). Everything for Sale? The Marketisation of UK Higher Education. London: Routledge.
Burris, V. (2004). The Academic Caste System: Prestige Hierarchies in PhD Exchange Networks. American Sociological
Review, 69(2), pp. 239-264.
Campbell, K., Vick, D. W., Murray, A. D. and Little, G. F. (1999). Journal Publishing, Journal Reputation, and the United
Kingdom's Research Assessment Exercise. Journal of Law and Society, 26(4), pp. 470-501.
Cole, J. R. and Cole, S. (1973). Social Stratification in Science. Chicago: Chicago University Press.
Davis, K. and Moore, W. E. (1944). Some Principles of Stratification. American Sociological Review, 10(2), pp. 242249.
Deem, R., Hillyard, S. and Reed, M. (2008). Knowledge, Higher Education, and the New Managerialism: The Changing
Management of UK Universities. Oxford: Oxford University Press.
Elton, L. (2000). The UK Research Assessment Exercise: Unintended Consequences. Higher Education Quarterly,
54(3), pp. 274-283.
Espeland, W. N. and Sauder, M. (2007). Rankings and reactivity. How public measures recreate social worlds.
American Journal of Sociology, 113(1), pp. 1-40.
Hamann, J. (2016). The visible hand of research performance assessment. Higher Education, 72(6), pp. 761-779.

18

These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

Hamann, J. (2017). Ranking Research: Rankings as Devices in Higher Education Governance. In: Lessenich, S., ed.,
Geschlossene Gesellschaften. Verhandlungen des 38. Kongresses der Deutschen Gesellschaft für Soziologie in
Bamberg 2016, Forthcoming.
Hare, P. G. (2003). The United Kingdom's Research Assessment Exercise: Impact on Institutions, Departments,
Individuals. Higher Education Management and Policy, 15(2), pp. 43-61.
HEFCE (2015) How we fund research. [online] Available at: http://www.hefce.ac.uk/rsrch/funding/mainstream/
[Accessed: 25.05.2017].
Henkel, M. (1999). The modernisation of research evaluation: The case of the UK. Higher Education, 38, pp. 105-122.
Hicks, D. (2012). Performance-based university research funding systems. Research Policy, 41(2), pp. 251-261.
Hirschauer, S. (2010). Editorial Judgements: A Praxeology of 'Voting' in Peer Review. Social Studies of Science, 40(1),
pp. 71-103.
Johnes, J. (1996). Performance assessment in higher education in Britain. European Journal of Operational Research,
89, pp. 18-33.
Kehm, B. M. and Leišytė, L. (2010). Effects of New Governance on Research in the Humanities – The Example of
Medieval History. In: Jansen, D., ed., Governance and Performance in the German Public Research Sector.
Disciplinary Differences, Berlin: Springer, pp. 73-90.
Lamont, M. (2009). How Professors Think. Inside the Curious World of Academic Judgement. Cambridge, MA, London:
Harvard University Press.
Laudel, G. (2005). Is External Research Funding a Valid Indicator for Research Performance? Research Evaluation,
14(1), pp. 27-34.
Lee, F. S., Pham, X. and Gu, G. (2013). The UK Research Assessment Exercise and the narrowing of UK economics.
Cambridge Journal of Economics, 37(4), pp. 693-717.
Maeße, J. (2015). Economic Experts. A Discursive Political Economy of Economics. Journal of Multicultural Discourses,
10(3), pp. 279-305.
Maeße, J. (2016). The elitism dispositif. Hierarchization, excellence orientation and organizational change in
European economics. Higher Education, 73(6), pp. 909-927.
Martin, B. R. (2011). The Research Excellence Framework and the 'impact agenda': are we creating a Frankenstein
monster? Research Evaluation, 20(3), pp. 247-254.
Martin, B. R. and Whitley, R. D. (2010). The UK Research Assessment Exercise. A Case of Regulatory Capture? In:
Whitley, R. D., Gläser, J. and Engwall, L., eds., Reconfiguring Knowledge Production. Changing Authority
Relationships in the Sciences and their Consequences for Intellectual Innovation, Oxford: Oxford University
Press, pp. 51-80.
McNay, I. (2003). Assessing the assessment: an analysis of the UK Research Assessment Exercise, 2001, and its
outcomes, with special reference to research in education. Science and Public Policy, 30(1), pp. 47-54.
Merton, R. K. (1968). The Matthew Effect in Science. Science, 159(3810), pp. 56-63.
Merton, R. K. (1973a). The normative structure of science. In: Merton, R. K., ed., The Sociology of Science, Chicago:
University of Chicago Press, pp. 267-278.
Merton, R. K. (1973b). The Sociology of Science. Theoretical and Empirical Investigations. Chicago: University of
Chicago Press.
Moed, H. F. (2008). UK Research Assessment Exercises: Informed judgments on research quality or quantity?
Scientometrics, 74(1), pp. 153-161.
Morris, N. and Rip, A. (2006). Scientists' coping strategies in an evolving research system: the case of life scientists in
the UK. Science and Public Policy, 33(4), pp. 253-263.
Münch, R. (2007). Die akademische Elite. Zur sozialen Konstruktion wissenschaftlicher Exzellenz. Frankfurt/M.:
Suhrkamp.
Münch, R. (2008). Stratifikation durch Evaluation. Mechanismen der Konstruktion und Reproduktion von
Statushierarchien in der Forschung. Zeitschrift für Soziologie, 37(1), pp. 60-80.
Münch, R. (2014). Academic Capitalism. Universities in the Global Struggle for Excellence. New York: Routledge.
Münch, R. and Schäfer, L. O. (2014). Rankings, Diversity and the Power of Renewal in Science. A Comparison between
Germany, the UK and the US. European Journal of Education, 49(1), pp. 60-76.
Paradeise, C., Reale, E., Bleiklie, I. and Ferlie, E. (eds.) (2009) University Governance. Western European Perspectives.
Dordrecht: Springer.

19

These are proofs of the final publication available at Springer International Publishing:
http://www.springer.com/gp/book/9783319539690

RAE (1996) 1996 Research Assessment Exercise, Unit of Assessment: 59 History. [online] Available at:
http://www.rae.ac.uk/1996/1_96/t59.html [Accessed: 08.08.2015].
RAE (2001a) 2001 Research Assessment Exercise. Unit of Assessment: 59 History. [online] Available at:
http://www.rae.ac.uk/2001/results/byuoa/uoa59.htm [Accessed: 08.08.2015].
RAE (2001b) Panel List History. [online] Available at: http://www.rae.ac.uk/2001/PMembers/Panel59.htm [Accessed:
08.08.2015].
RAE (2001c) Section 1: Overview of the Research Assessment Exercise. [online] Available at:
http://www.rae.ac.uk/2001/pubs/2_99/section1.htm [Accessed: 08.08.2015].
RAE
(2001d)
Submissions,
UoA
History.
[online]
Available
at:
http://www.rae.ac.uk/2001/submissions/Inst.asp?UoA=59 [Accessed: 08.08.2015].
RAE (2001e) What is the RAE 2001? [online] Available at: http://www.rae.ac.uk/2001/AboutUs/ [Accessed:
08.08.2015].
RAE (2008a) Quality profiles. [online] Available at: http://www.rae.ac.uk/aboutus/quality.asp [Accessed:
12.10.2014].
RAE (2008b) RAE 2008 Panels. [online] Available at: http://www.rae.ac.uk/aboutus/panels.asp [Accessed:
08.08.2015].
RAE
(2008c)
RAE
2008
quality
profiles
UOA
62
History.
[online]
Available
at:
http://www.rae.ac.uk/results/qualityProfile.aspx?id=62&type=uoa [Accessed: 08.08.2015].
RAE
(2008d)
RAE
2008
submissions,
UOA
62
History.
[online]
Available
at:
http://www.rae.ac.uk/submissions/submissions.aspx?id=62&type=uoa [Accessed: 08.08.2015].
REF (2011) Assessment framework and guidance on submissions. [online] Available at:
http://www.ref.ac.uk/media/ref/content/pub/assessmentframeworkandguidanceonsubmissions/GOS%20in
cluding%20addendum.pdf [Accessed: 08.08.2015].
REF (2012) Panel criteria and working methods, Part 2D: Main Panel D criteria. [online] Available at:
http://www.ref.ac.uk/media/ref/content/pub/panelcriteriaandworkingmethods/01_12_2D.pdf [Accessed:
08.08.2015].
REF (2014a) Panel Membership, Main Panel D and sub-panels 27-36. [online] Available at:
http://www.ref.ac.uk/media/ref/content/expanel/member/Main%20Panel%20D%20membership%20%28S
ept%202014%29.pdf [Accessed: 08.08.2015].
REF (2014b) REF 2014 Results & submissions, UOA 30 - History. [online] Available at:
http://results.ref.ac.uk/Results/ByUoa/30 [Accessed: 08.08.2015].
Salter, B. and Tapper, T. (2002). The External Pressure on the Internal Governance of Universities. Higher Education
Quarterly, 56(3), pp. 245-256.
Sharp, S. and Coleman, S. (2005). Ratings in the Research Assessment Exercise 2001 – the Patterns of University
Status and Panel Membership. Higher Education Quarterly, 59(2), pp. 153-171.
Slaughter, S. and Leslie, L. L. (1999). Academic Capitalism: Politics, Policies, and the Entrepreneurial University.
Baltimore: John Hopkins University Press.
Talib, A. A. (2001). The Continuing Behavioural Modification of Academics since the 1992 Research Assessment
Exercise. Higher Education Review, 33(3), pp. 30-46.
Tapper, T. and Salter, B. (2003). Interpreting the Process of Change in Higher Education: The Case of the Research
Assessment Exercises. Higher Education Quarterly, 57(1), pp. 4-23.
Weakliem, D. L., Gauchat, G. and Wrigt, B. R. E. (2012). Sociological Stratification: Change and Continuity in the
Distribution of Departmental Prestige. The American Sociologist, 43, pp. 310-327.
Weber, M. (1978). Economy and Society, 2 Vol. Berkeley, Los Angeles: University of California Press.
Werron, T. (2014). On Public Forms of Competition. Cultural Studies ↔ Critical Methodologies, 14(1), pp. 62-76.
Zuccala, A., Guns, R., Cornacchia, R. and Bod, R. (2014). Can We Rank Scholarly Book Publishers? A Bibliometric
Experiment with the Field of History. Journal of the American Society for Information Science and Technology,
66(7), pp. 1333-1347.

20

