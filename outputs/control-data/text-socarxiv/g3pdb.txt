Successes and struggles with
computational reproducibility:
Lessons from the Fragile Families Challenge ∗
David M. Liua and Matthew J. Salganikb
a

Department of Computer Science, Princeton University
b
Department of Sociology, Princeton University
Direct correspondence to dml3@alumni.princeton.edu

Abstract
Reproducibility is fundamental to science, and an important component of reproducibility
is computational reproducibility: the ability of a researcher to recreate the results in a published paper using the original author’s raw data and code. Although most people agree that
computational reproducibility is important, it is still difficult to achieve in practice. In this
paper, we describe our approach to enabling computational reproducibility for the 12 papers
in this special issue of Socius about the Fragile Families Challenge. Our approach draws on
two tools commonly used by professional software engineers but not widely used by academic
researchers: software containers (e.g., Docker) and cloud computing (e.g., Amazon Web Services). These tools enabled us to standardize the computing environment around each submission, which will ease computational reproducibility both today and in the future. Drawing on
our successes and struggles, we conclude with recommendations to authors and journals.

∗

We thank the Board of Advisers of the Fragile Families Challenge. Research reported in this publication was supported by the Russell Sage Foundation and the Eunice Kennedy Shriver National Institute of Child Health and Human
Development of the National Institutes of Health under Award Numbers P2-CHD047879. Funding for the Fragile
Families and Child Wellbeing Study was provided by the Eunice Kennedy Shriver National Institute of Child Health
and Human Development through grants R01-HD36916, R01-HD39135, and R01-HD40421 and by a consortium of
private foundations, including the Robert Wood Johnson Foundation. We thank Caitlin Ahearn, Drew Altschul, Nicole
Carnegie, Connor Gilroy, Brian Goode, Seth Green, Jake Hofman, Alex Kindel, Dawn Koffman, Daniel Rigobon, Julia Rohrer, and Janet Xu for helpful feedback on drafts of this manuscript. This paper draws on David Liu’s senior
thesis for the Department of Computer Science at Princeton University (Liu, 2018). The content of this paper is solely
the responsibility of the authors and does not necessarily represent the views of anyone else.

1

1

Introduction
There is broad agreement that reproducibility is generally important for establishing a sci-

entific claim. In this paper, we focus on a very narrow form of reproducibility: a researcher’s ability
to recreate the results in a published paper using the raw data and code of the original author. Following Stodden (2015), we call this “computational reproducibility”; although this same concept
has also been called “verifiability” (Freese and Peterson, 2017) and “repeatability” (Easterbrook,
2014; Collberg and Proebsting, 2016).1
Although computational reproducibility is widely considered to be a desirable goal, it is a
standard that a lot of scientific research does not currently meet. There are two main factors that
contribute to this shortcoming. First, the data and code used to create results in published papers
are often not available to other researchers, despite requirements by some journals, professional
organizations, and funding agencies. Second, even if the data and code are available, independent
researchers are often not able to reproduce the results. These two patterns have been empirically
established in the social sciences (Dewald et al., 1986; McCullough et al., 2006; Wicherts et al.,
2006; In’nami and Koizumi, 2010; Wicherts et al., 2011; Vanpaemel et al., 2015; Wicherts and
Crompvoets, 2017; Gertler et al., 2018; Chang and Li, 2018; Hardwicke et al., 2018; Stockemer
et al., 2018; Wood et al., 2018), as well as other scientific fields (Savage and Vickers, 2009; Ioannidis et al., 2009; Vandewalle et al., 2009; Gilbert et al., 2012; Ostermann and Granell, 2017;
Collberg and Proebsting, 2016; Andrew et al., 2015; Stodden et al., 2018; Konkol et al., 2019;
Hardwicke and Ioannidis, 2018; Alsheikh-Ali et al., 2011; Vines et al., 2014; Rowhani-Farid and
Barnett, 2016; Naudet et al., 2018; Campbell et al., 2019). Although the magnitude of the problem
differs by field and across time, the limited availability of replication materials and the limited
usefulness of the materials that are available are consistent patterns. In other words, a lack of
computational reproducibility is itself a reproducible finding.
There are many admirable attempts to improve the computational reproducibility. One of
1
For more on the somewhat inconsistent terminology in this area see Goodman et al. (2016), Patil et al.
(2016), Clemens (2017), Plesser (2018), Barba (2018), and Benureau and Rougier (2018).

2

these efforts is the Transparency and Openness Promotion (TOP) Guidelines (Nosek et al., 2015).
The TOP Guidelines provide a framework that journals can use to assess and improve their efforts
to make scientific communication more open. Despite being introduced quite recently, more than
1,000 journals have already adopted the TOP Guidelines in some form.2
The TOP Guidelines consist of eight standards, including one called “Data, Analytic Methods (Code), and Research Materials Transparency.” When focused on computational reproducibility, this standard has three levels:
• Level 1 (“available upon request”): Replication materials available upon request from authors
• Level 2 (“access before publication”): Replication materials submitted to a trusted archive
before publication
• Level 3 (“verification before publication”): Replication materials verified by journal and
then submitted to a trusted archive before publication
For this special issue, we attempted to meet the TOP Level 3 standard (“verification before
publication”). Further, in addition to complying with the standard as written,3 we attempted to give
replication materials a more central role in the process of scholarly communication. Our thinking
was heavily influenced by what could be called Donoho’s dictum:4
“An article about computational science in a scientific publication is not the scholarship itself, it is merely advertising of the scholarship. The actual scholarship is
2

https://cos.io/our-services/top-guidelines/
The introductory paragraph for the Data, Analytic Methods (Code), and Research Materials Transparency standard
is: “The policy of the [Journal of Research] is to publish papers only if the data, methods used in the analysis, and
materials used to conduct the research are clearly and precisely documented and are maximally available to any
researcher for purposes of reproducing the results or replicating the procedure. All materials supporting the claims
made by the author must be made available to the journal prior to publication. The journal, or an entity acting on
behalf of the journal, will verify that the findings are replicable using the authors data and methods of analysis. Failure
to replicate at this stage may result in the paper not being published.” See https://osf.io/9f6gx/ for the most
recent version.
4
Although this quote is attributed to David Donoho, Donoho was himself influenced by Jon Claerbout and colleagues (Claerbout and Karrenbach, 1992). See Barba (2018) for more historical background.
3

3

the complete software development environment and the complete set of instructions
which generated the figures.” (Buckheit and Donoho, 1995)
As we attempted to give replication materials a more central role in the process of scholarly communication, we made three inter-related changes to standard practice. First, inspired by
insights from other computational fields, we took a broader view of what should be included in
replication materials. In particular, rather than focusing on just data and code, we expanded our
focus to include the computing environment (Stodden et al., 2016; Tatman et al., 2018). Second,
rather than merely verifying the results in the manuscript, we focused our efforts toward making
the replication materials as useful as possible to future scholars. Finally, rather than assessing computational reproducibility after completing peer-review of the manuscript, we made computational
reproducibility a part of the review process.
We found that computational reproducibility was difficult to achieve in practice. Only two
of the 14 manuscripts that were submitted to the special issue were computationally reproducible
initially, even though we had access to code provided by the authors. After an intensive revision
process, seven of the 12 accepted manuscripts eventually became computationally reproducible.
These aggregate results mask important heterogeneity. Some submissions to the special issue used
computational approaches that are relatively common in sociology and the other social science
today, for example Ahearn and Brand (2019) and McKay (2019). For these submissions, computational reproducibility was relatively easy. Some other submissions, however, drew on more
computationally complex approaches from machine learning, for example Rigobon et al. (2019)
and Davidson (2019). These submissions were substantially more difficult to reproduce.
We think this heterogeneity has two important lessons. First, our results suggest that relatively modest efforts could yield large increases in computational reproducibility for the methods
that are typically used today. The second important lesson is about the future. We expect that
sociologists and other social scientists will increasingly use larger data sets and more computationally intensive methods, such as the methods used in this special issue. Our experience suggests
that as research becomes more computationally intensive, the problems with computational repro4

ducibility will likely get more severe unless researchers make changes to the process of scientific
publishing.
The remainder of this paper is organized as follows. In the next section we provide more
background on the Fragile Families Challenge and computational reproducibility. Then, we describe our process for ensuring computational reproducibility for this special issue. Next, we offer
recommendations for authors wanting to make their work more computationally reproducible, and
we offer recommendations for journals wanting to promote computational reproducibility. We also
provide three online appendices: a summary of the computational reproducibility status of each paper in the special issue; the reproducibility memo that we sent to authors during the peer-review
process; and instructions for using the Docker images we created to accompany the articles in this
special issue.

2

Background

2.1

Fragile Families Challenge
The Fragile Families Challenge is a scientific mass collaboration designed to yield insights

that can improve the lives of disadvantaged children in the United States. The Challenge is described in more detail in Salganik et al. (2018), but we describe it here briefly with a focus on issues that are particularly relevant to computational reproducibility. The Fragile Families Challenge
builds on the Fragile Families and Child Wellbeing Study, which collects data about a probability
sample of births in 1998-2000 in large U.S. cities with an oversample of non-marital births (Reichman et al., 2001). During the course of the Fragile Families and Child Wellbeing Study information
has been collected from the mother, father, and child, as well as other people in the child’s life (e.g.,
teachers).
The Challenge set out the goal of using data collected during the child’s first nine years to
predict six outcomes when the child was 15 years old (Figure 1).5
5

Participants in the Challenge attempted to predict six outcomes: grade point average (GPA) of the child, grit

5

The Challenge used a research design from machine learning called the Common Task
Method (Donoho, 2017). The Common Task Method involves three main elements: a common
target for predictive modeling, a common error metric (in our case, mean squared error), and
a common data set with both training data available to participants and held-out data used for
evaluation. Following this structure, we split the year 15 data for these six outcomes into three
groups: 1) a training set which we provided to participants, 2) a leaderboard set which participants
could access during the Challenge, and 3) a holdout set which participants could not access until
the first stage of the Challenge was complete (Hardt and Blum, 2015) (Figure 1). Participants in the
Challenge received the training set and a specially constructed background data file that contained
information about the family from birth to age 9; it included 4,242 families and 12,942 variables
about each family.6

2.2

Computational Reproducibility
The benefits of computational reproducibility—and increased access to data and code, more

generally—have already been articulated many times by researchers in many different fields: archaeology (Marwick, 2017), bioinformatics (Mangul et al., 2018), cell biology (Grning et al.,
2018), computational fluid mechanics (Mesnard and Barba, 2017), computer systems research (Collberg and Proebsting, 2016), economics (Anderson et al., 2008; Koenker and Zeileis, 2009; Orozco
et al., 2018), epidemiology (Peng et al., 2006; Coughlin, 2017; Shepherd et al., 2017), geosciences (Claerbout and Karrenbach, 1992; Gil et al., 2016; Konkol et al., 2019), high-energy
physics (Chen et al., 2018), hydrology (Hutton et al., 2016), mathematical and computational biology (Schnell, 2018), machine learning (Tatman et al., 2018; Hutson, 2018), neuroscience (Crook
et al., 2013; Manninen et al., 2017; Eglen et al., 2017; Mikowski et al., 2018), political science (King, 1995; Lupia and Elman, 2014; Alvarez et al., 2018), psychology (Clyburne-Sherin
of the child, material hardship of the household, whether the household was evicted from their home, whether the
primary caregiver participated in job training, and whether the primary caregiver lost his or her job. The choice of
these outcomes was driven by our scientific goals and ethical considerations; each outcome is described in more detail
elsewhere (Salganik et al., 2018; Lundberg et al., 2018).
6
For more on the construction of the Fragile Families Challenge background file, see Lundberg et al. (2018).

6

Figure 1: Fragile Families Challenge data structure. Participants built models predicting the
age 15 outcomes using data collected between the time the focal child was born and 9 years old.
We provided participants with the data represented by the white boxes. Submissions were scored
based on their predictive performance (mean squared error) for the observations represented by
the gray boxes, which were available only to organizers. The leaderboard set contained 1/8 of all
observations and was used to provide instant feedback on submissions. The holdout set contained
3/8 of observations and was used to produce final scores for all submitted models at the end of the
Challenge.
and Green, 2018), signal processing (Vandewalle et al., 2009), sociology (Freese, 2007), and statistics (Gentleman and Lang, 2007; Donoho, 2010; Stodden, 2015). We find these arguments convincing. So, for the purposes of this paper, we simply assume that computational reproducibility
is a desirable goal.7 We acknowledge, however, that this is a relatively low standard; it does not
guarantee correctness of the code or the scientific validity of the claims (Leek and Peng, 2015).8
There are many wonderful efforts underway to increase computational reproducibility, and
our approach shares many features with these other efforts. However, there are three things that
partially differentiate our approach from other approaches in the social sciences. First, we were
not motivated by the “reproducibility crisis” or issues related to “questionable scientific practices.”
The structure of the Challenge—with explicit holdout data and a clear pre-specified evaluation
7

This is not to say that access to data and code is always desirable (Levy and Johns, 2016). There are of course
exceptions, such as data that contains personally identifiable information or code that contains propriety business information. However, these exceptions do not call into question the general idea that for scientific research, computational
reproducibility is a desirable goal.
8
Although computational reproducibility does not prevent errors, it does make it easier to diagnose and understand
these errors if they are later discovered, see for example Heuberger (2019).

7

metric—make concerns about questionable scientific practices less relevant. Rather, our approach
was focused on increasing research impact through re-use.9
The second way that our approach differs from some other approaches in the social sciences is that it focus on the computing environment, in addition to data and code. Currently, a
significant subset of published social science research is based on relatively small datasets using
methods that can be deployed in a fairly basic computing environment. In other words, it is not
uncommon for many social scientists to do their computing on a basic laptop computer. However,
some social scientists are beginning to use much larger data sources and more resource-intensive
computational methods from machine learning. These are potentially exciting developments, but
as computational complexity increases, it becomes increasingly difficult to reproduce results using
data and code running in a different computing environment (Boettiger, 2015; Boisvert, 2016; Tatman et al., 2018). Instead, for computationally complex work, reproducibility often requires access
to the same computing environment, both hardware and software. Fortunately, software engineers
have already developed tools and techniques that make it possible to standardize a computing environment, and for our Challenge reproducibility work we borrowed liberally from these practices.
In particular, for our computational reproducibility work in the Fragile Families Challenge we used
software containers (e.g., Docker) to standardize the software environment and cloud computing
(e.g., Amazon Web Services) to standardize the hardware environment.
Software containers and cloud computing are not yet common for computational repro9
To illustrate the value of re-use, we briefly sketch three research projects that are enabled by the computational
reproducibility of the papers in the special issue. First, one could imagine trying to create a super-model that combines the best pieces of each of the individual papers. For example, imagine combining the imputation approaches
from Goode et al. (2019), the approach to missing data from Carnegie and Wu (2019), the approach to adding expert
knowledge from Filippova et al. (2019), and the machine learning approach in Rigobon et al. (2019). Might this
super-model—or some other combination of existing strategies—perform better than the best strategy used during the
Challenge? Second, rather than combining modeling components, future researchers could explore different ways to
ensemble predictions. In Team (2019) we show that one such approach to combining predictions—stacking (Wolpert,
1992; Breiman, 1996)—did not lead to large improvements. Might other approaches, particularly approaches that
require running code, lead to better ensembling results? Third and finally, the predictions submitted in the Challenge
did not include any estimate of uncertainty. However, as predictive models get deployed for high-stakes social decisions (Rudin, 2018), it would be desirable for them to also produce estimates of uncertainty. Using existing ideas from
the literature (e.g., King et al. (2000)) would it be possible to produce confidence intervals from these models? What
might there coverage properties be? We emphasize that these are just three possibilities, and we expect that many
other ideas might occur to other researchers.

8

ducibility in the social sciences so we will briefly review them.10 Software containers, such as
Docker, are designed to ensure that software that runs on one computer can run in exactly the same
way on another computer. Containers ensure portability through encapsulation. Roughly, software containers create an appropriately configured mini-computing environment within a larger
computing environment, and then make it easy to move this mini-computing environment from
one computer to another, independent of the operating systems. This mini-computing environment
can include all the supporting software that the research code depends on, and it can keep these
dependencies separate from all the other code that runs on the host computer. For example, the
results in Stanescu et al. (2019) were created with code that requires version 0.7.4 of the R package dplyr. Without a software container, anyone wishing to reproduce the results in the paper
would need to install version 0.7.4 of dplyr—and all the packages on which it depends—on their
computer. This installation process is easy with just one relatively new, high-quality package. But
when code requires many packages—particularly old and complex packages—installing all the
necessary dependencies can be difficult, frustrating, and error-prone. These difficulties become
even more extreme if the researcher attempting to reproduce these results is also working on another project that requires a different version of the same package. The ugly process of installing
dependencies and managing competing dependencies across projects is sometimes referred to as
“dependency hell.” Containers prevent dependency hell. They do this by making an appropriately configured mini-computing environment (e.g., with the necessary version of dplyr) that
can easily be installed on a new computer and hidden from the other software running on the new
machine.
There are two main things a researcher can do with containers: create them and work
inside of them. To create a container, a researcher starts by making a list of the all the things
to be included in the computing environment. With Docker, this configuration file is called a
“Dockerfile.” Figure 2 shows the Dockerfile for Stanescu et al. (2019). Then, the researcher
10

For more about software containers and virtual machines (a closely related concept), we recommend Merkel
(2014), Boettiger (2015), and Clyburne-Sherin and Green (2018). For more about cloud computing in the context of
scientific research, we recommend Dudley and Butte (2010) and Howe (2012).

9

executes the Dockerfile, and Docker creates a bundle of all the necessary software. With Docker,
this bundle of software is called a “Docker image.” The Dockerfile for Stanescu et al. (2019)
is relatively simple, but some were much more difficult to create. For example, the Dockerfile
for Davidson (2019) required 87 packages.11
Once a Docker image is created and published online, a new researcher can install it to their
computer and use the image to create an appropriately configured container. The new researcher
can then go inside of the container and work in a computing environment that was created to
support the replication of the original scientific results. Further, this computing environment is
separate from the larger computing environment on the new researcher’s computer. A researcher
downloading and using a Docker image does not need to understand exactly how it was created,
which means that it is much easier to use an image than create one. Although Docker itself may
be replaced in the future by alternative tools, we believe that the idea of software containers will
likely be important for computational reproducibility for the foreseeable future.
While software containers standardize the software environment, cloud computing can be
used to standardize the hardware environment. Roughly, a cloud computing environment is a huge
collection of computing resources, which can be subdivided so that different users can access different amounts of computing resources. Creating and managing cloud computing environments
is extremely complex, but from the perspective of a user, they are relatively simple. At the time
of the Fragile Families Challenge, large companies such as Amazon, Microsoft, and Google of11
The 87 required packages and version numbers for Davidson (2019) are: absl-py v0.6.1, appnope v0.1.0, astor v0.7.1, backcall v0.1.0, bleach v3.0.2, cloudpickle v0.6.1, cvxpy v1.0.10, cycler v0.10.0, dask v0.20.0, decorator
v4.3.0, defusedxml v0.5.0, dill v0.2.8.2, ecos v2.0.5, entrypoints v0.2.3, fancyimpute v0.4.0, fastcache v1.0.2, future v0.17.1, gast v0.2.0, grpcio v1.16.0, h5py v2.8.0, ipykernel v5.1.0, ipython v7.1.1, ipython-genutils v0.2.0, ipywidgets v7.4.2, jedi v0.13.1, Jinja2 v2.10, jsonschema v2.6.0, jupyter v1.0.0, jupyter-client v5.2.3, jupyter-console
v6.0.0, jupyter-core v4.4.0, Keras v2.2.4, Keras-Applications v1.0.6, Keras-Preprocessing v1.0.5, kiwisolver v1.0.1,
knnimpute v0.1.0, lime v0.1.1.32, Markdown v3.0.1, MarkupSafe v1.1.0, matplotlib v3.0.1, mistune v0.8.4, multiprocess v0.70.6.1, nbconvert v5.4.0, nbformat v4.4.0, networkx v2.2, notebook v5.7.0, np-utils v0.5.5.2, osqp v0.4.1,
pandas v0.23.4, pandocfilters v1.4.2, parso v0.3.1, patsy v0.5.1, pexpect v4.6.0, pickleshare v0.7.5, Pillow v5.3.0,
prometheus-client v0.4.2, prompt-toolkit v2.0.7, protobuf v3.6.1, ptyprocess v0.6.0, Pygments v2.2.0, pyparsing
v2.3.0, python-dateutil v2.7.5, pytz v2018.7, PyWavelets v1.0.1, PyYAML v3.13, pyzmq v17.1.2, qtconsole v4.4.2,
scikit-image v0.14.1, scikit-learn v0.20.0, scipy v1.1.0, scs v2.0.2, Send2Trash v1.5.0, six v1.11.0, sklearn v0.0,
statsmodels v0.9.0, tensorboard v1.12.0, tensorflow v1.12.0, termcolor v1.1.0, terminado v0.8.1, testpath v0.4.2, toolz
v0.9.0, tornado v5.1.1, traitlets v4.3.2, wcwidth v0.1.7, webencodings v0.5.1, Werkzeug v0.14.1, widgetsnbextension
v3.4.2.

10

Figure 2: The Dockerfile above specifies the dependencies required for Stanescu et al. (2019).
In addition to R, we install system packages, such as libssl-dev, and R packages, such as
dplyr. For the R packages, we first installed the newest version of the required packages along
with the necessary dependencies. However, sometimes these newest versions did not match the
versions used by the authors. In these situations, we then overwrote the newest version with the
one that was specified by the authors.

11

fered cloud computing environments that enabled people to rent computing resources by the hour.
For example, at the time of the Challenge, Amazon Web Services—Amazon’s cloud computing
environment—allowed people to rent computational resources with names like “m5.large” and
“t3.medium.” These different environments came with different types of processors, RAM, and
other computing resources. We chose to perform our reproducibility work on Amazon Web Services because we thought that this would increase the chance that other researchers will be able to
access a very similar hardware environment for the foreseeable future. Having access to a similar hardware environment is helpful because the performance and output of complex software can
sometimes depend on the hardware that is used to execute it.12 Future researches, however, will
not be required to use Amazon Web Services or any other cloud computing provider in order to
use the Docker images that we created. Together, software containers and cloud computing make
it easier for researchers to have access to a very similar computing environment to the one we used
when verifying computational reproducibility, both today and for years to come.
The third and final way that our approach differed from common approaches in social science is timing. To the best of our knowledge, most work on computational reproducibility review in
social science journals happens after the manuscript has already been conditionally accepted. Our
approach, however, involved computational reproducibility review simultaneous to manuscript review. We think that this simultaneous review appropriately emphasizes that the manuscript and
code work together to communicate a scientific finding.

3

Our process for the special issue
Although we had some experience with computational reproducibility in our roles as re-

searchers, we had no experience with computational reproducibility from the perspective of a journal editor. This inexperience meant that we ended up using a process that we now realize was
12

For example, the runtime typically depends on the amount of RAM, the number of processors, and the type of
processor (e.g., CPU vs GPU). For more about how the underlying computing hardware can impact the results of
scientific computing, see Diethelm (2012).

12

not ideal in several ways. Before offering our recommendations for the future, however, we will
describe the process that we used for the special issue.
In our call for papers for the special issue, we stated that manuscripts needed to be accompanied by open source code that would allow another researcher to reproduce all the figures and
tables in the manuscript.13 However, we did not provide specific guidelines about the form that the
code should take. Further, our call for papers had specific deadline by which manuscripts needed
to be submitted (October 16, 2017).
Fourteen manuscript were submitted. While these manuscripts were undergoing peer review, we attempted to reproduce the results in each manuscript. Having computational reproducibility reviews happening at the same time as manuscript peer reviews was consistent with our
goal of considering replication materials to be a central part of scholarly communication.
Our process of computational reproducibility review generally involved five steps: 1) create
a Docker image with the necessary software and packages; 2) move the authors’ code into the
Docker image and adjusting file paths to match those of the container; 3) adding the Challenge
data file to the container14 ; 4) run the authors’ code inside the container in our cloud computing
environment15 ; 5) compare the results created within our computing environment to the results
shown in the manuscript. We considered a manuscript to be computationally reproducible if we
could run the author’s code in our computing environment and regenerate 1) all the prediction.csv
files (these are the files participants created to record their predictions of all six outcomes for all
4,242 families) within our error tolerance16 ; 2) all the figures that were generated by the author’s
13
The full call for papers is included in Salganik et al. (2018), and here is the part most related to computational
reproducibility: “What are the requirements for the open source code? The code must take the Fragile Families
Challenge data files as an input and produce (1) all the figures and tables in your manuscript and supporting online
materials and (2) your final predictions. The code can be written in any language (e.g., R, Stata, Python). The code
should be released under the MIT license, but we will consider other permissive licenses in special situations.”
14
In order to help protect the privacy of participants in the Fragile Families and Child Wellbeing Study, authors were
not supposed to the include the Challenge data file in their computational reproducibility materials. For more about
the data access process during the Challenge, see Lundberg et al. (2018).
15
As our computational reproducibility review improved, we would run the same code twice and compare the
results. If these results didn’t match, we didn’t even attempt to compare to the manuscript.
16
We considered two numbers the same if they differed by less than 10−10 . We acknowledge that this is an arbitrary
standard, but this threshold was effective for us because most differences were either much larger or smaller. In other
situations a different threshold might be reasonable, and we hope that community standards develop for when two
numbers should be considered the same.

13

code; and 3) all the tables that were generated by the author’s code.
For some manuscripts, this process went smoothly, but for many others it was quite painful.
It was often extremely difficult to even run the authors’ code. In some cases, authors sent lots of
unnecessary code, and in other cases, authors did not submit all the code that was necessary.
Further, the code that was provided was often disorganized and poorly documented. Even if we
were able to get the code to run, it was also often difficult to compare the results produced by the
code to the results from the manuscript.
During this initial computational reproducibility review, we were able to approximately
reproduce the results in two of the 14 manuscripts. We had this low rate of success despite being
in frequent email contact with some authors and often devoting many hours trying to reproduce the
results of each manuscript.17
Initially, we had hoped to provide each author with individual feedback on their project’s
computational reproducibility, much as each author receives individual feedback on their manuscript.
Unfortunately, this proved to be too time consuming for us given our desire—and the desire of the
editors of Socius—to provide rapid feedback to the authors. Ultimately, each author received
individual feedback on their manuscript from editors of the special issue and about three anonymous reviewers, and each author received collective feedback on how computational reproducibility could be improved. This collective feedback—reproduced in the Appendix of this paper—was
developed inductively based on our struggles with the initial code submissions, as well as on our
review of existing papers about computational reproducibility.
All 14 manuscripts received a decision of “revise and resubmit,” and authors were instructed to improve both their manuscript and code based on the feedback that was provided.
Fortunately, we saw dramatic improvements in code structure and documentation between the
initial submissions and the revised versions. We don’t know, however, how much of this improve17

This low success rate roughly matches the rate for the Quarterly Journal of Political Science. From September
2012 to November 2015, the replication materials for 20 of the 24 accepted empirical papers were found to require
some kind of modification. Further, 14 out of 24 accepted empirical papers were found to have discrepancies between the results generated by authors’ code and those in their manuscripts (Eubank, 2016). However, the journal
International Interactions reports higher success rates. Between 2014 and 2016, they successfully replicated 22 of 33
manuscripts without needing to contact the authors (Colaresi, 2016).

14

ment should be attributed to the guidelines that we provided to authors and how much should
be attributed to other factors, such as the authors realizing that we were treating computational
reproducibility as a part of the review process.18
After the authors received this first round of feedback, the synchronization of the manuscript
review and the computational reproducibility review broke down. In some cases, our computational reproducibility review would proceeded while the manuscript was under review, and in
other cases it would proceeded while the authors were working on their revisions. This second
stage of the computational reproducibility review was particularly time-consuming because it required us to precisely identify—and attempt to eliminate—the discrepancies between the results
in the manuscript and the computationally reproducible results. This process was occasionally
frustrating for us, and we are sure that it was occasionally frustrating for the authors.
Eventually, the editors of Socius informed us that we needed to bring the review process
for the special issue to an end. We gave all authors with outstanding manuscripts a deadline of
November 13, 2018 to submit their final manuscript and final code. In the end, we completely
reproduced the results of seven of manuscripts; for the other manuscripts, we ran out of time (although for many of these we did make substantial progress). For each of the 12 manuscripts,
the code will be released along with an accompanying Docker image with the necessary dependencies. Thus, all papers in the special issue reached TOP Level 2 (“access before publication”),
and most reached TOP Level 3 (“verification before publication”). The appendix summarizes the
computational reproducibility of each paper in the special issue.
The most important lesson that we take from our experience is that ensuring computational
reproducibility was harder than we had anticipated. There was not one specific thing that caused
huge difficulties, but many of the difficulties came from a combination of three inter-related factors:
many moving parts, long feedback cycles, and communication difficulty. To illustrate these factors,
we will briefly summarize our attempts to replicate the results in Altschul (2019), which was
neither particularly easy nor particularly difficult.
18

One way to assess the impact of these factors and others would be some kind of randomized controlled trial, where
different authors are randomly assigned to different review processes. See, for example, Shah et al. (2018).

15

At first glance, the code for Altschul (2019) appeared straight forward to reproduce because
it is a single R file. Yet, this code required 13 R packages to be installed, all of which had to be
built from source—a more error-prone and time consuming form of package installation—because
our cloud computing environment used Linux.19 Further, these packages in turn depended on many
other packages which also had to be installed. Although each package can be built from source
with a high probability of success p, the entire process will fail if just one package fails, and the
probability that at least one package fails is 1 − pn , where n is the number of packages. So, as
the number of packages increases, the probability of at least one failure increases exponentially.
Beyond just installing packages, many of the submissions had a large number of components and a
failure of just one component caused a failure of the entire process. This is what we call the “many
moving parts problem.”
The difficulty of the many moving parts problem was exacerbated by a long delay between
execution and feedback. The process of debugging computational reproducibility issues often involves attempting to isolate the problem and then use trial-and-error to solve the problem. The
longer we had to wait to receive feedback, the longer it would take for us to complete many iterations of this isolation and trial-and-error process. For example, for Altschul (2019) it took about
one hour of computing time to build the Docker image, and many attempts to isolate and eliminate
discrepancies required building a new Docker image and then re-running the code, which would
often have to run for several hours. These long computing times made the process of debugging
very slow.
Finally, our slow debugging often had to go through many iterations because of communication difficulties. In the 13 months between initial submission and final publication, we exchanged
a total of 28 emails with Altschul. These exchanges spanned long periods of time not only because
we were also communicating with many other authors at the same time, but also because each reply could require hours of additional work. Although not true for Altschul (2019), we also noticed
that, in some cases, as time went on, it became increasingly difficult for the authors themselves to
19

Pre-compiled R package binaries are only available for computers using Windows and macOS.

16

respond to us quickly because it had been such a long time since they had worked with their own
code.
We describe these difficulties replicating Altschul (2019) not because it was a particularly
hard case, but in order to illustrate the kind of difficulties that we faced as a result of the many
moving parts problem, long feedback times, and communication difficulties. Undoubtedly many
of these difficulties were caused by our inexperience with computational reproducibility, and later
in the paper we offer suggestions for handling these issues more efficiently.
Stepping back, our experience with the Challenge was unusual in a number of ways, and
other editors may have different experiences with computational reproducibility in other settings.
In fact, we can imagine some reasons to expect that computational reproducibility will be easier in
other settings and some reasons to expect the opposite. We think there are three main reasons that
ensuring computational reproducibility should be easier in other settings. In sociology and the social sciences more generally, most researchers use simpler data processing and statistical modeling
pipelines than were used in the Challenge. We had the easiest time reproducing manuscripts that
use methods more common in the social sciences such as Ahearn and Brand (2019) and McKay
(2019). We had more difficulty, however, with manuscripts that included more machine learning
techniques, such as Davidson (2019), Rigobon et al. (2019), and Carnegie and Wu (2019). Given
the relatively small number of manuscripts it is hard to know precisely why those that used more
machine learning methods were harder, but some of the difficulties we encountered included much
longer run times and the use of parallelization. Second, the Challenge involved deadlines: there
was a hard deadline for submitting to the Challenge and another hard deadline for submitting to
the special issue. We think that these deadlines, which are uncommon in the social sciences, may
have caused code quality to suffer. Finally, we think that in the future, authors and editors will
have more experience with computational reproducibility (and we hope that some of the ideas in
this paper can help with that).
On the other hand, we think that achieving computational reproducibility could be harder in
other settings for three main reasons. First, in other settings there may be less buy-in from authors

17

and editors; in this case, we think the fact that the Fragile Families Challenge was a mass collaboration helped set the tone of open and reproducible research. Second, in other settings authors may
have less experience writing code, which could create barriers to computational reproducibility.
Third, in other settings, authors and editors will have to deal with the difficulties related to data
access, which did not arise here because all Challenge participants were using the same dataset
that we had provided.20

4

Recommendations for authors
Based on our experience with the special issue—as well as the suggestions of others (Na-

gler, 1995; Peng, 2009; Barnes, 2010; Sandve et al., 2013; Fehr et al., 2016; Bowers and Voors,
2016; Stodden et al., 2016; Eubank, 2016; Wilson et al., 2017; Marwick et al., 2018; Alvarez
et al., 2018; Grning et al., 2018; Benureau and Rougier, 2018; Tatman et al., 2018; Konkol et al.,
2019)—we have some recommendations for authors who wish to improve the computational reproducibility of their results now and in the future. We believe that these recommendations are
applicable to all authors, even if they are not using software containers and cloud computing.
We find it helpful to conceptualize the code used to create the results in a manuscript as a research pipeline (Peng and Eckel, 2009). The research pipeline accepts the rawest data and produces
all the final outputs in the manuscript (Figure 3). In order to promote computational reproducibility, we recommend that researchers make their research pipeline automated, modular, and friendly.
Before describing these three ideas in more detail—and offering a five-item checklist—we want
to emphasize that our experience with the Challenge suggest that most researchers do not create
automated, modular, and friendly research pipelines without effort. In the Challenge, the code that
was most effective had repeatedly gone through a process that software engineers call refactoring.
Roughly, refactoring involves taking code that already works and improving its internal structure
without changing its external behavior (Fowler et al., 1999). In professional software engineer20

For more on data sharing, privacy, and re-identification risk, we recommend Lundberg et al. (2018), Meyer
(2018), Salganik (2018), and Crosas et al. (2018).

18

Figure 3: Research pipeline based on Figure 1 of Peng and Eckel (2009). When writing code—
and refactoring code before submissions—authors can imagine a research pipeline that starts with
the rawest form of the data, what Peng and Eckel (2009) called the “measured data” and ends
with all the figures, tables, and numerical results in an article. In order to promote computational
reproducibility, we recommend that authors make their research pipeline automated, modular, and
friendly.
ing, refactoring is not considered a sign of poor work, rather it is considered a best-practice that
acknowledges the reality that software projects evolve over time and that design decisions made
early in the project may need to be change as the project evolves. In the context of the computational reproducibility of an academic paper, refactoring might include changes such as removing
code that is no longer used, improving variable names, and adding a loop or writing a function to
eliminate redundant blocks of code. Just as a researcher often writes, re-writes, and re-re-writes a
manuscript in order to communicate ideas as clearly as possible, we think that a researcher should
also expect to write, re-write, and re-re-write their code in order to end up with a computationally
reproducible research pipeline.

4.1
4.1.1

Three guiding principles
Automated
A research pipeline should run from start to finish without human intervention; it should

be fully automated. The desire for end-to-end automation is not often common during the research
19

process because authors are often focused on creating a specific component of the pipeline, such
as a specific figure. However, once a manuscript is complete, it is important that all the individual
software components seamlessly communicate with each other and can execute from start to finish
without any manual intervention to produce all the results that are presented in the manuscript.

4.1.2

Modular
As code becomes more complex, it becomes increasingly important to break it into inde-

pendent modules, where each module serves a specific purpose and the inputs and outputs of each
module are clear. Modular code offers many benefits during the research process and during the
process of preparing the code for others (as well as for your future self). While doing research,
modularity benefits authors by making the code easier to understand, maintain, and improve. Further, for analyses that involves long run time—such as many of the research pipelines in the Fragile
Families Challenge—modularity lets authors run only specific sections of their code.
Additionally, modularity helps computational reproducibility in three ways. First, it helps
future researchers understand how the code works; a well-designed, modular code architecture is
a wonderful form of documentation. Second, if computational reproducibility is not successful,
having modular code dramatically improves the chances to find and then fix problems. Finally,
modular code promotes refinement and reuse by future researchers, because it enables them to
easily focus on only the parts of the code that are most important to them.

4.1.3

Friendly
The final characteristic of a good research pipeline is that it is user-friendly. In other words,

even thought the code is written to be executed by a computer it should also be written to be read
by a person.21 A user-friendly research pipeline is organized, clean, and clear for someone else
without detailed knowledge of the code base. In the special issue, we learned that code that was
21

Here we are inspired by the idea of literate programming in Knuth (1984): “Instead of imagining that our main
task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a
computer to do.”

20

clear to authors was not always clear to us. To promote friendliness, therefore, we encourage
authors to attempt to put themselves in the place of others and ask: “What would I want if I was
trying to understand, reproduce, and then improve this code?”

4.2

Five-item checklist
The three principles that are described above—Automated, Modular, and Friendly—are

designed to provide high-level guidance to authors. However, these principles are intentionally
abstract, and they don’t always provide clear action-guiding advice. Therefore, we now present a
simple five-item checklist that is designed to help authors—and could be used by journals. This
checklist does not cover every possible situation, but it is designed to be helpful to a wide range of
social scientists, impose minimal costs, and be independent of the programming language used by
the researcher.

4.2.1

Run-all Script
We recommend that authors include a single “run-all” script that contains a sequence of

commands that reproduces all the results in the paper. For example, during the Challenge, we
suggested that authors write a bash script to call their Python, R, or Stata files.22 The run-all script
serves as a project blueprint; it provides a high level understanding of the relationships among the
scripts, without the need to understand each script individually. The run-all script was one of the
first things we reviewed when attempting to reproduce a result, and so it should be considered a
key piece of documentation. For example, Figure 4 reproduces the run-all scripts from Filippova
et al. (2019). The script first executes an RMarkdown file that contains data cleaning code before
executing a series of imputation and then modeling scripts. Without this run-all script it would
be nearly impossible for someone trying to reproduce these results to know the proper order of
execution of these 17 scripts and the appropriate way to move the output of some scripts into
another directory.
22

For more complex scripts, authors could also consider using a Makefile (Piccolo and Frampton, 2016).

21

Figure 4: This “run-all” script from Filippova et al. (2019) clarifies the exact commands needed to
execute all of the scripts as well as the order these scripts should be executed.
4.2.2

Informative Directory Structure
To supplement the run-all script, we recommend that authors organize their files into a

clear directory structure. For example, we asked authors to organize their files in three major
directories: code, data, and output. A standardized directory structure is helpful during the
execution process because some complex pipelines involve creating multiple intermediate files
and the directory structure helps maintain order. Authors can adjust their directory structure to
fit the nature of their code. For example, the directory structure in Rigobon et al. (2019)—shown
in Figure 5—made many common operations more convenient. First, all of the raw Challenge
data files—which are not included in the replication materials because of privacy concerns—were
meant to be placed in a clearly specified directory (/data). Second, all of the final results created
by the script were placed in a single directory named “final pred”. This enabled us to save a copy
of these results in “final pred author” before re-running the script, which made it easy for us to
compare the two sets of results.

22

Figure 5: Directory structure for Rigobon et al. (2019). Because all of the prediction files the authors generated resided in a single sub-directory, we were able to easily save a copy of the original
results (“final pred author”) before generating our own version (“final pred”) for comparison.
4.2.3

Requirements Files
We recommended that authors include a file listing all of the software and add-on packages

used, including the exact version numbers (e.g., “ggplot2 v3.1.0”). Understanding the software
requirements is helpful for reproducing results today, and it is necessary for reproducing results in
the future. Many of the papers in the special issue used open source software with a large number
of add-on packages, and these packages are regularly updated with improvements. While these
improvements are generally good for users, they are problematic for reproducibility. These new
version sometimes introduce what are called breaking changes, which cause code that used to work
to suddenly break. For example, the improvements between Python 2 and Python 3 introduced
some breaking changes into Compton (2019), which we discovered when we attempted to use
Python 3 to import pre-processed data files that had been serialized with Python 2. Further, new
versions sometimes cause much more subtle errors. For example, there was a change in Python’s
pseudo-random number generator between Python 3.2 and Python 3.3 that could cause some results
to fail to be computationally reproducible (Benureau and Rougier, 2018).
The deadline for initial submission of manuscript and code to the special issue was October 17, 2017, and the deadline for the final, revised submissions was November 13, 2018. During

23

this 13 month period many of the packages used by participants during the Challenge were improved. However, these improvements also lead to problems for computational reproducibility.
For example, we when we attempted to reproduce Raes (2019) using the most recent version of
all the packages used, the results only matched for 7 of the 11 prediction files. For the other files,
the differences were sometimes off by a constant and sometimes off by random noise (Figure 6).
However, after matching package versions exactly, we were about to reproduce the results exactly (Figure 6). We close by noting that the problems we had with package versions during the
relatively short time window of the special issue will grow over time. Without knowing specific
packages numbers, we think it will be nearly impossible for researchers in the future to reproduce
Challenge results, even if the data and code are otherwise available.

4.2.4

Helpful Code Headers
We recommended that researchers clearly document their code. Unfortunately, we found

that some authors struggled with this very general advice. Therefore, more specifically, we recommend that authors produce code header that contain the following information:
• Purpose (in 280 characters or less)
• Inputs (i.e., What files does this piece of code require? Is there any code that needs to be run
before this code?)
• Outputs (i.e., What files, tables or figure does this piece of code create as outputs?)
• Machine used (i.e., computer type (laptop, desktop, cluster), operating system)
• Expected runtime (e.g., seconds, minutes, hours, days, etc)
For example, Figure 7 presented the code header from the data cleaning script for Ahearn
and Brand (2019). Each piece of information helps the replicator understand what should happen
upon execution.

24

(a) Model 5 (GPA) before aligning package versions.

(b) Model 5 (GPA) after aligning package versions.

(c) Model 6 (GPA) before aligning package versions.

(d) Model 6 (GPA) after aligning package versions.

Figure 6: Examples from Raes (2019). Without specifying package versions, we could not replicate the results, even though we were using the exact same data and code. Further, there was no
clear pattern to the errors: sometimes they were off by a constant and sometimes they were off by
random noise. However, once we specified the exact package versions in our container, we were
able to reproduce the original results exactly.

25

Figure 7: The above code header is taken from the data cleaning script for Ahearn and Brand
(2019). The information clearly outlines the code’s structure and characteristics.
4.2.5

Run twice
Once authors have completed the other four items on their checklist, we recommend that

they run the code twice and ensure that it produces the same results. Running the code twice is
necessary if it uses a pseudo-random number generator. Roughly, every time a computer generates
a sequence of random numbers, the results will be different. But, these results are not truly random;
rather they are a pseudo-random sequence of numbers that are determined by a single seed value
that was used to initiate the sequence. If the code sets the seed value explicitly, then the results
will be reproducible, but if it does not explicitly set a seed value, the exact same code, using the
exact same data, and run on the exact same computer can produce different results.23 For example,
initially we were unable to reproduce the results of Stanescu et al. (2019) because their code did
not explicitly set a seed value. However, after they explicitly seeded their code, we were able
to reproduce their results. For some more computationally complex submissions, multiple seed
values had to be set for different packages that used different seeds. For example, Davidson (2019)
required five different seeds, ranging from the Python standard library’s seed to TensorFlow’s own
graph-level seed. Thus, for some complex submissions, such as Davidson (2019) and Roberts
23

For more on random number generation and seeding, see Press et al. (2007).

26

(2019), it was often difficult to detect if the code had been properly seeded, and an empirical
test—such as testing for consistency across runs in the same computing environment—was most
effective.
We hope that these three principles and this five-item checklist can help authors who want
to make their work more computationally reproducible for other researchers. There are also steps
that journals can take to promote reproducibility, and we offer some of those recommendations
next.

5

Recommendations for journals
Although we believe that individual authors have strong incentives to practice open and

reproducible research, journals and other institutions also play a key role in accelerating progress
toward open and reproducible science (Freese and King, 2018). Building on our experience with
computational reproducibility in this special issue, we offer a menu of four options that journals
can take to promote computational reproducibility, we make specific suggestions related to TOP
Level 3, and we present our preferred process.

5.1

Menu of options
Journals can promote computational reproducibility with sticks and carrots. That is, jour-

nals can force authors to comply with TOP Level 2 or TOP Level 3 (sticks). Or, they can induce
authors to comply these guidelines with benefits, such as faster review time or increased probability of acceptance (carrots). Of the carrots available to journals, we believe that a badging system
is a particularly promising option. Badges are small symbols that appear on published papers
that quickly and clearly indicate the open science practices associated with that paper (Figure 8).
Badges reward authors for following open science practices, and there is some evidence that badging systems have had their intended effect of promoting open and reproducible research (Kidwell
et al., 2016; Rowhani-Farid et al., 2017). Social scientists are probably most familiar with the badg-

27

ing system developed by the Center for Open Science and first used by the journal Psychological
Science starting in January 2014 (Eich, 2014; Kidwell et al., 2016; Rowhani-Farid et al., 2017).
However, similar badging systems have also been developed in other fields. The journal Biostatistics introduced a badging system in 2009 (Peng, 2009, 2011), and the Association for Computing
Machinery (ACM), a prominent professional association of computer scientists, recently developed
a badging system specifically focused on computational reproducibility (Boisvert, 2016). Badging
systems are often opt-in—that is, authors can choose to participate or not—which makes them
easier to implement than other systems.24
Given the two main levels of computational reproducibility—TOP Level 2 (“access before
publication”) and TOP Level 3 (“verification before publication”)—and two main policy options—
requirements and badges—we think that journals could consider four main options: 1) badge to
TOP Level 2; 2) require TOP Level 2; 3) require TOP Level 2 and badge to TOP Level 3; and
4) require TOP Level 3. In this special issue, we attempted to require TOP Level 3, and we were
not able to achieve this goal in the time available. Therefore, we ended up requiring TOP Level 2,
although we note that many of the articles in this special issue actually did achieve TOP Level 3.

5.2

Suggestions regarding TOP Level 3
For journals considering either requiring TOP Level 3 or badging to TOP Level 3, we have

five suggestions. First, journals should have a clear plan for who is going to do the additional work
that is required to ensure computational reproducibility—we think the most likely possibilities
include authors, reviewers, editors, existing journal staff, and new journal staff.25 In our case, we
24

Given that defaults can have a big impact on behavior (Johnson and Goldstein, 2003), we think that journals could
also consider a badging system that is opt-out. In other words, authors would be assumed to follow open science
practices and badges would mark papers where these practices are not followed. To the best of our knowledge, no
journal currently uses this kind of opt-out badging system.
25
We wish to emphasize that the people needed to do this additional work related to computational reproducibility
will need skills that are already in high demand in research and industry. For example, doing the computational reproducibility work for this special issue required basic familiarity with bash scripting, Git, Docker, and cloud computing;
in-depth familiarity with R and Python; and a strong background in statistics and machine learning. People with this
skill set are currently in high-demand, and we expect that for the foreseeable future, people who have the skills needed
to computationally verify scientific research are likely to have many other demands on their time.

28

(a) Badges from the Center for Open Science.

(b) Badges from the Association for Computing Machinery.

Figure 8: Example badges from the Center for Open Science (Kidwell et al., 2016) and the Association for Computing Machinery (Boisvert, 2016). Badges are small symbols that appear on
published papers that indicate the open science practices that apply to this particular article. They
are an inducement that journals have to promote different research practices.

29

did not realize how much extra work would be required, and we did not have a clear plan of who
would be responsible for it. Therefore, we ended up doing a lot of work helping authors make their
work more reproducible. This was in part because we did not want to reject any papers that failed
to achieve computational reproducibility. In retrospect, it is clear that we should have done a better
job clarifying the allocation of responsibilities with the authors.
Second, we believe that badging to TOP Level 3 will be substantially easier than requiring
TOP Level 3. In our case, it was relatively easy to verify that some of the manuscripts were
computationally reproducible but others were extremely difficult. We ended up spending roughly
80% of our time on 20% of the manuscripts. If we had chosen to badge to Level 3, we expect that
authors interested in computational reproducibility—who generally had results that were easier
to reproduce—would have attempted to earn the badge and others would not. Further, a badging
system would have enabled us to set clear time limits on the amount of time we would spend
on each submission, a process sometimes called “timeboxing.” For example, we could have told
authors that we would spend 6 hours attempting to reproduce their results, and if we could not
do it within that time, the authors would not receive a badge. We think that a timeboxed system
would help promote best practices by authors and would save a lot of time for people verifying
reproducibility.
Third, we think that journals—or perhaps publishers—should invest in tools that can facilitate the assessment of computational reproducibility. The online manuscript handling system
used by Socius does not currently provide tools to support sharing and review of code so we did
most of our computational reproducibility work through email, a process that introduced numerous
inefficiencies. As a first step, journals could build connectors to existing tools such as GitHub and
create simple computational reproducibility checklists that would become a part of the manuscript
submission process. Efforts to automate and standardize the process of assessing computational
reproducibility will, in the long run, decrease its cost and increase its probability of success.
Fourth, we think that journals just starting to deal with computational reproducibility should
draw insights from other journals that already have developed computational reproducibility and

30

open science policies, such as: Biostatistics (Peng, 2009), Psychological Science (Eich, 2014),
ACM Transactions on Mathematical Software (Heroux, 2015), International Interactions (Colaresi, 2016), Quarterly Journal of Political Science (Eubank, 2016), Journal of the American
Statistical Association (Baker, 2016), American Journal of Political Science (Jacoby et al., 2017),
ReScience (Rougier et al., 2017), Political Analysis (Alvarez et al., 2018), and Methods in Ecology
and Evolution (Freckleton, 2018). We made a mistake by not including detailed instructions about
computational reproducibility in our initial call for papers, and we could have easily developed
such instructions based on those used by these other journals.
Finally, for journals thinking of requiring TOP Level 3, we would encourage editors to
consider whether they have full support from all important stakeholders, including the publisher,
the journal’s editorial board, and the pool of potential reviewers and authors. One way to assess
this support concretely is with the following question: “Are we willing to reject a manuscript that
we would otherwise publish because it is not computationally reproducible?” If the answer is not
a resounding yes, we would not recommend attempting to require TOP Level 3. Further, editors
should assess whether stakeholders are willing to bear the costs that may come with requiring TOP
Level 3, which could be assessed with questions such as: “Are we willing to have our turn-around
time for manuscripts increase by one week (or one month or three months)?” and “Are we willing
to have the number of submissions decrease by 10% (or 20% or 50%)?” In our case, a core value
of Socius is “rapid dissemination of high-quality, peer-reviewed research, produced in time to be
relevant to ongoing research and public debates” (Keister and Moody, 2015), and that value did not
always align with the time requirements of our computational reproducibility verification process.

5.3

Our preferred model
For journals that current have no policies on computational reproducibility, we think that

a possible first step would be to require TOP Level 2 and badge to TOP Level 3. Badging to
TOP Level 3 would require code review, and so we briefly sketch our preferred work-flow, one
that elevates the code to a first-class status alongside each manuscript (Figure 9). When an author
31

Figure 9: Schematic of our preferred workflow for journals that are badging to TOP Level 3 (“‘verification before publication”). Adding a reproducibility review into the editorial process elevates
the code to a first-class status alongside each manuscript and allows community standards to vary
by field and evolve over time.
submits a manuscript, they also submit the code, data, and computing environment needed to
reproduce all the results in their paper. The manuscript and replication materials would then be
sent to several peer reviewers and to a reproducibility reviewer, who could be a student, a peer,
or a professional software engineer. The peer reviewers would write reports, much as they do
now; they would be free to assess the replication materials or not. The reproducibility reviewer
would, however, focus entirely on the replication materials. They would spend a fixed maximum
amount of time—say 6 hours—attempting to reproduce the results in the manuscript. Then they
would write a reproducibility report which would go to the editor and be considered along with the
manuscript reviews. If an author is encouraged to submit a revised manuscript, they would need
to reply to the comments of the peer reviewers and reproducibility reviewer. We think that adding
a reproducibility review into the editorial process appropriately draws attention to the code and
allows community standards to vary by field and evolve over time. Further, the introduction of a
reproducibility reviewer who will work for a fixed maximum amount of time on each submission
makes the costs of reproducibility reviews predictable and manageable.

32

6

Conclusion
In this paper we have described our experience with promoting computational reproducibil-

ity for the special issue of Socius about the Fragile Families Challenge. Our approach was inspired
by previous efforts, but it differed from other approaches in the social sciences in terms of focus,
motivation, and timing. Based on our success and struggles, we have offered recommendations to
authors who want to increase the computational reproducibility of their work and to journals that
want to promote computational reproducibility.
One important aspect to our approach to computational reproducibility was to expand the
focus and include the computing environment in addition to code and data. As sociologists and
other social scientists increasingly publish papers based on larger data sets and more computationally intensive methods, we expect the need to have this broader focus will increase. In addition
to the tools that we chose to use, there are a number of other tools that may be useful to future
researchers interested in computational reproducibility.26
Our work also raises questions about the costs of computational reproducibility. Our experience suggests that ensuring computational reproducibility of complex computational approaches
is going be time consuming for authors and journals. Who should bear these costs? Should we
have different computational reproducibility standards for different kinds of research? In what
situations would access to code be sufficient, even if that code cannot be re-run or if it could be
re-run but produces slightly different results? What about research that is computationally reproducible, but where the code is extremely hard to understand? What if results are computationally
reproducible in some computing environments but not others? How long after publication should
we reasonably expect results to be reproducible? Ultimately these are questions that have to be
answered by the scientific community, and we expect that the answers will vary from field to
field. Despite these important unresolved questions, we hope the approach that we took to promote
26
Some examples of tools that might be useful to other researchers are GUIdock (Hung et al., 2016), AlgoRun (Hosny et al., 2016), ReproZip (Chirigati et al., 2016)/ReproServer (Rampin et al., 2018), Singularity (Kurtzer
et al., 2017), Rocker (Boettiger and Eddelbuettel, 2017), Jug (Coelho, 2017), archivist (Biecek and Kosiski, 2017),
encapsulator (Pasquier et al., 2018), Binder (Jupyter et al., 2018), Packrat (Ushey et al., 2018), and Whole Tale (Brinckman et al., 2018).

33

computational reproducibility will increase the impact of the work published in the special issue,
further the research goals of the Fragile Families Challenge, and contribute to broader efforts to
create a more transparent and open system of scientific publishing.

34

References
Ahearn, C. E. and J. E. Brand 2019. Predicting Layoff among Fragile Families. Socius.
Alsheikh-Ali, A. A., W. Qureshi, M. H. Al-Mallah, and J. P. A. Ioannidis 2011. Public Availability
of Published Research Data in High-Impact Journals. PLOS ONE, 6(9):e24357.
Altschul, D. M. 2019. Leveraging multiple machine learning techniques to predict major life
outcomes from a small set of psychological and socioeconomic variables: a combined bottomup/top-down approach. Socius.
Alvarez, R. M., E. M. Key, and L. Nez 2018. Research Replication: Practical Considerations. PS:
Political Science & Politics, 51(2):422–426.
Anderson, R. G., W. H. Greene, B. D. McCullough, and H. D. Vinod 2008. The role of data/code
archives in the future of economic research. Journal of Economic Methodology, 15(1):99–119.
Andrew, R. L., A. Y. K. Albert, S. Renaut, D. J. Rennison, D. G. Bock, and T. Vines 2015. Assessing the reproducibility of discriminant function analyses. PeerJ, 3:e1137.
Baker, M. 2016. Why scientists must share their research code. Nature News.
Barba, L. A. 2018. Terminologies for Reproducible Research. arXiv:1802.03311 [cs]. arXiv:
1802.03311.
Barnes, N. 2010. Publish your computer code: it is good enough. Nature, 467(7317):753–753.
Benureau, F. C. Y. and N. P. Rougier 2018. Re-run, Repeat, Reproduce, Reuse, Replicate: Transforming Code into Scientific Contributions. Frontiers in Neuroinformatics, 11.
Biecek, P. and M. Kosiski 2017. archivist: An R Package for Managing, Recording and Restoring
Data Analysis. Journal of Statistical Software, 82(11).
Boettiger, C. 2015. An Introduction to Docker for Reproducible Research. SIGOPS Oper. Syst.
Rev., 49(1):71–79.
Boettiger, C. and D. Eddelbuettel 2017. An Introduction to Rocker: Docker Containers for R. The
R Journal, 9(2):527–536.
Boisvert, R. F. 2016. Incentivizing Reproducibility. Commun. ACM, 59(10):5–5.
Bowers, J. and M. Voors 2016. How to improve your relationship with your future self. Revista de
ciencia poltica (Santiago), 36(3):829–848.
Breiman, L. 1996. Stacked regressions. Machine Learning, 24(1):49–64.
Brinckman, A., K. Chard, N. Gaffney, M. Hategan, M. B. Jones, K. Kowalik, S. Kulasekaran,
B. Ludscher, B. D. Mecum, J. Nabrzyski, V. Stodden, I. J. Taylor, M. J. Turk, and K. Turner
2018. Computing environments for reproducibility: Capturing the Whole Tale. Future Generation Computer Systems.
35

Buckheit, J. B. and D. L. Donoho 1995. WaveLab and Reproducible Research. In Wavelets and
Statistics, A. Antoniadis and G. Oppenheim, eds., Lecture Notes in Statistics, Pp. 55–81. New
York, NY: Springer New York.
Campbell, H. A., M. A. Micheli-Campbell, and V. Udyawer 2019. Early Career Researchers
Embrace Data Sharing. Trends in Ecology & Evolution, 34(2):95–98.
Carnegie, N. B. and J. Wu 2019. Variable selection and parameter tuning for BART modeling in
the Fragile Families Challenge. Socius.
Chang, A. C. and P. Li 2018. Is Economics Research Replicable? Sixty Published Papers From
Thirteen Journals Say Often Not. Critical Finance Review, 7.
Chen, X., S. Dallmeier-Tiessen, R. Dasler, S. Feger, P. Fokianos, J. B. Gonzalez, H. Hirvonsalo,
D. Kousidis, A. Lavasa, S. Mele, D. R. Rodriguez, T. imko, T. Smith, A. Trisovic, A. Trzcinska,
I. Tsanaktsidis, M. Zimmermann, K. Cranmer, L. Heinrich, G. Watts, M. Hildreth, L. L. Iglesias,
K. Lassili-Perini, and S. Neubert 2018. Open is not enough. Nature Physics, P. 1.
Chirigati, F., R. Rampin, D. Shasha, and J. Freire 2016. ReproZip: Computational Reproducibility
With Ease. In Proceedings of the 2016 International Conference on Management of Data,
SIGMOD ’16, Pp. 2085–2088, New York, NY, USA. ACM.
Claerbout, J. and M. Karrenbach 1992. Electronic documents give reproducible research a new
meaning. In SEG Technical Program Expanded Abstracts 1992, SEG Technical Program Expanded Abstracts, Pp. 601–604. Society of Exploration Geophysicists.
Clemens, M. A. 2017. The Meaning of Failed Replications: A Review and Proposal. Journal of
Economic Surveys, 31(1):326–342.
Clyburne-Sherin, A. and S. A. Green 2018. Computational Reproducibility via Containers in
Social Psychology. PsyArXiv.
Coelho, L. P. 2017. Jug: Software for Parallel Reproducible Computation in Python. Journal of
Open Research Software, 5(1).
Colaresi, M. 2016. Preplication, Replication: A Proposal to Efficiently Upgrade Journal Replication Standards. International Studies Perspectives, 17(4):367–378.
Collberg, C. and T. A. Proebsting 2016. Repeatability in Computer Systems Research. Commun.
ACM, 59(3):62–69.
Compton, R. 2019. A Data-driven Approach to the Fragile Families Challenge: Prediction through
Principal Components Analysis and Random Forests. Socius.
Coughlin, S. S. 2017. Reproducing Epidemiologic Research and Ensuring Transparency. American
Journal of Epidemiology, 186(4):393–394.

36

Crook, S. M., A. P. Davison, and H. E. Plesser 2013. Learning from the Past: Approaches for
Reproducibility in Computational Neuroscience. In 20 Years of Computational Neuroscience,
J. M. Bower, ed., Springer Series in Computational Neuroscience, Pp. 73–102. New York, NY:
Springer New York.
Crosas, M., J. Gautier, S. Karcher, D. Kirilova, G. Otalora, and A. Schwartz 2018. Data policies
of highly-ranked social science journals. SocArXiv.
Davidson, T. 2019. Black Box Models and Sociological Explanations: Predicting High School
GPA Using Neural Networks. Socius.
Dewald, W. G., J. G. Thursby, and R. G. Anderson 1986. Replication in Empirical Economics: The
Journal of Money, Credit and Banking Project. The American Economic Review, 76(4):587–603.
Diethelm, K. 2012. The Limits of Reproducibility in Numerical Simulation. Computing in Science
Engineering, 14(1):64–72.
Donoho, D. 2017. 50 Years of Data Science. Journal of Computational and Graphical Statistics,
26(4):745–766.
Donoho, D. L. 2010.
11(3):385–388.

An invitation to reproducible computational research.

Biostatistics,

Dudley, J. T. and A. J. Butte 2010. Reproducible in silico research in the era of cloud computing.
Nature biotechnology, 28(11):1181–1185.
Easterbrook, S. M. 2014. Open code for open science? Nature Geoscience, 7:779–781.
Eglen, S., B. Marwick, Y. Halchenko, M. Hanke, S. Sufi, P. Gleeson, R. A. Silver, A. Davison,
L. Lanyon, M. Abrams, T. Wachtler, D. J. Willshaw, C. Pouzat, and J.-B. Poline 2017. Towards
standard practices for sharing computer code and programs in neuroscience. bioRxiv, P. 045104.
Eich, E. 2014. Business Not as Usual. Psychological Science, 25(1):3–6.
Eubank, N. 2016. Lessons from a Decade of Replications at the Quarterly Journal of Political
Science. PS: Political Science & Politics, 49(2):273–276.
Fehr, J., J. Heiland, C. Himpe, and J. Saak 2016. Best practices for replicability, reproducibility
and reusability of computer-based experiments exemplified by model reduction software. AIMS
Mathematics, 1(3):261–281.
Filippova, A., C. Gilroy, R. Kashyap, A. Kirchner, A. C. Morgan, K. Polimis, A. Usmani, and
T. Wang 2019. Humans in the Loop: Incorporating Expert and Crowdsourced Knowledge for
Predictions using Survey Data. Socius.
Fowler, M., K. Beck, J. Brant, W. Opdyke, and D. Roberts 1999. Refactoring: Improving the
Design of Existing Code. Addison-Wesley Professional.
Freckleton, R. P. 2018. Accessibility, reusability, reliability: Improving the standards for publishing code in Methods in Ecology and Evolution. Methods in Ecology and Evolution, 9(1):4–6.
37

Freese, J. 2007. Replication Standards for Quantitative Social Science: Why Not Sociology?
Sociological Methods & Research, 36(2):153–172.
Freese, J. and M. M. King 2018. Institutionalizing Transparency. Socius, 4:1–7.
Freese, J. and D. Peterson 2017. Replication in Social Science. Annual Review of Sociology,
43(1):147–165.
Gentleman, R. and D. T. Lang 2007. Statistical Analyses and Reproducible Research. Journal of
Computational and Graphical Statistics, 16(1):1–23.
Gertler, P., S. Galiani, and M. Romero 2018. How to make replication the norm. Nature,
554(7693):417.
Gil, Y., C. H. David, I. Demir, B. T. Essawy, R. W. Fulweiler, J. L. Goodall, L. Karlstrom, H. Lee,
H. J. Mills, J.-H. Oh, S. A. Pierce, A. Pope, M. W. Tzeng, S. R. Villamizar, and X. Yu 2016.
Toward the Geoscience Paper of the Future: Best practices for documenting and sharing research
from data to software to provenance. Earth and Space Science, 3(10):388–415.
Gilbert, K. J., R. L. Andrew, D. G. Bock, M. T. Franklin, N. C. Kane, J.-S. Moore, B. T. Moyers,
S. Renaut, D. J. Rennison, T. Veen, and T. H. Vines 2012. Recommendations for utilizing
and reporting population genetic analyses: the reproducibility of genetic clustering using the
program structure. Molecular Ecology, 21(20):4925–4930.
Goode, B. J., D. Datta, and N. Ramakrishnan 2019. Imputing Data for the Fragile Families Challenge: Identifying Similar Survey Questions with Semi-Automated Methods. Socius.
Goodman, S. N., D. Fanelli, and J. P. A. Ioannidis 2016. What does research reproducibility mean?
Science Translational Medicine, 8(341):341ps12.
Grning, B., J. Chilton, J. Kster, R. Dale, N. Soranzo, M. van den Beek, J. Goecks, R. Backofen,
A. Nekrutenko, and J. Taylor 2018. Practical Computational Reproducibility in the Life Sciences. Cell Systems, 6(6):631–635.
Hardt, M. and A. Blum 2015. The Ladder: A Reliable Leaderboard for Machine Learning Competitions. In International Conference on Machine Learning, Pp. 1006–1014.
Hardwicke, T. E. and J. P. A. Ioannidis 2018. Populating the Data Ark: An attempt to retrieve,
preserve, and liberate data from the most highly-cited psychology and psychiatry articles. PLOS
ONE, 13(8):e0201856.
Hardwicke, T. E., M. B. Mathur, K. MacDonald, G. Nilsonne, G. C. Banks, M. C. Kidwell, A. H.
Mohr, E. Clayton, E. J. Yoon, M. H. Tessler, R. L. Lenne, S. Altman, B. Long, and M. C. Frank
2018. Data availability, reusability, and analytic reproducibility: evaluating the impact of a
mandatory open data policy at the journal Cognition. Royal Society Open Science, 5(8):180448.
Heroux, M. A. 2015. Editorial: ACM TOMS Replicated Computational Results Initiative. ACM
Trans. Math. Softw., 41(3):13:1–13:5.

38

Heuberger, S. 2019. Insufficiencies in Data Material: A Replication Analysis of Muchlinski,
Siroky, He, and Kocher (2016). Political Analysis, 27(1):114–118.
Hosny, A., P. Vera-Licona, R. Laubenbacher, and T. Favre 2016. AlgoRun: a Docker-based
packaging system for platform-agnostic implemented algorithms. Bioinformatics, 32(15):2396–
2398.
Howe, B. 2012. Virtual Appliances, Cloud Computing, and Reproducible Research. Computing
in Science Engineering, 14(4):36–41.
Hung, L.-H., D. Kristiyanto, S. B. Lee, and K. Y. Yeung 2016. GUIdock: Using Docker Containers
with a Common Graphics User Interface to Address the Reproducibility of Research. PLOS
ONE, 11(4):e0152686.
Hutson, M. 2018. Artificial intelligence faces reproducibility crisis. Science, 359(6377):725–726.
Hutton, C., T. Wagener, J. Freer, D. Han, C. Duffy, and B. Arheimer 2016. Most computational
hydrology is not reproducible, so is it really science? Water Resources Research, 52(10):7548–
7555.
In’nami, Y. and R. Koizumi 2010. Can Structural Equation Models in Second Language Testing and Learning Research be Successfully Replicated? International Journal of Testing,
10(3):262–273.
Ioannidis, J. P. A., D. B. Allison, C. A. Ball, I. Coulibaly, X. Cui, A. C. Culhane, M. Falchi,
C. Furlanello, L. Game, G. Jurman, J. Mangion, T. Mehta, M. Nitzberg, G. P. Page, E. Petretto,
and V. van Noort 2009. Repeatability of published microarray gene expression analyses. Nature
Genetics, 41(2):149–155.
Jacoby, W. G., S. Lafferty-Hess, and T.-M. Christian 2017. Should Journals Be Responsible for
Reproducibility? Inside Higher Ed.
Johnson, E. J. and D. Goldstein 2003. Do Defaults Save Lives? Science, 302(5649):1338–1339.
Jupyter, P., M. Bussonnier, J. Forde, J. Freeman, B. Granger, T. Head, C. Holdgraf, K. Kelley,
G. Nalvarte, A. Osheroff, M. Pacer, Y. Panda, F. Perez, B. Ragan-Kelley, and C. Willing 2018.
Binder 2.0 - Reproducible, interactive, sharable environments for science at scale. Proceedings
of the 17th Python in Science Conference, Pp. 113–120.
Keister, L. A. and J. W. Moody 2015. Editorial. Socius, 1:1.
Kidwell, M. C., L. B. Lazarevi, E. Baranski, T. E. Hardwicke, S. Piechowski, L.-S. Falkenberg,
C. Kennett, A. Slowik, C. Sonnleitner, C. Hess-Holden, T. M. Errington, S. Fiedler, and B. A.
Nosek 2016. Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method
for Increasing Transparency. PLOS Biology, 14(5):e1002456.
King, G. 1995. Replication, Replication. PS: Political Science & Politics, 28(3):444–452.
King, G., M. Tomz, and J. Wittenberg 2000. Making the Most of Statistical Analyses: Improving
Interpretation and Presentation. American Journal of Political Science, 44(2):347–361.
39

Knuth, D. E. 1984. Literate Programming. The Computer Journal, 27(2):97–111.
Koenker, R. and A. Zeileis 2009. On reproducible econometric research. Journal of Applied
Econometrics, 24(5):833–847.
Konkol, M., C. Kray, and M. Pfeiffer 2019. Computational reproducibility in geoscientific papers:
Insights from a series of studies with geoscientists and a reproduction study. International
Journal of Geographical Information Science, 33(2):408–429.
Kurtzer, G. M., V. Sochat, and M. W. Bauer 2017. Singularity: Scientific containers for mobility
of compute. PLOS ONE, 12(5):e0177459.
Leek, J. T. and R. D. Peng 2015. Opinion: Reproducible research can still be wrong: Adopting a
prevention approach. Proceedings of the National Academy of Sciences, 112(6):1645–1646.
Levy, K. E. and D. M. Johns 2016. When open data is a Trojan Horse: The weaponization of
transparency in science and governance. Big Data & Society, 3(1):1–6.
Liu, D. M. 2018. Computational Reproducibility and the Fragile Families Challenge: Lessons
Learned and Suggestions for the Future. B.S.E. Thesis, Department of Computer Science,
Princeton University, Princeton, NJ.
Lundberg, I., A. Narayanan, K. Levy, and M. J. Salganik 2018. Privacy, ethics, and data access: A
case study of the Fragile Families Challenge. arXiv:1809.00103 [cs].
Lupia, A. and C. Elman 2014. Openness in Political Science: Data Access and Research Transparency: Introduction. PS: Political Science & Politics, 47(1):19–42.
Mangul, S., T. Mosqueiro, D. Duong, K. Mitchell, V. Sarwal, B. Hill, J. Brito, R. Littman, B. Statz,
A. Lam, G. Dayama, L. Grieneisen, L. Martin, J. Flint, E. Eskin, and R. Blekhman 2018. A
comprehensive analysis of the usability and archival stability of omics computational tools and
resources. bioRxiv, P. 452532.
Manninen, T., R. Havela, and M.-L. Linne 2017. Reproducibility and Comparability of Computational Models for Astrocyte Calcium Excitability. Frontiers in Neuroinformatics, 11.
Marwick, B. 2017. Computational Reproducibility in Archaeological Research: Basic Principles
and a Case Study of Their Implementation. Journal of Archaeological Method and Theory,
24(2):424–450.
Marwick, B., C. Boettiger, and L. Mullen 2018. Packaging Data Analytical Work Reproducibly
Using R (and Friends). The American Statistician, 72(1):80–88.
McCullough, B. D., K. A. McGeary, and T. D. Harrison 2006. Lessons from the JMCB Archive.
Journal of Money, Credit and Banking, 38(4):1093–1107.
McKay, S. 2019. When 4 10,000: The power of social science knowledge in predictive performance. Socius.

40

Merkel, D. 2014. Docker: Lightweight Linux Containers for Consistent Development and Deployment. Linux Journal, 2014(239).
Mesnard, O. and L. A. Barba 2017. Reproducible and Replicable Computational Fluid Dynamics:
Its Harder Than You Think. Computing in Science Engineering, 19(4):44–55.
Meyer, M. N. 2018. Practical Tips for Ethical Data Sharing. Advances in Methods and Practices
in Psychological Science, 1(1):131–144.
Mikowski, M., W. M. Hensel, and M. Hohol 2018. Replicability or reproducibility? On the
replication crisis in computational neuroscience and sharing only relevant detail. Journal of
Computational Neuroscience, 45(3):163–172.
Nagler, J. 1995. Coding Style and Good Computing Practices. PS: Political Science & Politics,
28(3):488–492.
Naudet, F., C. Sakarovitch, P. Janiaud, I. Cristea, D. Fanelli, D. Moher, and J. P. A. Ioannidis 2018.
Data sharing and reanalysis of randomized controlled trials in leading biomedical journals with
a full data sharing policy: survey of studies published in The BMJ and PLOS Medicine. BMJ,
360:k400.
Nosek, B. A., G. Alter, G. C. Banks, D. Borsboom, S. D. Bowman, S. J. Breckler, S. Buck, C. D.
Chambers, G. Chin, G. Christensen, M. Contestabile, A. Dafoe, E. Eich, J. Freese, R. Glennerster, D. Goroff, D. P. Green, B. Hesse, M. Humphreys, J. Ishiyama, D. Karlan, A. Kraut,
A. Lupia, P. Mabry, T. Madon, N. Malhotra, E. Mayo-Wilson, M. McNutt, E. Miguel, E. L.
Paluck, U. Simonsohn, C. Soderberg, B. A. Spellman, J. Turitto, G. VandenBos, S. Vazire, E. J.
Wagenmakers, R. Wilson, and T. Yarkoni 2015. Promoting an open research culture. Science,
348(6242):1422–1425.
Orozco, V., C. Bontemps, E. Maign, V. Piguet, A. Hofstetter, A. Lacroix, F. Levert, and J. M.
Rousselle 2018. How To Make A Pie: Reproducible Research for Empirical Economics &
Econometrics. Technical report, Toulouse School of Economics (TSE).
Ostermann, F. O. and C. Granell 2017. Advancing Science with VGI: Reproducibility and Replicability of Recent Studies using VGI. Transactions in GIS, 21(2):224–237.
Pasquier, T., M. K. Lau, X. Han, E. Fong, B. S. Lerner, E. Boose, M. Crosas, A. M. Ellison, and
M. Seltzer 2018. Sharing and Preserving Computational Analyses for Posterity with encapsulator. arXiv:1803.05808 [cs].
Patil, P., R. D. Peng, and J. Leek 2016. A statistical definition for reproducibility and replicability.
bioRxiv, P. 066803.
Peng, R. D. 2009. Reproducible research and Biostatistics. Biostatistics, 10(3):405–408.
Peng, R. D. 2011. Reproducible Research in Computational Science. Science, 334(6060):1226–
1227.

41

Peng, R. D., F. Dominici, and S. L. Zeger 2006. Reproducible Epidemiologic Research. American
Journal of Epidemiology, 163(9):783–789.
Peng, R. D. and S. P. Eckel 2009. Distributed Reproducible Research Using Cached Computations.
Computing in Science & Engineering, 11(1):28–34.
Piccolo, S. R. and M. B. Frampton 2016. Tools and techniques for computational reproducibility.
GigaScience, 5(1):30.
Plesser, H. E. 2018. Reproducibility vs. Replicability: A Brief History of a Confused Terminology.
Frontiers in Neuroinformatics, 11.
Press, W. H., S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery 2007. Numerical Recipes 3rd
Edition: The Art of Scientific Computing, 3rd edition. Cambridge, UK ; New York: Cambridge
University Press.
Raes, L. 2019. Predicting GPA at Age 15 in the Fragile Families and Child Wellbeing Study.
Socius.
Rampin, R., F. Chirigati, V. Steeves, and J. Freire 2018. ReproServer: Making Reproducibility
Easier and Less Intensive. arXiv:1808.01406 [cs]. arXiv: 1808.01406.
Reichman, N. E., J. O. Teitler, I. Garfinkel, and S. S. McLanahan 2001. Fragile Families: sample
and design. Children and Youth Services Review, 23(4):303–326.
Rigobon, D. E., E. Jahini, Y. Suhara, K. AlGhoneim, A. Alghunaim, Alex Sandy Pentland, and
A. Almaatouq 2019. Winning Models for GPA, Grit, and Layoff in the Fragile Families Challenge. Socius.
Roberts, C. V. 2019. Friend Request Pending: A Comparative Assessment of Engineering and
Social Science Inspired Approaches to Analyzing Complex Birth Cohort Survey Data. Socius.
Rougier, N. P., K. Hinsen, F. Alexandre, T. Arildsen, L. A. Barba, F. C. Y. Benureau, C. T.
Brown, P. d. Buyl, O. Caglayan, A. P. Davison, M.-A. Delsuc, G. Detorakis, A. K. Diem,
D. Drix, P. Enel, B. Girard, O. Guest, M. G. Hall, R. N. Henriques, X. Hinaut, K. S. Jaron,
M. Khamassi, A. Klein, T. Manninen, P. Marchesi, D. McGlinn, C. Metzner, O. Petchey, H. E.
Plesser, T. Poisot, K. Ram, Y. Ram, E. Roesch, C. Rossant, V. Rostami, A. Shifman, J. Stachelek,
M. Stimberg, F. Stollmeier, F. Vaggi, G. Viejo, J. Vitay, A. E. Vostinar, R. Yurchak, and T. Zito
2017. Sustainable computational science: the ReScience initiative. PeerJ Computer Science,
3:e142.
Rowhani-Farid, A., M. Allen, and A. G. Barnett 2017. What incentives increase data sharing in
health and medical research? A systematic review. Research Integrity and Peer Review, 2(1):4.
Rowhani-Farid, A. and A. G. Barnett 2016. Has open data arrived at the British Medical Journal
(BMJ)? An observational study. BMJ Open, 6(10):e011784.
Rudin, C. 2018. Please Stop Explaining Black Box Models for High Stakes Decisions.
arXiv:1811.10154 [cs, stat]. arXiv: 1811.10154.
42

Salganik, M. J. 2018. Bit by Bit: Social Research in the Digital Age. Princeton, NJ: Princeton
University Press.
Salganik, M. J., I. Lundberg, A. Kindel, and S. McLanahan 2018. Introduction to the specail issue
about the Fragile Families Challenge. Socius, P. in press.
Sandve, G. K., A. Nekrutenko, J. Taylor, and E. Hovig 2013. Ten Simple Rules for Reproducible
Computational Research. PLOS Computational Biology, 9(10):e1003285.
Savage, C. J. and A. J. Vickers 2009. Empirical Study of Data Sharing by Authors Publishing in
PLoS Journals. PLOS ONE, 4(9):e7078.
Schnell, S. 2018. Reproducible Research in Mathematical Sciences Requires Changes in our Peer
Review Culture and Modernization of our Current Publication Approach. Bulletin of Mathematical Biology, 80(12):3095–3105.
Shah, N. B., B. Tabibian, K. Muandet, I. Guyon, and U. Von Luxburg 2018. Design and analysis
of the nips 2016 review process. Journal of Machine Learning Research, 19(49):1–34.
Shepherd, B. E., M. Blevins Peratikos, P. F. Rebeiro, S. N. Duda, and C. C. McGowan 2017.
A Pragmatic Approach for Reproducible Research With Sensitive Data. American Journal of
Epidemiology, 186(4):387–392.
Stanescu, D., E. H. Wang, and S. Yamauchi 2019. Using LASSO to Assist Imputation and Predict
Child Wellbeing. Socius.
Stockemer, D., S. Koehler, and T. Lenz 2018. Data Access, Transparency, and Replication: New
Insights from the Political Behavior Literature. PS: Political Science & Politics, Pp. 1–5.
Stodden, V. 2015. Reproducing Statistical Results. Annual Review of Statistics and Its Application,
2(1):1–19.
Stodden, V., M. McNutt, D. H. Bailey, E. Deelman, Y. Gil, B. Hanson, M. A. Heroux, J. P. A.
Ioannidis, and M. Taufer 2016. Enhancing reproducibility for computational methods. Science,
354(6317):1240–1241.
Stodden, V., J. Seiler, and Z. Ma 2018. An empirical analysis of journal policy effectiveness for
computational reproducibility. Proceedings of the National Academy of Sciences, 115(11):2584–
2589.
Tatman, R., J. VanderPlas, and S. Dane 2018. A Practical Taxonomy of Reproducibility for Machine Learning Research. Reproducibility in Machine Learning Workshop at ICML 2018.
Team, F. F. C. 2019. Measuring the predictability of soical outcomes using a scientific mass
collaboraiton. Working paper.
Ushey, K., J. McPherson, J. Cheng, A. Atkins, and J. J. Allaire 2018. packrat: A Dependency
Management System for Projects and their R Package Dependencies.

43

Vandewalle, P., J. Kovacevic, and M. Vetterli 2009. Reproducible research in signal processing.
IEEE Signal Processing Magazine, 26(3):37–47.
Vanpaemel, W., M. Vermorgen, L. Deriemaecker, and G. Storms 2015. Are We Wasting a Good
Crisis? The Availability of Psychological Research Data after the Storm. Collabra: Psychology,
1(1).
Vines, T. H., A. Y. K. Albert, R. L. Andrew, F. Dbarre, D. G. Bock, M. T. Franklin, K. J. Gilbert,
J.-S. Moore, S. Renaut, and D. J. Rennison 2014. The Availability of Research Data Declines
Rapidly with Article Age. Current Biology, 24(1):94–97.
Wicherts, J. M., M. Bakker, and D. Molenaar 2011. Willingness to Share Research Data Is Related
to the Strength of the Evidence and the Quality of Reporting of Statistical Results. PLOS ONE,
6(11):e26828.
Wicherts, J. M., D. Borsboom, J. Kats, and D. Molenaar 2006. The poor availability of psychological research data for reanalysis. American Psychologist, 61(7):726–728.
Wicherts, J. M. and E. A. V. Crompvoets 2017. The poor availability of syntaxes of structural
equation modeling. Accountability in Research, 24(8):458–468.
Wilson, G., J. Bryan, K. Cranston, J. Kitzes, L. Nederbragt, and T. K. Teal 2017. Good enough
practices in scientific computing. PLOS Computational Biology, 13(6):e1005510.
Wolpert, D. H. 1992. Stacked generalization. Neural Networks, 5(2):241–259.
Wood, B. D. K., R. Mller, and A. N. Brown 2018. Push button replication: Is impact evaluation
evidence for international development verifiable? PLOS ONE, 13(12):e0209416.

44

A

Appendix

A.1

Replication status for each paper

• Ahearn: We were able to reproduce the three prediction.csv files within our error tolerance,
and we were able to reproduce Table 2. We did not attempt to reproduce the other Tables
and Figures because we did not deem them to be essential to reproducing the main results
in the paper, and the relevant files were not included in the replication materials. One thing
to note is that while the values in the prediction.csv files match nearly exactly, the row order
differed between the provided and generated prediction.csv files. As an example, rows 13
and 21 are swapped for model 1 (imputed data).
• Altschul: We were able to reproduce all 3 prediction.csv files exactly. Further, we were able
to reproduce Tables 1, 2, and 3 (with some tiny differences likely due to rounding). We note
that there is some manual post-processing required to take the output from table figures.txt
and create Table 1. We did not deem the figures necessary to reproduce.
• Carnegie: We ran out of time to verify the results. However, the code is available, and we
have created a Docker image with the necessary dependencies.
• Compton: We ran out of time to verify the results. However, the code is available, and we
have created a Docker image with the necessary dependencies.
• Davidson: We ran out of time to verify the results. However, the code is available, and we
have created a Docker image with the necessary dependencies.
• Gilroy: All 25 prediction.csv files are computationally reproducible. Initially, we were
able to exactly match 15 of the prediction.csv files, but not all of them. For the files that
did not match the differences were minor (differing by at most 0.284) and did not alter the
substantive conclusions of the paper. Working with the authors, we decided that these deviations were likely caused by differences between the authors’ computing environment and
45

our computing environment. Therefore, we decided that the final results would be those that
came from our computing environment, which is readily available (as opposed to the authors’ computing environment which is only available to researchers at a specific university
and will likely change over time). Further, we were able to reproduce Figures 2, 3, 4, and 5
(Figure 1 did not need to be reproduced).
• Goode: We were able to reproduce the one prediction.csv file. There are 3 tables, but they
were created manually and we didn’t attempt to reproduce them.
• McKay: We were able to reproduce the one prediction.csv file within our error tolerance.
Further, we were able to reproduce Table 1 and Figures 1 and 2. We did not attempt to
reproduce Table 2 because we did not deem them to be essential to reproducing the main
results in the paper, and the relevant files were not included in the replication materials.
• Raes: We were able to reproduce all 11 prediction.csv files within our error tolerance. Further, we were able to reproduce the tables. For Table 1 we were able to reproduce all rows
related to submitted predictions. For Table 2, we did not attempt to reproduce the results
because the table includes leaderboard and holdout scores. For Table 3, we only attempted
to reproduce the results related to model 6, and we were successful.
• Roberts: We ran out of time verifying the results from phase 1, but given those results, we
were able to match, within an error tolerance of 10−15 , the 150 MSE scores calculated in
phase 2. We did not deem the tables and figures necessary to reproduce as they were based
upon the leaderboard and holdout scores.
• Rigobon: We ran out of time to verify the results. However, the code is available, and we
have created a Docker image with the necessary dependencies.
• Wang: We were able to reproduce the single prediction.csv file. We were able to reproduce
Tables 1 and 2 and Figures 1 and 2.

46

Table 1 shows the software and hardware that we used to execute the code for eleven of the
twelve papers.27 All of the machines were access through Amazon Web Services and used a Linux
operating system known as the Amazon Linux AMI. The AMI ID specifies the exact operating system version we used. We determined the hardware configuration based on the computing demands
of each submission and a trial-and-error process. As a default, we instantiated machines of type
t2.2xlarge, which was the most powerful computing category in Amazon Web Services’ General
Purpose T2 series. For submissions that need more computing resources, we used the c5.9xlarge
instances.
First Author
Altschul
Carnegie
Compton
Davidson
Filippova
Goode
McKay
Raes
Roberts
Rigobon
Stanescu

Instance Type
t2.2xlarge
t2.2xlarge
t2.2xlarge
c5.9xlarge
c5.9xlarge
c5.9xlarge
t2.2xlarge
t2.2xlarge
t2.2xlarge
c5.9xlarge
t2.2xlarge

AMI ID
amzn-ami-hvm-2017.09.0.20170930-x86 64-gp2 (ami-8c1be5f6)
amzn-ami-hvm-2017.09.0.20170930-x86 64-gp2 (ami-8c1be5f6)
amzn-ami-hvm-2017.09.0.20170930-x86 64-gp2 (ami-8c1be5f6)
amzn2-ami-hvm-2.0.20181008-x86 64-gp2 (ami-0922553b7b0369273)
amzn2-ami-hvm-2.0.20181008-x86 64-gp2 (ami-0922553b7b0369273)
amzn2-ami-hvm-2.0.20181024-x86 64-gp2 (ami-013be31976ca2c322)
amzn-ami-hvm-2017.09.0.20170930-x86 64-gp2 (ami-8c1be5f6)
amzn-ami-hvm-2017.09.0.20170930-x86 64-gp2 (ami-8c1be5f6)
amzn-ami-hvm-2017.09.0.20170930-x86 64-gp2 (ami-8c1be5f6)
amzn2-ami-hvm-2.0.20181008-x86 64-gp2 (ami-0922553b7b0369273)
amzn-ami-hvm-2017.09.0.20170930-x86 64-gp2 (ami-8c1be5f6)

Table 1: All of our Amazon Web Services instances ran a distribution of Linux known as Amazon Linux AMI. The “AMI ID” specifies the exact version. We varied each instance’s hardware
specification (“Instance Type”) based on the code’s computational load.

A.2

Reproducibility memo
Here is a copy of the memo that we sent to all authors along with our reviews of their initial

manuscripts.
27

Ahearn and Brand (2019) required an installation of Stata, which is not open source software. Therefore, we
could not include Stata inside a Docker image. Ultimately, we executed the code on a private computing cluster with
the following specifications: Intel Xeon CPU E7-4850 v3 @ 2.20 GHz (4 processors) with 512 GB RAM, using a
64-bit operating system, operating Windows Server 2012 R2 Standard, running Stata 14.

47

Background behind reproducibility guidelines
First, wed like to step back from the details to describe the high-level goal. We want your
articles to be computationally reproducible, which means that another researcher could regenerate
the results in your paper using the Challenge data, your code, and any additional data that you have
created. Computational reproducibility will increase the impact of your work individually, and it
will increase the contribution of the Challenge collectively.
As weve learned during this first round of reviews, the goal of computational reproducibility is widely shared by scientists, easy to state, and tricky to achieve. Based on what weve learned
from your code, our thinking on how to achieve this goal issues has evolved. In particular, weve
been very influenced by the idea of a research pipeline described by Peng and Eckel (2014), which
is nicely captured by this figure: http://bit.ly/2qrTWXK.
The goal of this document is to provide you with guidelines that support computational
reproducibility of your entire research pipeline, which goes from raw data to final output. You
dont have to follow these guidelines exactly; if you devise a system that you think is better, you
are welcome to use it. But, if you have no system in place, we are going to strongly encourage that
you adopt these guidelines.

The Guidelines
The most important thing to keep in mind is that we are asking you to create one single
script named run all that executes all the necessary files to go from the raw data to the final results.
One way to do this is to write a shell script that calls the submission files in sequence. An example
of a simple shell script is shown below:
Running the above script will execute each line, one after another. Note that the screen shot
includes examples for many common languages. More background information on writing bash
scripts is available at this link. Of course, you may write the run all script in the language of your
choice so long as it can be executed from the command line.
While you are creating this script, we think it will be helpful to organize your input files,
48

intermediate files, and output files into a standard directory (i.e., folder) structure. We think that
this structure would help you create a modular research pipeline; see Peng and Eckel (2014) for
more on the modularity of a research pipeline. This modular pipeline will make it easier for us to
ensure computational reproducibility, and it will make it easy for other researchers to understand,
re-use, and improve your code.
Heres a basic structure that we think might work well for this project and others:
data/
code/
output/
README
LICENSE

In the data/ directory you can include:
• background.csv (this should not actually be included because of privacy constraints, but we
will put it here)
• train.csv (this should not actually be included because of privacy constraints, but we will put
it here)
• Supplemental materials such as metadata files, the constructed-data dictionary, the machinereadable codebook.
• Data that you have collected or created, such as a csv file that you manually created that has
your MSE scores on the holdout data and/or an analytic dataset created by your code.
In your code/ directory you can include:
• Executable “run-all” script that when run goes from raw inputs all the way to final outputs
(for this script we encourage you to think about the research pipeline idea from Peng and
Eckel 2014)
49

• Source code files each with a useful header (see FAQ).
• Package requirements
– For python submissions, please include a requirements.txt file. More information here.
– For R submissions, please list all libraries utilized in a file named requirements r.txt.
Include each library name on a new line.
In your output/ directory you can include:
• prediction.csv
• A subdirectory for tables
• A subdirectory for figures (we also recommend including all data files that can be used to
recreate the figures; see rule 7 of Sandve et al. 2013)
In addition to these three main directories, you should also include a README file and LICENSE
file. We have more information about these files in the FAQ below. We hope that these guidelines
are help, and please let us know if you have any questions.

Code resubmission process
Once you think you are ready to resubmit, heres a checklist that you can follow to help
ensure that your work will be computationally reproducible:
• I have written the kind of README file that I would like to read (see FAQ below)
• Each code file that Ive written has a header that will be helpful (see FAQ below)
• I’ve run the submission and I can get from raw files to final output using only materials in
my directories. Then, I’ve done this again and I get the same result. This second step helps
check for problems with seeding.
• I’ve considered refactoring my code (see FAQ below)
50

Finally, when you resubmit, we ask that you include a revision memo about the code, just as you
will about the manuscript. This revision memo should summarize changes that you have made. In
this revision memo, please also include a rough estimate of the cumulative amount of time it took
you to comply with these guidelines. We are asking for this time estimate because one objection to
computational reproducibility is that it is too burdensome for authors and we would like to assess
this empirically. Finally, please include any suggestions for how this process could have been
easier or more efficient.

F.A.Q.
What should go in the README file?
The README file should provide an overview of your code. For example, it could include a
diagram showing the different pieces of their code, their inputs and their outputs. If relevant,
please include expected warnings when executing the code. Mention any provided intermediate
results readers can utilize to decompose the submission into smaller pieces.
The README should also include something about your computing environment and expected run time; general terms are appropriate here. For example: I ran this on a modern laptop
(circa 2016) and it ran in a few minutes. or This code ran on high-performance cluster and took
one week. Finally, please clearly cite any open sourced content utilized in the submission, such
as resources shared in the http://www.fragilefamilieschallenge.org/blog/ or more general packages
distributed in the computation community.
What headers should be included at the top of each piece of code?
Based on the ideas in Nagler (1995), we think the following elements should be included at the top
of each piece of code:
• Purpose (in 140 characters or less)
• Inputs
• Outputs
51

• Machine used (e.g., laptop, desktop, cluster)
• Expected runtime (e.g., seconds, minutes, hours, days, etc)
• Set the seed at the beginning of each file (see rule 6 of Sandve et al. 2013)28
• All the package include statements (e.g., “library(ggplot2)” in R)
If you would like to deviate from this standard, please contact us.
How can I make my code easier to read?
It is hard to offer general advice, but one thing that we can recommend is at the end of the process
take some time to refactor your code). In our experience, code evolves over the course of a project,
and at the end it can be helpful to refactor in order to clean up the structure, improve variable
names, and promote modularity.
Even if you dont refactor your code, please include additional comments to helper functions
and code segments that may be obscure to new readers.
What is our standard for computational reproduciblity for the special issue?
Our standard for computational reproducibility for this special issue is that we should be able to
take whatever code and data you submit, add the Fragile Families Challenge data file, and then
reproduce all of the figures in your paper, all of the tables in your paper, and your predictions.csv
file.
What is not included in our standard for computational reproducibility for the special
issue?
We will not attempt to completely recreate your analysis from the written materials. Also, we
will not verify that your description in the paper matches the code. For example, if the paper says
that you use logistic regression to generate your predictions, we will not verify that the code also
uses logistic regression. Further, we will not verify the information that you have provided from
external sources. For example, if you write in the paper that your submission was 10th on the
28

Note added afterwards: This is incomplete. It also helps to set seed inside of functions that have randomness.
Also, there are different kinds of seeds. For example, in Python numpy uses a different seed from the standard python
seed.

52

leaderboard, we will not verify this fact. Finally, we will not verify any of the numbers that are
included in the text of the manuscript. For example, we would not verify a claim in the text such
as: dropping variables with no variation removes 10% of variables. As we hope this list illustrates,
our standard of computational reproducibility is in fact quite limited.
What license should I use?
We strongly recommend the MIT license. You can find it here. Simply replace with 2018 and with
the name of all co-authors of the paper, in the order they are listed in the paper. If you would like
to use some other license, please contact us.
What should I read to learn more about computational reproducibility?
Heres a partial list. If weve left off a good resource, please let us know (fragilefamilieschallenge@gmail.com).
Nagler (1995) “Coding Style Good Computing Practices” PS: Political Science & Politics.
Peng and Eckel (2009) “Distributed Reproducible Research Using Cached Computations”
Computing in Science & Engineering.
Sandvae et al (2013) “Ten Simple Rules for Reproducible Computational Research” PLOS
Computational Biology.
Stodden et al (2016) “Enhancing reproducibility for computational methods” Science.

A.3

Instructions for executing the code
The instructions below will walk you through executing the code for any Fragile Families

Challenge submission featured in the special issue of Socius.
1. Install Docker
You can install Docker at: https://www.docker.com/. Simply navigate to the ”Get Docker” tab.29
To verify that Docker is installed properly, open your favorite command line tool (e.g., Terminal
for Mac or Command Prompt for Windows). You should be able to run docker info and see
29
Docker should run smoothly for machines running Linux, Mac OS, and Windows 10 Pro. If you are running an
earlier version of Windows, we would recommend installing Docker onto an Amazon Web Services instance.

53

an output beginning with:

2. Pull the FFC Docker Repository
Once you have installed Docker, you will be able to download and run the FFC code. Each of the
submissions has already been packaged into a Docker image containing all of the code as well as
software / packages needed to execute. The images are labeled with the following tags:
First Author

Docker Image Tag

Rigobon, Daniel

rigobon winning 2019

Filippova, Anna

filippova humans 2019

Compton, Ryan

compton data 2019

McKay, Stephen

mckay when 2019

Carnegie, Nicole

carnegie variable 2019

Stanescu, Diana

stanescu using 2019

Raes, Louis

raes predicting 2019

Altschul, Drew

altschul leveraging 2019

Goode, Brian

goode imputing 2019

Roberts, Claudia

roberts friend 2019

Davidson, Thomas

davidson black 2019

The instructions below will proceed with the image tag stanescu using 2019. However, the
reader should replace every instance of stanescu using 2019 with the desired Docker tag from the
table above.

Download the desired image with the command: docker pull 2018dliu/replication:stanes
This should give the output shown below:
3. Run the Image
54

The next step is to run the Docker image, which will launch a Docker container, an isolated
computing environment, on your machine. Doing so simply involves typing: docker run -t
-d 2018dliu/replication:stanescu using 2019 into your terminal.
Those running the image tagged davidson black 2019 should use this command instead:
docker run -t -d -p 8888:8888 2018dliu/replication:davidson black 2019.
This command will expose one of the container’s ports for Jupyter’s notebook UI. Furthermore,
those running the image tagged rigobon winning 2019 should use the following command: docker

run -t -d -p 8888:8888 --user root 2018dliu/replication:rigobon winning 2019.
4. Explore the submitted code

You can now dig inside the running container to access its contents. The first step is identifying the name of the container. To do so simply type the command: docker container ls.
You will receive output similar to:
In the above output, the ID of the container is: 3d4de315125f. To access the con-

55

tainer, run docker exec -it [insert-container-ID] bash (e.g.: docker exec -it
3d4de315125f bash) . After you run this command, you will be inside of the container and have
access to all of the necessary code and dependencies. You can run ”ls” to view all of the files that
were submitted to the Challenge.
The README files contain instructions for executing the code. For several of the submissions, we have supplemented the author’s README with one of our own, providing additional
execution instructions. In all cases, you must first move the Fragile Families Challenge data files
into the container. You can move the files using the command docker cp, which will copy the
data files from the local file system into the container. Then execute the run all script to trigger the
computing pipeline. This file can be found in either the root or code directory.

56

