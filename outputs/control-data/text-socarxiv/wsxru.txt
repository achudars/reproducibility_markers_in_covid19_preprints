Best Practices for Managing Turnover in
Data Science Groups, Teams, and Labs
A Report from the Berkeley Institute for Data Science’s
Best Practices in Data Science Series
Dan Sholler1,2†* , Diya Das1,3† , Fernando Hoces de la Guardia4† , Chris
Hoffman5‡ , François Lanusse1,6,7‡ , Nelle Varoquaux1,11‡ , Rolando
Garcia8‡ , R. Stuart Geiger1‡ , Shana McDevitt9‡ , Scott Peterson10‡ , Sara
Stoudt1,11‡
1

Berkeley Institute for Data Science, University of California, Berkeley

2

rOpenSci

3

Department of Molecular and Cell Biology, University of California, Berkeley

These authors contributed

4

Berkeley Initiative for Transparency in the Social Sciences, University of California, Berkeley

equally to this work, order

5
6

Research IT, University of California, Berkeley
Berkeley Center for Cosmological Physics, University of California, Berkeley

7

Foundations of Data Analysis Institute, University of California, Berkeley

8

RISELab, University of California, Berkeley

9

QB3 Vincent J. Coates Genomics Sequencing Laboratory, University of California, Berkeley

*

Corresponding author:

dsholler@berkeley.edu
†

is alphabetical
‡

These authors contributed
equally to this work, order
is alphabetical
Published: 05 Mar 2019

10

Morrison Library and Graduate Services Library, University of California, Berkeley

11

Department of Statistics, University of California, Berkeley

DOI: 10.31235/osf.io/wsxru

Abstract:
License: Creative
Commons
Attribution (CC BY 4.0 Intl)

Turnover is a fact of life for any project, and academic research teams can face
particularly high levels of people who come and go through the duration of a project.
In this article, we discuss the challenges of turnover and some potential practices
for helping manage it, particularly for computational- and data-intensive research
teams and projects. The topics we discuss include establishing and implementing
data management plans, ﬁle and format standardization, workﬂow and process
documentation, clear team roles, and check-in and check-out procedures.

Recommended citation: Dan Sholler, Diya Das, Fernando Hoces de la Guardia,
Chris Hoffmann, François Lanusse, Nelle Varoquaux, Rolando Garcia, R. Stuart
Geiger, Shana McDevitt, Scott Peterson, Sara Stoudt. “Best Practices for Managing Turnover in Data Science Groups, Teams, and Labs.” BIDS Best Practices
in Data Science Series. Berkeley, CA: Berkeley Institute for Data Science. 2019.
doi:10.31235/osf.io/wsxru

1 of 8

Introduction
Turnover, or the arrival and departure of personnel from a project, is common in
academic research teams. Students, postdocs, research staff, visiting scholars, and
collaborators come and go from projects as their career and research opportunities
change. For this reason, labs and teams should have mechanisms in place to ensure
smooth transitions from one team member to the next. Data-intensive research
teams are no different; in fact, the nature of data-intensive research makes turnover
management essential to sustaining research efforts. As we have discussed in
previous Best Practices blog posts and meetings, data intensive research teams often
are heterogeneous in members’ backgrounds and expertise, require collaboration
across disciplinary and subdisciplinary boundaries, and weave together smaller,
easier to manage projects into larger, more complex research systems.
In this post, we discuss some of the challenges presented by turnover and lay out
some best (or “good enough”) practices for contending with the challenges. First, we
explain how data collection and accessioning can be standardized by ﬁrst piloting
the collection process and documenting it in suﬃcient detail. Next, we turn to issues
around funding requirements and turnover, arguing that teams should know and
consider funding agency requirements for data-intensive research (e.g., accessing,
using, and publishing datasets) and document team member roles for managing
adherence to funder rules. We conclude by emphasizing check-in and check-out
procedures as a foundational way of managing turnover in data-intensive research
teams and acknowledge the numerous questions unaddressed in our working group
meeting.
Best practices for data-intensive research have been the subject of several papers,
notably a paper of the same name and a follow-up paper by Greg Wilson and
colleagues (Wilson et al., 2014, 2017). We encourage all research groups to read and
reﬂect on how those practices may be of use. Our group’s discussion touched on
several of the same areas, though there is incomplete overlap as the focus of our
discussion was on how the diﬃculties posed by regular personnel turnover can be
mitigated by project pre-planning and documentation.

Data management should be a part of the project plan
Our Best Practices series meetings are centered on developing generalizable practices to promote reproducible data-intensive research. The components of reproducibility include the “the public availability of the data and software” (Rokem et al.,
2018, p. 7). Best practices for creating reproducible scripts and software packages

2 of 8

is a common topic of discussion–researchers who conduct data-intensive research
are often familiar with Git and GitHub or other software development and version
control systems, recognize the importance of code documentation, and engage with
one another in robust open source software communities.
Less often discussed are issues around data management, such as data accessioning
and permanent storage, dataset sharing, and data citation. Yet emergent data
services offer several ways forward in developing sound data management practices.
Examples include the Center for Open Science’s Open Science Framework, the
newly-formed partnership between California Digital Library’s DASH and Data Dryad
(Simms, 2018), discipline-speciﬁc repositories such as GenBank,1 and multipurpose
services such as ﬁgshare2 or Zenodo.3 DataVerse4 is a useful resource for evaluating
and comparing data management tools and services for your project.

Data management plans/protocols (DMPs)
Armed with knowledge of the available tools and services, research teams can begin
to develop data management plans (DMP) and consider how to address turnover
within the plan. Although DMPs are now a common (and sometimes frustrating)
mandate from many funders, teams often do not recognize the beneﬁcial role they
can play in managing turnover. For example, in our discussion, we emphasized that
DMPs should document the roles of each person in accessing, storing, backing up,
locating, and otherwise stewarding the data.
Commonly, much of this data management work is handled by “temporary” employees such as students and postdocs; in turn, documentation of the processes
and responsibilities is paramount to the success of the project. Additionally, one or
more team members should maintain responsibility for adhering to funder, journal,
and institutional policies about data access, storage, and sharing. This member can
consult with your institution’s Research Data Management (RDM) service, where
available (for example, UC-Berkeley’s RDM offers consulting5 ). The California Digital
Library’s DMPTool6 is also a robust tool for creating a tailored, team-speciﬁc DMP
(see also Sallans and Donnelly 2012 and Starr et al. 2012).
We noted that for individual research projects, though the protocols are often docu1

https://www.ncbi.nlm.nih.gov/genbank/
https://ﬁgshare.com
3
https://zenodo.org
4
https://dataverse.org
5
See more information at https://researchdata.berkeley.edu
6
https://dmptool.org

2

3 of 8

mented for pre-approval, recording of experimental observations or intermediate
data outputs is not necessarily standardized. Furthermore, data management can
vary by data type; it is important to construct a plan that aligns with your pipeline
and is accessible and easy to use for your research team, no matter members’
degree of technical proﬁciency (Geiger et al., 2018).

Standardization and documentation of data and ﬁle formats
With multiple individuals involved, it is especially important to not just plan for
standardization, but also document standards as part of a project plan at the start
of a project. We observed that journal and funding agency requirements and
infrastructure can have a large effect on degree of standardization. In genomics, for
example, raw sequence data is often expected to be shared at time of publication,
and the National Cancer Biotechnology Institute maintains a Sequence Read Archive
for data deposition7 (see also Leinonen et al. 2010).
We also noted that large research consortia, by their nature, must set standards
for data collection types. The Human Genome Project, for example, appears to be
responsible for the development of several ﬁle formats (Kent et al., 2002). Finally, it
is important to acknowledge that software availability has shaped the development
and adoption of other ﬁle formats (Cock et al., 2009). Within a research group, standardization of ﬁle formats and ﬁle organization is an important for the development
of reproducible analysis pipelines.
Setting internal standards around ﬁle structure and ﬁle management is one of the
practices for reproducibility that can provide the biggest bang for a (very small) buck.
Project TIER8 is an excellent resource as it provides detailed templates and clear
speciﬁcations on how, when, and where documentation should be created (see
also Medeiros and Ball 2017). In addition, workﬂow diagrams can be of assistance
in helping team members understand and adhere to project standards for data
storage and management. Some good examples of data workﬂow diagrams can
also be found in the edited collection The Practice of Reproducible Research (Kitzes
et al., 2018).

Documenting data collection and analysis procedures
While DMPs are more standardized in the physical and life sciences, those of us
from the social sciences (like economics, political science and psychology) also saw
7
8

https://www.ncbi.nlm.nih.gov/sra
https://www.projecttier.org/

4 of 8

potential in adopting some of the protocols and best practices outlined above.
There is plenty of space for learning, here we suggest just two examples. First, the
concept of a scientiﬁc lab notebook could be adopted in social science survey data
collection to create a log of all the main methodological decisions (designing the
instruments, ﬁeld and data intake logistics, etc.; see Schreier et al. 2006 ). Currently
many research groups track those decisions in an ad-hoc manner, but a standardized
procedure would help with later publication of all the materials.
Second, the use of data from administrative records (e.g., tax, hospital, or insurer
records) is an ever-growing practice in social science. One key problem is that
until now, most of the access to these rich data sets is restricted to a very small
number of researchers with the right connections. The absence of a clear pipeline to
access administrative records is justiﬁed on the the protection of conﬁdentiality and
privately identiﬁable information. We think that this debate could be enriched by
the experiences in genomics and other data-intensive disciplines that must deal with
the trade-off of obtaining access to highly sensitive information and sharing data
to ensure reproducibility. Unfortunately, we did not have time for a full discussion
during this meeting.
DMPs can be diﬃcult to construct without empirical grounding and contextual
expertise (i.e., without having begun data collection in the ﬁeld). Piloting can aid
in the process of constructing a plan. In the pilot phase, team members can begin
delineating each member’s roles and responsibilities. Even when a DMP exists,
piloting can help to reevaluate the feasibility of the proposal. Additionally, it might
also be useful to discuss and document authorship and credit attribution during this
process (i.e., which roles and responsibilities correspond to authorship opportunities
and orders).

Develop, Document, and Implement Check-In and CheckOut Procedures
While data standardization and documentation was a signiﬁcant focus of our discussion, the larger question of adaptability to turnover remained. From our experience,
we have found that implementation of check-in and check-out procedures can
help. While such procedures are often found in many groups from a regulatory
perspective, we suggest extending them to address project-speciﬁc concerns. At
check-in, we noted that it was important to discuss macro-scale questions in project
organization, addressing both technical requirements and the project management
scheme.

5 of 8

We also found it helpful to discuss desired outcomes and goalposts, both for the
project and for individual contributors, to ensure that contributors have a stake
in project success. Finally, we found it helpful to discuss all aspects of "how the
team works" - while many modes of work might be acceptable for any given project,
reducing friction with existing workﬂows is beneﬁcial. These topics ranged from
ﬁle structures and coding styles to codes of conduct and managing interpersonal
interactions.
Ideally, much of project documentation would be written during the course of a
project, but sometimes best intentions escape us. In addition, a team member’s
tacit knowledge still requires documentation. At the departure of a research group
member, we recommend a checkout procedure for computational projects alongside any existing checkout for administrative or experimental purposes. We also
recommend a review of job responsibilities, project, data and backup organization
against the documentation, in the event that items were added or removed without
prior documentation, so that this knowledge can be passed on to the next person.
We also explicitly recommend a review of any services that are tied to institutional
email addresses, as they may become deactivated. Access to resources and data
should be reviewed in this context as well, to ensure not only that any restrictions
on access after departure are met, but also that remaining team members have
rights to manage access. From an administrative standpoint, we also recommend
collecting non-institutional contact information from the departing team member,
should further questions arise, or for the sake of future publications.
Due to the brevity of our discussion, numerous topics were not addressed. Examples
include a more in-depth discussion of gaining and managing access to sensitive data,
and administrative concerns (e.g., formalizing collaborations) (Playford et al., 2016).
Like all Best Practices lunches, this discussion raised many topics of discussion for
future meetings. We continue to recommend that researchers adopt this format
for discussions in their own communities, so they are able to address the nuances
of their particular situations. However, we also derived great beneﬁt from an
interdisciplinary meeting that allowed us to contrast the practices of different ﬁelds.

Acknowledgments
This work was funded by the Gordon & Betty Moore Foundation (Grant GBMF3834)
and Alfred P. Sloan Foundation (Grant 2013-10-27), as part of the Moore-Sloan Data
Science Environments grant to the University of California, Berkeley.

6 of 8

References
Cock PJ, Fields CJ, Goto N, Heuer ML, Rice PM. The Sanger FASTQ ﬁle format for sequences
with quality scores, and the Solexa/Illumina FASTQ variants. Nucleic acids research. 2009;
38(6):1767–1771. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2847217/.
Geiger RS, Sholler D, Culich A, Martinez C, de la Guardia FH, Lanusse F, Ottoboni K, Stuart M,
Vareth M, Varoquaux N, Stoudt S, van der Walt S. Challenges of Doing Data-Intensive
Research in Teams, Labs, and Groups. In: BIDS Best Practices in Data Science Series.
Berkeley, CA: Berkeley Institute for Data Science; 2018.
https://doi.org/10.31235/osf.io/a7b3m.
Kent WJ, Sugnet CW, Furey TS, Roskin KM, Pringle TH, Zahler AM, Haussler D. The human
genome browser at UCSC. Genome research. 2002; 12(6):996–1006.
https://genome.cshlp.org/content/12/6/996.full.
Kitzes J, Turek D, Deniz F. The Practice of Reproducible Research : Case Studies and Lessons
from the Data-Intensive Sciences. Oakland: University of California Press; 2018.
http://practicereproducibleresearch.org.
Leinonen R, Sugawara H, Shumway M, Collaboration INSD. The Sequence Read Archive.
Nucleic Acids Research. 2010; 39. https://www.ncbi.nlm.nih.gov/pubmed/21062823.
Medeiros N, Ball RJ. Teaching Integrity in Empirical Economics: The Pedagogy of
Reproducible Science in Undergraduate Education. In: Hensley MK, Davis-Kahl S, editors.
Undergraduate Research and the Academic Librarian: Case Studies and Best Practices
Chicago: Association of College & Research Libraries; 2017.
https://scholarship.haverford.edu/cgi/viewcontent.cgi?article=1189.
Playford CJ, Gayle V, Connelly R, Gray A J. Administrative social science data: The challenge
of reproducible research. Big Data & Society. 2016; 3(2):2053951716684143.
https://doi.org/10.1177/2053951716684143, doi: 10.1177/2053951716684143.
Rokem A, Marwick B, Staneva V. Assessing Reproducibility. In: Kitzes J, Turek D, Deniz F,
editors. The Practice of Reproducible Research : Case Studies and Lessons from the
Data-Intensive Sciences Oakland: University of California Press; 2018.
https://www.practicereproducibleresearch.org/core-chapters/2-assessment.html.
Sallans A, Donnelly M. DMP Online and DMPTool: Different Strategies Towards a Shared
Goal. International Journal of Digital Curation. 2012; 7(2):123–129.
https://doi.org/10.2218/ijdc.v7i2.235.
Schreier AA, Wilson K, Resnik D. Academic research record-keeping: Best practices for
individuals, group leaders, and institutions. Academic medicine: journal of the
Association of American Medical Colleges. 2006; 81(1):42.
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3943904/.

7 of 8

Simms S, Letter to the Community: CDL and Dryad Partnership. California Digital Library;
2018. https://www.cdlib.org/cdlinfo/2018/05/30/
letter-to-the-community-cdl-and-dryad-partnership/.
Starr J, Willett P, Federer L, Horning C, Bergstrom ML. A Collaborative Framework for Data
Management Services: The Experience of the University of California. Journal of eScience
Librarianship. 2012; 1(2). https://escholarship.umassmed.edu/jeslib/vol1/iss2/7, doi:
10.7191/jeslib.2012.1014.
Wilson G, Aruliah DA, Brown CT, Chue Hong NP, Davis M, Guy RT, Haddock SHD, Huff KD,
Mitchell IM, Plumbley MD, Waugh B, White EP, Wilson P. Best Practices for Scientiﬁc
Computing. PLoS Biology. 2014 Jan; 12(1):e1001745. doi: 10.1371/journal.pbio.1001745.
Wilson G, Bryan J, Cranston K, Kitzes J, Nederbragt L, Teal TK. Good enough practices in
scientiﬁc computing. PLOS Computational Biology. 2017 Jun; 13(6):e1005510. doi:
10.1371/journal.pcbi.1005510.

8 of 8

