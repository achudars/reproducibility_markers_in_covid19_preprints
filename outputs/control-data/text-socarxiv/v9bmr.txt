Making it count.
An inferentialist account of computer simulation
Jaakko Kuorikoski*
University of Tampere, Finland
Samuli Reijula
University of Tampere, Finland

Keywords: inferentialism; scientific representation; computer simulation; similarity

Abstract
The dual problems of how an idealized model can represent and provide information about
its target have become a central topic of in the philosophy of science. We argue that several
current views are misguided in assuming that the epistemology of modeling and simulation
must build on a philosophical theory of the representation relation (e.g. isomorphism,
similarity). We extend Robert Brandom’s inferentialist account of meaning into scientific
representation to argue that representational language is explicatory, not explanatory, in
nature. We provide a broader philosophical rationale for inferential accounts of scientific
representation, and an epistemologically modest account of the role of models in terms of
inferential scorekeeping. We apply these views to the contested case of computer simulations
to argue that, although the praxis of simulation modeling resembles that of scientific
experimentation, simulations alone cannot lead to genuinely novel discoveries about the
world, as they are merely tools for keeping our reasoning straight.
1 Introduction
How can highly abstract and idealized models stand for things in the world, and how do they
allow us to learn new things about their targets?. These questions seem to arise from a deep
philosophical puzzle of scientific representation, which, due to a number of transformations
in philosophical discussions, has acquired a central position in the epistemology of science. As
the philosophical understanding of science moved from a theory-centered to a modeloriented view of scientific knowledge and practice, questions of approximate truth were
reformulated into problems of representation, questions of the logic of inductive inference to
ones about model-world inferences andaccounts of explanation by laws to the possibility of
explanation via mechanisms virtually isolated in models ([anonymized]). A formidable
explanatory burden has thus been placed on a philosophical account of the relationship
*1

Corresponding author. Prof. Jaakko Kuorikoski, Philosophy / SOC, 33014 Tampere University, Finland.

1

between our models and the external world. What characterizes theoretical modelling in
general is a certain epistemic strategy making use of surrogate reasoning: one first builds
something or sets something up, then investigates the properties of that constructed thing,
and finally ponders how the discovered properties of the constructed thing relate to the real
world (Hughes 1997). This poses an epistemic problem for those empiristically minded - how
can we learn something genuinely new about the world without directly empirically
investigating the target system itself? Surely the answer to this question has to invoke the fact
that the model represents its target. But what if all of this is just wrong?
In this article we argue that this burgeoning philosophical literature on representation is
indeed founded on a conceptually questionable basis. We make our case by providing a
comprehensive inferentialist account of the representational and epistemic roles of computer
simulations. We focus specifically on simulations because the practice of simulation modeling
most resembles scientific experimentation in various respects (e.g., Morgan 2005; Mäki 2005;
Winsberg 2010; Boge 2018), and it, therefore, provides the most striking case of the problem
of accounting for the epistemic added-value of models in general: We seem to learn
something genuinely new about the world by manipulating an artificial surrogate system and
then “observing” what the end result is. In analytical models, this discovery is a matter of
mathematical derivation of a result from the modelling assumptions, but in the case of
computer simulations, the analogy between modeling and experimentation feels more
compelling. Simulation modeling is therefore the context in which representationalist thinking
is most likely to lead into attributing magical epistemology to epistemic artefacts.
Our approach extends Robert Brandom’s work to the domain of scientific representation.
Inferentialism about model-based representation, broadly defined, states that it is misguided
to try to explain the epistemic role of models through concepts such as representation,
similarity, or various structure-preserving mappings, since the very relation of representation
is constituted by the role of the model in making inferences about the target system. In
contrast, we call referentialist accounts all views that start from an account of the
representational relation, and then use it to explain (successful) model-world inference.
Broadly inferential views have been applied to understanding modelling in general (see Suarez
2004; Contessa 2007; de Donato & Zamora-Bonilla 2009), but they have not yet been used to
account for the representational role of computer simulations. This is not surprising, since the
inferential conception is intuitively more compelling in the context of analytical models,
whereas computer simulations seem closer to actual experiments, thus including aspects of
mimesis, materiality, production of data, and surprise which seem to escape the inferential
framework.
We argue that a thoroughly inferentialist view of the ‘semantics’ of models (a view about the
representational content of a model, and about how a model can stand for its target system),
also implies a specific view of the epistemological reach of simulations (a view about how and
why we can learn something about the target by investigating a model) - a view we call the
scorekeeping view. This view denies that models and simulations have the capacity to produce
truly new knowledge in the same way as material experiments. It holds instead that their
2

epistemic value can be analyzed, without residue, as the improvement of the scope and
reliability of our inferences from given empirical premises to empirical conclusions made
possible by the model as an external scorekeeping device. Similar views about the
epistemology of simulations have been proposed by Otavio Bueno (2014), who argues that
the epistemic value of simulations resides in their ability to aid inference making, but then
explains this epistemic role in referentialist terms by appealing to structural relations of
(partial) homomorphisms.2 Claus Beisbart also defends a view about the epistemic reach of
simulations similar to ours, but does not take an explicit stance on the semantics of models
(2012; 2018, see also Beisbart & Norton 2012). Nevertheless, he seems to implicitly
presuppose some form of intentionalist referentialism, according to which representation is
brought about by the intentions of the modeler (2012, p.407), and also links the model
underlying the simulation to its intended target in terms of denotation (ibid. p.406).
Our position is in many respects compatible with those of Bueno and Beisbart, but our views
on the semantics and the epistemology of simulations rely on a broader outlook on the nature
of aboutness or representation in general in deploying Robert Brandom’s ideas of deontic
scorekeeping and inferential commitments (1994, Ch.3). We therefore complement the
inferential theory of scientific representation, originally put forward by Mauricio Suárez
(2004), by providing a positive philosophical rationale for the two jointly necessary conditions
Suarez proposes for representation, representational force and inference making, and we
explain the former in terms of the latter. Together, (inferentialist) inferentialism about
representation and the scorekeeping view of the epistemology of modeling show how models
and simulations act as inferential crutches in scientific reasoning and argumentation, helping
us to keep our reasoning straight. Put bluntly, scientific results are established by arguments.
Modeling does not help to establish the empirical premises of such arguments, but they aid
with the inferences that take us from those premises to the conclusions, our results.
We take the main advantages of our inferentialist approach to be twofold. First, in relation to
simulation models, the approach provides a novel account of the epistemic differences
between an artefact being a mere computational device, a computer simulation, and a true
experiment. Importantly, our account implies that such epistemic differences are sharp in the
sense that they are not blurred either by the fact that the practice of modelling does typically
involve an experimental practice, or because evidence from material experiments can often
only be applied to scientific reasoning by embedding them as assumptions in models. Our
account also has the surprising implication that a physically identical activity (model use) can
be epistemically regarded either as modelling or experimentation, depending on the doxastic
commitments of the participants. We illustrate our account with an example of computer
simulations of crowd dynamics.
Second, in relation to scientific models in general, the inferentialist approach relegates many
questions concerning the epistemology of scientific modeling from philosophy to scientific
fields in which they can be empirically answered. This is because, according to inferentialism,
2

Bueno also calls his view ‘inferential account computer simulation’, but we do not use this label here
as his account is not primarily about the representational properties of simulations.

3

representational vocabulary does not have an explanatory, but rather an explicative function:
Claims about representation do not pick out relational properties in the world, properties
which are somehow responsible for inferential successes, but instead, they simply summarize
the inferential activity itself. The task of investigating the successes and failures of these
inferential activities is not a philosophical or conceptual one, but an empirical question
concerning both the specific subject matter at hand, and the cognitive science of the use of
external inferential aids.
Section 2 introduces our approach to representation in general. In section 3 we show how
Brandomian inferentialism can be applied to scientific models, and how it leads to a an
epistemology of modeling that we call the scorekeeping view. In section 4, we extend the
inferentialist treatment to simulation modeling. Section 5 then introduces our example of
crowd dynamics and explores the epistemic difference between simulation, computation, and
true material experimentation. Section 6 concludes by addressing some of the powerful
intuitions motivating the referentialist accounts.
2 Inferentialism and representation. A primer
The question of scientific representation is a special case of the more foundational (perhaps
the foundational) philosophical question about the nature of aboutness: In virtue of what do
some things in the world stand in for other things? This question must be answered whether
we are theorizing about the meaning of words, the content of propositional attitudes, or the
epistemic role of scientific models.
The distinction between content and use of representations suggests two basic ways in which
we can approach the question of aboutness. We can aim to explain what we do with
representations (such as understanding the meaning of sentences or reasoning about the
world) in terms of the representational content and the representation relation between the
representation and the represented thing. This referentialist philosophical programme takes
as its points of departure a theory of the representation relation, such as the causal theory of
reference (in its many incarnations, see e.g., Fodor 1998), and then explains aspects of the
use of representations, such as the systematic and generative nature of language, with the
help of this relation. This direction of explanation seems very intuitive: surely it is by virtue of
the content of our representations that we are able to do things with them and surely it is by
virtue of the objective representation relation that the representations have the content they
do.
When trying to understand the representational capacity of scientific models, a proponent of
referentialism needs a substantial theory of the representation relation to explain the
informational content and the resulting ‘epistemic power’ of the model. A long tradition in
the philosophy of science employed the theory of reference to account for the meaning of
theoretical terms and, ultimately, to ground the (approximate) truth of scientific theories (see
Stich 1996). But when the focus turned to models, these semantic properties attributable to
propositionally structured content felt out of place. Proposed replacements for them include
similarity, (partial) isomorphism, and homomorphism. These mappings, so the suggestion
4

goes, establish suitably objective relationships between the structure of the representation
and the structure of its target, so that they can explain the representational content, and thus
the epistemic usefulness, of the model. Also many pragmatic accounts of scientific
representation emphasizing the importance of interpretation and thus moving beyond the
dyadic understanding of representation are still referentialist in this sense: Although they
might not be committed to any specific similarity or structure-preserving mapping, they do
rely on the concept of ‘denotation’, taken as a primitive, for linking the key model parts to the
world. Examples of such accounts are R.I.G Hughes’ DDI account of representation (Hughes
1997) and the recent DEKI account of model-based representation by Roman Frigg and James
Nguyen (2016).
The inferentialist analysis of representation holds that such referentialist direction of
explanation puts the cart before the horse. Use cannot be explained by content, which is in
turn explained by the representation relation, because content and representation are
constituted by use. That is, inferentialism inverses the explanatory roles of content and use.
According to inferentialist semantics, the content of linguistic expressions and propositional
attitudes are constituted by the inferential commitments undertaken by uttering the
expression (Brandom 1994). Roughly, the meaning of a sentence is determined by what other
sentences the utterer is now entitled to assent to and to what sentences she is taken to be
committed to. For example, if I claim that the temperature of a particular metal rod is 200C,
then I’m expected to be able to give reasons concerning measurement processes and
observable physical characteristics of the said rod. Some of these commitments have
important extra-linguistic aspects, such as the claims concerning acts of possible
measurement (subject to language entry-norms) and a practical commitment of not touching
it with bare hands (subject to exit-norms). As the popular metaphor has it, grasping the
meaning of an utterance is constituted by knowing one’s way in the web of reasons, which
the utterance is a part of (Sellars 1956).
According to the inferentialist view, full propositional content, genuine aboutness, is possible
only within this game of giving and asking for reasons (Brandom 1994). Thermostats and
simple organisms react to changes in their environment, but since they cannot relate these
actions to other possible actions, the relation between the stimulus and the response is one
of simple causation, devoid of any meaning in the full sense. The functioning of the thermostat
is caused by temperature, but is not about temperature. Such causal reactions are not correct
or incorrect in themselves, they just are. In contrast, meaning and aboutness are normative
in that there are correct and incorrect applications of a referring expression.
Here it should be noted that meaning and representation do not directly depend on the
intentions of a language user. The commitments and entitlements resulting from speech acts
are governed by the principles of language use in the linguistic community, not by the
intentions of a single language-user. Furthermore, the game of giving and asking for reasons
is an empirically and practically constrained practice, connected to the world through
perception and action. (Brandom 1994, 331). Hence, a proponent of inferentialism does not
deny the reality or objectivity of representation. There can be facts of the matter about
5

assertions (and internalized assertions, i.e. judgments) being correct and mistaken.
Furthermore, according to our view, an inferentialist need not be committed to a nonnaturalist ontology of primitive normative facts. She is only committed to an ontology that
includes normative acts of sanctioning moves made in the game of giving and asking for
reasons. Normativity of meaning is constituted by the other language-users keeping score of
what someone said, what follows from it, what is compatible and incompatible with the
utterance, and so on.
It is not our aim to argue extensively against the referentialist order of explanation here,
although our inferentialism is motivated by a deep skepticism concerning the existence of any
substantial representation (or reference) relation, which could explain inferential success (or
failure). Instead, we build a positive case for inferentialism by applying the inferentialist
approach first to scientific modeling in general, and then to computer simulation in
particular.3 The resulting analysis of representational notions does not deny the sensibility of
referential language, but instead, it helps us to understand what it is that we do when we use
the referentialist idiom.
3 Inferentialist inferentialism about models
Scientific models are surrogate systems used to make inferences about their targets. This
much is agreed on in the philosophical literature on models. Surrogate systems come in many
forms and fulfill different inferential functions. Models may be physical scale models, formal
models solved analytically with pen and paper, as well as computational models run on a
computer. Data models provide systematic and cognitively economic representations of the
salient features of data, phenomenological models representations of relevant patterns in the
modelled phenomena, and theoretical models aim to represent important features of the
causal mechanisms responsible for the modelled phenomenon.
The inferential conception of scientific representation first grew out of the difficulties that
similarity- and structural-mapping-based theories had in singling out the correct kind of
relation between a model and its target (such as iso- and homomorphisms) (Suarez 2004;
Contessa 2007). Such theories which portray representation as a factual two-place
relationship between representations and things in the world have difficulties in accounting
for the representation relation as non-symmetric, non-transitive and non-reflexive, and
struggle in trying to account for the possibility of misrepresentation (Suarez 2003).
Furthermore, in light of various modeling practices in the sciences, establishing the sufficiency
– or even necessity – of the existence of a particular dyadic relationship for all scientific
representation has proved difficult. In their replies to such criticisms, Bueno and French (2011)
concede that any dyadic relationship (such as a partial isomorphism) is not alone sufficient for
establishing representation in scientific practice – while at the same time insisting on the

3

As far as we know, Brandom does not address in length the question of how scientific theories and
models represent. However, see Brandom (1994), pp. 518-520 for his brief remarks on concrete
representational devices such as as maps and pictures.

6

necessity of structure-preserving mappings as the constitutive basis for such practices.4 We
return to questions of the role of similarity and structure in section 6. Now, however, we move
on to outline how the inferential conception can be developed into a more positive
inferentialist account of scientific representation.
According to Suarez’s (2004) inferential conception, scientific representation has two
necessary conditions: First, a model represents a target only if (1) its representational force
points towards it, and (2) the model allows competent and informed agents to draw specific
inferences regarding the target. Although the purpose of the second condition is to distinguish
scientific representation from mere stipulations or arbitrarily chosen signs, Suarez emphasizes
that establishing representation does not imply that the model represents its target
completely or even accurately. What makes Suarez’s conception a deflationary one is that it
does not postulate a deeper (explanatory) property (e.g., similarity or structural mapping)
behind the two conditions. Instead, on different occasions, the schema provided by the
inferential conception can be filled in in different ways.
The inferentialist nevertheless claims that something more can, in fact, be said: The inferences
of condition (2) constitute the representational force of condition (1). Brandom briefly
outlines the way in which non-linguistic artefacts can be discursively treated as
representations with the familiar example of maps: A map is treated as a representation of
some features of the terrain when it is possible to explicitly endorse or criticize inferences
from claims about the map to claims about the terrain. The capacity to use something as a
representation is therefore, in a sense, parasitic on the aboutness of the claims concerning
the map and the terrain, which can in turn be given a fully inferentialist explication (1994,
pp.518-519). This narrows down the different ways in which the representational force of an
epistemic artefact can be grounded in any given context and removes the need for any further
explanation for why precisely these activities are relevant for representation.
As Suarez also points out, representation is therefore an activity and, strictly speaking, any
talk of an object being a representation is only a shorthand for talking about its
representational potential. The representational potential of a model is constituted by the
inferential affordances it provides to the epistemic agents using it to manipulate and navigate
the world ([anonymized], [anonymized], Suarez 2004; Vorms 2012). In the context of models,
inferring means using formal (syntactic) rules to derive contentful expressions from other
contentful expressions in a (within-the-model) truth-preserving way. Such a notion of
representational content is a normative one: A model offers an accurate representation of its
target in virtue of facilitating correct (outside-the-model) inferences about it.5 Unlike in the
case of purely linguistic tokens, the correctness of model-based inferences is not ultimately

4

When defending the necessity claim in the section 9.2 of their article, the authors cite Bueno and
Colyvan (2011). Bueno and Colyvan, however, acknowledge that they have not established such a result
(sec. 6).
5
By correct we mean inferences resulting in true conclusions. And as with other semantic vocabulary,
‘true’ is to be understood in an inferentialist manner (as a prosentential operator for taking over the
commitments of another expression)Brandom 1994).

7

determined by social factors (in contrast to the socially-governed use of conventional
symbols), but by the empirical success of the inferences (e.g., accurate predictions about the
target) and effective language-exit moves (successful actions related to the target) (cf.
Brandom 1994, 518-519 on the use of maps) .
Different models afford different kinds of inferences. Descriptive models (such as data models
and phenomenological models) facilitate inferences concerning regularities and other
occurrent properties of the modelled phenomenon (e.g., statistical inference), and possibly
predictive inferences, given that the modelled system is not expected to undergo structural
changes. In addition, theoretical models facilitate inferences to counterfactual situations
(what-if-things-had-been-different-inferences) and, by virtue of this, provide explanatory
information (see Woodward 2003; [anonymized]).
Taking stock, and a as a concrete reference point for the observations below, we suggest the
following characterization:
{IMR} (Inferentialist analysis of model-based representation): A source object M represents a
target system S iff a competent epistemic agent A may use M to make inferences about
some subset P of the properties of S. P is the set of aspects of S captured by M.
The competence requirement is needed on the one hand to rule out inferences based on luck,
and, on the other hand, to save the intuition that an object can be said to be a representation,
even though only a small group of people can actually use it as a representation (although
being a representation is only a shorthand for having the potential to be used in an act of
representation). Finally, to say that the source object is only used to make inferences about a
specific subset of the properties of the target is to simply acknowledge that all representations
are partial.
As with representation in general, the inferentialist denies neither the existence nor the
objectivity of model-based representation. For example, causal models represent causal
relations in the target system insofar as they facilitate inferences to the consequences of
possible interventions on the modelled system. It is just that the representation relation does
not explain these inferences. It is constituted by them, or in a less metaphysical idiom, we can
legitimately say that a model represents a target because of such inferences. Representational
language is a meta-vocabulary which facilitates relating and comparing our differing individual
epistemic perspectives on our shared reality. Such vocabulary can be used to summarize
which things are – and should be – used to draw inferences about others. Therefore,
representational language does not itself “refer” to anything epistemically explanatory (to a
“representational hook” anchoring a word to its referent, or a model to its target). The
explanatory resources lie elsewhere and are purely empirical: The way in which a cognitive
agent can use an artifact to improve the reliability and scope of her inferences concerning
another object is explained by (i) the cognitive capacities of the agent, (ii) the causal properties
of the artifact and (iii) the properties of the object of interest. There is no additional
explanatory challenge to be met by a substantial philosophical theory of the representation
relation.
8

For example, consider how observing and manipulating an orrery, a clockwork model of the
solar system, facilitates inferences about the movement and positions of planets. Here the
source (orrery) is a device built in light of our knowledge of the structure of the solar system
(target), and physical interaction with the source facilitates inferences about the target..
Conceptually, by drawing conclusions about, for example, the relative positions of the planets
at a given time, the model user is making explicit some implications of the modeling
assumptions embodied in the model (e.g., relative distances of the planets from the sun). For
those implications to be about the target, the modeler must be committed to taking such
assumptions as being true of it. The relation between the inferential affordances of the
epistemic artifact and the cognitive agent is, however, an empirical object of research for the
cognitive sciences, not for philosophy. It is quite natural, then, for the inferentialist to view
model-assisted inference as a case of extended (or “scaffolded”) cognition, where the use of
external tools extends the limited internal cognitive capacities of the agent.6
Let us now examine three central implications of the inferentialist view.
(1) Model-based representation is neither subjective nor arbitrary. Although, in principle,
anything can be “used” as a model simply by stipulating that object A stands for object B,
whether A possesses any such intrinsic features which would enable the model user to
improve her inferences about B is certainly not up to the agent to decide. It is also important
to note that the mental “intention” of the agent does not by itself, as if by magic, establish
the representation relation. The relation holds due to the inferential allowances and
commitments that the act of declaring A as a representation of B establishes. For example, it
is certainly possible to use a beach ball as a model of the earth, but for most theoretical
purposes (above elementary school level), such a model would be epistemically useless:
whatever analogies one could find between the coloured stripes of the model object (the
source) and the topography of the earth would, in all likelihood, not play a part in any
inferences where the examination of the source implies yet unverified properties of the target
system. And even when it is possible to specify certain morphisms between the ball and earth,
they do not concern properties pertaining to any scientifically relevant aspects of the target.
(2) If the object M systematically leads a competent epistemic agent to make incorrect
inferences concerning the set of properties P, then M misrepresents those properties, and
thus partially misrepresents the system S (of course, the agent can also make mistakes in using
the model by making wrong within-model inferences, but this is a different kind of error). Like
any form of disagreement, misrepresentation is possible only against the backdrop of broader
agreement about the use of M, i.e., that the inferential use of M is mostly in line with the
established doxastic commitments that establish that the use of M is, nevertheless, about S.

6

We have no interest in taking part in debates on whether such modeling situations should be
considered as a case of genuine extended cognition, in contrast to, say, embedded cognition, where
the human agent retains a prioritized position (see Clark 2008, Giere 2002 vs. Rupert 2004). Here our
aim is only to suggest that, especially from epistemological perspective, the model could be thought of
as part of the inference-making system, together with the modeler.

9

A critic might now think that we are confusing two things: what representations are and what
we do with them (Bueno and French 2011). Surely the fact about whether a representation
carries information about its target should be prior and independent from whether there are
competent users using the representation to produce new knowledge about the target? As
stated above, the relevant properties of the artefact, the represented system, and the user all
exist independently and prior to the act of representation, but the artefact becomes a
representation of its target only in use. What comes to the latter part of the objection, we will
next argue that, indeed, (3) models and simulations should not be considered as sources of
genuinely new empirical knowledge.
4 The scorekeeping view of the epistemology of models
Inferentialism is an account of the ‘semantics’ of models, i.e. what it means for a model to
represent a target.Nevertheless, the denial of any substantial representation relation carrying
information from the target to the model also leads to an empiristically plausible view of the
epistemic status of models: the scorekeeping view. Since models are, by definition, external
inferential aids without any “extra” epistemic relation to their target, they cannot provide any
genuinely new empirical information about it (see Giere 2009). The function of models is to
improve the scope and reliability of the inferences from empirical assumptions concerning the
target system. More specifically, they do this by, quite literally, keeping score of the doxastic
commitments and inferential entitlements in our theorization about the world. According to
this view, the information learned from the use of models must already be implicit in the
modeling assumptions (commitments about the world outside the model):
{SKE} (Scorekeeping view of the epistemology of modelling): Models are external
doxastic scorekeeping devices. What turns an external artefact, such as a series of
equations on a paper or a material scale facsimile, into a model, is that some of its
features are treated as “assumptions” about something distinct from and beyond the
artefact, the target. By declaring such assumptions, the modeler undertakes doxastic
commitments – commitments to treat the consequences of these assumptions as also
being applicable, at least prima facie, to the target, and the artefact is constructed in
such a way as to facilitate the drawing of further implications of these assumptions.
The function of models is to keep our reasoning straight. Models embody and make explicit
doxastic commitments that were previously unavailable to the unaided reasoner and to the
epistemic community. By forcing the reasoner to explicitly formalize her assumptions and by
facilitating the evaluation of the reasoning, either in terms of correctness of derivation or in
terms of sensitivity analysis, models help others to keep score on the commitments
undertaken by the modeler. We suggest that ultimately all modeling can be thought of as
“extended argumentation.” This approach dovetails nicely with many scientists’ view of
modeling. For example, a textbook on ecological modeling states that ”Models do not
investigate nature. Instead, they investigate the validity of our own thinking, i.e. whether the
logic behind an argument is correct.” (Kokko 2007, p.7) In a similar tone, economist Paul
Krugman (1998, p.1834) describes the main function of modelling as “keeping things straight”
10

and “helping to focus and form intuitions” in contexts involving the adding-up of constraints,
indirect chains of causation and feedback effect.
In this sense, models can be compared to thought experiments. Their epistemic output is
conditional in nature: both thought experiments and models help us to infer what would
follow if certain premises (modeling assumptions) were true. However, unlike thought
experiments, modeling forces the theorist to make her assumptions public and explicit, and
models are often most useful in making sense of situations where our intuitions are not clear,
e.g., where there’s feedback, complex chains of causal influence, or several factors pulling into
different directions are in play (see Page 2018).
Of course, in all non-trivial cases of model building and use there is psychological novelty and
surprise. When the user of an orrery uses the model to predict the relative positions of planets
at a future time, these predictions are deductive implications of the implicit commitments
concerning the initial conditions and laws, materially realized in the mechanics of the orrery,
and undertaken by the modeler when using the artefact as a representation. Although the
user can learn new things about the target, because some of the implications of the
assumptions used in model construction were initially beyond the reach of her limited
unaided inferential abilities, what could be called the extended cognitive system consisting of
model together with the model user, learns nothing new (cf. Beisbart 2012, fn. 13). Hence,
just as there is no philosophical puzzle of the representation relation, there is no
epistemological puzzle of how new empirical knowledge can be gained without direct
empirical access to the target.
In sum, models make no epistemic miracles. They can only help to make explicit what was
implicit in the modelling assumptions. This applies to analytical models, computer
simulations, material simulations and scale-models alike. In contrast, a referentialist
explanation of epistemic value in terms of a substantial representation relation between the
model and the target is liable to lead to the mistaken idea that information can somehow be
conveyed via this special relation from the target to the model (as if by sucking information
through a referential straw).
5 Computer simulation as extended argumentation
The scorekeeping view downplays the analogy between modelling and experimentation and
is most intuitive in the case of analytically solvable theoretical models. Modeling by
manipulating a set of equations with pen and paper does not give rise to feelings of empirical
exploration and experimentation. Simulation seems like another matter altogether. By
simulation we mean the use of the partially autonomous behaviour of a surrogate artefact to
mimic some process in the target system (Beisbart 2012; [anonymized]).7 The surrogate
artefact can be a physical replica (an analogue simulation), such as a model wing in a wind

7

We do not want to enter definitional debates here. We stress the processual nature of simulations so
as to rule out purely computational, ‘non-mimetic’ procedures, such as Monte Carlo methods (see also
Hartmann 1996).

11

tunnel, or a program running on a computer. By partially autonomous we mean that once set
up and set running, a simulation run proceeds more or less independently of the intentions
and actions of the user. In part, it is this independent and processual behavior of the
simulation that makes simulation modeling seem like experimentation.
Here we focus on computer simulations, which also come in several different varieties. It is
relatively easy to see how simple toy models like the boid model of flocking behavior by
Reynolds (1987), or simple computational realizations of Schelling’s (1971) segregation model
can be understood to be argumentatively similar to thought experiments. Several
assumptions embodied in these models (e.g., about initial positions and movement rules of
the individuals) are not directly derived from empirical evidence, but are instead made by the
modeler him/herself, and such models are often used to argue for the (im)possibility certain
outcomes (Grüne-Yanoff 2009), or to suggest explanatory hypotheses about underlying
mechanisms.
However, across the sciences, simulation methods are also used in more data-driven ways.
For example, in the social sciences, agent-based models calibrated with large empirical
datasets have been used to study why U.S. citizens decide (not to) vote (Rolfe 2012), why
crowd disasters sometimes occur at mass events (Moussaid et al. 2011), and how residential
and workplace segregation emerges in real cities (Hedström & Åberg 2005). In addition to
explaining phenomena, such models are often used also for the purposes of prediction and
policy design (see Bruch & Atwell 2013). Especially the practice of empirically calibrated agentbased modeling (ECA) resembles empirical experimentation much more than it does analytical
modeling (Barberousse et al. 2009; Parker 2009; Winsberg 2009): an artificial replica of the
target system is created, it is left to “run”, and inferences are then made from the observed
results back to the target system. Unlike arguments, the simulation does not output
propositional “conclusions”, but masses of raw data from which the user must first infer back,
by means of further statistical modeling steps, to the properties of the epistemic artefact, and
only then to the properties of the simulated system. Due to the rich empirical calibration, such
models are also ostensibly ‘about’ concrete targets, rather than being computational
implementations of an abstract underlying model (cf. Bueno 2014).
Claus Beisbart and John Norton have argued that, despite appearances to the contrary, the
epistemic role of computer simulations can be fully understood in terms of argument-making:
A single run of a simulation is an execution of a series of commands and could thus be thought
of as a long deductive argument (Beisbart 2012; Beisbart & Norton 2012). 8 Although we agree
with Beisbart and Norton’s general take on the epistemological role of simulations, this is not
our inferentialist notion of an extended argument. In our view, the modeler is not in any
meaningful way taking on doxastic commitments that follow from any token execution of a
command in a single simulation run. In fact, true deductive “glitches” (to be contrasted to
ordinary programming bugs) in such runs are so rare that the modeler would not be held
8

Newell and Simon (1972) noted that a computer program is formally equivalent to a set of difference
equations as it determines the new state of the machine as a function of its previous state and new
inputs.

12

epistemically responsible for such clear “malfunctions” of the inferential apparatus. Also, as
Beisbart notes, due to roundoff errors, the argument of the modeler is not always the same
as that of the algorithm. What the modeler is inferentially committed to is that the
implemented algorithm itself does not contain programming errors or add anything
substantial to the results and that the results therefore follow only from the stated
assumptions. A single run is a concrete causal process and, as such, cannot really be correct
or incorrect, and a simulation is not an argument in itself, but a tool for constructing an
argument.
In our view, decisions regarding the input parameters and the architecture of a simulation
model ought to be conceived as premises in an argument, and salient features of simulation
outputs (or interpretations thereof) as its conclusions, the reliability of which is typically
assessed by means of sensitivity analysis, i.e. by tracking the changes in outputs resulting from
manipulations of the values of the input parameters and the model structure. For example, in
empirically calibrated agent-based models, the modeler has to make assumptions about
factors such as
[P1] the decision-rules employed by the model agents,
[P2] their form of organization (such as network structure) and interactions,
[P3] initial states and distribution of agent properties and behavioral rules, and
[P4] the structure of the environment. (see Railsback & Grimm 2019, p.99)
Such assumptions are based on earlier measurements and research, background theory or
simply educated guesses. What running the simulation makes possible is the computer-aided
tracking - quite literal scorekeeping - of the implications of such assumptions to relevant
macro-properties of the model population, including
[C1] its dynamics (equilibria, cycles),
[C2] spatial configurations (e.g., segregation patterns),
[C3] thresholds (e.g., tipping points), and
[C4] diversity effects.
The modeling assumptions regarding P1-P4 are the premises, and the statements about
macro-properties [C1-C4] the conclusions of interest of the computationally extended
argument. According to IMR, what makes the model a representation is that these premises
and conclusions are evaluated with respect to a target system. In agent-based modeling, the
premises, taken in isolation, are typically quite simple. The assumed behavioral rules of the
agents and their principles of mutual organization are often straightforward and by making
such assumptions, the modeler is undertaking conditional commitments that follow from
these rules. But due to the local interactions between agents, diversity in their properties, and
the sheer size of the simulated population, even after running the simulation, it is often
unclear to the model user, how and why a specific set of parameter values results in a
particular macro level outcome.

13

This inferential opacity of the simulation, i.e., that the sequence of simulation steps is not
epistemically accessible to the user (Humphreys 2004), often leads to talk about the
emergence of macro properties. We regard such talk as unfortunate and not conducive for
understanding the semantics and epistemology of modeling. The word ‘emergence’ simply
labels the point in which our unaided understanding of the dependence between the
assumptions and the results ends. There are important questions in the vicinity, however: (1)
a semantic question: What determines which parts of the model are really representational?
and (2) an epistemic question: How to distinguish genuine results from modeling artifacts?
These questions are particularly pressing for simulation models as the inferential connections
from premises to conclusions are too complex to be conclusively verified by the unaided
model user. This being the case, the relevant inferential system is the model-modeler pair, as
suggested above.
From the inferentialist perspective, the semantic and the epistemic problem are closely
linked. IMR suggests that a natural starting point for addressing questions about the semantics
of the model, i.e. what target systems the model represents and which parts of the model are
genuinely representative (in contrast to auxiliary assumptions, programming boilerplate etc.)
is to start with the explicitly stated goals of the model users. The statement of modeling goals
establishes the doxastic commitments regarding the kinds of things about which the modeler
is expected to make inferences, as well provide reasons for these inferences. Hence, the public
doxastic commitments determine how the model aims to represent its target. However, as
we emphasized above, the question about what the model successfully represents is not up
to the modelers intentions. Instead, that depends on the inferential capabilities that the
simulation makes possible.
6 Experiments, analogue models and computer simulations
With the inferentialist picture of simulations now laid out, let us return to the epistemic
differences between genuine experiments and simulations. Among others, Uskali Mäki (2005)
and Mary Morgan (2005) have emphasized the analogy between models and experiments in
that both involve manipulation of a surrogate object and isolation of a particular (causal)
factor of interest. Models are surrogate systems and manipulating them seems to be an
indirect way to learn about the target. Furthermore, the practice of modeling can often be
described in terms of uncovering a dependency between an independent and a dependent
variable of interest. Nevertheless, we have claimed that all modeling, even complex computer
simulation, must be understood as extended argumentation explicating the consequences of
independently justified empirical premises. We will now apply IMR and SKE in articulating a
set of crucial epistemic differences between, on the one hand, computer and analogue
simulations, and material experiments, on the other. We do this by examining a case in which
equation based models, simulations, as well as material experiments, have been applied to
investigate the same event: research on the 2010 Duisburg Love Parade tragedy. We do not
use this particular case out of morbid or sensationalist curiosity, but because the phenomenon
and its models are theoretically and conceptually easy to understand, thus making the crucial
epistemic differences as salient as possible.
14

Modeling crowd behavior
Pedestrian crowds have been studied by scientists for more than half a century now (Hanking
and Wright 1958). Henderson (1974) made early theoretical attempts to understand the
behavior of crowds by employing differential equations used in fluid dynamics. The notions of
flow, pressure, and phase transitions find natural application in the domain of large-scale
human crowds (Ball 2006). For example, as crowd density exceeds a critical threshold
(approximately 7 people / m2), people have little opportunity to control their movements,
and, consequently, shocks and waves get propagated in the crowd in a manner resembling
that of a liquid substance.
In addition to such macro models, various kinds of particle, field, and agent-based models
have recently been introduced. They aim to augment the understanding of crowd dynamics
by describing the crowd at the level of individuals, not of aggregate flows. Such models predict
various macro-level phenomena observed in crowds such as lane formation, stop-and-go
waves and herding (see Duives et al. 2012). For the current topic, it is notable that both the
fluid dynamics model and individual-based models are typically solved by using simulation
methods.
Recently, models of pedestrian behavior have been used to explain crowd disasters such as
the Love Parade tragedy in 2010. Love Parade was an electronic music festival organized in
various German cities since 1989. In the years prior to the 2010 Duisburg festival, the event
was yearly attended by more than 1 million visitors. In Duisburg, the event was organized at
a former train depot. The area is isolated from its surroundings by railroad tracks on side, and
a motorway on the other, and access to and from the festival grounds was through a inverseT shape structure consisting of two long pedestrian tunnels leading to a ramp to the festival
area. Only a few hours after the festival opened, severe congestion developed around the
ramp as new visitors entered and others were already trying to leave the area due to
congestion. In order to control the situation, the police set cordons in both tunnels to keep
people from accessing the festival grounds. After a short while, however, the cordons were
dissolved. This lead to the development of a two-way flow of people and high crowd densities
in the inverse-T structure. During a period of 30-60 minutes, 21 people were killed and several
hundred injured due mainly to compression asphyxia (Helbing & Mukerji 2012; Pretorius et
al. 2015).
In the media, escape panic is often stated as the cause of such crowd disasters (Helbing et al.
2000). The mass panic hypothesis suggests that in dense crowds, people start to move faster
and interactions become physical in nature. This causes some individuals to fall over and
possibly get trampled by the crowd. Furthermore, the behavior spreads in the crowd as people
pick up the agitated behavior. Helbing and Mukerji (2012) suggest, however, that it is not clear
what role mass panic played in the Duisburg disaster. Instead, based on an extensive analysis
of evidence from the festival grounds (e.g. CCTV and participant video), they suggest that the
casualties were caused by a phenomenon called crowd turbulence or crowd quake: at
dangerously high densities, the crowd becomes almost like a fluid mass, in which shock waves
can be propagated over long distances, practically compressing the lungs of the individuals so
15

that breathing may become difficult and injured individuals cannot be picked up or
transported to safety. However, it also appears that behavioral effects (panic) may explain
why the area close to an emergency staircase was the most hazardous one: people trying to
make their way to the staircase may have led to the emergence of local pressure spikes in the
ramp area.
Pretorius and colleagues (2015) employed an agent-based model to predict the conditions
under which crowd quakes and compressive asphyxia could occur. They simulated six
alternative scenarios using an ABM calibrated with detailed evidence about visitor arrival and
departure rates as well as geographical modeling of the festival grounds. The first simulation
of the actual events appeared to satisfactorily reproduce events according to the original
timeline. In the five remaining scenarios, the authors manipulated the occurrence of the
police cordons, effects of signposting, flow separation and alternative exit and entry
strategies. Their results suggest, for example, that were the temporary police cordons not set
in place, crowd densities would not have reached dangerously high levels and the catastrophe
could have been avoided.
Although the empirically calibrated modeling conducted by Pretorius et al. (2015) does share
many of the pragmatic elements of running material experiments, our SKE account of the
epistemic function of simulations suggests that even such highly mimetic models are
argumentative tools tracking the inferential entitlements of the doxastic commitments
encoded in the modeling assumptions. It is just that in the empirically calibrated models, the
assumptions include richer information about an actual scenario (e.g. the Duisburg event),
whereas theoretical toy models embody mainly general assumptions about the kinds of
processes involved in crowd dynamics. Despite appearances to the contrary, the processual
and mimetic characteristic of realistic, empirically calibrated simulations do not epistemically
set them apart from more abstract kinds of modeling involving no such virtual communities
of agents. The epistemic specialty of agent-based models lies elsewhere: unlike analytical
modeling, computational modeling allows the combination of a large set of premises - most
distinctively assumptions concerning various types of heterogeneity of agent properties.
From the point of view of the modeler, there is a clear aspect of genuine experimental praxis
and surprise in working with an ECA. When the modeler manipulates the external epistemic
artefact, she, as a boundedly rational agent, can learn genuinely new things about its
inferential properties. However, the genuinely novel material inferences involved in such
experimentation on the model do not concern the target of the model, but rather the
properties of the modelling apparatus itself. So what gets increased here is her understanding
of the model, and the modeler’s abilities to successfully manipulate the external inferential
apparatus in a goal-directed manner. This provides a legitimate indirect epistemic role for
“conceptual exploration” in providing better understanding of our inferential tools.
Robustness/sensitivity analysis is also an important part of learning about models because it
is a way of evaluating and alleviating the unreliability of the inferential apparatus brought
about by its inferential opaqueness.
16

From the point of view of the extended cognitive system comprised of the model user and the
model, however, manipulating a model9 is simply an argumentative move, a move in the game
of giving and asking for reasons, giving rise to a new inference from the altered assumptions
to a new conclusion. Different kinds of inferential roles for agent-based models (elaboration
of a possible mechanism, integration of data, prediction, policy-advice) require different kinds
of justificatory arguments and different manipulations of the model. But unlike in genuine
experiments, these justificatory arguments have to, in the end, rely on separate empirical
premises coming from outside of the model (inferential “model entry” moves).
For example, as Moussaid et al. (2016) point out, the fluid dynamics and particle based models
cannot provide understanding about the influence of behavioral effects (e.g. panic) on crowd
dynamics, as they make no commitments about the decision processes of the individuals. To
address this challenge, Moussaid and coauthors collected evidence of human behavior in such
situations by running group experiments using virtual reality headsets. Unlike modeling, such
experiments can act as sources of discovery about human behavioral effects in dense crowds.
Adding modeling assumptions regarding such behavioral effects into a simulation can provide
suggestions of the systemic effects of such patterns. Although such effects could not have
been foreseen by an unaided human reasoner, the effects were already implicit in the
empirically established modeling assumptions.
But why did the experiment carried out in virtual reality provide genuinely new information
whereas the agent-based models did not? And are we simply stipulating what is to be argued
for, or have we stretched the concept of argumentation so far as to also cover
experimentation as some kind of material argumentation? Although experiments and
simulations seem to incorporate a similar epistemic dynamic comprising of artefact
construction, manipulation and external inference, philosophical analyses of their epistemic
differences have been surprisingly varied. The common intuition is that true material
experiments are obviously epistemically more powerful than simulations. There is, however,
disagreement about where this advantage comes from.
The most obvious difference would seem to be that true experimentation involves direct
causal manipulation of the very stuff investigated, and many philosophers have articulated
this difference in terms of the material basis of the direct, experiment-based inferences.
However, Francesco Guala (2002) acknowledged that the simple intuition of direct
manipulation of the physical target is not enough to distinguish between the epistemic roles
of experiments and models, because both crucially involve an external validity inference from
a source to a separate target. Nevertheless, Guala argues that the difference lies in the “deep”
material basis of inferences based on true experiments (see also Morgan 2003). Wendy Parker
(2009) has also argued that materiality as such cannot be the thing that makes the difference,
because computer simulation studies are also fundamentally material experiments on the
computer (see also Barberousse at al. 2009). Margaret Morrison (2009) goes further in
denying that there is any fundamental difference, because both experimentation and
9

For example, running the otherwise same ABM (e.g., Schelling model) with the local social interactions
between agents constrained first to von Neumann neighborhood and then to Moore neighborhood.

17

simulation are model-mediated to similar degree and that at least some simulation results can
therefore be seen as indirect experimental measurements of the properties of the target
system.
We lack the space to argue against these views in detail10 and simply build on Eric Winsberg’s
work (2009), which pinpoints the difference to the fact that the extrapolative arguments from
simulation and experiment to their targets are justified differently: arguments from
simulations rely on ”principles of model building” whereas arguments from experiments rely
on material similarity. Although in broad agreement with Winsberg, we now argue that the
inferentialist perspective provides a more nuanced articulation of this epistemic difference.
As a starting point, let us consider what, if anything, distinguishes material simulations
(analogue models) like wind tunnels or physical fluid models of crowd dynamics from
computer simulations? Both kinds of model seem to share the experiment-like epistemic
dynamic, and although they have a different ”material” relation to their targets, they can both
be used to serve the same epistemic functions. Consider an anecdote about John von
Neumann told by Winsberg (2010). Von Neumann and his research team had been using a
physical wind tunnel to study highly intractable problems in fluid dynamics, but the use of the
wind tunnel involved various material inconveniences. When the laboratory finally acquired a
computer powerful enough to calculate the wanted results based on physical laws, von
Neumann expressed relief that they no longer needed the cumbersome physical wind-tunnel
to perform those very same calculations. Von Neumann had used the analogue model purely
as an inferential aid, a material calculator, without invoking any “same matter” inferences.
Although “material”, the wind tunnel was therefore not a true experiment. But what made
the epistemic difference? It cannot be the intentions of the modeler, because it is not clear
why private mental states should make such an epistemic difference. Mere intending does
not establish representation.
From the inferentialist perspective, the epistemic difference arises from the prior (public)
doxastic commitments undertaken by the modeler(s) and the consequent differences in the
justification for distinguishing genuine results from mere artefacts. When an external
inferential aid is treated as a simulation, these judgments follow directly from the
commitments defining the representational properties of the inferential apparatus, i.e., what
parts of the model are “fair-game” for empirical criticism. What it is for an external system to
be used as a representation of some target is that the inferences it facilitates are, in principle,
based on already articulated commitments about its target. In contrast, if the system is used
as a true experiment, then these judgments can also appeal to previously unknown properties
of the material basis of the system itself (open-ended “model entry” moves). Although also
inferences from experimental systems (laboratory set-ups or samples from a larger
populations) to their “targets” are also partly based on commitments about the target, the
10

We agree with Parker that, from the perspective of the modeler, computer simulation can be
described as involving experimentation on the computer, as the modeler learns new things about the
epistemic artefact used. Nevertheless, from the epistemically important perspective of the modelmodeler combination, simulation modeling is computationally extended argumentation w.r.t. to the
modelled target system.

18

experimental system is a representative, not a representation, of its target. For example, Von
Neumann’s wind tunnel was a model (a representation) of abstract fluid dynamics, in virtue
of his team being committed to only those results that really follow from the principles of fluid
dynamics. This makes the analogue model a representation of these abstract principles, not
of any concrete physical system. If the materiality of the wind tunnel had caused systematic
deviations from the results mathematically implied by the intractable fluid dynamics
equations, the results would have appropriately been judged to be artefacts. Thus physically
identical modelling situations (model-modeler-target) can be representationally distinct when
embedded in different games of giving and asking for reasons.
This underscores our point that representation is a social, even “normative”, accomplishment,
not a two-place relation in the world. The epistemic difference between true experiments and
virtual “experiments” (i.e. models) is neither arbitrary, up to the intentions of any particular
individual, nor a matter of an intrinsic material similarity or dissimilarity of the model and the
target. The material properties of the virtual experiments are in no direct way responsible for
their epistemic role. The epistemic difference is, nevertheless, categorical, and arises from the
doxastic commitments undertaken when treating an external inferential apparatus as a
representation of some target. These doxastic commitments determine what the model-user
can appeal to when drawing conclusions with the aid of the model, as well as the scope of
reasons that can be appealed to when criticizing these conclusions (i.e., whether they are
warranted or mere artefacts of the inferential apparatus).
7 “In the end, there has to be similarity …”
“Similarity, ever ready to solve philosophical problems and overcome obstacles, is a
pretender, an impostor, a quack.” (Goodman 1972)
“Invoking a relation (for instance some sort of isomorphism) between representing
and represented does not by itself contribute to the task of explaining what the
intelligibility of the representing consists in” (Brandom 1994, 74)
Having laid out our inferentialist account of models and simulations, we now return to the
powerful intuition underlying its alternative the referentialist order of explanation. According
to referentialism, there must be an explanation for the inferential success of a model in terms
of some kind of similarity or isomorphism relation between the model and the target. The
inferentialist does not, of course, deny that successfully representing models are, in some
respects, similar to their targets. The problem is with the presupposition that there has to be
a substantial fact of the matter about such a similarity that is be independent of our inferential
practices in a way that allows it to explain these inferential practices. We do not claim that
inferential practices should be taken as primitive brute facts without any explanation, but that
such explanations in terms of similarity or isomorphism are vacuous pseudo-explanations.
As critics of referentialist positions have repeatedly argued, there is no such thing as similarity
per se. Nelson Goodman famously quipped that everything is similar to everything in some
respect, and that the very notion therefore becomes vacuous. Hence, representation must
19

involve relevant similarities. As even the defenders of structural-referentialist accounts
currently appear to admit, such relevance cannot be determined without reference to our
practices (Bueno & Colyvan 2011; Weisberg 2013). We suggest that the meaning of a similarity
claim is explicable by the inferential commitments undertaken by the speaker: “A is similar to
B” obliges the utterer to assent or give reasons for claims of the form “If P(A), then P(B)”. If
most of these claims (where the properties P belong to some set taken as relevant in the
context) turn out to be false, then the similarity claim is false. Similarity cannot be used as a
general explanation, because claims of similarity just state that these inferences can be made
- which was just what was in need of an explanation to begin with.
However, this does not mean that judgements of similarity are arbitrary or subjective. We
cannot make analogical inferences successful (truth conducive) simply by wishing so.
Moreover, the success in each case is explained by the objective material properties of the
things compared, and by the (perfectly objective) cognitive make-up of the user of the
representation. What the inferentialist inversion does imply, however, is that there is no such
thing as naked similarity “out there.”
We suspect that not even the more sophisticated versions of the similarity account, such as
that of Michael Weisberg (2013), can escape this basic insight. Weisberg develops a measure
of similarity in terms of the ratio of shared and unshared features of the compared objects.
But this only shifts the problem a step further to the delineation of said features: one cannot
count features before they are defined, and the question of defining features is always a
question of relevance. And as Weisberg himself notes, such questions are always relative to
the community practices of the relevant field. The more consensus there is about what the
relevant features are, the less room there is for reasonable disagreement about similarity.
Accounts based on structure-preserving mappings seem to ameliorate the problem of
defining relevant similarity. Consider what is perhaps the most straightforward case,
isomorphism. Structures are isomorphic purely and objectively in virtue of the relations
between their parts. But, again, picking out the structures that concrete objects instantiate is
a matter of discerning the relevant from the irrelevant. As proponents of the structural
approach (Bueno and Colyvan 2011; Bueno 2014) themselves suggest, this already constitutes
an inferential step from the phenomenon to a model (“immersion”). In cases where targets
(and sources) are rich in potentially representable structures, understanding how and what a
model represents requires shifting attention from the structures themselves to the inferential
commitments determining which of the potentially identifiable structures are the relevant
ones.
It does not follow from inferentialism that (structural) similarities between model and target
could not have a role to play in any particular story about scientific modeling. In fact, we do
not find much to disagree with the way that the modeling-related reasoning practices are
described by Bueno (2014). The disagreement concerns the epistemic role of an account of
the representation relation – whether a conceptual analysis of the relation, formulated in
terms of structural similarities and denotation, plays a role in explaining epistemic success.
Unlike Bueno, we do not regard claims about representation as explanatory. Even in Bueno’s
20

own account, in which the epistemic adequacy of a model is described in terms of the model
capturing the correct counterfactual patterns of dependency between data inputs and model
outputs, it is not obvious that a theory of the representational relation as a partial morphism
is an informative part of the explanation of such adequacy. Instead, for the inferentialist, being
able to perceive the model and target as instantiating the same abstract structure is a
consequence of getting the modeling commitments right. Representational notions are
explicative and expressive, not explanatory (Brandom 1994, 330-333).
8 Conclusions
We have shown how inferentialism about the semantics of simulation models, combined with
the scorekeeping view of their epistemology, result in a satisfying analysis of claims about how
and what such virtual experiments represent, and how such claims are related to the
epistemic role of models and simulations in scientific practice. Inferentialism suggests that
claims about a model (or its components) successfully representing a target are ultimately
explicative, not explanatory nor justificatory. Claims about representation summarize our
current knowledge of the sphere of reliable model-based inferences concerning the actual
and possible targets - they do not refer to epistemically explanatory relationships in need of
a philosophical theory. Consequently, the inferentialist perspective raises concerns about the
fruitfulness of many aspects of the current philosophical research programme on scientific
representation.
The score-keeping view suggests that simulations should be understood as algorithmic
elements in arguments produced by the coupled model-modeler system, and as such they do
not provide genuinely new empirical knowledge about their targets. Although the praxis of
simulation has many elements in common with that of material experimentation, our analysis
suggests that the processual and mimetic characteristics of simulations do not have any
epistemic added value beyond what can be legitimately understood as argumentative moves
from model assumptions to conclusions.
The inferentialist framework could also be used to explicate possible differences between
simulations in natural science, especially physics, and social-science simulations. In physics,
for example, the greater confidence in background theory, i.e. the theoretical premises, and
the accuracy of measured input values can render simulations quite alike model-mediated
indirect measurements, as Morrison (2009) suggested. If such confidence greatly exceeds the
maximum reliability of any material experiment on the system itself, then the results from
virtual experimentation would epistemically trump material experimentation - at least within
the scope of the deductive consequences of the underlying theory.
References
Barberousse, A., Franceschelli, S., & Imbert, C. 2009. Computer simulations as experiments.
Synthese, 169, 557-574.
Beisbart, C. 2012. How can computer simulations produce new knowledge? European
Journal for Philosophy of Science 2: 395-434.
21

Beisbart, C. 2018. Are Computer Simulations Experiments? And If Not, How Are They Related
to Each Other? European Journal for Philosophy of Science 8 (2): 171-204.
Beisbart, C. & Norton, J. 2012. Why Monte Carlo Simulations Are Inferences and Not
Experiments. International Studies in the Philosophy of Science 26 (4): 403-422.
Boge, F. forthcoming. Why computer simulations are not inferences, and in what sense they
are
experiments.
European
Journal
for
Philosophy
of
Science.
https://doi.org/10.1007/s13194-018-0239-z
Brandom, R. 1994. Making it Explicit: Reasoning, representing, and discursive commitment.
Cambridge, MA: Harvard University Press.
Bruch, E. & Atwell, J. 2013. Agent-Based Models in Empirical Social Research, Sociological
Methods & Research. Online version.
Bueno, O. 2014. Computer simulations: an inferential conception, The Monist 97, 378-398.
Bueno, O. & French, S. (2011). How theories represent. British Journal for the Philosophy of
Science, 62 (4):857-894
Bueno, O., & Colyvan, M. 2011. An inferential conception of the application of mathematics,
Noûs 45, 345 – 374.
Bygren, M. 2013. Unpacking the causes of segregation across workplaces, Acta Sociologica
2013, 56, 3-19.
Clark, A. 2008. Supersizing the Mind. Oxford University Press.
de Donato, X., & Zamora-Bonilla, J. 2009. Credibility, idealisation, and model building: An
inferential approach. Erkenntnis 70: 101–118
Fodor, J. 1998. Concepts: Where cognitive science went wrong. Oxford: Clarendon Press.
Frigg, R. & Nguyen, J. 2016. The Fiction View of Models Reloaded. The Monist 99: 225-242.
Giere, R. 2002. Scientific cognition as distributed cognition. In P. Carruthers, & S. Stich & M.
Siegal (Eds.), The Cognitive Basis of Science (pp. 285–299). Cambridge: Cambridge University
Press.
Giere, R. 2009. Is computer simulation changing the face of experimentation? Philosophical
Studies 143: 59–62.
Grüne-Yanoff, T. 2009. Learning from minimal economic models. Erkenntnis 70, 81-99.
Guala, F. 2002. Models, simulations, and experiments. In L. Magnani & N. Nersessian. (Eds.),
Model-based reasoning: Science, technology, values (pp. 59–74). New York: Kluwer.

22

Hartmann, S. 1996. The World as a Process: Simulations in the Natural and Social Sciences, in
Hegselmann, R., Müller, U., & Troitzsch, K. (eds.). Modelling and Simulation in the Social
Sciences from the Philosophy of Science Point of View. Dordrecht: Kluwer, 77–100.
Hedström, P. & Åberg, Y. 2005. Quantitative research, agent-based modelling and theories of
the social, In Hedström 2005: Dissecting the Social. Cambridge University Press.
Henderson LF. The statistics of crowd fluids. Nature 1971;229:381–3.
Hughes, R. I. 1997. Models and representation. Philosophy of science, 64, S325-S336.
Kokko, H. 2007. Modelling for Field Biologists and Other Interesting People. Cambridge
University Press.
Krugman, P. 1998. Two Cheers for Formalism. The Economic Journal 108, 1829-1836.
Morgan, M. 2005. Experiments Versus Models: New Phenomena, Inference and Surprise.
Journal of Economic Methodology 12 (2), 317-329.
Morrison, M. 2009. Models, Measurement and Computer Simulation: The Changing Face of
Experimentation. Philosophical Studies 143 (1), 33 – 57.
Moussaid, M., Helbing, D., Theraulas, G. 2011. How simple rules determine pedestrian
behavior and crowd disasters. PNAS, 108, 6884–6888
Mäki, U. 2005. Models Are Experiments, Experiments Are Models. Journal of Economic
Methodology 12 (2), 303-315.
Page, Scott E. 2018. The Model Thinker: What You Need to Know to Make Data Work for
You. Basic Books.
Parker, W. 2009. Does matter really matter? Computer simulations, experiments, and
materiality. Synthese, 169, 483-496.
Railsback, S. F., & Grimm, V. 2019. Agent-based and individual-based modeling: a practical
introduction. Princeton university press.
Reynolds, 1987. Flocks, herds, and schools: A distributed behavioral model. Computer
Graphics 21:4, 25-34.
Rolfe, Meredith. 2012. Voter Turnout. Cambridge University Press.
Rupert, R. 2004. Challenges to the hypothesis of extended cognition. Journal of Philosophy,
389–428.
Schelling, T. 1971. Dynamic models of segregation. Journal of Mathematical Sociology, 1: 143–
186

23

Sellars, W. 1956. Empiricism and the Philosophy of Mind, in Minnesota Studies in the
Philosophy of Science, vol. I, H. Feigl & M. Scriven (eds.), Minneapolis, MN: University of
Minnesota Press, 1956: 253–329.
Stich, S. 1996. Deconstructing the Mind. New York: Oxford University Press.
Suarez, M. 2003. Scientific representation: against similarity and isomorphism, International
Studies in the Philosophy of Science, 17.
Suarez, M. 2004. An Inferential Conception of Scientific Representation, Philosophy of
Science, 71, 767–779.
Suarez, M. 2010. Scientific representation. Philosophy Compass 5, 91–101.
Vorms, M. 2012. Models and formats of representation in Humphreys & Imbert (eds.):
Models, Simulations, Representations. Routledge.
Weisberg, M. 2013. Simulation and Similarity. Oxford University Press
Winsberg, E. 2009. A tale of two methods. Synthese, 169, 575-592.
Woodward, J. 2003. Making Things Happen: A theory of causal explanation. New York: Oxford
University Press.

24

