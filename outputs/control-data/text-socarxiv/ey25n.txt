Making Big Data Small: Strategies to Expand Urban and
Geographical Research Using Social Media
Ate Poorthuis, Singapore University of Technology and Design
Matthew Zook, University of Kentucky
Published as: Poorthuis, A & M. Zook. (2017). Making big data small: strategies to expand
urban and geographical research using social media. Journal of Urban Technology 24 (4),
pp. 115-135. https://doi.org/10.1080/10630732.2017.1335153.
Abstract: While exciting, big data (particularly geotagged social media data) has proven
difficult for many urbanists and social science researchers to use. As a partial solution, we
propose a strategy that enables the fast extracting of only relevant data from large sets of
geosocial data. While contrary to many big data approaches – in which analysis is done on
the entire dataset – much productive social science work, can use smaller datasets – around
the same size as census or survey data – within standard methodological frameworks. The
approach we outline in this paper – including the example of a fully operating system –
offers a solution for urban researchers interested in these types of data but reluctant to
personally build data science skills.

1

We are facing a “data revolution” (Kitchin, 2015) that is changing the way that
researchers study the world, society and cities. While data was once a relatively rare
commodity – something to be laboriously collected – technological changes in sensors,
mobile devices and data storage combined with evolving social practices have resulted in
torrents of data that is big in terms of volume, variety and velocity (Laney, 2001). This
represents both a new opportunity (see Poorthuis et al, 2016) and a real challenge for
social science research, as we work to understand how big data can help our understanding
of the cities around us. In this paper, we present a strategy and platform that enables social
scientists interested in working with big data to extract relevant, relatively small data from
much larger data sets. As we show, this approach is suitable for many research questions in
social science and types of big data as it allows urban researchers to do actual social
science without the need to first become full-fledged data or computer scientists.
The challenge of big social media data
The category of big data is expansive – including sensor data, transactional records,
captured images and videos, text, etc. – and has become a critical term within both natural
and social science. Of particular interest to the social sciences are big data that come from
social media as they represent the day-to-day actions and utterances of ordinary people.
This incidental quality of social media is one of the fundamental aspects of big data in that
it is almost always secondary data; social media big data already exists out in the world and
is not primarily produced or collected for social science research. This means that, instead
of having full control over the collection of research data, or being confident that the data
meets basic statistical standards, big data often is a dataset of happenstance, rich with
potential but messily intertwined with dross. Although data and computer scientists are
well versed in the techniques and methods of working with big data, an exclusive focus on
data science can problematically devalue the hard-won theoretical and methodological
skills of social scientists. This most clearly evidenced by Chris Anderson’s (2008) heady
declaration of the “end of theory”.
Although Anderson’s sentiment is often cited as an example of overreach, there are
plenty of examples of mischaracterizing of big data, particularly regarding its ubiquity and
availability. For example, Arribas-Bel (2014), states:
“...all of these sources are available to researchers without the need to
pay any fee or reach exclusive deals with the company/institution
providing them. […] new datasets relating to virtually any quantifiable
aspect of human life are appearing” (p.45)
This characterization of easy availability, however, is problematic as the evergreater variety of datasets is certainly not easily or freely available. On the contrary, it is
the traditional data sources (e.g. census, local government data) that are most often freely
and widely accessible and moreover can also be analyzed with standard or off-the-shelf
software and methods. Gaining access to big data often involves paying fees or reaching a
licensing agreement and sometimes is not even possible for ‘outsiders’. Furthermore,
companies who own social media platforms have that goals are not necessarily aligned
with academic research. Even if social media data can indeed be accessed, actually
collecting and storing such data requires significant resources and skills that can go beyond
2

what is traditionally taught within Geography, City Planning or other social sciences. We
contend this challenge of access represents an emerging divide between those who are Big
Data ‘rich’ and those who are Big Data ‘poor’ (boyd & Crawford, 2012) and thus enabling
better access for a wide array of urban related research projects is a key goal. While we
focus on the specific case of social media, our proposed solution of “making big data small”
could be applied to other types of big data as well.
Accessing big data
In contrast to traditional governmental data sources available via mandate or
freedom of information requests, access to social media data depends on corporate
policies. Some big data companies make data available to the public through an Application
Programming Interface (API) — a defined set of rules and protocols that state how specific
data can be accessed or changed – but this not always the case (e.g. Facebook does not
provide an API for public or research use). APIs are often constructed because they
enhance a platform’s visibility, potentially increasing its usage by consumers and
developers. Researchers can use the same API to collect data from these platforms but this
often require the development of custom software, a skill that most social scientists do not
possess. Furthermore, larger data sources (e.g. Twitter) are often only available as a
stream. That means data is ephemeral: it is best ‘caught’ at the moment in which it is
produced as it becomes increasingly hard or expensive to access as time passes. For
academic use, this ‘store it or lose it’ principle is tricky; researchers may not know in
advance what data exactly to store. Research topics and questions emerge more slowly
than the stream of social media, and if data is not captured, access to it becomes costprohibitive or simply impossible. Even if a researcher succeeds in accessing and storing
relevant research data, the sheer volume of it can quickly outstrip standard storage
capacities and analytic methods.
Within the current literature, there are two distinct approaches to access that can be
roughly summarized as more elaborate collection or ‘mining’ frameworks versus ‘ad hoc’
data collection. GIScience has engaged with elaborate data collection systems well before
the onset of big data as the systematic collection, storage and analysis of large amounts of
data has been of long standing concern (Devogele, Parent, & Spaccapietra, 1998; Han et al.,
1997). This earlier work has subsequently been extended to geosocial and big data in
systems or ‘cyber-infrastructure’ focused on the storage and analysis of spatial data in a
high-performance and distributed computing setting (Wang, 2010). As a result the number
of papers and conference proceedings by GIScientists that study Twitter and its many
(technical) facets dwarfs the amount of research that actually uses said data to study
‘conventional’ urban and geographic phenomena. In other words, the majority of work with
big data actually focuses on methods/techniques and is fairly technical in nature (e.g.
Croitoru, Crooks, Radzikowski, & Stefanidis, 2013; Kumar, Morstatter, & Liu, 2014; L.
Smith, Liang, James, & Lin, 2015), particularly when compared to more traditional research
using big data to study cities and society (Baginski, Sui, & Malecki, 2014; Korson, 2014;
Shelton, Zook, & Graham, 2012).
Although minor implementation details differ, these systems – particularly ones
focused on geosocial media collection – are largely designed along the same lines. For
example, TweetTracker (Kumar et al., 2014) and GeoSocial Gauge (Croitoru et al., 2013)
3

both use the NoSQL database MongoDB, while SensePlace2 uses a Lucene full-text engine
(MacEachren, Robinson, & Jaiswal, 2011). All are designed to collect a stream of social
media data (Twitter in these cases) that matches a pre-defined set of keys as opposed to
the entire stream of data. It is up to the researcher to decide — up front — what is and
what is not relevant to collect. TweetTracker and Senseplace2 were specifically designed as
tools for use in disaster response and receive a moderate amount of attention from
researchers beyond the initial authors (as judged through citations and adoption in
published work). Many other systems do not fare as well and seem to be primarily a proof
of concept or prototype that illustrates that such data can indeed be captured and analyzed.
Many of these prototypes outlined in the literature also suffer from issues where the actual
implementation is very short on details (Kumar, Barbier, Abbasi, & Liu, 2011), not very
feasible in practical research cases (Huang & Xu, 2014) or not very extensible to application
beyond the limited scope of a test case (Fujita, 2013). This inability to extend elaborate
data collection systems has limited their effectiveness for the research community more
generally. Even the Library of Congress, tasked with archiving every tweet ever produced,
has not yet succeeded in making the Twitter archive easily accessible and usable for
academics — despite consulting from the private sector (Leetaru, Wang, Cao,
Padmanabhan, & Shook, 2013).
As a result, many researchers instead turn to more ‘ad hoc’ data collection via the
open source API libraries and plugins to social media that exist for virtually every
programming language. Although some researchers use (commercial) third-party services
(Kounadi, Lampoltshammer, Groff, Sitko, & Leitner, 2015) or an existing collection system
(Crampton et al., 2013; Stefanidis et al., 2013), most simply leverage available and
relatively simple open source tools. For example, within Geography, TwitterHitter (J. White
& Roth, 2010) was developed by cartographers and allows the researcher to both collect
and visualize data in a cartographically-sound manner. Outside of Geography, the Excel
plugin NodeXL (M. A. Smith et al., 2009) is popular. The plugin allows any researcher to
collect social media data and store it in a simple database, flat text files or spreadsheet. This
is a feasible strategy because in most cases the resulting datasets are rather small. For
example, over 80 percent of Twitter studies use datasets that are smaller than 10 million
records (Weller, 2014; Zimmer & Proferes, 2014), which is not at all ‘bigger’ than more
conventional data sets in quantitative social science. These datasets easily fit within the
memory of even a modest laptop without the need for special big data tools or software.
Some datasets are even smaller, for example Yang’s (2015) study of depression in Twitter
users uses a randomly sampled 402 tweets — only around 50,000 characters of total data.
Although size-wise this seems very small compared to the vast pool of billions of records
available, just because data exists does not mean it needs to be included in an analysis.
Korson’s (2014) qualitative analysis of 407 tweets around UN peacekeeping missions
shows clearly that there is value in Twitter data regardless of the total number of records.
While the ad hoc approach makes big geosocial media data more accessible for
research it suffers from two key weaknesses. First, the ability to study a phenomenon
longitudinally requires a researcher to maintain a constant always-on data collection effort.
Many such efforts are deployed within a desktop environment, which is difficult to
maintain over the long term. Weekends, vacation periods and power cuts all confound this
kind of setup and depending on the collection parameters, gathering data can fairly quickly
4

fill a hard drive and overwhelm existing backup systems. The second weakness is the lack
of standardization or documentation about data collection methods. While some of this
work is very explicit about its collection strategy and implementation (L. Li, Goodchild, &
Xu, 2013; J. Lin & Cromley, 2015), others are rather opaque (Kay, Zhao, & Sui, 2014;
Widener & Li, 2014; C. Xu, Wong, & Yang, 2013). This uncertainty is problematic as the
specific collection strategy can have a large impact on the resulting data and research
outcomes and thus any opaqueness increases uncertainty about outcomes. As just one
example, Morstatter et al. (2013) conducted an in-depth comparison of data acquired from
the freely accessible 1 percent sample of Twitter data versus the very expensive ‘firehose’
that includes 100 percent of all tweets and find several sources of bias within the sampled
data1.
Using big data by making it small
In short, efforts to use big data in urban research questions can quickly get bogged
down in mechanical hurdles that are beyond the traditional skill set of researchers. This
need not be the case and we argue for the need to develop approaches for working with big
data that does not require every individual researcher to develop skills more akin to
computer science than social science. While there is no single best approach, we outline a
technological solution – including a fully operational system – that provides social
scientists with access to big social media datasets without the need to become
programming experts. This solution, based at the University of Kentucky and called Data
on Local Life and You (or DOLLY), leverages the robustness and scalability of a serverbased data mining system on the back-end, with the usability advantages of ad hoc desktop
applications and the convenience of small, relevant datasets that minimizes technical
barriers.
As an ongoing test case (and research tool) DOLLY has collected all geotagged
tweets in the world, going back to June 2012, and enables real-time search and extraction
from its corpus of 12+ billion tweets. Twitter is a useful test case as it is one of the most
prolific social media platforms and while this paper focuses exclusively on the specifics
associated with Twitter data collection, DOLLY is designed to ingest smaller geosocial
media datasets without a problem. The exact same architecture and resources can also be
used to add additional social media platforms (e.g. Flickr, Instagram, Foursquare) to the
system relatively easily.
Paradoxically, a key attribute of DOLLY is its ability to make big data small and
allowing researchers to quickly extract only the relevant data. While many existing big data
approaches perform analysis on complete datasets spanning terabytes, in practice, most
social science work based on Twitter data source uses relatively small datasets similar in
size to other more conventional data sources such as census data. While such datasets
might be relatively large, especially when compared to studies that only look a few census
tracks or counties, these research projects simply have little use for big data methodologies

Particularly relevant for urbanists and geographers is that, for the subset of geotagged tweets, the difference
between the sample and the firehose is negligible.
1

5

used within data science to review datasets in their entirety. DOLLY is designed to allow
researchers to narrow down the universe of tweets quickly to a relevant dataset that
measures in thousands or tens of thousands, not millions or billions. In the same way that
word-processors allow users to focus on writing text, DOLLY abstracts away the computer
science and allows a social scientist to do social science.
Outlining the DOLLY approach
Given these current collection methods, our approach with DOLLY collects and
stores all geotagged tweets as they are produced and are delivered in Twitter’s data stream
and does so in a way that is consistent, reliable and minimizes collection bias by making
sure that all data in the world is collected. This is the big part of the data collection.
However, and most importantly, after initial collection and storage, DOLLY is designed to
make it easy for any researcher to search through billions of data points and extract only
the data that is germane to the research question. This approach effectively gives urban
researchers quick access to the small and relevant datasets that are needed for the vast
majority of research question, without any of the barriers to access or reliability issues
surrounding ad-hoc data collection. While our intention is to share the code developed for
DOLLY the challenges of maintaining the system – particularly the means to extract small
datasets – have delayed finalizing a clean and properly annotated version that would be
useful and coherent to others. In the meantime we make the data and system available for
qualified researchers on a per-request basis. In the following section, we will provide both
details on the exact approach used and discuss several lesson-learned during the
development of the project.
Complications with terms of service
An ongoing issue associated with Twitter data (or any data sourced from a private
company) are the Terms of Service (ToS) – to which every API user has to agree – and that
are continuously updated and changed. For example, earlier versions of the ToS prohibited
sharing data collected through the streaming API but under the current ToS (Twitter,
2015) it is possible to share collected data up to 50,000 records per user per day as long as
that process is not automated. The Twitter ToS also requires looking for deletion events in
the stream so that if a user deletes a certain tweet, it would also be deleted from stored
records as well. In short, while ToS requirements are framed as legal issues there are a
number of methodological challenges and ethical questions as well. For example, simply
checking for deletion events and acting on this information, introduces a whole new level of
operational complexity.
Although Twitter has cracked down on public websites and commercial services
that share data (“Export Twitter Followers and Friends using a Google Spreadsheet,” 2011;
“Removal of Export and Download / API Capabilities,” 2011; “The story of getting Twitter
data and its ‘missing middle’ | ScraperWiki,” 2014), many academics have shared Twitter
data for replication and further research (e.g. Dooms & De Pessemier, 2013) and Twitter
has so far been quiet on the matter. This makes the Terms of Service and how best to follow
them somewhat of a gray area for academics, particularly as the ethos of information
technology companies has been steeped in the academic traditions of knowledge sharing
and research using Twitter data serves as free PR for the company. Thus, it seems that in
6

some cases – particularly for non-commercial applications –the “hacker ethics” mentality
(Levy, 1984) seems relevant, although there is always potential for it to be trumped by a
more hardline and legalistic stance by Twitter.
Leveraging Twitter REST and streaming APIs
Until 2009 Twitter only provided a so-called REST API. This API allows developers
to build third-party applications that interface with Twitter using HTTP verbs (GET, POST,
DELETE etc.) to specific ‘endpoints’. For example, a GET request to /friends/ids would
get all the friends of a specific user, while a POST request to /statuses/update would
send a new tweet. This allows a developer to build an application on top of Twitter, or even
to build a Twitter client that mimics or improves on Twitter’s own web interface and
smartphone apps. The structure of REST API can be used to search Twitter’s database for
tweets that match certain criteria, although Twitter has constrained the search to the last
seven days in history and a maximum result set of approximately 3500 tweets. Excessive
use and polling of this endpoint plagued Twitter’s overburdened servers during its early
years so the company instituted a ‘rate limit’ on the number of times a person could query
an endpoint within a certain timeframe — making it less feasible to use in the collection of
larger datasets.
Twitter was originally very welcoming to researchers and allowed them to be white
listed to get a higher rate limit for academic research. It discontinued this program once the
monetary value of its data became clearer. In 2009, Twitter released a Streaming API to
make collecting some data a bit easier. Unlike the REST API, which actively needs to be
polled at specific intervals to get new data, with the Streaming API opens a long-lived
connection (a stream) to Twitter’s servers that receives updates whenever they are being
created. This is analogous to a mail client checking mail at specific intervals (a REST API
request) versus the push notifications on modern platforms that are sent as the new mail
arrives (a Streaming API always-on connection). There are three different streaming levels
available. The standard streaming level contains roughly a 1 percent sample of all tweets
and can be filtered by specific keywords, user ids or geographic bounding boxes. If filtering
is enabled, only those tweets matching the filter will be returned. Importantly, if the tweets
matching the filter amount to less than 1 percent of all tweets, all tweets matching the filter
will be returned instead of a random sample. An elevated access called the gardenhose
contains 10 percent of all tweets and was offered to academics free of charge in the early
years of Twitter’s operations2. DOLLY was grandfathered into gardenhose access level in
2011 but new applications from academics are now referred to Gnip, a private company
where such access can only be bought. Finally, there is a firehose level of access that
contains all publicly available tweets but this level is seldom given out and is cost-inhibitive
for most research projects (Morstatter et al., 2013).

The DOLLY project received an academic white listing in May 2009 for a different project (Dugundji,
Poorthuis, & van Meeteren, 2011; van Meeteren, Poorthuis, & Dugundji, 2009). This original white listing
allowed DOLLY to access the elevated garden hose (10%) streaming access without going to a third-party
commercial vendor in 2011.
2

7

The principle downside of streaming for research purposes is the nature of
streaming itself. The ‘use it or lose it’ approach means that a stream needs to be set up
before an event of interest occurs and when, for example, a researcher later realizes a
certain keyword is pertinent to the research at hand it cannot be retroactively added to the
dataset. As such, with the Streaming API it is especially important to cast a wide net and
collect as much data as might be possibly needed in the future. In addition, many topics of
interests are tweeted about infrequently and it can take many months or years before a
‘large enough’ dataset is collected for specific topics or areas under study.
The goal of the DOLLY system is to collect every single geotagged tweet and to do so,
it uses both the 1 percent as well as the 10 percent Streaming API. It sets up separate,
parallel streams at the 1-percent level that are each filtered by a different spatial bounding
box covering a continent. Each of these streams can collect 1 percent of tweets – if the filter
on a stream matches more than 1 percent, the result set will be sampled down to 1 percent.
Thus, by narrowing each stream to a relatively small bounding box, we ensure that the
geotagged tweets that fall within this box are (almost) always below 1 percent of all tweets.
The gardenhose access level collects a separate stream consisting of 10 percent of
all tweets, not just the subset of geotagged tweets. This is done to monitor the total amount
of tweets sent everyday, as well as determine what percentage of those tweets are
geotagged. As is shown in Figure 1, the total amount of tweets sent each day varies
between 200 and 400 million messages per day. Although Twitter, around their IPO in
2013, enthusiastically reported that 500 million tweets a day were sent (Twitter, 2013),
they have grown increasingly silent on the exact number and it seems likely that the
number of tweets per day has indeed stabilized around or below 400 million (Leetaru et al.,
2013; Oreskovic, 2015). Despite the increasing number of GPS-enabled smartphones, the
total share of geotagged tweets is relatively stable as well, around 2.5% of the total (see
Figure 2). As such, DOLLY captures around 10 million geotagged tweets worldwide
everyday via the seven parallel continental bounding boxes using 1 percent streams.
Although the number of tweets per day itself is relatively stable, their geographic origin is
constantly evolving. For example, since DOLLY began collecting data in 2012 some
countries in the Global South have emerged as more prominent on Twitter so that by 2015
Twitter has clearly evolved beyond exclusive presence in rich, English-speaking countries.
The linked ranking in Figure 3 shows the relative share of tweets for each country in 2012
and 2015 to show how the contours of Twitter use has evolved over time.

8

Number of tweets per day (7-day moving average)

400MM

300MM

200MM

100MM

0
04-13

06-13

08-13

10-13

12-13

02-14

04-14

06-14

08-14

Date (month-year)

Figure 1, Tweets per day in the 10% sample

9

10-14

12-14

02-15

04-15

Percentage of tweets that are geotagged (7-day moving average)

3

2

1

0
04-13

06-13

08-13

10-13

12-13

02-14

04-14

06-14

08-14

10-14

12-14

02-15

Date (month-year)

Figure 2, the relative share of geotagged tweets3

The fluctuations seen in Figure 1 and Figure 2 are not a result in the DOLLY methodology or system and are
tied to changes in (1) actual Twitter usage and/or (2) changes in Twitter’s public API. However we have been
unable to clarify with Twitter the exact cause of these changes.
3

10

04-15

United States

2012

2015

28.4

29.1

United States

20.1

Brazil

6.5

Japan

5.3

Indonesia

Brazil

12.2

Indonesia

7.6

Great Britain

6.6

Turkey

4.7

Spain

4.5

Malaysia

3.5

3.1

Spain

Mexico
France

3.1
3.0

Russia

2.5

3.1
2.9
2.8

Turkey
Malaysia
Argentina

Japan

2.1

2.4
2.2

Mexico
Russia

Saudi Arabia
Netherlands
Philippines
Italy

1.6
1.6
1.5
1.4

1.8
1.7

Great Britain
Philippines

1.5

Portugal

Argentina
Portugal
Uruguay

0.8
0.2
0.0

1.2
1.0
1.0
0.5
0.5

Italy
Uruguay
France
Saudi Arabia
Netherlands

Percentage of tweets sent from each country (relative to all tweets worldwide)
for most active countries on Twitter. Change from 2012 to 2015.

11

Figure 3, Change in relative share per country between 2012-2015

To set up these streams, worker scripts (‘streaming workers’) – written in Ruby
using the well-supported open source Tweetstream library (tweetstream, 2015) – connect
to Twitter and listen for incoming tweets. Each worker runs on a separate virtual machine
in a virtual private cloud at the University of Kentucky. This minimizes the impact of a
potential outage in any one machine as virtual machines can be moved to different physical
machines when an outage occurs. Furthermore, each of the streams is monitored by a
separate service that continuously checks if the stream is still operating correctly and, if
not, attempts to restart the process (“Easy, proactive monitoring of processes, programs,
files, directories, filesystems and hosts | Monit,” n.d.) If a worker script or other process is
repeatedly failing or are not running for several minutes, email warnings are sent to enable
quick human intervention. Apart from a few inevitable power outages, network
configurations errors and related issues4, this has enabled a continuous and uninterrupted
consistent collection of all geotagged data since June 2012.
Selecting a data format
Tweets arrive from the Streaming API in the JavaScript Object Notation (JSON) and
are kept in that format throughout the collection processes. Upon arrival a tweet is
submitted to a queue5 for further processing. Queuing systems are used in many
applications, including finance and high frequency trading to track and process
transactions in which speed and performance of message sending and delivery is of the
essence. Here, we use a messaging queue specifically to decouple the initial collection and
the subsequent filing away of tweets. As Twitter activity is spiky, for example during a
Superbowl event, tweets can sometimes stream in faster than they can be filed away. The
queue functions as a buffer and can smooth out peaks without the risk of losing tweets
because the storage process cannot keep up. The queue runs on two mirrored virtual
machines to enable high-availability failover.
With smaller amounts of data it would be sensible to store data in text files or a
simple relational database, but this does not scale well to a continuous multi-year
collection of data of this magnitude. Each day, 10 million tweets of raw data means about 8
GB of data (after compression) needs to be stored. This translates to about 9TB of data
storage for the 2012 to 2016 dataset, not taking into account backup and redundancy. As
such, storage of such datasets is not a trivial matter and needs to be carefully designed to

Although the University’s data center was outfitted with its own power generator and UPS the system was
impacted by power outages. Likewise, some configuration errors and software bugs resulted in some gaps
(generally of a few minutes or hours) while updates were preformed. While we discussed filling these gaps
the combination of a short time horizon for action (before Tweets were no longer available), the weight of
other demands on our time and lack of human resources made filling these gaps a lower priority than other
tasks crucial to keep the system going. While DOLLY was up approximately 99.99 percent of the time these
gaps bring home the difficulty in maintaining 100% uptime in long-term data collection.
4

5

The open source RabbitMQ that utilized the open AMQP standard is used here (Vinoski, 2006)

12

match both current and future needs. In the initial implementation phase of DOLLY,
Amazon’s cloud service was used to host database servers as well as storage and backup of
data. However, in 2011 Amazon’s virtual machines had notoriously slow IO operations
compared to dedicated servers and the database had trouble writing the amount of data to
disk fast enough (Hill & Humphrey, 2009; Juve, Deelman, Berriman, Berman, & Maechling,
2012; Juve et al., 2009; Ostermann et al., 2010). Furthermore, running multiple virtual
machines as well as storing large amounts of data can get expensive quickly; especially
since Amazon’s cloud service is based on recurring monthly costs. Therefore, after initial
testing we invested in servers and storage space to build our own private cloud as it was
much more economical than a recurring payment to a cloud service provider. It also
ensures the continued availability of the system and data within after the collection effort
or the entire project ends as the majority of costs are not incurred on a monthly recurring
basis but are paid upfront when hardware is bought.
Once a hardware configuration is selected, the next key question is the file format
used for storage. During the initial implementation of DOLLY in the fall of 2011, all tweets
were stored as compressed JSON text files. Once JSON is compressed, it is remarkably
space-efficient, convenient and robust: data corruption is less of an issue with text files;
they can be read by any software program; and tweets can be bundled per day and
archived away in off-site storage if and when local storage space becomes an issue. This is
exactly the way the Library of Congress is storing their Twitter archive (Congress, 2013)
and is also how Mapbox is storing a similar archive (Fisher, 2014). However, when one
wants to analyze data, the compressed JSON format becomes a major inconvenience. Even
to extract a small, relevant dataset of 1,000 tweets that match, say, a certain keyword, the
entirety of the data needs to be read and combed through. On a single server, this can take
many days — which is exactly why many big data companies use distributed systems like
Hadoop (T. White, 2012) or other map-reduce frameworks (Gates et al., 2009) that fan out
chunks of the data to be individually processed by a large set of machines (the map stage)
and, when done, the results are combined back on to one single machine (the reduce stage).
Relational, NoSQL and Elastisearch databases
Thus, while implementing a large cluster can speed up access, map-reduce
frameworks require a significant amount of skill and runs counter to the goal of increasing
access for non-technical social scientists. Other relational database solutions such as
MySQL and Postgresql can also be used to store large amounts of data, but they are not
optimal for big data research two reasons. First and foremost, they require the researcher
to design in advance a specific schema – the data fields and their types (integers, text etc.) –
for the dataset. This makes adding and querying data much more efficient but, as is the case
with social media like Twitter, the incoming data stream can often change its format,
adding or removing fields and changing data types without notice. Thus incoming data
could not be added unless the researcher engages in an ongoing cat-and-mouse game of
restating the schema and converting old data to the new format. Second, relational
databases, being a much older technology, are often built with single-server architecture in
mind. Although they can potentially distributed across servers (an absolute necessity given
the scale and growth of data), these configurations are often fragile and require significant
system administration.
13

For these reasons, DOLLY initially used CouchDB (J. C. Anderson, Lehnardt, & Slater,
2010) for the underlying data storage and then switched to MongoDB (Chodorow, 2013) —
the latter being a widely used solution for other big data collection systems (Croitoru et al.,
2013; Kumar et al., 2014). These are NoSQL databases and contrast to the relational
database approach, where each data observation is structured as a row with a set number
of fields, and instead use a document model. In NoSQL databases, each observation is a
‘document’ (rather than a row) and the fields in each document can be completely different
from one another. Furthermore, fields themselves can be nested, containing additional
fields as children. This structure fits neatly with the way tweets are natively stored (JSON)
and some NoSQL databases even use JSON in their underlying model.
These NoSQL databases are designed with big data in mind, i.e., scalability is a key
concern, and should theoretically work with many terabytes of data in a distributed cluster
(Abramova & Bernardino, 2013; Cattell, 2011; Pokorny, 2013). In practice, however,
MongoDB suffered from reliability issues, data corruption and scalability that required
constant and vigilant system administration. Furthermore, as the historical data kept
growing, MongoDB had escalating difficulty keeping up with the ingestion rate of Twitter
data collection and combined with cumbersome cluster maintenance procedures meant a
significant investment in system-administrator-type skills.
Given these short-comings, we converted the DOLLY system in the Spring of 2012 to
use ElasticSearch (Gormley & Tong, 2015) as its primary database store and querying
layer. ElasticSearch is a noSQL-like scalable, distributed database that supports full-text
search (built on top of Lucene), has very solid geo-spatial support, and can handle changes
in the structure of inserted data on-the-fly. Furthermore, it compresses data efficiently
(requiring less storage space) and scaling up by adding additional nodes is turnkey: when a
new node is added, all data is automatically distributed evenly over the cluster.
Furthermore, it has built-in redundancy by duplicating data one or many times across the
database, increasing the speed with which data can be retrieved but also making the entire
cluster more resilient.
DOLLY initially ran two ElasticSearch virtual machines with 500GB storage space
and 4GB of RAM each with additional nodes easily added when needed (or funding became
available). Over time, the storage layer for DOLLY grew to a total of 33 nodes with 1.1TB of
storage and 10GB of RAM each. Data for the most recent 12 months, which is queried and
analyzed more often, is replicated twice (thus stored three times in total) and older data is
replicated once. To ensure longevity of the dataset, a back-up of the entire dataset is run on
a daily basis to a storage facility in the same building as the private cluster and once a
week, a back-up is made to an offsite facility.
Adding metadata variables
Before data is inserted in the ElasticSearch datastore, a number of operations are
performed by a set of worker scripts (each running on a separate node) that read from the
messaging queue where tweets were deposited by the streaming scripts. Most importantly,
the geotag of each tweet is evaluated. There are three primary types of geotags attached to
tweets: one that is a relatively accurate latitude/longitude coordinate pair generated by a
smartphone (either through GPS or cell tower and Wi-Fi location); one that is a ‘place’,
14

something like ‘Brooklyn, NY’ or ‘USA’; and one that has both a place and a coordinate pair
(see Poorthuis, et al, 2016). Furthermore, the latitude/longitude coordinate is used to
perform a spatial join to larger areal units: country and state/province-level for all
countries6 and the addition of county and census tract levels for the US. This makes
searching for data for an entire country or a specific census tract (within the US) much
faster later on.

Access

Twitter Streaming API

Tweets come in from Twitter...

Stream Worker Nodes

... are collected by a set of parallel worker nodes...

RabbitMQ Message Queue

... sent into a processing queue that forms a buffer
for peak tweet volume..

Data Process and Insertion Worker Nodes

Another set of parallel worker nodes reads from this queue
and processes each tweet...

Storage
ElasticSearch Cluster

... which is then stored and indexed in ElasticSearch.

Search & Extract
Front End

Queries and data extraction to be done from a front end
that ‘speaks’ to ElasticSearch

Figure 4, Schematic overview of DOLLY system
The final piece of metadata added is a random number between 0 and 100 million to
allow a researcher to take fast random samples post-hoc. This is particularly helpful for
many research questions (particularly during exploratory stages), as often one only needs
to analyze a random sample. Databases, however are often not designed to allow for fast
random sampling at query time and thus doing so post-hoc can be expensive (Olken &
Rotem, Pokorny 1990) and the addition of a random number allows for a cheap range
query on that field: a query matching all numbers smaller than 10 million will then result in
a random 10% sample of all data. Both the geographic and random number metadata are
added to the tweet document before inserting into the ElasticSearch database (see Figure
4).

6

Using Natural Earth Data (“Natural Earth,” n.d.) and the PostGIS spatial database (Ramsey, 2005)

15

Indexing for search and extraction
Once data is collected and metadata added, the most important feature for social
scientists is to ensure that search and extraction functions can be performed quickly. Many
databases rely upon an indexed key for searching with the goal of adding index data as
quickly as possible as well as keeping creating indices to a bare minimum as index
construction is time and CPU expensive as well. This means extraction of data in nonanticipated ways -- searches on non-indexed fields -- becomes very ‘expensive’ as the entire
data set needs to be looked through.
The design of DOLLY takes the opposite approach: as computing power is relatively
cheap, as much data is indexed as possible and this is done immediately when tweets come
in. This means that when a search is actually conducted it can be searched very quickly
across virtually all fields. In practical terms, this means that the entire cluster (33
ElasticSearch nodes, 8 worker nodes) is continuously indexing data as it arrives, for fast
extraction via search later. Thus, a search for some specific data from 2013, leverages
computing cycles completed years earlier, invisibly shifting the time-space of computer
processing to the advantage of the researcher.
Instead of tables, ElasticSearch uses indices that in the case of DOLLY are organized
by month and the seven bounding boxes (i.e., continents) used for the Twitter Streaming
API. This both allows data to be managed on a month-by-month basis (for example,
decreasing replication for indices older than 12 months) as well as also further optimizing
queries. For example, to search and extract data from a specific continent and month, one
only needs to access that month’s index, twitter_na_2013-01, and a wider search can be
done via wildcards, searching in ‘twitter_na_2013*’ searches all indices for North America
created in 2013.
Within each index, every single field is individually indexed. For fields that contain
text (such as the actual tweet text and the user bio) a full-text index is constructed by
splitting the field into terms and creating n-grams – strings within each word up to the
length n – for each term. For example, a 1-gram or unigram of the word ‘Dolly’ would yield
[‘D’, ‘o’, ‘l’, ‘l’, ‘y’], a 2-gram or bigram would yield [‘Do’, ‘ol’, ‘ll’, ‘ly’], and so on. This allows
searches to match partial words and to search using wildcards (p?zza*) and even do fuzzy
string matching using Levenshtein’s string distance (Levenshtein, 1966) to account for
spelling variations, singular-plural, shortened words and general typos.
The coordinates of each tweet are spatially indexed as well so that, if needed, spatial
queries can be done based on bounding boxes, polygons or even a distance from a certain
point. ElasticSearch also calculates a geohash at multiple scale levels (“Geohashes,” n.d.). A
geohash was originally designed to express a location in URL-friendly format but is also a
very effective way of aggregating locations up to multiple scale levels. At the coarsest level,
geohashes divide the world into 32 cells (each about 5000 by 5000 km), each expressed by
one letter or number (e.g. ‘g’). The second level divides each cell in 32 sub-cells, which can
be expressed by a two-position hash, e.g. ‘gq’. The final level 12, where the hash has a
length of 12, e.g. ‘gq5h9z2ng91a’, has a precision of just a few centimeters. Having each
point pre-indexed in this way, also allows DOLLY to quickly aggregate individual tweets up

16

to grids of varying scale and compare spatial distributions of different queries at different
scales in a fast manner.
Putting DOLLY to Work - User interfaces and access
The final piece of the system is a series of user interfaces that provide query and
extraction access for a range of different skill levels. Researchers comfortable with coding
can interface with Dolly’s back-end directly through http requests sent from their
programming language of choice or from within the statistical language R. We have also
developed a light front-end website that allows full access to a boolean search logic, a
preview of the tweets in the search results and an export of the resulting data to CSV.
Within this interface, users could submit queries ranging from ‘give me a random sample of
50,000 tweets sent from New York City in the last 12 months’ to more complex ones that
use fuzzy matching and wildcards on words in either the tweet itself or the user biography.
Thus, even non-coders have a significant and compelling level of search and extract access.
While lower level querying – via R or some other programming language – allows for
automated queries, data processing and even visualization, this web based solution fulfills
our goal of providing non-programmer access to a comprehensive archive of all geotagged
tweets sent since June 2012.
A remaining challenge for future development is expanding the system’s front-end
to allow for a more user-friendly experience for preliminary data visualization. In its
current implementation, analysis of the data beyond a first cursory look at tabular data
requires export to a desktop GIS or statistical environment rather than housing the first
steps of exploratory analysis completely and reliably within the browser application. This
would provide researchers an immediate insight into the underlying spatial pattern at
hand, allowing for a quicker iteration in the research cycle and be especially fruitful in the
early stages of research projects where ideas, hypotheses and keywords are under
evaluation.
At present the main bottleneck facing DOLLY is that we lack the human resources to
effectively administer and secure access to it. As a result direct access is limited to those
within our research team, but we do work with qualified researchers who contact us
directly or via the DOLLY webpage to extract data for specific projects. Thus far, however,
we have not broadcast this availability widely primarily due to personnel and time
constraints (we have to retrieve the data ourselves) rather than technical or legal
limitations. The most recent ToS from Twitter clearly specifies the number of records that
can be returned – 50,000 per user per day – which seems a workable dataset size for many
social science questions. In short, DOLLY is not just a prototype of a big data collection and
analysis system (cf. Fujita, 2013; Huang & Xu, 2014) but a fully implemented platform.
This includes the use of DOLLY by approximately a dozen researchers at institutions
worldwide. Based on our input, these scholars have used datasets to study topics ranging
from examining urban displacement (Schaefer, 2014), to tracking the view sheds of wild
fires (Klein, 2014), to analyzing political and cultural discourse (Jung, 2015; Chew, 2016),
to studying the relationship between fast food and public health (Christian, 2014) as well
as a number of doctoral dissertations within human geography (Shelton, 2015; Stephens,
2012). DOLLY data is also currently in use with several ongoing research projects at UC
17

Santa Barbara, the University of South Carolina, the University of Amsterdam and the
University of Twente in the Netherlands ranging from hate speech and public health to the
spatial dispersion of knowledge.
In addition we have used DOLLY to search and extract relevant data for our own
articles (Crampton et al., 2013; Poorthuis & Zook, 2014; 2015; Poorthuis, Zook, Shelton,
Graham, & Stephens, 2015; Shelton, Poorthuis, Graham, & Zook, 2014; Zook & Poorthuis,
2014). To provide insight on the data retrieval and analysis process we offer a brief review
of one of our projects (Shelton, Poorthuis, & Zook, 2015) to show how DOLLY can be put to
work. This particular analysis engages with wide-spread understandings of mobility and
segregation in Louisville, KY and uses big geosocial media data as an indicator of the
mobility of different parts and populations of the city. More specifically our research
question was whether we could show that individuals from the West End (a predominantly
African-American and poorer neighborhood) inhabited and moved in different parts of the
city than those from the East End (a more well-off and predominantly white part of town).
Beginning with an initial set of 5.7 million geotagged tweets (the entirety sent in Louisville
from June 2012 to July 2014) we filtered down to only the 1.5 million tweets sent within
the neighborhoods we studied. Using this subset we next identified individual users who
(1) had at least 40 tweets in our data and (2) had sent the majority of tweets from one of
these two neighborhoods. This yielded 703 East End and 662 West End individuals sending
at total of 274,338 and 398,432 geotagged tweets respectively. In order to control for
‘power users’ who are responsible for a disproportional share of tweets, our final step was
to sample down for each user so that our final database for analysis was approximately
100,000 tweets from these 1,365 users, a reduction of approximately 50 times the initial
set with which we started.
In this example, the ability of DOLLY to shift from “big” to “small” data allowed us to
“go beyond the geotag” (Crampton et al, 2013) and study “the fundamentally fluid,
networked and relational nature of places in the city, as well as the dynamism of how
people live in and occupy these places” in order to “build upon our understandings of how
this data can reveal a more complex and nuanced set of socio-spatial relations than is
typically assumed.” (Shelton, Poorthuis, & Zook, 2015: 210). While we remain cautious
about the applicability of social media and other types of big data analysis to our research
(see also Goodspeed, 2013; Zook, 2017), we also see these data as a means to engage with
popular but unsubstantial understandings of society and space (see also Schweitzer, 2014).
In short, the DOLLY system – along with other similar approaches to big data – provide the
means for social science to successfully engage with the fast growing application of big data
to the world.
The future of big geosocial media data research
This article outlines a solution for making big data more accessible for social science
and urban research through a counter-intuitive approach that works to make big data
small. While not appropriate for all research questions – for example, data science
approaches that seek patterns across the totality of a phenomenon – it is extremely useful
for a range of geographic and urban research. Particularly because it allows scholars who
do not have the background or computing infrastructure necessary for collecting and
maintain social media datasets to access and use these data. Moreover, by collecting all
18

social media data (in this case geotagged tweets) a priori, it provides researchers with the
flexibility to form research questions and hypotheses within established workflows rather
than under the pressure of time, fearing that data about a particular phenomenon will
“time out” and become unavailable.
Of course, gaining access to data is but the first step of any research project and care
must be exercised in how it is visualized and analyzed. This is especially the case for
scholars using social media data for the first time or those unfamiliar with the details of
spatial analysis (Zook, 2017). In the case of the former, real care must be taken to ensure
that a social media event (a post, a tweet, etc.) is interpreted carefully and broadly rather
than simply assuming that it has only one meaning (Crampton et al, 2013). While sentiment
analysis provides some metrics on the meaning of a social meaning utterance, the use of
slang and the small size of individual social media events confound overly naive
interpretations. For those new to spatial data it is important look beyond clustering of
events, as these often are primarily stories of population densities. Rather, proper
normalization of the data as well as decisions about the type of areal units to be used are
essential when using social media data (see Poorthuis et al, 2016 and Zook, Poorthuis and
Donohue, 2016).
Despite these cautions, the case study of Louisville (Shelton, Poorthuis, & Zook,
2015) clearly shows the applicability of using big geosocial media data to understand cities.
Thus, DOLLY offers a way forward to use social media (as well as other big data sources) in
social science research that can successfully overcome the technical challenges of big data
and allow researchers to focus on their theoretical and methodological expertise rather
than the techniques of system administration.

References
Abramova, V., & Bernardino, J. (2013). NoSQL databases: MongoDB vs cassandra. the
International C* Conference (pp. 14–22). New York, New York, USA: ACM.
http://doi.org/10.1145/2494444.2494447
Anderson, C. (2008). The End of Theory: The Data Deluge Makes the Scientific
Method Obsolete. Retrieved August 14, 2015, from
http://archive.wired.com/science/discoveries/magazine/16-07/pb_theory
Anderson, J. C., Lehnardt, J., & Slater, N. (2010). CouchDB: the definitive guide.
O'Reilly Media, Inc.
Arribas-Bel, D. (2014). Accidental, open and everywhere: Emerging data sources for
the understanding of cities. Applied Geography, 49, 45–53.
http://doi.org/10.1016/j.apgeog.2013.09.012
Baginski, J., Sui, D., & Malecki, E. J. (2014). Exploring the Intraurban Digital Divide
Using Online Restaurant Reviews: A Case Study in Franklin County, Ohio. The Professional
Geographer, 66(3), 443–455. http://doi.org/10.1080/00330124.2013.866431

19

boyd, D., & Crawford, K. (2012). CRITICAL QUESTIONS FOR BIG DATA. Information,
Communication & Society, 15(5), 662–679.
http://doi.org/10.1080/1369118X.2012.678878
Cattell, R. (2011). Scalable SQL and NoSQL data stores. ACM SIGMOD Record, 39(4),
12–27. http://doi.org/10.1145/1978915.1978919
Chew, F. (2016). Using Tweets to Study Empire. Paper presented at the Broadcast
Education Association Conference.

Chodorow, K. (2013). MongoDB: the definitive guide. O'Reilly Media, Inc.
Christian, J. (2014, November). # epidemiology: Ecological analysis of fast food tweets in relation to
Behavioral Risk Factor Surveillance System data. In 142nd APHA Annual Meeting and Exposition
(November 15-November 19, 2014). APHA.

Congress, L. O. (2013). Update on the Twitter Archive At the Library of Congress
(Vol. 36, pp. 189–198). Library of Congress.
Crampton, J. W., Graham, M., Poorthuis, A., Shelton, T., Stephens, M., Wilson, M. W., &
Zook, M. A. (2013). Beyond the geotag: situating “big data” and leveraging the potential of
the geoweb. Cartography and Geographic Information Science, 40(2), 130–139.
http://doi.org/10.1080/15230406.2013.777137
Croitoru, A., Crooks, A., Radzikowski, J., & Stefanidis, A. (2013). Geosocial gauge: a
system prototype for knowledge discovery from social media. International Journal of
Geographical Information Science, 27(12), 2483–2508.
http://doi.org/10.1080/13658816.2013.825724
Devogele, T., Parent, C., & Spaccapietra, S. (1998). On spatial database integration.
International Journal of Geographical Information Science, 12(4), 335–352.
http://doi.org/10.1080/136588198241824
Dooms, S., & De Pessemier, T. (2013). Movietweetings: a movie rating dataset
collected from twitter. Presented at the … on Crowdsourcing and ….
Dugundji, E. R., Poorthuis, A., & van Meeteren, M. (2011). Modeling User Behavior in
Adoption and Diffusion of Twitter Clients. 2011 IEEE Third Int“l Conference on Privacy,
Security, Risk and Trust (PASSAT) / 2011 IEEE Third Int”l Conference on Social Computing
(SocialCom) (pp. 1372–1379). IEEE. http://doi.org/10.1109/PASSAT/SocialCom.2011.95
Easy, proactive monitoring of processes, programs, files, directories, filesystems and
hosts | Monit. (n.d.). Easy, proactive monitoring of processes, programs, files, directories,
filesystems and hosts | Monit. Retrieved August 13, 2015, from
https://mmonit.com/monit/
Export Twitter Followers and Friends using a Google Spreadsheet. (2011). Export
Twitter Followers and Friends using a Google Spreadsheet.
Fisher, E. (2014). Making the most detailed tweet map ever.

20

Fujita, H. (2013). Geo-tagged Twitter collection and visualization system.
Cartography and Geographic Information Science, 40(3), 183–191.
http://doi.org/10.1080/15230406.2013.800272
Gates, A. F., Natkovich, O., Chopra, S., Kamath, P., Narayanamurthy, S. M., Olston, C., et
al. (2009). Building a high-level dataflow system on top of Map-Reduce: the Pig experience.
Proceedings of the VLDB Endowment, 2(2), 1414–1425.
http://doi.org/10.14778/1687553.1687568
Geohashes. (n.d.). Geohashes. Retrieved August 16, 2015, from
https://www.elastic.co/guide/en/elasticsearch/guide/current/geohashes.html
Goodspeed, Robert. 2013. "The Limited Usefulness of Social Media and Digital Trace
Data for Urban Social Research." Seventh International AAAI Conference on Weblogs and
Social Media.
Gormley, C., & Tong, Z. (2015). Elasticsearch: The Definitive Guide. O'Reilly Media,
Inc.
Han, J., Koperski, K., Stefanovic, N., Han, J., Koperski, K., & Stefanovic, N. (1997).
GeoMiner: a system prototype for spatial data mining. ACM SIGMOD Record (Vol. 26, pp.
553–556). ACM. http://doi.org/10.1145/253262.253404
Hill, Z., & Humphrey, M. (2009). A quantitative analysis of high performance
computing with Amazon's EC2 infrastructure: The death of the local cluster? (pp. 26–33).
Presented at the 2009 10th IEEE/ACM International Conference on Grid Computing
(GRID), IEEE. http://doi.org/10.1109/GRID.2009.5353067
Huang, Q., & Xu, C. (2014). A data-driven framework for archiving and exploring
social media data. Annals of GIS, 20(4), 265–277.
http://doi.org/10.1080/19475683.2014.942697
Jung, J.-K. (2015). Code clouds: Qualitative geovisualization of geotweets. The
Canadian Geographer / Le Géographe Canadien, 59(1), 52–68.
http://doi.org/10.1111/cag.12133
Juve, G., Deelman, E., Berriman, G. B., Berman, B. P., & Maechling, P. (2012). An
Evaluation of the Cost and Performance of Scientific Workflows on Amazon EC2. Journal of
Grid Computing, 10(1), 5–21. http://doi.org/10.1007/s10723-012-9207-6
Juve, G., Deelman, E., Vahi, K., Mehta, G., Berriman, B., Berman, B. P., & Maechling, P.
(2009). Scientific workflow applications on Amazon EC2 (pp. 59–66). Presented at the
2009 5th IEEE International Conference On E-Science Workshops, IEEE.
http://doi.org/10.1109/ESCIW.2009.5408002
Kay, S., Zhao, B., & Sui, D. (2014). Can Social Media Clear the Air? A Case Study of the
Air Pollution Problem in Chinese Cities. The Professional Geographer, 1–13.
http://doi.org/10.1080/00330124.2014.970838
Kitchin, R. (2014). The data revolution: Big data, open data, data infrastructures and
their consequences. Sage.

21

Klein, K. R. (2014). Tracking a wildfire in areas of high relief using volunteered
geographic information: A viewshed application. ProQuest Dissertations and Theses. The
University of Utah, Ann Arbor.
Korson, C. (2014). Political Agency and Citizen Journalism: Twitter as a Tool of
Evaluation. The Professional Geographer, 1–10.
http://doi.org/10.1080/00330124.2014.970839
Kounadi, O., Lampoltshammer, T. J., Groff, E., Sitko, I., & Leitner, M. (2015). Exploring
Twitter to Analyze the Public’s Reaction Patterns to Recently Reported Homicides in
London. Plos One, 10(3), e0121848–17. http://doi.org/10.1371/journal.pone.0121848
Kumar, S., Barbier, G., Abbasi, M. A., & Liu, H. (2011). TweetTracker: An Analysis
Tool for Humanitarian and Disaster Relief, 1–2.
Kumar, S., Morstatter, F., & Liu, H. (2014). Twitter Data Analytics. New York, NY:
Springer New York. http://doi.org/10.1007/978-1-4614-9372-3
Laney, Doug. 2001. "3D data management: Controlling data volume, velocity and
variety." META Group Research Note 6:70.
Leetaru, K., Wang, S., Cao, G., Padmanabhan, A., & Shook, E. (2013). Mapping the
global Twitter heartbeat: The geography of Twitter. First Monday, 18(5), 290–307.
http://doi.org/10.1287/orsc.1050.0122
Levenshtein, V. (1966). Binary Codes Capable of Correcting Deletions, Insertions and
Reversals. soDiet. Soviet Physics-Doklady, 10(8). http://doi.org/10.1007/BF00053379
Levy, S. (1984). Hackers: Heroes of the Computer Revolution. New York: Doubleday.
Li, L., Goodchild, M. F., & Xu, B. (2013). Spatial, temporal, and socioeconomic
patterns in the use of Twitter and Flickr. Cartography and Geographic Information Science,
40(2), 61–77. http://doi.org/10.1080/15230406.2013.777139
Lin, J., & Cromley, R. G. (2015). Evaluating geo-located Twitter data as a control layer
for areal interpolation of population. Applied Geography, 58(C), 41–47.
http://doi.org/10.1016/j.apgeog.2015.01.006
MacEachren, A. M., Robinson, A. C., & Jaiswal, A. (2011). Geo-twitter analytics:
Applications in crisis management. 25th International ….
Morstatter, F., Pfeffer, J., Liu, H., & Carley, K. M. (2013, June 21). Is the Sample Good
Enough? Comparing Data from Twitter“s Streaming API with Twitter”s Firehose.
Natural Earth. (n.d.). Natural Earth. Retrieved August 15, 2015, from
http://www.naturalearthdata.com/
Olken, F., & Rotem, D. (1990). Random sampling from database files: A survey. In
Statistical and Scientific Database Management (Vol. 420, pp. 92–111). Berlin, Heidelberg:
Springer Berlin Heidelberg. http://doi.org/10.1007/3-540-52342-1_23
Oreskovic, A. (2015). Here's another area where Twitter appears to have stalled:
tweets per day. Retrieved August 15, 2015, from http://www.businessinsider.com/twittertweets-per-day-appears-to-have-stalled-2015-6
22

Ostermann, S., Iosup, A., Yigitbasi, N., Prodan, R., Fahringer, T., & Epema, D. (2010). A
Performance Analysis of EC2 Cloud Computing Services for Scientific Computing. In Cloud
Computing (Vol. 34, pp. 115–131). Berlin, Heidelberg: Springer Berlin Heidelberg.
http://doi.org/10.1007/978-3-642-12636-9_9
Pokorny, J. (2013). NoSQL databases: a step to database scalability in web
environment. International Journal of Web Information Systems, 9(1), 69–82.
http://doi.org/10.1108/17440081311316398
Poorthuis, A., & Zook, M. (2014). Artists and Bankers and Hipsters, Oh My! Mapping
Tweets in the New York Metropolitan Region. Cityscape: a Journal of Policy Development
and ….
Poorthuis, A., & Zook, M. (2015). Small Stories in Big Data: Gaining Insights From
Large Spatial Point Pattern Datasets. Cityscape: a Journal of Policy Development and ….
Poorthuis, A., Zook, M. A., Shelton, T., Graham, M., & Stephens, M. (2016). Using
Geotagged Digital Social Data in Geographic Research. In N. Clifford, S. French, M. Cope, & S.
Gillespie (Eds.), Key Methods in Geography (3rd ed.).
Ramsey, P. (2005). PostGIS manual. Refractions Research Inc.
Removal of Export and Download / API Capabilities. (2011, February 22). Removal
of Export and Download / API Capabilities. Retrieved August 13, 2015, from
https://twapperkeeper.wordpress.com/2011/02/22/removal-of-export-and-downloadapi-capabilities/
Schaefer, B. (2014). Social Media to Locate Urban Displacement: Assessing the Risk
of Displacement Using Volunteered Geographic Information in the City of Los Angeles. Los
Angeles.
Schweitzer, Lisa. 2014. "Planning and Social Media: A Case Study of Public Transit and
Stigma on Twitter." Journal of the American Planning Association 80 (3):218-238. doi:
10.1080/01944363.2014.980439.
Shelton, T. (2015, September 6). Geographies of Data: Toward a Relational SocioSpatial Analysis of Geotagged Social Media Data. Worcester.
Shelton, T., Poorthuis, A., & Zook, M. A. (2015). Social media and the city: Rethinking
urban socio-spatial inequality using user-generated geographic information. Landscape and
Urban Planning, 1–14. http://doi.org/10.1016/j.landurbplan.2015.02.020
Shelton, T., Poorthuis, A., Graham, M., & Zook, M. A. (2014). Mapping the data
shadows of Hurricane Sandy: Uncovering the sociospatial dimensions of “big data.”
Geoforum, 52, 167–179. http://doi.org/10.1016/j.geoforum.2014.01.006
Shelton, T., Zook, M. A., & Graham, M. (2012). The Technology of Religion: Mapping
Religious Cyberscapes. The Professional Geographer, 64(4), 602–617.
http://doi.org/10.1080/00330124.2011.614571
23

Smith, L., Liang, Q., James, P., & Lin, W. (2015). Assessing the utility of social media
as a data source for flood risk management using a real-time modelling framework. Journal
of Flood Risk Management, n/a–n/a. http://doi.org/10.1111/jfr3.12154
Smith, M. A., Shneiderman, B., Milic-Frayling, N., Mendes Rodrigues, E., Barash, V.,
Dunne, C., et al. (2009). Analyzing (social media) networks with NodeXL (pp. 255–264).
Presented at the the fourth international conference, New York, New York, USA: ACM.
http://doi.org/10.1145/1556460.1556497
Stefanidis, A., Cotnoir, A., Croitoru, A., Crooks, A., Rice, M., & Radzikowski, J. (2013).
Demarcating new boundaries: mapping virtual polycentric communities through social
media content. Cartography and Geographic Information Science, 40(2), 116–129.
http://doi.org/10.1080/15230406.2013.776211
Stephens, M. (2012, August 8). FROM GEO-SOCIAL TO GEO-LOCAL: THE FLOWS
AND BIASES OF VOLUNTEERED GEOGRAPHIC INFORMATION. Tucson.
The story of getting Twitter data and its “missing middle” | ScraperWiki. (2014). The
story of getting Twitter data and its “missing middle” | ScraperWiki. Retrieved August 13,
2015, from https://blog.scraperwiki.com/2014/08/the-story-of-getting-twitter-data-andits-missing-middle/
tweetstream. (2015, September 1). tweetstream/tweetstream. Retrieved August 16,
2015, from https://github.com/tweetstream/tweetstream
Twitter. (2013, August 16). New Tweets per second record, and how! | Twitter
Blogs. Retrieved September 21, 2015, from https://blog.twitter.com/2013/new-tweetsper-second-record-and-how
Twitter. (2015, May 18). Developer Agreement & Policy. Retrieved September 21,
2015, from https://dev.twitter.com/overview/terms/agreement-and-policy
van Meeteren, M., Poorthuis, A., & Dugundji, E. (2009). Mapping communities in
large virtual social networks: Using Twitter data to find the Indie Mac community (pp. 1–8).
Presented at the Engaging Data, IEEE. http://doi.org/10.1109/BASNA.2010.5730297
Vinoski, S. (2006). Advanced Message Queuing Protocol. IEEE Internet Computing,
10(6), 87–89. http://doi.org/10.1109/MIC.2006.116
Wang, S. (2010). A CyberGIS Framework for the Synthesis of Cyberinfrastructure,
GIS, and Spatial Analysis. Annals of the Association of American Geographers, 100(3), 535–
557. http://doi.org/10.1080/00045601003791243
Weller, K. (2014). Twitter Und Wahlen. In R. Reichert (Ed.), Big Data. Bielefeld.
White, J., & Roth, R. E. (2010). TwitterHitter: Geovisual analytics for harvesting
insight from volunteered geographic information. Presented at the Proceedings of
GIScience.
White, T. (2012). Hadoop: The definitive guide. O'Reilly Media, Inc.
Widener, M. J., & Li, W. (2014). Using geolocated Twitter data to monitor the
prevalence of healthy and unhealthy food references across the US. Applied Geography, 54,
189–197. http://doi.org/10.1016/j.apgeog.2014.07.017
24

Xu, C., Wong, D. W., & Yang, C. (2013). Evaluating the “geographical awareness” of
individuals: an exploratory analysis of twitter data. Cartography and Geographic
Information Science, 40(2), 103–115. http://doi.org/10.1080/15230406.2013.776212
Yang, W., & Mu, L. (2015). GIS analysis of depression among Twitter users. Applied
Geography, 60, 217–223. http://doi.org/10.1016/j.apgeog.2014.10.016
Zimmer, M., & Proferes, N. J. (2014). A topology of Twitter research: disciplines,
methods, and ethics. Aslib Journal of Information Management, 66(3), 250–261.
http://doi.org/10.1108/AJIM-09-2013-0083
Zook, M. (2017). Crowd-sourcing the Smart City: Using Big Geosocial Media Metrics
in Urban Governance. Big Data and Society. Forthcoming.
Zook, M. A., & Poorthuis, A. (2014). Offline Brews and Online Views: Exploring the
Geography of Beer Tweets. In The Geography of Beer (pp. 201–209). Dordrecht: Springer
Netherlands. http://doi.org/10.1007/978-94-007-7787-3_17
Zook, M., Poorthuis, A, Donohue, R. (2016). Mapping Spaces: Cartographic
Representations of Online Data. Chapter for the Handbook of Online Research Methods
second edition

25

