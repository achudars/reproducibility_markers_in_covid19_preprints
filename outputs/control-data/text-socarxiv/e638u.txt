Identification of Preferences in Forced-Choice
Conjoint Experiments: Reassessing the Quantity of
Interest*
Flavien Ganter
Columbia University

Forced-choice conjoint experiments have become a standard element of the experimental
toolbox in political science and sociology. Yet the literature has largely overlooked the fact
that conjoint experiments can be used for two distinct purposes: to uncover respondents’
multidimensional preferences, and to estimate the causal effects of some attributes on a profile’s selection probability in a multidimensional choice setting. This paper makes the argument that this distinction is both analytically and practically relevant, because the quantity
of interest is contingent on the purpose of the study. The vast majority of social scientists relying on conjoint analyses, including most scholars interested in studying preferences, have
adopted the average marginal component effect (AMCE) as their main quantity of interest.
The paper shows that the AMCE is neither conceptually nor practically suited to explore
respondents’ preferences. Not only is it essentially a causal quantity conceptually at odds
with the goal of describing patterns of preferences, but it also does generally not identify
preferences, mixing them with compositional effects unrelated to preferences. This paper
proposes a novel estimand—the average component preference (ACP)—designed to explore
patterns of preferences, and it presents a method for estimating it.

Draft.—Last version here.
March 15, 2021
* The materials required to verify the computational reproducibility of the results, procedures, and analyses in

this article are available on my GitHub page for the code, and on the Harvard Dataverse for the immigrant experiment data and the community experiment data. For her guidance and feedback at the various stages of this project,
I am deeply grateful to Maria Abascal. I also want to thank Delia Baldassarri, Guillaume Bied, Kate Khanna, Xavier
d’Haultfœille, and Teppei Yamamoto for their comments and suggestions on previous drafts. This paper also benefited from remarks provided by participants in the Columbia Sociology Stat Lab. Direct correspondance to Flavien
Ganter, Department of Sociology, Columbia University, 501 Knox Hall, 606 West 122nd Street, New York, NY
10027. Email: flavien.ganter@columbia.edu.

1

INTRODUCTION
Whereas typical factorial designs allow scholars to draw inferences about one or a limited number of interacted attributes (e.g. race and education), conjoint experiments expand this principle
to a large array of attributes and allow researchers to better account for the multidimensionality of social phenomena. In forced-choice conjoint experiments, in particular, respondents are
presented with several (usually two) profiles defined by a series of attributes. In an influential
example, respondents are shown profiles of immigrants characterized by their gender, level of
education, or occupation, among several other features (Hainmueller and Hopkins 2015). These
variables are randomly assigned among profiles and respondents, and each one is determined
independently from the others, or with only a limited number of restrictions. Respondents are
then asked to choose one, and only one, of the profiles they saw—in the immigrant experiment,
the immigrant to whom they would grant a visa.
Forced-choice conjoint experiments have been implemented to study a broad range of topics,
asking respondents to choose among pairs of immigrants, political candidates, job candidates,
or public policies—among many other examples (e.g., Carlson 2015; Hankinson 2018; Carey
et al. 2018). In a study comparing findings from vignette, conjoint and natural experiments,
Hainmueller et al. (2015) have shown that the effects estimated in the first two cases matched the
“real-world” effects obtained from natural experiments, and that conjoint experiments performed
even better than standard vignette experiments.
As I argue in this paper, forced-choice conjoint designs may be implemented to answer two
distinct types of questions. One pertains directly to respondents’ preferences, that is, to the patterns of favorability associated with each attribute and their relative importance. In that case,
the selection process realized in the experiment (e.g., choosing one immigrant who would hypothetically be granted a visa among a pair) is not of interest for itself; it is only an instrument
deployed to reveal the criteria that people use when making decisions. Preference-related questions based on Hainmueller and Hopkins’s (2015) immigrant experiment, for example, include:

2

Do Americans prefer German over Mexican immigrants? Do they care about immigrants’ country of origin at all?
The second type of questions is more directly concerned with the selection process reproduced in the experiment, and with its outcome from the perspective of the profiles under scrutiny.
Such selection-process questions based on the immigrant experiment include: How likely to be
granted a visa are Mexican immigrants? And how different would the selection probability of
German immigrants be, were they to go through the same visa-granting process?
Both types of questions have been largely conflated in the literature, and yet the distinction
is crucial both conceptually and in its practical implications. Conceptually, preference-related
questions are descriptive and aim to reveal latent parameters, namely respondents’ hierarchy of
preferences for profile attributes. Selection-process questions, by contrast, are causal and explore
counterfactual scenarios for profiles that would independently go through the same selection process. Practically, each type of questions involves a specific, distinct quantity of interest. As I will
show, selection-process estimands should capture the composition effects due to how profiles are
paired in the real world, whereas preference-related estimands should not. Therefore, identifying
what type of questions each study is asking is crucial to properly set up the experimental design,
and to choose the appropriate quantity of interest. The distinction between preference-related
and selection-process questions thus provides a framework for thinking about, designing, and
analyzing forced-choice conjoint analyses.
I will show that most studies based on conjoint experiments published in political science explicitly purport to answer preference-related questions. They routinely use Hainmueller et al.’s
(2014) framework and report the quantity proposed by these authors—the average marginal component effect (AMCE). The AMCE measures the effect of one attribute on the selection probability
of profiles. However, whereas the AMCE is appropriate for answering selection-process questions, it is not suited for exploring patterns of preferences. Conceptually, the AMCE is rooted in
a counterfactual logic that aims to estimate causal effects. Practically, it captures two analytically
distinct types of variations: variations due to preferences on the one hand, and variations due to
3

compositional effects related to the attribute distribution of profile pairs on the other hand. In
other words, the AMCE does not identify respondents’ patterns of preferences. The problem is
rooted in the fact that, in forced-choice conjoint experiments, profiles are not simply considered
on their own; instead, they are always selected at the expense of other profiles—a feature that
AMCEs do not fully account for.
Because the AMCE is a selection-process estimand, the vast majority of published articles
implementing forced-choice conjoint experiments do not estimate the appropriate quantity of
interest. Whether the claims made by these papers would hold with the correct estimand is
case-by-case. Conclusions will certainly hold for many, but not all, studies. In light of the surging popularity of forced-choice conjoint experiments in political science, it has become urgent
to reassess and properly define the quantities of interest that can be estimated, and how they
should—and should not—be used. In the case of preference-related questions, I propose a novel
estimand specifically designed to identify patterns of preferences in conjoint experiments, the
average component preference (ACP), and present an estimation method.
The arguments developed in this paper build on intense methodological discussions around
the AMCE, in the wake of Hainmueller et al.’s (2014) foundational paper. De la Cuesta et al.
(2019) have laid out the conditions of the external validity of conjoint analyses using AMCEs,
highlighting the importance of the profile distribution. Abramson et al. (2019; 2020) have
pointed out that, in spite of the common use of conjoint experiments to mirror elections, the
AMCE cannot generally be interpreted as reflecting majority preferences (but see Bansak et al.
2020). Leeper et al. (2019) have shown that, although AMCEs allow researchers to compare
causal effects across subgroups of respondents, they may be misleading when it comes to comparing patterns of preferences across subgroups.
The contributions of this paper are threefold: First, I draw out the distinction between
preference-related and selection-process questions and I spell out the conceptual and practical
implications of this distinction. In so doing, I advance a framework for designing and analyzing
forced-choice conjoint experiments. Second, I show that the AMCE is an estimand suited to
4

selection-process questions; as such, it is not suited to answer the preference-related questions
posed by the vast majority of conjoint studies in political science. Third, I define a novel estimand
that correctly identifies preferences and I provide a method for estimating this quantity.

SET UP
Notations
Following Hainmueller et al.’s (2014) notations, I consider a random sample of n respondents,
indexed by i. Each respondent is presented with one or several choice tasks between two alternatives (profiles), indexed by j ∈ {−1, 1}. Each profile is characterized by a set of L attributes
indexed by ℓ; attribute ℓ can take |Tℓ | levels, noted tℓ ∈ Tℓ = {1, . . . , |Tℓ |}.1 Tijℓ is profile j’s
attribute ℓ presented to respondent i, and the index [−j] (instead of j) denotes the values of the
other profile of the pair. The observed outcome is denoted by Yij and is equal to 1 if respondent
i picks profile j and 0 otherwise. Yij (Tij , Ti[−j] ) denotes the potential outcome of respondent i’s
choice when profiles j and −j are characterized by the vectors of attributes Tij and Ti[−j] , with
Tij ≡ (Tijℓ1 , . . . , TijℓL ). I also define the sub-vector Tij[−ℓ] ≡ Tij \ {Tijℓ }.
I additionally define a vector of respondents’ preferences P. Preferences can be expressed as
the direct pairwise pattern of favorability between levels: each coefficients ptℓ ,t′ℓ is the average
intensity of the preference for tℓ when compared to t′ℓ , and when other attributes are identical
in the pair. It is measured as the deviation of the selection probability from the situation of
indifference, ptℓ ,t′ℓ ≡ E[Yij (Tijℓ = tℓ , Ti[−j]ℓ = t′ℓ )] − .5, where the expectation is taken with
respect to the population of potential respondents. If respondents are indifferent between tℓ and
t′ℓ , the selection probability is .5 and ptℓ ,t′ℓ = 0. If tℓ is chosen with a probability .6, ptℓ ,t′ℓ =
.1 and, symmetrically, pt′ℓ ,tℓ = −.1. For continuous attributes, pℓ is the first derivative of the
selection probability with respect to this the difference in this attribute between two otherwise
identical profiles. When ptℓ ,t′ℓ is expressed as a conditional expectation—i.e., function of observed
outcomes—, I note p̂tℓ ,t′ℓ .
1

By convention, I set |Tℓ | = ∞ if the variable is continuous.

5

Assumptions
I make the same three standard identification assumptions as Hainmueller et al. (2014:8–9).
Specifically, I assume that the ordering of profiles within choice tasks does not affect respondents’ responses and that attributes are randomized across profiles. Profile randomization can
be either completely independent or conditionally independent; in the second case, a limited
number of attributes are jointly defined while the others are independently randomized. I also
distinguish the specific case of uniform randomization, where, within attributes, each level has
the same probability of appearing. If respondents are presented with several choice tasks, I additionally assume that the ordering of tasks within respondents does not affect potential outcomes.
Although the randomization assumption is generally guaranteed by design, the two ordering assumptions may not hold, but Hainmueller et al. (2014) provide straightforward ways to test them.
In any case, these assumptions are inherent to all conjoint designs, and they are unrelated to this
paper’s arguments.

PREFERENCES, SELECTION PROCESS, AND COMPOSITION EFFECTS
Despite the explosion in the number of conjoint studies in political science and the intense
methodological discussion around this experimental design, the literature has yet to acknowledge that conjoint experiments can be used to answer two distinct types of questions, and to
appreciate the practical implications of that distinction.
The first type of questions that scholars implementing conjoint experiments may want to ask
relates to respondents’ patterns of preferences. Studies asking preference-related questions are interested in why respondents favor some profiles over others. They typically try to uncover the
criteria that drive the decision-making process: Do people prefer male or female immigrants? Is
gender more determinant than countries of origin in people’s choices? How do these preferences
differ across subgroups? Here, the choice setting offered by conjoint experiments is but an instrument to reveal and explore respondents’ latent preferences. Hainmueller and Hopkins’s (2015)

6

immigrant experiment is a case in point: the authors are not interested in the United States’ visagranting process itself, which is not actually based on pairwise comparisons; the forced-choice
paired design is just a device that allows them to investigate preferences.
Social scientists could also field forced-choice conjoint experiments to study the selection
procedure itself, that is, not only as a proxy for something else. What I call selection-process questions specifically ask how different attributes affect the probability for profiles to be selected at
the end of the process: How likely is a male immigrant to get a visa? Are female immigrants
more or less likely to get a visa than male immigrants? The focus is thus not on respondents’
criteria of selection, but on the outcome of the selection process from the perspective of the
profiles under scrutiny. While the goal of preference-related questions is essentially to describe
latent parameters, selection-process questions clearly aim to estimate a causal effect.2 Voting conjoint experiments constitute the most obvious case in which scholars may ask selection-process
questions, trying to estimate a candidate vote share, the majority choice, or the probability to
win (Abramson et al. 2019, 2020; Bansak et al. 2020).
The distinction between preference-related and selection-process questions is not merely conceptual, or interpretational; it has practical implications for the analysis of conjoint experiments.
Crucially, the quantity of interest differs depending on the type of questions one asks. When asking preference-related questions, the focus is on the outcome of a specific comparison between
two profiles, within pairs. Were a German immigrant and a Mexican immigrant in the same
pair, how likely is it that respondents choose the former over the latter? When asking selectionprocess questions, on the other hand, the interest is essentially in the comparison between the
(potential) outcomes of profiles that would go through the selection process independently, and
which would not be directly compared in the course of this process. The difference is subtle
2

For examples of the use of descriptive versus causal vocabulary in papers performing conjoint analyses, see
Leeper et al. (2019). Note, however, that the distinction between preference-related and selection-process questions
does not directly map onto Leeper et al.’s (2019) distinction between descriptive and causal questions. Descriptive
and causal questions are both fundamentally interested in respondents’ preferences, but with a slightly different
perspective thereon. In selection-process questions, however, the interest is in the outcome of the selection process
itself, not in preferences.

7

but important: In the first case, we essentially aim to recover the vector of preferences P (or a
statistical summary thereof), so that the quantity of interest is independent of the probability
distribution of profile pairs in the real-world selection process. In the second case, the quantity
of interest depends on preferences, of course, but also on how likely these pairwise preferences
are to be actualized in the selection process under scrutiny, that is, on the real-world probability
distribution of the profile pairs.
To illustrate, consider Hainmueller and Hopkins’s (2015) immigrant experiment again. It
may be that respondents have strong preferences for female immigrants, but the likelihood to be
selected for a woman immigrant going through the process (that is, without knowing beforehand
if the other immigrant she will be compared to is a man or a woman) also depends on how
likely she is to compete against a man, and not another women. Assume that women are always
chosen when compared to men, so that preferences are characterized by pfemale,male = .5. If
most immigrant applicants are men (say, 90%), almost all women (90%) will compete against a
man, and 95% of women will get a visa (.9 ∗ 1 + .1 ∗ .5). The remaining 5% are women who
were competing against other women and for whom there was a tie. Now, if most applicants
are women (90%), most of these women (90%) will compete against other women, and only
55% will get a visa (.9 ∗ .5 + .1 ∗ 1). Preferences are the same in both cases, but as women are
more likely to be confronted to another woman in the latter, they are less often able to leverage
their “gender advantage.” At the limit, if all applicants were women, gender preferences would
not matter at all. If we want to capture respondents’ preferences, these variations correspond to
a purely compositional effect and are therefore irrelevant. But if we want to measure the realworld selection probabilities in the selection process under scrutiny, composition effects matter
and these variations are relevant, even if preferences have remained the same.
For selection-process estimands to accurately capture such composition effects, the real-world
probability distributions of profile pairs should be known by the researcher, and implemented in
the experimental design or used to reweigh the analysis, as highlighted by recent methodological
contributions (De la Cuesta et al. 2019). In many cases, however, these real-world probability
8

distributions are not known for the population of interest. This requirement thus severely limits
the ability to make selection-process claims based conjoint analyses, at least not without relying
on additional, and potentially strong identification assumptions (De la Cuesta et al. 2019).
Actually, the vast majority of political science articles relying on forced-choice conjoint experiments ask preference-related questions. To mention only a two examples, Mummolo and
Nall (2017) seek to “confirm that Democrats are, in fact, more likely than Republicans to prefer
living in more Democratic, dense, and racially diverse places” (abstract); and Carnes and Lupu
(2016) claim that “the average U.S. respondent was actually slightly more likely to prefer the
working-class candidates in [their] experiment over the white-collar ones” (p. 838).3 As detailed
in the supplemental information A, reviewing all 61 conjoint studies published or forthcoming
in six major political science journals since 2014, I was not able to find a single paper using
conjoint experiments to answer a selection-process question. In some cases, all involving voting experiments, the authors’ broader agenda consisted in explaining election outcomes, but the
specific question asked in these papers did focus on preferences as a mechanism for explaining
these outcomes. None of them, in particular, aimed to capture the composition effect specific to
selection-process questions.
This trend is in stark contrast with the recent methodological discussion, which has mostly
focused on the analysis of conjoint experiments in a selection-process perspective. De la Cuesta
et al. (2019) have highlighted the importance of properly specifying the distribution of profiles
so as to correctly calibrate the composition effect and ensure external validity. Abramson et al.
(2020), on the one hand, and Bansak et al. (2020), on the other, have debated the merits of several
quantities of interest in voting conjoint experiments in a typically selection-process perspective,
debating the aggregation rules underlying the various estimands under scrutiny. But these authors do not acknowledge that their own contribution and the bulk of the empirical literature
are using conjoint experiments in different ways, and with different goals. As a matter of fact,
they tend to conflate the two types of questions laid out, using the preference vocabulary while
3

Emphasis added in both quotations.

9

defining selection-process goals and quantities of interest (e.g., the change in the expected vote
share for a political candidate, in Bansak et al. 2020).

THE AMCE AS A SELECTION-PROCESS ESTIMAND
In their influential paper on causal inference in conjoint analysis, Hainmueller et al. (2014) do
not distinguish between preference-related and selection-process questions, and propose an omnibus quantity of interest—the average marginal component effect (AMCE). In my systematic review of all forced-choice conjoint analyses published or forthcoming in six major political science
journals (supplemental information A), I show that virtually all of them ask preference-related
questions, but also that all but two—published right after Hainmueller et al. (2014)—adopt the
AMCE (or a derivative thereof) as their quantity of interest. In this section, however, I show that
the AMCE is a selection-process estimand, not a preference-related estimand, which suggests a
gap between the goal of these articles and the quantity they routinely estimate and report.
Definition
The AMCE is “the marginal effect of attribute ℓ averaged over the joint distribution of the remaining attributes” (p. 10). Formally, the AMCE of tℓ1 with respect to tℓ0 can be expressed as

τ ℓ (tℓ1 , tℓ0 ) ≡ E Yij (tℓ1 , Tij[−ℓ] , Ti[−j] ) − Yij (tℓ0 , Tij[−ℓ] , Ti[−j] )

(Tij[−ℓ] , Ti[−j] ) ∈ (T[−ℓ] × T ) ∩ Tℓ ({tℓ1 , tℓ0 }, Tℓ )

(1)

where Tℓ ({tℓ1 , tℓ0 }, E) is the intersection of the support of P(Ti[−j]ℓ = t′ℓ , Tij[−ℓ] = tj , Ti[−j][−ℓ] =
t[−j] |Tijℓ = tℓ1 , Ti[−j][ℓ] ∈ E) and P(Ti[−j]ℓ = t′ℓ , Tij[−ℓ] = tj , Ti[−j][−ℓ] = t[−j] |Tijℓ = tℓ0 , Ti[−j][ℓ] ∈
E). This condition allows the AMCE to deal with restrictions applied to the randomization
(Hainmueller et al. 2014). Under completely independent randomization, the AMCE can be
non-parametrically estimated as the OLS coefficient of the focal level tℓ1 in a regression of each
profile outcome Yij on all levels of the focal attribute ℓ except the reference level tℓ0 .
Although respondents in forced-choice conjoint experiments compare attributes within pairs
10

of profiles, this definition—as well as other expressions provided in Hainmueller et al. (2014)—
tends to downplay the role of the focal attribute for the second profile of the pair (that is, the
second term of the comparison). To see how the second profile’s focal attribute factors in the
AMCE, I rewrite Equation 1 by decomposing in function of the values of Ti[−j]ℓ :4
τ ℓ (tℓ1 , tℓ0 ) =

X
tℓ ∈Tℓ


P Ti[−j]ℓ = tℓ |tℓ ∈ Tℓ ∩ Tℓ ({tℓ1 , tℓ0 }, Tℓ )
h
× E Yij (tℓ1 , tℓ , Tij[−ℓ] , Ti[−j][−ℓ] ) − Yij (tℓ0 , tℓ , Tij[−ℓ] , Ti[−j][−ℓ] )
(Tij[−ℓ] , Ti[−j][−ℓ] ) ∈ (T[−ℓ] ∩ Tℓ ({tℓ1 , tℓ0 }, {tℓ }))2

(2)
i

This shows that the AMCE uses direct comparisons involving both tℓ1 - and tℓ0 -profiles, but
also indirect comparisons that only include one of the two focal levels, and each comparison
is weighted by its probability of occurrence in the pool of profiles. In each iteration of the sum,
the expectation can be assimilated to a difference in direct pairwise preferences, that is, roughly,
ptℓ1 ,tℓ − ptℓ0 ,tℓ .
A counterfactual estimand
Conceptually, the AMCE is embedded in a causal, counterfactual, framework (Leeper et al. 2019).
Hainmueller et al. (2014) explicitly intend to “integrate conjoint analysis within the potential outcome framework for causal inference” (p. 1). This integration is not purely instrumental, as a way
to get convenient notations; it is central to the definition of the AMCE itself. In fact, Equation 1
shows that the AMCE is the (average) difference between two counterfactuals, defined by two
potential outcomes.
Although potential outcomes are defined based on the entire vector of attributes, the only one
that varies between both counterfactuals—the “treatment”—is the focal attribute ℓ for the j profile.
The value of ℓ for the other profile of the pair, −j, does not even appear as a standalone term and
is lumped together with the remaining attributes. The AMCE basically juxtaposes profiles that
4

This expression is obtained by direct application of the law of total expectation on Ti[−j]ℓ . The proof is omitted.

11

are in distinct pairs, and compared to the same other profile. This suggests that, conceptually,
the focus is on between-pair, not on within-pair, variations. In the immigrant experiment, the
interpretation of a −.024 AMCE for male immigrants is the following: had gender been switched
from female to male, the probability for a randomly selected immigrant to be given a visa would
have been lower by 2.4 percentage points, when compared to the same other profile. The AMCE
is thus primarily designed to measure the likelihood for a profile (e.g., male immigrant) to be
selected, were it to go through the selection-process under scrutiny in the experiment, rather than
to capture the choice criteria—preferences—implemented by respondents, which are expressed
within pairs.
To be clear, this argument does not necessarily invalidate the AMCE as an estimand for answering preference-related questions. However, it shows that the AMCE is not conceptually
designed to do so and, as a result, its interpretation in terms of preferences is not straightforward. The counterfactual structure of the AMCE introduces a reference category (tℓ0 ), but the
comparison between the main and the reference levels is not direct: the AMCE is not the selection probability of the main level when compared to the reference level, it is the difference in the
level-averaged selection probability between both levels. For similar reasons, the inclusion of a
reference level makes it difficult to compare AMCEs across subgroups of respondents (Leeper
et al. 2019).
Compositional effects
As a counterfactual estimand, the AMCE is constructed to be interpreted on its own, as a standalone quantity. In a causal perspective, the relevant comparison is between the two counterfactual quantities embedded in the AMCE; what matters is to make sure that counterfactuals are
correctly defined, and that the estimand can be estimated from the data. When the focus is on
preferences, however, we are not interested in a single counterfactual comparison, but in a holistic pattern. Rather than centering our attention on one estimate at a time, we would typically
interpret and compare the estimates for all levels of a given attribute all together, and then make

12

comparisons between attributes, to determine what the most discriminant attributes are. Estimands for studying preferences thus need to be comparable both within and between attributes.
Unfortunately, AMCEs are generally not fully comparable, and patterns of AMCEs do not
allow researchers to identify patterns of respondents’ preferences. Two issues challenge the identification of preferences with AMCEs.
Conditions.—The first challenge appears in the definition of the AMCE: the condition of the
expectation is a function of the focal levels tℓ1 and tℓ0 (Equation 1). When attributes are independently randomized, the condition can be ignored, but when there are restrictions in the randomization, each AMCE is calculated on a composition of the remaining attributes that is specific
to the levels considered. In the immigrant experiment, for example, the AMCE for doctors vs.
janitors is calculated on profiles of immigrants who have a college degree, whereas the AMCE
for waitors vs. janitors includes profiles of immigrants with no college degree.
As isolated causal quantities, these two AMCEs are internally consistent because the level of
education is standardized for each pair of occupations. But since both AMCEs are conditional on
different levels of education, they can only be compared if we are ready to assume that occupation
preferences are homogeneous across levels of education, or if we do not want to identify separately preferences for occupations and preferences for education. Otherwise, a compositional
effect confounds the pattern of preferences. The same kind of compositional effects challenges
the comparison of AMCEs when the pool of profiles is distributed following a real-world joint
distribution (De la Cuesta et al. 2019); fortunately, the issue disappears if one uses a real-world
marginal distribution instead, or a distribution in which all attributes are independent.
Ties.—The second issue challenges AMCEs’ comparability regardless of the attribute distribution. It flows from the fact that the AMCE is also calculated on pairs of profiles that do not
differ in the focal attribute, that is, when the focal attribute of the second profile is either tℓ1 or
tℓ0 . In the immigrant experiment, for example, the AMCE for gender is calculated on all pairs of
immigrants—mixed-gender pairs as well as same-gender pairs. But such ties carry no informa13

tion on preferences; respondents just could not make a decision between two tied profiles based
on the focal attribute (i.e., ptℓ ,tℓ = 0, and the direct pairwise selection probability is .5), and, in
some ways, the associated potential outcome does not even exist. Yet ties have non-zero weights
in the AMCE.
With ties, the average selection probability of the focal level is shrunk towards .5 regardless
of preferences, and the shrinkage is all the stronger as the share of ties in the pool of profiles
increases. As a result, the AMCE can be shrunk in either direction, and it can even change sign
with respect to an AMCE that would omit ties. Because of ties, each AMCE is bounded by an
interval determined by the share of ties:


± 1 − 1/2 ∗ P(Tijℓ = Ti[−j]ℓ = tℓ1 ) + P(Tijℓ = Ti[−j]ℓ = tℓ0 )

(3)

where I implicitly condition on Ti[−j]ℓ ∈ Tℓ ∩ Tℓ ({tℓ1 , tℓ0 }, Tℓ ).5 In a hypothetical pool of profiles
with no ties, AMCEs vary in [−1; 1]; and the interval gets narrower as the share of ties increases.
The effect of ties on the AMCE is a pure compositional effect; the bounds of the interval are
unrelated to respondents’ preferences and they only depend on the composition of the pool of
profiles. The inclusion of this compositional effect is relevant when one is actually interested
in the causal effect of the selection probability in a specific choice process, at least when the
probability for a profile to be tied reflects the real-world probability of the matching process.
However, it is irrelevant if the question pertains to preferences. In this case, comparisons between
AMCEs bounded by different intervals may be misleading.6
To further illustrate the argument, I focus on uniform randomization, which is by far the most
common setting in the empirical literature. As already noticed by Leeper et al. (2019), AMCEs of
5

The proof is reported in the supplemental information C.
Remarkably, several methodological articles justify the use of the AMCE to capture preferences by referring to
the utility framework, or by trying to give a microfounded or structural interpretation to the AMCE (Abramson
et al. 2019; Bansak et al. 2020). However, they do not go as far as actually comparing the structure of the AMCE
to the structure of preferences in the type of microfounded models they invoke. As a result, they fail to notice that
these models would ignore ties.
6

14

uniform attributes are bounded by ±[1 − 1/|Tℓ |], which only depends on the number of levels of
the focal attribute.7 Adding or removing levels mechanically changes the AMCE. Since bounds
are shared among levels of the same attribute in this case, within-attribute comparisons between
AMCEs can be performed. However, AMCEs for attributes that differ in the number of levels
should not be compared for AMCEs for attributes with numerous levels will be mechanically
bigger than AMCEs for attributes with few levels (in absolute value).
In the immigrant experiment, consider again that women are always preferred over men
conditional on other attributes (pfemale,male = .5), and that German immigrants are similarly
always preferred over non-German immigrants (pgerman,nongerman = .5). Although the selection
probability is 1 in both cases when ties are omitted, the AMCE for women is .5 and the AMCE
for Germans is .9.8 Indeed, AMCEs for gender can take values in the [−.5; .5] interval whereas
AMCEs for countries can take values in the [−.9; .9] interval. It would be incorrect to conclude
that preferences based on the country of origin are stronger than gender preferences; since both
types of preferences are equally strong. And, had we fielded a different version of this experiment,
one in which immigrants could only come from Germany and the reference category, the AMCE
for Germans would have been .5, just like the AMCE for gender.
The bounds provide a rule of thumbs to readers of conjoint studies using AMCEs: under
7
Under uniform randomization, the AMCE can be written as a proportion of an AMCE that omits ties (term in
curly bracket):


 



1
U
E Yij |Tijℓ = tℓ1 , Ti[−j]ℓ ∈ Tℓ \ {tℓ1 } − E Yij |Tijℓ = tℓ0 , Ti[−j]ℓ ∈ Tℓ \ {tℓ0 } ,
τ̂ ℓ (tℓ1 , tℓ0 ) = 1 −
|Tℓ |

which can also be expressed in terms of coefficients of P:









X
X
1
U
τ̂ ℓ (tℓ1 , tℓ0 ) = 1 −
P(Ti[−j]ℓ = tℓ )p̂tℓ1 ,tℓ −
P(Ti[−j]ℓ = tℓ )p̂tℓ0 ,tℓ .

|Tℓ | 


tℓ ∈Tℓ
 tℓ ∈Tℓ

tℓ ̸=tℓ1

tℓ ̸=tℓ0

A proof is provided in the supplemental information C.
8
Applying the expression of the AMCE under uniform randomization, the AMCE for women is (1 − .5)(1 −
0) + .5(.5 − .5) = .5 and the AMCE for German immigrants (1 − .9)(1 − 0) + .1(.5 − .5) = .9. For simplicity, I
further assume that countries other than Germany and the reference category—say, Iraq—have a middling selection
probability: German immigrants are systematically chosen against them, but they are systematically chosen against
Iraqi immigrants.

15

uniform randomization, comparable quantities can be obtained by multiplying the AMCE by
|Tℓ |/(|Tℓ | − 1). In the previous example, this rule gives us measures of preferences equal to 1 in
both cases (.5 ∗ 2 and .9 ∗ 10/9) and leads us to the correct conclusion that the preference for
women is of the same intensity as the preference for German immigrants.
In sum, the AMCE is both conceptually and practically a selection-process estimand. Conceptually, it is a causal quantity based on the comparison of two counterfactuals. Practically, it
does not identify respondents’ preferences and captures a compositional effect that reflects profile pairing and the existence of ties. So, when the pool of profiles replicates the real-world pool
of profiles, it may be a quantity of interest to answer selection-process questions. However, it is
not suited to answer preference-related questions although it is widely used to this purpose.

AVERAGE COMPONENT PREFERENCE FOR MEASURING PREFERENCES
Because neither the spirit nor the letter of the AMCE matches the goal commonly assigned
thereto, I now propose an alternative estimand that is specifically designed to capture preferences.
It overcomes the problems of the AMCE discussed in the previous section, hence I recommend
that scholars asking preference-related questions use it in lieu of the AMCE.
Definition
When exploring alternatives to the AMCE for measuring differences in preferences between subgroups of respondents, Leeper et al. (2019) suggest that scholars simply estimate the selection
probability of the profiles that have the focal level tℓ . By contrast with the AMCE, the selection
probability—or marginal mean, as the authors call it—is not scaled by a reference level (tℓ0 , in
the AMCE notations). It is thus conceptually much closer to an interest in preferences than the
AMCE. However, like the AMCE, it is calculated on contrasting profiles as well as ties. Therefore,
it does not identify preferences and cannot be used as such as a preference-related estimand.

16

Building on Leeper et al.’s (2019) idea, I define the average component preference (ACP) as


π ℓ (tℓ ; wtℓ ) ≡ E Yij (Tijℓ = tℓ , Ti[−j]ℓ ̸= tℓ )|wℓ − .5
X


=
wtℓ (t′ℓ ){E Yij (Tijℓ = tℓ , Ti[−j]ℓ = t′ℓ ) − .5}

(4)
(5)

t′ℓ ∈Tℓ
t′ℓ ̸=tℓ

where wℓ is a vector of level weights subject to

P

t′ℓ ∈Tℓ
t′ℓ ̸=tℓ

wtℓ (t′ℓ ) = 1.9 The ACP is the selection

probability of a tℓ -profile when compared to a non-tℓ -profile, shifted by .5 out of convenience. It
explicitly excludes ties, which do not convey any information on respondents’ preferences. For
tied profiles, indeed, the term in curly brackets in Equation 5 would be equal to zero so that
the quantity would shrink towards zero as well. The quantity is shifted so that all ACPs vary
around zero, a situation in which respondents neither prefer nor reject tℓ over different profiles
on average. The stronger the preference for tℓ , the higher the ACP, and vice-versa. Although
ACPs can technically be interpreted individually, they would better be interpreted holistically, as
an overall pattern of preferences. The smallest ACP corresponds to the level that is, on average,
less liked, and the highest ACP to the level that is preferred overall. The distance between ACPs
indicates the intensity of preferences, and it is standardized across attributes so as to allow for
between-attribute comparisons.
Direct pairwise preferences (ptℓ ,t′ℓ )—the coefficients of the vector of preferences P—would
have constituted an obvious quantity of interest, but because P is typically of high dimension,
they are practically difficult to handle in most cases. Fortunately, the ACP is directly related to
P:
π ℓ (tℓ ; wtℓ ) =

X

wtℓ (t′ℓ )ptℓ ,t′ℓ

(6)

t′ℓ ∈Tℓ
t′ℓ ̸=tℓ

The ACP is simply the average of the coefficients of P involving the focal level tℓ , but omitting
9

For continuous attributes, the ACP can be defined as
∂E[Yij (Tijℓ , Ti[−j]ℓ )]
.
πℓ ≡
∂(Tijℓ − Ti[−j]ℓ )

17

ptℓ ,tℓ . Thus, the ACP is but a statistical summary of P. Equation 6 also shows that a null ACP
does not necessarily mean that respondents are indifferent between tℓ and any other level. It
may alternatively be the case that tℓ is preferred over some of the remaining levels; that still other
remaining levels may be preferred over tℓ ; and that these preferences compensate each other in
intensity. This interpretational complication, shared with the AMCE, disappears when ACPs are
interpreted holistic, as a pattern of preferences, rather than as individual, internally consistent
ACPs.10
In Equations 5 and 6, direct pairwise preferences may be weighted so as to give more importance to some levels than to others, if one has a substantive reason to do so. Each level may
arguably be given a weight reflecting its importance in the real world (De la Cuesta et al. 2019).
Because the question of respondents’ preferences is analytically distinct from the question regarding how often these preferences will be expressed, however, I suggest as a general rule for exploring respondents’ preferences that all levels be assigned the same weight, that is, wtℓ (t′ℓ ) =

1
,
|Tℓ |−1

for all t′ℓ ∈ Tℓ \ {tℓ }. This choice can be understood as a way to standardize the distribution onto
preferences are projected. Other choices are possible, but they should be thoroughly motivated
and discussed.
As defined in Equations 4 and 5, the ACP provides a quantity of interest that is conceptually
in line with preference-related questions, that accounts for the paired dimension of the experimental design, that identifies respondents’ preferences, and that is fully comparable within- and
across attributes.11 That is why I argue that the ACP should be preferred over the AMCE by
scholars primarily interested in respondents’ preferences. When attributes are independently
10

Another feature shared by ACPs and AMCEs to keep in mind during the interpretation is that the preferences
captured by both quantities are intrinsically multidimensional, and they differ from the “pure” preferences measured
in unidimensional settings, where only one attribute varies at once (Bansak et al. 2020). In multidimensional settings,
the relative importance of attributes moderates “pure” preferences so that both so that both are integrated in the
measure of preferences. But the relative importance of attributes is a component of respondents’ preferences, that
is why the fact that ACPs (like AMCEs) capture composite preferences is not inherently good or bad. It depends
on the specific research question, but it should be clear that a conjoint experiment alone cannot separately identify
both components.
11
In the supplemental information B, I define additional quantities of interest specifically designed to explore the
relative discriminatory power of attributes.

18

randomized, Equation 5 can be rewritten as a function of observed outcomes:

π̂ ℓ (tℓ ; wtℓ ) =

X



wtℓ (t′ℓ ){E Yij |Tijℓ = tℓ , Ti[−j]ℓ = t′ℓ − .5}

(7)

t′ℓ ∈Tℓ
t′ℓ =
̸ tℓ

and the ACP can be estimated with data from forced-choice conjoint experiments.
Conditional ACP for implausible profiles
When attributes are not independently randomized, however, the ACP cannot be estimated
with data from forced-choice conjoint experiments, unless there are no interactions between
attributes. This is a relatively strong assumption, that is why I define an additional quantity of
interest, derived from the ACP, which makes it possible to deal with attributes that are not all
independently randomized.
Attributes are not independent at two occasions: (1) when scholars want the pool of profiles
to replicate the real-world joint attribute distribution, and (2) when they want to avoid some combinations of attributes that are considered implausible (e.g., a doctor with no formal education,
in the immigrant experiment). The former case is essentially helpful to weight the interactions
between the focal attribute and the remaining attributes based on their actual probability (De la
Cuesta et al. 2019). By so doing, however, it makes it impossible to make comparisons between
levels, since each level of the focal attribute is associated with a different remaining attribute composition. Preferences are thus mixed with a compositional effect, so that this feature does not
allow for asking preference-related questions.
The second case, however, is more critical because there actually are profiles that just do not
make sense. Because the ACP is not constructed within a counterfactual logic, the empty counterfactual issue AMCEs are subject too is not a concern (Hainmueller et al. 2014), yet identification
is prevented by the lack of independence between attributes. To see why, consider the ACPs for
occupations in the immigrant experiment. The ACP for doctors is an average of all direct pairwise preferences involving a doctor and a different occupation. In particular, one of these direct
19

pairwise preferences concerns doctors (who necessarily have a college degree) and janitors (who
can show any level of education). By naively applying Equation 7, we would compare profiles
who all have a graduate degree to profiles that may or may not have a graduate degree—that
is, two heterogeneous groups. Therefore, this quantity does not identify preferences related to
occupations alone and mixes them with preferences related to the level of education.
Fortunately, profile implausibilities are usually defined on two attributes only, so that attribute
independence holds conditionally on these attributes. For example, occupation is independent
from other attributes conditional on the level of education. This means that we can identify and
compare ACPs for selected levels of education. Hence I define the conditional average component
preference (CACP) as


π ℓ (tℓ ; wtℓ |Tijℓ′ , Ti[−j]ℓ′ ) ≡ E Yij (Tijℓ = tℓ , Ti[−j]ℓ ̸= tℓ )|wℓ , Tijℓ′ , Ti[−j]ℓ′ − .5

(8)

which is identified in experimental data from forced-choice conjoint experiments if, conditional
on ℓ′ , ℓ is randomized independently from other attributes. When analyzing such data with implausible profiles, a set of CACPs of particular interest is calculated conditional on (Tijℓ′ , Ti[−j]ℓ′ ) ∈
(TℓU′ |ℓ )2 , where TℓU′ |ℓ is the set of levels of ℓ′ that are not subject to the restriction with ℓ, that is, the
levels of ℓ′ that are compatible with all levels of ℓ. For the occupation attribute in the immigrant experiment, this corresponds to all levels of education equivalent or superior to a two-year
college degree. ACPs conditional on this set can be calculated for all levels of ℓ, and they are
all defined on the same composition of remaining attributes. As a result, all levels of the focal
attribute can be confidently compared, provided the condition is kept in mind during the interpretation. These CACPs allow scholars to explore patterns of preferences related to immigrants’
occupations among educated migrants.
These CACPs ignore restricted levels of ℓ′ , but ACPs for unrestricted levels of ℓ can be calculated for a less restricted set of pairs so as to integrate information from restricted levels of ℓ′ .
Specifically, CACPs for unrestricted levels of ℓ can be conditioned on “comparable pairs,” that is,
20

pairs that involve two unrestricted levels of ℓ, or a restricted level of ℓ and two unrestricted levels
of ℓ′ . In the immigrant experiment, the ACP for janitor conditional on “comparable pairs” would
be calculated on all janitor profiles compared to waiter, child care provider, gardener, construction worker, teacher, and nurse profiles (regardless of education); and on profiles of janitors
with a college degree compared to financial analyst, computer programmer, research scientist,
and doctor profiles. This condition would exclude pairs involving a janitor with no college degree and either a financial analyst, a computer programmer, a research scientist, or a doctor (all
need to have a college degree). When the focal level if restricted, this condition boils down to
the previous one.
Practically, I suggest calculating CACPs for conditionally independently randomized attributes under both conditions. The first condition should be used to make comparisons among
restricted levels and across restricted and unrestricted levels; the second condition to make comparisons among unrestricted levels. If we are not interested in identifying preferences for the
levels that are not independently randomized, however, comparisons between all levels can be
made based on “comparable pairs” only, keeping in mind the fundamental heterogeneity that
underlies the comparisons.
That we need to define conditional ACPs to deal with implausible profiles is not a weakness
of the ACP with respect to the AMCE; rather, it is a clarification. In fact, the AMCE is by default
defined as a conditional estimand, and the condition is defined on an ad hoc basis, to ensure the
quantity’s internal consistency. Unfortunately, the conditional nature of the AMCE is typically
omitted during the interpretation. For example, Hainmueller and Hopkins (2015) plot all AMCEs obtained under conditionally independent randomization side by side as if they all referred
to the same quantity and if comparisons were all straightforward. By defining an explicitly conditional ACP, distinct from the (unconditional) ACP for independently randomized attributes, I
intend to make the choice of the condition an explicit process, and to ensure that the conditional
nature of the CACP is taken into account when scholars interpret it.

21

Preferences between subgroups
Researchers often want to compare patterns of preferences across subgroups of respondents, such
as men and women, or Republicans and Democrats. Because the ACP is not subject to the same
structural limitations as the AMCE (see Leeper et al. 2019), it can be used directly for comparing
preferences between subgroups, by taking the difference between the ACPs calculated on respondents of each group. When there are implausible profiles, these differences in ACPs are similarly
not identified from experimental data, and differences in CACPs should be considered.

ESTIMATION AND INFERENCE OF THE ACP
In this section, I present a method for estimating ACPs. Proofs of consistency of the estimators
are available, along with Monte-Carlo simulation evidence, in the supplemental information D.
An example of implementation of this method is available online as an R function.
A framework for paired data
I embed the proposed estimation method in a framework specifically designed to account for
the paired nature of data from forced-choice conjoint experiments. In this framework, one observation in the data corresponds to one choice between two profiles, as opposed to one profile
in the AMCE framework. The choice outcome is characterized by a dummy Zi equal to 1 if the
first profile is selected; and each attribute ℓ is defined by two variables Ti1ℓ and Ti2ℓ . By virtue of
randomization, Zi can be equivalently defined as being equal to 1 if the second profile is selected.
For any ℓ and i, a series of variables (Viℓtℓ t′ℓ )(tℓ ,t′ℓ )∈Tℓ2 can be constructed from Ti1ℓ and Ti2ℓ as

Viℓtℓ t′ℓ



if Tijℓ = tℓ , Ti[−j]ℓ = t′ℓ and tℓ =
̸ t′ℓ
1
≡ −1 if Tijℓ = t′ℓ , Ti[−j]ℓ = tℓ and tℓ =
̸ t′ℓ


0
otherwise

(9)

Figure 1 illustrates this procedure in the case of a three-modality attribute: the original pool of
profiles (on the left) allows for the construction of the paired data set (on the right). It is worth
noting that both data sets contain the same information for pairs of profiles that differ in the focal
22


Yij 1{Tijℓ = 1} 1{Tijℓ = 2} 1{Tijℓ = 3}
i
1
1
0
0
1

0
0
0
1
1

0
0
1
0
3

1
0
1
0
3

 ..
.
.
.
..
.
.
.
 .
.
.
.
.

1
0
0
1
N−1

0
0
0
1
N−1

0
1
0
0
N
1
0
1
0
N
(pool of profiles)


j
1



Zi Viℓ12 Viℓ13 Viℓ23
i
−1

1
1
0
1
0
1 



0
−1
0
0
0
2 



−→  .

.. 
.
.
.
.
.
.
.
.
.
.
. 
.
.
.
. 




1
1
0
0
−1
N
−
1


−1
0
1
0
0
N
1
(paired data set)
−1

Figure 1: Pool of profiles and paired data set. i ∈ {1, . . . , N} indexes choice tasks and j ∈ {1, 2} profiles
within choice tasks. Yij is a dummy variable equal to 1 if profile ij is chosen, Zi is a dummy variable equal
to 1 if profile i1 is chosen over profile i2. For k ∈ {1, 2, 3}, 1{Tijℓ = k} is a dummy variable equal to 1
if profile ij’s attribute ℓ takes level k. For (t, t′ ) ∈ {(1, 2), (1, 3), (2, 3)}, Viℓtt′ is a trichotomous variable
equal to 1, −1 or 0 as defined in Equation 9.

attribute. For them, the pool of profiles can be reconstructed from the contrasting data set. For
the pairs that do not differ in the focal attribute, all covariates are fixed to 0 (see the paired data
set’s second row) so that they do not interfere in the estimation of the coefficients of interest. All
in all, we only lose information that is irrelevant for our purpose.
Estimation
In this framework, ACPs can be consistently estimated as
π̂ˆ ℓ (tℓ ; wtℓ ) =

X

wtℓ (t′ℓ )δ̂ tℓ t′ℓ

(10)

t′ℓ ∈Tℓ
t′ℓ ̸=tℓ

where δ̂ tℓ t′ℓ is the estimated average deviation of the selection probability of a tℓ -profile compared
to a t′ℓ -profile from the situation of indifference (where the selection probability is .5, that is).
Noting that δ̂ tℓ t′ℓ = −δ̂ t′ℓ tℓ , the coefficients (δ̂ ℓtℓ t′ℓ )(tℓ ,t′ℓ )∈Tℓ2 can be obtained from a linear regression

23

of Zi on a constant and the series of trichotomous variables (Viℓtℓ t′ℓ )(tℓ ,t′ℓ )∈Tℓ2 :
Zi = α +

X

δ tℓ t′ℓ Viℓtℓ t′ℓ + ΓVi[−ℓ] + εi

(11)

(tℓ ,t′ℓ )∈Tℓ2



where Vi[−ℓ] ≡ V


iℓ′ tℓ′ t′ℓ′

(tℓ′ ,t′ℓ′ )∈Tℓ2′ ,
ℓ′ ∈{1,...,L}
ℓ′ ̸=ℓ

for any i. Preference identification is not contingent on the

inclusion of Vi[−ℓ] in the model, but adjusting for the values taken by the remaining attributes
may improve estimation precision. An interesting feature of this method is that it could also be
used to estimate directly the entire vector of average preferences P; indeed, p̂tℓ ,t′ℓ = δ̂ tℓ t′ℓ . I give an
illustration of this option in the simulation exercise available in the supplemental information D.
When some attributes are conditionally independently randomized, an analogous method
can be applied and CACPs can be estimated as
π̂ˆ ℓ (tℓ ; wtℓ |Tijℓ′ , Ti[−j]ℓ′ ) =

X

wtℓ (t′ℓ )δ̂ tℓ t′ℓ (Tijℓ′ , Ti[−j]ℓ′ )

(12)

t′ℓ ∈Tℓ
t′ℓ ̸=tℓ


where the coefficients δ̂


tℓ t′ℓ

(Tijℓ′ , Ti[−j]ℓ′ )

t′ℓ ∈Tℓ
t′ℓ ̸=tℓ

could be obtained by fitting model 11 restricted to

the pairs of profiles that meet the condition on (Tijℓ′ , Ti[−j]ℓ′ ). The ACP for continuous attribute
is simply obtained as the coefficient of the difference between attribute ℓ in both profiles in the
regression of Zi on this difference and a constant. Finally, the identification assumption tests proposed by Hainmueller et al. (2014) in the AMCE framework can be straightforwardly extended
to the ACP framework.
Inference
Inference for Equations 10 and 12 can be derived by delta method (Bishop et al. 2007:486–501),
simulations (King et al. 2000; Gelman and Hill 2007) or non-parametric bootstrapping (Efron
and Tibshirani 1994). Delta method and simulations would be carried out using the variancecovariance matrix obtained from the estimation of Equation 11. If respondents are asked to
24

examine several pairs of profiles, the estimation of the variance-covariance matrix should also
correct for clustering at the respondent level. In the boostrapping case, clustering should also be
taken into account, by resampling respondents instead of pairs of profiles.

EMPIRICAL ILLUSTRATION
To illustrate the use and interpretation of ACPs, I calculate the ACPs for job experience, job plans,
and occupations in Hainmueller and Hopkins’s (2015) immigrant experiment (Figure 2), using
the R function available online. The occupation attribute is not fully independently randomized,
but only conditional on education; financial analysts, research scientists, doctors, and computer
programmers must also be assigned at least two years of college. In this case, I report ACPs conditional on a college education (unrestricted levels; orange diamonds) as well as ACPs conditional
“comparable pairs” (blue squares).
ACPs’ absolute values can be interpreted if we want to take each ACP as a standalone quantity. For example, the ACP for immigrants with no plans to look for work once in the US is
−21.3; when compared to an immigrant who will look for work or already has a contact with
an employer, immigrants with no plans to look for work have a 28.7% (.5 − .213) chance to be
selected for a visa. On the other hand, immigrants who already have a contract with an employer
have a 66.7% (.5+.167) chance to be selected for a visa when they are compared to an immigrant
who does not have a contract.
Because preferences are relative, a more holistic approach to ACPs generally makes more
sense than interpreting each ACP separately. For each attribute, the pattern of ACPs reflects
the pattern of (average) preferences both in directionality and intensity. For example, respondents strongly prefer immigrants who already have a contract with an employer, and they have
a strong reluctance to grant a visa to those who do not even plan to look for work once in the US.
Immigrants who do not have a contract yet but have plans to find a job in the US occupy an intermediary position between both types, though they are closer to the former type. To interpret
preference intensity, it may be helpful to use other attributes as benchmarks—which is possible
25

Job Experience
None
1-2 years
3-5 years
5+ years
Job Plans:
No plans to look for work
Will look for work
Interviews with employer
Contract with employer
Job:
Janitor
Waiter
Child care provider
Gardener
Financial analyst
Construction worker
Computer programmer
Teacher
Nurse
Research scientist
Doctor
-0.2

-0.1
ACP

0.0

CACP, Comparable pairs

0.1

0.2

CACP, Unrestricted levels

Figure 2: Preferences associated with immigrants’ job experience, job plans, and occupation measured by
average component preferences (ACPs). Bars represent 95% confidence intervals with clustering at the
respondent level. Partial replication of Hainmueller and Hopkins’s (2015) immigrant experiment.

because ACPs are comparable across attributes. As an example, having a contract rather than
just planning on looking for work once in the US has a roughly equivalent effect on respondents’
preferences as having more than five years of job experience rather than no experience at all.
The interpretation is slightly more complicated for conditionally independently randomized
attributes because the CACPs are not all conditional on the same restriction. For the occupation attribute, CACPs represented by orange diamonds are conditional on a level of education
at least equivalent to a two-year college degree, whereas blue diamond CACPs are calculated for
all “comparable pairs,” as I defined them earlier on. There might be differences between them.
For example, while teachers are clearly preferred over construction workers among immigrants
with a college degree, the difference is thin (and not statistically significant) when one includes
immigrants with a lower level of education. There are two approaches here: If we want to examine preferences for occupations specifically—disentangled from preferences for education, that
is—we need to compare ACPs of the same type (same color) only. On the contrary, if we are
ready to take each occupation as a package that includes a specific distribution of education (so
that doctors can be preferred because they have a higher level of education, and not only because
of the job itself), cross-condition comparisons can be made.
Overall, these findings are consistent with the ones obtained with AMCEs; this is reassuring,
but it should not be taken as a general rule. As a counter-example, I reanalyze the data from
Mummolo and Nall’s (2017) experiment, in which the authors asked respondents to choose between two communities characterized by their housing cost, school quality, racial composition,
type of area, among other components. The authors seek to examine the relationship between
people’s preferences (measured by the conjoint experiment, among other things) and people’s
actual moving behavior; their conjoint analysis is thus clearly implemented to measure patterns
of preferences. Here, I focus on school quality and on the type of place, and I do not distinguish
between subgroups of respondents. Figure 3 reports AMCEs and ACPs for both attributes.
Considering each attribute separately, both estimands tell a similar story: respondent prefer
high school quality communities, and small towns or suburban areas over cities and, to a lesser
27

ACP

AMCE

School quality:
5/10
9/10

Type of place:
City – downtown area
City - more residential area
Rural area
Small town
Suburban - only houses
Suburban - downtown area
-0.2

-0.1

0.0

0.1

0.2

-0.2

-0.1

0.0

0.1

0.2

Figure 3: Preferences associated with communities’ type and school quality measured by average component preferences (ACPs) and average marginal component effects (AMCEs). Bars represent 95% confidence intervals with clustering at the respondent level. Partial replication of Mummolo and Nall’s (2017)
community experiment.

extent, over rural areas. Yet, relying on AMCEs, one may want to conclude that the intensity of
the type of place preference is similar to the intensity of the school quality preference, that is,
both attributes are roughly similarly important for respondents. If anything, the former is even
slightly more determinant than the latter, as the AMCE for suburban / downtown area is significantly bigger than the AMCE for a 9/10 school quality. ACPs point to a different conclusion,
however; the range of school quality ACPs is substantially wider than the range of type of place
ACPs, which suggests that school quality matters significantly more—not less—for respondents
than the type of place. When exploring preferences, the ACP is the appropriate quantity of interest, so that the predominance of school quality over the type of place is the correct conclusion.
Here, AMCEs under-estimate the importance of school quality. Specifically, they are misleading
because there are more profiles tied on the school quality attribute (about 50%) than on the type
of place attribute (about 17%). Mummolo and Nall’s (2017) argument goes beyond these two attributes and their main conclusions still hold, fortunately; what this example illustrates, however,
is that the AMCE is not a reliable quantity for answering preference-related questions.

CONCLUSION
In this article, I reassessed the quantity of interest when investigating preferences with forcedchoice conjoint experiments, and proposed a framework to think about the design and the analysis of such experiments. The literature using and studying conjoint experiments has tended to
mix two distinct goals that can be assigned to conjoint designs, namely the exploration of patterns of preferences and the estimation of the causal effect of attributes on the outcome of the
selection process. This distinction between types of research questions is analytically important,
but it is also of crucial practical relevance. As I argue, each goal entails a specific quantity of
interest—and yet virtually all studies have adopted the AMCE as an omnibus estimand regardless of the research question. If the AMCE is certainly a quantity of great interest in studies asking
selection-process questions, it is not suited for answering preference-related questions. Conceptually, there is a mismatch between describing patterns of preferences, on the one hand, and a
29

counterfactual estimand embedded in a causal framework. And technically, the AMCE conflates
respondents’ preferences and compositional effects that essentially depend on the experimental
design, not on preferences.
As a result, because the AMCE does not identify preferences, scholars should limit the use
of AMCEs to non-paired or non-forced-choice conjoint designs (e.g., Jasso 2006; Flores and
Schachter 2018), or to forced-choice designs that seek to answer questions that are genuinely
concerned with the selection process operationalized in the experiment, for itself and not as
an instrument to measure preferences. For studies interested in respondents’ preferences, I defined a novel estimand—the average component preference (ACP). The ACP accurately captures
preferences and can be identified with data obtained from canonical forced-choice conjoint experiments. It can also be used to compare preferences between subgroups of respondents.
Because the ACP fully accounts for the paired dimension of the data by systematically comparing the two profiles of a pair instead of pooling them all, however, the extension to designs
that compare more than two profiles is not straightforward. Yet, when it comes to investigating
preferences, the advantage of such a design over a simple paired design remains to be shown.
Not only are designs with more than two options unnecessary for identifying respondents’ preferences, they may also be more cognitively demanding on respondents, resulting in estimates
that are noisier and more complicated to interpret.

REFERENCES
Abramson, Scott F, Korhan Kocak, Asya Magazinnik, and Anton Strezhnev. 2020. “Improving Preference Elicitation in Conjoint Designs Using Machine Learning for Heterogeneous Effects.” Working Paper. https://polmeth.theopenscholar.com/files/polmeth2020/files/
polmeth_magazinnik.pdf.
Abramson, Scott F, Korhan Koçak, and Asya Magazinnik. 2019. “What Do We Learn About
Voter Preferences From Conjoint Experiments?” Working Paper. https://scholar.princeton.
edu/sites/default/files/kkocak/files/conjoint_draft.pdf.
Bansak, Kirk, Jens Hainmueller, Daniel J. Hopkins, and Teppei Yamamoto. 2020. “Using Conjoint
30

Experiments to Analyze Elections: The Essential Role of the Average Marginal Component
Effect (AMCE).” Working Paper. https://cpb-us-w2.wpmucdn.com/web.sas.upenn.edu/dist/
f/49/files/2020/04/amce_04302020.pdf.
Bishop, Yvonne M., Stephen E. Fienberg, and Paul W. Holland. 2007. Discrete Multivariate Analysis: Theory and Practice. New York: Springer-Verlag.
Carey, John M., Kevin R. Carman, Katherine P. Clayton, Yusaku Horiuchi, Mala Htun, and Brittany Ortiz. 2018. “Who Wants to Hire a More Diverse Faculty? A Conjoint Analysis of Faculty
and Student Preferences for Gender and Racial/Ethnic Diversity.” Politics, Groups, and Identities 8:535–553.
Carlson, Elizabeth. 2015. “Ethnic Voting and Accountability in Africa: A Choice Experiment in
Uganda.” World Politics 67:353–385.
Carnes, Nicholas and Noam Lupu. 2016. “Do Voters Dislike Working-Class Candidates? Voter
Biases and the Descriptive Underrepresentation of the Working Class.” American Political
Science Review 110:832–844.
De la Cuesta, Brandon, Naoki Egami, and Kosuke Imai. 2019.
“Improving the
External Validity of Conjoint Analysis:
The Essential Role of Profile Distribution.”
Working Paper.
https://scholar.princeton.edu/negami/publications/
improving-external-validity-conjoint-analysis-essential-role-profile.
Efron, Bradley and R. J. Tibshirani. 1994. An Introduction to the Bootstrap. CRC Press.
Flores, René D. and Ariela Schachter. 2018. “Who Are the “Illegals”? The Social Construction of
Illegality in the United States.” American Sociological Review 83:839–868.
Gelman, Andrew and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel / Hierarchical Models. Cambridge University Press.
Hainmueller, Jens, Dominik Hangartner, and Teppei Yamamoto. 2015. “Validating Vignette
and Conjoint Survey Experiments against Real-World Behavior.” Proceedings of the National
Academy of Sciences 112:2395–2400.
Hainmueller, Jens and Daniel J. Hopkins. 2015. “The Hidden American Immigration Consensus:
A Conjoint Analysis of Attitudes toward Immigrants.” American Journal of Political Science
59:529–548.

31

Hainmueller, Jens, Daniel J. Hopkins, and Teppei Yamamoto. 2014. “Causal Inference in Conjoint Analysis: Understanding Multidimensional Choices via Stated Preference Experiments.”
Political Analysis 22:1–30.
Hankinson, Michael. 2018. “When Do Renters Behave Like Homeowners? High Rent, Price
Anxiety, and NIMBYism.” American Political Science Review 112:473–493.
Jasso, Guillermina. 2006. “Factorial Survey Methods for Studying Beliefs and Judgments.” Sociological Methods & Research 34:334–423.
King, Gary, Michael Tomz, and Jason Wittenberg. 2000. “Making the Most of Statistical Analyses:
Improving Interpretation and Presentation.” American Journal of Political Science 44:347–361.
Leeper, Thomas J., Sara B. Hobolt, and James Tilley. 2019. “Measuring Subgroup Preferences in
Conjoint Experiments.” Political Analysis 28:1–15.
Mummolo, Jonathan and Clayton Nall. 2017. “Why Partisans Do Not Sort: The Constraints on
Political Segregation.” The Journal of Politics 79:45–59.

32

