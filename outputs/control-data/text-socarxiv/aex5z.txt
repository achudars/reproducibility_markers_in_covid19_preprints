I Saw You in the Crowd:
Credibility, Reproducibility, and Meta-Utility

Nate Breznau
breznau.nate@gmail.com
University of Bremen
CRC 1342 ‚ÄúThe Global Dynamics of Social Policy‚Äù

Abstract
Crowdsourcing enables novel forms of research and knowledge production. It uses cyberspace to
collect diverse research participants, coordinate projects and keep costs low. Recently social
scientists began crowdsourcing their peers to engage in mass research targeting a specific topic.
This enables meta-analysis of many analysts‚Äô results obtained from a single crowdsourced
research project, leading to exponential gains in credibility and scientific utility. Initial
applications demonstrate positive returns for both original and replication research using various
research instruments, and secondary or experimental data. It can provide more reliable Bayesian
priors for selecting models and is an untapped mode of theory production that greatly benefit
social science. Finally, in addition to the credibility and reproducibility gains, crowdsourcing
embodies many core values of the Open Science Movement because it promotes community and
equality among scientists.

1

Crowdsourcing is a new word. Google Books‚Äô first recorded usage of ‚Äúcrowdsourcing‚Äù is 19991.
It originally referred internet users adding content to websites, the largest example being
Wikipedia (Zhao and Zhu 2014). As a scientific practice, its roots go back to open engineering
competitions in the 1700s if not earlier, where the collective ideas of many scientists overcame
otherwise insurmountable problems. Openly tapping into a large population breaks down barriers
of time, money and data. The solution is simple. Some centralized investigator or team poses a
question or problem to the crowd. Sometimes tasks are co-creative, like the programming of
packages by users of R or Stata, or the development of Wiki content. Other times they are
discrete like asking individuals to donate computing power2, respond to a survey or perform
specific tasks. Using crowdsourced methods, scientists, citizens, entrepreneurs and even
governments more effectively address society's most pressing problems such as cancer or global
warming (Howe 2008).
Although the public are the typical crowd, new landmark studies involved researchers
crowdsourcing other researchers. The human genome project for example involved major byline
contributions from almost 3,000 researches. In the social sciences, crowdsourcing of researchers
is brand new. In 2013, Silberzahn et al. (2018) called together researchers from across disciplines
and geographic locations to analyze the same dataset to discover if football referees gave more
red cards to darker skinned players3. Klein et al. (2014) sourced labs across the world to try and
reproduce several high profile experimental psychological studies. In the Crowdsourced
Replication Initiative, Breznau, Rinke and Wuttke et al. (2019) demonstrated that crowdsourcing
is also useful for replication, structured online deliberation and macro-comparative research.
Studies like are a paradigm change within political and social research.
In the knowledge business, crowdsourcing is highly cost effective. The average
researcher lacks the means to survey or conduct experiments with samples outside of
universities. Crowdsourcing platforms such as Amazon‚Äôs Mechanical Turk (‚ÄòmTurk‚Äô) changed
this by creating access to even-more-convenient convenience samples from all over the world
(Palmer and Strickland 2016). Others use these platforms to crowdsource globally competitive
labor to perform paid tasks of scientific value (Berinsky, Huber, and Lenz 2012). Blockchain
technology uses decentralized crowd computing. Many things crowds already do such as
‚ÄòTweeting‚Äô or ‚Äògeotagging‚Äô images, offer free possibilities to test hypotheses of social and
political relevance (Nagler and Tucker 2015; Salesses, Schechtner, and Hidalgo 2013).
Wikipedia had fantastic success given the strong willingness of publics around the world to add

1

As of July 26th, 2019.
For example: https://www.worldcommunitygrid.org/, or cryptocurrency using blockchain technology.
3
Also known as ‚Äúsoccer‚Äù in a minority of countries.
2

2

and monitor content. The Wiki-model is the basis for all kinds of crowd creations4. There is a
Wikiversity5 crowdsourcing teaching and the ReplicationWiki6 for listing and identifying
replications across the social sciences (H√∂ffler 2017). In a similar vein, there are efforts to
crowdsource open peer review and crowdsource ideas for what researchers should study from
potential stakeholders outside of science.
Some might question why many researchers are necessary when there are pre-existing
datasets and a single scholar can run every possible statistical model configuration, and when
technologies of machine learning are so advanced. When a single scholar or machine throws all
possible variables into their models like a ‚Äòkitchen sink‚Äô, they tend to be highly effective at
maximizing their goal of prediction. There is a subtle but crucial difference between the human
approach where, ‚Äòif ùëã predicts (or perhaps does not predict) ùëå, then their relationship might be of
interest‚Äô (Leamer 1978), and the machine approach where, ‚Äòif ùëã predicts ùëå, then ùëã is useful‚Äô.
However, when humans or machines start running every possible model as is the typical machine
learning strategy, they implicitly test every possible hypothesis and potentially every possible
theory. Many models imply a data-generating process that is theoretically impossible, like
predicting biological sex as an outcome of party affiliation or right-party votes in 1970 from
GDP in 2010. If we want something more than causal inference, human supervision and causal
logic is necessary to rule out impossible realities that a computer cannot identify on its own
(Pearl 2018).
Crowdsourcing is epistemologically different. It is not a ‚Äòkitchen sink‚Äô method. The
landmark Silberzahn et al. study demonstrated that seemingly innocuous decisions in the data
analysis phase of research can change results dramatically. The effect of prior information and
beliefs, the idiosyncratic features of researchers in other words, are something Bayesian
researchers raised warning signals about long ago, e.g., (Jeffreys [1939]1998). This is echoed in
modern discussions about the unreliability and irreproducibility of research (Dewald, Thursby,
and Anderson 1986; Gelman and Loken 2014; Wicherts et al. 2016). However, grappling with
this uncertainty is problematic. Identification of informative priors usually involves consulting an
expert such as the researcher doing the work, or using simulations wherein the user selected
parameters may or may not reflect reality. The big objection here is that the distribution of prior
beliefs across researchers is unknown or at least speculative at best (Gelman 2008). The
Silberzahn et al. and Breznau et al. studies offer a real observation of these priors because all
researchers should develop a theoretically causally plausible model for answering the research
4

Just check the growing list on the Wikipedia crowdsourcing projects page!
https://en.wikipedia.org/wiki/List_of_crowdsourcing_projects
5
https://en.wikiversity.org/wiki/Teaching_and_Learning_Online
6
http://replication.uni-goettingen.de/wiki/index.php/Main_Page

3

question given the data. This goes even further than posterior model selection, because the
crowdsourced researchers scrutinized the other researchers‚Äô models prior to seeing the results.
This practice is extremely useful because it is not one expert guessing about the relative value of
a model but potentially one hundred of them, or more.
The reduction of model uncertainty is good from a Bayesian perspective, but also
demands theory from a causal perspective. Statisticians are well aware that without the correct
model specification, parameter estimates are also incorrect with respect to the research question.
Thus, rather than having 8.8 million false positives after running 9 billion different regression
models (Mu√±oz and Young 2018), humans can identify correct, or at least ‚Äòbetter‚Äô, model
specifications using causal theory and logic (Clark and Golder 2015; Pearl 2018). The diversity
of results in the Silberzahn et al. and Breznau et al. studies helped identify key variables and
modeling strategies that had the ‚Äòpower‚Äô to change the conclusions of any given replication and
appear to change the predictive process leading to red cards or leading to a significant negative
effect of immigration on social policy preferences. These were not just variables among millions
of models, but variables among carefully constructed plausible models. This shifts the discussion
away from the results and even the Bayesian priors, to the nature of the data-generating model
and its specific components. This requires human logic and theory.
Machine learning is perhaps not entirely a-theoretical. It is almost always supervised by
humans. Imagine a machine develops a policy to solve world hunger that requires global
socialization of food production. Interesting idea, but entirely out of line with international
political realities and the market forces that motive food production. Human arbiters instead can
guide machines to plausible solutions that fit with political or financial realities faced in the
government and private sectors. But machines cannot apply discriminating logic, or can they?
The U.S. Military‚Äôs Defense Advanced Research Projects Agency (DARPA) finds the human
versus machine question so important that it currently funds the SCORE project (Systematizing
Confidence in Open Research and Evidence) pitting machines against research teams in
reviewing the credibility of research across disciplines. The SCORE architects believe that
machines might be capable of determining the credibility of social and behavioral research better
‚Äì faster and more cost effective ‚Äì than humans. As social scientists we should naturally be
skeptical of this hypothesis (Grimmer 2015). In this case, crowdsourcing provides DARPA the
exact method it needs to answer this question. This project is the largest crowdsourcing of social
researchers to date, with already over 500 participating7. It is a major demonstration that
crowdsourcing can bring together the interests of academics and policymakers.

7

https://www.darpa.mil/program/systematizing-confidence-in-open-research-and-evidence
4

At the heart of the SCORE project are questions of credibility and reliability. These
topics are also at the center of scandals and conflicts infecting science at the moment that is
rapidly increasing researchers and funding agencies‚Äô interests in replication (Eubank 2016;
Ishiyama 2014; Laitin and Reich 2017; Stockemer, Koehler, and Lentz 2018). But there are
limits to what replications offer, and replications take on diverse formats and goals (Clemens
2015; Freese and Peterson 2017). Currently, the decisions if, what and how to replicate is
entirely a researcher‚Äôs prerogative. Few engage in replications at all meaning that any given
original study is lucky to have just one replication8. Journals have thus far been hesitant to
publish replications, especially of their own publications, even if these replications overturn
preposterous sounding claims such as precognition (Ritchie, Wiseman and French 2012), or
identify major mistakes in the methods (Gelman 2013; Breznau 2015). Crowdsourcing could
change this because journals may see this as cutting-edge research rather than just another
replication (Silberzahn and Uhlmann 2015).
Perhaps more importantly, crowdsourced replications offer reliability at a meta-level.
Replications, experimental reproductions and original research alike, suffer from what Leamer
(1978) refers to as ‚Äúmetastatistics‚Äù problems. Again researcher degrees of freedom are the issue.
If researchers exercise their degrees of freedom such that ostensibly identical research projects
come to different results, say more than 5% of the time, than any one study is not reliable by
most standards. Breznau, Rinke and Wuttke (2018) simulated this problem and deduced that 4-7
independent replications are necessary to get a majority of direct replications, meaning simple
reproductions, arriving at similar effect sizes within 0.01 of an original study in a 95%
confidence interval. Later, the results from their crowdsourced project show that even after
correcting major mistakes in code, 11% of the effects were substantively different from the
original: as in a change in significance or direction (ibid. 2019). S√∏ndergaard (1994) reviewed
replications of the Values Survey Model employed by Hofstede, one of the most cited and
replicated studies in social science, and found that 19 out of 28 replications (only 68%) came to
roughly the same relative results. Lack of reproducibility is not restricted to quantitative
research. For example, Forscher et al. (2019) investigated thousands of National Institutes of
Health grant reviews and found that using the typical 3-5 reviewer approach achieves a 0.2 interrater reliability. Increasing this to an extreme of 12 reviewers per application reaches only 0.5
reliability. Both scores are far below any acceptable standard for researchers evaluating the same
data.
There is much to learn about why researchers come to different results. It appears that
intentional decisions are only one factor (Yong 2012); and that context, versioning and

8

Hofstede‚Äôs Values Survey, the observer effect and the backfire effect are notable exceptions.

5

institutional constraints also play a role (Breznau 2016; Breznau, Rinke and Wuttke 2019).
Crowdsourcing addresses this meta-uncertainty because PIs can hold certain research factors
constant such as the method, data and exact framing of the hypothesis, thus exponentially
increasing the power to both discover why researchers results differ and meta-analyze a selected
topic (Uhlmann et al. 2018). Combined with the rapidly expanding area of specification curve
analysis, crowdsourcing provides a new way of increasing credibility for political and social
research both at the population and meta-level of the researchers themselves (Simonsohn,
Simmons and Nelson 2015; Rohrer, Egloff and Schmukle 2017). Hopefully these developments
are tangible outcomes that help increase public, private and government views of social science.
The relevance of crowdsourcing for collaborative theory construction is an untapped but
promising avenue for the future. In the Breznau et al. study, online deliberation using the Kialo
platform with close PI moderation provided a means for discussing model uncertainty with
subjective ranking of arguments by participants. The Kialo program guides participants to where
their attention is most needed, like moving the higher ranked arguments to the top of the tree
structure, jumping to yet unevaluated claims and alerts for new arguments. This platform could
be used to construct theory. Using online deliberation to develop theory would depart from the
current system that favors individualism by rewarding the novelty of individual researchers,
going instead toward a system of consensus building and direct responsiveness to theoretical
claims. It could resolve the perpetual problem of scholars, areas and disciplines talking ‚Äòpast‚Äô
each other. It would also provide a leap forward technologically, as we currently see ‚Äòcollective‚Äô
theory construction in a primitive format among conference panels or journal symposia. These
are limited to a few invited participants and involve a specific event in space and time. This is a
narrow collaborative process in the face of structured crowdsourced deliberations where
potentially thousands of participants can engage somewhat at their convenience over many
months. In Kialo for example, participants vote on the veracity of others‚Äô positions, thus they can
contribute to an ongoing debate without having to write any arguments themselves. These
debates can remain permanently in the public sphere with a stable URL. Kialo also provides
extensive data for analyzing deliberative processes and outcomes9.
The benefits of crowdsourcing are not only on the scientific output or theoretical side.
Both the Silberzahn et al. and Breznau et al. studies had structured interaction among
researchers. This fostered community engagement and exchange of ideas and methodological
knowledge. The Breznau et al. study specifically asked participants about their experiences,
finding that learning and enjoyment were common among them. For example, 69% reported that
participation was enjoyable, 22% were neutral, and only 7% found it not enjoyable. Of 188

9

This is not to say that Kialo is the only option, but it seems to be one of the best suited for crowdsourcing.

6

participants the retention rate was 87% form start-to-finish. This demonstrates motivation across
the spectrum of PhD students, postdocs, professors and non-academic professionals.
Crowdsourcing potentially requires a large time investment on behalf of the participating
researchers. This initial evidence suggests that they may personally develop during the process,
in addition to contributing to answering socially-relevant questions. Crowdsourcing may not be a
high density method, but it seems sustainable as an occasional largescale project.
The practice of crowdsourcing researchers embodies many principles amenable to the
Open Science Movement, such as open participation, transparency, reproducibility and better
practices. All researchers who are willing and able may participate in a crowdsourced project,
thus it tends to promote access across institutional, private/public and geographic boundaries.
The three crowdsourced projects mentioned herein all relied heavily on transparent research
practices taking advantage of the Open Science Framework. It takes extensive work to prepare
and share data and instructions with several researchers in a setting where mistakes are serious
problems due to their propagation on a meta-level. This naturally makes it easy to take the next
step and make the whole project transparent and reproducible. The Breznau et al. study posted a
pre-analysis plan to help safeguard against hypothesizing after results are known (HARKing). In
the two projects involving data analysis, researchers came together from across disciplines to
work collaboratively, something rare and potentially helpful to the social sciences.
A key principle of Open Science and crowdsourcing also has to do with incentive
structure. The status attainment of scholars is theoretically infinite, limited only by the number of
people willing to cite that scholar‚Äôs work. This leads to ego-maniacs and destructive behaviors
(S√∏rensen 1996). This is the status quo of academia. In a crowdsourced endeavor, the
crowdsourced researcher participants are equal. There is no chance for cartelism, veto or
exclusionary practices so long as the principal investigators (PIs) are careful moderators. Thus,
in its ideal form, crowdsourced projects should give authorship to all participants who complete
the tasks assigned to them. The PIs of a crowdsourced endeavor naturally have more to gain than
the participants because the research is their brainchild and they will be known as the PIs in the
academic community. None the less, they also have more to lose in case such a massive
implementation of researcher human capital was to fail, thus the participants have good reason to
trust the process. The net gains should be positive for everyone involved in a well-executed
crowdsourced project. The beauty of this vis-√†-vis the Open Science Movement is that the
current publish-or-perish model is turned on its head into something positive, where all benefit
by getting a publication, and any self-citation thereafter is actually a community good.
Crowdsourcing is not without boundaries. There is a limited supply of labor.
Crowdsourcing of a replication or original research essentially asks researchers to engage in a
project that may require as much empirical work as one of their ‚Äòown‚Äô projects. Moreover, the
7

newness of this method means that researchers may not see the value in investing their time.
Researchers may want to consider (a) the novelty of an original crowdsourced research study and
(b) whether the proposed project fits with open and ethical science ideals before contributing
their precious time. Finally, they should consider (c) the importance of the topic. In the case of
replications, part of the topic is to end the replication crisis. Here, Bates (2016) suggests a
formula. He argues that priority for ending the crisis can be measured as influence (citations)
divided by evidence (number of replications, where zero might logically be 0.01). The larger the
number, the higher the replication priority. However, I suggest extending this formula to include
socio-political relevance, and to determine this, let the crowd decide what is most useful to
replicate. The results can rank studies in terms of impact, and then use these rankings as a
weighting coefficient on the right-hand side of this equation. This would offer the possibility to
both end the replication crisis and solve pressing societal problems.

8

References
Bates, Timothy. 2016. ‚ÄúWhat Should We Replicate? Things That Will End the Replication Crisis.‚Äù
Medium Op-Ed. https://medium.com/@timothycbates/what-should-we-replicate-things-that-willend-the-replication-crisis-cb09ce24b25f
Berinsky, Adam J, Gregory A Huber, and Gabriel S Lenz. 2012. ‚ÄúEvaluating Online Labor Markets for
Experimental Research: Amazon.Com‚Äôs Mechanical Turk.‚Äù Political Analysis 20(3): 351‚Äì368.
Brady, David, and Ryan Finnigan. 2014. ‚ÄúDoes Immigration Undermine Public Support for Social
Policy?‚Äù American Sociological Review 79(1): 17‚Äì42.
Breznau, Nate. 2015. ‚ÄúThe Missing Main Effect of Welfare State Regimes: A Replication of ‚ÄòSocial
Policy Responsiveness in Developed Democracies‚Äô by Brooks and Manza.‚Äù Sociological Science
2: 420‚Äì441.
Breznau, Nate. 2016. ‚ÄúSecondary Observer Effects: Idiosyncratic Errors in Small-N Secondary Data
Analysis.‚Äù International Journal of Social Research Methodology 19(3): 301‚Äì318.
Breznau, Nate, Eike Mark Rinke, and Alexander Wuttke. 2018. ‚ÄúPre-Registered Analysis Plan for ‚ÄòHow
Many Replicators‚Äô Experiment‚Äù. Mannheim Centre for European Social Research. SocArXiv
https://osf.io/hkpdt.
______. 2019. ‚ÄúHow Reliable Are Replications? Measuring Routine Researcher Variability in MacroComparative Secondary Data Analyses.‚Äù Forthcoming preprint, SocArXiv.
Breznau, Nate, Eike Mark Rinke, and Alexander Wuttke et al. 2019. Crowdsourced Replication Initiative:
Executive Report. Mannheim Centre for European Social Research. SocArXiv
https://osf.io/preprints/socarxiv/6j9qb/.
Clark, William Roberts, and Matt Golder. 2015. ‚ÄúBig Data, Causal Inference, and Formal Theory:
Contradictory Trends in Political Science?: Introduction.‚Äù PS: Political Science & Politics 48(1):
65‚Äì70.
Clemens, Michael A. 2015. ‚ÄúThe Meaning of Failed Replications: A Review and Proposal.‚Äù Journal of
Economic Surveys 31(1): 326‚Äì342.
Dewald, William G., Jerry G. Thursby, and Richard G. Anderson. 1986. ‚ÄúReplication in Empirical
Economics: The Journal of Money, Credit and Banking Project.‚Äù The American Economic Review
76(4): 587‚Äì603.
Eubank, Nicholas. 2016. ‚ÄúLessons from a Decade of Replications at the Quarterly Journal of Political
Science.‚Äù PS: Political Science & Politics 49(2): 273‚Äì276.
Forscher, Patrick, Markus Brauer, William Cox, and Patricia Devine. 2019. How Many Reviewers Are
Required to Obtain Reliable Evaluations of NIH R01 Grant Proposals? . PsyArXiv Preprint.
https://psyarxiv.com/483zj.
Freese, Jeremy, and David Peterson. 2017. ‚ÄúReplication in Social Science.‚Äù Annual Review of Sociology
43(1): 147‚Äì165.
Gelman, Andrew. 2013. ‚ÄúEthics and Statistics: It‚Äôs Too Hard to Publish Criticisms and Obtain Data for
Republication.‚Äù Chance 26(3): 49‚Äì52.
______. 2008. ‚ÄúObjections to Bayesian Statistics.‚Äù Bayesian Analysis 3(3): 445‚Äì49.
Gelman, Andrew, and Eric Loken. 2014. ‚ÄúThe Statistical Crisis in Science.‚Äù American Scientist 102(6):
460.
Grimmer, Justin. 2015. ‚ÄúWe Are All Social Scientists Now: How Big Data, Machine Learning, and
Causal Inference Work Together.‚Äù PS: Political Science & Politics 48(1): 80‚Äì83.
H√∂ffler, Jan. 2017. ‚ÄúReplicationWiki - Improving Transparency in the Social Sciences.‚Äù D-Lib Magazine
23(3/4).
Howe, Jeff. 2008. Crowdsourcing: How the Power of the Crowd Is Driving the Future of Business. New
York, NY: Random House Business Books.
Ishiyama, John. 2014. ‚ÄúReplication, Research Transparency, and Journal Publications: Individualism,
Community Models, and the Future of Replication Studies.‚Äù PS: Political Science & Politics
9

47(1): 78‚Äì83.
Jeffreys, Harold. 1998. Theory of Probability. 3rd ed. New York: Clarendon Press/Oxford: Oxford
University Press.
Klein, Richard A. et al. 2014. ‚ÄúInvestigating Variation in Replicability: A ‚ÄòMany Labs‚Äô Replication
Project.‚Äù Social Psychology 45(3): 142‚Äì52.
Laitin, David D, and Rob Reich. 2017. ‚ÄúTrust, Transparency, and Replication in Political Science.‚Äù PS:
Political Science & Politics 50(1): 172‚Äì175.
Leamer, Edward E. 1978. Specification Searches: Ad Hoc Inference with Nonexperimental Data. New
York: Wiley.
Mu√±oz, John, and Cristobal Young. 2018. ‚ÄúWe Ran 9 Billion Regressions: Eliminating False Positives
through Computational Model Robustness.‚Äù Sociological Methodology 48(1):1-33.
Nagler, Jonathan, and Joshua A. Tucker. 2015. ‚ÄúDrawing Inferences and Testing Theories with Big
Data.‚Äù PS: Political Science & Politics 48(1): 84‚Äì88.
Palmer, Joshua C., and Justin Strickland. 2016. ‚ÄúA Beginner‚Äôs Guide to Crowdsourcing: Strengths,
Limitations and Best Practices.‚Äù Psychological Science Agenda.
https://www.apa.org/science/about/psa/2016/06/changing-minds.
Pearl, Judea. 2018. ‚ÄúTheoretical Impediments to Machine Learning with Seven Sparks from the Causal
Revolution.‚Äù ArXiv preprint. https://arxiv.org/abs/1801.04016
Ritchie, Stuart J., Richard Wiseman, and Christopher C. French. 2012. ‚ÄúFailing the Future: Three
Unsuccessful Attempts to Replicate Bem‚Äôs ‚ÄòRetroactive Facilitation of Recall‚Äô Effect.‚Äù PLOS
ONE 7(3).
Rohrer, Julia M, Boris Egloff, and Stefan C Schmukle. 2017. ‚ÄúProbing Birth-Order Effects on Narrow
Traits Using Specification-Curve Analysis.‚Äù Psychological Science 28(12): 1821‚Äì1832.
Salesses, Philip, Katja Schechtner, and C√©sar A. Hidalgo. 2013. ‚ÄúThe Collaborative Image of The City:
Mapping the Inequality of Urban Perception.‚Äù PLOS ONE 8(7).
Silberzahn, R et al. 2018. ‚ÄúMany Analysts, One Data Set: Making Transparent How Variations in
Analytic Choices Affect Results.‚Äù Advances in Methods and Practices in Psychological Science
1(3): 337‚Äì356.
Silberzahn, Raphael, and Eric L. Uhlmann. 2015. ‚ÄúCrowdsourced Research: Many Hands Make Light
Work.‚Äù Nature 526: 189‚Äì191.
Simonsohn, Uri, Joseph P. Simmons, and Leif D. Nelson. 2015. ‚ÄúSpecification Curve: Descriptive and
Inferential Statistics on All Reasonable Specifications.‚Äù SSRN Working Paper.
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2694998.
S√∏ndergaard, Mikael. 1994. ‚ÄúResearch Note: Hofstede‚Äôs Consequences: A Study of Reviews, Citations
and Replications.‚Äù Organization Studies 15(3): 447‚Äì56.
S√∏rensen, Aage B. 1996. ‚ÄúThe Structural Basis of Social Inequality.‚Äù American Journal of Sociology
101(5): 1333‚Äì65.
Stockemer, Daniel, Sebastian Koehler, and Tobias Lentz. 2018. ‚ÄúData Access, Transparency, and
Replication: New Insights from the Political Behavior Literature.‚Äù PS: Political Science &
Politics 51(4): 799‚Äì803.
Uhlmann, Eric Luis et al. 2018. ‚ÄúScientific Utopia: III. Crowdsourcing Science.‚Äù PsyArXiv.
https://psyarxiv.com/vg649.
Wicherts, Jelte M et al. 2016. ‚ÄúDegrees of Freedom in Planning, Running, Analyzing, and Reporting
Psychological Studies: A Checklist to Avoid p-Hacking.‚Äù Frontiers in Psychology 7: 1832.
Yong, Ed. 2012. ‚ÄúReplication Studies: Bad Copy.‚Äù Nature News 485(7398): 298.
Zhao, Yuxiang, and Qinghua Zhu. 2014. ‚ÄúEvaluation on Crowdsourcing Research: Current Status and
Future Direction.‚Äù Information Systems Frontiers 16(3): 417‚Äì434.

10

