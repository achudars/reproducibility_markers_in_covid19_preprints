Preprint of the published article: Felicitas Macgilchrist (2019) Cruel optimism in edtech: when the digital data
practices of educational technology providers inadvertently hinder educational equity, Learning, Media and
Technology, 44:1, 77-86, DOI: 10.1080/17439884.2018.1556217

Cruel optimism in edtech: When the digital data practices of educational
technology providers inadvertently hinder educational equity
Felicitas Macgilchrist
Media|Transformation Department, Georg Eckert Institute for International Textbook Research,
Braunschweig, Germany; Institute for Educational Science, University of Goettingen, Goettingen,
Germany. Email: macgilchrist@leibniz-gei.de; Twitter: @discoursology.
As digital data become increasingly central to education, hopes for educational equity
are pinned more strongly on educational technology providers. This paper examines the
data practices of edtech providers who are not simply making token gestures towards
justice and equality. Drawing on ethnographic interviews and Berlant’s notion of cruel
optimism, it presents three data stories. The paper suggests that datafication in education
provides a showcase of cruel optimism, i.e., when the object of desire is blocking one’s
flourishing. The conclusion considers the constitutive paradoxes of datafied education,
and implications for education in the current phase of edu-technical transformation.
Keywords: educational technology, datafication, data practices, inequality, ethnography

‘[E]veryone needs enhanced digital skills to participate fully in society’ (European
Commission 2014: 3; see also Working Group 2017: 15). Governments and philanthropists
worldwide strive to provide access to technology in the belief that overcoming what has been
called the ‘digital divide’ will ameliorate disadvantage and increase educational equity. Yet it
is evident that the way digital technology is used today is (perhaps inadvertently) strengthening
existing socio-economic inequalities. Studies on technological change and social inequality
have identified correlations between differential access, usage, attitudes, skills, discourses,
structures, and infrastructures and a range of individual and societal domains, including life
chances, civic engagement, health care, political activism, health behaviour, entrepreneurship,
social capital, age, education, disability, gender, ‘race’ and class (ITU 2017a, 2017b; Robinson
et al. 2015; van Deursen and van Dijk 2013).

1

Simply providing fast broadband or high-quality hardware and software has not created
a ‘pipeline to prosperity’ (Eynon and Huw in press). Instead, it has exacerbated entrenched
inequalities and created new challenges (DiMaggio and Garip 2012; Eynon and Geniets 2016;
Warschauer 2003).
Today, these new challenges include the inequalities enacted by ‘data practices’.
Algorithmic security assessments target specific subgroups, whose applications are
systematically denied (Leurs and Shepherd 2017). When statistical models predict which
children are at risk of abuse, human experts defer to these computerised risk assessments
(Eubanks 2018). Venture capitalists push algorithmic decision-making as the basis for
personalised learning; predictive analytics are being proposed that could help to discourage
lower-income students ‘at risk’ of dropping out of university from going in the first place
(Watters 2017). ‘Digital redlining’, i.e., the categorisation of students into classed bodies with
differential access to information through regulation, tracking and filtering, actively limits the
opportunities of some (mainly working class) students (Gilliard and Culik 2016). A growing
body of work on data activism explores ways that existing power relations are being challenged
by mobilising data to enhance social justice (Fritsch 2018; Gutiérrez 2018; Kennedy 2018).
In between algorithmic control mechanisms and data justice, an open question remains
how those working ‘on the ground’, developing the personalised learning technologies or
advocating for better data practices, describe their data practices. This paper draws on extended
ethnographic interviews to ask how actors in educational technology talk about data practices.
Although many clearly make token gestures towards equity and social justice, the focus in this
paper is specifically on those who voice a deep commitment to data ethics and to increasing
equality. How do they express this commitment? How does this aid our understanding of the
hopes pinned on technology in the datafied present?

2

Concepts and methods
‘Datafication’ is understood here as the rendering of myriad forms of information about
education, in particular about learning, into ‘machine-readable digital data, which can then be
subjected to sophisticated forms of processing, calculation, analysis, interpretation,
visualisation and circulation’ (Williamson 2018, xv).
An ethnographic approach was adopted to explore how edtech organisations working
at describe their own approach to digital data. This paper emerges from a study in which openended, ethnographic interviews were conducted with twelve stakeholders in data-based
educational technology for schools (K-12) in the USA in 2017, including CEOs and CTOs of
successful start-ups, managers at major educational publishing houses (now rebranded as
‘learning companies’), and advocates for a better understanding and use of student data. The
goal of this paper is not a systematic typology of different data practices, but an in-depth look
at three ‘rich points’. Rich points, drawing on Agar (2006), are those moments which surprise,
confuse or intrigue the ethnographer. As noted above, previous studies have observed the
cynicism and market orientation of edtech providers. A similar stance was observed among
some interviewees (see Macgilchrist 2017). Rich points emerged as it became clear that three
interviewees’ references to pedagogical care, ethics and equity were not being made as
tokenistic gestures.
However, the goal of this paper is also not to identify straightforward ‘good practices’
among educational technology providers. Instead, taking an ethnographic sensibility means
assuming that data practices will be ‘messy’ (Lather 2010). The goal is to identify tensions
which point to broader issues in contemporary society; to complexify understandings of data
practices and the potential effects of datafication by taking a close look at specific cases and
engaging with the stories told by the people involved. Although, for instance, digital data ‘can’
be subjected to sophisticated forms of processing, etc (as Williamson 2018 writes), in practice,

3

the use of data is often relatively unsophisticated (Selwyn 2016). The descriptive question for
this paper is thus how these individuals express a commitment to ethics and equity. The
analytical question is how this complexifies our understanding of the hopes pinned on
technology to enable educational equity.
To analyse the interviews, the concept of ‘cruel optimism’ became relevant. Berlant
describes ‘cruel optimism’ as those moments in which something we desire is actually
hindering our ability to attain it (Berlant 2011, 1): Optimistic is the animating, sustaining,
energizing belief in ‘the good life’, and in the struggles and change required to reach this good
life: in the case of educational technology, the good life would be the equity and social justice
enabled through the use of digital technology. This optimism is cruel when it is tied to fraying
fantasies of the good life, e.g. when people remain attached to the fantasies of romantic love,
upward mobility or solidarity of political systems despite their fragility (Berlant 2011, 21).
Berlant traces the conditions under which affective attachments to fantasies of the good
life remain powerfully attractive, even as they block the satisfactions they seem to offer to
individuals and collectives (2011, 13). Cruel optimism is a concept for thinking about the
‘chaos of the present moment’ (2011, 260) of ordinary lives ‘disorganized’ by capitalism in its
many contemporary enactments (2011, 8) but when catch-all concepts like ‘neoliberalism’ do
not offer enough complexity, nuance or ambivalence (for analyses of cruel optimism in
education, see, e.g., Moore and Clarke 2016; Rasmussen 2013; Zembylas 2016).
The following section presents three data stories to illustrate three enactments of cruel
optimism in educational data practices: (i) generating data to close the achievement gap, (ii)
protecting data to ensure privacy, and (iii) using data to expose inequity. The conclusion
reflects on what these stories mean for education in the current phase of edu-technical
transformation.

4

Data stories
Data story 1: Generating data to close the achievement gap
The first data story concerns a for-profit data-driven literacy platform. The website says it ‘is
built around self-paced and collaborative learning, freeing teachers to truly personalize
instruction for individuals and small groups’; ‘Lessons are differentiated for each learner’s
readiness level so teachers can challenge and inspire students with “just right” texts and tasks’.1
The website cites several independent reports and case studies from schools which demonstrate
that students using the platform outperformed students using other literacy approaches: The
platform helped students ‘achieve an extra year’s worth of growth in one school year’, achieve
‘1.5-2.5 average years of grade-level growth when compared to NWEA2 grade-level norms’,
and gain ‘6.29 test-score points above the control group on the NWEA MAP test’. This latter
finding is presented as the equivalent of ‘closing the achievement growth gap by: 264% for
Low-Income Students, 456% for Black Students and 749% for Hispanic Students’.
During an interview, the CEO, Nancy (pseudonym), a former English teacher and then
Director of Curriculum and Instruction for approx. 115 schools, refers not to the ‘achievement
gap’ but to the ‘opportunity gap’.
B: But the most innovative things are still those very tactical things. Like a teacher being able
to give feedback during class time. Most English teachers have never done that because it's
really hard to do. Because you've got all these little munchkins running around. You know? So/
I: Yeah. But it's something that's always wanted?
B: But it has tremendous outcomes on the other hand. It's the number one way to close the
opportunity gap is to give rapid feedback. It's the number one thing we can do. Doing it during
class time is the only way we can actually do it. But it's literally never conceived of as a thing
you should be doing during your class time. (interview 2017-03-28_US06_83).

1

To retain the relative anonymity of the interviewees, the sources of descriptions are not provided. All
information is from the websites and/or interviews conducted in 2017.
2

This refers to standardized assessment tools available from the not-for-profit organisation NWEA.
5

Nancy prioritizes pedagogical practices which provide opportunities. Core practices for
fostering student success are everyday tactical innovations like giving rapid feedback during
class time. This practice is integrated into the platform’s design: The colour-coded data
dashboard gives teachers a quick overview. It makes grading faster by automating the reading
assessment and by giving a standards-aligned rubric for teachers to assess writing within the
platform so that students see the feedback within the class.
Asked how the platform uses data about individual students, Nancy responds: ‘I think
that that picture not only of what performance levels you've attained, but your growth over time
and quite honestly the topics you've been exposed to, that's the real richness’ (interview 201703-28_US06_49):
Because, you know, if I said to you what's a main idea, you would be able to tell me the
definition as an educated person. If I asked you to read a paragraph on baroque music, you
would be able to tell me the main idea of that paragraph. If you know nothing about
baroque music you won't be able to tell me what you've read. You'll be able to point out
the feature that was the main idea, that would be clear to you. But what those words mean,
you may or may not understand. And that's where topical knowledge is actually one of the
things that is very uneven. Our assessments [in the USA] are supposed to be generic, but
they're not. […] If you're a student who just got here from Mexico and never heard of the
Civil War, it doesn't mean you can't read, it doesn't mean you can't think. It means you've
never heard of Abraham Lincoln, you have no idea who the Confederates are. You don't
know who won, you don't know/ Like it's just, it's lack of exposure to actual content
knowledge that is usually the biggest source of inequity in knowledge. (Interview 201703-28_US06_49-51)

Since assessments in the USA assume a shared nationally specific knowledge base, this content
plays a large role in ‘inequity in knowledge’. The platform aims to teach students how to
identify a text’s ‘main idea’ even if they have not been exposed to the topic, and it aims to
expose students to the central topics that are likely to be assessed. In this way, it aims to support
students who are often marginalized by the school system to achieve as well as their

6

mainstreamed peers. Overall, by offering tools to enact tactical innovations, the platform is
designed to function as a ‘scaled-up’ version of continuous professional development (CPD)
for the teachers who use it (interview 2017-03-28_US06_5).

Reflection 1: The cruel optimism of generating data to close the achievement gap
The optimism seems clear here: the numeric data generated through empirical research show
the benefit of using this literacy platform, especially for disadvantaged students; data
dashboards support teachers; individual students engage with assessment-relevant content
knowledge. The optimism becomes ‘cruel’ when it involves attachment to the belief that
closing an ‘achievement gap’, and equalizing or levelling the school system, will lead to greater
equality across society.
Understanding ‘educational equity’ as closing the gap on test achievements among
subgroups of students (generally minority and nonminority students) has been sharply critiqued
by a range of scholars (Noguera and Rios 2012; Milner 2013; Ladson-Billings 2006; Carter
and Welner 2013). The metaphor of the achievement gap is often associated with a discourse
of individual or community deficit, since it presupposes that it is the student who achieves or
does not. The metaphor ‘disguises the accumulation of societal and educational exclusions of
and prejudices toward historically marginalized students, their families, and their communities
that leads to the so-called gaps’ (Scheurich et al. 2017, 508f.).
Nancy does not reproduce this deficit discourse. Although the website taps into the
discourse of closing the ‘achievement growth gap’, she consistently refers to opportunities and
the ‘opportunity gap’ (interview 2017-03-28_US06_31, 55, 85). This concept shifts attention
from educational outputs (performance) to inputs (resources). It shifts responsibility from the
individual to the structural opportunities made available to students who have historically been
excluded or disadvantaged by the school system. Nancy prioritizes several systemic ‘gaps’
which undergird differences in individual test achievement: She talks about teacher quality,
7

teacher training and a challenging curriculum (see Irvine 2010, p. xii, cited in Scheurich et al.
2017). These factors are internal to the educational system. However, further entrenched
inequalities arguably remain outwith the remit of any edtech company: ‘the school funding
gap; […] the wealth and income gap; the employment opportunity gap; the affordable housing
gap; the health care gap; the nutrition gap; the school integration gap; and the quality childcare
gap’ (ibid.).
A relation of cruel optimism appears in the fantasies projected onto personalised
learning technologies like this literacy platform. Governments, philanthropists and parents
worldwide are channelling hopes and finances into hardware and software which aim to redress
the achievement gap. Advocates for a more effective use of educational data aim to give
policymakers ‘the evidence they need to direct scarce resources in ways that truly work for
students’ (Data Quality Campaign 2017). When enthusiasm for data-driven technology focuses
attention on unachievable fantasies of an equitable good life within contemporary regimes of
capital, it diverts attention from other ethical, social, political and economic questions about
contemporary education (Rasmussen 2013). Instead, the fantasy of equality is projected onto
socio-technical mediators that enable minor interruptions in unjust practices but arguably
disable major political or economic transformation (Berlant 2011, 25).

Data story 2: Protecting data to ensure privacy
The second data story draws from an interview with Amelia (pseudonym), the CEO of a notfor-profit learning platform for mathematics. It describes itself as a comprehensive curriculum
for maths, designed to cover the whole school year, replacing the textbook.
I asked whether parents of teachers have raised privacy concerns, given the amount of
data required to personalise the individual digital lessons and give teachers an overview of

8

students’ learning. The question was met with a passionate response highlighting ethics and
values.
So, you know, I'm a mom, and, you know, I think about if there was like some app out
there collecting information on my children, I would find the server firm and burn it to the
ground. So, like, I think the people who are vigilantly protecting children and focused on
potentially crazy views on data, I largely agree with them. Right? So, I'm coming with that
mind-set of do not touch children's data. Do not profile children. I mean selling children's
information is like revolting to consider. But even developing a full profile of an actual
child where you can attribute any personal information to that child is/ that's revolting to
think about. So I start there, which is that I agree with what might be the kind of left-wing
nut jobs who don't want student data pulled together. My head of product lab also agrees.
So, she and I just start with that mind-set. (Interview 2017-04-07_US10_79)

Later in the interview as we continued to speak about data and privacy, she critiques her peers
in the edtech community who ‘whine’ about the challenges of encoding privacy into the
technology. For her, ‘this whole contortion people make around student data privacy interfering
with edtech is like just total bullshit’ (interview 2017-04-07_US10_85).
That would be like a toy manufacturer being like, “It's so hard not to put chemicals on
these toys that we make. This is so hard! We're so upset about these rules.” That's
disgusting, that's a disgusting mind-set. Your mind-set should be, “My own baby could put
this in their mouth. And so I will do my absolute best with like the best intentions in my
heart to make this safe.” And that is a value, right? (Interview 2017-04-07_US10_87)

The language and specificity here break with the smooth responses of some other interviews,
where the interviewees assured me that they of course take data privacy very seriously, and
then swiftly moved on to other topics or waited for the next question. Amelia’s responses
indicates a level of annoyance with this debate (and perhaps with the interviewer’s naïve
questions) that speaks to a deep commitment to protecting the data generated by the platform
to ensure the young users’ privacy.

9

This ethical commitment translates into the company’s business, technical and legal
practices. Amelia emphasises several times that complying with legislation and safeguarding
data is simply ‘the cost of doing business with children’ (interview 2017-04-07_US10_85).
Her company has no problem showing academic outcomes and personalising learning ‘but it
is much more tricky and complex’. Problems require more complex solutions, partnering with
apps is more complex. ‘And we just take that complexity as a cost of doing business’ (interview
2017-04-07_US10_85). ‘We just take a lot of extra steps. We spend money and effort and we
take a lot of extra steps’ (interview 2017-04-07_US10_89). The task of edtech companies is,
in her view, to take this cost and ‘lead on it’ (interview 2017-04-07_US10_85).
Leading on it means engaging with the legislation. In other interviews I had heard about
instances of company lawyers negotiating with schools or districts to make clear that a data
privacy agreement would hinder their ability to work. Occasionally, the lawyers manage to
effect changes to the agreements to make them more workable (interview 2017-0317_US01_102). Amelia agreed with these other interviewees that many current privacy laws
are not practicable but distanced herself from this practice.
So, we have no interest in like skirting student data, doing the bare minimum, pushing the
laws, framing them/ Like a lot of laws make no sense. And clearly the people who wrote
them have never even looked at an Excel database. And so what you're sharing when
people talk about, “Well, we need to baselines to figure out gains and we need to profile
to be able to personalize,” that's fair. And a lot of this, a lot of the laws kind of don't make
sense. But the thing that we put on top of that is that's fine that the laws don't make sense,
but like actually, we agree with where they're trying to get to. Their ultimate goal is dead
right. So, basically, we do back bends to follow it and to lead on it. (Interview 2017-0407_US10_79)

Key to this data story are ownership rights. ‘So, here's the big idea, here's the most important
idea: We don't own that data. It's not ours. It belongs to the school’ (interview 2017-0407_US10_97). Almost two million students were using the product at the time of the interview.
10

Apart from the case studies on the website, the company releases no information about the
schools, districts or states that they are working with (interview 2017-04-07_US10_83).
Amelia tells me of funders who say things like: ‘Hey we're interested in Michigan. Will you
send us all the schools that use your product in Michigan?’. And this edtech provider will say
no (interview 2017-04-07_US10_83).
This also means schools maintain control over the future of the data. If a school asks
them to delete the entire account, the company will check back: We do a hard delete, nothing
will be retrievable, please confirm before we proceed. And if the school confirms, Amelia
considers this their ‘business obligation’ to delete the data. Compare this, she says, to a credit
card company, which will certainly not delete your data if you cancel your credit card, because
the company owns the data, not the customer. In this sense, and while acknowledging its flaws,
she praises the Californian SOPIPA legislation because the ‘most important’ thing now is
moving forward towards school-centred ownership (interview 2017-04-07_US10_10).
Leading on privacy also includes finding technical solutions. Asked about whether
certain things are not possible if privacy is engineered into the software in a serious way, which
is a view I had heard in previous interviews, she answers, ‘I mean you can do anything, right?
So, I think it's all about doing things smartly’ (interview 2017-04-07_US10_97). She describes,
for instance, how they double-blind anonymise the data so that even she herself cannot find the
names of individual students. And she disputes those in the edtech field who claim that student
data privacy interferes with learning: These are not ‘real’ scenarios. It does increase the
complexity, but: ‘If you're really trying to solve that problem, you can solve it’ (interview 201704-07_US10_99).

Reflection 2: The cruel optimism of protecting data to ensure privacy
Four dimensions of society’s optimism meet in this data story. First, that values can be
translated into ethical socio-technical practice. Second, that other edtech providers (including
11

for-profit firms) can also see data privacy as a cost of business. Third, that legislation is moving
forward in the right direction, and can safeguard future data practices. Finally, that technical
solutions can be found to data-relevant problems. These four together should ensure that the
student data generated through edtech are not exploited, that education remains a public good,
and that students are not treated primarily as revenue-generating data servants. This optimism
becomes cruel precisely because of the sense of possibility it creates for the wider world: that
student data not be exploited, and young people not seen as sources of profitable data.
In this story, the school, where young people spend much of their waking lives, is a
protected space. However, young people also live, first, within the everyday commercial space
of global technology firms whose revenue stems primarily from the legitimated exploitation of
user data for advertising. They live, second, within the political space of illicit data-mining and
data analysis, flagged by, for instance, the Cambridge Analytica case in which a political
consulting firm used profile data from 50 million Facebook users to target users with political
campaign messages during the 2016 presidential election in the USA. And, third, young people
attend schools within an educational policy space in which ‘digital redlining’ limits their
opportunities to participate actively in public life (Gilliard and Culik 2016).
An optimism which focuses on the use of technology in classrooms, or in schools,
neglects how these broader spaces in which public education unfolds are intimately interwoven
with technology. This optimism hints at a new dimension of post-democracy (Crouch 2004;
Stalder 2015). Crouch (2004) argued that professional politicians, business elites and capitalist
economic interests are now making decisions behind closed doors that were previously made
with the active participation of ordinary people. The content of history curricula and textbooks,
for instance, has been a frequent site of public debate and political contestation (Fuchs and
Bock 2018). Today, alongside the political and business elite, the technological elite also
increasingly makes decisions that shape public life. In education, no matter how good the

12

motives, and how pedagogically well-founded the decisions, it is a post-democratic moment
when the ability to make these decisions has shifted from publicly accountable government
officials, policy-makers or educators, to developers, programmers, designers and other staff in
private edtech organisations.3
As I noted above, this is not to criticise the not-for-profit company which designs a far
better and more just privacy policy, embedded in a thoughtful pedagogical approach, than many
other technology providers. It is, however, to reflect on the cruelty of society’s optimism that
focussing attention on protecting young people’s data (in schools) can repair or resist the
commercially-oriented ‘datavaillance’ (Clarke 1988) in their everyday data-saturated lives. In
this data story, ethical data policy and practice may help us survive the constant monitoring of
our lives through mainstream technology. Amelia’s passion for privacy marks a respite offered
in this educational space; but it also unfolds within a broader sociotechnical move towards
post-democracy (see Berlant 2011, 117, 122; Stalder 2015).

Data story 3: Using data to expose inequity
The third data story begins with the ethical challenges of data practices which are not illegal
but seem inappropriate. Sena (pseudonym), a staff member of a national non-profit
organisation which advocates for effective data use, said that in these situations, her
organisation tries to ‘get people more involved in decision-making and understanding why data
is being used for different purposes’ (interview 2017-03-21_US03_156). The goal is for people
to be more informed, to appreciate the uses that benefit them, and to have a platform to disagree
with those uses that seem detrimental. When I ask who tends to get involved in these
discussions, assuming it may be higher income families who have more time an financial

3

Projects such as the Schulbuch-O-Mat propose open source alternatives which prioritise public
participation, collective deliberation and community control over the algorithms for adaptive processes
(www.schulbuch-o-mat.de).
13

resources to spare, she replies:
Sometimes the biggest champions are actually lower income families, because they know
that they've been receiving inequitable educational opportunities. So they can/ If they are
sure that the data's being used appropriately, not to sort of punish their child, but/ they can
often be real champions. Because they see it as a way to expose inequities. (Interview
2017-03-21_US03_158)

To my question about whether data have actually been used to address entrenched inequalities,
Sena says ‘Hopefully’: She sees good data practices being developed at state level that uncover
inequities, but there are few systems in place that lead to significant changes in situated
practice. In one example she mentions, the school system in Washington, D.C., analysed its
data to create an equity report card for each school. They found that schools were giving harsher
discipline to African American students than to white students. Those in charge looked at the
data, formed a task force and are working to make a change. ‘But’, she added, ‘that's more of
an outlier than it hopefully will be’ (interview 2017-03-21_US03_162). Reacting to my
comment that ‘there could be a lot of use of data for that [sort of change], but it has to be pushed
by somebody, right?’, she reflected:
Yeah. Yeah, it's always hard when the data shows something bad or difficult. That's where
it's really hard to take action. Because/ And I think that's why sometimes people are
hesitant to use data, because if you don't see something, it's not a problem. But once you
see it, you might have to do something about it. (Interview 2017-03-21_US03_170)

In this data story, people hesitate to use data, not because it is impossible to address structural
inequalities, but because it is too possible, albeit incredibly difficult, to take action.

Reflection 3: The cruel optimism of using data to expose inequity
In this data story, the visibility of data has an ontological function: it brings problems into
existence. As Žižek (2014) argues in another context, we ‘didn't really learn anything from
14

WikiLeaks we didn't already presume to be true – but it is one thing to know it in general and
another to get concrete data’. The data make something a problem that requires action. They
become toxic (Berlant 2011, 24). Optimistic is the belief that data can be used to expose
inequality and bring about change. The story identifies ‘forms of data, data tools and data
techniques’ that are being used ‘to possibly empower otherwise sub-ordinated groups’ (Selwyn
2016, 66). But this optimism can seem cruel when the responsibility is placed on ‘otherwise
subordinated groups’ or individuals to do the exposing.
It is widely recognised that some students are treated more harshly than others. The
apparent objectivity and indisputability of quantification now produces inequality and racism
as problems for observers and stakeholders who did not previously feel an obligation to act
(see Prinsloo and Slade 2017). At the same time, these are human stories. It takes families or
individuals in government offices to use data in ways which expose inequalities. This equality
aspect of rendering data actionable is not encoded into the software. Human decision-making
is overlaid on the machine-readable data. Using data becomes an act that ‘create[s] new modes
of sense perception and induce[s] novel forms of political subjectivity’ (Rancière 2004, 9; see
also Williamson 2016).
In this data story, using data to expose inequity extends participation in the governing
of education. Families are armed with politicised data visualizations of their experience of
inequitable educational opportunities. However, this individualises the responsibility to
become informed and expose injustice. Given the volumes of investor capital that flow towards
the design and implementation of data technologies compared to the resources for interventions
– even those that are only ameliatory – at ground level, this becomes a relation of cruel
optimism. The visibility of data implies an obligation for officials to act. It operates as a
technology of subjectivation of individual actors, creating activist citizens. But it also

15

establishes new norms (of individual responsibility) against which parents or staff can evaluate
themselves, and potentially find themselves failing, worn out and frustrated.

Concluding thoughts
Overall, the paper aimed to illustrate the workings of optimistic attachments in education, as
the ‘sustaining inclination to return to the scene of the fantasy that enables you to think that
this time, nearness to this thing, will help you or a world to become different in just the right
way’ (Berlant 2011, 2, italics in original). The data stories point to the constitutive paradoxes
of education which are ‘sticky’ and cannot be cleaned away. The stories of generating data,
protecting data and using data show hope for change. They also show how the fantasy of
equality is projected onto a socio-technical mediator (a personalised literacy platform, data
privacy practices, an active parent armed with data visualisations) which enables a small
interruption in inequality, but also blocks attention to the weakening of the fantasy of an
equitable life in today’s increasingly post-democratic world.
The three stories reflect on the relations of cruel optimism across education today. The
stories highlight a selection of actors that are not only paying lip service to goals of equality,
justice, participation. At the same time, their optimism for a better future through edu-technical
interventions (whether for-profit or non-profit) is intimately entangled with today’s
predominant global economic structure. The optimism is shared by many of those making
policy decisions on what to fund, who to support, which kinds of data infrastructures,
technologies and practices to push forward. When hardware and software are being financed
as the means to close the achievement gap, protect privacy and expose inequalities, this often
passionate attachment to technological solutions blocks interventions in historically rooted,
structural inequalities.
But at the same time, these stories highlight spaces in which pedagogical relationships,
care and solidarity are made more relevant than in many other spaces dominated by data
16

technologies. The belief that education (or edtech) can change socio-economic inequalities is
an ‘animating, sustaining fantasy’, indicative of the ‘exhausting pragmatics’ of the messy,
hyperconnected, always-on world of today (Berlant 2011, 261). Even if justice is impossible
through education (see Rasmussen 2013), a cruelly optimistic education which values equality
as a utopian goal makes school more bearable for its users than educational technology that
place profitability before all other values. This suggests that optimism is necessary to survive
the impasse of educating in the world today, even if it cannot fix the problems of inequality.
The concept of ‘cruel optimism’ helps us – at least, that is the optimism of this article – to think
about the fragile fantasies involved in ‘thinking otherwise’ about the datafied present, while
also remembering that cruel optimism ‘is better than no optimism at all’ (Berlant 2011, 16).

Disclosure statement
No potential conflict of interest was reported by the author.
Notes on contributor
Felicitas Macgilchrist is Head of the ‘Media|Transformation’ department at the Georg Eckert
Institute of International Textbook Studies, Braunschweig, and Professor of Media Research
at the University of Goettingen, Germany. Her current research draws on ethnography,
discourse studies and cultural theory to explore the socio-political implications of educational
technology. ORCID: http://orcid.org/0000-0002-2828-4127

References
Agar, Michael. 2006. "An ethnography by any other name." Forum: Qualitative Social
Research 7 (4):Art 36.
Berlant, Lauren. 2011. Cruel Optimism. Durham: Duke University Press.
Biesta, Gert. 1998. "Say You Want a Revolution...Suggestions for the Impossible Future of
Critical Pedagogy." Educational Theory 48 (4):499-510.
17

Carter, Prudence, L., and Henry Welner. 2013. Closing the Opportunity Gap: What America
Must Do to Give Every Child an Even Chance. Oxford: Oxford University Press.
Clarke, Roger. 1988. "Information Technology and Dataveillance." Accessed 15 March 2018.
http://www.rogerclarke.com/DV/CACM88.html.
Crouch, Colin. 2004. Post-Democracy. Cambridge: Polity.
Data

Quality

Campaign.

"DQC

Fact

Sheet."

Accessed

15

March

2018.

https://2pido73em67o3eytaq1cp8au-wpengine.netdna-ssl.com/wpcontent/uploads/2016/03/DQC-Fact-Sheet-2017-08302017.pdf.
DiMaggio, Paul, and Filiz Garip. 2012. Network Effects and Social Inequality. Annual Review
of Sociology 38 (1):93-118.
European

Commission.

Digital

Agenda

for

Europe

2014.

Available

from

http://europa.eu/pol/index_en.htm http://europa.eu/!bY34KD.
Eubanks, Virginia. 2018. Automating Inequality: How High-Tech Tools Profile, Police, and
Punish the Poor: St. Martin's Press.
Eynon, Rebecca, and Anne Geniets. 2016. The digital skills paradox: how do digitally excluded
youth develop skills to use the internet? Learning, Media and Technology 41 (3):463-479. doi:
10.1080/17439884.2014.1002845
Eynon, Rebecca, and Davies Huw. in press. Is digital upskilling the next generation our
'pipeline to prosperity'? New Media and Society.
Fritsch, Katrin. 2018. Towards an Emancipatory Understanding of Widespread Datafication,
Accessed 1 September 2018. http://dx.doi.org/10.2139/ssrn.3122269
Fuchs, Eckhardt, and Annekatrin Bock. 2018. Palgrave Handbook of Textbook Studies.
London: Palgrave.

18

Gilliard, Chris, and Hugh Culik. 2017. "Digital Redlining, Access, and Privacy." Accessed 28
September

2017.

https://www.commonsense.org/education/privacy/blog/digital-redlining-

access-privacy.
Gutiérrez, Miren. 2018. Data Activism and Social Change. London: Palgrave.
Irvine, J.J. 2010. "Foreword." In Culture, Curriculum, and Identity in Education, edited by H.
Richard Milner. New York: Palgrave Macmillan.
ITU. 2017a. Fast-forward progress: leveraging tech to achieve the global goals. Geneva
(Switzerland), International Telecommunication Union. Accessed 3 September 2018.
http://www.itu.int/en/sustainableworld/Pages/report-hlpf-2017.aspx.
ITU. 2017b. Measuring the Information Society Report. Geneva (Switzerland), International
Telecommunication Union. Accessed 1 September 2018. http://www.itu.int/en/ITUD/Statistics/Documents/facts/ICTFactsFigures2017.pdf.
Kennedy, Helen. 2018. Living with Data: Aligning Data Studies and Data Activism Through
a Focus on Everyday Experiences of Datafication. Krisis: Journal for Contemporary
Philosophy 1:18-30.
Ladson-Billings, Gloria. 2006. "From the achievement gap to the education debt:
Understanding achievement in U.S. schools." Educational Researcher 35 (7):3-12.
Lather, Patti. 2010. Engaging Science Policy: From the Side of the Messy. New York: Peter
Lang.
Leurs, Koen, and Tamara Shepherd. 2017. Datafication and discrimination. In The Datafied
Society, edited by M. T. Schäfer and K. van Es. Amsterdam: Amsterdam University Press.
Macgilchrist, Felicitas. 2017. Backstaging the teacher: On learner-driven, school-driven and
data-driven change in educational technology discourse. Culture-Society-Education 12 (2):83103.

19

Milner, H. Richard. 2013. "Rethinking Achievement Gap Talk in Urban Education." Urban
Education 48 (1):3-8. doi: 10.1177/0042085912470417.
Moore, Alex, and Matthew Clarke. 2016. "‘Cruel optimism’: teacher attachment to
professionalism in an era of performativity." Journal of Education Policy 31 (5):666-77. doi:
10.1080/02680939.2016.1160293.
Noguera, Pedro, and Victor Rios. 2012. "Reframing the Achievement Gap." Contexts 11 (4):810.
Prinsloo, Paul, and Sharon Slade. 2017. An elephant in the learning analytics room: the
obligation to act. Paper presented at the LAK ’17 Proceedings of the Seventh International
Learning Analytics & Knowledge Conference, ACM, New York, NY.
Rancière, Jacques. 2004. The Politics of Aesthetics: The Distribution of the Sensible. London:
Continuum.
Rasmussen, Mary Lou. 2013. "‘Cruel Optimism’ and Contemporary Australian Critical Theory
in Educational Research."

Educational Philosophy and Theory 47 (2):192-206. doi:

10.1080/00131857.2013.793929.
Robinson, Laura, Shelia R. Cotten, Hiroshi Ono, Anabel Quan-Haase, Gustavo Mesch,
Wenhong Chen, Jeremy Schulz, Timothy M. Hale, and Michael J. Stern. 2015. Digital
inequalities and why they matter. Information, Communication & Society 18 (5):569-582.
Scheurich, James Joseph, Vicki L Bonds, Jada A Phelps-Moultrie, Brandon J Currie, Troy A
Crayton, Alycia M Elfreich, Catherine D Bhathena, Tiffany S Kyser, and Nathaniel A
Williams. 2017. "An initial exploration of a community-based framework for educational
equity with explicated exemplars." Race, ethnicity and education 20 (4):508-26.
Selwyn, Neil. 2016. "'There's so much data': Exploring the realities of data-based school
governance." European Educational Research Journal 15 (1):54-68.
Stalder, Felix. 2015. Kultur der Digitalität. Berlin: Suhrkamp.

20

van Deursen, Alexander J. A. M., and Jan A. G. M. van Dijk. 2013. The digital divide shifts to
differences in usage. New Media & Society 16 (3):507-526.
Warschauer, Mark. 2003. Technology and Social Inclusion: Rethinking the Digital Divide.
Cambridge, MA: MIT Press.
Watters, Audrey. 2017. "The Weaponization of Education Data.” Accessed 1 September 2018.
https://hackeducation.com/2017/12/11/top-ed-tech-trends-weaponized-data.
Williamson, Ben. 2018. Big Data and Education. London: Sage.
Working Group on Education. Digital skills for life and work

2017. Available from

http://unesdoc.unesco.org/images/0025/002590/259013e.pdf.
Zembylas, Michalinos. 2016. "Political depression, cruel optimism and pedagogies of
reparation: questions of criticality and affect in human rights education." Critical Studies in
Education 59 (1):1-17. doi: 10.1080/17508487.2016.1176065.
Žižek, Slavoj. 2014. "How WikiLeaks opened our eyes to the illusion of freedom " The
Guardian, 19 June.

21

