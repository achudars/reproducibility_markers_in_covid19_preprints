Resistance to Adoption of Best Practices
A Report from the Berkeley Institute for Data Science’s
Best Practices in Data Science Series
Dan Sholler1,2†* , Sara Stoudt1,3† , Chris Kennedy1,4,5‡ , Fernando Hoces de
la Guardia6‡ , François Lanusse1,7,8‡ , Karthik Ram1,2,9‡ , Kellie Ottoboni1,3‡ ,
Marla Stuart1,10‡ , Maryam Vareth1,11‡ , Nelle Varoquaux1,3‡ , Rebecca
Barter1,3‡ , R. Stuart Geiger1‡ , Scott Peterson12‡ , Stéfan van der Walt1‡
*

Corresponding author:

dsholler@berkeley.edu
†

These authors contributed
equally to this work

‡

These authors contributed

1

Berkeley Institute for Data Science, University of California, Berkeley

2

rOpenSci

3

Department of Statistics, University of California, Berkeley

4

D-Lab, University of California, Berkeley

5

Division of Biostatistics, University of California, Berkeley

6

Berkeley Initiative for Transparency in the Social Sciences, University of California, Berkeley

7

Berkeley Center for Cosmological Physics, University of California, Berkeley

8

Foundations of Data Analysis Institute, University of California, Berkeley

9

Berkeley Initiative in Global Change Biology, University of California, Berkeley

equally to this work, order
is alphabetical
Published: 12 Mar 2018
DOI: 10.31235/osf.io/qr8cz
License: Creative
Commons
Attribution (CC BY 4.0 Intl)

10

School of Social Welfare, University of California, Berkeley

11

Department of Radiology and Biomedical Imaging, University of California, San Francisco

12

Morrison Library and Graduate Services Library, University of California, Berkeley

Abstract:
There are many recommendations of “best practices” for those doing data science,
data-intensive research, and research in general. These documents usually present
a particular vision of how people should work with data and computing, recommending speciﬁc tools, activities, mechanisms, and sensibilities. However, implementation
of best (or better) practices in any setting is often met with resistance from individuals and groups, who perceive some drawbacks to the proposed changes to everyday
practice. We offer some deﬁnitions of resistance, identify the sources of researchers’
hesitancy to adopt new ways of working, and describe some of the ways resistance
is manifested in data science teams. We then offer strategies for overcoming resistance based on our group members’ experiences working alongside resistors
or resisting change themselves. Our discussion concluded with many remaining
questions left to tackle, some of which are listed at the end of this piece.
Recommended citation: Dan Sholler, Sara Stoudt, Chris Kennedy, Fernando Hoces de
la Guardia, François Lanusse, Karthik Ram, Kellie Ottoboni, Marla Stuart, Maryam Vareth,
Nelle Varoquaux, Rebecca Barter, R. Stuart Geiger, Scott Peterson, and Stéfan van der Walt.
"Resistance to Adoption of Best Practices." BIDS Best Practices in Data Science Series. Berkeley
Institute for Data Science: Berkeley, California. 2019. 10.31235/osf.io/qr8cz

1 of 9

Introduction
In prior Best Practices in Data Science meetings, we have discussed some of the
challenges of doing data-intensive research in teams (Geiger et al., 2018b), including
managing turnover (Sholler et al., 2019); fostering a diverse and inclusive data
science team (Geiger et al., 2018a), and creating and sustaining robust workﬂows.
We have steadily developed some best (or better) practices for dealing with these
challenges, as have other researchers and practitioners in academia and industry
(Wilson et al., 2014; Goodman et al., 2014; Sandve et al., 2013).
However, implementation of best (or even “good enough” (Wilson et al., 2017))
practices in any setting is often met with resistance from individuals and groups,
who perceive some drawbacks to the proposed changes to everyday practice. We
offer some deﬁnitions of resistance, identify the sources of researchers’ hesitancy to
adopt new ways of working, and describe some of the ways resistance is manifested
in data science teams. We then offer strategies for overcoming resistance based on
our group members’ experiences working alongside resistors or resisting change
themselves. Our discussion concluded with many remaining questions left to tackle,
some of which are listed at the end of this piece.

What is resistance?
Resistance is a common phenomenon that arises in response to change. While we
might “know it when we see it,” resistance is also a somewhat-thoroughly studied
theoretical concept developed in the information systems and organization science
literatures. Research interest in resistance is driven by its costly impact on organizational change efforts, particularly as it relates to the implementation of a new
enterprise technology (e.g. Joshi, 1991; Lapointe and Rivard, 2005; Marakas and
Hornik, 1996; Markus, 1983). In general, this literature ascribes acts of resistance
to perceived loss of power, unwanted changes to everyday practice, and overall
stress about the possibility of change. The perceived threat of a new technology or
practice can lead to active avoidance or misuse, ultimately jeopardizing the goal of
the change effort. As we describe below, each of these elements is visible in efforts
to implement best practices in data-intensive research teams.

Sources of resistance to best practices
Building on what constitutes resistance, we turned our attention to why members
of data-intensive research teams might resist new modes of working with data.
Some of the reasons, such as inertia, are intuitive: Researchers don’t want to change

2 of 9

long-held practices that have presumably worked for carrying out their work in the
past. Other reasons, such as concerns about the longevity of a new practice or tool,
are rooted in skepticism about “fads” in science, generally, and scientiﬁc computing,
speciﬁcally.

Inertia, or “I’ve been doing it this way my entire career”
Perhaps the most obvious reason for resistance to adoption of best practices is
inertia, or the tendency for core research practices to endure over time. Several
group members noted that efforts to engage collaborators with new approaches to
data management, data manipulation and analysis, manuscript writing, and other
research tasks were met with resistance, often in the form of collaborators sticking
to their existing practices. In some cases, resistors justiﬁed their hesitancy because
the proposed new practice or tool didn’t match the full capabilities of the original
(e.g., track changes is often a sticking point in attempts to move collaborative writing
toward LaTeX or Google Drive). Even though many aspects of the workﬂow may
be made easier, more reproducible, and/or more transparent by the new tool, a
bottleneck remains. This deters users from adoption to avoid switching back and
forth between “best” and functional.

Time and resource constraints
Even if team members welcome the idea of change, the switching costs might be
perceived as too high to justify adopting new practices. For example, consider a
scenario in which a new programming environment or software package offers
enhanced capabilities to analyze data. Who is responsible for the time and cost of
converting the team’s previous work to appropriate formats? Labs with decades of
work could conceivably need to create a staff or postdoc position focused solely
on conversion, an effort that could detract from research efforts and be diﬃcult to
justify to some funders.
Relatedly, we discussed a theme that has persisted throughout all of our Best
Practices sessions: adoption of best practices breaks down in time crunches. In
other words, ﬁnding the “right” time to adopt new practices is situationally diﬃcult,
as research teams are continually and simultaneously writing grants, collecting data,
conducting analyses, writing up results, and otherwise balancing time-sensitive
tasks. This juggling act leaves little time for learning new practices, particularly for
practices with steep learning curves. And when deadlines loom, researchers seem
to be more likely to revert to old, comfortable practices to meet demands.

3 of 9

Worry about longevity of new approaches
Researchers might also have concerns that a new practice or tool is merely a passing
fad rather than a paradigmatic shift in the way research is done. Several group
members raised this point as an example of why they or their collaborators resist
change. The issue of the longevity of a given best practice is often implicit in how
people talk about changes in data-intensive science, yet there are few resources
for evaluating the long-term potential of tools and approaches. Researchers might
feel more comfortable working with software backed by paid technical support,
for example, than working with software supported by a distributed open source
community.

Forms of resistance
No matter the source of resistance, researchers tend to resist in similar ways. Resistance, both in the literature on organizational change and in our group members’
experiences, tends to take the form of outright refusal (either individually or by multiple members of the team), of vocalized disdain or hesitancy, or of subtly sabotaging
efforts to shift practices by bottlenecking the change process.

Avoidance and refusal
A common form of resistance is simple avoidance of the tool or practice or a refusal
to update a skillset. A poignant, frequently discussed example is some researchers’
reluctance to switch from proprietary software to open source software (e.g. Stata,
SAS, SPSS → R, Matlab → Python, Microsoft Oﬃce → LaTeX, Google Suite). We often
frame examples like these as a problem of technical aptitude. Not every researcher
pays mind to coding or workﬂow issues; rather, they view these practices as means
to an end, and motivation and ﬂexibility to change tools may be lacking.

Bottlenecking
Perhaps as a result of avoidance, team projects are limited by their most resistant
member. In a collaborative project, it can be diﬃcult to coordinate work if members
are using different tools (e.g. different team members writing up results in Word,
Google Docs, and LaTeX). This is especially true for interdisciplinary teams where
members bring different norms from their respective ﬁelds. In these situations, one
or more people often has to serve as the go-between and manage the overhead
work of converting between formats or ensuring compatibility between workﬂows.
In severe cases, resistors can bottleneck the work by using a practice or tool that
does not align with what the rest of the team uses. Then, the path of least resistance

4 of 9

often becomes agreeing on a tool or practice that is not “best” as deﬁned by the
majority of the team.

Vocalized Resistance
Collaborators may go beyond bottlenecking a project or engaging in other passive
forms resistance. Another form of resistance in team and community settings
occurs when a member actively lobbies against the adoption of or complains about
a particular tool or practice. Any energy or momentum for change is often used
up in debating rather than moving towards a new practice. This form of resistance
can be especially tricky to navigate given disparate power dynamics of the vocal
members of the community.

Strategies for overcoming resistance
Demonstrating value
Evidence that the best practice will help solve a problem, save time, and/or enhance
reproducibility and transparency can alleviate resistance. Demonstrating problemdriven value in both the short- and long-term is key. For example, if there is a
particular recurring pain point that can be overcome with a switch to a new tool, a
quick change may be perceived as justiﬁed by members of the team. Finding these
pain points and addressing them with the new practice—even if the problem is not
the primary target of the change—can help to sway resistors. These “quick hits”
can supplement efforts to show that a change will lead to increased productivity or
eﬃciency in the long run and promote openness to trying out the new practice.
A related approach to demonstrating value is showcasing a polished product, such
as a beautiful new visualization of old data or a speedy re-analysis. These attentiongrabbing demonstrations can entice a resistant person to consider a new tool or
practice. Following the compelling, potentially more complicated, example with a
set of distilled, smaller and easier examples will familiarize the user with the new
tool and illustrate its accessibility.

Offering credit and rewards
If the beneﬁts of switching to the best practice are hidden or not equitably distributed among team members, the time spent learning a new tool can seem like a
waste without an obvious or immediate payoff. Treating the best practice as part of
professional development and having dedicated time set aside to learn new skills

5 of 9

can transform the act of learning from a burden to a reward. The Carpentries1 organization provides a high-proﬁle example of how to make computational skill-building
a rewarding experience. Another, related way to “get credit” for updating one’s
skillset is to create some output throughout the learning process. This approach can
include something informal (e.g. a series of blog posts documenting the process and
establishing a record of an evolving skill set) or formal (e.g. a handbook or methods
paper introducing the new tool or workﬂow to other researchers).

Mandates
The above strategies can be employed within small or large research teams. Moving
entire disciplines toward best practices, though, likely requires some action from
powerful institutions in the ﬁeld. Our group discussed the potential impact of journal
policies for manuscripts (e.g., requiring submissions to be in LaTeX or archiving
suﬃcient data and code) on shifting researchers toward best practices. Funders
have also begun to take steps toward requiring researchers to engage in what
funding agencies view as best practices, such as mandating data management plans
in all proposals or urging teams to house their data in appropriate repositories. To
be sure, principal investigators may hold some similar mandate power within their
own labs (as we have mentioned in previous series posts).

Procedural justice and providing support
Studies in organizational behavior have demonstrated that people are more satisﬁed
with the outcome of a decision when they perceive justice in the decision-making
process, no matter the positive or negative valence of the decision’s outcome (e.g.
Folger and Konovsky, 1989; McFarlin and Sweeney, 1992). In this vein, researchers
might be less resistant to changes to their practice if they have input in how the new
practice is selected and implemented. Involving the entire research team in decisions
like software selection and workﬂow design, then, provides a way of minimizing
friction when a change is made. Providing support for researchers post-adoption is
equally important. Having access to regular help and timely troubleshooting as a
user faces the learning curve of a new tool or practice can help a resistant person
become more conﬁdent in making a change to their workﬂow. Whether assistance
comes from paid services or members of an open source community, a guaranteed
support system can make a difference in the willingness to move towards best
practices.
1

https://thecarpentries.org

6 of 9

Summary and Conclusions
Data-intensive research teams should expect to encounter some resistance when
changing practices. Comfort and familiarity with existing ways of working, hesitancy
to invest time and resources into switching, and concern over the long-term viability of the new practice all factor into researcher decisions to resist. Faced with
resistance, teams can employ some of the above strategies to push along changes
and improvements to practice. We have covered some of the approaches that
might be used, but we are sure there are many more and welcome any additions
and feedback on what we have presented here. In particular, we hope to spark
discussion on some remaining questions:
• What is a best practice, anyway? We spent some time considering this question. We, as a group, tend to equate the use of open source tools and related
open practices as “best practices.” This is owing to our own experiences and
studies demonstrating open research is “associated with increases in citations,
media attention, potential collaborators, job opportunities, and funding opportunities” (McKiernan et al., 2016, p. 1). However, we are aware that this
is a narrow deﬁnition of “best” and may beneﬁt from critiques and revisions,
including adjusting our focus to “agreed-upon practices.”
• Who gets to decide what a best practice is? When is it appropriate for
a journal, funder, PI, or other decision-maker to impose practices or tools
through use of power? What opportunities are there for those impacted by
such decisions to raise issues or get support, particularly when compliance is
an unfunded mandate? Resistance often arises for good reasons that should
be taken into account when making change decisions. Mandates may underplay or ignore resistors’ reasons and miss valuable opportunities for reﬂection
on the impact of new practices.
• How do we strike a balance between “best” and “good enough” practices? Researchers are often time- and resource-constrained, so asking for
major changes to everyday practice may augment resistive behaviors. We
might be best served by making requests that meet researchers in the middle
and providing avenues for incremental changes.

Acknowledgments
This work was supported by the Gordon & Betty Moore Foundation (GBMF3834) and
Alfred P. Sloan Foundation (2013-10-27), as part of the Moore-Sloan Data Science
Environments grant to the University of California, Berkeley.

7 of 9

References
Folger R, Konovsky MA. Effects of procedural and distributive justice on reactions to pay
raise decisions. Academy of Management. 1989; 32(1):115–130. https://www.jstor.org/
stable/256422.
Geiger RS, DeMasi O, Culich A, Zoglauer A, Das D, de la Guardia FH, Ottoboni K, Fenner M,
Varoquaux N, Barter R, Barnes R, Stoudt S, Dorton S, van der Walt S. Best Practices for
Fostering Diversity and Inclusion in Data Science. In: BIDS Best Practices in Data Science
Series Berkeley Institute for Data Science; 2018.https://doi.org/10.31235/osf.io/8gsjz.
Geiger RS, Sholler D, Culich A, Martinez C, de la Guardia FH, Lanusse F, Ottoboni K, Stuart
M, Vareth M, Varoquaux N, Stoudt S, van der Walt S. Challenges of Doing Data-Intensive
Research in Teams, Labs, and Groups. In: BIDS Best Practices in Data Science Series Berkeley
Institute for Data Science; 2018.https://doi.org/10.31235/osf.io/a7b3m.
Goodman A, Pepe A, Blocker AW, Borgman CL, Cranmer K, Crosas M, Di Stefano R, Gil Y,
Groth P, Hedstrom M, Hogg DW, Kashyap V, Mahabal A, Siemiginowska A, Slavkovic A. Ten
Simple Rules for the Care and Feeding of Scientiﬁc Data. PLoS Computational Biology.
2014 Apr; 10(4):e1003542. doi: 10.1371/journal.pcbi.1003542.
Joshi K. A model of users’ perspective on change: the case of information systems technology implementation. MIS Quarterly. 1991; p. 229–242. https://www.jstor.org/stable/
249384.
Lapointe L, Rivard S. A multilevel model of resistance to information technology implementation. MIS Quarterly. 2005; 29(3). https://www.jstor.org/stable/25148692.
Marakas GM, Hornik S. Passive resistance misuse: overt support and covert recalcitrance
in IS implementation. European Journal of Information Systems. 1996; 5(3):208–219.
https://doi.org/10.1057/ejis.1996.26.
Markus ML. Power, politics, and MIS implementation. Communications of the ACM. 1983;
26(6):430–444. https://doi.org/10.1145/358141.358148.
McFarlin DB, Sweeney PD. Distributive and procedural justice as predictors of satisfaction
with personal and organizational outcomes. Academy of Management. 1992; 35(3):626–
637. https://doi.org/10.5465/256489.
McKiernan EC, Bourne PE, Brown CT, Buck S, Kenall A, Lin J, McDougall D, Nosek BA, Ram K,
Soderberg CK, et al. Point of view: How open science helps researchers succeed. Elife.
2016; 5:e16800. https://doi.org/10.7554/eLife.16800.
Sandve GK, Nekrutenko A, Taylor J, Hovig E. Ten Simple Rules for Reproducible Computational Research. PLoS Computational Biology. 2013; 9(10). doi: 10.1371/journal.pcbi.1003285.
Sholler D, Das D, de la Guardia FH, Hoffmann C, Lanusse F, Garcia R, Geiger RS, McDevitt
S, Peterson S, Stoudt S. Best Practices for Managing Turnover in Data Science Grouups,

8 of 9

Teams, and Labs. In: BIDS Best Practices in Data Science Series Berkeley Institute for Data
Science; 2019.https://doi.org/10.31235/osf.io/TBD.
Wilson G, Aruliah DA, Brown CT, Chue Hong NP, Davis M, Guy RT, Haddock SHD, Huff KD,
Mitchell IM, Plumbley MD, Waugh B, White EP, Wilson P. Best Practices for Scientiﬁc
Computing. PLoS Biology. 2014 Jan; 12(1):e1001745. doi: 10.1371/journal.pbio.1001745.
Wilson G, Bryan J, Cranston K, Kitzes J, Nederbragt L, Teal TK. Good enough practices
in scientiﬁc computing. PLOS Computational Biology. 2017 Jun; 13(6):e1005510. doi:
10.1371/journal.pcbi.1005510.

9 of 9

