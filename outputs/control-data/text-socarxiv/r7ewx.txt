Can we distill fundamental sentiments from natural language use?
Evaluating word embeddings as a complement to survey-based
ratings of affective meaning.

Austin van Loon
Jeremy Freese
ABSTRACT

Central to affect control theory are culturally shared meanings of
concepts. That these sentiments overlap among members of a culture presumably reflects
their roots in the language use that members observe. Yet the degree to which the
affective meaning of a concept is encoded in the way linguistic representations of that
concept are used in everyday symbolic exchange has yet to be demonstrated. The question
has methodological as well as theoretical significance for affect control theory, as language
may provide an unobtrusive, behavioral method of obtaining EPA ratings complementary to
those heretofore obtained via questionnaires. We pursue a series of studies that evaluate
whether tools from machine learning and computational linguistics can capture the
fundamental affective meaning of concepts from large text corpora. We develop an
algorithm that uses word embeddings to predict EPA profiles available from a recent EPA
dictionary derived from traditional questionnaires, as well as novel concepts collected using
an open-source web app we have developed. Across both a held-out portion of the
available data as well as the novel data, our predictions correlate with survey-based
measures of the E, P, and A ratings of concepts at a magnitude greater than 0.85, 0.8, and
0.75 respectively.

Austin van Loon is a PhD Candidate in the Department of Sociology at Stanford University. His
research focuses on how the meanings attached to social categories as well as concepts and
heterogeneity therein explains various outcomes, including political polarization, racialized
police brutality, staying at home during the COVID-19 pandemic, and gender homophily in
organizations.

Jeremy Freese is Professor of Sociology at Stanford University.

Affect control theory (ACT) is an active and long-running theoretical program in sociological
social psychology concerned with cultural sentiments and the interpretation of events (Heise
1987, 2007; MacKinnon and Robinson 2014; Robinson and Smith-Lovin 2018). In general, the
theory strives to understand how individuals define situations through culturally shared
meanings, and how these definitions reciprocally interact with the emotions of individuals—
linking together culture, cognition, and emotions. Recent work in ACT has examined how
stereotypes lead to biased information transmission (Hunzaker 2016), the cultural-cognitive
underpinnings of occupational prestige (Freeland and Hoey 2018), the consequences of
dementia on notions of the self (Francis et al 2020), and the cultural bases of the gender gaps in
both wages (Freeland and Harnois 2020) and business leadership (Kroska and Cason 2019).
Affect control theory's roots in symbolic interactionism are evident from its emphasis on how
reactions to situations are based on the concepts used to interpret situations and the meanings
attached to those concepts. That there is considerable overlap among members of a culture in
the meanings attached to concepts enables actors to anticipate how others will understand and
react to situations (Robinson, Smith-Lovin, and Wisecup 2006; MacKinnon and Heise 2010). It is
also what allows ACT to provide a rigorous formal apparatus for using culture-level information
about the sentiments attached to concepts to predict actors' interpretations and behaviors
(Heise 2007).
As for how persons acquire their understanding of the denotative and connotative meanings of
concepts, the obvious, long-understood answer is by observing and participating in natural
language use. While symbolic interactionism has always centered the importance of language
for interpreting events, recent attention to language in social psychology has been more
intermittent (Holtgraves 2014). Historically, the study of language use has been hampered by
methodological challenges in capturing, manipulating, or using such data at significant scale.
Radical progress has been made on these issues in recent years, and the amount of naturallanguage text data available to researchers has grown to incredible scales, along with
increasingly sophisticated methods for harnessing such data (Gentzkow 2019).
Text generation is a public act through which individuals seek to communicate with and be
understood by others by deploying the shared understanding of linguistic symbols. One might
therefore expect text to contain implicit information about the fundamental sentiments
attached to concepts in a culture. However, affect control theory distinguishes between the
transient impressions of concepts in particular contexts and the fundamental impressions of
concepts in the abstract. As language use is almost always contextualized, it is possible that
concept use in naturally generated text corpora never transcends transient impressions in a
way that provides systematically useful information about fundamental impressions, even
when they are very large. Here we test whether information about fundamental impressions

can be extracted from large text corpora using contemporary analytic methods, and, if so, to
what degree.
The stakes here are not only theoretical. Crucial to the empirical application of affect control
theory are ratings of concepts on the affective dimensions of evaluation, potency, and activity
(EPA). If text does indeed contain high-fidelity information about these fundamental
impressions, then we might be able to reverse engineer fundamental impressions of concepts
from statistical patterns in language use. If so, then we would have an unobtrusive, behavioral
method of measuring affective profiles, which could be applied much more broadly than
measures based on eliciting EPA ratings of concepts from a set of raters, significantly expanding
the set of questions affect control theorists could address.
We pursue these questions with four related studies. All of them make use of “word
embeddings,” which are representations of words in a high-dimensional space based on the
statistical co-location of words in large corpora of text. The first explicitly tests whether the
position of words in this high-dimensional space is related to the fundamental sentiment of the
concept for which the word is its root. We find strong evidence that this is the case. The
remaining three studies all use predictive modeling to consider how well word embeddings can
be used to predict questionnaire-based ratings of corresponding concepts. Using deep learning
algorithms for predictive modeling alongside word embeddings, we find that information from
natural language corpora can be used to predict questionnaire-based EPA ratings with
impressive accuracy. Our results suggest that one can now produce, for the general US
population, a reasonable proxy of EPA ratings for any concept that is linguistically represented
with reasonable frequency in American English.
Affect Control Theory and the Measurement of Fundamental Sentiments
In symbolic interactionism, language is not simply a tool for describing reality, but the means of
constituting it as an object that may be purposefully acted upon (Heise 2019). Mead (1934: 78)
emphasized that language "does not simply symbolize a situation or object which is already
there..." but rather "makes possible the existence or the appearance of that situation or
object... [language] is part of the mechanism whereby the situation or object is created." From
this perspective, language is not only essential to the meaningful constitution of the external
world, but also the self as a coherent and intentional actor (MacKinnon and Heise 2010; Francis
et al. 2020). Language, acquired early in life from others, is simultaneously theorized as a
principal means for conscious thought, learning about the world, and interacting with others
(Berger and Luckmann 1966). This gives language a central role in symbolic interactionist's view
of individuals as thoroughly and fundamentally social (McCall 2006).
Affect control theory follows in this tradition by emphasizing the centrality of language in
situational understanding (MacKinnon 1994). In affect control theory, situations are

characterized by the identities (e.g., “doctor”, “cop”) of actors, modifiers (“friendly”, “timid”)
that describe those actors, and behaviors (“help”, “humiliate”) directed from one actor towards
another. These constituent concepts hold both denotative and affective meanings. Following
the seminal work of Osgood et al (1957), the affective meanings of a concept may be described
quantitatively by a position within a three-dimensional socio-cognitive space: its evaluation
(good-bad), potency (strong-weak), and activity (active-inactive). Given this information about
the set of concepts with which an individual defines a situation, ACT provides predictions about
how that individual will emotionally respond to the situation, to what degree they will feel
discomfort about the situation, and how specifically the individual might cognitively reframe
the situation to reduce this discomfort (Heise 1987, 2007; Robinson, Smith-Lovin, and Wisecup
2006). To borrow an example from Hunzaker (2016: 1226-7) and others, if someone in a
contemporary western context were to conceptualize a situation as “the mother kicks the
baby”, they would feel a strong sense of cognitive discomfort from conceptualizing a situation
as a good, powerful actor (“mother”) performing a bad, moderately powerful action (“kick”)
towards a good and powerless object (“baby”).
Affect control theory affords precise predictions about the interpretation of and behavior in
situations from the meanings of the situation's elements. The measurement of the fundamental
sentiments associated with those elements are therefore vital to the generation of empirical
predictions from the theory. Historically, researchers in ACT have spent considerable resources
collecting “EPA dictionaries” or sets of EPA ratings for a large number of concepts from raters
who share a common cultural context. Several such dictionaries have been made available as
public goods that are used by many researchers in the community.1
Another tenet of symbolic interactionism is that shared meanings for symbols are acquired
through interaction with others. The overlapping understanding of concepts that allows
different raters to provide EPA ratings that correlate substantially with one another reflects an
overarching coherence to the language use to which they have each been exposed (Rogers
2019). This point animates our interest in whether the fundamental sentiments that have
heretofore been measured via questionnaires might be induced directly from aggregate
patterns in the usage and manipulation of linguistic symbols. There are reasons, however, to
doubt the prospects of this endeavor. Importantly, ACT makes the distinction between
fundamental impressions, or the affective meanings of concepts in the abstract, and transient
impressions, or the affective meaning associated with a particular, contextualized instance of
that concept (Heise 1987). EPA ratings typically seek to measure fundamental impressions.
Taking the concept of “mother” as an example, the traditional questionnaire method presents

1

See http://affectcontroltheory.org/resources-for-researchers/data-sets-for-simulation/

for an extensive collection of these dictionaries.

simply “A mother is” to raters, along with semantic dimension scales for each dimension in
turn.
The impression of “mother” collected in this way is typically highly positive, moderately potent,
and slightly to moderately active in contemporary western contexts. However, if you witness a
particular “mother” perform some set of actions (e.g. berate or neglect their child), your
impression of them might change. In text, linguistic manifestations of a concept (e.g. any time
you see the word “mother” appear in an English fiction book) are contextualized to some
degree; if anything, given that most language is used for concrete purposes, one might expect
context to predominate abstract meanings. As a result, it is not clear a priori whether
fundamental impressions can be accurately extracted from naturally generated language.
At the same time, that natural language might encode this information is suggested by the
popularity in computational linguistics of sentiment dictionaries, which are collections of words
that are treated as being predictive of some underlying sentiment in the texts in which they
appear. The most commonly used sentiment dictionaries are “positive” and “negative” words
(e.g. Hu and Liu 2004) that intuitively resembles the evaluative dimension of EPA ratings.
However, sentiment dictionaries and EPA dictionaries accomplish distinct goals. A sentiment
dictionary identifies what words provide the clearest signal of the overall sentiment of a piece
of text (e.g. a movie review), while an EPA dictionary quantifies the sentiments of each
conceptual element represented in a text, however strong or weak those might be.
Consequently, the existence of sentiment dictionaries does not necessarily imply that
fundamental sentiments across a set of given concepts can be usefully inferred.
Ahothali and Hoey (2015) show that ACT can be used to predict the affective interpretations of
news headlines from the fundamental sentiments of the concepts invoked therein. In doing so,
they develop a label propagation algorithm to extend EPA ratings to concepts that had not been
measured via surveys. By linking sentiment analysis to theoretical results from ACT, their results
bolster hopes that natural language use contains retrievable information about fundamental
sentiments. However, their label propagation method requires extensive information about the
relatedness of different words (in their case defined by experts) and produced predictions that
were off by an average of nearly one unit on the nine-unit scale conventionally used for EPA
ratings. If more accurate predictions of the fundamental sentiments of concepts could be
induced from patterns in natural language data, such a method might prove extremely valuable
for ACT researchers.
Since the research community of ACT relies heavily on centrally-collected EPA dictionaries, the
expansiveness of these dictionaries influences the scope of questions that can be interrogated
by ACT researchers (at least without the time and expense of collecting a new dictionary). The
use of surveys is limiting, as the marginal costs of survey ratings do not decrease with scale:

collecting ratings for twice as many concepts requires about twice as many rater-hours. The
unfortunate result is that even the largest EPA dictionaries to date still exclude many common
identities, behaviors, and modifiers. In contrast, an alternative measurement strategy based on
extracting fundamental impressions from corpora of natural language would reduce the
marginal cost of estimating the EPA ratings of each additional concept reasonably represented
in the corpora to effectively zero.
Word Embeddings
Word-embedding models may be the key to creating such a measurement strategy. When
trained upon large corpora of naturally generated text, these models represent words as
vectors in a potentially high-dimensional space. Proximity in this space models the extent to
which words appear in similar contexts in the corpus, or the tendency to appear in texts near
the same other words (for details see Mikolov et al. 2013; Pennington, Socher, and Manning
2014). Unlike ACT’s three canonical dimensions, none of the potentially hundreds of
dimensions in a word-embedding model is intended to have any substantive interpretation in
isolation.
The semantic similarity that is explicitly modeled by word embeddings has been shown to
capture consensual cognitive associations between concepts in recent empirical applications.
That is, after training a word embedding on a corpus, the closeness of words in the resulting
high-dimensional space has been shown to correspond with other information about the
similarity of words. As examples, word embeddings have been shown to encode undesirable
human biases (Caliskan, Bryson, and Narayanan 2017) including gender and ethnic stereotypes
(Garg et al 2018); cultural associations between genres of music and race/class categories
(Kozlowski et al 2019); and even latent scientific knowledge (Tshitoyan et al 2019). It is our hope
that we can leverage the correspondence between cultural meanings and the semantic
similarity captured by word embeddings to measure the fundamental impressions of concepts
linguistically represented in large text corpora.
Importantly, “words” in word-embedding models are not equivalent to the “concepts” of ACT.2
For example, when ACT questionnaires ask raters about the affective meanings associated with
the identity of a mother, the prompt is to rate “A mother is.” This study pursues whether
potentially complex combinations of the dimensions of a word-embedding model’s
representation of the word at the root of a concept (e.g. “mother” in “a mother is”) captures
the same information contained in EPA ratings of the corresponding concept.
In summary, we contend that the manipulation and use of natural language by many individuals
within a cultural context implicitly contains information about the fundamental sentiments of
2

Disregarding contextualized word embeddings, which we describe below

concepts, and that aggregating this information via word embeddings might provide us a way to
augment traditional survey methods in building more expansive EPA dictionaries. In order to
demonstrate this, we will first test whether measures derived from simple algebraic operations
on publicly available word embeddings significantly correlate with concepts’ EPA ratings in an
existing EPA dictionary. Then, we evaluate whether we can, in a machine learning paradigm,
train a deep learning algorithm to leverage word embeddings to accurately predict the EPA
ratings of concepts.
Study 1
We begin by seeking simply to evaluate whether the information contained by a concept’s
questionnaire-based EPA rating is implicitly encoded in the vectors produced by a word
embedding algorithm. We take advantage of how questionnaire measures of EPA ratings
typically deploy a semantic differential scale, in which exemplar words for each concept are
presented as anchors for each side of the scale. For example, the positive side of such scales is
sometimes anchored by “good,” “clean,” and “beautiful,” while the negative side is anchored
by “bad,” “dirty,” and “ugly.” These anchor words are represented by word embedding models
as points in the same high-dimensional space as the words corresponding to the concepts we
seek to measure the EPA ratings of. If word embeddings capture the same information as
questionnaire-based EPA ratings, we would expect concepts with more positive questionnairebased ratings to be closer in word-embedding space to the positive anchor words than they are
to the negative anchor words. To this end, we assessed whether and how well relative
proximity to positive versus negative anchor words correlates with questionnaire-based EPA
ratings.
Data
The survey-based EPA data we use is the “US Online 2015” dictionary collected by Smith-Lovin
et al. (2019), which estimated the EPA ratings in the general US population of 2,473 concepts.
The dictionary used many raters per concept (Nraters=88-135) and the reliabilities of the
resulting mean ratings are very high (~.99). We removed all concepts for which the “root” of
the concept was more than one word (e.g., “being an old man is”; “being a street musician is”),
leaving us with 1,982 concepts (605 modifiers, 671 identities, and 706 behaviors).
The word-embedding vectors we use are the pre-defined and commonly-used Word2Vec
embeddings trained on the Google News Corpus. These embeddings are freely available
online3. While full details are provided elsewhere (Mikolov et al. 2013), in brief Word2Vec is an
algorithm which attempts to use surrounding words to predict the word appearing in each
position in the provided corpus, proceeding iteratively until it develops a high-dimensional
3

https://code.google.com/archive/p/word2vec/

vector that predicts well. We use the “root” of a concept (e.g. “musician” in “a musician is”) to
locate each concept in the high-dimensional space defined by word-embedding vectors. One
weakness of this approach is that multiple concepts (e.g. “to be a judge is” and “to judge
someone is”) might map to a single root (“judge”), although, as we note below, newer wordembedding techniques might address this limitation.
Method
Table 1 provides the anchor words we used for each dimension.4 For each dimension, we
defined vectors in the word embedding space corresponding to the centroids of the positive
and negative anchor words. We then took the cosine distance5 of the root’s position in the
embedding space from the negative centroid less the cosine distance of its position from the
positive centroid.6 This is conceptually equivalent to projecting the word onto an E, P, and A
number line defined in the word embedding space by using the anchors above as endpoints.
Finally, we computed the correlation of these measures with the mean E, P, and A ratings for
the concept in the survey that was used to compile the US Online 2015 dictionary.

Table 1. Anchor words for each dimension used in Study 1.
Negative Anchor Words

Dimension

Positive Anchor Words

“bad,” “dirty,” “ugly”

Evaluation

“good,” “clean,” “beautiful”

“powerless,” “small,” “weak”

Potency

“powerful,” “big,” “strong”

“old,” “inactive,” “quiet”

Activity

“young,” “active,” “loud”

Results and Discussion
Across all concepts in the US Online 2015 dictionary, estimated EPA ratings from the word
embeddings and survey correlate substantially. For E, P, and A ratings, the Pearson correlation
coefficients are 0.71, 0.55, and 0.24 respectively (all p < 0.001). 7 This indicates that word

4

We do not claim that or test whether the anchor words we use are the best words that could have been used for
this measurement strategy, and future work should explore more systematically which anchor words work best,
which may well vary by context and application.
5
We also tried Euclidean distance, which correlated marginally more with survey measures. We use cosine
distance because of its wide use in computational linguistics, and because it is more robust to alternative
specifications (e.g. changes to the dimensionality of the word-embedding space).
6
This results in ratings ranging from -0.42 and 0.42 to -0.48 and 0.48 depending on the dimension in question, but
scaling these ratings will not impact the correlation we examine.
7
Spearman correlation coefficients are very similar in magnitude.

embeddings capture the fundamental sentiments of concepts to some degree. It is worth reemphasizing that the word embeddings we use are completely unsupervised—that is, they are
generated from corpora in a completely inductive, atheoretical way—so any information
contained within reflects naturally-occurring statistical patterns in the corpora. Thus, our
hypothesis that natural language generation is structured by the fundamental sentiments of the
concepts invoked therein is supported. These results suggest that if a researcher has access to a
corpus of text generated by members of any population or sub-population sufficient to
generate word embeddings, they can estimate EPA ratings for concepts represented in that
corpus, thereby greatly advancing the range of questions and contexts that can be studied by
affect control researchers.
At the same time, even as the procedure used in this study produced ratings from wordembeddings that are significantly correlated with questionnaire ratings, the correlation is not as
high as one might hope. If we wish to use word embeddings to measure the fundamental
sentiments of concepts in lieu of survey ratings, then ideally the correlations would be higher,
especially for the activity dimension. Thus, in our further studies outlined below, we utilize
methods specifically optimized to take in the word-embeddings of concepts and predict their
fundamental sentiment as if it were included in the US Online 2015 survey, with the hopes that
in doing so we can achieve a higher correlation between embedding-based and survey-based
measures.
Study 2
Predictive modeling, or forms of data analysis emphasizing predicting the values of variables
rather than uncovering the relationships between them, has made great strides in recent
decades (see e.g. Hastie et al 2009). One pertinent advance is the proliferation of so-called
“deep learning”, which has been applied to problems from detecting breast cancer (Bejnordi et
al 2017; Shen et al 2019) to self-driving cars (Bojarski et al 2016). Here, we turn these deep
learning models to the problem of predicting the EPA rating of a concept in the US Online 2015
Dictionary from the concept’s word embedding.
Data
For this study, we seek to predict the EPA ratings of concepts in the US Online Dictionary. We
used multiple sets of pre-trained word embeddings trained using different algorithms and
corpora, thus leveraging slightly different kinds of information encoded from different sources
of naturally occurring language. The two algorithms on which our embeddings are trained are
Word2Vec (Mikolov et al. 2013) and GloVe (for global vector representation; Pennington,
Socher, and Manning 2014)8. Here we include three sets of GloVe embeddings trained on three
8

GloVe reduces a giant sparse word co-occurrence matrix using a log-bilinear model.

different corpora9 and the Word2Vec embeddings used in study 1. Just like the Word2Vec
embeddings used in study 1, the GloVe embeddings we use here are freely available online.10
Method
Our preferred predictions from this study are produced by a series of multilayer perceptrons
(MLPs). An MLP is the simplest form of a so-called “deep neural network” (Hastie et al 2009).
The “network” is made up of “neurons” organized into multiple sequential “layers.” At the first
layer (the “input layer”), each neuron takes on the value of a feature (i.e. variable) associated
with an observation. In our case, each “feature” is one dimension in a word embedding.
Through training, the network learns how to weight the connections between neurons in
proceeding layers through backward propagation of errors (or “backpropagation”) so that each
subsequent layer “learns” increasingly complicated patterns in the data, allowing the algorithm
to accurately predict the outcome variable of the observations it is trained on.
Data and Evaluation
Generating predictions from data introduces risk of overfitting: mistaking noise in the data for
meaningful patterns, resulting in increased error when predicting data on which the model was
not trained. In this respect, the conventional practice in social science of reporting model fit
with respect to the same sample on which parameter estimates are based can be an inflated
indicator of how well the model would actually predict out-of-sample. The standard solution to
this problem in predictive modeling is to split data into multiple parts. The model is trained on
one part (the “training set”), and its performance is evaluated on a different part (the “test
set”). Since the model’s predictive capability is assessed on the test set, which the model did
not have access to during training, the concern for over-fitting is largely assuaged.
We use the same data set of 1,982 concepts used in Study 1. We randomly selected 1,574
concepts to be the training set, leaving 408 (approximately 20%) to be our test set. We found
that having three separate MLPs predicting E, P, and A respectively produced marginally better
performance than having a single MLP predict all three simultaneously11. Our MLPs were first
trained on our training set, during which each learned to make predictions about the E, P, and A
9

Wikipedia, Twitter, and Common Crawl
https://nlp.stanford.edu/projects/glove/

10
11

We selected the hyperparameters for our models through cross-validation. Our MLP that predicted E
ratings had hidden layers sizes of 550, 100, and 10 and had an alpha regularization parameter of 0.03.
For P ratings our MLP had the same hidden layer sizes as for E, but an alpha regularization parameter of
0.005. Our MLP which predicted A ratings had hidden layer sizes of 550 and 100 and had an alpha
regularization parameter of 0.00007. All MLPs used a constant learning rate, ReLu activiation, and the
Adam optimization method.

ratings of concepts from the various word embeddings of the “root” of the concept. After
training, the model predicted the E, P, and A ratings for each concept in the test set. To be
comparable to study 1 results, we present the Pearson correlations between these out-ofsample predicted ratings and the mean ratings in the US Online 2015 dictionary.
Results
Our MLPs predicted the EPA ratings of concepts with surprising accuracy. The correlations
between the survey and word-embedding derived measures of concepts’ E, P, and A ratings in
the test set were 0.87, 0.83, and 0.76, respectively (all p < 0.001). Recall that the corresponding
correlations achieved in Study 1 were 0.71, 0.55, and 0.24, demonstrating that predictive
modeling achieves great gains in this application. The biggest gains in terms of magnitude of the
correlation coefficient are in measuring the activity dimension of concepts’ fundamental
sentiment, where we observe an over 200% increase in the magnitude of the correlation
between survey-based and word embedding-based measurements.
Figure 1 presents Q-Q plots of the errors (bottom row). In these plots, deviations from the line
indicate divergence from the expected distribution if errors were normally distributed. The
plots reveal that there are more observations with large errors than what we would expect if
errors merely reflected chance variation, and that the largest such deviations are for ratings of
E followed by P.
To understand these large errors, we examined the worst-fitting concepts for each dimension
(see Supplemental Materials, Appendix A). These included various clear examples in which the
concepts are represented by a word that is more commonly used in a different way. For
example, the model fails to predict the strong negative potency ratings of the identity “shrimp”
and “nobody,” but these words are more often used with a more neutral connotation than
referring to a person as being a shrimp or a nobody. Likewise, the model did not predict the
strongly positive ratings on E and P for the behavior “save,” which may reflect the word being
often used in more mundane ways (e.g., saving money, saving a file). The worst-fitting concept
across all three dimensions was the behavior “cripple,” for which the errors are consistent with
its target sense of “to cripple someone” being outnumbered by its (often offensive) use as a
noun. In other words, even though predictive performance is strong overall, the use of a
concept’s root as the proxy for the concept as measured in EPA ratings does result in larger
error in some instances.

Table 2. Performance of MLP Predictions on Test Set, by Concept Type.

Evaluation
Category

Potency

Activity

R

MAE

R

MAE

R

MAE

N

Identity

.87

.64

.80

.62

.72

.57

137

Behavior

.79

.88

.60

.56

.63

.55

156

Modifier

.95

.51

.93

.44

.82

.62

115

FIGURE 1 ABOUT HERE
Discussion
Study 2 demonstrated that predictive modeling provides considerable gains compared to the
more mathematically simple but theoretically informed measurements used in study 1. The
tradeoff, however, is that our MLPs learned specifically to predict the ratings from the US
Online 2015 dictionary. This differs conceptually from the methodology used in study 1 which
could be applied to any corpus of text without the need for survey ratings at all. Study 2, then,
demonstrated the capability of using predictive modeling and word embeddings to augment
rather than replace survey methods for measuring EPA ratings. That is, given a dictionary of
survey-based EPA measurements of some set of concepts and an appropriate text corpus, these
results demonstrate that deep learning can be used to map word embeddings to EPA ratings,
allowing the researcher to impute the fundamental sentiments of any other concept as if it
were included in the original survey. Though we demonstrated this for the US Online 2015
dictionary in particular, this same methodology could reasonably be applied to many other
dictionaries collected by EPA researchers in different times and places.
Study 3
While randomly dividing the US Online 2015 data into a training set and a withheld test set
should eliminate any overfitting, one might nevertheless be concerned that study 2 results are
overoptimistic about how well the algorithm might mirror EPA ratings of concepts not part of
the US Online 2015 dictionary.
We sought to evaluate the performance of our EPA prediction algorithm on ratings of concepts
that were not in the US Online 2015 dataset. We first generated a set of new concepts using an
algorithm we developed which utilizes machine learning and the same word embeddings used
for our EPA prediction algorithm. Then, we predicted the EPA ratings of these newly generated

concepts and posted our predictions to a time-stamped public archive as a method of preregistration, to allay any potential concern that our predictions had been adjusted post hoc to
fit the new data better. Only after posting these predictions did we collect ratings from
respondents we hired over Amazon Mechanical Turk using an open-source web application we
developed to allow ACT researchers to collect EPA ratings online more efficiently.
New Concept Generation
Ideally, the training set, test set, and our new data would each be a random sample from an
identically specified universe of concepts. As the training and test sets draw on existing data,
however, and since the universe of English concepts is not defined, we instead sought to
develop a list of concepts that were identities, behaviors, and modifiers akin to those collected
in the US-Online data, without actually including any of those concepts. We were concerned
any non-systematic way of selecting concepts might introduce inadvertent biases, especially
excluding concepts because of an intuition that they might perform poorly.
Accordingly, we sought to develop an algorithm using the aforementioned word embedding
data to develop our list. We began with the words in the US-Online dataset. Instead of trying to
predict the EPA profiles of these words as above, we used the word embedding vectors to train
a series of models to predict whether words were the root of either an identity, behavior, or
modifier. Then we considered the tokens in the word embedding vocabularies that were
classified into exactly one of these categories.
The US Online 2015 dictionary does not include any examples of words that are neither
identities, behaviors, nor modifiers. Our first iteration therefore resulted in many suggested
words that were clear false positives (i.e. words that were predicted to be an identity, behavior,
or modifier but were in fact none of these). We then iteratively hand-coded the output of the
algorithm, denoting both true positives identified by the model (i.e. words correctly identified
as an identity, behavior, or modifier) as well as false positives as such. We then included these
new hand-coded examples in the data on which the algorithm was trained for again, iterating
this process until we were satisfied with the algorithm’s performance. This is akin to a
“reinforcement learning” approach in machine learning (Kaelbling et al 1996).
We stopped when our algorithm achieved reasonable performances for purposes here. The
result was still imperfect and required some pruning by hand. For example, the model was not
fully successful in eliminating verbs that were not behaviors of the sort central to ACT – actions
one person can do to another person. Ultimately, we selected 80 identities, 77 behaviors, and
100 modifiers for which to collect new data. Before collecting data, we posted our predictions
for these new words as an appendix of a working paper on SocArXiV (BLINDED).
Rating Collection

To collect ratings, we developed an easily customizable, open-source web application for
collecting EPA ratings using the programming language R and its open-source library Shiny. We
have made it publicly available for other researchers to use and modify. We developed the
application because online EPA ratings have usually been collected using proprietary survey
platforms like Qualtrics, which can be cumbersome for researchers to set up and even then, do
not allow full randomization of what concepts are presented to raters in what order. Our web
application is designed to interface with Google Sheets, so that a user specifies settings and
concepts in one spreadsheet and the collected data is stored in another. Figure 2 contains a
screenshot from the web application.
FIGURE 2 ABOUT HERE
We collected ratings using Amazon Mechanical Turk, the same platform used to collect the US
Online 2015 data. In addition to ratings for the 257 new concepts (hereafter referred to as the
“new set”), we re-collected ratings on a random 150 concepts from the US Online 2015
dictionary (the “repeated set”), in order to determine whether there were any systematic
differences in ratings between the two datasets. Each rater provided 90 ratings in a session, and
we collected 18,748 ratings in all (209 sessions), for an average of 15.4 ratings for each concept
on each dimension.
These totals do not count sessions excluded due to poor-quality responses. To determine
whether a session should be excluded, we followed practices described by Cannon (2019).
Specifically, we calculated a "trouble score" for each rater based on different concerns about
rating qualities (see Supplemental Materials, Appendix B), and we dropped raters with trouble
scores of 2 or more (13 of 188).12
Results and Discussion
Table 3 presents Pearson correlations and MAE for our predictions in the new set. Figure 3
provides the same plot for the new set as presented in study 2 for the test set. In terms of the
magnitude of the correlations, overall performance is similar to what we observed in study 2:
better in the new set for E and worse for P and A. The fit of E for behaviors was notably better
in the new set, while predictions of identities were worse, as were predictions for P among

12

In calculating this trouble score, we mostly used the same criteria as the US-Online dataset, with two exceptions.
First, the US-Online data collection asked about time spent living in the United States and used that as a trouble
criterion. Although we did use US IP-location as a restriction when posting the task on Mechanical Turk, we did not
ask raters for any personal information, including nativity. In computing the trouble score, we instead substituted
low item-rest correlations as an indicator of respondents who gave unusual responses relative to others. Second,
as we were collecting fewer ratings per respondent, we were more concerned about the potential effects of a few
individuals who gave many extreme responses (compared to the median 5% extreme responses among raters),
and so we included this as a trouble criterion. All decisions about exclusions were made without examining
whether it improved the fit of our predictions.

modifiers. As before, E and modifiers were predicted best, and the model performed worst in
predicting P and A for behaviors. That the new set results so well replicate the test set results
bolsters the idea that deep learning methods can be paired with word embeddings to estimate
concepts’ fundamental sentiments.
Table 3. Performance of MLP predictions in new set
Evaluation
Category

Potency

Activity

R

MAE

R

MAE

R

MAE

N

Overall

.89

.70

.79

.68

.74

.73

257

Identity

.81

.78

.75

.78

.70

.79

80

Behavior

.87

.71

.63

.65

.59

.71

77

Modifier

.93

.62

.86

.63

.83

.69

100

FIGURE 3 ABOUT HERE
The new set predictions perform even better when we consider the reliabilities for the ratings
collected via person ratings. The US-Online data collected a large number of ratings per
concept, and so reliabilities for its ratings were extremely high (usually exceeding .99). We
collected many fewer ratings, and the median reliabilities of our ratings were .92 for E and .93
for P and A. The suggests that the correlations between predicted and observed ratings in the
new set were about 4% lower than they would have been if concepts had been measured with
the same reliability as the US-Online data. Reliability corrections increase our correlation
estimates from .89 to .93 for E, .79 to .82 for P, and .74 to .77 for A.13
Measurement reliabilities also bear on evaluating the MAE for our predictions. The theoretical
best-case for a prediction algorithm would be if it exactly matched the true parameter values
for each rating. Given the standard errors and implied normal approximation, the MAE for the
new set in this best-case would still be about .29 for E and about .33 for P and A. In the test set,

13

We estimate the standard error of a concept’s mean rating (SE) as SD/sqrt(N), where SD is the standard
deviation of the concept’s ratings and N is the number of raters. From this we estimate the reliability of ratings as
1-(SE/SD)2. Given the reliability (rel) of two variables, we adjust the correlation--cor(x,y)--for reliability by
cor(x,y)/sqrt(rel(x)×rel(y)).

the corresponding numbers are .10 for E and .12 for P and A, or about .2 less, which is larger
than differences in MAE between the test and new set for any concept type on any dimension.
Also, systematic differences between the US-Online data and our new data collection may also
have increased MAE in the new set. Although analysis of the 150 concepts in the US-Online
dataset for which we recollected ratings indicated a high correlation of old and new ratings for
all three dimensions (r > .9), there was evidence of mean differences for both P and A. Mean
ratings of the same concepts in the new data were .23 lower in P and .22 lower in A. This could
be due to unknown differences in MTurk respondents, differences in exclusion criteria, or
differences in instrumentation. Our modeling technique is neither intended nor able to account
for such systematic differences between the data on which it is trained and novel data. While
mean differences do not affect correlation coefficients, they do have implications for MAE.
Adjusting ratings of P and A in the new set by the aforementioned differences improved the
MAE by about .09.
In other words, while predictions performed worse for the new ratings we collected than for
the held-out words US Online 2015 data, the magnitude of the difference is entirely consistent
with measurement differences in the two collections of ratings, rather than degradation in the
performance of the algorithm itself. Results for the new set were already encouraging, and
there are several reasons to think the observed performance might be understated due to
random error and systematic differences in the data collection.14
Study 4
While we regard the results from studies 2 and 3 as extremely encouraging, they should not be
taken as the expected upper limit of performance for such algorithms. The science of predictive
modeling advances at a dizzying pace, and this is especially true of the fields of deep learning
and computational linguistics, two fields which this exercise lies at the intersection of. As a final
study, we tested whether and to what degree recent advances in contextual word embeddings,
specifically the development of BERT (Bidirectional Encoder Representations from
Transformers; Devlin et al 2019), an algorithm that produces context-sensitive embeddings that
have supplanted Word2Vec and GloVe in some state-of-the-art applications, would improve our
results. In this vein, we replicated studies 2 and 3, but used embeddings resulting from BERT
instead of Word2Vec or GloVe.
Embeddings

14

A small caveat is that the new set had fewer deviations from the expected normal error distribution, likely
suggesting that the new set had fewer words that were simply a bad fit for our algorithm. That said, the biggest
deviation of all was in the new set: while the model expected "sacrifice" to be viewed as good, the target sense of
one person sacrificing another in EPA ratings was (unsurprisingly) perceived as very bad by human raters.

BERT is a recent advancement in so-called “contextual embeddings”. Unlike traditional word
embeddings, which assign a static vector representation to an atomized token (implicitly
assuming the meaning of the word has single, stable meaning across uses), contextual
embedding models represent as a vector the meaning of a particular usage of a word based on
the context in which that word is found. For instance, different vectors characterize the word
“judge” in “to be a judge is” versus “to judge someone is.” In practice, BERT corresponds to a
pre-trained (on a very large compilation of English language texts of many different genres)
neural network, to which sentences or phrases are passed and embeddings for each token in
the sentence or phrase as well as a phrase or sentence-level embedding is returned.
We were faced with multiple ways to extract the meaning of any particular concept. When
given the input “to be a judge”, BERT produces an embedding for the entire phrase as well as a
separate embedding for each word in that phrase (that is nonetheless sensitive to the content
of the rest of the phrase). We could have used the concept-level embedding or simply the
contextualized embedding of the “root” of the concept (“judge” in this example), analogous to
our previous studies. Through experimentation on the training sets, we found that the latter
option outperformed the former, so we take that approach.
Training and Evaluation
Through experimentation within the training set, we found that predicting E, P, and A ratings
simultaneously with a single ensemble of MLP models15 achieved higher accuracy than using
three separate MLPs as in studies 2 and 3. Therefore, we used this approach to train an MLP
ensemble on the same training set used in study 2.16 We assess this model’s performance with
the same test set as study 2 as well as the new concepts we collected for study 3.
Results
Contrary to our expectations, we find that the BERT embeddings paired with our MLP did not
on the whole outperform the models in studies 2 and 3. On the test set used in study 2, the
embeddings-based E, P, and A ratings correlated with their survey-based counterparts at 0.89,
0.81, and 0.76 (all p < 0.001). On the novel concepts for which we collected ratings for Study 3,
the survey-based and embeddings-based predictions correlated for E, P, and A at 0.88, 0.79,
and 0.71 (all p < 0.001). The correlations between survey-based and embedding-based
measures of E, P, and A across our four studies are summarized in Table 4 below. While BERT
intuitively might seem to help bridge the gap between the “words” of word-embedding models
15

“Ensemble” refers to the practice of training many predictive models on random subsets of the training data and
taking the average prediction from these different models as the final prediction for observations in the test set.
16 Our ensemble trained 150 MLPs, each on an 80% random sample of the training set with hidden layer sizes of
1000, 1000, and 500, an alpha regularization parameter of 0.0000000001, a constant learning rate, and utilizing
the Adam optimization algorithm.

and the “concepts” pursued by ACT, it did not yield a predictive improvement here. We
certainly would not conclude from this that the algorithm developed for Study 2 and 3
approximates the ceiling of possible performance of machine learning approaches for
predicting questionnaire-based ratings, but it may be that achieving appreciable improvements
over our effort here is harder than we supposed.
Table 4. Summary of Studies and Results
US Online 2015 data*

Newly-collected data

Study

Embedding(s)

E

P

A

E

P

A

Study 1

Word2Vec

0.71

0.55

0.24

--

--

--

Study 2

Word2Vec; GloVe

0.87

0.83

0.76

--

--

--

Study 3

Word2Vec; GloVe

--

--

--

0.89

0.79

0.74

Study 4

BERT

0.89

0.81

0.76

0.88

0.79

0.71

*Study 1 values are assessed on all concepts in the US Online data while Studies 2 and 4 are assessed on a withheld random
20% test set.

Conclusions
ACT provides predictions about how individuals, after having internalized cultural sentiments,
respond emotionally, cognitively, and behaviorally to situations they define using particular
concepts. Implicitly, it is assumed that individuals learn these shared meanings through
observing the usage and manipulation of symbols by others early in life, and also by engaging in
symbolic exchange with others. As such, we expected the fundamental sentiments
conventionally provided by raters would also be available in large samples of naturally
generated language. Recent advances in the study of natural language processing and artificial
intelligence have partially overcome many barriers to extracting cultural meaning from
language use, making a rigorous assessment of this assumption possible. Word embeddings in
particular inductively extract aggregate patterns in large text corpora and have been
demonstrated to capture widely held cognitive associations between words and concepts.
We show in a theoretically informed way that word embeddings indeed capture the
fundamental sentiments of concepts that are of interest to ACT researchers. Specifically, we
show that the semantic similarity between the word that is the “root” of a concept and words
that are used as endpoints in traditional semantic differential scales to measure the
fundamental sentiments of concepts significantly correlates with survey-based measurements
of that concept’s EPA profile. This supports the claim that the culturally informed exchange of

symbols between two or more members of a society contains sufficient information for others
to adduce the fundamental sentiments of concepts through observation and pattern induction.
The research community of ACT at present largely relies on centrally collected, questionnairebased EPA dictionaries to do empirical research. These surveys, while an invaluable research
tool, have inherent limitations. We test whether researchers can use deep learning in concert
with information extracted from natural language via word embeddings to augment traditional
semantic differential surveys, decreasing the marginal cost of estimating the EPA profile of any
additional concepts to running a few lines of code. We demonstrate using a predictive modeling
paradigm that we are can accurately learn how to map word embeddings to the EPA profiles of
concepts included in the US Online 2015 EPA dictionary. We further demonstrate that we are
able to accurately predict the EPA ratings of concepts that were not included in this dictionary.
Using our algorithm, researchers can augment traditional EPA dictionaries to include any
concept for which they have text data documenting the patterned way in which the linguistic
representation of that concept is used in natural symbolic exchange. In this process, we also
develop an algorithm which automatically generates the sorts of concepts that are of interest
to ACT researchers as well as an open-sourced web application that more efficiently elicits and
collects EPA ratings from respondents. We make all of these resources publicly available to the
ACT community.
While we were able to predict the EPA profile of concepts with surprising accuracy, the
accuracy we achieve should not be considered the upper limit of predictive performance. We
had expected our use of a more recent word embedding algorithm (BERT) would provide still
better predictions, but our application of it was not successful. Nevertheless, computational
linguistics and machine learning are fast-moving fields. Future research might consider
experimenting with the approach we take in our first study, and test whether different word
embedding algorithms, different corpora, or different anchor words might increase
performance. Additional work might also collaborate with professional machine learning
practitioners or hold a “common task” competition (Salganik et al 2020) where many teams of
researchers compete to produce models which most accurately predict the EPA profiles of
concepts. Hosting a competition on the platform Kaggle17 could be a next step for researchers
interested in building more accurate EPA dictionary augmentations.
The methods we evaluate here allow for ACT researchers to address questions that would
otherwise be impractical. While our efforts have focused on trying to measure consensual,
culture-wide fundamental sentiment, an important direction of work in ACT has been
examining variation in cultural sentiments by members of different subgroups (Thomas & Heise
1995; Kroska 2008; Sewell & Heise 2010; Rogers 2019). Word-embeddings are corpus-specific,
17

https://www.kaggle.com/c/about/host/

so if separate corpora of text from authors of different cultures or culture subgroups are
assembled, one could use the methodology presented in study 1 to examine variation in
estimated EPA ratings across these corpora. One might also use historical text corpora and EPA
dictionaries in conjunction with the methodologies described studies 2, 3, and 4 to more
comprehensively track the changes in the affective meanings of concepts over time.
We hope that this research helps better connect the ACT community to the worlds of artificial
intelligence and computational linguistics. Given ACT’s quantitative orientation and interest in
the use of symbols such as language and the recent proliferation of computational social
science in sociology more broadly, the potential for fruitful cross-fertilization seems vast (see
also Hoey et al 2018). While here we seek primarily to use tools from artificial intelligence and
computational linguistics to serve the ACT community, we believe that the theoretical insights
of ACT have much more to offer to the computational linguistics community than has been
utilized thus far.
References
Bejnordi, B. E., Veta, M., Van Diest, P. J., Van Ginneken, B., Karssemeijer, N., Litjens, G., ... & Geessink, O.
(2017). Diagnostic assessment of deep learning algorithms for detection of lymph node
metastases in women with breast cancer. Jama, 318(22), 2199-2210.
Berger, P. L. and T. Luckmann. (1966). The Social Construction of Reality: A Treatise in the Sociology of
Knowledge. New York: Penguin.
Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., ... & Zhang, X. (2016). End to
end learning for self-driving cars. arXiv preprint arXiv:1604.07316.
Caliskan, A., Bryson, J. J., & Narayanan, A. (2017). Semantics derived automatically from language
corpora contain human-like biases. Science, 356(6334), 183-186.
Cannon, Bryan C. 2019. “Processing ACT Data.” Unpublished white paper.
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep
Bidirectional Transformers for Language Understanding.” ArXiv:1810.04805 [Cs].
Dingwall, Nicholas and Christopher Potts. 2018. “Mittens: An Extension of GloVe for Learning DomainSpecialized Representations.” Pp. 212–217 in Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 2 (Short Papers). New Orleans, Louisiana: Association for Computational
Linguistics.
Francis, L. E., Lively K. J., König, A., Hoey, J. (2020). “The Affective Self: Preservation of Self-Sentiments in
Late-Life Dementia.” Social Psychology Quarterly 83(2): 152-173.
Freeland, R. E., & Harnois, C. E. 2020. Bridging the Gender Wage Gap: Gendered Cultural Sentiments, Sex
Segregation, and Occupation-Level Wages. Social Psychology Quarterly: 0190272519875777.

Freeland, R. E., & Hoey, J. 2018. The Structure of Deference: Modeling Occupational Status Using Affect
Control Theory. American Sociological Review, 83(2): 243–277.
Garg, N., Schiebinger, L., Jurafsky, D., & Zou, J. (2018). Word embeddings quantify 100 years of gender
and ethnic stereotypes. Proceedings of the National Academy of Sciences, 115(16), E3635E3644.
Gentzkow, M., Kelly, B., & Taddy, M. (2019). Text as data. Journal of Economic Literature, 57(3), 535-74.
Hastie, Trevor. Tibshirani, Robert. Friedman, Jerome. 2009. The Elements of Statistical Learning: Data
Mining, Inference, and Prediction. Springer, New York, NY.
Heise, David R. "Affect control theory: Concepts and model." Journal of Mathematical Sociology 13, no.
1-2 (1987): 1-33.
Heise, David R. Expressive order: Confirming sentiments in social actions. Springer Science & Business
Media, 2007.
Heise, David R. (2019). Cultural meanings and social institutions: Social organization through language.
Cham, Switzerland: Palgrave Macmillan.
Hoey, J., Schöder, T., Morgan, J., Rogers, K. B., Rishi, D., Nagappan, M. (2018). Artificial intelligence and
social simulation: Studying group dynamics on a massive scale. Small Group Research 49(6) 647683.
Holtgraves, T. (Ed.). (2014). The Oxford handbook of language and social psychology. Oxford University
Press, USA.
Hu, M., & Liu, B. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge discovery and data mining (pp. 168-177).
Hunzaker, M. B. F. 2016. Cultural Sentiments and Schema-Consistency Bias in Information
Transmission. American Sociological Review, 81(6): 1223–1250.
Kaelbling, Leslie P.; Littman, Michael L.; Moore, Andrew W. (1996). "Reinforcement Learning: A Survey".
Journal of Artificial Intelligence Research. 4: 237–285. arXiv:cs/9605103. doi:10.1613/jair.301.
Kozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. “The Geometry of Culture: Analyzing the
Meanings of Class through Word Embeddings.” American Sociological Review 84(5):905–49.
Kroska, A. (2008). Examining Husband-Wife Differences in the Meaning of Family Financial Support.
Sociological Perspectives 51(1): 63-90.
Kroska, A. and Cason, T. C. (2019.) “The Gender Gap in Business Leadership: Exploring an Affect Control
Theory Explanation.” Social Psychology Quarterly 82(1): 75-97.
Lee, B. K., Lessler, J., & Stuart, E. A. (2010). “Improving propensity score weighting using machine
learning.” Statistics in medicine, 29(3): 337-346.
MacKinnon, Neil J. (1994). Symbolic Interactionism as Affect Control. Albany: State University of New York Press.

MacKinnon, Neil J. and David R. Heise (2010). Self, Identity, and Social Institutions. New York: Palgrave
Macmillan.
MacKinnon, N. J. and D. Robinson. “Back to the Future: 25 Years of Research in Affect Control Theory.”
Advances in Group Processes 31: 139-173.
McCall, G. J. (2006). “Symbolic Interaction.” Pp 1-23 in Contemporary Social Psychological Theories,
edited by P. J. Burke. Stanford, CA: Stanford University Press.
Mikolov, T., Chen, K., Corrado, G., & Dean, J. 2013. “Efficient estimation of word representations in
vector space”. arXiv preprint, arXiv: 1301.3781.
Osgood, C. E., Suci, G. J. and Tannenbaum, P. H. The Measurement of Meaning. Urbana, IL: University of
Illinois Press.
Pennington, J., Socher, R., & Manning, C. D. 2014. Glove: Global vectors for word representation.
In Proceedings of the 2014 conference on empirical methods in natural language processing
(EMNLP) (pp. 1532-1543).
Peters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. 2018. “Deep Contextualized Word Representations.” ArXiv:1802.05365 [Cs].
Robinson, D. T., & Smith-Lovin, L. 2018. “Affect control theories of social interaction and self.” In P. J.
Burke (Ed.), Contemporary social psychological theories. p. 139–165. Stanford University Press.
Robinson, D. T., Smith-Lovin, L., & Wisecup, A. K. (2006). Affect control theory. In Handbook of the
sociology of emotions (pp. 179-202). Springer, Boston, MA.
Rogers, K. B. (2019). Sources of consensus and variegation in cultural affective meanings. Social
Currents 6(3): 219-238.
Salganik, M. J., Lundberg, I., Kindel, A. T., Ahearn, C. E., Al-Ghoneim, K., Almaatouq, A., ... & Datta, D. (2020).
Measuring the predictability of life outcomes with a scientific mass collaboration. Proceedings of the
National Academy of Sciences, 117(15), 8398-8403.
Sewell, A. A. and Heise, D. R. (2010). “Racial Differences in Sentiments: Exploring Variant Cultures.” International
Journal of Intercultural Relations 34: 400-12.

Shen, L., Margolies, L. R., Rothstein, J. H., Fluder, E., McBride, R., & Sieh, W. (2019). Deep learning to
improve breast cancer detection on screening mammography. Scientific reports, 9(1), 1-12.
Smith-Lovin, Lynn, Dawn T. Robinson, Bryan C. Cannon, Brent H. Curdy, and Jonathan H. Morgan. 2019.
“Mean Affective Ratings of 968 Identities, 853 Behaviors, and 660 Modifiers by Amazon
Mechanical Turk Workers in 2015.” University of Georgia: Distributed at UGA Affect Control
Theory Website: http://research.franklin.uga.edu/act/
Thomas, L. and Heise, D. R. (1995) “Mining error variance and hitting pay dirt: Discovering systematic
variation in social sentiments.” Sociological Quarterly 36: 425-39.

Tshitoyan, V., Dagdelen, J., Weston, L., Dunn, A., Rong, Z., Kononova, O., ... & Jain, A. (2019).
Unsupervised word embeddings capture latent knowledge from materials science
literature. Nature, 571(7763), 95-98.
Yarkoni, T., & Westfall, J. 2017. “Choosing prediction over explanation in psychology: Lessons from machine
learning.” Perspectives on Psychological Science, 12(6): 1100-1122.

FIGURES

Figure 1. Plots of predicted versus observed means in the test set (top; X=Y line plotted for reference);
histogram of prediction errors (middle); and Q-Q plot of errors versus inverse normal (bottom).

Figure 2. Example screen from our ACT Ratings Tool.

Figure 3. Plots of predicted versus observed means in the new set (top); histogram of prediction errors
(middle); and Q-Q plot of errors versus inverse normal (bottom).

SUPPLEMENTAL MATERIALS
Appendix A. Worst performing predictions for each dimension in test set
Concept
Evaluation
Save
Baby
Forget
Feed
Liberate
Escape
Scheme
Flee
Cripple
Con
Potency
Shrimp
Nobody
Spare
Save
Baby
Cripple
Distress
Feed
Parent
Judge
Activity
Cripple
Nobody
Concede
Slack
Deadbeat
Bore
Humble
Reserved
Silly
Hound

Type

Predicted

Actual

Difference

Behavior
Identity
Behavior
Behavior
Behavior
Behavior
Behavior
Behavior
Behavior
Behavior

-0.48
-0.79
1.96
-0.47
-0.49
-1.26
0.29
-3.01
-0.59
-0.22

3.52
2.88
-1.67
2.97
2.94
2.10
-2.70
-0.06
-3.48
-2.90

-4.00
-3.67
3.63
-3.45
-3.43
-3.36
2.99
-2.96
2.88
2.68

Identity
Identity
Behavior
Behavior
Identity
Behavior
Behavior
Behavior
Identity
Behavior

0.58
0.04
-0.42
0.49
-0.51
-1.77
-1.50
0.24
0.34
2.08

-2.46
-2.74
2.31
3.15
-2.92
0.48
0.63
2.32
2.30
0.13

3.04
2.78
-2.72
-2.67
2.41
-2.25
-2.12
-2.08
-1.96
1.95

Behavior
Identity
Behavior
Modifier
Identity
Identity
Behavior
Modifier
Modifier
Behavior

-1.81
0.14
1.23
-0.06
-0.15
-0.04
-1.48
-0.33
-0.17
0.24

1.60
-2.21
-0.96
-2.17
-2.25
-1.94
0.39
-2.16
1.63
2.02

-3.41
2.35
2.18
2.12
2.10
1.90
-1.87
1.83
-1.80
-1.78

Appendix B. “Trouble score” criteria for excluding raters
+1 if > than 85% in a given direction
+1 if > than 40% extreme ratings (+2 if > 75%)
+1 if > than 85% of ratings within .3 of zero
+1 if > than 35% of ratings skipped (+2 if >70%)
+1 if median time is < 2.25 secs (+2 if < 2 secs)
+1 if item-rest correlation is < .4 (+2 if < .1)

Appendix C. Worst performing predictions for each dimension in new set.
Concept
Type
Predicted
Actual
Difference
Evaluation
Sacrifice
Behavior
1.07
-3.40
4.47
Pious
Modifier
2.21
-0.88
3.09
Superhero
Identity
0.62
3.58
-2.96
Impregnate
Behavior
-1.92
0.63
-2.55
Plump
Modifier
1.74
-0.75
2.49
Pornstar
Identity
0.44
-1.76
2.20
Girly
Modifier
-0.98
1.21
-2.18
Validate
Behavior
0.05
2.15
-2.11
Psychic
Identity
2.05
0.03
2.02
Eliminate
Behavior
-1.43
-3.42
1.99
Potency
Dodge
Behavior
0.78
-1.96
2.74
Queen
Identity
0.94
3.61
-2.67
Affluent
Modifier
0.88
3.45
-2.57
Elitist
Modifier
-0.86
1.70
-2.56
Monarch
Identity
0.29
2.83
-2.54
Superhero
Identity
1.25
3.61
-2.36
Unethical
Modifier
0.60
-1.74
2.34
Kindergartener
Identity
-0.25
-2.57
2.33
Privileged
Modifier
-0.08
2.17
-2.25
Peasant
Identity
-1.21
-3.33
2.12
Activity
Perky
Modifier
0.21
2.97
-2.76
Salute
Behavior
1.66
-0.85
2.51
Hunter
Identity
1.13
-1.31
2.44
Pornstar
Identity
0.67
2.93
-2.26
Stalk
Behavior
0.82
-1.41
2.23
Upbeat
Modifier
0.73
2.93
-2.20
Goofy
Modifier
0.24
2.40
-2.16
Kindergartener
Identity
0.63
2.74
-2.11
Slaughter
Behavior
1.84
3.71
-1.87
Ninja
Identity
1.42
-0.38
1.80

