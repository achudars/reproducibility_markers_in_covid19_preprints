Proofs, for final version see
http://publikationen.soziologie.de/index.php/kongressband_2016/article/view/390
Cite as:
Hamann, J., 2017: „Ranking Research: Rankings as Devices in Higher Education Governance", in: Lessenich, S. (Ed.):
Geschlossene Gesellschaften. Verhandlungen des 38. Kongresses der Deutschen Gesellschaft für Soziologie in
Bamberg 2016.
Copyright (c) 2017 Geschlossene Gesellschaften - 38. Kongress der Deutschen Gesellschaft für Soziologie

Ranking Research: Rankings as Devices in Higher
Education Governance
Julian Hamann

Beitrag zur Ad-hoc-Gruppe »Rankings – historisch-soziologisch gesehen«

Ranking Research Performance in the United Kingdom
In the last decades, most countries in the western world have introduced some form of centralized
quality assessment and control in academia (Geuna, Martin 2003; Paradeise et al. 2009). New
technologies of control and measurement have been implemented in higher education systems, with
‘quality’ and ‘excellence’ becoming catchphrases for governance regimes that are located at the interface of the state, the market, and academia (Maeße 2016; Münch 2014). Ranking systems and league
tables are proven devices that rank order the results of evaluations and visualize them for a broader
public (Shin et al. 2011).
One of the earliest attempts to systematically assess research performance and rank respective
institutions has been made in the United Kingdom (UK). Since 1986, assessments of research quality
are delivered roughly every five years by the Research Assessment Exercise (RAE) and, since 2014, by
the Research Excellence Framework (REF). The assessments can therefore be considered to be
amongst the most sophisticated research performance assessments in the world. 1 They follow a seemingly straightforward principle: “Institutions conducting the best research receive a larger proportion of
the available grant so that the infrastructure for the top level of research in the UK is protected and
developed.” (RAE 2001c) The idea is to identify where to allocate scarce resources, to compare research
performance within and between departments and universities, and to concentrate funds in selected
institutions. Rankings play an important role in this set-up.
The assessments grade – and thus, quantify – the quality of research across dozens of fields and
more than 150 institutions. They are conducted by the four UK higher education funding bodies and
based on a peer review system that is organized in subject-specific assessment panels. The panels are
charged with “identifying excellence in the rich diversity of research” (REF 2012) they cover. The panels

1

Studies on the evolution of the RAE/REF over the years abound (Bence, Oppenheim 2005; Martin,
Whitley 2010). While the RAE 2001 has above all been characterized by a grade inflation and a
subsequently more concentrated funding policy by the Higher Education Funding Council for England,
the main change in the RAE 2008 was the introduction of research profiles for each department, based
on what proportion of its publications was judged to be of national or international quality. The most
important novelty of the REF is that ‘output quality’ (now weighed at 65 percent) is supplemented with
‘impact’ (20 percent) and ‘research environment’ (15 percent) (REF 2011).

JULIAN HAMANN
evaluate most notably research output (normally publications) from designated research staff that has
been selected for assessment by the respective higher education institutions. Hence, the subject panels have to assess a broad variety of research that is conducted in hyper-differentiated research fields.
This process can be expected to be extremely intricate and complicated (Sharp, Coleman 2005; cf.
Lamont, Huutoniemi 2011). Nonetheless, in the end a grade system is employed that appears rather
simplistic and vague (see Table 1).
Table 1: Definition of Research Quality According to RAE 2008 and REF 2014

Rating

Description

4*

Quality that is world-leading in terms of originality, significance and rigour

3*

Quality that is internationally excellent in terms of originality, significance
and rigour but which nonetheless falls short of the highest standards of
excellence

2*

Quality that is recognised internationally in terms of originality, significance
and rigour

1*

Quality that is recognised nationally in terms of originality, significance and
rigour

Unclassified

Quality that falls below the standard of nationally recognised work. Or work
which does not meet the published definition of research for the purposes
of this assessment.

Sources: (RAE 2008b; REF 2011)
Based on the grade system displayed in Table 1, the most recent REF 2014 assessed the research of
52,061 researchers at 154 universities. Overall, 76 percent of all submissions have been awarded with
the two best grades, 4* and 3* (cf. REF 2014b, 2014c). This share is crucial, because the assessments
are more than just symbolic distinctions. If this were the case, they would be open to interpretation.
Merely symbolic assessment results would have their own potency, but in the end they would be fuzzy
enough to be contested, or even ignored – in fact, this is the case with a number of rankings in the
higher education sector. However, as it is, the RAE/REF assessments are translated into very robust
material classes because they inform the allocation of public research funding by the UK funding councils. The most recent REF, for example, is used to allocate £1.6 billion annually for the next six years.
Eighty percent of these funds are distributed among departments rated 4*, 20 percent are going to 3*
research, while other grade scales are not funded (Wilsdon 2015; HEFCE 2015). This material foundation underscores the vital significance the assessments have for the academic landscape in the UK.
The very selective distribution of funds seems to rectify the quite generous grading that is carried out
by the subject-specific panels.
This paper proceeds with a brief review of the literature on rankings in the higher education sector.
It distinguishes literature on standardization effects and on stratification effects. The main section of
the paper presents different rankings that are produced from the RAE/REF data. Among them are
rankings produced directly by the RAE/REF, and rankings that media outlets produce drawing on the
RAE/REF data. Informed by case examples, the closing section discusses the performativity of rankings.

2

RANKING RESEARCH: RANKINGS

AS

DEVICES

IN

H I G H E R E D U C AT I O N G O V E R N A N C E

It looks into the different implications and effects rankings and the associated performance assessments have on higher education institutions, their personnel, and the personnel’s practices.

Ranking Research: Effects and Defects
The RAE/REF have been evaluating research in the UK since the 1980s. Since the very beginning, the
assessments have been scrutinized by research on performance assessments and rankings. Overall,
the assessments have responded to the critique they received, thereby becoming more and more
differentiated and elaborate. Hence, it is worthwhile to retrace the co-evolution of technical critique of
criteria, calculation, and compilation of the rankings, as well as their actual development (cf. Tomlin
1998). However, not every shortcoming of research performance assessment can be addressed by
reforming the assessment device, and indeed, the question is whether effects that are identified as
shortcomings in the literature are actually deficiencies, or, instead, very much intended consequences.
Some effects, intended or not, are more fundamentally part of the very nature of research performance assessment. Two such symptoms are highlighted in the following.
One body of literature addresses symptoms of adaption to the criteria of performance assessments (Espeland, Sauder 2007). Adaptions of academic practice have been identified for a number of
western countries (cf. Benner, Sandström 2000 for Sweden; Harman 2005 for Australia; Münch 2008
for Germany; Leišytė, Westerheijden 2014 for the Netherlands; Sayer 2014 for the UK) and for several
disciplines (cf. Campbell et al. 1999 for law studies; Morris, Rip 2006 for neuro sciences; Kehm, Leišytė
2010 for medieval history; Lee et al. 2013 for economics; Hamann 2016 for history). In all cases,
assessments seem to have the effect that researchers stay mostly within the mainstream areas of
their field in order to maintain chances of receiving funding. In light of these developments, it becomes
increasingly difficult to determine research excellence, since adaptations to the assessment criteria –
and panel members’ reluctance to penalize their own field – produce ever increasing proportions of
‘excellent’ research (Tapper, Salter 2004; Wooding et al. 2015).
In addition to literature on standardization symptoms, a second body of literature discusses
stratification effects of research performance assessments, rankings, and the allocation of resources.
Focusing on new strata of managerial accountability, studies have revealed that status assignments
create new patterns of authority (Whitley et al. 2010), for example between panel members and the
colleagues whose research they are judging (Sharp, Coleman 2005; Sayer 2014), between research
active personnel and teaching oriented personnel (Elton 2000; Tapper, Salter 2002), or between
university management and the departments in which they intervene (Henkel 1999; Yokoyama 2006).
It has been argued that effects of academic elitism and stratification are not limited to the academic
field, but occur in other societal realms as well (Maeße 2016; cf. Maeße, Hamann 2016). This body of
literature understands procedures of resource distribution as mechanisms for the construction of
status hierarchies. The identification of ‘excellence’ is thus impeded by the structurally unequal
distribution of social, economic, and symbolic resources (Burris 2004; Morgan 2004; Münch 2008;
Hamann 2017).

3

JULIAN HAMANN

Ranking Research Performance: the Rankings Produced by the
RAE/REF
Most rankings in the higher education sector are produced and distributed by single sources. Among
these are consultancies, as is the case for the so-called Shanghai Ranking, think tanks like the German
Center for Higher Education Development (CHE), news outlets, such as the Canadian magazine
Maclean’s and The Economist, or political actors like the United States National Research Council. In
the current case example, rankings are not only produced by the RAEs/REF themselves – and thus,
indirectly, by the four UK funding councils that organize them – but also by media outlets like the
Times Higher Education magazine (THE) and The Guardian. It is a peculiarity of the current example
that media outlets not only distribute existing rankings, but also produce their own versions that draw
on the RAE/REF data. From this emerges a variety of rankings illustrating how the RAEs/REF’s influence
goes beyond funding policy. The rankings also shape the public perception of departments in a way
that seems to be unique to the UK. This section discusses two ‘generations’ of RAE/REF rankings as well
as The Guardian and THE rankings.
The rankings produced by the RAE 2001 proceed either from different subjects within one higher
education institution (see Figure 1-1 for the University of Cambridge), or from departments at different
higher education institutions that are ranked according to the same subject (see Figure 1-2 for history).
The first column, “2001 rating”, designates the assigned grade. The second column, “proportion of staff
selected”, states the number of faculty members within a department who have been registered as
researchers and thus have their research assessed.2 The column on the very right designates the number of full-time equivalent (FTE) research positions at a department.
Two aspects seem worth highlighting for the RAE 2001 rankings in Figures 1-1 and 1-2. First, the
assessment itself simultaneously produces rankings for universities (Figure 1-1) and for departments
(Figure 1-2). In other words, the assessment offers two competing perspectives that draw on two distinct levels of comparison: an inter- and an intra-university perspective. It seems counterintuitive for a
ranking to operate on two equally important comparative orders. Second, the rank order in both rankings is numerical when it follows the order of the subject areas that have been assessed at a higher
education institution (Figure 1-1), and alphabetical when it ranks the departments that have been
assessed in one subject area (Figure 1-2). A performance-based ranking is only available when the data
is downloaded in an Excel file.

2

A = 95–100 percent of the staff at a department, B = 80–94.9 percent of the staff, C = 60–79.9 percent,
etc.

4

RANKING RESEARCH: RANKINGS

AS

DEVICES

IN

H I G H E R E D U C AT I O N G O V E R N A N C E

Source: (RAE 2001a)
Figure 1-1: RAE 2001 ranking by higher education institution

Source: (RAE 2001b)
Figure 1-2: RAE 2001 ranking by subject
Thirteen years and two assessments later, the REF 2014 rankings still offer two competing levels of
comparison and employ numerical and alphabetical rank orders (Figures 2-1, 2-2). However, there has
been an important innovation since 2001. Results are published as graded quality profiles rather than
fixed point scales. This was supposed to allow the funding bodies to “identify pockets of excellence

5

JULIAN HAMANN
wherever these might be found”, and to reduce the “'cliff edge' effect where fine judgements at the
grade boundaries could have significant funding impacts” (RAE 2008a).

Source: (REF 2014c)
Figure 2-1: REF 2014 ranking by subject

Source: (REF 2014b)
Figure 2-2: REF 2014 ranking by institution

6

RANKING RESEARCH: RANKINGS

AS

DEVICES

IN

H I G H E R E D U C AT I O N G O V E R N A N C E

Looking more closely at Figures 2-1 and 2-2, we also see that research performance has been
differentiated. Going beyond a general statement on quality, which is still included in the category
‘overall quality’, three elements of assessment distinguish, first, research output, covering mainly
publications; second, the research environment of a department, ranging from the number of PhD
students over cooperation to third party funding; and third, the ‘societal impact’ of research, which is
supposed to capture the social relevance of research (REF 2011, 2014a). Thus, in both Figures 2-1 and
2-2, the rankings depict overall quality profiles as well as the sub-profiles regarding research output,
societal impact, and research environment that are awarded to each submission. Overall quality profiles show the proportion judged to meet each starred level, while sub-profiles show the proportion of
research output, societal impact, and research environment judged to meet each starred level.
In addition to the rankings that are produced directly by the RAE/REF, media outlets take up the
assessment results in order to construct their own rankings from the data.3 The Guardian, for example, published a ‘Power Ranking’ by Research Fortnight, a newsletter on research policy (Figure 3). The
‘Power Ranking’ does not depict research performance, but research funding – or, to be more precise,
expected research funding.

Source: (The Guardian 2014)
Figure 3: Research Fortnight’s ‘Power Ranking’ in The Guardian, based on the REF 2014
The ‘Power Ranking’ is based on the following metrics (weighted by research staff): Power rating in
column six, depicting how the university’s Quality Index score relates to the top performing university,
in this case Oxford; Quality Index in column seven, a weighting of the university’s score based on the
expected funding allocation; and Market share in column eight, illustrating how much Research Fortnight predicts the university will get of the overall available funding next year based on its results.

3

Both rankings discussed in the following were published on the same day or one day after the REF
2014 results were officially published. This demonstrates the high significance this topic has in UK
media.

7

JULIAN HAMANN
According to the self-description, the “rankings reflect what the REF is about, i.e., the money” (The
Guardian 2014).
Times Higher Education, a London-based magazine publishing its own influential World University
Rankings, is another media outlet that dissects the RAE/REF results in order to build its own rankings.
From this data, THE offers alternative rankings, among them an ‘Intensity Ranking’ that sets out to map
university performance against the proportion of eligible research staff submitted (Times Higher
Education 2014c), a ‘Table of Excellence’ that ranks institutions according to the average points of their
overall quality profiles (Times Higher Education 2014b), and subject ratings that rank departments
according to their grades in different subjects (Figure 4).

Source: (Times Higher Education 2014a)
Figure 4: Times Higher Education ranking by subject, based on the REF 2014
In THE’s subject ranking, ranks are mainly assigned by the grade point average (GPA) of the overall
quality profiles, which in turn are made up of the sub-profiles for research output, societal impact, and
research environment. The GPAs are compared to GPAs of the previous RAE 2008 assessment. THE’s
use of average points is worth highlighting because it subverts the idea of concentrating on differentiated profiles, and instead aggregates these (back) to simple numeric measures. While the REF itself
does not produce or use GPAs, it is no coincidence that THE draws on a measure that is supposedly
more marketable in mass media.

The Performativity of Rankings: what are Rankings actually doing?
Instead of a conclusion, the paper closes with a reflection on a couple of effects and implications of
higher education rankings in general and the RAE/REF rankings in particular. Not all of the following
points are effects of rankings according to a narrow statistical notion of causality. They can partially be
ascribed to research performance assessment in the broader sense, and thus refer back to the literature reviewed in a previous section of this paper. In combination with the literature review, this section

8

RANKING RESEARCH: RANKINGS

AS

DEVICES

IN

H I G H E R E D U C AT I O N G O V E R N A N C E

also indicates hitherto neglected effects that future research will have to concentrate on, for example
systematic disciplinary comparisons and perspectives that focus on the social processes that rankings
build on.
First, rankings construct research fields. Any research activity that is submitted for assessment has
to be assigned to a subject area, which is called Unit of Assessment (UoA) in assessment lingo. Some
UoAs comprise traditional academic disciplines, for example, physics (UoA9). However, the grid of the
UoAs does not necessarily follow disciplinary distinctions. For instance, depending on their specialization, physicians can submit their research to no less than four UoAs (UoA1 ‘Clinical Medicine’; UoA2
‘Public Health, Health Services and Primary Care’; UoA3 ‘Allied Health Professions, Dentistry, Nursing
and Pharmacy’; UoA4 ‘Psychology, Psychiatry and Neuroscience’). In the end, this institutional structure
determines how research is represented, perceived, and funded. Applying a unified matrix to elements
that are different in nature, the formalized grid of the RAE/REF is an example of a process that has
been described as commensuration (cf. Espeland, Stevens 1998).
Second, rankings reduce complexity. They make ‘research quality’, a very complicated, ambiguous,
and vague notion that is constructed anew in every specific situation (Lamont 2009), comparable by
translating it into numbers. This ambitious task is entrusted to peers in the subject panels, who review
each submission and grade it according to the uniform grade system. Usually, quality is only intelligible
to members of the same hyper-differentiated sub-sub-discipline. The rankings, conversely, make
researchers and their performance comparable and governable (cf. Bovens et al. 2014; Morrissey 2013).
Third, rankings visualize performance differences. Building on the reduction of complexity
described in the previous paragraph, rankings compare research outputs and distinguish higher
education institutions according to the differences that have been determined. Rankings present these
hierarchical differences to the public. The RAE was introduced along with New Public Management
measures in the 1980s. For this neoliberal governance approach, the public visibility of performance
differences is an important contribution of rankings. It allows for transparency and public accountability,
and thus signals the trustworthiness of the funding system (cf. Strathern 2000a, 2000b).
Fourth, rankings legitimize funding decisions. The RAE/REF rankings reveal on which basis the UK
higher education funding bodies make their funding decisions. The rankings illustrate, for example,
why the University of Cambridge receives a particular amount of funding while Plymouth University
receives a different amount, and they show why some philosophy departments receive less money
than clinical medicine institutions. In this light, rankings are devices that facilitate negotiations about
funding decisions in the first place; they further legitimize and thus stabilize evaluative decisions (cf.
Walker et al. 1986).
Fifth, rankings standardize research. The literature strongly suggests that rankings and performance assessments encourage researchers to concentrate on risk-averse normal science. Indeed, the
literature on RAE/REF shows that the assessments reward articles in high impact journals in disciplines
as diverse as neurosciences (Morris, Rip 2006), history (Hamann 2016), law (Campbell et al. 1999), or
economics (Lee et al. 2013). Scholarship on performance assessment explains this with the reactivity
of rankings that standardize research by rewarding the adaption to assessment criteria (cf. Espeland,
Sauder 2007).
Sixth, rankings stratify the higher education landscape. While the RAE/REF employ a meritocratic
rhetoric, stating that “institutions conducting the best research receive a larger proportion of the
available grant” (RAE 2001c), they also reproduce existing inequalities in the distribution of resources
in the field. Economic, social, and symbolic resources correlate with respective ranks (Münch, Schäfer
2014; Hamann 2017). By rewarding higher education institutions that already have greater resources

9

JULIAN HAMANN
at their disposal, the rankings not only consecrate and legitimize existing inequalities (cf. Merton 1968
on the Matthew effect), but also further reproduce stratifications (cf. Bourdieu 1988). These inequalities
are not necessarily based on research performance (alone).
The manifold performativity of rankings demonstrates that rankings have a number of effects and
implications – in other words, rankings do a number of quite diverse things. According to perspectives
in cultural sociology, modern capitalist societies tend to credentialize power with certificates, degrees,
and other labels (Collins 1979; Bourdieu et al. 1981). In this view, rankings create symbolic dominance
by commensurating different entities, constructing distinctions and comparisons, making visible differences, and consecrating those at the top of the ranking. However, the rankings that are produced from
the RAE/REF are fertile illustrative material because, unlike a number of other rankings in the higher
education sector, they go beyond symbolic differences and distinctions. They are devices that couple
rank order with research funding, and thus develop the potency to influence and sustainably modify
higher education by standardizing research and reproducing structural inequalities. Thus, from a
materialist perspective of political economy (Maeße 2015; Jessop et al. 2008), the current case example
illustrates how symbolic differences are translated into material classes, and how material classes are
in turn legitimized by symbolic consecration (Bourdieu 1988).

References
Bence, V., Oppenheim, C. 2005: The evolution of the UK's research assessment exercise: Publications,
performance and perceptions. Journal of Educational Administration and History, Vol. 37, Issue 2, 137–
155.
Benner, M., Sandström, U. 2000: Institutionalizing the triple helix: Research funding and norms in the
academic system. Research Policy, Vol. 29, Issue 2, 291–301.
Bourdieu, P. 1988: Homo Academicus. Cambridge: Polity Press.
Bourdieu, P., Boltanski, L., de Saint-Martin, M., Maldidier-Pargamin, P. (Hg.) 1981: Titel und Stelle. Über die
Reproduktion sozialer Macht. Frankfurt am Main: Europäische Verlags-Anstalt.
Bovens, M., Goodin, R. E., Schillemans, T. (eds.) 2014: The Oxford handbook of public accountability. Oxford:
Oxford University Press.
Burris, V. 2004: The academic caste system: Prestige hierarchies in PhD exchange networks. American
Sociological Review, Vol. 69, Issue 2, 239–264.
Campbell, K., Vick, D. W., Murray, A. D., Little, G. F. 1999: Journal publishing, journal reputation, and the
United Kingdom's Research Assessment Exercise. Journal of Law and Society, Vol. 26, Issue 4, 470–501.
Collins, R. 1979: The credential society: A historical sociology of education and stratification. New York:
Academic Press.
Elton, L. 2000: The UK Research Assessment Exercise: Unintended consequences. Higher Education
Quarterly, Vol. 54, Issue 3, 274–283.
Espeland, W. N., Sauder, M. 2007: Rankings and reactivity. How public measures recreate social worlds.
American Journal of Sociology, Vol. 113, Issue 1, 1–40.
Espeland, W. N., Stevens, M. L. 1998: Commensuration as a social process. Annual Review of Sociology, Vol.
24, 313–343.
Geuna, A., Martin, B. R. 2003: University research evaluation and funding: An international comparison.
Minerva, Vol. 41, Issue 4, 277–304.

10

RANKING RESEARCH: RANKINGS

AS

DEVICES

IN

H I G H E R E D U C AT I O N G O V E R N A N C E

Hamann, J. 2016: The visible hand of research performance assessment. Higher Education, Vol. 72, Issue 6,
1–19.
DOI: 10.1007/s10734-015-9974-7
761-779
Hamann, J. 2017: The production of research elites. Research performance assessment in the United
Kingdom. In R. Bloch, A. Mitterle, C. Paradeise, T. Peter (eds.), Elite
universities
andproduction
the production
of
Universities
and the
of Elites.
academic elites.
Discourses,
policies
and strategies
of excellenceinand
prestige
in higher
New
Discourses,
policies
and strategies
of excellence
and stratification
higher
education.
Neweducation.
York: Palgrave
York: Palgrave
Macmillan, forthcoming.
Macmillan,
175-199.
Harman, G. 2005: Australian social scientists and transition to a more commercial university environment.
Higher Education Research & Development, Vol. 24, Issue 1, 79–94.
HEFCE. 2015: How we fund research. Higher Education Funding Council England,
http://www.hefce.ac.uk/rsrch/funding/mainstream/ (Accessed 15. January 2016).
Henkel, M. 1999: The modernisation of research evaluation: The case of the UK. Higher Education, Vol. 38,
Issue 1, 105–122.
Jessop, B., Fairclough, N., Wodak, R. 2008: Education and the knowledge-based economy in Europe. London:
Sense Publishers.
Kehm, B. M., Leišytė, L. 2010: Effects of new governance on research in the humanities. The example of
medieval history. In D. Jansen (ed.), Governance and performance in the German public research sector.
Disciplinary differences. Berlin: Springer, 73–90.
Lamont, M. 2009: How professors think. Inside the curious world of academic judgement. Cambridge, MA:
Harvard University Press.
Lamont, M., Huutoniemi, K. 2011: Comparing customary rules of fairness: Evaluative practices in various
types of peer review panels. In C. Camic, N. Gross, M. Lamont (eds.), Social knowledge in the making.
Chicago: University of Chicago Press, 209–232.
Lee, F. S., Pham, X., Gu, G. 2013: The UK Research Assessment Exercise and the narrowing of UK economics.
Cambridge Journal of Economics, Vol. 37, Issue 4, 693–717.
Leišytė, L., Westerheijden, D. 2014: Research evaluation and its implications for academic research in the
United Kingdom and the Netherlands. Discussion papers des Zentrums für HochschulBildung, Nr. 12014, 3–32. Dortmund: Technische Universität Dortmund.
Maeße, J. 2015: Economic experts. A discursive political economy of economics. Journal of Multicultural
Discourses, Vol. 10, Issue 3, 279–305.
Maeße, J. 2016: The elitism dispositif. Hierarchization, excellence orientation and organizational change in
economics. Higher Education, accepted for publication.
Maeße, J., Hamann, J. 2016: Die Universität als Dispositiv. Die gesellschaftliche Einbettung von Bildung und
Wissenschaft aus diskurstheoretischer Perspektive. Zeitschrift für Diskursforschung, Heft 2016-1, 29–50.
Martin, B. R., Whitley, R. D. 2010: The UK Research Assessment Exercise. A case of regulatory capture? In R.
D. Whitley, J. Gläser, L. Engwall (eds.), Reconfiguring knowledge production. Changing authority
relationships in the sciences and their consequences for intellectual innovation. Oxford: Oxford
University Press, 51–80.
Merton, R. K. 1968: The Matthew effect in science. Science, Vol. 159, Issue 3810, 56–63.
Morgan, K. J. 2004: The Research Assessment Exercise in English universities, 2001. Higher Education, Vol.
48, Issue 4, 461–482.
Morris, N., Rip, A. 2006: Scientists' coping strategies in an evolving research system: The case of life scientists
in the UK. Science and Public Policy, Vol. 33, Issue 4, 253–263.
Morrissey, J. 2013: Governing the academic subject: Foucault, governmentality and the performing
university. Oxford Review of Education, Vol. 39, Issue 6, 797–810.

11

JULIAN HAMANN
Münch, R. 2008: Stratifikation durch Evaluation. Mechanismen der Konstruktion und Reproduktion von
Statushierarchien in der Forschung. Zeitschrift für Soziologie, 37. Jg., Heft 1, 60–80.
Münch, R. 2014: Academic capitalism. Universities in the global struggle for excellence. New York: Routledge.
Münch, R., Schäfer, L. O. 2014: Rankings, diversity and the power of renewal in science. A comparison
between Germany, the UK and the US. European Journal of Education, Vol. 49, Issue 1, 60–76.
Paradeise, C., Reale, E., Bleiklie, I., Ferlie, E. (eds.) 2009: University governance. Western European
perspectives. Dordrecht: Springer.
RAE. 2001a: 2001 Research Assessment Exercise. Institution: H-0114, University of Cambridge.
http://www.rae.ac.uk/2001/Submissions/UoA.asp?HESAInst=H-0114 (Accessed 28. December 2016).
RAE. 2001b: 2001 Research Assessment Exercise. Unit of assessment: 59, History.
http://www.rae.ac.uk/2001/results/byuoa/uoa59.htm (Accessed 08. August 2015).
RAE. 2001c: What is the RAE 2001? http://www.rae.ac.uk/2001/AboutUs/ (Accessed 08. August 2015).
RAE. 2008a: Changes since the RAE 2001. http://www.rae.ac.uk/aboutus/changes.asp (Accessed 08. August
2015).
RAE. 2008b: Quality profiles. http://www.rae.ac.uk/aboutus/quality.asp (Accessed 12. October 2014).
REF. 2011: Assessment framework and guidance on submissions.
http://www.ref.ac.uk/media/ref/content/pub/assessmentframeworkandguidanceonsubmissions/GOS%2
0including%20addendum.pdf (Accessed 08. August 2015).
REF. 2012: Panel criteria and working methods, part 2D: Main panel D criteria.
http://www.ref.ac.uk/media/ref/content/pub/panelcriteriaandworkingmethods/01_12_2D.pdf (Accessed
08. August 2015).
REF. 2014a: Annex A. Assessment criteria and level definitions.
http://www.ref.ac.uk/media/ref/content/pub/REF%2001%202014%20-%20Annexes.pdf (Accessed 16.
January 2016).
REF. 2014b: View results and submissions by institution. http://results.ref.ac.uk/Results/SelectHei (Accessed
15. January 2016).
REF. 2014c: View results and submissions by UOA. http://results.ref.ac.uk/Results/SelectUoa (Accessed 15.
January 2016).
Sayer, D. 2014: Rank hypocrisies. The insult of the REF. New York: Sage.
Sharp, S., Coleman, S. 2005: Ratings in the Research Assessment Exercise 2001. The patterns of university
status and panel membership. Higher Education Quarterly, Vol. 59, Issue 2, 153–171.
Shin, J. C., Toutkoushian, R. K., Teichler, U. (eds.) 2011: University rankings. Theoretical basis, methodology
and impacts on global higher education. Dordrecht: Springer.
Strathern, M. 2000a: Audit cultures: Anthropological studies in accountability, ethics and the academy.
London/New York: Routledge.
Strathern, M. 2000b: The tyranny of transparency. British Educational Research Journal, Vol. 26, Issue 3, 309–
321.
Tapper, T., Salter, B. 2002: The external pressures on the internal governance of universities. Higher
Education Quarterly, Vol. 56, Issue 3, 245–256.
Tapper, T., Salter, B. 2004: Governance of higher education in Britain: The significance of the Research
Assessment Exercise for the founding council model. Higher Education Quarterly, Vol. 58, Issue 1, 4–30.
The Guardian 2014: University Research Excellence Framework 2014: The full rankings.
http://www.theguardian.com/news/datablog/ng-interactive/2014/dec/18/university-research-excellenceframework-2014-full-rankings (Accessed 15. January 2016).

12

RANKING RESEARCH: RANKINGS

AS

DEVICES

IN

H I G H E R E D U C AT I O N G O V E R N A N C E

Times Higher Education 2014a: REF 2014 results by subject.
https://www.timeshighereducation.com/features/ref-2014-results-by-subject/2017594.article,
https://www.timeshighereducation.co.uk/sites/default/files/Attachments/2014/12/17/x/o/z/sub-1401.pdf (Accessed 15. January 2016).
Times Higher Education 2014b: REF 2014 results: Table of excellence.
https://www.timeshighereducation.com/news/ref-2014-results-table-of-excellence/2017590.article
(Accessed 16. January 2016).
Times Higher Education 2014c: REF 2014: Winners and losers in 'intensity' ranking.
https://www.timeshighereducation.com/news/ref-2014-winners-and-losers-in-intensityranking/2017633.article (Accessed 16. January 2016).
Tomlin, R. 1998: Research League Table: Is there a better way? Higher Education Quarterly, Vol. 52, Issue 2,
204–220.
Walker, H. A., Thomas, G. M., Zelditch, M. 1986: Legitimation, endorsement, and stability. Social Forces, Vol.
64, Issue 3, 620–643.
Whitley, R. D., Gläser, J., Engwall, L. (eds.) 2010: Reconfiguring knowledge production. Changing authority
relationships in the sciences and their consequences for intellectual innovation. Oxford: Oxford
University Press.
Wilsdon, J. 2015: In defence of the Research Excellence Framework. The Guardian.
http://www.theguardian.com/science/political-science/2015/jul/27/in-defence-of-the-ref (Accessed 04.
January 2016).
Wooding, S., van Leeuwen, T. N., Parks, S., Kapur, S., Grant, J. 2015: UK doubles its "world-leading" research
in life sciences and medicine in six years: Testing the claim? PLOS ONE, Vol. 10, Issue 7: e0132990.
Yokoyama, K. 2006: The effect of the research assessment exercise on organisational culture in English
universities: Collegiality versus managerialism. Tertiary Education and Management, Vol. 12, Issue 4,
311–322.

13

