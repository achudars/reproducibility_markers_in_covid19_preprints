bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

DeepMF: Deciphering the Latent Patterns in Omics Profiles with a
Deep Learning Method
Lingxi Chen1,+ , Jiao Xu1,+ , Shuai Cheng Li1,*
1 City University of Hong Kong, 83 Tat Chee Ave, Kowloon Tong, HongKong, China
+ Equal Contributor
* Correspondence: shuaicli@cityu.edu.hk

Abstract
With recent advances in high-throughput technologies, matrix factorization techniques are increasingly being utilized for mapping quantitative omics profiling matrix data into low-dimensional
embedding space, in the hope of uncovering insights in the underlying biological processes. Nevertheless, current matrix factorization tools fall short in handling noisy data and missing entries,
both deficiencies that are often found in real-life data. Here, we propose DeepMF, a deep neural
network-based factorization model. DeepMF disentangles the association between molecular featureassociated and sample-associated latent matrices, and is tolerant to noisy and missing values. It
exhibited feasible subtype discovery efficacy on mRNA, miRNA, and protein profiles of medulloblastoma cancer, leukemia cancer, breast cancer, and small-blue-round-cell cancer, achieving the highest
clustering accuracy of 76%, 100%, 92%, and 100% respectively. When analyzing data sets with 70%
missing entries, DeepMF gave the best recovery capacity with silhouette values of 0.47, 0.6, 0.28,
and 0.44, outperforming other state-of-the-art MF tools on the cancer data sets Medulloblastoma,
Leukemia, TCGA BRCA, and SRBCT. Its embedding strength as measured by clustering accuracy
is 88%, 100%, 84%, and 96% on these data sets, which improves on the current best methods 76%,
100%, 78%, and 87%. DeepMF demonstrated robust denoising, imputation, and embedding ability.
It offers insights to uncover the underlying biological processes such as cancer subtype discovery. Our
implementation of DeepMF can be found at https://gitlab.deepomics.org/jiaox96/DeepMF.

Introduction

1

Recent advances in high-throughput technologies have eased the quantitative profiling of biological
data and enabled many in silico studies to elucidate complex biological processes [1]. In many
cases, the biological data are captured in a matrix with molecular features such as gene, mutation
locus, or species as rows and samples/repetition as columns. Values in the matrices are typically
measurements such as expression abundances, mutation levels, or species counts. Based on the
assumption that samples with similar phenotype (or molecular features) that participate in a similar
biological process will share similar distribution of biological variation [1], patterns shared by a
significant number of entries in these matrices may yield insights on important biological processes.
1/22

2
3
4
5
6
7
8
9

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

Clustering through methods like k-means and hierarchical clustering have been used to identify
these patterns [2, 3]. The success of these studies are contingent on the ability of these clustering
methods to capture the underlying structures or models of the interaction patterns.
Matrix factorization (MF), as given by the formula AM ×N ≈ U M ×K ×V K×N in Figure 1A, is a
popular approach to the problem. Numerous researches have applied MF to identify latent structures
(U and V ) from a given matrix of biological data (AM ×N ). A good factorization technique would
ensure that as much information as possible from A is conserved [1, 2]. Here, we hope to uncover K
hidden signatures from the complex biological processes. We refer to U as the molecular feature
latent matrix, since the values in each column of U are continuous weights illustrating the relative
participation of a molecule in each inferred biology process signature. We call V the sample latent
matrix, as each row of V depicts the fractions of samples in the matched biological process signature.
Molecular features or sample subgroups can be detected by finding patterns in the molecular feature
latent matrix and sample latent matrix, respectively. MF has been successfully applied to multiple
data modalities [1]. For instance, it has been used to detect leukemia cancer subtype based on
expression profiles. By combining gene expression and DNA methylation data, MF has been used
to classify HPV subtypes in head and neck tumors [4]. It has also been used to define COSMIC
mutational signatures in pan-cancer studies [5–7].
MF methods, such as Principal Component Analysis (PCA), Independent Component Analysis
(ICA), and Non-Negative Matrix Factorization (NMF), are widely used to extract the low-dimensional
latent structure from high-dimensional biological matrix [1]. Intuitively, PCA finds governing
variation in high-dimensional data, securing the most important biological process signatures that
differentiate between samples [8]. ICA separates mixed signal matrix into statistically independent
biological process signatures [9]. NMF-based approaches extracted latent matrices with non-negative
constraints [10, 11].
Despite the effectiveness of MF in interpreting biological matrices, several limitations persist
in practice. First, real-world data are often plagued with many types of noises, e.g. systematic
noise, batch effect, and random noise [12], which potentially mask signals in the downstream process.
Second, high throughput omics data frequently suffer from missing values due to various experimental
settings [13], whereas the majority of MF tools have no support for input matrix with missing
values. At present, the standard practice to deal with these two problems is to perform denoising
and imputation prior to MF. However, even when these problems are mitigated, the MF techniques
mentioned would still be unable to uncover any non-linear relationship, since they assume a linear
association between molecular feature latent variables and sample latent variables.
In this work, we propose a deep neural network-based matrix factorization framework, DeepMF
(Figure 1 B), which learns the non-linear association between molecular feature latent matrix
and sample latent matrix, tolerant with noisy and missing entries. DeepMF demonstrated robust
denoising, imputation, and embedding ability in simulated instances. It outperformed the existing
MF tools on subtype discovery in omics profiles of medulloblastoma cancer, leukemia cancer, breast
cancer, and small-blue-round-cell cancer, with the highest clustering accuracy on all the four datasets
collected for this work. Furthermore, with 70% data randomly removed, DeepMF demonstrated the
best recovery capacity with silhouette values 0.47, 0.6, 0.28, and 0.44. It also displayed the best
embedding power on the four datasets, with clustering accuracy of respectively 88%, 100%, 84%,
and 96%, which improves on the current best methods 76%, 100%, 78%, and 87%.

2/22

10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

Background

53

Recent advances in high-throughput technologies have eased the quantitative profiling of biological
data and enabled many in silico studies to elucidate complex biological processes [1]. In many
cases, the biological data are captured in a matrix with molecular features such as gene, mutation
locus, or species as rows and samples/repetition as columns. Values in the matrices are typically
measurements such as expression abundances, mutation levels, or species counts. Based on the
assumption that samples with similar phenotype (or molecular features) that participate in a similar
biological process will share similar distribution of biological variation [1], patterns shared by a
significant number of entries in these matrices may yield insights on important biological processes.
Clustering through methods like k-means and hierarchical clustering have been used to identify
these patterns [2, 3]. The success of these studies are contingent on the ability of these clustering
methods to capture the underlying structures or models of the interaction patterns.
Matrix factorization (MF), as given by the formula AM ×N ≈ U M ×K ×V K×N in Figure 1A, is a
popular approach to the problem. Numerous researches have applied MF to identify latent structures
(U and V ) from a given matrix of biological data (AM ×N ). A good factorization technique would
ensure that as much information as possible from A is conserved [1, 2]. Here, we hope to uncover K
hidden signatures from the complex biological processes. We refer to U as the molecular feature
latent matrix, since the values in each column of U are continuous weights illustrating the relative
participation of a molecule in each inferred biology process signature. We call V the sample latent
matrix, as each row of V depicts the fractions of samples in the matched biological process signature.
Molecular features or sample subgroups can be detected by finding patterns in the molecular feature
latent matrix and sample latent matrix, respectively. MF has been successfully applied to multiple
data modalities [1]. For instance, it has been used to detect leukemia cancer subtype based on
expression profiles. By combining gene expression and DNA methylation data, MF has been used
to classify HPV subtypes in head and neck tumors [4]. It has also been used to define COSMIC
mutational signatures in pan-cancer studies [5–7].
MF methods, such as Principal Component Analysis (PCA), Independent Component Analysis
(ICA), and Non-Negative Matrix Factorization (NMF), are widely used to extract the low-dimensional
latent structure from high-dimensional biological matrix [1]. Intuitively, PCA finds governing
variation in high-dimensional data, securing the most important biological process signatures that
differentiate between samples [8]. ICA separates mixed signal matrix into statistically independent
biological process signatures [9]. NMF-based approaches extracted latent matrices with non-negative
constraints [10, 11].
Despite the effectiveness of MF in interpreting biological matrices, several limitations persist
in practice. First, real-world data are often plagued with many types of noises, e.g. systematic
noise, batch effect, and random noise [12], which potentially mask signals in the downstream process.
Second, high throughput omics data frequently suffer from missing values due to various experimental
settings [13], whereas the majority of MF tools have no support for input matrix with missing
values. At present, the standard practice to deal with these two problems is to perform denoising
and imputation prior to MF. However, even when these problems are mitigated, the MF techniques
mentioned would still be unable to uncover any non-linear relationship, since they assume a linear
association between molecular feature latent variables and sample latent variables.
In this work, we propose a deep neural network-based matrix factorization framework, DeepMF
(Figure 1 B), which learns the non-linear association between molecular feature latent matrix
and sample latent matrix, tolerant with noisy and missing entries. DeepMF demonstrated robust

3/22

54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

denoising, imputation, and embedding ability in simulated instances. It outperformed the existing
MF tools on subtype discovery in omics profiles of medulloblastoma cancer, leukemia cancer, breast
cancer, and small-blue-round-cell cancer, with the highest clustering accuracy on all the four datasets
collected for this work. Furthermore, with 70% data randomly removed, DeepMF demonstrated the
best recovery capacity with silhouette values 0.47, 0.6, 0.28, and 0.44. It also displayed the best
embedding power on the four datasets, with clustering accuracy of respectively 88%, 100%, 84%,
and 96%, which improves on the current best methods 76%, 100%, 78%, and 87%.

98
99
100
101
102
103
104

Method

105

Matrix Factorization by Deep Neural Network

106

In this section, we introduce the DeepMF architecture and the loss function used for its training.
Unless stated otherwise, symbols in bold font refer to vectors or matrices.
Matrix Factorization

107
108

109

In Figure 1C, assume the input matrix A is of dimension M × N , where M is the number of
features, and N is the number of samples. A row represents a feature while a column represents a
sample or a replication. The element Aij refers to the measured values for feature F i on sample S j ,
1 ≤ i ≤ M, 1 ≤ j ≤ N.
Matrix factorization assumes the dot product of feature latent factor ui and sample latent factor
v j to capture the interactions between feature F i and sample S j , where ui and v j are vectors of
size K which encode structures that underlie the data; that is, the predicted element of feature F i
on sample S j is calculated as:
X
Âij ≈
uik v jk = u>i v j

110
111
112
113

k

The predicted matrix Â can be thought of as the product of the feature latent factor matrix U and
sample latent factor matrix V , Â ∈ RM ×N ≈ U × V , where U ∈ RM ×K , V ∈ RK×N , K  M, N .
The objective function is minU ,V ||A − Â||2F .
Framework architecture

114
115
116

117

DeepMF is modeled to learn the complex interactions between two latent factors U and V based
on neural network. Figure 1B illustrates the network architecture of DeepMF. The input layer has
M neurons, corresponding to M features in the matrix. The output layer has N nodes to model
the N column samples. DeepMF is to capture the non-linear interaction between U and V . As
illustrated in Figure 1B, the network utilizes L hidden layers of K nodes each. All the nodes in the
hidden layers are fully connected and paired with ReLU activation function. The number of nodes,
K, corresponds to the dimensionality of the latent space in matrix factorization. The network is
be sufficiently complex in order to approximate f (U , V ), a non-linear function for the interaction
between feature and sample latent factors.

4/22

118
119
120
121
122
123
124
125
126

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

Training

127

M ×N

i

The matrix A ∈ R
contains M features. Each feature F corresponds to one input data point
xi ∈ RM and output label y i ∈ RN , where xi is one-hot encoded and y i is the i-th row of matrix A.
M

z
x = [0...0
i

}|
1
|{z}

{
0...0]

i-th feature, F i
i

y = Ai
The loss function consists of two parts, one for global trends and one for local trends. For a pair
of feature F i and sample S j , global proximity refers to the proximity between real measurement Aij
and predicted value Âij . The preservation of global proximity is fundamental in matrix factorization.
On the other hand, if two samples possess many common features, they tend to be similar. We
refer to this similarity as sample local proximity. We define f eature local proximity similarly.
By introducing these local proximities into the loss function, we aim to identify and preserve the
sample-pairwise and feature-pairwise structures in the low-dimensional latent space.
For global proximity, we minimize the L2-norm of the residual:
Lglobal

M
1 X i
||y − ŷ i ||2F
=
M i=1

128
129
130
131
132
133
134

(1)

S
For the local proximities, we use feature local proximity S F
M ×M and sample local proximity S N ×N
as supervised information. They respectively constrain the similarity of the latent representations of
features and samples. Given matrix AM ×N , we obtain the feature similarity matrix S F
M ×M and
sample similarity matrix S SN ×N as

1
1 + ||Ak − Al ||2F
1
=
> 2
1 + ||A>
k − Al ||F

sF
kl =

(2)

sSkl

(3)

>
where Ak and Al refer to the k-th and l-th row of matrix A. A>
k and Al refer to the k-th and l-th
column of matrix A.
With S F and S S , we define Llocal to preserve the local proximity of learned latent matrices U
and V :

Llocal =

M
X

+

137
138

2
sF
kl ||U k − U l ||F

k,l=1
N
X

135
136

(4)
sSkl ||V >
k

−V

> 2
l ||F

k,l=1
>
where U k and U l refer to the k-th and l-th row of feature latent matrix U , V >
k and V l refer to
the k-th and l-th column of sample latent matrix V .

5/22

139
140

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

The objective function Llocal incurs a penalty when similar features and similar samples are
embedded far away in the latent space. Hence, two features or samples with low similarity will be
driven nearer in the embedding space. To prevent this, we first identify the remote sample-sample
or feature-feature pair from feature and sample local proximity matrices by k-means. Then we mark
their local similarity to zero to exclude them from Llocal constraints.
To avoid overfitting and constrain the latent matrices U and V , an L2-norm regularization is
incorporated with U , V , and model hidden layer weights W hidden .
Lreg = ||U ||2F + ||V ||2F + ||W hidden ||2F

141
142
143
144
145

(5)

Our final loss function incorporates all the above constraints, with two additional hyperparameters
α and β, as follows:
Lmix = Lglobal + αLlocal + βLreg .

(6)

Dealing with missing value

146

To be tolerant to missing values, DeepMF discards the missing entries in back-propagation by a
variational L2-norm. Denote ξ as a missing value.
M N
1 XX
lossij
M i=1 j=1

0, y ij = ξ,
lossij =
||y ij − ŷ ij ||2F , y ij =
6 ξ.

Lmissing
=
global

(7)

Then, DeepMF can infer a missing value Aαβ by utilizing the trained model
M

z
x = [0...0
α

α

}|
1
|{z}

{
0...0]

α-th feature, F α
α

ŷ = DeepMF.predict(x )
Âαβ = ŷ α
β.
DeepMF architecture parameter selection

147

If the data assumes C (C ≥ 2) clusters with respect to samples, we recommend that the network
structure be pruned as guided by the validation loss Lmix in the range of K ∈ [2, C] and L ∈ [1, +∞).
For a matrix V K×N,(K<N ) , a rank of C is enough to represent the latent hierarchical structure for
a C-clustering problem, thus K ≤ C. To extract simple patterns like the linear association between
feature and sample, L = 1 suffices. A larger L would provide more complexity in the latent space of
DeepML. For hyperparameter tuning, we recommend running each K, L combination more than ten
times with different random weights initialization to avoid possible local optima.
DeepMF interpretation

148
149
150
151
152
153
154

155

DeepMF operates on the basis of the ML formula ÂM ×N = fK,L (U M ×K , V K×N ), where fK,L refers
to a collection of non-linear mapping, U is the feature latent factor matrix, and V is the sample
6/22

156
157

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

latent factor matrix (Figure 1). It learns about missing values in training, and imputes them in
prediction. Since DeepMF is trained by minimizing the loss between A and Â, denoising is built
into the learning process.
The two extracted matrices U and V are modeled to uncover the underlying latent structures of
the features and samples, respectively. They can hence be applied to features and samples related
clustering and pattern recognition tasks for data interpretation.

Simulation data generation

158
159
160
161
162
163

164

To evaluate DeepMF, we simulated three patterns, each which consists of matrices of sizes 1000 × 600,
10 × 6, and 100 × 60 as shown in Figures 2, S1, and S2. Then, we randomly removed 10%, 50%,
and 70% of the matrices to make them sparse.

165
166
167

Cancer subtyping experiments

168

For real datasets, the four cancer datasets as follows are used.

169

Cancer data preparation

170

Medulloblastoma data set Gene expression profiles from childhood brain tumors medulloblastomas were obtained from Brunet’s work [2]. It consists of classic and desmoplastic subtypes of size
25 and 9, respectively. We further extracted the top 100 differentially expressed genes using “limma”
R package [14].
Leukemia data set The Leukemia data set was obtained from R package “NMF” with the
command “data(esGolub)” [10]. It stores Affymetrix Hgu6800 microarray expression data from 38
Leukemia cancer patients, where 19 patients with B cell Acute Lymphoblastic Leukemia (B-cell
ALL), eight patients with T cell Acute Lymphoblastic Leukemia (T-cell ALL), as well as 11 patients
with Acute Myelogenous Leukemia (AML). The 236 most highly diverging genes were selected by
comparison on their coefficient of variation using limma R package [14].
TCGA BRCA data set A subset of human breast cancer data generated by The Cancer Genome
Atlas Network (TCGA) was obtained from R package mixOmics [15]. It holds 150 samples with
three subtypes Basal-like, Her2, and LumA, of size 45, 30 and 75, respectively. The top 55 correlated
mRNA, miRNA, and proteins which discriminate the breast cancer subtypes subgroups Basal, Her2,
and LumA were selected using the mixOmics DIABLO model.
SRBCT data set The Small Round Blue Cell Tumors (SRBCT) data set holds the expression
profiles of the top 96 ranked genes [16]. It contains 63 samples of four classes, Burkitt Lymphoma
(BL, 8 samples), Ewing Sarcoma (EWS, 23 samples), Neuroblastoma (NB, 12 samples), and
Rhabdomyosarcoma (RMS, 20 samples). The processed and normalized data were acquired from
the R mixOmics package [15].

7/22

171
172
173
174

175
176
177
178
179
180

181
182
183
184
185

186
187
188
189
190

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

Decomposition baselines

191

We compared the decomposition efficacy on DeepMF against four methods, PCA (FactoMineR [17]),
ICA (fastICA [18]), Bayesian-based NMF (CoGAPS [11]), and gradient-based NMF (NMF [10]).
We fit all model with log-treated matrices. All tools were executed with their recommended settings;
that is, prcomp function in package “FactoMineR”; fastICA with algorithm type “parallel”, function
“logcosh”, alpha 1, method “R”, row normalization 1, maxit 200, tol 0.0001; CoGAPS with 5000
iterations; NMF with method “brunet” and 200 runs.
As CoGAPS and NMF accept only non-negative values, we used NMF.posneg to transform the
input matrices into corresponding non-negative matrices.
Imputation baselines

192
193
194
195
196
197
198
199

200

We evaluated the DeepMF imputation efficiency by comparing it with two popular imputation
approaches, MeanImpute, and SVDImpute.
MeanImpute MeanImpute adopted the approach that the missing entries are to be substituted
by the mean of the current values of a particular feature in all samples. We used the mean impute
function in the R package “CancerSubtypes”.
SVDImpute SVDImpute first centers the matrix, replaces all missing values by 0, decomposes
the matrix into the eigenvectors. Then, SVDImpute predicts the NA values as a linear combination
of the k most significant eigenvectors [19]. We chose SVDImpute as an imputation baseline since the
mechanism behind it is similar to DeepMF. The k most significant eigenvectors can be analogized
to the k-dimensional latent matrix in DeepMF. We used R package “pcaMethods” in practice.
Evaluation Metrics

201
202

203
204
205

206
207
208
209
210

211

Silhouette width The silhouette width measures the similarity of a sample to its class compared
to other classes [20]. It ranges from -1 to 1. A higher silhouette value implies a more appropriate
clustering. A silhouette value near 0 intimates overlapping clusters, and a negative value indicates
that the clustering has been performed incorrectly.
We adopted the silhouette width to evaluate the model’s denoising and imputation power. We
used the ground-truth subtype classes as the input cluster labels. Then, the silhouette width for a
given matrix was calculated with Euclidean distance using the R package “cluster”.
Adjusted Rand Index We also used the adjusted Rand index to evaluate the clustering accuracy.
The adjusted Rand index measures the similarity between predicted clustering results and actual
clustering labels [21]. A value close to 0 indicates random labeling, and a value of 1 demonstrates
100% accuracy of clustering.
To check the cancer subtyping effectiveness of different matrix factorization tools. We first used
the R hierarchy clustering packaging “hclust” to obtain the sample latent factor matrices in order
to partition samples into subgroups, through the Euclidean distance and “ward.D2” linkage. Then,
we computed the adjusted Rand index to measure the clustering accuracy via the R package “fpc”.

8/22

212
213
214
215
216
217
218

219
220
221
222
223
224
225
226

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

Results

227

Denoising, imputation, and embedding evaluation on synthetic data

228

To evaluate the denoising, imputation, and embedding efficacy of DeepMF, we first generated three
patterns A, B and C, each which consists of matrices of size 1000 × 600, 10 × 6, and 100 × 60 in
(Figures 2, S1, S2). Matrices with pattern A hold three subgroups in feature and sample. Pattern
B has two subgroups in feature and three subgroups in sample. Pattern C matrices are transposed
of pattern B of dimension 600 × 1000, 6 × 10, and 60 × 100. Then we generated sparse matrices
randomly by dropping the entries of matrices with rate 10%, 50%, and 70%.
Figures 2, S1, S2 show the performance of DeepMF on the raw matrix and sparse matrix with
size 1000 × 600, 10 × 6, and 100 × 60, respectively. In Figures 2 A, S1 A, S2 A, the DeepMF
predicted matrices significantly reduced the noisy and missing entries. In spite of the noise and
70% missing entries, the feature latent factors and sample latent factors generated by DeepMF
consistently uncovered ground truth feature subgroups and sample subgroups with 100% accuracy.
The same conclusion applies to pattern B and pattern C (Figures 2 B-C, S1 B-C, S2 B-C). We
note that pattern B matrices and pattern C matrices are transposed, which suggests that DeepMF
can uncover the feature and sample subclasses either from a feature-sample matrix or its transposed
matrix. Since fitting a matrix with N < M is more efficient than a matrix with N > M in DeepMF,
it may be beneficial to do so, as long as it is unnecessary to adhere to the paradigm of “treating the
feature as row and sample as column” [1].

DeepMF accurately elucidates cancer subtypes on multiple cancer omics
data sets
To discover complex biological processes from massive amounts of high-throughput matrix data,
researchers customarily separate features or samples with similar profiles into biologically significant
partitions, with the assistance of clustering or pattern recognition techniques. Here, to demonstrate
how DeepMF can assist in this biological discovery, we collected a series of cancer omics data sets,
namely the Medulloblastoma data set (mRNA) [2], Leukemia data set (mRNA) [2, 10], TCGA
BRCA data set (mRNA, miRNA, protein) [15], and small blue round cell tumor (SRBCT) data set
(mRNA) [15, 16]. Then, we employed them as benchmark sets for cancer subtyping analysis.
We first verified the correctness of the output matrices. Figure 3 shows that DeepMF reduced
the noise in raw matrices while preserving cancer subtype structures on all cancer omics data sets.
Silhouette validation corroborated that the in-cluster similarity and out-cluster separation were
enhanced after DeepMF processing; that is, the average silhouette value was increased from 0.26 to
0.56 for Medulloblastoma data set, from 0.35 to 0.66 for Leukemia data set, from 0.19 to 0.47 for
TCGA BRCA data set, from 0.31 to 0.58 for SRBCT data set, respectively (see Figure 3).
We then checked whether the DeepMF produced sample latent matrix preserves the cancer
subtype information. We compared the decomposition efficiency on DeepMF against four traditional
matrix factorization methods, PCA (FactoMineR [17]), ICA (fastICA [18]), Bayesian-based NMF
(CoGAPS [11]), and gradient-based NMF (NMF [10]). We fitted high dimensional raw matrices
into DeepMF and the above four tools, extracted the low-dimensional sample latent matrices with
rank K = 2 for Medulloblastoma data set, rank K = 3 for Leukemia data set, rank K = 3 for
TCGA BRCA data set, and rank K = 4 for SRBCT data set, respectively. The DeepMF structure
configuration in training is listed in Table S1. To escape from local optima caused by DeepMF
random weight initialization, we conducted ten different runs for each data set configuration and
9/22

229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245

246
247

248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

selected the latent matrices with minimal loss. Next, we applied hierarchical clustering into obtained
sample latent matrices (Figure S3). Clustering accuracy is evaluated by the adjusted Rand index,
which measures the overlap between the inferred clusters and ground-truth subtype, a score of 0
signifies random labeling and 1 denotes perfect inference. In Figure 3, DeepMF outperforms all
four methods and manifests the best embedding strength, with highest clustering accuracy of 76%
for Medulloblastoma data set, 92% for TCGA BRCA data set, and 100% accuracy for Leukemia
and SRBCT data sets.

270
271
272
273
274
275
276

DeepMF captures the cancer subtype patterns despite 70% random dropouts277
Several studies have suggested that missing values in large-scale omics data can drastically obstruct
the interpretation of complex biological processes, such as unsupervised cancer subtyping [22]. At
present, this is most commonly treated by imputing the missing values before performing downstream
analysis of multi-omics data. To evaluate the imputation efficiency of DeepMF, we randomly dropout
70% entries on all four cancer data sets, then fit the sparse matrices into DeepMF and two imputation
baselines: MeanImpute and SVDImpute. We selected MeanImpute by considering its popularity.
From the perspective of imputation mechanism, we can regard SVDImpute as a linear analogy of
DeepMF. The DeepMF structure configuration in training is listed in Table S1. To avoid local
optima, we conducted ten different runs for each data set configuration and picked the one with
minimal loss. Figure 4 demonstrates that for all 70% missing rate data sets, both DeepMF and
SVDImpute recovered distinctive cancer subtype structures, while the MeanImpute approach was
unable to reconstruct a clearly visible pattern. Silhouette validation confirmed that DeepMF reduced
the most substantial interior cluster heterogeneity and out-cluster similarity, with the largest average
silhouette value of 0.47 for the Medulloblastoma data set, 0.6 for the Leukemia data set, 0.28 for
TCGA BRCA data set, and 0.44 for SRBCT data set.
Remainder that alongside the imputation process, DeepMF produced sample latent matrix.
To investigate whether missing entries will hinder DeepMF in matrix decomposition, we applied
hierarchical clustering into sample latent matrices generated by sparse matrices (Figure S4) and
computed the clustering accuracy with ground-truth subtyping labels (Figure 4). Since the four
matrix factorization tools do not accept input with missing values, we fitted the high dimensional
matrices treated by MeanImpute and SVDImpute into four baseline approaches, then obtained the
corresponding low-dimensional sample latent matrices with rank K = 2 for Medulloblastoma data
set, rank K = 3 for Leukemia data set, rank K = 3 for TCGA BRCA data set, and rank K = 4 for
SRBCT data set, respectively. Figure 4 E shows that in terms of clustering accuracy, DeepMF
outperforms all eight imputation and factorization combinations, exhibiting the best embedding
power with clustering accuracy of 88% for Medulloblastoma data set, 100 % accuracy for TCGA
BRCA data set, 84% for Leukemia, and 96% for SRBCT data sets.

Discussion

278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304

305

In this paper we presented DeepMF, a supervised learning approach to the dimension reduction
problem. Unlike current approaches, the method is designed to have high tolerance with respect to
noisy data and missing values. Experiments using synthetic and real data corroborated this fact,
showing DeepMF to be particularly suited for subtype discovery on omics data.
We have not addressed several issues. The first is with regard to the choice of the three hyperparameters K, L, W in DeepMF. The choice of the reduced dimensionality K is arguably difficult,
10/22

306
307
308
309
310
311

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

since it is an open problem for the entire dimension reduction research community. Different
combination of K, L might lead to distinct molecular feature and sample latent matrices. To find
the optimal network structure for accurate biological signature interpretation, we defined Lmix to
guide the hyperparameter search. Otherwise, we resort to multiple trials for the tuning of these
parameters.
In this paper we have used DeepMF only on mRNA, miRNA, and protein data. However,
DeepMF is not limited to these data modality. Human metabolome profiles can certainly benefit
from analysis using DeepMF, since the data is known to often suffer from missing values. We intend
to apply DeepMF to metabolome and discover signatures beneficial to human health.
In this study, we only utilized the sample latent matrix for subtype detection, we plan to employ
molecular feature latent matrix to uncover gene functional pathways in future work.

Conclusion

312
313
314
315
316
317
318
319
320
321
322

323

MF-based analyses are commonly used in the interpretation of high-throughput biological data.
Our proposed DeepMF is an MF-based deep learning framework which overcomes traditional
shortcomings such as noise and missing data. Our experiments on simulation data and four omics
cancer data sets established DeepMF’s feasibility in denoising, imputation, and in discovering the
underlying structure of data.

Abbreviations

324
325
326
327
328

329

MF Matrix Factorization

330

NGS Next-Generation Sequencing

331

PCA Principal Component Analysis

332

ICA Independent Component Analysis

333

NMF Non-negative Matrix Factorization

334

ALL Acute Lymphoblastic Leukemia

335

AML Acute Myelogenous Leukemia

336

TCGA The Cancer Genome Atlas Network

337

BRCA Breast Cancer

338

SRBCT Small Round Blue Cell Tumors

339

BL Burkitt Lymphoma

340

EWS Ewing Sarcoma

341

NB Neuroblastoma

342

RMS Rhabdomyosarcoma

343

11/22

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

Competing interests

344

The authors declare that they have no competing interests.

345

Ethics approval and consent to participate

346

Not applicable.

347

Consent for publication

348

Not applicable.

349

Availability of data and materials

350

The source code can be found in https://gitlab.deepomics.org/jiaox96/DeepMF

351

Funding

352

Publication costs are funded by the GRF Research Projects 9042348 (CityU 11257316). The work
described in this paper was also supported by the project.

Acknowledgements

353
354

355

We would like to express sincere gratitude to Prof. Yen Kaow Ng (Universiti Tunku Abdul Rahman)
for manuscript revision.

Author’s contributions

356
357

358

S. C. L. conceived the idea.
S. C. L., L. C. designed the network.
L. C, J. X. implemented the network.
L. C, J. X. conducted the analysis.
L. C. drafted the manuscript.
S. C. L. supervised the project, revised the manuscript.
All authors read and approved the final manuscript.

359
360
361
362
363
364
365

References

366

1. Genevieve L Stein-O’Brien, Raman Arora, Aedin C Culhane, Alexander V Favorov, Lana X
Garmire, Casey S Greene, Loyal A Goff, Yifeng Li, Aloune Ngom, Michael F Ochs, et al.
Enter the matrix: factorization uncovers knowledge from omics. Trends in Genetics, 2018.

12/22

367
368
369

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

2. Jean-Philippe Brunet, Pablo Tamayo, Todd R Golub, and Jill P Mesirov. Metagenes and
molecular pattern discovery using matrix factorization. Proceedings of the national academy
of sciences, 101(12):4164–4169, 2004.
3. Fuyan Hu, Yuxuan Zhou, Qing Wang, Zhiyuan Yang, Yu Shi, and Qingjia Chi. Gene expression
classification of lung adenocarcinoma into molecular subtypes. IEEE/ACM transactions on
computational biology and bioinformatics, 2019.
4. Elana J Fertig, Ana Markovic, Ludmila V Danilova, Daria A Gaykalova, Leslie Cope, Christine H Chung, Michael F Ochs, and Joseph A Califano. Preferential activation of the hedgehog
pathway by epigenetic modulations in hpv negative hnscc identified with meta-pathway
analysis. PloS one, 8(11):e78127, 2013.
5. Ludmil B Alexandrov, Serena Nik-Zainal, David C Wedge, Peter J Campbell, and Michael R
Stratton. Deciphering signatures of mutational processes operative in human cancer. Cell
reports, 3(1):246–259, 2013.
6. Ludmil B Alexandrov, Serena Nik-Zainal, David C Wedge, Samuel AJR Aparicio, Sam Behjati,
Andrew V Biankin, Graham R Bignell, Niccolo Bolli, Ake Borg, Anne-Lise Børresen-Dale,
et al. Signatures of mutational processes in human cancer. Nature, 500(7463):415, 2013.
7. Ludmil Alexandrov, Jaegil Kim, Nicholas J Haradhvala, Mi Ni Huang, Alvin WT Ng, Arnoud
Boot, Kyle R Covington, Dmitry A Gordenin, Erik Bergstrom, Nuria Lopez-Bigas, et al. The
repertoire of mutational signatures in human cancer. BioRxiv, page 322859, 2018.
8. Weihong Zhao, Jiancheng Luo, and Shunchang Jiao. Comprehensive characterization of cancer
subtype associated long non-coding rnas and their clinical implications. Scientific reports,
4:6591, 2014.
9. Su-In Lee and Serafim Batzoglou. Application of independent component analysis to microarrays. Genome biology, 4(11):R76, 2003.
10. Renaud Gaujoux and Cathal Seoighe. A flexible r package for nonnegative matrix factorization.
BMC bioinformatics, 11(1):367, 2010.
11. Elana J Fertig, Jie Ding, Alexander V Favorov, Giovanni Parmigiani, and Michael F Ochs.
Cogaps: an r/c++ package to identify patterns and biological process activity in transcriptomic
data. Bioinformatics, 26(21):2792–2793, 2010.
12. CS Wilhelm-Benartzi, DC Koestler, MR Karagas, JM Flanagan, BC Christensen, KT Kelsey,
CJ Marsit, EA Houseman, and R Brown. Review of processing and analysis methods for dna
methylation array data. British journal of cancer, 109(6):1394, 2013.
13. Tero Aittokallio. Dealing with missing values in large-scale studies: microarray data imputation
and beyond. Briefings in bioinformatics, 11(2):253–264, 2009.
14. Matthew E Ritchie, Belinda Phipson, Di Wu, Yifang Hu, Charity W Law, Wei Shi, and
Gordon K Smyth. limma powers differential expression analyses for rna-sequencing and
microarray studies. Nucleic acids research, 43(7):e47–e47, 2015.

13/22

370
371
372

373
374
375

376
377
378
379

380
381
382

383
384
385

386
387
388

389
390
391

392
393

394
395

396
397
398

399
400
401

402
403

404
405
406

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

15. Florian Rohart, Benoit Gautier, Amrit Singh, and Kim-Anh Lê Cao. mixomics: An r
package for ‘omics feature selection and multiple data integration. PLoS computational biology,
13(11):e1005752, 2017.
16. Javed Khan, Jun S Wei, Markus Ringner, Lao H Saal, Marc Ladanyi, Frank Westermann, Frank
Berthold, Manfred Schwab, Cristina R Antonescu, Carsten Peterson, et al. Classification and
diagnostic prediction of cancers using gene expression profiling and artificial neural networks.
Nature medicine, 7(6):673, 2001.
17. Sébastien Lê, Julie Josse, François Husson, et al. Factominer: an r package for multivariate
analysis. Journal of statistical software, 25(1):1–18, 2008.
18. JL Marchini, C Heaton, and BD Ripley. fastica: Fastica algorithms to perform ica and
projection pursuit. R package version, 1(0), 2013.
19. Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor Hastie, Robert
Tibshirani, David Botstein, and Russ B Altman. Missing value estimation methods for dna
microarrays. Bioinformatics, 17(6):520–525, 2001.
20. Peter J Rousseeuw. Silhouettes: a graphical aid to the interpretation and validation of cluster
analysis. Journal of computational and applied mathematics, 20:53–65, 1987.
21. William M Rand. Objective criteria for the evaluation of clustering methods. Journal of the
American Statistical association, 66(336):846–850, 1971.
22. Dongdong Lin, Jigang Zhang, Jingyao Li, Chao Xu, Hong-Wen Deng, and Yu-Ping Wang. An
integrative imputation method based on multi-omics datasets. BMC bioinformatics, 17(1):247,
2016.

407
408
409

410
411
412
413

414
415

416
417

418
419
420

421
422

423
424

425
426
427

Figures

428

Supplementary Figures

429

Supplementary Tables

430

Table S1. DeepMF training configuration on cancer
Data set
70% dropout K L alpha
Medulloblastoma
No
2
2
0.01
Leukemia
No
3 1
0.01
TCGA BRCA
No
3
2
0.01
SRBCT
No
4
2
0.01
Medulloblastoma
Yes
2
2
0.01
Leukemia
Yes
3 1
0.01
TCGA BRCA
Yes
3
1
0.01
SRBCT
Yes
4 2
0.01

data sets
beta
kmeans k
0.0001
3
0.0001
3
0.0001
3
0.0001
3
0.0001
3
0.0001
2
0.0001
2
0.0001
2

14/22

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

A Training process of DeepMF
)*
0

1

0

34

, layer

1

37

12
K … K

M

N

Forward Propagation

0

3564

0

B DeepMF ! = -(&, ')

+*
14
12

+*

K … K

K … K

N
3567

1867
1864

Backward Propagation

#

, layer

&

37

3564

C Traditional MF ! = & ×'

)*

M

34

, layer

M

3567

0
0

)*

+*

"
=

'

N

(

!

#
×

'

"

&

Figure 1. DeepMF Structure Overview
(A) The training process of DeepMF. The rectangle represents the K dimensional gene latent vector
u ∈ RK or sample latent vector v ∈ RK .
(B-C) Illustration of DeepMF and MF, respectively.

15/22

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

A

Raw Data

10% Dropout

50% Dropout

70% Dropout

Before
DeepMF

After
DeepMF

B
Before
DeepMF

After
DeepMF

C
Before
DeepMF

After
DeepMF

Figure 2. DeepMF performance on 1000 × 600 synthetic matrices
DeepMF denoising, imputation, and factorization performance on 1000 × 600 synthetic matrices
with different pattern. Inside each pattern, from left to right: raw matrix, 10% random dropout,
50% random dropout, 70% random dropout; from top to bottom: before DeepMF, and DeepMF.
The horizontal line plot show the sample latent factors, the vertical line plot refer to feature latent
factors.
A Matrix with pattern A B Matrix with pattern B C The transpose matrix of pattern B
16/22

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

Before
DeepMF

A

After
DeepMF

Before
DeepMF

B

After
DeepMF

Value

Value
10

8

8

6

6

4

4
2

Type

Type

ALL_B−cell

Classic

0.26

0.0
0.2
Silhouette

Desmoplastic

0.56

0.4−0.50

0.00
Silhouette

0.75

0.35

0.0 0.1 0.2 0.3 0.4 0.5 0.0
Silhouette

C

ALL_T−cell

0.66

0.2 0.4 0.6
Silhouette

AML

0.8

D
Value

Value

2

2

1

0

0

−2

−1

−4

−2

Type

Type

EWS

Basal

0.19

0.0

E

0.47

0.1 0.2 0.3
Silhouette

Clustering Accuracy

0.75
0.50

0.56

0.66

0.56

0.66

0.76

0.0
1

0.91

0.31

LumA

0.00 0.25 0.50
Silhouette

1.00

BL

Her2

1

1

0.2 0.4
Silhouette

0.92
0.9 0.86
0.82 0.86

0.6
1

RMS

0.00 0.25 0.50 0.75
Silhouette
1

1

0.95 1

Tool
PCA

0.62

ICA
CoGAPS

0.25
0.00

NB

0.58

NMF
DeepMF

Medulloblastoma

Leukemia

TCGA BRCA

SRBCT

Figure 3. DeepMF denoising and factorization on cancer data sets
A-D The heatmap presentation and Silhouette width of four cancer data sets. Left: before DeepMF.
Right: after DeepMF. A Medulloblastoma data set B Leukemia data set C TCGA BRCA data set
D SRBCT data set E Clustering accuracy of cancer subtyping on sample latent matrices generated
by five matrix factorization tools on different cancer data sets.

17/22

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

70% Dropout

A

After MeanImpute

After SVDImpute

After DeepMF

10

Type
Classic
Desmoplastic

0.15

0.4

−0.2−0.10.0 0.1 0.2 0.3
Silhouette

0.47

−0.2 0.0 0.2 0.4 0.6
Silhouette

−0.4

After SVDImpute

After DeepMF

Type

8

ALL_B−cell

6

ALL_T−cell
AML

4

0.08

0.0 0.4
Silhouette

C

−0.1 0.0 0.1
Silhouette

0.48

0.2 0.0

0.2
0.4
Silhouette

0.6

0.6 0.0

0.2 0.4 0.6
Silhouette

D

Value

Value
2
1
0
−1

Basal
LumA

Clustering Accuracy

0.25
0.00

0.05

−0.1

0.0 0.1
Silhouette

0.76
0.66
0.66

0.75

0.2
0.88

0.660.66

0.24

0.28

−0.25 0.00 0.25
Silhouette

−0.6 −0.3 0.0 0.3
Silhouette

0.9

0.9
0.64

1

0.910.91
0.76

BL

−2.5

NB

0.06

RMS

−0.1
RMS

1

0.780.77

0.78

0.52

0.46

0.44
0.1

EWS

0.0

Her2

1.00
0.50

Type

2.5

Type

−2

E

After MeanImpute

Value

Value
9
8
7
6
5
4
3

70% Dropout

B

0.84
0.78
0.7 0.74
0.65

0.36

0.0
0.1
Silhouette

−0.2 0.0 0.2 0.4 0.6
Silhouette

0.96
0.870.870.870.87
0.79

0.810.82
0.6

MeanImpute + PCA
MeanImpute + ICA

MeanImpute + CoGAPS
MeanImpute + NMF
SVDImpute + PCA
SVDImpute + ICA

0.05

Medulloblastoma

0.44

0.0 0.2 0.4 0.6 0.8
Silhouette

SVDImpute + CoGAPS

Leukemia

TCGA BRCA

SRBCT

SVDImpute + NMF
DeepMF

Figure 4. DeepMF’s imputation and factorization effect on cancer data sets with 70% random
dropout
A-D The heatmap presentation and Silhouette width of four cancer data sets with 70% random
dropout. The gray tiles in heatmap indicate missing entries. From left to right: matrix with 70%
random dropout, after mean impute, after SVDImpute, after DeepMF.
A Medulloblastoma data set B Leukemia data set C TCGA BRCA data set D SRBCT data set E
Clustering accuracy of cancer subtyping on sample latent matrices generated by two imputations
and five matrix factorization tools on different cancer data sets with 70% random dropout.

18/22

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

Raw Data

10% Dropout

3

3

3

2

2

2

2

1

1

1

1

0

0

0

0

−1

−1

−1

−1

●

●
●
●

●
●
●

●
●

●
●

●

●

2
1
0
−1

●

●

●

●

●

●

●

●

●

●

●

B

●

●
●

●

●
●

●

1.0
0.5
0.0
−0.5
−1.0

●
●
●

●

●

●

●

●

●
●

●

●

●

−0.4
−0.2
0.0
0.2
0.4
0.6
●

●
●
●

0.50
0.25
0.00
−0.25
−0.50
−0.75

●

●
●

●
●

●

2

1

1

1

1

0

0

0

0

−1

−1

−1

−1

−2
−1
0
1
●
●
●

●

●●●

●
●
●

●●
●

●
●
●

●●●

● ●●

●●
●

●●

● ● ● ● ● ●
● ● ● ●
● ● ● ● ●
●
● ● ● ●
● ● ● ●

● ● ●

● ● ●

● ●

●●
●

0.4
0.2
0.0
−0.2

●
●●

●●●
●
● ● ● ● ● ● ●

●
●
● ● ● ● ● ● ● ●
●
● ●
●
●
● ● ● ● ● ● ●
●

●

●●
●

●
●
●
●
●

● ●●

●●

1.0
0.5
0.0
−0.5
−1.0
−1.5

1
0
−1

●
●

●

3

−0.10
−0.05
0.00
0.05
●

●

●
●

2

●

●

●

●
●●

0.4
0.0
−0.4

●
●●●●●●
●●●●●●●
●

●
●●

●
●●●

●●
● ●

0.75
0.50
0.25
0.00
−0.25

Value

Value

Value

3

3

3

3

2

2

2

2

1

1

1

1

0

0

0

0

−1

−1

−1

−1

−0.10
−0.05
0.00
0.05
0.10

Value

−0.05
0.00
0.05
0.10
●

●

2

●●
● ●
●●
● ●
● ●●
● ●●
● ●
● ●
●●
● ●

●

●

2

●
●
●

●

●

1.0
0.5
0.0
−0.5

Value

Before
DeepMF

●

●
●
●

3

●

●

●

3

●

After
DeepMF

●
●

3

●

C

●
●

●

Value

●● ●

● ● ●
● ● ●

●

●

● ●
●

●
● ● ●
● ●
●
● ●
● ●
● ● ● ● ● ● ● ● ● ●

●

Value

● ●

● ● ●

●

●●
●
●
●●
●
● ●
● ●
●
● ●●
●
●●
● ●●
●●●
●●
●
●
● ●

Value

Before
DeepMF

After
DeepMF

●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
● ●●
●●
●
● ●●
● ●●

−0.5
0.0
0.5
1.0

●

●

●
●●
●
●●
●
●●
●●
●
●
●
●
●
●
● ●
●
● ●
●
● ●
●
●
●
●

●

●
●
●
●
●
●
● ● ●
●
● ●
●
● ●
● ● ●
● ● ●
● ●●
● ● ●

●
●
●
●
●

●

●

●

●

●

●
●

●
●
●

●
●
●

1.0
0.5
0.0
−0.5
−1.0
−1.5

●
●

●
●
●

●
●
●

●
●
●
●

●
●

●
●
●

●
●
●

1.0
0.5
0.0
−0.5

−0.5
0.0
0.5

●

●

●
●
●
●
●
●

−1
0
1

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●

−0.4
−0.2
0.0
0.2

Value

3

−0.2
−0.1
0.0
0.1
0.2
0.3

Value

−0.1
0.0
0.1
0.2
●
●
●

70% Dropout

Value

Before
DeepMF

After
DeepMF

50% Dropout

Value

−0.6
−0.3
0.0
0.3
0.6

A

● ●
●
●
●
●●
●
● ●●
●
●
● ●
● ●●
● ●●
● ●●
● ●●

●● ●
● ● ●
●
● ●●
● ● ●
● ●●
● ●●
● ●●
● ●●
● ●●

●●

●

●

●
●

●
●

●

●

●
●
●

●

●
●
●

0.8
0.4
0.0
−0.4

Figure S1. DeepMF performance on 10 × 6 synthetic matrices
DeepMF denoising, imputation, and factorization performance on 10 × 6 synthetic matrices with
different pattern. Inside each pattern, from left to right: raw matrix, 10% random dropout, 50%
random dropout, 70% random dropout; from top to bottom: before DeepMF, and DeepMF. The
horizontal line plot show the sample latent factors, the vertical line plot refer to feature latent
factors.
A Matrix with pattern A B Matrix with pattern B C The transpose matrix of pattern B

19/22

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

Raw Data

10% Dropout
Value

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

1.00

0.75

0.75

0.75

0.75

0.50

0.50

0.50

0.50

0.25

0.25

0.25

0.25

0.00

0.00

0.00

0.00

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.25
0.00
−0.25

Value
0.75

0.50

0.50

0.50

0.50

0.25

0.25

0.25

0.25

0.00

0.00

0.00

0.00

−0.2
−0.1
0.0
0.1
0.2

1.00

0.75

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.4
0.2
0.0
−0.2

●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●

●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
● ●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●●

●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.4
0.2
0.0
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● −0.2
●
●
●
●
●
●
●
●
●

0.4
0.2
0.0
−0.2
Value

1.00

1.00

0.75

0.75

0.75

0.75

0.50

0.50

0.50

0.50

0.25

0.25

0.25

0.25

0.00

0.00

0.00

0.00

−0.2
0.0
0.2

Value

1.00

−0.2
0.0
0.2

Value

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

1.00

−0.2
0.0
0.2

−0.2
0.0
0.2
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

Value
1.00

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.50
0.25
0.00
−0.25
−0.50

0.75

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

1.00

Value

After
DeepMF

●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.25
0.00
−0.25
−0.50

Value

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●

Before
DeepMF

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.75

0.4
0.2
0.0
−0.2

C

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●●●
●
●●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●●
●
●
●
●
●
●●
● ●
●
●
●
●●
●
●
●
●
●
●
●
●●
●●
●● ●●
●
●●
●
●
●●
●●
●●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●●
●
●
●
●●
●●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●●
●●
●
● ●
●
●
●
●
●
●
●
●
●●
●
●
●

1.00

−0.2
0.0
0.2
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
● ●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●●
● ●●●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.4
0.2
0.0
−0.2
−0.4

Value

After
DeepMF

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

−0.4
−0.2
0.0
0.2
0.4

−0.2
0.0
0.2
0.4
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●●
● ●● ●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
● ●
●
●
●
●
●
●
●
● ●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

Before
DeepMF

−0.2
0.0
0.2
0.4

1.00

1.0
0.5
0.0
−0.5
−1.0

B

Value

1.00

−0.2
0.0
0.2

After
DeepMF

70% Dropout
Value

1.00

−0.04
0.00
0.04

Before
DeepMF

50% Dropout
Value

−0.3
−0.2
−0.1
0.0
0.1
0.2

A

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
● ●
●
● ●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.2
0.0
−0.2
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● −0.4
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●

●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●●
●
●●
●
●
●
● ●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
● ●●
●
●
● ●
●
●
●
●
● ●
●
●
●
●
● ●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.25
0.00
−0.25

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●●
●●
●
●
●●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.4
0.2
0.0
−0.2

Figure S2. DeepMF performance on 1000 × 600 synthetic matrices
DeepMF denoising, imputation, and factorization performance on 1000 × 600 synthetic matrices
with different pattern. Inside each pattern, from left to right: raw matrix, 10% random dropout,
50% random dropout, 70% random dropout; from top to bottom: before DeepMF, and DeepMF.
The horizontal line plot show the sample latent factors, the vertical line plot refer to feature latent
factors.
A Matrix with pattern A B Matrix with pattern B C The transpose matrix of pattern B

20/22

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

A

PCA

Type
PC1
PC2

ICA

Type
S1
S2

Type

CoGAPS

S1
S2

Type

NMF

S1
S2

Type

DeepMF

C

B

S1
S2

10 Type
Classic
5
Desmoplastic
0

Type
PC1
PC2
PC3

2 Type
1
Classic
0
Desmoplastic
−1
−2

Type
S1
S2
S3

Type
Classic
Desmoplastic

20 Type
10
ALL_B−cell
0
ALL_T−cell
−10
AML
−20

D
Type
PC1
PC2
PC3

6 Type
4
Basal
2
Her2
0
−2
LumA
−4
−6

Type
PC1
PC2
PC3
PC4

5
0
−5
−10

Type
EWS
BL
NB
RMS

Type
ALL_B−cell
ALL_T−cell
AML

Type
S1
S2
S3

2
1
0
−1
−2

Type
Basal
Her2
LumA

Type
S1
S2
S3
S4

2
1
0
−1
−2

Type
EWS
BL
NB
RMS

Type
S1
S2
S3

1 Type
0.8
ALL_B−cell
0.6
ALL_T−cell
0.4
AML
0.2

Type
S1
S2
S3

Type
Basal
Her2
LumA

Type
S1
S2
S3
S4

1
0.8
0.6
0.4
0.2
0

Type
EWS
BL
NB
RMS

0.9 Type
0.8
Classic
0.7
0.6
Desmoplastic
0.5
0.4
0.3

Type
S1
S2
S3

0.8 Type
0.7
ALL_B−cell
0.6
0.5
ALL_T−cell
0.4
AML
0.3
0.2

Type
S1
S2
S3

0.5 Type
0.4
Basal
0.3
Her2
0.2
LumA
0.1
0

Type
S1
S2
S3
S4

Type
Classic
Desmoplastic

Type
S1
S2
S3

1
0
−1
−2

Type
ALL_B−cell
ALL_T−cell
AML

Type
S1
S2
S3

Type
Basal
Her2
LumA

Type
S1
S2
S3
S4

−5

0.9
0.8
0.7
0.6
0.5
0.4

1
0.5
0

1
0

−1
−2

0.8
0.6
0.4
0.2
0

0.1
0

−0.1

0.3
0.2
0.1
0

Type
EWS
BL
NB
RMS

1
Type
0.5
EWS
0
BL
−0.5
NB
−1
RMS

Figure S3. Hierarchical Clustering plot for sample latent matrices
Sample latent matrices are generated by five matrix factorization tools on different cancer data sets.
From top to bottom, each row represents sample latent matrices generated by PCA, ICA, CoGAPS,
NMF, DeepMF.
A Medulloblastoma data set B Leukemia data set C TCGA BRCA data set D SRBCT data set

21/22

bioRxiv preprint doi: https://doi.org/10.1101/744706; this version posted August 22, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC-ND 4.0 International license.

A

B
Type

MeanImpute + PCA

MeanImpute + ICA

PC1
PC2

Type

2

S1

0

S2

Type

MeanImpute + CoGAPS

S1
S2

Type

MeanImpute + NMF

S1
S2

Type

SVDImpute + PCA

PC1
PC2

Type

SVDImpute + ICA

S1
S2

Type

SVDImpute + CoGAPS

S1
S2

Type

SVDImpute + NMF

S1
S2

Type

DeepMF

6
4
2
0
−2

S1
S2

−2

Type
Classic
Desmoplastic

Type
Classic
Desmoplastic

Type
PC1
PC2

Type
Classic
Desmoplastic

0
−5

Type

3
2
1
0
−1
−2
−3

S1
S2

Type
S1
S2
S3

0.8 Type
Classic
0.7
0.6
Desmoplastic
0.5
0.4
0.3

Type

10 Type
Classic
5
Desmoplastic
0

Type

S1
S2
S3

PC1
PC2

−5

PC3

1 Type
Classic
0
Desmoplastic
−1
−2
−3

Type

Type
Classic
Desmoplastic

Type

0.9
0.8
0.7
0.6
0.5
0.4

5

PC3

S3

−4

0.8
0.6
0.4
0.2

C

S1
S2
S3

0.8
0.6
0.4
0.2

0.7
0.6
0.5
0.4
0.3
0.2

10
0
−10
−20

2
1
0
−1
−2

D

Type
ALL_B−cell
ALL_T−cell
AML

Type

Type
ALL_B−cell
ALL_T−cell
AML

Type

PC1
PC2
PC3

S1
S2
S3

4

Type
EWS
BL
NB
RMS

1
0.8
0.6
0.4
0.2

Type
Basal
Her2
LumA

Type
S1
S2
S3
S4

0.8
0.6
0.4
0.2

Type
Basal
Her2
LumA

Type
S1
S2
S3
S4

0.35 Type
0.3
EWS
0.25
0.2
BL
0.15
0.1
NB
0.05
RMS
0

Type
Basal
Her2
LumA

Type
PC1
PC2
PC3
PC4

10
5
0
−5
−10

Type
ALL_B−cell
ALL_T−cell
AML

Type

10

PC2

0

S3

2
0

−2
−4

Type
EWS
BL
NB
RMS

Type
EWS
BL
NB
RMS

PC1

5

PC3

−5

Type
ALL_B−cell
ALL_T−cell
AML

Type

3
2
1
0
−1
−2
−3

Type
Basal
Her2
LumA

Type
S1
S2
S3
S4

Type
ALL_B−cell
ALL_T−cell
AML

Type

1
0.8
0.6
0.4
0.2
0

Type
Basal
Her2
LumA

Type
S1
S2
S3
S4

1
0.8
0.6
0.4
0.2
0

0.3 Type
0.25
Basal
0.2
Her2
0.15
0.1
LumA
0.05
0

Type
S1
S2
S3
S4

0.25 Type
0.2
EWS
0.15
BL
0.1
NB
0.05
RMS
0

Type
Basal
Her2
LumA

Type
S1
S2
S3
S4

0.5

S1
S2
S3

0.8

0.2

Type
ALL_B−cell
ALL_T−cell
AML

Type

0.4

1.5 Type
Classic
1
Desmoplastic
0.5
NA
0

Type

1.5 Type
1
ALL_B−cell
0.5
ALL_T−cell
0
AML
−0.5
−1

Type

S3

Type
S1
S2
S3
S4

0.2
0.15
0.1
0.05
0

S1

S3

S2

Type
Basal
Her2
LumA

S2

S3

S2

S1

4
3
2
1
0
−1
−2
−3

Type

S2

Type

0.6

Type
EWS
BL
NB
RMS

Type
ALL_B−cell
ALL_T−cell
AML

S1

0.8 Type
0.7
Classic
0.6
0.5
Desmoplastic
0.4
NA
0.3
0.2

S1

6
4
2
0
−2

Type

0.8
0.6
0.4
0.2

S3

Type
PC1
PC2
PC3
PC4

Type
ALL_B−cell
ALL_T−cell
AML

S2

S1

2
1
0
−1
−2
−3

Type
Basal
Her2
LumA

S1
S2
S3

S1
S2
S3

S1
S2
S3

−10

0.5
0
−0.5
−1

4
2
0
−2

0
−0.5
−1

Type
EWS
BL
NB
RMS

Type
EWS
BL
NB
RMS

Type
EWS
BL
NB
RMS

Figure S4. Hierarchical Clustering plot for sample latent matrices generated from 70% random
dropout data sets
Sample latent matrices are generated by two imputation tools and five matrix factorization tools
on different cancer data sets with 70% random dropout. From top to bottom, each row represents
sample latent matrices generated by meanImpute + PCA, meanImpute + ICA, meanImpute +
CoGAPS, meanImpute + NMF, SVDImpute + PCA, SVDImpute + ICA, SVDImpute + CoGAPS,
SVDImpute + NMF, DeepMF.
A Medulloblastoma data set B Leukemia data set C TCGA BRCA data set D SRBCT data set

22/22

