bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

Biologically inspired model of associative memory storage with noisy neurons
and synapses
Chi Zhang, Danke Zhang, and Armen Stepanyants
Department of Physics and Center for Interdisciplinary Research on Complex Systems,
Northeastern University, Boston

ABSTRACT
Neural networks in the brain function reliably despite various sources of errors and noise present
at every step of signal transmission. Here, we examine the effects of such fluctuations during
associative memory storage on properties of biologically constrained McCulloch and Pitts
networks. Our results show that networks, loaded with associative memories to capacity, display
many structural and dynamical features observed in local cortical circuits. We predict that noisy
inhibitory and excitatory connections in the cortex are depressed or completely eliminated during
learning, and that neuron classes that operate with low firing rates have low connection
probabilities and strengths.

INTRODUCTION
Brain networks can reliably store and retrieve memories despite various sources of errors and noise
in signal transmission. To explore the effects of reliability in memory storage on brain connectivity,
we examined a model of associative memory storage in a biologically constrained network of
McCulloch and Pitts neurons [1]. The problem of associative learning [2-4] has received much
attention over the years due in part to its theoretical tractability (see e.g. [5-8]) and the facility to
incorporate biologically inspired elements, such as sign-constrained postsynaptic connections
(inhibitory and excitatory) (see e.g. [9-11]), homeostatically constrained presynaptic connections
(see e.g. [12]), and robustness to noise [6,7]. Yet, various sources of noise that accompany signal
1

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

transmission in the brain [13-16] are not fully accounted for by the associative learning models.
At a high-level description, errors in the input to a neuron combine with fluctuations in synaptic
transmission and the neuron’s intrinsic sources of noise, producing spiking errors in the neuron’s
output. The latter are, in turn, injected into the network, completing the error propagation cycle
(Figure 1A). The associative learning models generally make two unrealistic assumptions, which
preclude the analysis of this cycle. First, they assume that intrinsic noise has a fixed range, and
second, they allow no errors during the recall of stored memories (but see Supplementary Material
of Brunel et al. [10]). The two assumptions are tightly linked and removing the first necessitates
eliminating the second. For example, if the distribution of intrinsic noise is Gaussian, associative
memories cannot be retrieved without errors. These assumptions are both inconsistent with
experimental evidence and unnecessary. Therefore, we developed a network model of associative
memory storage which incorporates the complete error propagation cycle.

RESULTS
Network model of associative memory storage in the presence of errors and noise
We consider an all-to-all potentially (structurally) connected network [17] of Ninh inhibitory and
(N − Ninh) excitatory McCulloch and Pitts neurons (Figure 1A). The network is faced with a task
of learning a sequence of consecutive states, X 1  X 2  ... X m 1 , in which X  is a binary vector
representing target activities of all neurons at a time step μ. In this process, individual neurons (e.g.
neuron i) must independently learn to associate inputs they receive from the network, X  , with
the corresponding outputs derived from the associative memory sequence, X i 1 . The neurons
must learn these input-output associations by adjusting the weights of their input connections, J ij
(weight of connection from neuron j to neuron i). Similar to [12], two biologically inspired
constraints are imposed on the learning process. First, the l1-norm of input connection weights of
each neuron is fixed during learning,

1
N

N

J
j 1

ij

 w . Here, parameter w is referred to as the average

absolute connection weight. Second, the signs of output connection weights of each neuron

2

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

(inhibitory or excitatory) do not change during learning, J ij g j  0 . Here, parameter g j  1 if
neuron j is excitatory and –1 if it is inhibitory. Biological motivations for these constraints have
been previously discussed [12].
In contrast to previous studies [6,7,10-12,18,19], we account for the fact that learning in the brain
is accompanied with multiple sources of errors and noise. Within the associative memory storage
model, these sources can be divided into three categories (orange lightning signs in Figure 1A):
(1) spiking errors, or errors in X  , (2) synaptic noise, or noise in J ij , and (3) intrinsic noise, which
combines all other sources of noise affecting the neurons’ postsynaptic potentials. The last
category includes background synaptic activity and stochasticity of ion channel states, and, in the
model, it is equivalent to noise in the neurons’ thresholds of firing, h. In the following, we use an
asterisk to denote the quantities containing errors or noise (e.g. X * ), whereas variables without
asterisks represent the mean (for h and Jij) or target (for X  ) values.
Target neuron activities, e.g. X i , are independently drawn from neuron-dependent Bernoulli
probability distributions: 0 with probability 1 – fi and 1 with probability fi. Spiking errors in neuron
activity states are introduced with Bernoulli trials by making independent and random changes









with probabilities P X i*  0 | X i  1  pi for spike failures and P X i*  1| X i  0  pi for
erroneous spikes (Figure 1B). Without loss of generality we assume that these two types of spiking
errors are balanced, f i pi  1  f i  pi , and do not affect the neuron’s firing probability, fi. This
relation allows us to describe both types of spiking errors in terms of the neuron’s overall spiking
error probability, ri  f i pi  1  f i  pi .
To describe synaptic noise, we use the quantal model of Del Castillo and Katz [20]. According to
this model, connection weights J ij* on different trials are drawn from a Binomial distribution, in
which the variance is proportional to the mean, var  J ij*  

h  syn
N

J ij . The dimensionless

coefficient βsyn is referred to as the synaptic noise strength, and a factor of h/N is introduced for
convenience. We assume that intrinsic noise is Gaussian distributed across trials with the mean

3

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

 

h*  h and variance var h* 

h2 int2
. Here, βint is a dimensionless coefficient called intrinsic
N

noise strength, and, as before, a factor of h2/N is introduced for convenience.
The above described model can be summarized as follows:




N

   J ij* X *j   h*   X i* 1 ;   1, , m, i  1, , N
 j 1

N
1
 J ij  w; i  1,, N
N j 1

J ij g j  0; i, j  1, , N

P  X i  1  fi ; P  X


J ij*  J ij ; var  J ij*  

*
i

 0 | X i  1  p ; P  X


h syn
N


i

*
i

 1| X i  0   p



i

(1)

J ij

h 2 int2
h  h; var  h  
N
*

*

A neuron is said to have learned the presented set of associations successfully if, in the presence
of input spiking errors, synaptic and intrinsic noise, the fraction of output errors it produces does
not exceed its assigned output error probability, ri. The described associative memory storage
model is governed by parameters N, Ninh/N, h, w, βint, βsyn,  f i  , and ri  , and the task is to find

 

connection weights, J ij , that satisfy all the requirements of Eqs. (1).

Solution of the model
Because individual neurons in the model learn independently from each other and have separate
sets of constraints, Eqs. (1) can be split into N independent single-neuron learning problems, which
can be solved numerically (see Supplemental Material). We verified that memories loaded into
individual neurons at certain noise strengths can be successfully recalled at the network level in
the presence of the same or slightly lower noise strengths (Figure S1). Figure 1C shows that for a
relatively low memory load, the probability of successful learning by a neuron is close to 1. With
4

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

increasing load, the learning problems becomes not feasible, and the success probability undergoes
a transition from 1 to 0. Memory load corresponding to the success probability of 0.5 is referred
to as the neuron’s associative memory storage capacity. With increasing network size, N, the
transition from successful learning to inability to learn the entire set of associations becomes
sharper, and the neuron’s capacity monotonically approaches its N → ∞ limit. In this limit, the
associative memory storage problem can be solved with the replica method [21,22] (see
Supplemental Material), and the capacity is said to be critical.
In the case of homogeneous inputs to the neuron, fi  f and ri  r , solution of the model depends
on a combination of intrinsic and synaptic noise, referred to as the postsynaptic noise strength (see
Supplemental Material):

 post  int2   syn fN

w
h

(2)

Figure 1D illustrates the dependence of single-neuron critical capacity on postsynaptic noise
strength and spiking error probability in the homogeneous case. As expected, because intrinsic and
synaptic noise make the learning problem more challenging, a neuron’s capacity is a decreasing
function of postsynaptic noise strength, Eq. (2).

5

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

Figure 1: Associative memory storage in a recurrent network of inhibitory and excitatory neurons in the presence of
errors and noise. A. Cycle of error propagation through the network. Inhibitory neurons (red circles) and excitatory
neurons (blue triangles) form an all-to-all potentially (structurally) connected network. Red and blue arrows represent
*

*

actual (functional) connections. Spiking errors, X , synaptic noise, J ij* , and intrinsic noise, h , accompany signal
transmission (orange lightning signs). Errors in the neurons’ outputs are injected back into the network, becoming
spiking errors in the next time step. B. Fluctuations in postsynaptic potentials for two associations with target neuron
outputs 0 (left) and 1 (right). Large black dots denote postsynaptic potentials in the absence of noise and errors. Small
dots represent postsynaptic potentials on different trials in the presence of noise and errors. Orange areas under the
postsynaptic potential probability densities (solid lines) represent probabilities of erroneous spiking (left) and spike
failures (right). C. The probability of successful learning by a neuron is a sharply decreasing function of associative
sequence length, m (or memory load m/N). Solid curves represent probabilities of successful learning by a neuron with
N = 200, 400, and 800 inputs. Values of all other parameters of the neuron are provided in the figure. At 0.5 success
probability the neuron is said to be loaded to capacity. The dashed black line represents the theoretical (critical)
capacity obtained with the replica method in the N → ∞ limit. D. Map of critical capacity as a function of postsynaptic
noise strength and spiking error probability.

6

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

Neuron-to-neuron connectivity in homogeneous associative networks
One of the most salient features of sign-constrained associative network models, such as the one
described in this study, is that finite fractions of inhibitory and excitatory connections assume zero
weights at capacity [9], mirroring the trend observed in many local cortical networks. In the
following, we compare connection probabilities (Pcon) and coefficients of variation (CV) of nonzero connection weights in associative networks at capacity to connection probabilities and CV of
unitary postsynaptic potentials (uPSP) obtained experimentally. To that end, we used the dataset
compiled in [23] based on 87 electrophysiological studies describing neuron-to-neuron
connectivity for 420 local cortical projections (lateral distance between neurons < 100 μm). Figure
2A shows that the average inhibitory Pcon (38 studies, 9,522 connections tested) is significantly
larger (p < 10-10, two sample t-test) than the average excitatory Pcon (67 studies, 63,020 connections
tested). Associative networks exhibit a similar trend (Figures 2B, C). In particular, in the regions
of postsynaptic noise strength and spiking error probability demarcated with dashed isocontours
and arrows in Figures 2B, C, the model results are consistent with the middle 50% of experimental
measurements.
Figure 2D shows that the average CV of inhibitory uPSP (10 studies, 503 connections recorded)
is slightly lower than that for excitatory (36 studies, 3,956 connections recorded), and this trend is
reproduced by associative networks in the entire region of considered postsynaptic noise strength
and spiking error probability (Figures 2E, F). As before, there is a region in these maps in which
results of the model are consistent with the middle 50% of CV of uPSP measurements.

7

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

Figure 2: Comparison of structural properties of model and cortical networks. A. Inhibitory and excitatory connection
probabilities reported in 87 studies describing 420 local cortical projections. Each dot represents the result of a single
study/projection. B, C. Maps of inhibitory and excitatory connection probabilities as functions of postsynaptic noise
strength and spiking error probability. Dashed isocontours and arrows demarcate the interquartile ranges of
experimentally observed connection probabilities from (A). The red contour outlines a region of parameters which is
consistent with all structural and dynamical measurements in cortical networks considered in this study. D-F. Same
for CV of inhibitory and excitatory connection weights. (A) and (D) are adapted from [23].

Spontaneous dynamics in homogeneous associative networks
Individual neurons in the model associative network can produce irregular and asynchronous
spiking activity, similar to what is observed in cortical networks. To quantify the degree of this
8

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

similarity we compared CV of inter-spike-intervals (ISI) and cross-correlation coefficients of
randomly initialized spiking network activity in the model to those measurements obtained
experimentally. Dashed isocontour in Figure 3A outlines a region of postsynaptic noise strength
and spiking error probability in which the model CV of ISI is consistent with the 0.7-1.1 range
measured in different cortical systems [24-28]. Similarly, there is a region of postsynaptic noise
strength and spiking error probability (Figure 3B) in which calculated spike train cross-correlation
coefficients are in agreement with the interquartile range of cortical measurements, 0.04-0.15 [29].
The degree of asynchrony in spontaneous spiking activity in the associative network increases with
postsynaptic noise strength, which can be explained by the decrease in connection probability
(Figures 2B, C) and, consequently, reduction in the amount of common input to the neurons.
It had been shown that irregular and asynchronous activity can result from balance of inhibitory
and excitatory postsynaptic inputs to individual cells [30,31]. In a balanced state, the magnitudes
of these inputs are much greater than the threshold of firing, but, due to a high degree of anticorrelation, these inputs nearly cancel, and firing is driven by fluctuations. Figure 3C shows a
region of parameters in which neurons in the associative model function in a balanced regime.
Because it is difficult to simultaneously measure inhibitory and excitatory postsynaptic inputs to a
neuron, anti-correlations of inhibitory and excitatory inputs have only been measured in nearby
cells, 0.4 [32,33]. As within-cell anti-correlations are expected to be stronger than between-cell
anti-correlations, 0.4 can be viewed as a lower bound for the former (dashed isocontour and arrow
in Figure 3C).

9

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

Figure 3: Comparison of dynamical properties of model and cortical networks. A. CV of ISI for spontaneous (not
learned) activity as a function of postsynaptic noise strength and spiking error probability. Dashed isocontour and
arrows demarcates a region of high CV values that is in general agreement with experimental measurements. B. Same
for cross-correlation coefficient of neuron spike trains. C. Same for anti-correlation coefficient of inhibitory and
excitatory postsynaptic inputs received by a neuron. The red contour outlines a region of parameters which is
consistent with all structural and dynamical measurements considered in Figures 2 and 3.

The seven error-noise regions obtained bases on the properties of neuron-to-neuron connectivity
(Figure 2) and network dynamics (Figure 3) have a non-empty intersection (red contour in Figures
2 and 3). In this biologically plausible region, properties of the associative networks are consistent
with the considered experimental measurements. This suggests that, during learning and memory
retrieval, the postsynaptic noise strength must lie in the 20-50 range, and the spiking error
probability must be less than 0.06. The low value of spiking error probability is consistent with
experimental observations of reliability of firing patterns evoked by time-varying stimuli in vivo
[28] and in vitro [34].

Properties of heterogeneous associative networks
General formulation and solution of the associative network model, Eqs. (1), makes it possible to
investigate the properties of networks composed of heterogeneous populations of inhibitory and
excitatory neurons. Specifically, we examined the effects of distributed (neuron class specific)
spiking error probabilities and firing probabilities on properties of neuron-to-neuron connectivity
at critical capacity. Figures 4B, C show that the probabilities and weights of inhibitory and
excitatory connections monotonically decrease with increasing r. Therefore, connections
originating from unreliable presynaptic neurons (high r) are significantly depressed. Analysis of
connectivity in networks of neurons with distributed firing probabilities (Figures 4D-F) shows that
the probabilities of inhibitory connections monotonically increase with f, while the probabilities
of excitatory connections exhibit a non-monotonic behavior. One consequence of the results shown
in Figures 4B, C and Figures 4E, F is that connection probability must be positively correlated
with connection weight, regardless of the source of heterogeneity (r or f). Indeed, this conclusion
10

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

is supported by the data from 16 local laminar (same cortical layer) and inter-laminar (different
layers) excitatory-to-excitatory projections from mouse barrel cortex [35] (Figure 4G). The same
trend was also observed for excitatory-to-inhibitory, inhibitory-to-excitatory, and inhibitory-toinhibitory projections in mouse visual cortex [36].

Probability density

30

Connection probability

N→∞
Ninh/N = 0.2
Nw/h = 70
f = 0.2
βpost = 40
<r> = 2-6

40

20
10
0

0

0.05

0.1

0.15

0.2

Spiking error probability, r

0.1

0

2

Connection probability

3

1

0

0

0.2

0.4

0.6

0.8

1

Connection probability

Probability density

0.2

E
N→∞
Ninh/N = 0.2
Nw/h = 70
βpost = 40
r = 2-6
<f> = 0.2

0.25

Firing probability, f

Inhibitory
Excitatory

0.3

0.25

D

C

0.4

Average connection weight

B

0

0.05

0.1

0.15

0.2

Spiking error probability, r

F

1
0.8
0.6
0.4
0.2
0

0

0.2

0.4

0.6

0.8

Firing probability, f

1

700
600
500
400
300
200
100
0

0.25

Average connection weight

A

0

0.05

0.1

0.15

0.2

0.25

0.2

0.4

0.6

0.8

1

Spiking error probability, r

1200
1000
800
600
400
200
0

0

Firing probability, f

G

0.20
0.15
0.10
0.05
0.00
0.20

0.40

0.60

0.80

1.00

Average uPSP, [mV]

Figure 4: Properties for connections in associative networks of heterogeneous neurons. A-C. Connection probability
(B) and average non-zero connection weight (C) for inhibitory (red) and excitatory (blue) connections in a network of
neurons with distributed spiking errors. Spiking error probabilities of inhibitory and excitatory inputs were randomly
drawn from a log-normal distribution with <r> belonging to the feasible parameter region (red contour in Figures 2
and 3) (A). Values of all other parameters of the model neurons are shown in (A). Noisy connections have lower

11

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

probabilities and weights. D-F. Same for a network of neurons with heterogeneous firing probabilities. The
postsynaptic noise strength in (A) and (D) and the spiking error probability in (D) belong to the feasible parameter
region. Numerical values of all other parameters are explained in [12]. G. Connection probability versus average uPSP
for 16 local laminar and inter-laminar excitatory projections from mouse barrel cortex [35]. Black line is the best
linear fit.

Motivated by the agreement between the results of the associative network model and
measurements from various cortical systems, we put forward two falsifiable predictions. First, we
predict that in cortical networks, inhibitory and excitatory connections originating from unreliable
neurons (or neuron classes) must have lower connection probabilities and average uPSP (Figures
4B, C). We also predict that inhibitory and excitatory neurons (or neuron classes) operating with
low firing rates must establish weak and low probability connections (Figures 4E, F).

CONCLUSION
In summary, we built a comprehensive description of the error propagation cycle into the model
of associate memory storage. The model was solved theoretically using the replica method in the
limit of infinite network size and numerically for large but finites networks. We note that intrinsic
noise and postsynaptic spiking errors were first incorporated into the model of associative memory
storage by Brunel et al. [10] (see Supplementary Material therein). However, Brunel et al. does
not include homeostatic constraints and learning by inhibitory inputs, nor does it examine the
complete error propagation cycle, as synaptic noise and presynaptic spiking errors were not
considered. We examined the properties of neuron-to-neuron connectivity and dynamics in
associative networks at capacity and determined a region of parameters in which the model results
are in general agreement with the seven examined features of cortical networks. We show that
heterogeneity of neuron population (e.g. different neuron classes) leads to a positive correlation
between connection probability and weight, a conclusion supported by measurements from mouse
barrel and visual cortices. These results inspired us to put forward two experimentally testable
predictions related to cortical connectivity. Finally, we confirmed that memories loaded into
individual neurons at a given noise strength can be successfully recalled at the network level in the
12

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

presence of the same or slightly lower noise strength (Figure S1). Therefore, errors and noise
present during learning can be beneficial, as they increases the robustness of stored memories to
fluctuations during memory recall, though, at the expense of memory storage capacity.

ACKNOWLEDGEMENTS
This work is supported by the AFOSR grant FA9550-15-1-0398 and the NSF grant IIS-1526642.

SUPPLEMENTAL MATERIAL
This Supplemental Material describes a model of associative memory storage by a recurrent
network of inhibitory and excitatory McCulloch and Pitts neurons. The model builds on previous
studies of associative memory (see e.g. [6,7,10-12,18,19]), and provides an all-inclusive account
of the error propagation cycle, from pre- and postsynaptic spiking errors, to synaptic noise, to
intrinsic noise. In addition, the model incorporates a number of constraints motivated by the
experimental data. The model is solved theoretically with the replica method in the limit of infinite
network size [21,22], and numerically for large, but finite networks. The model gives rise to a
comprehensive list of predictions regarding the structure and dynamics of the neural network at
the critical memory storage capacity that are consistent with a large number of experimental studies
of connectivity in local cortical circuits.

Perceptron model with biologically inspired constraints in the presence of errors and noise
We consider a single perceptron-like neuron that receives Ninh inhibitory and (N − Ninh) excitatory
input connections. The neuron is faced with a task of learning a set of m input-output associations

X



, y   , where X  and y  are a binary vector and a scalar describing the neuron’s input and

the target output at time step μ. The neuron must learn these input-output associations by adjusting

the weights of its connections, Jj, in the presence of two constraints. First, the l1-norm of
13

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

connection weights must remain fixed during learning. Second, the signs of inhibitory and
excitatory connection weights must not change during learning.
Learning in the model is accompanied with three types of errors or noise. These include, pre- and
postsynaptic spiking errors, or errors in X  and y  , synaptic noise, or noise in J, and intrinsic
noise, or noise in the neuron’s threshold of firing, h. In the following, we use asterisks to denote
quantities containing errors or noise (e.g. X * ), whereas variables without asterisks represent the
mean (for h and Jj) or original (for X  and y  ) values.
Individual elements of X  ( X j ), are independently drawn from input dependent Bernoulli
probability distributions: 0 with probability 1 – fj and 1 with probability fj. Errors in X  are
introduced with Bernoulli trials by making independent and random changes with probabilities

P  X *j   0 | X j  1  p j and P  X *j   1| X j  0   p j . Similarly, the target outputs, y  , are
independently drawn from a Bernoulli probability distribution: 0 with probability 1 – fout and 1
with probability fout. The probabilities of error in the neuron’s output must be bounded,


P  y*  0 | y   1  pout
and P  y*  1| y   0   pout
. In the following, we only consider

spiking error probabilities in the ranges, p j ,out  f j ,out and p j ,out  1  f j ,out , which is required for
the stability of the replica theory calculation.
We assume that the neuron’s connection weights fluctuate based on the quantal model of synaptic
transmission of Del Castillo and Katz [20]. According to this model, connection weight values J *j
on different trials are described with a Binomial distribution in which J *j  J j and the variance
is proportional to the mean, var  J *j  

h syn
N

J j . The dimensionless coefficient βsyn is referred to

as the synaptic noise strength, and a factor of h/N is introduced for convenience.
All other sources of noise contributing to the neuron’s postsynaptic potential (e.g. background
synaptic activity and stochasticity of ion channel states) are referred to as intrinsic noise. In the
model, these sources are folded into h* , which is assumed to be Gaussian distributed with h*  h

14

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

h 2  int2
and var  h  
. Here, βint is a dimensionless coefficient called intrinsic noise strength, and
N
*

a factor of h2/N is introduced for convenience.
The above described model can be summarized as follows:


N



   J *j X *j   h*   y* ,   1, , m
 j 1

N
1
 J jgj  w
N j 1

J j g j  0,

j  1, , N

P  X j  1  f j ; P  X *j   0 | X j  1  p j ; P  X *j   1| X j  0   p j

(S1)



; P  y*  1| y   0   pout
P  y   1  f out ; P  y*  0 | y   1  pout

J *j  J j ; var  J *j  
h*  h; var  h*  

h syn
N

Jj

h 2  int2
N

In these expressions, θ is the Heaviside step-function, fj is the firing probability of input j, fout is
the firing probability of the neuron. To enforce sign-constraints on connection weights we
introduced parameters gj, which equal 1 if the input j is excitatory and –1 if it is inhibitory.
Parameter w is referred to as the average absolute connection weight. The neuron is faced with the
task of finding connection weights,  J j  , that satisfy Eqs. (S1) for a given set of model parameters:


N , m, h, w,  g j  ,  f j  , f out ,  p j  ,  p j  , pout
, pout
,  syn , int .

Reformulation of the model for the large N limit

In the limit of large N, the Central Limit Theorem ensures that the neuron’s postsynaptic potential
(PSP),

N

J
j 1

*
j

X *j  , at any time step is Gaussian distributed. Therefore, the deviation of PSP from

15

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

N

the threshold of firing, I *   J *j X *j   h* , is also Gaussian distributed with mean and standard
j 1

deviation given by the following expressions:
N

I    J j 1  p j  X j  p j 1  X j    h
j 1

1

 N 
h
h 2  int2   2
      J 2j p j 1  p j  X j  p j 1  p j 1  X j   syn J j g j 1  p j  X j  p j 1  X j  

N
N 2  
 j 1 









(S2)
As a result, the inequality constraints on the probabilities of output spike errors in Eqs. (S1) can
be expressed in terms of I  and   :

I   2erf 1 1  2 pout
  ,

I    2erf 1 1  2 pout
  ,

y  1

(S3)

y  0

The above two inequalities can be combined into a single expression that must hold, assuming the
association μ is successfully learned:

2y








 1 I   2 erf 1 1  2 pout
 y   erf 1 1  2 pout 1  y    

(S4)

Additional assumptions required for the replica theory solution

Following the procedure outlined in [11,12], we assume that the model parameters m/N,  f j  , fout,

p  , p ,

j


j



pout
, pout
, βsyn, and βint are intensive, or of order 1 with respect to N. In addition, we

h 

assume that connection weights are inversely proportional to the system size,  J j  J j  , and
N 


 

refer to J j

as scaled connection weights. This particular scaling is traditionally used in

associative memory models [10], and it has been shown that in the biologically plausible high16

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

weight regime, Nwf  h , many model results become independent of this assumption [23]. It
follows from the second line of Eqs. (S1) that w 

h
w , and we refer to w as scaled average
N

absolute connection weight.
The complete model, rewritten in terms of the scaled variables, contains one equality and m + N
inequality constraints:

2 y



1
 1 
N

N



 J 1  p  X   p 1  X    1 
j 1

j


j

j


j

j







2

erf 1 1  2 pout
 y   erf 1 1  2 pout 1  y   
N
1

 1 N 2 
2
2









   J j p j 1  p j  X j  p j 1  p j 1  X j    syn J j g j 1  p j  X j  p j 1  X j    int   ,   1, , m
 N j 1

1 N 
 J j g j  w
N j 1
J g  0, j  1, , N



j







j

P  X j  1  f j ; P  y   1  f out

(S5)

Replica theory solution of the model





We begin by calculating the volume of the connection weight space,   X  , y   , in which Eqs.
(S5) hold for a given set of associations,  X  , y   :

17

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.



 X  , y 



1
   dJ j   J j g j  
j 1
j 1
N
N

N






m


J j g j  w      2 y   1 

j 1
  1 

N

1

N

 J 1  p  X   p 1  X    1 

1

N

  J  p 1  p  X   p 1  p 1  X     

N


j

j

j 1
N


j

2
j

j 1




j

j


j

j




j

j





2


y   erf 1 1  2 pout
erf 1 1  2 pout
1 y  


N


j

j

syn



J j g j 1  p j  X j  p j 1  X j 



1

2


2 
 int  

 


(S6)
The typical volume of this solution space, typical , is defined through the averaging of

 

ln   X  , y  

 over the set of associations  X



, y   , and is calculated by introducing n replica

systems:

 

ln  typical   ln   X , y



The quantity   X  , y  



 X  , y 



n



n

X  , y 





  



 X  , y 
X  , y

N ,n





 J 1  p  X   p 1  X    1 

1

N

  J   p 1  p  X   p 1  p  1  X     

j 1
N

j 1


j

a
j

a
j

2


j



n

1

N

N


j

j


j

j

j


j

n0

n

X  , y 

1
(S7)

n

can be rewritten as a single multidimensional integral:

1
   dJ aj   J aj g j   
 
j , a 1
j , a 1
a 1
X , y 
N
N ,n

 lim








j


m ,n


J aj g j  w      2 y   1 

j 1
  ,a 1 

N





2

erf 1 1  2 pout
 y   erf 1 1  2 pout 1  y   
N

j

syn



J aj g j 1  p j  X j  p j 1  X j 



1

2

2 
 int  
  
 X  , y 

(S8)
This integral is calculated by following a previously established procedure [11,12]. Below, we
outline the main steps of this calculation.
18

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

To calculate the average over the associations, we first decouple X  and y  by introducing two
new sets of variables:

 a,
N

 a, 

N

1
N

 J 1  p  X   p 1  X    1

1
N

  J   p 1  p  X   p 1  p 1  X     




j

a
j

j 1

N

a 2
j

j 1


j

j


j


j

j


j

j


j

j

syn





J aj g j 1  p j  X j  p j 1  X j    int2 


(S9)
These variables are next incorporated into Eq. (S8) with the help of Dirac δ-functions:



 X  , y 
m ,n



, a 1



m ,n





n

X  , y 

   2 y   1 



  1 


  

, a 1

N



 , a 1
m ,n

 a ,



a,



1
N



a,

1
N

N ,n

N ,n

j , a 1

j , a 1



   dJ aj   J aj g j

n



N

a 1



j 1

   N1  J

a
j

 m ,n d  a ,  m,n
g j  w  
d  a ,  

  ,a 1 N  ,a 1

1


 2 erf 1 1  2 pout
 y   erf 1 1  2 pout 1  y    a,  2  
 y 



N





 J 1  p  X   p 1  X    

j

a
j

j 1


j

j

j



  J   p 1  p  X   p 1  p 1  X     
N

j 1

a
j

2


j


j

j


j


j

j

syn


J aj g j 1  p j  X j  p j 1  X j   int2  
  
X 





(S10)
Symbol d' in this expression and thereafter is designated for 0 to ∞ integration, whereas d is used
for integration from -∞ to ∞. In the following step, the Heaviside step-functions and the δ-functions
are replaced with their Fourier representations, which makes it possible to perform the averaging
over the associations:

19

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.





   J

n

 X  , y 
N ,n

j , a 1

m ,n

e


iˆ a ,

a
j



gj

N ,n
m ,n
d  a ,  d ˆ a ,  m,n d u a ,  duˆ a ,  m ,n d  a ,  dˆ a ,  n dkˆ a
   dJ aj 

 2 

2
2
j , a 1
a 1 2 / N
 , a 1
 , a 1
, a 1
X  , y 

n

 e


ikˆ a 



N

j 1

a 1

N 

a ,



2
 iˆ a , a , iˆ a , int

, a 1



 J aj g j  Nw 


m,n

e


 iuˆ

a , a ,

u

, a 1

m, N

e




iA j

n

1 
1





iuˆ a ,   a , Cout
 iuˆ a ,   a ,  Cout
 a ,  2 
 a ,  2  


f e 



 (1  f out )e


out


 , a 1 




m ,n

 ˆ a , J aj 
N
a 1

i  syn g j A j
N

n

iB

n

ˆ a , J aj  Nj ˆ a ,  J aj 
a 1

2



a 1

C j  n a , a 
 ˆ J j 

2 N  a 1




2

, j 1

(S11)
The following notation is used in the above expression:
Aj  1  p j  f j  p j 1  f j 

B j  f j p j 1  p j   1  f j  p j 1  p j 
C j  f j 1  f j  1  p j  p j 

2

(S12)



 2erf 1 1  2 pout
Dout


We note that parameters A, B, and C, defined in Eqs. (S12) are nonnegative.
Next, we decouple the products containing indices j and µ by introducing three sets of order
parameters and embedding them into Eq. (S11) with the help of Dirac δ-functions as was done in
Eqs. (S10, S11):

20

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

N

1
N

 A J
j

j 1

 1

a
j



 X  , y 
n

e



isˆa N  N s a



a 1



sa
,
N


N

1
N

j 1

n

n

X  , y 
n

 eiN ˆ
a

a

 
a 1

n ,n

 eiNqˆ

 

a
a
syn A j g j J j  B j J j



  int2   a ,

1
N

N

 C J
j 1

j

a
j

J bj  q a ,b

dkˆ a n ds a dsˆ a n d  a d ˆ a n dq a ,b dqˆ a ,b




2 / N a 1 2 / N a 1 2 / N a ,b 1 2 / N
a ,b a ,b

q

a ,b 1

a 1

2

n

e

 iNkˆ a w



a 1

 n
a
a
a n
a
a
ˆa n
 d  d   d u duˆ  d  dˆ
 a 1 2 a 1 2 a 1 2




i  uˆ a   a  Dout
 f out e a1


n

a



 (1  f out )e

i

 uˆ a   a  Dout
n

a 1

a

 





m

  ˆ a   a  s a ˆ a  a  a uˆ au a  2  ˆ a ˆb qa ,b 
a ,b1
 
e a1


n

1

i

N ,n

N ,n

 dJ    J
j , a 1

a
j

j , a 1

a
j

n

gj

 e

n ,n

ikˆ a

N

N

N

j 1

j 1

j 1

 J aj g j isˆa  Aj J aj iˆ a    syn Aj g j J aj  B j  J aj 

a 1

2

2 
 int
 n ,n


e

 iqˆ a ,b

N

 C j J aj J bj

(S13)

j 1

a ,b 1

After integrating over  a , ˆ a ,  a , and ˆ a we obtain:



 X  , y 

e



n

n

X  , y 

 
a 1

dkˆ a n ds a dsˆ a n d  a dˆ a n dq a ,b dqˆ a ,b




2 / N a 1 2 / N a 1 2 / N a ,b 1 2 / N

n
n ,n
 n
N i
sˆa  kˆ a w  i  aˆ a  i
q a ,b qˆ a ,b  n G E ( s a , q a ,b )  nG S

a
1
a
1
a
,b 1






 

  





kˆ ,sˆ ,qˆ  
a

a

a ,b



n
n
1



uˆ a uˆ b q a ,b 
a 
i  uˆ a  s a u a  Dout
 i  uˆ a  s a  u a  Dout
1  n d u a duˆ a  2 a
,b 1
a
a

1
1

s

q
e
f
e
(1
f
)
e



,
,
ln
      n    2
out
 out
a 1


n
n
n
2
2 
a b a ,b 
 n
i   kˆ a g j  sˆa A j  g j J a i  ˆ a   syn A j J a  B j  J a   int
N
  iC j  J J qˆ
1
1


,
a
a
a
a
b
a
a 1
a 1
a ,b 1
ˆ




ˆ
GS k , sˆ  ,   , qˆ  
 ln  d J e

n N j 1   a 1


n

G E



a

 

a

a ,b



a

  





(S14)
The above integral is calculated by using the steepest descent method combined with the
assumption of a replica symmetric saddle point, s a  s ,  a    0 , q a , a  q0 , q a  b  q , kˆ a  kˆ ,
sˆ a  sˆ , ˆ a  ˆ , qˆ a , a  qˆ0 , and qˆ a  b  qˆ :

21

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.



 X  , y 



n

X



,y





~e







ˆ   iˆ  iq qˆ iqqˆ  G  s , , q , q   G kˆ , sˆ ,ˆ , qˆ , qˆ
Nn i sˆ  kw
0 0
E
0
S
0



n
n
n
n
2
1



i  uˆ a  s u a  Dout

 i  uˆ a  s  u a  Dout
  
 uˆ a   12 q  uˆ a uˆb 
1  n d u a duˆ a  2 q0 
a 1
a b 1
a 1
a 1


 (1  f out )e
GE  s,  , q0 , q   ln  
e
f e
 out

n  a 1 2



n
n
n
n
2
a 2
a b
2 
ˆ
ˆ j  g j  J a iˆ    syn A j J a  B j  J a   int
 iC j qˆ0   J   iC j qˆ  J J 
1 1 N  n  a i kg j  sA

a 1
a 1 
a 1
a b1
ˆ


ln
GS k , sˆ, ˆ , qˆ0 , qˆ 
d
J
e
 

n N j 1   a 1







(S15)





The non-redundant, replica symmetric saddle point coordinates s,  , q0 , q, kˆ, sˆ, ˆ , qˆ0 , qˆ satisfy the
following system of nine equations and one inequality:
GE  s,  , q0 , q 
GE  s,  , q0 , q 
GE  s,  , q0 , q  iqˆ
iqˆ
iˆ GE  s,  , q0 , q 
 0;
 ;
 0;
 ;



s

q0
q



GS kˆ, sˆ, ˆ , qˆ0 , qˆ



kˆ

GS kˆ, sˆ, ˆ , qˆ0 , qˆ
qˆ0

  iw ;



GS kˆ, sˆ, ˆ , qˆ0 , qˆ

  i;

sˆ
GS kˆ, sˆ, ˆ , qˆ0 , qˆ

  iq ;



0

qˆ



GS kˆ, sˆ, ˆ , qˆ0 , qˆ

  iq;   0

ˆ

  i ;

(S16)
To simplify the expressions for GE and GS we employ the Hubbard-Stratonovich transformation
(see e.g. [12]) and take the n  0 limit:


 d uduˆ  1 q0uˆ 2  2 x

ln
dx
f
   out   2 e 2


 d uduˆ  1 q0uˆ 2  2 x  1 quˆ  1 quˆ 2 iuˆ  s u  Dout    
2
2
(1  f out ) ln  
e 2

 2




GE  s,  , q0 , q  







e x

1
GS kˆ, sˆ, ˆ , qˆ0 , qˆ 
N

2

N





j 1 

ˆ
2
ˆ j g j 2 x

 i kg j  sA
dx ln  e iˆ int  d Je



e x

2

(S17)

22



1
1


 quˆ  quˆ 2  iuˆ s u  Dout
2
2






 

 



ˆ syn A j J i C j  qˆ0  qˆ   B j ˆ J 2
 iC j qˆ  i

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

Integrals in the arguments of the natural logarithm functions can be expressed in terms of
complementary error functions:









 f ln  erfc   s  Dout   x 2q    (1  f ) ln  erfc  s  Dout   x 2q     ln 2
dx
out
   out  




2  q0  q 
2  q0  q 








2


ikˆ isgˆ j Aj iˆ syn Aj  2 x iC j qˆ 
2

 ikˆ  isg

4 i  C j  qˆ0  qˆ   B j ˆ 
N  x
ˆ syn A j  2 x iC j qˆ 
ˆ j A j  i
2
e
1
e
ˆ int
 i



ˆ
GS k , sˆ, ˆ , qˆ0 , qˆ   
dx ln e
erfc 


N j 1  
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
2
2
i
C
q
q
B

i
C
q
q
B





 j 0  j 
 j  0  j   




GE  s,  , q0 , q  





e x

2



(S18)
The following substitutions transform the replica symmetric saddle point coordinates into the real
domain, u 


Dout
 s

2q

, 

q0  q
qˆ  qˆ
ˆ
ikˆ
isˆ
 0 , t  iqˆ , z 
,   0
, 
,
 
q̂
q
qˆ
2 iqˆ
iqˆ

where they can be obtained by solving the following problem:



 u  x 
 u  x   
dx  f out ln  erfc  
   (1  f out ) ln  erfc 
    ln 2
 
  
  



2
   2 zg j A j  t  syn A j  2 x C j 






2

2



C
B
j
j
N  x
   2 zg j Aj   t  syn Aj  2 x C j  

2
1
e
 e

erfc 
GS  , t , , z ,     
dx ln  e  t int



N j 1  
2 C j t  B j t
2 C j  B j






GE  u , u ,   





e x

2



GE  u , u ,   GE  u , u ,  
GE  u , u ,   t  Dout  Dout  
GE  u , u ,  
2t
1

 0;


;
;
2
u
u

u
 2  u  u  
  u  u  
2



GE  u , u ,   t  Dout  Dout  
GS  , t , , z ,  
GS  , t , , z ,  

  w t ;
 2 t ;
    ;
3
u

  u  u 
z
2



Dout
 Dout


GS  , t , , z ,  
GS  , t , , z ,  
 t ;
 t
2


2  u  u 
2



GS  , t , , z ,    Dout  Dout        w
z



  ;   0;   0; u  u  0
2
t
t
2 t
2  u   u 
2

(S19)

23

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

Replica theory solution at critical capacity

With increasing number of associations m, typical shrinks and approaches zero at the maximum
(critical) capacity of the neuron,  c 

mc
. In this limit  q0  q  goes to zero as well, and the saddle
N

point equations Eqs. (S19) can be expanded asymptotically in terms of 1/  and 1/  . In the
leading order these equations yield:
(1  f out ) F  u   f out F  u   0


 2t  Dout  Dout 
f out D  u   (1  f out ) D  u  
 c  u  u  2

(1  f out ) F  u  

2t

c

1

 u  u 

 t      Dout  Dout 
(1  f out ) F  u   
3
c
 u  u 


1
N



2

   2 zg j Aj   t  syn Aj 
  2 w t

2 Cj



Cj

N

2

 C   B  F  
j 1

j

j

   2 zg j Aj   t  syn Aj 
2 t

2
C
j 1
j
j
j


   2 zg j Aj   t  syn Aj 
B jC j
1 N
1
  t   int2     
D

2


2 N j 1  C   B  
2
N
2 Cj
j
j


1
N

1
N

Aj g j C j

N

 C   B  F  

C 2j

N


j 1

C   B  
j

j

2

   2 zg j Aj   t  syn Aj
D

2 Cj


N

 syn Aj C j

C j t  syn Aj

j 1

C j  B j

   2 zg j Aj   t  syn Aj
F 

2 Cj



  Dou t  Dout
 t

2

 u  u 


   2 zg j Aj   t  syn Aj

F


t  C j  B j  
2 Cj
j 1
  0;   0; u  u  0
1
4N

N



2


 Dout  Dout        w  2 z
    int2     
2

2 t
2  u  u 

2

(S20)
Special functions E, F, and D, in Eqs. (S20) are defined as follows:

24






bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

1
1  erf  x  
2
1  x2
F  x 
e  x 1  erf  x  
E  x 

(S21)



D  x   xF  x   E  x 
After replacing  t with y , and eliminating variables,  , t ,  ,  , and  , we arrive at the final
system of six equations and one inequality. This system contains six latent variables u , x ,  , y ,
and z which determine the critical capacity of the neuron,  c :
(1  f ) F u  f F u  0
   out   
out


4  u  u  f out E  u   (1  f out ) E  u 
x 
2


f out F  u   (1  f out ) F  u 

Dout
 Dout



1 N
   2 zAj g j  y  syn Aj 
Cj
 
  2 wy

F 

2 Cj
 N j 1 C j x  B j 


 1 N Aj g j C j    2 zA g  y  A 
j j
syn j
  2y
F 
 


N
C
x

B
2
C
1
j

j
j
j




2
B
    2 zA g  y  A
1 N
Cj
u  u  

j
j j
syn j

 
Cj  D

2
2


 
2 Cj
 N j 1  C j x  B j   2  Dout  Dout 

 

 N  A C
   2 zAj g j  y  syn Aj 
j
 1  syn j
  2 w  4 z  4 int2 y
F 


 N j 1 C j x  B j
2 Cj



u  u  0
f out D  u   (1  f out ) D  u  1
c  x
2
 fout E  u   (1  fout ) E  u   N
2

C 2j

N


j 1

C x  B 
j

j

2


   int2 y 2  yw  2 yz



   2 zAj g j  y  syn Aj
D

2 Cj







(S22)
These equations were solved in MATLAB to produce the results shown in Figure 4 of the main
text

(network

of

heterogeneous

neurons),

and

the

https://github.com/neurogeometry/Associative_Learning_with_Noise

25

code

is

available

at

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

We note that Eqs. (S22) are consistent with the solution described in the Supplementary Material
of Brunel et al. [10], who solved a similar problem by minimizing the probability of postsynaptic
spiking errors for a given intrinsic noise level. However, the model of Brunel et al. does not account
for the homeostatic constraints and learning by inhibitory inputs, nor does it consider the complete
error propagation cycle by omitting synaptic noise and presynaptic spiking errors.

Distribution of input weights at critical capacity

Connection probabilities, Pcon, and probability densities for non-zero input weights, pPSP, at critical
capacity can be calculated as previously described [10-12]. The result depends on the set of latent
variables of Eqs. (S22):
   2 zA j g j  y  syn A j
Pi con  E  

2 Cj


 

piPSP J 

i 






  g i J 
   2 zA j g j  y  syn A j
 
2 i wE

2 Cj







e

 J
  2 zAi gi  y  syn A j 


gi 
 2 w

2 Ci
i



2

(S23)

Ci

  Ci x  Bi 
2 wy

For a given input, i, there is a finite probability that the connection weight is zero, while the
probability density of non-zero connection weights is a truncated Gaussian with a standard
deviation  i w .

Solution in the case of balanced spiking errors

26

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

Here, we consider a scenario in which the expected numbers of erroneous spikes and spike failures
are equal, and, as a result, these spiking errors do not affect the input and output firing probabilities

 f  and
j

f out :
p j 1  f j   p j f j ,

j  1, , N

(S24)



pout
f out
1  fout   pout

In this case, it is more convenient to express the results in terms of spiking error probabilities:
rj  p j 1  f j   p j f j

(S25)



rout  pout
f out
1  fout   pout

Eqs. (S22, S23) remain unchanged, but the coefficients defined in Eqs. (S12) transform into:
Aj  f j


rj

B j  rj  1 
 4 f j 1  f j  




rj

C j  f j 1  f j  1 
 2 f j 1  f j  



2

(S26)


rout 

Dout
 2erf 1  1 

 1  f out  

r 

Dout
 2erf 1  1  out 
f out 


Figure 4 of the main text shows the solutions of Eqs. (S22, S23) for log-normal distributions of
spiking error probabilities and firing probabilities. The contribution of noisy inhibitory and
excitatory inputs to the neuron’s postsynaptic potential is suppressed, as such inputs have lower
strengths and connection probabilities (Figures 4B, C).

Solution in the case of two homogeneous classes of inputs and balanced spiking errors
27

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

In this case, there are Ninh inhibitory inputs and (N − Ninh) excitatory inputs, and all inputs and the
output have the same firing probability, f, and the same spiking error probability, r. Eqs. (S22, S23)
simplify significantly after the introduction of new variables, v 

  2 zf  y  syn f
2 C

:

(1  f ) F  u   fF  u   0

N exc
2
 N inh
 N F  v   N F  v   

N exc
2
 N inh
 N F  v   N F  v    wf



2
2
N exc
2
 N inh
 2  post

  u  u   2  N D  v   N D  v     2

2

2  post
2




 1
fE  u   (1  f ) E  u 


  
v  v    v  v  

  u  u  


fF  u   (1  f ) F  u 


  wf

u  u  0





 2  u  u 
c 
4
2 post
4

2

fD  u   (1  f ) D  u   1
 N
N
 v  v    v  v    inh D  v   exc D  v  
2 

N

  N
 fF  u   (1  f ) F  u    wf
2

con
Pinh
/ exc  E  v 

 

PSP

pinh
/ exc J 

   J 

  v 
2 wE

e

 J


 v 
 2 w


2

(S27)
Intrinsic and synaptic noise in Eqs. (S27) is entirely contained within the parameter  post , while
spiking error probabilities appear only in ξ and ζ:

28

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

 post  int2  w  syn f
r  4 f 1  f   r   1 
r

erf 1 
2 
2  2 f 1  f   r  
 1 f

 

2 f 1  f 

 1 
r
 erf 1 
w  2 f 1  f   r  
 1 f


r 
1 
  erf  1   
f 



2

(S28)


r 
1 
  erf 1   
f 



We note that due to the imposed constraints on spiking error probabilities, p j ,out  f j ,out and
p j ,out  1  f j ,out , combined with the definition of r in Eqs. (S25), r  2 f 1  f  , and parameters ξ

and ζ are positive. In the main text, parameter  post is referred to as the postsynaptic noise strength.
Results shown in Figures 1-3 of the main text are expressed in terms of  post and r.

Numerical solution of the model for finite N

For a finite number of inputs, solution to the problem outlined in Eqs. (S5) can be obtained
numerically. To that end, we make the problem feasible by introducing a slack variable, s   0 ,
for every association, and choose the solution that minimizes the sum of these variables:
 m

arg min   s  

J j    1 

 2 y  1  N1





Dout
y   Dout
1 y 








J j 1  p j  X j  p j 1  X j    1   s 


N
j 1

N

1

 1 N 2 
2









2
   J j p j 1  p j  X j  p j 1  p j 1  X j    syn J j g j 1  p j  X j  p j 1  X j   int   ,   1, , m
 N j 1

N
1
 J j g j  w
N j 1
J g  0, j  1, , N



j





j



s  0,   1, , m

(S29)

29



bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

Eqs. (S29) were solved in MATLAB by using the fmincon.m function. This code is available at
https://github.com/neurogeometry/Associative_Learning_with_Noise

Memory storage and recall in an associative network of model neurons

We consider a McCulloch and Pitts neural network [1] of Nexc excitatory and (N - Nexc) inhibitory
neurons. The network state at time μ is described by a binary vector of neural activities, X  . The
network is loaded with temporal sequences of network states, X 1  X 2  ... X m 1 , by modifying
the weights of connections between neurons {Jij} (weight of connection from neuron j to neuron
i). As described above, there are sign and homeostatic constraints on connection weights of
individual neurons. To learn a temporal sequence, individual neurons (i) must independently
associate inputs they receive from the network, X  , with the corresponding outputs derived from
the associative sequence, X i 1 . As before, learning in the model is accompanied with spiking
errors, or errors in X  , synaptic noise, or noise in J, and intrinsic noise, or noise in the neuron’s
threshold of firing, h, and we use asterisks to denote quantities containing errors. We assume that
neurons in the network have identical firing thresholds, h, average absolute connection weights, w,
synaptic and intrinsic noise strengths, βsyn and βint, but may have different firing probabilities, fi,
and spiking error probabilities, ri. This network model of associative memory storage in the
presence of errors and noise can be summarized as follows:


N



   J ij* X *j   h*   X i* 1 ;   1, , m, i  1, , N
 j 1

N
1
 J ij g j  w; i  1, , N
N j 1

J ij g j  0; i, j  1, , N
P  X i  1  f i ; P  X i*  0 | X i  1 
J ij*  J ij ; var  J ij*  
h*  h; var  h*  

h syn
N

J ij

h 2 int2
N
30

ri
ri
; P  X i*  1| X i  0  
2 fi
2 1  f i 

(S30)

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

To load a given memory sequence into the network, Eqs. (S30) were solved independently for
every neuron in the network as describe in the previous section. To retrieve a loaded memory
sequence, the network is initialized at the beginning of the loaded sequence, X 1 , and at each
subsequent step of memory playout synaptic and intrinsic noise are added independently to every
connection and every neuron in the network. The sequence is said to be retrieved successfully if
the network states during the retrieval do not deviate substantially from the loaded states (Figure
S1 A). In practice, there is no need to precisely define the threshold amount of deviation. This is
because for large networks the Hamming distance between the loaded and retrieved sequences,
normalized by N, either remains within ~ 2r or diverges to ~ 2 f 1  f  , which is much greater in
the feasible parameter region considered in the main text (Figure S1 B). Figure S1 C shows the
probability of retrieving the entire stored sequence and the average retrieved fraction of the stored
sequence length for different values of postsynaptic noise. The transition from successful retrieval
to retrieval failure is relatively sharp. It illustrates that the stored sequences can be successfully
retrieved at (or slightly below) the post synaptic noise level present during learning.

μ=3
μ=4
Neuron 2
Neuron 3

…
…
Neuron 4

μ=m

0.4
0.3
0.2
0.1
0
0

10

20

30

40

Time step, µ

50

60

1

1
0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0
0

20

40

60

80

Retrieved length fraction

Neuron N

μ=2

N = 400
Ninh/N = 0.2
Nw/h = 70
f = 0.2
βpost = 40
r = 2-6

βpost during
learning

…
μ =1

C

B 0.5

Retrieval probability

Loaded sequence
Successful memory retrieval
Failed memory retrieval

Neuron 1

Normalized Hamming distance

A

0
100

Postsynaptic noise strength, βpost

Figure S1: Recall of memory sequences stored in a network of model neurons. A. Illustration of memory playout
during successful (blue) and failed (red) retrieval. The loaded memory sequence is shown in black. B. Normalized
Hamming distances between the loaded and retrieved sequences as functions of time step. Successfully retrieved
sequences do not deviate from the loaded sequences by more than a threshold amount (dashed line). C. Probability of
successful memory retrieval (left y-axis) and retrieved length fraction (right y-axis) as functions of postsynaptic noise
strength during retrieval. The dashed line indicates the postsynaptic noise strength during learning.

31

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

REFERENCES

[1]

W. McCulloch and W. Pitts, Bull Math Biol 5, 115 (1943).

[2]

D. O. Hebb, The organization of behavior; a neuropsychological theory (Wiley, New
York,, 1949).

[3]

F. Rosenblatt, Cornell Aeronautical Laboratory Report 85-460-1 (1957).

[4]

M. L. Minsky and S. Papert, Perceptrons : an introduction to computational geometry
(MIT Press, Cambridge, Mass., 1988), Expanded edn.

[5]

T. M. Cover, IEEE Trans. EC 14, 326 (1965).

[6]

E. Gardner, J. Phys. A: Math. Gen. 21, 257 (1988).

[7]

E. Gardner and B. Derrida, J. Phys. A: Math. Gen. 21, 271 (1988).

[8]

J. J. Hopfield, Proc Natl Acad Sci U S A 79, 2554 (1982).

[9]

H. M. Kohler and D. Widmaier, J. Phys. A: Math. Gen. 24, L495 (1991).

[10]

N. Brunel, V. Hakim, P. Isope, J. P. Nadal, and B. Barbour, Neuron 43, 745 (2004).

[11]

J. Chapeton, T. Fares, D. LaSota, and A. Stepanyants, Proc Natl Acad Sci U S A 109,
E3614 (2012).

[12]

J. Chapeton, R. Gala, and A. Stepanyants, Front Comput Neurosci 9, 74 (2015).

[13]

C. Allen and C. F. Stevens, Proc Natl Acad Sci U S A 91, 10380 (1994).

[14]

G. B. Ermentrout, R. F. Galan, and N. N. Urban, Trends Neurosci 31, 428 (2008).

[15]

A. A. Faisal, L. P. Selen, and D. M. Wolpert, Nature reviews. Neuroscience 9, 292 (2008).

[16]

Y. Yarom and J. Hounsgaard, Physiological reviews 91, 917 (2011).

[17]

A. Stepanyants and D. B. Chklovskii, Trends Neurosci 28, 387 (2005).

[18]

N. Brunel, Nature neuroscience 19, 749 (2016).

[19]

R. Rubin, L. F. Abbott, and H. Sompolinsky, Proc Natl Acad Sci U S A 114, E9366 (2017).

[20]

J. Del Castillo and B. Katz, The Journal of physiology 124, 560 (1954).

[21]

S. F. Edwards and P. W. Anderson, J. Phys. F: Metal Phys. 5, 965 (1975).

[22]

D. Sherrington and S. Kirkpatrick, Physical Review Letters 35, 1792 (1975).

[23]

D. Zhang, C. Zhang, and A. Stepanyants, bioRxiv, 320432 (2018).

[24]

M. N. Shadlen and W. T. Newsome, The Journal of neuroscience : the official journal of
the Society for Neuroscience 18, 3870 (1998).

[25]

C. F. Stevens and A. M. Zador, Nature neuroscience 1, 210 (1998).

32

bioRxiv preprint doi: https://doi.org/10.1101/583922; this version posted March 21, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

[26]

G. R. Holt, W. R. Softky, C. Koch, and R. J. Douglas, Journal of neurophysiology 75, 1806
(1996).

[27]

W. R. Softky and C. Koch, The Journal of neuroscience : the official journal of the Society
for Neuroscience 13, 334 (1993).

[28]

G. T. Buracas, A. M. Zador, M. R. DeWeese, and T. D. Albright, Neuron 20, 959 (1998).

[29]

M. R. Cohen and A. Kohn, Nature neuroscience 14, 811 (2011).

[30]

C. van Vreeswijk and H. Sompolinsky, Neural computation 10, 1321 (1998).

[31]

C. van Vreeswijk and H. Sompolinsky, Science 274, 1724 (1996).

[32]

M. Graupner and A. D. Reyes, The Journal of neuroscience : the official journal of the
Society for Neuroscience 33, 15075 (2013).

[33]

M. Okun and I. Lampl, Nature neuroscience 11, 535 (2008).

[34]

Z. F. Mainen and T. J. Sejnowski, Science 268, 1503 (1995).

[35]

S. Lefort, C. Tomm, J. C. Floyd Sarria, and C. C. Petersen, Neuron 61, 301 (2009).

[36]

X. Jiang, S. Shen, C. R. Cadwell, P. Berens, F. Sinz, A. S. Ecker, S. Patel, and A. S. Tolias,
Science 350, aac9462 (2015).

33

