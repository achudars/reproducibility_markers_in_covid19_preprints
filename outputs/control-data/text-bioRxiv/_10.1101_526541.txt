bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

Absolute and relative pitch processing in the
human brain: Neural and behavioral evidence
Simon Leipold a, Christian Brauchli a, Marielle Greber a, Lutz Jäncke a, b, c

Author Affiliations
a
Division Neuropsychology, Department of Psychology, University of Zurich, 8050 Zurich, Switzerland
b
University Research Priority Program (URPP), Dynamics of Healthy Aging, University of Zurich, 8050 Zurich,
Switzerland
c
Department of Special Education, King Abdulaziz University, 21589 Jeddah, Kingdom of Saudi Arabia
Corresponding Authors
Simon Leipold
Binzmühlestrasse 14, Box 25
CH-8050 Zürich
Switzerland
simon.leipold@uzh.ch
Lutz Jäncke
Binzmühlestrasse 14, Box 25
CH-8050 Zürich
Switzerland
lutz.jaencke@uzh.ch
Keywords
Absolute Pitch, Multivariate Pattern Analysis, Neural Efficiency, Pitch Processing, fMRI
Acknowledgements
This work was supported by the Swiss National Science Foundation (SNSF), grant no. 320030_163149 to LJ. We
thank our research interns Anna Speckert, Chantal Oderbolz, Désirée Yamada, Fabian Demuth, Florence Bernays,
Joëlle Albrecht, Kathrin Baur, Laura Keller, Marilena Wilding, Melek Haçan, Nicole Hedinger, Pascal Misala,
Petra Meier, Sarah Appenzeller, Tenzin Dotschung, Valerie Hungerbühler, Vanessa Vallesi, and Vivienne Kunz
for their invaluable help in data acquisition and research administration. Without their help, this research would
not have been possible. Furthermore, we thank Anja Burkhard for her support within the larger absolute pitch
project, Roger Luechinger and Jürgen Hänggi for their assistance in specifying the MRI sequences, Silvano Sele
for helpful discussions regarding the searchlight analysis, and Carina Klein, Stefan Elmer, and all other members
of the Auditory Research Group Zurich (ARGZ) for their valuable comments on the experimental procedure.

1

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

1

Abstract

2

Pitch is a primary perceptual dimension of sounds and is crucial in music and speech perception. When listening

3

to melodies, most humans encode the relations between pitches into memory using an ability called relative pitch

4

(RP). A small subpopulation, almost exclusively musicians, preferentially encode pitches using absolute pitch

5

(AP): the ability to identify the pitch of a sound without an external reference. In this study, we recruited a large

6

sample of musicians with AP (AP musicians) and without AP (RP musicians). The participants performed a pitch-

7

processing task with a Listening and a Labeling condition during functional magnetic resonance imaging. General

8

linear model analysis revealed that while labeling tones, AP musicians showed lower blood oxygenation level

9

dependent (BOLD) signal in the inferior frontal gyrus and the presupplementary motor area — brain regions

10

associated with working memory, language functions, and auditory imagery. At the same time, AP musicians

11

labeled tones more accurately suggesting that AP might be an example of neural efficiency. In addition, using

12

multivariate pattern analysis, we found that BOLD signal patterns in the inferior frontal gyrus and the

13

presupplementary motor area differentiated between the groups. These clusters were similar, but not identical

14

compared to the general linear model-based clusters. Therefore, information about AP and RP might be present

15

on different spatial scales. While listening to tones, AP musicians showed increased BOLD signal in the right

16

planum temporale which may reflect the matching of pitch information with internal templates and corroborates

17

the importance of the planum temporale in AP processing. Taken together, AP and RP musicians show diverging

18

frontal activations during Labeling and, more subtly, differences in right auditory activation during Listening. The

19

results of this study do not support the previously reported importance of the dorsolateral prefrontal cortex in

20

associating a pitch with its label.

2

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

1

Introduction

2

Pitch is a primary perceptual dimension of sounds and plays a crucial role in music and speech perception (Plack

3

et al. 2005). In humans, there exist differential mechanisms to encode pitches into memory. Most individuals

4

encode pitches in relation to other pitches using an ability called relative pitch (RP). With the exception of

5

individuals suffering from amusia (tone deafness), all humans are able to identify changes in pitch contour by

6

making higher-lower judgements — even from a very young age (Plantinga and Trainor 2005). Trained musicians

7

can also identify the exact musical interval (e.g., a perfect fifth) between pitches (McDermott and Oxenham 2008).

8

A small subpopulation, almost exclusively comprised of musicians, preferentially encodes pitches in absolute

9

terms (Miyazaki and Rakowski 2002). These musicians possess absolute pitch (AP), the ability to identify the

10

pitch of a sound without an external reference (Zatorre 2003; Levitin and Rogers 2005; Deutsch 2013). In the

11

following, musicians with AP are referred to as AP musicians and musicians without AP as RP musicians.

12

A cognitive theory of AP, the two-component model, postulates that AP consists of two separate processes: The

13

first component (pitch memory) comprises long-term representations of pitches which presumably exist in all

14

humans to some extent. The second component (pitch labeling) comprises the associations between the long-term

15

pitch representations and meaningful labels (e.g., C#). These associations exist exclusively in AP musicians

16

(Levitin 1994).

17

Although there has been a recent increase in neuroscientific AP research, the neural mechanisms underlying AP

18

have been only partly identified. More than 20 years ago, it was first reported that AP musicians have a more

19

pronounced left-right asymmetry of the planum temporale, a brain region located immediately posterior to

20

Heschl’s gyrus on the superior temporal plane (Schlaug et al. 1995). Follow-up studies found that this asymmetry

21

might be driven by a smaller size of the right planum temporale in AP musicians rather than by a larger left planum

22

temporale (Keenan et al. 2001; Wilson et al. 2009; Wengenroth et al. 2014). With regard to the neurophysiology

23

of AP, a seminal study used positron emission tomography (PET) to investigate pitch processing in AP and RP

24

musicians (Zatorre et al. 1998). While listening to tones, AP musicians showed a unique increase in cerebral blood

25

flow (CBF) in the left posterior dorsolateral prefrontal cortex (DLPFC). Because this region has been implicated

26

in associative learning (Petrides et al. 1993), it was proposed that the CBF increase reflects the retrieval of the

27

association between the pitch and its label from long-term memory. While labeling musical intervals, CBF

28

increases in the posterior DLPFC were observed in both AP and RP musicians, but only RP musicians showed

29

increases in the right inferior frontal gyrus (IFG). These increases were interpreted as reflecting working memory

30

demands related to the RP ability (Zatorre et al. 1998).
3

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

31

In the general population, the prevalence of AP is roughly estimated to be less than one in 10,000 (Bachem 1955).

32

Therefore, it is unsurprising that previous neuroscientific studies examining AP used small sample sizes. However,

33

small samples result in low statistical power, which increases both the occurrence of false-negative and false-

34

positive results (Button et al. 2013). As a consequence, previous neuroscientific AP studies reported inconsistent

35

or even conflicting results. In this study, we aimed to counteract the statistical problems associated with small

36

sample sizes by collecting and analyzing data from a large sample of musicians (n = 101). Using fMRI, we revisited

37

the topic of pitch processing in AP and RP musicians. Similar to the aforementioned PET study, we employed a

38

pitch-processing task comprising two experimental conditions (Listening vs. Labeling). Both AP and RP

39

processing represented adequate strategies to solve the task due to its low difficulty (Itoh et al. 2005). Because

40

individuals possessing AP preferentially encode pitches absolutely and non-possessors preferentially encode

41

pitches relatively (Miyazaki and Rakowski 2002), the task allowed us to contrast AP and RP processing by

42

comparing AP musicians with RP musicians.

43

According to the two-component model, AP musicians differ from RP musicians by having an association between

44

the long-term representation of a pitch and its label (Levitin 1994). The retrieval of this pitch-label association

45

might already occur during Listening and, to successfully perform the task, it must occur during Labeling (Zatorre

46

et al. 1998). At the same time, AP musicians need not rely on working memory processes during Labeling (Itoh et

47

al. 2005). For these reasons, we predicted smaller differences in AP musicians between Listening and Labeling

48

both in BOLD signal responses and behavior. Because of their suggested role in AP processing, we expected an

49

involvement of the posterior DLPFC and/or the planum temporale in AP musicians during Listening. Furthermore,

50

we expected an involvement of the IFG in RP musicians during Labeling because of its association with working

51

memory. Apart from conventional general linear model (GLM) analysis, we applied multivariate pattern analysis

52

(MVPA) to the unsmoothed fMRI data to localize brain regions differentiating between AP and RP musicians. As

53

a complement to GLM analysis, MVPA is sensitive to group-specific information being present in fine-grained

54

voxel patterns which is not detectable using conventional analyses (Kriegeskorte and Bandettini 2007).

55

Additionally, and independently from the other analyses, we investigated ROIs previously associated with AP for

56

group differences which are homogeneous across a brain region but too subtle to be detected by voxel-wise

57

analysis. ROI analysis provides more statistical power than the voxel-wise analyses due to the lower number of

58

tests and thus, a less conservative correction for multiple comparisons (Poldrack 2007).

59

4

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

60

Materials and Methods

61

Participants. Fifty-two AP musicians and 50 RP musicians completed the pitch-processing task. Due to a technical

62

error during the fMRI data export, one participant of the AP group was excluded, leaving the data of 101

63

participants for data analysis. The two groups were matched for sex, handedness, age, musical experience, and

64

intelligence (see Table 1).

65

Group assignment of the participants was based on self-report and confirmed by a tone-naming test (see below).

66

Using both the information from self-report and a tone-naming test is advantageous because the assignment does

67

not rely on an arbitrary cut-off concerning the tone-naming scores. In the rare case that a (potential) participant

68

had indicated to be an AP musician in the initial online application form but then showed tone-naming scores

69

around chance level (8.3%), we did not invite this participant for the imaging experiments in the laboratory. On

70

the other hand, we did invite participants who had indicated to be RP musicians and then showed a high level of

71

proficiency in tone-naming that was above chance level (and reiterated in the laboratory that they do not possess

72

AP); please note that we did not regroup these participants as AP musicians. Furthermore, we statistically assessed

73

if the group of RP musicians as a whole, and each RP musician individually, performed above chance level in the

74

tone-naming test. On the group level, we found strong evidence that RP musicians performed better than chance

75

(one sample t-test against 8.3%; t(49) = 5.74, P < 10-6, Cohen’s d = 0.81). On the individual level, 56 % of the RP

76

musicians performed above chance level according to a binomial test for each individual participant. Figure 1A

77

shows the distribution of tone-naming scores. It is plausible that RP musicians performing above chance level used

78

an internal reference (e.g., tuning standard 440 Hz) in combination with RP processing (or another yet unknown

79

strategy) to solve the tone-naming test. Within RP musicians, tone naming did not correlate with age of onset of

80

musical training (Pearson’s r = 0.06, P = 0.67) or with cumulative musical training (r = 0.17, P = 0.22).

81

5

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

82
83

Table 1. Participant characteristics.
Continuous measures given as mean ± standard deviation.
AP musicians

RP musicians

Number of participants

51

50

Sex (female / male)

23 / 28

24 / 26

Handedness (right / left / both)

45 / 4 / 2

45 / 4 / 1

Age

26.22 ± 4.91 years

25.22 ± 4.43 years

Tone-naming score

76.76 ± 20.00 %

23.93 ± 19.26 %

Musical aptitude (AMMA) - total

65.94 ± 6.20

63.32 ± 6.97

Musical aptitude (AMMA) - tonal

32.27 ± 3.68

30.48 ± 4.24

Musical aptitude (AMMA) - rhythm

33.67 ± 2.79

32.84 ± 3.03

Age of onset of musical training

6.12 ± 2.38

6.52 ± 2.42

Cumulative musical training

16111.52 ± 12590.62 hours

13903.90 ± 10072.36 hours

Crystallized intelligence (MWT-B)

27.62 ± 5.23

29.10 ± 4.72

Fluid intelligence (KAI)

124.03 ± 32.06

134.48 ± 26.91

84

Abbreviations: AMMA = Advanced Measures of Music Audiation, AP = absolute pitch, KAI = Kurztest für

85

allgmeine Basisgrößen der Informationsverarbeitung, MWT-B = Mehrfachwahl-Wortschatz-Intelligenztest, RP =

86

relative pitch.

87

88
89
90
91
92
93
94

Figure 1. Tone-naming proficiency and fMRI task design. (A) Distribution of group-wise tone-naming scores. The
dashed grey line represents chance level (8.3%). (B) A single trial of the fMRI task consisted of stimulus
presentation, response, and scan acquisition. The TR was longer than the TA of a single scan, so stimuli could be
presented in silence. AP = absolute pitch, RP = relative pitch, TA = acquisition time, TR = repetition time.

95

All participants were either music professionals, music students, or highly trained amateurs between 18 and 37

96

years. Participants were recruited in the context of a larger project investigating AP (Greber et al. 2018; Leipold

97

et al. 2019; Brauchli et al. 2019; Burkhard et al. 2019), which involved multiple experiments using different

98

imaging modalities (MRI, EEG). None of the participants reported any neurological, audiological, or severe

99

psychiatric disorders, substance abuse, or other contraindications for MRI. The absence of hearing loss was

100

confirmed by pure tone audiometry (ST20, MAICO Diagnostics GmbH, Berlin, Germany). Demographical data

101

(sex, age, handedness) and part of the behavioral data (tone-naming proficiency, musical aptitude, and musical

6

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

102

experience) was collected with an online survey tool (www.limesurvey.org). Self-reported handedness was

103

confirmed using a German translation of the Annett questionnaire (Annett 1970). Musical aptitude was measured

104

using the Advanced Measures of Music Audiation (AMMA) (Gordon 1989). Crystallized intelligence was

105

estimated in the laboratory using the Mehrfachwahl-Wortschatz-Intelligenztest (MWT-B) (Lehrl 2005) and fluid

106

intelligence was estimated using the Kurztest für allgmeine Basisgrößen der Informationsverarbeitung (KAI)

107

(Lehrl et al. 1991). All participants provided written informed consent and were paid for their participation. The

108

study was approved by the local ethics committee (www.kek.zh.ch) and conducted according to the principles

109

defined in the Declaration of Helsinki.

110

Sample Size Determination. We did not conduct a formal power analysis to determine the sample size for a given

111

effect size and given power in advance of data acquisition. As data from AP musicians is extremely difficult to

112

acquire due to their rarity (see Introduction), it was not possible to realistically plan for a specific number of

113

participants to recruit. We rather recruited as many AP musicians as possible within a period of two years, given

114

the limited financial and human resources available. The number of RP musicians was continuously updated to

115

match the number of AP musicians already recruited at that time. With our final sample of about 50 participants

116

per group, we had > 80% power to detect moderate to large effects (Cohen’s d > 0.6) in a two sample t-test setting.

117

Please note that power analyses in the context of neuroimaging studies are difficult to perform (Mumford 2012),

118

as effect sizes from previous smaller studies are probably inflated (Ioannidis 2008; Poldrack et al. 2017), and

119

power analyses based on pilot studies are biased (Albers and Lakens 2018).

120

Tone-Naming Test. Participants completed a tone-naming test to assess their tone-naming proficiency (Oechslin

121

et al. 2010). The test was carried out online at home and participants were instructed to do the test in a silent

122

environment where they could not be disturbed. During the test, 108 pure tones were presented in a

123

pseudorandomized order. Each tone from C3 to B5 (twelve-tone equal temperament tuning, A4 = 440 Hz) was

124

presented three times. The tones had a duration of 500 ms and were masked with Brownian noise (duration = 2000

125

ms), which was presented immediately before and after the tone. Participants were instructed to identify both the

126

chroma and the octave of the tones (e.g., C4) within 15 s of tone presentation. To calculate a score of tone-naming

127

proficiency, the percentage of correct chroma identifications was used. Octave errors were disregarded (Deutsch

128

2013). Therefore, the chance level identification performance was at 8.3%.

129

Experimental Procedure. During fMRI scanning, participants performed a pitch-processing task (Zatorre et al.

130

1998; Itoh et al. 2005). The auditory stimuli used in the task consisted of three pure tones with different frequencies,

131

and a segment of pink noise. The frequencies of the pure tones were 262 Hz (C4 in twelve-tone equal temperament
7

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

132

tuning), 294 Hz (D4), and 330 Hz (E4). The pure tones and the noise segment had a duration of 350 ms with a 10

133

ms linear fade-in and a 50 ms linear fade-out. Therefore, all stimuli had an identical temporal envelope. The stimuli

134

were created using Audacity (version 2.1.2, www.audacityteam.org). The pure tones and noise segments were

135

presented via MRI-compatible headphones (NordicNeuroLab AS, Bergen, Norway).

136

The fMRI task was constructed as a rapid event-related design: Stimuli were presented in a randomized order and

137

empty trials (without an auditory stimulus) were used to increase the efficiency of the design (Henson 2007).

138

Within a trial, first, a stimulus (pure tone or noise segment) was presented for 350 ms; the participants were given

139

1,500 ms from stimulus onset to respond. Then, 500 ms after stimulus onset, we acquired a functional scan for

140

2,300 ms. Finally, the trial ended with 200 ms silence before the next trial began. Due to the prolonged repetition

141

time (TR) of 3,000 ms between two scans in comparison with the acquisition time (TA) of 2,300 ms, the stimuli

142

were presented in the silent period (700 ms) between the acquisitions of two subsequent scans. Therefore, there

143

was no interference of scanner noise on the perception of the stimuli (Eden et al. 1999; Shah et al. 2000). The

144

inter-trial interval (between two auditory stimuli) was varied using a jitter consisting of multiples of the TR (1–4

145

TRs). A visualization of the fMRI task is given in Figure 1B.

146

There were four runs in total. In each run, 39 pure tones (13 per chroma) and 39 noise segments were presented.

147

The order of the stimuli was kept constant across the runs. Therefore, the auditory stimulation was identical in all

148

runs. During the whole task, a black fixation cross on a gray background was presented on a screen. Stimulus

149

presentation was controlled by Presentation software (version 17.1, www.neurobs.com). All stimuli and the

150

stimulus presentation scripts are available online on the Open Science Framework (https://osf.io/ybghd/).

151

The task consisted of two experimental conditions: a Listening condition and a Labeling condition. These

152

conditions only differed in the instructions given to the participants. In the Listening condition, participants had to

153

press one response pad button (right middle finger) when they heard a pure tone, and another button (right index

154

finger) when they had heard a noise segment. In the Labeling condition, participants had to label the pure tones by

155

pressing one of three corresponding buttons on the response pad (right middle, ring, and little finger in response

156

to C4, D4, and E4, respectively) and another button (right index finger) when they had heard a noise segment. The

157

participants were instructed not to verbally respond and to respond as quickly and as accurately as possible. The

158

accuracy of the responses and the response time were recorded via the response pad (4 button curved right, Current

159

Designs INC, Philadelphia, PA, USA). Both conditions lasted for two runs each. The Listening condition always

160

preceded the Labeling condition to avoid spillover effects from the Labeling onto the Listening condition. If the

8

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

161

order had been the other way around, AP musicians might have been tempted to still covertly label the tones in the

162

Listening condition.

163

Statistical Analysis of Behavioral Data. In-scanner behavioral measures (response accuracy and response time)

164

were analyzed in R (version 3.3.2, www.r-project.org). Separately for each measure, we performed a mixed-design

165

ANOVA with a within-subject factor Condition (Listening vs. Labeling) and a between-subject factor Group (AP

166

vs. RP). Subsequently, the two measures were separately compared within each condition using Welch’s t-tests.

167

Next, we calculated differences in both measures by subtracting the Listening from the Labeling condition for each

168

subject. These differences were then compared between the groups again using Welch’s t-tests. Finally, the

169

differences were correlated with the tone-naming scores using the Pearson correlation coefficient. The significance

170

level was set to P < 0.05. Generalized eta-squared (η2G) was used as an effect size for effects within an ANOVA

171

and Cohen’s d for t-tests.

172

Imaging Data Acquisition and Preprocessing. Imaging data was acquired on a Philips Ingenia 3.0 T MRI system

173

(Philips Medical Systems, Best, The Netherlands), equipped with a commercial 15-channel head coil. Whole-brain

174

functional images were acquired in four runs using a T2*-weighted gradient echo (GRE) echo planar imaging

175

(EPI) sequence (scan duration of one run = 380 s). The T2*-weighted sequence had the following parameters: TR

176

= 3000 ms, TA = 2300 ms, echo time (TE) = 35 ms, flip angle α = 90º, number of axial slices = 38, slice gap = 0.6

177

mm, slice scan order = interleaved, field of view (FOV) = 220 x 220 x 136 mm3, acquisition voxel size = 3.0 x 3.0

178

x 3.0 mm3, reconstructed voxel size = 2.75 x 2.75 x 3.6 mm3, reconstruction matrix = 80 x 80, number of dummy

179

scans = 3, total number of scans = 122.

180

In addition, a whole-brain structural image was acquired using a T1-weighted GRE turbo field echo sequence (scan

181

duration = 350 s). The T1-weighted sequence had the following parameters: TR = 8100 ms, TE = 3.7 ms, flip angle

182

α = 8º, number of sagittal slices = 160, FOV = 240 x 240 x 160 mm3, acquisition voxel size = 1.0 x 1.0 x 1.0 mm3,

183

reconstructed voxel size = 0.94 x 0.94 x 1.0 mm3, reconstruction matrix = 256 x 256. The whole scanning session

184

lasted around 50 minutes and also involved resting-state fMRI and DTI. The results of these imaging modalities

185

are discussed in other publications.

186

The functional images and the structural images were preprocessed using SPM12 (version 6906,

187

www.fil.ion.ucl.ac.uk/spm/software/spm12). The following preprocessing steps were performed in succession

188

using default settings unless otherwise stated: (i) Slice time correction. (ii) Motion correction by a rigid body

189

transformation using six parameters (three translations and three rotations). We did not use unwarping as we had

190

not collected data to correct geometrical distortions caused by susceptibility-induced magnetic field
9

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

191

inhomogeneities. (iii) Coregistration of the structural image to the mean functional image. (iv) Segmentation and

192

bias field correction of the structural image and estimation of the deformation field to map the image to the T1-

193

weighted MNI152 template. (v) Normalization of the functional images using the estimated deformation field. (vi)

194

Interpolation to an isotropic voxel size of 3.0 mm. (vii) Smoothing of the functional images with an 8 mm full

195

width at half maximum (FWHM) three-dimensional Gaussian kernel. The quality of the normalization was visually

196

inspected to confirm proper execution.

197

GLM Analysis. Subject-wise first-level analysis was performed in SPM12. The voxel-wise BOLD signal time

198

series was modeled using a GLM. The first-level design matrix contained, for each run separately, two regressors

199

of interest (onsets of pure tones, onsets of noise segments) and one regressor of no interest (onsets of button

200

presses). These regressors were modeled by convolving delta functions with the canonical double-gamma

201

hemodynamic response function (HRF). Furthermore, we included the six motion parameters estimated during

202

preprocessing as nuisance regressors and applied a high-pass filter (cutoff = 128 s) to remove low-frequency drifts.

203

The following first-level contrasts of interest were calculated: Tones

204

Noise Labeling. Following the logic of cognitive subtraction, these contrasts reflect BOLD signal increases associated

205

with pitch processing.

206

Second-level random effects analysis was performed using non-parametric permutation tests as implemented in

207

SnPM13 (www.warwick.ac.uk/snpm). Permutation tests depend on fewer assumptions than standard parametric

208

approaches and provide an exact control of the family-wise error (FWE) rate (Nichols and Holmes 2002; Eklund

209

et al. 2016). For the second-level analysis, we used a 2 x 2 mixed factorial design to investigate the interaction

210

between Group (AP vs. RP) and Condition (Listening vs. Labeling). To facilitate the interpretation of the

211

interaction, difference images were created for each subject by subtracting the contrast image of the Listening

212

condition (Tones Listening > Noise Listening) from the contrast image of the Labeling condition (Tones

213

Labeling).

214

musicians (cluster-wise inference, 10000 permutations, cluster defining threshold (CDT) P < 0.001). An

215

anatomically defined mask was used to restrict the search space of the analysis to a priori defined brain regions

216

previously associated with AP and RP processing. To create this mask, we used probability maps of the following

217

bilateral brain regions included in the Harvard-Oxford cortical atlas (http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Atlases).

218

(i) Heschl’s gyrus, (ii) planum temporale, (iii) planum polare, (iv) superior temporal gyrus (anterior and posterior

219

division), (v) superior frontal gyrus, (vi) middle frontal gyrus, (vi) inferior frontal gyrus (pars opercularis and pars

220

triangularis), (vii) superior parietal lobule, (ix) gyrus supramarginalis (anterior and posterior division), and (x)
10

Listening

> Noise

Listening

and Tones

Labeling

Labeling

>

> Noise

These difference images were entered in SnPM13 as inputs for a two sample t-test to compare AP and RP

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

221

angular gyrus. The probability maps were then combined, thresholded and binarized at 10% probability using the

222

utility fslmaths (http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Fslutils). Using a mask to restrict the search space alleviates

223

the problem of multiple comparisons as less voxels are tested for an effect (Poldrack 2007). This particular mask

224

furthermore reflects prior knowledge that has been accumulated about AP and RP in many studies over the years.

225

Structural and functional alterations of auditory regions in the superior temporal cortex have been repeatedly linked

226

to AP processing (Schlaug et al. 1995; Keenan et al. 2001; Wilson et al. 2009; Jäncke et al. 2012; Schulze et al.

227

2013; Wengenroth et al. 2014; Kim and Knösche 2016, 2017; McKetton et al. 2019; Brauchli et al. 2019). Dorsal

228

and ventral frontal areas have been associated with both AP and RP (Zatorre et al. 1998; Ohnishi et al. 2001;

229

Bermudez et al. 2009; Wengenroth et al. 2014; Dohn et al. 2015; Brauchli et al. 2019), and there is evidence that

230

parietal areas contribute to AP (Loui et al. 2012; Brauchli et al. 2019) and RP (Schulze et al. 2009; Foster and

231

Zatorre 2010b, a).

232

Two follow-up analyses with the same mask were performed. To determine the effects of condition within each

233

group, we entered the difference images as inputs for a one sample t-test for each group separately (cluster-wise

234

inference, 10000 permutations, CDT P < 0.001). To determine the effects of group within each condition, we

235

entered the first-level contrast images (Tones

236

one sample t-test for each condition separately (cluster-wise inference, 10000 permutations, CDT P < 0.001). The

237

significance level for all analyses was set to  = 0.05, FWE-corrected for multiple comparisons.

238

Additionally, we performed a GLM-based whole-brain analysis to explore effects located outside of brain regions

239

previously associated with AP or RP. This exploratory analysis extended the search space to all brain regions of

240

the Harvard-Oxford cortical and subcortical atlases (excluding the cerebral white matter, the brain stem, and the

241

lateral ventricles). In this whole-brain analysis, we employed the same second-level analysis steps as described

242

above for the restricted analysis.

243

MVPA. We carried out a specific type of MVPA, namely searchlight analysis as implemented in PyMVPA

244

(version 2.6.1, www.pymvpa.org) to detect brain regions containing fine-grained BOLD signal patterns which

245

differentiated between AP and RP musicians (Kriegeskorte et al. 2006; Etzel et al. 2013). Due to the high

246

computational demands, all analyses were carried out on the ScienceCloud of the University of Zurich

247

(www.s3it.uzh.ch). Searchlight analysis, sometimes called information-based brain mapping, builds a map of

248

voxels which are informative regarding group status (searchlight analysis can also be used to analyze information

249

about different stimuli or experimental conditions). A machine learning classifier uses local BOLD signal patterns

250

to classify the participants as belonging to one of the two groups. Brain regions which contain clusters of
11

Listening

> Noise

Listening,

Tones

Labeling

> Noise

Labeling)

as inputs for a

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

251

informative voxels are differentially activated in the two groups (Kriegeskorte et al. 2006; Kriegeskorte and

252

Bandettini 2007).

253

Searchlight analysis was performed on the unsmoothed functional images. To some extent, smoothing removes

254

the fine-grained patterns of activation which were here analyzed for information about group status (Kriegeskorte

255

and Bandettini 2007). Analogous to the GLM analysis, two first-level contrasts were computed in SPM12 (this

256

time using the unsmoothed images): Tones

257

again calculated a difference image for each subject by subtracting the contrast image of the Listening condition

258

from the contrast image of the Labeling condition.

259

In total, we performed three searchlight analyses using the different images (difference images, Listening contrast

260

images, Labeling contrast images) as inputs. In all analyses, a sphere was moved across all voxels of the

261

anatomically defined mask that was also used in the GLM analysis. Each sphere had a radius of three voxels (9

262

mm) and consisted of one center voxel and (at most) 122 surrounding voxels. In every sphere, a linear support

263

vector machine (C = 1) was trained and tested using a 5-fold cross-validation. For the cross-validation, the input

264

images were pseudorandomly partitioned into five chunks under the restriction that each chunk contained the same

265

number of images of AP musicians and RP musicians. One chunk contained 11 images of AP musicians (instead

266

of 10), because our analyzed sample included 51 AP and 50 RP musicians. The average classification accuracy of

267

the five folds was written in the location of the center voxel to create a map of classification accuracies (i.e. an

268

information map).

269

To assess the statistical significance of informative clusters, we used non-parametric permutation testing (Nichols

270

and Holmes 2002). For this purpose, each of the three searchlight analyses was repeated with permuted group

271

labels (10000 permutations). For every iteration, the group labels were randomly permuted within each chunk. We

272

used this restriction to balance the number of images per group in each chunk. The resulting permutation set was

273

fixed for the whole searchlight analysis (i.e. across all center voxels of the mask) to preserve the spatial dependency

274

between neighboring center voxels (Stelzer et al. 2013). All properties of the searchlight analyses with the

275

permutated labels were identical to the analyses with the real labels (e.g., classifier parameters, cross-validation

276

scheme). The permutation procedure resulted in a null distribution of 10000 information maps.

277

Next, both the empirical information map (created with the real labels) and the null information maps (created

278

with the permuted labels) were thresholded with a CDT of P < 0.001 using custom MATLAB R2016a functions.

279

Subsequently, we formed clusters of the above-threshold voxels using CoSMoMVPA (version 1.1.0,

280

www.cosmomvpa.org). The maximum cluster size of each null information map was extracted to form a null
12

Listening

> Noise Listening and Tones Labeling > Noise Labeling. In addition, we

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

281

distribution of cluster sizes. Finally, the P value of the clusters in the empirical information map was calculated as

282

the proportion of cluster sizes under the null distribution that were larger than the empirical cluster size. The

283

significance level was set to  = 0.05, FWE-corrected.

284

ROI Analysis. In addition to the voxel-wise GLM and searchlight analyses, the mean BOLD signal changes in a

285

priori defined ROIs were compared between groups using MarsBaR (version 0.44, www.marsbar.sourceforge.net).

286

We defined four ROIs which have been previously associated with AP processing: left planum temporale (Schlaug

287

et al. 1995; Wilson et al. 2009), right planum temporale (Keenan et al. 2001; Wilson et al. 2009; Wengenroth et

288

al. 2014), left DLPFC (Zatorre et al. 1998; Ohnishi et al. 2001; Bermudez and Zatorre 2005), and right DLPFC

289

(Bermudez and Zatorre 2005).

290

The ROIs were created as spheres (radius = 10 mm) based on MNI coordinates. We used anatomically defined

291

coordinates for the planum temporale and functionally defined coordinates for the DLPFC, because the planum

292

temporale can be delineated by anatomical landmarks, whereas the DLPFC is primarily a functional region. The

293

coordinates of the left (x = -44, y = -34, z = 11) and right planum temporale (x = 41, y = -31, z = 15) were derived

294

from the Harvard-Oxford cortical atlas planum temporale probability map by choosing the voxel with the highest

295

probability in the left and the right hemisphere. The coordinates of the left DLPFC (x = -40, y = 9, z = 42) were

296

taken from a seminal study investigating pitch processing in AP, which was the first to associate this brain region

297

with the retrieval of the pitch-label association while AP musicians were listening to tones (Zatorre et al. 1998).

298

The original study reported the coordinates in Talairach space, so we transformed the coordinates into MNI space

299

(Lacadie et al. 2008). The coordinates of the left hemispheric region were flipped at the midsagittal plane to derive

300

the coordinates of the right DLPFC (x = 40, y = 9, z = 42). For each subject and ROI, we extracted first-level

301

contrast values from the Listening condition (Tones Listening > Noise Listening). For each ROI, these contrast

302

values were compared between AP and RP musicians using Welch’s t-tests in R. The significance level was set to

303

 = 0.0125, FWE-corrected for multiple ROIs.

304

At the request of a reviewer, we conducted a supplemental ROI analysis for the bilateral Heschl’s gyrus. Recent

305

studies have implicated both structure and function of Heschl’s gyrus in AP processing (Wengenroth et al. 2014;

306

McKetton et al. 2019; Brauchli et al. 2019). Coordinates for left (x = -44, y = -24, z = 11) and right Heschl’s gyrus

307

(x = 42, y = -20, z = 9) were anatomically defined, analogous to the planum temporale, by choosing the voxel with

308

the highest probability per hemisphere of the Harvard-Oxford cortical atlas Heschl’s gyrus probability map. In line

309

with the exploratory character of this analysis, here, we used a significance level of  = 0.05, uncorrected.

13

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

310

Results

311

Behavior. Demographical and behavioral characteristics of the AP musicians (n = 51) and the RP musicians (n =

312

50) were compared using Welch’s t-tests. The two groups did not differ in age (t(98.3) = 1.07, P = 0.29), age of onset

313

of musical training (t(98.9) = -0.84, P = 0.40), cumulative musical training (t(95.19) = 0.97, P = 0.33), crystallized

314

intelligence (t(96.4) = -1.48, P = 0.14), and fluid intelligence (t(96.7) = -1.78, P = 0.08). As predicted, AP musicians

315

had a substantially higher tone-naming score than RP musicians (t(99) = 13.53, P < 10-15). There was a trend towards

316

a higher musical aptitude in AP musicians as quantified by the AMMA total score (t(97.2) = 1.99, P = 0.05). Follow-

317

up analyses of the AMMA subscores showed that this difference was driven by a slightly higher tonal score in AP

318

musicians (t(96.5) = 2.27, P = 0.03), but there was no difference regarding the rhythm score (t(98.0) = 1.42, P = 0.16).

319

Descriptive statistics of participant characteristics are given in Table 1.

320

The in-scanner behavioral measures were analyzed using a mixed-design ANOVA with a within-subject factor

321

Condition (Listening vs. Labeling) and a between-subject factor Group (AP vs. RP). As shown in Figure 2A, the

322

mixed-design ANOVA of the response accuracy revealed an interaction between the factors Group and Condition

323

(F(1,99) = 8.37, P = 0.005, η2G = 0.02). The difference in response accuracy between the two conditions (Labeling

324

minus Listening) was smaller in AP than in RP musicians (Welch’s t-test, t(79.1) = 2.88, P = 0.005, d = 0.57).

325

Furthermore, this difference correlated with the tone-naming score (r = 0.41, P < 0.001). On average, the response

326

accuracy was higher in the Listening condition than in the Labeling condition, so this correlation indicates a smaller

327

difference for participants with a higher tone-naming score (see Figure 2C). Additional follow-up analyses showed

328

a higher response accuracy for AP musicians in the Labeling condition (Welch’s t-test, t(73.4) = 2.88, P = 0.005, d

329

= 0.57), but not in the Listening condition (Welch’s t-test, t(87.7) = 1.10, P = 0.28, d = 0.22). As shown in Figure

330

2B, the mixed-design ANOVA of the response time revealed a Group x Condition interaction (F(1,99) = 8.85, P =

331

0.004, η2G = 0.01). The condition difference in response time was smaller in AP musicians (Welch’s t-test, t(95.6) =

332

-2.97, P = 0.004, d = 0.59). Again, this difference correlated with the tone-naming score (r = -0.31, P = 0.002) (see

333

Figure 2D). Descriptive statistics of the in-scanner behavioral measures are given in Table 2.

14

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

334
335
336
337
338
339
340
341
342
343
344
345
346

Figure 2. In-scanner behavioral measures (response accuracy and response time). (A) Interaction between Group
(AP vs. RP) and Condition (Listening vs. Labeling) as revealed by a mixed-design ANOVA of the response
accuracy (F(1,99) = 8.37, P = 0.005, η2G = 0.02). The interaction is characterized by smaller differences between
Listening and Labeling in AP musicians than RP musicians (t(79.1) = 2.88, P = 0.005, d = 0.57). Additionally, AP
musicians demonstrated higher response accuracy in the Labeling condition (t(73.4) = 2.88, P = 0.005, d = 0.57).
(B) Group x Condition interaction as revealed by a mixed-design ANOVA of response time (F(1,99) = 8.85, P =
0.004, η2G = 0.01), again characterized by smaller condition differences in AP musicians (t(95.6) = -2.97, P = 0.004,
d = 0.59). (C) Correlation between the condition difference in response accuracy (Labeling minus Listening) and
tone-naming score (r = 0.41, P < 0.001). Note that the positive correlation indicates a smaller difference for
participants with a higher tone-naming score. (D) Correlation between the condition difference in response time
and tone-naming score (r = -0.31, P = 0.002). AP = absolute pitch, RP = relative pitch.

347
348

Table 2. In-scanner behavioral measures.
Measures given as mean ± standard deviation.

349

AP musicians

RP musicians

Response accuracy Listening

96.69 ± 2.68 %

95.97 ± 3.82 %

Response accuracy Labeling

96.88 ± 3.14 %

94.12 ± 6.04 %

Response time Listening

550.23 ± 125.60 ms

516.20 ± 114.61 ms

Response time Labeling

642.33 ± 145.40 ms

649.29 ± 139.13 ms

Abbreviations: AP = absolute pitch, RP = relative pitch

350

15

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

351

BOLD Signal Changes. The BOLD signal changes were analyzed using a voxel-wise GLM in combination with

352

a second-level mixed factorial design., Using the mask restricting the search space to brain regions previously

353

associated with AP or RP, we found a Group x Condition interaction which was characterized by smaller BOLD

354

signal condition differences in AP musicians, paralleling the in-scanner behavioral measures. As shown in Figure

355

3A, this interaction was detected in three frontal clusters (see Table 3 for details). FWE-corrected P values (PFWE)

356

and the number of voxels (k) of clusters are given in brackets. The clusters were localized in the right IFG, pars

357

opercularis (PFWE < 0.001, k = 407) and the left IFG, pars opercularis (PFWE = 0.003, k = 169). A third cluster was

358

localized in the presupplementary motor area (preSMA) of the dorsomedial prefrontal cortex (PFWE = 0.005, k =

359

141). The exploratory whole-brain analysis for the Group x Condition interaction yielded virtually identical

360

clusters with the same maxima. These clusters were slightly more extended than in the restricted analysis. For full

361

transparency, we made the unthresholded t-maps of the whole-brain analyses available on NeuroVault

362

(Gorgolewski et al. 2015), https://neurovault.org/collections/4906/.

363
364

Table 3. Group x Condition interaction of BOLD signal
The coordinates (x, y, z) are in MNI space. The clusters are ordered according to their size.
Contrast
AP < RP

AP < RP
AP < RP

Brain Region
Right IFG,
pars opercularis
Left IFG,
Pars opercularis
preSMA

k

tmax

x

y

z

PFWE

407

5.90

48

8

14

< 0.001

169

5.00

-54

11

5

0.003

141

4.23

-6

17

47

0.005

365

Abbreviations: AP = absolute pitch, IFG = inferior frontal gyrus, k = number of voxels, preSMA =

366

presupplementary motor area, RP = relative pitch

367
368

As shown in Figure 3B and 3C, using the restricted search space, follow-up analyses within each group separately

369

revealed similar BOLD signal differences between the two conditions with the exception of the three clusters

370

described above (bilateral IFG, preSMA). In the bilateral IFG and the preSMA, only RP musicians showed

371

increased BOLD signal in the Labeling condition. In addition, both groups showed increases in the bilateral

372

intraparietal sulcus (IPS) and the bilateral DLPFC (see Table 4). These increases were stronger and more

373

distributed in RP musicians, again indicating larger condition differences. Further follow-up analyses within each

374

condition revealed that there were no group differences in the Listening condition (see Figure 3D). In contrast, AP

375

musicians showed lower BOLD signal in the Labeling condition in the right IFG (PFWE < 0.001, k = 312), the left

376

IFG (PFWE = 0.003, k = 195), and the preSMA (PFWE = 0.005, k = 134). These clusters were equivalent to the
16

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

377

clusters of the Group x Condition interaction (see Figure 3E and Table 5). The whole-brain analysis yielded again

378

virtually identical results with slightly extended clusters. Unthresholded t-maps of the whole-brain follow-up

379

analyses are available on NeuroVault (https://neurovault.org/collections/4906/).

380
381
382
383
384
385
386
387
388
389
390
391
392
393

Figure 3. Results of the GLM analysis. (A) Group x Condition interaction characterized by smaller condition
differences in AP musicians in the right inferior frontal gyrus (IFG), pars opercularis (PFWE < 0.001, k = 407), left
IFG, pars opercularis (PFWE = 0.003, k = 169), and presupplementary motor area (preSMA) of the dorsomedial
prefrontal cortex (PFWE = 0.005, k = 141). Cold colors indicate AP < RP. (B) Follow-up analysis within AP
musicians revealed increases during Labeling in bilateral intraparietal sulcus (IPS) and bilateral dorsolateral
prefrontal cortex (DLPFC). Hot colors indicate Labeling > Listening and cold colors indicate Listening > Labeling.
(C) Follow-up analysis within RP musicians revealed similar increases during Labeling in bilateral IPS and
bilateral DLPFC and unique increases in the bilateral IFG and the preSMA. Hot colors indicate Labeling >
Listening and cold colors indicate Listening > Labeling. (D) Follow-up analysis within the Listening condition
revealed no group differences. (E) Follow-up analysis within the Labeling condition revealed equivalent clusters
to the Group x Condition interaction in the right IFG (PFWE < 0.001, k = 312), the left IFG (PFWE = 0.003, k = 195),
and the preSMA (PFWE = 0.005, k = 134). Cold colors indicate AP < RP. AP = absolute pitch, RP = relative pitch.

394

17

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

395
396

Table 4. Condition differences in BOLD signal within each group.
The coordinates (x, y, z) are in MNI space. The clusters are ordered according to the contrast, the group, and the

397

cluster size.
Group
AP

AP

AP

AP

Contrast
Labeling >
Listening
Labeling >
Listening
Labeling >
Listening
Labeling >
Listening

Brain Region

k

tmax

x

y

z

PFWE

Left IPS

101

5.37

-42

-31

50

0.009

Left DLPFC

69

4.56

-27

-7

47

0.01

Right DLPFC

41

4.59

27

-4

50

0.03

Right IPS

29

4.51

36

-43

44

0.04

1290

9.85

-57

8

23

< 0.001

1053

10.95

54

8

14

< 0.001

Left IPS

622

7.01

-33

-46

38

< 0.001

Right IPS

595

7.14

39

-46

44

< 0.001

Right SPL

58

4.48

30

-34

62

0.02

Left DLPFC

156

5.05

-21

38

41

0.004

Left frontal pole

91

5.93

-9

56

23

0.01

Right SPL

58

4.72

12

-46

71

0.02

Left angular gyrus

45

4.97

-51

-64

32

0.03

Left IFG,
RP

Labeling >

pars opercularis,

Listening

preSMA,
Left DLPFC

RP

RP

RP

AP

RP

RP

RP

RP

Labeling >
Listening
Labeling >
Listening
Labeling >
Listening
Listening >
Labeling
Listening >
Labeling
Listening >
Labeling
Listening >
Labeling
Listening >
Labeling

Right IFG,
pars opercularis,
Right DLPFC

398

Abbreviations: AP = absolute pitch, DLPFC = dorsolateral prefrontal cortex, IPS = intraparietal sulcus, IFG =

399

inferior frontal gyrus, k = number of voxels, preSMA = presupplementary motor area, RP = relative pitch, SPL =

400

superior parietal lobule.

401

18

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

402
403

Table 5. Group differences in BOLD signal in the Labeling condition
The coordinates (x, y, z) are in MNI space. The clusters are ordered according to their size.
Contrast
AP < RP

AP < RP
AP < RP

Brain Region
Right IFG,
pars opercularis
Left IFG,
Pars opercularis
preSMA

k

tmax

x

y

z

PFWE

312

4.63

45

11

23

< 0.001

195

4.17

-42

8

23

0.003

134

4.69

9

23

44

0.005

404

Abbreviations: AP = absolute pitch, IFG = inferior frontal gyrus, k = number of voxels, preSMA =

405

presupplementary motor area, RP = relative pitch

406
407

Group Decoding by Searchlight Analysis. In addition to the voxel-wise GLM, we used searchlight analysis to

408

localize BOLD signal patterns which differentiate between the two groups (Kriegeskorte et al. 2006). For the main

409

analysis, we used the difference in BOLD signal patterns between the two conditions as the input. As shown in

410

Figure 4A, group status could be decoded in the left IFG, pars triangularis (PFWE = 0.01, k = 29). The mean

411

classification accuracy within the cluster was 72.5%. In comparison to the left IFG cluster from the GLM Group

412

x Condition interaction, this cluster was located more anteriorly on the IFG. Follow-up analyses were performed

413

with the patterns of each condition separately. Analogous to the GLM analysis, group status could not be decoded

414

based on patterns in the Listening condition. In contrast, group status could be decoded based on Labeling patterns

415

in the preSMA (PFWE < 0.001, k = 81, mean classification accuracy = 70.6%). This cluster substantially overlapped

416

with the preSMA cluster from the GLM (see Figure 4A). However, a complete overlap should not be expected,

417

because searchlight analysis is known to cause slight distortions in the localization (Etzel et al. 2013).

418

Regional Mean BOLD Signal Changes. Finally, we extracted the mean BOLD signal changes from a priori

419

defined ROIs. The bilateral planum temporale and the bilateral DLPFC were used as ROIs as these regions have

420

previously been associated with AP processing (Schlaug et al. 1995; Zatorre et al. 1998; Keenan et al. 2001;

421

Ohnishi et al. 2001; Bermudez and Zatorre 2005; Wilson et al. 2009; Wengenroth et al. 2014). It has been proposed

422

that AP musicians automatically retrieve the pitch-label association from long-term memory when confronted with

423

tones (Itoh et al. 2005). Therefore, the group comparison of mean BOLD signal changes was only performed in

424

the Listening condition (Zatorre et al. 1998; Ohnishi et al. 2001; Itoh et al. 2005). As described above, we did not

425

find group differences during Listening with the voxel-wise GLM analysis and the searchlight analysis. However,

426

these analyses may miss subtle effects related to the automatic retrieval because of their conservative correction

427

for multiple comparisons (Poldrack 2007). As shown in Figure 4B, AP musicians showed increased mean BOLD
19

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

428

signal in the right planum temporale (Welch’s t-test, t(94.6) = 2.66, P = 0.01, d = 0.53), but not in the left planum

429

temporale, the left DLPFC, and the right DLPFC (all P > 0.10). The exploratory ROI analysis of bilateral Heschl’s

430

gyrus did not reveal group differences in mean BOLD signal in the left Heschl’s gyrus (P = 0.20). Also, the mean

431

BOLD signal in the right Heschl’s gyrus did not significantly differ between the groups (P = 0.09), although there

432

was descriptively a tendency towards higher BOLD signal in AP musicians associated with a small effect size (d

433

= 0.34).

434
435
436
437
438
439
440
441
442
443
444
445

Figure 4. Results of the searchlight analysis and the ROI analysis. (A) Left: Group status could be decoded in the
left inferior frontal gyrus (IFG), pars triangularis (PFWE = 0.01, k = 29) based on the difference in BOLD signal
patterns between Listening and Labeling (shown in red-yellow). The cluster is located more anteriorly on the IFG
compared to the Group x Condition cluster from the GLM analysis (shown in green). Right: Group status decoding
in the presupplementary motor area (preSMA, PFWE < 0.001, k = 81) based on patterns in the Labeling condition
(shown in red-yellow). There is substantial overlap with the preSMA cluster revealed by the GLM group
comparison during Labeling (shown in green). Hot colors represent the classification accuracy. (B) AP musicians
show higher mean BOLD signal chances during Listening in the right planum temporale (t(94.6) = 2.66, P = 0.01, d
= 0.53), but not in the left planum temporale or the bilateral dorsolateral prefrontal cortex (DLPFC). AP = absolute
pitch, RP = relative pitch.

20

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

446

Discussion

447

In this study, we investigated AP and RP processing in the human brain using task-based fMRI in a large sample

448

of musicians. The GLM analysis revealed smaller BOLD signal differences between Listening and Labeling in AP

449

musicians than in RP musicians. The smaller differences between the conditions were driven by lower BOLD

450

signals in AP musicians during Labeling in the left- and right-sided pars opercularis of the IFG and the preSMA.

451

The in-scanner behavioral measures (response accuracy and response time) mirrored the fMRI data by showing

452

smaller differences between Listening and Labeling in AP musicians. Using MVPA, we found that group status

453

could be decoded in the left-sided pars triangularis of the IFG based on the difference in BOLD signal patterns

454

between Listening and Labeling. Furthermore, group decoding was also possible in the preSMA based on BOLD

455

signal patterns obtained in the Labeling condition. Lastly, the ROI analysis revealed a higher mean BOLD signal

456

in AP musicians during Listening in the right planum temporale which was not detected by the GLM analysis and

457

the MVPA.

458

The IFG is an important target region for auditory information which is propagated from the auditory cortex to the

459

IFG along the ventral stream (the “what” pathway) of auditory processing (Rauschecker and Scott 2009). In this

460

context, the IFG has been repeatedly linked with auditory working memory functions (Schulze et al. 2018). More

461

specifically, the IFG has been associated with working memory for pitch, as shown by both PET and fMRI studies

462

(Zatorre et al. 1994; Gaab et al. 2003). In this study, we observed BOLD signal increases in RP musicians

463

bilaterally in the IFG during Labeling. This increase was not observable in AP musicians. As RP musicians need

464

to use their RP ability to successfully complete the task, it is plausible that the signal increase in the IFG reflects

465

pitch working memory processes as an important aspect of RP processing (McDermott and Oxenham 2008). This

466

interpretation is fully in line with the results of the PET study described in the introduction (Zatorre et al. 1998).

467

In this study, RP musicians, but not AP musicians, showed CBF increases in IFG while they were labeling musical

468

intervals. More evidence for the association between RP processing and working memory comes from a number

469

of electrophysiological studies investigating the P300 component of the auditory event-related potential. The P300

470

presumably reflects the updating of auditory information in working memory. Several studies found an absent or

471

reduced P300 component in AP musicians not relying on RP processing. In contrast, RP musicians show a normal

472

P300 amplitude (Klein et al. 1984; Itoh et al. 2005).

473

Apart from being implicated in working memory, the IFG has been strongly associated with language functions.

474

In the left hemisphere, the pars opercularis (Brodmann area 44) and the pars triangularis (Brodmann area 45) of

475

the IFG are known as Broca’s area, a brain region traditionally associated with speech production, but also heavily
21

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

476

involved in speech perception (Friederici 2011). In the right hemisphere, the IFG is linked to the perception of

477

prosody (pitch changes in speech) (Buchanan et al. 2000). Therefore, the BOLD signal increases in RP musicians

478

in bilateral IFG might reflect language-related processes. More concretely, the RP musicians might have engaged

479

in covert articulation of the tone labels as a part of their strategy to label the tones. In contrast, it seems that the

480

AP musicians do not rely on a verbal code to successfully complete the task. This is in accordance with behavioral

481

evidence demonstrating non-verbal coding strategies in AP musicians (Zatorre and Beckett 1989), and fMRI

482

evidence showing atypically similar BOLD signal in AP musicians during the perception of tonal and verbal

483

stimulus material (Schulze et al. 2013).

484

Mirroring the bilateral IFG BOLD signal increases, the preSMA showed signal increases in RP musicians during

485

Labeling. In addition, the BOLD signal patterns during Labeling in the preSMA contained information about group

486

status. Thus, AP and RP processing were accompanied by differential BOLD signal patterns. The preSMA is

487

anatomically connected to the IFG via the frontal aslant tract and has been implicated in speech production and

488

processing (Catani et al. 2013). More importantly, the preSMA plays a key role in the auditory imagery of pitch

489

(Lima et al. 2016). Auditory imagery generally refers to the generation of auditory information in the absence of

490

sound perception. However, auditory imagery can also involve auditory information that is generated in addition

491

to the currently perceived information. Consequently, RP musicians might have imagined the pitches of previously

492

heard tones to determine the pitch of the current tone. This interpretation is in line with the anecdotal observation

493

that RP musicians often covertly sing pitches in order to identify the musical intervals. It is important to note that

494

the working memory and the language explanations of the IFG and preSMA involvement during Labeling are not

495

mutually exclusive. There is evidence that largely overlapping brain regions are involved in auditory working

496

memory for verbal material and non-verbal material, for example, pitches (Koelsch et al. 2009).

497

The results from the GLM analysis and the MVPA did not fully converge with regard to the localization of the

498

group differences. Most notably, using MVPA, we found that group status could be decoded from BOLD signal

499

patterns in the left-sided pars triangularis of the IFG whereas the GLM revealed BOLD signal differences in the

500

pars opercularis. As mentioned above, these two regions constitute Broca’s area. In a previous study using MVPA,

501

it was shown that BOLD signal patterns in Broca’s area contain speech-related information which was not

502

detectable with GLM analysis (Lee et al. 2012). MVPA is more sensitive to information in fine-grained patterns

503

which are preserved in unsmoothed fMRI data (Kriegeskorte and Bandettini 2007). At the same time, there has

504

been a debate about whether or not Broca’s area should be divided into subareas executing different functions

505

(Friederici 2011). Consequently, we propose that the BOLD signal patterns in the pars triangularis represent
22

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

506

information about AP and RP on a smaller spatial scale. In contrast, the differences in the pars opercularis might

507

be more homogeneous and therefore detectable by the GLM analysis. Further studies should elucidate the

508

potentially differential roles of these two brain regions in pitch processing.

509

Although showing lower BOLD signal in the IFG and preSMA during Labeling, the AP musicians identified the

510

tones more accurately than RP musicians. Therefore, AP processing might be more efficient than RP processing

511

with regard to the use of neural resources. Neural efficiency has been discussed in relation to intelligence, where

512

it has been proposed that more intelligent individuals show lower BOLD signal while performing cognitive tasks

513

(Neubauer and Fink 2009). In this study, there were no group differences in psychometrically evaluated

514

intelligence. Neural efficiency is often observed in tasks of low or moderate difficulty and predominantly in brain

515

regions of the frontal cortex (Neubauer and Fink 2009). Both of these prerequisites are present in this study. The

516

efficiency of AP processing might be related to the automatic retrieval of the pitch-label association which

517

presumably occurs immediately after the pitch is encoded (Itoh et al. 2005). This process is often described as

518

effortless (Deutsch 2013). RP requires more processing steps because after the encoding, the pitch needs to be

519

compared to a previous pitch held in working memory and subsequently, the exact interval between those two

520

pitches needs to be determined. One might speculate that the presumed neural efficiency of AP processing could

521

be a reason for its continued existence throughout human evolution despite its negligible role in music and speech

522

perception (McDermott and Oxenham 2008). On the other hand, it could also be argued that AP musicians did not

523

use the IFG and preSMA at all during Labeling, and thus, the notion of more efficient neural processing might be

524

misplaced, as AP musicians might have used different brain regions than RP musicians and not the same regions

525

more efficiently (see Neubauer and Fink 2009). Following this line of reasoning, AP musicians may have relied

526

on different cognitive processes during Labeling than RP musicians. However, there are two lines of evidence that

527

speak against the AP-specific use of fundamentally different neural resources in the Labeling condition: First, from

528

the unthresholded statistical map displaying differences in BOLD signal between Listening and Labeling within

529

AP musicians, one can observe that AP musicians did, to some extent, activate the bilateral IFG and the preSMA

530

more during Labeling than during Listening (see https://neurovault.org/images/117517/). Thus, they actually used

531

the same, or at least similar, brain regions as RP musicians during Labeling. Second, recent behavioral studies

532

have demonstrated that AP processing and higher cognitive functions (e.g., working memory) are more closely

533

related than previously thought (Van Hedger et al. 2015; Van Hedger and Nusbaum 2018). Hence, it is possible

534

that AP processing is not completely independent of higher cognitive functions but relies less on them than RP

535

processing.

23

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

536

During Listening, the AP musicians showed larger BOLD signals than RP musicians in the right planum temporale.

537

We observed this increase exclusively with the ROI analysis, so the effect seems to be spatially restricted and too

538

subtle to be detected by analyses employing a conservative correction for multiple comparisons. As described in

539

the introduction, the planum temporale has been associated with AP processing from the very beginning of

540

neuroscientific AP research (Schlaug et al. 1995). It is part of the non-primary auditory cortex and has an important

541

role in the processing of a diverse range of sounds (Griffiths and Warren 2002). In this study, the increase in signal

542

was restricted to the right hemisphere. This finding is consistent with previous studies reporting anatomical

543

differences in AP musicians in the right planum temporale (Keenan et al. 2001; Wilson et al. 2009; Wengenroth

544

et al. 2014) and with an influential theory on the importance of the right hemispheric auditory cortex in music

545

processing (Zatorre et al. 2002). However, its exact role in AP processing is still unclear. With regard to auditory

546

processing in general, it has been proposed that the planum temporale matches incoming auditory information with

547

information that is stored in templates which are not located in the planum temporale itself (Griffiths and Warren

548

2002). According to the two-component model, AP musicians possess long-term representations of pitches

549

associated with meaningful labels. These representations could well be characterized as internal templates to which

550

incoming information is matched (Levitin 1994; Levitin and Rogers 2005). Therefore, we propose that in AP

551

musicians, incoming auditory information, more precisely the extracted pitch information, is matched with these

552

internal pitch templates by computations performed in the right planum temporale. The templates themselves could

553

be represented in more anterior regions of the right temporal lobe which are implicated in semantic memory

554

(Binder and Desai 2011). In line with this idea, two recent studies investigating AP musicians have found evidence

555

for differential structural and functional connectivity along the right-hemispheric ventral stream of auditory

556

processing, especially in the planum polare which is located immediately anterior to Heschl’s gyrus (Kim and

557

Knösche 2016, 2017). Thus, it will be interesting for future studies trying to consolidate the findings of AP-specific

558

alterations in posterior and anterior secondary auditory cortices.

559

In contrast to the previously described PET study, we did not find group differences in the posterior DLPFC during

560

Listening. In the PET study, the involvement of the DLPFC was attributed to the automatic retrieval of the pitch-

561

label association in AP musicians (Zatorre et al. 1998). The current results do not support this interpretation. In

562

both groups, we observed bilateral DLPFC BOLD signal increases during Labeling. These increases were

563

accompanied by higher BOLD signal in the bilateral IPS, again in both groups. Both the DLPFC and the IPS are

564

important parts of a network strongly linked to top-down attentional control (Corbetta and Shulman 2002).

565

Therefore, it is possible that the DLPFC involvement is related to unspecific attentional processes rather than the

566

specific retrieval of the pitch-label association.
24

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

567

In conclusion, the current results indicate a possible involvement of working memory, language-related processes,

568

and auditory imagery in RP processing, mediated by the bilateral IFG and the preSMA. AP musicians do not show

569

BOLD signal increases in the IFG and the preSMA during Labeling. At the same time, AP musicians label the

570

tones with a higher accuracy. This suggests that AP might be an example of neural efficiency, which is

571

characterized by higher behavioral performance in combination with a lower use of neural resources. Using

572

MVPA, we detected differential BOLD signal patterns in the IFG and the preSMA. Therefore, these regions might

573

contain information differentiating AP from RP on a small spatial scale. Finally, during Listening, the AP

574

musicians show a specific signal increase in the right planum temporale, possibly reflecting the matching of pitch

575

information with internal templates. Taken together, AP and RP musicians show diverging frontal activations

576

during Labeling and, more subtly, differences in right auditory activation during Listening. The results of this

577

study do not support the previously reported importance of the posterior DLPFC in associating a pitch with its

578

label.

579

25

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

580

Compliance with Ethical Standards

581

Conflict of Interest: The authors declare that they have no conflict of interest.

582

Ethical approval: All procedures performed in studies involving human participants were in accordance with the

583

ethical standards of the institutional and/or national research committee and with the 1964 Helsinki declaration

584

and its later amendments or comparable ethical standards.

585

Informed consent: Informed consent was obtained from all individual participants included in the study.

586

Funding: This work was supported by the Swiss National Science Foundation (SNSF), grant no. 320030_163149

587

to LJ.

588

26

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

589

References

590

Albers C, Lakens D (2018) When power analyses based on pilot data are biased: inaccurate effect size estimators

591
592

and follow-up bias. J Exp Soc Psychol 74:187–195. doi: 10.1016/J.JESP.2017.09.004
Annett M (1970) A classification of hand preference by association analysis. Br J Psychol 61:303–321. doi:

593

10.1111/j.2044-8295.1970.tb01248.x

594

Bachem A (1955) Absolute pitch. J Acoust Soc Am 27:1180–1185. doi: 10.1121/1.1908155

595

Bermudez P, Lerch JP, Evans AC, Zatorre RJ (2009) Neuroanatomical correlates of musicianship as revealed by

596

cortical thickness and voxel-based morphometry. Cereb Cortex 19:1583–1596. doi: 10.1093/cercor/bhn196

597

Bermudez P, Zatorre RJ (2005) Conditional associative memory for musical stimuli in nonmusicians: Implications

598
599

for absolute pitch. J Neurosci 25:7718–7723. doi: 10.1523/jneurosci.1560-05.2005
Binder JR, Desai RH (2011) The neurobiology of semantic memory. Trends Cogn Sci 15:527–536. doi:

600
601

10.1016/J.TICS.2011.10.001
Brauchli C, Leipold S, Jäncke L (2019) Univariate and multivariate analyses of functional networks in absolute

602

pitch. Neuroimage 189:241–247. doi: 10.1016/J.NEUROIMAGE.2019.01.021

603

Buchanan TW, Lutz K, Mirzazade S, et al (2000) Recognition of emotional prosody and verbal components of

604

spoken language: An fMRI study. Cogn Brain Res 9:227–238. doi: 10.1016/S0926-6410(99)00060-9

605

Burkhard A, Elmer S, Jäncke L (2019) Early tone categorization in absolute pitch musicians is subserved by the

606
607

right-sided perisylvian brain. Sci Rep 9:1419. doi: 10.1038/s41598-018-38273-0
Button KS, Ioannidis JPA, Mokrysz C, et al (2013) Power failure: why small sample size undermines the reliability

608
609

of neuroscience. Nat Rev Neurosci 14:365–376. doi: 10.1038/nrn3475
Catani M, Mesulam MM, Jakobsen E, et al (2013) A novel frontal pathway underlies verbal fluency in primary

610
611

progressive aphasia. Brain 136:2619–2628. doi: 10.1093/brain/awt163
Corbetta M, Shulman GL (2002) Control of goal-directed and stimulus-driven attention in the brain. Nat Rev

612

Neurosci 3:201–215. doi: 10.1038/nrn755

613

Deutsch D (2013) Absolute pitch. In: The Psychology of Music. Elsevier, pp 141–182

614

Dohn A, Garza-Villarreal EA, Chakravarty MM, et al (2015) Gray- and white-matter anatomy of absolute pitch

615

possessors. Cereb Cortex 25:1379–1388. doi: 10.1093/cercor/bht334

616

Eden GF, Joseph JE, Brown HE, et al (1999) Utilizing hemodynamic delay and dispersion to detect fMRI signal

617

change without auditory interference: The behavior interleaved gradients technique. Magn Reson Med

618

41:13–20. doi: 10.1002/(SICI)1522-2594(199901)41:1<13::AID-MRM4>3.0.CO;2-T

619

Eklund A, Nichols TE, Knutsson H (2016) Cluster failure: Why fMRI inferences for spatial extent have inflated
27

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

620
621

false-positive rates. Proc Natl Acad Sci U S A 113:7900–7905. doi: 10.1073/pnas.1602413113
Etzel JA, Zacks JM, Braver TS (2013) Searchlight analysis: Promise, pitfalls, and potential. Neuroimage 78:261–

622
623

269. doi: 10.1016/j.neuroimage.2013.03.041
Foster NEV, Zatorre RJ (2010a) Cortical structure predicts success in performing musical transformation

624
625

judgments. Neuroimage 53:26–36. doi: 10.1016/J.NEUROIMAGE.2010.06.042
Foster NE V, Zatorre RJ (2010b) A role for the intraparietal sulcus in transforming musical pitch information.

626
627

Cereb Cortex 20:1350–1359. doi: 10.1093/cercor/bhp199
Friederici AD (2011) The brain basis of language processing: From structure to function. Physiol Rev 91:1357–

628
629

1392. doi: 10.1152/physrev.00006.2011
Gaab N, Gaser C, Zaehle T, et al (2003) Functional anatomy of pitch memory—an fMRI study with sparse

630

temporal sampling. Neuroimage 19:1417–1426. doi: 10.1016/S1053-8119(03)00224-6

631

Gordon EE (1989) Advanced Measures of Music Audiation. GIA Publications

632

Gorgolewski KJ, Varoquaux G, Rivera G, et al (2015) NeuroVault.org: a web-based repository for collecting and

633

sharing unthresholded statistical maps

634

10.3389/fninf.2015.00008

of the human

brain. Front Neuroinform 9:8. doi:

635

Greber M, Rogenmoser L, Elmer S, Jäncke L (2018) Electrophysiological Correlates of Absolute Pitch in a Passive

636

Auditory Oddball Paradigm: a Direct Replication Attempt. eneuro 5:ENEURO.0333-18.2018. doi:

637

10.1523/ENEURO.0333-18.2018

638

Griffiths TD, Warren JD (2002) The planum temporale as a computational hub. Trends Neurosci 25:348–353. doi:

639
640

10.1016/S0166-2236(02)02191-4
Henson R (2007) Efficient experimental design for fMRI. In: Statistical Parametric Mapping: The Analysis of

641

Functional Brain Images. Academic Press London, pp 193–210

642

Ioannidis JPA (2008) Why most discovered true associations are inflated. Epidemiology 19:640–648

643

Itoh K, Suwazono S, Arao H, et al (2005) Electrophysiological correlates of absolute pitch and relative pitch.

644
645

Cereb Cortex 15:760–769. doi: 10.1093/cercor/bhh177
Jäncke L, Langer N, Hänggi J (2012) Diminished whole-brain but enhanced peri-sylvian connectivity in absolute

646
647

pitch musicians. J Cogn Neurosci 24:1447–1461. doi: 10.1162/jocn_a_00227
Keenan JP, Thangaraj V, Halpern AR, Schlaug G (2001) Absolute pitch and planum temporale. Neuroimage

648
649

14:1402–1408. doi: 10.1006/nimg.2001.0925
Kim S-G, Knösche TR (2017) Resting state functional connectivity of the ventral auditory pathway in musicians

650

with absolute pitch. Hum Brain Mapp 38:3899–3916. doi: 10.1002/hbm.23637
28

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

651

Kim SG, Knösche TR (2016) Intracortical myelination in musicians with absolute pitch: Quantitative

652
653

morphometry using 7-T MRI. Hum Brain Mapp 37:3486–3501. doi: 10.1002/hbm.23254
Klein M, Coles MGH, Donchin E (1984) People with absolute pitch process tones without producing a P300.

654
655

Science 223:1306–1309. doi: 10.1126/science.223.4642.1306
Koelsch S, Schulze K, Sammler D, et al (2009) Functional architecture of verbal and tonal working memory: An

656
657

FMRI study. Hum Brain Mapp 30:859–873. doi: 10.1002/hbm.20550
Kriegeskorte N, Bandettini PA (2007) Analyzing for information, not activation, to exploit high-resolution fMRI.

658
659

Neuroimage 38:649–662. doi: 10.1016/j.neuroimage.2007.02.022
Kriegeskorte N, Goebel R, Bandettini PA (2006) Information-based functional brain mapping. Proc Natl Acad Sci

660
661

U S A 103:3863–3868. doi: 10.1073/pnas.0600244103
Lacadie CM, Fulbright RK, Rajeevan N, et al (2008) More accurate Talairach coordinates for neuroimaging using

662

non-linear registration. Neuroimage 42:717–725. doi: 10.1016/J.NEUROIMAGE.2008.04.240

663

Lee Y-S, Turkeltaub P, Granger R, Raizada RDS (2012) Categorical speech processing in Broca’s area: An fMRI

664

study using multivariate pattern-based analysis. J Neurosci 32:3942–3948. doi: 10.1523/JNEUROSCI.3814-

665

11.2012

666

Lehrl S (2005) Mehrfachwahl-Wortschatz-Intelligenztest MWT-B, 5th edn. Spitta Verlag

667

Lehrl S, Gallwitz A, Blaha L, Fischer B (1991) Kurztest für allgemeine Basisgrößen der Informationsverarbeitung.

668

Vless Verlag

669

Leipold S, Oderbolz C, Greber M, Jäncke L (2019) A reevaluation of the electrophysiological correlates of absolute

670

pitch and relative pitch: No evidence for an absolute pitch-specific negativity. Int J Psychophysiol 137:21–

671

31. doi: 10.1016/J.IJPSYCHO.2018.12.016

672

Levitin DJ (1994) Absolute memory for musical pitch: Evidence from the production of learned melodies. Percept

673
674

Psychophys 56:414–423. doi: 10.3758/bf03206733
Levitin DJ, Rogers SE (2005) Absolute pitch: Perception, coding, and controversies. Trends Cogn Sci 9:26–33.

675
676

doi: 10.1016/j.tics.2004.11.007
Lima CF, Krishnan S, Scott SK (2016) Roles of supplementary motor areas in auditory processing and auditory

677
678

imagery. Trends Neurosci 39:527–542. doi: 10.1016/J.TINS.2016.06.003
Loui P, Zamm A, Schlaug G (2012) Enhanced functional networks in absolute pitch. Neuroimage 63:632–640.

679
680

doi: http://dx.doi.org/10.1016/j.neuroimage.2012.07.030
McDermott JH, Oxenham AJ (2008) Music perception, pitch, and the auditory system. Curr Opin Neurobiol

681

18:452–463. doi: 10.1016/J.CONB.2008.09.005
29

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

682

McKetton L, DeSimone K, Schneider KA (2019) Larger auditory cortical area and broader frequency tuning

683
684

underlie absolute pitch. J Neurosci 1532–18. doi: 10.1523/JNEUROSCI.1532-18.2019
Miyazaki K, Rakowski A (2002) Recognition of notated melodies by possessors and nonpossessors of absolute

685
686

pitch. Percept Psychophys 64:1337–1345. doi: 10.3758/BF03194776
Mumford JA (2012) A power calculation guide for fMRI studies. Soc Cogn Affect Neurosci 7:738–742. doi:

687
688

10.1093/scan/nss059
Neubauer AC, Fink A (2009) Intelligence and neural efficiency. Neurosci Biobehav Rev 33:1004–1023. doi:

689
690

10.1016/J.NEUBIOREV.2009.04.001
Nichols TE, Holmes AP (2002) Nonparametric permutation tests for functional neuroimaging: A primer with

691
692

examples. Hum Brain Mapp 15:1–25. doi: 10.1002/hbm.1058
Oechslin MS, Meyer M, Jäncke L (2010) Absolute pitch — functional evidence of speech-relevant auditory acuity.

693
694

Cereb Cortex 20:447–455. doi: 10.1093/cercor/bhp113
Ohnishi T, Matsuda H, Asada T, et al (2001) Functional anatomy of musical perception in musicians. Cereb Cortex

695

11:754–760. doi: 10.1093/cercor/11.8.754

696

Petrides M, Alivisatos B, Evans AC, Meyer E (1993) Dissociation of human mid-dorsolateral from posterior

697

dorsolateral frontal cortex in memory processing. Proc Natl Acad Sci U S A 90:873–877. doi:

698

10.1073/pnas.90.3.873

699

Plack CJ, Oxenham AJ, Fay RR, Popper AN (eds) (2005) Pitch: Neural Coding and Perception. Springer, New

700
701

York, NY
Plantinga J, Trainor LJ (2005) Memory for melody: Infants use a relative pitch code. Cognition 98:1–11. doi:

702
703

10.1016/J.COGNITION.2004.09.008
Poldrack RA (2007) Region of interest analysis for fMRI. Soc Cogn Affect Neurosci 2:67–70. doi:

704
705

10.1093/scan/nsm006
Poldrack RA, Baker CI, Durnez J, et al (2017) Scanning the horizon: towards transparent and reproducible

706
707

neuroimaging research. Nat Rev Neurosci 18:115–126. doi: 10.1038/nrn.2016.167
Rauschecker JP, Scott SK (2009) Maps and streams in the auditory cortex: Nonhuman primates illuminate human

708
709

speech processing. Nat Neurosci 12:718–724. doi: 10.1038/nn.2331
Schlaug G, Jäncke L, Huang Y, Steinmetz H (1995) In vivo evidence of structural brain asymmetry in musicians.

710
711

Science 267:699–701. doi: 10.1126/science.7839149
Schulze K, Gaab N, Schlaug G (2009) Perceiving pitch absolutely: comparing absolute and relative pitch

712

possessors in a pitch memory task. BMC Neurosci 10:1–13. doi: 10.1186/1471-2202-10-106
30

bioRxiv preprint doi: https://doi.org/10.1101/526541; this version posted March 18, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY 4.0 International license.

713

Schulze K, Koelsch S, Williamson V (2018) Auditory working memory. In: Bader R (ed) Springer Handbook of

714
715

Systematic Musicology. Springer, Berlin, Heidelberg, pp 461–472
Schulze K, Mueller K, Koelsch S (2013) Auditory stroop and absolute pitch: an fMRI study. Hum Brain Mapp

716
717

34:1579–1590. doi: 10.1002/hbm.22010
Shah NJ, Steinhoff S, Mirzazade S, et al (2000) The effect of sequence repeat time on auditory cortex stimulation

718

during phonetic discrimination. Neuroimage 12:100–108. doi: 10.1006/nimg.2000.0588

719

Stelzer J, Chen Y, Turner R (2013) Statistical inference and multiple testing correction in classification-based

720

multi-voxel pattern analysis (MVPA): Random permutations and cluster size control. Neuroimage 65:69–

721

82. doi: 10.1016/j.neuroimage.2012.09.063

722

Van Hedger SC, Heald SLM, Koch R, Nusbaum HC (2015) Auditory working memory predicts individual

723

differences in absolute pitch learning. Cognition 140:95–110. doi: 10.1016/J.COGNITION.2015.03.012

724

Van Hedger SC, Nusbaum HC (2018) Individual differences in absolute pitch performance: Contributions of

725

working memory, musical expertise, and tonal language background. Acta Psychol (Amst) 191:251–260.

726

doi: 10.1016/J.ACTPSY.2018.10.007

727

Wengenroth M, Blatow M, Heinecke A, et al (2014) Increased volume and function of right auditory cortex as a

728
729

marker for absolute pitch. Cereb Cortex 24:1127–1137. doi: 10.1093/cercor/bhs391
Wilson SJ, Lusher D, Wan CY, et al (2009) The neurocognitive components of pitch processing: Insights from

730
731

absolute pitch. Cereb Cortex 19:724–732. doi: 10.1093/cercor/bhn121
Zatorre RJ (2003) Absolute pitch: A model for understanding the influence of genes and development on neural

732
733

and cognitive function. Nat Neurosci 6:692–695. doi: 10.1038/nn1085
Zatorre RJ, Beckett C (1989) Multiple coding strategies in the retention of musical tones by possessors of absolute

734
735

pitch. Mem Cognit 17:582–589. doi: 10.3758/BF03197081
Zatorre RJ, Belin P, Penhune VB (2002) Structure and function of auditory cortex: Music and speech. Trends

736
737

Cogn Sci 6:37–46. doi: 10.1016/S1364-6613(00)01816-7
Zatorre RJ, Evans AC, Meyer E (1994) Neural mechanisms underlying melodic perception and memory for pitch.

738
739

J Neurosci 14:1908. doi: 10.1523/JNEUROSCI.14-04-01908.1994
Zatorre RJ, Perry DW, Beckett CA, et al (1998) Functional anatomy of musical processing in listeners with

740

absolute pitch and relative pitch. Proc Natl Acad Sci U S A 95:3172–3177. doi: 10.1073/pnas.95.6.3172

741

31

