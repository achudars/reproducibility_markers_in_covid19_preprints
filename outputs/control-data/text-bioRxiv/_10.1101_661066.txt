bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

Decoding Speech and Music Stimuli from the Frequency
Following Response
Steven Losorelli⇤1 , Blair Kaneshiro1 , Gabriella A. Musacchia1,2 ,
Nikolas H. Blevins1 , and Matthew B. Fitzgerald1
1

Department of Otolaryngology Head & Neck Surgery, Stanford University School
of Medicine, Palo Alto, CA, USA
2

Department of Audiology, University of the Pacific, San Francisco, CA, USA
June 5, 2019

⇤

Corresponding author: slosorelli@stanford.edu

1

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

Abstract

The ability to di↵erentiate complex sounds is essential for communication. Here, we propose using a machine-learning approach, called classification, to objectively evaluate auditory
perception. In this study, we recorded frequency following responses (FFRs) from 13 normalhearing adult participants to six short music and speech stimuli sharing similar fundamental
frequencies but varying in overall spectral and temporal characteristics. Each participant
completed a perceptual identification test using the same stimuli. We used linear discriminant
analysis to classify FFRs. Results showed statistically significant FFR classification accuracies using both the full response epoch in the time domain (72.3% accuracy, p < 0.001) as
well as real and imaginary Fourier coefficients up to 1 kHz (74.6%, p < 0.001). We classified
decomposed versions of the responses in order to examine which response features contributed
to successful decoding. Classifier accuracies using Fourier magnitude and phase alone in the
same frequency range were lower but still significant (58.2% and 41.3% respectively, p < 0.001).
Classification of overlapping 20-msec subsets of the FFR in the time domain similarly produced
reduced but significant accuracies (42.3%–62.8%, p < 0.001). Participants’ mean perceptual
responses were most accurate (90.6%, p < 0.001). Confusion matrices from FFR classifications
and perceptual responses were converted to distance matrices and visualized as dendrograms.
FFR classifications and perceptual responses demonstrate similar patterns of confusion across
the stimuli. Our results demonstrate that classification can di↵erentiate auditory stimuli from
FFR responses with high accuracy. Moreover, the reduced accuracies obtained when the FFR
is decomposed in the time and frequency domains suggest that di↵erent response features contribute complementary information, similar to how the human auditory system is thought to
rely on both timing and frequency information to accurately process sound. Taken together,
these results suggest that FFR classification is a promising approach for objective assessment
of auditory perception.

Keywords

Frequency following response

Brain decoding

2

EEG Classification

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

1

1

Introduction

2

Sound identification is a central feature of human communication. To successfully identify and

3

assign appropriate labels to di↵erent sounds, individuals must be able to distinguish between them

4

based on their spectro-temporal acoustic characteristics. Discrimination of di↵erent speakers and

5

instruments is based on the perception of more complex spectro-temporal features called timbral

6

qualities. These include the shape of the spectral envelope over time, the center frequency of sound

7

spectrum, and frequency range emphasis (e.g., high versus low overtones). Finally, pitch perception

8

in both speech and music most often varies proportionally with the fundamental frequency (F0).

9

Given the importance of these acoustic characteristics for sound identification, it is thought that

10

speech understanding therefore requires distinct and precise neural representations for di↵erent

11

acoustic signals. In many instances, the integrity of these neural representations is inferred by tests

12

of speech understanding or sound identification, such that good performance on these measures

13

is generally thought to reflect neural encoding of sufficient integrity. Such tests are widely used

14

in clinical audiologic practice when assessing performance of individuals with normal hearing and

15

hearing loss (Lawson & Peterson, 2011). However, some individuals (e.g., young children) may

16

not be able to reliably respond to signals, or—in the case of significant hearing loss—may have

17

difficulty reporting what is heard with their hearing aid or cochlear implant. For these populations,

18

an objective measure of the neural representation is of considerable interest for both clinicians and

19

researchers.

20

In human listeners the neural representation of speech and non-speech sounds is often investi-

21

gated using auditory evoked potentials (AEPs) (Atcherson & Stoody, 2012; Hall III, 2007c). In

22

clinical audiologic practice, the auditory brainstem response (ABR) (Jewett et al., 1970; Hood et

23

al., 1991; Hall III, 2007e; Brueggeman & Atcherson, 2012; Davis et al., 1985) and the auditory

24

steady-state response (ASSR) (Galambos et al., 1981; Stapells et al., 1984; Hall III, 2007b; Strick-

25

land & Needleman, 2012) are widely used within an audiologic battery to detect the presence and

26

degree of hearing loss. While these measures are excellent at helping to identify the degree and

27

configuration of hearing loss, they provide little information regarding sound identification. For

28

these reasons, discriminability of speech and non-speech sounds has been investigated through two

1

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

29

specific components of cortical AEPs, called the mismatch negativity (MMN) and P300. Both of

30

these potentials are elicited by an ‘oddball’ paradigm in which a sequence of standard sounds is

31

interrupted by an infrequent change along any acoustic dimension (e.g., frequency, intensity, dura-

32

tion, location; for review, see Atcherson & White (2012)). The neural response to the infrequent

33

stimulus di↵ers from that observed to the frequent stimulus, and the size of this change has been

34

shown to reflect the discriminability of these stimuli (Näätänen & Alho, 1995, 1997). MMN pro-

35

vides a neurophysiological measure of acoustic change detection when sounds are not attended to

36

(Näätänen et al., 1993) and the P300 is modulated by attention to such changes (Davis (1964); for

37

review, see Hall III (2007d)). The MMN is limited in use and scope because not all people exhibit

38

this potential despite normal discriminability performance. In addition, both the MMN and P300

39

are modulated by state of arousal and diminished during sleep, making it difficult to test in infant

40

populations.

41

One AEP candidate which may be better suited for determining whether the neural response

42

of a signal is sufficient for identification on an individual level is the frequency following response

43

(FFR, for review see N. A. Kraus et al. (2017)). The FFR is similar to the ABR in that both

44

evoked potentials are recorded in the same manner, and generators that produce them overlap—

45

namely the cochlea, VIIIth cranial nerve and inferior colliculi (Smith et al. (1975); Gardi et al.

46

(1979); Sohmer et al. (1977); for review see Hall III (2007a)). The responses di↵er in that while the

47

transient peaks of the ABR resolve after about 10 msec, the peaks of the FFR closely ‘follow’ the

48

stimulating frequency in a steady-state response for as long as the stimulus continues (Hoormann et

49

al. (1992); for review see Bhagat (2012)). The FFR has been used widely to investigate the neural

50

encoding of complex tones (Greenberg et al., 1987), music (Bidelman & Krishnan, 2009; Musacchia

51

et al., 2007), and speech sounds (Skoe & Kraus, 2010). Seminal studies into the FFR elicited by

52

speech stimuli have shown that the strength of this response is greatest when auditory stimuli are

53

correctly classified into speech categories (G. Galbraith et al., 1995) and is selectively enhanced to

54

forward-running speech (G. C. Galbraith et al., 2004). Because the FFR is intimately related to

55

the acoustics of the incoming signal, a number of potential applications have been suggested for

56

its use. For example, the FFR is thought to be related to auditory experience, such that stronger

57

FFRs are observed with greater degrees of musical skill (Musacchia et al., 2007; N. Kraus & Strait,
2

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

58

2015; Parbery-Clark et al., 2012; Wong et al., 2007) or language experience (Krishnan et al., 2005;

59

S. Wang et al., 2016; Song et al., 2008). Similarly the FFR strength has been correlated with

60

the ability to understand speech in the presence of background noise, and with development in

61

infants (Anderson et al., 2015; Musacchia et al., 2018) and children (Russo-Ponsaran et al., 2004;

62

Banai et al., 2007). Overall, these studies demonstrate the potential of the FFR to examine the

63

integrity of neural coding of speech signals. However, most of these reports are correlational in

64

nature (e.g., comparing FFR strength with some other variable) and do not address the question

65

of sound identification.

66

One approach to predicting sound identification from AEP data is the use of classification.

67

This refers to an analysis technique which attempts to predict a stimulus label or cognitive state

68

from the corresponding brain response (Donchin, 1969; Blankertz et al., 2001). One advantage of

69

classification is that it can be applied to responses to multiple stimuli, which more closely emulates

70

what is required of participants when performing tests of speech identification. The classification

71

approach also enables complex, high-dimensional data to be analyzed in its entirety, or in spatial

72

and/or temporal subsets using a ‘searchlight’ approach (Su et al., 2012; Kaneshiro et al., 2015).

73

To date, classification has been widely applied to cortical magnetoencephalography (MEG) and

74

electroencephalography (EEG) data to study perception of visual object categories (Simanova et

75

al., 2011; Carlson et al., 2013; Kaneshiro et al., 2015), and to a lesser extent music (Schaefer et al.,

76

2011; Sankaran et al., 2018).

77

Classification-based approaches have also been applied to neurophysiological correlates of speech

78

identification using FFR and AEP data. Results show that the spectral amplitude of the FFR in

79

F0 and F1 bands can be used to correctly classify vowels with up to 70–80% accuracy (Sadeghian

80

et al., 2015) and that spectral information related to the F2 band can be used to classify cortical

81

evoked responses to vowels on the basis of single-trial data (Kim et al., 2014). Data acquired with

82

an innovative combination of single-trial classification and machine-learning methods support the

83

notion that the spectral amplitude of the FFR may be used to correctly predict vowel categorization

84

into learned and novel vowel categories (Yi et al., 2017). Moreover, temporal information contained

85

in the phase of theta oscillations (2–9 Hz) could correctly classify eight phonetic categories such that

86

confusion matrices from phase and perceptual responses were not statistically distinguishable from
3

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

87

one another (R. Wang et al., 2012). Finally, a recent report demonstrated that vectors of timing

88

from cortical responses could be combined with vectors of spectral information derived from the

89

FFR to successfully classify brain responses into speech categories in patients with mild cognitive

90

impairment (Bidelman et al., 2017).

91

Taken together, these data support early phonetic classification research (Näätänen et al., 1978),

92

and provide strong evidence that AEPs, and in particular the FFR, have potential to accurately

93

predict speech identification when classification algorithms are applied. However, the components of

94

the neural response underlying the classification results are poorly understood. For example, most

95

investigations focused their classification analyses on spectral and temporal methods independently;

96

only Lee & Bidelman (2017) appears to have combined both into their identification algorithm.

97

Thus, it is unclear precisely which components of the FFR contribute to accurate classification of

98

signals. Moreover, the range of stimuli which have been used to acquire FFR data for classification

99

is rather narrow to date and has consisted largely of vowels. This is relevant because speech

100

consists of both consonants and vowels and varies considerably in its spectral content over time

101

(e.g., transitioning from consonants to vowels (CV) within a given phoneme). In such cases, it is

102

unclear whether the accuracy of a classification approach changes over time, and if so, how the time

103

course of those changes manifest. Finally, the extent to which classification can identify stimuli of

104

completely di↵erent types is uncertain. For example, can responses to speech stimuli and musical

105

notes be separated, and appropriate labels applied to each type? Here we addressed these questions

106

by classifying FFR responses to CV phonemes and musical stimuli, and comparing the output of

107

the classifier to the results of a perceptual-identification task. We then decomposed the FFR in

108

order to identify which features of the neural response drive classification. Finally, we employed a

109

temporal searchlight approach to classify these stimuli at di↵erent time points of the response with

110

the intent of relating accuracy of these classification windows to acoustic features of the speech and

111

music stimuli.

4

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

112

2

Methods

113

2.1

114

This research protocol was approved by Stanford University’s Institutional Review Board. All par-

115

ticipants provided written informed consent before engaging in any research activities. Thirteen

116

adult participants (7 female) participated in the study. Participants ranged in age from 20–35 years

117

(mean 24 years), were fluent in English, and had no cognitive or decisional impairments. Normal

118

hearing was confirmed with audiological hearing thresholds <20 dB HL across octave frequencies

119

ranging from 500–4000 Hz. Each participant completed a general demographic and musical back-

120

ground questionnaire.

Participants and Stimuli

121

The stimulus set comprised three CV phonemes—ba, da, di —and three musical notes labeled pi-

122

ano, bassoon, and tuba based on their timbral qualities. Time- and frequency-domain visualizations

123

of the stimuli are shown in Panel A of Figure 1. These stimuli were chosen due to their availability

124

in the system used for data collection—advancing our long-term goal of clinical feasibility—as well

125

as on the basis of their shared and distinct auditory features. For example, ba and da are identical in

126

sustained vowel content but di↵er in their initial consonant, and can thus be distinguished percep-

127

tually on the basis of a formant transition. On the other hand, da and di share similar (though not

128

acoustically identical) initial consonants and di↵er in the sustained vowel. The musical stimuli vary

129

in their onset characteristics as well as the amplitude and phase structure of their harmonics. The

130

F0 of each stimulus ranged from 97–107 Hz (ba 100.1 Hz, da 100.1 Hz, di 106.7 Hz, piano 98.3 Hz,

131

bassoon 100.0 Hz, tuba 97.3 Hz), and remained consistent across the sustained portion of each wave-

132

form. We made minor modifications to the stimuli using Audacity1 software, first trimming each

133

waveform to 135 msec—a reasonable duration for collection of FFR responses (Russo-Ponsaran et

134

al., 2004; Bidelman & Krishnan, 2009; Co↵ey et al., 2016; Skoe & Kraus, 2010)—and then applying

135

a linear fade out over the last cycle of each waveform to eliminate o↵set transients.
1

http://www.audacityteam.org

5

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

A
Amplitude
Amplitude

1
1
0
-10
0

Magnitude
(A.U.)
( V) ( V) Magnitude
(A.U.)
Magnitude
Magnitude
(A.U.) (A.U.) Voltage
Voltage
Magnitude
Magnitude
(A.U.) (A.U.)
Amplitude

-1

ba

da

di

piano

bassoon

tuba

ba

da
da

di

piano

bassoon

tuba

50

100

150 0

50

100

150

0

50

100

0

50

100
100

150
150 00

50
50

100
100

150
150 00

50
50

100
100

50

100

150 0

50

100

150 0

50

100

150

400
600
200
500
400
0
200 0

500

50
50

100
100

150
150 00

50

100

150 0

50

100
100

150
150

500
500

200
300
200
600
100
200
400
1000
100 0
200
0
0
0
0

500

1000 0

500

1000 0

500

1000 0

500

1000 0

500

1000

1000
1000 00

500
500

1000
1000 00

500
500

1000
1000 00

500
500

1000
1000 00

500
500

1000
1000 0
0

500
500

1000
1000

Frequency
Frequency (Hz)
(Hz)

ba

1
0.5
0

-1
-0.5
0
300

1000 0

Frequency (Hz)

0
0
0
0
0.5

0
-0.5
0

150
150 00

Time
Time (msec)
(msec)

600

B

150 0

Time (msec)

50

da

100

150 0

50

di

100

150

0

50

piano

100

150 0

bassoon

tuba

50

100

150 0

50

100

150 0

50

100

150

50
50

100
100

150
150 00

50

100

150 0

50

100
100

150
150

Time (msec)
50

100
100

150
150 00

50
50

100
100

150
150 00

50
50

100
100

150
150 00

Time
Time (msec)
(msec)

500

1000 0

500

1000 0

500

1000 0

500

1000 0

500

1000 0

500

1000

500
500

1000
1000 00

500
500

1000 0
0
1000

500
500

1000
1000

Frequency (Hz)
500
500

1000
1000 00

500
500

1000
1000 00

500
500

1000
1000 00

Voltage ( V)

Frequency
Frequency (Hz)
(Hz)
0.5

Magnitude (A.U.)

Figure 1: Stimulus and response visualizations. (A) Stimuli are 135 msec in length and include three
0
consonant-vowel
phonemes—ba, da, di —and three musical notes representative of di↵erent instruments—
piano, bassoon,
and
tuba. Stimulus F0s are all near 100 Hz, but stimuli vary in fine temporal structure and
-0.5
0
50
100
150 0
50
100
150 0
50
100
150 0
50
100
150 0
50
100
150 0
50
100
150
harmonic content. (B) Global-average FFR responses
(N=13;
2,500 responses per stimulus per participant).
Time
(msec)
Responses vary
in temporal and spectral composition; the strongest spectral peaks occur at F0 for every
300
stimulus. 200
100
0
0

136

2.2

Data Collection

137

Each participant completed an FFR recording session and perceptual test. As even short-term

138

training has been shown to modulate FFR responses (Wong et al., 2007; Song et al., 2008; Carcagno

139

& Plack, 2011; Skoe et al., 2013; Russo et al., 2005) the FFR session was always conducted first.

140

The combined sessions, including hearing screen and application of sensors, took approximately two

141

hours for each participant.

500

1000 0

500

1000 0

500

1000 0

500

1000 0

500

1000 0

500

1000

Frequency (Hz)

142

The FFR session involved six recording blocks, with a single stimulus presented in each block.

143

Ordering of blocks was randomized for each participant. Stimuli were presented with a 70-msec

144

inter-stimulus interval (ISI, i.e., a silent interval of 70 msec between the conclusion of one stimulus

145

and onset of the next) in alternating polarity. During recording, the participant was seated in a

146

chair in a dimly lit room. A film without sound was presented to reduce participant movement

147

during data acquisition; participants were also allowed to sleep during the recordings. Stimuli were
6

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

148

played binaurally at 75 dB(C) via electrically shielded Etymotic ER-3 Insert Earphones.2

149

FFR responses were collected using the Advanced Research Module of the Intelligent Hearing

150

Systems (IHS) SmartEP platform,3 which is approved for clinical use. Electrodes were placed at the

151

frontal midline (Fz) with nasion reference and ground on bilateral earlobes. Electrode impedance

152

was measured under 5 k⌦ at the start of each recording session. FFR responses were recorded at

153

a sampling rate of 20 kHz, band-pass filtered at acquisition between 30–1,500 Hz and segmented

154

from -20.9–183.85 msec relative to stimulus onset. All FFRs were collected with an ongoing artifact

155

rejection criterion of ±35 µV. In each block, recording continued until 2,500 usable responses to

156

stimulus repetitions—or sweeps—were obtained. A block was restarted if more than 250 rejections

157

(10% of total sweeps) occurred.

158

As the goal of this study is to establish the feasibility of FFR classification under intact percep-

159

tion, we additionally included a perceptual test in order to verify that participants could behaviorally

160

discriminate among the stimuli. The perceptual test began with a short familiarization phase de-

161

signed to help participants associate a label with a given sound, and to familiarize them with the

162

task. Here, the stimulus set was presented five times in order while the corresponding stimulus label

163

(phoneme or instrument name) was shown onscreen. The participant then completed the test phase.

164

In a test trial, a stimulus was played with no corresponding label shown, and the participant sub-

165

sequently indicated the perceived label of that stimulus. Responses were given in a six-alternative

166

forced-choice (6AFC) paradigm without feedback. Each stimulus was presented 20 times for a total

167

of 120 test trials; trial ordering was randomized for each participant. The perceptual-identification

168

task was written in Matlab4 using the Psychophysics Toolbox (Brainard, 1997) and was completed

169

on a laptop; stimuli were played through Sennheiser HD 650 headphones.

170

2.3

171

FFR data were exported from the IHS system on a per-participant, per-stimulus basis in 100-sweep

172

averages—the minimum number of sweeps that our system could average for export. These FFRs

173

were calculated by averaging an equal number of responses to a stimulus presented in alternating

FFR Data Export and Preprocessing

2

https://www.etymotic.com/
https://www.ihsys.com/
4
https://www.mathworks.com
3

7

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

174

polarity, which Aiken & Picton (2008) define as the envelope FFR. Across all participants and

175

stimuli, this produced 1,950 100-sweep averages (325 per stimulus). All subsequent preprocessing

176

and analysis was performed in Matlab. We epoched the response data to a time interval between 5–

177

145 msec after stimulus onset to account for the latency of the FFR (Glaser et al., 1976; Hoormann

178

et al., 1992)—resulting in a vector of 2,801 time samples—and then centered each epoched average

179

via subtraction of the mean. We henceforth refer to these epoched, centered 100-sweep averages as

180

trials.

181

All experimental data are made available under a CC-BY 3.0 License from the Stanford Digital

182

Repository (Losorelli et al., 2019).5 Analysis code and preprocessed data are available on GitHub

183

(also under a CC-BY 3.0 license).6 Organization of data and code files is detailed in Supplementary

184

Figure S1, and mapping of participant identifiers from raw to preprocessed data files is documented

185

in Supplementary Table S1.

186

2.4

187

Classification is a machine-learning task which aims to assign correct categorical labels to observa-

188

tions of data. In the current context, a classifier is trained by building a statistical model from FFRs

189

(observations) and their respective stimulus identifiers (labels). Then, in the test phase, FFR obser-

190

vations are input to the classifier without labels, and the classifier returns the predicted labels. We

191

compare the labels predicted by the classifier with the actual labels of the test observations in order

192

to compute the classification accuracy, which for this study is the percentage of test observations

193

with correctly predicted test labels.

FFR Classification

194

In order to determine how a classifier will perform in a predictive setting, it is good practice to

195

exclude test observations from the training phase of building the model. One way to achieve this

196

is by performing cross validation. In this procedure, the data are first divided into non-overlapping

197

subsets—or folds—of roughly equal size. Classification is subsequently performed in an iterative

198

fashion, withholding each fold once for testing and training on the remaining folds. K folds thus

199

implies K train-test iterations, and this is referred to as K-fold cross validation.
5
6

https://purl.stanford.edu/cp051gh0103
https://github.com/slosorelli/FFR-classification-2019

8

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

200

A compact representation of classifier performance, in addition to classification accuracy, is the

201

confusion matrix. For the present study, element (i, j) of a confusion matrix denotes the number

202

of test observations actually belonging to category i that were predicted to belong to category j

203

by the classifier. Values on the diagonal represent numbers of correct classifications. Our reported

204

confusion matrices contain classifier predictions aggregated across all cross-validation folds.

205

We performed FFR classifications and visualized the results using the publicly available Mat-

206

ClassRSA Matlab toolbox (B. C. Wang et al., 2017). As the classifier chose among 6 possible

207

stimulus labels in this multi-category classification task, and we collected the same amount of

208

response data to each stimulus, chance level was 1/6 (16.67%). All classifications used Linear Dis-

209

criminant Analysis (LDA) (Hastie et al., 2009). To speed processing time of each classification,

210

the dimensionality of the input data matrix was first reduced along the feature dimension (time

211

samples or frequency bins) using Principal Components Analysis (PCA), retaining as many PCs

212

were needed in order to explain 99% of the variance. In each cross-validation fold, PCA and number

213

of PCs to retain was computed on only the training observations and then applied to the test obser-

214

vations. Statistical significance was assessed via permutation test (Golland & Fischl, 2003): Each

215

classification was performed once on the intact data and 1,000 times with stimulus labels shu✏ed

216

independently of the response data, and the corresponding p-value was computed by comparing the

217

observed accuracy against the null distribution of permuted accuracies.

218

2.4.1

219

For classification of FFRs in the time domain, the trial data from all participants were combined,

220

and classification was performed using 10-fold cross validation. The trial-by-time matrix input

221

to the classifier was of size 1,950⇥2,801, and the features describing each observation were time

222

samples of response data. Trial ordering was randomized prior to partitioning for cross-validation

223

in order to better distribute the participants’ data among the folds. In a follow-up classification, we

224

further averaged the trials within-participant into pseudo-trials (Guggenmos et al., 2018) comprising

225

5 trials (500 sweeps) of response to a given stimulus, as used in Sadeghian et al. (2015). This

226

produced an aggregated pseudo-trial-by-time matrix of size 390⇥2,801 across participants—leaving

227

the feature dimension unchanged while decreasing the number of observations for classification

Time-Domain Classification

9

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

228

(but likely improving their SNR). As classification of 500-sweep pseudo-trials was found to produce

229

higher accuracy than classification of 100-sweep trials, we performed all subsequent classifications

230

on 500-sweep representations of the data.

231

We next classified the data using a leave-one-participant-out (LOO) cross-validation scheme.

232

Here, we performed 13-fold cross validation, where in each fold all observations from a single par-

233

ticipant were withheld for testing, while the model was trained on the data from the remaining

234

participants. We performed two such classifications. First, training and testing was performed

235

using the within-participant pseudo-trials computed previously; then, pseudo-trials for training in

236

each fold were computed across-participant on a per-stimulus basis, and the resulting model was

237

tested on the within-participant pseudo-trials of the holdout test participant.

238

2.4.2

239

One way in which the FFR di↵ers from event-related cortical EEG responses is in its direct encoding

240

of the auditory stimulus. Therefore, we speculated that FFR classification might be feasible in the

241

frequency domain. To prepare the data for these analyses, we first computed the FFT of within-

242

participant pseudo-trials. Next, taking into consideration the filter settings at data acquisition and

243

the frequency range in which meaningful information is encoded by the FFR (Skoe & Kraus, 2010),

244

we retained for further analysis only the 141 complex values corresponding to frequencies between

245

0–1,000 Hz.

Frequency-Domain Classification

246

A waveform can be fully characterized in the frequency domain via magnitude and phase values—

247

or equivalently by its real and imaginary coefficients—at each frequency bin. For our first frequency-

248

domain analysis, we classified this complex representation to confirm that accuracy would be compa-

249

rable to time-domain classification accuracy. Here we used 10-fold cross validation, and the feature

250

vector comprised real and imaginary Fourier coefficients (which occupy a shared data scale while

251

phase and magnitude values do not) from 0–1,000 Hz, for a pseudo-trial-by-coefficient input matrix

252

of size 390⇥282.

253

We next performed classifications on magnitude and phase alone. Fourier magnitude denotes the

254

amount of energy at each frequency bin while phase describes precise timing information at each

255

frequency. Therefore, if our previous classifications of complete responses were successful, these
10

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

256

analyses could potentially elucidate the relative contributions of magnitude and phase in successful

257

decoding. Over the same range of frequencies used above, we classified Fourier magnitudes, com-

258

puted as the absolute values of each complex coefficient. We separately classified phase, computed

259

as the angle at each frequency bin. Each of these classifications involved 10-fold cross validation and

260

operated on an input pseudo-trial-by-feature (Fourier magnitude or phase) matrix of size 390⇥141.

261

2.4.3

262

Our analyses so far have considered time- and frequency-domain representations of the FFR over the

263

full response epoch. With Fourier magnitude and phase classifications, we decomposed the FFR in

264

order to determine whether classification accuracy could be explained by specific response features.

265

For our final classifications, we decomposed the response in a di↵erent fashion, now over temporal

266

subsets of the response epoch. This technique is referred to as temporal ‘searchlight’ (Su et al.,

267

2012) and has been found to reveal useful information about the dynamics of visual object category

268

processing in cortical EEG classification (Kaneshiro et al., 2015). For our searchlight analysis, we

269

performed separate classifications on overlapping temporal windows 20 msec in length with a 10-

270

msec (50%) overlap between windows, for a total of 13 windows. In each temporal window, we

271

classified four representations of the response: Time domain (390⇥400 input matrix); frequency

272

domain, real and imaginary coefficients (390⇥40 input matrix); frequency domain, magnitude only

273

(390⇥20 input matrix); and frequency domain, phase only (390⇥20 input matrix). All classifications

274

used 10-fold cross validation and the within-participant 500-sweep pseudo-trials used previously.

275

We performed multiple comparison correction across the resulting 13 p-values using False Discovery

276

Rate (Benjamini & Yekutieli, 2001).

277

2.5

278

While FFR confusion matrices were constructed by means of a computational classification proce-

279

dure, in the perceptual case the classification is e↵ectively performed by each participant in their

280

reporting of perceived stimulus categories. Therefore, the perceptual confusion matrices could be

281

constructed by directly comparing each participant’s reported results with the actual stimulus labels

Temporal Searchlight Classification

Analysis of Perceptual Responses

11

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

282

of the trials. Reported global perceptual accuracy was obtained by aggregating confusion matrices

283

across participants. We assessed statistical significance of perceptual responses by comparing ob-

284

served results with a null distribution of accuracies obtained by randomly permuting actual labels

285

1,000 times for each participant.

286

2.6

287

As the confusion matrix provides a more comprehensive summary of a classifier’s performance than

288

accuracy alone, we visualized the confusion matrix for every classification. In addition, classifier

289

confusions can be treated as measures of similarity among the classes (Shepard, 1974), and conse-

290

quently a confusion matrix can be treated as a proximity matrix, converted to a distance matrix, and

291

visualized hierarchically as a dendrogram. We created dendrograms from confusion matrices using

292

the procedure outlined in (Kaneshiro et al., 2015): The distance matrix was created by first scaling

293

each row of the confusion matrix by its respective diagonal entry (achieving unity self-similarity),

294

symmetrizing the matrix using the geometric mean, and finally computing distances linearly as

295

1 similarity; following this, hierarchical clustering was performed using unweighted pair grouping

296

method with averaging (UPGMA) linkage. In the resulting tree visualizations, the y-axes denotes

297

distance, and the height a given tree must be traversed in order to travel between any two classes

298

is the distance between those classes.

299

3

300

3.1

301

For our first analyses, we classified FFRs in the time domain. To start, we input what was our closest

302

representation of ‘single-trial’ FFRs—the 100-sweep averaged trials output by the IHS system—and

303

performed 10-fold cross validation. Here, the mean accuracy across cross-validation folds was 61.4%

304

(p < 0.001) compared to chance level of 16.67%. The next classification of 500-sweep pseudo-trials

305

was more successful, with a mean accuracy of 72.3% (p < 0.001). The confusion matrix and

306

dendrogram of the pseudo-trial classification are shown in Panel A of Figure 2. Examination of

Visualization of Confusion Matrices

Results
Time-Domain Classification

12

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

307

the confusion matrix indicates that not all stimuli were decoded with equal accuracy. Rather, the

308

classifier tended to confuse responses to ba and da, as shown by large values in the o↵-diagonal.

309

Responses to di and piano also tended to be confused, though to a lesser extent, while responses

310

to bassoon and tuba classified best (classwise accuracies of 87.7% and 90.8%, respectively). The

311

accompanying dendrogram makes the structure of the confusion-based similarities more clear, with

312

ba/da forming the tightest category cluster (smallest distance), followed by di /piano.
A

B

ba

Predicted stimulus
di
piano

ba

da

55.4

36.9

0.0

3.1

bassoon

tuba

3.1

1.5

C
Predicted stimulus
di
piano

ba

da

ba

41.5

35.4

1.5

32.3

50.8

Predicted stimulus
di
piano

bassoon

tuba

ba

da

bassoon

tuba

9.2

4.6

7.7

ba

35.4

44.6

3.1

6.2

3.1

7.7

4.6

1.5

3.1

7.7

da

29.2

52.3

3.1

3.1

7.7

4.6

D

100
90

da

36.9

50.8

3.1

1.5

4.6

3.1

di

0.0

1.5

73.8

24.6

0.0

0.0

di

4.6

1.5

63.1

27.7

1.5

1.5

di

0.0

3.1

63.1

26.2

3.1

4.6

piano

1.5

3.1

15.4

75.4

4.6

0.0

piano

7.7

3.1

21.5

55.4

10.8

1.5

piano

6.2

4.6

29.2

50.8

7.7

1.5

40
30

bassoon

3.1

1.5

0.0

7.7

87.7

0.0

tuba

0.0

6.2

0.0

0.0

3.1

90.8

bassoon

3.1

1.5

0.0

13.8

80.0

1.5

bassoon

1.5

3.1

0.0

12.3

83.1

0.0

20

tuba

0.0

4.6

1.5

4.6

4.6

84.6

tuba

0.0

3.1

1.5

0.0

1.5

93.8

10
0

1

1

1

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.3

0.2

0.2

0.2

0.9
0.8
0.7
0.6
0.5
0.4

0.1

0.1
0

0

ba

da

bassoon

di

piano

tuba

Across-participant averaging

60
50

Distance

1

Identity line
Linear fit

70

% classified

Actual stimulus

80

da

0.8

P2

P4
P6

P12 P7
P3P5
P8

P9

0.6
P1

P11

P13

0.4

0.2

P10

r=0.91, p<0.001

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Within-participant averaging

0.1
ba

da

di

piano

bassoon

tuba

0

ba

da

di

piano

bassoon

tuba

Figure 2: Time-domain classification results. Classification results for 500-sweep pseudo-trials, visualized as confusion matrices and dendrograms. (A) Pseudo-trials were computed within-participant,
and across-participant classification was performed using 10-fold cross validation. Mean accuracy was
72.3% (p <0.001). Greatest confusion occurred between responses to ba and da. Tuba responses classified
with highest accuracy. (B) Single participants were withheld for testing during (13-fold) cross validation. Pseudo-trials of the training data were computed within-participant. Mean accuracy was 62.6%
(p <0.001). (C) Single participants were withheld for testing during (13-fold) cross validation. Pseudotrials of the training data were computed across-participant. Mean accuracy was 63.1% (p <0.001). (D)
Scatter plot of within-participant versus across-participant accuracies for single test participant accuracies
from (B) and (C).

313

In a clinical setting, the predictive power of classification becomes especially relevant for assessing

314

responses from previously unseen patients. To explore the feasibility of this scenario, we next

315

iteratively trained the classifier on data from all but one participant and then tested on the data

316

from that holdout participant. As participant-specific attributes of the test data cannot be taken

317

into account during training, this is a more challenging task. However, it also more closely resembles

318

the application of FFR classification in a real-world setting. We performed these classifications in

319

two ways, first training the model on pseudo-trials averaged within-stimulus and within-participant,

320

and next on pseudo-trials averaged within-stimulus but across-participant. When pseudo-trials
13

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

321

of the training partitions were averaged within-participant (Figure 2B), mean classifier accuracy

322

was 62.6% (p <0.001); results were similar when training pseudo-trials were computed across-

323

participant, with an overall classification accuracy of 63.1% (p <0.001) (Figure 2C). In both cases,

324

the structure of the confusions as shown in the confusion matrices and dendrograms corresponded to

325

that obtained when data from all participants were distributed among the training and testing folds

326

(Figure 2A). Finally, a comparison of holdout participant accuracies between the two pseudo-trial

327

averaging procedures (Figure 2D) indicated that accuracies from across-participant averaging were

328

highly correlated with those from within-participant averaging (rho =0.91, p <0.001). Individual

329

participant accuracies were statistically significant (p <0.05, FDR corrected) for all participants

330

except P10.

331

3.2

332

While cortical EEG classification studies typically operate in the time domain of the response,

333

for the present study we also explored whether FFRs could be classified in the frequency domain.

334

For our first frequency-domain analysis, we input real and imaginary Fourier coefficients between

335

0–1,000 Hz to the classifier. As expected, the resulting classification accuracy was similar to that

336

obtained with the time-domain response (74.6%, p <0.001). As can be seen in the confusion matrix

337

and dendrogram in Panel A of Figure 3, the structure of similarities was also similar to that of the

338

time-domain classification, with strongest similarity between ba/da, followed by di /piano.

Frequency-Domain Classification

339

We next decomposed the frequency-domain representation of the responses into Fourier mag-

340

nitudes and phases for frequencies up to 1,000 Hz, and classified each of these representations

341

separately. Classification of magnitudes produced an overall accuracy of 58.2% (p <0.001), while

342

classification of phase values was less successful, with an overall accuracy of 41.3% (p <0.001). As is

343

shown in the confusion matrix and dendrogram in Panels B and C of Figure 3, confusions between

344

ba and da were exacerbated, with these two categories now displaying a negative distance (result-

345

ing from a greater number of misclassifications than correct classifications between the categories),

346

while di /piano formed the second-closest category cluster and tuba classified most successfully in

347

both cases. We note that the structure of the phase-classification dendrogram matches those of

14

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

A

B

C

Figure 3: Frequency-domain classification results. Classification of 500-sweep pseudo-trials, computed within-participant, was performed using 10-fold cross validation and visualized as confusion matrices
and dendrograms. (A) Classification of real and imaginary FFT coefficients up to 1,000 Hz. Mean accuracy
was 74.6% (p <0.001). (B) Classification of FFT magnitudes up to 1,000 Hz. Mean accuracy was 58.2%
(p <0.001). (C) Classification of FFT phase angles up to 1,000 Hz. Mean accuracy was 41.3% (p <0.001).
For all feature representations, ba and da responses were confused most by the classifier.

348

the time-domain and complex frequency-domain dendrograms, while classification of magnitudes

349

produced a slightly di↵erent similarity structure.

350

3.3

351

Our final classifications were conducted on 20-msec temporal subsets of the response, with 10-msec

352

overlap between time windows. This approach would highlight whether a particular range of time

353

in the response—corresponding to the formant transition or to the sustained portion, for example—

354

was especially essential to the success of classifications conducted across all time. Each temporal

355

classification was performed in the time domain, on real and imaginary Fourier coefficients, and on

356

Fourier magnitude and phase separately. Temporally resolved per-class and overall accuracies for

357

time-domain classifications are shown in Figure 4A. Here, we observed a gradual decline in overall

358

classification accuracy over the time course of the response. We also found that the stimuli varied

359

in their decodability over time. For instance, ba and da were best decoded in the early portion of

360

the response and decline thereafter, while tuba responses classified worst in the early response, but

Temporal Searchlight

15

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

361

from the fourth time window (35–55 msec after stimulus onset) were the best-classifying responses

362

among the stimulus categories.

A

100

90

80

Classifier accuracy (%)

70

60

50

40

30
ba
da
di
piano
bassoon
tuba
Mean

20

10

0

B

5

15

25

35

45

55

65

75

85

95

105

115

125

135

145

Time (msec)
Predicted stim ulus

55-75 msec: 55.13%
no

Ba

sso

Ba
oo
ss
Ba

ss

1

1

1

1

0.5

0.5

0.5

0.5

0.5

0.5

0.5

0

0

0

0

0

0

0

o
o
Di Pian asso
B

n

Ba

Da Tub

a

o
o
Di Pian asso
B

n

Ba

a

Da Tub

on
o
Di Pian asso
B

Ba

a

Da Tub

on
o
Di Pian asso
B

-0.5

-0.5

-0.5

-0.5

-0.5

-0.5

Da

Ba

on

Da asso
B

o
a
Di Pian Tub

1.54 9.23 1.54 7.69 3.08 76.92

Tu

Ba

3.08 6.15 1.54 4.62 4.62 80.00

1

a

6.15 1.54 50.77 20.00 12.31 9.23
7.69 6.15 16.92 53.85 6.15 9.23

1

Ba Tub

on
ba
Tu

10.77 24.62 10.77 7.69 43.08 3.08

1

-0.5

sso

Ba

29.23 20.00 6.15 10.77 23.08 10.77

i
D

12.31 1.54 6.15 10.77 69.23 0.00

o

n
Pia

Di

Pi
an

7.69 3.08 13.85 55.38 9.23 10.77

Da

41.54 20.00 13.85 10.77 12.31 1.54

D
a

44.62 40.00 3.08 3.08 6.15 3.08

Tu

Tu

3.08 4.62 0.00 10.77 3.08 78.46

Ba

30.77 40.00 7.69 9.23 6.15 6.15

10.77 6.15 55.38 16.92 1.54 9.23

i

65-85 msec: 47.69%

on
ba
Tu

oo

Pi
an
ss
Ba

Pia

D

16.92 4.62 1.54 9.23 64.62 3.08

Di

Pi
an

6.15 3.08 21.54 46.15 10.77 12.31

D
a

43.08 33.85 6.15 6.15 4.62 6.15

Da

o

Ba

29.23 38.46 7.69 3.08 15.38 6.15

oo

oo
ss
4.62 7.69 6.15 3.08 4.62 73.85

Ba

n

on
ba
Tu

sso

Ba

o

Pia

n

6.15 1.54 3.08 18.46 66.15 4.62

no

Di

6.15 3.08 56.92 27.69 0.00 6.15

i

1.54 0.00 21.54 56.92 13.85 6.15

D
a

41.54 49.23 1.54 1.54 3.08 3.08

Da

ba

Ba

41.54 38.46 0.00 1.54 3.08 15.38

Tu

9.23 7.69 3.08 0.00 4.62 75.38

Ba

ss

oo

Ba

ba

45-65 msec: 51.54%

on
ba
Tu

sso

Ba

D

Ba

Pia

0.00 4.62 41.54 33.85 3.08 16.92

i

3.08 1.54 1.54 10.77 81.54 1.54

no

Di

Pi
an

1.54 3.08 26.15 49.23 13.85 6.15

D
a

35.38 53.85 0.00 1.54 0.00 9.23

Da

o

Ba

36.92 40.00 1.54 0.00 1.54 20.00

3.08 3.08 50.77 26.15 4.62 12.31

i

35-55 msec: 54.87%

on
ba
Tu

n

sso

ba

Ba

D

Pia

Tu

9.23 9.23 1.54 3.08 3.08 73.85

Ba

ss
Ba

no

Di

Pi
an
1.54 0.00 1.54 9.23 83.08 4.62

D
a

20.00 61.54 3.08 0.00 3.08 12.31

Da

o

Ba

44.62 26.15 4.62 4.62 0.00 20.00

1.54 4.62 23.08 58.46 6.15 6.15

ba

23.08 18.46 4.62 7.69 6.15 40.00

Ba

n

on
ba
Tu

D

sso

o

D
a

Ba

Tu

ba

no

oo

Pi
an
oo
Ba

Pia

n

0.00 9.23 4.62 3.08 67.69 15.38

Di

3.08 3.08 55.38 20.00 1.54 16.92

i

9.23 9.23 21.54 49.23 1.54 9.23

Da

Pi
an

D
o

23.08 41.54 4.62 4.62 3.08 23.08
10.77 3.08 56.92 18.46 3.08 7.69

i

Ba

44.62 21.54 3.08 6.15 0.00 24.62

Tu

Distance

Ba

ba

on
sso uba
T

D

Ba

o

no

Pia

n

Di

n

D
a

Da

ss

Actual stimulus

Ba

Ba

25-45 msec: 57.95%

ba

15-35 msec: 62.82%

5-25 msec: 50.00%

Ba

Da

on a
o
Di Pian asso Tub
B

Ba

Da asso
B

on

o
a
Di Pian Tub

Predicted stim ulus

85-105 msec msec: 42.31%

75-95 msec: 47.18%
Ba

Da

Di

no

Pia

Ba

on
sso uba
T

Ba

Da

Di

Pia

no

Ba

95-115 msec: 45.64%

on
sso uba
T

Ba

Da

no

Di

Pia

Ba

105-125 msec: 45.13%

on
sso uba
T

Ba

Da

no

Di

Pia

115-135 msec: 44.62%

on
ba
Tu

sso

Ba

Ba

Da

no

Di

Pia

125-145 msec: 45.64%

on
ba
Tu

sso

Ba

Ba

Da

Di

Pia

no

Ba

on
sso uba
T

Distance

9.23 16.92 3.08 7.69 63.08 0.00
1.54 15.38 4.62 3.08 0.00 75.38

3.08 7.69 58.46 15.38 9.23 6.15
12.31 15.38 18.46 27.69 20.00 6.15
15.38 18.46 4.62 7.69 50.77 3.08
1.54 12.31 1.54 1.54 1.54 81.54

Ba
D
a

Ba
D
a

1

1

1

1

1

1

0.5

0.5

0.5

0.5

0.5

0.5

0

0

0

0

0

0

a

Ba Tub

Da

o
o
Di Pian asso
B

n

Ba

Da Tub

a

on
o
Di Pian asso
B

Ba

a

Da Tub

on
o
Di Pian asso
B

Ba

a

Da Tub

on
o
Di Pian asso
B

43.08 18.46 15.38 9.23 6.15 7.69
3.08 15.38 46.15 16.92 7.69 10.77
16.92 7.69 16.92 41.54 15.38 1.54
10.77 13.85 4.62 7.69 61.54 1.54

80

60

40

20

3.08 7.69 4.62 4.62 0.00 80.00
0

-0.5

-0.5

-0.5

-0.5

-0.5

-0.5

26.15 32.31 9.23 15.38 12.31 4.62

% classified

7.69 23.08 26.15 18.46 20.00 4.62

35.38 16.92 13.85 15.38 10.77 7.69

D
i

4.62 0.00 60.00 26.15 6.15 3.08

D
i

D
ss
Pi
an
oo
o
n

29.23 24.62 6.15 15.38 16.92 7.69

32.31 23.08 4.62 18.46 15.38 6.15

Ba
ss
Pi
Tu
an
oo
ba
o
n

4.62 6.15 3.08 6.15 1.54 78.46

Ba

15.38 12.31 1.54 15.38 55.38 0.00

a

13.85 20.00 24.62 32.31 9.23 0.00

i

4.62 7.69 56.92 20.00 3.08 7.69

Ba
ss
Pi
Tu
an
oo
ba
o
n

Ba
D
a

29.23 20.00 7.69 18.46 16.92 7.69

29.23 23.08 6.15 10.77 20.00 10.77

Tu
b

Ba
D
a
i
D
ss
Pi
an
oo
o
n

30.77 26.15 7.69 7.69 20.00 7.69

Tu
b

Ba

Ba
D
a
D

9.23 10.77 4.62 4.62 1.54 69.23

a

4.62 15.38 18.46 35.38 20.00 6.15

ss
Pi
an
oo
o
n

3.08 12.31 3.08 12.31 1.54 67.69

6.15 6.15 47.69 23.08 10.77 6.15

12.31 10.77 10.77 7.69 56.92 1.54

Ba

12.31 13.85 10.77 6.15 56.92 0.00

30.77 20.00 7.69 16.92 15.38 9.23

a

4.62 6.15 20.00 44.62 7.69 16.92

i

6.15 6.15 47.69 21.54 12.31 6.15

24.62 27.69 7.69 12.31 18.46 9.23

Tu
b

D

i

D
a
ss
Pi
an
oo
a
o
n

30.77 33.85 4.62 7.69 15.38 7.69

Tu
b

Ba

Actual stimulus

Ba

100

32.31 29.23 9.23 6.15 13.85 9.23

Ba

on

Da asso
B

o
a
Di Pian Tub

Ba

Da

on a
o
Di Pian asso Tub
B

Figure 4: Temporal searchlight classification. Classifications were performed on temporal subsets of
the time-domain response (500-sweep pseudo-trials, calculated within-participant), using 20-msec windows
advancing in 10 msec increments. (A) Classifier accuracies of single categories over time with mean accuracies overlaid. (B) Mean accuracies, confusion matrices, and dendrograms for each temporal searchlight
classification.

363

The accompanying confusion matrices and dendrograms provide insight into the stimulus struc16

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

364

ture over the time course of the neural response. Responses to ba and da, which have formed the

365

closest category cluster in all analyses thus far, similarly clustered together here, with the notable

366

exception of the first time window (5–25 msec after stimulus onset). Our secondary cluster of

367

di /piano was generally present, although these two categories separated during a later section of

368

the response.

369

As with classification of the full 5–145-msec window, temporal searchlight results for the real

370

and imaginary coefficients up to 1,000 Hz performed similarly to the time-domain searchlight clas-

371

sifications. Temporal searchlight of Fourier magnitude and Fourier phase classifications performed

372

worse than the fully characterized time-domain and frequency-domain responses. The results for

373

the three frequency-domain searchlight classifications are included in Supplementary Figure S2–S4.

374

3.4

375

Each FFR recording was accompanied by a separate perceptual identification test. Given that intact

376

stimuli were used in the experiment and all participants had normal hearing, we expected perceptual

377

accuracy to be near 100%. Indeed, perceptual performance was high, with mean accuracy of 90.6%

378

(p <0.001). As overall perceptual accuracies were high for each stimulus (84.9%–98.1%, Figure 5),

379

we observed greater distance among all categories.

Perceptual Results

380

Perceptual accuracy of all but one participant exceeded 75% (Supplementary Figure S5). We did

381

not observe a significant correlation between perceptual performance and leave-one-participant-out

382

FFR classification performance when pseudo-trials were computed within-participant (rho =0.04,

383

p =0.66).

384

4

385

In this study we have demonstrated that FFRs elicited by both speech and music stimuli can be

386

successfully classified, and that the pattern of classification approximates that observed with a

387

perceptual-identification task. Here, overall accuracy on the perceptual-identification task was

388

90.6%, while the overall classifier accuracy was 72.3%.

389

phonemes and musical instruments is similar to that observed with vowels alone (⇠70–80%; (Sadeghian

Discussion

17

Our classifier accuracy for these CV

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

Predicted stimulus
di
piano

ba

da

bassoon

tuba

ba

94.2

5.8

0.0

0.0

0.0

0.0

da

8.8

90.8

0.0

0.4

0.0

0.0

di

0.0

0.8

98.1

1.2

0.0

0.0

piano

0.0

0.0

0.0

88.4

3.1

8.5

bassoon

1.2

0.0

0.4

2.7

87.3

8.5

tuba

0.0

0.0

0.0

5.4

9.7

84.9

100
90

70
60
50
40

% classified

Actual stimulus

80

30
20
10
0

1

Distance

0.8

0.6

0.4

0.2

0

bassoon

tuba

piano

di

ba

da

Figure 5: Perceptual identification results. Participants completed a 6AFC perceptual identification
task separately from the FFR session. Mean confusion matrix and dendrogram across participants (N=13).
Overall accuracy was 90.6% (p <0.001).

390

et al., 2015), and contributes to a small but growing body of research on FFR decoding and novel

391

analysis approaches.

392

Most FFR research to date has focused on descriptive approaches, which include obtaining the

393

neural response by averaging thousands of stimulus presentations, and then comparing the FFR

394

strength to performance on another task (see N. A. Kraus et al. (2017) for a review). In contrast,

395

classification allows one to determine the extent to which an individual’s FFR responses can be

396

correctly assigned stimulus category labels. This analysis approach thus more closely emulates the

397

process of sound identification that humans perform repeatedly across the lifespan.

398

Our results suggest that overall classifier performance is heavily driven by accurate labeling of

399

responses to musical instrument and di stimuli. For these stimuli, time-domain classwise accuracy

400

ranged from 74% for di to 91% for tuba (Figure 2A). In contrast, responses to ba and da phonemes

401

classified at 55.4% and 50.8%, respectively. While these accuracies exceed the six-class chance

402

level of 16.7%, they are notably lower, and the majority of misclassifications occur between the

403

two categories. One plausible explanation is that the di↵erence in classifier accuracy for these

404

FFRs reflects how robustly the acoustic characteristics of the signal are represented in the neural
18

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

405

response. For example, da and di di↵er in the vowel portion of the CV phoneme, while ba and

406

da di↵er largely in the formant transition from the consonant to the vowel but are identical in the

407

sustained vowel region. Notably, the vowel-evoked FFR amplitude is greater and sustained over a

408

longer time range than that of consonants (Skoe et al., 2015), which may contribute to a classifier

409

that is more heavily weighted towards the vowel-evoked response. Future research could therefore

410

study in greater detail the contribution of FFR encoding of steady-state and transient acoustic

411

features to successful classification when these attributes are varied in a parametric fashion, in

412

order to better understand the contributions of each.

413

We used di↵erent decompositions of our response in order to determine the contributions of the

414

transient and steady state acoustics. First, we applied the classifier to 20-msec moving windows

415

to assess temporal dynamics of FFR decoding across time. Results from these analyses suggest

416

that accuracy is highest when the entire response is analyzed, regardless of which time frame was

417

analyzed. However, relative accuracies of the temporal windows provide insight into the time-

418

varying features of the stimulus that may be driving the classifier. Responses to tuba, for example,

419

classify much better during the sustained portion of the response relative to the onset. In contrast,

420

responses to ba and da are most successfully decoded in the region of the formant transition. These

421

findings support the idea that similarities among stimuli are reflected in similarities of the neural

422

response. Our conclusion is that for classification to optimally identify stimuli with more transient

423

acoustic characteristics such as consonants, the feature vector passed into the classifier may need

424

to be modified to emphasize specific components of the neural response more heavily (e.g., give

425

higher frequencies heavier weighting in an e↵ort to better capture transient changes associated with

426

formant transitions).

427

To further understand the classification process used here, we not only decomposed the response

428

in time, but also assessed classification accuracy according to frequency-domain features. We in-

429

dependently analyzed complex frequency values as well as phase and magnitude components of

430

response spectra. Independent classification accuracy of Fourier magnitude and phase was 58.2%

431

and 41.3%, respectively, compared to 72.3% accuracy of the global response (e.g., combined phase

432

and magnitude). Our data suggest that the integration of magnitude and phase information, as

433

well as the corresponding temporal characteristics, contributes to optimal classification accuracy
19

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

434

with our method.

435

We have shown that classification can produce significant and interpretable results using at least

436

one-fourth fewer stimulus repetitions than are required for averaging-based analyses (4,000–6,000

437

averages per trial; Skoe & Kraus (2010)). As multivariate approaches such as decoding can make use

438

of multiple response features at once during analysis, they exhibit increased sensitivity compared

439

to univariate averaging-based approaches (Haynes & Rees, 2006). The high accuracies obtained in

440

the present study suggest that the classifier is discovering patterns in the observations that may

441

not be readily distinguishable to human observers. This reduction in number of trials aligns with

442

the approach taken in cortical M/EEG classification studies, many of which operate on single trials

443

of response (Kaneshiro et al., 2015; Sankaran et al., 2018) as opposed to up to hundreds of trials

444

needed for averaged ERP analyses (Woodman, 2010). In addition to classifying FFRs with 10-fold

445

cross validation—in which each participant’s response data was distributed among training and

446

test folds—we also carried out cross validation with single-participant test partitions in order to

447

determine whether data from a participant who was not used to train the model could be success-

448

fully classified. Compared to the 10-fold case, results in this scenario showed an approximately

449

10% reduction in overall accuracy, for both within- and across-participant averaging of training

450

observations; these results suggest that individual di↵erences play a role in FFR classification.

451

The final observation is that, while the classification accuracy was clearly above chance, it still

452

remained below that observed on the perceptual identification task. Both Sadeghian et al. (2015)

453

and our results show classification for vowels occurring on approximately 70-80% of trials, as opposed

454

to the high perceptual accuracy observed here for our CV stimuli (91-98%). This suggests that

455

further refinement of the feature input to the classifier or classifier algorithm choice (i.e., how the

456

classifer weighs particular features of the input) is necessary in order to identify speech signals at a

457

rate of accuracy exhibited by that of individuals with normal hearing. Resolving this issue is a crucial

458

step toward future clinical applications. In contrast to the speech signals, classifier and behavioral

459

accuracy were more similar for the musical instruments (84.6% versus 86.9%, respectively). We

460

speculate that the increased classifier accuracy in the music condition reflects the ability of the

461

FFR to encapsulate spectro-temporal features needed to distinguish between instruments. The

462

relationship between perceptual and FFR classifiers may also be impacted by the relative ability
20

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

463

of listeners to correctly identify di↵erent musical instruments. Here, participants 10, 12, and 13

464

had perceptual-identification scores for the musical instruments which were much lower than those

465

of the other participants (individual participant confusion matrices are shown in Supplementary

466

Figure S5). Given that all participants had normal hearing, a plausible interpretation of these data

467

is that the individuals could discriminate among the stimuli, but had inconsistent label mappings

468

for the instruments. We conjecture that these results may not reflect the true perceptual abilities

469

of the participant, but rather difficulties with assigning the correct instrument label.

470

We identify several opportunities for improvement in future research. First, and perhaps most

471

important would be improvements to the classification algorithm itself. For example, improvements

472

in feature selection or other aspects of the classifier could reduce the necessary data size for optimal

473

classification, the optimal number of sweeps to be averaged in trials or pseudo-trials, or the total

474

analysis time. Such improvements to the classifier could better account for individual di↵erences

475

when training the model. In the present study we did not have enough data to both train and test

476

on individual participants. Reducing the necessary amount of data to build a reliable model could

477

enable this step and is likely to prove crucial for modeling data from listeners with hearing loss who

478

use hearing aids or cochlear implants. A second opportunity for improvement would be explore the

479

extent to which small di↵erences in stimuli influence the classification accuracy. Here, the stimuli

480

di↵ered slightly in F0 (97–107 Hz), raising the possibility that these small di↵erences may have

481

influenced the accuracy of our classifier. While we cannot rule out this possibility, we speculate

482

that any e↵ect was minimal because our classifier accuracy was largely similar to that observed

483

with data obtained with F0-aligned stimuli (Sadeghian et al., 2015). Nonetheless, future research

484

could examine FFR classifications for stimuli whose F0 frequencies are exactly matched, as in that

485

case classification would be driven entirely by di↵erences in spectral and temporal structure.

486

In closing, this study provides evidence that FFR classification can be employed to discriminate

487

among responses to CV phonemes and musical instruments. In the present study, we verified

488

that FFR classification is successful under intact perception, and explored the potential stimulus

489

features that drive FFR decoding accuracy. These data add further evidence to a growing body

490

of literature that EEG decoding approaches hold considerable promise for clinical applications

491

in patients with hearing loss, or investigations of the developing auditory system when objective
21

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

492

measures of behavioral abilities are desired.

493

5

494

The authors gratefully acknowledge Karanvir Singh and Vivian Lou for their assistance with data

495

collection; Bernard Wang for adding requested features to the classification code; Duc Nguyen for

496

her help in preparing the code for the perceptual identification test; and Malcolm Slaney for helpful

497

feedback on a draft of the manuscript.

498

References

499

Aiken, S. J., & Picton, T. W. (2008). Envelope and spectral frequency-following responses to vowel sounds.

Acknowledgments

500

Hearing Research, 245 (1), 35–47.

Retrieved from http://www.sciencedirect.com/science/article/pii/

501

S0378595508001743 doi: https://doi.org/10.1016/j.heares.2008.08.004

502

Anderson, S., Parbery-Clark, A., White-Schwoch, T., & Kraus, N. (2015, June). Development of subcortical speech

503

representation in human infants. The Journal of the Acoustical Society of America, 137 (6), 3346–3355. Retrieved

504

from http://europepmc.org/articles/PMC4474946 doi: 10.1121/1.4921032

505
506

507
508

Atcherson, S. S., & Stoody, T. M. (2012). Introduction to auditory evoked potentials. In S. S. Atcherson &
T. M. Stoody (Eds.), Auditory electrophysiology (pp. 1–7). New York, NY: Thieme.
Atcherson, S. S., & White, L. (2012). Cortical event-related potentials. In S. S. Atcherson & T. M. Stoody (Eds.),
Auditory electrophysiology (pp. 138–160). New York, NY: Thieme.

509

Banai, K., Abrams, D., & Kraus, N. (2007, September). Sensory-based learning disability: Insights from brainstem

510

processing of speech sounds. International journal of audiology, 46 (9), 524–532. Retrieved from https://doi.org/

511

10.1080/14992020701383035 doi: 10.1080/14992020701383035

512
513

514
515

Benjamini, Y., & Yekutieli, D. (2001). The control of the false discovery rate in multiple testing under dependency.
The Annals of Statistics, 29 (4), 1165–1188. Retrieved from http://www.jstor.org/stable/2674075
Bhagat, S. (2012). Frequency-following responses. In S. S. Atcherson & T. M. Stoody (Eds.), Auditory electrophysiology (pp. 85–104). New York, NY: Thieme.

22

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

516

Bidelman, G. M., & Krishnan, A. (2009). Neural correlates of consonance, dissonance, and the hierarchy of musical

517

pitch in the human brainstem. Journal of Neuroscience, 29 (42), 13165–13171. Retrieved from http://www

518

.jneurosci.org/content/29/42/13165 doi: 10.1523/JNEUROSCI.3900-09.2009

519

Bidelman, G. M., Lowther, J. E., Tak, S. H., & Alain, C. (2017, March). Mild cognitive impairment is characterized

520

by deficient brainstem and cortical representations of speech. The Journal of neuroscience : The official journal

521

of the Society for Neuroscience, 37 (13), 3610–3620. Retrieved from https://doi.org/10.1523/JNEUROSCI.3700

522

-16.2017 doi: 10.1523/jneurosci.3700-16.2017

523

Blankertz, B., Curio, G., & Müller, K.-R. (2001). Classifying single trial EEG: Towards brain computer inter-

524

facing.

525

ural and synthetic (pp. 157–164). Cambridge, MA, USA: MIT Press. Retrieved from http://dl.acm.org/

526

citation.cfm?id=2980539.2980561

527
528

529
530

In Proceedings of the 14th international conference on neural information processing systems: Nat-

Brainard, D. (1997). The psychophysics toolbox. Spatial vision, 10 (4), 433436. Retrieved from https://doi.org/
10.1163/156856897X00357 doi: 10.1163/156856897x00357
Brueggeman, P. M., & Atcherson, S. M. (2012). Threshold estimation using the auditory brainstem response. In
S. S. Atcherson & T. M. Stoody (Eds.), Auditory electrophysiology (pp. 203–219). New York, NY: Thieme.

531

Carcagno, S., & Plack, C. J. (2011, Apr 12). Pitch discrimination learning: Specificity for pitch and harmonic

532

resolvability, and electrophysiological correlates. Journal of the Association for Research in Otolaryngology, 12 (4),

533

503. Retrieved from https://doi.org/10.1007/s10162-011-0266-3 doi: 10.1007/s10162-011-0266-3

534

Carlson, T., Tovar, D. A., Alink, A., & Kriegeskorte, N. (2013). Representational dynamics of object vision: The

535

first 1000 ms. Journal of Vision, 13 (10). Retrieved from http://www.journalofvision.org/content/13/10/

536

1.abstract doi: 10.1167/13.10.1

537

Co↵ey, E. B., Herholz, S. C., Chepesiuk, A. M., Baillet, S., & Zatorre, R. J. (2016, March). Cortical contributions to

538

the auditory frequency-following response revealed by MEG. Nature communications, 7 , 11070. Retrieved from

539

http://europepmc.org/articles/PMC4820836 doi: 10.1038/ncomms11070

540

Davis, H. (1964, July). Enhancement of evoked cortical potentials in humans related to a task requiring a decision.

541

Science (New York, N.Y.), 145 (3628), 182–183. Retrieved from https://doi.org/10.1126/science.145.3628

542

.182 doi: 10.1126/science.145.3628.182

543

Davis, H., Hirsh, S., Turpin, L., & Peacock, M. (1985). Threshold sensitivity and frequency specificity in auditory

544

brainstem response audiometry. Audiology, 24 (1), 54–70. Retrieved from https://www.tandfonline.com/doi/

545

abs/10.3109/00206098509070097 doi: 10.3109/00206098509070097

23

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

546

Donchin, E. (1969, September). Discriminant analysis in average evoked response studies: The study of single trial

547

data. Electroencephalography and clinical neurophysiology, 27 (3), 311314. Retrieved from https://doi.org/

548

10.1016/0013-4694(69)90061-3 doi: 10.1016/0013-4694(69)90061-3

549

Galambos, R., Makeig, S., & Talmacho↵, P. (1981). A 40-hz auditory potential recorded from the human scalp.

550

Proceedings of the National Academy of Sciences, 78 (4), 2643–2647. Retrieved from https://www.pnas.org/

551

content/78/4/2643 doi: 10.1073/pnas.78.4.2643

552

Galbraith, G., Arbagey, P., Branski, R., Comerci, N., & Rector, P. (1995, November). Intelligible speech encoded

553

in the human brain stem frequency-following response. Neuroreport, 6 (17), 2363–2367. Retrieved from http://

554

europepmc.org/abstract/MED/8747154

555

Galbraith, G. C., Amaya, E. M., de Rivera, J. M. D., Donan, N. M., Duong, M. T., Hsu, J. N., . . . Tsang,

556

L. P. (2004, September). Brain stem evoked response to forward and reversed speech in humans. Neuroreport,

557

15 (13), 2057–2060. Retrieved from https://doi.org/10.1097/00001756-200409150-00012

558

00001756-200409150-00012

doi: 10.1097/

559

Gardi, J., Merzenich, M., & McKean, C. (1979). Origins of the scalp recorded frequency-following response in

560

the cat. Audiology : Official organ of the International Society of Audiology, 18 (5), 358-381. Retrieved from

561

http://europepmc.org/abstract/MED/496719

562

Glaser, E. M., Suter, C. M., Dashei↵, R., & Goldberg, A. (1976). The human frequency-following response: Its

563

behavior during continuous tone and tone burst stimulation. Electroencephalography and Clinical Neurophysiology,

564

40 (1), 25–32. Retrieved from http://www.sciencedirect.com/science/article/pii/0013469476901760 doi:

565

https://doi.org/10.1016/0013-4694(76)90176-0

566

Golland, P., & Fischl, B. (2003). Permutation tests for classification: Towards statistical significance in image-based

567

studies. In C. Taylor & J. A. Noble (Eds.), Information processing in medical imaging (pp. 330–341). Berlin,

568

Heidelberg: Springer Berlin Heidelberg.

569

Greenberg, S., Marsh, J., Brown, W., & Smith, J. (1987). Neural temporal coding of low pitch. i. human frequency-

570

following responses to complex tones. Hearing research, 25 (2-3), 91–114. Retrieved from https://doi.org/

571

10.1016/0378-5955(87)90083-9 doi: 10.1016/0378-5955(87)90083-9

572

Guggenmos, M., Sterzer, P., & Cichy, R. M. (2018). Multivariate pattern analysis for MEG: A comparison of

573

dissimilarity measures. NeuroImage, 173 , 434–447. Retrieved from http://www.sciencedirect.com/science/

574

article/pii/S1053811918301411 doi: https://doi.org/10.1016/j.neuroimage.2018.02.044

575
576

Hall III, J. W. (2007a). Anatomy and physiology principles of auditory evoked responses. In New handbook of
auditory evoked responses (3rd ed., pp. 41–47). Pearson Education, Inc.

24

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

577
578

579
580

581
582

583
584

Hall III, J. W. (2007b). Frequency-specific auditory brainstem response (abr) and auditory steady-state response
(assr). In New handbook of auditory evoked responses (3rd ed., pp. 258–312). Pearson Education, Inc.
Hall III, J. W. (2007c). Overview of auditory neurophysiology: Past, present and future. In New handbook of auditory
evoked responses (3rd ed., pp. 1–34). Pearson Education, Inc.
Hall III, J. W. (2007d). P300 response. In New handbook of auditory evoked responses (3rd ed., pp. 518–548).
Pearson Education, Inc.
Hall III, J. W. (2007e). Pediatric clinical applications and populations. In New handbook of auditory evoked responses
(3rd ed., pp. 313–365). Pearson Education, Inc.

585

Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning (2nd ed.). Springer.

586

Haynes, J. D., & Rees, G. (2006). Decoding mental states from brain activity in humans. Nature Reviews Neu-

587

roscience, 7 (7), 523–534. Retrieved from https://www.nature.com/articles/nrn1931

588

10.1038/nrn1931

589

doi: https://doi.org/

Hood, L. J., Berlin, C. I., He↵ner, R. S., Morehouse, C., Smith, E. G., & Barlow, E. K.

(1991).

Objec-

590

tive auditory threshold estimation using sine-wave derived responses. Hearing Research, 55 (1), 109–116. Re-

591

trieved from http://www.sciencedirect.com/science/article/pii/037859559190097S doi: https://doi.org/

592

10.1016/0378-5955(91)90097-S

593

Hoormann, M., Falkenstein, M., Hohnsbein, J., & Blanke, L. (1992). The human frequency-following response

594

(FFR): Normal variability and relation to the click-evoked brainstem response. Hearing Research, 59 (2), 179–

595

188. Retrieved from http://www.sciencedirect.com/science/article/pii/0378595592901143 doi: https://

596

doi.org/10.1016/0378-5955(92)90114-3

597

Jewett, D. L., Romano, M. N., & Williston, J. S. (1970). Human auditory evoked potentials: Possible brain stem

598

components detected on the scalp. Science, 167 (3924), 1517–1518. Retrieved from https://science.sciencemag

599

.org/content/167/3924/1517 doi: 10.1126/science.167.3924.1517

600

Kaneshiro, B., Perreau Guimaraes, M., Kim, H.-S., Norcia, A. M., & Suppes, P. (2015, 08). A representational

601

similarity analysis of the dynamics of object processing using single-trial EEG classification. PLOS ONE , 10 ,

602

1—27. Retrieved from https://doi.org/10.1371/journal.pone.0135697

603

Kim, J., Lee, S.-K., & Lee, B. (2014, June). EEG classification in a single-trial basis for vowel speech perception

604

using multivariate empirical mode decomposition. Journal of neural engineering, 11 (3), 036010. Retrieved from

605

https://doi.org/10.1088/1741-2560/11/3/036010 doi: 10.1088/1741-2560/11/3/036010

25

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

606
607

608
609

Kraus, N., & Strait, D. L. (2015, 3). Emergence of biological markers of musicianship with school-based music
instruction. Annals of the New York Academy of Sciences, 1337 (1), 163–169. doi: 10.1111/nyas.12631
Kraus, N. A., White-Schoch, T., Fay, R. R., & Popper, A. N. (2017). The frequency-following response: A window
into human communication. Springer.

610

Krishnan, A., Xu, Y., Gandour, J., & Cariani, P. (2005, September). Encoding of pitch in the human brainstem

611

is sensitive to language experience. Brain research. Cognitive brain research, 25 (1), 161–168. Retrieved from

612

https://doi.org/10.1016/j.cogbrainres.2005.05.004 doi: 10.1016/j.cogbrainres.2005.05.004

613

Lawson, G. D., & Peterson, M. E. (2011). Speech audiometry. Plural Publishing.

614

Lee, S., & Bidelman, G. (2017, 01). Objective identification of simulated cochlear implant settings in normal-hearing

615

listeners via auditory cortical evoked potentials. Ear and Hearing, 38 , 1. doi: 10.1097/AUD.0000000000000403

616

Losorelli, S., Kaneshiro, B., Musacchia, G. A., Singh, K., Blevins, N. H., & Fitzgerald, M. B. (2019). Stanford

617

Translational Auditory Research Laboratory - Frequency following response dataset 1 (STAR-FFR-01). In Stan-

618

ford digital repository. Retrieved from https://purl.stanford.edu/cp051gh0103

619

Musacchia, G., Ortiz-Mantilla, S., Roesler, C. P., Rajendran, S., Morgan-Byrne, J., & Benasich, A. A. (2018,

620

August). E↵ects of noise and age on the infant brainstem response to speech. Clinical neurophysiology : Official

621

journal of the International Federation of Clinical Neurophysiology. Retrieved from https://doi.org/10.1016/

622

j.clinph.2018.08.005 doi: 10.1016/j.clinph.2018.08.005

623

Musacchia, G., Sams, M., Skoe, E., & Kraus, N. (2007, October). Musicians have enhanced subcortical auditory

624

and audiovisual processing of speech and music. Proceedings of the National Academy of Sciences of the United

625

States of America, 104 (40), 15894-15898. Retrieved from http://europepmc.org/articles/PMC2000431 doi:

626

10.1073/pnas.0701498104

627

Näätänen, R., & Alho, K.

(1995).

Mismatch negativity–a unique measure of sensory processing in audi-

628

tion. The International journal of neuroscience, 80 (1-4), 317-337. Retrieved from https://doi.org/10.3109/

629

00207459508986107 doi: 10.3109/00207459508986107

630

Näätänen, R., & Alho, K. (1997). Mismatch negativity–the measure for central sound representation accuracy.

631

Audiology & neuro-otology, 2 (5), 341-353. Retrieved from https://doi.org/10.1159/000259255 doi: 10.1159/

632

000259255

633

Näätänen, R., Gaillard, A. W., & Mäntysalo, S. (1978). Early selective-attention e↵ect on evoked potential reinter-

634

preted. Acta Psychologica, 42 (4), 313–329. Retrieved from http://www.sciencedirect.com/science/article/

635

pii/0001691878900069 doi: https://doi.org/10.1016/0001-6918(78)90006-9

26

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

636

Näätänen, R., Paavilainen, P., Tiitinen, H., Jiang, D., & Alho, K. (1993, September). Attention and mismatch

637

negativity. Psychophysiology, 30 (5), 436-450. Retrieved from https://doi.org/10.1111/j.1469-8986.1993

638

.tb02067.x doi: 10.1111/j.1469-8986.1993.tb02067.x

639

Parbery-Clark, A., Tierney, A., Strait, D., & Kraus, N. (2012, September). Musicians have fine-tuned neural

640

distinction of speech syllables. Neuroscience, 219 , 111–119. Retrieved from http://europepmc.org/articles/

641

PMC3402586 doi: 10.1016/j.neuroscience.2012.05.042

642

Russo, N. M., Nicol, T. G., Zecker, S. G., Hayes, E. A., & Kraus, N.

(2005).

Auditory training im-

643

proves neural timing in the human brainstem. Behavioural Brain Research, 156 (1), 95–103. Retrieved from

644

http://www.sciencedirect.com/science/article/pii/S0166432804001809

645

j.bbr.2004.05.012

646
647

doi: https://doi.org/10.1016/

Russo-Ponsaran, N., Nicol, T., Musacchia, G., & Kraus, N. (2004, 05). Brainstem responses to speech syllables.
Clinical Neurophysiology, 115 . doi: 10.1016/S1388-2457(04)00144-0

648

Sadeghian, A., Dajani, H. R., & Chan, A. D. (2015, April). Classification of speech-evoked brainstem responses to

649

english vowels. Speech Commun., 68 (C), 69–84. Retrieved from http://dx.doi.org/10.1016/j.specom.2015

650

.01.003 doi: 10.1016/j.specom.2015.01.003

651
652

Sankaran, N., Thompson, W., Carlile, S., & Carlson, T. (2018, January). Decoding the dynamic representation of
musical pitch from human brain activity. Scientific reports, 8 (1), 839. doi: 10.1038/s41598-018-19222-3

653

Schaefer, R. S., Farquhar, J., Blokland, Y., Sadakata, M., & Desain, P. (2011, May). Name that tune: Decoding music

654

from the listening brain. NeuroImage, 56 (2), 843–849. Retrieved from https://doi.org/10.1016/j.neuroimage

655

.2010.05.084 doi: 10.1016/j.neuroimage.2010.05.084

656
657

Shepard, R. N. (1974, Dec 01). Representation of structure in similarity data: Problems and prospects. Psychometrika, 39 (4), 373–421. Retrieved from https://doi.org/10.1007/BF02291665 doi: 10.1007/BF02291665

658

Simanova, I., van Gerven, M., Oostenveld, R., & Hagoort, P. (2011, 12). Identifying object categories from event-

659

related EEG: Toward decoding of conceptual representations. PLOS ONE , 5 (12), 1–12. Retrieved from https://

660

doi.org/10.1371/journal.pone.0014465 doi: 10.1371/journal.pone.0014465

661
662

Skoe, E., & Kraus, N. (2010, June). Auditory brainstem response to complex sounds: A tutorial. Ear and hearing,
31 (3), 302–324. doi: 10.1097/AUD.0b013e3181cdb272

663

Skoe, E., Krizman, J., Anderson, S., & Kraus, N. (2015, June). Stability and plasticity of auditory brainstem

664

function across the lifespan. Cerebral cortex (New York, N.Y. : 1991), 25 (6), 1415–1426. Retrieved from http://

665

europepmc.org/articles/PMC4428291 doi: 10.1093/cercor/bht311

27

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

666

Skoe, E., Krizman, J., Spitzer, E., & Kraus, N. (2013). The auditory brainstem is a barometer of rapid auditory

667

learning. Neuroscience, 243 , 104–114. Retrieved from http://www.sciencedirect.com/science/article/pii/

668

S0306452213002248 doi: https://doi.org/10.1016/j.neuroscience.2013.03.009

669

Smith, J., Marsh, J., & Brown, W. (1975, November). Far-field recorded frequency-following responses: Evidence

670

for the locus of brainstem sources. Electroencephalography and clinical neurophysiology, 39 (5), 465-472. Retrieved

671

from https://doi.org/10.1016/0013-4694(75)90047-4 doi: 10.1016/0013-4694(75)90047-4

672

Sohmer, H., Pratt, H., & Kinarti, R. (1977, May). Sources of frequency following responses (FFR) in man. Elec-

673

troencephalography and clinical neurophysiology, 42 (5), 656-664. Retrieved from https://doi.org/10.1016/

674

0013-4694(77)90282-6 doi: 10.1016/0013-4694(77)90282-6

675

Song, J. H., Skoe, E., Wong, P. C., & Kraus, N. (2008). Plasticity in the adult human auditory brainstem following

676

short-term linguistic training. Journal of Cognitive Neuroscience, 20 (10), 1892–1902. Retrieved from https://

677

doi.org/10.1162/jocn.2008.20131 doi: 10.1162/jocn.2008.20131

678
679

680
681

Stapells, D., Linden, D., Suffield, J., Hamel, G., & Picton, T. (1984). Human auditory steady state potentials. Ear
and hearing, 5 (2), 10511. Retrieved from http://europepmc.org/abstract/MED/6724170
Strickland, J. N., & Needleman, A. (2012). Auditory steady-state responses: Clinical applications. In S. S. Atcherson
& T. M. Stoody (Eds.), Auditory electrophysiology (pp. 222–236). New York, NY: Thieme.

682

Su, L., Fonteneau, E., Marslen-Wilson, W., & Kriegeskorte, N. (2012). Spatiotemporal searchlight representational

683

similarity analysis in EMEG source space. In Proceedings of the 2012 second international workshop on pattern

684

recognition in neuroimaging (pp. 97–100). Washington, DC, USA: IEEE Computer Society. Retrieved from

685

http://dx.doi.org/10.1109/PRNI.2012.26 doi: 10.1109/PRNI.2012.26

686

Wang, B. C., Norcia, A. M., & Kaneshiro, B. (2017). MatClassRSA: A Matlab toolbox for M/EEG classification

687

and visualization of proximity matrices. bioRxiv . Retrieved from https://www.biorxiv.org/content/early/

688

2017/11/05/194563 doi: 10.1101/194563

689

Wang, R., Perreau-Guimaraes, M., Carvalhaes, C., & Suppes, P. (2012, December). Using phase to recognize english

690

phonemes and their distinctive features in the brain. Proceedings of the National Academy of Sciences of the

691

United States of America, 109 (50), 20685–20690. doi: 10.1073/pnas.1217500109

692

Wang, S., Hu, J., Dong, R., Liu, D., Chen, J., Musacchia, G., & Liu, B. (2016). Voice pitch elicited frequency following

693

response in Chinese elderlies. Frontiers in aging neuroscience, 8 , 286. Retrieved from http://europepmc.org/

694

articles/PMC5126065 doi: 10.3389/fnagi.2016.00286

28

bioRxiv preprint doi: https://doi.org/10.1101/661066; this version posted June 5, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY 4.0 International license.

695

Wong, P. C., Skoe, E., Russo, N. M., Dees, T., & Kraus, N. (2007, April). Musical experience shapes human

696

brainstem encoding of linguistic pitch patterns. Nature neuroscience, 10 (4), 420-422. Retrieved from http://

697

europepmc.org/articles/PMC4508274 doi: 10.1038/nn1872

698
699

Woodman, G. F. (2010, November). A brief introduction to the use of event-related potentials in studies of perception
and attention. Attention, perception & psychophysics, 72 (8), 2031–2046. doi: 10.3758/APP.72.8.2031

700

Yi, H., Xie, Z., Reetzke, R., Dimakis, A., & Chandrasekaran, B. (2017, 03). Vowel decoding from single-trial

701

speech-evoked electrophysiological responses: A feature-based machine learning approach. Brain and Behavior ,

702

7 , e00665. doi: 10.1002/brb3.665

29

