bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

1

Efficient pan-cancer whole-slide image classification and outlier

2

detection using convolutional neural networks

3

Seda Bilaloglu1,2, Joyce Wu1,†, Eduardo Fierro1,†, Raul Delgado Sanchez1,†,

4

Paolo Santiago Ocampo5, Narges Razavian2, Nicolas Coudray3,4,#, Aristotelis

5

Tsirigos3,5,#

6

1

Center for Data Science, New York University, New York, NY, USA

7

2

Department of Population Health, New York University School of Medicine,

8

New York, NY, USA

9

3

Applied Bioinformatics Laboratories, New York University School of

10

Medicine, New York, NY, USA

11

4

12

of Medicine, New York, NY, USA.

13

5

14

York, NY, USA

15

†

These authors contributed equally to this work

16

#

Corresponding authors:

17

Aristotelis Tsirigos, PhD

18

227 East 30th Street, New York, NY 10016

19
20

Email: Aristotelis.Tsirigos@nyulangone.org

21

Nicolas Coudray, PhD

22

227 East 30th Street, New York, NY 10016

23
24

Email: Nicolas.Coudray@nyulangone.org

Skirball Institute, Department of Cell Biology, New York University School

Department of Pathology, New York University School of Medicine, New

25
1

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

26

Abstract

27

Visual analysis of solid tissue mounted on glass slides is currently the

28

primary method used by pathologists for determining the stage, type and

29

subtypes of cancer. Although whole slide images are usually large (10s to

30

100s

31

assessment is necessary to reduce the risk of misdiagnosis. In an effort to

32

address the many diagnostic challenges faced by trained experts, recent

33

research has been focused on developing automatic prediction systems for

34

this multi-class classification problem. Typically, complex convolutional

35

neural network (CNN) architectures, such as Google’s Inception, are used to

36

tackle

37

architecture, PathCNN, which allows for more efficient use of computational

38

resources and better classification performance. Using this improved

39

architecture, we trained simultaneously on whole-slide images from multiple

40

tumor

41

reduction analysis of the weights of the last layer of the network capture

42

groups of images that faithfully represent the different types of cancer,

43

highlighting at the same time differences in staining and capturing outliers,

44

artifacts and misclassification errors. Our code is available online at:

45

https://github.com/sedab/PathCNN.

thousands

this

sites

pixels

problem.

and

wide),

Here,

an

we

corresponding

exhaustive

introduce

a

non-neoplastic

46
47
2

though

greatly

tissue.

time-consuming

simplified

CNN

Dimensionality

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

48

Introduction

49

Examination of hematoxylin & Eosin (H&E) images of surgical resections or

50

tissue biopsies by pathologists is often the first step in identifying and

51

characterizing a tumor. Given the size of such images (from tens to

52

hundreds of thousands of pixels wide), the complexity and heterogeneity of

53

neoplastic and non-neoplastic pathology, this task is non-trivial with some

54

risk to miss small tumor foci or misclassifying minor variants or features

55

present in the image. The benefit of distinguishing non-neoplastic conditions

56

from neoplastic tissue is obvious, but quickly and accurately differentiating

57

between subtypes can also be critical for initiating targeted therapies [1].

58

For example, in the case of lung cancer, the efficacy of conventional

59

chemotherapies differs for the two major subtypes of non-small cell lung

60

cancer (NSCLC), squamous cell carcinoma and adenocarcinoma. Certain

61

agents may be less efficient [2] or more invasive for patients presenting

62

squamous cell carcinoma [3]. On the other hand, adenocarcinoma is more

63

likely to contain genetic mutations which may be treated with targeted

64

therapy, such as EGFR mutations or ALK rearrangements [4]. Although

65

neoplastic subtypes are often visually distinct, diagnostic agreement for

66

classifying adenocarcinomas and squamous cell carcinomas of the lung has

67

been found to be relatively low (κ = 0.41 - 0.46 among community

68

pathologists, κ = 0.64 - 0.69 among pulmonary pathology experts and κ =

69

0.55 - 0.59 among all pathologists under study) [5]. The difference between
3

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

70

community pathologists and pulmonary pathology experts suggests that

71

automated technology may aid the diagnostic process in areas lacking

72

specialized

73

algorithmic decision tree approaches that use only limited amounts of

74

information presented in these images [6]. For the same task on lung

75

cancer, Yu et. al [7] used an automatic image-segmentation pipeline to

76

identify the tumor nuclei and tumor cytoplasm from which they extracted

77

image features. Several classical machine learning approaches have been

78

developed to achieve classification of lung [8,9] and breast [10,11] cancers.

79

Advances in deep learning have paved the way for artificial neural networks

80

to surpass most traditional machine learning methods in the field of image

81

processing. In particular, convolutional neural networks (CNNs) consists of

82

convolutional layers which exploit locality and stationarity and were first

83

proposed by Le Cun et al. [12] These approaches have quickly risen to the

84

state-of the-art on almost all image-based tasks including medical imaging

85

[13, 14]. One of the benefits of using a convolutional neural network

86

architecture is that using domain knowledge to handcraft an image feature

87

extraction system is not required. Conveniently, feature extraction is an

88

automatic task for neural models, and is trained end-to-end in a CNN [15].

89

In recent years, a large number of high resolution digital Whole Slide Images

90

(WSI) of H&E stained tissue slides have been captured by pathologists [16].

91

With an increasing amount of “big data”, CNN has become an excellent

expertise.

Furthermore,

4

most

pathologists

follow

simple

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

92

candidate for WSI processing tasks and it has been adapted by many studies

93

already for tasks like prediction of kidney function and segmentation

94

[17,18],

95

osteosarcoma [20], colon cancer detection [21] and analysis of tumor-

96

infiltrating lymphocytes [42]. In 2014 the GoogLeNet [22] architecture won

97

the Imagenet Large Scale Visual Recognition Challenge Since then, it has

98

been adapted for a wide range of medical imaging studies [23–25], such as

99

for breast and skin cancer detection where it outperforms human experts.

100

Khosravi et al. [26] compared a basic CNN architecture with different

101

versions of Inception and Resnet architectures for classifying pathology

102

images. They obtained an accuracy of %100 and %79 for intra and inter

103

images for tumor subtype discrimination using fine-tuned Inception-V3 on

104

TCGA lung data [27]. Coudray et al. [28] trained an implementation of

105

Inception V3 on the lung cancer classification task (inter-images) and

106

obtained a macro AUC of 0.97 (AUC ∼0.993 for distinguishing between

107

neoplastic and non-neoplastic tissue, and 0.947 for distinguishing between

108

the neoplastic subtypes). The high performance of these Convolutional

109

Networks architectures comes with a high computational cost as they require

110

extremely large numbers of parameters and operations for this level of

111

accuracy which makes the deployment challenging [29,30]. Therefore,

112

software optimization and efficient architecture design becomes a key for

113

many

breast

deep

cancer

learning

detection

[19],

applications to
5

[43],

address

tumor

the

prediction

trade-offs

for

between

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

114

performance and efficiency. In this work, we implement, train and evaluate

115

a simple CNN architecture named PathCNN. We show that the proposed

116

architecture converges faster, uses less memory and achieves better

117

performance than complex architectures. Importantly, PathCNN’s more

118

efficient use of computational resources, allowed us to simultaneously train

119

on images from multiple tumor sites, subtypes and corresponding normals.

120

This in turn, uncovered several cases of misclassified images, outliers (e.g.

121

fibrosis, tubular atrophy, calcification) as well as staining irregularities and

122

other artifacts in the TCGA dataset. In short, PathCNN can be applied to

123

quickly classify whole-slide images and detect outliers in the data.

124
125

Materials and Methods

126

Dataset compilation

127

High-resolution (20x magnification, 0.5um per pixel) tissue whole-slide

128

images of lung cancer, kidney cancer, breast cancer and non-neoplastic

129

(normal) tissue were downloaded from the Genomic Data Commons

130

Database from The Cancer Genome Atlas (TCGA) [31, 32]. The following

131

datasets were used for this study: the lung cancer dataset was composed of:

132

680 slides of Lung Squamous Cell Carcinoma (TCGA-LUSC), 733 slides of

133

Lung Adenocarcinoma (TCGA-LUAD) and 494 Normal tissue slides; the

134

kidney cancer dataset was composed of 147 images of Kidney Chromophobe

135

Renal Cell Carcinoma (TCGA-KICH), 896 images of Kidney Clear cell Renal
6

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

136

Cell Carcinoma (TCGA-KIRC), 345 images of Kidney Papillary Renal Cell

137

Carcinoma (TCGA-KIRP) and 659 Normal tissue images; the breast cancer

138

dataset was composed of 1412 images of Breast Invasive Carcinoma (TCGA-

139

BRCA) and 337 Normal tissue images. Our overall computational strategy is

140

summarized in Figure 1 and is further explained in the following sections.

141
142

Data pre-processing

143

The whole-slide images were tiled into non-overlapping 512×512 pixel tiles

144

using open source package openslide [33]. This tiling strategy is commonly

145

employed for high-resolution medical images, otherwise the resolution would

146

be limited by the GPU memory [34]. 753 slides (94 lung, 430 kidney and

147

229 breast) initially uploaded were removed because of compatibility and

148

readability issues. Additionally, tiles with more than 25% of background

149

were filtered out, as they are unlikely to contain informative features. This

150

process generated nearly 665,812 tiles for lung, 1,077,076 for kidney and

151

655,885 for breast. Before feeding the images into the network, we further

152

down-sampled the tiles to 299 × 299 pixels to make the network

153

comparable to Google’s Inception V3, which requires 299 × 299 pixel inputs.

154
155

Data split into training, validation and test sets

156

For the three cancer types, the data was split into 70%, 15% and 15% into

157

training, validation and test datasets respectively (Table 1). To avoid
7

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

158

leakage, we ensured that all slides (and tiles) that came from the same

159

patient were included in the same dataset (training, validation, or test). This

160

ensured that the model could not simply learn histologic features that are

161

patient-specific to achieve good performance at validation and test time.

162
163

Table 1: Number of tiles and slides in the lung, kidney and breast

164

datasets.
Training

Validation

Testing

(tiles/slides)

(tiles/slides)

(tiles/slides)

LUAD

176,839/513

42,485/111

40,063/110

LUSC

218,260/473

46,687/104

41,969/103

Normal lung

339,572/460

81,303/99

73,979/100

KICH

98,817/102

21,665/23

20,059/22

KIRC

221,023/621

60,147/139

57,372/136

KIRP

68,114/242

20,162/49

14,863/54

Normal kidney

66,724/349

16,700/82

16,085/83

421,467/988

96,992/211

92,461/213

26,091/211

10,064/63

8,810/63

BRCA
Normal breast
165
166

Data augmentation

167

Data augmentation creates new sample points by making alterations to the

168

existing dataset and is widely used to increase the robustness of the image

169

classifier [8,35,36]. It is a form of regularization, as it helps highly complex

8

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

170

neural networks to avoid over-fitting by creating more training examples for

171

the network to learn from. The rotational orientation does not change the

172

classification of the tissue and varying dying methods and lighting

173

environments means that the same histological identifying features may be

174

slightly different in color and saturation between slides. Therefore, using the

175

Tranforms library from Torchvision [37], along with self-written functions

176

and classes, we augmented the training data by randomly introducing

177

positional transforms such as: resize, a horizontal flip, or a rotation of 0º,

178

90º, 180º or 270º degrees. Additionally, we randomly adjusted the hue,

179

brightness, contrast, saturation of the image as visualized in Figure 1d and

180

normalized ((image - mean) / std) before passing them to the model.

181
182

Efficient deep learning architecture development

183

A typical ResNet type block of layers for convolutional neural networks is:

184

Convolution → Non-linearity → Batch Normalization → Pooling → Dropout

185

[26]. Our network uses the same construction, that is similar to the first few

186

layers of the Google Inception V3 model. We initially used 3 convolution

187

blocks going from 32 → 64 → 80 features. These blocks did not include a

188

pooling function, so we added a final linear layer to map 25,920 features

189

down to the number of classes. However, we found that without pooling the

190

features together throughout the blocks, the model was unable to properly

191

perform inference, which may be why the Inception model follows these
9

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

192

upsampling blocks with several layers of pooling blocks. Therefore, we tried

193

a 4 convolutional block network going from 16 → 32 → 64 → 32 output

194

features, using pooling in every block. With this simpler architecture, the

195

model was able to learn, but we were only able to obtain a macro AUC of

196

∼0.903 on the validation set. We concluded that a deeper network may

197

better represent the complexity of the problem. The final architecture we

198

developed can be visualized in the Figure 1. We used 6 layers of

199

convolutional blocks 16 → 32 → 64 → 64 → 128 → 64, followed by a fully

200

connected layer that mapped 5184 features down to the number of classes.

201

The first block uses a kernel size of 5, while the following blocks all used a

202

kernel size of 3. Furthermore, the first two blocks did not contain a pooling

203

layer,

204

downsampling with max pooling.

as

we

wanted

to

upsample

the

number

of

features

before

205
206

Hyper-parameter tuning

207

In our experiments, we found that tuning the dropout rate was very

208

important for model performance. With a dropout rate of p = 0.5, we were

209

only able to obtain an AUC of ∼0.857 on the validation set, while when we

210

decreased dropout to 0.1, we were able to consistently reach macro AUC

211

above 0.9. For aggregation, we used the average score across all the tiles

212

for each class which outperformed the proportion method. For nonlinearity,

213

we used leaky relu as it performed slightly better than relu only. For hyper10

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

214

parameter tuning we performed a search similar to coordinate descent

215

algorithm, where we varied a particular hyper-parameter until we found the

216

best performing model, then moved on to optimize the next hyper-

217

parameter. After several iterations to find a proper learning rate, we found

218

that the best learning rate was 0.001. As for the initialization, Xavier

219

initialization helped get the model achieve better performance earlier (AUC

220

0.94 after 1 epoch vs 0.89) compared to Gaussian initialization. For data

221

augmentation, we used random horizontal flip, random rotations and

222

random change in brightness, contrast, saturation and hue of an image. We

223

found significant boost in performance from AUC of 0.93 to above 0.96. As

224

for the optimizers, both Adam and RMSProp outperformed SGD; we used

225

Adam in our final architecture. For more details, see Supplemental

226

Information.

227
228

Inception V3 architecture

229

To assess the performance of our architecture, we replicated the work done

230

by Coudray et al. [28] on lung cancer task using the pipeline publicly

231

available at [39]. To this end, we used the same training loop and

232

aggregation methods, but with Google’s Inception V3 architecture. Inception

233

increases representational power for image classification by using” Network

234

in Network” approach [38] and reduces computational needs [8]. The

235

architecture consists of blocks of a Stem (Convolution → Convolution →

11

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

236

Convolution padded → MaxPooling → Convolution → Convolution →

237

Convolution) → 3x Inception → 5 x Inception → 2 x Inception → AvgPooling

238

→ Dropout(40%) → Fully connected → Softmax with blocks going from 32 →

239

32 → 64 → 64 → 80 → 192 → 288 → 768 → 1280 → 2048 → 1000 features.

240

[22]. The inception layer combines convolution blocks with various filter

241

sizes and pooling layer which helps with classification when there is a large

242

variation in the location and the size of the information presented in the

243

image [44].

244
245

Tile aggregation

246

While the neural network was trained on a per-tile basis, the model must

247

perform classification on a per-slide basis. Thus, the per-tile scores from the

248

neural network were aggregated for the final prediction using two different

249

methods: (1) per-slide average score, where the average score across all

250

the tiles for each class is calculated, and (2) per-slide tile proportion, where

251

the class with the maximum score is assigned to each tile. The final score is

252

the proportion of tiles assigned to each class.

253
254

Model evaluation metrics

255

The Area Under the Curve (AUC) was used as the evaluation metric. AUC is

256

traditionally defined only for binary classification, but the lung and kidney

257

tasks had more than two classes. Therefore, first the AUC of individual
12

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

258

classes using a one-vs-all method were calculated. Then, the macro-average

259

(macro-AUC), which is the unweighted mean of the individual class AUCs

260

were calculated. Additionally we calculated the following metrics to further

261

evaluate the performance and the robustness of our results: Average of

262

Precision and Recall; Cohen’s kappa (Eq.1), an indicator of agreement of

263

results commonly used for multi-class and imbalanced class problems;

264

Jaccard Coefficient (Eq.2), a measure of similarity and calculated by

265

Intersection over Union of predicted and true labels; Log-loss/cross entropy,

266

the negative log-likelihood of the true labels given a probabilistic classifier’s

267

predictions [27] and it indicates much the predictions vary from the actual

268

labels.
(1)

269
270

Where

271

of chance agreement between the predictions and the actual labels.

272
273
274
275
276
277

is relative observed agreement and

J X,Y

is the hypothetical probability

|X∩Y| / |X∪Y| (2)

Where X and Y are the prediction and actual label set.

278

Pan-cancer t-SNE visualization

279

For the pan-cancer analysis, we used our models’ representation of each

280

image in later layers before reducing them into class probabilities, then

281

clustered those representations using Distributed Stochastic Neighbor

282

Embedding (t-SNE). To achieve this, we used the last layer (fully connected
13

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

283

layer) weights which embeds the information in an array of size 5184 that is

284

later reduced to class probabilities to predict the label. We only extracted the

285

weight that belongs to tiles that were correctly classified by the model to

286

represent the given image, to increase the confidence interval of the

287

clustering. Then t-SNE was used to visualize these high dimensional

288

embedding (number of slides x 5184) in 2-dimensional space.

289
290

Data and code availability

291

Code and Jupyter notebooks to help reproduce the analysis are available at

292

https://github.com/sedab/PathCNN.

293
294

Results

295

Simplified deep learning model achieves high accuracy and fast

296

convergence

297

To evaluate our deep learning architecture, we assessed its performance on

298

the lung dataset (LUAD, LUSC and normal lung from TCGA) and directly

299

compared to the state-of-the-art Inception V3 architecture previously used

300

for this task [37,28]. When there was no time constrains, we were able to

301

obtain a macro AUC of 0.979 for the TCGA validation set at epoch 10 (when

302

the model fully converged) and an AUC of 0.957 for the TCGA test set on our

303

baseline lung cancer task, comparable to previously reported AUC achieved

304

by Inception V3 [28]. Detailed comparison of macro AUCs between Inception
14

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

305

and PathCNN as a function of the number of steps on validation set can be

306

seen in Figure 2a. The model does a nearly perfect job when the

307

classification task is only between tumor and normal lung tissue, with AUC of

308

0.99. Differentiating between the two types of lung cancer, LUAD and LUSC,

309

does not perform equally well, with individual one-vs-all AUC of 0.93 for

310

both classes (Table 2). This agrees with previously published results [27,

311

28] showing that tumors are visually distinct from normal tissue, while

312

cancer types or subtypes can be more challenging to visually differentiate.

313

The results from Yu et al. [7] also follow a similar pattern with a significant

314

difference in AUC between the two tasks. Examples of predictions of our

315

model and Inception V3 on selected lung adenocarcinoma (LUAD) and lung

316

squamous cell carcinoma (LUSC) slides are visualized using heat-maps

317

(Supplementary Figure 1).

318
319

Table 2: Performance of lung type classification models. Validation

320

and Test Macro AUCs (with 95% confidence interval) for the proposed model

321

and for Inception V3.
Normal

LUAD

LUSC

Validation 0.9980 (0.9957,
0.9998)

0.9617 (0.9428,
0.9780)

0.9725 (0.9584,
0.9846)

Test

0.9331 (0.9064,
0.9563)

0.9324 (0.9049,
0.9549)

0.9985 (0.9964,
0.9999)

322

15

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

323

Memory usage and speed comparison

324

Both models were trained on NVIDIA P100 GPU with 100GB memory. During

325

training, our model was much faster compared to inception (Figure 2b): our

326

model required only 0.68 sec per iteration, whereas inception needed 1.8

327

sec per iteration. Furthermore, the average memory occupied at RAM was

328

lower than inception (3GB vs 4GB). Also, our model is much faster at test

329

time: 0.29 sec per tile compared to 0.044 sec with inception, i.e. more than

330

6 times faster.

331
332

Evaluation on independent cohorts

333

We then tested the performance of both models on independent datasets

334

collected the NYU Medical Center [28]. More specifically, we tested the

335

models on two types of specimens: surgical resections (98 frozen slides and

336

140 FFPE slides) and biopsies (102 slides). ROC curves of the performance

337

of our model on each of the three collections of whole-slide images are

338

drawn

339

comparable to those published previously: the LUAD/LUSC AUC on frozen

340

slides is slightly lower (0.85/0.90 for PathCNN vs 0.91/0.94 for Inception),

341

while the ones on FFPE (0.91/0.97 vs 0.83/0.93) and Biopsies (0.86/0.90 vs

342

0.83/0.86) are greater.

in

Supplemental

Figure

2.

The

performances

343
344

Impact of training dataset size on performance
16

are

overall

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

345

To measure how the amount of data used effect the model performances,

346

we performed a down-sampling analysis as follows: we trained the model on

347

reduced datasets by sampling slides (sampled at 1/9th, 1/3rd and 3/4th after

348

ordering by name) and the full set (number of slides = 149, 446, 1000 and

349

1335) and tested on the same full test set from TCGA. Then we calculated

350

the average of AUCs obtained by testing on checkpoints after down-sampled

351

datasets

352

summarized in Figure 2c, we found that PathCNN generalized to the

353

external dataset much better for when trained on smaller datasets compared

354

to Inception (p = 0.0952, 0.0027, 0.0025, 2.6E-14). Furthermore, our

355

model had much faster convergence and showed better convergence-AUC

356

trade-off for the external datasets (Figure 2d).

validation

sets

performance

converged

during

training.

As

357
358

Impact of augmentation on performance

359

We found that the augmentation had a big effect on model performance.

360

AUC

361

(Supplemental Figure 3). Furthermore, interestingly we observed faster

362

overfitting when the training data was not augmented.

increased

by

~0.02

(p<0.001)

on

the

TCGA

test

data

set

363
364

Performance on kidney and breast whole-slide images

365

We then retrained our model on all TCGA datasets: lung, kidney and breast.

366

Although optimized on the lung cancer dataset, our model transferred quite
17

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

367

well to the kidney and breast classification tasks. While we did retrain the

368

entire network (in contrast to a transfer learning approach), we found that

369

even without re-tuning the hyper-parameters, the AUCs were very high

370

(ROC curves are shown in Figure 3a-c). In fact, we did not see the same

371

kind of drop in performance between the validation and tests sets, and they

372

were able to achieve a macro-AUC around 0.998 for kidney (in 12 epochs)

373

and 0.994 for breast (in 14 epochs) (Table 3 and Table 4).

374
375

Table 3: Validation and test performance of classification into lung,

376

kidney and breast. Validation and Test Macro AUCs (with 95% confidence

377

interval) for the proposed model.
Lung

Kidney

Breast

Validation

0.9784 (0.9503,
0.9993)

0.9986 (0.9953,
1.0000)

0.9992 (0.9989,
0.9995)

Test

0.9565 (0.9093,
0.9997)

0.9980 (0.9929,
1.000)

0.9937 (0.9935,
0.9980)

378
379

Table 4: Validation and test performance of classification into kidney

380

classes. Validation and Test Macro AUCs (with 95% confidence interval) for

381

the proposed model.

Validation

Normal

KICH

KIRC

KIRP

0.9999
(0.9998,
1.000)

0.9983
(0.9960,
1.0000)

0.9973
(0.9942,
0.9996)

0.9980
(0.9954,
0.9998)

18

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

Test

0.9990
(0.9976,
0.9999)

0.9935
(0.9867,
0.9989)

0.9940
(0.9891,
0.9975)

0.9917
(0.9837,
0.9974)

382
383

Lastly, the performance of our algorithm is evaluated using various

384

statistical metrics: AUC, average of Precision and Recall, Cohen’s kappa,

385

Jaccard Coefficient, and Log-loss. We found high degree of agreement and

386

similarity between the model class predictions and the actual classes as

387

indicated by the Cohen’s kappa and Jaccard coefficient in Table 5. Similar to

388

precision, recall and log-loss we see higher values with breast followed by

389

kidney and lung.

390
391

Table 5: Performance evaluation using various statistics measures.
AUC

Precision

Recall

Cohen’s

kappa

Jaccard
Log-loss
coefficient

Breast

0.9937

0.9863

0.9524

0.9362

0.9783

0.0862

Kidney

0.9980

0.9466

0.9301

0.9327

0.9551

0.173

Lung

0.9565

0.8812

0.8826

0.8114

0.875

0.4034

392
393

t-SNE visualization of the breast-kidney-lung model and outlier

394

detection

395

We then used PathCNN to simultaneously train for all the breast, kidney and

396

lung types/subtypes and corresponding normals (BRCA, KIRP, KIRC, KICH,

397

LUSC, LUAD and normal breast, kidney, lung). We obtained macro AUC of
19

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

398

0.987 on the validation set and 0.989 on the test set. ROC curves can be

399

seen in Figure 3a-d. We then performed dimensionality reduction on the

400

weights of the last fully-connected layer using t-Distributed Stochastic

401

Neighbor Embedding (t-SNE) [45] and visualized the result in two

402

dimensions (Figure 3e, each dot represents a whole-slide image). Overall,

403

we observed a clear separation by organ (breast, kidney or lung), and by

404

cancer type or normal within each organ (see blue, green and brown areas

405

in Figure 3e). However, we also noticed smaller clusters of slides (clusters

406

A through H) outside these three large areas. We then asked a pathologist to

407

examine all these slides and annotate them. These slides and the

408

pathologist’s annotations – grouped by slide type – are listed in Table 6.

409

More specifically, area A represents a cluster of KIRP cases characterized by

410

intense basophilic staining. Area B is a single KIRC outlier (with a slide

411

composed of predominantly fat) and area C a KIRP outlier (that shows

412

significant frozen artifact). Area D is a normal kidney slide with interstitial

413

fibrosis, tubular atrophy and calcification- reactive features that can also be

414

seen among KIRP cases. Area E includes two normal breast outliers: these

415

slides were found to be overstained.. In area F, the pathologist found normal

416

lung outliers, mainly hypereosinophilic, while one case represented a sample

417

from a benign collapsed lung. Additional normal lung outliers were found in

418

area G, with intense basophilic staining, and one case of benign lung tissue

419

with emphysematous change. Lastly, area H comprises LUAD and LUSC
20

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

420

outliers,

mainly

characterized

by

a

washed

out

staining

pattern.

421

Representative examples of slides belonging to each cluster are shown in

422

Supplemental Figure 4.

423
424

Table 6: Annotation of t-SNE outliers by pathologist.
Area Discrepancy
A
KIRP cluster

B
C
D
E

KIRC outliers
KIRP outlier
Normal kidney among
KIRP
Normal breast outliers

F

Normal lung outliers

G

Normal lung outliers

H
H

LUAD outlier
LUSC outliers

Sample ID
TCGA‐B1‐A655‐01A‐01‐TS1
TCGA‐BQ‐5885‐01A‐01‐BS1
TCGA‐2Z‐A9JQ‐01A‐01‐TS1
TCGA‐2Z‐A9JE‐01A‐01‐TS1
TCGA‐UZ‐A9PV‐01A‐01‐TSA
TCGA‐B8‐A8YJ‐01A‐01‐TS1
TCGA‐KL‐8337‐01A‐01‐BS1
TCGA‐GL‐8500‐11A‐01‐TS1
TCGA‐BH‐A0C3‐11A‐01‐TSA
TCGA‐BH‐A1ET‐11B‐02‐TSB
TCGA‐22‐4593‐11A‐01‐TS1
TCGA‐73‐4659‐11A‐01‐BS1
TCGA‐73‐4670‐11A‐01‐BS1
TCGA‐49‐4510‐11A‐01‐TS1
TCGA‐22‐5477‐11A‐01‐TS1
TCGA‐44‐6775‐11A‐03‐TS3
TCGA‐44‐5645‐11A‐03‐TS3
TCGA‐44‐5645‐11A‐02‐TS2
TCGA‐86‐7714‐01A‐01‐TS1
TCGA‐85‐8664‐01A‐01‐TS1
TCGA‐85‐8666‐01A‐01‐TS1
TCGA‐85‐8288‐01A‐01‐TS1

Interpretation
hyperbasophilic staining
hyperbasophilic staining
hyperbasophilic staining
hyperbasophilic staining
hyperbasophilic staining
predominantly fat
frozen artifact
significant interstitial fibrosis,
tubular atrophy, calcification
overstained slide
overstained slide
hypereosinophilic staining
hypereosinophilic staining
hypereosinophilic staining
benign collapsed lung
hypereosinophilic staining
emphysematous change
hyperbasophilic staining
hyperbasophilic staining
washed out
washed out
washed out
washed out,
large pieces of cartilage

425
426

Discussion

427

With the increasing number of accumulated WSIs, there is a significant

428

opportunity for developing effective diagnostic tools using Machine Learning
21

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

429

techniques to exploit visual features in pathology images that are not

430

detectable by the human eye [40]. Recent developments in computer vision

431

has opened new doors for the development of such tools which comes with

432

the expense of computational requirements. Using our simple convolutional

433

neural network approach, we have exceeded the performance of Inception

434

V3 network [7] trained for one week on the lung cancer classification task

435

with a macro AUC of 0.957. This difference is reflected in the reduced

436

training time of 2 days as compared to 1 week for the deep network. This

437

also means it will be significantly faster at prediction time; aggregation and

438

prediction for a particular slide image took around 30 minutes. Limited

439

resources in a realistic health care setting means that this decrease in

440

diagnostic turnaround time could translate to positive real-world outcomes

441

for patients. We found that using data augmentation, tuning the learning

442

rate, reducing the drop-out rate, and increasing the depth were the most

443

important factors for achieving the best performance. With a simpler

444

architecture, we were able to reach comparable results to a very deep

445

network.

446

optimization, better results and higher efficiency can be achieved. For lung

447

cancer, the individual class AUCs reflect how the network was better at

448

distinguishing between Solid Tissue Normal (non-neoplastic tissue) and the

449

two types of non-small cell lung cancer, with an AUC of around 0.999. On

450

the other hand, the network’s performance is not as good when trying to

With

further

hyper-parameter

22

tuning

and

further

software

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

451

differentiate between the two different classes of cancer: Lung Squamous

452

Cell Carcinoma (TCGA-LUSC) and Lung Adenocarcinoma (TCGA-LUAD).

453

Another positive outcome from our work is that it generalizes well to kidney

454

and breast cancers. In fact, the AUC scores improved in comparison to the

455

ones achieved for lung: macro AUC of 0.997 and 0.996 on the test set

456

respectively. As with lung cancer, the model performed significantly better at

457

distinguishing non-neoplastic tissue from neoplastic tissue for both of these

458

cancer types. For breast cancer, there were only two possible classes: non-

459

neoplastic and neoplastic tissue. Just as the case with lung cancer, we found

460

that our model can easily distinguish between these two classes even though

461

we did not further tune the hyper parameters specific to this data set. This

462

suggests that the framework we developed is well suited for the general task

463

of distinguishing neoplastic tissue from non-neoplastic tissue. For the kidney

464

cancer task, there were more neoplastic subtypes (3 compared to 2 for

465

lung). Nevertheless, we found that the model was almost perfectly able to

466

distinguish between them and achieved a macro AUC better than the lung

467

cancer task. On the test set, the individual class AUCs were 0.999, 0.994,

468

0.994, and 0.992 for Solid Tissue Normal, TCGA-KICH, TCGA-KIRC, TCGA-

469

KIRP, respectively. This is likely due to the fact that these neoplastic

470

subtypes are more visually distinct than the lung cancer subtypes, and

471

therefore the task is easier overall. Another possible explanation could be

472

due to the difference in the available training data. After tiling, we had
23

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

473

around 1077k (925 slides) kidney cancer tiles compared to 666k (1928) lung

474

cancer tiles in their respective training sets, a 62% increase. This

475

phenomenon may be explained by the pre-processing step that removed any

476

tile with more than 25% background. The kidney tissue slides may be

477

denser with richer information.

478
479

Acknowledgments

480

We would like to thank the Applied Bioinformatics Laboratories (ABL) for

481

providing

482

interpretation of the data. ABL is a shared resource, partially supported by

483

the Cancer Center Support Grant, P30CA016087, at the NYU School of

484

Medicine Laura and Isaac Perlmutter Cancer Center. This work has used

485

computing resources, services and staff expertise at both the NYU Medical

486

Center and NYU IT High-Performance Computing Facilities. The slide images

487

and the corresponding cancer information were uploaded from the Genomic

488

Data Commons portal https://gdc-portal.nci.nih.gov and are in whole or part

489

based

490

http://cancergenome.nih.gov/). The data used were publicly available

491

without restriction, authentication or authorization necessary.

bioinformatics

upon

data

support

generated

and

by

492
493

Bibliography

24

helping

the

with

TCGA

the

analysis

Research

and

Network

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

494

[1] Bryan A. Chan, Brett G.M. Hughes Targeted therapy for non-small cell

495

lung cancer: current standards and the promise of the future. Translational

496

Lung Cancer Research 4, 36-54 (2015).

497
498

[2] Andrew G. Nicholson, Andrew G. Nicholson, David Gonzalez, Pallav Shah,

499

Matthew J. Pynegar, Manjiri Deshmukh, Alexandra Rice, Sanjay Popat

500

Refining the diagnosis and egfr status of non-small cell carcinoma in biopsy

501

and cytologic material, using a panel of mucin staining, ttf-1, cytokeratin 5/6

502

and p63, and egfr mutation analysis. The Journal of Pathology, 220:S2,

503

2010.

504
505

[3] Giorgio Scagliotti, Nasser Hanna, Frank Fossella, Katherine Sugarman,

506

Johannes Blatter, Patrick Peterson, Lorinda Simms, and Frances A Shepherd.

507

The differential efficacy of pemetrexed according to nsclc histology: a review

508

of two phase iii studies. The oncologist, 14(3):253–263, 2009. 23

509
510

[4] Michael Snyder. Genomics and Personalized Medicine: What Everyone

511

Needs to Know. Oxford University Press, 2016.

512
513

[5] Juneko E Grilley-Olson, D Neil Hayes, Dominic T Moore, Kevin O Leslie,

514

Matthew D Wilkerson, Bahjat F Qaqish, Michele C Hayward, Christopher R

515

Cabanski, Xiaoying Yin, Mark A Socinski, et al. Validation of interobserver
25

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

516

agreement in lung cancer assessment: Hematoxylin-eosin diagnostic

517

reproducibility for non–small cell lung cancer: The 2004 world health

518

organization classification and therapeutically relevant subsets. Archives of

519

pathology & laboratory medicine, 137(1):32–40, 2012.

520
521

[6] Ugljesa Djuric, Gelareh Zadeh, Kenneth Aldape, Phedias Diamandis

522

Precision histology: how deep learning is poised to revitalize

523

histomorphology for personalized cancer care. npj Precision Oncology. 2017;

524

1(1)

525
526

[7] Kun-Hsing Yu, Ce Zhang, Gerald J Berry, Russ B Altman, Christopher

527

R´e, Daniel L Rubin, and Michael Snyder.Predicting non-small cell lung

528

cancer prognosis by fully automated microscopic pathology image features.

529

Nature communications, 7, 2016.

530
531

[8] Xin Luo, Xiao Zang, Lin Yang, Junzhou Huang, Faming Liang, Jaime

532

Rodriguez-Canales, Ignacio I. Wistuba, Adi Gazdar, Yang Xie, Guanghua Xiao

533

Comprehensive Computational Pathological 24 Image Analysis Predicts Lung

534

Cancer Prognosis. Journal of Thoracic Oncology 12, (2017).

535
536

[9] Peter W. Hamilton, Yinhai Wang, Clinton Boyd, Jacqueline A. James,

537

Maurice B. Loughrey, Joseph P. Hougton, David P. Boyle, Paul Kelly, Perry
26

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

538

Maxwell, David McCleary, James Diamond, Darragh G. McArt, Jonathon

539

Tunstall, Peter Bankhead, and Manuel Salto-Tellez Automated tumor analysis

540

for molecular profiling in lung cancer. Oncotarget. 2015;6(29):27938-27952.

541
542

[10] Fei Dong , Humayun Irshad , Eun-Yeong Oh, Melinda F. Lerwill, Elena F.

543

Brachtel, Nicholas C. Jones, Nicholas W. Knoblauch, Laleh Montaser-

544

Kouhsari, Nicole B. Johnson, Luigi K. F. Rao, Beverly Faulkner-Jones, David

545

C. Wilbur, Stuart J. Schnitt, Andrew H. Beck Computational pathology to

546

discriminate benign from malignant intraductal proliferations of the breast.

547

PLoS ONE 9(12): e114885. https://doi.org/10.1371/journal.pone.0114885 .

548
549

[11] Lin-Wei Wang , Ai-Ping Qu , Jing-Ping Yuan, Chuang Chen, ShengRong

550

Sun, Ming-Bai Hu, Juan Liu , Yan Li Computer-based image studies on tumor

551

nests mathematical features of breast cancer and their clinical prognostic

552

value. PLoS ONE 8(12): e82314.

553

https://doi.org/10.1371/journal.pone.0082314.

554
555

[12] Yann Le Cun and Yoshua Bengio. Word-level training of a hand25

556

written word recognizer based on convolutional neural networks. In Pattern

557

Recognition, 1994. Vol. 2-Conference B: Computer Vision & Image

558

Processing., Proceedings of the 12th IAPR International. Conference on,

559

volume 2, pages 88–92. IEEE, 1994.
27

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

560
561

[13] Hayit Greenspan, Bram van Ginneken, Ronald M. Summers Guest

562

Editorial Deep Learning in Medical Imaging: Overview and Future Promise of

563

an Exciting New Technique in IEEE Transactions on Medical Imaging, vol. 35,

564

no. 5, pp. 1153-1159, May 2016.

565
566

[14] Dinggang Shen, Guorong Wu,and Heung-Il Suk Deep Learning in

567

Medical Image Analysis in Medical Image Analysis. Annual Review of 438

568

Biomedical Engineering 19, 221-248 (2017).

569
570

[15] Dan C. Cires¸an, Ueli Meier, Jonathan Masci, Luca M. Gambardella,

571

Jurgen Schmidhuber Flexible, High Performance Convolutional Neural

572

Networks for Image Classification . In IJCAI ProceedingsInternational Joint

573

Conference on Artificial Intelligence, volume 22, 1237

574
575

[16] Daisuke Komura, Shumpei Ishikawa Machine learning methods for

576

histopathological image analysis

577

https://arxiv.org/ftp/arxiv/papers/1709/1709.00786.pdf

578
579

[17] David Ledbetter,Long Van Ho, Kevin V Lemley Prediction of Kid26 ney

580

Function from Biopsy Images Using Convolutional Neural Networks

581

https://arxivorg/pdf/170201816pdf 2017:1–11
28

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

582
583

[18] Michael Gadermayra,c, Ann-Kathrin Dombrowskia, Barbara Mara

584

Klinkhammerb, Peter Boorb, Dorit Merhofa CNN Cascades for Segmenting

585

Whole Slide Images of the Kidney https://arxiv.org/pdf/1708.00251.pdf

586
587

[19] Angel Cruz-Roa, Hannah Gilmore, Ajay Basavanhally, Michael Feldman,

588

Shridar Ganesan, Natalie Shih, John E. Tomaszewski, Fabio A. Gonzalez,

589

Anant Madabhushi less Accurate and reproducible invasive breast cancer

590

detection in whole-slide images: A Deep Learning approach for quantifying

591

tumor extent. Scientific Reports 446 7 (2017).

592
593

[20] Rashika Mishra, Ovidiu Daescu, Patrick Leavey, Dinesh Rakheja, Anita

594

Sengupta Histopathological diagnosis for viable and nonviable tumor

595

prediction for osteosarcoma using convolutional neural network International

596

Symposium on Bioinformatics Research and Applications. (ed Springer) 12-

597

23

598
599

[21] Korsuk Sirinukunwattana, Shan E Ahmed Raza, Yee-Wah Tsang, David

600

R. J. Snead, Ian A. Cree, Nasir M. Rajpoot Locality Sensitive Deep Learning

601

for Detection and Classification of Nuclei in Routine 27 Colon Cancer

602

Histology Images IEEE TRANSACTIONS ON 449 MEDICAL IMAGING 35,

603

1196-1206 (2016).
29

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

604
605

[22] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,

606

and Zbigniew Wojna. Rethinking the inception architecture for computer

607

vision. CoRR, abs/1512.00567, 2015. URL http://arxiv.org/abs/ 1512.00567.

608
609

[23] Yun Liu, Krishna Gadepalli, Mohammad Norouzi, George E. Dahl, Timo

610

Kohlberger, Aleksey Boyko, Subhashini Venugopalan, Aleksei Timofeev,

611

Philip Q. Nelson, Greg S. Corrado, Jason D. Hipp, Lily Peng, Martin C.

612

Stumpe Detecting Cancer Metastases on Gigapixel Pathology Images (arXiv

613

preprint arXiv:1703.02442); 2017

614
615

[24] Dayong Wang, Aditya Khosla, Rishab Gargeya, Humayun Irshad Andrew

616

H Beck Deep Learning for Identifying Metastatic Breast Cancer. arXiv

617

preprint arXiv:1606.05718 (2016). 441

618
619

[25] Andre Esteva, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M.

620

Swetter, Helen M. Blau, Sebastian Thrun Dermatologist-level classification of

621

skin cancer with deep neural networks. Nature. 2017; 542: 115–118

622
623

[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun Deep Residual

624

Learning for Image Recognition 2016 IEEE Conference on Com28 puter

30

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

625

Vision and Pattern Recognition (CVPR), Las Vegas, NV, 2016, pp. 770-

626

778.doi: 10.1109/CVPR.2016.90

627
628

[27] Pegah Khosravi, Ehsan Kazemi, Marcin Imielinski, Olivier

629

Elemento,Iman Hajirasouliha Deep Convolutional Neural Networks Enable

630

Discrimination of Heterogeneous Digital Pathology Images EBioMedicine

631

(2018).

632
633

[28] Nicolas Coudray, Andre L Moreira, Theodore Sakellaropoulos, David

634

Fenyo, Narges Razavian, and Aristotelis Tsirigos. Classification and mutation

635

prediction from non-small cell lung cancer histopathology images using deep

636

learning. bioRxiv, page 197574, 2017.

637
638

[29] Jason Cong, Bingjun Xiao Minimizing Computation in Convolutional

639

Neural Networks —ICANN 2014; Wermter, S., Weber, C., Duch, W.,

640

Honkela, T., Koprinkova-Hristova, P., Magg, S., Palm, G., Villa, A.E.P., Eds.;

641

Springer International Publishing: Cham, Switzerland, 2014; pp. 281–290.

642
643

[30] Vivienne Sze, Yu-Hsin Chen, Joel Emer, Amr Suleiman, Zhengdong

644

Zhang Hardware for Machine Learning: Challenges and Opportunities In

645

Proceedings of the 2017 IEEE Custom Integrated Circuits Conference (CICC),

646

Austin, TX, USA, 30 April–3 May 2017; pp. 1–8. 29
31

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

647
648

[31] David A. Gutman, Jake Cobb, Dhananjaya Somanna, Yuna Park,

649

Fusheng Wang, Tahsin Kurc, Joel H. Saltz, Daniel J. Brat, Lee A D Cooper.

650

Cancer Digital Slide Archive: an informatics resource to support integrated in

651

silico analysis of TCGA pathology data. Cancer Digital Slide Archive: an

652

informatics resource to support integrated in silico analysis of TCGA

653

pathology data, Journal of the American Medical Informatics Association,

654

Volume 20, Issue 6, 1 November 2013, Pages 1091–1098,

655

https://doi.org/10.1136/amiajnl2012-001469.

656
657

[32] Nci’s genomic data commons. URL https://gdc.cancer.gov/. Accessed:

658

2017-10-01.

659
660

[33] Adam Goode, Benjamin Gilbert, Jan Harkes, Drazen Jukic, Mahadev

661

Satyanarayanan OpenSlide: A vendor-neutral software foundation for digital

662

pathology. J Pathol Inform [serial online] 2013 [cited 2018 Jun 23];4:27.

663

Available from:

664

http://www.jpathinformatics.org/text.asp?2013/4/1/27/119005

665
666

[34] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:

667

Convolutional networks for biomedical image segmentation. In International

32

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

668

Conference on Medical Image Computing and ComputerAssisted

669

Intervention, pages 234–241. Springer, 2015.

670
671

[35] Alex Krizhevsky , Ilya Sutskever , Geoffrey E. Hinton Imagenet 30

672

classification with deep convolutional neural networks In Annual Conference

673

on Neural Information Processing Systems (NIPS), pp. 1106–1114, 2012

674
675

[36] Marzieh Fadaee, Arianna Bisazza, Christof Monz Data Augmentation for

676

Low-Resource Neural Machine Translation arXiv:1705.00440, 2017.

677
678

[37] Datasets, Transforms and Models specific to Computer Vision,

679

https://github.com/pytorch/vision

680
681

[38] Min Lin, Qiang Chen, Shuicheng Yan Network In Network arXiv preprint

682

arXiv:1312.4400, (2013)

683
684

[39] Classification of Lung cancer slide images using deep-learning

685

https://github.com/ncoudray/DeepPATH

686
687

[40] Anant Madabhushi, George Lee Image analysis and machine learning in

688

digital pathology: Challenges and opportunities Imaging Med 2009;1:7.

689
33

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

690

[41] Naobumi Tochigi, Sanja Dacic, Marina Nikiforova, Kathleen M Cieply,

691

and Samuel A Yousem. Adenosquamous carcinoma of the lung: a

692

microdissection study of kras and egfr mutational and amplification status in

693

a western patient population. American journal of clinical pathology,

694

135(5):783–789, 2011. 31

695
696

[42] Joel Saltz, Rajarsi Gupta,Le Hou, Tahsin Kurc,Pankaj Singh, Vu Nguyen,

697

Dimitris Samaras, Kenneth R. Shroyer, Tianhao Zhao, Rebecca Batiste, John

698

Van Arnam, The Cancer Genome Atlas Research Network, Ilya Shmulevich,

699

Arvind U.K. Rao, Alexander J. Lazar, Ashish Sharma, and V´esteinn

700

Thorsson Spatial Organization and Molecular Correlation of Tumor-Infiltrating

701

Lymphocytes Using Deep Learning on Pathology Images Cell reports.

702

2018;23(1):181- 193.e7. doi:10.1016/j.celrep.2018.03.086.

703
704

[43] Nima Habibzadeh Motlagh, Mahboobeh Jannesary, HamidReza

705

Aboulkheyr, Pegah Khosravi, Olivier Elemento, Mehdi Totonchi, Iman

706

Hajirasouliha. Breast Cancer Histopathological Image Classification: A Deep

707

Learning Approach. bioRxiv 242818; doi: https://doi.org/10.1101/242818

708
709

[44] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,

710

Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.

34

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

711

Going Deeper with Convolutions In Proceedings of the IEEE Conference on

712

Computer Vision and Pattern Recognition, pages 1–9, 2015. 32

713
714
715
716
717

[45] van der Maaten, L. & Hinton, G. E. Visualizing data using t-SNE. J.
Mach. Learn.Research 9, 2579–2605 (2008).

718

35

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

719

Figure Legends

720
721

Figure 1. Data and Workflow. (a) Whole-slide images of resected patient

722

tumors were downloaded from the Genomic Data Common database, (b)

723

Slides were then split into a training (70%), a validation (15%) and a test

724

set (15%); slides from the same patient were included in only one of the

725

three sets, (c) Slides were tiled by non-overlapping 512x512 pixels

726

windows, omitting those with over 50% background, (d) Training data was

727

randomly augmented using different approaches, (e) Architecture of the

728

Convolutional Neural Network used for training.

729
730

Figure 2. Evaluation of PathCNN and comparison with Inception v3.

731

(a) Macro AUC at each training step on TCGA training dataset, (b)

732

Computing resource requirements: training time (seconds per iteration),

733

training memory (Gb) and test time (seconds per tile), (c) Impact of the

734

size of the training dataset on the test AUC, (d) Convergence time (x-axis)

735

vs AUC (y-axis) for each architecture and training set size (darker color

736

represents larger training set). Color assignments: Green=PathCNN and

737

Orange=Inception.

738
739

Figure 3. PathCNN detects misclassified slides and staining artifacts

740

in TCGA. (a) ROC curve on the test set of lung images (normal lung, LUAD
36

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

741

and LUSC), (b) ROC curve on the test set of kidney images (normal kidney

742

KICH, KIRC and KIRP), (c) ROC curve on the test set of breast images

743

(normal breast and BRCA), (d) ROC curve for all slides in the test set, (e) t-

744

SNE visualization of the PathCNN model trained on all cancer types and

745

corresponding normal (lung, kidney and breast); detailed description of the

746

identified clusters in Table 7.

747
748

Supplemental Figure 1. Visualization of images and classification

749

probability

750

Squamous Cell Carcinoma (TCGA-LUSC), (b) Aggregated prediction using

751

tiles of the slide for the proposed model, (c) Aggregated prediction using

752

tiles of the slide for Inception V3, (d) Original Whole Slide image with Lung

753

Adenocarcinoma (TCGA-LUAD), (e) Aggregated prediction using tiles of the

754

slide for the proposed model, (f) Aggregated prediction using tiles of the

755

slide

756

Adenocarcinoma (TCGA-LUAD), (h) Aggregated prediction using tiles of the

757

slide for the proposed model, (i) Aggregated prediction using tiles of the

758

slide for Inception V3.

for

heatmaps.

Inception

(a)

V3, (g)

Original

Original

Whole

Whole

Slide

Slide

image

image

with

with

Lung

Lung

759
760

Supplemental Figure 2. PathCNN performance on independent NYU

761

cohort. (a) ROC curve on slides prepared from tumor resections (Frozen),

37

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

762

(b) ROC curve on slides prepared from biopsies (Frozen and FFPE), (a) ROC

763

curve on slides prepared from tumor resections (FFPE).

764
765

Supplemental

Figure

3.

Impact

of

augmentation

on

PathCNN

766

performance. Macro AUC at each training step on TCGA training dataset.

767

Color assignments: Blue=no augmentation, Orange=augmentation enabled.

768
769

Supplemental Figure 4. Representative slides from outlier analysis.

770

Representative slides from areas A through H marked on the t-SNE plot of

771

Figure 3e.

772
773

38

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

Supplemental Information

774
775
776

Nonlinearity

777

In our experiments, we found that both ReLU and LeakyReLU worked well,

778

but using LeakyReLU resulted in slightly better performance. Therefore for

779

our final architecture, we used LeakyReLU with negative slope 0.01 as the

780

non-linearity.

781
782

Dropout

783

In our experiments, we found that tuning the dropout rate was very

784

important for model performance. When we first used the default dropout

785

rate of p = 0.5, we were only able to obtain an AUC of $\sim$0.857. We

786

realized that the dropout rate may be too high, especially as we were

787

applying dropout after every convolutional block. In comparison, the

788

Inception model only applies one layer of dropout after all of the blocks.

789

When we decreased dropout to 0.1, we were able to consistently reach

790

macro AUC above 0.9. Another explanation may also be due to the use of

791

batch

792

normalization and dropout are regularization methods, so the use of batch

793

normalization reduces the need for dropout. In this case, the higher dropout

794

of $p = 0.5$ was causing our model to underfit.

normalization

within

each

convolutional

795
39

block.

Both

batch

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

796

Aggregation method

797

We tried both the average score and proportion of tiles methods for

798

aggregating the tiles, as described in the methodology section 3.7.1.

799

found that the methods were comparable, though the average method

800

performed between 0.003 and 0.008 AUC better on the three datasets.

801

Intuitively, this makes sense: Assigning each tile a class based on the

802

maximum probability would treat a tile that is assigned a class with 99\%

803

probability the same as one with 60\% probability. Therefore, taking a

804

"majority vote" method per

805

information for the prediction.

We

tile could potentially discard some useful

806
807

Other hyperparameters

808

We didn't run a full grid search on all the hyperparameters and options listed

809

in the methodology section. However, we performed a search similar to

810

coordinate descent algorithm where we varied a particular hyperparameter

811

until we found the best performing model, then moved on to optimize the

812

next hyperparameter. With this method, we were still able to draw some

813

insights into the effect of particular hyperparameters on model performance.

814

One of the first challenges we faced was finding the proper learning rate for

815

the model to learn. For learning rates smaller than 1e-5, the model tended

816

to get stuck with no major decreases in loss after one epoch. On the other

817

hand, when the learning rate was larger than 0.001, the loss function
40

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

818

diverged, and the AUC jumped around unstably. After several iterations, we

819

found that the best learning rate was 0.001. We also tried implementing a

820

decaying learning rate approach, which should help the model achieve a

821

better minimum once it is closer to the the optimum. However, this

822

approach did not end up outperforming the best learning rate with an

823

adaptive momentum based optimizer. As for the initialization, although

824

Gaussian initialization worked well, Xavier initalization helped get the model

825

achieve better performance earlier (AUC 0.94 after 1 epoch vs 0.89).

826

However, we found that the final AUC achieved by Xavier initialization was

827

comparable to the Gaussian initialization at the end of training. Surprisingly,

828

we found that He initialization diverged and did not outperform the Gaussian

829

initialization as expected. This initialization, originally derived to deal with a

830

novel Parametric Rectifier Linear Unit (PReLU) \cite{bib11}, perhaps may

831

not apply as well to the LeakyReLU units (a specific case of the proposed

832

PReLU) we ended up using. For data augmentation, we determined the

833

acceptable limits of variation from the original image by manually visualizing

834

several sample outputs. For example, to augment of brightness for each tile,

835

a value was sampled uniformly at random between 0.75 and 1.25; that is,

836

the brightness would not be changed by more than 25\%. Similarly, we

837

chose a maximum contrast difference of $\pm$25\%, saturation difference

838

of $\pm$25\%, and hue difference of $\pm$5\%. We believe that these

839

exact limits could be further tuned, but our heuristic approach already
41

bioRxiv preprint doi: https://doi.org/10.1101/633123; this version posted May 14, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under
aCC-BY-NC 4.0 International license.

840

showed a significant boost in performance. Without augmentation, our

841

model was only able to obtain macro AUC around 0.93 on the validation set

842

for the lung cancer task. With data augmentation, we were able to

843

consistently achieve macro AUCs above 0.96. In terms of the optimizers,

844

both Adam and RMSProp performed similarly. SGD, on the other hand, did

845

not perform as well. This is likely because SGD does not have a momentum-

846

type mechanism to adaptively adjust the learning rate. Use of SGD can

847

require a lot more careful hyperparameter tuning than the adaptive methods

848

such as Adam and RMSProp.

849

42

Figure 1
a. Import data

b. Split data

c. Tiling & filtering

d. Augmentation
resize

Training

contrast brightness
Validation

Test

hue

saturation

rotation

e. Training

Figure 2
c

a

1
0.95

AUC

0.9
0.85
0.8
0.75
0.7

0

500

1000

1500

d
1.8

convergence - AUC

1
0.95

0.68

0.9

4070680
3140640

0.85
0.8

0.04441636
0.028557742

AUC

b

Test time training training time
(sec/tile) AveRSS (KB) (sec/iteration)

number of slides used for training

0.75
0.7

0

20000

40000

60000

80000

convergence step

100000

120000

Figure 3
a

d

e

lung
A

kidney
D

breast

F

b

H

B
C

c
E
G

Supplemental Figure 1

Supplemental Figure 2

a

b

c

Supplemental Figure 3

1
0.98
0.96
0.94
0.92
0.9
0.88
0.86
0.84
0.82
0.8

TCGA LUSC
0.98
0.96
0.94
0.92

AUC

AUC

TCGA LUAD

0.9
0.88
0.86
0.84

1

2

3

4

5

6

7

8

9

10 11 12 13 14 15

epoch

without augmentation

with augmentation

0.82
0.8

1

2

3

4

5

6

7

8

epoch

9

10

11

12

13

14

15

Supplemental Figure 4
KIRP cluster
TCGA-B1-A65501A-01-TS1
KIRC outliers
TCGA-B8-A8YJ01A-01-TS1
KIRP outliers
TCGA-KL-833701A-01-BS1
Normal Kidney
among KIRP
TCGA-GL-850011A-01-TS1
Normal Breast
outliers
TCGA-BH-A1ET11B-02-TSB

Normal Lung
outliers
TCGA-734670-11A01-BS1
Normal Lung
outliers
TCGA-445645-11A03-TS3
LUAD
outliers
TCGA-867714-01A01-TS1
LUSC outliers
TCGA-858288-01A01-TS1

