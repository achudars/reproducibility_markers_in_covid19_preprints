medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Gaze, Visual, Myoelectric, and Inertial Data of
Grasps for Intelligent Prosthetics
Matteo Cognolato1,2* , Arjan Gijsberts3 , Valentina Gregori3,4 ,
Gianluca Saetta5 , Katia Giacomino6 ,
Anne-Gabrielle Mittaz Hager6 , Andrea Gigli3 , Diego Faccio7 ,
Cesare Tiengo7 , Franco Bassetto7 , Barbara Caputo3,8 ,
Peter Brugger5 , Manfredo Atzori1 , Henning Müller1
October 15, 2019
1. Information Systems Institute, University of Applied Sciences Western
Switzerland (HES-SO Valais), Sierre, Switzerland
2. Rehabilitation Engineering Laboratory, Department of Health Sciences
and Technology, ETH Zurich, Zurich, Switzerland
3. Istituto Italiano di Tecnologia, Genoa, Italy
4. Department of Computer, Control, and Management Engineering, University of Rome La Sapienza, Rome, Italy
5. Department of Neurology, University Hospital of Zurich, Zurich, Switzerland
6. Department of Physical Therapy, University of Applied Sciences Western
Switzerland (HES-SO Valais), Leukerbad, Switzerland
7. Clinic of Plastic Surgery, Padova University Hospital, Padova, Italy
8. Politecnico di Torino, Turin, Italy
*corresponding authors: Matteo Cognolato (matteo.cognolato@hevs.ch), Manfredo Atzori (manfredo.atzori@hevs.ch), and Henning Müller (henning.mueller@hevs.ch)
Abstract
Hand amputation is a highly disabling event, having severe physical
and psychological repercussions on a person’s life. Despite extensive efforts devoted to restoring the missing functionality via dexterous myoelectric hand prostheses, natural and robust control usable in everyday life is
still challenging. Novel techniques have been proposed to overcome the
current limitations, among which the fusion of surface electromyography
with other sources of contextual information. We present a dataset to
investigate the inclusion of eye tracking and first person video to provide
more stable intent recognition for prosthetic control. This multimodal
dataset contains surface electromyography and accelerometry of the forearm, and gaze, first person video, and inertial measurements of the head
recorded from 15 transradial amputees and 30 able-bodied subjects performing grasping tasks. Besides the intended application for upper-limb

NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

prosthetics, we also foresee uses for this dataset to study eye-hand coordination in the context of psychophysics, neuroscience, and assistive
robotics.

Background & Summary
The human hand is said to separate man from primates. We use it to sense and
interact with the environment, and it has an essential role in social interactions
and communication. Losing a hand therefore has severe physical and psychological consequences on a person’s life. Myoelectric hand prostheses have enabled
amputees to restore some level of the missing functionality. In their conventional
form, these use surface electromyography (sEMG) from an antagonist pair of
muscles to open and close a simple gripper. A large number of pattern recognition (PR) approaches have been developed over the years aiming to increase the
functionality and control of more dexterous prostheses [51, 10, 27]. Despite these
efforts, PR-based prostheses have not managed to deliver their full potential in
practical clinical applications [42, 47]. One of the primary causes is that they
are sensitive to the variability of sEMG, limiting the robustness and reliability
in long-term practical applications [45, 38, 43, 10, 22, 50, 21, 14]. This limitation
is inherent in the use of sEMG as a control modality, since myoelectric signals
are known to depend on factors such as electrode displacement, inconsistency
of the skin-electrode interface, changes in arm position, and characteristics of
the amputation.
Several techniques have been proposed to address this variability and thus
improve the stability of the prosthetic control [10, 22, 2, 24]. One approach
relies on fusing sEMG with complementary sources of information, such as gaze
and computer vision [9, 13, 17]. Studies on eye-hand coordination have in fact
shown that gaze typically anticipates and guides grasping and manipulation [30,
36, 18]. In other words, humans will look at an object they intend to grasp
before executing the movement itself. Gaze behavior therefore holds valuable
information for recognizing not only the intention to grasp an object, but also
to identify which object the person intends to grasp. The set of likely grasps
can then be estimated based on the size, shape, and affordances of this object.
For large objects, this may be further refined by considering the exact part or
side of the object on which the person fixates.
There are two compelling motivations for using gaze behavior in the context
of upper limb prosthetics. The first is that gaze behavior is not necessarily
affected by the amputation. Any information on muscular activation recorded
from the residual limb, such as sEMG, is inherently degraded due to muscular
reorganization after the amputation and subsequent atrophy due to reduced use.
The second advantage is that integrating some level of autonomy in a prosthesis
can lower the physical and psychological burden placed on its user. Fatigue is
one of the causes of variability in sEMG data, so reducing it can help to stabilize
control.
This paper presents the MeganePro dataset 1 (MDS1) acquired during the

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Figure 1: Overall view of the acquisition setup.
MeganePro project, which was designed based on the experience gained during
the NinaPro project [4]. The goals of the MeganePro project were to investigate
the use of gaze and computer vision to improve the control of myoelectric hand
prostheses for transradial amputees, and to better understand the neurocognitive effects of amputation. This dataset was acquired to explore the first goal:
it contains multimodal data from 15 transradial amputees and 30 able-bodied
subjects while involved in grasping and manipulating objects using 10 common grasps. Throughout the exercise, we acquired sEMG and accelerometry
using twelve electrodes on the forearm. Gaze, first person video, and inertial
measurements of the head were recorded using eye tracking glasses.
In the following sections, we will describe the experimental protocol, the processing procedures, and the resulting data records. Although prosthetic control
was our motivation for acquiring this dataset, it is our belief that the data
will find broader applications. For instance, scientists working in neuroscience,
psychophysics, or rehabilitation robotics may use it to study gaze and manipulation independently as well as via their coordination. In particular, the data
allow direct comparison of able-bodied subjects and transradial amputees in an
identical experimental setting. As such, it may contribute not only to develop
better prostheses, but also a better understanding of human behavior and the
implications of amputation.

Methods
The acquisition of the dataset consisted of defining an experimental protocol
and determining the requirements in terms of devices and software. A general
overview of the setup and protocol is shown in Figure 1. Once the ethical
approval was obtained, we proceeded with subject recruitment. In the following,
all these phases will be described in detail.

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Ethical Requirements
The experiment was designed and conducted in accordance with the principles
expressed in the Declaration of Helsinki. Ethical approval for our study was
requested to and approved by the Ethics Commission of the canton of Valais in
Switzerland and by the Ethics Commission of the Province of Padova in Italy.
Prior to the experiment, each subject was given a detailed written and oral
explanation of the experimental setup and protocol. They were then required
to give informed consent to participate in the research study.

Subject Recruitment
A total of 15 transradial amputees and 30 able-bodied subjects were recruited
for this study. The former group consists of 13 male and 2 female (13.3%) participants with an average age of (47.13 ± 14.16) years. As seen in Table 1, among
them there are several causes for amputation and different preferences with respect of prosthetic use. To remove possible confounding variables, we recruited
a control group of able-bodied participants that matched the former group as
much as possible in terms of age and gender. This group is composed of 27 male
and 3 female (10.0%) subjects, with an average age of (46.63 ± 15.11) years.

Acquisition Setup
We designed an acquisition setup that allowed us to perform eye tracking and to
acquire sEMG from the forearm, while leaving the subjects as free as possible
in their movements. Custom developed software interfaced with the acquisition devices and provided stimuli instructing the subjects when to grasp which
object.
Sensors
The electrical activity of the forearm muscles was recorded with a Delsys Trigno
Wireless sEMG System (Delsys Inc., USA)1 . This system consists of sEMG
electrodes with an inter-sensor latency lower than 500 µs and a signal baseline
noise lower than 750 nV Root Mean Square (RMS). In addition, a three-axial
accelerometer is embedded in each electrode. Up to 16 electrodes communicate
wirelessly with the base station, which is connected through a USB 2.0 cable
to the acquisition laptop. The sEMG data are sampled at 1926 Hz and the
accelerometer at 148 Hz.
The gaze behavior and first person video were recorded with the Tobii Pro
Glasses 2 (Tobii AB, Sweden)2 . The head unit of this device is similar to regular
eyeglasses and equipped with four eye cameras recording the eye movement, a
camera capturing the scene in front of the subject, an Inertial Measurement
Unit (IMU), and a microphone. It weighs only 45 g and corrective lenses can
1 http://www.delsys.com/
2 http://www.tobiipro.com/

ID

Age

Transradial Amputees

101
102
103
104
105
106
107
108
109
110
111
112
113
114
115

52
39
63
49
73
70
36
35
65
38
38
33
28
52
36

Able-bodied Subjects

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40

27
63
49
32
67
68
38
63
55
29
48
62
39
53
29
45
68
62
58
66
39
34
69
57
29
28
31
29
33
29

Handedness Language

Amputation Amputation Years since
Side
Cause
Amputation

M
M
M
M
M
M
M
M
M
M
M
F
M
M
F

right
right
ambidextrous
right
right
left
right
right
right
right
right
right
right
right
right

IT
IT
IT
IT
IT
IT
IT
IT
IT
IT
IT
IT
IT
IT
IT

right
right
right
right
right
left
left
right
left
left
right
left
left
bilateral
left

M
M
M
M
M
M
M
M
M
M
M
M
M
M
M
M
M
M
M
M
M
M
M
M
F
M
M
F
F
M

right
right
right
left
right
right
right
ambidextrous
right
right
left
left
right
right
right
right
right
right
right
right
right
right
right
right
ambidextrous
right
right
right
ambidextrous
right

EN
FR
FR
FR
DE
DE
FR
FR
FR
FR
FR
FR
FR
FR
FR
FR
FR
FR
FR
FR
FR
EN
FR
DE
EN
IT
EN
EN
EN
FR

Gender

electrocution
electrocution
trauma
trauma
trauma
trauma
trauma
trauma
trauma
trauma
trauma
oncological
trauma
trauma
burn

2
4
3
18
6
5
7
9
1
14
10
13
7
35
8

Prosthesis

cosmetic
cosmetic
myoelectric
myoelectric
body-powered
body-powered
body-powered
myoelectric
cosmetic
myoelectric
myoelectric
cosmetic
myoelectric
myoelectric
cosmetic

Residual
Limb
Length [%]

60–80
60–80
60–80
80–100
40–60
80–100
20–40
0–20
80–100
20–40
40–60
60–80
40–60
n/a
n/a

Table 1: Participant characteristics. The table reports the ID of the subject
in the dataset, their age, their gender and their handedness for all the subjects. Clinical parameters about the amputation(s) are also reported for the
transradial amputees. The rightmost column indicates the relative length of
the residual limb with respect to the contralateral limb.

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

be applied for subjects with visual deficit. The eyes are tracked using corneal
reflection and dark pupil methods with automatic parallax and slippage compensation. This eye tracking data are sampled at 100 Hz with a theoretical
accuracy and precision of 0.5◦ and 0.3◦ RMS. Video with Full HD resolution
(1920 px × 1080 px) is recorded at 25 frames per second by the scene camera
with a horizontal and vertical field of view of approximately 82◦ and 52◦ . The
head unit is connected via a cable to a portable recording unit that is responsible for transmitting the data wirelessly and storing them on an memory card.
A rechargeable and replaceable Li-ion battery powers both head and recording units and provides a maximum recording time of approximately 120 min.
The system can quickly and easily be calibrated with a single point calibration
procedure.
Acquisition Software
The role of the acquisition software is to simultaneously acquire and store the
data from all sensor devices, and to guide the subject through the exercises. To
ensure high performance and low latency, the application was developed in C++
and based on the multithreaded producer-consumer pattern as implemented in
CEINMS [40]. More specifically, for each data source there is dedicated producer
thread responsible for acquiring its data, assign a high-resolution timestamp,
and then store these in a queue. Per queue there is at least one consumer
thread that stores available data to a file. The advantage of this architecture
is to uncouple the data acquisition from I/O latencies when saving the data to
persistent storage.
A Graphical User Interface (GUI) developed in Qt53 was added to the application to demonstrate using videos and images how to perform the exercises.
In previous studies, such as NinaPro [3], subjects simply had to mimic a grasp
movement that was shown in a video. This approach is not compatible with
the present study, since it would have influenced the subject’s gaze behavior.
We therefore opted to avoid the need for visual attention and to instruct the
subjects via vocal instructions during the real exercise. These instructions were
automatically synthesized by a text-to-speech engine, which allowed us to prepare instructions in English, Italian, French, and German. At the start of the
exercise subjects could choose the language they were most comfortable with.
Grasp Types and Objects
A total of ten grasp types were selected for our exercise from well-known hand
taxonomies [16, 46, 15, 23] based on their relevance for Activities of Daily Living (ADLs) [8]. These grasp types were then matched with three household
objects each that would regularly be manipulated using a given grasp. An
overview of the ten grasp types and eighteen objects is given in Table 2. When
possible, these pairings were chosen to obtain a many-to-many relationship between objects and grasp types; that is, a grasp could be used with multiple
3 https://www.qt.io/

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

objects and vice versa. This condition helped to limit the possibility that the
mere presence of an object would be sufficient to unequivocally determine the
grasp type.
In the case that an object is not usually found on a table (e.g., a door handle
or a door lock), a custom made support was created. To avoid complications
during the exercise, the key, bulb, lid of the jar, and the screw used with the
screwdriver were modified such that they could not be completely removed from
the support. Furthermore, the pull tab of the can was bent to facilitate the
execution of the grasp.
During the exercise, at least five objects were simultaneously placed in the
scene with a predefined position. The presence of multiple objects in front
of the subjects helped to simulate a real environment, while a standardized
object arrangement minimized the possibility of an error during the placement
of the objects. In addition, these objects were spread throughout the subject’s
reachable workspace so that grasps had to be performed while reaching in various
directions.

Acquisition Protocol
After the ethical requirements were fulfilled, the subject’s personal data, such as
age, gender, height, weight, and handedness were collected. The amputees were
asked additional information regarding the amputation, such as cause, type,
years since amputation, and prosthesis use. A summary of this information is
reported in Table 1.
The subjects were asked to sit with the forearm comfortably leaning on a
desk. The skin was cleaned with isopropyl alcohol and twelve sEMG electrodes
were placed in two arrays around the right forearm or residual limb. An array
of eight electrodes was placed equidistantly around the forearm, starting at
the radio-humeral joint and moving in the direction of pronation. A second
array was located approximately 45 mm more distally and aligned with the
gaps between electrodes one and two, three and four, and so on (see Figure 1).
In addition to the Trigno-specific adhesive strips, a latex-free elastic band was
wrapped around the electrodes to assure a good and reliable interface with the
skin.
The subject was then asked to wear the Tobii Pro glasses, where we made
sure to choose the nose pad that aligned the eye tracking cameras’ appropriately.
Once the subject felt ready, the Tobii Pro glasses were calibrated using the builtin one point target calibration procedure and, in case of success, the subject was
asked to perform a calibration assessment. This assessment consisted in asking
the subject to fixate a black cross against a green background that was displayed
on a monitor at a distance of about 1.3 m. This cross remained fixed for 3 s in
the same location before alternating to another one out of five positions in total.
The actual exercise consisted of repeatedly executing the grasps on the set
of corresponding objects, as described in Table 2. In the first part, which we
will refer to as static, subjects were requested to grasp the objects without actually moving or lifting them. Prior to executing a grasp, videos in first and

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

third person perspectives demonstrated the grasp and the three corresponding
objects. The subjects were however instructed to execute the movements as
naturally as possible, rather than attempting to mimic the exact kinematics of
the demonstration videos. During these videos, the subject was free to decide
whether to perform or not the grasps to get confident about the exercise. Once
the grasp and objects had been shown, the subject had to repeat each combination four times while seated and then another four times while standing. This
change in position was intended to include variability in the limb position in the
data. The duration of the movement and rest periods depends on the selected
language for the vocal instructions. A grasp interval lasted approximately 5.2 s,
5.7 s, 5.9 s, and 6.0 s for English, Italian, French, and German. A rest period
followed for about 4.1 s, 4.7 s, 4.7 s, and 4.7 s for English, Italian, French, and
German. The exact order of the objects within each repetition was randomized
to avoid learning and habituation effects. A vocal command guided the subject
through the exercise, indicating which object to grasp, when to return to the
rest position, and when to stand up or sit. These instructions were accompanied
by a static image of the current grasp type, which was intended as an undistracting reminder of the stimulus. When finishing a grasp series, an experimenter
changed the object scene when necessary before resuming with the next grasp.
The static part of the exercise was followed by a dynamic one. This second
part followed the same structure as the first, but in this case the grasp had
to be used to perform a functional task with an object. The motivation for
this variation was again to introduce variability in the data, but this time by
means of a dynamic, goal-oriented movement. As can be seen in Table 3, these
functional movements reflect common activities with the combination of grasp
type and object, such as drinking from a can held with a medium wrap. To limit
the total duration of the exercise, the ten grasps were only combined with two
objects each and executed either standing or seated. This position was chosen
based on how the action would usually be performed in real life, for instance a
door is more frequently opened while standing.
After the exercise finished the amputated subjects underwent three more
exercises and a comprehensive questionnaire to investigate the neurocognitive
effects of the amputation. The data and results of those exercises and the
questionnaire will be published separately.

Post-Processing
A number of processing steps were applied to the raw data acquired with the
protocol described above. The objective of these steps was to sanitize the data,
synchronize all modalities, and remove identifying information from the videos.
In the following we describe all procedures in detail.
Timestamp Correction
Due to an unfortunate implementation error, during a number of acquisitions
the modalities were assigned timestamps from individual clocks. To unify all

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Object

Object Part

Vocal Instruction

1
2
3

bottle
can
door handle

1
2
3

take the bottle
can
door handle

4
5
24

mug
key
pencil case

23
5
6

handle
key
zip

mug
key
zip

plate
book
drawer

plate
book
drawer

Grasp

bottle
can
door handle

1

medium wrap

2

lateral

3

parallel extension

7
8
9

plate
book
drawer

7
8
9

4

tripod grasp

1
4
9

bottle
mug
drawer

10
4
11

cap
mug
knob

cap of the bottle
mug
knob of the drawer

5

power sphere

12
13
5

ball
bulb
key

12
13
5

ball
bulb
key

ball
light bulb
keys

6

precision disk

15
13
12

jar
bulb
ball

26
13
12

lid
bulb
ball

jar
light bulb
ball

7

prismatic pinch

16
5
2

clothespin
key
can

16
27
25

clothespin
keyring
pull tab

clothespin
keys
can

8

index finger extension

21
18
19

remote
knife
fork

17
18
19

button
knife
fork

9

adducted thumb

20
21
22

screwdriver
remote
wrench

20
21
22

screwdriver
remote
wrench

screwdriver
remote
wrench

prismatic four finger

18
19
22

knife
fork
wrench

18
19
22

knife
fork
wrench

knife
fork
wrench

10

point at a button of the remote
take the knife
fork

Table 2: Overview of the grasp types and objects for the static condition of
the exercise. The columns indicate the ID and name of the grasp as commonly
reported literature [23, 16], the ID and name of the object, and in some cases
a further refinement indicating the ID and name of the part of the object that
was involved in the grasping. The fourth column reports the vocal command
given to the subject.

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Object

Object Part

2
3

can
door handle

2
3

can
door handle

drink from the can
open and close the door handle

standing

5
24

key
pencil case

5
6

key
zip

turn the key in the lock
open and close the pencil case

standing

plate
book

7
8

plate
book

lift the plate
lift the book

standing

bottle
drawer

10
11

cap
knob

open and close the cap of the bottle
open and close the drawer

standing
standing

Grasp
1

medium wrap

2

lateral

Vocal Instruction

Position

3

parallel extension

7
8

4

tripod grasp

1
9

5

power sphere

12
5

ball
key

12
5

ball
key

move the ball to the right and back
move the keys forwards and backwards

6

precision disk

15
13

jar
bulb

26
13

lid
bulb

open and close the lid of jar
screw and unscrew the light bulb

seated

7

prismatic pinch

16
5

clothespin
key

16
27

clothespin
keyring

squeeze the clothespin
move the keys forwards and backwards

seated

8

index finger extension

21
18

remote
knife

17
18

button
knife

press a button on the remote control
cut bread with the knife

seated

9

adducted thumb

20
22

screwdriver
wrench

20
22

screwdriver
wrench

turn the screwdriver
move the wrench to the right and back

seated

prismatic four finger

18
19

knife
fork

18
19

knife
fork

move the knife forwards and backwards
move the fork to the right and back

seated

10

Table 3: Overview of grasp types and objects for the dynamic condition of
the exercise. The first three columns provide information as described in Table 2. The rightmost column indicates whether the movement was executed
while seated or standing.
timestamps in a shared clock, the offset of all clocks was estimated and corrected
with respect to the clock of the sEMG modality using statistics of their relative
timing collected during trial acquisitions. Validations on the remaining unaffected acquisitions confirm that the maximum deviation of our estimate from
the ground truth is less than 12 ms.
sEMG and Accelerometer Data
For computational efficiency, the sEMG and accelerometer streams from the
Delsys Trigno device were acquired and timestamped in batches. During postprocessing, individual timestamps were assigned to each sample via piecewise
linear interpolation. A new piece is created if the linear model would result in
a deviation of more than 100 ms, which may happen if the fit is skewed due to
missing or delayed data.
For the sEMG data, we furthermore filtered outliers by replacing samples
that exceeded 30 standard deviations from the mean within a sliding window
of 1 s with the preceding sample. The signals were subsequently filtered for
power-line interference at 50 Hz (and its harmonics) using a Hampel filter, which
interpolates the spectrum in processing windows only when it detects a clear
peak [1]. Contrary to the more common notch filter, this method does not affect
the spectrum if there is no interference.

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Gaze Data
The data from the Tobii Pro glasses were acquired as individually timestamped
JavaScript Object Notation (JSON) messages. During post-processing, these
messages were decoded and separated based on their type. Messages that arrived
out-of-order were filtered and the resulting set of messages was used to determine
the skew and offset between the computer clock and the one of the Tobii Pro
glasses. This routine removes the constant part of the transmission delay as
much as possible, while avoiding the possibility to antedate any events. The
messages that relate directly to gaze information, such as gaze points, pupil
diameter and so on, were then grouped together based on their timestamps.
Stimulus
The text-to-speech engine that was used to give vocal instructions introduced
noticeable delays in the corresponding changes of the stimulus. We measured
these delays for all sentences and languages, and moved the stimulus changes
forward by these amounts during post-processing. Furthermore, small refinements were made to the object column, whereas its more specific object-part
counterpart was calculated based on a fixed mapping from the original stimulus
information.
Synchronization
For the standard data records (see following section) all modalities were resampled at the original 1926 Hz sampling rate of the sEMG stream. For real-valued
signals, this was done using linear interpolation, while for discrete signals we
used nearest-neighbor interpolation. The signals that indicate the time and index of the MP4 video were handled separately using a custom routine, since
they require to identify the exact change-point where one video transitions to
the next.
Concatenation
The static and dynamic parts of the protocol were acquired independently and
therefore produced separate sets of raw acquisition files. Furthermore, our acquisition protocol and software allowed to interrupt and resume the acquisition,
either at request of the subject or to handle technical problems. After applying the previous processing steps to the individual acquisition segments, they
were concatenated to obtain the standard data record. During this merging, we
incremented the timestamps and video counter to ensure that they are monotonically increasing. Furthermore, if part of the protocol was repeated when
resuming the acquisition we took care to insert the novel segment at exactly the
right place to avoid duplicate data.

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Relabeling
The response of the subject and therefore the sEMG activation may not be
aligned perfectly with the stimulus. As a consequence, the stimulus labels
around the on- or offset of a grasp movement may be incorrect, resulting in an
undue reduction in recognition performance. We addressed this shortcoming by
realigning the stimulus boundaries with the procedure described by Kuzborskij
et al. [35]. In short, this method optimizes the log-likelihood of a rest-grasp-rest
sequence on the whitened sEMG data within a feasible window that spans from
1 s before until 2 s after the original grasp stimulus. As opposed to the uniform prior used in the earlier method, we instead adopted a smoothed variant
of the original stimulus label as prior with p = 0.6. The recalculated stimulus
boundaries have been saved in addition to the original ones.
Removing Identifying Information
All videos were checked manually for identifying information of anyone other
than the experimenters. The segments of video that were marked as privacysensitive were subsequently anonymized with a Gaussian blur. In this procedure, we took care to re-encode only the private segments and to preserve the
exact number and timestamps of all frames. In addition, the audio stream was
removed from all videos for privacy reasons.

Code Availability
The post-processing procedure described above was implemented in Matlab and
executed in compiled form with Matlab 2016b. The relabeling procedure instead
was implemented in Python and interpreted with Python 3.6. Censoring of the
videos was done using a custom Bash script that interfaced with the FFmpeg
4.1.3 video manipulation tool. The GNU Parallel tool was used to run jobs in
parallel [48]. A copy of the code that was used to create the data records from
the raw recordings is publicly available [25].

Data Records
The data that were acquired and processed with the described methodology were
stored in Harvard’s Dataverse [12]. For each subject, this repository contains
two data files in Matlab format and a series of corresponding videos encoded
with MPEG-4 AVC in an MP4 container. The primary data file contains the
concatenated sEMG data at their original sampling rate, to which all other
modalities were resampled. An exhaustive listing of all the fields in these files
is provided in Table 4.
Since resampling may not always be desirable due to its impact on the the
signal spectrum, we also provide an auxiliary data file with all non-sEMG modalities at their original sampling rate. Each of these includes a timestamp column,
which can be used to synchronize them with each other or with the sEMG data

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Field
grasp
grasprepetition
object
objectpart
objectrepetition
position
dynamic
regrasp
regrasprepetition
reobject
reobjectpart
reobjectrepetition
reposition
redynamic
acc
emg
gazepoint
gazepoint_invalid
gazepoint3D
gazepoint3D_invalid
gazedirectionleft
gazedirectionleft_invalid
gazedirectionright
gazedirectionright_invalid
pupilcenterleft
pupilcenterleft_invalid
pupilcenterright
pupilcenterright_invalid
pupildiameterleft
pupildiameterleft_invalid
pupildiameterright
pupildiameterright_invalid
tobiiacc
tobiiacc_invalid
tobiigyr
tobiigyr_invalid
tobiits
vts
mp4videoidx
pts
tspipelineidx
tsvideoidx
ts

Columns
1
1
1
1
1
1
1
1
1
1
1
1
1
1
36
12
2
1
3
1
3
1
3
1
3
1
3
1
1
1
1
1
3
1
3
1
1
1
1
1
1
1
1

Units

g
V

mm

mm
mm
mm
mm
m s−2
◦ −1

s

s
s
s

s

Description
ID of the desired grasp
repetition counter for the desired grasp
ID of the target object
ID of the target object part
repetition counter for the target object
seated or standing position
static or dynamic grasp indicator
realigned ID of the desired grasp
realigned repetition counter for the desired grasp
realigned ID of the target object
realigned ID of the target object part
realigned repetition counter for the target object
realigned seated or standing indicator
realigned dynamic grasp indicator
3-axis acceleration of the 12 electrodes
myoelectric activity of the 12 electrodes
2D gaze point relative to the scene image
invalidity indicator for “gazepoint”
3D gaze point in world coordinates
invalidity indicator for “gazepoint3D”
3D gaze direction of the left eye
invalidity indicator for “gazedirectionleft”
3D gaze direction of the right eye
invalidity indicator for “gazedirectionright”
3D position for the pupil center of the left eye
invalidity indicator for “pupilcenterleft”
3D position for the pupil center of the right eye
invalidity indicator for “pupilcenterright”
pupil diameter of the left eye
invalidity indicator for “pupildiameterleft”
pupil diameter of the right eye
invalidity indicator for “pupildiameterright”
3-axis acceleration of the Tobii
invalidity indicator for “tobiiacc”
3-axis angular velocity of the Tobii
invalidity indicator for “tobiigyr”
timestamp in the Tobii clock
MP4 video timestamp
counter for the MP4 video
TS presentation timestamp
TS pipeline ID
counter for the TS video
timestamp in the computer clock

Table 4: Fields contained in the standard data record.

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

in the primary data file. The fields and their detailed specification is shown in
Table 5.

Technical Validation
Our intended purpose for the dataset is to investigate the fusion of sEMG with
gaze behavior. In this section we therefore concentrate on validating these two
modalities. Some of these analyses are low-level to ensure the quality of the
recorded signals, while others are meant to verify that the data can in fact be
used for the motivations for which it was created.

Gaze Data
Error Validation
The quality of gaze data primarily depends on the correctness of the initial
calibration phase of the Tobii Pro glasses. Validating the effectiveness of this
calibration consists in acquiring gaze data while the user is focused on a known
target and subsequently comparing the measured gaze location with this ground
truth [29, 7]. We used the data recorded during the calibration assessment
described in section Acquisition Protocol to evaluate the effectiveness of the
calibration as well as possible quality degradation. These data were typically
collected at the beginning and end of an exercise. If the exercise was interrupted,
the procedure was shown again before resuming it. We determined the ground
truth by manually locating the cross position in pixels at intervals of 0.2 s using
custom software. Since we also included calibration data for the other exercises
done jointly as part the MeganePro project, a total of 498 acquisitions were
processed in this manner.
The quality of eye tracking is often quantified in terms of accuracy and precision [29, 41, 7]. For each axis, the former measures the systematic error, that
is, the mean offset between the actual and expected gaze locations. Precision,
on the other hand, measures the dispersion around the gaze position and thus
the random error of the gaze point. In Figure 2 these values are visualized with
respect to the location within the video frame. This separation is intentional,
as the eye tracking appears to be more accurate and precise in the center of
the frame, namely (−3.5 ± 19.4) px and (−1.5 ± 29.6) px on the x and y axes.
Moving away from the center, the gaze results systematically shift towards the
borders of the frame and its random error increases. We only visualize regions
where we acquired at least 40 validation samples. Pooling all data, the overall
accuracy and precision is (−0.8 ± 25.8) px and (−9.9 ± 33.6) px on the horizontal
and vertical axes. At a typical manipulation distance of 0.8 m, this corresponds
to a real-world precision and accuracy of approximately (−0.4 ± 11.5) mm and
(−4.4 ± 14.9) mm. This is deemed sufficiently accurate considering the size of
the household objects used in our experimental protocol.
To establish whether the calibration deteriorated over time, we compared
the accuracy and precision collected at the beginning of an acquisition with

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Field
stimulus

acc
gaze

tobiiacc

tobiigyr

vts

pts

Columns
1
2
3
4
5
6
7
8
1
2-37
1
2
3-4
5
6
7-9
10
11
12
13
14
15-17
18
19-21
22
23-25
26
27-29
30
1
2
3
4-6
1
2
3
4-6
1
2
3
4
5
1
2
3
4
5

Description
timestamp in the computer clock
ID of the desired grasp
ID of the target object
repetition counter for the desired grasp
repetition counter for the target object
seated or standing position
ID of the target object part
static or dynamic grasp
timestamp in the computer clock
3-axis acceleration of the 12 electrodes
timestamp in the computer clock
timestamp in the Tobii clock
2D gaze point relative to the scene image
Tobii latency estimate
invalidity indicator for 2D gaze point
3D gaze point in world coordinates
invalidity indicator for 3D gaze point
pupil diameter of the left eye
invalidity indicator for left pupil diameter
pupil diameter of the left eye
invalidity indicator for right pupil diameter
pupil center of the left eye
invalidity indicator for left pupil center
pupil center of the right eye
invalidity indicator for right pupil center
gaze direction of the left eye
invalidity indicator for left gaze direction
gaze direction of the right eye
invalidity indicator for right gaze direction
timestamp in the computer clock
timestamp in the Tobii clock
invalidity indicator for accelerometer data
3-axis acceleration of the Tobii
timestamp in the computer clock
timestamp in the Tobii clock
invalidity indicator for gyroscope data
3-axis angular velocity of the Tobii
timestamp in the computer clock
timestamp in the Tobii clock
invalidity indicator for vts syncronization
MP4 video timestamp
counter for the MP4 video
timestamp in the computer clock
timestamp in the Tobii clock
invalidity indicator for pts syncronization
TS presentation timestamp
TS pipeline ID

Table 5: Fields contained in each acquisition segment of the auxiliary data
record with original sampling.

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Height (px)

0

540

1080

0

960
Width (px)

1920

Figure 2: The accuracy and precision of the eye tracking with respect to the
location within the video frame. For each patch, the shift of the ellipse center
with respect to the cross indicates the accuracy in either axis of the gaze within
that patch. The radii of the ellipse on the other hand indicates the precision.
those taken at the end. In total, we considered 210 uninterrupted acquisitions
in which there was a calibration validation routine both at the beginning and
the end. We found no statistically significant difference in accuracy (sign test,
p = 0.95 and p = 1.0 in the horizontal and vertical axes) or precision (sign test,
p = 0.24 and p = 0.37) indicating that drift does not pose an issue for the gaze
data.
Statistical Parameters
To statistically describe a user’s gaze behavior during the exercises and validate
it against related literature, we first identified fixations and saccades in our eye
tracking data using the Identification Velocity Threshold (IVT) method [44]. To
ensure that we could calculate the angular velocity of both eyes for a maximum
number of samples, we linearly interpolated gaps of missing pupil data when
shorter than 0.075 s [39]. We used a threshold of 70 ◦ /s to discriminate between
fixations and saccades [34]. When the Tobii Pro glasses failed to produce a valid
eye-gaze point, even after interpolating small gaps, the corresponding sample
was marked as invalid. Excluding one subject who had strabismus, the percentage of such invalid samples ranged between (1.7 to 21.0) % and (4.3 to 30.7) %
for able-bodied and amputated subjects. Sequences of events of the same type
were then merged into segments identified by a time range and processed following the approach described by Komogortsev et al. [34]. First, to filter noise
or other disturbances, fixations separated by a short saccadic period of less than
0.075 s and 0.5◦ amplitude are merged. Second, fixations shorter than 0.1 s are

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Subjects
Able-bodied
Amputated

Mean [s]
0.429
0.432

Percentiles [s]
25th 50th 75th
0.170
0.160

0.260
0.240

0.470
0.440

Table 6: Statistical parameters of the duration of fixations for able-bodied and
amputated subjects.
marked as invalid and excluded from the analysis.
In the resulting sequence of gaze events, the majority of invalid data are located between two periods of saccades, namely (92.2 ± 2.7) % and (92.6 ± 3.9) %
for able-bodied and amputated subjects. This indicates that the Tobii Pro
glasses fail predominantly to register high velocity data. Devices with sampling
frequency lower than 250 Hz have indeed been categorized as “fixation pickers” [31] and often do not provide reliable results for saccades. For this reason,
in the following analysis we concentrate instead on fixation events.
Figure 4 shows the distribution of the duration of fixations for both types
of subjects. The characteristics of these distributions, summarized in Table 6,
coincide with those described in analogous studies [30, 33, 19]. For instance, the
mean values are similar to the mean duration of around 0.5 s reported by Hessels
et al. [28]. Moreover, both median duration and the range between the 25th and
75th percentile are comparable with the results of Johansson et al. [30], who
report 0.286 s as median duration and (0.197 to 0.536) s as range between the
same percentiles. These similarities confirm the quality of eye tracking data and
highlight that the subjects maintained a natural gaze behavior throughout the
exercise.
Since one of the intended uses of these data is to investigate gaze behavior in
anticipation of object manipulation, we also verified that subjects indeed looked
at the object when asked to manipulate it. With the help of an automated
approach to detect objects, we determined for each grasp trial whether the
distance between the gaze point and the boundary of the object of interest was
at least once less than 20 px. On average, this was the case in 95.9 % of the
subject’s trials. Manual examination of the remaining 4.1 % indicated that the
fixation exceeded the distance 20 px not because of lack of subject engagement,
but due to a low accuracy calibration of the Tobii Pro glasses. Regardless, in
the vast majority of the trials subjects visually located and fixated the object
prior to its manipulation. For purely illustrative purposes, an example of gaze
behavior of an able-bodied subject while grasping the door handle and the bottle
is given in Figure 3.

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Density

Figure 3: Example of gaze points overlapped onto the scene camera video. Each
circle represents a fixation, where the diameter indicates the duration of each
fixation and the number the order of the fixations. In this case the subject was
asked to grasp the door handle and the bottle.

Able-bodied
Amputated

4
2
0

0

0.2

0.4

0.6
0.8
Time (s)

1

1.2

Figure 4: Distribution of the fixation length histogram for able-bodied (blue)
and amputated (red) subjects. The shaded areas indicate the 10th and 90th
percentiles, while the solid line represents the median.

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

6
4
2
0

300
Frequency (Hz)

PSD (V2 /Hz)

·10−12

0

200 400 600 800
Frequency (Hz)

(a) Power Spectral Density

200
100
0

0

20
40
60
80 100
Exercise duration (%)
(b) Median Frequency

Figure 5: The distribution (a) of the power spectral densities and (b) the median
frequency throughout the duration of the exercise. The solid line indicates the
median over all subjects and electrodes, while the shaded area indicates the 10th
and 90th percentiles.

Myoelectric Signals
Spectral Analysis
To assure the soundness of the recorded sEMG, we first analyzed the spectral
properties and compared these with known results from literature. For each subject and for each channel, we calculated the power spectral density via Welch’s
method with a Hann window of length 1024 (approximately 530 ms) and 50%
overlap). Figure 5a shows the distribution over these densities via its median
and the range between the 10th and 90th percentiles. The first observation is
that nearly all of the energy of the signals is contained within (0 to 400) Hz, as is
typical for sEMG [6]. Furthermore, there is no sign of powerline interference at
50 Hz or its harmonics, confirming the efficacy of the filtering approach detailed
in the previous section.
We do however observe a rather large variability of densities among subjects
and electrodes. The reason is that the spectrum and amplitude of sEMG depends on the exact position of an electrode over a muscle [37]. In our protocol,
none of the electrodes was positioned precisely on a muscle belly, thus causing
a wide variety in the spectrum. In some cases the signal may even be almost
absent (e.g., an electrode over the radial bone).
The same variability is also noticeable in Figure 5b, which reports the distribution of the median frequency over all subjects and electrodes throughout the
entire exercise. The median frequencies we find are close to the approximately
(120 to 130) Hz typically reported for the flexor digitorum superficialis [11, 32],
which is one on the muscles we primarily recorded from with our electrode positioning. Finally, we note that the distribution of the median frequency remains

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Accuracy

100%
82.46

80%
60%

75.1
63.59

78.5

72.31

65.93

Able-bodied
Amputated
LDA
4-TD

k-NN
RMS

KRLS
mDWT
exp-χ2

Figure 6: Classification accuracies for able-bodied and amputated subjects when
predicting the grasp type with three different types of classifiers.
relatively stable over time, indicating that there are no persistent down- or
upward shifts in the spectrum.
Grasp Classification
As a more high-level validation of the sEMG signals, we verify that these can
indeed be used to discriminate the grasp a subject was performing, which is
one of the anticipated applications of this data set. We employ the standard
window-based classification approach described by Englehart and Hudgins [20]
with a window length of 400 samples (approximately 208 ms) and 95% overlap
between successive windows. As feature-classifier combinations we consider:
• a (balanced) Linear Discriminant Analysis (LDA) classifier used with the
popular four time-domain features [20];
• k-Nearest Neighbors (KNN) applied on RMS features [4]; and
• Kernel Regularized Least Squares (KRLS) with a nonlinear exponential χ2
kernel and marginal Discrete Wavelet Transform (mDWT) features [4, 26].
In contrast to prior work where we employed a single train-test split [4, 26],
the classification accuracy is defined here as the average accuracy of 4-fold cross
validation. In each of these folds, one of the four repetitions per grasp-objectposition combination was used as held-out test data, while the remaining three
repetitions formed the training data. Similarly, any hyperparameters were optimized via nested 3-fold cross validation on the train repetitions. For all methods,
the training data were downsampled with a factor 10 for computational reasons,
while the data used for hyperparameter optimization were downsampled with
an additional factor 4.

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

The results in Figure 6 show a median classification accuracy between 63%
and 82% for either able-bodied or amputated subjects, depending on the classification method. This is significantly higher than the approximately 50% accuracy a baseline classifier would achieve by simply predicting the most frequent
rest class, confirming the discriminative power of the sEMG signals for the
grasp type. Although a quantitative comparison with related work is of limited
value due to discrepancies in experimental setup and protocol, the current results are a couple percentage points higher than those presented by Atzori et al.
[4]. The most likely explanation is the lower number of grasps (i.e., only 10
rather than 40), which inevitably boosts performance. As well, for amputees,
the fact that we are considering subjects with different clinical parameters from
previous studies may contribute to influence the performance difference, since
it has been shown that remaining forearm length, phantom limb sensation intensity, and years passed since the amputation can influence the classification
outcome [5].

Usage Notes
Relabeling
The data records come with both the original stimulus as well as a variant that
has been aligned to actual sEMG activation (see section Relabeling). The latter
variant is preferable when the stimulus is used as ground-truth of the grasp
type, such as grasp recognition, since it reduces the number of incorrect labels
due to response times. In other studies one may actually be interested in these
response times, such as when investigating the psychophysical response to the
vocal instruction. In these cases we advise users to use the original stimulus.

Trial Repetitions
In the experimental protocol, each new grasp started with two videos in first and
third person perspective to introduce the subject to the grasp and the objects.
Although subjects were encouraged to practice the grasps during this phase,
some of them did not do this and focused on the video on the computer screen.
For this reason the hand and gaze behavior is unreliable and users of the dataset
are advised to remove movements where the (re)objectrepetition column has a
value of −3 or −2.

sEMG & Accelerometer data
A problem with sEMG and accelerometer data concerned electrode number 8
(the 8th column of the emg variable and the 22nd , 23rd , and 24th columns of
the acc variable). Unfortunately, the data from this electrode seem unreliable.
Myoelectric and accelerometer data recorded by this electrode sometimes have
a substantially lower amplitude than the others, indicating a probable hardware

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

issue. We therefore recommend to carefully consider this aspect when using the
data.
S024 & S039
Due to the difficulties reported in the previous subsection, no myoelectric data
were received from electrode number 8 during these acquisitions. Therefore,
the sEMG and accelerometer data for these subjects were recorded from eleven
electrodes instead of twelve.
S108
The high amputation level of S108 prevented the placement of the second array
of electrodes. Therefore, only the first array consisting of eight electrodes was
placed on the residual limb of this subject.

Gaze
It is known that various factors, such as physical characteristics of the test participant, recording environment, eye tracker features, and quality of calibration
affect the gaze data quality [29]. The experiment was intentionally relatively unconstrained (e.g., the head was not fixed and subjects could move their torso) to
encourage natural behavior. In the following cases we have identified potential
issues with gaze data that may require consideration.
S111
A high percentage (∼ 30%) of invalid gaze data were obtained for this subject.
The gaze data are included in the dataset for completeness.
S114
For this subject, calibration could not be performed due to a strabismus condition. In such cases, the Tobii Pro glasses use a built-in default calibration.
This allowed us to continue the experiment, but the quality of eye tracking is
significantly worse [49]. The difficulty in tracking the subject’s eye in this particular condition is highlighted by the high proportion of invalid gaze samples
(∼ 50%). Although included in the dataset, the use of the gaze data for this
subject should be carefully evaluated.
S115
A specific physical condition of this subject prevented a stable placement of the
Tobii Pro glasses. The slippage compensation of the device may have helped
to limit the consequences, as the proportion of invalid gaze samples (∼ 10%)
indicates a proper tracking of the eye. This condition should nonetheless be
taken into account when analyzing the gaze and Tobii Pro glasses’ IMU data
for this subject.

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Video
MPEG transport stream (MPEG-TS) videos were recorded wirelessly via the
Tobii Pro glasses Application Programming Interface (API). The variables pts,
tspipelineidx, and tsvideoidx of the data records serve to synchronize the data
with these videos. We however release only the MP4 videos, this for two main
reasons: to limit the size of the dataset and to ensure a high quality of the scene
camera videos, since the MP4 videos were immune to packet loss problems.

Communication
Being able to select among four languages allowed the subjects to choose the
one they were most comfortable with and, in most case, it corresponded to
their native language. For S111 only, none of the languages available in the
acquisition software were appropriate. We were however able to communicate
in Italian and, in case of difficulties, communication was facilitated by a relative
of the subject translating from Italian to the subject’s native language.

Acknowledgements
This work was supported by the Swiss National Science Foundation Sinergia
project #160837 “MeganePro” and by the Hasler Foundation in the project ELGAR PRO. We thank the subjects for their participation in the Myo-Electricity,
Gaze And Artificial-intelligence for Neurocognitive Examination & Prosthetics (MeganePro) project. We thank Prof. G. Pajardi, Dr. E. Rosanda and the
team of the hand surgery and rehabilitation department of the Ospedale San
Giuseppe MultiMedica Group in Milan, Italy for their help in recruiting subjects
for this study. Furthermore, we thank Mara Graziani, Francesca Giordaniello,
and Francesca Palermo for their help in developing a preliminary version of the
acquisition protocol and testing the acquisition software.

Author contributions
MC created the acquisition protocol, developed the acquisition software,
performed data acquisition, and wrote the paper.
AG created the acquisition protocol, developed the post-processing procedure and software, performed data analysis, and wrote the paper.
VG created the acquisition protocol, performed data analysis, and wrote the
paper.
GS helped with the creation of the acquisition protocol and with the data
acquisition, obtained ethical approval, and reviewed the paper.
KG recruited the participants, helped with the data acquisition, checked the
videos for private data, and reviewed the paper.
A-G MH recruited the participants and reviewed the paper.

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

AG helped with the post-processing software, performed data analysis, and
reviewed the paper.
DF recruited the participants, helped with the data acquisition and reviewed
the paper.
CT recruited the participants and reviewed the paper.
FB recruited the participants and reviewed the paper.
BC ideated the project and reviewed the paper.
PB ideated the project and reviewed the paper.
MA ideated the project, created the acquisition protocol, obtained ethical
approval, helped with the data acquisition, and reviewed the paper.
HM ideated the project and reviewed the paper.

Competing financial interests
The authors declare no competing financial interests.

References
[1] David P. Allen. A frequency domain hampel filter for blind rejection of sinusoidal interference from electromyograms. Journal of Neuroscience Methods, 177(2):303–310, 2009. ISSN 0165-0270. doi: 10.1016/
j.jneumeth.2008.10.019.
[2] Manfredo Atzori and Henning Müller. Control Capabilities of Myoelectric
Robotic Prostheses by Hand Amputees: A Scientific Research and Market
Overview. Frontiers in systems neuroscience, pages 1–13, 2015.
[3] Manfredo Atzori, Arjan Gijsberts, Claudio Castellini, Barbara Caputo,
Anne-Gabrielle Mittaz Hager, Simone Elsig, Giorgio Giatsidis, Franco
Bassetto, and Henning Müller. Electromyography data for non-invasive
naturally-controlled robotic hand prostheses. Scientific Data, 1, 12 2014.
doi: 10.1038/sdata.2014.53. Data Descriptor.
[4] Manfredo Atzori, Arjan Gijsberts, Henning Müller, and Barbara Caputo.
Classification of hand movements in amputated subjects by semg and accelerometers. In 2014 36th Annual International Conference of the IEEE
Engineering in Medicine and Biology Society, pages 3545–3549, 8 2014. doi:
10.1109/EMBC.2014.6944388.
[5] Manfredo Atzori, Arjan Gijsberts, Claudio Castellini, Barbara Caputo,
Anne-Gabrielle Mittaz Hager, Elsig Simone, Giorgio Giatsidis, Franco Bassetto, and Henning Müller. Clinical parameter effect on the capability to
control myoelectric robotic prosthetic hands. Journal of Rehabilitation Research and Development, 53(3):345–358, 2016.
[6] John V. Basmajian and Carlo J. De Luca. Muscles alive. Williams &
Wilkins, Baltimore, 5th ed. edition, 1985.

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

[7] Pieter Blignaut and Daniël Wium. Eye-tracking data quality as affected
by ethnicity and experimental design. Behavior research methods, 46(1):
67–80, 2014.
[8] Ian M. Bullock, Joshua Z. Zheng, Sara De La Rosa, Charlotte Guertler,
and Aaron M. Dollar. Grasp frequency and usage in daily household and
machine shop tasks. IEEE Transactions on Haptics, 6(3):296–308, 2013.
ISSN 19391412. doi: 10.1109/TOH.2013.6.
[9] Claudio Castellini and Giulio Sandini. Gaze tracking for robotic control in
intelligent teleoperation and prosthetics. In Proceedings of {COGAIN} Communication via Gaze Interaction, pages 73–77, 2006. doi: 10.13140/
2.1.4591.3285.
[10] Claudio Castellini, Panagiotis Artemiadis, Michael Wininger, Arash
Ajoudani, Merkur Alimusaj, Antonio Bicchi, Barbara Caputo, William
Craelius, Strahinja Dosen, Kevin Englehart, Dario Farina, Arjan Gijsberts, Sasha B. Godfrey, Levi Hargrove, Mark Ison, Todd Kuiken, Marko
Marković, Patrick M. Pilarski, Rüdiger Rupp, and Erik Scheme. Proceedings of the first workshop on peripheral machine interfaces: Going beyond
traditional surface electromyography. Frontiers in Neurorobotics, 8(Aug.):
1–17, 2014. ISSN 16625218. doi: 10.3389/fnbot.2014.00022.
[11] Edward A. Clancy, Mark V. Bertolina, Roberto Merletti, and Dario Farina.
Time- and frequency-domain monitoring of the myoelectric signal during
a long-duration, cyclic, force-varying, fatiguing hand-grip task. Journal of
Electromyography and Kinesiology, 18(5):789 – 797, 2008. ISSN 1050-6411.
doi: https://doi.org/10.1016/j.jelekin.2007.02.007.
[12] Matteo Cognolato, Arjan Gijsberts, Valentina Gregori, Gianluca Saetta,
Katia Giacomino, Anne-Gabrielle Mittaz Hager, Andrea Gigli, Diego Faccio, Cesare Tiengo, Franco Bassetto, Barbara Caputo, Peter Brugger, Manfredo Atzori, and Henning Müller. MeganePro dataset 1 (MDS1), 2019. The
dataset will be released once this manuscript is accepted in a peer-reviewed
scientific journal (it is currently submitted to Scientific Data).
[13] Elaine A. Corbett, Konrad P. Kording, and Eric J. Perreault. Real-time
fusion of gaze and emg for a reaching neuroprosthesis. In 2012 Annual
International Conference of the IEEE Engineering in Medicine and Biology
Society, pages 739–742, Aug. 2012. doi: 10.1109/EMBC.2012.6346037.
[14] Francesca Cordella, Anna Lisa Ciancio, Rinaldo Sacchetti, Angelo Davalli,
Andrea Giovanni Cutti, Eugenio Guglielmelli, and Loredana Zollo. Literature Review on Needs of Upper Limb Prosthesis Users. Frontiers in Neuroscience, 10:209, may 2016. ISSN 1662-453X. doi: 10.3389/fnins.2016.00209.
[15] Beau Crawford, Kai Miller, Pradeep Shenoy, and Rajesh Rao. Real-Time
Classification of Electromyographic Signals for Robotic Control. In Proceedings of AAAI, pages 523–528, 2005.

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

[16] Mark R. Cutkosky. On grasp choice, grasp models, and the design of hands
for manufacturing tasks. IEEE Transactions on Robotics and Automation,
5(3):269–279, 1989.
[17] Andrea D’Avella and Francesco Lacquaniti. Control of reaching movements
by muscle synergy combinations. Frontiers in Computational Neuroscience,
7(Apr.):1–7, 2013. ISSN 1662-5188. doi: 10.3389/fncom.2013.00042.
[18] Loni Desanghere and Jonathan J. Marotta. The influence of object shape
and center of mass on grasp and gaze. Frontiers in Psychology, 6(1537),
Oct. 2015. ISSN 1664-1078. doi: 10.3389/fpsyg.2015.01537.
[19] Andrew T. Duchowski. Eye Tracking Methodology: Theory and Practice.
Springer-Verlag, Berlin, Heidelberg, 2003.
[20] Kevin Englehart and Bernard Hudgins. A robust, real-time control scheme
for multifunction myoelectric control. IEEE Transactions on Biomedical Engineering, 50(7):848–854, 2003. ISSN 0018-9294. doi: 10.1109/
TBME.2003.813539.
[21] Dario Farina and Sebastian Amsüss. Reflections on the present and future
of upper limb prostheses. Expert Review of Medical Devices, 13(4):321–324,
apr 2016. ISSN 1743-4440. doi: 10.1586/17434440.2016.1159511.
[22] Dario Farina, Ning Jiang, Hubertus Rehbaum, Ales Holobar, Bernhard
Graimann, Hans Dietl, and Oskar C. Aszmann. The Extraction of Neural
Information from the Surface EMG for the Control of Upper-Limb Prostheses: Emerging Avenues and Challenges. IEEE Transactions on Neural
Systems and Rehabilitation Engineering, 22(4):797–809, Jul. 2014. ISSN
1534-4320. doi: 10.1109/TNSRE.2014.2305111.
[23] Thomas Feix, Javier Romero, Heinz-Bodo Schmiedmayer, Aaron M. Dollar,
and Danica Kragic. The GRASP Taxonomy of Human Grasp Types. IEEE
Transactions on Human-Machine Systems, 46(1):66–77, feb 2016. ISSN
2168-2291. doi: 10.1109/THMS.2015.2470657.
[24] Ghazal Ghazaei, Ali Alameer, Patrick Degenaar, Graham Morgan, and
Kianoush Nazarpour. Deep learning-based artificial vision for grasp classification in myoelectric hands. Journal of Neural Engineering, 14(3):036025,
jun 2017. ISSN 1741-2560. doi: 10.1088/1741-2552/aa6802.
[25] Arjan Gijsberts. MeganePro Script Dataset (MDSScript), 2019. The
dataset will be released once this manuscript is accepted in a peer-reviewed
scientific journal (it is currently submitted to Scientific Data).
[26] Arjan Gijsberts, Manfredo Atzori, Claudio Castellini, Henning Müller, and
Barbara Caputo. Movement error rate for evaluation of machine learning
methods for semg-based hand movement classification. IEEE Transactions
on Neural Systems and Rehabilitation Engineering, 22(4):735–744, 7 2014.
ISSN 1534-4320. doi: 10.1109/TNSRE.2014.2303394.

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

[27] Maria Hakonen, Harri Piitulainen, and Arto Visala. Current state of digital
signal processing in myoelectric interfaces and related applications. Biomedical Signal Processing and Control, 18:334–359, 2015. ISSN 17468108. doi:
10.1016/j.bspc.2015.02.009.
[28] Roy S. Hessels, Diederick C. Niehorster, Chantal Kemner, and Ignace T.C.
Hooge. Noise-robust fixation detection in eye movement data: Identification
by two-means clustering (i2mc). Behavior research methods, 49(5):1802–
1823, 2017.
[29] Kenneth Holmqvist, Marcus Nyström, and Fiona Mulvey. Eye tracker data
quality: What it is and how to measure it. Eye Tracking Research and
Applications Symposium (ETRA), 03 2012.
[30] Roland S. Johansson, Göran Westling, Anders Bäckström, and J. Randall
Flanagan. Eye–hand coordination in object manipulation. Journal of Neuroscience, 21(17):6917–6932, 2001.
[31] Keith S Karn. “saccade pickers” vs.“fixation pickers”: the effect of eye
tracking instrumentation on research. In Proceedings of the 2000 symposium
on Eye tracking research & applications, pages 87–88. ACM, 2000.
[32] Shashikala Kattla and Madeleine M. Lowery. Fatigue related changes in
electromyographic coherence between synergistic hand muscles. Experimental brain research, 202(1):89–99, 2010.
[33] Thomas Kinsman, Karen Evans, Glenn Sweeney, Tommy Keane, and Jeff
Pelz. Ego-motion compensation improves fixation detection in wearable eye
tracking. Eye Tracking Research and Applications Symposium (ETRA), 03
2012.
[34] Oleg V. Komogortsev, Denise V. Gobert, Sampath Jayarathna, Do Hyong Koh, and Sandeep M. Gowda. Standardization of automated analyses of oculomotor fixation and saccadic behaviors. IEEE Transactions on
Biomedical Engineering, 57(11):2635–2645, 2010.
[35] Ilja Kuzborskij, Arjan Gijsberts, and Barbara Caputo. On the challenge of classifying 52 hand movements from surface electromyography.
In Annual International Conference of the IEEE Engineering in Medicine
and Biology Society (EMBC), pages 4931–4937, 8 2012. doi: 10.1109/
EMBC.2012.6347099.
[36] Michael F. Land. Eye movements and the control of actions in everyday
life. Progress in Retinal and Eye Research, 25(3):296–324, may 2006. ISSN
13509462. doi: 10.1016/j.preteyeres.2006.01.002.
[37] Carlo J. De Luca. The use of surface electromyography in biomechanics.
Journal of applied biomechanics, 13(2):135–163, 1997.

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

[38] Ning Jiang, Strahinja Dosen, K-R Muller, and Dario Farina. Myoelectric
Control of Artificial Limbs—Is There a Need to Change Focus? [In the
Spotlight]. IEEE Signal Processing Magazine, 29(5):152–150, Sep. 2012.
ISSN 1053-5888. doi: 10.1109/MSP.2012.2203480.
[39] Anneli Olsen. The tobii i-vt fixation filter. Tobii Technology, 2012.
[40] Claudio Pizzolato, David G. Lloyd, Massimo Sartori, Elena Ceseracciu,
Thor F. Besier, Benjamin J. Fregly, and Monica Reggiani. CEINMS: A
toolbox to investigate the influence of different neural control solutions
on the prediction of muscle excitation and joint moments during dynamic
motor tasks. Journal of Biomechanics, 48(14):3929–3936, Nov. 2015. ISSN
00219290. doi: 10.1016/j.jbiomech.2015.09.021.
[41] Eyal M. Reingold. Eye tracking research and technology: Towards objective
measurement of data quality. Visual cognition, 22(3-4):635–652, 2014.
[42] Linda Resnik, He Helen Huang, Anna Winslow, Dustin L Crouch, Fan
Zhang, and Nancy Wolk. Evaluation of emg pattern recognition for upper
limb prosthesis control: a case study in comparison with direct myoelectric
control. Journal of neuroengineering and rehabilitation, 15(1):23, 2018.
[43] Aidan D. Roche, Hubertus Rehbaum, Dario Farina, and Oskar C. Aszmann.
Prosthetic Myoelectric Control Strategies: A Clinical Perspective. Current
Surgery Reports, 2(3):44, Mar. 2014. ISSN 2167-4817. doi: 10.1007/s40137013-0044-8.
[44] Dario D. Salvucci and Joseph H. Goldberg. Identifying fixations and saccades in eye-tracking protocols. In Proceedings of the 2000 Symposium on
Eye Tracking Research & Applications, ETRA ’00, pages 71–78, New York,
NY, USA, 2000. ACM. ISBN 1-58113-280-8. doi: 10.1145/355017.355028.
[45] Erik Scheme and Kevin Englehart. Electromyogram pattern recognition for
control of powered upper-limb prostheses: State of the art and challenges
for clinical use. Journal of Rehabilitation Research and Development, 48
(6):643–660, 2011. ISSN 0748-7711. doi: 10.1682/JRRD.2010.09.0177.
[46] Fredrik C. P. Sebelius, Birgitta N. Rosen, and Göran N. Lundborg. Refined myoelectric control in below-elbow amputees using artificial neural
networks and a data glove. Journal of Hand Surgery, 30(4):780–789, 2005.
[47] Ann Simon, Kristi Turner, Laura Miller, Levi Hargrove, and Todd Kuiken.
Pattern recognition and direct control home use of a multi-articulating hand
prosthesis. In 2019 IEEE 15th International Conference on Rehabilitation
Robotics (ICORR). IEEE, 7 2019.
[48] Ole Tange. Gnu parallel - the command-line power tool. ;login: The
USENIX Magazine, 36(1):42–47, Feb 2011. doi: http://dx.doi.org/10.5281/
zenodo.16303. URL http://www.gnu.org/s/parallel.

medRxiv preprint doi: https://doi.org/10.1101/19010199; this version posted October 29, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

[49] Tobii AB. Tobii Pro Glasses 2 SDK Developer’s Guide, 2015.
[50] Ivan Vujaklija, Dario Farina, and Oskar Aszmann. New developments in
prosthetic arm systems. Orthopedic Research and Reviews, Volume 8:31–39,
Jul 2016. ISSN 1179-1462. doi: 10.2147/ORR.S71468.
[51] Massimiliano Zecca, Silvestro Micera, M. C. Carrozza, and Paolo Dario.
Control of Multifunctional Prosthetic Hands by Processing the Electromyographic Signal. Critical Reviews in Biomedical Engineering, 30(4-6):459–
485, 2002. ISSN 0278-940X. doi: 10.1615/CritRevBiomedEng.v30.i456.80.

