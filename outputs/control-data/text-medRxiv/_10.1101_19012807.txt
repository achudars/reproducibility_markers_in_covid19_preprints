medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

1

Accurate influenza forecasts using type-specific incidence data for small geographical

2

units

3

James Turtle1, Pete Riley1, Michal Ben-Nun1, Steven Riley1,2

4

1. Predictive Science Inc, CA, USA

5

2. MRC Centre for Outbreak Analysis and Modelling, Department of Infectious Disease

6
7

Epidemiology, School of Public Health, Imperial College London, UK
Email address for correspondence: s.riley@imperial.ac.uk

NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.

1

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

8

Abstract

9

Influenza incidence forecasting is used to facilitate better health system planning and could

10

potentially be used to allow at-risk individuals to modify their behavior. For example, the US

11

Centers for Disease Control and Prevention (CDC) runs an annual competition to forecast

12

influenza-like illness (ILI) at the regional and national levels in the US, based on a standard

13

discretized incidence scale. Here, we use a suite of forecasting models to analyze type-

14

specific incidence at the smaller spatial scale of clusters of nearby counties. We used data

15

from point-of-care (POC) diagnostic machines over three seasons, in 10 clusters, capturing:

16

57 counties; 1,061,891 total specimens; and 173,909 specimens positive for Influenza A.

17

Total specimens were closely correlated with comparable CDC ILI data. Mechanistic models

18

were substantially more accurate when forecasting influenza A positive POC data than total

19

specimen POC data, especially at longer lead times. Also, models that fit individual counties

20

separately were better able to forecast clusters than were models that directly fit to

21

aggregated cluster data. Public health authorities may wish to consider developing

22

forecasting pipelines for type-specific POC data in addition to ILI data. Simple mechanistic

23

models will likely improve forecast accuracy when applied at small spatial scales to

24

pathogen-specific data before being scaled to larger geographical units and broader

25

syndromic data. Highly local forecasts may enable new public health messaging to

26

encourage at-risk individuals to temporarily reduce their social mixing.

2

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

27

Introduction

28

Influenza infections cause substantial morbidity and mortality across all geographical areas

29

and sociodemographic groups (1). Forecasting pipelines -- of data and associated analytics -

30

- can create knowledge of current and likely future incidence and therefore have high

31

potential value to individuals and health planners (2), potentially leading to at-risk individuals

32

choosing to modify their behaviour to reduce their individual risk when incidence is high (3).

33

Also, health systems could more efficiently manage scarce resources such as intensive care

34

services that are impacted by local influenza incidence but also utilized by many different

35

healthcare pathways (4). More generally, seasonal influenza is an ideal case study for

36

influenza pandemics, with forecasting during influenza pandemics and other global health

37

outbreaks is a key priority to support rapid investment decisions and optimization of available

38

resources (5, 6).

39

Current forecasting pipelines for influenza have focused on the use of syndromic data for

40

large geographically diffuse populations (7). For example, the US Centers for Disease

41

Control and Prevention (CDC) forecasting challenge produces public forecasts of influenza-

42

like illness rates at regional and national scales for the US, based on a long-running national

43

syndromic surveillance system that has proven robust during a pandemic (8). Participating

44

teams use a variety of methods. Ensembles of fully automated models (2) have been shown

45

to improve average performance over individual statistical (9) and mechanistic models (10)

46

but do not yet outperform crowd-sourced expert opinion (11) for key forecasting targets (7).

47

Here, we consider the possibility that alternate data streams may be much more closely

48

related to the underlying biology and hence be inherently more forecastable using

49

mechanistic models (10). Specifically, that more local and pathogen-specific data may permit

50

more accurate forecasting pipelines with mechanistic models than are currently possible

51

using more geographically aggregated syndromic data (7) or low resolution type-specific data

52

cross references with local syndromic proxy data (12). We apply a set of previously validated

53

models (10) to incidence data from geolocated point-of-care diagnostic devices (13), and use

54

a skill scoring system that is analogous to that used thus far (14, 15) for the CDC prospective

55

competition to compare the forecastability of alternate data streams.

56

Results

57

We obtained 3,258,166 specimen results from 12,227 point-of-care (POC) machines in the

58

United States starting from week 27, 2016 through week 26, 2019. These data were down-

59

selected to obtain clusters of nearby counties with at least 250 tested specimens in any

60

season, resulting in 10 clusters, capturing: 57 counties (Table S2); 1,061,891 total
3

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

61

specimens; and 173,909 specimens positive for Influenza A (Figure 1, see Methods). A

62

cluster containing metropolitan Atlanta was just below the average season requirements for

63

2016-17 but had very high coverage for 2017-18 and 2018-19 and was thus included.

64

Total specimens (TS) and specimens positive for influenza A (PA) both showed broadly

65

similar dynamics, however, there were some intriguing differences between them (Figure 1).

66

As would be expected, the more specific PA data had a clearer difference between off-

67

season and in-season. Also, regardless of noise during periods of low activity, the county-

68

level PA time series produced sharper epidemics, with 6.05 weeks average duration at or

69

above half maximum incidence compared with 8.84 weeks for the TS data. For clusters, the

70

peaks became slightly wider with a width at half peak for PA averaging 6.48 weeks and TS

71

averaging 9.57 weeks. The TS data showed a high correlation with influenza-like illness data

72

for the enclosing CDC region (overall average Pearson coefficient 0.96, Fig S1, Table S1).

73

When averaging across all clusters, at all forecast weeks, for all years; we found substantial

74

evidence that PA data could be more accurately forecast than could TS data (Figs 2, S2, S3,

75

S4). We assessed accuracy using a similar scoring system as that used for influenza by the

76

US CDC (7). The historical maximum range was separated into 131 bins and we used our

77

model to assign a probability to every possible category. Forecast skill was defined as the

78

sum of probabilities assigned to the eventually observed bin and the 5 immediately lower and

79

higher bins (for a total of 11 bins). Forecast score was the natural log of forecast skill. For

80

example, if we assigned a total probability of 20% to bins in the skill window, then the score

81

would be ln(0.2) = -1.6. The best model for 2-week ahead forecasts scored an average of -

82

1.41 for PA data compared with -2.55 for TS data. Although the accuracy of the forecast did

83

vary over the season, the superiority of best fits to PA data over TS data was maintained

84

across the whole season. In fact, the accuracy of the best fit model for PA data with a 10-

85

week lead time was still better than that of the TS data at a 2-week lead time.

86

We considered specific examples to understand how and why the models were better able to

87

fit these data in different scenarios. For example, after testing all forecast models at all

88

forecast times for the Colorado cluster for 2016-17, the best fitting model was better able to

89

capture the rapid acceleration of incidence and the shape of the peak PA data stream than

90

was possible for the best fitting model for the TS data (Fig 3 A and B). Despite not being able

91

to accurately fit the decay phase of the epidemic, the best fitting model for the PA data still

92

achieved a substantially better score of -0.51 than the score of -0.82 that was possible for the

93

corresponding TS data. We note that a score of -0.51 corresponds to 60% of forecasts falling

94

within the skill window. However, for the same cluster for the following year, the same did not

95

apply. The best model fits to both datasets were substantially worse than the year before,
4

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

96

largely due to the cluster exhibiting a double peak (Fig 3 C and D). The average single-peak

97

line through the slightly diffuse TS data performed slightly better than did a similar line

98

through the more sharply peaked PA data. We note that our data and models do not

99

distinguish between subtypes of influenza A and that both H1N1 and H3N2 were circulating

100

during similar periods in this season.

101

Among the mechanistic models, coupled models generally performed better than uncoupled

102

models or models that fit the cluster incidence directly (Fig 2). Also, the accuracy of

103

mechanistic models relative to non-mechanistic average-based models compared for TS and

104

PA data suggests that mechanistic models are able to capture more information from the PA

105

data. For the TS data, the 5-week rolling historical average model outperformed the best

106

mechanistic models for all time scales from 2- to 10-weeks. However, for PA data, even at a

107

10-week lead time, the best mechanistic model substantially outperformed the 5-week

108

average historical model. We note that there were few years available to the historical

109

models in these data, therefore the performance of historical-average models would likely

110

improve over time as the training data set grows.

111

Patterns in the posterior densities for key mechanistic parameters also suggest that when the

112

models are fit to the PA data they are capturing more of the underlying biology (Fig 4). The

113

posterior median for the basic reproductive number R0 was consistent from year to year and

114

location to location when fit to the PA data. For coupled models, cluster R0s were higher

115

(mean 1.321; 95% CI [1.317, 1.324]) than for uncoupled models (mean 1.176; 95% CI

116

[1.174, 1.177]). Given that transmissibility is reflected by R0-1, this difference is substantial

117

and likely reflects the need for uncoupled models to have longer durations (resulting from

118

lower R0) because the overall cluster incidence was not driven by epidemics in

119

subpopulations taking off at different times. This theory is supported by an inverse

120

relationship for the estimated proportion of infections which result in clinical cases pC.

121

Estimates of pC were lower for coupled models than for uncoupled models because

122

subpopulations with higher R0s require fewer of their infections to result in cases to achieve

123

the same level of incidence as those from the uncoupled model. Despite the coupled model

124

outperforming the uncoupled model, estimates for the parameters that determined the

125

strength of coupling were not well constrained when fit to the data (Fig S5).

126

Discussion

127

We have shown that for a large set of mechanistic models, using a skill scoring system

128

directly analogous to current best practise, cluster-level influenza incidence data based on

129

type-confirmed test results are substantially better forecast than are data based only on the

130

volume of respiratory samples being tested. We have also shown a close correlation
5

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

131

between the number of respiratory samples being taken in each cluster and the pattern of

132

influenza-like illness reported to the national surveillance system for the geographical region

133

enclosing each cluster. Among these mechanistic models, those that fit to subpopulation

134

county data in order to forecast cluster patterns are more accurate than those that do not.

135

Also, the relative difference between the mechanistic models and simple historical average

136

models is much greater when forecasting the type-specific data than when forecasting total

137

samples.

138

A limitation of our study is that we compared only forecasts from our suite of mechanistic and

139

historical average models against the two datasets. Therefore, it is possible that other

140

approaches that we did not try may have been able to forecast the TS data more effectively

141

than the PA data. However, the difference in our ability to forecast the two types of data was

142

considerable and significantly greater than the difference in performance between our

143

approach and other high performing participants in the CDC challenge study, which is

144

measured on a similar scale (7). Also, intuitively, the shape of epidemics in the PA data was

145

more consistent with a short-generation-time infectious disease epidemic: the degree of

146

noise prior to the start of the season was low and the epidemic itself was more peaked, as

147

measured by width at half maximum, suggesting that the substantial improvement in

148

forecastability using a mechanistic approach is consistent with the PA data being much more

149

representative of the underlying biological process.

150

A previous study has compared the forecastability of pathogen-specific data and found

151

evidence of slightly increased forecastability of local type- and subtype-specific data, albeit

152

using different accuracy metrics (12). However, the virological data available to the prior

153

study represented a considerably higher spatial units than the separate ILI data. More local

154

sub-type epidemics were inferred by combining regional laboratory-confirmed positive

155

proportions with Google Flu Trends (16) estimates of state and municipality ILI data, implicitly

156

assuming that the temporal distribution of types and subtypes was uniform for large

157

geographical regions. We suggest that aggregated positive tests at the geographical and

158

temporal scales of analysis contain maximal information that is not present in similar inferred

159

data streams.

160

Our results suggest a number of additional refinements to our forecasting workflow that may

161

further improve model forecast accuracy. We assumed that each cluster consisted only of

162

the sub-populations for which data were available. It may be that the inclusion of

163

epidemiological ‘dark matter’ -- the intervening and surrounding populations that were not

164

observed -- could increase accuracy. This would substantially increase the computational

165

complexity of the approach, but could potentially fill-in surveillance gaps at the same time as
6

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

166

increasing the accuracy and robustness of the forecasts. Also, we have treated these as

167

weekly data, which reflects their current reporting pattern. However, accuracy for now-

168

casting and short term forecasting could be improved substantially by looking at daily

169

incidence which are available from this surveillance network. Although some work would be

170

required to make a day-of-week adjustment (17), accuracy of 1-week ahead and 2-week

171

ahead forecasts would likely increase substantially over and above the gains described here.

172

Public health authorities may wish to consider the routine use of type- and subtype-specific

173

point-of-care data for influenza surveillance in addition to traditional ILI surveillance to enable

174

more accurate local influenza forecasts during seasonal and pandemic periods. Our results

175

were for 10 geographical clusters based only on three seasons worth of data (so a maximum

176

of two training years). Very soon, the network of machines from which these data were

177

obtained and other similar networks will have much more complete geographical and

178

temporal coverage (18). Similar networks are being established in many other populations.

179

While solely laboratory-based virological data have historically suffered from dramatic

180

changes in testing volumes at the start of a pandemic, there is no reason to believe that the

181

same changes will occur in these largely non-hospital point-of-care machines: or that any

182

such changes should they occur would be worse than those that occur in influenza-like

183

illness surveillance networks.

184

The accuracy of retrospective forecasts presented here extends prior understanding of local

185

dynamics of the transmission of influenza, and suggests plausible novel public health

186

strategies. Other studies have shown that aggregating local forecasts can improve accuracy

187

over direct forecasts for larger geographical units, but have not focussed on the potential

188

accuracy at small spatial scales (10, 19). Also, more descriptive ecological analyses have

189

demonstrated the potential for improved local forecasts by characterizing how humidity and

190

population density are correlated with average properties of epidemic curves (20). Building

191

on these prior studies, the smaller spatial unit and temporal accuracy of the results presented

192

here could be used as a public health intervention to give individuals the opportunity to

193

modify their behaviour over short periods of time, based on very local information. In

194

particular, people at high risk of severe disease may choose to reduce their social mixing

195

during short periods where the risk of infection locally can be accurately predicted to be high.

196

Materials and Methods

197

Deployment of test machines and capture of test data

198

Our data are gathered from a near-real time network of benchtop Sofia and Sofia 2 POC

199

diagnostic machines (Quidel, San Diego, USA). At the start of the study period in July 2016,
7

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

200

the network consistent of 1676 machines located in physician offices (880), hospitals (257),

201

urgent care clinics (427) and other/not reported (112) settings. At the end of the study period

202

the network had grown to 12345 machines with 5,736 in physician offices, 1,206 in hospitals,

203

1,194 in urgent care clinics, and 4,209 in other/not reported settings. The standard kit tests

204

for influenza A and B. Data are automatically transmitted to Quidel. Reporting cadence can

205

vary depending on device connectivity, but most devices report results within a day of

206

testing.

207

Selection of counties and clusters

208

Individual strain tests were sorted into weekly totals for each county represented in the

209

dataset. Of the approximately 3240 counties and county equivalents in the United States,

210

1258 are represented in this dataset. However, to omit sparse data streams, counties that

211

did not report for at least 40 weeks in both the 2016-17 and 2017-2018 seasons were

212

discarded. Next, counties with less than 250 specimens tested in any single season of 2016-

213

17, 2017-18, or 2018-2019 were discarded. These requirements reduced the number of

214

considered counties from 1258 to 136. This county filtering process was based on data

215

reported up to and including week 3 of 2019.

216

To identify localized groups within the 136 qualifying counties, we applied a k-means

217

clustering algorithm for the number of clusters k. Cluster distances were based on county

218

population centroids c (lat/lon). For a single cluster aj with N counties and geographic cluster

219

centroid bj, we define the intra-cluster distance as the sum of squares

𝑎

220

𝑐

𝑏

221

where ci is the centroid of the i-th county of cluster j. The k-means algorithm was applied

222

while varying k over the interval [2, 60]. The total intra-cluster sum of squares (over all

223

clusters) stopped decreasing at approximately k=38. So we chose k=38. Smaller, sparsely

224

spaced clusters were removed by requiring 𝑁

225

The one exception to these rules is the Atlanta cluster. Most Atlanta counties do not pass

226

the incidence minimums for the 2016-2017 season. However, county coverage for 2017-18

227

and 2018-19 is the best of any large metropolitan area in the United States, so Altlanta was

228

included for these seasons.

229

This process resulted in 10 county clusters, covering 57 counties as described in Table S2.

4and 𝑎 /𝑁

0.5.

8

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

230

Mechanistic modelling framework

231

The following sections offer a summary of the model framework, covariate data collection,

232

and fitting procedure used for this work. For a more complete technical discussion of these

233

topics, see (10). These concepts were executed using the software package Dynamics of

234

Interacting Community Epidemics (DICE). DICE is a publicly available R-package available

235

for download with documentation and examples (see Data Availability).

236

The foundation of our model is the common Susceptible-Infectious-Recovered (SIR) set of

237

equations (21). We include terms to modulate the force of infection for specific humidity and

238

county coupling (10). The probability of inter-county transmission is evaluated using a

239

distance-based mobility kernel. Here, the distance between counties is calculated as the

240

Euclidean distance between counties’ population centroids.

241

The spatial aspect of the model is characterized in three ways: direct, uncoupled, and

242

coupled. The self-describing ‘direct’ fit matches the model directly to cluster data. When

243

applying the uncoupled model to a cluster, each constituent county is fit independently. The

244

individual county forecasts are then aggregated to produce a cluster forecast. The coupled

245

approach simultaneously fits the coupled model to all constituent counties.

246

Specific humidity data was taken from the National Centers for Environmental Prediction

247

(NCEP)/National Center for Atmospheric Research (NCAR) Reanalysis-2 project (22). This

248

provides climate model evaluations 1979-present on a 1.875° spatial grid. The reanalysis-2

249

spatial grid is layered with GADM (the Database of Global Administrative Areas) county

250

definitions (23) and the SocioEconomic Data and Applications Center (SEDAC) population

251

densities (24) to produce weekly population-weighted averages for specific humidity in each

252

county.

253

Fitting the models

254

Here we define incidence as the population count that was ILI symptomatic (TS) or influenza

255

infectious (PA) and presented to a reporting clinic for testing. Model equations are applied to

256

the full population, so incidence is returned from the model as a proportion of the new

257

infectious. This proportional scaling accounts for both the portion of the population covered

258

by reporting clinics and the proportion of the infectious population that sought medical

259

evaluation/treatment.

260

For each fit data series, we calculate the Log-Likelihood (LLK) that the data is a Poisson

261

expression of model values. We then use a Metropolis-Hastings type Markov Chain Monte

262

Carlo (MCMC) process (27) with adaptive step size to map the likelihood in parameter space.
9

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

263

When the coupled model is used, LLK of the cluster is calculated as a population-weighted

264

average of the individual county LLKs.

265

Forecast Scoring

266

For each forecast target, our model produces a distribution of outcomes. This distribution is

267

binned for evaluation based on the scoring system used by the CDC Influenza Challenge (8)

268

for the 2016-17, 2017-18 and 2018-19 seasons. The maximum range is separated into 130

269

equally sized bins with a 131st bin for values over the maximum previously observed. We

270

re-scaled the bins for total number of tests and the number of tests positive for influenza A so

271

that all three metrics peak in the same bin (Figure S1). Forecast probabilities for the

272

observed bin (red shading) as well as the 5 pre- and pro-ceeding bins (green shading) are

273

summed to produce forecast skill. Forecast score is the natural log of skill, with scores below

274

-10 set to -10. Note that due to (28) and the response (29), starting with the 2019-20

275

influenza season, the CDC Influenza challenge will define forecast skill as only the probability

276

in the observed bin. Here we retain the 11-bin skill window for consistency with matching

277

Influenza Challenge seasons.

278

Null Models

279

For a given season, cluster, week, and data metric, Null models were evaluated using data

280

from the same cluster, metric, and week, but different seasons. The most basic Null model

281

‘N.pt’ is a point forecast calculated as the mean of the Null data. The ‘N.d’ model is a

282

distribution fit to the same data. For intensity targets we use a log normal distribution and for

283

timing targets a normal distribution. However, both of these models suffer from the same

284

problem: due to the small number of seasons in the dataset, the model is typically being fit to

285

only 2 data points. The third Null model ‘N.d5’ was designed to mitigate this problem. For a

286

given forecast week, ‘N.d5’ uses the historic data from a 5-week window including the

287

forecast week as well as the 2 pre- and pro-ceeding weeks to fit a log normal distribution.

288

Ex. for epidemic week 4 of 2017, the data from epidemic weeks 2, 3, 4, 5, and 6 for years

289

2018 and 2019 are fit to a log normal. This distribution then gets to binned to serve as the

290

N.d5 model forecast.

291

Data availability statement

292

All weekly incidence data for the smallest spatial unit (counties) are available immediately

293

from the authors for non-commercial use in a format that can be used immediately with the

294

forecasting tools used to generate these results. The forecasting tools are available with

295

documentation and examples from https://github.com/predsci/DICE_quidel_manuscript.
10

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

296

[Note to reviewers and editors - this URL is for review only. At final publication, this will be

297

changed to the standard public URL for DICE].

298

Acknowledgements

299

We thank John Timerius (Quidel Inc) for the provision of these data and for helpful

300

discussions.

11

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

301

Figures

302
303

Figure 1. Incidence for each cluster for syndromic (Total Specimens, LHS) and

304

specimens positive for A, RHS).

305

Each row of pixels represents a single county with counties grouped into clusters (bounded

306

by tick marks) and then into seasons. Seasons are indicated by years on the y-axis (with a

307

blank row between years). Within seasons, clusters are ordered by latitude and within

308

clusters, counties are also ordered by latitude. Cluster peak weeks are marked with a gray

309

vertical line. In this figure, the number of reports for each week is normalized for each county

310

and year using the maximum number for that year (Legend, RHS).

12

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

311
312

Figure 2. Legend on next page.

13

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

313

Figure 2. Forecast scores lead times from 2- to 10-weeks ahead, for different specimen

314

types.

315

Pixel colour shows forecast score (see main text) for a given observation week averaged

316

across clusters and seasons, e.g. the 4-week ahead forecast for week 48 was made using

317

data only upto week 44. Averages across all weeks for a given model are printed on the RHS

318

of each row of pixels. Model type is shown on LHS y-axis tick labels: C.H, coupled model

319

with humidity modulated contact rate; C.F, coupled model with fixed contact rate; U.H,

320

uncoupled with humidity; U.F, uncoupled with fixed contact rate; D.H, model directly fitted to

321

cluster with humidity term; D.F, model directly fitted to cluster with fixed contact rate; N.pt,

322

null model made from simple model of that week for all other seasons; N.d, null model made

323

from fitting a log normal to all observations for that week from other years; and N.d5, null

324

model made from fitting log-normal to the observation week, two weeks prior and two weeks

325

following for all other years (see main text). Models are ordered approximately from least

326

complex on the bottom rows to most complex on the top row. See Figures S2 and S3 for

327

complete 1-10 weeks ahead forecast results.

14

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

328
329

Figure 3. Illustrative high-scoring and lower-scoring forecasts for the Colorado

330

cluster.

331

Here the incidence data is shown in black and shaded cones depict the central 50% forecast

332

windows for 1-4 week ahead. Forecasts were generated for every week, but only every third

333

forecast is shown here for clarity. Colour progression from left to right indicates time of

334

forecast. Solid lines show fit to data at the point the forecast was assumed to have been

335

made. Dashed lines show forecasts beyond the time at which data were assumed to be

336

available. Mean score is the average score across MMWR forecasted weeks and 1 to 4

337

weeks ahead targets.

15

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

338
339

Figure 4. Posterior parameter densities by season, cluster and model type.

340

Bars show central 95% of posterior credibility interval with symbol showing posterior mean.

341

Colour indicates model type and symbol shape indicates season. Values for each cluster are

342

grouped together as per x-axis tick labels.

16

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

343

References

344
345

1.

A. D. Iuliano, et al., Estimates of global seasonal influenza-associated respiratory
mortality: a modelling study. Lancet 391, 1285–1300 (2018).

346
347
348

2.

N. G. Reich, et al., A collaborative multiyear, multimodel assessment of seasonal
influenza forecasting in the United States. Proc. Natl. Acad. Sci. U. S. A. (2019)
https:/doi.org/10.1073/pnas.1812594116.

349
350
351

3.

B. J. Cowling, et al., Community Psychological and Behavioral Responses through the
First Wave of the 2009 Influenza A(H1N1) Pandemic in Hong Kong.
http://dx.doi.org/10.1086/655811 202, 867–876 (2010).

352
353

4.

M. E. Kitler, P. Gavinio, D. Lavanchy, Influenza and the work of the World Health
Organization. Vaccine 20, S5–S14 (2002).

354
355

5.

S. Gandon, T. Day, C. J. E. Metcalf, B. T. Grenfell, Forecasting Epidemiological and
Evolutionary Dynamics of Infectious Diseases. Trends Ecol. Evol. 31, 776–788 (2016).

356
357

6.

World Health Organization, “WHO public health research agenda for influenza 2017
update” (12/2017).

358
359

7.

C. J. McGowan, et al., Collaborative efforts to forecast seasonal influenza in the United
States, 2015-2016. Sci. Rep. 9, 683 (2019).

360
361

8.

M. Biggerstaff, et al., Results from the centers for disease control and prevention’s
predict the 2013-2014 Influenza Season Challenge. BMC Infect. Dis. 16, 357 (2016).

362
363
364

9.

L. C. Brooks, D. C. Farrow, S. Hyun, R. J. Tibshirani, R. Rosenfeld, Flexible Modeling of
Epidemics with an Empirical Bayes Framework. PLoS Comput. Biol. 11, e1004382
(2015).

365
366

10. M. Ben-Nun, P. Riley, J. Turtle, D. P. Bacon, S. Riley, Forecasting national and regional
influenza-like illness for the USA. PLoS Comput. Biol. 15, e1007013 (2019).

367
368

11. D. C. Farrow, et al., A human judgment approach to epidemiological forecasting. PLoS
Comput. Biol. 13, e1005248 (2017).

369
370

12. S. Kandula, W. Yang, J. Shaman, Type- and Subtype-Specific Influenza Forecast. Am.
J. Epidemiol. 185, 395–402 (2017).

371
372
373
374

13. J. Dunn, et al., Prompt detection of influenza A and B viruses using the BD VeritorTM
System Flu A+ B, Quidel® Sofia® Influenza A+ B FIA, and Alere BinaxNOW® Influenza
A&B compared to real-time reverse transcription-polymerase chain reaction (RT-PCR).
Diagn. Microbiol. Infect. Dis. 79, 10–13 (2014).

375
376

14. J. Bracher, On the multibin logarithmic score used in the FluSight competitions. Proc.
Natl. Acad. Sci. U. S. A. 116, 20809–20810 (2019).

377
378

15. N. G. Reich, et al., Reply to Bracher: Scoring probabilistic forecasts to maximize public
health interpretability. Proc. Natl. Acad. Sci. U. S. A. 116, 20811–20812 (2019).

379
380
381
382

16. D. R. Olson, K. J. Konty, M. Paladini, C. Viboud, L. Simonsen, Reassessing Google Flu
Trends data for detection of seasonal and pandemic influenza: a comparative
epidemiological study at three geographic scales. PLoS Comput. Biol. 9, e1003256
(2013).
17

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

383
384

17. P. Riley, A. A. Cost, S. Riley, Intra-Weekly Variations of Influenza-Like Illness in Military
Populations. Mil. Med. 181, 364–368 (2016).

385
386
387

18. H. V. Thornton, et al., Assessing the potential of upper respiratory tract point-of-care
testing: a systematic review of the prognostic significance of upper respiratory tract
microbes. Clin. Microbiol. Infect. (2019) https:/doi.org/10.1016/j.cmi.2019.06.024.

388
389
390

19. S. Pei, S. Kandula, W. Yang, J. Shaman, Forecasting the spatial transmission of
influenza in the United States. Proc. Natl. Acad. Sci. U. S. A. (2018)
https:/doi.org/10.1073/pnas.1708856115.

391
392

20. B. D. Dalziel, et al., Urbanization and humidity shape the intensity of influenza epidemics
in U.S. cities. Science 362, 75–79 (2018).

393
394

21. H. W. Hethcote, The mathematics of infectious diseases. SIAM Rev. 42, 599–653
(2000).

395
396

22. E. Kalnay, et al., The NCEP/NCAR 40-Year Reanalysis Project. Bull. Am. Meteorol. Soc.
77, 437–472 (1996).

397
398

23. G. A. Areas, GADM database of Global Administrative Areas, version 2.0. Berkeley, CA:
University of Berkeley (2012).

399
400

24. C. T. Lloyd, A. Sorichetta, A. J. Tatem, High resolution global gridded data for use in
population studies. Sci Data 4, 170001 (2017).

401
402

27. W. R. Gilks, S. Richardson, D. Spiegelhalter, Markov Chain Monte Carlo in Practice.
Chapman and Hall/CRC. (1995).

403
404

28. J. Brachner, On the Mulitbin Logarithmic Score used in the FluSight Competitions.
PNAS. 201912147 (2019).

405
406
407

29. N. G. Reich, D. Osthus, E. L. Ray, T. K. Yamana, M. Biggerstaff, M. A. Johansson, R.
Rosenfeld, J. Shaman, Reply to Brachner: Scoring Probabilistic Forecasts to Maximize
Public Health Interpretability. PNAS. 201912694 (2019).

408

18

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

409

Supporting Information

19

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

410
411

Figure S1. Similarity between total samples time series and influenza-like illness data

412

and bin-scoring design, illustrated for 2016-2017 season in the San Antonio / Austin

413

area cluster. Bins were scaled to each metric such that all peaks fall in the same bin. The

414

skillful bins for the peak week are shown in green shading. Forecast probabilities in these

415

bins, in addition to the observed bin (red shading), are summed to calculate skill. The

416

Centers for Disease Control and Prevention percentage ILI (CDC %ILI) is for the enclosing

417

CDC region (state of Texas).

20

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

418
419

Figure S2. Legend on next page.

21

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

420

Figure S2. Mean scores for Total Specimens forecasts 1-10 weeks ahead. Pixel colour

421

shows forecast score (see main text) for a given observation week averaged across clusters

422

and seasons, e.g. the 4-week ahead forecast for week 48 was made using data only upto

423

week 44. Averages across all weeks for a given model are printed on the RHS of each row of

424

pixels. Model type is shown on LHS y-axis tick labels: C.H, coupled model with humidity

425

modulated contact rate; C.F, coupled model with fixed contact rate; U.H, uncoupled with

426

humidity; U.F, uncoupled with fixed contact rate; D.H, model directly fitted to cluster with

427

humidity term; D.F, model directly fitted to cluster with fixed contact rate; N.pt, null model

428

made from simple model of that week for all other seasons; N.d, null model made from fitting

429

a log normal to all observations for that week from other years; and N.d5, null model made

430

from fitting log-normal to the observation week, two weeks prior and two weeks following for

431

all other years (see main text). Models are ordered approximately from least complex on the

432

bottom rows to most complex on the top row.

22

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

433
434

Figure S3. Legend on next page.

23

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

435

Mean scores for Positive-for-A forecasts 1-10 weeks ahead. Pixel colour shows forecast

436

score (see main text) for a given observation week averaged across clusters and seasons,

437

e.g. the 4-week ahead forecast for week 48 was made using data only upto week 44.

438

Averages across all weeks for a given model are printed on the RHS of each row of pixels.

439

Model type is shown on LHS y-axis tick labels: C.H, coupled model with humidity modulated

440

contact rate; C.F, coupled model with fixed contact rate; U.H, uncoupled with humidity; U.F,

441

uncoupled with fixed contact rate; D.H, model directly fitted to cluster with humidity term; D.F,

442

model directly fitted to cluster with fixed contact rate; N.pt, null model made from simple

443

model of that week for all other seasons; N.d, null model made from fitting a log normal to all

444

observations for that week from other years; and N.d5, null model made from fitting log-

445

normal to the observation week, two weeks prior and two weeks following for all other years

446

(see main text). Models are ordered approximately from least complex on the bottom rows to

447

most complex on the top row.

24

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

448
449

Figure S4. Peak target mean forecast scores for total specimens and positive for A.

450

The panels are divided by target (top - Peak Week, bottom - Peak Intensity) and data metric

451

(left - total specimens tested, right - specimens positive for A). Forecast scores for each

452

square are averaged across all clusters and all seasons. The mean value of each row

453

appears in the AVG column on the right. The first column of model abbreviations is

454

interpreted: C-coupled, U-uncoupled, D-direct, N-Null. The second column of abbreviation is:

455

H-humidity modulated contact rate, F-fixed contact rate, pt-point forecast, d-log normal

456

distribution.

25

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

457
458

Figure S5. Coupling parameter mean and central 80% interval. Results were compiled

459

from MCMC chains across all models and forecast weeks for a given cluster and season.

460

Coupling is modeled with the offset power law relationship described in (10) using free

461

parameters for saturation distance and power. Briefly, the probability of coupling occurring

462

across space is determined by a function with a plateau followed by a power law drop-off.

463

Saturation distance is the width of the plateau and ‘power’ is the power coefficient of the

464

drop-off.

26

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

465

Table S1. Pearson correlations between data metrics. Correlations are calculated for

466

each cluster and season, and comparing total specimens, positive for, A and CDC %ILI.

467

From the resulting distributions, we have extracted the median, 20%, and 80% quantiles.

468

27

medRxiv preprint doi: https://doi.org/10.1101/19012807; this version posted November 22, 2019. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
It is made available under a CC-BY 4.0 International license .

469

Table S2. County cluster information.

470

28

