---
title: "ODDPub Validation for SocArXiv and arXiv"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
library(tidyverse)
library(knitr)

arxiv_open_data_results <- 
  read_csv(here::here("outputs/data/arxiv_open_data_results.csv"))
# all_arxiv_results <- 
#   read_csv(here::here("outputs/data/arxiv_results.csv")) # All COVID arXiv papers (3,812)
arxiv_2019_open_data_results <- 
  read_csv(here::here("outputs/control-data/arxiv_2019_open_data_results.csv"))
# all_arxiv_2019_results <- 
#   read_csv(here::here("outputs/control-data/arxiv_data_2019.csv"))

# "attributes.has_data_links" gives indication of data availability
socarxiv_open_data_results <- 
  read_csv(here::here("outputs/data/socarxiv_open_data_results.csv"))
socarxiv_2019_open_data_results <- 
  read_csv(here::here("outputs/control-data/socarxiv_2019_open_data_results.csv"))
# socarxiv_metadata_2019 <- 
#   read_csv(here::here("outputs/control-data/socarxiv_metadata_2019.csv"))
```

# SocArXiv

## 2019

### Data

SocArXiv allows authors to input a link to their data source/repository upon submission of a pre-print. This link can then be accessed via the API metadata. The presence of a data link was used as an indicator that a pre-print provides open data for the purposes of validating the ODDPub algorithm. When available, a data link is stored under the variable name "attributes.data_links". The data was manipulated using functions from the R package tidyverse [@tidyverse] to create a binary variable indicating data availability or lack thereof. We assume "attributes.data_links" to indicate the true availability of data for the purposes of validating the ODDPub algorithm. It is possible, however, that some authors failed to indicate their data availability in the proper field upon posting to SocArXiv, and thus some of the false positive may in fact be true positives. This was not verified directly in order to allow us to more easily consider the whole sample in our validation.

Against the data availability indicated by pre-print authors in our 2019 sample, the ODDPub algorithm performed with an accuracy of 93 percent, a sensitivity of 52 percent, and a specificity of 94 percent. In our 2020 and 2021 sample, the algorithm performed with an accuracy of 79 percent, a sensitivity of 29 percent, and a specificity of 92 percent. Specific predictions are broken down in Table \@ref(tab:confusion-matrix-2019) and Table \@ref(tab:confusion-matrix-2020).

It is unclear the precise inclusion criteria for data submitted to the data link field. It is possible that some of the links provided lead to data sets that are publicly available for reuse, which would not constitute "open data" by the ODDPub algorithm's definition, in which case the accuracy could potentially be higher in reality than 93 percent and 79 percent in the samples considered.

```{r, confusion-matrix-2019}
# SocArXiv 2019 Confusion Matrix
soc_2019_check <- socarxiv_2019_open_data_results %>%
  mutate(data_link_available = case_when(is.na(attributes.data_links) ~ "No data linked", TRUE ~ "Data linked")) %>%
  count(data_link_available, is_open_data) %>%
  pivot_wider(names_from = data_link_available, values_from = n) %>%
  mutate(is_open_data = case_when(is_open_data == 1 ~ "Open data detected", TRUE ~ "No open data detected")) %>%
  rename(`ODDPub algorithm`= is_open_data) %>%
  arrange(desc(`ODDPub algorithm`))
soc_2019_check %>% kable(caption = "ODDPub predictions for open data compared with data links provided by authors, 2019 sample")
```

```{r, accuracy-2019}
soc_2019_accuracy <- (as.numeric(soc_2019_check[1,2] + soc_2019_check[2,3])/nrow(socarxiv_2019_open_data_results)) %>% round(2)
soc_2019_sensitivity <- (as.numeric(soc_2019_check[1,2])/sum(soc_2019_check$`Data linked`)) %>% round(2)
soc_2019_specificity <- (as.numeric(soc_2019_check[2,3])/sum(soc_2019_check$`No data linked`)) %>% round(2)

tibble(Metric = c("Accuracy", "Sensitivity", "Specificity"),
      Value = c(soc_2019_accuracy, soc_2019_sensitivity, soc_2019_specificity)) %>% kable(caption = "ODDPub prediction accuracy, 2019 sample")
```

```{r false-negatives-2019, eval = FALSE}
socarxiv_2019_open_data_results %>%
  filter(is_open_data == 0 & !is.na(attributes.data_links)) %>%
  select(title, links.preprint_doi, attributes.has_data_links, attributes.why_no_data, attributes.data_links, is_open_data, open_data_category, open_data_statements)
```


```{r false-positives-2019, eval = FALSE}
socarxiv_2019_open_data_results %>%
  filter(is_open_data == 1 & is.na(attributes.data_links)) %>%
  select(title, links.preprint_doi, attributes.has_data_links, attributes.why_no_data, attributes.data_links, is_open_data, open_data_category, open_data_statements)
```

### Code




## 2020/2021

Code was not manually verified for COVID-19 related papers.

```{r confusion-matrix-2020}
# SocArXiv 2019 Confusion Matrix
soc_2020_check <- socarxiv_open_data_results %>%
  mutate(data_link_available = case_when(is.na(attributes.data_links) ~ "No data linked", TRUE ~ "Data linked")) %>%
  count(data_link_available, is_open_data) %>%
  pivot_wider(names_from = data_link_available, values_from = n) %>%
  mutate(is_open_data = case_when(is_open_data == 1 ~ "Open data detected", TRUE ~ "No open data detected")) %>%
  rename(`ODDPub algorithm`= is_open_data) %>%
  arrange(desc(`ODDPub algorithm`))
soc_2020_check %>% kable(caption = "ODDPub predictions for open data compared with data links provided by authors, COVID-19-related pre-prints sample")
```

```{r accuracy-2020}
soc_2020_accuracy <- (as.numeric(soc_2020_check[1,2] + soc_2020_check[2,3])/nrow(socarxiv_open_data_results)) %>% round(2)
soc_2020_sensitivity <- (as.numeric(soc_2020_check[1,2])/sum(soc_2020_check$`Data linked`)) %>% round(2)
soc_2020_specificity <- (as.numeric(soc_2020_check[2,3])/sum(soc_2020_check$`No data linked`)) %>% round(2)

tibble(Metric = c("Accuracy", "Sensitivity", "Specificity"),
      Value = c(soc_2020_accuracy, soc_2020_sensitivity, soc_2020_specificity)) %>% kable(caption = "ODDPub prediction accuracy, COVID-19-related pre-prints sample")
```
```{r false-negatives-2020, eval = FALSE}
socarxiv_open_data_results %>%
  filter(is_open_data == 0 & !is.na(attributes.data_links)) %>%
  select(title, links.preprint_doi, attributes.has_data_links, attributes.why_no_data, attributes.data_links, is_open_data, open_data_category, open_data_statements)
```

```{r false-positives-2020, eval = FALSE}
socarxiv_open_data_results %>%
  filter(is_open_data == 1 & is.na(attributes.data_links)) %>%
  select(title, links.preprint_doi, attributes.has_data_links, attributes.why_no_data, attributes.data_links, is_open_data, open_data_category, open_data_statements)
```



# ArXiv

## 2019

We verified the accuracy of the ODDPub algorithm on a subset of our analyzed pre-prints from 2019 from arXiv. We took a simple random sample of 100 papers. In the original validation process, the annotators stratified by detection status prior to sampling to ensure relatively high representation of papers where open data/code was detected. Since the major concern for our manual verification is potential false negatives, this biased representation was unnecessary. Open data and code status were verified first via the "Code & Data" tab on each pre-print's page on the arXiv website, and then via checking for an explicit data availability section within the PDF and keyword search. Results were recorded manually in Excel (arxiv_2019_validation_sample.csv). This mimics the procedure outlined for the original validation of ODDPub.

Many of the pre-prints in arXiv did not use data or code, namely those from pure mathematics and physics. There were also several that reused other publicly or privately available data sets, and regardless of whether or not they were shared alongside the paper, these do not count as open data according to [ODDPub standards](https://www.biorxiv.org/content/10.1101/2020.05.11.088021v1.full.pdf).
```{r arxiv-2019-verification-sample, eval = FALSE}
set.seed(100)
arxiv_2019_sample <- arxiv_2019_open_data_results[sample.int(nrow(arxiv_2019_open_data_results), 100),] %>%
  select(id, title, link_pdf, link_abstract, is_open_data:open_data_statements) %>%
  mutate(file_path = paste0(id, ".pdf"), .before = id) %>%
  mutate(manual_open_data = NA, manual_data_statement = NA, manual_open_code = NA, manual_code_statement = NA, note = NA) # added for validation process
arxiv_2019_sample
# write_csv(arxiv_2019_sample, here::here("validation", "arxiv_2019_validation_sample.csv"))

open_pdf <- function(x){
  system2('open', args = here::here("outputs", "control-data", "pdf-arxiv", x))
}

open_link <- function(x){
  system2('open', args = x)
}

# Open pdfs in sets of 10 (run one at a time)
# lapply(arxiv_2019_sample$file_path[1:10], open_pdf)
# lapply(arxiv_2019_sample$link_abstract[1:10], open_link)
# 
# lapply(arxiv_2019_sample$file_path[11:20], open_pdf)
# lapply(arxiv_2019_sample$link_abstract[11:20], open_link)
# 
# lapply(arxiv_2019_sample$file_path[21:30], open_pdf)
# lapply(arxiv_2019_sample$link_abstract[21:30], open_link)
# 
# lapply(arxiv_2019_sample$file_path[31:40], open_pdf)
# lapply(arxiv_2019_sample$link_abstract[31:40], open_link)
# 
# lapply(arxiv_2019_sample$file_path[41:50], open_pdf)
# lapply(arxiv_2019_sample$link_abstract[41:50], open_link)
# 
# lapply(arxiv_2019_sample$file_path[51:60], open_pdf)
# lapply(arxiv_2019_sample$link_abstract[51:60], open_link)
# 
# lapply(arxiv_2019_sample$file_path[61:70], open_pdf)
# lapply(arxiv_2019_sample$link_abstract[61:70], open_link)
# 
# lapply(arxiv_2019_sample$file_path[71:80], open_pdf)
# lapply(arxiv_2019_sample$link_abstract[71:80], open_link)
# 
# lapply(arxiv_2019_sample$file_path[81:90], open_pdf)
# lapply(arxiv_2019_sample$link_abstract[81:90], open_link)
# 
# lapply(arxiv_2019_sample$file_path[91:100], open_pdf)
# lapply(arxiv_2019_sample$link_abstract[91:100], open_link)

```

```{r arxiv-confusion-data-2019}
arxiv_2019_manual_results <- read.csv(here::here("validation", "arxiv_2019_validation_sample_COMPLETE.csv"))

arxiv_2019_data_check <- arxiv_2019_manual_results %>%
  count(is_open_data, manual_open_data) %>%
  pivot_wider(names_from = manual_open_data, values_from = n) %>%
  mutate(is_open_data = case_when(is_open_data == 1 ~ "Open data detected", TRUE ~ "No open data detected")) %>%
  select(is_open_data, `1`, `0`) %>%
  rename(Predicted = is_open_data, `Data available` = `1`, `No data available` = `0`) %>%
  arrange(desc(Predicted))

arxiv_2019_data_check

```

```{r arxiv-confusion-code-2019}

arxiv_2019_code_check <- arxiv_2019_manual_results %>%
  count(is_open_code, manual_open_code) %>%
  pivot_wider(names_from = manual_open_code, values_from = n) %>%
  mutate(is_open_code = case_when(is_open_code == 1 ~ "Open code detected", TRUE ~ "No open code detected")) %>%
  select(is_open_code, `1`, `0`) %>%
  rename(Predicted = is_open_code, `Code available` = `1`, `No code available` = `0`) %>%
  arrange(desc(Predicted))

arxiv_2019_code_check
```

```{r arxiv-accuracy-data-2019}
arc_2019_accuracy <- as.numeric(arxiv_2019_data_check[1,2] + arxiv_2019_data_check[2,3])/nrow(arxiv_2019_manual_results)
arc_2019_sensitivity <- as.numeric(arxiv_2019_data_check[1,2])/sum(arxiv_2019_data_check$`Data available`) %>% as.numeric()
arc_2019_specificity <- as.numeric(arxiv_2019_data_check[2,3])/sum(arxiv_2019_data_check$`No data available`)

tibble(Metric = c("Accuracy", "Sensitivity", "Specificity"),
      Value = c(arc_2019_accuracy, arc_2019_sensitivity, arc_2019_specificity))
```
```{r arxiv-accuracy-code-2019}
arc_2019_code_accuracy <- as.numeric(arxiv_2019_code_check[1,2] + arxiv_2019_code_check[2,3])/nrow(arxiv_2019_manual_results)
arc_2019_code_sensitivity <- as.numeric(arxiv_2019_code_check[1,2])/sum(arxiv_2019_code_check$`Code available`) %>% as.numeric()
arc_2019_code_specificity <- as.numeric(arxiv_2019_code_check[2,3])/sum(arxiv_2019_code_check$`No code available`)

tibble(Metric = c("Accuracy", "Sensitivity", "Specificity"),
      Value = c(arc_2019_code_accuracy, arc_2019_code_sensitivity, arc_2019_code_specificity))
```